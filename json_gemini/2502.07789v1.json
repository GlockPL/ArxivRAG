{"title": "Do AI assistants help students write formal specifications? A study with ChatGPT and the B-Method.", "authors": ["Alfredo Capozucca", "Daniil Yampolskyi", "Alexander Goldberg", "Maximiliano Cristi\u00e1"], "abstract": "This paper investigates the role of AI assistants, specifically OpenAI's ChatGPT, in teaching formal methods (FM) to undergraduate students, using the B-method as a formal specification technique. While existing studies demonstrate the effectiveness of AI in coding tasks, no study reports on its impact on formal specifications. We examine whether ChatGPT provides an advantage when writing B-specifications and analyse student trust in its outputs. Our findings indicate that the AI does not help students to enhance the correctness of their specifications, with low trust correlating to better outcomes. Additionally, we identify a behavioural pattern with which to interact with ChatGPT which may influence the correctness of B-specifications.", "sections": [{"title": "I. INTRODUCTION", "content": "We are part of the community that believes it is important to teach formal methods (FM) to computer scientists [1] and that FM should be taught at the undergraduate level [2]. One possibility is to teach FM for formalising requirements. This can be taught to undergraduate students who are already fa- miliar with the fundamental concepts of software engineering. By the end of the course students should be able to write the formal specification for some given requirements.\nIt is a fact that students in higher-education use AI assis- tants [3], [4]. As educators, we are concerned with the level of performance that AI assistants may have in writing formal specifications, and in particular, how this may impact expected learning outcomes.\nEvidence has shown that AI assistants can correctly com- plete coding tasks from introductory programming courses with minimal effort [5]\u2013[7]. However, to the best of our knowledge, no study has investigated the degree to which AI assistants can provide help for novice students with writing formal specifications. Knowing if AI assistants are able to produce correct formal specifications is highly relevant for courses.\nIn this paper we examine if using OpenAI's ChatGPT [8] (referred to as the AIA) provides an advantage to undergrad- uate students when tasked with writing formal specifications using the B-method [9]. Moreover, we analyse what types of interactions lead to the maximisation of this advantage. In ad- dition, we investigate whether students perceive an advantage when using the AIA to write their B-specifications. To do this, we designed a user study, inspired by the work of Perry et al. [10].\nWe designed the study to be coupled with a course that reunites all the required characteristics, but without causing any disruptions to its delivery and minimising any overhead for instructors. This not only ensures that participants represent the main targeted population, but also eases its reproducibility. We centred our study on three research questions:\n\u2022 RQ1: Does the AIA help students to write B- specifications?\n\u2022 RQ2: Do students trust the AIA to write B-specifications?\n\u2022 RQ3: How do students' behaviour when interacting with the AIA affect the degree of correctness of their B- specification?\nWe found that using the AIA to write B-specifications does not help students perform better than when it is absent. Nevertheless, there is some, albeit weak, evidence that the AIA may help students specify B-abstract machine's state variables and find (but not write) the machine's operations (Section IV-A). We also found that most students had a low level of trust in the AIA. This lack of trust confirms that little advantage can be obtained from using the AIA. Students who expressed low level of trust are correlated with those who produced better B-specifications. (Section IV-B). The analysis of the participants' interactions with the AIA suggests the existence of a behavioural pattern that may lead to an enhancement of the correctness of the B-specification (Section"}, {"title": "II. BACKGROUND & RELATED WORKS", "content": "A. The B-method\nThe B-Method [11] is a formal method belonging to the category of state-based formalisms. The central idea of this method is to use the notion of an abstract machine to specify the possible states of a system. The transitions from one state to another are defined by the machine's operations. An abstract machine contains invariants, which describe properties of interest over the machine. A machine is said to be consistent when the invariant holds in every state. The B-language is used to specify the abstract machine, its invariants, and operations. The B-method also includes a process known as refinement, which aims to gradually transform an abstract machine into a concrete implementation, while ensuring that the invariants are preserved at each step of the transformation. The B-method is a success story for the FM community, not only for the research progress it has brought to the field but also for its proven track record in the industrial sector [12].\nB. AIA and FM in education.\nWe did not (at the time of writing this article) find any work reporting on how AIAs can help students to write formal specifications, or how it can be used to support learning of FM within computer science education.\nThe closest work we found reporting on how AIAs can support FM is related to formal program verification [13]. This work analyses the capacity of ChatGPT (same AIA as ours) to generate valid and useful loop invariants for C programs. This work deals with aspects of FM related to formal verification of code (C programs, in this case), whereas our objective is to train students to formally specify requirements. Clearly, two different use cases of FM."}, {"title": "III. METHODS", "content": "This section outlines the study design by detailing the composition of the participant pool, the procedure to assess the correctness of B-specifications, the activities performed by participants, and the questions they were asked. Moreover, the procedure used when analysing the participants' prompts is described. Finally, this section also includes a comprehensive explanation of the experimental protocol, in order to ensure the reproducibility of the study, along with a presentation of the study's compliance with ethical regulations.\nA. Study design\nOur study is designed as a single group pretest-posttest quasi-experiment [14]. In both tests we measure each partic- ipant's capacity to write formal specifications using the B- Method. This capacity is determined based on the correctness level (dependent variable) of the B-specification submitted by each participant as a result of having performed a guided activity. The criterion used to determine the correctness level of a B-specification is detailed in Section III-C.\nBefore the pretest takes place, each participant receives training on formal requirements specification using the B- Method. This training is part of a course from which we recruit the participants for the study. Details about the course and participants are given in Section III-B.\nThe B-specification, used to measure the level of correctness during the pretest, is written by each participant using the Atelier B tool\u00b9 and without any assistance other than the class notes, examples, and reference documents provided during the training.\nThe treatment we provide to participants consists in utilising the AIA to write a subsequent B-specification. This AIA is provided as a new resource that participants can use on top of the previously available ones; the Atelier B tool, class notes, examples, and reference documents provided during the training. This second B-specification is the result of performing a second guided activity, but it is similar in terms of objectives and complexity to the first one. Guided activities are described in Section III-D.\nThe second B-specification is used to measure the level of correctness corresponding with the posttest. The variance of the correctness scores between the pretest and posttest is used to determine to what extent the AIA helps to write B- specifications, in line with RQ1.\nWe ask participants to answer a questionnaire immediately following the submission of their second B-specification. This structured questionnaire (detailed in Section III-E) is designed to understand the participant's perception in terms of (p.1) how helpful the AIA is for specifying a B-specification, (p.2) their level of confidence that the submitted B-specification is correct, and (p.3) how much of this confidence is due to the help provided by the AIA during the process of writing the actual B-specification.\nThe degree to which the participant perceived the AIA to be useful (i.e. part (p.1) of the questionnaire) is compared with the level of correctness of the submitted solution. This allows us to correlate how the believed help of the AIA relates with the correctness of the submitted B-specification. These findings are used to answer RQ1.\nParts (p.2) and (p.3) of the questionnaire seek to ascertain the participant's level of confidence in the correctness of the provided B-specification, and how much of this confidence is due to the help of the AIA. The answers collected in these parts of the questionnaire are used to determine to what extent the participants trust the help provided by the AIA when writing a B-specification, thus contributing to answering RQ2.\nAlong with the B-specification, we ask each participant to submit their interactions with the AIA. These interactions contain both the prompt entered by the participant and the response generated by the AIA. Knowing how each participant has interacted with the AIA during the process of writing the B-specification allows us to recognise how much of an AIA- generated answer was reused by a participant to produce the submitted B-specification. This metric provides quantitative"}, {"title": "F. Prompt analysis", "content": "We adapt Perry et al. [10]'s taxonomy to categorise the prompts provided by participants. This taxonomy is described as follows:\n\u2022 SPECIFICATION: user provides a natural language description of requirements to be specified (e.g. ''menu is a collection of dishes that are available in the food truck (ex.: cheese burrito, chicken burrito, etc)'').\n\u2022 INSTRUCTION: user indicates to the AIA what B-language instruction to use (e.g. ''for the recordSale operation we can use SELECT instead of the normal IF THEN.'').\n\u2022 QUESTION: user asks the AIA a question (e.g. 'but what do you mean by the dot ?'').\n\u2022 LANGUAGE: user indicates the target language to use (e.g. ''write me a b specification for ...'').\n\u2022 TEXT CLOSE: normalised edit distance between prompt and question text\u00b9 is less than 0.25 (i.e. the strings are very similar).\n\u2022 MODEL CLOSE: normalised edit distance between prompt and the previous AIA's output is less than 0.4 (i.e. the strings are similar).\n\u2022 HELPER: prompt includes examples of B-specifications.\n\u2022 PROBLEM: user reports a problem with some parts of a B-specification (e.g. \u2018\u2018I get an error in Atelier b saying ... \u2032 \u2032).\n\u2022 WARMUP: prompt includes contextual information.\n\u2022 ROLE: user instructs the AIA to play a particular role.\n\u2022 COMMAND: user provides a command that is informa- tive for the AIA (e.g. ``No, it should not be 1, it should be the same as before.'').\nThe process of categorising the prompts is done manually. It was executed by 3 independent researchers, who met at the end of the process to reach a consensus on the final categorisation. Notice that a same prompt may belong to multiple categories.\nHaving categorised prompts, allows us to calculate the average position of a type of prompt (i.e. category) over the dialogue of a particular participant. A position ranges from 0 to 1, where 0 is the beginning of the dialogue and 1 is the end of the dialogue. Thus, a prompt type with an average position falling in the range [0,0.33] means that prompts of this category are likely to be used at the beginning of the dialogue. Conversely, a type of prompt with average position in [0.66, 1] indicates that prompts in this category are used more towards the end.\nKnowing the average position of each type of prompt for each participant allows us to determine the overall position distribution of each type of prompt over participants. The distribution of the categories over the dialogue period allows"}, {"title": "G. Protocol", "content": "To ensure the reproducibility and clarity of our study, we provide a step-by-step description of the protocol used to run the experiment.\n(1) Pre-settings: Our participants are a sub-set of the students following the undergraduate course where the study is embedded. By the time the experiment is performed, each student (regardless of whether they want to participate in the study or not) has already completed the first guided activity as it is part of the course. The B-specifications submitted by students are assessed for correctness. The level of correctness of this specification represents 30% of the student's final mark in the course. This provides sufficient motivation to ensure that every student who intends to pass the course participates in this guided activity. However, it is worth noting that for the sake of the study, we retain only the correctness assessment of students who agree to participate in the study.\n(2) Info session: We present in 30 minutes what the study is about, how it is done, and how personal data will be processed. We clearly explain to the students that to participate in the study they must have a registered account to use the AIA. During the presentation we distribute the information notice and authorisation form that every student has to return duly signed to be able to participate in the study. We give students the time until the experiment starts (2 weeks later) to decide whether to participate or not.\n(3) Experiment kick off: We recall the objectives of the experiment while collecting the duly signed agreement forms. We present the questions to be answered in the questionnaire"}, {"title": "H. Ethics", "content": "We received the approval of our institution's ERP to perform the study. Students signed consent forms prior to the experi- ments, ensuring they were aware of how their personal data would be processed and used. Students were also informed that not participating in the study would not represent a disadvantage. We made explicit in the study's information notice that the learning outcomes to be acquired during the course are still guaranteed regardless of their participation in the study."}, {"title": "IV. RESULTS", "content": "A. RQ1: Does the AIA help students to write B-specifications?\nTable II presents a summary of the level of correctness of the submissions. It reveals a decay in performance between the pretest and posttest, which appears in both iterations. This is a first hint towards the idea that the AIA does not help the students perform better.\nNext, we zoom in from the overall performance of partici- pants to investigate whether the AIA made a difference in any of the correctness assessment criterion dimensions. Figure 1 shows how participants performed in each dimension during the pretest and posttest. The results show that no increase in performance was observed in any dimension, in terms of median value, when using the AIA. However, it is worth noticing that iteration 2's results show not only an overall close median performance between the pretest and posttest, but also a better performance of some participants when the AIA was"}, {"title": "V. DISCUSSION", "content": "A. Suggestions to educators\nThe findings show that, as of today, the AIA cannot generate \"good\" B-specifications. The same findings, however, show that the AIA may provide some help during the process of creating the B-specification, but the correctness level of the final B-specification largely relies on the knowledge and skills of the student. This means that allowing students to use the AIA when writing B-specifications does not represent a risk for determining whether the expected learning outcomes have been acquired (assuming they are solely based on the correct- ness of the provided B-specification). Therefore, instructors are not required to invest time and effort into constructing complex assessment environments where access to the AIA is prohibited while other, necessary, third-party services remain accessible. Instructors may also use this information to warn students about the risks of blindly trusting AIA-generated answers without having mastered the concepts of the B- method and the B-language.\nFindings also show that the level of help provided by the AIA is increasing. This means that it may eventually generate \"good\" B-specifications. Once this arrives, relying only on the correctness of the B-specification will no longer suffice for judging whether a student has acquired the expected learning outcomes. Instructors may want to run the study in their own course as a way of monitoring the progress of the AIA towards its assessment criterion. Knowing how close the AIA gets to a minimal passing mark may help them adjust the course's assessment criterion, the settings of the assessment environment, or both.\nB. Suggestions to students\nThe findings provide insights into the types of prompts that contribute to obtaining \"good\" B-specifications. These insights also indicate at which moment certain types of prompts should be used to eventually reach a \u201cgood\u201d B-specification. Needless to say, the benefits a student may gain from the AIA is directly"}, {"title": "VI. THREATS TO VALIDITY", "content": "One important limitation of our study is the sample size of each iteration. Given the small sample size, each iteration of the study has low statistical power to detect smaller effects. This means that the AIA might actually have had a small effect on specifying correct B-specifications, but due to insufficient statistical power it was not detected. Due to the context where the study is embedded, it is quite unlikely to get a sample size large enough to detect small effects. Thus, rather than making decisions on evidence from a small single group, we decided to do two independent executions of the study to base our findings in trends appearing on both executions.\nAs sample sizes are not expected to grow, it would be hard to introduce a control group to the experiment. Without this group, it is difficult to conclude if the effects detected between the pretest and posttest are due to the use of the AIA. We made efforts to ensure that the same conditions apply (assessment criteria, complexity of the activities, and evaluators) to the pretest and posttest, such that effects may be attributed only to the treatment (i.e. use of the AIA). Nevertheless, it is worth proportional to the pre-acquired knowledge and developed skills on the B-method and B-language.\nThe findings show that students who included portions of AIA-generated answers, either entirely or partially, in their B- specifications performed worse than those who used little to no AIA-generated text. This is evidence that the AIA should be used with care and mainly in settings in which a certain level of mastery has already been acquired."}, {"title": "VII. CONCLUSION", "content": "Our study highlights the limited impact of OpenAI's Chat- GPT on undergraduate students' ability to write formal speci- fications using the B-method. While students expressed low trust in the AI, which correlated with better performance,"}]}