{"title": "Commonsense Knowledge Editing Based on Free-Text in LLMs", "authors": ["Xiusheng Huang", "Yequan Wang", "Jun Zhao", "Kang Liu"], "abstract": "Knowledge editing technology is crucial for maintaining the accuracy and timeliness of large language models (LLMs). However, the setting of this task overlooks a significant portion of commonsense knowledge based on free-text in the real world, characterized by broad knowledge scope, long content and non instantiation. The editing objects of previous methods (e.g., MEMIT) were single token or entity, which were not suitable for commonsense knowledge in free-text form. To address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduced Knowledge Localization for Free-Text(KLFT) method, revealing the challenges associated with the distribution of commonsense knowledge in MLP and Attention layers, as well as in decentralized distribution. Next, we propose a Dynamics-aware Editing Method(DEM), which utilizes a Dynamics-aware Module to locate the parameter positions corresponding to commonsense knowledge, and uses Knowledge Editing Module to update knowledge. The DEM method fully explores the potential of the MLP and Attention layers, and successfully edits commonsense knowledge based on free-text. The experimental results indicate that the DEM can achieve excellent editing performance. The code and dataset file in URL 1.", "sections": [{"title": "1 Introduction", "content": "Large-scale Language Models (LLMs) have demonstrated remarkable performance in various natural language processing tasks (Huang et al., 2021, 2022). Nevertheless, errors or outdated knowledge are inevitable in LLMs (Meng et al., 2022a). Directly fine-tuning a large language model demands significant computational resources (Gupta et al., 2023), making it economically prohibitive and limiting its popularity as a preferred approach (Ding et al., 2023).\nKnowledge editing serves as an effective approach to update LLMs. Existing knowledge editing methods predominantly concentrate on editing triple-based facts such as entity-relation pairs (Meng et al., 2022b), events (multiple triplets) (Peng et al., 2024; Liu et al., 2024). These approaches commonly utilize strategies involving neuron localization and editing (Meng et al., 2022a; Ju et al., 2023), assuming that entities and phrases within factual triplets are stored in a limited set of neurons. By manipulating these select neurons, knowledge editing can be accomplished. As shown in Figure 1, factual knowledge editing involves rectifying outdated triplets like <America, President, Trump> to accurate ones like <America, President, Biden>.\nHowever, in real-world scenarios, structured entity-relation triplets often fall short in adequately describing many knowledge pieces, especially when it comes to commonsense knowledge edge (Hwang et al., 2021). The data characteristics of commonsense knowledge are broad knowledge scope, long content and non instantiation, which limits the effectiveness of traditional knowledge editing methods. In addition, when using LLMs, users often need to obtain commonsense knowledge in the form of free-text, rather than structured entity level information. This user preference indicates that commonsense knowledge editing based on triplet forms does not meet their needs. Therefore, we propose a more challenging commonsense knowledge editing task based on free-text, which has wider practicality.\nCompared to previous methods, commonsense knowledge editing based on free-text presents some new challenges, as shown below: (1) The previous knowledge localization methods (e.g. Causal Tracing (Meng et al., 2022a)) typically used the probability value of the editing target as the response value of the knowledge storage location. The success of this method is based on the fact that the editing target is a single token or entity. However, the editing target of commonsense knowledge based on free-text editing has multiple tokens, which limits the effectiveness of previous methods. (2) Previous knowledge editing methods typically assumed that factual knowledge was stored on a single or small number of neurons, and knowledge editing could be achieved through operations on a small number of neurons. However, the experiments conducted in Section 3 indicate that commonsense knowledge based on free-text does not conform to this assumption. Commonsense knowledge based on free-text has a wide range of storage locations, is more dispersed, and is less prone to localization. Therefore, previous knowledge editing methods are insufficient for handling commonsense knowledge editing based on free-text.\nTo address the aforementioned challenges, we conducted experiments from two perspectives: knowledge localization and knowledge editing. Firstly, we introduce a Knowledge Localization for Free-Text(KLFT) method that include knowledge location and recall. Specifically, knowledge localization experiments are utilized to determine whether commonsense knowledge is stored in the local hidden states of transformers, as well as to explore the form of storage. The knowledge recall experiment is used to verify whether specific hidden states storing commonsense knowledge have a significant contribution to that knowledge. Two experiments together indicate that, in comparison to triple facts, commonsense knowledge predominantly resides in the MLP layers and Attention (Attn) layers, the storage of knowledge is not local but rather dispersed throughout. This means that the previous editing methods (e.g., editing local layers in ROME(Meng et al., 2022a) and PMET (Li et al., 2024)) were unreasonable.\nSecondly, we propose a Dynamics-aware Editing Method(DEM). Specifically, we introduce a Dynamic-aware module for real-time detection of the storage location of each commonsense knowledge, and selected the layer with the highest contribution to knowledge as the editing layer. Subsequently, we employ a Knowledge Editing module to perform targeted knowledge editing on specific MLP and Attn layers. The experimental results validated the effectiveness of the method.\nTo address the issue of insufficient commonsense knowledge datasets for editing based on free-text, we have developed Commonsense Knowledge Editing Benchmark (CKEBench). This dataset has 15600 samples and six evaluation indicators, which is more challenging than the existing dataset. To the best of our knowledge, we are the first to introduce an Commonsense Knowledge Editing Benchmark. Additionally, we investigate the storage and recall of commonsense knowledge and propose an effective editing method. Our contributions can be summarized as follows:\n\u2022 We constructed a Commonsense Knowledge Editing Benchmark (CKEBench) dataset that provides a benchmark for editing Commonsense knowledge based on free-text.\n\u2022 Through Knowledge Localization for Free-Text (KLFT), we found that compared to triple facts, commonsense knowledge predominantly resides in the MLP layers and Attn layers, the storage of knowledge is not local but rather dispersed throughout.\n\u2022 To edit commonsense knowledge based on free-text, we propose a Dynamics-aware Editing Method(DEM). Specifically, the DEM includes a Dynamic-aware Module and a Knowledge Editing Module. The experimental results validated the effectiveness of the method."}, {"title": "2 Constructing CKEBench Dataset", "content": "In this section, we constructed an Commonsense Knowledge Editing Benchmark(CKEBench). This datasets consist of 15,600 samples."}, {"title": "2.1 Dataset Construction", "content": "Based on the ATOMIC (Sap et al., 2019) database, we constructed a Commonsense Knowledge Editing Benchmark(CKEBench). ATOMIC is a well-known commonsense database that was developed by Allen Institute and subsequently optimized for its version (Hwang et al., 2021). The CKEBench contains 23 types of relationships and describes commonsense knowledge based on free-text, they fall into three natural categories based on their meaning: physical-entity, social- interaction and event-centered commonsense."}, {"title": "2.2 Dataset Preparation", "content": "In ATOMIC, the data format is < Event1, Relationship, Event2>, which contains some unrecognized markers (e.g. _, etc.) and invalid characters (e.g. &, etc.), which we manually filter out. In addition, the relationship types in ATOMIC are abbreviated and not easily understood by humans. Even if ATOMIC provides corresponding annotations, it is still not enough to form a smooth statement when constructing the prompt. as shown in the Appendix A, we have rewritten the 23 relationship categories in ATOMIC into templates that can be read by humans and counted their sample sizes. Afterwards, we will use the reorganized dataset dataset as the initial data to construct the CKEBench dataset."}, {"title": "2.3 Dataset Analysis", "content": "After filtering and rewriting, we obtained a total of 15600 high-quality samples, of which \"xAttr\" had the highest number of samples, totaling 3224. The average length of \"Commonsense Prompt\" is 72 tokens, and the average length of Target Answer is 16 tokens. After testing on LLaMA-3 (8B) (Touvron et al., 2023a), the Perplexity (PPL) of the dataset is 7.3, indicating that the text of the entire dataset is smoother and the quality of the dataset is higher. The appendix C shows an example."}, {"title": "3 Knowledge Localization for Free-Text", "content": "To locate commonsense knowledge based on free-text within LLMs, we propose a Knowledge Localization for Free-Text (KLFT) method, which involves two experiments : knowledge location and recall."}, {"title": "3.1 KLFT Method", "content": "Inspired by causal tracing (Meng et al., 2022a), we adopt KLFT method to explore the way knowledge is stored. Similar to the causal tracing, a clean run that predicts the fact, a corrupted run where the prediction is damaged, and a corrupted-with-restoration run that tests the ability of a single state to restore the prediction.\n\u2022 In the clean run, we pass a commonsense prompt $x=[x_1,...,x_T]$ into model $F_\\theta$ and collect all hidden activations ${h_i \\in \\mathbb{R}^{L\\times T}, l \\in [1, L]}$, L represents the number of hidden layers in the model. Table 1 provides an Sample 1 illustration with the commonsense prompt: \" PersonX about to get married, as a result, PersonX wants to\", the expected target answer is \"live happily ever after\".\n\u2022 In the corrupted run, there are 23 relationship categories in the CKEBench. We consider the text before the relationship as the subject, and the text after the relationship as the object. The subject is obfuscated from $F_\\theta$ before the network runs. Concretely, immediately after $x$ is embedded as $[h_1, h_2, ..., h_T]$, we set $h'_i = h_i + \\delta$ for all indices $i$ that correspond to the subject entity, where $\\delta \\in N(0, \\sigma^2)$. $F_\\theta$ is then allowed to continue normally, giving us a set of corrupted activations ${h'_i \\in \\mathbb{R}^{L\\times T}, l \\in [1, L]}$. Because $F_\\theta$ loses some information about the subject, it will likely return an incorrect answer.\n\u2022 In the corrupted-with-restoration run, we have the $F_\\theta$ run calculations on noise embeddings, except in some tokens $x_l$ and layers $l'$. Afterwards, we hook $F_\\theta$ and forced it to output clean state $h_l$. Future calculations can continue without intervention. Afterwards, The ability of a few clean states to restore correct facts afterwards indicates their importance in the calculation graph.\nThe probability value $P_\\theta$ of restoring the target answer will be used as the contribution of this layer $l'$ to common sense knowledge. The larger $P_\\theta$, the greater the probability that commonsense knowledge is stored in this layer. For commonsense knowledge based on free-text, the target answer is"}, {"title": "3.2 Knowledge Location", "content": "3.2.1 Locating commonsense knowledge before decoupling\nWe compared the differences between factual and commonsense knowledge in storage locations by KLFT method. As show in the Figure 2, the fact prompt is \"Beats Music is owned by\", the target answer is \"Apple\", the commonsense knowledge is sample 3 in Table 1. The horizontal axis represents the layers in LLMs, and the vertical axis represents the tokens $x_i$ of different knowledge. The depth of color is determined by $P_\\theta$, and the larger $P_\\theta$, the darker the color, indicating a higher probability of storing knowledge in that layer.\nUnlike factual knowledge, which is typically stored in fixed MLP layers(Meng et al., 2022a), commonsense knowledge is not limited to specific layer neurons. Evidence of storage can be observed in both the MLP and Attn layers.\n3.2.2 Locating commonsense knowledge after decoupling\nCommonsense knowledge is non instantiation and is often abstractly represented. By contrast, facts are usually instanciated. To more accurately locate commonsense knowledge and decouple it from factual elements, we perform multiple same-type text replacements for the factual elements that may be contained in free text. For example, we replace \"personX\" in free-text with multiple person names and take the intersection of the located results.\nAs shown in Figure 3, we obtained the storage situation of commonsense knowledge decoupled from factual knowledge (The \"Mean\" column). Unlike factual knowledge, which is stored in the middle and front layers of MLP in LLMs, we found that commonsense knowledge is dispersed in the MLP and Attn layers, which poses a challenge for commonsense knowledge editing.\n3.2.3 Locating commonsense knowledge of the entire dataset\nWe conducted KLFT experiment on each relationship category, selecting 100 samples for each relationship category, totaling 2300 samples. The experiment selected top k=3 layers as the storage location of knowledge. As shown in the Figure 4, the storage location of the MLP layer is mainly in the middle and front layers, but other layers also store some knowledge. Unlike the experimental results of MLP, the knowledge storage in the Attn layer is relatively scattered, with most layers storing knowledge."}, {"title": "3.3 Knowledge Recall", "content": "To verify the conclusions of commonsense knowledge based on free-text in localization, we recorded the contribution of MLP and Attn layers to knowledge during the recall process.\nExperimental design. After passing through each layer of parameters in the model, the information flow undergoes certain changes, which we consider as an indicator to evaluate the contribution of parameter layers to knowledge. We hook model $F_\\theta$ and obtain the hidden states ${h^{in}_l, h^{out}_l \\in [1,L]}$. Specifically, we directly compare the hidden states $h^{in}_l$ and $h^{out}_l$ passing through the $l$-th parameter layers, utilizing cosine similarity as the evaluation metric. At the same time, we utilize the $h^{in}_l$ and $h^{out}_l$ as the input for the final prediction $lm\\_head$ layer of the model, then obtain the corresponding predicted token probabilities $p^{in}_l$ and $p^{out}_l$. We take tokens with top k=50 as candidate sets, and use the Simpson algorithm to calculate the similarity between the $p^{in}_l$ and $p^{out}_l$.\nSimpson Similarity $= \\frac{\\mid P^{\\prime}_{in} \\cap P^{\\prime}_{out} \\mid}{min(P^{\\prime}_{in}, P^{\\prime}_{out})}$ (1)\nData selection. For factual and commonsense knowledge, we selected 1150 samples each to explore the process of knowledge recall. Among them, there are a total of 23 relationship categories for commonsense knowledge, with 50 samples selected for each relationship category. We assume that the similarity is inversely proportional to the contribution of corresponding knowledge. When the similarity is close to zero, it indicates that the layer has the greatest impact on knowledge during the knowledge recall process.\nResult analysis. As shown in the Figure 5, The similarity is close to zero, indicating that the parameter layer is not very helpful for the final prediction. For the MLP layer, the similarity of factual knowledge is much greater than zero in the middle part and close to zero in the rest, while the similarity of commonsense knowledge is only close to zero in the middle and front parts. For the Attn layer, the similarity between factual knowledge and commonsense knowledge is close to zero at most layer, but there is also a certain difference in values. The experimental results show that the localization results of the KLFT method are consistent with the parameter layer response of knowledge recall process. For commonsense knowledge based on free-text, which is mainly stored in the middle and front layers of MLP as well as most Attn layers."}, {"title": "4 Dynamics-aware Editing Method", "content": "To edit commonsense knowledge based on free-text, we propose a Dynamics-aware Editing Method(DEM). Specifically, the DEM includes a Dynamics-aware Module and Knowledge Editing Module."}, {"title": "4.1 Dynamics-aware Module", "content": "Through section 3, we conclude that unlike factual knowledge, commonsense knowledge is stored in the MLP and Attn layers, and the storage locations of knowledge are relatively scattered. The existing knowledge editing methods always edit all factual knowledge at fixed parameter layer. For example, when editing all samples on GPT-J (6B) (Wang and Komatsuzaki, 2021) model, the edited layers for the ROME (Meng et al., 2022a) and PMET (Li et al., 2024) methods are fixed [5] and [3,4,5,6,7,8], respectively, which is obviously unreasonable for editing commonsense knowledge.\nAs shown in the Figure 6, we propose a Dynamics-aware module for selecting MLP and Attn layers for editing. When commonsense prompts $x = [x_1,..., x_T]$ input to model $F_\\theta$, the information flow will change after passing through parameters layer. We hook $F_\\theta$ to obtain the last token's hidden state ${h^{(T)in}_l, h^{(T)out}_l|l \\in [1, L]}$. The $h^{(T)in}_l$ and $h^{(T)out}_l$ represent the hidden states of the token's input and output in $l$-th layer, respectively. Then we utilize Cosine Similarity as an indicator for selecting editing layers:\nCosine\\_Similarity = $\\frac{h^{(T)'in} \\cdot h^{(T)'out}}{\\mid\\mid h^{(T)in} \\mid\\mid \\cdot \\mid\\mid h^{(T)out} \\mid\\mid}$ (2)\nthe closer the Cosine Similarity is to zero, the greater the contribution of this layer to knowledge. Select layers with top k=3 for editing."}, {"title": "4.2 Knowledge Editing Module", "content": "We edit the selected layers $\\hat{l}$ of the dynamic perception module in section 4.1. For a given question $x = [x_1,..., x_T]$, where $x_i$ represents the $i$-th token of the question, and $T$ represents the number of question tokens. The model $F_\\theta$ generates text by iteratively sampling from a conditional token distribution $P(\\theta_1, ..., \\theta_n|x_1, ..., x_T)$, where $\\theta_j$ represents the $j$-th token of the output. We utilize ${h_i \\in \\mathbb{R}^{L\\times T}, l \\in [1, L]}$ to represent the hidden state of $x_i$ in the $l$-th layer.\n4.2.1 Step1: Obtaining Incremental Weights\nDEM first computes the target answer representations in the selected layers $\\hat{l}$ of MLP and Attn by simultaneously optimizing the TC (Transformer Component, namely MLP and Attn) hidden states. Secondly, DEM updates both MLP and Attn weights in the critical layers through target answer $\\theta_j$ representations. Overall, DEM optimizes an objective function to obtain target weights (Meng et al., 2022b):\nW_{MLP}, W_{Attn} \\equiv argmin((\\sum_{i=1}^{n}(\\mid\\mid W_{ki} - V_i \\mid\\mid^2 + \\sum_{i=n+1}^{n+u} (\\mid\\mid W_{ki} - V_i \\mid\\mid^2)))$ (3)\nwhere $k_i \\in \\mathbb{R}^{K} and v_i \\in \\mathbb{R}^{V}$ represent the sets of keys and values, respectively, encoding the commonsense prompt in $\\hat{l}$-th layer. $\\mathbb{1}(\\mid\\mid W_{ki} - v_i \\mid\\mid^2$ indicates that we want to retain $n$ pieces of knowledge, while $\\sum_{i=n+1}^{n+u}(\\mid\\mid W_{ki} - V_i \\mid\\mid^2 in-dicates that we want to modify $u >> 1$ pieces of knowledge. We represent the keys and val- ues as matrices stacked horizontally: $[k_1|k_2|...|k_n] \\cong K$ and $[V_1|V_2|...|V_n] \\cong V$, and we consider the target weight $W_{MLP}$ and $W_{Attn}$ as the sum of the origi-nal weight $W_{MLP}$ and $W_{Attn}$, and the incremental weight $\\bigtriangleup$ (i.e. $W_{MLP} = W_{MLP} + \\bigtriangleup W_{MLP}$ and $W_{Attn} = W_{Attn} + \\bigtriangleup W_{Attn}$). Based on the deriva-tion from MEMIT (Meng et al., 2022b), the formal expression for the incremental weight is:\n$\\bigtriangleup_{MLP} = R_{MLP} (k_{MLP})^T (C_{MLP} + \\mu k_{MLP} (k_{MLP})^T)^{-1}$\n$\\bigtriangleup_{Attn} = R_{Attn} (k_{Attn})^T (C_{Attn} + \\mu k_{Attn} (k_{Attn})^T)^{-1}$ (4)\nwhere $R_{MLP} = V_{MLP} - W_{MLP} K_{MLP}$ represents the residual between the values $V_{i MLP}$(namely target answer representations) correspond-ing to the keys $K_{MLP}$ of the target knowledge and the model original knowledge $W_{MLP} K_{MLP}$. $C_{MLP} \\cong k_{Attn}(k_{Attn})^T = E[kk^T]$ is an esti-mate of the set of previously memorized keys ob-tained through sampling. Here, $\\mu$ is a hyperparam-eter which balances the degree of model modifica-tion and preservation."}, {"title": "4.2.2 Step2: Updating Weights", "content": "As shown in the Figure 6, $\\theta_a$ and $\\theta_m$ are the hidden states of the Attn and MLP of the $l$-th layer and the $i$-th token, respectively. The general forms of the Attn and MLP at the $l$-th layer and the $i$-th token x are given by:\n$\\theta^{i}_{a} = V_{Attn} Attn'(\\gamma(\\eta(\\theta^{i-1}_1, \\theta^{i-1}_2, ..., \\theta^{i-1}_{i-1})))$\n$\\theta^{i}_{m} = W_{mlp} \\Phi(W_\\gamma(\\eta(\\theta^{i-1})) (5)\nWhere $W^{out}_{Toaten}$ and $W^{out}_{I}$ are the output weights of the Attn and MLP at the $l$-th layer, respectively. $W^{i}$ are the input weights of the MLP at the $l$-th layer. The $\\Phi$ represents the non-linear activation function.\nDEM adds optimizable parameters $\\delta_a$ and $\\delta_m$ to hidden states $v_{im}$ and $v_{ia}$ at the $l$-th layer, respectively. DEM retains the optimized hidden state of MLP and Attn to update their weights separately, denoted as $\\theta^{i}_{in} = \\theta^{i}_{m} + \\delta_m = argminL(v_{im})$ and $\\theta^{i}_{a} + \\delta_a = argminL(v_{ia})$. The formulas $L(v^m_i)$ and $L(v^a_i)$ are similar, with the main difference being their application in MLP and Attn calculations. The $L(\\theta^{im}_i)$ is defined as follows:\nL(vm = \\alpha D_{KL}(P_{F_\\theta^{+}}[y_m \\mid \\rho_m] || P_{F_\\theta}[y_m \\mid \\rho_m]) + \\beta\\cdot \\sum_{j=1}^{p}( -log  P_\\theta^{+}[y_m  \\mid \\eta_{prev}, p^{<T}(x))].\nWhere $F\\theta$( $\\alpha_a+ \\theta_{i}$ = $\\theta_a^{i} + \\delta_a$) represents the op- timizable parameters $\\delta_a$ is added to the hidden states of Attn at the $l$-th layer of the model $F_\\theta$. The $\\alpha$ and $\\beta$ are hyperparameters used to balance reliability and specificity. \\eta_{prev}; p(x) is uti-lized to enhance the prefix of target knowledge generalization and commonsense knowledge gen-eralization (such as randomly replacing person names).Simultaneously calculate KL divergence and stack the calculation results into matrix $V_l$. With this, DEM follows the same algorithm steps as PMET (Li et al., 2024) to update MLP and Attn weights."}, {"title": "5 Experiments", "content": "In the section, we investigated the effectiveness of DEM method and existing editing methods in editing commonsense knowledge based in free-text."}, {"title": "5.1 Experimental Setup", "content": "Baselines and Datasets. Our experiments are conducted on GPT-J (6B) (Wang and Komatsuzaki, 2021) and LLaMA-2 (7B) (Touvron et al., 2023b). The baseline methods include the learning-based method MEND, and locating and editing the methods Fine-Tuning (FT+W) (Zhu et al., 2020), MEND (Mitchell et al., 2021), MEMIT (Meng et al., 2022b) and PMET (Li et al., 2024). We chose the CKEBench dataset we constructed as the benchmark.\nEvaluation. For CKEBench datasets, the target answer is an free-text that contains multiple tokens. Therefore, we utilize the GPT-4 (Achiam et al., 2023) model to determine the similarity between the generated text and the original text as the experimental result. Similar to the factual knowledge, the evaluation metrics include Score, Efficiency, Generalization, Specificity, Fluency and Consistency. In addition, we have added a Commonsense indicator to evaluate the ability of the method to edit commonsense knowledge. The data in the \"sub_neighborhood_prompts\" at Appendix C is utilized to evaluate this indicator."}, {"title": "5.2 Overall Results", "content": "We conduct experiments on commonsense knowledge datasets to verify the effectiveness of our method DEM.\nResults on the GPT-J (6B). The Table 2 shows that DEM performs better than baselines methods. Specifically, DEM built upon GPT-J (6B), is +4.5 better on indicator Score than PMET, and obtains a new state-of-the-art(SOTA) result. Meanwhile, our method achieves 13.8% improvements of Commonsense score on the true/false questions dataset. The significant performance gain of our method over the baselines demonstrates that the proposed DEM is very effective for this task.\nResults on the LLaMA-2 (7B). As show in Table 2, Our method improves upon the basic PMET method by 15.7% and 5.6% in term of F1 Commonsense score and Specificity score on the LLaMA-2 (7B), respectively. Meanwhile, our DEM achieves 3.0% improvements of Score. We attribute the improvements to that our method DEM takes advantage of Dynamics-aware and Knowledge Editing Module, thus achieving superior performance than the previous model PMET."}, {"title": "5.3 Ablation Study", "content": "To show the efficacy of our proposed techniques, we conduct an ablation study experiment by turning off one component at a time. 1) w/o DA, which removes the Dynamics-aware module; 2) w/o EM, which does not edit MLP layers in the Knowledge Editing module, only the Attn layers; 3) w/o EA, which does not edit Attn layers in the Knowledge Editing module, only the MLP layers; . We present the results of ablation study in Table 3. From the results, we can observe that:\n(1) Effectiveness of Dynamics-aware module. When we remove the Dynamics-aware module from the DEM, the Score drops by 10.2% on commonsense knowledge dataset. It proves the Dynamics-aware module is very effective for the task.\n(2) Effectiveness of not editing MLP layers. Not editing the MLP layer, the performance drops significantly. Specifically, the Efficacy score drops from 60.3% to 18.8% on the commonsense dataset.\n(3) Effectiveness of not editing Attn layers. Compared without editing Attn layers, our method DEM achieves 1.8% improvements of Efficacy score on the commonsense dataset. It demonstrates that the Attn layer is crucial for editing common-sense knowledge."}, {"title": "6 Related Work", "content": "The existing knowledge editing dataset can be divided into triplet form and event form. In triplet format dataset, commonsense knowledge dataset includes PEP3k and 20Q (Porada et al., 2021; Gupta et al., 2023), factual knowledge includes ZsRE (Levy et al., 2017), CounterFact (Meng et al., 2022a), Fact Verification (Mitchell et al., 2022), Calibration (Dong et al., 2022), MQuAKE (Zhong et al., 2023) and RaKE (Wei et al., 2023). In event format dataset, datasets with only factual knowledge, including ELKEN (Peng et al., 2024) and EVEDIT (Liu et al., 2024).\nThe previous editing methods mainly focused on editing knowledge in the form of triples, with a small amount of knowledge in the form of editing events. The methods for editing triplet forms mainly include : (1)Locate-Then-Edit method (Dai et al., 2021; Meng et al., 2022a,b; Li et al., 2024), (2) Memory-based method (Mitchell et al., 2022; Madaan et al., 2022; Zhong et al., 2023; Zheng et al., 2023), (3) Hyper-network method (Mitchell et al., 2021; De Cao et al., 2021; Tan et al., 2023). The method for editing event forms is Self-Edit (Liu et al., 2024)."}, {"title": "7 Conclusion", "content": "In this paper, we aim to edit commonsense knowledge based on free-text. Firstly, we constructed CKEBench dataset that provides a benchmark for editing Commonsense knowledge based on free-text. Additionally, we propose a KLFT method, and concluded that commonsense knowledge is dispersed in the MLP and Attn layers. Finally, we propose the DEM method to edit commonsense knowledge, and the experimental results verify the effectiveness of this method."}, {"title": "8 Limitations", "content": "Due to limitations in computing resources, we did not conduct relevant experiments on larger language models."}]}