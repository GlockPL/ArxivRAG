{"title": "Can a Machine Distinguish High and Low Amount of Social Creak in Speech?", "authors": ["Anne-Maria Laukkanen", "Sudarsana Reddy Kadiri", "Shrikanth Narayanan", "Paavo Alku"], "abstract": "Objectives: Increased prevalence of social creak particularly among female speakers has been reported in several studies. The study of social creak has been previously conducted by combining perceptual evaluation of speech with conventional acoustical parameters such as the harmonic-to-noise ratio and cepstral peak prominence. In the current study, machine learning (ML) was used to automatically distinguish speech of low amount of social creak from speech of high amount of social creak.\nMethods: The amount of creak in continuous speech samples produced in Finnish by 90 female speakers was first perceptually assessed by two voice specialists. Based on their assessments, the speech samples were divided into two categories (low vs. high amount of creak). Using the speech signals and their creak labels, seven different ML models were trained. Three spectral representations were used as feature for each model.\nResults: The results show that the best performance (accuracy of 71.1%) was obtained by the following two systems: an Adaboost classifier using the mel-spectrogram feature and a decision tree classifier using the mel-frequency cepstral coefficient feature.\nConclusions: The study of social creak is becoming increasingly popular in sociolinguistic and vocological research. The conventional human perceptual assessment of the amount of creak is laborious and therefore ML technology could be used to assist researchers studying social creak. The classification systems reported in this study could be considered as baselines in future ML-based studies on social creak.\nKeywords: social creak, creaky voice, machine learning, spectral feature", "sections": [{"title": "1. Introduction", "content": "Creaky voice refers to a raspy, rough sounding voice that is often low-pitched. Keating et al. ([1]) classified creaky voice into seven sub-types: prototypical creaky voice, vocal fry, multiply-pulsed voice, aperiodic voice, tense or pressed voice and non-constricted creak. Although creaky\nvoice shares perceptual and acoustic similarities with dysphonia (such as perceptual roughness, strain or markedly low pitch, as well as irregular, multiple pulsed and even chaotic signal structure, see e.g., [2], [3]), it is a variant of voice use, and all the above mentioned subtypes of creaky voice appear in healthy speakers ([1], [4]. Creaky voice appears very often in sentence endings, and it can be used to signal phrase ending or turn taking in a conversation ([5], [6], [7], [8]). In some languages, it is used to create phonemic contrasts ([9], [10], [11]). Furthermore, it may be used to express emotions and attitudes, e.g., it has been related to expression of hesitance, complaining, boredom, relaxation, intimacy, contentedness ([12], [13]) or anger of low-activation ([14]). In addition to specific linguistic and emotional expression use, creaky voice appears to be a general sociolinguistic marker. It may appear in all sentence positions, and it is used in different age groups ([15]) and as well in professional (radio) speakers as in untrained speakers ([7]). More or less continuous creaky voice may be used as a marker of social status and authority ([16]) or belonging to a certain social group ([17]). It may also be a habit, seemingly without any particular role as a marker, unless the role is then to signal informality, like it seems to be the case in Estonian speakers ([18]). Creaky voice may be a preferred choice also because it is possible to establish using a low subglottic pressure, at least the subtype 'vocal fry' ([19]). In this study, we name all creak in healthy speakers' voices as \u2018social creak'. Due to the similarities between creaky voice and characteristics of dysphonia, continuous creaky voice use may be even called as social/voluntary dysphonia.\nThe prevalence of social creak has increased remarkably ([16], [20]). It appears to be more prevalent in females than in males ([16], [21], [22]). Several studies have reported high prevalence of creak in different groups of Finnish speakers. For instance, Piril\u00e4 et al.([23]) observed that up to 54% of teachers' (N = 24) speech consisted of creaky voice. According to Ketolainen et al. ([24]), 60% of 16-17 year old males and all except for one of the studied females (N= 40 in both groups) used a notable amount of creaky voice. According to [4], 73.2 % of female university students (N = 104) used slight to moderate amount of creaky voice. A recent study ([25]) reported that the prevalence of creaky voice use among Finnish female university students has increased significantly from the 1990's to 2010's, while among male students the change was not significant. However, in both groups, a tendency for increase in creaky voice use was observed. More specifically, it was found that the number of speakers who were perceptually rated as having 'a lot of creak' or 'quite a lot of creak' in their speech had increased from 5.9% to 20.4% in males and from 7% to 31.8% in females. In all the previous studies ([4], [16], [20], [21] \u2013 [25]), the amount of creak has been quantified either solely or primarily based on auditory perception.\nCreaky voice in its different forms has several similarities with dysphonia: irregularity, super low pitch and characteristics of tense voice with a long closed phase and low pulse amplitude [1]. Dysphonia refers to vocal impairment as recognized by a clinician. According to the World Health Organization, an impairment is \u201cany loss or abnormality of psychological, physiological or anatomical structure or function.\u201d Characteristics of dysphonia include hoarseness, breathiness, voice fatigue, decreased vocal volume or other disturbances that limit the person's performance in work-related and social functions [26]. Therefore it is challenging to distinguish 'habitual creaky voice' (i.e., social creak or voluntary dysphonia) from real dysphonia, either perceptually or with traditional acoustic analysis methods. Perceptual voice quality assessment in general is challeng-ing due to many reasons ([27]). A key reason is that it is difficult to extract and scale single"}, {"title": "3.1. Pre-processing", "content": "In the current study, a speech sample corresponds to a recorded signal of continuous speech. Therefore, the recorded raw speech sample includes sections of silence between sentences and words, and it needs to be pre-processed before it can be used in the system training and testing. In the first pre-processing step, the input speech sample is normalized by dividing the time domain signal waveform by its highest amplitude value. Subsequently, segments of silence are removed from the processed normalized speech sample using the sound exchange (SoX) tool [49]."}, {"title": "3.2. Feature Extraction", "content": "Next, the pre-processed input speech sample is converted into an acoustic feature vector. Three widely used spectral features (spectrogram, mel-spectrogram, and MFCCs) are extracted and com-pared in this study. These features have been shown to provide promising results in various tasks (e.g., automatic speech recognition [50], speaker recognition [51], and the classification of voice qualities [44, 52]). All these features are computed in time frames by using a frame length of 100 ms and a frame shift of 5 ms. These settings were selected based on our previous study [53], which indicated that classification accuracy improves by using a frame length that is longer (e.g., 100 ms) than the widely-used frame length of a few tens milliseconds (e.g., 20 ms). For the computation of the spectrogram feature, speech is windowed frame-wise using a Hamming window. The amplitude spectrum is then estimated using a 1024-point fast Fourier transform (FFT). Subsequently, the logarithm of the amplitude spectrum is computed to get a 513-dimensional feature vector for each time frame. To derive the mel-spectrogram feature, the amplitude spectrum of the input speech sample is passed through an 128-channel mel-filterbank. The resulting mel-spectrogram is then transformed into the logarithmic decibel scale. This process yields a 128-dimensional feature vec-tor for each time frame. The computation of the MFCCs involves employing the discrete cosine transform (DCT) on the mel-scale spectrum. From the resultant mel-cepstrum, first 13-cepstral coefficients (excluding the 0th coefficient) are considered. In addition to the static coefficients, their first and second derivatives are calculated, which results in a 39-dimensional feature vector for each time frame.\nFor all three feature extraction methods described above, the frame-wise computed features are merged into sample-wise features using statistical functionals as in [53, 54]. This is conducted using altogether eight different functionals (mean, standard deviation, median, skewness, kurtosis, minimum, maximum, and range). After this stage, the final feature dimension per speech sample is 513 \u00d7 8 = 4104, 128 \u00d7 8 = 1024, and 39 \u00d7 8 = 312 for the spectrogram, mel-spectrogram and MFCCs, respectively."}, {"title": "3.3. Classifiers", "content": "Altogether seven different ML-based classifiers are compared: Support Vector Machine with linear kernel (SVM-linear), SVM with radial basis function kernel (SVM-RBF), Random Forest (RF), Multilayer Perceptron (MLP), Logistic Regression (LR), Decision Tree (DT), and Adaboost. All the ML classifiers were implemented using the Scikit-learn library [55].\nThe hyperparameters of the ML classifiers are optimized using a grid search strategy where a set of possible parameter combinations are first formed for each of the classifiers and the optimal combination is then searched. The parameters of the grid search are presented in Table 1."}, {"title": "3.4. Evaluation", "content": "To evaluate the performance of the ML models, the leave-one-speaker-out (LOSO) cross-validation scheme is employed. During each iteration, the data of one speaker is designated as the testing dataset, while the remaining speakers' data are used to train the classifiers. To stan-dardize the training and testing datasets, z-score normalization is applied, utilizing the mean and standard deviation values derived from the training data. The evaluation metric (accuracy) is aver-aged across all iterations to represent the model's overall performance."}, {"title": "5. Discussion and Conclusions", "content": "Several studies (e.g., [16], [20], [25]) have reported increased prevalence of social creak espe-cially among female speakers. Previous studies on social creak are based on conventional auditory-perceptual evaluation where the amount of creak in speech is assessed by voice specialists. Since such subjective evaluations are time-consuming, costly and may be subject to personal biases of raters, the use of automatic, ML-based assessment is an attractive new approach to study social creak. In the current study, ML-based approaches were developed to automatically classify the amount of social creak from continuous speech signals. We investigated a binary classification problem to distinguish speech signals into two categories of social creak (speech of low amount of social creak vs. speech of high amount of social creak). We built several ML-based systems based on supervised learning where an ML-based classifier was first trained using speech signals and their binary labels (low amount of social creak vs. high amount of social creak), which were ob-tained by averaging the amount of perceived creak rated on a 9-point Likert scale. All the systems were based on a three-stage architecture consisting of a pre-processing stage, a feature extraction stage and a classifier stage. We trained and tested altogether 21 systems by using three popular spectral feature representations and seven known ML models as the classifier.\nThe study showed that there were large differences in accuracy between the compared systems. The worst system (based on the MLP classifier and the spectrogram feature) gave an accuracy of 52.3%, which is just barely above the chance level 50% while the two best machines (based either"}]}