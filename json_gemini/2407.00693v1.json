{"title": "BAPO: Base-Anchored Preference Optimization for Personalized Alignment in Large Language Models", "authors": ["Gihun Lee", "Minchan Jeong", "Yujin Kim", "Hojung Jung", "Jaehoon Oh", "Sangmook Kim", "Se-Young Yun"], "abstract": "While learning to align Large Language Models (LLMs) with human preferences has shown remarkable success, aligning these models to meet the diverse user preferences presents further challenges in preserving previous knowledge. This paper examines the impact of personalized preference optimization on LLMs, revealing that the extent of knowledge loss varies significantly with preference heterogeneity. Although previous approaches have utilized the KL constraint between the reference model and the policy model, we observe that they fail to maintain general knowledge and alignment when facing personalized preferences. To this end, we introduce Base-Anchored Preference Optimization (BAPO), a simple yet effective approach that utilizes the initial responses of reference model to mitigate forgetting while accommodating personalized alignment. BAPO effectively adapts to diverse user preferences while minimally affecting global knowledge or general alignment. Our experiments demonstrate the efficacy of BAPO in various setups.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) (Achiam et al., 2023; Touvron et al., 2023) have been successfully aligned with human preferences across various applications, ranging from summarization tasks to enhancing reasoning capabilities (Ouyang et al., 2022; Stiennon et al., 2020; Tunstall et al., 2023b; Wang et al., 2023a). This alignment process involves collecting human feedback by presenting pairs of responses generated from the same user prompt and asking users to choose their preferred response (Bai et al., 2022; Cui et al., 2023; Lee et al., 2023; Cheng et al., 2023). The LLMs learn from this preference data to produce responses that better match human preferences, effectively addressing the challenge of converting complex human expectations into tangible training objectives (Ouyang et al., 2022; Ji et al., 2023; Xu et al., 2024). Known as preference optimization, this approach has become essential in the final stages of LLM training (Meta, 2024; Abdin et al., 2024; Jiang et al., 2024).\nHowever, the common assumption in preference optimization is that all users share a uniform set of general preferences (Bai et al., 2022; Rafailov et al., 2024; Zheng et al., 2023), leading LLMs to align with an average of these preferences, as derived from collective feedback data (Jafari et al., 2024; Li et al., 2024; Guo et al., 2024). While effective for broadly accepted preferences like helpfulness and harmlessness, this approach does not account for the diversity of individual preferences in real-world scenarios (Jang et al., 2023; Zeng et al., 2023; Cheng et al., 2023; Zhong et al., 2024). For example, given the same context, one user might prefer a humorous response, while another might prefer a concise one. This reliance on averaged preferences often fails to capture the unique preferences of each user. This is known as the Condorcet Paradox (Gehrlein, 1983, 2002) in social choice theory, where no single response consistently satisfies all users, leading to non-transitive preferences (Wang et al., 2024a; Munos et al., 2023).\nRecent studies have begun to tackle this challenge by fine-tuning instruction-tuned LLMs for personalized alignment (Jang et al., 2023; Zeng et al., 2023; Rame et al., 2024). Although these personalized preference optimization approaches enable support for diverse user preferences (Li et al., 2024; Zhong et al., 2024; Guo et al., 2024), the impact of learning to meet personalized preferences on previously acquired knowledge (Jin and Ren, 2024; Lu et al., 2024), such as global knowledge (Dou et al., 2023) and general alignment (Lin et al., 2023), remains underexplored.\nIn this work, we systematically analyze how personalizing LLMs according to diverse user preferences impacts their global knowledge and general alignment. Our findings reveal that the extent of knowledge loss heavily depends on these preferences, often inducing significant declines in specific areas of knowledge. This suggests that the conventional Kullback-Leibler (KL) constraints between the policy model and the reference model (Schulman et al., 2017; Rafailov et al., 2024), which is based solely on the tokens appearing in preferred or dispreferred responses (Pal et al., 2024; Azar et al., 2024; Zheng et al., 2023), fail to prevent the forgetting that occurs during personalized preference optimization.\nTo address this issue, we start by analyzing the initial responses, referred to as base responses, of instruction-tuned models to the given prompts and observe how their likelihood of producing these responses changes over training steps. We discover that personalizing preferences to enhance the distinction between preferred and dispreferred responses not only diminishes the likelihood of producing the dispreferred responses, but also lowers the likelihood of generating these base responses.\nWe hypothesize that aligning the reference and policy models, especially focusing on the tokens that appear in the base response, is essential for preserving global knowledge and ensuring general alignment. To this end, we introduce a novel preference optimization method named as Base-Anchored Preference Optimization (BAPO). BAPO aims to maintain the likelihood that the policy model will produce a base response originating from the reference model during personalized preference optimization."}, {"title": "2 Personalized Preference Optimization", "content": "In this section, we first introduce preference optimization for LLM alignment. Next, we examine how personalized preferences impact the existing knowledge of instruction-tuned LLMs. We suggest that the typical KL-constraint in preference optimization is not effective in preventing forgetting."}, {"title": "2.1 Preliminary: Preference Optimization", "content": "Consider a dataset of pairwise preferences, denoted as $D = {x^i, y^w_i, y^l_i}_{i=1}^{N}$. In this dataset, for each prompt $x^i$, the responses $y^w_i$ and $y^l_i$ represent the preferred (i.e., chosen) and dispreferred (i.e., rejected) responses, respectively. Our goal is to optimize the policy model $\\pi_{\\theta}(y|x)$ to maximize the expected value of the ideal reward function $r^*(x, y)$ that aligns with human preferences:\n$\\pi^* = \\text{arg max}_{\\pi_{\\theta}} E_{y \\sim \\pi_{\\theta}(\\cdot|x)} [r^*(x, y)]$. (1)\nA common approach to modeling the reward function is using the Bradley-Terry model (Bradley and Terry, 1952), which models the human preference distribution $p^*(y_1 > y_2 | x)$ as follows:\n$p^*(y_w > y_l | x) = \\frac{\\text{exp}(r^*(x, y_w))}{\\text{exp}(r^*(x, y_w)) + \\text{exp}(r^*(x, y_l))}.$ (2)\nNote that the Bradley-Terry model assumes that for each prompt x, the paired comparison probabilities $p(Y_w > y_l | x)$ reflect a consistent human preference ordering across all possible responses, depending solely on the reward difference between responses $r^*(x, y_w) \u2013 r^*(x, y_l)$.\nRLHF Using the reward function defined in Equation 2, Reinforced Learning from Human Feedback (RLHF) (Ouyang et al., 2022; Stiennon et al., 2020) initially trains a reward model $r_{\\phi}(x, y)$ that produces a single scalar prediction for the reward value. In the subsequent RL phase, this reward model guides the LLM to align the learned preference with the reference model $\\pi_{ref}$, which has undergone supervised fine-tuning (SFT) from a pre-trained LLM as follows:\n$\\mathcal{L}_{RLHF} = E_{x\\sim D, y\\sim \\pi_{\\theta}(y|x)} [r_{\\phi}(x, y)  - \\beta D_{KL} [\\pi_{\\theta}(y|x)||\\pi_{ref}(y|x)]]$. (3)\nwhere $\\beta$ corresponds to the regularization strength of KL-Divergence between the policy model $\\pi_{\\theta}$ and the reference model $\\pi_{ref}$.\nDPO By simplifying Equation 3, Direct Preference Optimization (DPO) (Rafailov et al., 2024) optimizes the maximum likelihood of the policy model $\\pi_{\\theta}$ without the need to train a separate explicit reward model as follows:\n$\\mathcal{L}_{DPO} = E_{(x,y_w,y_l)\\sim D} \\text{log} \\sigma\\big( \\beta \\text{log} \\frac{\\pi_{\\theta}(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta \\text{log} \\frac{\\pi_{\\theta}(y_l|x)}{\\pi_{ref}(y_l|x)}\\big)$. (4)\nHere, $\\beta$ represents the KL-regularization strength in RLHF. Note that in both RLHF and DPO, this KL-constraint depends only on the tokens appearing in $y_w$ and $y_l$, the responses directly related to the preference ranking comparison."}, {"title": "2.2 Forgetting from Personalization", "content": "To understand how preference heterogeneity affects the extent of forgetting, we conduct an experimental study using heterogeneous preference datasets: P-Soups (Jang et al., 2023) and DSP (Cheng et al., 2023). We fine-tune the instruction-tuned Phi-3-mini (Abdin et al., 2024) model using DPO (Rafailov et al., 2024) with LORA (Hu et al., 2021). Please refer to the detailed setups provided in Section 4."}, {"title": "2.3 Knowledge in Base Response", "content": "The significant variation in performance after personalized preference optimization suggests that the typical KL-divergence constraints (Ouyang et al., 2022; Rafailov et al., 2024; Zheng et al., 2023) used in general preference optimization (Bai et al., 2022; Tunstall et al., 2023b) still suffer from forgetting induced by preference heterogeneity.\nWe hypothesize that the original response from the initial reference model $\\pi_{ref}$, which contains intact global knowledge and aligns with general alignment, is influenced by learning to meet diverse individual preferences. We take a closer look at personalized preference optimization to understand how adapting to heterogeneous preferences affects the likelihood of generating specific responses, represented by $\\text{log}[\\pi_{\\theta} (y_{(.)}|x) \u2013 \\pi_{ref}(y_{(.)}|x)]$.\nThe observations in Figure 5 verify our conjecture. Personalizing preferences to enhance the distinction between the chosen response $y_w$ (i.e., preferred) and the rejected response $y_l$ (i.e., dispreferred) not only reduces the likelihood of producing the rejected response but also lowers the likelihood of generating base responses $y_b$. In this context, KL-divergence constraints between the reference and policy models on tokens found in these chosen and rejected responses do not help maintain the likelihood of base responses. Based on our findings, we consider leveraging the tokens appearing in the base responses to encourage knowledge preservation during personalized preference optimization. Please see more details in Appendix C."}, {"title": "3 Proposed Method: BAPO", "content": "In this section, we introduce Base-Anchored Preference Optimization (BAPO). Our primary motivation is to use the initial response from the instruction-tuned model before it undergoes personalized preference optimization. By anchoring the policy model to this initial response, the personalized policy model can effectively retain its original global knowledge and general alignment while still accommodating diverse user preferences."}, {"title": "3.1 Base-Anchored Preference Optimization", "content": "Consider the base response $y_b$ from the reference model $\\pi_{ref}$ and the policy model $\\pi_{\\theta}$, which we fine-tune for personalized preferences. The example in Figure 4 showcases how the chosen, base, and rejected responses differ for the same given user prompt. The core concept of BAPO is to preserve the knowledge contained in this base response during the optimization for diverse preferences.\nBase Anchor BAPO ensures that the policy model\u2019s likelihood of producing the base response $y_b$ ($\\pi_{\\theta}(y_b|x)$) remains closely aligned with that of the reference model ($\\pi_{ref}(y_b|x)$):\n$\\mathcal{L}_{Anchor} \\coloneqq \\text{max} \\big(0, \\text{log} \\frac{\\pi_{ref}(y_b|x)}{\\pi_{\\theta}(y_b|x)} \\big)$. (5)\nNote that the base anchor loss $\\mathcal{L}_{Anchor}$ becomes 0 if the policy model $\\pi_{\\theta}$ assigns a higher likelihood to the base response $y_b$ than the reference model does ($\\pi_{\\theta}(y_b|x) > \\pi_{ref}(y_b|x)$). Intuitively, if the policy model is already more confident in the base response than the reference model, there's no need to penalize it further. The BAPO objective $\\mathcal{L}_{BAPO}$ is defined as follows:\n$\\mathcal{L}_{BAPO} = \\mathcal{L}_{DPO} + \\lambda \\cdot \\mathcal{L}_{Anchor}$. (6)\nHere, $\\lambda$ controls the strength of the anchoring effect. In our main experiments, we set $\\lambda$ to 5."}, {"title": "3.2 Theoretical Analysis", "content": "We assess the impact of BAPO on personalized preferences by analyzing how information from base responses aids in aligning personal preferences. We assume linear utility and reward functions, with their respective unknown parameters having distinct nonzero components.\nAssumption 1 A utility function $G^*$ exists for general alignment with global knowledge, and a reward function $L^*$ measures personal alignment.\nAssumption 2 For the response $y$ and context $x$, the functions $G^*$ and $L^*$ are linear, defined as: $G^*(x,y) = (\\phi(x,y),\\boldsymbol{0})$ and $L^*(x,y) = (\\boldsymbol{0},\\phi(x,y))$ where $\\phi(x,y)$ is a $d$ dimensional feature vector of $y$ and $x$. Additionally, $G^*$, $L^*$ have non-intersecting nonzero components on $d-k$ and the $k$ dimensions respectively."}, {"title": "4 Experiment", "content": "In this section, we introduce Base-Anchored Preference Optimization (BAPO). Our primary motivation is to use the initial response from the instruction-tuned model before it undergoes personalized preference optimization. By anchoring the policy model to this initial response, the personalized policy model can effectively retain its original global knowledge and general alignment while still accommodating diverse user preferences."}, {"title": "4.1 Experimental Setups", "content": "Datasets We use two preference datasets for personalized preference optimization: P-Soups (Jang et al., 2023) and DSP (Cheng et al., 2023).\n\u2022 Personalized Soups (P-Soups) include Style preferences, organized into three dimensions: P1, P2, and P3. Each dimension features two contrasting types, A and B. In Table 1, we briefly describe the preference types.\n\u2022 Domain Specific Preference (DSP) includes Domains preferences: Academy, Business, Entertainment, and Literature & Art.\nEach dataset is composed of user queries and a set of responses for each query. In our pairwise preference format, for each user query x, we select a response that aligns with a specific preference as the chosen response yw. Responses from other preferences are designated as rejected responses yl. More details are provided in Appendix A."}, {"title": "4.2 Performance on Knowledge Preservation", "content": "In Table 2, we evaluate how fine-tuning the Phi-3-mini model affects its performance in the Global Knowledge across diverse preferences. This evaluation includes the DPO (Rafailov et al., 2024) and other preference optimization methods such as RSO (Liu et al., 2023), IPO (Azar et al., 2024), DPOP (Pal et al., 2024), and ORPO (Hong et al., 2024). The 'Base' in the table indicates the initial performance of the Phi-3-mini model.\nThe results show that the baseline methods significantly declines the performance on global knowledge datasets, while our BAPO method effectively maintains consistent performance across various evaluated datasets. We highlight that BAPO has advantageous characteristics, keeping performance variations due to preference heterogeneity remarkably low. In contrast to the baseline method, which exhibits high fluctuation and large variance across varying preferences, BAPO provides more reliable results with minimal variation.\nWe observe that personalized preferences do not necessarily lead to forgetting. In fact, they can sometimes enhance performance on certain datasets based on the types of preferences involved. For example, fine-tuning with domain-specific preferences in DSP datasets often results in improved performance on MMLU datasets. Notably, the ORPO method, which does not use a reference model during preference optimization, consistently outperforms other approaches in such cases. This implies there exists a natural trade-off in the use of reference models between preserving existing knowledge and acquiring new knowledge."}, {"title": "4.3 Performance on Alignment", "content": "General Alignment Table 3 presents the evaluation of general and personalized alignment after the personalized preference optimization. Ideally, the personalized model should maintain general alignment while accommodating its specific personalized preferences. We first evaluate whether the fine-tuned models maintain general alignment after personalization by using the HHH-Alignment datasets (Askell et al., 2021). The results show that BAPO effectively preserves the level of general alignment seen in the initial reference model, with minimal variation across diverse preference types.\nPersonalized Alignment We evaluate personalized alignment by measuring Reward Accuracy, which assesses whether the policy model prefers the selected response yw over the dis-preferred response yr for user prompts x in the validation set. The results show that BAPO more effectively accommodate personalized performance compared to other baselines. This demonstrates that utilizing base responses can boost sample efficiency for reward signals related to personalized preferences."}, {"title": "4.4 Ablation Study", "content": "Model Architecture We conduct further experiments with the instruction-tuned Gemma-2B model (Team et al., 2024), referred to as Gemma-2B-it, which has also undergone RLHF for general alignment after the SFT stage. The results presented in Table 4 validates the robust efficacy of BAPO across different model architectures."}, {"title": "5 Related Work", "content": "Learning from Human Feedback Often employed as the final stage of instruction tuning (Tunstall et al., 2023b; Jiang et al., 2024; Meta, 2024), learning from human feedback refines the policy language model to generate responses that align with human preferences (Ji et al., 2023; Zheng et al., 2023). This process usually involves collecting human preferences for pairs of candidate responses to differentiate between those that are preferred and those that are dispreferred. The two main approaches used are Reinforcement Learning from Human Feedback (RLHF) (Stiennon et al., 2020; Ouyang et al., 2022) and Direct Preference Optimization (DPO) (Rafailov et al., 2024). RLHF involves training a separate reward model that is then utilized in the subsequent reinforcement learning phase (Bai et al., 2022; Tunstall et al., 2023b). In contrast, DPO does not rely on a reward model but directly establishes a mapping between the reward function and the optimization objective (Pal et al., 2024; Liu et al., 2023). Our work specifically addresses the challenge of accommodating personalized preferences within the DPO framework. We focus on maintaining the likelihood of base responses from the reference model during the personalization process, ensuring the preservation of global knowledge and general alignment.\nForgetting in LLM Fine-tuning The issue of forgetting previously acquired knowledge during the fine-tuning on heterogeneous data has been extensively discussed (Wang et al., 2023b, 2024b), highlighting a fundamental trade-off between preserving old knowledge and acquiring new knowledge (Parisi et al., 2019; McCloskey and Cohen, 1989). In the context of LLMs, recent research has shown that the knowledge from pre-training can be compromised by supervised fine-tuning (SFT) on instruction data (Dou et al., 2023; Dong et al., 2023). A similar issue, known as alignment tax, occurs in preference optimization (Lin et al., 2023; Lu et al., 2024). While prior research has addressed the forgetting induced by the LLM fine-tuning (Biderman et al., 2024; Luo et al., 2023), the effects of accommodating specific user preferences and the impact of their heterogeneity remain largely unexplored. In our study, we investigate how personalized preference optimization affects both global knowledge and general alignment.\nPersonalized Preference in LLM The use of a single scalar reward to represent user preferences presents a significant limitation when users have diverse and conflicting preferences (Ji et al., 2023; Zheng et al., 2023). To address this issue, some studies have explored clustering users who presumably share the same reward (Chakraborty et al., 2024; Park et al., 2024). Others have considered defining a reward function with multiple objective dimensions (Jafari et al., 2024; Yang et al., 2024) to achieve Pareto optimality among them (Guo et al., 2024; Zhong et al., 2024; Wang et al., 2024a). Additionally, some approaches involve merging model parameters trained for each dimension to accommodate the diverse combinations expressed by those dimension (Jang et al., 2023; Rame et al., 2024). In our study, we focus on the impact of preference heterogeneity on forgetting during personalized preference optimization, assuming that each user has a specific, definitive type of preference."}, {"title": "6 Conclusion", "content": "This study explores the degree and nature of forgetting caused by personalized preference optimization in instruction-tuned LLMs. Our findings indicate a reduced likelihood of generating original responses, alongside a decrease in the generation of dispreferred responses. To address this, we introduce Base-Anchored Preference Optimization (BAPO), a method that anchors the likelihood of base responses during the preference optimization process. This approach effectively preserves global knowledge and general alignment while successfully accommodating personalized preferences. We have conducted extensive experiments to validate the efficacy of BAPO and its benefits."}, {"title": "Limitations", "content": "While BAPO effectively preserves existing knowledge by leveraging the base response, it is important to note that if the base model is biased, the fine-tuned personalized model may also exhibit a similar bias. This consideration is crucial for machine learning practitioners. In our experiments, we utilized Q-LoRA for fine-tuning. Although we conducted an ablation study varying model capacity by adjusting the LoRA rank, a full-finetune might show different tendencies. Nonetheless, using LoRA fine-tuning for personalization is a common approach in LLM context. Regarding computational costs, although BAPO requires the use of a base response, potentially increasing memory and computational demands during training, most of these costs can be mitigated by pre-generating the base responses and caching them as offline datasets. Concerning the anchoring strength hyperparameter, A, the extent and scope of forgetting may vary based on the type of knowledge and the personalized preference types. Ideally, the \u5165 value should be adaptively assigned, but this aspect is left for future research."}, {"title": "Ethical Considerations", "content": "In developing and implementing our approach to align LLMs with personalized preferences, we must consider the potential implications. While BAPO aims to accommodate diverse user preferences, it is crucial to ensure this customization does not unintentionally reinforce harmful biases or perpetuate discrimination. Additionally, we must protect user privacy and data security, ensuring that personalization does not expose sensitive information or compromise user anonymity. Finally, maintaining a balance between personalized alignment and the integrity of general knowledge is essential to avoid scenarios where excessive personalization might result in misinformation or a loss of objective truth."}, {"title": "D Proof of Proposition 1", "content": "First, observe that under the linear reward model assumption, maximum likelihood estimation (MLE) of unknown parameter 0 is obtained as follows:\n$\\widehat{\\theta}_{MLE} = \\text{argmin}_{\\theta \\in \\Theta_B} \\mathcal{L}_{BT}(\\theta) = \\text{argmin}_{\\theta \\in \\Theta_B} \\sum_{i=1}^{n} - \\text{log} (\\sigma ((\\theta, \\phi(x^i, y^w_i) \u2013 \\phi(x^i, y^l_i)))),$ (7)\nwhere, $\\sigma$ is a sigmoid function and $(x^i, y^w_i, y^l_i)$ denotes i-th (out of n) preference sample with context $x^i$, winning and losing response $y^w_i, y^l_i$ respectively. Also, and $\\Theta_B = {\\theta \\in \\mathbb{R}^d : ||\\theta|| \\leq B}$.\nIn order to prove the Proposition 1, we bring the latest result for the sample complexity bound for the linear preference model which is provided in the Lemma 3.1 of (Zhu et al., 2023). We restate the lemma here for the completeness.\nLemma 1 Assume, $\\phi(y, x) < L$ for all possible response, context pairs (y, x) and $\\theta^+ < B$ is unknown parameter for linear model in eq. 7. Then, for $\\lambda > 0$, constant C' > 0 and estimator $\\widehat{\\theta}_{MLE}$ for the Bradley-Terry model loss (eq. 7), the following confidence bound holds with probability 1 \u2013 \u03b4:\n$|| \\widehat{\\theta}_{MLE} \u2013 \\theta^+||_{D+\\lambda I} < C'\\sqrt{\\frac{d+\\text{log}(1/\u03b4)}{\u03b3 n}} + \u03bb B^2$.\nHere, $\u03a3_D = \\frac{1}{n} \\sum_{i=1}^{n} (\\phi(x^i, y^w_i) \u2013 \\phi(x^i, y^l_i)) (\\phi(x^i, y^w_i) \u2013 \\phi(x^i, y^l_i))^T$, $\u03b3 = 1/(2+\\text{exp}(-LB)+\\text{exp}(LB))$.\nNow, suppose we have full information of $\\theta^G$ and without loss of generality, $\\theta^G$ has nonzero values on the first d-k dimensions. (Note that according to Assumption. 2, this automatically implies that $\\theta^L$ has nonzero components only on the last k dimensions.) Here, for the ease of analysis, we truncate only nonzero parts of $\\theta^L$ to make it a k-dimensional vector. Also, denote $\\phi_L(y, x) = \\phi(y, x)_{d\u2212k+1:d}$ be the last k dimensional part of feature vector that governs personalization reward L. With this, we can calculate MLE of BT model only for the last k dimensional components in the following way:\n$\\widehat{\\theta}_{MLE} = \\text{argmin}_{\\theta_L \\in \\Theta_B'} \\mathcal{L}_{BT}(\\theta_L) = \\text{argmin}_{\\theta_L \\in \\Theta_B'} \\sum_{i=1}^{n} - \\text{log} (\\sigma ((\\theta_L, \\phi_L(x^i, y^w_i) \u2013 \\phi_L(x^i, y^l_i)))),$ (9)\nwhere $\\Theta_{B'} = {\\theta \\in \\mathbb{R}^k : ||\\theta|| \\leq B'}$.\nNow, with Lemma. 1 and Assumptions 1, 2, it is easy to see the following confidence bound for the $\\theta^L$ holds by the following lemma.\nLemma 2 With $\\phi(y, x)_{d\u2212k+1:d} \\leq L'$ for all possible response, context pairs (y, x) and $\\theta^L < B'$. Then, for $\\lambda' > 0$, constant C'' > 0 and unknown parameter $\\widehat{\\theta}_{MLE}$ from the modified Bradley-Terry model loss (eq. 9), the following confidence bound holds with probability 1 \u2013 \u03b4:\n$|| \\widehat{\\theta}_{MLE} \u2013 \\theta^L||_{\u03a3_{D'}+\u03bb'I} < C''\\sqrt{\\frac{k + \\text{log}(1/\u03b4)}{\u03b3' n}} + \u03bb' B'^2.$\nHere, $\u03a3_{D'} = \\frac{1}{n} \\sum_{i=1}^{n} (\\phi_L(x^i, y^w_i) \u2013 \\phi_L(x^i, y^l_i)) (\\phi_L(x^i, y^w_i) \u2013 \\phi_L(x^i, y^l_i))^T$, $\u03b3' = 1/(2 + \\text{exp}(-L'B') + \\text{exp}(L'B'))$.\nCombining Lemma 1 and Lemma 2, we see that Proposition. 1 holds."}, {"title": "E Complete Example Responses of Figure 1", "content": ""}, {"title": "F Complete Example Responses of Figure 4", "content": ""}]}