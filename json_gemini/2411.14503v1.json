{"title": "PLANNING-DRIVEN PROGRAMMING: A LARGE LANGUAGE MODEL PROGRAMMING WORKFLOW", "authors": ["Chao Lei", "Yanchuan Chang", "Nir Lipovetzky", "Krista A. Ehinger"], "abstract": "The strong performance of large language models (LLMs) on natural language processing tasks raises extensive discussion on their application to code generation. Recent work suggests multiple sampling approaches to improve initial code generation accuracy or program repair approaches to refine the code. However, these methods suffer from LLMs' inefficiencies and limited reasoning capacity. In this work, we propose an LLM programming workflow (LPW) designed to improve both initial code generation and subsequent refinements within a structured two-phase workflow. Specifically, in the solution generation phase, the LLM first outlines a solution plan that decomposes the problem into manageable sub-problems and then verifies the generated solution plan through visible test cases. Subsequently, in the code implementation phase, the LLM initially drafts a code according to the solution plan and its verification. If the generated code fails the visible tests, the plan verification serves as the intended natural language solution to consistently inform the refinement process for correcting bugs. We further introduce SLPW, a sampling variant of LPW, which initially generates multiple solution plans and plan verifications, produces a program for each plan and its verification, and refines each program as necessary until one successfully passes the visible tests. Compared to the state-of-the-art methods across various existing LLMs, our experimental results show that LPW significantly improves the Pass@1 accuracy by up to 16.4% on well-established text-to-code generation benchmarks, especially with a notable improvement of around 10% on challenging benchmarks. Additionally, SLPW demonstrates up to a 5.6% improvement over LPW and sets new state-of-the-art Pass@1 accuracy on various benchmarks, e.g., 98.2% on HumanEval, 84.8% on MBPP, 64.0% on APPS, and 35.3% on CodeContest, using the advanced LLM GPT-40 as the backbone.", "sections": [{"title": "1 INTRODUCTION", "content": "Code generation, also known as program synthesis, studies the automatic construction of a program that satisfies a specified high-level input requirement (Gulwani et al., 2017). Recently, large language models (LLMs) pre-trained on extensive code-related datasets (Brown et al., 2020; Meta, 2024; Li et al., 2023; Roziere et al., 2023; Achiam et al., 2023; Muennighoff et al., 2023) have shown success in code-related tasks, such as code generation from natural language descriptions, also named as text-to-code generation (Chen et al., 2021; Austin et al., 2021; Li et al., 2022), code translation (Pan et al., 2024; Yang et al., 2024), and code completion (Izadi et al., 2024). However, LLM-based code generation remains challenging due to stringent lexical, grammatical, and semantic constraints (Scholak et al., 2021). To overcome these challenges, multiple initial programs are generated (Chen et al., 2021; Chowdhery et al., 2023), followed by different best-program selection strategies to improve code generation performance over LLMs (Li et al., 2022; Chen et al., 2023a; Zhang et al., 2023; Ni et al., 2023).\nCode generation substantially benefits from the empirical insights of human programmers. In practice, human programmers develop high-quality code by consistently identifying and rectifying errors through the analysis of test case executions, rather than a single effort (Huang et al., 2023c; Chen et al., 2023b). Different studies have refined programs based on execution results and LLM-"}, {"title": "2 PROBLEM FORMULATION", "content": "We follow the problem formulation for text-to-code generation as outlined in Jiang et al. (2023), Chen et al. (2023b), and Zhong et al. (2024). The text-to-code generation problem is formulated as a triple $P = (Q, T_v, T_h)$, where Q represents the problem specifications described in natural language, and $T_v$ and $T_h$ are sets of visible and hidden tests, each containing input-output pairs $(t^i, t^o) \\in T = T_v \\cup T_h$. The goal is to leverage the LLM M to generate a program function $f, M \\rightarrow f$, that maps each input $t^i$ to its corresponding output $t^o$ for all pairs in T, i.e., $f(t^i) = t^o$, for $(t^i, t^o) \\in T$. We note that $T_h$ remains hidden during both solution generation and code implementation phases and only becomes visible if the generated f passes $T_v$. In LPW, for all components shown in Figure 1, the problem description Q is, by default, concatenated with task-specific prompts to produce the desired response from LLMs."}, {"title": "3 WORKFLOW STRUCTURE", "content": "In this section, we first detail the two phases of LPW separately and then elaborate on the iterative update strategies used in each phase.\nSolution Generation. Figure 2 displays the overall workflow of the solution generation phase in LPW (part (a)), with an example programming problem for illustration (part (b)). LPW leverages the self-planning approach introduced by Jiang et al. (2023) to abstract and decompose the problem description Q into a strategic and adaptable plan \u03a0 at the start of the solution generation phase. For a problem in HumanEval described by block (1) in Figure 2, its example solution plan is illustrated at block (3). However, the LLM-generated plan \u03a0 may occasionally be incorrect, misguiding subsequent program generation. To avoid this, LPW queries the LLM to verify \u03a0 against all visible tests $T_v$. The LLM-responded plan verification $A(\\Pi, T_v)$ delivers a step-by-step analysis, including all intermediate results and final derived outputs for all visible tests $T_v$ based on \u03a0. For each $t_v \\in T_v$, its verification $A(\\Pi, \\{t_v\\})$ compares the derived output $t'$ to with the ground-truth output $t^o$ to assess the correctness of \u03a0, as outlined at block 4 in Figure 2. If \u03a0 is successfully verified on all visible tests, where in $A(\\Pi, T_v), t' = t^o, \\forall t_v \\in T_v$, then the plan verification $A(\\Pi, T_v)$ is reviewed by the LLM again to ensure the accuracy of all intermediate results, since each intermediate step result is used in locating bugs and providing refinement suggestions when compared with the code runtime information on the failed visible test. If all intermediate outputs in $A(\\Pi, T_v)$ are validated as correct by the LLM as shown at block 5 in Figure 2, $A(\\Pi, T_v)$ is treated as the intended solution for $T_v$. The plan \u03a0 and its verification $A(\\Pi, T_v)$ serve as the output of the solution generation phase, guiding code development and refinements in the code implementation phase.\nCode Implementation. Figure 3 shows the overall workflow of the code implementation phase in LPW (part (a)), using the same problem from Figure 2 as an illustration (part (b)). LPW develops"}, {"title": "4 LPW WITH SAMPLING", "content": "Text-to-code generation benefits from both multiple sampling and debugging. These two approaches have evolved orthogonally. We propose a sampling variant of LPW, referred as SLPW. SLPW follows the same workflow and update mechanism as LPW but incorporates multiple plan samples {\u03a01,... Ik} and program samples {f1,... fq}. SLPW generates k plan samples at the beginning of the solution generation phase. For each iteration, SLPW leverages the UCB algorithm to competitively select a plan \u03a0 with the highest upper confidence interval. Then, SLPW performs the same verification process as LPW for \u03a0. When SLPW verifies \u03a0 over each visible test and the verification fails on a visible test, it uses the number of visible tests where the plan verification derives an accurate final output as a reward to update the confidence interval of \u03a0, and \u03a0 is replaced with the revised plan \u03a0'. Alternatively, when SLPW checks the correctness of intermediate outputs in"}, {"title": "5 EXPERIMENTS", "content": "Benchmarks. We evaluate LPW and SLPW on the well-established text-to-code benchmarks HumanEval, MBPP, HumanEval-ET, and MBPP-ET, where the given context outlines the intended functionality of the program to be synthesized. HumanEval-ET and MBPP-ET introduce approximately 100 additional hidden tests, covering numerous edge cases for each problem in HumanEval and MBPP, thus being regarded as more reliable benchmarks for code evaluation (Dong et al., 2023a). In HumanEval and HumanEval-ET, we treat the test cases described in the task description as visible tests, typically 2-5 per task. For MBPP, we consider its test set that contains 500 problems with 3 hidden tests per problem. We set the first hidden test as the visible test and treat the other two as hidden, consistent with studies (Chen et al., 2023b; Zhong et al., 2024; Ni et al., 2023; Shi et al., 2022). MBPP-ET uses the same set of problems and visible tests for each problem as MB\u0420\u0420.\nExperimental Setup. We compare LPW and SLPW with the representative code generation approaches Self-Planning (SP) (Jiang et al., 2023), Self-Debugging (+Expl) (SD) (Chen et al., 2023b) and Large Language Model Debugger (LDB) (Zhong et al., 2024). SP relies solely on the LLM-generated solution plan to produce the program solution in a single attempt without refinements. SD uses a rubber duck debugging approach in LLMs, where LLMs are prompted to provide explanations of generated programs as feedback for debugging. LDB, a state-of-the-art LLM debugger, segments"}, {"title": "6 CASE STUDY", "content": "Figure 5 illustrates example message fragments from LPW in the 120th problem of HumanEval using the GPT-3.5 backbone. LPW successfully generates the correct program, while Baseline, SP, SD, and LDB all fail. This problem requires to return a sorted array with the maximum k numbers. However, in the problem description (block (a)), the unspecified order in the output array introduces ambiguity, confusing other methods. LPW struggles at the initial solution plan (block (c)), while the"}, {"title": "7 RELATED WORK", "content": "Program Synthesis. Program synthesis remains an open challenge of generating a program within a target domain-specific language (DSL) from given specifications. One prevalent approach involves searching the large space of possible programs. For example, generalized planning whose solution is formalized as a planning program with pointers (Segovia-Aguas et al., 2024; Lei et al., 2023) has demonstrated promising results in synthesizing program solutions for abstract visual reasoning tasks (Lei et al., 2024) when the DSL is carefully designed. However, hand-crafted DSLs often suffer from limited generalization capacity, and the huge search space diminishes its effectiveness. Recently, large language models trained on vast corpora have excelled in natural language processing (NLP) tasks and have been extended to code generation e.g., GPT-series (Achiam et al., 2023; OpenAI, 2024), Llama-series (Meta, 2024; Roziere et al., 2023; Touvron et al., 2023), and Claude-series (Anthropic, 2024). LPW and SLPW leverage the strengths of LLMs in NLP tasks to generate intended solutions in natural language. These text-based solutions demonstrate high-quality logical reasoning steps and satisfactory accuracy, thereby effectively aiding subsequent code generation.\nPrompting Techniques. To imitate the logical chain in human brain when tackling reasoning tasks, prompting methods direct LLMs to decompose problems into solvable sub-problems (Jiang et al., 2023; Zhou et al., 2023; Lightman et al., 2024; Dhuliawala et al., 2023) and progressively infer the correct answer with intermediate outputs, as exemplified by chain-of-thought prompting (Wei et al., 2022; Kojima et al., 2022). Inspired by these studies, LPW and SLPW decompose a text-to-code problem into several sub-problems described by the solution plan and follow the chain-of-thought prompting idea to verify the solution plan against visible tests with step-by-step analysis. The generated plan and its verification provide step-by-step natural language instructions for code generation, aiding LLMs in both the initial code development and subsequent refinements.\nCode Refinement. Accurate program solutions often require iterative refinements due to model limitations (Zhong et al., 2024; Chen et al., 2023b; Shinn et al., 2023). Various interactive approaches have been proposed to optimize debugging performance in LLMs, such as human feedback Chen et al. (2024); Le et al. (2022); Wu et al. (2023), trained models (Huang et al., 2023b; Le et al., 2022; Yasunaga & Liang, 2021), LLM-generated explanations (Chen et al., 2023b; Madaan et al., 2023; Shinn et al., 2023; Tang et al., 2023), and execution results (Zhong et al., 2024; Holt et al., 2024; Tian & Chen, 2023). Current state-of-the-art LLM debuggers, such as Self-Debugging and LDB, repair various seed programs to create the program solution. However, they encounter difficulties when the initial code substantially deviates from the original intent. Besides, without safeguarding, the refined code frequently diverges from the problem specifications. In contrast, LPW and SLPW develop initial code that adheres to the validated intended solution through plan verification, minimizing deviations from the problem description. The plan verification further guides code refinement, ensuring alignment with the problem specifications."}, {"title": "8 CONCLUSION", "content": "We propose LPW, a large language model programming workflow, for text-to-code generation tasks, which enables LLMs to accurately draft an initial program and effectively correct bugs. LPW uses various advanced code generation techniques and efficiently incorporates them into a two-phase development model. We further present SLPW, a sampling variant of LPW, where multiple initial programs are generated and then competitively refined as necessary. We evaluate LPW and SLPW on well-established text-to-code generation benchmarks across various LLMs. LPW significantly improves code generation accuracy compared to other existing approaches. SLPW achieves new state-of-the-art Pass@1 accuracy, with 98.2% on HumanEval, 84.8% on MBPP, 64.0% on APPS, and 35.3% on CodeContests benchmarks using GPT-40 as the backbone. These results highlight"}]}