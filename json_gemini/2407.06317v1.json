{"title": "ENHANCED SAFETY IN AUTONOMOUS DRIVING: INTEGRATING\nLATENT STATE DIFFUSION MODEL FOR END-TO-END\nNAVIGATION", "authors": ["Jianuo Huang", "Zhenlong Fang"], "abstract": "With the advancement of autonomous driving, ensuring safety during motion planning and navigation\nis becoming more and more important. However, most end-to-end planning methods suffer from a lack\nof safety. This research addresses the safety issue in the control optimization problem of autonomous\ndriving, formulated as Constrained Markov Decision Processes (CMDPs). We propose a novel, model-\nbased approach for policy optimization, utilizing a conditional Value-at-Risk based Soft Actor Critic\nto manage constraints in complex, high-dimensional state spaces effectively. Our method introduces\na worst-case actor to guide safe exploration, ensuring rigorous adherence to safety requirements\neven in unpredictable scenarios. The policy optimization employs the Augmented Lagrangian\nmethod and leverages latent diffusion models to predict and simulate future trajectories. This dual\napproach not only aids in navigating environments safely but also refines the policy's performance by\nintegrating distribution modeling to account for environmental uncertainties. Empirical evaluations\nconducted in both simulated and real environment demonstrate that our approach outperforms\nexisting methods in terms of safety, efficiency, and decision-making capabilities. Code is available at\nhttps://github.com/arnohuangjianuo/safeRL", "sections": [{"title": "Introduction", "content": "In the rapidly evolving field of autonomous driving, ensuring the safety of vehicles during the exploration phase\nis very important [1]. Traditional end-to-end methods often struggle to guarantee safety, particularly in complex,\nhigh-dimensional environments [2, 3]. The increased complexity of the state space in such scenarios not only makes the\nsampling and learning processes inefficient but also complicates the pursuit of globally optimal policies. The outcomes\nof inadequate safety measures can be severe, ranging from irreversible system damage to significant threats to human\nlife.\nReinforcement Learning (RL) has achieved significant success across various fields [11, 19]. Furthermore, Deep\nlearning (DL) is known for its strong perception ability, while reinforcement learning (RL) excels in decision-making.\nBy combining the strengths of DL and RL, deep reinforcement learning (DRL) offers a solution to the decision-planning\nproblem in complex obstacle avoidance scenarios. Unlike traditional motion planning methods, DRL can enhance the\nadaptability and generalization ability across different scenarios, thus overcoming the limitations of traditional methods\nand providing a more efficient and effective solution. Mnih et al.proposed a deep Q-network model that combines\nconvolutional neural networks and Q-learning from traditional RL to address high-dimensional perception-based\ndecision problems [19]. They employed a deep Q-network (DQN) to evaluate the Q-function for Q-learning. This\napproach has been widely adopted and serves as the primary driver for deep RL. DRL empowers robots with perceptual\nand decision-making abilities by processing input data to yield the output in an end-to-end manner [40]. The end-to-end\nmotion planning method treats the system as a whole, making it more robust [28, 5]. Furthermore, deep reinforcement\nlearning can handle high-dimensional and nonlinear environments by leveraging neural networks to learn complex state\nand action spaces[39, 46]. In contrast, traditional heuristic algorithms require manual design of the state and action\nspaces and may need rules to be redesigned when encountering new scenarios, leading to algorithm limitations and\nperformance bottlenecks. Previous work in reinforcement learning (RL) and deep learning (DL) has demonstrated\nadvancements in handling complex scenarios, but traditional methods face limitations due to their reliance on manual\ndesign for state and action spaces, which cannot adapt well to new scenarios and often result in performance bottlenecks\nand adaptability issues.\nRecognizing these challenges, several approaches have been proposed. One of them is the framework of constrained\nMarkov Decision Processes (CMDPs), which have aimed to strike a balance between reward maximization and risk\nminimization by optimizing the trade-off between exploration and exploitation [20, 10]. Building on these previous\nworks, this study redefines the control optimization problem within the context of CMDPs. We introduce a novel,\nmodel-based policy optimization approach that incorporates the Augmented Lagrangian method, and latent diffusion\nmodels, specifically designed to address safety constraints effectively in autonomous navigation tasks, meanwhile,\noptimizing the navigation ability.\nOur methodology initiates with a latent variable model aimed at producing extended-horizon trajectories, thereby\nboosting our system's ability to predict future states more accurately. To further strengthen the safety of our model, we\nincorporate a conditional Value-at-Risk (VaR) within the Soft Actor-Critic (SAC) framework, which ensures that safety\nconstraints are met through the use of the Augmented Lagrangian method for efficiently solving the safety-constrained\noptimization challenges.\nFurthermore, in tackling scenarios of extreme risk, our approach includes a worst-case scenario planning method that\nleverages a \"worst-case actor\" during policy exploration to ensure the safest possible outcomes even in potentially\ndangerous conditions. To enhance our model's performance, we have integrated a latent diffusion model for state\nrepresentation learning, refining our system's ability to interpret complex environmental data.\nThe efficacy of our proposed approach is validated through extensive experiments. Our empirical findings demonstrate\nthe capability of our approach to manage and mitigate risks effectively in high-dimensional state spaces, thereby\nenhancing the safety and reliability of autonomous vehicles in complex driving environments.\nOur main contributions are listed below:\n\u2022 We introduce latent diffusion models for state representation learning, which enable the forecasting of future\nobservations, rewards, and actions. This capability allows for the simulation of future trajectories directly"}, {"title": "Related work", "content": "2.1 Safe reinforcement learning\nSafe Reinforcement Learning (Safe RL) is a method that combines safety measures with the usual learning process for\nagents in complex environments [27]. The development of safe and reliable autonomous driving systems requires a\ncomprehensive focus on safety throughout the control and decision-making processes. In this context, safe reinforcement\nlearning (Safe RL) emerges as a powerful tool for training controllers and planners capable of navigating dynamic\nenvironments while adhering to safety constraints.\nSafe RL algorithms address the safety challenge through various approaches. Notable examples include Constrained\nPolicy Optimization (CPO) [28], which optimizes policies within explicitly defined safety constraints to prevent unsafe\nactions. Trust Region Policy Optimization (TRPO) [29] and Proximal Policy Optimization (PPO) [30] adapt these\nalgorithms to incorporate safety considerations by introducing penalty terms or barrier functions. Gaussian Processes\n(GPs) model [31] environmental uncertainty, enabling safe exploration by quantifying the risk associated with different\nactions. Model Predictive Control (MPC)-based Safe RL [32] [33] predicts future states and optimizes actions over a\nfinite horizon while adhering to safety constraints.\nIn the context of autonomous driving, Safe RL plays a crucial role in ensuring safety. Applications include high-level\ndecision-making for tasks like lane changing [10] and route planning [34], considering the behavior of surrounding\nvehicles and potential conflicts. Low-level control integrates Safe RL with traditional control methods like MPC,\nensuring adherence to safety constraints while following the planned path [35] [36]. The combination of RL with\nrule-based systems encoding traffic laws guarantees that learned policies comply with regulations. Safety verification\ntechniques provide guarantees that learned policies remain safe even in the presence of uncertainties and dynamic\nobstacles. [37] [38]\nThe diverse Safe RL algorithms and their applications underscore their critical role in developing adaptable and safe\ncontrol systems. Continuous advancements in these methods will be crucial for the successful real-world deployment of\nautonomous vehicles.\n2.2 Reinforcement Learning with Latent State\nLatent dynamic models, which represent a cornerstone in the modeling of time-series data within the field of re-\ninforcement learning (RL), facilitate a profound understanding of hidden states and dynamic changes in complex\nenvironments [40, 41, 42]. These models capture essential relationships between unobservable internal states and\nobserved data, significantly enhancing the predictive capabilities of RL agents.\nIn the RL context, latent dynamic models operate under probabilistic frameworks, employing Bayesian inference or\nmaximum likelihood estimation to deduce both model parameters and hidden states with increased accuracy [43, 44].\nThese frameworks enable the modeling of environments in a way that aligns with the probabilistic nature of real-world\ndynamics.\nMathematically, latent dynamic models are characterized by:\n\u2022 State Transition Equation:\n$s_{t+1} = f(s_t, a_t, e_t)$\nwhere $s_t$ denotes the latent state at time t, $a_t$ the action taken, and $e_t$ the stochasticity inherent in the\nenvironment. The function f may be deterministic or stochastic, encapsulating the uncertainty of state\ntransitions.\n\u2022 Observation Equation:\n$O_t = g(s_t, d_t)$"}, {"title": "Problem Modeling", "content": "In the problem of autonomous navigation, we model the interaction of an autonomous agent with a dynamic and uncertain\nenvironment using a finite-horizon Markov Decision Process (MDP), defined by the tuple M ~ (S, O, A,P, r, \u03b3).\nHere, $S \\subset \\mathbb{R}^n$ represents a high-dimensional continuous state space, and $A \\subset \\mathbb{R}^m$ denotes the action space. State\ntransitions are defined by $s_{t+1} \\sim P(\\cdot | s_t, a_t)$, reflecting the stochastic nature of the environment.\nObservations O, derived from the state space, are typically high-dimensional images captured by sensors and processed\nvia a latent diffusion model to enhance state representation. This allows for a more comprehensive understanding of\nenvironmental dynamics, which is important for navigating complex scenarios. The reward function $r : S\\times A \\times S \\rightarrow \\mathbb{R}$\nand the discount factor \u03b3\u2208 [0, 1] guide the agent's policy $\u03c0_\u03b8$, which generates actions based on processed observations\n$O_t$.\nWe emphasize safety through the integration of a conditional Value-at-Risk (VaR) metric within our decision-making\nframework, addressing the worst-case scenarios with a novel approach that involves latent state diffusion for robust state\nrepresentation learning. Safety mechanisms are formalized through a subset $S_u \\subset \\mathbb{R}^n$, where entering a state $s_t \\in S_u$\nindicates a potential safety violation, monitored by a safety function K. The objective extends beyond maximizing\ncumulative rewards to ensuring minimal safety violations:\n$\\max_\\theta J(\u03c0_\u03b8) = E_{p(s_t, a_t, s_{t+1})} [\\sum_{t=0}^{T} \u03b3^t r(s_t, a_t, s_{t+1})]$ s.t., $\\sum_{t=0}^{T}\u03ba(s_t) \u2264 D, a_t \\sim \u03c0_\u03b8 (\\cdot | O_t). $\nHere, \u03ba(st) \u2208 {0,1} serves as an indicator of safety violations, with D \u2208 R representing the maximum allowable\nsafety violations, aiming for D \u2192 0 to enhance operational safety. This safety-constrained MDP framework allows the\nagent to learn navigation policies that maximize efficiency and ensure safety, effectively balancing between achieving\nhigh performance and adhering to critical safety constraints."}, {"title": "Methodology", "content": "4.1 Constrained Markov Decision Process formulation\nReinforcement Learning is the process of continuous interaction between the agent and the environment. In this\nstudy, we focus on the safety of autonomous driving, which requires a trade-off between reward and safety. We\nformulate the control optimization problem as a Constraint Markov Decision Process (CMDP). CMDP is described\nby (S, A, p, r, c, d, y), including the state space, action space, transition model, reward, cost, constraint, and discount\nfactor. Each step, the agent receives reward r and cost c, the optimization target is to maximize the reward in the safety\nconstraint in Equation 2.\n$\\max_\u03c0 E_{(s_t,a_t)\u223c\u03c4_\u03c0} [\u2211_t \u03b3^t r(s_t, a_t)]$\ns.t. $E_{(s_t,a_t)\u223c\u03c0} [\u2211_t \u03b3^t c(s_t, a_t)] \u2264 d$\nwhere \u03c0, \u03c4\u03c0 denotes for the policy and the trajectory distribution of policy.\n4.2 Build the Latent Diffusion Model for State Representation\nOur model designs a sophisticated latent model consisting of three primary components: a representation model, a\ntransition model, and a reward model. Each component plays a crucial role in the system's ability to predict and navigate\ncomplex environments by learning from both observed and imagined trajectories.\n\u2022 Representation Model: The representation model is crucial for establishing a robust latent space based on past\nexperiences. It is formalized as $p(S_\u03c4 | S_{\u03c4\u22121}, A_{\u03c4\u22121}, O_\u03c4)$, where the model predicts the next state by synthesizing"}, {"title": "Build Safety Guarantee", "content": "In our framework, p denotes the distribution sampling from environment interactions, while q represents predictions\ngenerated from the latent imagination. We use 7 to index time steps within the latent space. The core innovation of our\nmodel lies in its ability to create and refine a latent imagination space that predicts future trajectories. This capability\nenables the agent to safely explore and learn optimal behaviors by iteratively refining the latent space to avoid unsafe\nstates and maximize efficiency.\naction is not only optimal in terms of performance but also satisfies predefined safety criteria, ensuring that the control\naction adheres to safety standards before execution.\nThis structured approach ensures a robust safety guarantee mechanism, integrating both performance optimization and\nrigorous safety compliance, which is crucial for autonomous systems operating in dynamic and uncertain environments.\nSimilar approach can be found in Latent imagination e.g., Dreamer [13], and got great performance. However, Dreamer\ndid not take safety constraints into consideration. Though Dreamer achieved high reward target, without safety, the\nagent might cause irreversible incident in order to reach higher reward. It is significant to introduce the constraints of\nsafety to balance reward and risk.\nIn the latent imagination, we leverage the distributional reinforcement learning to solve the safety-constrained RL\nproblem [24]. Instead of following a policy \u03c0 to get an action value, distributional RL considers on the expectation of\nvalue and cost [25]. We focus on the actor-critic policy and build a model-based algorithm together with the latent\nimagination. Soft actor-critic (SAC) [26] introduces the concept of maximum entropy balancing the exploitation and\nexploration:\n$\\pi^* = \\argmax_\u03c0 \\sum_{t=0}^{T}E_{(s_t,a_t)\u223c\u03c1_\u03c0}[\u03b3^t (r(s_t, a_t) + \u03b2H(\u03c0(\\cdot|s_t))]$\nwhere \u03c0* denotes the optimal policy, \u1e9e denotes the stochasticity of \u03c0.\nAgents can learn the optimal policy without safety concern, however, irreversible situation such as collision cannot\nbe accepted in autonomous driving. We consider the safety constraints and formulate it as a Lagrangian method with\nconstraints:\n$\\max_\u03c0 E_{(s_t,a_t)\u223c\u03c1_\u03c0} [\\sum_{t} \u03b3^t r(s_t, a_t)]$\ns.t. $E_{(s_t,a_t)\u223c\u03c1_\u03c0}[\\sum_{\u03c4}c(s_t, a_t)] \u2264 d$\n$E_{(s_t,a_t)\u223c\u03c1_\u03c0}[-log(\u03c0_t(a_t|s_t))] \u2265 H_0 \u2200t$\n$h(s_t) \u22640  (Safety Constraint)$\n$h(s_{t+1}) \u2264 (1-\u03b1)h(s_t)  (Control Barrier Function)$\nTo better use the constraints of safety, barrier function is also leveraged to modify the risk value in distributional RL so\nthat the agent can evaluate when to explore more and when to be conservative. In this formulation, For a constraint with\nrelative degree m, the generalized control barrier function is:\n$h(s_{t+m}) \u2264 (1-\u03b1)h(s_t)$\nFor a certain risk level a, we optimize policy until \u0393 satisfies:\n$\u0393_\u03c0(s, a, \u03b1) = CVaR_\u03b1 = Q^\u03c0(s, a) + \u03b1^{-1}\u00a2(\u03a6^{-1}(\u03b1)) \\sqrt{V(s, a)}$\n$\u0393_\u03c0(s_{t+m}, a_{t+m}, a) \u2264 (1 \u2212 \u03b1)\u0393_\u03c0(s_t, a_t, a)$\nwhere a is the conservativeness coefficient.\nCritic, Instead of Q(s, a) is the expectation of long-term cumulative costs from starting point (s, a), denoted by:\n$C_\u03c0(s,a) = \\sum_t V^t c(s_t, a_t) | s_0 = s, a_0 = a$\nFollowing policy \u03c0,the probability distribution of Cr(s, a) is modeled as $p^\u03c0(C | s, a)$, such that:\n$Q_\u03c0(s, a) = E_{p^\u03c0} [C | s, a].$\nThe distributional Bellman Operator $T^\u03c0$ is defined as:\n$T^\u03c0C(s,a) = c(s, a) + C^\u03c0(s', a')$ where $s' \u223c p(\\cdot | s,a), a' \u223c \u03c0(\u00b7 | a')$. We approximate the distribution C\u2081(s, a) with a Gaussian distribution C\u2081(s, a) \u223c $N (Q(s,a), V(s, a))$, where $V(s, a) = E_{p^\u03c0} [C^2 | s, a] \u2013 Q_\u03c0(s, a)^2.$\n- To estimate $Q^\u03c0$, we can use Bellman function:\n$Q^\u03c0(s, a) = c(s, a) + \u03b3\u2211_{s'\u2208S} p (s' | s, a) \u2211_{\u03b1' \u2208A}\u03c0 (\u03b1' | s') Q^\u03c0(s', a').$\n- The projection equation for estimating V\u03c0(s, a) is:\n$V(s,a) = c(s, a)^2 - Q(s, a)^2 + 2\u03b3c(s, a) \u2211_{s'\u2208S} p (s' | s,a) \u2211_{\u03b1' \u2208A}\u03c0(\u03b1' | s') Q^\u03c0(s', a') +$\n$+\u03b3^2 \u2211_{s'\u2208S}p (s' | s,a) \u2211_{\u03b1' \u2208A}\u03c0 (\u03b1' | s') V^\u03c0(s', a') + \u03b3^2 \u2211_{s'\u2208S} p (s' | s, a) \u2211_{\u03b1' \u2208A} \u03c0 (\u03b1' | s') Q^\u03c0(s', a')^2.$\nWe use two neural networks parameterized by \u03bc and \u03b7 respectively to estimate the safety-critic:\n$Q_\u03bc(s, a) \u2192 Q^\u03c0(s, a)$ and $V_\u03b7(s, a) \u2192 V^\u03c0(s,a)$\n2-Wasserstein distance: $W_2(\u03bc, v) = ||Q1 - Q2||^2 + trace ((V_1 + V_2-2(V_2^{1/2}V_1V_2^{1/2})^{1/2}) ). The 2-Wasserstein\ndistance can be computed as the Temporal Difference (TD) error based on the projection equations $Q^\u03c0(s, a)$ and\n$V^\u03c0(s, a)$ to update the safety-critic, i.e., we will minimize the following values:\n$J_c(\u03bc) = E_{(s_t,a_t)\u223cD} ||\u2206Q(s_t, a_t, \u03bc) ||^2$\n$J_v(\u03b7) = E_{(s_t,a_t)\u223cD} trace (\u2206V (s_t, a_t, \u03bc))$\nWhere, \u03bc and \u03b7 are two parameters that represents the two neural network in safety critic, Jc(\u03bc) is the loss function of\n$Q_\u03bc(s, a)$ and Jv(\u03b7) is the loss function of V\u03b7(s, a). We can get:\n$\u2206Q (s_t, a_t, \u03bc) = Q_\u03bc (s_t, a_t) \u2013 Q^\u03c0(s, a)$\n$\u25b3V (s_t, a_t, \u03b7) = V_\u03b7 (s_t, a_t) + V^\u03c0 (s_t, a_t) - 2 (V_\u03bc (s_t, a_t) V (s_t, ar) V (s_t, at))^{1-2}$\nwhere $Q^\u03c0_\u03bc (s_t, a_t)$ and $VF\u03b7 (s_t, a_t)$ is the TD target.\nActor, We focus on the expectation of cumulative reward and cost to avoid the impact of random sampling from\nthe Gaussian distribution. A parameter a is designed to set the risk attention, denotes the risk-aware level on safety.\n\u03b1\u2208 (0, 1), the smaller scalar means more pessimistic in safety, whereas the larger one expect a less risk-aware state.\nWe set a a-percentile $F^{-1}(1 \u2013 a)$, based on the Conditional Value at Risk (CVaR), we can get $CVaR_\u03b1 =$\n$E_{p^\u03c0} [C | C\u2265 F^{-1} (1 \u2013 a)]$. Given the risk-aware level manually by different situation or standard, the control\noptimization problem can be modified into:\n$E_{\u03c1^\u03c0} [C (s_t, a_t) | C (s_t, a_t) \u2265 F^{-1} (1-\u03b1)] \u2264 d t$\nConsidering the critic mentioned before, the safety measurements can be changed into:\n$\u0393_\u03c0(s, a, \u03b1) = CVaR_\u03b1 = Q^\u03c0_\u03bc(s,a) + \u03b1^{-1}\u00a2(\u03a6^{-1}(\u03b1)) \\sqrt{V(s,a)}$\nAs the safety critic, we also define a reward critic with the neural network parameter & and loss function $J_R (V_i)$. Given\nthe risk level, we use $\u0393_\u03c0(s, a, \u03b1) \u2264 d t$ to find the optimal policy \u03c0.\n4.4 Value-at-Risk Based Soft Actor-Critic for Safe Exploration\nAs shown in Algorithm 1, we build the latent imagination by updating the representation model, transition model\nand reward model. We predict the future trajectories based on the transition model, and regard trajectories as latent\nimagination for continuous actor-critic policy optimization. Given a specific risk-aware level, new safety constraint is\nset. Considering the expectation of cumulative cost and reward, we can update the model by distributional RL. After\ncontinuous learning in the latent imagination for some discrete time step, the agent interacts with environment and gets\nnew reward and cost data."}, {"title": "Experiments", "content": "5.1 Environmental Setup\n5.1.1 Experimental Setup in CARLA Simulator\nIn our study, we utilized the CARLA simulator to construct and evaluate various safety-critical scenarios that challenge\nthe response capabilities of autonomous driving systems. CARLA provides a rich, open-source environment tailored\nfor autonomous driving research, offering realistic urban simulations.\nSpecific scenario\nWe designed specific scenarios in CARLA to assess different aspects of autonomous vehicle behavior, as shown in\nFigure 5. These scenarios include:\n\u2022 Traffic Negotiation: Multiple vehicles interact at a complex intersection, testing the vehicle's ability to\nnegotiate right-of-way and avoid collisions.\n\u2022 Highway: Simulates high-speed driving conditions with lane changes and merges, evaluating the vehicle's\ndecision-making speed and accuracy.\n\u2022 Obstacle Avoidance: Challenges the vehicle to detect and maneuver around suddenly appearing obstacles like\nroadblocks.\n\u2022 Braking and Lane Changing: Tests the vehicle's response to emergency braking scenarios and rapid lane\nchanges to evade potential hazards.\nThese scenarios are crucial for validating the robustness and reliability of safety protocols within autonomous vehicles\nunder diverse urban conditions.\nUrban Driving Environments\nAdditionally, we tested the vehicles across three different urban layouts in CARLA, depicted in Figure 5. We pick up\nthose towns based on the following reasons:\n\u2022 Town 6: Features a typical urban grid which simplifies navigation but tests basic traffic rules adherence.\n\u2022 Town 7: Includes winding roads and a central water feature, introducing complexity to navigation tasks and\nrequiring advanced path planning.\n\u2022 Town 10: Presents a dense urban environment with numerous intersections and limited maneuvering space,\nmaking it ideal for testing advanced navigation strategies.\nThe detailed simulation environments provided by CARLA, combined with the constructed scenarios, enable compre-\nhensive testing of autonomous driving algorithms, ensuring that they can operate safely and efficiently in real-world\nconditions.\n5.2 Design of the Reward Function\nOur autonomous system employs a multifaceted reward function designed to optimize both the efficiency and safety of\nnavigation. The reward function is segmented into various components that collectively ensure the vehicle adheres to\noperational standards:\nVelocity Compliance Reward (R): This reward is granted for maintaining a specified target velocity, promoting\nefficient transit and fuel economy:\n$R_v = {1 \\text{ if } |v_{\\text{current}} - v_{\\text{target}}| \\text{ otherwise}}$\nwhere $v_{\\text{current}}$ is the vehicle's current velocity, $v_{\\text{target}}$ is the target velocity, and \u039b is a factor that penalizes deviations from\nthis target."}, {"title": "Evaluate robustness", "content": "We further try to evaluate the robustness of our proposed framework to different types of obstacles. By adjusting the\nspeed of moving obstacles, we can simulate various levels of dynamic complexity and observe how well the model\nanticipates and reacts to changing trajectories and potential hazards. This test not only demonstrate the model's obstacle\navoidance skills but also can evaluate its potential real-world applicability and robustness under varying speeds and\ndensities of pedestrian traffic. The moving speed of the dynamic object is 1,2,3m/s. We tried to fix the start points and\ngoal points."}, {"title": "Lane Maintenance Reward (R)", "content": "This reward encourages the vehicle to remain within the designated driving lane:\n$R_l = {1 \\text{ if } d_{\\text{offset}} \\text{ otherwise}}$\nwhere $d_{\\text{offset}}$ is the lateral displacement from the lane center, and $d_{\\text{max}}$ is the threshold beyond which penalties are\nincurred.\nOrientation Alignment Reward (R): This component penalizes the vehicle for incorrect heading angles:\n$R_o = \\frac{1}{1 + \u03bc \\text{influences the strictness of the alignment requirement.}}\nExploration Incentive Reward (R): A novel component introduced to encourage exploration of less frequented\npaths, enhancing the robustness of the navigation strategy:\n$R_e = exp(-vN_{\\text{visits}})$\nwhere $n_{\\text{visits}}$ counts the number of times a specific path or region has been traversed, and v is a decay factor that reduces\nthe reward with repeated visits.\nComposite Reward Calculation: The overall reward (Rtotal) is a composite measure, formulated as follows:\n$R_{\\text{total}} = w_v \\cdot R_v + \u03c9_l \\cdot R_l + \u03c9_o R_o + \u03c9_e R_e$\nwhere \u03c9\u03bd, \u03c9\u03b9, wo, and we are weights that prioritize different aspects of the reward structure based on strategic objectives.\n5.3 Evaluation Metrics\nTo rigorously assess the performance of autonomous driving systems in our simulation, we employ a comprehensive set\nof metrics that encompass various aspects of driving quality, including safety, efficiency, and rule compliance. The\nmetrics are defined as follows:\n1. Route Completion (RC): This metric quantifies the percentage of each route successfully completed by the\nagent without intervention. It is defined as:\n$RC = \\frac{1}{N} \u2211_{i=1}^N R_i \u00d7 100%$\nwhere Ri represents the completion rate for the i-th route. A penalty is applied if the agent deviates from the\ndesignated route, reducing RC proportionally to the off-route distance.\n2. Infraction Score (IS): Capturing the cumulative effect of driving infractions, this score is calculated using a\ngeometric series where each infraction type has a designated penalty coefficient:\n$IS = \u03a0 (p_j)^{#infractions_j}$\nj={Ped, Veh, Stat, Red}\nCoefficients are set as $p_{\\text{ped}} = 0.50, p_{\\text{veh}} = 0.60, p_{\\text{stat}} = 0.65$, and $p_{\\text{red}} 0.70$ for infractions involving\npedestrians, vehicles, static objects, and red lights, respectively.\n3. Driving Score (DS): This is the primary evaluation metric on the leaderboard, combining Route Completion\nwith an infraction penalty:\n$DS = \\frac{1}{N} \u2211_{i=1}^N R_i \u00d7 P_i$\nwhere Pi is the penalty multiplier for infractions on the i-th route."}, {"title": "Collision Occurrences (CO):", "content": "This metric quantifies the frequency of collisions that occur during autonomous\ndriving. It provides an important measure of the safety and reliability of the driving algorithm. A lower\nCO value indicates that the system is better at avoiding collisions, which is critical for the safe operation of\nautonomous vehicles. This metric is defined as:\n$CO = \\frac{\\text{Number of Collisions}}{\\text{Total Distance Driven}} \u00d7 100$\nwhere the Number of Collisions is the total count of collisions encountered by the autonomous vehicle, and\nthe Total Distance Driven is the total distance covered by the vehicle during the testing period. This metric is\nexpressed as a percentage to standardize the measure across different distances driven.\n5. Infractions per Kilometer (IPK): This metric normalizes the total number of infractions by the distance\ndriven to provide a measure of infractions per unit distance:\n$IPK = \\frac{\u2211_{i=1}^N I_i}{\u2211_{i=1}^N K_i}$\nwhere $I_i$ is the number of infractions on the i-th route and K\u2081 is the distance driven on the i-th route.\n6. Time to Collision (TTC): This metric estimates the time remaining before a collision would occur if the\ncurrent velocity and trajectory of the vehicle and any object or vehicle in its path remain unchanged. It is a\ncritical measure of the vehicle's ability to detect and react to potential hazards in its immediate environment:\n$TTC = min(\\frac{d}{V_{rel}})$\nwhere d represents the distance to the closest object in the vehicle's path and $v_{rel}$ is the relative velocity towards\nthe object. A lower TTC value indicates a higher immediate risk, triggering more urgent responses from the\nsystem.\n7. Collision Rate (CR): This metric quantifies the frequency of collisions during autonomous operation, providing\na direct measure of safety in operational terms:\n$CR= \\frac{\\text{Number of Collisions}}{\\text{Total Distance Driven}}$\nexpressed in collisions per kilometer. This metric helps in evaluating the overall effectiveness of the collision\navoidance systems embedded within the autonomous driving algorithms.\nThese metrics collectively provide a robust framework for evaluating autonomous driving systems under varied driving\nconditions, facilitating a detailed analysis of their capability to navigate complex urban environments while adhering to\ntraffic rules and maintaining high safety standards.\n5.4 Baseline setup\n\u2022 Dreamer [13]: this is a reinforcement learning agent designed to solve long-horizon tasks purely from images\nby leveraging latent imagination within learned world models. It stands out by processing high-dimensional\nsensory inputs through deep learning to efficiently learn complex behaviors.\n\u2022 LatentSW-PPO [12]:Wang et al proposes a novel reinforcement learning (RL) framework for autonomous\ndriving that enhances safety and efficiency. This framework incorporates a latent dynamic model to capture\nthe environment's dynamics from bird's-eye view images, boosting learning efficiency and reducing safety\nrisks through synthetic data generation. It also introduces state-wise safety constraints using a barrier function\nto ensure safety at each state during the learning process.\n\u2022\nFurthermore, we also propose several ablative versions of our method to evaluate the performance of each sub-module.\n\u2022 Safe Autonomous Driving with Latent End-to-end Navigation(SAD-LEN ): This version removes the latent\ndiffusion component, relying solely on traditional latent state representation without the enhanced generaliza-\ntion and predictive capabilities provided by diffusion processes. The primary focus is on evaluating the impact\nof the latent state representation on navigation performance without the diffusion-based enhancements.\n\u2022 Autonomous Driving with End-to-end Navigation and Diffusion (AD-END): This version eliminates the safe\nguarantee mechanisms. It focuses on the combination of end-to-end navigation with diffusion models to\nunderstand how much the safety constraints contribute to overall performance and safety."}, {"title": "Results and analysis", "content": "6.1 Evaluating Safety and Efficiency during Exploratory\nWe first evaluate the performance of those scenarios constructed in Figure 5. The main evaluation criteria are taken\nfrom above. We sample different training configurations and plot the reward curve and record the evaluation criteria in\nthe table.\nIn our comprehensive evaluation of autonomous driving systems, the Enhanced Safety in Autonomous Driving:\nIntegrating Latent State Diffusion Model for End-to-End Navigation (ESAD-LEND) demonstrated superior performance\nacross multiple metrics in a simulated testing environment. Achieving the highest Driving Score (92.2%) and Route\nCompletion (98.3%), ESAD-LEND outstripped all comparative methods, including the baseline SAC, which lagged\nsignificantly at a Driving Score of 78.2% and Route Completion of 90.1%. Moreover, ESAD-LEND exhibited\nremarkable compliance and safety, recording the lowest Infraction Score (0.5%), indicative of fewer traffic violations\nand enhanced adherence to safety protocols. Operational efficiency was also notably superior, with the lowest scores\nin Collision Rate and Risk Indicators, suggesting reduced incidences and smoother operational flow. Additionally,"}, {"title": "Evaluate generalization ability", "content": "We try furthermore two experiments"}, {"title": "ENHANCED SAFETY IN AUTONOMOUS DRIVING: INTEGRATING\nLATENT STATE DIFFUSION MODEL FOR END-TO-END\nNAVIGATION", "authors": ["Jianuo Huang", "Zhenlong Fang"], "abstract": "With the advancement of autonomous driving, ensuring safety during motion planning and navigation\nis becoming more and more important. However, most end-to-end planning methods suffer from a lack\nof safety. This research addresses the safety issue in the control optimization problem of autonomous\ndriving, formulated as Constrained Markov Decision Processes (CMDPs). We propose a novel, model-\nbased approach for policy optimization, utilizing a conditional Value-at-Risk based Soft Actor Critic\nto manage constraints in complex, high-dimensional state spaces effectively. Our method introduces\na worst-case actor to guide safe exploration, ensuring rigorous adherence to safety requirements\neven in unpredictable scenarios. The policy optimization employs the Augmented Lagrangian\nmethod and leverages latent diffusion models to predict and simulate future trajectories. This dual\napproach not only aids in navigating environments safely but also refines the policy's performance by\nintegrating distribution modeling to account for environmental uncertainties. Empirical evaluations\nconducted in both simulated and real environment demonstrate that our approach outperforms\nexisting methods in terms of safety, efficiency, and decision-making capabilities. Code is available at\nhttps://github.com/arnohuangjianuo/safeRL", "sections": [{"title": "Introduction", "content": "In the rapidly evolving field of autonomous driving, ensuring the safety of vehicles during the exploration phase\nis very important [1]. Traditional end-to-end methods often struggle to guarantee safety, particularly in complex,\nhigh-dimensional environments [2, 3]. The increased complexity of the state space in such scenarios not only makes the\nsampling and learning processes inefficient but also complicates the pursuit of globally optimal policies. The outcomes\nof inadequate safety measures can be severe, ranging from irreversible system damage to significant threats to human\nlife.\nReinforcement Learning (RL) has achieved significant success across various fields [11, 19]. Furthermore, Deep\nlearning (DL) is known for its strong perception ability, while reinforcement learning (RL) excels in decision-making.\nBy combining the strengths of DL and RL, deep reinforcement learning (DRL) offers a solution to the decision-planning\nproblem in complex obstacle avoidance scenarios. Unlike traditional motion planning methods, DRL can enhance the\nadaptability and generalization ability across different scenarios, thus overcoming the limitations of traditional methods\nand providing a more efficient and effective solution. Mnih et al.proposed a deep Q-network model that combines\nconvolutional neural networks and Q-learning from traditional RL to address high-dimensional perception-based\ndecision problems [19]. They employed a deep Q-network (DQN) to evaluate the Q-function for Q-learning. This\napproach has been widely adopted and serves as the primary driver for deep RL. DRL empowers robots with perceptual\nand decision-making abilities by processing input data to yield the output in an end-to-end manner [40]. The end-to-end\nmotion planning method treats the system as a whole, making it more robust [28, 5]. Furthermore, deep reinforcement\nlearning can handle high-dimensional and nonlinear environments by leveraging neural networks to learn complex state\nand action spaces[39, 46]. In contrast, traditional heuristic algorithms require manual design of the state and action\nspaces and may need rules to be redesigned when encountering new scenarios, leading to algorithm limitations and\nperformance bottlenecks. Previous work in reinforcement learning (RL) and deep learning (DL) has demonstrated\nadvancements in handling complex scenarios, but traditional methods face limitations due to their reliance on manual\ndesign for state and action spaces, which cannot adapt well to new scenarios and often result in performance bottlenecks\nand adaptability issues.\nRecognizing these challenges, several approaches have been proposed. One of them is the framework of constrained\nMarkov Decision Processes (CMDPs), which have aimed to strike a balance between reward maximization and risk\nminimization by optimizing the trade-off between exploration and exploitation [20, 10]. Building on these previous\nworks, this study redefines the control optimization problem within the context of CMDPs. We introduce a novel,\nmodel-based policy optimization approach that incorporates the Augmented Lagrangian method, and latent diffusion\nmodels, specifically designed to address safety constraints effectively in autonomous navigation tasks, meanwhile,\noptimizing the navigation ability.\nOur methodology initiates with a latent variable model aimed at producing extended-horizon trajectories, thereby\nboosting our system's ability to predict future states more accurately. To further strengthen the safety of our model, we\nincorporate a conditional Value-at-Risk (VaR) within the Soft Actor-Critic (SAC) framework, which ensures that safety\nconstraints are met through the use of the Augmented Lagrangian method for efficiently solving the safety-constrained\noptimization challenges.\nFurthermore, in tackling scenarios of extreme risk, our approach includes a worst-case scenario planning method that\nleverages a \"worst-case actor\" during policy exploration to ensure the safest possible outcomes even in potentially\ndangerous conditions. To enhance our model's performance, we have integrated a latent diffusion model for state\nrepresentation learning, refining our system's ability to interpret complex environmental data.\nThe efficacy of our proposed approach is validated through extensive experiments. Our empirical findings demonstrate\nthe capability of our approach to manage and mitigate risks effectively in high-dimensional state spaces, thereby\nenhancing the safety and reliability of autonomous vehicles in complex driving environments.\nOur main contributions are listed below:\n\u2022 We introduce latent diffusion models for state representation learning, which enable the forecasting of future\nobservations, rewards, and actions. This capability allows for the simulation of future trajectories directly"}, {"title": "Related work", "content": "2.1 Safe reinforcement learning\nSafe Reinforcement Learning (Safe RL) is a method that combines safety measures with the usual learning process for\nagents in complex environments [27]. The development of safe and reliable autonomous driving systems requires a\ncomprehensive focus on safety throughout the control and decision-making processes. In this context, safe reinforcement\nlearning (Safe RL) emerges as a powerful tool for training controllers and planners capable of navigating dynamic\nenvironments while adhering to safety constraints.\nSafe RL algorithms address the safety challenge through various approaches. Notable examples include Constrained\nPolicy Optimization (CPO) [28], which optimizes policies within explicitly defined safety constraints to prevent unsafe\nactions. Trust Region Policy Optimization (TRPO) [29] and Proximal Policy Optimization (PPO) [30] adapt these\nalgorithms to incorporate safety considerations by introducing penalty terms or barrier functions. Gaussian Processes\n(GPs) model [31] environmental uncertainty, enabling safe exploration by quantifying the risk associated with different\nactions. Model Predictive Control (MPC)-based Safe RL [32] [33] predicts future states and optimizes actions over a\nfinite horizon while adhering to safety constraints.\nIn the context of autonomous driving, Safe RL plays a crucial role in ensuring safety. Applications include high-level\ndecision-making for tasks like lane changing [10] and route planning [34], considering the behavior of surrounding\nvehicles and potential conflicts. Low-level control integrates Safe RL with traditional control methods like MPC,\nensuring adherence to safety constraints while following the planned path [35] [36]. The combination of RL with\nrule-based systems encoding traffic laws guarantees that learned policies comply with regulations. Safety verification\ntechniques provide guarantees that learned policies remain safe even in the presence of uncertainties and dynamic\nobstacles. [37] [38]\nThe diverse Safe RL algorithms and their applications underscore their critical role in developing adaptable and safe\ncontrol systems. Continuous advancements in these methods will be crucial for the successful real-world deployment of\nautonomous vehicles.\n2.2 Reinforcement Learning with Latent State\nLatent dynamic models, which represent a cornerstone in the modeling of time-series data within the field of re-\ninforcement learning (RL), facilitate a profound understanding of hidden states and dynamic changes in complex\nenvironments [40, 41, 42]. These models capture essential relationships between unobservable internal states and\nobserved data, significantly enhancing the predictive capabilities of RL agents.\nIn the RL context, latent dynamic models operate under probabilistic frameworks, employing Bayesian inference or\nmaximum likelihood estimation to deduce both model parameters and hidden states with increased accuracy [43, 44].\nThese frameworks enable the modeling of environments in a way that aligns with the probabilistic nature of real-world\ndynamics.\nMathematically, latent dynamic models are characterized by:\n\u2022 State Transition Equation:\n$s_{t+1} = f(s_t, a_t, e_t)$\nwhere $s_t$ denotes the latent state at time t, $a_t$ the action taken, and $e_t$ the stochasticity inherent in the\nenvironment. The function f may be deterministic or stochastic, encapsulating the uncertainty of state\ntransitions.\n\u2022 Observation Equation:\n$O_t = g(s_t, d_t)$"}, {"title": "Problem Modeling", "content": "In the problem of autonomous navigation, we model the interaction of an autonomous agent with a dynamic and uncertain\nenvironment using a finite-horizon Markov Decision Process (MDP), defined by the tuple M ~ (S, O, A,P, r, \u03b3).\nHere, $S \\subset \\mathbb{R}^n$ represents a high-dimensional continuous state space, and $A \\subset \\mathbb{R}^m$ denotes the action space. State\ntransitions are defined by $s_{t+1} \\sim P(\\cdot | s_t, a_t)$, reflecting the stochastic nature of the environment.\nObservations O, derived from the state space, are typically high-dimensional images captured by sensors and processed\nvia a latent diffusion model to enhance state representation. This allows for a more comprehensive understanding of\nenvironmental dynamics, which is important for navigating complex scenarios. The reward function $r : S\\times A \\times S \\rightarrow \\mathbb{R}$\nand the discount factor \u03b3\u2208 [0, 1] guide the agent's policy $\u03c0_\u03b8$, which generates actions based on processed observations\n$O_t$.\nWe emphasize safety through the integration of a conditional Value-at-Risk (VaR) metric within our decision-making\nframework, addressing the worst-case scenarios with a novel approach that involves latent state diffusion for robust state\nrepresentation learning. Safety mechanisms are formalized through a subset $S_u \\subset \\mathbb{R}^n$, where entering a state $s_t \\in S_u$\nindicates a potential safety violation, monitored by a safety function K. The objective extends beyond maximizing\ncumulative rewards to ensuring minimal safety violations:\n$\\max_\\theta J(\u03c0_\u03b8) = E_{p(s_t, a_t, s_{t+1})} [\\sum_{t=0}^{T} \u03b3^t r(s_t, a_t, s_{t+1})]$ s.t., $\\sum_{t=0}^{T}\u03ba(s_t) \u2264 D, a_t \\sim \u03c0_\u03b8 (\\cdot | O_t). $\nHere, \u03ba(st) \u2208 {0,1} serves as an indicator of safety violations, with D \u2208 R representing the maximum allowable\nsafety violations, aiming for D \u2192 0 to enhance operational safety. This safety-constrained MDP framework allows the\nagent to learn navigation policies that maximize efficiency and ensure safety, effectively balancing between achieving\nhigh performance and adhering to critical safety constraints."}, {"title": "Methodology", "content": "4.1 Constrained Markov Decision Process formulation\nReinforcement Learning is the process of continuous interaction between the agent and the environment. In this\nstudy, we focus on the safety of autonomous driving, which requires a trade-off between reward and safety. We\nformulate the control optimization problem as a Constraint Markov Decision Process (CMDP). CMDP is described\nby (S, A, p, r, c, d, y), including the state space, action space, transition model, reward, cost, constraint, and discount\nfactor. Each step, the agent receives reward r and cost c, the optimization target is to maximize the reward in the safety\nconstraint in Equation 2.\n$\\max_\u03c0 E_{(s_t,a_t)\u223c\u03c4_\u03c0} [\\sum_t \u03b3^t r(s_t, a_t)]$\ns.t. $E_{(s_t,a_t)\u223c\u03c0} [\\sum_t \u03b3^t c(s_t, a_t)] \u2264 d$\nwhere \u03c0, \u03c4\u03c0 denotes for the policy and the trajectory distribution of policy.\n4.2 Build the Latent Diffusion Model for State Representation\nOur model designs a sophisticated latent model consisting of three primary components: a representation model, a\ntransition model, and a reward model. Each component plays a crucial role in the system's ability to predict and navigate\ncomplex environments by learning from both observed and imagined trajectories.\n\u2022 Representation Model: The representation model is crucial for establishing a robust latent space based on past\nexperiences. It is formalized as $p(S_\u03c4 | S_{\u03c4\u22121}, A_{\u03c4\u22121}, O_\u03c4)$, where the model predicts the next state by synthesizing"}, {"title": "Build Safety Guarantee", "content": "In our framework, p denotes the distribution sampling from environment interactions, while q represents predictions\ngenerated from the latent imagination. We use 7 to index time steps within the latent space. The core innovation of our\nmodel lies in its ability to create and refine a latent imagination space that predicts future trajectories. This capability\nenables the agent to safely explore and learn optimal behaviors by iteratively refining the latent space to avoid unsafe\nstates and maximize efficiency.\naction is not only optimal in terms of performance but also satisfies predefined safety criteria, ensuring that the control\naction adheres to safety standards before execution.\nThis structured approach ensures a robust safety guarantee mechanism, integrating both performance optimization and\nrigorous safety compliance, which is crucial for autonomous systems operating in dynamic and uncertain environments.\nSimilar approach can be found in Latent imagination e.g., Dreamer [13], and got great performance. However, Dreamer\ndid not take safety constraints into consideration. Though Dreamer achieved high reward target, without safety, the\nagent might cause irreversible incident in order to reach higher reward. It is significant to introduce the constraints of\nsafety to balance reward and risk.\nIn the latent imagination, we leverage the distributional reinforcement learning to solve the safety-constrained RL\nproblem [24]. Instead of following a policy \u03c0 to get an action value, distributional RL considers on the expectation of\nvalue and cost [25]. We focus on the actor-critic policy and build a model-based algorithm together with the latent\nimagination. Soft actor-critic (SAC) [26] introduces the concept of maximum entropy balancing the exploitation and\nexploration:\n$\\pi^* = \\argmax_\u03c0 \\sum_{t=0}^{T}E_{(s_t,a_t)\u223c\u03c1_\u03c0}[\u03b3^t (r(s_t, a_t) + \u03b2H(\u03c0(\\cdot|s_t))]$\nwhere \u03c0* denotes the optimal policy, \u1e9e denotes the stochasticity of \u03c0.\nAgents can learn the optimal policy without safety concern, however, irreversible situation such as collision cannot\nbe accepted in autonomous driving. We consider the safety constraints and formulate it as a Lagrangian method with\nconstraints:\n$\\max_\u03c0 E_{(s_t,a_t)\u223c\u03c1_\u03c0} [\\sum_{t} \u03b3^t r(s_t, a_t)]$\ns.t. $E_{(s_t,a_t)\u223c\u03c1_\u03c0}[\\sum_{\u03c4}c(s_t, a_t)] \u2264 d$\n$E_{(s_t,a_t)\u223c\u03c1_\u03c0}[-log(\u03c0_t(a_t|s_t))] \u2265 H_0 \u2200t$\n$h(s_t) \u22640  (Safety Constraint)$\n$h(s_{t+1}) \u2264 (1-\u03b1)h(s_t)  (Control Barrier Function)$\nTo better use the constraints of safety, barrier function is also leveraged to modify the risk value in distributional RL so\nthat the agent can evaluate when to explore more and when to be conservative. In this formulation, For a constraint with\nrelative degree m, the generalized control barrier function is:\n$h(s_{t+m}) \u2264 (1-\u03b1)h(s_t)$\nFor a certain risk level a, we optimize policy until \u0393 satisfies:\n$\u0393_\u03c0(s, a, \u03b1) = CVaR_\u03b1 = Q^\u03c0(s, a) + \u03b1^{-1}\u00a2(\u03a6^{-1}(\u03b1)) \\sqrt{V(s, a)}$\n$\u0393_\u03c0(s_{t+m}, a_{t+m}, a) \u2264 (1 \u2212 \u03b1)\u0393_\u03c0(s_t, a_t, a)$\nwhere a is the conservativeness coefficient.\nCritic, Instead of Q(s, a) is the expectation of long-term cumulative costs from starting point (s, a), denoted by:\n$C_\u03c0(s,a) = \\sum_t V^t c(s_t, a_t) | s_0 = s, a_0 = a$\nFollowing policy \u03c0,the probability distribution of Cr(s, a) is modeled as $p^\u03c0(C | s, a)$, such that:\n$Q_\u03c0(s, a) = E_{p^\u03c0} [C | s, a].$\nThe distributional Bellman Operator $T^\u03c0$ is defined as:\n$T^\u03c0C(s,a) = c(s, a) + C^\u03c0(s', a')$ where $s' \u223c p(\\cdot | s,a), a' \u223c \u03c0(\u00b7 | a')$. We approximate the distribution C\u2081(s, a) with a Gaussian distribution C\u2081(s, a) \u223c $N (Q(s,a), V(s, a))$, where $V(s, a) = E_{p^\u03c0} [C^2 | s, a] \u2013 Q_\u03c0(s, a)^2.$\n- To estimate $Q^\u03c0$, we can use Bellman function:\n$Q^\u03c0(s, a) = c(s, a) + \u03b3\u2211_{s'\u2208S} p (s' | s, a) \u2211_{\u03b1' \u2208A}\u03c0 (\u03b1' | s') Q^\u03c0(s', a').$\n- The projection equation for estimating V\u03c0(s, a) is:\n$V(s,a) = c(s, a)^2 - Q(s, a)^2 + 2\u03b3c(s, a) \u2211_{s'\u2208S} p (s' | s,a) \u2211_{\u03b1' \u2208A}\u03c0(\u03b1' | s') Q^\u03c0(s', a') +$\n$+\u03b3^2 \u2211_{s'\u2208S}p (s' | s,a) \u2211_{\u03b1' \u2208A}\u03c0 (\u03b1' | s') V^\u03c0(s', a') + \u03b3^2 \u2211_{s'\u2208S} p (s' | s, a) \u2211_{\u03b1' \u2208A} \u03c0 (\u03b1' | s') Q^\u03c0(s', a')^2.$\nWe use two neural networks parameterized by \u03bc and \u03b7 respectively to estimate the safety-critic:\n$Q_\u03bc(s, a) \u2192 Q^\u03c0(s, a)$ and $V_\u03b7(s, a) \u2192 V^\u03c0(s,a)$\n2-Wasserstein distance: $W_2(\u03bc, v) = ||Q1 - Q2||^2 + trace ((V_1 + V_2-2(V_2^{1/2}V_1V_2^{1/2})^{1/2}) ). The 2-Wasserstein\ndistance can be computed as the Temporal Difference (TD) error based on the projection equations $Q^\u03c0(s, a)$ and\n$V^\u03c0(s, a)$ to update the safety-critic, i.e., we will minimize the following values:\n$J_c(\u03bc) = E_{(s_t,a_t)\u223cD} ||\u2206Q(s_t, a_t, \u03bc) ||^2$\n$J_v(\u03b7) = E_{(s_t,a_t)\u223cD} trace (\u2206V (s_t, a_t, \u03bc))$\nWhere, \u03bc and \u03b7 are two parameters that represents the two neural network in safety critic, Jc(\u03bc) is the loss function of\n$Q_\u03bc(s, a)$ and Jv(\u03b7) is the loss function of V\u03b7(s, a). We can get:\n$\u2206Q (s_t, a_t, \u03bc) = Q_\u03bc (s_t, a_t) \u2013 Q^\u03c0(s, a)$\n$\u25b3V (s_t, a_t, \u03b7) = V_\u03b7 (s_t, a_t) + V^\u03c0 (s_t, a_t) - 2 (V_\u03bc (s_t, a_t) V (s_t, ar) V (s_t, at))^{1-2}$\nwhere $Q^\u03c0_\u03bc (s_t, a_t)$ and $VF\u03b7 (s_t, a_t)$ is the TD target.\nActor, We focus on the expectation of cumulative reward and cost to avoid the impact of random sampling from\nthe Gaussian distribution. A parameter a is designed to set the risk attention, denotes the risk-aware level on safety.\n\u03b1\u2208 (0, 1), the smaller scalar means more pessimistic in safety, whereas the larger one expect a less risk-aware state.\nWe set a a-percentile $F^{-1}(1 \u2013 a)$, based on the Conditional Value at Risk (CVaR), we can get $CVaR_\u03b1 =$\n$E_{p^\u03c0} [C | C\u2265 F^{-1} (1 \u2013 a)]$. Given the risk-aware level manually by different situation or standard, the control\noptimization problem can be modified into:\n$E_{\u03c1^\u03c0} [C (s_t, a_t) | C (s_t, a_t) \u2265 F^{-1} (1-\u03b1)] \u2264 d t$\nConsidering the critic mentioned before, the safety measurements can be changed into:\n$\u0393_\u03c0(s, a, \u03b1) = CVaR_\u03b1 = Q^\u03c0_\u03bc(s,a) + \u03b1^{-1}\u00a2(\u03a6^{-1}(\u03b1)) \\sqrt{V(s,a)}$\nAs the safety critic, we also define a reward critic with the neural network parameter & and loss function $J_R (V_i)$. Given\nthe risk level, we use $\u0393_\u03c0(s, a, \u03b1) \u2264 d t$ to find the optimal policy \u03c0.\n4.4 Value-at-Risk Based Soft Actor-Critic for Safe Exploration\nAs shown in Algorithm 1, we build the latent imagination by updating the representation model, transition model\nand reward model. We predict the future trajectories based on the transition model, and regard trajectories as latent\nimagination for continuous actor-critic policy optimization. Given a specific risk-aware level, new safety constraint is\nset. Considering the expectation of cumulative cost and reward, we can update the model by distributional RL. After\ncontinuous learning in the latent imagination for some discrete time step, the agent interacts with environment and gets\nnew reward and cost data."}, {"title": "Experiments", "content": "5.1 Environmental Setup\n5.1.1 Experimental Setup in CARLA Simulator\nIn our study, we utilized the CARLA simulator to construct and evaluate various safety-critical scenarios that challenge\nthe response capabilities of autonomous driving systems. CARLA provides a rich, open-source environment tailored\nfor autonomous driving research, offering realistic urban simulations.\nSpecific scenario\nWe designed specific scenarios in CARLA to assess different aspects of autonomous vehicle behavior, as shown in\nFigure 5. These scenarios include:\n\u2022 Traffic Negotiation: Multiple vehicles interact at a complex intersection, testing the vehicle's ability to\nnegotiate right-of-way and avoid collisions.\n\u2022 Highway: Simulates high-speed driving conditions with lane changes and merges, evaluating the vehicle's\ndecision-making speed and accuracy.\n\u2022 Obstacle Avoidance: Challenges the vehicle to detect and maneuver around suddenly appearing obstacles like\nroadblocks.\n\u2022 Braking and Lane Changing: Tests the vehicle's response to emergency braking scenarios and rapid lane\nchanges to evade potential hazards.\nThese scenarios are crucial for validating the robustness and reliability of safety protocols within autonomous vehicles\nunder diverse urban conditions.\nUrban Driving Environments\nAdditionally, we tested the vehicles across three different urban layouts in CARLA, depicted in Figure 5. We pick up\nthose towns based on the following reasons:\n\u2022 Town 6: Features a typical urban grid which simplifies navigation but tests basic traffic rules adherence.\n\u2022 Town 7: Includes winding roads and a central water feature, introducing complexity to navigation tasks and\nrequiring advanced path planning.\n\u2022 Town 10: Presents a dense urban environment with numerous intersections and limited maneuvering space,\nmaking it ideal for testing advanced navigation strategies.\nThe detailed simulation environments provided by CARLA, combined with the constructed scenarios, enable compre-\nhensive testing of autonomous driving algorithms, ensuring that they can operate safely and efficiently in real-world\nconditions.\n5.2 Design of the Reward Function\nOur autonomous system employs a multifaceted reward function designed to optimize both the efficiency and safety of\nnavigation. The reward function is segmented into various components that collectively ensure the vehicle adheres to\noperational standards:\nVelocity Compliance Reward (R): This reward is granted for maintaining a specified target velocity, promoting\nefficient transit and fuel economy:\n$R_v = {1 \\text{ if } |v_{\\text{current}} - v_{\\text{target}}| \\text{ otherwise}}$\nwhere $v_{\\text{current}}$ is the vehicle's current velocity, $v_{\\text{target}}$ is the target velocity, and \u039b is a factor that penalizes deviations from\nthis target."}, {"title": "Evaluate robustness", "content": "We further try to evaluate the robustness of our proposed framework to different types of obstacles. By adjusting the\nspeed of moving obstacles, we can simulate various levels of dynamic complexity and observe how well the model\nanticipates and reacts to changing trajectories and potential hazards. This test not only demonstrate the model's obstacle\navoidance skills but also can evaluate its potential real-world applicability and robustness under varying speeds and\ndensities of pedestrian traffic. The moving speed of the dynamic object is 1,2,3m/s. We tried to fix the start points and\ngoal points."}, {"title": "Lane Maintenance Reward (R)", "content": "This reward encourages the vehicle to remain within the designated driving lane:\n$R_l = {1 \\text{ if } d_{\\text{offset}} \\text{ otherwise}}$\nwhere $d_{\\text{offset}}$ is the lateral displacement from the lane center, and $d_{\\text{max}}$ is the threshold beyond which penalties are\nincurred.\nOrientation Alignment Reward (R): This component penalizes the vehicle for incorrect heading angles:\n$R_o = \\frac{1}{1 + \u03bc \\text{influences the strictness of the alignment requirement.}}\nExploration Incentive Reward (R): A novel component introduced to encourage exploration of less frequented\npaths, enhancing the robustness of the navigation strategy:\n$R_e = exp(-vN_{\\text{visits}})$\nwhere $n_{\\text{visits}}$ counts the number of times a specific path or region has been traversed, and v is a decay factor that reduces\nthe reward with repeated visits.\nComposite Reward Calculation: The overall reward (Rtotal) is a composite measure, formulated as follows:\n$R_{\\text{total}} = w_v \\cdot R_v + \u03c9_l \\cdot R_l + \u03c9_o R_o + \u03c9_e R_e$\nwhere \u03c9\u03bd, \u03c9\u03b9, wo, and we are weights that prioritize different aspects of the reward structure based on strategic objectives."}, {"title": "Collision Occurrences (CO):", "content": "This metric quantifies the frequency of collisions that occur during autonomous\ndriving. It provides an important measure of the safety and reliability of the driving algorithm. A lower\nCO value indicates that the system is better at avoiding collisions, which is critical for the safe operation of\nautonomous vehicles. This metric is defined as:\n$CO = \\frac{\\text{Number of Collisions}}{\\text{Total Distance Driven}} \u00d7 100$\nwhere the Number of Collisions is the total count of collisions encountered by the autonomous vehicle, and\nthe Total Distance Driven is the total distance covered by the vehicle during the testing period. This metric is\nexpressed as a percentage to standardize the measure across different distances driven.\n5. Infractions per Kilometer (IPK): This metric normalizes the total number of infractions by the distance\ndriven to provide a measure of infractions per unit distance:\n$IPK = \\frac{\u2211_{i=1}^N I_i}{\u2211_{i=1}^N K_i}$\nwhere $I_i$ is the number of infractions on the i-th route and K\u2081 is the distance driven on the i-th route.\n6. Time to Collision (TTC): This metric estimates the time remaining before a collision would occur if the\ncurrent velocity and trajectory of the vehicle and any object or vehicle in its path remain unchanged. It is a\ncritical measure of the vehicle's ability to detect and react to potential hazards in its immediate environment:\n$TTC = min(\\frac{d}{V_{rel}})$\nwhere d represents the distance to the closest object in the vehicle's path and $v_{rel}$ is the relative velocity towards\nthe object. A lower TTC value indicates a higher immediate risk, triggering more urgent responses from the\nsystem.\n7. Collision Rate (CR): This metric quantifies the frequency of collisions during autonomous operation, providing\na direct measure of safety in operational terms:\n$CR= \\frac{\\text{Number of Collisions}}{\\text{Total Distance Driven}}$\nexpressed in collisions per kilometer. This metric helps in evaluating the overall effectiveness of the collision\navoidance systems embedded within the autonomous driving algorithms.\nThese metrics collectively provide a robust framework for evaluating autonomous driving systems under varied driving\nconditions, facilitating a detailed analysis of their capability to navigate complex urban environments while adhering to\ntraffic rules and maintaining high safety standards."}]}]}