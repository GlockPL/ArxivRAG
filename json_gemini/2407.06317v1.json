{"title": "ENHANCED SAFETY IN AUTONOMOUS DRIVING: INTEGRATING\nLATENT STATE DIFFUSION MODEL FOR END-TO-END\nNAVIGATION", "authors": ["Jianuo Huang", "Zhenlong Fang"], "abstract": "With the advancement of autonomous driving, ensuring safety during motion planning and navigation\nis becoming more and more important. However, most end-to-end planning methods suffer from a lack\nof safety. This research addresses the safety issue in the control optimization problem of autonomous\ndriving, formulated as Constrained Markov Decision Processes (CMDPs). We propose a novel, model-\nbased approach for policy optimization, utilizing a conditional Value-at-Risk based Soft Actor Critic\nto manage constraints in complex, high-dimensional state spaces effectively. Our method introduces\na worst-case actor to guide safe exploration, ensuring rigorous adherence to safety requirements\neven in unpredictable scenarios. The policy optimization employs the Augmented Lagrangian\nmethod and leverages latent diffusion models to predict and simulate future trajectories. This dual\napproach not only aids in navigating environments safely but also refines the policy's performance by\nintegrating distribution modeling to account for environmental uncertainties. Empirical evaluations\nconducted in both simulated and real environment demonstrate that our approach outperforms\nexisting methods in terms of safety, efficiency, and decision-making capabilities. Code is available at\nhttps://github.com/arnohuangjianuo/safeRL", "sections": [{"title": "1 Introduction", "content": "In the rapidly evolving field of autonomous driving, ensuring the safety of vehicles during the exploration phase\nis very important [1]. Traditional end-to-end methods often struggle to guarantee safety, particularly in complex,\nhigh-dimensional environments [2, 3]. The increased complexity of the state space in such scenarios not only makes the\nsampling and learning processes inefficient but also complicates the pursuit of globally optimal policies. The outcomes\nof inadequate safety measures can be severe, ranging from irreversible system damage to significant threats to human\nlife.\nReinforcement Learning (RL) has achieved significant success across various fields [11, 19]. Furthermore, Deep\nlearning (DL) is known for its strong perception ability, while reinforcement learning (RL) excels in decision-making.\nBy combining the strengths of DL and RL, deep reinforcement learning (DRL) offers a solution to the decision-planning\nproblem in complex obstacle avoidance scenarios. Unlike traditional motion planning methods, DRL can enhance the\nadaptability and generalization ability across different scenarios, thus overcoming the limitations of traditional methods\nand providing a more efficient and effective solution. Mnih et al.proposed a deep Q-network model that combines\nconvolutional neural networks and Q-learning from traditional RL to address high-dimensional perception-based\ndecision problems [19]. They employed a deep Q-network (DQN) to evaluate the Q-function for Q-learning. This\napproach has been widely adopted and serves as the primary driver for deep RL. DRL empowers robots with perceptual\nand decision-making abilities by processing input data to yield the output in an end-to-end manner [40]. The end-to-end\nmotion planning method treats the system as a whole, making it more robust [28, 5]. Furthermore, deep reinforcement"}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Safe reinforcement learning", "content": "Safe Reinforcement Learning (Safe RL) is a method that combines safety measures with the usual learning process for\nagents in complex environments [27]. The development of safe and reliable autonomous driving systems requires a\ncomprehensive focus on safety throughout the control and decision-making processes. In this context, safe reinforcement\nlearning (Safe RL) emerges as a powerful tool for training controllers and planners capable of navigating dynamic\nenvironments while adhering to safety constraints.\nSafe RL algorithms address the safety challenge through various approaches. Notable examples include Constrained\nPolicy Optimization (CPO) [28], which optimizes policies within explicitly defined safety constraints to prevent unsafe\nactions. Trust Region Policy Optimization (TRPO) [29] and Proximal Policy Optimization (PPO) [30] adapt these\nalgorithms to incorporate safety considerations by introducing penalty terms or barrier functions. Gaussian Processes\n(GPs) model [31] environmental uncertainty, enabling safe exploration by quantifying the risk associated with different\nactions. Model Predictive Control (MPC)-based Safe RL [32] [33] predicts future states and optimizes actions over a\nfinite horizon while adhering to safety constraints.\nIn the context of autonomous driving, Safe RL plays a crucial role in ensuring safety. Applications include high-level\ndecision-making for tasks like lane changing [10] and route planning [34], considering the behavior of surrounding\nvehicles and potential conflicts. Low-level control integrates Safe RL with traditional control methods like MPC,\nensuring adherence to safety constraints while following the planned path [35] [36]. The combination of RL with\nrule-based systems encoding traffic laws guarantees that learned policies comply with regulations. Safety verification\ntechniques provide guarantees that learned policies remain safe even in the presence of uncertainties and dynamic\nobstacles. [37] [38]\nThe diverse Safe RL algorithms and their applications underscore their critical role in developing adaptable and safe\ncontrol systems. Continuous advancements in these methods will be crucial for the successful real-world deployment of\nautonomous vehicles."}, {"title": "2.2 Reinforcement Learning with Latent State", "content": "Latent dynamic models, which represent a cornerstone in the modeling of time-series data within the field of re-\ninforcement learning (RL), facilitate a profound understanding of hidden states and dynamic changes in complex\nenvironments [40, 41, 42]. These models capture essential relationships between unobservable internal states and\nobserved data, significantly enhancing the predictive capabilities of RL agents.\nIn the RL context, latent dynamic models operate under probabilistic frameworks, employing Bayesian inference or\nmaximum likelihood estimation to deduce both model parameters and hidden states with increased accuracy [43, 44].\nThese frameworks enable the modeling of environments in a way that aligns with the probabilistic nature of real-world\ndynamics.\nMathematically, latent dynamic models are characterized by:\n\u2022 State Transition Equation:\n$S_{t+1} = f(s_t, a_t, e_t)$\nwhere $s_t$ denotes the latent state at time t, $a_t$ the action taken, and $e_t$ the stochasticity inherent in the\nenvironment. The function f may be deterministic or stochastic, encapsulating the uncertainty of state\ntransitions.\n\u2022 Observation Equation:\n$O_t = g(s_t, d_t)$"}, {"title": "2.3 Diffusion model-based RL", "content": "Recent advances in diffusion models have found significant applications in reinforcement learning (RL), particularly in\nthe domain of offline RL where interaction with the environment is constrained. Notable for their success in generative\ntasks, diffusion models are adapted to address critical challenges in RL such as the distributional shift and extrapolation\nerrors commonly observed in offline settings [18, 17].\nStudies like [16] demonstrate that diffusion probabilistic models can effectively generate plausible future states and\nactions, facilitating robust policy learning from static datasets. Furthermore, latent diffusion models (LDMs) reduce\ncomputational demands and enhance learning efficiency by encoding trajectories into a compact latent space before\ndiffusion, thus capturing complex decision dynamics [15]. These methods have improved the stability and efficacy of\nQ-learning algorithms by ensuring that generated actions remain within the behavioral policy's support, thus mitigating\nthe risk of policy deviation due to poor sampling [14].\nThis integration of diffusion techniques into RL frameworks represents a promising frontier for developing more capable\nand reliable autonomous systems, particularly in environments where traditional learning methods fall short."}, {"title": "3 Problem Modeling", "content": "In the problem of autonomous navigation, we model the interaction of an autonomous agent with a dynamic and uncertain\nenvironment using a finite-horizon Markov Decision Process (MDP), defined by the tuple M ~ (S, O, A,P, r, \u03b3).\nHere, $S \\subset \\mathbb{R}^n$ represents a high-dimensional continuous state space, and $A \\subset \\mathbb{R}^m$ denotes the action space. State\ntransitions are defined by $s_{t+1} ~ P(\\cdot | s_t, a_t)$, reflecting the stochastic nature of the environment.\nObservations O, derived from the state space, are typically high-dimensional images captured by sensors and processed\nvia a latent diffusion model to enhance state representation. This allows for a more comprehensive understanding of\nenvironmental dynamics, which is important for navigating complex scenarios. The reward function $r : S\\times A\\times S \\rightarrow \\mathbb{R}$\nand the discount factor \u03b3\u2208 [0, 1] guide the agent's policy $\\pi_{\\theta}$, which generates actions based on processed observations\n$O_t$.\nWe emphasize safety through the integration of a conditional Value-at-Risk (VaR) metric within our decision-making\nframework, addressing the worst-case scenarios with a novel approach that involves latent state diffusion for robust state\nrepresentation learning. Safety mechanisms are formalized through a subset $S_u \\subset \\mathbb{R}^n$, where entering a state $s_t \u2208 S_u$\nindicates a potential safety violation, monitored by a safety function K. The objective extends beyond maximizing\ncumulative rewards to ensuring minimal safety violations:\n$\\max_\\theta J (\\pi_\\theta) = E_{p(s_t, a_t, s_{t+1})} [\\sum_{t=0}^T \\gamma^t r (S_t, a_t, S_{t+1})]$\ns.t., $\\sum_{t=0}^T k (S_t) \\le D$, $a_t ~ \\pi_\\theta (\\cdot | O_t)$.\n(1)\nHere, \u03ba(st) \u2208 {0,1} serves as an indicator of safety violations, with D \u2208 R representing the maximum allowable\nsafety violations, aiming for D \u2192 0 to enhance operational safety. This safety-constrained MDP framework allows the\nagent to learn navigation policies that maximize efficiency and ensure safety, effectively balancing between achieving\nhigh performance and adhering to critical safety constraints."}, {"title": "4 Methodology", "content": ""}, {"title": "4.1 Constrained Markov Decision Process formulation", "content": "Reinforcement Learning is the process of continuous interaction between the agent and the environment. In this\nstudy, we focus on the safety of autonomous driving, which requires a trade-off between reward and safety. We\nformulate the control optimization problem as a Constraint Markov Decision Process (CMDP). CMDP is described\nby (S, A, p, r, c, d, y), including the state space, action space, transition model, reward, cost, constraint, and discount\nfactor. Each step, the agent receives reward r and cost c, the optimization target is to maximize the reward in the safety\nconstraint in Equation 2.\n$\\max_\\pi E_{(\\tau_\\pi) ~ \\pi} [\\sum_t \\gamma^t r (s_t, a_t)]$\ns.t. $E_{(\\tau_\\pi) ~ \\pi} [\\sum_t c(s_t, a_t)] \\le d.$\n(2)\nwhere \u03c0, \u03c4\u03c0 denotes for the policy and the trajectory distribution of policy."}, {"title": "4.2 Build the Latent Diffusion Model for State Representation", "content": "Our model designs a sophisticated latent model consisting of three primary components: a representation model, a\ntransition model, and a reward model. Each component plays a crucial role in the system's ability to predict and navigate\ncomplex environments by learning from both observed and imagined trajectories.\n\u2022 Representation Model: The representation model is crucial for establishing a robust latent space based on past\nexperiences. It is formalized as $p(S_T | S_{T\u22121}, A_{T\u22121},O_T)$, where the model predicts the next state by synthesizing"}, {"title": "4.3 Build Safety Guarantee", "content": "function, $\\arg \\max Q(s, z)$, which selects the optimal latent state for\naction execution based on maximizing expected rewards and ensuring compliance with safety standards. The chosen"}, {"title": "4.4 Value-at-Risk Based Soft Actor-Critic for Safe Exploration", "content": "As shown in Algorithm 1, we build the latent imagination by updating the representation model, transition model\nand reward model. We predict the future trajectories based on the transition model, and regard trajectories as latent\nimagination for continuous actor-critic policy optimization. Given a specific risk-aware level, new safety constraint is\nset. Considering the expectation of cumulative cost and reward, we can update the model by distributional RL. After\ncontinuous learning in the latent imagination for some discrete time step, the agent interacts with environment and gets\nnew reward and cost data.\n$\\pi^* = \\arg \\max \\sum E_{(s_t, a_t) ~ \\rho_\\pi} [\\gamma^t (r (S_t, a_t) + \\beta H (\\pi (. | S_t))]$\n(3)\nwhere \u03c0* denotes the optimal policy, \u1e9e denotes the stochasticity of \u03c0.\nAgents can learn the optimal policy without safety concern, however, irreversible situation such as collision cannot\nbe accepted in autonomous driving. We consider the safety constraints and formulate it as a Lagrangian method with\nconstraints:\n$\\max_\\pi E_{(\\tau_\\pi) ~ \\rho_\\pi} [\\sum_t \\gamma^t r (S_t, a_t)]$\ns.t. $E_{(\\tau_\\pi) ~ \\rho_\\pi} [\\sum_t c (S_t, a_t)] \\le d$\n$E_{(\\tau_\\pi) ~ \\rho_\\pi} [-log (\\pi_t (a_t | S_t))] \\ge H_0 \\forall t$\n(4)\n$h(s_t) \\le 0$ (Safety Constraint)\n$h(s_{t + 1}) \\le (1-a)h(s_t)$ (Control Barrier Function)\nTo better use the constraints of safety, barrier function is also leveraged to modify the risk value in distributional RL so\nthat the agent can evaluate when to explore more and when to be conservative. In this formulation, For a constraint with\nrelative degree m, the generalized control barrier function is:\n$h (s_{t+m}) \\le (1-a)h (s_t)$\nFor a certain risk level a, we optimize policy until \u0393 satisfies:\n$\\Gamma_{\\pi}(s, a, \\alpha) = CVaR_{\\alpha} = Q_\\pi(s,a) + \\alpha^{-1} \\phi (\\Phi^{-1}(\\alpha)) \\sqrt{V(s, a)}$\n$\\Gamma_{\\pi} (S_{t+m}, a_{t+m}, a) \\le (1 \u2212 \\alpha)\\Gamma_{\\pi} (S_t, a_t, a)$\nwhere a is the conservativeness coefficient.\nCritic, Instead of $Q(s, a)$ is the expectation of long-term cumulative costs from starting point (s, a), denoted by:\n$C(s,a) = \\sum \\gamma^t c (S_t, a_t) | S_0 = s, a_0 = a$\nFollowing policy \u03c0,the probability distribution of $C_\\pi (s, a)$ is modeled as $p^{\\pi} (C | s, a)$, such that:\n$Q_\\pi(s, a) = E_p [C | s, a]$.\nThe distributional Bellman Operator T\u2122 is defined as:\n$T^{\\pi}C(s,a) = c(s, a) + C (s', a')$\ns' ~ p(. | s,a), \u03b1' ~ \u03c0(\u00b7 | \u03b1'). We approximate the distribution $C_\\pi (s, a)$ with a Gaussian distribution $C_\\pi (s, a) ~ \\mathcal{N} (Q_\\pi(s,a), V(s, a))$, where $V(s, a) = E_{p\u03c0} [C^2 | s, a] \u2013 Q_\\pi(s, a)^2$.\n- To estimate $Q_\\pi$, we can use Bellman function:\n$Q_\\pi(s, a) = c(s, a) + \\gamma \\sum_{s'\\in S} p (s' \\s, a) \\sum_{a' \\in A} \\pi (a' | s') Q_\\pi (s', a').$\n- The projection equation for estimating $V_\\pi(s, a)$ is:\n$V_\\pi(s,a) = c(s, a)^2 - Q_\\pi(s, a)^2 + 2\\gamma c(s, a) \\sum_{s'\\in S} p (s' | s,a) \\sum_{a' \\in A} \\pi(a' | s') Q_\\pi (s', a') +$\n$+ \\gamma^2 \\sum_{s'\\in S}p (s' | s,a) \\sum_{a' \\in A} \\pi (a' | s') V_\\pi (s', a') + \\gamma^2 \\sum_{s'\\in S} p (s' | s, a) \\sum_{a' \\in A} \\pi (a' | s') Q_\\pi (s', a')^2.$\nWe use two neural networks parameterized by \u03bc and \u03b7 respectively to estimate the safety-critic:\n$Q_\\mu(s, a) \\rightarrow Q_\\pi(s, a)$ and $V_{\\eta}(s, a) \\rightarrow V_\\pi(s,a)$\n2-Wasserstein distance: $W_2(u, v) = ||Q_1 - Q_2||^2 + trace (V_1 + V_2-2(V_2^{1/2}V_1V_2^{1/2})^{1/2})$ The 2-Wasserstein\ndistance can be computed as the Temporal Difference (TD) error based on the projection equations $Q_\\pi(s, a)$ and\n$V_\\pi(s, a)$ to update the safety-critic, i.e., we will minimize the following values:\n$Jc(\\mu) = E_{(S_t,a_t)~D} ||\\triangle Q (s_t, a_t, \\mu) ||^2$\n$Jv (\\eta) =E_{(S_t,a_t)~D} trace (\\triangle V (s_t, a_t, \\mu))$\nWhere, \u03bc and \u03b7 are two parameters that represents the two neural network in safety critic, Jc (\u03bc) is the loss function of\n$Q_\\mu(s, a)$ and Jv (n) is the loss function of $V_{\\eta}(s, a)$. We can get:\n$\\triangle Q (s_t, a_t, \\mu) = Q_\\mu (s_t, a_t) \u2013 Q^*_\\mu(s, a)$\n$\\triangle V (s_t, a_t, \\eta) = V (s_t, a_t) + V^* (s_t, a_t) - 2 (V_{\\mu} (s_t, a_t) V^*_{\\mu} (s_t, a_t) V (s_t, a_t))^{1-2}$\n$\\Gamma_{\\pi}(s, a, \\alpha) = CVaR_{\\alpha} = Q_\\pi(s,a) + \\alpha^{-1} \\phi (\\Phi^{-1}(\\alpha)) \\sqrt{V(s,a)}$\n(6)\nwhere $Q^*_{\\mu} (s_t, a_t)$ and $V^*_{\\mu} (s_t, a_t)$ is the TD target.\nActor, We focus on the expectation of cumulative reward and cost to avoid the impact of random sampling from\nthe Gaussian distribution. A parameter a is designed to set the risk attention, denotes the risk-aware level on safety.\n\u03b1\u2208 (0, 1), the smaller scalar means more pessimistic in safety, whereas the larger one expect a less risk-aware state.\nWe set a a-percentile $F^{-1}_{\u03c0}(1 \u2013 a)$, based on the Conditional Value at Risk (CVaR), we can get CVaR\u03b1 =\n$E_p [C | C\u2265 F^{-1}_{\u03c0} (1 \u2013 a)]$. Given the risk-aware level manually by different situation or standard, the control\noptimization problem can be modified into:\n$\\Gamma_{\\pi}(s, a, \\alpha) \\le d_t$ \nAs the safety critic, we also define a reward critic with the neural network parameter & and loss function JR (Vi). Given\nthe risk level, we use $\\Gamma_{\\pi}(s, a, \\alpha) \\le d_t$ to find the optimal policy \u03c0."}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Environmental Setup", "content": ""}, {"title": "5.1.1 Experimental Setup in CARLA Simulator", "content": "In our study, we utilized the CARLA simulator to construct and evaluate various safety-critical scenarios that challenge\nthe response capabilities of autonomous driving systems. CARLA provides a rich, open-source environment tailored\nfor autonomous driving research, offering realistic urban simulations.\nSpecific scenario\nWe designed specific scenarios in CARLA to assess different aspects of autonomous vehicle behavior, as shown in\nFigure 5. These scenarios include:\n\u2022 Traffic Negotiation: Multiple vehicles interact at a complex intersection, testing the vehicle's ability to\nnegotiate right-of-way and avoid collisions.\n\u2022 Highway: Simulates high-speed driving conditions with lane changes and merges, evaluating the vehicle's\ndecision-making speed and accuracy.\n\u2022 Obstacle Avoidance: Challenges the vehicle to detect and maneuver around suddenly appearing obstacles like\nroadblocks.\n\u2022 Braking and Lane Changing: Tests the vehicle's response to emergency braking scenarios and rapid lane\nchanges to evade potential hazards.\nThese scenarios are crucial for validating the robustness and reliability of safety protocols within autonomous vehicles\nunder diverse urban conditions.\nUrban Driving Environments"}, {"title": "5.2 Design of the Reward Function", "content": "Our autonomous system employs a multifaceted reward function designed to optimize both the efficiency and safety of\nnavigation. The reward function is segmented into various components that collectively ensure the vehicle adheres to\noperational standards:\nVelocity Compliance Reward (R\u03c5): This reward is granted for maintaining a specified target velocity, promoting\nefficient transit and fuel economy:\n$R_\u03c5= \\begin{cases} 1 & \\text{if } \\upsilon_{\\text{current}} \\approx \\upsilon_{\\text{target}} \\\\ \\frac{1}{1+ \\lambda|\\upsilon_{\\text{current}} - \\upsilon_{\\text{target}}|} & \\text{otherwise} \\end{cases}$\n(7)\nwhere \u03c5current is the vehicle's current velocity, \u03c5target is the target velocity, and \u03bb is a factor that penalizes deviations from\nthis target."}, {"title": "Lane Maintenance Reward (Rl)", "content": "This reward encourages the vehicle to remain within the designated driving lane:\n$R_l = \\begin{cases} 1 & \\text{if } d_{\\text{offset}} \\le 0 \\\\ 1 - \\frac{d_{\\text{max}} - d_{\\text{offset}}}{d_{\\text{max}}} & \\text{if } d_{\\text{offset}} > d_{\\text{max}} \\\\ -1 & \\text{otherwise} \\end{cases}$\n(8)\nwhere doffset is the lateral displacement from the lane center, and dmax is the threshold beyond which penalties are\nincurred.\nOrientation Alignment Reward (Ro): This component penalizes the vehicle for incorrect heading angles:\n$R_o = \\frac{1}{1 + \\mu |\\theta_{\\text{current}} - \\theta_{\\text{ideal}}|}$\n(9)\nwhere \u03b8current is the vehicle's current orientation, \u03b8ideal is the ideal orientation along the road, and \u03bc is a constant that\ninfluences the strictness of the alignment requirement.\nExploration Incentive Reward (Re): A novel component introduced to encourage exploration of less frequented\npaths, enhancing the robustness of the navigation strategy:\n$R_e = \\exp(-\\nu N_{\\text{visits}})$ (10)\nwhere nvisits counts the number of times a specific path or region has been traversed, and v is a decay factor that reduces\nthe reward with repeated visits.\nComposite Reward Calculation: The overall reward (Rtotal) is a composite measure, formulated as follows:\n$R_{\\text{total}} = \\omega_v \\cdot R_v + \\omega_l \\cdot R_l + \\omega_o \\cdot R_o + \\omega_e \\cdot R_e$\n(11)\nwhere \u03c9\u03bd, \u03c9\u03b9, wo, and \u03c9e are weights that prioritize different aspects of the reward structure based on strategic objectives."}, {"title": "5.3 Evaluation Metrics", "content": "To rigorously assess the performance of autonomous driving systems in our simulation, we employ a comprehensive set\nof metrics that encompass various aspects of driving quality, including safety, efficiency, and rule compliance. The\nmetrics are defined as follows:\n1. Route Completion (RC): This metric quantifies the percentage of each route successfully completed by the\nagent without intervention. It is defined as:\n$RC = \\frac{1}{N} \\sum_{i=1}^N R_i \\times 100 \\%$\n(12)\nwhere Ri represents the completion rate for the i-th route. A penalty is applied if the agent deviates from the\ndesignated route, reducing RC proportionally to the off-route distance.\n2. Infraction Score (IS): Capturing the cumulative effect of driving infractions, this score is calculated using a\ngeometric series where each infraction type has a designated penalty coefficient:\n$IS = \\prod_{j=\\{\\text{Ped, Veh, Stat, Red}\\}} (P_j)^{\\#\\text{infractions}_j}$\n(13)\nCoefficients are set as pped = 0.50, pveh = 0.60, Pstat = 0.65, and Pred 0.70 for infractions involving\npedestrians, vehicles, static objects, and red lights, respectively.\n3. Driving Score (DS): This is the primary evaluation metric on the leaderboard, combining Route Completion\nwith an infraction penalty:\n$DS = \\frac{1}{N} \\sum_{i=1}^N R_i \\times P_i$\n(14)\nwhere Pi is the penalty multiplier for infractions on the i-th route."}, {"title": "5.4 Baseline setup", "content": "\u2022 Dreamer [13]: this is a reinforcement learning agent designed to solve long-horizon tasks purely from images\nby leveraging latent imagination within learned world models. It stands out by processing high-dimensional\nsensory inputs through deep learning to efficiently learn complex behaviors.\n\u2022 LatentSW-PPO [12]:Wang et al proposes a novel reinforcement learning (RL) framework for autonomous\ndriving that enhances safety and efficiency. This framework incorporates a latent dynamic model to capture\nthe environment's dynamics from bird's-eye view images, boosting learning efficiency and reducing safety\nrisks through synthetic data generation. It also introduces state-wise safety constraints using a barrier function\nto ensure safety at each state during the learning process.\n\u2022\nFurthermore, we also propose several ablative versions of our method to evaluate the performance of each sub-module.\n\u2022 Safe Autonomous Driving with Latent End-to-end Navigation(SAD-LEN ): This version removes the latent\ndiffusion component, relying solely on traditional latent state representation without the enhanced generaliza-\ntion and predictive capabilities provided by diffusion processes. The primary focus is on evaluating the impact\nof the latent state representation on navigation performance without the diffusion-based enhancements.\n\u2022 Autonomous Driving with End-to-end Navigation and Diffusion (AD-END): This version eliminates the safe\nguarantee mechanisms. It focuses on the combination of end-to-end navigation with diffusion models to\nunderstand how much the safety constraints contribute to overall performance and safety."}, {"title": "6 Results and analysis", "content": ""}, {"title": "6.1 Evaluating Safety and Efficiency during Exploratory", "content": "We first evaluate the performance of those scenarios constructed in Figure 5. The main evaluation criteria are taken\nfrom above. We sample different training configurations and plot the reward curve and record the evaluation criteria in\nthe table."}, {"title": "6.2 Evaluate generalization ability", "content": "We try furthermore two experiments, one is taken from map in Carla Figure6, and another is using the real environment\nconstructed as shown in Figure 7."}, {"title": "6.3 Evaluate robustness", "content": "We further try to evaluate the robustness of our proposed framework to different types of obstacles. By adjusting the\nspeed of moving obstacles, we can simulate various levels of dynamic complexity and observe how well the model\nanticipates and reacts to changing trajectories and potential hazards. This test not only demonstrate the model's obstacle\navoidance skills but also can evaluate its potential real-world applicability and robustness under varying speeds and\ndensities of pedestrian traffic. The moving speed of the dynamic object is 1,2,3m/s. We tried to fix the start points and\ngoal points."}, {"title": "7 Conclusions", "content": "In our work, we propose a novel end-to-end algorithm for autonomous driving with safe reinforcement learning. We\nbuild a latent imagination model to generate future trajectories, allowing the agent to explore in an imagined horizon first.\nThis approach helps to avoid irreversible damage during the training process. Additionally, we introduce a Value-at-Risk\n(VaR) based Soft Actor Critic to solve the constrained optimization problem. Our model-based reinforcement learning"}]}