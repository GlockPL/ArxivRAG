{"title": "MILE: A Mutation Testing Framework of In-Context Learning Systems", "authors": ["Zeming Wei", "Yihao Zhang", "Meng Sun"], "abstract": "In-context Learning (ICL) has achieved notable success in the applications of large language models (LLMs). By adding only a few input-output pairs that demonstrate a new task, the LLM can efficiently learn the task during inference without modifying the model parameters. Such mysterious ability of LLMs has attracted great research interests in understanding, formatting, and improving the in-context demonstrations, while still suffering from drawbacks like black-box mechanisms and sensitivity against the selection of examples. In this work, inspired by the foundations of adopting testing techniques in machine learning (ML) systems, we propose a mutation testing framework designed to character-ize the quality and effectiveness of test data for ICL systems. First, we propose several mutation operators specialized for ICL demonstrations, as well as corresponding mutation scores for ICL test sets. With com-prehensive experiments, we showcase the effectiveness of our framework in evaluating the reliability and quality of ICL test suites. Our code is available at https://github.com/weizeming/MILE.", "sections": [{"title": "1 Introduction", "content": "In the past few years, Large Language Models (LLMs) [1,63,42,4] have achieved milestone success across a variety of tasks [22,53,47]. In particular, the In-Context Learning (ICL) [7,14] property of LLMs has been recognized as a key emerging ability of LLMs [28,39]. By prompting a few input-label demonstrations as the context, LLMs can be adapted efficiently to new tasks without modifying any model parameters. This enigmatic characteristic of LLMs has sparked significant research interest in comprehending [54,33,11,48,18] and utilizing [51,52,36,49] ICL in diverse scenarios.\nHowever, ICL has been shown to have notable reliability issues, such as strong dependence on the selection of examples [3], the order sensitivity [29,62] of the demonstrations, and vulnerabilities against adversarial attacks [52,46,37]. To mitigate these issues, a series of works have been proposed to automatically organize demonstrations [29,3] or design intrinsically robust ICL mechanisms [38,59,16]. While these works mainly focus on improving the robustness of ICL, how to select high-quality test suites for evaluating ICL systems remains an open research problem. Moreover, as the computational cost of LLMs becomes significantly higher than that of conventional deep neural networks [55], the need for high-quality datasets to conduct more efficient and accurate evaluations is emphasized even further.\nOn the other hand, mutation testing [24] techniques have showcased impressive potential in studying the reliability defects and test suite quality of machine learning (ML) systems [32,45,58,21]. By regarding the ML system as the software under test (SUT) [12], several mutation testing methods have been designed for different ML paradigms including deep learning [32,40,20], reinforcement learning [43,31], and unsupervised learning [30]. Specifically, similar to mutation testing for general software systems, these methods apply mutators particularly designed for the machine learning models or training data, and then study the behavior differences between the original model and the mutant models. Since the primary goal of mutation testing is to assess the efficacy of test cases in characterizing faults in the ML model, test suites showcasing superior performance disparities between the original model and the mutant models are deemed of better quality.\nIn this paper, driven by the observation that ICL systems also encounter robustness issues and demand high-quality test cases, we propose MILE, a Mutation testing framework for In-context LEarning systems. First, we propose mutators specialized for ICL systems. Unlike mutation testing on conventional deep learning systems that consider both data and model mutators [32,20], we primarily focus on mutation operations on the ICL prompt since ICL systems typically use a static pre-trained LLM and mainly concentrate on designing demonstrations. Taking into account the characteristics of ICL, such as sensitivity to the orders and strong dependence on the labels, we propose a kit of mutators including demonstration-level ones and prompt-level ones. Meanwhile, we design corresponding mutation scores for MILE. Besides classic mutation scores, we propose a group-wise mutation score that takes into consideration the diversity of defects within the prompt. This score is helpful for identifying how well test suites can characterize diverse defects, beyond just evaluating the test set as a whole.\nWe finally evaluate our MILE framework across benchmark datasets and popular LLMs. Similar to existing mutation testing frameworks [32,20], we sample test data from uniform or non-uniform classes to simulate high- or low-quality datasets and calculate the mutation scores on them. The experiment results suggest that our mutation scores have a strong correlation to the quality of the test sets, showcasing the effectiveness of our framework in measuring the quality of test suites. In addition, we take an in-depth analysis of each mutator to better understand their sensitivity to the defects within the ICL prompts, which is helpful for mutation operation selection and allocation for testing ICL systems with different scenarios."}, {"title": "2 Preliminaries", "content": "In this section, we provide background information and define formal notations for ICL and mutation testing.\nIn-context learning (ICL). ICL [7,14] is an intriguing property that emerges in LLMs in which they learn a specific task demonstrated by a few input-label pair examples. By keeping the model parameters static, prompting a system message that briefly describes the task and a set of input-label pairs demonstrating the task, the LLM can learn a mapping between the inputs and labels, and then successfully predict the label of a new input query attached behind the demonstrations in the prompt. Specifically, the definition of an ICL system can be formulated as follows:\nDefinition 1 (In-context Learning System). An ICL system consists of a pre-trained LLM $M(\\cdot)$ that returns a response $M(p)$ for any prompt $p$, a system prompt $p_s$, and a set of in-context demonstrations $D = \\{(x_1, y_1), (x_2,y_2),\\cdots, (x_k, y_k)\\}$. For any test prompt $x_{test}$, the model gathers all sources to form the ICL prompt $p^* = [p_s, x_1, y_1,x_2,y_2,\\ldots,x_k,y_k,x_{test}]$ and return the final response by $M(p^*)$.\nAn example of an ICL prompt for the RTE task [10] is illustrated in the following block. In this task, the goal is to determine whether the hypotheses can be derived from the premises, as instructed in the system message (lines 1-2). Then, 2 demonstrations consisting of inputs (the premises and hypotheses) and labels (answer \u2191 or \u2193) are attached behind the system prompt. Generally, the shots (number) of demonstrations are much more than 2. Finally, the prompt ends with a querying input for inference. From the text, we can know that the hypothesis \"Qatar is located in Doha\" cannot be derived from the premise, which is the same as the 2nd demonstration, so the correct output from the model should be\u2193.\nMutation Testing. Test cases play a crucial role in characterizing and evaluating the vulnerability and reliability of software systems. As a pioneering technique, mutation testing was first proposed in the 1970s [34,24,25] to measure the quality of test suites for software systems. Generally, mutation testing aims to replicate potential faults and vulnerabilities in the system to determine which test cases can effectively detect them. To this end, the mutation testing first artificially mutates a normal system to introduce fault with a set of pre-defined mutation operators (mutators). Then, given a test suite, its quality judged by this testing framework is determined by the ratio of the mutants that are killed by this dataset, as formally stated in the following definition.\nDefinition 2 (Mutation Testing). Consider a program $P$, a set of mutation operators $O = \\{o_1, o_2,\\ldots, o_m\\}$, and a test set $T = \\{(X_1,Y_1), (X_2,Y_2),\\cdots, (X_n, Y_n)\\}$ where each $X_i$ is an input and each $Y_i$ is a label. With each mutator $o_i$ turns the program $P$ into a mutant program $o_i(P) = P'$, a mutation testing process evaluates $o_i(P)$ on all $(X_i, Y_i)$ and studies the difference between the performance of $P$ and the mutants $\\{o_1(P),o_2(P),\\ldots,o_m(P)\\}$.\nSo far, mutation testing has been acknowledged as one of the most fundamental software testing techniques, which is widely adopted in scenarios like fault localization [35] and software repairment [19]. In particular, mutation testing has proven to be successful in evaluating the adequacy of test datasets by providing a metric to determine whether existing tests have good fault-revealing capabilities. In the context of ML systems, a representative application of mutation testing is to assess the quality of test sets by treating the model as a program, and when the mutated models (mutants) output false prediction, this mutant can be regarded as killed. We provide more related work on applying mutation testing for ML systems in Section 5."}, {"title": "3 Mutation Testing For In-Context Learning", "content": "In this section, we present MILE, our mutation testing framework for in-context learning systems. We begin with a brief overview of the testing pipeline and general design for mutation operator and score, then put forward our solutions to them respectively.\n3.1 Overview\nSimilar to existing mutation testing techniques for ML systems, we devise a two-stage testing framework consisting of mutant generation and test set evaluation. However, in contrast to traditional machine learning approaches that train models from scratch (i.e. with random parameter initialization), ICL systems usually use a pre-trained static LLM and concentrate on creating in-context demonstrations. As a result, we only consider mutations in the demonstrations while keeping the LLM unchanged.\nThe overall pipeline of our proposed MILE is elaborated in Algorithm 1. In line 1, we first obtain the mutated in-context demonstrations $D'$ from $D$ with all mutators. Then, by incorporating these demonstrations into the original LLM $M$, we obtain ICL models $M'$ and $M'_i$ (line 2). The second stage is to evaluate the test set $T$ with the mutants. In line 5, we first filter out the examples that are misclassified by the original model. Following existing work [32], we primarily illustrate our framework on classification tasks, but it can be easily adapted to other scenarios like regression tasks by adding a threshold function. Further, in line 6 for these passed test cases, we track all mutant predictions on them and finally calculate the mutation scores based on these outputs and true labels, as detailed in the following sections.\n3.2 Mutation Operators for ICL\nIn this section, we propose several mutation operators specialized for ICL prompts. Considering the principle of the mutation operator, which is to characterize potential faults and the sensitivity of a program that may have suffered, we design mutators based on possible problems and the sensitivity of ICL prompts, and divide them into demonstration-level and prompt-level ones.\nDemonstration-level mutation operators. First, we consider demonstration-level mutations for a single demonstration $(x_i, y_i)$ that modify $x_i$ or $y_i$ to construct a mutant ICL prompt, including:\nNoisy Labels (NL). ICL is known to be sensitive to the noise of labels in the demonstrations [9,17]. However, recent research emphasizes the potential of scaling ICL to very large volumes [2,5] where ensuring label accuracy becomes challenging, leading to potential concerns about noisy labels within the prompt. Therefore, we first propose a Noisy Label (NL) mutator which randomly replaces a correct label in the demonstration: $y_i \\leftarrow y', i \\sim Uniform([1...k]), y' \\in Y - \\{y_i \\}$ where y is all class labels.\nOut-of-distribution Labels (OL). Similar to the Noisy Labels mutator, we also consider another common reliability issue that the label assigned to data may be out-of-distribution (OOD), as the OOD detection is still a not fully addressed problem [27,56]. Unlike the NL mutator which injects a false label, this Out-of-distribution Labels mutator replaces the original label with one that does not belong to the task classes, e.g. a special token: $y_i \\leftarrow z,i \\sim Uniform([1...k]), z \\notin Y$. Intuitively, the OOD label mutator may be more moderate than the noisy label mutator, as verified in our experiments.\nBlurred Inputs (BI). In addition to mutating the labels in the demonstrations, we further consider potential issues in the inputs $x_i$. As stated in prior research [33], high-quality inputs are essential for helping the language model better understand the task. Therefore, we suggest simulating questionable inputs in the demonstrations by blurring the input content: $x_i\\leftarrow x_{i, i \\sim Uniform([1...k])}$. In our implementation, we achieve this by simply truncating the input to its prefix.\nPrompt-level mutation operators. We also consider prompt-level mutation, where we maintain input-label pairs for each individual demonstration but explore mutating between different demonstrations, including:\nDemonstration Shuffle (DS). The order of the demonstrations can have a significant impact on the ICL prompts, as noted in previous studies [29,16]. Therefore, test cases for which the prediction changed after re-ordering the demonstrations would be considered as being near the decision boundary, indicating that they may be effective test cases [32,30]. This motivates us to propose the Demonstration Shuffle mutator that randomly re-orders all demonstrations in the prompt: $(x_i, y_i) \\leftarrow (x_{\\sigma(i)}, y_{\\sigma(i)})$, where $\\{\\sigma(1), \\sigma(2),\\ldots,\\sigma(k)\\}$ is a random permutation of $[1...k]$.\nOut-of-distribution Demonstrations (OD). Similar to the proposed OOD Label mutator, we also consider another form of OOD mutator that introduces a self-consistent OOD demonstration $(x', y')$ from a different dataset, which may also distract the model from the target task: $(x_i, Y_i) \\leftarrow (x', y'), i \\sim Uniform([1...k])$.\nDemonstration Repetition (DR). Finally, we consider the demonstration repetition mutator. The training data repetition mutator was suggested for deep learning with the idea that the same data point might be gathered repeatedly from similar sources [32]. In the case of ICL prompts, repetition or very similar prompts might be seen as unnecessary. As a result, we propose the Demonstration Repetition mutator that incorporates repeated demonstrations into the prompt: $(x_{i+j}, Y_{i+j}) \\leftarrow (x_i, Y_i), i \\sim Uniform([1...k]), j = 1, 2, ..., N$ where N is the times of repetition.\nWe present the implementation details of each mutator in experiments in Section 4. Based on these mutators, we further devise the mutation scores in the testing framework in the following.\n3.3 Mutation Scores\nWe first consider the standard mutation score in the context of mutation testing, which is defined as the ratio of mutators killed by (i.e. misclassify any case in) the test set. Based on the notations presented in Section 2, this metric can be formulated as:\nDefinition 3 (Standard Mutation Score). The standard mutation score $MS_S$ is defined by\n$MSS(M,O,T) = \\frac{\\#\\{o_i|\\exists j, M'_i(X_j) \\ne Y_j\\}}{\\#O},$ (1)\nwhere $\\#S$ denotes the cardinality of set S. Please note that in this section we abuse the notation T to denote the test samples that are correctly classified by M. Apart from the standard metric, we are also interested in the test set's ability to identify different types of defaults. As outlined in the previous section, the ICL system may have various potential defects. Hence, a high-quality test set should be able to detect a variety of vulnerabilities, measured by the average number of mutator groups killed by the test cases. Motivated by this notion, we propose a Group-wise mutation score as follows:\nDefinition 4 (Group-wise Mutation Score). Suppose that the mutation operators can be divided into K_groups $O = \\{O_1, O_2,\\ldots, O_K\\}$. The group-wise mutation score $MSG$ is defined by\n$MSG(M,O,T) = \\frac{\\sum_{i=1}^{\\#T}\\sum_{j=1}^{K}I(\\exists o_i \\in O_j, M'_i(X_1) \\ne Y_1)}{\\#T \\times K},$ (2)\nwhere $I()$ is the indicator function. Intuitively, $MSG$ measures how many groups of mutators can be killed on average, i.e. $\\frac{\\#}{\\ K} \\sum_{j=1}^{K}I(\\exists o_i \\in O_j, M'_i(X_1) \\ne Y_i)$. We divide this by K for normalization. This metric underscores the diversity among different mutator groups. This metric is useful for preventing inflation of mutation scores when a test case can only kill mutators from a few groups. In practice, we consider all mutators that are generated from the same operator in the previous section as one group, thus we generally have 6 mutator groups in this testing framework."}, {"title": "4 Experiments", "content": "In this section, we conduct evaluations across diverse datasets and LLMs to evaluate and comprehend our MILE framework. We start by elaborating the experiment set-ups, and then showcasing the effectiveness of MILE on measuring dataset quality. Finally, we analyze and compare the mutators for a better understanding of them.\n4.1 Experiment Set-up\nDatasets. Following common practice in ICL research [59], we consider 5 popular datasets:\n(1) SST-2 [41] (Stanford Sentiment Treebank) is a binary single-sentence classification dataset that is used for sentiment analysis.\n(2) AGnews [60] (AG's News Topic Classification Dataset) is a collection of news articles categorized into four different classes: World, Sports, Business, and Sci/Tech.\n(3) RTE [10] (Recognizing Textual Entailment) contains pairs of sentences where the goal is to determine if the second sentence logically follows from the first.\nMutant implementation details. We provide the details of implementing each mutator:\n(1) Noisy Labels (NL): For each input-label demonstration, we randomly flip the label to another possible class in this task and obtain 20 mutant prompts.\n(2) OOD Labels (OL): For each demonstration, we replace the label with a special token '&', obtaining 20 mutants.\n(3) Blurred Inputs (BI): For each demonstration, we truncate the input with its first-half prefix, getting 20 mutants.\n(4) Demonstration Shuffle (DS): To keep the number of mutants the same as other operators, we randomly generate 20 permutations of [1...20] and apply these orders to the demonstration set.\n(5) OOD Demonstrations (OD): For each demonstration, we replace it with 1 input-output pair randomly sampled from the WMT [6] dataset, which is a machine translation task from English to France.\n(6) Demonstration Repetition (DR): For each demonstration, we insert two same demonstrations behind it, obtaining 20 mutants.\nFinally, with 20 mutants generated by each mutation operator, we collect 120 mutants in total for each vanilla ICL prompt.\n4.2 Overall Assessment\nUniform and Non-uniform datasets. Our main evaluation aims to evaluate whether the mutation score can reflect the quality of the test set. Following existing evaluation frameworks [32,20], we simulate the quality of the test set through the aspect of the uniformity of the classes. Specifically, a good dataset consists of samples uniformly sampled from all classes, while a dataset consisting of samples from imbalanced classes is considered of poor quality.\nAs such, we first construct a dataset that is uniformly sampled from all classes (abbreviated as uni.), and also construct non-uniformly sampled datasets (abbreviated as non.). Specifically, 50% samples of the dataset are from one single class (called biased class), and another 50% samples are uniformly sampled from all classes. To make our evaluation results more robust, we create non-uniformly sampled datasets by enumerating all possible biased classes, and then report the average scores across these datasets. We first set the controlled number n as the half-size of the complete dataset, and control the size test set as $\\alpha n$ in our main evaluation. We also investigate the impact of dataset size on the mutation scores in the following.\nMutation score comparison. Based on the settings presented above, we evaluate the standard mutation score (MSs) and group-wise mutation score (MSG) on all datasets and models, and report them in Table 3 and Table 4 respectively.\nAs shown in Table 3, for all tasks the MSs score of uni. dataset consistently outperforms non. dataset, with 67.6% v.s. 44.2% on average, indicating a strong correlation between the dataset quality and the mutation score from MILE. Such a significant gap applies to all 3 models, e.g. 69.5% v.s. 43.8% for the Vicuna model, verifying the university of this correlation among different LLMs. For most of the datasets, this property still holds, like the model-averaged score for SST2 exhibits a gap higher than 30%. There are also exceptional cases like QNLI and RTE tasks for Falcon, where the score is almost the same. However, when reviewing Table 2 we can find that Falcon performs poorly on them (near random guess), thus these outliers do not affect our claims.\nFurther, from the group-wise mutation scores in Table 4, we can still observe a strong gap between the scores of uni. and non. datasets, with an averaged score of 32.7% for uni. to 14.3% for non. datasets. As a metric with considerations of mutant diversity, the MSG score also aligns with the superiority of uni. over non. datasets in terms of the comprehensiveness of ICL evaluation. Moreover, the score itself also has an explicit semantic that indicates how many groups of mutants can be detected by each test case on average. For example, since Vicuna achieves 36% on uni. dataset in the AGnews task, we know that each sample in Vicuna can cover 36% \u00d7 6\u2248 2 groups of mutants on average.\nVarying dataset size. We also conduct an analysis of the scores by varying the size of the test set. To this end, we sampled multiple test sets with sizes [20%, 40%, 60%, 80%, 100%] \u00d7 n. The results (averaged over 5 datasets) are summarized by the models in Figure 1. For all models, the score superiority of the uni. datasets (blue lines) over non. datasets (red lines) are consistent among different set sizes, further confirming the strong correlation between the scores calculated by MILE. Moreover, an interesting observation is that MSs gradually increases as the test set becomes larger, since intuitively a larger dataset can cover more mutants. However, the MSG does not necessarily increase since it is averaged on instance-wise.\n4.3 Mutator Analysis\nIn this experiment, we take a closer look at the sensitivity of the ICL model against each mutant group. This analysis aims to better understand the characteristics of each mutation operator, which is beneficial to selecting and allocating mutators for new LLMs or tasks when applying MILE."}, {"title": "4.4 Threats to Validity", "content": "In this paper, we acknowledge the following threats to validity and explain our solutions to them. First, the selection of the LLMs and datasets can be a threat to validity. In our experiments, we have evaluated MILE across 3 LLMs and 5 datasets. Due to computational resource limitations, the models are limited to 7b size, thus selecting a larger model or closed-source model is a potential threat to validity. Besides, the random sampling of the uniform or non-uniform class datasets is also a threat to validity. To deal with this concern, we fixed random seeds in our experiments to ensure reproducibility. Moreover, it is also possible that the model is significantly sensitive or insensitive against some particular biased class during non-uniform sampling. In our experiment, we enumerated all possible biased classes and averaged all scores over these non-uniform datasets to address this issue. Finally, there are also exceptional cases that the score comparison between the two datasets does not align with our overall observation, but when revisiting the vanilla accuracy of the models in these datasets we can find that the model is not capable of conducting reasonable in-context inference on these tasks, and thus would not affect any of our claims. Overall, we can wrap up the experiment part with the conclusion that the scores from MILE indeed have strong correlations to the test dataset quality, justifying their effectiveness as a metric for dataset quality evaluation. Moreover, we suggest that the mutator sensitivity could be used to generate mutants in new settings."}, {"title": "5 Related Work", "content": "5.1 Robustness and Evaluation of In-Context Learning\nDiscovered from the GPT-3 model [7], the intriguing ICL ability of LLMs has attracted widespread interest in understanding [54,33,11,48], utilizing [51,52,36,49], improving [50,59,62,16] this learning paradigm. However, though having been studied by a series of works [16], the robustness issue of the in-context demonstrations is still an unaddressed problem. The ICL performance is very sensitive to the selection and order of demonstrations [29], as well as the noise in the labels [9,17], both posing safety concerns in their real-world applications. To select better demonstration sets, Zhao et al. attribute the sensitivity to the bias of language models toward predicting certain answers, and propose to fit calibration parameters that cause the prediction to be uniform across classes [62]. Wang et al. propose to select in-context demonstrations through the Bayesian lens that regard the LLMs as latent variable models [48]. There are also other works that attempt to design intrinsically robust ICL against demonstration ordering like Zhang et al. propose BatchICL [59], an order-agnostic ICL inference algorithm, and Fang et al. propose InvICL [16], which identifies two crucial factors in the design of ICL including information non-leakage and context interdependence to achieve invariance in ICL.\nApart from focusing on the mechanism of ICL, few works have been dedicated to designing the evaluation specialized for ICL, and most of the existing works still solely conduct ICL evaluation with general LLM benchmarks like Alpaca Eval [26], or purely based on conventional natural language processing datasets like SST2. Recently, Chen et al. propose ICLEval [8], the first benchmark particularly designed for ICL evaluation with two key sub-abilities of LLMs, including exact copying and rule learning. Besides, the evaluation designed for the quality of test cases for ICL remains unexplored.\n5.2 Mutation Testing for Machine Learning Systems\nIn recent years, leveraging mutation testing in machine learning (ML) testing has become a popular research topic [32,58,21]. The testing procedure typically consists of 2 stages, including mutating the ML system through different aspects to simulate potential faults within the system, and then evaluating the dataset on the original model and the mutant models to characterize the quality of the dataset or the system. As a pioneering study, Ma et al. propose the Deep-Mutation [32], which proposes various mutators for deep neural networks from source-level (training data and model architecture) to model-level (parameters and architecture after training). Then, under controlled experiments, they show that the mutation score is able to reflect the dataset quality for ML systems. Concurrently, Shen et al. propose Munn [40], including five mutation operators designed with the characteristics of neural networks and investigations on how mutation affects neural networks and how neural depth affects mutation analysis. Subsequently, Humbatova et al. propose DeepCrime [23], which defines 35 deep learning mutation operators and conducts empirical studies about real faults in deep learning systems.\nGoing beyond conventional deep learning systems, there are also other works dedicated to applying mutation testing techniques in other learning paradigms and scenarios. Hu et al. propose DeepMutation++ [20], extending the DeepMutation framework to both feed-forward and stateful recurrent neural networks. Lu et al. propose MTUL [31], a mutation testing framework for unsupervised learning systems. Besides, Wang et al. [45] propose to leverage mutation testing for adversarial example detection during inference, based on the intuition that adversarial samples are more sensitive against model mutations. Similarly, Zhang et al. propose to apply mutation testing to detect jailbreaking attacks against LLMs [61]. On the other position, Yu et al. propose GPUFuzzer [57], leveraging mutation techniques to craft jailbreaking prompts for LLMs. However, although preliminary work has been done on introducing mutation testing for LLMs, the use of mutation testing for ICL systems has not been explored."}, {"title": "6 Conclusion", "content": "In this paper, we propose MILE, a mutation testing framework of in-context learning (ICL) systems, aiming to evaluate the test suite quality for ICL models. For mutation operators, we consider demonstration-level and prompt-level ones, specialized for ICL prompts. Besides the standard mutation score, we also propose a group-wise mutation score to better understand the model sensitivity against inter-group mutants. With comprehensive experiments across popular LLMs and datasets, we demonstrate the strong correlation between the test set quality and mutation score calculated by MILE, showcasing the effectiveness of using MILE to evaluate the test suite quality. We further investigate the model sensitivity against different kinds of mutants and provide suggestions for designing mutators when applying MILE for different testing goals. Overall, our work provides a new technique for evaluating and improving ICL systems."}]}