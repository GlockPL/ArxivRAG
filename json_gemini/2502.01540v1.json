{"title": "What is a Number, That a Large Language Model May Know It?", "authors": ["Raja Marjieh", "Veniamin Veselovsky", "Thomas L. Griffiths", "Ilia Sucholutsky"], "abstract": "Numbers are a basic part of how humans represent and describe the world around them. As a consequence, learning effective representations of numbers is critical for the success of large language models as they become more integrated into everyday decisions. However, these models face a challenge: depending on context, the same sequence of digit tokens, e.g., 911, can be treated as a number or as a string. What kind of representations arise from this duality, and what are its downstream implications? Using a similarity-based prompting technique from cognitive science, we show that LLMs learn representational spaces that blend string-like and numerical representations. In particular, we show that elicited similarity judgments from these models over integer pairs can be captured by a combination of Levenshtein edit distance and numerical Log-Linear distance, suggesting an entangled representation. In a series of experiments we show how this entanglement is reflected in the latent embeddings, how it can be reduced but not entirely eliminated by context, and how it can propagate into a realistic decision scenario. These results shed light on a representational tension in transformer models that must learn what a number is from text input.", "sections": [{"title": "1. Introduction", "content": "Numbers play a pivotal role in many aspects of human cognition (Dehaene, 2011). Starting from a narrow sense of magnitude in infancy, humans gradually acquire nuanced representations of number that capture abstract properties such as \"even\" and \"odd\" (Miller & Gelman, 1983; Piantadosi et al., 2014). Decades of research have been devoted to understanding how humans process and represent numbers (Miller & Gelman, 1983; Nieder & Dehaene, 2009; Cheyette & Piantadosi, 2020; Tenenbaum, 1999; Piantadosi, 2016). In fact, this endeavor dates back to some of the earliest applications of neural networks to artificial intelligence (McCulloch & Pitts, 1943; McCulloch, 1961) when researchers sought to define computing machines (a simulated \"man\") that can intelligently process numbers.\nThe question of how computing machines represent number assumes key importance as modern neural networks, in particular large language models (LLMs), are becoming integrated into everyday decisions (Zhu et al., 2025; Hanna et al., 2023; Stolfo et al., 2023; McCoy et al., 2024a;b; Tian et al., 2023). Since these models learn representations by predicting tokens in text, LLMs face a challenge: depending on context, the same sequence of digit tokens, e.g., 101 or 911, can be treated as a number or as a string. This duality introduces an issue akin to polysemy and homonymy (Vicente & Falkum, 2017), but it extends across different symbolic systems. What kind of representations arise from this duality, and what are its downstream implications? This is not straightforward to answer, in part because LLMs lack the experience that shapes humans' number representations, and in part because model internals are not always available and diagnostic tests can be challenging to design.\nHere we address this challenge by leveraging tools from cognitive science for characterizing representations (Shepard, 1980; 1987; Tversky & Hutchinson, 1986; Tenenbaum & Griffiths, 2001; Marjieh et al., 2024a). Specifically, we elicit similarity judgments across number pairs from six modern LLMs using an appropriate prompt. We then use those judgments to construct detailed maps (similarity matrices) that capture how the models organize numbers. Crucially, this technique can applied to any model without the need to access its internals. Moreover, when the model internals are available, this approach can be combined with probing techniques to evaluate how prompt behavior is reflected in the structure of the latent embeddings.\nWe show that despite variation in model training data and size, all models exhibit highly regular patterns that can be adequately decomposed in terms of a psychologically-motivated numerical distance based on a Log-Linear rep-"}, {"title": "2. Related Work", "content": "The processing and representation of numbers in large language models is a topic of active research. Number representations have recently been studied within pretrained language models using techniques from mechanistic interpretability. For example, Zhu et al. (2025) devised a dataset of addition problems to show that LLMs encode the value of numbers linearly in that context. Likewise, circuit analysis has been adopted to characterize how smaller language models compute \"greater than\" (Hanna et al., 2023) and basic arithmetic operations (Stolfo et al., 2023). Other work studied how simple models solve modular arithmetic tasks (Nanda et al., 2023a). More behavioral approaches have also been explored. Tian et al. (2023) looked at how model generations of numbers can be used to test for answer calibration. Likewise, McCoy et al. (2024a; 2024b) showed how a Bayesian framework can be used to characterize how LLM performance on various tasks, including numeric ones, depends on the probability of the input, task, and output."}, {"title": "2.2. Probing and Representation Analysis in LLMs", "content": "Probing is a well-established technique for studying the internal representations of machine learning models (Alain & Bengio, 2018; Belinkov, 2021). Dating back to BERT (Devlin et al., 2019), probing has been used to explore various aspects of representation such as how language models encode sentence structure (Tenney et al., 2019; Manning et al., 2020; Rogers et al., 2021). More recently, probing has become a common technique in studying larger language models. In the work by Zhu et al. (2025) mentioned earlier, the authors designed linear probes to extract number value from latent embeddings. Probes have also been used to discover how language models process concepts across layers (Gurnee et al., 2023), and for extracting uncertainty in language model generations (Kadavath et al., 2022). Similarly, probes, and latent analysis more broadly, have been used in the world models literature, e.g., to show that language models trained on Othello games develop a world model of the game (Nanda et al., 2023b), and that they learn to represent space and time (Gurnee & Tegmark, 2024; Gurnee et al., 2023; Nylund et al., 2023). More recent approaches to studying representations in language models have also been proposed like activation patching (Meng et al., 2022), sparse autoencoders (Cunningham et al., 2023;"}, {"title": "2.3. Behavioral Analysis of Language Models", "content": "By combining the prompt comprehension capabilities of large language models with the wide range of paradigms available in the behavioral sciences, a growing line of work proposes new tools for characterizing behavior in LLMs. For example, Marjieh et al. (2024b) used a suite of perceptual judgment tasks from psychophysics to characterize sensory knowledge in LLMs. Binz & Schulz (2023) leveraged paradigms from cognitive psychology to study how GPT-3 solves various tasks. Bai et al. (2024) used tools from social psychology to diagnose implicit racial and gender bias in LLMs. Webb et al. (2023) subjected LLMs to analogical tasks to study their abstract reasoning capacities. Our work takes a similar approach to this line of work, and combines it with modern probing techniques to study LLM behavior and representation in the domain of number."}, {"title": "3. Probing Number Representations in LLMs with Similarity Judgments", "content": "Our approach builds on the paradigm of similarity judgments from cognitive science (Shepard, 1962; 1980; Tversky & Hutchinson, 1986; Tenenbaum & Griffiths, 2001; Marjieh et al., 2024a), which is also closely related to representational similarity analysis (RSA) from neuroscience (Kriegeskorte et al., 2008). Given a domain of interest $D$, a set of representative items from that domain $\\{X_1,X_2,..., X_N\\} \\subset D$ (or 'stimuli'), and an agent A whose representation $M(D)$ one would like to characterize, the paradigm proceeds by eliciting pairwise similarity judgments from A across all pairs of items (\u2018How similar is the item $x_i$ to the item $x_j$?'). These judgments are then aggregated into a similarity matrix $s_{ij}$ which defines a notion of proximity on $D$ that can be used to characterize $M(D)$. Since the notion of similarity is by construction neutral, it leaves it to the agent to impose its own structure on $s_{ij}$.\nIn our case, the domain of interest is the set of non-negative integers $\\{0,1,2,... \\}$, and the agent is the LLM in question. We consider a representative sample of state-of-the-art models, namely, GPT-40 (Hurst et al., 2024), two variants of Llama-3.1 (8b and 70b) (Dubey et al., 2024), DeepSeek-V3 (Liu et al., 2024), Claude-3.5-Sonnet (Anthropic, 2024), and Mixtral-8x22B (Jiang et al., 2024). We then apply the similarity technique in a series of experiments that span different contexts as well as behavioral (prompt-level) and internal (embedding-level) analyses and examine how the observed patterns decompose in terms of theoretical string and numerical metrics. We detail those experiments in what follows."}, {"title": "4. Methodology", "content": "Our experiments fall into three categories, all of which are associated with a certain behavioral prompt for eliciting LLM judgments over numerical quantities. The prompts are provided in Appendix A. In the first category, we elicited similarity judgments over all pairs of numbers in the range 0 - 999 ('How similar are the two numbers on a scale of 0 (completely dissimilar) to 1 (completely similar)?'; see Appendix A). We then constructed symmetric similarity matrices $s_{ij}$ by averaging over both presentation orders (i.e., $s(x_i, x_j)$ and $s(x_j, x_i)$), and then evaluated the structure that emerged (see Evaluation Metrics below) and the way it is affected by context. We focused on the range 0 - 999 because all integers within it are represented as unique tokens in the models considered, which controls for any tokenization-specific differences. In the second category, we extended the similarity analysis for one of the models for which we had internal access (Llama-3.1-8b) and probed the extent to which its internal token embeddings reflected the external behavior through a linear transformation (see Probing Language Models). Finally, in the third category we constructed a naturalistic decision scenario in which the model had to select from two available numerical quantities $q_1, q_2$ (a compound concentration in a test tube) the one that is most similar to a desired quantity $q_0$ (further details in Experiments and Appendix A). We constructed such triplets $(q_0, q_1, q_2)$ in a way that the answer would diverge depending on whether the quantities are treated as numbers or strings (see Constructing Close Triplets below) which then allowed us to probe the string-bias of models."}, {"title": "4.2. Models", "content": "We evaluated four open-source and two closed-source models. The open source models consist of Llama-3.1-8b-Instruct, Llama-3.1-70b-Instruct, DeepSeek-V3, Mixtral-8x22B-Instruct-V0.1, and the closed source include GPT-40 and Claude-3.5-Sonnet. Henceforth, we will refer to the Llama and Mixtral models as Llama-3.1-8b, Llama-3.1-70b, and Mixtral-8x22B to improve readability. Since generating 1000 \u00d7 1000 similarities requires sampling 1,000,000 pairwise comparisons which costs around $160 for Claude-3.5-Sonnet, we limit the model evaluations across all contexts to one run at a temperature of zero to get the most likely answer across the different settings. For comparison, we include results from an additional run in the basic similarity context at a temperature of 0.7 in Appendix F."}, {"title": "4.3. Evaluation Metrics", "content": "To characterize the extent to which the derived similarity matrices $s_{ij}$ were string-like or number-like, we regressed their"}, {"title": "4.4. Probing Language Models", "content": "When generating an output, a language model first tokenizes the input sentence into a sequence of tokens $x_1, ..., x_n \\in V$ where $V$ is some vocabulary. Then during the forward pass, the model incrementally transforms the initial embeddings (extracted from a vocabulary embedding layer) through each layer. The internal representations at each layer \u2014 commonly referred to as residuals \u2014 capture evolving latent features. These residuals can be analyzed to probe specific properties of the input or model behavior."}, {"title": "4.5. Constructing Close Triplets", "content": "Our goal was to construct diagnostic triplets for the naturalistic decision scenario such that (i) the options are numerically close to each other but there is an unambiguous correct answer, and (ii) the options are very different in terms of their edit distance relative to the target. To do that, we constructed the target quantity $q_0$ by randomly sampling three digits from the set $\\{2, . . ., 9\\}$ with replacement and combining them into a number (e.g., 3, 3, 1 \u2192 331). Then, we constructed a Levenshtein-aligned option by subtracting 1 from the largest decimal entry (i.e., 331 \u2192 231). Finally, we constructed a Log-aligned option by keeping the largest decimal entry from the target and randomly sampling two new integers from the range $\\{1, ..., 9\\}$ excluding the digits that appeared in the other two numbers (e.g., 357). Thus, the result would be (331, 231, 357) in our example which satisfies the desired properties. We repeated the process also for five digit numbers to probe how things change for longer numbers (e.g., 25337, 15337, 26886). We sampled 10,000 such triplets from each length category and took the unique subset. Overall, there were 6,474 unique three digit triplets, and 9,995 five digit triplets."}, {"title": "5. Experiments", "content": "In the first experiment, we elicited similarity judgments across all pairs of integers in the range 0-999 ('How similar are the two numbers?'; see Appendix A for the full prompt). We begin by presenting the qualitative patterns in the raw data and then quantify them using a suitable regression analysis. The resulting LLM matrices are highly structured with a dominant area around the diagonal that in some cases (e.g., GPT-40 and DeepSeek-V3) takes a block-diagonal form, in addition to other sub-diagonal structures. These patterns appear to blend the properties found in two theoretical similarity matrices associated with string edit distance (Levenshtein) and numerical distance (Log-Linear) highlighted in the rightmost panels of Figure 1 (see Methodology). To further see whether these patterns arise from a tension between string and integer representations, in a following experiment we resolved the ambiguity of the similarity prompt by specifying the 'type' of the token using int () and str() contexts (see Appendix A for prompts). The results are shown in Figure 2A. In this case, we found that the context intervention pushed the matrices in opposing directions. For example, GPT-4o loses its block diagonal structure in the int () context, whereas Claude-3.5-Sonnet gains it in the str() context. Similar trends can also be found in the other models.\nTo quantify the above, we regressed the Levenshtein and Log-Linear distance measures against the LLM similarity judgments in the three contexts considered. A breakdown of the explained variance (coefficient of determination $R^2$) per condition are provided in Figure 2B. Overall, in the default context we found that a linear combination of the numerical and Levenshtein distance measures is sufficient to achieve an average $R^2$ of .726 (95% CI of mean: [.725, .726]), with the separate components each contributing significantly (Log-Linear only $R^2$ CI: [.607, .609], Levenshtein only $R^2$ CI: [.213, .215]). We note that replacing the Log-Linear distance with a simple linear $l_1$ distance diminishes combined average performance to an $R^2$ of .567 (95% CI: [.567, .568]; see Appendix Figure 6). In the other contexts, the metrics were correspondingly pushed in opposing directions. In the str() case, the mean $R^2$ CIs were: Combined: [.620, .621], Log-Linear: [.410, .412], and Levenshtein: [.309, .311]. On the other hand, in the int () context these were: Combined: [.721, .722], Log-Linear: [.645, .646], and Levenshtein: [.156, .158].\nNext, we evaluated the extent to which the string-like similarity generalizes to less common number bases. The idea here is that if the model is relying on edit distance, then the effect should persist irrespective of how uncommon the underlying numerical basis is. To that end, we elicited ad-"}, {"title": "5.2. Probing Internal Representations", "content": "The previous experiments showed that when an LLM is prompted for similarity judgments between two different integer tokens, the resulting behavioral metric is a combination of string and integer distance. In this section, we show how this also holds on the representational (i.e., token-embedding) level in a model for which we have internal access (Llama-3.1-8b). To do that, we train linear probes from the last token in the prompt to decode the Log-Linear and Levenshtein distances between the integers in question (see Methodology). This is non-trivial as there is no guarantee that such a linear transformation exists unless the model encodes the relevant information in its embedding (this can be seen also from the fact that our probes had 4,096 parameters, corresponding to the latent dimension, fitted on 9,500 data points, and then evaluated on 250,000 data points; see Methodology). In Figure 4 we illustrate the decoded similarity matrices from the embeddings. Here we notice a pattern that is consistent with the behavioral (prompt-based) data. In the string probe case (right panel in Figure 4), the model is able to capture the edit-distance similarity between two numbers; whereas in the integer probe case (left panel in Figure 4), the decoded pattern is indeed much more log-linear as can be seen from the diagonal area in the matrix. Interestingly, however, string similarities still bleed into the representation as can be seen from the sub-diagonal structures. Quantitatively, the Pearson correlation between the string probe and the Levenshtein and Log-Linear measures is .650 and .527, respectively. For the integer probe, the"}, {"title": "5.3. Behavioral Implications", "content": "In the final experiment we wanted to see whether there are behavioral implications for the number-string tension in a naturalistic scenario. To do that, we presented the models with an empirical setting in which they required a test tube with a certain unavailable compound concentration (given in parts per million units; ppm). The models were asked to choose from two available test tubes the one containing the most similar concentration to the one desired. Crucially, we constructed diagnostic concentration triplets that lead to divergent answers depending on whether a Levenshtein distance is used or a Log-Linear one (see Methodology). Here is an example prompt: 'You require a compound with a concentration of approximately 785 ppm. Two test tubes are available: one containing 685 ppm and the other 791 ppm. Your task is to determine which test tube provides the most similar concentration to your required dosage.' Clearly, a concentration of 791 ppm is much more similar to 785 ppm than 685 ppm, but a model with strong string bias may erroneously choose 685 ppm which shares more digits with 785 ppm. We then evaluated the percentage of incorrect (Levenshtein-consistent) answers as a measure of string bias. In addition to the 3-digit triplets, we also considered 5-digit ones to see whether the problem may be exacerbated by the inclusion of more digits (e.g., 22565, 12565, and 28743; see Methodology). We also considered both orders of presentation to see whether there are any ordering effects when the Levenshtein-aligned option is presented first vs. the Log-Linear one ('Reverse' setting). The results are shown in Figure 5. We found that all models had some degree of string-bias errors but the range varied greatly and was enhanced for longer numbers. In the 3-digit case the largest percentage (averaged over order) was 36.86% for Llama-3.1-8b (followed by 11.38% for Llama-3.1-70b), and the smallest was 0.02% for GPT-40. Likewise, in the 5-digit case, the largest percentage was 47.03% for Llama-3.1-8b (followed by 19.04% for Llama-3.1-70b), and the smallest was 0.41% for GPT-40. Interestingly, the Llama-3.1 models exhibited substantially larger degrees of bias, as well as some degree of order effects in some cases (note that this cannot be attributed to a generic presentation bias since in that case we wouldn't expect to see a difference between the 3-digit and 5-digit scenarios). Overall, these results suggest that the number-string tension can manifest itself in a realistic context, and that the extent that such a context can suppress the stringiness behavior varies across models."}, {"title": "6. Discussion", "content": "We have provided evidence for a representational tension in large language models that learn to represent numbers by processing text input. This tension is reflected externally (through prompt-elicited similarity judgments), internally (through embedding probes), and could also propagate into a quantitative decision scenario."}]}