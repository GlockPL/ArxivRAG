{"title": "Retrieval-Augmented Process Reward Model for Generalizable Mathematical Reasoning", "authors": ["Jiachen Zhu", "Congmin Zheng", "Jianghao Lin", "Kounianhua Du", "Ying Wen", "Yong Yu", "Jun Wang", "Weinan Zhang"], "abstract": "While large language models (LLMs) have significantly advanced mathematical reasoning, Process Reward Models (PRMs) have been developed to evaluate the logical validity of reasoning steps. However, PRMs still struggle with out-of-distribution (OOD) challenges. This paper identifies key OOD issues, including step OOD-caused by differences in reasoning patterns across model types and sizes-and question OOD, which arises from dataset shifts between training data and real-world problems. To address these issues, we introduce Retrieval-Augmented Process Reward Model (Retrieval-PRM), a novel framework designed to tackle these OOD issues. By utilizing a two-stage retrieval-enhanced mechanism, RetrievalPRM retrieves semantically similar questions and steps as a warmup, enhancing PRM's ability to evaluate target steps and improving generalization and reasoning consistency across different models and problem types. Our extensive experiments demonstrate that RetrievalPRM outperforms existing baselines across multiple real-world datasets. Our open-source contributions include a retrieval-enhanced dataset, a tuning framework for PRM training, and the RetrievalPRM model, establishing a new standard for PRM performance.", "sections": [{"title": "1 Introduction", "content": "While large language models (LLMs) have advanced mathematical reasoning (OpenAI, 2023; Dubey et al., 2024; Zhu et al., 2024; Shao et al., 2024; Yang et al., 2024b), they remain prone to critical flaws: explicit errors (e.g., miscalculations, logical inconsistencies) and implicit risks where correct answers mask flawed intermediate steps. Even when final results are accurate, LLMs often generate plausible-but-incorrect reasoning chains, eroding trust in their problem-solving processes (Lightman et al., 2023). To address this, Process Reward Models (PRMs) (Lightman et al., 2023; Wang et al., 2024b) have been developed to rigorously evaluate the logical validity of intermediate steps (Cobbe et al., 2021), mirroring human pedagogical practices that prioritize reasoning quality over answer correctness.\nExisting works (Wang et al., 2024a; 01 Team, 2024; Zheng et al., 2024) frame PRM as a binary classification problem. They train PRM on open-source base LLMs such as Qwen (Yang et al., 2024b) or Llama (Dubey et al., 2024) using human-annotated dataset (Lightman et al., 2023) or automated process supervision method (Wang et al., 2024b; Luo et al., 2024; Qin et al., 2024). Although these approaches show great performance and empirical success, they still face kinds of out-of-distribution challenges. We believe the out-of-distribution (OOD) problem can be viewed from the following perspectives:\nFirstly, Step OOD may occur because of different processes generated by different models. Due to the high cost of manual annotation, there are very few accurately labeled PRM expert datasets, such as PRM800K and ProcessBench, with processes generated by GPT (OpenAI, 2023) and Qwen (Yang et al., 2024b), respectively. However, different model types (e.g., GPT, Qwen, Llama(Dubey et al., 2024)) approach problem-solving differently. As is shown in Figure 2, when facing the same question, GPT-40 tends to analyze and calculate, while Qwen-72B tends to solve questions directly. They have different solution styles. Therefore, using process data generated by one model to train a PRM and then applying it to guide another model leads to an OOD issue. Moreover, models of different sizes also exhibit different reasoning processes. Larger models, like exceptional students, tend to have clearer and more accurate reasoning steps, while smaller models tend to have very short reasoning chains, as shown in Figure 2.\nSecondly, Question OOD emerges because of dataset shift. Current PRM datasets contain only a limited number of problems. For example, Math Shepherd and PRM800K cover problems from the GSM8K and MATH datasets, with GSM8K being at the elementary to middle school level and MATH at the high school to university level. However, real-world problems are far more diverse, such as those in the Olympic math competition dataset (He et al., 2024), leading to OOD issues in other datasets. As shown in the Figure 1, we used Sentence-BERT (Reimers, 2019) to encode all the problems from the three datasets and visualized the distribution with t-SNE. It is evident that the distributions differ, and since both Olympic and MATH problems are typically from high school-level exams, they are semantically closer to each other than to GSM8K.\nTo address this issue, we propose a new framework, Retrieval Augmented Process Reward Model (RetrievalPRM), which leverages a Two-stage Retrieval-enhanced Mechanism to help PRMS solve the OOD problem. we retrieve relevant questions and steps in these two stages to address the issues of question OOD and step OOD, respectively. Specifically, when predicting a step for a given question, we select semantically similar questions based on their embeddings, placing them at the beginning of the entire prompt. Additionally, we select more fine-grained, similar steps and use them as references when predicting the correctness of the step. These retrieved questions and steps serve as a kind of warm-up for PRM, acting as example problems for reference. They not only help stimulate PRM's potential by warming up but also allow the system to handle more difficult problems by identifying similarities, thus alleviating OOD issues.\nOur main contributions are summarized as follows:\n\u2022 To the best of our knowledge, we are the first to highlight the key OOD problems in Process Reward Models (PRMs), particularly the question OOD and step OOD, which arise due to differences in reasoning patterns across model types (e.g., GPT, Qwen), model sizes (1.5B, 72B) and varying problem difficulties in real-world datasets.\n\u2022 We introduce the Retrieval-Augmented Process Reward Model (RetrievalPRM) framework, which utilizes a Two-stage Retrieval-enhanced"}, {"title": "2 Preliminary", "content": "In this section, we formulate the whole problem and introduce PRM as a binary classification model."}, {"title": "2.1 Problem Formulation", "content": "We denote the Math dataset as $D = {(q_i, S_i, y_i)}^N_{i=1}$, where N is the number of data instances. The input $q_i$ is the $i^{th}$ Math question. $s_i = {s_i^1, s_i^2, ..., s_i^{n_i}}$ are the solution steps, where $n_i$ is the step number of solution $s_i$. $Y_i = {y_i^1, y_i^2, ..., y_i^{n_i}}$ and the label $y_i^j$ indicates the correctness from the 1st step to the jth step.\n\n$y_i^j = \begin{cases} 1, (s_i^1, ..., s_i^j) is\\ correct\\ for\\ q_i; \\ 0, otherwise. \\end{cases}$ (1)"}, {"title": "2.2 ORM vs. PRM", "content": "Outcome-supervised Reward Models are introduced (ORM) by (Cobbe et al., 2021), where verifiers are trained for judging the final correctness of generated solutions. ORM only predicts the final label $y_i^{n_i}$, which can be formulated as\n$Vi, \u00eeni = ORM(qi, si, ..., S_{i}^{ni}).$ (2)\nBuilding on this, the concept of process reward models (PRM) is introduced as a more granular and transparent approach. Not only does PRM evaluate the final solutions but it also assesses intermediate"}, {"title": "2.3 Large Language Model for PRM scoring", "content": "When directly adopting LLMs as the PRM for scoring, we need to convert the data $(q_i, s_i, y_i)$ with a hard prompt template. The whole template example is illustrated in Appendix B.2.\nThe textual input consists of the question $q_i$ and steps $s_i$, followed by a binary question about the correctness of these steps.\nTo obtain the floating-point correctness estimation $y_i^j \u2208 [0, 1]$ instead of discrete word tokens '+' or '-', we apply bidimensional softmax over the corresponding logits of the binary key answer tokens (ie., + & -) from LLMs to accomplish the correctness estimation during evaluation:\n$y_i^j = \\frac{exp(l_{i,+})}{exp(l_{i,+}) + exp(l_{i,-})} \u2208 (0,1).$ (4)\nwhere $l_{i,+}$ and $l_{i,-}$ are the logits of token + and - in the $i^{th}$ instance, respectively.\nIt is important to note that the estimated PRM scoring $y_i^j$ is used solely for evaluation on the testing set. If training is involved, we maintain the standard instruction tuning and causal language modeling paradigm for LLMs. In this way, we don't need to replace the language model head with binary classification head which is the last layer of LLM."}, {"title": "3 Methodology", "content": "In this section, we introduce our proposed RetrievalPRM framework in detail."}, {"title": "3.1 Overview of RetrievalPRM", "content": "The RetrievalPRM is developed to address the problem of out-of-distribution (OOD) scenarios in mathematical problem-solving, specifically focusing on both question OOD and step OOD. According to Figure 3, traditional PRM models are constrained by predefined solution steps and are unable to handle unseen questions or steps effectively, especially when the problem context shifts or the solution process deviates from previously seen examples. RetrievalPRM overcomes this challenge by incorporating a Two-stage Retrieval-enhanced Mechanism that dynamically fetches relevant questions and steps from a large pool of questions and their solutions. These retrieved questions and steps serve as a kind of warm-up for PRM, acting as example problems for reference. They not only help stimulate PRM's potential by warming up but also allow the system to handle more difficult problems by identifying similarities."}, {"title": "3.2 Two-stage Retrieval-enhanced Mechanism", "content": "The core of RetrievalPRM is the Two-stage Retrieval-enhanced Mechanism, which consists of two key phases: Question-level Retrieval and Step-level Retrieval."}, {"title": "3.2.1 Question-level Retrieval", "content": "The first stage of retrieval tackles the question OOD issue. As is shown in Figure 3, the retrieval pool is the question database $D_q = {q_i}_{i=1}^N$. During retrieval process, we treat:\n\u2022 Query: the target question $q_t$.\n\u2022 Key: all $q_i$ in the retrieval pool.\n\u2022 Value: all the $(q_i, s_i)$ pair in the retrieval pool.\nWe calculate their similarities $< q_i, q_t >$ to match the most similar n questions. Specifically, all questions will first pass through a Sentence-BERT model to encode questions and obtain their semantic representations.\n${e_{qi}}_{i=1}^N = SentenceBERT({q_i}_{i=1}^N)$ (5)\nwhere $e_{qi} \u2208 R^D$ is the embedding vector of the question $q_i$.\nAnd then all the embeddings undergo Principle Component Analysis (PCA) (Kurita, 2021) for dimensionality reduction to extract the most important dimensions.\n${e'_{qi}}_{i=1}^N = PCA({e_{qi}}_{i=1}^N)$ (6)\nwhere $e'_{qi} \u2208 R^d$ is the embedding after dimension reduction.\nFinally, we compute the cosine similarity between the target question and the entire question pool, selecting the top-k most similar questions and inputting them into the text.\n$<q_i, q_t> = \\frac{e'_{qt}e'_{qi}}{\\|e'_{qt}\\| \\|e'_{qi}\\|}$ (7)\nNow we sort the vector ${<q_i, q_t>}_{i=1}^N$ of similarity and choose top-k $(q_i, s_i)$ pairs as reference questions $q_r$ and put them in RetrievalPRM's input together with the target question. Furthermore, we store all the solutions ${s_i}_{i=1}^N$ of top-m (m > k) questions in a new database to conduct a further step-level retrieval."}, {"title": "3.2.2 Step-level Retrieval", "content": "We place step-level retrieval in the second stage of the two-stage retrieval process, rather than as a separate module, for two key reasons:\nFirstly, for a solution to be meaningful, both the question and the steps must be similar. For example, two different types of questions might both use the letter \"p\" to represent an unknown variable, but in some problems, \"p\" represents a prime number, while in others, it represents probability. This results in steps that may appear similar but have entirely different meanings, rendering the retrieved steps potentially unhelpful.\nSecondly, since there are many possible solutions to a question, this leads to a large number of steps. If the majority of these steps are irrelevant, the time spent calculating similarities becomes inefficient. By placing step-level retrieval in the second stage, we can save both time and computational resources."}, {"title": "3.3 Retrieval-based System Prompt", "content": "In RetrievalPRM, The system prompt serves as the instruction set for the model, framing the problem and directing it to evaluate each step of the solution. Besides the traditional system prompt for PRM, the Retrieval-based System Prompt (RetSP) is extended with additional instructions, as shown in the red sentence in Figure 3, which encourages the model to leverage knowledge from reference questions. For example, we inform PRM that step labels \"+\" and \"-\" represent correct and incorrect steps, respectively. At the same time, to avoid noise, we specify that if the reference question or step contains no relevant or helpful information, it should not be considered. These retrieval-based system prompts give PRM a more flexible thinking process, enabling it to actively decide whether to use retrieval-based knowledge.\nWe define reference questions of $q_i$ as $q_i^r$ and reference steps as $s_i^r$. The whole input $x_i^n$ of predicting the $n^{th}$ step of $q_i$ in RetrievalPRM can be formulated as:\n$x_i^n = (RetSP, q_r^1, q_r^2, ..., q_r^n, q_i, s_i^1,..., s_i^{n-1}, s_i^n, s_i^r, Y_i^n),$\n$Y_i^n = PRM(x_i^n)$ (8)\nwhere $s_i^n$ is the $n^{th}$ step of solution $s_i$.\nAccording to the input template above, it is worth noting that when predicting step n, we assume that steps 1 through n-1 are correct (Luo et al., 2024; Zheng et al., 2024). At this point, the most important task for PRM is to predict step n, so PRM can only access the reference steps for step n and cannot see the reference steps for steps 1~n-1."}, {"title": "4 Experiments", "content": "In this section, we present the experimental settings and results. Our implementation code of Retrieval-PRM is publicly available."}, {"title": "4.1 Experiment Setup", "content": "4.1.1 Datasets\nDatasets are categorized into two kinds: Math reasoning datasets, and prm training datasets.\nMath Reasoning Datasets\nWe conduct experiments on four public and widely used datasets in mathematical reasoning tasks: GSM8K (Cobbe et al., 2021) which contains math problems from elementary to middle school, MATH (Hendrycks et al., 2021) which contains math problems from basic to university level, OlympiadBench (He et al., 2024) which involves questions from the Mathematical Olympiad, Omni-MATH (Gao et al., 2024b) which covers multi-domain high-difficulty problems. Further details are provided in Appendix C.\nExcept for GSM8K, which focuses on grade school math problems, the other three datasets feature problems of competition or Olympiad-level difficulty.\nPRM training datasets\nWe conduct experiments on two publicly available datasets for PRM:\nPRM800K (Lightman et al., 2023): Based on the MATH dataset, it contains 800,000 manually annotated step-level correctness labels for training the Process Reward Model. It relies on expensive manual annotations.\nMath-Shepherd (Wang et al., 2024b): It generates 400,000 machine-annotated step-level labels (covering MATH and GSM8K datasets) by automatically building process supervision data, without manual annotation."}, {"title": "4.1.2 Evaluation Metrics", "content": "We evaluate our model in a public PRM benchmark ProcessBench (Zheng et al., 2024). The aim is to judge whether PRM can find the first wrong step. It divides data into two parts: samples with incorrect and correct final answers and then conducts harmonic mean on the accuracy of these two parts to get the final F1-score. Moreover, we think since the sample number of each part isn't balanced, We add an additional metric: weighted arithmetic mean of these two parts, which is shown in Table 1 as ArithACC."}, {"title": "4.1.3 Baselines", "content": "Following (Zheng et al., 2024), we divide all baselines into two parts:\n(1) Open-source PRM, including Skywork (01 Team, 2024), Qwen2.5-PRM (Zheng et al., 2024), Math-Shepherd (Wang et al., 2024b) and RLHFlow (Xiong et al., 2024). These models are binary classification PRMs.\n(2) Language Models as Critic, including Llama (Dubey et al., 2024), Qwen2 (Yang et al., 2024b), Qwen2.5 (Team, 2024), Qwen2.5-MATH (Yang et al., 2024a), Qwen2.5-Coder (Hui et al., 2024), GPT-40 (OpenAI et al., 2024). These models are promoted to judge the steps with the help of majority voting.\nFurther details of these baselines are provided in Appendix A due to article length limitations."}, {"title": "4.1.4 Implementation Details", "content": "Details like base models, hyperparameters, prompts, and training sizes are provided in Appendix B due to the article length limitations."}, {"title": "4.2 Overall Performance", "content": "We evaluate RetrievalPRM against existing baselines on ProcessBench, and the results are presented in Table 1. The findings are as follows:\n\u2022 RetrievalPRM-7B surpasses all open-source PRM baselines, achieving the highest performance. Notably, the most significant improvement is observed on OmniMATH, the most challenging dataset, with performance gains increasing as dataset difficulty rises. This phenomenon may stem from the fact that most baseline PRMs are trained on human- or machine-annotated datasets such as PRM800K or Math-Shepherd, which primarily focus on GSM8K or MATH and exhibit OOD issues when applied to more complex datasets. In contrast, our RetrievalPRM effectively mitigates the OOD problem through its retrieval-based approach, demonstrating the efficacy of our Two-stage Retrieval-enhanced Mechanism.\n\u2022 When comparing models of different scales, RetrievalPRM outperforms all evaluated language models, including Qwen2.5-72B-Instruct and Llama3.3-70B-Instruct, with the sole exception of QwQ-32B-Preview. Remarkably, RetrievalPRM achieves this with a model size of just 7B. This highlights that PRMs, being both lightweight and task-specific, maintain strong competitiveness and potential compared to LLMs as critics."}, {"title": "4.3 Ablation Study", "content": "We analyze two main components in the Two-stage Retrieval-enhanced Mechanism: Question-level Retrieval and Step-level Retrieval\u2014through the following ablations:\nRetrievalPRM (Ours): The complete version of our proposed method.\nRetrievalPRM (w/o Step-level Retrieval): This variant retains only the Question-level Retrieval, removing Step-level Retrieval during both training and inference.\nRetrievalPRM (w/o Question-level Retrieval): This variant retains only the Step-level Retrieval, removing Question-level Retrieval during both training and inference.\nRetrievalPRM (w/o Question-level and Step-level Retrieval): In this variant, both Question-level and Step-level Retrieval are removed during training and inference.\nThe performance of these variants is presented in Table 2, from which we can draw the following observations:\n\u2022 The performance of RetrievalPRM (w/o Step-level Retrieval) remains almost identical to that of RetrievalPRM on GSM8K and MATH but exhibits a slight decline on OlympiadBench and OmniMATH. This can be attributed to the fact that Step-level Retrieval information is partially absorbed by Question-level Retrieval. As a result, Question-level Retrieval alone may be sufficiently effective for relatively easy datasets, as the reference steps it provides contain adequate knowledge for step prediction. However, for more challenging datasets, Step-level Retrieval becomes significantly more crucial, as it offers finer-grained guidance essential for handling complex problem-solving processes.\n\u2022 RetrievalPRM (w/o Question-level Retrieval) shows lower performance, as it relies solely on Step-level Retrieval. The model lacks knowledge of reference questions, which is useful to alleviate question OOD, restricting its overall performance.\n\u2022 RetrievalPRM (w/o both Retrieval) performs the worst, which is expected, demonstrating the effectiveness of both question-level and Step-level Retrieval."}, {"title": "4.4 Hyperparameter Study", "content": "Figure 4 illustrates the impact of the number of retrieval questions on the model's performance. The findings are as follows:\nCompared to Top-0, where no retrieval questions are used, models that incorporate retrieval questions show improved performance, highlighting the importance of Question-level Retrieval. It inspires us that Reference questions are important for PRM to get warmup, no matter how many reference questions there are.\nThe performance of Top-3 exhibits a slight decline, potentially due to two factors: (1) An excessive number of reference questions may lead to an overly long input prompt, making it difficult for PRMs to comprehend or extract key information effectively. (2) A limited retrieval pool might result in later reference questions being less relevant than earlier ones, increasing the likelihood of misjudgments in the model's predictions."}, {"title": "5 Related Works", "content": "5.1 Process Reward Models\nProcess reward models (PRMs) have demonstrated significant advantages over traditional outcome reward models (ORMs) (Cobbe et al., 2021) in enhancing process-level reasoning accuracy and improving long-process reasoning abilities in model training (Lightman et al., 2023; Uesato et al., 2022). A growing number of PRMs have been proposed for application in process-level reinforcement learning with human feedback (RLHF) (Wang et al., 2024b; Qin et al., 2024; Xia et al., 2025; 01 Team, 2024). For instance, Lightman et al. (2023) made a substantial contribution by releasing a large set of human-annotated data at the process level, opening up new research opportunities for multi-step reasoning.\nAdditionally, Wang et al. (2024b) introduces an automatic, self-supervised pipeline for generating process-level labels and training PRMs, enabling efficient data generation. Xia et al. (2025) employs PRMs as automatic evaluators to assess the accuracy of multi-step reasoning in language models (LMs). With the surge in PRM-focused research and data curation, numerous PRMs (01 Team, 2024; Xiong et al., 2024; Sun et al., 2024; Gao et al., 2024a; Wang et al., 2024a) have been proposed. Additionally, several studies focus on leveraging natural language feedback from large language models (LLMs) as rewards, which are termed critic models (McAleese et al., 2024; Zhang et al., 2024; Gao et al., 2024a).\nHowever, most existing PRMs trained on math datasets such as GSM8K and MATH inevitably encounter Out-of-distribution issues, which can be divided into two categories: question OOD, where PRMs trained on simpler or medium-difficulty datasets lack understanding of questions from more challenging datasets, and step OOD, where different base models and model sizes in LLMs lead to different step distributions for the same question. This is reflected in differences in chain length, problem-solving approaches, and methods. To address these issues, we propose the RetrievalPRM framework to tackle the OOD problems encountered in the current PRM field, achieving promising results."}, {"title": "5.2 Retrieval-Augmented Generation", "content": "Retrieval-augmented generation (RAG) enhances language models by dynamically integrating external knowledge, pioneered by (Lewis et al., 2021) through their joint retrieval-generation architecture. Subsequent advances refined this paradigm. Guu et al. (2020) introduced REALM to co-train retrieval and generation modules via masked language modeling, while Izacard and Grave (2021) proposed Fusion-in-Decoder (FiD) to process multi-document contexts efficiently. Research further optimized retrieval precision through dense passage embeddings (Karpukhin et al., 2020) and scaled retrieval to web-level corpora (Borgeaud et al., 2022)."}, {"title": "6 Conclusion", "content": "In this paper, we have addressed the significant out-of-distribution (OOD) challenges faced by Process Reward Models (PRMs), particularly step OOD and question OOD. By introducing the Retrieval Augmented Process Reward Model (RetrievalPRM), we propose an effective solution that leverages a Two-stage Retrieval-enhanced Mechanism to improve the generalization of PRMs across diverse models and problems. Extensive experiments on multiple real-world datasets have shown that RetrievalPRM consistently outperforms existing methods, highlighting its effectiveness in tackling OOD issues."}, {"title": "7 Limitation", "content": "RetrievalPRM has two main limitations. Firstly, the retrieval pool is only constructed from PRM800K and Math-Shepherd at present, which is relatively small and limits the diversity and breadth of the mathematical problems. Second, using Sentence-BERT to embed questions and steps struggles to capture the full complexity of mathematical problems as semantic similarity doesn't mean knowledge similarity in Math problems. As a result, the naive cosine similarity calculated through embeddings may fail to accurately reflect the true similarity between two questions."}]}