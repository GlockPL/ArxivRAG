{"title": "Large Language Models as a Tool for Mining Object Knowledge", "authors": ["Hannah YoungEun An", "Lenhart K. Schubert"], "abstract": "Commonsense knowledge is essential for machines to reason about the world. Large language models (LLMs) have demonstrated their ability to perform almost human-like text generation. Despite this success, they fall short as trustworthy intelligent systems, due to the opacity of the basis for their answers and a tendency to confabulate facts when questioned about obscure entities or technical domains. We hypothesize, however, that their general knowledge about objects in the everyday world is largely sound. Based on that hypothesis, this paper investigates LLMs' ability to formulate explicit knowledge about common physical artifacts, focusing on their parts and materials. Our work distinguishes between the substances that comprise an entire object and those that constitute its parts-a previously underexplored distinction in knowledge base construction. Using few-shot with five in-context examples and zero-shot multi-step prompting, we produce a repository of data on the parts and materials of about 2,300 objects and their subtypes. Our evaluation demonstrates LLMs' coverage and soundness in extracting knowledge. This contribution to knowledge mining should prove useful to Al research on reasoning about object structure and composition and serve as an explicit knowledge source (analogous to knowledge graphs) for LLMs performing multi-hop question answering.", "sections": [{"title": "1 Introduction", "content": "Recently, large language models (LLMs) have gained considerable attention in the natural language processing (NLP) community for generating almost human-like text and generalizing their knowledge for NLP downstream tasks (Bhavya et al., 2022; Bubeck et al., 2023; Moghaddam and Honey, 2023). However, one reason that LLMs fall short of artificial general intelligence is that the knowledge underlying their responses is entirely implicit, aside from the ongoing dialogue contents they directly reference. As a result, they are apt to provide flawed justifications for both correct and incorrect conclusions involving step-by-step commonsense reasoning, without allowing users \"patch\" faulty assumptions or inference steps. Additionally, they tend to hallucinate facts, especially regarding named entities sparsely represented on the web.\nIn this work, we study the feasibility of generating explicit general knowledge, drawing on language models' implicit knowledge and language understanding ability, specifically for knowledge about the part structure and material composition of physical objects. Such knowledge, common even to young children, enables explainable reasoning and decision-making in combination with other commonsense knowledge, e.g., in considering whether scissors can be used to shorten a shoestring or a screw, or if a basketball or a wine bottle can be pushed off a patio table safely to the flagstone surface beneath. While LLMs may often answer adequately in such cases, when they don't, they cannot simply be told how to correct their knowledge; unlike people, they learn by abstraction from massive data rather than by simply being told.\nWhile there has been work on mining or crowdsourcing semantic features that characterize human conceptual knowledge, no large-scale existing work makes a distinction between the substances that comprise an entire object and the substances that comprise an object's parts. For example, a violin is mostly made of wood; however, more precisely, the body, fingerboard, pegs, and many other components of are made of wood while the strings are made from non-wooden materials, such as catgut, nylon, and steel. Our approach mines part and material knowledge from LLMs and expresses it in an explicit form. This knowledge can then be integrated into the knowledge base of AI systems, either supplementing or replacing black-box models and potentially enhancing explainability. From the resulting resource, which encompasses the data of 2,314 physical objects, we evaluated a subset of these items for coverage and quality through cross-dataset comparisons and intrinsic assessments. The results indicate that the majority of the extracted knowledge aligns with human understanding, but depending on the prompting method used, the knowledge can sometimes be overly simplified or excessively specific, exceeding typical layman knowledge. All data and source code are publicly available to support further research\u00b9."}, {"title": "2 Related Work", "content": "2.1 Pattern-based extraction\nThe pattern-based approach in knowledge acquisition identifies relationships in text by matching predefined lexical patterns. While it offers consistent results, it struggles with complex data patterns and implicit knowledge that is typically not documented. Early efforts, like Berland and Charniak (1999) and Poesio et al. (2002), relied on hand-built patterns to detect part-whole relationships. Later, search by Girju et al. (2006) and van Hage et al. (2006) automated the process using web data, refining it further with classifiers to determine meronymy relationships. Girju et al. used a two-way classifier, while Poesio and Almuhareb (2005) employed a multi-way classifier to categorize detected attributes. More recent advancements, such as Tesfaye. and Zock. (2012), used vector similarity to cluster co-occurring nouns, enhancing both accuracy and coverage.\n2.2 Human annotation\nHuman annotation often involves significant expenses and time due to the need for detailed and accurate annotations. When data is collected through crowdsourcing, ensuring quality control becomes challenging, especially when complex semantic relationships need to be precisely captured.\nSeveral knowledge resources integrate crowd-sourced and expert annotations to address these challenges. ConceptNet (Speer et al., 2017) incorporates both methods to build a commonsense knowledge graph, linking concepts with relations like MadeOf and Partof. However, the annotation schema can lead to ambiguity; MadeOf describes an object as a whole, while Partof identifies components without specifying their materials, even when the parts are made of different materials from other parts. For example, the tuples (bicycle, MadeOf, metal) and (bicycle seat, Partof, bicycle) does not specify a bicycle saddle, which is a part of a bicycle seat, can be made of leather. WordNet (Miller, 1995) provides an expert-curated lexical database, where human annotators organize words into semantic hierarchies and mark part-whole relations (meronymy). Its concise definitions sometimes encode part and material information, as in the example of 'felt-tip pen,' whose definition states the writing tip is made of felt. Similarly, the ParRoT dataset, developed by Gu et al. (2023), offers a fine-grained, human-annotated collection of part lists for 100 everyday objects. This dataset goes beyond major components, detailing sub-parts (e.g., the reflective glass and spring in a flashlight) to provide a comprehensive model of objects, particularly in terms of their structural and functional relationships.\nSemantic feature datasets, such as McRae norms (McRae et al., 2005) and CSLB concept property norms (Devereux et al., 2014), rely on human expertise to generate reliable annotations. These datasets include features like part-whole relations and material composition such as has_a and made_of. The McRae norms distinguish between made_of, used for substances, and made_from, used for origins (e.g., prune made from plums). Additionally, they differentiate between essential components (e.g., an engine or a door) and non-essential parts (e.g., a bed's comforter) or functional aspects (e.g., an elevator's capacity), assigning them distinct labels.\n2.3 Use of LLMs\nLLMs excel at capturing complex patterns and implicit world knowledge from their pre-training, enabling them to generate and retrieve comprehensive information with high linguistic fluency. However, challenges remain, including issues with data interpretability and the risk of generating factually inaccurate content. Several studies demonstrate both the potential and limitations of these models in generating and structuring such knowledge.\nTransOMCS (Zhang et al., 2020) builds a large commonsense graph by extracting knowledge from linguistic data but struggles with misinterpretations, such as (cement, MadeOf, consist) and (dog, Partof, walker), illustrating both scalability and the risks of generating incorrect data. ATOMIC20 (Hwang et al., 2021) combines crowdsourcing with LLMs, creating 1.33 million tuples. However, it introduces imprecision by merging both part and material information under the MadeUpOf relation. Similarly, part-related knowledge is sometimes merged with other properties under HasProperty, e.g., (bicycle, HasProperty, two wheels). ASCENT++ (Nguyen et al., 2022) refines knowledge extraction through clustering, yet still produces odd outputs, such as (crane, MadeOf, many different colors) and (desk, HasA, multilingual staff), reflecting persistent noise. Meanwhile, Hansen and Hebart (2022) employ GPT-3 to generate semantic features for 1,854 object concepts, producing detailed outputs (e.g., 'scissors have two blades, handles, and a pivot point'). However, material descriptions about parts remain imperfect (e.g., 'scissors are made of metal', missing mention of plastic handles), and occasional incoherent responses appear (e.g., 'Wineglass is a glass, it is clear...What are the properties of a bat? It is used in sport'). These examples demonstrate LLMs' potential for structuring knowledge while highlighting ongoing issues with precision and noisy outputs."}, {"title": "3 Method", "content": "In this section, we describe the methodologies used to identify subtypes, parts, and materials of various entities. We employ few-shot in-context learning and zero-shot multi-step learning, both leveraging GPT-4 Turbo (gpt-4-1106-preview) (OpenAI, 2023). The process begins with constructing a list of common physical entities, followed by few-shot and zero-shot learning independently, to generate separate datasets. These datasets allow us to compare their outcomes and ensure wide coverage in object classification. The prompts used for both methods are provided in Appendices B and C."}, {"title": "3.1 Common physical objects", "content": "To gather commonsense knowledge about the material composition of objects, we focus on artifacts since their part structure, and especially their material composition, is generally clearer than those of natural objects. Starting with entities from Wikidata (Vrande\u010di\u0107 and Kr\u00f6tzsch, 2014), we select those classified as artificial physical objects or structures. Abstract entities are removed through subclass filtering and keyword matching. We then further refine by retaining only entries that exist in WordNet and have corresponding Wikipedia links. Finally, GPT-4 (OpenAI, 2023) filters the list to include common, physical, standalone objects, resulting in a list of 2,314 entries. Full details of the filtering process are provided in Appendix A."}, {"title": "3.2 Few-shot in-context learning", "content": "We employ a few-shot learning method, using five in-context examples, to classify subtypes of entities based on their essential parts and identify those parts and the materials commonly used in them. This approach is divided into two stages. In the first stage, the model is prompted to enumerate common subtypes and their constituent parts for each entity. Subtype classification is strictly based on the presence or absence of essential, unique parts, explicitly excluding non-essential variations in size, shape, material, or function. If an entity has no subtypes, the model is instructed to specify this and list only the parts. In the second stage, after identifying subtypes and their parts, we extract typical materials used for these parts, excluding any that are primarily used for joining, stitching, or finishing. The model is guided to use specific conjunctions ('and,' 'or,' 'and/or') to reflect exclusivity, common combinations, or required co-occurrence of materials. A simplified example of the prompt with one in-context demonstration and its corresponding output is shown in Figure 1."}, {"title": "3.3 Multi-step prompting", "content": "We also employ a zero-shot prompting that is broken down into a sequence of sub-questions to identify and categorize subtypes, parts, and materials. Although each sub-question operates under a zero-shot framework, the multi-step sequence is designed to guide the LLM through complex classification tasks in a structured and incremental manner. This approach ensures that the model handles multifaceted object classifications step-by-step. Figure 2 provides an overview of the zero-shot multi-step classification algorithm, illustrating how each step builds on the previous one to refine the categorization process. The process begins with the list of common physical objects identified in Section 3.1, followed by steps to classify their subtypes and constituent elements, as detailed below.\n1. Are there different types? This first prompt asks whether the entity can be divided into distinct subtypes, ensuring that only objects with meaningful variation in their essential parts proceed through further classification steps.\n2. List different types. If the entity is determined to have subtypes, the next step involves prompting the model to generate a comprehensive list of those subtypes. The prompt emphasizes focusing on physically distinct subtypes, filtering out superficial design variations or synonyms.\n3. Extract common types. Once subtypes are identified, this prompt asks how recognizable each subtype is to the general public. This step narrows the focus to widely recognized objects.\n4. Does it have parts? For each identified subtype (or for the original entity if no subtypes are found), this prompt asks whether it has multiple, clearly distinct parts.\n5. Are the parts made of the same materials? If the entity or subtype has parts, it checks whether these parts are composed of the same materials.\n6. What is it made of? If the entity/subtype does not distinct parts, the model is prompted to list the materials that are typically used in it.\n7. List its parts and the materials. This prompt gathers a detailed list of the object's parts and their material composition for the entire object.\n8. List its parts and their respective materials. This prompt provides a granular view by identifying the object's parts and the specific materials used for each individual part."}, {"title": "4 Results & Analysis", "content": "The acquired datasets are hierarchically organized into entities (Wikipedia entries), subtypes, and subsubtypes, with parts and materials listed at the lowest level. The few-shot data includes over 6,000 items, while zero-shot data covers nearly 27,300 items. Both datasets feature thousands of subtypes, though the zero-shot data is more extensive. Most items list multiple parts and materials, with zero-shot items averaging around eight parts and two materials, while few-shot items tend to have slightly less. For additional specifics, refer to Appendix D.\n4.1 Recall based on external datasets\nTo evaluate coverage of our dataset, we calculate cross-dataset recall by comparing items from five reference datasets-ParRoT, CSLB, McRae, WordNet, and ConceptNet-to our own. For ConceptNet, we exclude entries imported from WordNet to avoid double counting. A random selection of 20 entities per dataset is used, replacing items without matches in our dataset or lacking relevant data. For example, some entities from WordNet and McRae (e.g., socks and diving suit) are replaced due to missing both part and material data, while 37 ConceptNet entities (e.g., helmet and microwave) are similarly excluded. Duplicate items (e.g., power supply cord and power cord), design- or shape-related features (e.g., open top of a bucket), and items that are not physically attached (e.g., airplane pilot) are also removed. ConceptNet's data quality is inconsistent, with problematic entries like no lead as part of a pencil and accent on second syllable as part of an umbrella. In the case of WordNet, part and material information embedded within glosses is treated as relevant information.\nTo measure cross-dataset recall, we score each reference item based on its presence in our dataset: full credit (1 point), half credit (0.5 points), and no credit (0 points). Recall is calculated as the total score divided by the total number of reference items. Full credit is assigned when 1) a matching item is found (e.g., lens of spectacles from WordNet matches lenses of glasses), 2) the reference item is a substance and our dataset contains a suitable container for it (e.g., propellant of a fire extinguisher from ParRoT matches pressure cartridge), or 3) a reference item, which is a supertype of another reference item, is satisfied by an item in our dataset not mentioned in the reference dataset (e.g., when metal, aluminium, and light metal is listed, we interpret light metal as \"aluminium and some other metals\", so light metal matches titanium). Half credit is given when 1) the item in our dataset is inclusive of the reference item (e.g., seats of an airplane from CSLB matches cabin of an airplane, straw used in a hut from McRae matches thatch) or 2) our item represents a more general type of the reference item (e.g., side carry handle of a suitcase from ParRoT matches handle, rubberized fabric of a raincoat from WordNet matches waterproof fabric)."}, {"title": "4.2 Intrinsic evaluation", "content": "To evaluate the quality of our dataset, we conduct intrinsic evaluations using three metrics: precision, recall, and distinctive feature evaluation. We sample 120 entities from the list of common physical objects obtained in Section 3.1 and apply a filtering process to ensure that the number of items evaluated does not exceed 1,000 per category, with a few exceptions (details in Appendix F.3). The intrinsic evaluation tasks are conducted on Amazon Mechanical Turk (MTurk), where workers answer multiple-choice questions, with responses collected from three different workers for each question. Sample questions and a detailed breakdown of the collected responses are provided in Appendix E and F.\nPrecision We evaluate whether an item is likely to be a subtype, part, or material of a given item. Workers are asked to categorize items according to multiple-choice options, including Likely, Unlikely, Unable (to answer), and Uncertain. The precision results (Table 2) show that most items were marked as Likely across all three categories (Subtype, Part, and Material). In particular, the zero-shot data received a higher portion of Likely responses than the few-shot data, especially for subtypes. In the few-shot data, a notable portion of responses for subtypes were marked as Unable, which may indicate that workers were unfamiliar with specific items or found the subtype names unclear or confusing. For the Part category, there is a higher percentage of Unlikely and Unable responses, compared to the other categories. This suggests that identifying essential parts poses more challenges for both LLMs and human evaluators, possibly due to the complexity involved in reasoning about parts. Interestingly, all categories had a very low percentage of responses marked as Uncertain (less than 1%), for both few-shot and zero-shot data. This suggests that, while there is some unfamiliarity, workers rarely expressed outright uncertainty about their evaluations.\nTo better understand the reasoning behind the significant portion of Unlikely responses for the part items, we further analyze these Unlikely responses. Table 3 provides a detailed breakdown of the specific reasons why certain parts were deemed unlikely, and the respective proportions for each reason. The most dominant reason that the parts are labeled with Unlikely in both few-shot and zero-shot data (47.67% and 44.13%, respectively), was that the workers identified the parts listed in the dataset as a feature rather than a part. There are also a substantial portion (30.81% and 32.03%) of the parts identified as Not an essential part; workers judged many of the parts listed by the LLM as peripheral to the item's existence, i.e., the part is often shown/used with the item, but it is either not essential, or not included with or attached to the item. The same analysis for material items, which had relatively few Unlikely responses, is provided in Appendix F.2.\nRecall In a manner similar to the evaluation of precision, we assess whether the provided lists for subtypes, parts, or materials of an item are complete. The sum-"}, {"title": "4.3 Dataset-comparison Evaluation", "content": "Human-annotated data creation We also evaluate the quality of our dataset using comparisons with human annotations. For this, we create a human-annotated dataset to capture the detailed part structures and material compositions of 120 entities. These entities are randomly sampled from the object list provided in Section 3.1, ensuring no overlap with the 120 entities used for intrinsic evaluation in the previous section. The annotation process followed a structured approach, conducted by in-house lab members and the authors, guided by a set of criteria (in Appendix G). Annotators relied on Wikipedia as the primary reference source but were not limited to it; they were also allowed to consult external sources, such as online searches, when necessary. Subtypes were annotated up to two levels-subtypes and subsubtypes-when relevant, ensuring consistency with the depth used in few-shot and zero-shot methods. Subtypes were included only when they reflected meaningful differences in part structure or material composition. For entries with existing Wikipedia pages, we strictly adhered to Wikipedia's naming conventions. Much like few-shot and zero-shot methods, our dataset captures detailed part structures, including optional components, and appropriately uses conjunctions like 'and/or' to reflect material combinations\u00b2. We also annotated instances where uniform materials spanned multiple parts of an entity."}, {"title": "5 Conclusion", "content": "Our study explores the potential and challenges of using LLMs to extract structured knowledge about the subtypes, parts, and materials of physical objects. We found that few-shot and zero-shot methods offer unique advantages, balancing specificity, coverage, and clarity in ways that sometimes exceed human-annotated data. This work advances efforts toward AI systems that are not only accurate, but also explainable, adaptable, and transparent."}, {"title": "Limitations", "content": "While our proposed work demonstrates promising results in extracting and organizing part and material knowledge from LLMs, it is not without limitations. These limitations span several areas, including the nature of knowledge representation, learning techniques, domain specificity, computational scalability, and risks of inconsistency.\nFirst, a fundamental limitation arises from the nature of the knowledge being extracted. Different people"}, {"title": "Ethics Statement", "content": "This research acknowledges the potential risks associated with the misuse of certain information. While the study includes the classification of objects such as weapons, caution is advised when accessing this data. Some details, such as the parts and materials of dangerous devices, could be exploited by individuals with harmful intent. However, in cases where the risk of misuse was deemed too high, the LLM deliberately withheld specific information. For example, the LLM listed 'Suicide vest' as a subtype of 'Explosive device,' but it refused to generate parts or material details to mitigate potential danger.\nAll code and data used and generated in this project are publicly and freely accessible to promote transparency and reproducibility while encouraging responsible use."}, {"title": "A Constructing common physical objects", "content": "Since we are interested in acquiring commonsense knowledge about the world, and the material composition of physical objects is an important aspect of such knowledge, our initial aim is to collect a list of physical objects that many people perceive or interact with. We are focusing on artifacts since their part structure, and especially their material composition, is generally clearer than those of natural objects. Consider for example humans, bushes, mountains, or stars; also, we plan to extend this work to provide usage (telic) information about objects, an aspect that applies much more clearly and consistently to artifacts than to natural objects.\nFor this, we first obtain all Wikidata entities from its JSON dump (downloaded on May 26th, 2023). We select entities that are subclasses of artificial physical objects, artificial physical structures, or artificial entities. The selected list still contains abstract entities such as 'process' and 'genre' due to multiple inheritances in Wikidata. To address this, we apply two layers of filtering:\n1. Keyword-based filtering: If the entity's Wikipedia title contains specific keywords, it is excluded. Examples of such keywords include law, protein, pattern, physics, topology, and unit.\n2. Subclass-based filtering: If an entity belongs to specific subclasses, it and all its descendant categories are excluded. Examples of these subclasses are concept, detection, genre, formal system, organism, and sound.\nThe list of entities is further filtered to ensure that it contains only commonly known nouns that occur as dictionary entries. As a dictionary source, we use the library module for WordNet, a lexical database, in NLTK (https://www.nltk.org). We filter out entries based on the availability of the entries and their synonyms in WordNet; we filter out entries if neither the entry noun itself nor any of its synonyms exist in WordNet, and if the entry does not have any link from Wikidata to its associated WordNet synset. Wikidata entries without a link to their corresponding Wikipedia articles are also excluded. Lastly, to refine the entries to include only physical, commonly encountered, and readily perceived objects, we ask GPT-4 a sequence of four filtering prompts. First, we use a prompt to identify readily perceived entities that a sixth-grader would know, then filter for physical objects. Next, we narrow the list to count nouns, and finally, ensure the remaining items are individual, standalone objects. The prompts in the filtering process can be found below."}, {"title": "A.1 Prompt to identify readily perceived entities", "content": "How likely are the following 50 things to be commonly recognized by a typical sixth-grader? Add ' - [likely / probably likely / probably unlikely / unlikely] to be recognized by sixth-graders' after the nouns in the list. Please do not alter the names within parentheses."}, {"title": "A.2 Prompt to identify physical entities", "content": "Could you classify the following 50 nouns based on whether they primarily refer to standalone physical objects, standalone built structures, substances, or neither? Add ' - is a [physical object / built structure / substance / neither]' after the nouns in the list. Please do not alter the names within parentheses.\nHere are the criteria for each category:\nPhysical objects: Tangible items that can exist independently, or items that might be part of a larger entity but can be replaced.\nBuilt structures: Man-made constructions that serve as physical places or infrastructure.\nSubstances: Any substance with uniform characteristics or any matter that can be best characterized by their chemical composition.\n- Neither: None of the above."}, {"title": "A.3 Prompt to identify count nouns", "content": "Could you classify the following 50 nouns based on whether they are in general used as a mass noun or a count noun? Add ' - mass noun' or ' - count noun' accordingly after the nouns in the list."}, {"title": "A.4 Prompt to identify individual standalone entities", "content": "Could you classify the following 50 nouns based on whether they are typically described as an entity on their own, or composed of multiple standalone entities? Add' a single entity', - a group of components but commonly referred to as a single item', or ' - a group of multiple standalone items' accordingly after the nouns in the list."}, {"title": "B Few-shot in-context learning prompts", "content": "B.1 Prompt for subtypes and parts\nPlease list common categories and their sub-categories, and their constituent parts of the given entity. Each type must be distinguished solely by the unique presence of their essential parts or components. Only list essential"}, {"title": "B.2 Prompt for materials", "content": "Please list the materials that the listed parts of the given entity are typically made of. Exclude any materials used for joining, stitching or dying.\nAllow any necessary repetition in materials across different parts. Avoid using \"sometimes\", \"such as\", and parentheses in your response. Connect the materials with one of the following conjunctions:\n\"and\": all listed materials are typically used together\n\"or\": each of the materials from the list is used exclusively\n\"and/or\": some of the listed materials are typically used in combination"}, {"title": "C Zero-shot in-context learning prompts", "content": "C.1 Prompt to assess whether an entity has subtypes\nAre there any essential, non-optional parts\n1) that are present in one type of [noun] but absent in another and\n2) that would be recognized by most people?\nSimply say \"yes\" or \"no\".\nC.2 Prompt to generate a list of subtypes\nIn numbered points, please simply list physically distinct types of noun, where each type is distinguished by unique, externally visible, essential parts.\nExclude any categories that share the same essential external components and functions. The listed categories should reflect differences in their primary operation rather than just external design variations or connections.\nAlso, avoid from your list any categories that merely represent design variations, subtypes, or alternate names for the same tool. Format each entry as a complete noun without using 'traditional', 'and', 'or', nouns indicating materials, or any prepositional phrases such as 'with' in the names.\nC.3 Prompt to identify common subtypes\nHow likely would the following types of noun be recognized by most people? Add \" - [likely / probably likely / probably unlikely / unlikely] recognized by most people\" after the nouns in the list. Please do not alter the names"}, {"title": "C.4 Prompt to assess whether an object has parts", "content": "How many parts does noun have? Specifically, how many clearly distinct parts that are attached to it or inseparable from it? Please simply say the number of parts."}, {"title": "C.5 Prompt to assess whether an object has uniform materials across different parts", "content": "Are distinct parts of noun made of the same materials? Say \"yes\" or \"no\"."}, {"title": "C.6 Prompt to identify materials of an object", "content": "In one line, please list solely the types of materials that noun are typically made of. Avoid using \"sometimes\", and connect the materials with a conjunction, e.g., 'glass, plastic, and/or metal'. Exclude any materials used for joining, stitching or dying.\nHere are the conjunctions you can use:\n\"and\": all listed materials are typically used together\n\"or\": each of the materials from the list is used exclusively\n\"and/or\": some of the listed materials are typically used in combination.'"}, {"title": "C.7 Prompt to identify parts and materials of an object", "content": "1) Starting your paragraph with \u201c<Parts>\\n\u201d, in numbered points, please list clearly distinct, essential parts of noun with succinct descriptions followed by \":\". For each part, insert a new line that starts with \"- Optional:\". Answer with \"yes\" or \"no\".\n2) Starting your paragraph with \u201c<Materials>: \", in new bullet points, please list solely the materials that a typical noun is entirely made of. Avoid using \u201csometimes\", and connect the materials with a conjunction, e.g., '<Materials>: glass, plastic, and/or metal'. Exclude any materials used for joining, stitching or dying. Here are the conjunctions you can use. - \"and\": all listed materials are typically used together\n\"or\": each of the materials from the list is used exclusively\n\"and/or\": some of the listed materials are typically used in combination.\nKeep your answers very simple, in terms a second-grader would understand."}, {"title": "C.8 Prompt to identify an object's parts and their materials", "content": "In numbered points, please list the clearly distinct, essential parts of noun that are attached to it or inseparable from it, with succinct descriptions following \":\". Things that have multiple independent uses, such as 'battery', don't count as a part. You may use \"internal mechanism\" as a part for anything that is not visible from the outside.\nFor each part, insert a new line that starts with \"- Optional:\\\u201d. Answer with \"yes\" or \"no\".\nThen again, for each part, insert a new line that starts with \"- Materials:\" and mention the materials the part is typically made of. List the materials, avoiding using \"sometimes\", and connect the materials with a conjunction, e.g., '- Materials: glass, plastic, and/or metal'. Here are the conjunctions you can use.\n\"and\": all listed materials are typically used together\n\"or\": each of the materials from the list is used exclusively\n\"and/or\": some of the listed materials are typically used in combination.\nKeep your answers very simple, in terms a second-grader would understand."}, {"title": "DOverview of the acquired subtypes, parts, and materials", "content": "The acquired datasets are hierarchically organized into entities (Wikipedia entries), subtypes, and subsubtypes, with parts and materials. At the most granular level, individual items at the lowest level in the hierarchy are characterized by their constituent parts and materials. If certain items lack subtypes or subsubtypes, the parts and materials associated with the higher-level entity are listed instead.\n\u2022 Total items: The occurrences of items that contain associated parts and/or materials, across all entities.\nFew-shot: 6,275 items\nZero-shot: 27,285 items"}, {"title": "D.1 Distribution of subtypes and subsubtypes", "content": "\u2022 Entities without subtypes: the number of entities are classified without distinct subtypes.\nFew-shot: 1,167 entities\nZero-shot: 677 entities\n\u2022 Few-shot subtypes\n4,851 unique subtyeps all entities\n5,056 unique subtypes per entity\n84 unique subsubtypes\n\u2022 Zero-shot subtypes\n8,486 unique subtyeps across all entities\n8,987 occurrences of subtypes per entity\n21,329 unique subsubtypes across all entities\n22,489 unique subsubtypes per entity\n22,780 unique subsubtypes for every entity and each of its subtypes\nThe difference in the number of unique occurrences of subtypes or subsubtypes is due to overlapping entries. Below is an example of such overlaps:\n\u2022 Kitchen utensil > Ladle > Soup ladle\n\u2022 Spoon > Ladle > Soup ladle\n\u2022 Spoon > Serving spoon > Soup ladle"}, {"title": "D.2 Distribution of parts and materials", "content": "\u2022 Few-shot items without parts\n190 items (out of 1,167) lack parts\n104 subtypes (out of 5,024) lack parts\n3 subsubtypes (out of 84) lack parts\n\u2022 Zero-shot items without parts\n92 items (out of 677) lack parts\n207 subtypes (out of 3,828) lack parts\n360 subsubtypes (out of 22,780) lack parts\n\u2022 Average number of parts\nFew-shot: 4.36 parts per item\nZero-shot: 8.14 parts per item\n\u2022 Average number of materials\nFew-shot: 2.04 materials per item\nZero-shot: 2.37 materials per item"}, {"title": "E Sample questions on Amazon Mechanical Turk", "content": "E.1 Precision\nSubtype precision\nIs Bucket hat commonly considered as a plausible type of Cap?\n1. Likely.\n2. Unlikely because Bucket hat is a part of Cap.\n3. Unlikely for other reasons.\n4. Unable to answer because I am not familiar with Cap.\n5. Unable to answer because I am not familiar with Bucket hat.\n6. Unable to answer because either Cap or Bucket hat is named poorly.\n7. I am just not sure about anything.\nPart precision\nIs single cord commonly considered as a plausible part of Staff sling?\n1. Likely.\n2. Unlikely because Staff sling does not have any parts.\n3. Unlikely because single cord is rather a feature than a part.\n4. Unlikely because single cord is rather a material used in the Staff sling.\n5. Unlikely because single cord is often shown/used with Staff sling, but single cord is either not essential, or not included with or attached to Staff sling.\n6. Unlikely because single cord is a material used in Staff sling, rather than a part.\n7. Unlikely because single cord is irrelevant to Staff sling.\n8. Unlikely for other reasons.\n9. Unable to answer because I am not familiar with Staff sling.\n10. Unable to answer because I am not familiar with single cord.\n11. Unable to answer because either Staff sling or single"}]}