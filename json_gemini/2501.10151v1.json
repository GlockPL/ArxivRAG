{"title": "Topology-Driven Attribute Recovery for Attribute Missing Graph Learning in Social Internet of Things", "authors": ["Mengran Li", "Junzhou Chen", "Chenyun Yu", "Guanying Jiang", "Ronghui Zhang", "Yanming Shen", "Houbing Herbert Song"], "abstract": "With the advancement of information technology, the Social Internet of Things (SIoT) has fostered the integration of physical devices and social networks, deepening the study of complex interaction patterns. Text Attribute Graphs (TAGS) capture both topological structures and semantic attributes, enhancing the analysis of complex interactions within the SIoT. However, existing graph learning methods are typically designed for complete attributed graphs, and the common issue of missing attributes in Attribute Missing Graphs (AMGs) increases the difficulty of analysis tasks. To address this, we propose the Topology-Driven Attribute Recovery (TDAR) framework, which leverages topological data for AMG learning. TDAR introduces an improved pre-filling method for initial attribute recovery using native graph topology. Additionally, it dynamically adjusts propagation weights and incorporates homogeneity strategies within the embedding space to suit AMGs' unique topological structures, effectively reducing noise during information propagation. Extensive experiments on public datasets demonstrate that TDAR significantly outperforms state-of-the-art methods in attribute reconstruction and downstream tasks, offering a robust solution to the challenges posed by AMGs. The code is available at https://github.com/limengran98/TDAR.", "sections": [{"title": "I. INTRODUCTION", "content": "WITH the rapid advancement of technologies like arti- ficial intelligence, the Internet of Things (IoT), and social media, the integration of physical and social domains has become increasingly prominent [1]\u2013[3]. Among them, the Social Internet of Things (SIoT) particularly focuses on the social aspects within IoT and is widely applied in various scenarios such as social networks [4], [5], privacy protection [6], [7], and recommendation systems [8], [9]. In SIoT, Text Attribute Graphs (TAGs) are widely employed to describe and analyze community relationships, as they can simultaneously capture both the topological structure of edges and the se- mantic attributes of nodes [10]\u2013[13]. However, due to issues such as privacy protection and incomplete data collection, TAGs often suffer from missing attributes, known as Attribute Missing Graphs (AMGs), as shown in Figure 1 (a). Since most graph learning methods and models are designed for complete attributed graphs, AMGs present significant challenges to traditional graph learning [14]\u2013[16]. As shown in Figure 1 (b), entities in TAGs often have incomplete attribute features. Effectively analyzing AMGs has become a pressing prob- lem. Traditionally, reconstructing missing attributes relies on statistical and linear algebra methods, such as feature interpo- lation [17], [18] and matrix completion [19], [20]. However, these methods perform poorly when handling unstructured graph data. To address these limitations, graph neural networks (GNNs) like graph convolutional networks (GCN) [21], graph attention networks (GAT) [22], and GraphSAGE [23] have been developed to leverage graph topology for attribute prop- agation between nodes, but these methods are not specifically designed for AMGs and cannot effectively recover missing attributes, resulting in poor performance. Recognizing this gap, researchers have turned to generative techniques to better handle missing data in AMGs. Methods like autoencoders (AEs) [24], variational graph autoencoders (VGAEs) [25], and generative adversarial networks (GANs) [26] enhance attribute learning by embedding auxiliary tasks. For example, SAT [27], [28] combines AEs and GANs, using dual encoders to learn both attribute and structural repre- sentations of AMGs and generating missing attributes using structural representations. SVGA [29] employs a structured variational graph autoencoder architecture and incorporates Gaussian Markov random fields (GMRFs) [30] into the distri- bution of latent variables to accurately estimate node features. In addition, methods like CSAT [31] and AmGCL [32] in- troduce contrastive learning into AMG processing, utilizing mutual information maximization to generate rich node rep- resentations and enhance model robustness. To further refine attribute recovery, ITR [33] initially fills missing attributes using graph structural information, followed by adaptive re- finement. MATE [34] integrates multi-view information to impute missing graph attributes, enhancing the effectiveness and robustness of attribute imputation through a multi-view fusion strategy. AIAE [35] employs a dual encoder based on knowledge distillation to more effectively integrate existing attributes and structural information during the encoding stage and enhances decoding capability with a multi-scale decoder featuring masking. Despite the effectiveness of existing methods in certain contexts, they face notable limitations and challenges. First, existing methods often neglect the proper initialization of missing node attributes before inputting them into the net- work. Arbitrary or inaccurate initialization of these missing attributes can introduce significant noise, adversely affecting the representations of nodes with known attributes. Second, as illustrated in Figure 1 (c), the topology of AMGs is more complex than that of fully attributed graphs. AMGS feature four types of node relationships based on whether nodes have known or unknown attributes: known-known (Akk), known-unknown (Aku), unknown-known (Auk), and unknown-unknown (Auu). Existing methods often treat all nodes uniformly, failing to account for the varying impor- tance and influence of different node types, which leads to inaccuracies in recovering missing attributes. Third, dur- ing the learning process of AMGs, the embedding space faces issues of attribute dispersion between connected nodes and misleading attribute similarities between non-connected nodes. Current methods lack robust mechanisms to distinguish whether variations in node similarity of embedding space stem from the true underlying structure of the graph or are artifacts introduced by missing data. To overcome these challenges, we propose a novel frame- work named Topology-Driven Attribute Recovery (TDAR), which leverages graph topology through a multitask learning mechanism to more comprehensively and accurately handle AMGs. TDAR introduces three key solutions to address the challenges of AMGs: Topology-Aware Attribute Propagation (TAAP): TAAP views attribute propagation as a Dirichlet energy minimiza- tion process. By utilizing the graph's topological features, it propagates known node attributes to pre-fill those of nodes with missing attributes. TAAP employs both global propaga- tion, ensuring globality across the graph, and a known reset mechanism, which prevents the distortion of original known attributes during the process. This approach directly tackles the challenge of initializing missing attributes, reducing noise and improving feature quality. Embedding Space Propagation Confidence (ESPC): To adjust attribute recovery based on node importance, the ESPC strategy assigns dynamic attention weights to nodes according to their topological positions in the graph (known or unknown attributes). This method compensates for potential errors in information propagation, ensuring more accurate modeling of diverse topological relationships within AMGs. Node Homogeneity Score (NHS) and Non-Linkage Similarity Calibration (NLSC): NHS evaluates homogene- ity among neighboring nodes, ensuring similar nodes clus- ter together in the embedding space, promoting coher- ence. NLSC calibrates misleading similarities between non- connected nodes that may appear similar in the embed- ding space, preventing incorrect interpretations and preserving structural integrity. In summary, TDAR not only improves the initialization of missing attributes but also intelligently models the complex topological relationships inherent in AMGs, all while effec- tively filtering out noise and redundancy. Our contributions include the following innovations: We introduce the TAAP, which utilizes the topological structure of the graph to pre-fill missing node attributes. Unlike conventional methods, TAAP leverages global graph connectivity to guide context-aware pre-filling, reducing noise in generative models and ensuring higher- quality attribute recovery from the outset. We propose the ESPC mechanism to account for the varying significance of nodes with both known and miss- ing attributes. ESPC dynamically adjusts node influence by assigning attention weights based on each node's topological position. We enhance graph embedding by aligning node simi- larities with their topological relationships through the NHS and NLSC. These mechanisms mitigate the impact of noisy or redundant connections, ensuring that both connected and non-connected nodes are represented ac- curately in the embedding space. Extensive experiments on public datasets validate the effectiveness of our framework, demonstrating superior performance in attribute reconstruction, node classifica- tion, and clustering tasks compared to existing state-of- the-art methods. The organization of this paper is as follows: Section II provides a review of related work. Section III describes the problem"}, {"title": "II. RELATED WORK", "content": "Deep graph learning performs well in graph community detection and analysis. GNNs [36], [37] have unique advan- tages in capturing complex relationships and features in graph structures, significantly improving the performance of tasks such as community detection, node classification, and link prediction. GCN [21] efficiently learns node representations through first-order approximate graph convolution operations and is widely used in node classification and graph embedding. GAT [22] introduces an attention mechanism to dynamically weight the features of neighboring nodes according to their importance, thereby improving the flexibility and accuracy of node representation. GraphSAGE [23] generates node em- beddings by sampling and aggregating node neighborhood features, enabling it to handle large-scale graph data. In terms of self-supervised models, GAE [25] applies autoencoders to graph data, learns node embeddings by encoding node features and reconstructing graph structures. VGAE [25] combines the advantages of VAE [24] and GAE, and can better handle the uncertainty and complex structure in graph data by learning the implicit distribution of node features through variational inference. Recent methods such as Graph Transformer [38] use the Transformer architecture to capture global and local information in graph data. GraphGPS [39] is committed to enhancing the ability of graph neural networks (GNNs) in capturing graph structure information and node positions. GraphGPS improves the performance of the model in vari- ous graph-related tasks by seamlessly combining two types of information: global position encoding and local structure encoding. Although these methods are highly effective for general graph representation learning, they are not specifically de- signed to address the challenges posed by AMGs. As a result, they struggle to recover missing attributes, which is critical for accurate analysis in AMG contexts. Our TDAR framework not only builds upon the foundational concepts of methods like GCN and GAE, utilizing message passing for missing attribute reconstruction, but also introduces more advanced strategies that address the unique challenges presented by AMGs."}, {"title": "B. Attribute Missing Graph Learning", "content": "To address the issue of missing node attributes in graph learning, diverse deep learning strategies have been explored. Early methods focused on simple feature aggregation tech- niques. Simcsek et al. [40] utilized mean pooling to aggregate neighboring node features. Huang [41] and Chen et al. [42] employed techniques based on attributed random walks for generating node embeddings within bipartite graphs that in- clude node attributes. Generative models have been widely used for handling missing data. Yoon et al. [43] applied GANs [26] to synthesize absent data, thereby mirroring the real data's distribution. Vincent et al. [44] and Spinelli et al. [45] innovatively introduced a denoising graph auto-encoder (GAE), encoding edge-based similarity patterns to extract robust features and complete missing attribute reconstruction. Taguchi et al. [46] transformed missing attributes into Gaussian mixture mod- els, enabling the use of GCNs in networks with incomplete attributes. Chen et al. [27], Jin et al. [28], and Yoo et al. [29] focused on learning and probabilistic modeling of node representations. They built on dual auto-encoder architectures and used techniques like GANs [26] and GMRFs [30], under the guidance of shared latent space assumptions and structural information. Recently, self-supervised learning has gained traction in node representation learning for AMGs. Li et al. [31] lever- aged contrastive learning to identify patterns among nodes, aggregating information from diverse samples, and employing a Transformer architecture for modeling node relationships. Zhang et al. [32] introduced a novel self-supervised learn- ing structure using contrastive learning, effectively handling missing node attributes in attribute graphs. Huo et al. [47] effectively improved the performance of GNNs on incomplete graphs by separately designing feature-level and structure-level teacher models and avoiding interference between features and structures through a distillation process. Several methods have employed autoencoders to address missing attributes by leveraging the graph's structure. Tu et al. [33] proposed an imputation initialization method to initially fill missing attributes by leveraging the graph's structural information, followed by adaptive improvement of the imputed latent variables. Peng et al. [34] developed an attribute im- putation method for the input space, incorporating parameter initialization and graph diffusion to generate multi-view infor- mation. Tenorio et al. [48] introduced a method for recovering node features in graph data by using graph autoencoders and considering local graph structure, effectively addressing the problem of completely missing node features. Xia et al. [35] proposed a novel attribute imputation autoencoder that employs a dual encoder based on knowledge distillation and a multi-scale decoder with masking to improve the accuracy, robustness, and generative ability of imputation for AMGs. Although existing methods relying on generative models have made significant progress in AMG learning, several key challenges remain unresolved, such as the pre-filling of initial attribute features, the impact of missing attribute nodes on con- fidence in the embedding space, and the topological guidance for attribute reconstruction. To more fully leverage the rich context provided by the graph's topological propagation and embedding space,TDAR leverages native graph topology to propagate known attributes and pre-fill missing ones, ensuring both global consistency and local refinement. It dynamically adjusts propagation weights based on node importance, cap- turing relationships across different graph scales. Additionally, it enhances robustness by aligning the embedding space with the graph's structure and attributes, leading to more accurate attribute reconstruction."}, {"title": "C. Feature Propagation for Attribute Recovery", "content": "Feature Propagation (FP), a simple and efficient method, has been applied in AMG learning, primarily relying on the principle of Dirichlet energy minimization [49]. Dirichlet energy minimization ensures that attributes in the graph change smoothly across its structure. This implies that the attributes of a node are influenced by, and tend to be similar to, those of its neighboring nodes. Therefore, this method can efficiently and effectively pre-fill missing attributes. Rossi et al. [15] utilized FP to leverage graph structures, while Um et al. [50] extended this method by introducing a pseudo-confidence weighted mechanism for feature propagation. However, traditional methods such as FP often require mul- tiple iterations, which may lead to excessive feature smooth- ing, and known attribute features are not updated during iteration. Our method introduces a small global average value during the update of unknown attributes and allows known attribute features to participate in the iteration process. This reduces the number of iterations and refines feature processing, thereby enhancing efficiency and accuracy."}, {"title": "D. Graph Learning in SIoT", "content": "The development of the SIoT has driven researchers to explore the social relationships between IoT devices and their application scenarios [51]. Jung et al. [52] proposed a method to quantify social strength in SIoT, enhancing system performance by analyzing the social relationships between devices. Khelloufi et al. [53] designed a service recommenda- tion system based on social relationships, focusing on service matching and personalized recommendations between devices. Guo et al. [54] proposed a deep learning-embedded SIOT system to address ambiguity-aware social recommendations. By leveraging social relationships between devices and deep learning models, the system provides more intelligent rec- ommendation services. Sun et al. [55] proposed an inte- grated PCA and DAEGCN model for movie recommendation, leveraging both social and physical network information in SIoT. Yang et al. [56] introduced a learning-driven task- optimized group search algorithm, which improves task al- location and execution efficiency in the SIoT environment by optimizing the search process between devices. Mohana et al. [57] developed an Al-powered classification, clustering, and navigation simulator (CCNSim) for SIoT, aiming to enhance communication efficiency and navigation capabilities between devices, advancing the intelligence of SIoT systems. Jing et al. [9] designed a dual preference perception network, targeting fashion recommendation scenarios and further enhancing the personalization level of recommendation systems in SIoT. In the field of graph learning, researchers have been explor- ing how to utilize graph-structured data to improve various tasks. Wu et al. [8] proposed an efficient adaptive graph convolutional network (EAGCN), which was applied to item recommendation in SIoT, effectively improving recommenda- tion accuracy and adaptability. Wang et al. [3] designed a minority-weighted graph neural network model that focuses on addressing the imbalance in node classification in social networks, ensuring that the information from minority nodes is better learned and captured. Additionally, Chen et al. [1] utilized knowledge graph embeddings to achieve zero-shot text classification, particularly suited for label-free classification scenarios in social media data. Li et al. [2] further explored how to improve node classification performance in social networks through the dual mutual robust graph convolutional network in weakly supervised settings. Furthermore, Jiang et al. [58] proposed a trust-based fraud detection model (TFD), which utilizes graph GCN to analyze device interactions in SIoT. SIoT and graph learning are closely related research fields, where the social relationships and interactions between IoT devices naturally form complex graph structures. However, the issue of missing data limits the effectiveness of traditional graph learning methods when handling the complex graph structures in SIoT. To address this, we propose an effective AMG handling strategy aimed at filling in the missing at- tributes in SIoT, thereby further enhancing the applicability and potential of graph learning techniques in SIoT."}, {"title": "III. PRELIMINARIES", "content": "We define a graph G = (V, E) with missing node attributes. Here, V = VU Vk represents the node set, where Vk and Vu denote nodes with unknown and known attributes, respectively. Among a total of N nodes, only k nodes possess attributes, satisfying |k|+|u| = N. E represents the set of edges, and M denotes the number of edges. The node attribute feature matrix is represented as X = [Xk] \u2208 RN\u00d7F, where F is the number of attribute features. Other relevant information includes the adjacency matrix A = [Akk Aku] \u2208 {0,1}N\u00d7N, the degree matrix D, symmetrically normalized adjacency matrix \u00c2 = D-1/2AD-1/2, and the Laplacian matrix L = I \u2013 \u00c2. The objective of this work is to reconstruct the attribute features X of the graph with missing attributes and apply them to downstream tasks."}, {"title": "B. Feature Propagation", "content": "FP [15], [50] considers attribute propagation as a process of Dirichlet energy minimization [49], enhancing feature smooth- ness across the topology [59]. Using the graph Laplacian matrix L, FP can estimate the properties of unknown nodes from the known nodes. FP aims to find a smooth attribute distribution that minimizes changes on the graph, achievable by minimizing the Dirichlet energy function E(X) = XTLX. Setting its derivative to zero, we have:\n$\\begin{aligned}\n&\\qquad E(X) = \\begin{bmatrix} X_k^T & X_u^T \\end{bmatrix} \\begin{bmatrix} L_{kk} & L_{ku} \\\\ L_{uk} & L_{uu} \\end{bmatrix} \\begin{bmatrix} X_k \\\\ X_u \\end{bmatrix} \\\\\n&\\qquad \\frac{\\partial E(X)}{\\partial X_u} = 0 \\\\\n&\\qquad L_{uk}X_k + L_{uu}X_u = 0 \\\\\n&\\qquad X_u = -L_{uu}^{-1}L_{uk}X_k.\n\\end{aligned}$                            (1)"}, {"title": "IV. METHODOLOGY", "content": "The TDAR framework provides a comprehensive end-to- end approach for learning from graphs with missing attributes. It employs a self-supervised mechanism that integrates the processes of reconstructing missing node attributes and lever- aging graph topology. Built on the GAE architecture, TDAR introduces four key strategies: TAAP enriches the initialization of missing attributes using topological insights; ESPC dy- namically balances the embedding space; and NHS, alongside NLSC, collaboratively refines the identification of node rela- tionships within the graph. Figure 2 illustrates the framework of TDAR."}, {"title": "A. Topology-Aware Attribute Propagation", "content": "In the context of AMGs, nodes are influenced by their topological structure and connections with neighboring nodes. Leveraging these inherent structures and connection patterns aids in reconstructing missing attributes [15], [50], [60]. \u03a4\u03bf address this, we introduce the TAAP, which utilizes graph topology to pre-fill missing attributes by identifying key de- pendencies. By providing a structured initialization based on the graph's inherent connectivity, TAAP reduces noise during the learning process, allowing for more accurate attribute reconstruction and improved model stability. Equation (3) represents the general form of FP [15], notably requiring no learnable parameters, thus making it suitable for pre-filling missing features in TAAP, enhancing performance in subsequent network framework learning. However, this method does not fully account for certain special cases; for instance, iterative convergence often requires a large num- ber of iterations (typically 40), which can lead to excessive feature smoothing. Additionally, during the iteration process, the refined features of the known attribute nodes are not updated. To address these issues, our TAAP modifies the FP process by incorporating an additional matrix \u0398: a small global average is introduced during the update process for unknown attributes to reduce the number of iterations, and known attribute features are allowed to participate in the iteration process, ultimately influencing the output of refined features. Therefore, the iterative process of Equation (2) can be re-expressed as:\n$\\begin{aligned}\nX^{(l+1)} &= X^{(l)} \\\\\n&=\\begin{bmatrix} X_k^{(l+1)} \\\\\nX_u^{(l+1)} \\end{bmatrix} = \\begin{bmatrix} X_k \\\\ A_{uk} X_k^{(l)} + A_{uu} X_u^{(l)} \\end{bmatrix} + \\Theta \\\\\n&=\\begin{bmatrix} A_{kk} & A_{ku} \\\\\nA_{uk} & A_{uu} \\end{bmatrix} \\begin{bmatrix} X_k \\\\\nX_u^{(l)} \\end{bmatrix} +\\begin{bmatrix} \\alpha X \\\\\n(1-\\beta)X^{(l)}\\end{bmatrix} \\qquad (7)\n\\end{aligned}$\nwhere X represents the mean of the known attribute features of the lth layer, and \u03b1 controls global propagation and \u03b2 controls the known reset, which are 0.05 and 0.1, respectively, in this paper. Based on this, the update process of Equation (3) is re-expressed as:\n$\\begin{aligned}\n\\qquad X_k^{(l+1)} &= A_{ku}X_u^{(l)} + A_{kk}X_k \\\\\n\\qquad X_u^{(l+1)} &= A_{uk}X_k^{(l)} + A_{uu}X_u^{(l)} \\\\\n\\qquad X^{(l+1)} &= X^{(0)} + \\beta X^{(l+1)}.\\\\\n\\end{aligned}$                          (8)\nTAAP enhances the comprehensiveness of feature propagation and the smoothness of the reset process by introducing global information and known resets. Unlike FP, TAAP does not increase complexity during iterations, allowing for rapid pre- filling of unknown attributes (experimental results in Figure 7"}, {"title": "B. Embedding Space Propagation Confidence", "content": "After feature pre-filling with TAAP, our TDAR can further obtain latent embeddings Z = GNN(X, \u00c2, \u03c6) through the encoder defined in Equation (4). To balance the relationships between nodes in the latent space and optimize the signal-to- noise ratio, we introduced the ESPC strategy to distinguish the impact of different types of nodes on Z. By using distance encoding [61], [62], we enhance the confidence of node connections, thereby improving the fidelity of information in the embedding space."}, {"title": "1) Topological Position Function:", "content": "In addressing the prop- agation confidence of each node's position within the embed- ding space, we introduce two distance functions, fk2u(v) and fu2k(v), for capturing bidirectional influences between nodes with unknown and known attributes. The function fk2u(v) computes the shortest path from an unknown attribute node v \u2208 Vu to the nearest known attribute node in Vk, thereby determining the confidence level of the unknown node:\n$\\begin{aligned}\n&\\qquad f_{k2u}(v) = \\begin{cases}\n&0 \\qquad \\text{if } v \\in V_k \\\\\n&\\underset{w \\in V_k}{\\text{min}} d(v, w) \\qquad \\text{if } v \\in V_u\n\\end{cases},     (9)\n\\end{aligned}$\nwhere d(v,w) represents the shortest path distance using Breadth First Search (BFS) traversal [50]. Conversely, fu2k(v) quantifies the positional confidence of a known node by counting its direct connections to unknown nodes:\n$\\begin{aligned}\n&\\qquad f_{u2k}(v) = \\begin{cases}\n&0 \\qquad \\text{if } V_u \\\\\n&|\\{w \\in N(v): w \\in V_u\\}| \\qquad\n\\end{cases},          (10)\n\\end{aligned}$\nwhere N(v) denotes the neighborhood of node v. Using distance for unknown nodes ensures information propagates via the shortest path, reducing attenuation and noise. Using neighbor count for known nodes reflects their importance and influence in the network; nodes with more neighbors typically have higher connectivity and centrality."}, {"title": "2) Confidence Weight Matrix:", "content": "In the context of graphs with missing attributes, accurately capturing the complex relationships between nodes is crucial. Our strategy, focused on balancing local and global information, results in dynamic latent embeddings. We introduce a weighting strategy to achieve this balance, factoring in both known and unknown attributes from neighboring nodes. To modulate the influence of node relationships, we use a distance attenuation factor \u03b1 \u2208 (0,1). This factor helps in differentiating the confidence levels for unknown and known nodes. The weighting for unknown nodes Vu decreases with increasing distance from known nodes Vk. In contrast, the weighting for known nodes increases with the number of con- nected unknown nodes. This dual approach balances distant and local influences, integrating both global and local node characteristics. The confidence weight matrix W\u2208RNXD is defined as:\n$\\begin{aligned}\n&\\qquad W = F_{k2u} + (1 - F_{u2k}),         (11)\n\\end{aligned}$\nwhere Fk2u \u2208 RNXD and Fu2k \u2208 RNXD expand fk2u and fu2k."}, {"title": "3) Embedding Space Weighting:", "content": "To elucidate the associ- ations among various feature dimensions and potential node relationships, we examine the latent feature Z, which is of dimension D. We compute a correlation matrix C\u2208RD\u00d7D:\n$\\begin{aligned}\n&\\qquad C_{ij} = \\begin{cases}\n&\\frac{\\sum_{v=1}^{V}(Z_{iv}-\\bar{Z_i}) (Z_{jv}-\\bar{Z_j})}{\\sqrt{\\sum_{v=1}^{V}(Z_{iv}-\\bar{Z_i})^2 \\sum_{v=1}^{V}(Z_{jv}-\\bar{Z_j})^2}} \\qquad \\text{if } i \\neq j \\\\\n&1 \\qquad \\text{if } i = j        (12)\n\\end{cases},\n\\end{aligned}$\nwhere Zi and Zj are the means of Zi and Zj. The final step combines the confidence weight matrix W with the correlation matrix C to produce B \u2208 RN\u00d7D.\n$\\begin{aligned}\n&\\qquad B = W \\odot (Z - \\bar{Z}) \\times C,    (13)\n\\end{aligned}$\nwhere \u2299 denotes element-wise multiplication. The dynamic embeddings are then merged with the original embeddings to form a comprehensive representation:\n$\\begin{aligned}\n&\\qquad E = Z + \\epsilon B,     (14)\n\\end{aligned}$\nwhere \u03f5 is a small constant. ESPC reduces the risk of in- formation propagation errors by dynamically adjusting confi- dence levels in node relationships for different missing types, ensuring a precise and representative node influence in the embedding space. It is important to emphasize that E is used solely for calculating loss and is not updated in the original embeddings E within the GAE architecture."}, {"title": "C. NHS and NLSC", "content": "In the embedding space, assuming that similar attributes automatically imply proximity can introduce inaccuracies, par- ticularly in the presence of incomplete or corrupted data [63]\u2013 [65]. Ensuring the precise representation of node attributes and their graph connections is essential for effective learning. Based on the latent variables E obtained from ESPC, we introduce two key strategies: NHS evaluates the consistency among adjacent nodes, while NLSC corrects the misleading similarities among unconnected nodes. By leveraging the graph's topological relationships as prior knowledge, NHS and NLSC enhance the accuracy and stability of node embeddings, ensuring that structural integrity is maintained even in the presence of missing or corrupted data."}, {"title": "1) Node Homogeneity Score:", "content": "Node homogeneity is evalu- ated based on the proximity of their embeddings. Nodes with close embeddings often share similar attributes or neighboring nodes, indicating homogeneity. We use cosine similarity to measure structural similarity between nodes. The NHS, con- sidering both embedding and structure spaces, is defined as:\n$\\begin{aligned}\n&\\qquad NHS = \\sum_{(i,j)\\in E} P(v_i \\leftrightarrow v_j|E) = \\sum_{(i,j)\\in E} \\sigma(sim(E_i, E_j)),   (15)\n\\end{aligned}$\nwhere sim(Ei, Ej) = Ei\u00b7Ej , (i,j) \u2208 E indicates an edge between nodes i and j."}, {"title": "2) Non-Linkage Similarity Calibration (NLSC):", "content": "During embedding learning, the spatial distribution of original at- tributes may change, leading to non-linked nodes in the graph appearing similar in the embedding space without actual relationships. NLSC corrects these misleading similarities to ensure accurate representations in embeddings. The similarity between non-linked nodes is calculated as\n$\\begin{aligned}\n\\sum_{(i,j) \\notin E} \\sigma(sim(E_i, E_j)). NLSC activates a calibration for node\n\\end{aligned}$\npairs without a connection in the graph if their embedding similarity exceeds a threshold \u03c4 (set to 0.2 in this paper):\n$\\begin{aligned}\n&\\qquad NLSC = P(v_i \\leftrightarrow v_j|E) = \\begin{cases}\n&1 \\qquad \\text{if } i = j \\\\\n&0 \\qquad \\text{if } sim(E_i, E_j) < \\tau \\\\\n&log(1+e^{-sim(E_i, E_j)}) \\qquad \\text{if } sim(E_i, E_j) \\ge \\tau        (16)\n\\end{cases}\n\\end{aligned}$\nWhile NHS ensures that the attribute similarities are faith- fully represented among connected nodes, NLSC safeguards against false similarities among unconnected nodes. These mechanisms work in tandem to ensure that the embedding space accurately reflects both the attribute and structural properties of the graph, leading to more robust and reliable graph analysis."}, {"title": "D. Multi-task Optimization Learning", "content": "1) Reconstruction Loss: The refined attributes X generated by TAAP along with the symmetrically normalized adjacency matrix \u00c2, are passed as input to GAE for the computation of the latent embeddings E. Simultaneously, to ensure authentic- ity of the reconstruction, the reconstruction loss is calculated as the Mean Squared Error (MSE) between the known true attributes Xk and the known refined attributes Xk:\n$\\begin{aligned}\n&\\qquad L_{TAAP} = \\frac{1}{|k|} \\sum_{i=1}^{|k|} (X_i - (\\hat{X}_k)_i )^2 .         (17)\n\\end{aligned}$"}, {"title": "2) NHS Loss:", "content": "NHS effectively measures neighborhood con- sistency, making embedding learning dynamic and adaptive to neighborhood homogeneity. High NHS values suggest signifi- cant neighborhood homogeneity, while low values indicate the need for more attention in neighborhood information aggrega- tion for embedding updates. The NHS-based loss function is given by:\n$\\begin{aligned}\n&\\qquad L_{NHS} = \\sum_{(i,j)\\in E} log(1 + e^{-sim(E_i, E_j)}).              (18)\n\\end{aligned}$"}, {"title": "3) NLSC Loss:", "content": "NLSC effectively distinguishes among nodes that are similar in attributes but not actually related, enhancing the model's robustness against noisy and incomplete data. This calibration leads to improved generalization in downstream tasks. The loss function based on NLSC is defined as:\n$\\begin{aligned}\n&\\qquad L_{NLSC} = \\begin{cases}\n&0 \\qquad \\text{if } sim(E_i, E_j) < \\tau \\\\\n&\\sum_{(i,j)} (log(1 + e^{-sim(E_i, E_j)}) \\qquad \\text{if } sim(E_i, E_j) \\ge \\tau (19)\n\\end{cases}\n\\end{aligned}$\nOur TDAR framework integrates the loss functions of TAAP, NHS, and NLSC into Equation (6). The total loss"}, {"title": "E. Complexity Analysis", "content": "Analyzing the computational complexity of TDAR reveals its scalability and operational efficiency by evaluating the roles of nodes (N), edges (M), attribute features (F), and latent embedding dimensions (D). The TAAP shows a complexity of O(MF), indicating direct proportionality to the number of edges and features. The GAE adds complexity of O((F+D)(M+ND)), incorporating both feature and dimensionality effects alongside node and edge interactions. Computing the functions fk2u(v) and fu2k(v), which involve traversing nodes and their connections, yields O(N + M). For the ESPC, processing the covariance matrix C and calculating node attributes result in O(D2). The com- plexity of forming matrices W and B is O(ND) each. For the NHS, the average computational complexity is O(N log N), necessitating analysis of each node and its neighbors. The NLSC potentially escalates to O(N2), due to exhaustive non-link evaluations across nodes. Summarizing, TDAR's overall computational load inte- grates these components into a comprehensive complexity of O ((M + N)FD + N2 + ND\u00b2 + N log N). This effectively simplifies to O(N2), situating TDAR as a competitive option relative to other sophisticated methodologies, as evidenced by existing research [29], [34]."}, {"title": "V. EXPERIMENTATION AND ANALYSIS", "content": "This section addresses the following research questions: RQ1: How does TDAR perform on downstream tasks such as feature reconstruction, classification, and clustering? RQ2: What are the roles of each module in TDAR, as revealed by ablation studies? RQ3: How does TDAR demonstrate robustness under vary- ing input missing rates? RQ4: What is the homogeneity capability of TDAR? RQ5: How do changes in hyperparameters affect TDAR's performance? RQ6: How visually effective is the quality of the learned graph representation learned by TDAR? RQ7: How does TDAR's performance manifest in a specific case?"}, {"title": "F. Robustness under Varying Missing Rates (RQ3)", "content": "In Figure 4", "methods": "SVGA, ITR, MATE, and TDAR across four datasets at varying missing data rates (0.2-0.8). The results clearly demonstrate that TDAR consistently achieves the best per- formance under different missing rates on the Recall@k and nDGC@k metrics. Further"}]}