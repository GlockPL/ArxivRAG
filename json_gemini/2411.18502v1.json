{"title": "Isometry pursuit", "authors": ["Samson Koelle", "Marina Meila"], "abstract": "Isometry pursuit is a convex algorithm for identifying orthonormal column-submatrices of wide matrices. It consists of a novel normalization method followed by multitask basis pursuit. Applied to Jacobians of putative coordinate functions, it helps identity isometric embeddings from within interpretable dictionaries. We provide theoretical and experimental results justifying this method. For problems involving coordinate selection and diversification, it offers a synergistic alternative to greedy and brute force search.", "sections": [{"title": "Introduction", "content": "Many real-world problems may be abstracted as selecting a subset of the columns of a matrix representing stochastic observations or analytically exact data. This paper focuses on a simple such problem that appears in interpretable learning and diversification. Given a rank D matrix $X \\in R^{D \\times P}$ with $P > D$, select a square submatrix $X_{\\cdot S}$ where subset $S \\subset [P]$ satisfies $|S| = D$ that is as orthonormal as possible.\nThis problem arises in interpretable learning specifically because while the coordinate functions of a given feature space may have no intrinsic meaning, it is sometimes possible to generate a dictionary of interpretable features which may be considered as potential parametrizing coordinates. When this is the case, selection of candidate interpretable features as coordinates can take the above form. While implementations vary across data and algorithmic domains, identification of such coordinates generally aids mechanistic understanding, generative control, and statistical efficiency.\nThis paper shows that an adapted version of the algorithm in Koelle et al. [1] leads to a convex procedure that can improve upon greedy approaches such as those in Cai and Wang [2], Chen and Meila [3], Kohli et al. [4], Jones et al. [5] for finding isometries. The insight leading to isometry pursuit is that multitask basis pursuit applied to an appropriately normalized $X$ selects orthonormal submatrices. Given vectors in $R^{D}$, the normalization log-symmetrizes length and favors those closer to unit length, while basis pursuit favors those which are orthogonal. Our results formalize this intuition within a limited setting, and show the usefulness of isometry pursuit as a trimming procedure prior to brute force search for diversification and interpretable coordinate selection. We also introduce a novel ground truth objective function against which we measure the success of our algorithm, and discuss the reasonableness of the trimming procedure."}, {"title": "Background", "content": "Our algorithm is motivated by spectral and convex analysis."}, {"title": "Problem", "content": "Our goal is, given a matrix $X \\in R^{D \\times P}$, to select a subset $S \\subset [P]$ with $|S| = D$ such that $X_{\\cdot S}$ is as orthonormal as possible in a computationally efficient way. To this end, we define a ground truth loss function that measures orthonormalness, and then introduce a surrogate loss function that convexifies the problem so that it may be efficiently solved."}, {"title": "Interpretability and isometry", "content": "Our motivating example is the selection of data representations from within sets of putative coor-dinates: the columns of a provided wide matrix. Compared with Sparse PCA [6, 7, 8], we seek a low-dimensional representation from the set of these column vectors rather than their span.\nThis method applies to interpretability, for which parsimony is at a premium. Interpretability arises through comparison of data with what is known to be important in the domain of the problem. This knowledge often takes the form of a functional dictionary. Evaluation of independence of dictionary features arises in numerous scenarios [9, 10, 11]. The requirement that dictionary features be full rank has been called functional independence [10] or feature decomposability [12], with connection between dictionary rank and independence via the implicit function theorem. Besides independence, the metric properties of such dictionary elements are of natural interest. This is formalized through the notion of differential.\nDefinition 1 The differential of a smooth map $\\phi : M \\rightarrow N$ between D dimensional manifolds $M \\subseteq R^{B}$ and $N \\subseteq R^{P}$ is a map in tangent bases $x_{1} ... x_{D}$ of $T_{\\xi}M$ and $y_{1} ... y_{D}$ of $T_{\\phi(\\xi)}N$ consisting of entries\n$\\qquad D\\phi(\\xi) = \\begin{pmatrix}\n\\frac{\\partial \\phi_{1}}{\\partial x_{1}}(\\xi) & ... & \\frac{\\partial \\phi_{1}}{\\partial x_{D}}(\\xi) \\\\\n: & : & : \\\\\n\\frac{\\partial \\phi_{D}}{\\partial x_{1}}(\\xi) & ... & \\frac{\\partial \\phi_{D}}{\\partial x_{D}}(\\xi)\n\\end{pmatrix}\\qquad(1)$\nIt is not always necessary to explicitly estimate tangent spaces when applying this definition. The most commonly encountered manifolds are vector spaces for which the tangent spaces are trivial. This is the case for full-rank tabular data, for which isometry has a natural interpretation as a type of diversification, and often for the latent spaces of deep learning models. In this case, $B = D$.\nDefinition 2 A map $\\phi$ between D dimensional submanifolds with inherited Euclidean metric $M \\subset R^{B}$ and $N \\subset R^{P} \\phi$ is an isometry at a point $\\xi \\in M$ if\n$\\qquad D\\phi(\\xi)^{T} D\\phi(\\xi) = I_{D}.\\qquad(2)$\nThat is, $\\phi$ is an isometry at $\\xi$ if $D\\phi(\\xi)$ is orthonormal.\nThe applications of pointwise isometry are themselves manifold. Pointwise isometric embeddings faithfully preserve high-dimensional geometry. For example, Local Tangent Space Alignment [13], Multidimensional Scaling [14] and Isomap [15] non-parametrically estimate embeddings that are as isometric as possible. Another approach stitches together pointwise isometries selected from a dictionary to form global embeddings [4]. The method is particularly relevant since it constructs such isometries through greedy search, with putative dictionary features added one at a time.\nThat $D\\phi$ is orthonormal has several equivalent formulations. The one motivating our ground truth loss function comes from spectral analysis.\nProposition 1 The singular values $\\sigma_{1} ... \\sigma_{D}$ are equal to 1 if and only if $U \\in R^{D \\times D}$ is orthonormal.\nOn the other hand, the formulation that motivates our convex approach is that orthonormal matrices consist of D coordinate features whose gradients are orthogonal and of unit length.\nProposition 2 The component vectors $u_{1} ... u_{D} \\in R^{B}$ form a orthonormal matrix if and only if, for all $d_{1}, d_{2} \\in [D]$,\n$\\qquad (u_{d_{1}}, u_{d_{2}}) = \\begin{cases}\n1 & d_{1} = d_{2} \\\\\n0 & d_{1} \\neq d_{2}\n\\end{cases}$"}, {"title": "Subset selection", "content": "Given a matrix $X \\in R^{D \\times P}$, we compare algorithmic paradigms for solving problems of the form\n$\\qquad \\underset{S \\in \\binom{[P]}{D}}{arg \\min} l(X_{\\cdot S}) \\qquad(3)$\nwhere $\\binom{[P]}{D} = {A \\subset [P] : |A| = D}$. Brute force algorithms consider all possible solutions. These algorithms are conceptually simple, but have the often prohibitive time complexity $O(C_{i}\\binom{P}{D})$ where $C_{i}$ is the cost of evaluating $l$. Greedy algorithms consist of iteratively adding one element at a time to $S$. This algorithms have time complexity $O(C_{p}D)$ and so are computationally more efficient than brute force algorithms, but can get stuck in local minima. Formal definitions are given in Section 6.1.\nSometimes, it is possible to introduce an objective which convexifies problems of the above form. Solutions\n$\\qquad \\underset{\\beta}{arg \\min} f(\\beta) : Y = X \\beta \\qquad(4)$\nto the overcomplete regression problem $Y = X\\beta$ are a classic example [16]. When $f(\\beta) = ||\\beta||_{0}$, this problem is non-convex, and is thus suitable for greedy or brute algorithms, but when $f(\\beta) = ||\\beta||_{1}$, the problem is convex, and may be solved efficiently via interior-point methods. When the equality constraint is relaxed, Lagrangian duality may be used to reformulate as a so-called Lasso problem, which leads to an even richer set of optimization algorithms.\nThe form of basis pursuit that we apply is inspired by the group basis pursuit approach in Koelle et al. [10]. In group basis pursuit (which we call multitask basis pursuit when grouping is dependent only on the structure of matrix-valued response variable $y$) the objective function is $f(\\beta) = ||\\beta||_{1,2} := \\sum_{p=1}^{P} ||\\beta_{p\\cdot}||_{2}$ [17, 18, 19]. This objective creates joint sparsity across entire rows $\\beta_{p\\cdot}$, and was used in Koelle et al. [10] to select between sets of interpretable features."}, {"title": "Method", "content": "We adapt the group lasso paradigm used to select independent dictionary elements in Koelle et al. [10, 1] to select pointwise isometries from a dictionary. We first define a ground truth objective computable via brute and greedy algorithms that is uniquely minimized by orthonormal matrices. We then define the combination of normalization and multitask basis pursuit that approximates this ground truth loss function. We finally give a brute post-processing method for ensuring that the solution is D sparse."}, {"title": "Ground truth", "content": "We'd like a ground truth objective to be minimized uniquely by orthonormal matrices, invariant under rotation, and depend on all changes in the matrix. Deformation [4] and nuclear norm [20] use only a subset of the differential's information and are not uniquely minimized at unitarity, respectively. We therefore introduce an alternative ground truth objective that satisfies the above desiderata and has convenient connections to isometry pursuit.\nThis objective is\n$\\qquad l_{c}: R^{D \\times P} \\rightarrow R^{+}\\qquad(5)$\n$\\qquad X \\mapsto \\sum_{d=1}^{D} g(\\sigma_{d}(X), c) \\qquad(6)$\nwhere $\\sigma_{d}(X)$ is the $d$-th singular value of $X$ and\n$\\qquad g: R^{+} \\times R^{+} \\rightarrow R^{+}\\qquad(7)$\n$\\qquad t, c \\mapsto \\frac{e^{t} + e^{-t}}{2e^{c}} \\qquad(8)$\nUsing Proposition 1, we can check that $l_{c}$ is uniquely maximized by orthonormal matrices. Moreover, $g$ is convex, and $l_{c}(X^{-1}) = l_{c}(X)$ when $X$ is invertible."}, {"title": "Normalization", "content": "Since basis pursuit methods tend to select longer vectors, selection of orthonormal submatrices requires normalization such that both long and short candidate basis vectors are penalized in the subsequent regression. We introduce the following definition.\nDefinition 3 (Symmetric normalization) A function $q : R^{D} \\rightarrow R^{+}$ is a symmetric normalization if\n$\\qquad \\underset{v \\in R^{D}}{arg \\max} q(v) = {v : ||v||_{2} = 1}\\qquad(10)$\n$\\qquad q(v) = q(\\frac{v}{||v||_{2}})\\qquad(11)$\n$\\qquad q(v_{1}) = q(v_{2}) \\forall v_{1}, v_{2} \\in R^{D} : ||v_{1}||_{2} = ||v_{2}||_{2}.\\qquad(12)$\nWe use such functions to normalize vector length in such a way that vectors of length 1 prior to normalization have longest length after normalization and vectors are shrunk proportionately to their deviation from 1. That is, we normalize vectors by\n$\\qquad n: R^{D} \\rightarrow R^{D}\\qquad(13)$\n$\\qquad v \\rightarrow q(v)v\\qquad(14)$\nand matrices by\n$\\qquad w: R^{D \\times P} \\rightarrow R^{D}\\qquad(15)$\n$\\qquad X_{\\cdot p} \\rightarrow n(X_{\\cdot p}) \\forall p \\in [P].\\qquad(16)$\nIn particular, given $c > 0$, we choose $q$ as follows.\n$\\qquad q_{c}: R^{D} \\rightarrow R^{+}\\qquad(17)$\n$\\qquad v \\mapsto \\frac{e^{||v||_{2}} + e^{-||v||_{2}}}{2e^{c}}\\qquad(18)$\nBesides satisfying the conditions in Definition 3, this normalization has some additional nice properties. First, $q$ is convex. Second, it grows asymptotically log-linearly. Third, while $exp(-|logt|) = exp(-max(t, 1/t))$ is a seemingly natural choice for normalization, it is non smooth, and the LogSumExp [20] replacement of $max(t, 1/t)$ with $log(exp(t) + exp(1/t))$ simplifies to 18 upon exponentiation. Finally, the parameter $c$ grants control over the width of the basin, which may be useful for avoiding numerical issues arising close to 0 and $\\infty$."}, {"title": "Isometry pursuit", "content": "Isometry pursuit is the application of multitask basis pursuit to the normalized design matrix $w(X, c)$ to identify submatrices of $X$ that are as orthonormal as possible. Define the multitask basis pursuit penalty\n$\\qquad || \\cdot ||_{1,2}: R^{P \\times D} \\rightarrow R^{+}\\qquad(19)$\n$\\qquad \\beta \\mapsto \\sum_{p=1}^{P} ||\\beta_{p\\cdot}||_{2}.\\qquad(20)$\nGiven a matrix $Y \\in R^{D \\times D}$, the multitask basis pursuit solution is\n$\\qquad \\hat{\\beta}_{MBP}(X, Y) := \\underset{\\beta \\in R^{P \\times D}}{arg \\min} ||\\beta||_{1,2}: Y = X \\beta.\\qquad(21)$"}, {"title": "Theory", "content": "The intuition behind our application of multitask basis pursuit is that submatrices consisting of vectors which are closer to 1 in length and more orthogonal will have smaller loss. A key initial theoretical assertion is that ISOMETRYPURSUIT is invariant to choice of basis for X.\nProposition 3 Let $U \\in R^{D \\times D}$ be orthonormal. Then $S(\\hat{\\beta}(UX)) = S(\\hat{\\beta}(X))$.\nA proof is given in Section 6.2.1. This has as an immediate corollary that we may replace $I_{D}$ in the constraint by any orthonormal D \u00d7 D matrix.\nWe also claim that the conditions of the consequent of Proposition 2 are satisfied by minimizers of the multitask basis pursuit objective applied to suitably normalized matrices in the special case where a rank D orthonormal submatrix exists and |S| = D.\nProposition 4 Let $w_{c}$ be a normalization satisfying the conditions in Definition 3. Then $\\underset{X_{\\cdot S} \\in R^{D \\times D}}{arg \\min} \\beta_{c}(X_{\\cdot S})$ is orthonormal and, given $X$ is orthonormal, $||\\beta||_{1,2}: I_{D} = w(X, c)\\beta = D$.\nWhile this Proposition falls short of showing that an orthonormal submatrix will be selected should one be present, it provides intuition justifying the preferential efficacy of ISOMETRYPURSUIT on real data. A proof is given in Section 6.2.2."}, {"title": "Two-stage isometry pursuit", "content": "Since we cannot ensure either that |S| = D or that a orthonormal submatrix $X_{\\cdot S}$ exists, we first use the convex problem to prune and then apply brute search upon the substantially reduced feature set."}, {"title": "Experiments", "content": "Say you are hosting an elegant dinner party, and wish to select a balanced set of wines for drinking and flowers for decoration. We demonstrate TWOSTAGEISOMETRYPURSUIT and GREEDYSEARCH on the Iris and Wine datasets [22, 23, 24]. This has an intuitive interpretation as selecting diverse elements that reflects the peculiar structure of the diversification problem. Features like petal width are rows in X. They are features on the basis of which we may select among the flowers those which are most distinct from another. Thus, in diversification, P = n.\nWe also analyze the Ethanol dataset from Chmiela et al. [25], Koelle et al. [10], but rather than selecting between bourbon and scotch we evaluate a dictionary of interpretable features - bond torsions - for their ability to parameterize the molecular configuration space. In this interpretability use case, columns denote gradients of informative features. We compute Jacoban matrices of putative parametrization functions and project them onto estimated tangent spaces (see Koelle et al. [10] for preprocessing details). Rather than selecting between data points, we are selecting between functions which parameterize the data.\nFor basis pursuit, we use the SCS interior point solver [26] from CVXPY [27, 28], which is able to push sparse values arbitrarily close to 0 [29]. Statistical replicas for Wine and Iris are created by resampling across [P]. Due to differences in scales between rows, these are first standardized. For the Wine dataset, even BRUTESEARCH on SIP is prohibitive in D = 13, and so we truncate our inputs to D = 6. For Ethanol, replicas are created by sampling from data points and their corresponding tangent spaces are estimated in B = 252."}, {"title": "Discussion", "content": "We have shown that multitask basis pursuit can help select isometric submatrices from appropriately normalized wide matrices. This approach - isometry pursuit - is a convex alternative to greedy methods for selection of orthonormalized features from within a dictionary. Isometry pursuit can be applied to diversification and geometrically-faithful coordinate estimation. Our experiments exemplify these applications, but more can be done. One potential application is diversification in recommendation systems [30, 31, 32] and other retrieval systems such as in RAG [33, 34, 35, 36, 37]. Another is decomposing interpretable yet overcomplete dictionaries in transformer residual streams, with each token considered as generating its own tangent space [12, 38].\nCompared with the greedy algorithms used in such areas [39, 40, 41, 42, 43, 44, 45, 46, 47, 34], the convex reformulation may add speed and convergence to a global minima. The comparison of greedy [48, 49, 50, 51] and convex [16, 52, 53] basis pursuit formulations has a rich history, and theoretical understanding of the behavior of this approximation is evolving. Diversification problems have been cited as NP-hard, and isometry pursuit can be considered analogous to them in the sense of basis pursuit and the lasso against best subset selection, with the caveat that best subset selection of the basis pursuit loss minimizer isn't totally equivalent to isometry pursuit even though they share the same unique optimum. Characterization of solutions resulting from removal of the restriction P = D on the conditions of Proposition 4 may help justify the second selection step. That the solution of a lasso problem can sometimes be a non-singleton set is well-known [54, 55, 56, 57, 58, 59, 60, 61, 62, 63, 64]. Perhaps surprisingly, it appears empirically that for isometry pursuit that this can occur even when the design matrix is not in general position.\nThis convex set appears to contain the sparsest solution. The convergence of SCS algorithm to the 2-norm minimizing solution due to the Lagrangian dual constraint penalty and the convexity of the loss minimizer preimage suggest that a related two stage procedure always succeeds in identifying the brute ||||1,2 minimizer. Related conditions have been discussed in Donoho [65], Mishkin and Pilanci [61], and we examine this topic experimentally in Section 6.4.\nAlgorithmic variants include the multitask lasso [66] extension of our estimator, as well as characteri-zation of D function selection within RB. Tangent-space specific variants have been studied in more detail in Koelle et al. [10, 1] with additional grouping across datapoints, and a corresponding variant of the isometry theorem that missed non-uniqueness was claimed in Koelle [67]. Comparison of our loss with curvature - whose presence prohibits D element isometry - could prove fertile, as could comparison with the so-called restricted isometry property used to show guaranteed recovery at fast convergence rates in supervised learning [68, 66]."}, {"title": "Algorithms", "content": "We give definitions of the brute and greedy algorithms for the combinatorial problem studied in this paper. The brute force algorithm is computationally intractable for all but the smallest problems, but always finds the global minima.\nBRUTESEARCH(Matrix $X \\in R^{D \\times P}$, objective $f$)\n1: for each combination $S \\subset {1, 2, ..., P}$ with $|S| = D$ do\n2: Evaluate $f(X_{\\cdot S})$\n3: end for\n4: Output the combination $S^{*}$ that minimizes $f(X_{\\cdot S})$\nGreedy algorithms are computationally expedient but can get stuck in local optima [69, 70], even with randomized restarts [71].\nGREEDYSEARCH(Matrix $X \\in R^{D \\times P}$, objective $f$, selected set $S = \\emptyset$, current size $d = 0$)\n1: if $d = D$ then\n2: Return S\n3: else\n4: Initialize $S_{best} = S$\n5: Initialize $f_{best} = \\infty$\n6: for each $p \\in {1, 2, ..., P} \\setminus S$ do\n7: Evaluate $f(X_{\\cdot(S \\cup {p})})$\n8: if $f(X_{\\cdot(S \\cup {p})}) < f_{best}$ then\n9: Update $S_{best} = S \\cup {p}$\n10: Update $f_{best} = f(X_{\\cdot(S \\cup {p})})$\n11: end if\n12: end for\n13: Return GREEDYSEARCH(X, f, Sbest, d + 1)\n14: end if"}, {"title": "Proofs", "content": "In this proof we first show that the penalty $||\\beta||_{1,2}$ is unchanged by unitary transformation of $\\beta$.\nProposition 5 Let $U \\in R^{D \\times D}$ be unitary. Then $||\\beta||_{1,2} = ||\\beta U||$.\nProof:\n$\\qquad ||\\beta U||_{1,2} = \\sum_{p=1}^{P} ||\\beta_{p\\cdot}U||\\qquad(25)$\n$\\qquad = \\sum_{p=1}^{P} ||\\beta_{p\\cdot}||\\qquad(26)$\n$\\qquad = ||\\beta||_{1,2}\\qquad(27)$\nWe then show that this implies that the resultant loss is unchanged by unitary transformation of X.\nProposition 6 Let $U \\in R^{D \\times D}$ be unitary. Then $\\hat{\\beta}(UX) = \\hat{\\beta}(X)U$.\nProof:\n$\\qquad \\hat{\\beta}(UX) = arg \\underset{\\beta \\in R^{P \\times D}}{min} ||\\beta||_{1,2}: I_{D} = UX\\beta\\qquad(28)$\n$\\qquad = arg \\underset{\\beta \\in R^{P \\times D}}{min} ||\\beta||_{1,2} : U^{-1}U = U^{-1}UX\\beta U\\qquad(29)$\n$\\qquad = arg \\underset{\\beta \\in R^{P \\times D}}{min} ||\\beta||_{1,2}: I_{D} = X\\beta U\\qquad(30)$\n$\\qquad = arg \\underset{\\beta \\in R^{P \\times D}}{min} ||\\beta U||_{1,2} : I_{D} = X\\beta U\\qquad(31)$\n$\\qquad = arg \\underset{\\beta \\in R^{P \\times D}}{min} ||\\beta||_{1,2} : I_{D} = X\\beta.\\qquad(32)"}, {"title": "Proof of Proposition 4", "content": "Proposition 7 Let $w_{c}$ be a normalization satisfying the conditions in Definition 3. Then $\\underset{X_{\\cdot S} \\in R^{D \\times D}}{arg \\min} \\beta_{c}(X_{\\cdot S})$ is orthonormal and, given $X$ is orthonormal, $||\\beta||_{1,2}: I_{D} = w(X, c)\\beta = D$.\nProof: The value of D is clearly obtained by $\\beta$ orthonormal, since by Proposition 3, for X orthogonal, without loss of generality\n$\\qquad B_{dd'} = \\begin{cases}1 & d = d' \\in {1...D} \\\\\n0 & otherwise\\end{cases}\\qquad(33)$\nThus, we need to show that this is a lower bound on the obtained loss.\nFrom the conditions in Definition 3, normalized matrices will consist of vectors of maximum length (i.e. 1) if and only if the original matrix also consists of vectors of length 1. Such vectors will clearly result in lower basis pursuit loss, since longer vectors in X require smaller corresponding covectors in $\\beta$ to equal the same result.\nTherefore, it remains to show that X consisting of orthogonal vectors of length 1 have lower loss compared with X consisting of non-orthogonal vectors. Invertible matrices $X_{\\cdot S}$ admit QR decompo-sitions $X_{\\cdot S} = QR$ where $Q$ and $R$ are orthonormal and upper-triangular matrices, respectively [72].\nDenoting Q to be composed of basis vectors $[e_{1} ... e_{D}]$, the matrix R has form\n$\\qquad R = \\begin{bmatrix}\n(e_{1}, X_{\\cdot S_{1}}) & (e_{1}, X_{\\cdot S_{2}}) & ... & (e_{1}, X_{\\cdot S_{D}}) \\\\\n0 & (e_{2}, X_{\\cdot S_{2}}) & ... & (e_{2}, X_{\\cdot S_{D}}) \\\\\n0 & 0 & ... & ... \\\\\n0 & 0 & ... & (e_{D}, X_{\\cdot S_{D}})\n\\end{bmatrix}\\qquad(34)$\nThus, $|R_{dd}| < ||X_{\\cdot S_{d}}||_{2}$ for all $d \\in [D]$, with equality obtained only by orthonormal matrices. On the other hand, by Proposition 3, $l_{c}(X) = l_{c}(R)$ and so $||\\beta||_{1,2} = ||R^{-1}||_{1,2}$. Since $R$ is upper triangular it has diagonal elements $B_{dd} = R_{dd}$ and so $||\\beta_{d\\cdot}|| \\geq ||X_{\\cdot S_{d}}||^{-1} = 1$. That is, the penalty accrued by a particular covector in $\\beta$ is bounded from below by 1 - the inverse of the length of the corresponding vector in $X_{\\cdot S}$ - with equality occurring only when $X_{\\cdot S}$ is orthonormal."}, {"title": "Support cardinalities", "content": "Figure 3 plots the distribution of |\u015cIP| from Table 1 in order to contextualize the reported means. While typically SIP| << P, there are cases for Ethanol where this is not the case that drive up the means."}, {"title": "Proposition 4 deep dive", "content": "As mentioned in Section 5, the conditions under which the restriction P = D in Proposition 4 may be relaxed are of theoretical and practical interest. The results in Section 4 show that there are circumstances in which the GREEDYSEARCH performs better than TWOSTAGEISOMETRYPURSUIT, so clearly TWOSTAGEISOMETRYPURSUIT does not always achieve a global optimum. Figure 4 gives results on the line of inquiry about why this is the case based on the reasoning presented in Section 5. In these results a two-stage algorithm achieves the global optimum of a slightly different brute problem, namely brute optimization of the multitask basis pursuit penalty || . ||1,2. That is, brute search on || ||1,2 gives the same result as the two stage algorithm with brute search on || ||1,2 subsequent to isometry pursuit. This suggests that failure to select the global optimum by TWOSTAGEISOMETRYPURSUIT is in fact only due to the mismatch between global optimums of brute optimization of the multitask penalty and the isometry loss given certain data. Theoretical formalization, as well as investigation of what data configurations this equivalence holds for, is a logical follow-up."}, {"title": "Timing", "content": "While wall-time of algorithms is a non-theoretical quantity that depends on implementation details, it provides valuable context for practitioners. We therefore report the following runtimes on a 2021 Macbook Pro. The particularly high variance for brute force search in the second step of TWOSTAGEISOMETRYPURSUIT is likely due to the large cardinalities reported in Figure 3."}]}