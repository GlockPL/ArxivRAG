{"title": "PitVQA++: Vector Matrix-Low-Rank Adaptation for Open-Ended Visual Question Answering in Pituitary Surgery", "authors": ["Runlong He", "Danyal Z. Khan", "Evangelos B. Mazomenos", "Hani J. Marcus", "Danail Stoyanov", "Matthew J. Clarkson", "Mobarakol Islam"], "abstract": "Vision-Language Models (VLMs) in visual question answering (VQA) offer a unique opportunity to enhance intra-operative decision-making, promote intuitive interactions, and significantly advancing surgical education. However, the development of VLMs for surgical VQA is challenging due to limited datasets and the risk of overfitting and catastrophic forgetting during full fine-tuning of pretrained weights. While parameter-efficient techniques like Low-Rank Adaptation (LoRA) and Matrix of Rank Adaptation (MORA) address adaptation challenges, their uniform parameter distribution overlooks the feature hierarchy in deep networks, where earlier layers, that learn general features, require more parameters than later ones. This work introduces PitVQA++ with an open-ended PitVQA dataset and vector matrix-low-rank adaptation (Vector-MoLoRA), an innovative VLM fine-tuning approach for adapting GPT-2 to pituitary surgery. Open-Ended PitVQA comprises around 101,803 frames from 25 procedural videos with 745,972 question-answer sentence pairs, covering key surgical elements such as phase and step recognition, context understanding, tool detection, localization, and interactions recognition. Vector-MoLoRA incorporates the principles of LORA and MoRA to develop a matrix-low-rank adaptation strategy that employs vector ranking to allocate more parameters to earlier layers, gradually reducing them in the later layers. Our approach, validated on the Open-Ended PitVQA and EndoVis18-VQA datasets, effectively mitigates catastrophic forgetting while significantly enhancing performance over recent baselines. Furthermore, our risk-coverage analysis highlights its enhanced reliability and trustworthiness in handling uncertain predictions. Our source code and dataset is available at https://github.com/HRL-Mike/PitVQA-Plus.", "sections": [{"title": "I. INTRODUCTION", "content": "The integration of Vision-Language Models (VLMs) into surgical workflows has the potential to revolutionize intra-operative decision-making, fostering intuitive surgeon-AI in-teractions and transforming surgical education. In the context of visual question answering (VQA), VLMs can provide real-time insights into surgical phases, steps, and instrument us-age, addressing the growing demand for intelligent, context-aware systems in operating rooms. For complex procedures such as endonasal pituitary surgery, which demands precise anatomical navigation and dynamic decision-making [1], VLMs could offer invaluable support by analyzing intra-operative videos and images to predict surgical outcomes and guide interventions [2], [3]. However, developing VLMs tailored for surgical applications presents unique challenges, primarily due to the scarcity of domain-specific datasets and the increased risk of overfitting during fine-tuning of pretrained models originally trained on massive datasets, which can compromise their generalization capabilities [4], [5]. Existing parameter-efficient fine-tuning techniques, such as Low-Rank Adaptation (LoRA) [6] and Matrix of Rank Adaptation (MORA) [7], offer promising solutions but fail to account for the hierarchical feature distribution inherent in deep neural networks. This shortcoming motivates the need for innovative approaches that adaptively allocate parameters to layers based on their roles in feature extraction and representation.\nBridging this gap necessitates the creation of enriched surgical VQA datasets that challenge VLMs with tasks demanding deeper reasoning and diverse contextual interpre-tations. Several surgical VQA datasets have been proposed for different surgical scenarios, including EndoVis18-VQA [8] for nephrectomy, SSG-VQA [9] for laparoscopic chole-cystectomy, and Kvasir-VQA [10] for colonoscopic proce-dures. Nevertheless, most existing surgical VQA datasets are designed for basic tasks or simple reasoning tasks, such as instrument recognition [9], instrument localization [10], and tool-tissue interactions [8]. Additionally, these datasets are primarily close-ended, focused on classification tasks with single-word answers, limiting VLM's natural language gen-erative capability and confining it to discriminative outcomes. Such constrained answering scheme, containing very limited"}, {"title": "II. RELATED WORK", "content": "Early VQA methods relied on basic networks and simple fusion mechanisms, while subsequent approaches enhanced cross-modal interaction through question-guided attention and region-based representations. Latest research focuses on a broad Vision-Language Pre-training (VLP) paradigm based on the Transformer architecture [16]. In this paradigm, approaches can be categorized into three types according to the ways of visual feature extraction: (I) Object-based meth-ods (e.g., ViLBERT [17], VL-BERT [18], and VisualBERT [19]) that utilize Faster R-CNN for ROI feature extraction; (II) Convolution-based methods like Pixel-BERT [20] that employ CNNs; and (III) Image-patch-based methods that divide images into sequences of patches for processing, such as CLIP [21], BLIP [22], and LLaVA [23]. These models, trained on large-scale datasets to align visual and textual features into a shared embedding space, demonstrate strong few-shot transfer capabilities on downstream tasks such as VQA. Recent works [4], [24] attempt to transfer general VQA models to the biomedical domain through post-training on domain datasets. These works require high computational cost and have not been considered for surgical applications.\nRecent studies mainly applied pre-trained visual and tex-tual encoders on surgical VQA tasks [2], [8], [9], [11], [25]. Specifically, the visual features of surgical frames are extracted by CNNs (e.g., VGGNet, ResNet [26]) or Vision transformer (ViT) [27], and the textual features of surgical questions are gained through stacked transformer layers [19]. Early approaches, such as VisualBERT-RM [8] and SurgicalGPT [11], relied on concatenating visual and textual representations, followed by a self-attention layer. However, this concatenation leads to computational inefficiency due to the elongated embeddings, often requiring additional MLP layers for feature projections [11]. Yuan et al. [9] used scene graph generation for surgical VQA, but its multistage training is complex, computationally expensive, and reliant on prior task predictions. Existing methods primarily focus on close-ended surgical VQA, while there is a lack of datasets and approaches suitable for open-ended surgical VQA tasks.\nWith the rising interest in foundation models, researchers begin to apply large language models (LLMs) for surgical VQA tasks. He et al. [2] employed cross-attention mecha-nism to model the correlations between visual and textual features, and a LLM to decode vision-language embeddings."}, {"title": "III. METHODOLOGY", "content": "LoRA is designed to minimize trainable parameters during fine-tuning by decom-posing weight updates into two low-rank matrices. Specifi-cally, it keeps the original pre-trained weights \\(W_o \\in \\mathbb{R}^{d \\times k}\\) frozen and introduces matrices \\(B \\in \\mathbb{R}^{d \\times r}\\) and \\(A \\in \\mathbb{R}^{r \\times k}\\) for efficient adaptation, where the rank r is significantly smaller than both dimensions d and k. This approach leverages the concept of \"intrinsic rank\" to prevent catastrophic forgetting, with the final forward computation combining the original transformation \\(W_o x\\) with the low-rank adaptation \\(BAx\\), as shown in Equation 1:\n\\[h = W_o x + \\Delta W x = W_o x + BAx \\tag{1}\\]\nwhere both the pre-trained weights \\(W_o\\) and the low-rank update \\(\\Delta W = BA\\) operate on the input \\(x \\in \\mathbb{R}^{k}\\) with their outputs being summed. This adaptation mechanism can be applied to all dense layers in transformer blocks, including query (q), key (k), value (v), and output (o) projections.\nMoRA aims to achieve matrix-rank updates while maintaining the same parameter efficiency as LoRA. Given a pre-trained weight matrix \\(W_o \\in \\mathbb{R}^{d \\times k}\\), MoRA employs a square matrix \\(M \\in \\mathbb{R}^{\\hat{k} \\times \\hat{k}}\\) for updating:\n\\[h = W_o x + \\Delta W x = W_o x + f(M g(x)) \\tag{2}\\]\nwhere \\(g()\\) and \\(f(.))\\) are non-parametric operators for compression and decompression, respectively, and M is a trainable square matrix that processes the compressed representation. Specifically, \\(g: \\mathbb{R}^k \\rightarrow \\mathbb{R}^{\\hat{k}}\\) reduces the k-dimensional input to \\(\\hat{k}\\) dimensions, \\(f : \\mathbb{R}^{\\hat{k}} \\rightarrow \\mathbb{R}^{d}\\) increases ther dimensions back to d dimensions, while \\(M: \\mathbb{R}^{\\hat{k}} \\rightarrow \\mathbb{R}^{\\hat{k}}\\) transforms within the compressed space. These two operators also have corresponding function, \\(\\overline{g} : \\mathbb{R}^{\\hat{k} \\times \\hat{k}} \\rightarrow \\mathbb{R}^{r \\times k}\\) and \\(\\overline{f}:\\mathbb{R}^{r \\times k} \\rightarrow \\mathbb{R}^{d \\times k}\\), to transform M into \\(\\Delta W\\). For any input \\(x \\in \\mathbb{R}^k\\), the following should hold:\n\\[f(Mg(x)) = \\Delta W x \\tag{3}\\]\nwhere \\(\\Delta W = \\overline{f}(\\overline{g}(M))\\). Following RoPE [30], the com-pression and decompression operators can be expressed as:\n\\[g(x) = [b^0\\ b^1\\dots\\ b^{n-1}] \\tag{4}\\]\n\\[f(x) = concat(x) \\tag{5}\\]\n\\[\\Delta W = \\begin{bmatrix} P_0 & 0 & \\dots & 0\\\\ 0 & P_1 & 0 & 0\\\\\\vdots & 0 & \\ddots & \\vdots\\\\ 0 & 0 & \\dots & P_{n-1} \\end{bmatrix} \\tag{6}\\]\nwhere \\(concat(x)\\) denotes the operation of flattening x into a vector, \\(b_i\\) and \\(P_{i}, j \\in \\{0,1,...,n - 1\\}\\), denote the corresponding values of \\(X_{jr:(j+1)r}\\) and M after rotation, respectively. b and p are defined as follow:\n\\[b = \\begin{bmatrix} R_{01,3}^2 & 0 & \\dots & 0\\\\ 0 & R_{02,3}^2 & 0\\\\\\vdots & \\vdots & & X_{jr:(j+1)r} \\\\ 0 & 0 & \\dots & R_{0n,3}^2 \\end{bmatrix} \\tag{7}\\]\n\\[P_i = M \\begin{bmatrix} R_{01,3}^2 & 0 & \\dots & 0\\\\ 0 & R_{02,3}^2 & 0\\\\\\vdots & \\vdots & & R_{0n,3}^2 \\\\ 0 & 0 & \\dots & R_{0n,3}^2 \\end{bmatrix} \\tag{8}\\]\nwhere \\(\\theta_k = 10000^{-2(k-1)/\\hat{k}}, k \\in \\{1,2,...,\\hat{k}}\\) and \\(R_{\\theta_{k}, j}\\) is a rotation matrix:\n\\[R_{\\theta_{k}, j} = \\begin{bmatrix} cos j\\theta_k & -sin j\\theta_k \\\\ sin j\\theta_k & cos j\\theta_k \\end{bmatrix} \\tag{9}\\]\nVector-LoRA [31] extends the tradi-tional LoRA approach by introducing adaptive rank alloca-tion across network layers. While maintaining the core low-rank adaptation principle, it uniquely assigns varying ranks to different layers based on their hierarchical importance in the network. The method defines rank adjustments through a rank vector \\(r_i\\), as shown in equation 10. For any adaptation"}, {"title": "B. Proposed Method", "content": "Our Open-ended PitVQA dataset consists of 25 videos of endoscopic pituitary surgery collected at the National Hospital of Neurology and Neurosurgery in London, UK. The videos are similar to the dataset used in the MICCAI PitVis challenge [32]. The surgeries were recorded using a high-definition Karl Storz endoscope at 720p resolution and stored in .mp4 format. The study proceeded with patient informed consent and local governance committee approval. Following a standardized annotation framework derived from an international consensus study on pituitary surgery workflow [33], the videos were comprehensively annotated to include surgical phases, procedural steps, instrument utilization, and description of surgical activities. The annotation process involved collaborative work by two neurosurgical residents experienced in pituitary surgery, with final validation provided by an attending neurosurgeon. We extracted surgical frames from each video at a rate of 1 fps and removed any frames that exhibited blur or occlusion. Our final dataset comprises 101,803 frames, with individual video contributions ranging from 2,443 to 7,179 frames.\nFor each frame, we generated question-answer pairs using pre-defined templates, resulting in a collection of 745,972 pairs. The dataset contains 59 distinct questions distributed among 6 major categories, with lengths varying from 7 to 11 words. \nVector-MoLoRA: The low-rank update matrix of LoRA or Vector-LoRA limits the ability to effectively learn new knowledge compared to fully fine-tuning (FFT) [7]. While MORA attempts to address this limitation by employing a square matrix to achieve high-rank updates, it uses a static rank allocation designed consistently across different layers. This fixed-rank design overlooks the hierarchical feature of neural networks, limiting its flexibility during fine-tuning. To address the limitations of low-rank updating and MoRA's rigid rank allocation, we design Vector-MoLoRA, which combines matrix-rank and low-rank adaptation with vector ranking for the surgical VQA task.\nVector-MoLORA specifies rank adjustment through two separate rank vectors: \\(r_\\ell\\) defines the layer-wise LoRA rank, while \\(r_m\\) determines the rank of MoRA matrices for each layer, as shown in Eq. 12 and 13, respectively.\n\\[r_\\ell = [r_1^1 r_2^1 \\dots r_l^1] \\tag{12}\\]\n\\[r_m = [r_m^1 r_m^1 \\dots r_m^1] \\tag{13}\\]\nwhere \\(r_i\\) denotes the LoRA rank at layer i, and \\(r_m\\) indi-cates the dimension of MoRA matrices at the corresponding layer. The fundamental computation of Vector-MoLoRA is similar to Eq. 11 but extends it with layer-specific MORA adaptation. The weight update consists of two components: \\(\\Delta W\\) for LORA adaptation and \\(\\Delta W_m\\) for MoRA adaptation:\n\\[h = W_o x + \\Delta W_l x + \\Delta W_m x \\tag{14}\\]\nAs shown in Fig. 3, for the transformer layer i in the GPT-2 backbone, the updating of Vector-MoLoRA weights can be described by Eq. 15:\n\\[h(r_l, r_m) = W_i x_i + B_{ri} A_{ri} x_i + f_{rim} (M_{rim} g_{rim} (x_i)) \\tag{15}\\]\nwhere \\(W_i \\in \\mathbb{R}^{d \\times k}\\) is the original pre-trained weights, \\(B_{ri} \\in \\mathbb{R}^{d \\times ri}\\) and \\(A_{ri} \\in \\mathbb{R}^{ri \\times k}\\) are the LoRA adaptation"}, {"title": "IV. EXPERIMENTS", "content": "In addition to our open-ended PitVQA dataset, we also evaluate our model on a public benchmark dataset of open-ended EndoVis18-VQA [8]. This dataset contains 13,790 question-answer pairs derived from 2,086 surgical scenes across 14 nephrectomy surgery videos. The questions consist of 6-9 words, while answers contain 5-10 words, with a"}, {"title": "D. Ablation Studies", "content": "We evaluate our method through three ablation studies: (1) experimental verification of Vector-MoLoRA's effectiveness in mitigating catastrophic forgetting, (2) analysis of vector configurations and their performance impact, and (3) reliabil-ity analysis for Vector-MoLoRA using risk coverage curves.\nMitigation of Catastrophic Forgetting: Table II shows the effectiveness of Vector-MoLoRA in mitigating catas-trophic forgetting during transfer learning from open-ended PitVQA to EndoVis18-VQA. First, we fine-tune the PitVQA++ backbone network (i.e., multimodal encoder and GPT-2 decoder) on open-ended PitVQA dataset with frozen Vector-MoLoRA adapters. Using these pre-trained weights, we then conduct two fine-tuning strategies on EndoVis18-VQA: 1) Full Fine-tuning of the PitVQA++ backbone net-work, and 2) Vector-MoLoRA adaptation with a frozen GPT-2 decoder. As shown in the results, PitVQA++ with Vector-MoLORA achieves superior performance on the pre-trained domain validation while maintaining comparable performance on the training domain validation compared to its fully fine-tuned version. This demonstrates Vector-MOLORA's effectiveness in mitigating catastrophic forgetting during transfer learning.\nAnalysis of Different Vector Configurations: Ta-ble III presents the performance of PitVQA++ with Vector-MOLORA across different vector configurations on the Open-Ended PitVQA dataset, with each row representing a distinct configuration. The third configuration achieves the best per-formance, where MoRA and LoRA vectors are decremented every two layers, with MoRA decreasing by 8 and LoRA decreasing by 4. The first, second, and fourth configurations maintain the same step length but start with different MoRA and LoRA rank values. The last three configurations use smaller step sizes and rank values. The results show that the model performance improves as the rank values of MORA and LoRA vectors increase, reaching its peak with MoRA ranks decreasing from 64 to 24 and LoRA ranks from 32 to 12. However, further increasing the rank values leads to performance degradation, as larger ranks introduce more parameters, making the model prone to overfitting.\nThe risk-coverage analysis in Fig. 5 demonstrates that our proposed Vector-MoLoRA ex-hibits greater reliability than LoRA and MORA by achieving higher performance (ROUGE-L) while requiring fewer ex-pert referrals for uncertain predictions. For example, when the 20% most uncertain samples are excluded during infer-ence, our method achieves a 1.55% ROUGE-L improvement, whereas LoRA and MoRA achieve -0.06% and 0.87%, respectively. This highlights Vector-MoLoRA's superior con-fidence calibration, ensuring that uncertain predictions are handled more effectively. By referring only the most am-biguous cases to clinicians, our approach enhances trust in AI-driven decision-making, optimizing intraoperative guid-ance and supporting more efficient postoperative training programs."}, {"title": "V. DISCUSSION AND CONCLUSIONS", "content": "In this study, we introduced PitVQA++, a novel Vision-Language Model (VLM) tailored for open-ended Visual Question Answering (VQA) in endonasal pituitary adenoma surgery. Our approach integrates Vector-MoLoRA, an in-novative parameter-efficient adaptation strategy that dynam-ically allocates low-rank and matrix-rank updates across transformer layers based on hierarchical feature importance. In addition, we presented Open-Ended PitVQA, a compre-hensive surgical VQA dataset that captures intraoperative decision-making, procedural steps, and surgical tool inter-actions, significantly expanding the scope of open-ended question-answering in surgical AI.\nExperimental evaluations on both Open-Ended PitVQA and the EndoVis18-VQA dataset demonstrated that PitVQA++ outperforms existing state-of-the-art surgical VQA models, particularly in generating more contextually relevant and linguistically precise responses. The ablation studies provided further insights into the efficacy of Vector-MoLoRA, validating its ability to mitigate catastrophic forgetting while enhancing fine-tuning efficiency. Our risk-coverage analysis further confirmed that Vector-MoLoRA achieves higher reliability by improving performance under uncertainty, requiring fewer expert referrals for ambiguous predictions compared to LoRA and MoRA, thus enhancing its applicability for real-time clinical decision support.\nThese findings highlight the potential of PitVQA++ as a trustworthy AI assistant for intraoperative decision-making, offering surgeons context-aware, interpretable, and highly reliable responses. Future research directions include expand-ing the dataset to incorporate multimodal surgical cues (e.g., audio and haptic feedback), refining uncertainty quantifica-tion techniques for improved trust calibration, and exploring more advanced vision-language fusion mechanisms to fur-ther enhance system robustness and usability in real-world surgical settings."}]}