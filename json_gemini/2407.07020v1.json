{"title": "Less is More: Efficient Brain-Inspired Learning for Autonomous Driving Trajectory Prediction", "authors": ["Haicheng Liao", "Yongkang Li", "Zhenning Li", "Chengyue Wang", "Yuming Huang", "Chunlin Tian", "Zilin Bian", "Kaiqun Zhu", "Guofa Li", "Jia Hu", "Ziyuan Pu", "Zhiyong Cui", "Chengzhong Xu"], "abstract": "Accurately and safely predicting the trajectories of surrounding vehicles is essential for fully realizing autonomous driving (AD). This paper presents the Human-Like Trajectory Prediction model (HLTP++), which emulates human cognitive processes to improve trajectory prediction in AD. HLTP++ incorporates a novel teacher-student knowledge distillation framework. The \"teacher\" model equipped with an adaptive visual sector, mimics the dynamic allocation of attention human drivers exhibit based on factors like spatial orientation, proximity, and driving speed. On the other hand, the \"student\" model focuses on real-time interaction and human decision-making, drawing parallels to the human memory storage mechanism. Furthermore, we improve the model's efficiency by introducing a new Fourier Adaptive Spike Neural Network (FA-SNN), allowing for faster and more precise predictions with fewer parameters. Evaluated using the NGSIM, HighD, and MoCAD benchmarks, HLTP++ demonstrates superior performance compared to existing models, which reduces the predicted trajectory error with over 11% on the NGSIM dataset and 25% on the HighD datasets. Moreover, HLTP++ demonstrates strong adaptability in challenging environments with incomplete input data. This marks a significant stride in the journey towards fully AD systems.", "sections": [{"title": "1 Introduction", "content": "As we stand on the brink of a revolution in autonomous vehicles (AVs), the essential challenge that emerges is not just in the engineering of these vehicles but in endowing them with a cognitive prowess akin to human drivers. Central to this journey is the challenge of trajectory prediction, a task that requires a nuanced understanding of both the external driving environment and the internal cognitive processes of decision-making [15]. Traditional deep-learning models in AVs have made significant strides in data processing and pattern recognition but still encountered a bottleneck in complex and data-missing scenery. Given that human drivers are able to navigate complex scenes with remarkable adaptability and foresight, we aim to emulate some of the decision-making processes of human drivers to address this issue. At the heart of human decision-making process in driving lies a sophisticated network of neural activities, which involves multiple regions of the brain, each playing a distinct yet interconnected role [30]. The occipital and temporal lobes, responsible for visual processing, work in concert with the prefrontal and parietal cortex, which are central to decision-making and spatial reasoning [33]. It is this harmonious interplay that allows human drivers to anticipate potential hazards and make informed decisions based on a blend of current sensory input and past experience [2].\nDrawing inspiration from these cognitive mechanisms, our Human-Like Trajectory Prediction (HLTP++) model aims to replicate the essence of human decision-making. Due to the dual advantages of being lightweight and capable of applying cognitive mechanisms, we utilize knowledge distillation as the overarching framework. As shown in Figure 1, the \"teacher\" model of HLTP++ is designed to mimic the brain's visual processing faculties, employing a neural network architecture with visual pooling to prioritizes and interprets incoming visual data. This aspect of the model is analogous to the way the occipital and temporal lobes filter, process, and relay visual information to other parts of the brain. The \"student\" model of HLTP++, in turn, introduces a novel Spike Neural Network FA-SNN that represents the decision-making and reasoning aspects of the prefrontal and parietal cortex. Our goal is to create a model that not only processes information as the human brain does but also adapts to new information faster with similar flexibility and accuracy, aiming to address the current issues of high parameter counts and unhuman-like predicted trajectories. Overall, the contributions of HLTP++ are multifaceted:\n\u2022 We introduce a novel visual pooling mechanism to emulate the dynamic visual sector of human observation and adaptively self-adjust its attention to different agents in central and peripheral vision in real time under different scenes. Additionally, the Fourier Adaptive Spike Neural Network (FA-SNN) is proposed to handle traffic scenes with missing data by mimicking the neuronal pulse propagation process in human brain.\n\u2022 The HLTP++ presents a heterogeneous teacher-student knowledge distillation framework by using Knowledge Distillation Modulation (KDM) for multi-level tasks. This approach automatically adjusts the ratio between loss functions, significantly facilitating model training in complex trajectory distillation situations.\n\u2022 Benchmark tests on the NGSIM, HighD, and MoCAD datasets have shown that the HLTP++ outperforms existing top baselines by considerable margins, demonstrating its superior robustness and accuracy in various traffic conditions, including highways and dense urban environments. Notably, it exhibits remarkable performance even with fewer input observations and in scenarios characterized by missing data."}, {"title": "2 Related Work", "content": "Trajectory Prediction. In the realm of AVs, deep learning has catalyzed the adoption of various neural network architectures. Prior studies have explored Recurrent Neural Networks (RNNs) [35], social pooling [42], Graph Neural Networks (GNNs) [20, 23], attention mechanisms [44, 25], and Transformers framework [21, 45, 22] to enhance spatial and temporal feature extraction from trajectory data. Concurrently, some studies [16, 8] focus on improving safety in trajectory predictions, while others [14, 27] develop end-to-end models incorporating multimodality.\nHowever, existing models' computational intensity and lack of human-centric driving cognition pose challenges for real-time application and safety. To address this, we introduce a lightweight yet robust model that mimics human driving habits, enhancing safety for real-time trajectory prediction.\nKnowledge Distillation. Knowledge distillation, originally proposed by Hinton et al. [13], involves a complex \"teacher\" model transferring knowledge to a simpler \"student\" model. Initially aimed at model compression, its applications have expanded to improve generalization [41] and increase accuracy [34], while reducing computational and time requirements [3, 28]. However, there have been only a few applications in the autonomous driving field [23]. This study integrates human cognitive processes into the trajectory prediction paradigm by developing knowledge distillation method. The \"teacher\" model emulates human reasoning and effectively transfers this knowledge to the \"student\u201d model, simulating human driving behavior. In addition, we propose a novel multi-level task learning paradigm in the loss functions of our model for efficient training.\nSpike Neural Network. In recent years, there has been an increasing demand for edge computing applications. To address this, third-generation neural networks have been proposed. The most prominent representative among them is the SNN. Itis influenced by the neurons of the biological nervous system, which use discrete spikes for information encoding and transmission [10]. Research has shown the effectiveness of SNNs [38], particularly in terms of energy-saving, making them suitable for embedded deployment in vehicular systems. Traditionally, the primary focus of SNN studies has been on reducing inference time, often at the expense of overlooking improvements in adaptability. It has been observed that existing SNNs lack adaptability and temporal feature extraction, primarily due to fixed threshold constraints. Hence, this study introduces FA-SNN to improve the adaptability and temporal analysis of the trajectory prediction model."}, {"title": "3 Methodology", "content": "The principal objective of this study is to predict the trajectory of the target vehicle, encompassing all traffic agents within the sensing range of the AV in a mixed autonomy environment where AVs coexist with human-driven vehicles. At any time t, the state of the ith traffic agents can be denoted as $p^i_t$, where $p = (x, y)$ represents the 2D trajectory coordinate. Given the trajectory coordinate data of the target vehicle (superscript 0) and all its observed traffic agents (superscripts from 1 to n) in a traffic scene in the interval $[1, T_{obs}]$, denoted as $X = \\{p^i_t\\}^{n,T_{obs}}_{i=0,t=1} \\in R^{(n+1)\\times T_{obs} \\times 2}$, the model aims to predict a probabilistic multi-modal distribution over the future trajectory of the target vehicle, expressed as $P(Y|X)$. Here, $Y = \\{y_t\\}^{T_f}_{t=T_{obs}+1} \\in R^{T_f\\times 2}$ is the predictive future trajectory coordinates of the target vehicle over a time horizon $T_f$, and $y^i = \\{(p^{i,1}_{t};c^{i,1}), (p^{i,2}_{t};c^{i,2}), ..., (p^{i,C}_{t};c^{i,C})\\}$ encompasses both the potential trajectory and its associated maneuver likelihood $(\\sum c^i=1)$, with C denoting the total number of potential trajectories predicted. Notably, we focus on optimising the output of the \"student\" model, and the predictive future trajectory coordinates $Y^{stu}$ in this study is the output of the \"student\" model, formulated as $Y^{stu} = Y$.\nHLTP++ describes scenarios by focusing on the relative positions of traffic agents, aligning with human spatial understanding. It preprocesses historical data into two key spatial forms: 1) visual vectors S that capture relative position, velocity, and acceleration $S = \\{S_{\\Delta p}, S_{\\Delta s}, S_{\\Delta a}\\} \\in R^{(n+1)\\times T_{obs}\\times 4}$ of the target vehicle relative to its neighbors; 2) context matrices M describing speed and direction angle differences $M = \\{M_{\\Delta s}, M_{\\Delta \\theta}\\} \\in R^{(n+1)\\times T_{obs} \\times 2}$ among the surrounding agents."}, {"title": "3.3 Overall Architecture", "content": "Figure 2 illustrates the architecture of HLTP++. We use a novel pooling mechanism with an adaptive visual sector for data preprocessing. This sector dynamically adapts to capture important cues in different traffic situations which is similar to attention allocation. Additionally, the model also employs a teacher-student heterogeneous network distillation approach for human-like trajectory prediction. i) The \"teacher\" model. The Temporal Encoder and the Spatial Encoder within the \"teacher\" model process visual vectors and context matrices to produce temporal and spatial feature vectors, respectively. These vectors are then fed into the Fusion Module to fuse two modalities. The output of the Fusion Module is then fed into the Teacher Multimodal Decoder, which enables the prediction of different potential maneuvers for the target vehicle, each with associated probabilities. ii) The \"student\" model. The Fourier Adaptive SNN first processes trajectory temporal information from the Visual Pooling by imitating the transmission of neurons. Then the output matrix is fed into the Student Multimodal Decoder which is similar to the teacher. Besides self-training, the \"student\" model acquires knowledge from the \"teacher\" model using a Knowledge Distillation Modulation (KDM) training strategy. This approach ensures accurate trajectory predictions while requiring fewer input observations."}, {"title": "3.4 Visual Pooling Mechanism", "content": "Research [37] shows that human drivers, constrained by brain's working memory, focus mainly on a few external agents in their central visual field, especially in high-risk situations. The driver's visual sector, influenced by speed, narrows at higher speeds for focused attention and widens at lower speeds for broader awareness.\nHLTP++ introduces a visual pooling mechanism that emulates this adaptive visual attention. It features an adaptive visual sector that adjusts the field of view based on vehicle speed. Specifically, in contrast to models with uniform attention distribution, we propose a visual weight matrix $H_{vision}$ that adapts to changing focus in the central visual field at different speeds. Speed thresholds at 0km/h, 30km/h, 60km/h, and 90km/h define distinct values of the visual sectors. This approach refines the understanding of attention during driving. The visual weight matrix $H_{vision}$ is then integrated by the input visual vectors S. Formally,\n$\\bar{S} = H_{vision} S,$\nThis equation produces visual vectors $\\bar{S}$ that encapsulate human drivers' varying attention patterns."}, {"title": "3.5 Teacher Model", "content": "The \"teacher\" model integrates Temporal and Spacial encoders to closely emulate human visual perception, mirroring the retinal processing of the human drivers. As an enhancement, it further employs iTransformer framework in the decoder to effectively extract spatio-temporal interactions.\nTemporal Encoder. In real-world driving, the human brain, with its limited processing capacity, prioritizes information to facilitate efficient decision-making. Therefore, allocating distinct attention to different features is essential, as it reduces the cognitive load on the brain in processing non-essential information. We employ a LSTM layer to process temporal information, followed by a multi-head attention mechanism for attention allocation.\nSpatial Encoder. Human drivers focus on their central vision while continuously monitoring their peripheral vision through side and rear-view mirrors to understand their surroundings, including nearby vehicles, pedestrians, and road conditions. To replicate this peripheral monitoring, especially during maneuvers, we introduce the Spatial Encoder. It processes a quarter of the time-segmented matrices $M\\in R^{(n+1)\\times \\frac{T_{obs}}{4} \\times 2}$ with a $1 \\times 1$ convolutional layer for channel expansion, followed by a $3 \\times 3$ layer for specific feature extraction, incorporating batch normalization and dropout for robustness. Enhanced with Graph Attention Networks and ELU activation, it produces spatial vectors $O_s$.\nFusion Module. The combined outputs of the Spatial Encoder $O_s$ and the Temporal Encoder $O_t$, are fused and then fed into the iTransformer architecture [29] for advanced spatio-temporal interaction analysis, generating hidden states I. Furthermore, we use the disparities between temporal and spatial features to generate a loss, denoted as $L_{st}$, which serves as one of the loss functions for training the \"teacher\" model.\nTeacher Multimodal Decoder. The decoder of the \"teacher\" model, based on a Gaussian Mixture Model (GMM), accounts for uncertainty in trajectory prediction by evaluating multiple possible maneuvers and their probabilities. Specifically, built on the base layer of estimated maneuvers C, the model assumes that the probability distribution for trajectory predictions follows a Gaussian framework:\n$P_{\\Omega}(Y|C, X) = \\sum^C_{i} N(Y| \\mu(X), \\Sigma(X))$\nwhere X represents the input to our model, and $\\Omega = [t+1,..., t+t_f]$ symbolizes the estimable parameters of the distribution. Each $\\Omega = [\\mu, \\Sigma]$ represents the mean and variance for the predicted trajectory at time t. Then, in the next layer, the multimodal predictions are conceptualized as a GMM:\n$P(Y|X) = \\sum_{i} P (c_i|X) P_N (Y|c_i, X)$\nwhere $c_i$ denotes the i-th maneuver in C.\nThen, the hidden states are processed through softmax activation and a MLP layer, forming a probability distribution $O_{tea}$ over potential future trajectories. This approach balances the precision and validity, which is critical for dynamic and uncertain driving environments."}, {"title": "3.6 Student Model", "content": "The FA-SNN is introduced in the \"student\" model, which emphasizes the short-term and fewer observations, with visual vectors $\\bar{S}$ ($T_{obs} = 8$), utilizing visual vectors and a lightweight architectural design for efficient learning. In a departure from the complex framework of the \"teacher\" model, the \"student\" model employs a more lightweight architecture to produce a multimodal prediction distribution $O_{stu}$. By learning the behavioral paradigm from the \"teacher\" model, the \"student\" model is able to make human-like predictions even when constrained by limited observations.\nSpecifically, The FA-SNN proposed in this study is an enhanced version of the traditional SNN model. It addresses the challenge of fitting difficulties during training by introducing adaptive adjustments and optimizing temporal feature extraction. The approach is based on the idea that neurons in an SNN should adapt to different scenarios, which is mainly reflected in the adjustment of the threshold magnitude. Inspired by the Leaky Integrate-and-Fire (LIF) mechanism, the Fourier Transform (FT) is used to extract specific features. More specifically, the FA-SNN's forward propagation involves three essential processes: Charging, Leakage, and Firing Processes.\nCharging Process. Similar to traditional perceptron neurons, the current neuron is charged by aggregating input spike sequences from previous neurons through varying weights at discrete time steps $\\{t_0, t_1, ..., t_n, ..., t_w\\}$.\nLeakage Process. In the LIF mechanism, neurons experience leakage due to voltage differences in their surroundings. The internal voltage V of the spiking neuron tends towards an equilibrium voltage U over time t, adhering to the differential equation $\\frac{dV}{dt} = \\frac{U-V}{\\eta} = -\\eta$, where $\\eta = 1$ denote the leakage decay rate, indicating the magnitude of voltage decay. This dynamic is pivotal for the neuron's voltage stabilization, allowing the calculation of future voltage states. After solving the above equation, we can obtain:\n$V(t_n) = U - Ce^{-\\frac{t_n}{\\eta}}$\nwhere C is a constant value. This allows us to calculate the voltage at the next moment $t_{n+1}$:\n$V(t_{n+1}) = V(t_n + dt) = e^{-\\frac{dt}{\\eta}} (V(t) - U) + U$\nFiring Process. The firing process is activated based on the spike magnitude through an activation function. Given a spike threshold $U_0$, the voltage $V' (t_{n+1})$ can be defined as follows:\n$V'(t_{n+1})=\\begin{cases}V(t_{n+1}) - U_0, & V(t_{n+1}) > U_0\\\\V(t_{n+1}), & V(t_{n+1}) \\le U_0\\end{cases}$\nUnlike traditional models with a fixed spike threshold, the proposed FA-SNN employs a learnable threshold, adjusting dynamically to preserve temporal features. Importantly, a Fast Fourier Transform (FFT) is applied to the pre-firing spike value $V' (t_{n+1})$ to incorporate frequency information, enhancing the model's representation capability:\n$F[V(t_{n+1})] = \\sum_{T=0}^{N}V(t_{n+1}).e^{-i\\frac{2\\pi(n + 1) T}{N+1}}$\nwhere i is the imaginary unit. According to Euler's formula:\n$e^{-i\\frac{2\\pi(n + 1) T}{N+1}} = cos[-\\frac{2\\pi(n + 1) T}{N+1}] + isin[-\\frac{2\\pi(n + 1) T}{N+1}]$\nwhere $A = cos[-\\frac{2\\pi(n + 1) T}{N+1}]$ and $B = sin[-\\frac{2\\pi(n + 1) T}{N+1}]$ respectively represent the real and imaginary parts of $e^{-i\\frac{2\\pi(n + 1) T}{N+1}}$. We then compute the power spectrum $W = (|A| + |B|)^2$ to serve as the output feature, where \"||\" represent the absolute value symbol.\nBackpropagation. Due to the discontinuity of the activation function used during SNN firing, conventional chain-rule differentiation is infeasible. To circumvent this, the gradient G is redefined, factoring in the spike threshold and introducing parameters like the absolute width $w_a$, gradient width $w_g$ and gradient scale s:\n$G(V'(t_{n+1})) = \\frac{s}{w_a}exp(-\\frac{|V'(t_{n+1}) - U_0|}{w_a})$\nwhere $w_a = U_0 \\cdot w_g$, and $w_g = 0.5$, s = 1.0."}, {"title": "4 Training", "content": "For the \"teacher\" model, we follow a standard protocol, using 3 seconds of observed trajectory for input ($T_{obs} = 16$) and predicting a 5-second future trajectory ($T_f = 25$). To extract complex knowledge from the dataset, we allow slight overfitting during training. The training loss function of the teacher model consists of three parts: trajectory loss $L_{traj}$, maneuver loss $L_{man}$ and temporal-spacial loss $L_{st}$ from iTransformer.\nTrajectory Loss. In our trajectory prediction process, we treat the output 2D trajectory coordinates $P = (x,y)$ as a bivariate Gaussian distribution. Therefore, we could use Negative Log-Likelihood (NLL) loss $L_{traj}$ to measure the disparity between the prediction and the ground truth. Considering the total number C of potential trajectories predicted, with $P_{tea}^{pred}$ and $P_{gt}$ representing the teacher model's predicted trajectory coordinates and the ground truth coordinates respectively, the trajectory loss $L_{traj}$ for the teacher model can be formulated as: $L_{traj}^{tea} = -log \\sum^C_{i=1} N(P_{tea}^{pred}, P_{gt})$. Mathematically, for a specific data point, given the model's predicted probability distribution $P(Y|X)$, where Y represents the output future trajectory and X denotes the input features, the NLL Loss $L_{NLL}$ is defined as follows:\n$L_{NLL} = -log(P(Y|X)),$\nManeuver Loss. To address the potentially detrimental effects of mis-classifying maneuver types on trajectory prediction accuracy and robustness, we adopt the Mean Squared Error (MSE) loss $L_{man}$ to measure the disparity between the predicted maneuver types $M_{tea}^{pred}$ and the ground truth maneuver types $M_{gt}$, which is formulated as: $L_{man} = \\sum^C_{i=1}\\sum^C_{m=1} (M_{tea}^{pred}, M_{gt})$, so the total loss function of teacher model is formulated as follows:\n$L_{tea} = L_{traj} + L_{man} + L_{st},$"}, {"title": "4.2 Student Training", "content": "The \"student\" model is trained to predict 5-second future trajectories with fewer input observations. To improve its predictive performance with limited observations, we decouple the total loss function of the student model L as the student loss (especially refers to the loss function exclusively associated with the \u201cstudent\" model) $L_{stu}$ and distillation loss $L_{dis}$. The student loss, similar to that of the \"teacher\" model, quantifies the discrepancy between the model's predicted trajectories, maneuvers and their ground truths. Formally,\n$L_{stu} = L_{traj}^{stu} + L_{man}^{stu}$ \n$=\\sum_{t}^{T_f} \\sum_{c}^{C}(N (P_{stu}^{pred}, P_{gt}) +  \\sum_{t}^{T_f} \\sum_{m}^{C} (M_{stu}^{pred}, M_{gt}))$\nwhere $P_{pred}$ and $M_{pred}$ represent the predicted 2D coordinates and maneuvers of the \"student\" model.\nMoreover, we apply the MSE loss to measure the disparity between the outputs of the teacher and the \"student\" model:\n$L_{dis} = L_{traj}^{dis} + L_{man}^{dis}$ \n$=  \\sum_{t}^{T_f} (P_{tea}^{pred}, P_{stu}^{pred}) + (M_{tea}^{pred}, M_{stu}^{pred})$\nHence, the total loss function of the \"student\" model is formulated as $L = L_{stu} + L_{dis}$. Then, we propose a method for tuning multiple tasks that evaluates the importance of different loss functions and autonomously adjusts the weights between them for efficient training."}, {"title": "4.3 Knowledge Distillation Modulation", "content": "Given that C is composed of sub-loss functions from multiple tasks, determining the proportionality relationships between them poses a challenging problem. Both $L_{stu}$ and $L_{dis}$ are further decomposed into maneuver loss function $L_{traj}$ and trajectory coordinate loss function $L_{man}$:\n$L_{stu} = L_{traj}^{stu} + L_{man}^{stu}, L_{dis} = L_{traj}^{dis} + L_{man}^{dis},$\nDrawing the inspiration from the notable work [17], we incorporate the KDM to weight the trajectory loss and the distillation loss, with homoscedastic uncertainty. To the best of our knowledge, we are the first to propose a multi-level, multi-task hyperparameter tuning approach to autonomously adjust knowledge distillation hyperparameters during training in this field. Our approach defines a multi-level task where the overarching training loss function is composed of an ensemble of sub-loss functions. Each of these sub-loss functions is further composed of additional sub-loss functions that share some degree of similarity.\nFollowing the approach outlined in Kendall et al. [17] for the first level of the loss function, we obtain the following equation:\n$L_{stu} = \\frac{1}{2 \\sigma_{traj}^{stu} }L_{traj}^{stu} (W) +  \\frac{1}{2 \\sigma_{man}^{stu} }L_{man}^{stu}(W) + log\\sigma_{traj}^{stu} + log\\sigma_{man}^{stu}$ \n$L_{dis} = \\frac{1}{2 \\sigma_{traj}^{dis} }L_{traj}^{dis} (W) +  \\frac{1}{2 \\sigma_{man}^{dis} }L_{man}^{dis}(W) + log\\sigma_{traj}^{dis} + log\\sigma_{man}^{dis}$\nwhere $\\sigma_{traj}$, $\\sigma_{man}$ are the learnable uncertainty variances, W represents trainable parameters of the model. Since L is composed of $L_{stu}$ and $L_{dis}$, we use the multi-task tuning approach for the second level:\n$L = \\frac{1}{2 \\sigma_{stu} }L_{stu} (W) +  \\frac{1}{2 \\sigma_{dis} }L_{dis}(W) + log\\sigma_{stu} + log\\sigma_{dis}$\nCombining Eq. 15 and Eq. 16, we obtain the following formulation:\n$L(W, \\sigma_{traj}, \\sigma_{man},  \\sigma_{stu}, \\sigma_{dis}) =  \\frac{1}{2 \\sigma_{stu} } \\frac{1}{2 \\sigma_{traj}^{stu} }L_{traj}^{stu} + \\frac{1}{2 \\sigma_{stu} } \\frac{1}{2 \\sigma_{man}^{stu} }L_{man}^{stu} +  \\frac{1}{2 \\sigma_{dis} } \\frac{1}{2 \\sigma_{traj}^{dis} }L_{traj}^{dis} + \\frac{1}{2 \\sigma_{dis} } \\frac{1}{2 \\sigma_{man}^{dis} }L_{man} + F (\\sigma_{traj}, \\sigma_{man}, \\sigma_{stu}, \\sigma_{dis})$\nwhere F equals to $log\\sigma_{traj} + log\\sigma_{man} + log\\sigma_{stu} + log\\sigma_{dis}$. To ensure uniformity in the derived equations from different level-segmenting approach, we modify F as $F = log(\\sigma_{traj}\\sigma_{man}\\sigma_{stu}\\sigma_{dis})$."}, {"title": "5 Experiment", "content": "Datasets. We conduct experiments using three esteemed datasets: NGSIM [9], HighD [18], and MoCAD [24]. These three datasets cover various traffic conditions, including on highways and in dense urban environments.\nMetric. Root Mean Square Error (RMSE) and average RMSE is applied as our primary evaluation metric, which is commonly used as a measure to calculate the square root of the average squared prediction error in autonomous driving.\nImplementation Details. HLTP++ is developed using PyTorch and trained on an A40 48G GPU. We use the Adam optimizer along with CosineAnnealingWarmRestarts for scheduling, with a training batch size of 256 and learning rates ranging from 10-3 to 10-5. Unless specified, all evaluation results are based on the \"student\" model of HLTP++."}, {"title": "5.2 Experiment Results", "content": "Comparison with the State-of-the-art (SOTA) Baselines. Our comprehensive evaluation demonstrates HLTP++'s superior performance compared to state-of-the-art baselines, as detailed in Table 1. It notably achieves gains of 11.2% for long-term (5s) and 11.4% for average predictions on the NGSIM dataset. The corresponding outstanding performance is also evident on the HighD dataset and the MACAD dataset. It is noteworthy that our model HLTP++(h), despite utilizing only 1.5 seconds of input data (half of the input of other baselines), achieves comparable prediction accuracy. This highlights the adaptability and robustness of HLTP++.\nComparing Model Performance and Complexity. As detailed in Table 2, our benchmarking against SOTA baselines reveals that HLTP++ models outperform in all metrics while maintaining a minimal parameter count. Specifically, HLTP++ reduce parameters by 56.91% and 33.51% compared to WSiP and CS-LSTM, respectively. Compared to HLTP++(SM), the \"teacher\" model of HLTP++, HLTP++(TM), achieve the second best score in three datasets, while maintaining a larger number of parameters and slower inference speed. However, HLTP++ maintain the lowest inference time while achieve the best accuracy in trajectory prediction. Utilizing the Knowledge Distillation Module (KDM), HLTP++ retains the lightweight advantages of the HLTP++(SM), while concurrently enhancing its predictive capabilities by assimilating knowledge gleaned from the teacher model, thereby surpassing the performance of the teacher model itself. This highlights the efficiency and adaptability of our lightweight \"teacher-student\" knowledge distillation framework, offering a balance between accuracy and computational resources."}, {"title": "5.3 Ablation Studies", "content": "Ablation Study for Core Components. Table 3 shows that our ablation study evaluates the performance of HLTP++ using six model variations, each omitting different components. The data in Table 4 clearly indicate that the performance of all models degrades when components are removed, as compared to the baseline model. Notably, integrating the iTransformer and a multimodal probabilistic maneuvering module significantly improves the accuracy. This underscores their vital function in encapsulating the spatio-temporal dynamics among vehicles. Furthermore, ablation studies A and B provide empirical evidence supporting the utility of incorporating cognitive mechanisms similar to human brain processes."}, {"title": "Ablation Study for FA-SNN.", "content": "To further showcase the effectiveness of our proposed FA-SNN, we conducted an ablation study in HLTP++ by replacing the FA-SNN with the standard SNN, SNN only with the Fourier Transform (FT), and the SNN only with the adaptive spike threshold (AST). Table 5 demonstrates that incorporating the AST and FT in SNN can significantly improve the predictive performance of the model. This underscores the FA-SNN's ability to capture and extract spatio-temporal interactions in complex scenes."}, {"title": "Ablation Study for KDM.", "content": "To better illustrate the impact of KDM on HLTP++, we present the curves of different loss functions during the training process, as shown in Figure 4 (1) and 4 (2). Specifically, $L_{traj}^{tea}$, $L_{traj}^{stu}$, $L_{traj}^{stu}(KDM)$, and $L_{traj}^{tea}(KDM)$ represent the trajectory losses of training the teacher model independently, training the student model independently, training the student model with KDM, respectively. Accordingly, $L_{man}^{tea}$, $L_{man}^{stu}$, and $L_{man}^{stu}(KDM)$ denote the maneuver losses. From Figure 4 it can be seen that both $L_{traj}^{stu}(KDM)$ and $L_{man}^{stu}(KDM)$ have initially higher values during training, but they quickly exhibit exponential decay as training progresses. This ensures a more efficient initial descent of the model during training and allows for adjustments at a finer granularity in later stages, where the traditional loss functions fail to achieve.\nFurthermore, in Figure 4 (3)-(5), we compare the trajectory loss and maneuver loss between the teacher model, the student model without KDM, and the student model with KDM. In Figure 4 (3) and Figure 4 (4), there is a significant difference in the loss values between the instructor and student models, and as training progresses, the difference between them stabilizes around 40, differing by two orders of magnitude. Even in these two subplots, it is difficult to observe a noticeable decrease in maneuver loss. This implies that without KDM adjustments, there would be a severe imbalance in the ratio between trajectory loss and maneuver loss, causing the model to lean more towards trajectory fitting while neglecting lane change information. However, Figure 4 (5) shows that although the magnitudes of $L_{traj}^{stu}(KDM)$ and $L_{man}^{stu}(KDM)$ are significantly different at the beginning of training, they quickly converge as training progresses. This indicates that adaptive tuning through KDM allows for a better balance of the proportions between different loss, thus achieving a balanced optimization of loss functions for different tasks."}, {"title": "Ablation Study for Missing Data.", "content": "We introduce a missing test set on the NGSIM dataset, focusing on scenarios where part of the historical data is missing. The set is divided into five subsets based on varying durations of data absence. For example, subset tm=0.4 indicates a missing trajectory data duration of 0.4 seconds, which was imputed using linear interpolation. The results in Table 6 show that HLTP++ outperforms all baselines even with 1.6s missing data, highlighting its adaptability and deep understanding of traffic dynamics."}, {"title": "6 Conclusion", "content": "This study presents a novel trajectory prediction model (HLTP++) for AVs. It addresses the limitations of previous models in terms of parameter heaviness and applicability. HLTP++ is based on a multi-level task knowledge distillation network, providing a lightweight yet efficient framework that maintains prediction accuracy. Importantly, HLTP++ adapts to scenarios with missing data and reduced inputs by simulating human observation and making human-like predictions. The empirical results indicate that HLTP++ excels in complex traffic scenarios and achieves SOTA performance. In future work, we plan to feed multimodal data, such as Bird's Eye View (BEV), multi-view camera images and Lidar, into the HLTP++ model to further enhance the scene understanding of the model."}]}