{"title": "Demystifying the Communication Characteristics for Distributed Transformer Models", "authors": ["Quentin Anthony*", "Benjamin Michalowicz*", "Jacob Hatef", "Lang Xu", "Mustafa Abduljabbar", "Aamir Shafi", "Hari Subramoni", "Dhabaleswar K. (DK) Panda"], "abstract": "Deep learning (DL) models based on the transformer architecture have revolutionized many DL applications such as large language models (LLMs), vision transformers, audio generation, and time series prediction. Much of this progress has been fueled by distributed training, yet distributed communication remains a substantial bottleneck to training progress. This paper examines the communication behavior of transformer models that is, how different parallelism schemes used in multi-node/multi-GPU DL Training communicate data in the context of transformers. We use GPT-based language models as a case study of the transformer architecture due to their ubiquity. We validate the empirical results obtained from our communication logs using analytical models. At a high level, our analysis reveals a need to optimize small message point-to-point communication further, correlations between sequence length, per-GPU throughput, model size, and optimizations used, and where to potentially guide further optimizations in framework and HPC middleware design and optimization.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) such as ChatGPT [1], Gemini [2], and Llama [3] are revolutionizing multiple in-dustries with their ability to perform a range of tasks from customer service to creative content generation. LLMs are typically pre-trained with internet-scale, pre-processed data that allows them to learn the intricacies of human languages. After pre-training, LLMs undergo a fine-tuning process in a supervised setting that allows them to excel in down-stream tasks like generation, summarization, translation, and question/answering. Modern LLMs utilize a large number of parameters that imply increased computational and memory requirements during training. A higher number of parameters allows the model to capture more intricate relationships and nuances in language, leading to improved performance on a range of downstream tasks."}, {"title": "A. Motivation", "content": "As an LLM's size increases, training requires a large number of GPUs for a considerable amount of time on modern HPC systems, and it is significantly bottlenecked by how quickly data can be exchanged between parallel training processes. Here, the messaging stack including the communication fabric plays a pivotal role. At large scales, such a bottleneck leads to lower Model FLOPs Utilization (MFU) [4] for training. For instance, MegaScale [5] reports a 55.2% MFU on 12,288 GPUs for training a 175-billion parameter model. To empha-size this point, Figures 1 and 2 show how communication begins to dominate computation at increasing scales for 13-billion and 20-billion parameter GPT-2-based models. We are motivated by this to conduct a thorough characterization study to understand the communication stage during LLM training."}, {"title": "B. Problem Statement", "content": "Good communication performance is critical for scaling LLM training on large HPC systems. This paper aims to study and analyze communication strategies used by state-of-the-art Deep Learning (DL) training frameworks on leading-class supercomputers. Our objective is to learn the volume of data exchanged as well as communication primitives employed, number of calls, and message sizes involved-between paral-lel processes at different scales from various parallelization strategies. This detailed analysis needs to be conducted in the context of input datasets, model architectures, and model sizes. This characterization study will aid the next generation of communication runtimes to meet the performance require-ments of LLM training workloads and increase the effective utilization of large-scale systems."}, {"title": "C. Challenges", "content": "Figure 3 shows just how many combinations someone must consider when characterizing LLM communication on AI/HPC systems, from frameworks such as Megatron-LM [6], Llama [7], and DeepSpeed [8] and parameter count/model size, to choice of communication middleware [9], [10], [11], to parallelism strategies [12], [13], [14], all the way down to the hardware on which training/characterization takes place."}, {"title": "D. Proposed Solution", "content": "Given the complexity and importance of understanding communication in emergent transformer-based workloads, we adopt a systematic approach that combines empirical results with analytical modeling to study communication behavior for various parallelism schemes and sequence lengths. Through this, we aim to give an in-depth understanding of the com-munication overheads associated with parallelism schemes commonly used in transformer models, which form the foun-dational architecture of LLMs. Our analysis covers a range of model optimizers, including ZeRO-1, ZeRO-2, ZeRO-3, and ZeRO++, as well as Data Parallelism, Pipeline Parallelism, and Tensor Parallelism for up to 13B parameter models. In line with the adopted analytical models, we present system-agnostic measurements for each parallelism scheme. Measure-ments include 1) the collective communication type 2) the data volumes per collective 3) the proportions, frequency, and message sizes for each collective. We also examine the impact of sequence length on communication volumes per collective pattern for Data-Parallel and Model-Parallel environments. This technique is particularly valuable for researchers and developers of collective communication libraries, as it provides insights into which collectives to enhance and which mes-sage ranges to target to improve LLM training performance. Additionally, we conduct interconnect-specific evaluations, measuring latency for particular collectives on AMD Infinity Fabric and HPC-Slingshot 11 GPU and node interconnects. This aims to understand the communication overhead for the underlying calls at the OMB microbenchmark level, using the same communication backend as employed by our training framework of choice, GPT-NeoX[15]."}, {"title": "E. Contributions", "content": "Our contributions are as follows:\n1) We combine empirical results with analytical models to study communication behavior for various parallelism schemes and sequence lengths.\n2) We provide an in-depth understanding of the communica-tion overheads associated with Data, Pipeline, and Tensor parallelism schemes commonly used in transformer mod-els.\n3) We present system-agnostic and system-specific mea-surements for each parallelism scheme, including col-lective communication types, data volumes, proportions, frequency, and message sizes.\n4) We examine the impact of sequence length on commu-nication volumes per collective pattern for Data-Parallel and Model-Parallel environments.\n5) We conduct interconnect-specific evaluations, measuring latency and bandwidth for the particular collectives used by the studied LLM models. The analysis is conducted on AMD Infinity Fabric and HPE-Slingshot 11 GPU and node interconnects.\nTo the best of our knowledge, this is the first study to systematically characterize communication for distributed transformer models across multiple parallelism schemes and sequence lengths, providing detailed insights into collective communication types, data volumes, and distri-butions, and combining these results with the interconnect-specific collective communication benchmarking on the Frontier supercomputer."}, {"title": "F. Paper Breakdown", "content": "The rest of this paper is broken down as follows. Section II explains the background of LLMs and parallelism schemes used to train them and other DL models on HPC clusters. Section III details the set of equations used to model com-munication volume for each parallelism scheme used in this paper. Sections IV and V break down our experimental results"}, {"title": "II. BACKGROUND", "content": "The current trend in Natural Language Processing (NLP) favors transformer models [16] for their exceptional accu-racy and computational efficiency. The original transformer architecture is designed for machine translation and contains two main components: an Encoder and a Decoder. Modern adaptations of transformers for language modeling utilize either the Encoder or Decoder depending on the specific task, such as BERT [17] and GPT-2 [18].\nA transformer layer is structured with a self-attention block followed by a two-layer multi-layer perceptron (MLP), com-posed of two GEMMs and a GeLU non-linearity (ReLU for the original version [16]). Each encoder or decoder block includes multiple such layers, each featuring multi-head attention, MLP, normalization, and residual connections.\nWe consider a single encoder or decoder with multiple transformer layers. Initially, input tokens are processed through a word embedding table and combined with positional em-beddings, resulting in a 3-D tensor of size (sequence length x micro-batch size \u00d7 hidden dimension) [19]. Each transformer layer processes this tensor through a self-attention block with multiple attention heads and a two-layer MLP that quadru-ples the hidden size and then reduces it back. The output size remains consistent across layers, and the final output is projected back to the vocabulary dimension for cross-entropy loss calculation."}, {"title": "B. Parallelism Techniques", "content": "Larger models are more sample-efficient given a fixed compute budget [20], [21], leading to a massive increase in model parameter count. Training billion/trillion-parameter transformer models is a memory-intensive task since it requires efficient distribution of multiple training parameters (model weights, optimizer states, gradients, and activations).\nIn Data Parallelism [22], a training mini-batch is divided among multiple workers and each worker maintains a full model replica. Data parallelism can achieve near-linear scaling in training data throughput by increasing the mini-batch size in proportion to the number of available workers. Typically, an Allreduce on all the workers is required to synchronize the gradients before updating the model weights on each local replica. Data Parallelism is communication-bound since the achievable bandwidth and latency of the Allreduce greatly affect iteration time given a worker's memory is consumed by the model and other training parameters. However, data parallelism requires that model size must fit in the limited GPU memory and additional optimizer and hyper-parameter tuning to ensure convergence with large global batch size [23].\nPipeline Parallelism mainly focuses on distributing layers of models among GPU workers and executes these layers in a pipeline order. Since activation computation relies on depen-dencies between different layers, inevitable GPU idle times, known as pipeline bubbles are present in this paradigm, there have been various research efforts in reducing such bubbles [24], [25]. In terms of communication, pipeline parallelism involves point-to-point GPU communication to pass along activations between layers.\nTensor Parallelism [26] aims at exploiting the inherent parallelism inside GEMM operations and distribute these com-putations along specific directions (rows, columns) and use synchronization among workers to gather the results, thus en-suring correctness. State-of-the-art implementations distribute the MLP blocks and Self-Attention blocks [26]. Results are collected and aggregated using Allreduce and Allgather. It is a common practice to limit tensor parallelism degree within a compute node since intra-node bandwidth is typically larger than inter-node bandwidth [27]."}, {"title": "C. Zero Redundancy Optimizer", "content": "Data parallel training requires each rank to hold a copy of all model optimizer states, gradients, and parameters. [28] Zero Redundancy Optimizer (ZeRO) reduces memory constraints by removing redundant information, and partitioning model data across data parallel ranks. ZeRO is divided into three stages, ZeRO-1, ZeRO-2, and ZeRO-3. Given a certain degree of data parallelism, each ZeRO stage partitions different training parameters. ZeRO-1 partitions optimizer states across workers. Each worker only needs to store and update its partitions. At the end of each training step, an allgather is required to collect the fully updated model weights. ZeRO-2 further partitions gradients and reduces them to only update the corresponding parameters. After gradient reduction, the memory can be released immediately, which will further alle-viate memory pressure on a worker. Such a process requires Reduce-Scatter to distribute and reduce the gradients. ZeRO-1 and ZeRO-2 produce the same communication volume as stan-dard data parallelism [28]. ZeRO-3 applies model parameter partitioning on top of optimizer states and gradients. However, stage 3 requires an extra allgather to collect parameters from all other processes as needed in forward and backward com-putation which typically incurs 1.5x communication volume compared to data parallelism baseline (Figure 5).\nZeRO++ applies various optimizations towards ZeRO-3, aiming at reducing communication volume and featuring a bandwidth-aware partitioning strategy. Specifically, ZeRO++ integrates blocked-based quantization kernels [29] into model weights and gradient communications to drastically reduce message size. It also keeps a secondary parameter partition within a compute node so that high-latency inter-node All-gather can be avoided due to low interconnect bandwidth [30]."}, {"title": "III. PERFORMANCE MODEL", "content": "This section breaks down each component that makes up our performance model."}, {"title": "A. Data Parallelism and ZeRO", "content": "To calculate the total parameters in a transformer, we have the embedding and unembedding blocks of size $V \\times h$ each. If embedding and unembedding parameters are tied (i.e. shared), this leads to a total of $V \\times h$ parameters from embeddings. Since all configurations in this paper use untied embeddings, we have $2V \\times h$ embedding parameters. We also have the position embeddings of size $sh$. The attention matrices are four separate matrices of dimension $h \\times h$, leading to $4h^2$ attention parameters per layer. Multilayer perceptron (MLP) blocks for our models are composed of two fully-connected linear projections of size $h \\times xh$ and $xh \\times h$, where $x$ is the expansion factor. For GPT-NeoX model architectures, the conventional projection factor is 4 [31], so we have $2xh^2 = 8h^2$ MLP parameters per layer. We then have a layernorm each layer with both gains and biases on each of the $Q, K, V$ and the first MLP linear projection, leading to $8h$ layernorm parameters per layer. Finally, we add the final layernorm of size $2h$ to get a total number of parameters in Equation 1 below.\n$param\\_count = 2Vh + sh + L(12h^2 + 8h) + 2h$ (1)\nConsidering a message size of m, the communication vol-ume for the Allreduce collective is $2 \\times m(\\frac{d-1}{d})$. The commu-nication volume for Allgather, Reduce_scatter, and Reduce is simply $m(\\frac{d-1}{d})$.\nThe communication volume per iteration for distributed data parallelism (DDP) just comes from the gradient Allreduce, which gives the total volume per iteration given in Equation 2 below. ZeRO-1 and ZeRO-2 simply replace this Allreduce call with separate Reduce_scatter and Allgather calls [28], so they have the same communication volume as DDP. Therefore, the communication volume (in units of parameters) from DP (Allreduce), ZeRO-1, and ZeRO-2 (Allgather/Reduce_scatter) is given by:\n$2 * param\\_count * (\\frac{d-1}{d})$ (2)\nThe communication volume for ZeRO-3 is 50% higher due to an extra Allgather of parameters, which is necessary before the forward pass because parameters are now also sharded across ranks (See II-C and [28]). Therefore, the ZeRO-3 communication volume (in units of parameters) is given by:\n$3 * param\\_count * (\\frac{d-1}{d})$ (3)"}, {"title": "B. Model Parallelism", "content": "The communication volume for pipeline parallelism comes from the point-to-point communication of forward activations and backward gradients. The send or receive between two pipeline stages is of size bsh, therefore the aggregate commu-nication volume across all stages in a single training iteration is given in Equation 4 below (in units of parameters and where d is the number of devices, or GPUs, used in training). Notably, the first stage doesn't have to receive activations and the last GPU doesn't have to send activations (and vice-versa with gradients), so we multiply by p 1 instead of p.\n$2bsh \\times (p-1)$ (4)\nThe communication volume per iteration for tensor paral-lelism comes from 6 Allreduce operations per layer (2 in the forward pass, 2 for activation recomputation, 2 in the backward pass). Further, an additional Allreduce operation is performed at the embedding. Each Allreduce incurs a volume of 2m, leading to a total of $(12L+2)$ volume for messages of size bsh. Since these Allreduce operations are across t ranks, they're multiplied by a factor of $\\frac{t-1}{t}$.\n$(12L + 2) * bsh * (\\frac{t-1}{t})$ (5)\nFor 3D parallelism, one simply updates the tensor paral-lelism equation to be L \u2192 L/p. This implies that the total communication volume here is additive."}, {"title": "IV. SYSTEM SETUP", "content": "This section explains the experiments run, and insights gained from our results. All experiments were run on the OLCF Frontier supercomputer. See Table II for more informa-tion on hardware and software specifics. For details on Frontier compute node topology, please refer to Figure 6. Regarding the use of Microsoft's DeepSpeed: we would like to note that communication/compute overlap is not possible when logging is turned on, which allowed us to obtain communication results featured in Section V with the following profiling numbers.\nTo facilitate easier training of the models involved, we utilize EleutherAI's \u201cGPT-NeoX", "enwik8": "ataset used features a vocabulary size of 50304 after padding to help with reducing performance runtime anomalies."}, {"title": "V. PERFORMANCE CHARACTERIZATION", "content": "Here, we explore the communication behavior of different Data-Parallel schemes such as pure data parallelism or dif-ferent levels of DeepSpeed's ZeRO[28]. Per the cost models referenced in Section III, DDP and ZeRO-1 and 2 should approximately achieve a volume proportional to twice the parameter count, and ZeRO-3 should achieve a communication volume equal to three times that of the parameter count."}, {"title": "1) Breakdown of Communication Volume: ZeRO differences", "content": "Figure 8 shows communication breakdowns of each selected model size using one of ZeRO-1/2/3 (run on one node for all models except the 13B-parameter model due to memory errors. The models, as shown later still accurately hold up regardless of scale for a given model size). We want to note that Broadcast is included as a notion to the start-of-training parameter broadcast/distribution required, as this still incurs a level of overhead during initialization. Allreduce is still a significant portion of the communication in ZeRO-1/2 thanks to the fact that, aside from the 13B-parameter model, all other models can easily fit onto one of Frontier's MI250X GPUs with DDP. We would also like to note the general trend of decreasing broadcast impact as the model size increases, and this is also shown in Figure 7, where each breakdown is them modeled as a percentage of the total communication volume."}, {"title": "2) Breakdown of Message Sizes and Frequency", "content": "As the model size increases, more message sizes for each communication call will be utilized, and to varying frequency levels. Figure 9 showcases 2-Node, 8 GCDs/Node experiments for 19-million, 1.3-billion, and 13-billion parameter models while using ZeRO-3. More verbose logging from DeepSpeed shows how message sizes get grouped into different categories for different functions; in the case of the 1.3-billion parameter model, many of the smaller messages (on the order of kilo-bytes) are used for parameter exchange among each process. Larger messages from 10s to 100s of megabytes are used for gradient aggregation (instead of an Allreduce as done in pure data parallelism). The main takeaway: Even though DL models such as LLMs operate using massive message sizes, optimizations at smaller message sizes should be treated as equally important."}, {"title": "3) Comparison to Performance Model", "content": "Figure 10 shows how the 19M, 125M, 1.3B, and 13B-parameter models match up to the predicted communication volumes based on the Data-Parallel and ZeRO-based formulas from Section III. In general, our prediction aligns well with the communication volume observed across all model sizes and all parallelism schemes (DDP, ZeRO-1/2/3). Note that we are able to predict 13B communication volume under a Distributed Data-Parallel scenario but training parameters will exceed worker memory in action, causing an OOM error."}, {"title": "B. Model Parallelism Communication Volume Analysis (Tensor and Pipeline)", "content": "This section explores the differing communication behaviors for tensor/pipeline parallelism and a combination of them in parallel (model parallelism)."}, {"title": "1) Breakdown of Communication Volume", "content": "Figures 12 shows how differing levels of tensor and pipeline parallelism can affect communication volume. The first im-mediate observation is the domination of Allgather operations despite the use of point-to-point operations in any config-uration utilizing a mix of pipeline and tensor parallelism. Only pure pipeline parallelism avoids this with the next-largest bottleneck being calls to Allreduce2. Returning to the figures in Section I-A we noted that pipeline parallelism has an interesting anomaly: the receive op-eration is the only one to suffer from cold-cache performance,"}, {"title": "2) Comparison to Performance Model", "content": "Figure\u00b3 134 shows how the 19M, 125M, 1.3B, and 13B-parameter models perform and match up to the predicted communication volumes based on the Tensor and Pipeline Parallelism formulas from Section III. Here, we are primarily interested in the send/receive volume (pipeline parallelism-related) and/or Allreduce communication (tensor parallelism)."}, {"title": "C. Sequence Length Experiments", "content": "This section examines how sequence length impacts com-munication behavior for Data-Parallel and Model-Parallel en-vironments. Experiments here were all run on 2 Nodes, 8 GCDs/Node with the 1.3B-parameter model.\nFigure 14a shows the Allgather communication volume (where applicable) for both data and model parallelism. To reduce redundancy, we will note that this does not change across increasing sequence length values, from 512 to 4096 or higher. However, we do note that optimizations and sequence length do have an impact on throughput. Figure 14b shows how different levels of ZeRO impact throughput. While we see an approximate 2-2.5x increase in TFlops per GPU, ZERO optimizations will more often than not result in a decrease of flops for the given sequence length.\nCompared to data parallelism and ZeRO, there is more variation in the \"key\" components tensor/pipeline/model par-allelism. While pure tensor parallelism makes sole use of Allreduce, pure pipeline parallelism and model parallelism make use of point-to-point operations as well, and contrary to the above, these volumes increase with token size (see Sections III and V-B). Figure 15b shows an approximate doubling/slightly-larger-than-2x increase in communication volume with increasing sequence-length values while Figure 15a directly shows a 2x increase with increasing sequence-length values. Similar to the data-parallel results, we also see an increase in throughput as shown in figure 15c. For brevity, we only show when we have two pipeline stages or a tensor parallelism value of two. Ultimately, the use of tensor parallelism will allow for a higher TFLOP-per-GPU count over pipeline parallelism (up to almost 2x more), though this has an inverse relationship with point-to-point communication (where applicable as pure tensor parallelism does not use point-to-point) in communication volume."}, {"title": "VII. CONCLUSIONS", "content": "We have presented a characterization of LLM communi-cation behavior on the Frontier supercomputer. This has been done by combining a rigorous performance model for multiple parallelism schemes and multiple experiments utilizing cur-rent state-of-the-art training frameworks with precise profiling of communication and compute. We have provided insights into potential optimizations for communication middleware for small-message communication. For future pending work, given that the Frontier system represents one combination, we would like to examine further parallelism schemes here such as multi-dimensional parallelism and expert parallelism. We would also like to examine how all the schemes presented here might change on current and upcoming systems with new or maturing communication and software stacks such as Aurora at Argonne National Lab (Intel GPUs and Intel CPUs) or the upcoming Vista cluster at the Texas Advanced Computing Center (NVIDIA Grace Hopper)."}]}