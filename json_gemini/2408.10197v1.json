{"title": "Demystifying the Communication Characteristics for Distributed Transformer Models", "authors": ["Quentin Anthony", "Benjamin Michalowicz", "Jacob Hatef", "Lang Xu", "Mustafa Abduljabbar", "Aamir Shafi", "Hari Subramoni", "Dhabaleswar K. (DK) Panda"], "abstract": "Deep learning (DL) models based on the transformer architecture have revolutionized many DL applications such as large language models (LLMs), vision transformers, audio generation, and time series prediction. Much of this progress has been fueled by distributed training, yet distributed communication remains a substantial bottleneck to training progress. This paper examines the communication behavior of transformer models that is, how different parallelism schemes used in multi-node/multi-GPU DL Training communicate data in the context of transformers. We use GPT-based language models as a case study of the transformer architecture due to their ubiquity. We validate the empirical results obtained from our communication logs using analytical models. At a high level, our analysis reveals a need to optimize small message point-to-point communication further, correlations between sequence length, per-GPU throughput, model size, and optimizations used, and where to potentially guide further optimizations in framework and HPC middleware design and optimization.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) such as ChatGPT [1], Gemini [2], and Llama [3] are revolutionizing multiple industries with their ability to perform a range of tasks from customer service to creative content generation. LLMs are typically pre-trained with internet-scale, pre-processed data that allows them to learn the intricacies of human languages. After pre-training, LLMs undergo a fine-tuning process in a supervised setting that allows them to excel in downstream tasks like generation, summarization, translation, and question/answering. Modern LLMs utilize a large number of parameters that imply increased computational and memory requirements during training. A higher number of parameters allows the model to capture more intricate relationships and nuances in language, leading to improved performance on a range of downstream tasks.\nAs an LLM's size increases, training requires a large number of GPUs for a considerable amount of time on modern HPC systems, and it is significantly bottlenecked by how quickly data can be exchanged between parallel training processes. Here, the messaging stack including the communication fabric plays a pivotal role. At large scales, such a bottleneck leads to lower Model FLOPs Utilization (MFU) [4] for training. For instance, MegaScale [5] reports a 55.2% MFU on 12,288 GPUs for training a 175-billion parameter model. To emphasize this point, Figures 1 and 2 show how communication begins to dominate computation at increasing scales for 13-billion and 20-billion parameter GPT-2-based models. We are motivated by this to conduct a thorough characterization study to understand the communication stage during LLM training.\nGood communication performance is critical for scaling LLM training on large HPC systems. This paper aims to study and analyze communication strategies used by state-of-the-art Deep Learning (DL) training frameworks on leading-class supercomputers. Our objective is to learn the volume of data"}, {"title": "II. BACKGROUND", "content": "The current trend in Natural Language Processing (NLP) favors transformer models [16] for their exceptional accuracy and computational efficiency. The original transformer architecture is designed for machine translation and contains two main components: an Encoder and a Decoder. Modern adaptations of transformers for language modeling utilize either the Encoder or Decoder depending on the specific task, such as BERT [17] and GPT-2 [18].\nA transformer layer is structured with a self-attention block followed by a two-layer multi-layer perceptron (MLP), composed of two GEMMs and a GeLU non-linearity (ReLU for the original version [16]). Each encoder or decoder block includes multiple such layers, each featuring multi-head attention, MLP, normalization, and residual connections.\nWe consider a single encoder or decoder with multiple transformer layers. Initially, input tokens are processed through a word embedding table and combined with positional embeddings, resulting in a 3-D tensor of size (sequence length x micro-batch size \u00d7 hidden dimension) [19]. Each transformer layer processes this tensor through a self-attention block with multiple attention heads and a two-layer MLP that quadruples the hidden size and then reduces it back. The output size remains consistent across layers, and the final output is projected back to the vocabulary dimension for cross-entropy loss calculation.\nLarger models are more sample-efficient given a fixed compute budget [20], [21], leading to a massive increase in model parameter count. Training billion/trillion-parameter transformer models is a memory-intensive task since it requires efficient distribution of multiple training parameters (model weights, optimizer states, gradients, and activations).\nIn Data Parallelism [22], a training mini-batch is divided among multiple workers and each worker maintains a full model replica. Data parallelism can achieve near-linear scaling in training data throughput by increasing the mini-batch size in proportion to the number of available workers. Typically, an Allreduce on all the workers is required to synchronize the gradients before updating the model weights on each local replica. Data Parallelism is communication-bound since the achievable bandwidth and latency of the Allreduce greatly affect iteration time given a worker's memory is consumed by the model and other training parameters. However, data parallelism requires that model size must fit in the limited GPU memory and additional optimizer and hyper-parameter tuning to ensure convergence with large global batch size [23].\nPipeline Parallelism mainly focuses on distributing layers of models among GPU workers and executes these layers in a pipeline order. Since activation computation relies on dependencies between different layers, inevitable GPU idle times, known as pipeline bubbles are present in this paradigm, there have been various research efforts in reducing such bubbles [24], [25]. In terms of communication, pipeline parallelism involves point-to-point GPU communication to pass along activations between layers.\nTensor Parallelism [26] aims at exploiting the inherent parallelism inside GEMM operations and distribute these computations along specific directions (rows, columns) and use synchronization among workers to gather the results, thus ensuring correctness. State-of-the-art implementations distribute the MLP blocks and Self-Attention blocks [26]. Results are collected and aggregated using Allreduce and Allgather. It is a common practice to limit tensor parallelism degree within a compute node since intra-node bandwidth is typically larger than inter-node bandwidth [27].\nData parallel training requires each rank to hold a copy of all model optimizer states, gradients, and parameters. [28] Zero Redundancy Optimizer (ZeRO) reduces memory constraints by removing redundant information, and partitioning model data across data parallel ranks. ZeRO is divided into three stages, ZeRO-1, ZeRO-2, and ZeRO-3. Given a certain de-"}, {"title": "III. PERFORMANCE MODEL", "content": "This section breaks down each component that makes up our performance model.\nTo calculate the total parameters in a transformer, we have the embedding and unembedding blocks of size V \u00d7 h each. If embedding and unembedding parameters are tied (i.e. shared), this leads to a total of V \u00d7 h parameters from embeddings. Since all configurations in this paper use untied embeddings, we have 2V \u00d7 h embedding parameters. We also have the position embeddings of size sh. The attention matrices are four separate matrices of dimension h \u00d7 h, leading to 4h2 attention parameters per layer. Multilayer perceptron (MLP) blocks for our models are composed of two fully-connected linear projections of size h \u00d7 xh and xh \u00d7 h, where x is the expansion factor. For GPT-NeoX model architectures, the conventional projection factor is 4 [31], so we have 2xh2 = 8h2 MLP parameters per layer. We then have a layernorm each layer with both gains and biases on each of the Q, K, V and the first MLP linear projection, leading to 8h layernorm parameters per layer. Finally, we add the final layernorm of size 2h to get a total number of parameters in Equation 1 below.\nConsidering a message size of m, the communication volume for the Allreduce collective is 2 \u00d7 m($\\frac{d-1}{d}$). The communication volume for Allgather, Reduce_scatter, and Reduce is simply m($\\frac{d-1}{d}$).\nThe communication volume per iteration for distributed data parallelism (DDP) just comes from the gradient Allreduce, which gives the total volume per iteration given in Equation 2 below. ZeRO-1 and ZeRO-2 simply replace this Allreduce call with separate Reduce_scatter and Allgather calls [28], so they have the same communication volume as DDP. Therefore, the communication volume (in units of parameters) from DP (Allreduce), ZeRO-1, and ZeRO-2 (Allgather/Reduce_scatter) is given by:\n$2 * param_count * (\\frac{d-1}{d})$\nThe communication volume for ZeRO-3 is 50% higher due to an extra Allgather of parameters, which is necessary before the forward pass because parameters are now also sharded across ranks (See II-C and [28]). Therefore, the ZeRO-3 communication volume (in units of parameters) is given by:\n$3 * param_count * (\\frac{d-1}{d})$\nThe communication volume for pipeline parallelism comes from the point-to-point communication of forward activations and backward gradients. The send or receive between two pipeline stages is of size bsh, therefore the aggregate communication volume across all stages in a single training iteration is given in Equation 4 below (in units of parameters and where d is the number of devices, or GPUs, used in training). Notably, the first stage doesn't have to receive activations and the last GPU doesn't have to send activations (and vice-versa with gradients), so we multiply by p 1 instead of p.\n$2bsh \u00d7 (p-1)$\nThe communication volume per iteration for tensor parallelism comes from 6 Allreduce operations per layer (2 in the forward pass, 2 for activation recomputation, 2 in the backward pass). Further, an additional Allreduce operation is performed at the embedding. Each Allreduce incurs a volume of 2m, leading to a total of (12L+2) volume for messages of size bsh. Since these Allreduce operations are across t ranks, they're multiplied by a factor of $\\frac{t-1}{t}$.\n$(12L + 2) * bsh * (\\frac{t-1}{t})$\nFor 3D parallelism, one simply updates the tensor parallelism equation to be L \u2192 L/p. This implies that the total communication volume here is additive."}, {"title": "IV. SYSTEM SETUP", "content": "This section explains the experiments run, and insights gained from our results. All experiments were run on the OLCF Frontier supercomputer. See Table II for more information on hardware and software specifics. For details on Frontier compute node topology, please refer to Figure 6. Regarding the use of Microsoft's DeepSpeed: we would like to note that communication/compute overlap is not possible when logging is turned on, which allowed us to obtain communication results featured in Section V with the following profiling numbers.\nTo facilitate easier training of the models involved, we utilize EleutherAI's \u201cGPT-NeoX", "enwik8": "ataset used features a vocabulary size of 50304 after padding to help with reducing performance runtime anomalies."}, {"title": "V. PERFORMANCE CHARACTERIZATION", "content": "Here, we explore the communication behavior of different Data-Parallel schemes such as pure data parallelism or different levels of DeepSpeed's ZeRO[28]. Per the cost models referenced in Section III, DDP and ZeRO-1 and 2 should approximately achieve a volume proportional to twice the parameter count, and ZeRO-3 should achieve a communication volume equal to three times that of the parameter count.\nFigure 8 shows communication breakdowns of each selected model size using one of ZeRO-1/2/3 (run on one node for all models except the 13B-parameter model due to memory errors. The models, as shown later still accurately hold up regardless of scale for a given model size). We want to note that Broadcast is included as a notion to the start-of-training parameter broadcast/distribution required, as this still incurs a level of overhead during initialization. Allreduce is still a significant portion of the communication in ZeRO-1/2 thanks to the fact that, aside from the 13B-parameter model, all other models can easily fit onto one of Frontier's MI250X GPUs with DDP. We would also like to note the general trend of decreasing broadcast impact as the model size increases, and this is also shown in Figure 7, where each breakdown is then modeled as a percentage of the total communication volume.\nAs the model size increases, more message sizes for each communication call will be utilized, and to varying frequency levels. Figure 9 showcases 2-Node, 8 GCDs/Node experiments for 19-million, 1.3-billion, and 13-billion parameter models while using ZeRO-3. More verbose logging from DeepSpeed shows how message sizes get grouped into different categories for different functions; in the case of the 1.3-billion parameter model, many of the smaller messages (on the order of kilobytes) are used for parameter exchange among each process. Larger messages from 10s to 100s of megabytes are used for gradient aggregation (instead of an Allreduce as done in pure data parallelism). The main takeaway: Even though DL models such as LLMs operate using massive message sizes, optimizations at smaller message sizes should be treated as equally important.\nFigure 10 shows how the 19M, 125M, 1.3B, and 13B-parameter models match up to the predicted communication volumes based on the Data-Parallel and ZeRO-based formulas from Section III. In general, our prediction aligns well with the communication volume observed across all model sizes and all parallelism schemes (DDP, ZeRO-1/2/3). Note that we are able to predict 13B communication volume under a Distributed Data-Parallel scenario but training parameters will exceed worker memory in action, causing an OOM error.\nThis section explores the differing communication behaviors for tensor/pipeline parallelism and a combination of them in parallel (model parallelism).\nFigures 12 shows how differing levels of tensor and pipeline parallelism can affect communication volume. The first immediate observation is the domination of Allgather operations despite the use of point-to-point operations in any configuration utilizing a mix of pipeline and tensor parallelism. Only pure pipeline parallelism avoids this with the next-largest bottleneck being calls to Allreduce.\nReturning to the figures in Section I-A we noted that pipeline parallelism has an interesting anomaly: the receive operation is the only one to suffer from cold-cache performance,\nThis section examines how sequence length impacts communication behavior for Data-Parallel and Model-Parallel environments. Experiments here were all run on 2 Nodes, 8 GCDs/Node with the 1.3B-parameter model.\nFigure 14a shows the Allgather communication volume (where applicable) for both data and model parallelism. To reduce redundancy, we will note that this does not change across increasing sequence length values, from 512 to 4096 or higher. However, we do note that optimizations and sequence length do have an impact on throughput. Figure 14b shows how different levels of ZeRO impact throughput. While we see an approximate 2-2.5x increase in TFlops per GPU, ZERO optimizations will more often than not result in a decrease of flops for the given sequence length.\nCompared to data parallelism and ZeRO, there is more variation in the \"key\" components tensor/pipeline/model parallelism. While pure tensor parallelism makes sole use of Allreduce, pure pipeline parallelism and model parallelism make use of point-to-point operations as well, and contrary to the above, these volumes increase with token size (see Sections III and V-B). Figure 15b shows an approximate doubling/slightly-larger-than-2x increase in communication volume with increasing sequence-length values while Figure 15a directly shows a 2x increase with increasing sequence-length values. Similar to the data-parallel results, we also see an increase in throughput as shown in figure 15c. For brevity, we only show when we have two pipeline stages or a tensor parallelism value of two. Ultimately, the use of tensor parallelism will allow for a higher TFLOP-per-GPU count over pipeline parallelism (up to almost 2x more), though this has an inverse relationship with point-to-point communication (where applicable as pure tensor parallelism does not use point-to-point) in communication volume."}, {"title": "VII. CONCLUSIONS", "content": "We have presented a characterization of LLM communication behavior on the Frontier supercomputer. This has been done by combining a rigorous performance model for multiple parallelism schemes and multiple experiments utilizing current state-of-the-art training frameworks with precise profiling of communication and compute. We have provided insights into potential optimizations for communication middleware for small-message communication. For future pending work, given that the Frontier system represents one combination, we would like to examine further parallelism schemes here such as multi-dimensional parallelism and expert parallelism. We would also like to examine how all the schemes presented here might change on current and upcoming systems with new or maturing communication and software stacks such as Aurora at Argonne National Lab (Intel GPUs and Intel CPUs) or"}]}