{"title": "AutoMIR: Effective Zero-Shot Medical Information Retrieval without Relevance Labels", "authors": ["Lei Li", "Xiao Zhou", "Xiangxu Zhang", "Zheng Liu"], "abstract": "Medical information retrieval (MIR) is essential for retrieving relevant medical knowledge from diverse sources, including electronic health records, scientific literature, and medical databases. However, achieving effective zero-shot dense retrieval in the medical domain poses substantial challenges due to the lack of relevance-labeled data. In this paper, we introduce a novel approach called Self-Learning Hypothetical Document Embeddings (SL-HyDE) to tackle this issue. SL-HyDE leverages large language models (LLMs) as generators to generate hypothetical documents based on a given query. These generated documents encapsulate key medical context, guiding a dense retriever in identifying the most relevant documents. The self-learning framework progressively refines both pseudo-document generation and retrieval, utilizing unlabeled medical corpora without requiring any relevance-labeled data. Additionally, we present the Chinese Medical Information Retrieval Benchmark (CMIRB), a comprehensive evaluation framework grounded in real-world medical scenarios, encompassing five tasks and ten datasets. By benchmarking ten models on CMIRB, we establish a rigorous standard for evaluating medical information retrieval systems. Experimental results demonstrate that SL-HyDE significantly surpasses existing methods in retrieval accuracy while showcasing strong generalization and scalability across various LLM and retriever configurations. CMIRB data and evaluation code are publicly available at: https://github.com/CMIRB-benchmark/CMIRB", "sections": [{"title": "1 INTRODUCTION", "content": "Medical information retrieval (MIR) [7, 22] focuses on retrieving relevant medical information from extensive data sources, such as electronic health records, scientific papers, and medical knowledge databases, based on specific medical queries. Its applications are wide-ranging, supporting doctors in clinical decision-making [30], assisting patients in finding health information [24], and aiding researchers in accessing relevant studies [46].\nIn information retrieval (IR), there are two mainstream paradigms: lexical-based sparse retrieval methods, such as BM25 [28], and embedding-based dense retrieval approaches [4, 10, 41]. Dense retrievers have shown strong performance, particularly in domains where large annotated datasets are available [13]. These datasets are typically created by manually matching queries with relevant documents, a process that becomes impractical when dealing with collections containing millions or billions of items. As a result, many retrieval tasks face the challenge of having limited or no in-domain labeled examples.\nGiven these challenges, several studies [13, 41] have explored transfer learning for zero-shot retrieval, where dense retrievers are trained on large, high-resource datasets and evaluated on queries from different domains. MS MARCO [1], a dataset containing a substantial number of manually annotated query-document pairs, is commonly used for this purpose. An alternative to transfer learning is contrastive learning [6], which follows a two-stage pretraining and fine-tuning pipeline to develop general-purpose text embedding models. The pretraining stage leverages weakly supervised data gathered through large-scale web crawling, while the fine-tuning stage uses high-quality text pairs derived from data mining or manual annotation [16, 40]. However, the availability of such large-scale datasets or high-quality text pairs cannot always be assumed, particularly in non-English languages or specialized domains.\nRecently, large language models (LLMs) have demonstrated exceptional performance in zero-resource retrieval scenarios, primarily due to their extensive knowledge and robust text generation capabilities [11, 29, 36]. This makes them particularly effective in situations where labeled data is scarce or unavailable. One such approach, HyDE (Hypothetical Document Embeddings)[5], employs zero-shot prompts to guide an instruction-following language model to generate hypothetical documents, effectively narrowing the semantic gap between the query and the target document. Similarly, Query2doc[36] uses few-shot prompting of LLMs to generate pseudo-documents, which are then used to expand the original query. However, applying these methods to medical information retrieval presents three critical challenges:"}, {"title": "2 RELATED WORK", "content": "In recent years, with rapid advancements in deep learning and natural language processing (NLP) technologies, researchers in the field of Information Retrieval (IR) have increasingly focused on leveraging advanced text representation methods to improve retrieval system performance. Contriever [10] leverages unsupervised contrastive learning for dense retrieval. BGE [40] enhances Chinese general embeddings through training on large-scale text pairs. GTE [16] employs multi-stage contrastive learning for multilingual applications. mE5 [35] uses weakly supervised pretraining and fine-tuning for multilingual embeddings. BMRETRIEVER [42] specializes in English biomedical retrieval. These works demonstrate the impact of well-structured training strategies on effective retrieval across domains. Beyond embedding-based techniques, large language models (LLMs) have demonstrated exceptional performance in zero-resource retrieval scenarios, primarily due to their extensive knowledge and robust text generation capabilities. GAR [23] enriches query semantics with generated content. HyDE [5] generates hypothetical documents for the retriever, effectively narrowing the semantic gap between the query and the target document."}, {"title": "3 METHODOLOGY", "content": "Zero-shot document retrieval is a crucial component of the search systems. Given a user query q and a document set D = {d1, ..., dn} where n represents the number of document candidates, the goal of a retrieval model (Mr) is to fetch documents that align with the user's genuine search intent for the current query q. These models map an input query q and a document d into a pair of vectors <vq, vd>, using their inner product as a similarity function s(q, d):\ns(q, d) =< Mr(q), Mr(d) >\n(1)\nThe retrieval models then identify the top-k documents, denoted as D, which have the highest similarity scores when compared to the query q. Earlier studies [4, 13] utilized dual-tower architectures to encode queries and documents separately. However, more recent research [10, 40, 41] indicates that using the same architecture for both query and document encoding provides greater robustness in low-resource information retrieval tasks. In this work, we utilize the Mt model to serve as both the query encoder and the document encoder."}, {"title": "3.2 Hypothetical Document Embedding (HyDE)", "content": "Large language models have achieved remarkable success in text generation across various natural language processing tasks, including question answering [18], text generation [3], and dialogue systems. Recently, there has been a growing interest in utilizing these models to generate relevant documents based on queries, thereby improving retrieval accuracy. Hypothetical Document Embeddings (HyDE) [5] decompose dense retrieval into two tasks: a generative task executed by an instruction-following language model and a document-document similarity task executed by a retrieval model. Initially, the query is fed to the generative model with the instruction to \"write a document that answers the question,\" creating a hypothetical document. Specifically, a large language model such as ChatGPT or InstructGPT can serve as the generative model (Mg). By deploying prompts, the user query is supplied, and the model generates the hypothetical documents:\nd' = Mg(q, Prompt1)\n(2)\nTo better fuse the documents, they sample N documents [d'1, ..., d'N] from generated hypothetical documents. Subsequently, an unsupervised contrastive encoder (such as Contriever) is used to encode these documents into an embedding vector vq.\nUq = \\frac{1}{N+1} [M_r(q) + \\sum_{k=1}^N M_r(d'_k)]\n(3)\nThe inner product is then computed between M_r(d) and the set of all document vectors:\ns(q, d) =< vq, Mr(d) > Vd \u2208 D\n(4)\nThis vector identifies a neighborhood in the corpus embedding space, from which similar real documents are retrieved based on vector similarity. This straightforward plug-and-play paradigm makes HyDE easy to deploy in real-world applications for solving retrieval tasks."}, {"title": "3.3 SL-HyDE", "content": "Applying HyDE to the medical domain presents two primary challenges: (1) LLMs often lack specialized medical domain knowledge, and (2) retrievers may struggle to effectively encode medical texts due to inadequate training on medical corpora. These challenges hinder the successful implementation of HyDE technology in the medical field, making it difficult to achieve significant performance improvements in retrieval tasks. A common strategy to supplement medical domain knowledge involves fine-tuning with labeled medical data [33, 38, 42, 45]. However, these approaches rely on high-quality, manually constructed data to adapt general models to the medical domain. Unfortunately, obtaining such high-quality labeled data in practice is particularly challenging, making the training of a medical LLM highly costly.\nIn this paper, we introduce a Self-Learning Hypothetical Document Embedding (SL-HyDE) mechanism designed to leverage the potential of unlabeled corpora in the medical domain. SL-HyDE consists of two main modules: a self-learning generator and a self-learning retriever. The overall framework is depicted in Figure 1."}, {"title": "3.4 Self-Learning Generator", "content": "A common strategy for equipping a large model with medical domain knowledge involves continued pretraining on unlabeled corpora, supervised fine-tuning on labeled data, and reinforcement learning to align the model with human preferences. However, continued pretraining typically requires substantial computational resources and extensive datasets. Consequently, supervised fine-tuning with smaller datasets has gained popularity as an efficient method for adapting large models. In this paper, we utilize the generator and retriever in SL-HyDE to create a labeled dataset based on an unsupervised corpus. The labels are entirely generated by SL-HyDE, eliminating the need for external labeled data collection.\nWe begin with an unlabeled corpus, specifically utilizing a medical encyclopedia dataset, Huatuo26M_encyclopedia\u00b9, as our foundational text. To construct queries, we employ a robust offline LLM, Qwen2.5\u00b2, leveraging in-context learning [2, 25]. This approach generates meaningful queries in a specific format, using prompts such as: \"Please generate a medical question based on the provided medical text. Demonstration: [DEMONSTRATION] Text: [TEXT]\".\nq = LLM(d, Prompt2)\n(5)\nAfter generating the query, we employ the generator to produce a hypothetical document that encapsulates the correct information based on the true target document. We intentionally avoid using the true target document as the output label because the primary role of the generator is to craft a hypothetical document that aids the retriever in locating the true target document. Expecting the generator to replicate the exact target document itself would be overly demanding and unrealistic.\n{d\u2081, ...d'K} = Mg(q, d, Prompt3)\n(6)\nConsidering that the hypothetical documents generated by the generator may not always be ideal for retrieving the true target document. Ideally, the optimal hypothetical document is one that enhances the ranking position of the target document during retrieval. To achieve this, we employ the retriever Mr to rank and select the optimal hypothetical document. Specifically, the generator creates K hypothetical documents for a given query, and each hypothetical document is utilized by the retriever to search the corpus for the most relevant documents. We then calculate the ranking position ri of the true target document for d':\nri = rank(d, Mr(d'_i, D)), i = 1, ..., K\n(7)\nd* = d'_{arg \\underset{k}{min} r_i}\n(8)\nThe higher the ranking, the more effective the hypothetical document. We select the generated document with the highest rank (i.e., the lowest ri). At this stage, we have constructed a question-answer pair in the form of (q, d*), where the query serves as the question, and the generated document acts as the answer. We then employ a supervised fine-tuning approach to train the generator using this dataset, Dllm = {(q, d*)|q \u2208 Q}. The standard supervised fine-tuning (SFT) loss is calculated as follows:\nLsft = - \\sum_{q \\in Q} \\sum_t log P(q_t|q_{<t>})\n(9)\nFor the training data of the self-learning generator, there is no need to rely on supervision signals from labeled medical data. Instead, we utilize only unlabeled corpora, leveraging the generator and retriever within the SL-HyDE framework to create the supervision signals. More importantly, the self-learning generator trained"}, {"title": "3.5 Self-Learning Retriever", "content": "The training of retrievers typically involves two stages: unsupervised pre-training and supervised fine-tuning. Unsupervised pre-training aims to harness large volumes of unlabeled text to learn domain knowledge, while supervised fine-tuning employs high-quality labeled datasets, often alongside hard negative mining, to further teach the model to differentiate between similar texts. For example, GTE [16] utilizes ~800M text pairs during the unsupervised pre-training stage and ~3M pairs for fine-tuning. In this work, our goal is not to train a medical retriever with vast amounts of data but rather to adapt a general-purpose retriever to the medical domain through fine-tuning with a small amount of data.\nNow, we have a passage from the corpus D and its corresponding query q. This < q, d > combination already constitutes the query-document labeled data necessary for retriever fine-tuning. However, since HyDE retrieves documents by encoding both the query and the hypothetical document to locate the target document, we propose using a < (q, d'), d > triplet as the labeled data for retriever training. This method's advantage lies in bridging the gap between the training and inference stages for the retriever, ensuring consistency in data format across both processes, thereby enhancing retriever performance.\nIn addition to the unlabeled medical corpus, we now have a corresponding query. This (q, d) combination already forms the query-document labeled data required for retriever training. However, considering that HyDE retrieves documents by simultaneously encoding both the query and the hypothetical document to locate the target document, we propose using a (q, d', d) triplet as the labeled data for retriever training. This method narrows the gap between the training and inference stages for the retriever, ensuring the two stages operate under the same data format.\nTo achieve this, we utilize the fine-tuned generator Mg from the previous stage to generate hypothetical documents for all queries, constructing a labeled fine-tuning dataset Demb = {(q,d',d >)}. Following previous research [15, 40], we enhance the complexity of the training data through hard negative mining using the retriever, which creates challenging negative examples that closely resemble the queries. The hard negative dataset D\u00af is mined from the original corpus D, employing the ANN-style sampling strategy in [41]:\nD\u00af = ANN(Mr (q, d'))\n(10)\nIn addition to the negatives mined from the corpus, we also incorporate in-batch negatives. We then apply contrastive learning loss for supervised fine-tuning of the retriever, with the objective function formulated as follows:\nLemb = min \\sum_{(q,d)} -log \\frac{e^{s(q,d)/\\tau}}{e^{s(q,d)/\\tau} + \\sum_{D^-}e^{s(q,d^-)/\\tau}}\n(11)\nwhere t is the temperature coefficient, and B represents the negative samples within the batch. The similarity score s(q, d) incorporates the generated document, as described in Equation 4.\nThus far, we have developed a retriever equipped with medical domain knowledge that is coherently adapted to the characteristics"}, {"title": "3.6 SL-HyDE vs. HyDE", "content": "Our approach, SL-HyDE, builds upon HyDE with several enhancements while retaining some similarities. First, both SL-HyDE and HyDE follow the same inference process for information retrieval tasks. Each uses a large model to generate a hypothetical document based on the query, which the retriever then employs to locate the most relevant document. Second, neither SL-HyDE nor HyDE requires labeled data, which allows for rapid deployment. HyDE is especially advantageous in real-world scenarios where efficient retrieval can be executed simply by selecting a generator and a retriever. However, for tasks needing domain-specific knowledge, such as medical information retrieval, deploying HyDE directly may not yield optimal results. One potential strategy is to fine-tune the generator and retriever separately using labeled medical data before deploying the HyDE framework. The primary challenge here is in acquiring labeled data, and fine-tuning the models separately often leads to suboptimal performance.\nSL-HyDE improves upon this by integrating a self-learning mechanism, transforming HyDE into a trainable end-to-end framework. This mechanism enables both the generator and the retriever to better adapt to the medical domain without relying on labeled medical data. Supervision signals for the generator's training are derived from the retriever, and vice versa, facilitating mutual enhancement through this self-learning process. This holistic approach results in improved performance in retrieval tasks. Overall, SL-HyDE offers an efficient and convenient solution for enhancing HyDE's performance in the medical domain, particularly when dealing with unlabeled corpora."}, {"title": "4 CMIRB BENCHMARK", "content": "The CMIRB benchmark is a specialized multi-task dataset designed specifically for medical information retrieval. Our collection and construction methodology is guided by the following principles: (i) Domain Specificity: The benchmark focuses exclusively on medical tasks, providing a targeted evaluation setting. (ii) Task Diversity: It includes a variety of query types and document formats to reflect real-world medical scenarios. (iii) Task Difficulty: The tasks are crafted to challenge existing models, thereby pushing the boundaries of medical retrieval research."}, {"title": "4.1 Task Definition", "content": "Figure 2 provides an overview of the tasks and datasets available in CMIRB. The benchmark comprises the following five task types: Medical Knowledge Retrieval: Retrieve relevant medical knowledge snippets from textbooks or encyclopedias based on a given medical entity query.\nMedical Consultation Retrieval: Extract relevant doctor's responses to online medical consultation questions posed by patients, simulating the retrieval of expert medical advice.\nMedical News Retrieval: Focus on retrieving news articles that address queries related to COVID-19.\nMedical Post Retrieval: Retrieve the content of a forum post corresponding to its title."}, {"title": "4.2 Data Construction", "content": "The data corpus for CMIRB is partially sourced from existing datasets and supplemented by high-quality data collected from relevant medical websites.\nFor the medical knowledge retrieval task, we develop three datasets: the medical exam retrieval dataset MedExam, the medical encyclopedia knowledge retrieval dataset DuBaike, and the medical disease knowledge retrieval dataset DXYDisease. For MedExam, the queries and corpus are derived from the MEDQA(MCMLE) test dataset and MedTextbook(MCMLE) [12]. We concatenate the multiple-choice question and options as the query. For DuBaike, the queries are derived from the DuReader dataset [8], while the documents consist of medical knowledge pages from Baidu Baike\u00b3. These queries are generated by users while searching on Baidu Search or Baidu Zhidao. For DXYDisease, we collect question-document pairs from the online disease knowledge website DingXiangYuan\u2074. The questions consist of pre-constructed inquiries about various aspects of diseases, such as definitions and symptoms.\nFor the medical consultation retrieval task, we collect three online patient-doctor consultation datasets: MedicalRetrieval, CmedqaRetrieval, and DXYConsult. The MedicalRetrieval dataset is sourced from [20], and the CmedqaRetrieval dataset comes from [27]. Additionally, we compile several doctor-patient dialogues from the DingXiang Yuan, to create DXYConsult. Compared to the former two datasets, the query in DXYConsult includes more detailed patient information, such as symptoms, medication usage, and pending diagnosis questions.\nFor the medical news retrieval task, we utilize the CovidRetrieval dataset from [27]. This dataset features manually annotated high-quality questions related to COVID-19 as queries, with corresponding news articles as the documents.\nIn the medical post retrieval task, we curate IIYiPost by crawling posts from the IIYi forum, a platform where medical professionals discuss various medical topics. This dataset includes both post titles and the full post content.\nFor the medical literature retrieval task, we develop two datasets based on the CSL data [14]: the citation retrieval dataset CSLCite and similar literature retrieval dataset CSLRel. To construct CSLCite, we utilize journal titles from CSL as queries and extract their cited references from WanFangMedical as positive documents. Similarly, we select the most similar paper recommended by the website as the positive documents for the CLSRel dataset.\nTo ensure dataset quality, we design a filtering process, as outlined in Algorithm 1. We use ChatGPT to exclude non-medical data and remove low-quality query-document pairs. For the MedExam and DuBaike datasets, beyond the initial filtering, we implement a query-document matching process to find the most similar positive document for each query, detailed in Appendix A.1. After processing, we compile a total of 10 datasets. The statistical details of these datasets are presented in Table 1. Statistics reveal that query lengths vary significantly, ranging from a single title to a full article. Similarly, document lengths span from brief physician responses to extensive medical knowledge texts. This diversity ensures that our benchmark is both comprehensive and practically meaningful."}, {"title": "5 EXPERIMENTS", "content": "The experimental results for various retrieval models, including SL-HyDE, on the CMIRB benchmark are presented in Table 2. We make the following observations.\n(1) BM25 remains highly competitive in specific retrieval scenarios. As a lexical retriever, it ranks documents based on TF-IDF matching scores calculated between queries and documents. Despite underperforming on the overall CMIRB benchmark, it displays strong results in tasks like medical news retrieval (78.9 vs. 73.33 for BGE) and medical post retrieval (66.95 vs. 67.13 for BGE). This can be attributed to the higher keyword overlap in these datasets, which aligns with BM25's strengths.\n(2) No single retrieval model achieves optimal performance across all tasks. PEG and GTE each deliver the best performance on four datasets, while BGE and mE5 each excel in achieving the top results on one dataset. Dense models with better performance often utilize contrastive learning, pretraining on large-scale unlabeled data followed by fine-tuning on labeled data. Variations in training data distribution influence model effectiveness across different datasets, suggesting the need for specialized approaches.\n(3) SL-HyDE consistently outperformed HyDE across all ten datasets. While HyDE shows slight overall improvements over BGE, it excels in medical knowledge retrieval but underperforms in medical consultation tasks. This discrepancy could be due to LLM's stronger handling of encyclopedia-type knowledge compared to the nuanced domain of patient-doctor consultations. In contrast, SL-HyDE achieved improvements over HyDE in all tasks, owing to its self-learning mechanism, which effectively enhances medical knowledge integration within both the generator and the retriever."}, {"title": "5.3 Performance Analysis", "content": "The core components of SL-HyDE are the generative LLM and the retrieval model. In this section, we explore the effects of varying these components."}, {"title": "6 CONCLUSIONS", "content": "In this paper, we introduce an automated framework for zero-shot medical information retrieval, named SL-HyDE, which operates without the need for relevance labels. Utilizing an unlabeled medical corpus, we employ a self-learning, end-to-end training framework where the retriever guides the generator's training, and the generator, in turn, enhances the retriever. This process integrates medical knowledge to create hypothetical documents that are more effective in retrieving target documents. Furthermore, we present a comprehensive Chinese medical information retrieval benchmark, evaluating mainstream retrieval models against this new standard.\nExperimental findings demonstrate that SL-HyDE consistently improves retrieval accuracy over HyDE across ten datasets. Additionally, SL-HyDE shows strong adaptability and scalability, effectively enhancing retrieval performance across various combinations of generators and retrievers."}]}