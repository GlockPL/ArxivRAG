{"title": "On Learning Action Costs from Input Plans", "authors": ["Marianela Morales", "Alberto Pozanco", "Giuseppe Canonaco", "Sriram Gopalakrishnan", "Daniel Borrajo", "Manuela Veloso"], "abstract": "Most of the work on learning action models focus on learning the actions' dynamics from input plans. This allows us to specify the valid plans of a planning task. However, very little work focuses on learning action costs, which in turn allows us to rank the different plans. In this paper we introduce a new problem: that of learning the costs of a set of actions such that a set of input plans are optimal under the resulting planning model. To solve this problem we present LACFIP, an algorithm to learn action's costs from unlabeled input plans. We provide theoretical and empirical results showing how LACFIP can successfully solve this task.", "sections": [{"title": "Introduction", "content": "Classical planning is the task of choosing and organizing a sequence of deterministic actions such that, when applied in a given initial state, it results in a goal state (Ghallab, Nau, and Traverso 2004). Most planning works assume the actions' dynamics or domain model, i.e., how actions change the state, are provided as an input and turn the focus to the efficient synthesis of plans. This is a strong assumption in many real-world planning applications, where domain modeling is often a challenging task (Kambhampati 2007). Motivated by the difficulty of crafting action models, several works have tried to automatically learn domain models from input observations (Yang, Wu, and Jiang 2007; Gregory and Lindsay 2016; Arora et al. 2018; Aineto, Celorrio, and Onaindia 2019; Gragera et al. 2023; Garrido 2023). Although they make different assumptions on the type of observations (full or partial plan, access to intermediate states, noisy observations, etc.), most works solely focus on learning the actions' dynamics but not their associated cost.\nIn this paper we argue that learning action costs is as important as learning the actions' dynamics. While actions' dynamics allow us to determine the validity of traces in a domain model, the actions costs allow us to get the quality of each of these traces, which is needed whenever we want to generate good plans. Moreover, there are many real-world planning applications where the actions' dynamics are known, but their cost is either unknown and we aim to learn it from scratch; or approximate and we aim to refine it. In both cases we can use data in the form of observed plans to acquire this knowledge.\nConsider the case of a navigation tool that suggests routes to drivers. In this domain the actions' dynamics are clear: cars can move through different roads and taking an action (i.e., taking an exit) will change the car's position. The navigation tool will typically aim to generate the route with the shortest driving time, i.e., the least costly or optimal plan. To do that, it makes some assumptions about the cost of each action (driving times): for example, being a function of the distance. While this can be a good proxy, it can be further refined by observing the actual plans coming from users of the navigation tool. By observing these plans we cannot only get more accurate driving times, but also understand which routes users prefer and adjust the model accordingly. Financial planning is yet another example where we have access to many plans, actions' dynamics are known, but properly estimating their cost for different people is crucial and challenging. Pozanco et al. (2023) aim to generate realistic financial plans by maximizing their likelihood. To do that, they assign lower costs to more likely actions, i.e., saving $5 in memberships is less costly than increasing the salary by $1,000. Pozanco et al. mention that these costs can be given or inferred from data but do not provide further details on how to do it. Like in the navigation tool case, here we could gather observed plans on how users are saving and spending money to achieve their financial goals. By doing this, we could more accurately assign costs to each action so as to generate plans that better align with user preferences.\nIn this paper we introduce a new problem: that of learning the costs of a set of actions such that a set of input plans are optimal under the resulting planning model. We formally prove that this problem does not have a solution for an arbitrary set of input plans, and relax the problem to accept solutions where the number of input plans that are turned optimal is maximized. We also present variations of this problem, where we guarantee input plans are the only optimal plans allowed; or we try to minimally modify an existing cost function instead of learning it from scratch. We then introduce LACFIPk, a common algorithm to solve these tasks. Empirical results across different planning domains show how LACFIPk can be used in practice to learn (or adapt)"}, {"title": "Preliminaries", "content": "A classical planning tasks can be defined as follows:\nDefinition 1 (STRIPS planning task). A STRIPS planning task can be defined as a tuple P = (F, A, I,G, C), where F is a set of fluents, A is a set of actions, I \u2286 F is an initial state, G \u2286 F is a goal state, and C : A \u2192 \u2115\u207a is the cost function that associates a cost to each action.\nA states cF is a set of fluents that are true at a given time. With S we refer to all the possible states defined over F. Each action a \u2208 A is described by a set of preconditions PRE(a), add effects ADD(a), delete effects DEL(a), and cost c(a). An action a is applicable in a state s iff PRE(a) \u2286 s. We define the result of applying an action in a state as \u03b3(s, a) = (s \u2216 DEL(a)) \u222a ADD(a).\nA sequence of actions \u03c0 = (a\u2081, ..., a\u2099) is applicable in a state s\u2080 if there are states (s\u2081, ..., s\u2099) such that a\u1d62 is ap- plicable in s\u1d62\u208b\u2081 and s\u1d62 = \u03b3(s\u1d62\u208b\u2081, a\u1d62). The resulting state after applying a sequence of actions is \u0393(s, \u03c0) = s\u2099, and c(\u03c0) = \u2211\u1d62\u208c{1,...,n} c(a\u1d62) denotes the cost of \u03c0. A state s \u2208 S is reachable iff there exists a sequence of actions \u03c0 applicable from I such that s \u2286 \u0393(I, \u03c0). With SR \u2286 S we refer to the set of all reachable states of the planning task. A sequence of actions is simple if it does not traverse the same state s \u2208 SR more than once. A plan \u03c0\u2081 is a subset of a plan \u03c0\u2c7c iff the sequence of actions \u03c0\u2081 = (a\u2081, ..., a\u2099) is contained in the sequence of actions that conform \u03c0\u2c7c. We denote this condition as \u03c0\u2081 \u2282 \u03c0\u2c7c.\nThe solution to a planning task P is a plan, i.e., a sequence of actions such that G \u2286 \u0393(I, \u03c0). We denote as \u03a0(P) the set of all simple solution plans to planning task P. Also, given a plan \u03c0, we denote its alternatives, i.e., all the other sequence of actions that can solve P as \u03a0' = \u03a0(P) \u2216 \u03c0.\nDefinition 2 (Optimal plan). A plan \u03c0 optimally solves a planning task P iff its cost is lower or equal than that of the rest of alternative plans solving P:\n$$c(\\pi) \\leq c(\\pi'), \\quad \\forall \\pi' \\in \\Pi$$\nWe will use the boolean function IS_OPTIMAL(\u03c0, P) to evaluate whether \u03c0 optimally solves P (1) or not (0)."}, {"title": "Learning Action Costs from Input Plans", "content": "We are interested in learning the costs of a set of actions such that the input plans are optimal under the resulting planning model. The underlying motivation is that by aligning the action's costs to the input plans, the new model will be able to generate new plans that better reflect the observed behavior. Initially, we assume actions do not have an associated cost a priori. To accommodate this, we extend the potential values that a cost function can have, to include empty values C : A \u2192 \u2115\u207a \u222a {\u2205}. We denote C = \u2205 iff \u2200a \u2208 A, c(a) = \u2205. We then formally define a cost function learning task as follows:\nDefinition 3 (Cost Function Learning Task). A cost function learning task is a tuple CFL = \u27e8T, M\u27e9 where:\n\u2022 T = (P\u2081, ..., P\u2099) is a sequence of planning tasks that share F, A, and C = \u2205.\n\u2022 M = (\u03c0\u2081, ..., \u03c0\u2099) is a corresponding sequence of simple plans that solves T.\nThe solution to a CFL task is a common cost function (common across all tasks and plans) C.\nLet us examine the problem definition. We assume we have access to the full plan \u03c0\u1d62 as well as the planning task P\u1d62 that it solves. However, unlike previous works on action's cost learning (Gregory and Lindsay 2016; Garrido 2023), we do not require to know the total cost of each plan. Moreover, we also assume that all the planning tasks and plans share the same vocabulary, i.e., they have a common set of fluents F and actions A. We also assume the common cost function is initially unknown. Finally, we do not make any assumption on the initial and goal state of the input tasks and plans. The above assumptions are not restrictive and hold in many real-world applications such as the ones described in the Introduction. For example, in the navigation scenario F and A will remain constant as long as the city network (map) does not change, which will only occur when a new road is built. In this domain we will get input plans with different starting points (I) and destinations (G), which is supported by our problem definition. One could argue that in the navigation scenario it is trivial to annotate each plan with its actual duration. While this might be true, it is clearly not so for other applications where action's cost capture probabilities or user preferences, such as financial planning.\nGoing back to Definition 3, we purposely left open the characterization of a CFL solution, only restricting it to be a common cost function C shared by all the input tasks. We did this because we are interested in defining different solution concepts depending on the properties the cost function C should have. In the next subsections we formalize different solutions to cost function learning tasks."}, {"title": "Turning All the Input Plans Optimal", "content": "The first objective we turn our attention to is trying to find a common cost function under which all the input plans are optimal. We refer to such solutions as Ideal Cost Functions.\nDefinition 4 (Ideal Cost Function). Given a CFL task, we define an ideal cost function ICF that solves it as a common cost function C under which all the plans in M are optimal. The quality of an ideal cost function is defined as follows:\n$$\\min \\sum_{a \\in A} c(a),$$\ns.t. \u2211\u1d62\u2091\u209c,\u2098 IS_OPTIMAL(\u03c0\u1d62, P\u1d62 = (F, A, I\u1d62, G\u1d62, C)) = |M|,(3)\nAn ICF is optimal iff no other cost function C' yields a lower value in Equation (2) while satisfying Constraint (3).\nIdeal Cost Functions are not guaranteed to exist for arbitrary CFL tasks, since Constraint (3) cannot always be satisfied. The difficulty lies in the inter dependencies and potential conflicts between plans. In particular, this not only refers to the actions shared between the input plans, but also to the multiple alternative plans that an input plan can have, i.e., all the other sequence of actions that can achieve G from I."}, {"title": "Remark 1", "content": "Note that since we are considering only simple plans, the set of alternative plans \u03a0'\u1d62 that a plan \u03c0\u1d62 \u2208 M can have is finite.\nWe can, now, show that there is no guarantee that an ICF solution exists for a CFL task.\nTheorem 1. Given a CFL task, it is not guaranteed that there exists an ICF solution.\nProof. Given a CFL = \u27e8T, M\u27e9 where T = (P\u1d62,P\u2c7c), and M = (\u03c0\u1d62, \u03c0\u2c7c), let us assume that an ICF solution exists. Let us also assume that there are alternative plans \u03c0'\u1d62 \u2208 \u03a0'\u03c0\u1d62 and \u03c0'\u2c7c \u2208 \u03a0'\u03c0\u2c7c for \u03c0\u1d62 and \u03c0\u2c7c respectively, such that \u03c0'\u2c7c \u2282 \u03c0\u1d62 and \u03c0'\u1d62 \u2282 \u03c0\u2c7c. Since \u03c0\u1d62 is optimal, from Definition 2 we have:\nc(\u03c0\u1d62) < c(\u03c0'\u1d62) (4)\nFrom the assumption \u03c0'\u2c7c \u2282 \u03c0\u1d62, there is at least one more action in \u03c0\u1d62 than in \u03c0'\u2c7c. Moreover, since the minimum cost for each action is 1, then:\nc(\u03c0'\u2c7c) < c(\u03c0\u1d62) (5)\nSince \u03c0\u2c7c is also optimal, again by Definition 2:\nc(\u03c0\u2c7c) < c(\u03c0'\u2c7c) (6)\nFrom (4), (5) and (6) we have\nc(\u03c0\u2c7c) < c(\u03c0'\u2c7c)\nc(\u03c0\u1d62) < c(\u03c0'\u1d62) < c(\u03c0\u2c7c) < c(\u03c0'\u2c7c) (7)\nHowever, from \u03c0'\u1d62 \u2282 \u03c0\u2c7c, as the case above, there are at least one more action in \u03c0\u2c7c, we have\nc(\u03c0'\u1d62) < c(\u03c0\u2c7c) (8)\nFrom (7) and (8) we get a contradiction from the assumption that there exists an ICF."}, {"title": "Remark 2", "content": "Observe that the optimality of a plan \u03c0\u1d62 \u2208 M only depends on the costs of the actions (a\u2081, ..., a\u2099) \u2208 A occurring in \u03c0\u1d62 or in \u03a0'\u1d62. The costs assigned to the rest of the actions do not affect \u03c0\u1d62's optimality."}, {"title": "Maximizing the Optimal Input Plans", "content": "Given that ICFs are not guaranteed to exist for every CFL task, we now relax the solution concept and focus on cost functions that maximize the number of plans turned optimal. We refer to such solutions as Maximal Cost Functions.\nDefinition 5 (Maximal Cost Function). Given a CFL task, we define a maximal cost function MCF that solves it as a common cost function C under which a maximum number of plans in M are optimal. We formally establish the quality of a maximal cost function as follows:\n$$\\max \\sum_{i \\in T,M} IS_OPTIMAL(\\pi_i, P_i = (F, A, I_i, G_i, C)),$$\n$$\\min \\sum_{a \\in A} c(a),$$ (10)\nA maximal cost function MCF is optimal iff no other cost function C' yields a higher value in Equation (9), and, if tied, a lower value in Equation (10).\nWhile ICFs are not guaranteed to exist, it is easy to show that there is always a MCF that solves a CFL task. This is because even a cost function under which none of the plans in M are optimal would be a valid MCF solution. One might think that we can go one step further and ensure that there is a trivial cost function C guaranteeing that at least one of the plans in M will be optimal in the resulting model. This trivial cost function would consist on assigning a cost k to all the actions in one of the plans \u03c0\u1d62 \u2208 M, and a cost k + |SR| to the rest of the actions in A. Unfortunately, this is not always the case. In particular, if the input plan contains redundant actions (Nebel, Dimopoulos, and Koehler 1997; Salerno, Fuentetaja, and Seipp 2023), i.e., actions that can be removed without invalidating the plan, then it is not possible to turn \u03c0\u2081 optimal."}, {"title": "Making Input the Only Optimal Plans", "content": "Up to now we have explored the problem of turning a set of input plans optimal. We have showed that this task does not always have a solution when we want to make all the plans in M optimal (ICF), and therefore, we focused on maximizing the number of plans that are optimal under the resulting cost function (MCF). These solutions still allow for the existence of other alternative plans with the same cost. In some cases, we might be interested in a more restrictive setting, where only the input plans are optimal. In other words, there is no optimal plan \u03c0\u2081 such that \u03c0\u2081 \u2209 M. This can be the case of applications where we are interested in generating optimal plans that perfectly align with the user preferences, preventing the model from generating optimal plans outside of the observed behavior. For this new solution we are still focusing on maximizing the number of plans in M that are optimal, but now we want them to be the only optimal ones. We formally define this new solution as follows:\nDefinition 6 (Strict Cost Function). Given a CFL task, we define a strict cost function SCF as a common cost function C under which a maximum number of plans in M are the only optimal plans.\nThe quality of a strict cost function that solves a CFL task is defined as in the case of MCF (Definition 5). However, there is now a difference in Definition 2 which defines an optimal plan (used in the boolean function IS_OPTIMAL(\u03c0, P) in Equation (9)). The condition for a plan's cost being lower than or equal to (\u2264) the cost of its alternative plans is re- placed, in the strict approach, by a condition of being strictly lower than (<). Similarly to MCF, there is always a SCF that solves a CFL task, and we cannot guarantee the existence of an ideal cost function for SCF. In particular, Theorem 1 also applies to the SCF solution, with the exception that the definition of an optimal plan has changed as specified earlier."}, {"title": "Adapting an Existing Cost Function", "content": "In practice we might already have an approximate cost function C that we would like to refine with observed plans, rather than learning a cost function from scratch as we have focused on so far. In other words, the cost function C in the sequence of planning task T is not empty as in Definition 3. We then introduce a new task with initial costs as follows:\nDefinition 7 (Cost Function Refinement Task). A cost function refinement task is a tuple CFL\u00a9 = \u27e8T, M\u27e9 where:\n\u2022 T = (P\u2081, ...,P\u2099) is a sequence of planning tasks that share F, A, and C = C.\n\u2022 M = (\u03c0\u2081, ..., \u03c0\u2099) is a corresponding sequence of simple plans that solves T.\nThe solution to a CFL\u0108 task is a common cost function C.\nWe denote with MCFC and SCFC when we have MCF and SCF solutions respectively, but for a CFLC task. We then formally redefine the quality of MCFC and SCFC solutions by slightly modifying Definition 5. In this case, we change the secondary objective of minimizing the sum of actions' costs (Equation (10)) to minimizing the difference between the solution cost function C and the approximate cost function re- ceived as input C (Equation (11) below):\n$$\\min \\sum_{a \\in A} |c(a) - \\tilde{c}(a)|$$\nwhere c(a) refers to the cost of each action given by the initial cost function C. As before, it is easy to see that MCFC (SCF) solutions have the same properties as their MCF (SCF) counterparts, i.e., there is always a cost function C that solves CFLC task, but we cannot guarantee the exis- tence of a C that makes all the plans in M optimal."}, {"title": "Summary", "content": "Let us summarize the different tasks we have presented so far (and their solutions) with the example illustrated in Table 1. The first row of the table shows the CFL and CFLC tasks containing 6 states labeled with letters. The actions, depicted with edges, consist on moving between two connected states. There are two input plans M = (\u03c0\u2081, \u03c0\u2082) such that \u03c0\u2081 = [(move A B), (move B D)] and \u03c0\u2082= [(move A C), (move C E), (move E F)]. Colored states indicate they are an initial or goal state for one of the input plans.\nThe next rows depict optimal solutions for each of the two tasks. The MCF solution ensures both input plans are optimal, while assigning the minimum cost to each action (1) (Definition 5). The SCF solution guarantees that the two input plans are the only optimal alternatives under the returned cost function C. That solution would also turn all the plans optimal under the MCF definition, but would be suboptimal as it has a higher sum of action's costs. This is because the cost of (move C D) and (move D F) need to be increased from 1 to 2 in order to force that there are no optimal plans outside M. On the right side, the approximate cost function C is minimally modified to guarantee that the input plans are optimal. In the case of MCFC, this is achieved by reducing by one the cost of (move A C) and (move E F). In the case of SCFC, we need to decrease the cost of (move C E) and (move E F), and increase the cost of (move D F) to 2 in order to force the stricter SCF\u014c requirement."}, {"title": "Solving Cost Function Learning Tasks", "content": "Algorithm 1 describes LACFIPk, an algorithm to learn action's costs from input plans. It receives the cost function learning (refinement) task to solve (T), the desired solution (S), and a parameter k that determines the number of alternative plans to be computed for each plan \u03c0\u1d62 \u2208 M. Higher k values indicate a higher percentage of \u03a0'\u1d62 is covered, with k = \u221e meaning that the whole set of alternative plans is computed. With these two inputs LACFIPk first generates a Mixed-Integer Linear Program (MILP) to assign a cost to each action, i.e., to compute the solution cost function C. This MILP is shown below (Equations 12-17) to compute MCF solutions for CFL tasks. MILPs for the other solutions and tasks are similar and can be found in the Appendix.\n$$\\max \\limits_{\\vec{\\pi}} \\omega_1 \\sum_{\\pi \\in M} x_{\\pi} - \\omega_2 \\sum_{a \\in A_M} y_a$$\ns.t. \u03a3Ya \u2264 \u03a3Ya + M(1 \u2212 2\u03c0,\u03c0\u03b1), \u03c0 \u0395 \u039c, \u03c0\u00b2 \u2208 \u03a0\u201d (13)\n\u03b1\u0395\u03c0\n|\u03a0*|\u2212 \u03a3 \u2248\u03c0,\u03c0\u03b1 \u2264 M(1 \u2212 x\u03c0), \u03c0\u2208 \u039c\n\u03c0\u03b1 \u0395\u03a0\u03c0\n\u03a7\u03c0\u2208 {0,1}, \u03c0\u0395\u039c (15)\nYa \u2265 1, a \u2208 AM\n\u0396\u03c0,\u03c0\u03b1 \u2208 {0,1}, \u03c0\u03b5 \u039c, \u03c0\u00b2 \u0395\u03a0\nWe have three sets of decision variables. The first, x, are binary decision variables that will take a value of 1 if plan \u03c0 is optimal in the resulting domain model, and 0 otherwise (Equation (15)). The second, 2\u03c0,\u03c0\u03b1 are binary decision vari- ables that will take a value of 1 if plan has a lower or equal cost than alternative plan \u03c0\u00aa, and 0 otherwise (Equa- tion (17)). Finally, ya are integer decision variables that assign a cost to each action a \u2208 AM. This subset of actions AM \u2282 A represents the actions which cost needs to be set in order to make the plans in M optimal (see Remark 2). Constraint (13) enforces the value of the z variables. This is done by setting M to a large number, forcing 2\u03c0,\u03c0\u00aa to be 1 iff c(\u03c0) = \u03a3\u03b1\u03b5\u03c0 \u03a5\u03b1 \u2264 \u03a3\u03b1\u03b5\u03c0\u03b1 Ya = c(\u03c0\u00b2). Similarly, Con- straint (14) ensures that x = 1 iff \u03c0 has a lower or equal cost than the rest of its alternative plans.\nWe aim to optimize the objective function described in Equation (12), where we have two objectives. The first ob- jective, weighted by w\u2081, aims to maximize the number of optimal plans. The second objective, weighted by w2, aims to minimize the total cost of the actions in the learned model. As described in Algorithm 1, LACFIPk will first try to maxi- mize the number of plans that can be turned optimal by set- ting the weights to w\u2081 = 1 and w2 = 0 (line 2). Solving the MILP with these weights will give us a cost function C that makes Q plans optimal. Then, LACFIPk generates a new MILP with a new constraint (line 5), enforcing that the new solution has to turn exactly Q plans optimal. This second MILP is solved with w\u2081 = 0 and w2 = 1 in order to find the optimal cost function C, which makes Q plans optimal and minimizes the sum of action's costs. After that, LACFIPk updates the cost function C (line 7) by assigning a cost to the actions the MILP does not reason about, i.e., the actions A\u2216AM that do not affect the optimality of the plans in M. For example, in the case of MCF solutions, this function assigns the minimum cost (1) to these actions. This updated cost function C is finally returned by LACFIPk as the solu- tion to the cost function learning task. LACFIPk's optimality proof can be found in the Appendix."}, {"title": "Evaluation", "content": "Experimental Setting\nBenchmark. We ran experiments in four planning do- mains: BARMAN, OPENSTACKS, TRANSPORT and GRID, which is a navigation domain where an agent can move in the four cardinal directions to reach its desired cell. We chose them since we wanted to get a representative yet small set of domains. For each domain, we fix the problem size (F and A) and generate 50 different problems by varying I and G. For BARMAN, OPENSTACKS and TRANSPORT we use Seipp, Torralba, and Hoffmann (2022)'s PDDL generator, while for GRID we randomly generated different prob- lems by changing the initial and goal state of the agent. The problem sizes were chosen to allow computation of multiple alternatives in reasonable time. For example, the GRID size is 10 \u00d7 10, and BARMAN tasks have 3 ingredients, 3 cock- tails and 4 shots. Then, we use SYMK (Speck, Mattm\u00fcller, and Nebel 2020) to compute 100 simple plans for each planning task Pi. This represents our pool of 50 \u00d7 100 = 5000 tuples (Pi, \u03c0i) per each domain. We generate CFL tasks by randomly selecting 10\u00b9, 10\u00b2, or 10\u00b3 of these tuples from the pool. For each CFL task size we generate 10 random prob- lems, i.e., 10 different sets of (Pi, \u03c0i) tuples, therefore having a total of 3 \u00d7 10 = 30 CFL tasks tasks per each domain. The same tasks are transformed into CFL\u1ee0 tasks by using the cost function in the original planning task as C.\nApproaches and Reproducibility. We evaluate LACFIPk on the above benchmark. In particular, we run it with four different input values: k = 10\u00b9, k = 10\u00b2, k = 10\u00b3, and k = 10\u2074. All the versions use SYMK to compute the set of k alternative plans, and solve the resulting MILPs using the CBC solver (Forrest and Lougee-Heimer 2005). We compare LACFIPk against BASELINE, an algorithm that assigns either (i) the minimum cost (1) to all the actions when solv- ing CFL tasks; or (ii) the cost prescribed by the approximate cost function when solving CFLC. Both algorithms have been implemented in Python, and leverage FAST DOWN- WARD (Helmert 2006) translator to get the grounded actions of a planning task. Experiments were run on AMD EPYC 7R13 CPUs @ 3.6Ghz with a 8GB memory bound and a total time limit of 1800s per algorithm and cost function learning task.\nResults\nWe only report here results when computing MCF solutions for CFL tasks due to space constraints. Results for the other tasks and solution concepts can be found in the Appendix.\nPerformance Analysis. Table 2 presents the results of our experiments. Each domain contains 10 CFL tasks of varying sizes: 10\u00b9, 10\u00b2 and 10\u00b3, as displayed in the second row of the Table. These are solved by our five algorithms (first column of the table). The cells display the ratio of plans made op- timal, represented by the mean and standard deviation, for all problem instances commonly solved by at least one al- gorithm. This ratio is computed by using the cost function C returned by each algorithm and using it to solve each of the planning tasks Pi \u2208 T. In the case of MCF, we verify that optimally solving Pi with C yields the same cost as the sum of action's costs of the input plan, i.e., \u03a3\u03b1\u03b5\u03c0, c(\u03b1). When this holds, input plan \u03c0i optimally solves task Pi and we annotate a 1. Otherwise, we annotate a 0 meaning that C does not make \u03c0i optimal. Similar validation checks are con- ducted for the other tasks and solution concepts, and their details can be checked in the Appendix. Cells shaded in gray indicate that the given algorithm failed to solve any of the 10 CFL tasks of that size. The remaining cells with values are color-coded to indicate the ratio of optimal plans achieved: lighter colors represent a lower ratio of optimal plans, while darker colors indicate a higher ratio of optimal plans. This color gradient provides a visual representation of each algo- rithm's effectiveness in turning input plans optimal across different problem sizes and domains.\nWe identify two main trends. Firstly, LACFIPk is consis- tently better than the baseline, and its performance tends to improve as k increases. This improvement is not necessarily monotonic, as we can see in GRID where k = 10\u00b3 obtains the best results. This is expected, since with any k value other than \u221e, the MILP is not considering all the alterna- tives and might be leaving out some of the important ones, i.e., those that can affect the input's plan optimality. The ra- tio of plans turned optimal seems to saturate with low k val- ues, suggesting that in some domains we might not need to compute many alternatives to achieve good results. As the planning tasks grow, LACFIPk scales worse, with CFL tasks in BARMAN or TRANSPORT where it cannot produce solu- tions within the time bound when k > 10\u00b2. Secondly, as we increase the size of the CFL tasks, more conflicting prob- lems with redundant actions can arise, reducing the ratio of plans that can be made optimal. For example, in BARMAN, we can see a gradient in color from left to right as the size of the CFL tasks increases. In OPENSTACKS and TRANS- PORT, the problems are even more complex, and SYMK fails to compute alternative plans within the time bound."}, {"title": "Execution Time Analysis", "content": "Figure 1 illustrates the total execution time (log scale) of each algorithm as we increase the CFL task size. Executions exceeding 1800s are shown above the dashed line. As we can see, BASELINE's execution time remains constant as the CFL size increases, being able to re- turn cost functions in less than 10s in all cases. On the other hand, LACFIPk takes more time as more plans need to be turned optimal. For example, while it can solve all but two CFL task of size 10\u00b9 in GRID, 12 time out when |CFL| = 10\u00b2. Increasing k is the factor that affects LACFIPk the most, as we can see in the linear (logarithmic) increase in the execution time regardless of the CFL size and domain.\nFinally, we analyzed how each component of LACFIPk af- fects the total running time. For lower values of k, SYMK can compute the alternative plans in few seconds, and most of the running time is devoted to the MILP. On the other hand, when k \u2265 10\u00b3, computing the alternative plans takes most of the time, with problems where LACFIPk spends the 1800s running SYMK with no available time to run the MILP."}, {"title": "Related Work", "content": "Automated Planning\nMost planning works on domain learning focus on acquiring the action's dynamics (preconditions and effects) given a set of input plans (Yang, Wu, and Jiang 2007; Cresswell, McCluskey, and West 2013; Aineto, Celorrio, and Onaindia 2019). These works usually overlook the task of learning the action's cost model, as their main interest is learning to gen- erate valid rather than good plans as we do.\nNLOCM (Gregory and Lindsay 2016) and LVCP (Garrido 2023) are two notable exceptions in the literature, being able to learn both the action's dynamics and cost model. Al- though differing in the input assumptions and guarantees, both approaches require each plan to be annotated with its total cost. Then, they use constraint programming to assign a cost to each action such that the sum of their costs equals the cost of the entire plan. Our work differs from them in three main aspects. First, we focus on learning the action's cost model, while NLOCM and LVCP can also learn the action's dynamics. Second, we do not require to know the total cost of each input plan, which is a restrictive assumption in many cases. Unlike them, LACFIPk can learn the action costs with minimal knowledge, i.e., just a property shared by the in- put traces, such as optimality. Finally, these approaches have been mainly evaluated on syntactic metrics such as precision and recall of the generated model against the ground truth. This way of evaluating domain learning success has been lately criticized (Behnke and Bercher 2024; Garrido and Gragera 2024), as these metrics do not capture the seman- tic relationship between the original and learned models. In our case, we prove both theoretically and empirically how LACFIPk can learn how to generate optimal plans from the input traces. The resulting models can then be used to solve novel planning tasks in a way that matches the user pref- erences, i.e., the observed plans. The same spirit of learn- ing or improving a model in order to generate better plans, i.e., plans that matches the real-world/user preferences, is present in (Lanchas et al. 2007), where they aim to learn the actions' duration from the execution of a plan. They use relational regression trees to acquire patterns of the states that affect the actions duration. While they do not provide any theoretical guarantees on the resulting models, we for- mally prove that the input plans are optimal under the new cost function. Moreover, LACFIPk only focuses on the plans rather than in the intermediate states to learn C."}, {"title": "Inverse Reinforcement Learning", "content": "Inverse Reinforcement Learning (IRL) (Ng, Russell et al. 2000) is the task of inferring the reward function of an agent given its observed behavior. We can establish a relationship between learning action costs and IRL by assuming that (i) the input plans are observations of that agent acting in the environment; and (ii) action costs are the reward function we try to learn. However, IRL differs from learning action costs in few aspects. It is defined over Markov Decision Processes (MDPs), while we learn the action costs in the context of classical planning. Most approaches assume experts' obser- vations aim at optimizing a single reward function (i.e., sin- gle goal), which is in stark contrast with our setting where every trace may be associated with a different goal. Among the closest works to our setting in the IRL literature we have Choi and Kim (2012) and Michini and How (2012). How- ever, the first one assumes trajectories coming from experts belong to clusters each one with a different underlying re- ward structure, and the second one assumes reward func- tions can be represented as the combination of simple sub- goals. In our setting, no such assumptions are required."}, {"title": "Conclusions and Future Work", "content": "We have introduced a new problem: that of learning the costs of a set of actions such that a set of input plans"}]}