{"title": "On Learning Action Costs from Input Plans", "authors": ["Marianela Morales", "Alberto Pozanco", "Giuseppe Canonaco", "Sriram Gopalakrishnan", "Daniel Borrajo", "Manuela Veloso"], "abstract": "Most of the work on learning action models focus on learning the actions' dynamics from input plans. This allows us to specify the valid plans of a planning task. However, very little work focuses on learning action costs, which in turn allows us to rank the different plans. In this paper we introduce a new problem: that of learning the costs of a set of actions such that a set of input plans are optimal under the resulting planning model. To solve this problem we present LACFIP, an algorithm to learn action's costs from unlabeled input plans. We provide theoretical and empirical results showing how LACFIP can successfully solve this task.", "sections": [{"title": "Introduction", "content": "Classical planning is the task of choosing and organizing a sequence of deterministic actions such that, when applied in a given initial state, it results in a goal state (Ghallab, Nau, and Traverso 2004). Most planning works assume the actions' dynamics or domain model, i.e., how actions change the state, are provided as an input and turn the focus to the efficient synthesis of plans. This is a strong assumption in many real-world planning applications, where domain modeling is often a challenging task (Kambhampati 2007). Motivated by the difficulty of crafting action models, several works have tried to automatically learn domain models from input observations (Yang, Wu, and Jiang 2007; Gregory and Lindsay 2016; Arora et al. 2018; Aineto, Celorrio, and Onaindia 2019; Gragera et al. 2023; Garrido 2023). Although they make different assumptions on the type of observations (full or partial plan, access to intermediate states, noisy observations, etc.), most works solely focus on learning the actions' dynamics but not their associated cost.\nIn this paper we argue that learning action costs is as important as learning the actions' dynamics. While actions' dynamics allow us to determine the validity of traces in a domain model, the actions costs allow us to get the quality of each of these traces, which is needed whenever we want to generate good plans. Moreover, there are many real-world planning applications where the actions' dynamics are known, but their cost is either unknown and we aim to learn it from scratch; or approximate and we aim to refine it. In both cases we can use data in the form of observed plans to acquire this knowledge.\nConsider the case of a navigation tool that suggests routes to drivers. In this domain the actions' dynamics are clear: cars can move through different roads and taking an action (i.e., taking an exit) will change the car's position. The navigation tool will typically aim to generate the route with the shortest driving time, i.e., the least costly or optimal plan. To do that, it makes some assumptions about the cost of each action (driving times): for example, being a function of the distance. While this can be a good proxy, it can be further refined by observing the actual plans coming from users of the navigation tool. By observing these plans we cannot only get more accurate driving times, but also understand which routes users prefer and adjust the model accordingly. Financial planning is yet another example where we have access to many plans, actions' dynamics are known, but properly estimating their cost for different people is crucial and challenging. Pozanco et al. (2023) aim to generate realistic financial plans by maximizing their likelihood. To do that, they assign lower costs to more likely actions, i.e., saving $5 in memberships is less costly than increasing the salary by $1,000. Pozanco et al. mention that these costs can be given or inferred from data but do not provide further details on how to do it. Like in the navigation tool case, here we could gather observed plans on how users are saving and spending money to achieve their financial goals. By doing this, we could more accurately assign costs to each action so as to generate plans that better align with user preferences.\nIn this paper we introduce a new problem: that of learning the costs of a set of actions such that a set of input plans are optimal under the resulting planning model. We formally prove that this problem does not have a solution for an arbitrary set of input plans, and relax the problem to accept solutions where the number of input plans that are turned optimal is maximized. We also present variations of this problem, where we guarantee input plans are the only optimal plans allowed; or we try to minimally modify an existing cost function instead of learning it from scratch. We then introduce LACFIPk, a common algorithm to solve these tasks. Empirical results across different planning domains show how LACFIPk can be used in practice to learn (or adapt)"}, {"title": "Preliminaries", "content": "A classical planning tasks can be defined as follows:\nDefinition 1 (STRIPS planning task). A STRIPS planning task can be defined as a tuple P = (F, A, I,G, C), where F is a set of fluents, A is a set of actions, I \u2286 F is an initial state, G \u2286 F is a goal state, and C : A \u2192 N\u207a is the cost function that associates a cost to each action.\nA states \u2286 F is a set of fluents that are true at a given time. With S we refer to all the possible states defined over F. Each action a \u2208 A is described by a set of preconditions PRE(a), add effects ADD(a), delete effects DEL(a), and cost c(a). An action a is applicable in a state s iff PRE(a) \u2286 s. We define the result of applying an action in a state as \u03b3(s, a) = (s \\ DEL(a)) \u222a ADD(a).\nA sequence of actions \u03c0 = (a1,..., an) is applicable in a state so if there are states (81. ..., Sn) such that a\u017c is applicable in si-1 and si = y(Si\u22121, ai). The resulting state after applying a sequence of actions is \u0393(s, \u03c0) = Sn, and c(\u03c0) = \u2211i={1,...,n} c(ai) denotes the cost of \u03c0. A state s\u2208 S is reachable iff there exists a sequence of actions \u03c0 applicable from I such that s \u2286 \u0393(\u0399, \u03c0). With SR \u2286 S we refer to the set of all reachable states of the planning task. A sequence of actions is simple if it does not traverse the same state s \u2208 SR more than once. A plan \u03c0\u2081 is a subset of a plan \u03c0; iff the sequence of actions \u03c0\u2081 = (a1, ..., an) is contained in the sequence of actions that conform \u03c0;. We denote this condition as \u03c0\u2081 \u2282 \u03c0j.\nThe solution to a planning task P is a plan, i.e., a sequence of actions such that G \u2286 \u0393(\u0399, \u03c0). We denote as II(P) the set of all simple solution plans to planning task P. Also, given a plan \u03c0, we denote its alternatives, i.e., all the other sequence of actions that can solve P as II\" = II(P) \\ \u03c0.\nDefinition 2 (Optimal plan). A plan \u03c0 optimally solves a planning task P iff its cost is lower or equal than that of the rest of alternative plans solving P:\n```latex\nc(\u03c0) \u2264 c(\u03c0'), \u2200\u03c0' \u2208 \u03a0\u03c0\n```\nWe will use the boolean function IS_OPTIMAL(\u03c0, P) to evaluate whether \u03c0 optimally solves P (1) or not (0)."}, {"title": "Learning Action Costs from Input Plans", "content": "We are interested in learning the costs of a set of actions such that the input plans are optimal under the resulting planning model. The underlying motivation is that by aligning the action's costs to the input plans, the new model will be able to generate new plans that better reflect the observed behavior.\nInitially, we assume actions do not have an associated cost a priori. To accommodate this, we extend the potential values that a cost function can have, to include empty values C : A \u2192 N\u207a \u222a {\u2205}. We denote C = \u2205 iff \u2200a \u2208 A, c(a) = \u00d8. We then formally define a cost function learning task as follows:\nDefinition 3 (Cost Function Learning Task). A cost function learning task is a tuple CFL = \u27e8T, M\u27e9 where:\n\u2022 T = (P1, ..., Pn) is a sequence of planning tasks that share F, A, and C = \u00d8.\n\u2022 \u039c = (\u03c01, ..., \u03c0\u03b7) is a corresponding sequence of simple plans that solves T.\nThe solution to a CFL task is a common cost function (common across all tasks and plans) C.\nLet us examine the problem definition. We assume we have access to the full plan \u03c0\u2081 as well as the planning task Pi that it solves. However, unlike previous works on action's cost learning (Gregory and Lindsay 2016; Garrido 2023), we do not require to know the total cost of each plan. Moreover, we also assume that all the planning tasks and plans share the same vocabulary, i.e., they have a common set of fluents F and actions A. We also assume the common cost function is initially unknown. Finally, we do not make any assumption on the initial and goal state of the input tasks and plans. The above assumptions are not restrictive and hold in many real-world applications such as the ones described in the Introduction. For example, in the navigation scenario F and A will remain constant as long as the city network (map) does not change, which will only occur when a new road is built. In this domain we will get input plans with different starting points (I) and destinations (G), which is supported by our problem definition. One could argue that in the navigation scenario it is trivial to annotate each plan with its actual duration. While this might be true, it is clearly not so for other applications where action's cost capture probabilities or user preferences, such as financial planning.\nGoing back to Definition 3, we purposely left open the characterization of a CFL solution, only restricting it to be a common cost function C shared by all the input tasks. We did this because we are interested in defining different solution concepts depending on the properties the cost function C should have. In the next subsections we formalize different solutions to cost function learning tasks."}, {"title": "Turning All the Input Plans Optimal", "content": "The first objective we turn our attention to is trying to find a common cost function under which all the input plans are optimal. We refer to such solutions as Ideal Cost Functions.\nDefinition 4 (Ideal Cost Function). Given a CFL task, we define an ideal cost function ICF that solves it as a common cost function C under which all the plans in M are optimal. The quality of an ideal cost function is defined as follows:\n```latex\nmin \u2211c(a),\n\u03b1\u0395\u0391\n```\ns.t. \u03a3 IS_OPTIMAL(\u03c0i, Pi = (F, A, Ii, Gi, C)) = |M| \n```latex\niET,M\n```\nAn ICF is optimal iff no other cost function C' yields a lower value in Equation (2) while satisfying Constraint (3).\nIdeal Cost Functions are not guaranteed to exist for arbitrary CFL tasks, since Constraint (3) cannot always be satisfied. The difficulty lies in the inter dependencies and potential conflicts between plans. In particular, this not only refers to the actions shared between the input plans, but also to the multiple alternative plans that an input plan can have, i.e., all the other sequence of actions that can achieve G from I."}, {"title": "Remark 1.", "content": "Note that since we are considering only simple plans, the set of alternative plans \u03a0\u2122i that a plan \u03c0\u2081 \u2208 \u039c can have is finite.\nWe can, now, show that there is no guarantee that an ICF solution exists for a CFL task.\nTheorem 1. Given a CFL task, it is not guaranteed that there exists an ICF solution.\nProof. Given a CFL = \u27e8T,M\u27e9 where T = (Pi,Pj), and M = (\u03c0\u03af, \u03c0j), let us assume that an ICF solution exists. Let us also assume that there are alternative plans \u03c0\u2208 \u03a0\u03c0\u03af and \u03c0\u2208 \u03a0; for \u03c0\u2081 and \u03c0\u0135 respectively, such that \u03c0\u0458 \u2282 \u03c0\u0458 and \u03c0\u2081 \u2282 \u03c0\u2081. Since \u03c0\u2081 is optimal, from Definition 2 we have:\n```latex\nc(\u03c0i) < c(\u03c0j)\n```\nFrom the assumption \u3160 \u2282\u3160j, there is at least one more action in \u03c0; than in . Moreover, since the minimum cost for each action is 1, then:\n```latex\nc(\u03c0i) < c(\u03c0j)\n```\nSince \u03c0; is also optimal, again by Definition 2:\n```latex\nc(\u03c0j) < c(\u03c0i)\n```\nFrom (4), (5) and (6) we have\n```latex\nc(\u03c0i) < c(\u03c0j) < c(\u03c0i) < c(\u03c0j)\n```\nHowever, from \u3160 \u2282 \u03c0\u03af, as the case above, there are at least one more action in \u03c0\u2081, we have\n```latex\nc(\u03c0j) < c(\u03c0i)\n```\nFrom (7) and (8) we get a contradiction from the assumption that there exists an ICF."}, {"title": "Example 1.", "content": "Let T = (P1, P2), and let M = (\u03c01, \u03c02) where \u03c01 = [(move A C), (move C B)] and \u03c02 = [(move A B), (move B C)]. These plans are displayed in the Figure below, where \u3160\u2081 is represented by the orange arrows, and 2 by the blue ones.\nLet us assume that there is a ICF solution, i.e. there exists a cost function C under which all plans in M are optimal.\nSuppose the cost function C assigns the minimum costs for the actions that formed \u03c0\u2081: c(move AC) = 1 and c(move C B) = 1. Then, we have c(\u03c01) = 2. Since \u03c0\u2081 is optimal, the cost function C has to assign greater or equal costs to its alternative plans. Thus, c(move A B) = 2.\nOn the other hand, since we assume C exists, then \u03c0\u2082 is also optimal. The cost function C assigns the minimum costs to its actions, however, we already have c(move AB) = 2. We then assign c(move B C) = 1. Therefore, we have C(\u03c02) = 3. But there exists an alternative plan for \u03c02, which is: c(move A C) = 1. Then, \u03c0\u2082 is not optimal. A similar conclusion is reached if we start with \u03c01. Thus, there is no ICF solution that guarantees all plans in M are optimal."}, {"title": "Remark 2.", "content": "Observe that the optimality of a plan \u03c0\u03af \u2208 \u039c only depends on the costs of the actions (ai,...,an) \u2208 A occurring in \u03c0\u2081 or in \u220f\u2122i. The costs assigned to the rest of the actions do not affect \u03c0i's optimality."}, {"title": "Maximizing the Optimal Input Plans", "content": "Given that ICFs are not guaranteed to exist for every CFL task, we now relax the solution concept and focus on cost functions that maximize the number of plans turned optimal. We refer to such solutions as Maximal Cost Functions.\nDefinition 5 (Maximal Cost Function). Given a CFL task, we define a maximal cost function MCF that solves it as a common cost function C under which a maximum number of plans in M are optimal. We formally establish the quality of a maximal cost function as follows:\n```latex\nmax \u2211 IS_OPTIMAL(\u03c0i, Pi = (F, A, Ii, Gi, C)),\niET,M\n```\n```latex\nmin \u2211c(a),\n\u03b1\u0395\u0391\n```\nA maximal cost function MCF is optimal iff no other cost function C' yields a higher value in Equation (9), and, if tied, a lower value in Equation (10).\nWhile ICFs are not guaranteed to exist, it is easy to show that there is always a MCF that solves a CFL task. This is because even a cost function under which none of the plans M are optimal would be a valid MCF solution. One might think that we can go one step further and ensure that there is a trivial cost function C guaranteeing that at least one of the plans in M will be optimal in the resulting model. This trivial cost function would consist on assigning a cost k to all the actions in one of the plans \u03c0\u03af \u0395 \u039c, and a cost k + SR to the rest of the actions in A. Unfortunately, this is not always the case. In particular, if the input plan contains redundant actions (Nebel, Dimopoulos, and Koehler 1997; Salerno, Fuentetaja, and Seipp 2023), i.e., actions that can be removed without invalidating the plan, then it is not possible to turn \u03c0 optimal. Example 2 illustrates this case.\nExample 2. Let T = (P1) represented in the Figure below, where the initial state is displayed by the set of blocks on the left, and the goal state by the blocks on the right. Let M = (\u03c0\u2081) be the plan that solves P1, where \u03c0\u03b9 = [unstack(D, C'), putdown(D), unstack(B, A), putdown(B), pickup(A), stack(A, B)].\nLet \u03c0 = [(unstack(B, A), putdown(B), pickup(A), stack(A, B))] be an alternative plan. For \u03c0\u2081 to be an optimal plan, the costs of the actions in \u03c0\u2081 must be lower than or equal to those of \u03c0\u2081. However, this is not possible because in \u03c0\u2081 there are redundant actions and there is no other action in \u03c0\u2081 to which we can assign a higher cost, thereby making the total cost of \u03c0\u2081 lower than that of \u03c0\u2081. Therefore, there is no cost function C such that \u03c0\u2081 is optimal.\nAlthough in some extreme scenarios no input plan can be turned optimal, this is not the general case as we will see."}, {"title": "Making Input the Only Optimal Plans", "content": "Up to now we have explored the problem of turning a set of input plans optimal. We have showed that this task does not always have a solution when we want to make all the plans in M optimal (ICF), and therefore, we focused on maximizing the number of plans that are optimal under the resulting cost function (MCF). These solutions still allow for the existence of other alternative plans with the same cost. In some cases, we might be interested in a more restrictive setting, where only the input plans are optimal. In other words, there is no optimal plan \u03c0\u2081 such that \u03c0\u03b9 \u00a3 \u039c. This can be the case of applications where we are interested in generating optimal plans that perfectly align with the user preferences, preventing the model from generating optimal plans outside of the observed behavior. For this new solution we are still focusing on maximizing the number of plans in M that are optimal, but now we want them to be the only optimal ones. We formally define this new solution as follows:\nDefinition 6 (Strict Cost Function). Given a CFL task, we define a strict cost function SCF as a common cost function C under which a maximum number of plans in M are the only optimal plans.\nThe quality of a strict cost function that solves a CFL task is defined as in the case of MCF (Definition 5). However, there is now a difference in Definition 2 which defines an optimal plan (used in the boolean function IS_OPTIMAL(\u03c0, \u03a1) in Equation (9)). The condition for a plan's cost being lower than or equal to (\u2264) the cost of its alternative plans is replaced, in the strict approach, by a condition of being strictly lower than (<). Similarly to MCF, there is always a SCF that solves a CFL task, and we cannot guarantee the existence of an ideal cost function for SCF. In particular, Theorem 1 also applies to the SCF solution, with the exception that the definition of an optimal plan has changed as specified earlier."}, {"title": "Adapting an Existing Cost Function", "content": "In practice we might already have an approximate cost function C that we would like to refine with observed plans, rather than learning a cost function from scratch as we have focused on so far. In other words, the cost function C in the sequence of planning task T is not empty as in Definition 3. We then introduce a new task with initial costs as follows:\nDefinition 7 (Cost Function Refinement Task). A cost function refinement task is a tuple CFL\u00a9 = \u27e8T,M\u27e9 where:\n\u2022 T = (P1, ...,Pn) is a sequence of planning tasks that share F, A, and C = C.\n\u2022 \u039c = (\u03c01, ..., \u03c0\u03b7) is a corresponding sequence of simple plans that solves T.\nThe solution to a CFL\u0108 task is a common cost function C.\nWe denote with MCFC and SCFC when we have MCF and SCF solutions respectively, but for a CFLC task. We then formally redefine the quality of MCFC and SCFC solutions by slightly modifying Definition 5. In this case, we change the secondary objective of minimizing the sum of actions' costs"}, {"title": "Summary", "content": "Let us summarize the different tasks we have presented so far (and their solutions) with the example illustrated in Table 1. The first row of the table shows the CFL and CFLC tasks containing 6 states labeled with letters. The actions, depicted with edges, consist on moving between two connected states. There are two input plans M = (\u03c01, \u03c02) such that \u03c0\u2081 = [(move A B), (move B D)] and \u03c02= [(move A C), (move CE), (move E F)]. Colored states indicate they are an initial or goal state for one of the input plans.\nThe next rows depict optimal solutions for each of the two tasks. The MCF solution ensures both input plans are optimal, while assigning the minimum cost to each action (1) (Definition 5). The SCF solution guarantees that the two input plans are the only optimal alternatives under the returned cost function C. That solution would also turn all the plans optimal under the MCF definition, but would be suboptimal as it has a higher sum of action's costs. This is because the cost of (move CD) and (move D F) need to be increased from 1 to 2 in order to force that there are no optimal plans outside M. On the right side, the approximate cost function C is minimally modified to guarantee that the input plans are optimal. In the case of MCFC, this is achieved by reducing by one the cost of (move A C) and (move E F). In the case of SCFC, we need to decrease the cost of (move C E) and (move E F), and increase the cost of (move D F) to 2 in order to force the stricter SCF\u014c requirement."}, {"title": "Solving Cost Function Learning Tasks", "content": "Algorithm 1 describes LACFIPk, an algorithm to learn action's costs from input plans. It receives the cost function learning (refinement) task to solve (T), the desired solution (S), and a parameter k that determines the number of alternative plans to be computed for each plan \u03c0\u03af \u0395 \u039c. Higher k values indicate a higher percentage of I\u2122i is covered, with k = \u221e meaning that the whole set of alternative plans is computed. With these two inputs LACFIPk first generates a Mixed-Integer Linear Program (MILP) to assign a cost to each action, i.e., to compute the solution cost function C. This MILP is shown below (Equations 12-17) to compute MCF solutions for CFL tasks. MILPs for the other solutions and tasks are similar and can be found in the Appendix.\n```latex\nmaximize W1 \u2211 X\u03c0 - \u03c92 \u2211 Ya\n\u03a0\u0395\u039c\n\u03b1\u0395\u0391\u039c\n```\n```latex\nS.t. \u2211Ya \u2264 \u2211Ya + M(1 \u2212 2\u03c0,\u03c0\u03b1), \u03c0 \u0395 \u039c, \u03c0\u00aa \u2208 \u03a0\u03c0\n\u03b1\u0395\u03c0\n```\n```latex\n\u03b1\u0395\u03c0\u03b1\n```\n```latex\n|\u03a0*|\u2212 \u2211 \u2248\u03c0,\u03c0\u00aa \u2264 M(1 \u2212 x\u03c0), \u03c0\u2208 \u039c\n\u03c0\u03b1 \u0395\u03a0\u03c0\n```\n```latex\n\u03a7\u03c0\u2208 {0,1}, \u03c0\u0395\u039c\n```\n```latex\nYa \u2265 1, a \u2208 AM\n```\n```latex\n\u0396\u03c0,\u03c0\u03b1 \u2208 {0,1}, \u03c0\u03b5 \u039c, \u03c0\u00b2 \u0395\u03a0\n```\nWe have three sets of decision variables. The first, x, are binary decision variables that will take a value of 1 if plan \u03c0 is optimal in the resulting domain model, and 0 otherwise (Equation (15)). The second, 2\u3160,\u3160\u00aa are binary decision variables that will take a value of 1 if plan has a lower or equal cost than alternative plan \u03c0\u00aa, and 0 otherwise (Equation (17)). Finally, ya are integer decision variables that assign a cost to each action a \u2208 AM. This subset of actions AM C A represents the actions which cost needs to be set in order to make the plans in M optimal (see Remark 2). Constraint (13) enforces the value of the z variables. This is done by setting M to a large number, forcing 2\u03c0,\u03c0\u00aa to be 1 iff c(\u03c0) = \u03a3\u03b1\u03b5\u03c0 \u03a5\u03b1 \u2264 \u03a3\u03b1\u03b5\u03c0\u03b1 Ya = c(\u03c0\u00b2). Similarly, Constraint (14) ensures that x = 1 iff \u03c0 has a lower or equal cost than the rest of its alternative plans.\nWe aim to optimize the objective function described in Equation (12), where we have two objectives. The first objective, weighted by w\u2081, aims to maximize the number of"}, {"title": "Evaluation", "content": "Experimental Setting\nBenchmark. We ran experiments in four planning do- mains: BARMAN, OPENSTACKS, TRANSPORT and GRID, which is a navigation domain where an agent can move in the four cardinal directions to reach its desired cell. We chose them since we wanted to get a representative yet small set of domains. For each domain, we fix the problem size (F and A) and generate 50 different problems by varying I and G. For BARMAN, OPENSTACKS and TRANSPORT we use Seipp, Torralba, and Hoffmann (2022)'s PDDL generator, while for GRID we randomly generated different prob- lems by changing the initial and goal state of the agent. The problem sizes were chosen to allow computation of multiple alternatives in reasonable time. For example, the GRID size is 10 \u00d7 10, and BARMAN tasks have 3 ingredients, 3 cock- tails and 4 shots. Then, we use SYMK (Speck, Mattm\u00fcller, and Nebel 2020) to compute 100 simple plans for each plan-"}, {"title": "Performance Analysis.", "content": "Table 2 presents the results of our experiments. Each domain contains 10 CFL tasks of varying sizes: 101, 102 and 103, as displayed in the second row of the Table. These are solved by our five algorithms (first column of the table). The cells display the ratio of plans made optimal, represented by the mean and standard deviation, for all problem instances commonly solved by at least one al- gorithm. This ratio is computed by using the cost function C returned by each algorithm and using it to solve each of the planning tasks Pi \u2208 T. In the case of MCF, we verify that optimally solving Pi with C yields the same cost as the sum of action's costs of the input plan, i.e., \u03a3\u03b1\u03b5\u03c0, c(\u03b1). When this holds, input plan \u03c0\u2081 optimally solves task Pi and we annotate a 1. Otherwise, we annotate a 0 meaning that C does not make \u03c0\u2081 optimal. Similar validation checks are con- ducted for the other tasks and solution concepts, and their details can be checked in the Appendix. Cells shaded in gray indicate that the given algorithm failed to solve any of the 10 CFL tasks of that size. The remaining cells with values are color-coded to indicate the ratio of optimal plans achieved: lighter colors represent a lower ratio of optimal plans, while darker colors indicate a higher ratio of optimal plans. This color gradient provides a visual representation of each algo- rithm's effectiveness in turning input plans optimal across different problem sizes and domains.\nWe identify two main trends. Firstly, LACFIPk is consis- tently better than the baseline, and its performance tends to improve as k increases. This improvement is not necessarily"}, {"title": "Execution Time Analysis.", "content": "Figure 1 illustrates the total execution time (log scale) of each algorithm as we increase the CFL task size. Executions exceeding 1800s are shown above the dashed line. As we can see, BASELINE's execution time remains constant as the CFL size increases, being able to return cost functions in less than 10s in all cases. On the other hand, LACFIPk takes more time as more plans need to be turned optimal. For example, while it can solve all but two CFL task of size 10\u00b9 in GRID, 12 time out when |CFL| = 102. Increasing k is the factor that affects LACFIPk the most, as we can see in the linear (logarithmic) increase in the execution time regardless of the CFL size and domain.\nFinally, we analyzed how each component of LACFIPk affects the total running time. For lower values of k, SYMK can compute the alternative plans in few seconds, and most of the running time is devoted to the MILP. On the other hand, when k \u2265 103, computing the alternative plans takes most of the time, with problems where LACFIPk spends the 1800s running SYMK with no available time to run the MILP."}, {"title": "Related Work", "content": "Automated Planning\nMost planning works on domain learning focus on acquiring the action's dynamics (preconditions and effects) given a set of input plans (Yang, Wu, and Jiang 2007; Cresswell, McCluskey, and West 2013; Aineto, Celorrio, and Onaindia 2019). These works usually overlook the task of learning the action's cost model, as their main interest is learning to generate valid rather than good plans as we do.\nNLOCM (Gregory and Lindsay 2016) and LVCP (Garrido 2023) are two notable exceptions in the literature, being able to learn both the action's dynamics and cost model. Although differing in the input assumptions and guarantees, both approaches require each plan to be annotated with its total cost. Then, they use constraint programming to assign a cost to each action such that the sum of their costs equals the cost of the entire plan. Our work differs from them in three main aspects. First, we focus on learning the action's cost model, while NLOCM and LVCP can also learn the action's dynamics. Second, we do not require to know the total cost"}, {"title": "Inverse Reinforcement Learning", "content": "Inverse Reinforcement Learning (IRL) (Ng, Russell et al. 2000) is the task of inferring the reward function of an agent given its observed behavior. We can establish a relationship between learning action costs and IRL by assuming that (i) the input plans are observations of that agent acting in the environment; and (ii) action costs are the reward function we try to learn. However, IRL differs from learning action costs in few aspects. It is defined over Markov Decision Processes (MDPs), while we learn the action costs in the context of classical planning. Most approaches assume experts' observations aim at optimizing a single reward function (i.e., single goal), which is in stark contrast with our setting where every trace may be associated with a different goal. Among the closest works to our setting in the IRL literature we have Choi and Kim (2012) and Michini and How (2012). However, the first one assumes trajectories coming from experts belong to clusters each one with a different underlying reward structure, and the second one assumes reward functions can be represented as the combination of simple sub- goals. In our setting, no such assumptions are required."}, {"title": "Conclusions and Future Work", "content": "We have introduced a new problem: that of learning the costs of a set of actions such that a set of input plans are optimal under the resulting planning model. We have formally proved that this problem does not have a solution for an arbitrary set of input plans, and have relaxed the problem to accept solutions where the number of input plans that are turned optimal is maximized. We have also presented al- ternative solutions to these tasks, and introduced LACFIPk, a common algorithm to compute solutions to cost function learning tasks. Although the theoretical guarantees are only achieved when k = \u221e, i.e., all the alternative plans are considered, empirical results show that LACFIPk can achieve good results in few seconds with lower k values.\nIn future work we would like to extend our definitions and algorithms to handle bounded suboptimal plans as inputs. Our framework can also be easily extended to accept plans with extra annotations such as their weight. Despite its ef-"}, {"title": "Appendix A - LACFIP optimality", "content": "Theorem 2. LACFIPk guarantees MCF optimal solutions for CFL tasks when k \u221e.\nProof. In order to prove the optimality of LACFIPk algorithm, we aim to show that the algorithm performs lexicographic optimization, and that the MILP is properly encoded. To prove that LACFIPk is properly doing lexicographic optimization, we need to demonstrate that it satisfies Definition 5. Observe that lines 3 and 6 of the algorithm correspond to Equations (9) and (10) of Definition 5 respectively. In line 3, the MILP is solved by setting w\u2081 = 1 and w2 = 0, values which are then substituted into Equation (12). The optimal plans are calculated by the sum of x, which takes the value 1 for optimal plans and 0 for not optimal ones. Solving the MILP with these weights will give us a cost function C that makes Q plans optimal, and therefore, satisfying Definition 5, Equation (9). Then, the algorithm generates a new MILP with a new constraint (line 5), enforcing that the new solution has to turn exactly Q plans optimal. This second MILP is solved, in line 6, with w\u2081 = 0 and w2 = 1 with the goal to find the optimal cost function C, which makes Q plans optimal and minimizes the sum of action's costs, i.e., corresponding to Equation (10).\nWe now have to check that the MILP is properly encoded. As mentioned earlier, line 3 is responsible for maximizing the number of optimal plans. It is essential to ensure that the values of the x variables are properly enforced. This strictly depends on Equation (14). Observe that since we have k = \u221e, then we are considering all the alternative plans II", "cases": "nIf is an optimal plan"}, {"cases": "nIf \u03c0 is an optimal plan, then the sum of the actions' costs of \u03c0 is lower than or equal to the sum of the actions' costs of \u03c0\u00aa. Regardless of the value that \u2248\u3160,\u3160\u00aa takes, the inequality holds. However, \u2248\u03c0,\u03c0\u00aa"}]}