{"title": "CHROKNOWLEDGE: UNVEILING CHRONOLOGICAL KNOWLEDGE OF LANGUAGE MODELS IN MULTIPLE DOMAINS", "authors": ["Yein Park", "Chanwoong Yoon", "Jungwoo Park", "Donghyeon Lee", "Minbyul Jeong", "Jaewoo Kang"], "abstract": "Large language models (LLMs) have brought significant changes to many aspects\nof our lives. However, assessing and ensuring their chronological knowledge re-\nmains challenging. Existing approaches fall short in addressing the accumulative\nnature of knowledge, often relying on a single time stamp. To overcome this,\nwe introduce CHROKNOWBENCH, a benchmark dataset designed to evaluate\nchronologically accumulated knowledge across three key aspects: multiple do-\nmains, time dependency, temporal state. Our benchmark distinguishes between\nknowledge that evolves (e.g., scientific discoveries, amended laws) and knowledge\nthat remain constant (e.g., mathematical truths, commonsense facts). Building on\nthis benchmark, we present CHROKNOWLEDGE (Chronological Categorization\nof Knowledge), a novel sampling-based framework for evaluating and updating\nLLMs' non-parametric chronological knowledge. Our evaluation led to the follow-\ning observations: (1) The ability of eliciting temporal knowledge varies depending\non the data format that model was trained on. (2) LLMs partially recall knowledge\nor show a cut-off at temporal boundaries rather than recalling all aspects of knowl-\nedge correctly. Thus, we apply our CHROKNOWPROMPT, an in-depth prompting\nto elicit chronological knowledge by traversing step-by-step through the surround-\ning time spans. We observe that our framework successfully updates the overall\nknowledge across the entire timeline in both the biomedical domain (+11.9%) and\nthe general domain (+2.8%), highlighting its positive effect in refining temporal\nknowledge. This non-parametric approach also enables knowledge updates not\nonly in open-source models but also in proprietary LLMs, ensuring comprehensive\napplicability across model types. We perform a comprehensive analysis based\non temporal characteristics of CHROKNOWPROMPT and validate the potential of\nvarious models to elicit intrinsic temporal knowledge through our method.", "sections": [{"title": "INTRODUCTION", "content": "Do large language models (LLMs) possess the ability to understand and track the history of knowledge\nas time progresses? In other words, can these models, which represent the cutting edge of modern\nartificial intelligence, reason appropriately about questions that involve evolving facts? Although\nsome details remain controversial, knowledge-like science-is built upon accumulation (Zeigler,\n2012; Picho et al., 2016). From raw data to information and to knowledge, every bit is cumulative\nwhich contributes to progress across all domains. This accumulation forms the foundation for higher-\nlevel reasoning, which is akin to wisdom in navigating the complexities of our world (Rowley, 2007).\nGiven that LLMs are trained on vast and diverse corpora and are now integral to numerous applications\nin our daily lives, they must remain accurate and up-to-date to ensure reliability. Early versions of\nChatGPT (OpenAI, 2022), for instance, sometimes produced inaccurate or absurd responses like the"}, {"title": "PRELIMINARIES", "content": "To distinguish and evaluate the knowledge levels of language models, we utilize the Sampling-based\nKnowledge Categorization (SliCK) framework (Gekhman et al., 2024). This approach starts by\nsampling the model M's answers to questions using various few-shot exemplar sets D. The sampling\nis conducted under two temperature conditions: $\\tau = 0$ and $\\tau > 0$. Then, it categorizes the degree\nto which the model knows each piece of knowledge into four levels: HighlyKnown, MaybeKnown,\nWeaklyKnown, and Unknown.\nBased on Gekhman et al. (2024), we make the following modifications as follows: (1) We append a\ntemporal component t to the conventional {subject (s), relation (r), object (o)} triplet structure,\nallowing us to evaluate the model's knowledge across different time stamps; (2) We merge the two\ncategories (MaybeKnown and WeaklyKnown) that represent recallable knowledge states into a single\ncategory (Partial Correct); (3) By using time attribute, we also renamed the HighlyKnown and\nUnknown to the Correct and Incorrect, respectively. Our detailed definitions and descriptions are\nprovided in Table 1. Although this setting allows us to categorize the model's sampled responses\nmore precisely regarding time attribute, it only captures the model's knowledge at specific time points\nt, limiting our ability to observe changes over time. We address this limitation in Section 6."}, {"title": "ELICITING KNOWLEDGE USING DIVERSE TEMPLATES", "content": "Since models prefer different formats when eliciting their knowledge, it is important to use varied\napproaches to accurately assess their understanding (Zhou et al.). While we initially evaluate the\nmodel's knowledge using a standard triplet format, relying on a single template may not sufficiently\ncapture the full extent of the model's knowledge. Thus, following Hendrycks et al. (2021), we also\nemploy a well-known format, multiple-choice question answering (MCQA), to elicit the model's\nknowledge more effectively. As a result, we propose two templates for measuring how much\nknowledge the model holds: triplets (hereafter referred to as Generation) and MCQA. Each template\nis designed with appropriate few-shot exemplars and corresponding matching rules. For example, in\nGeneration, due to the complexity of evaluating responses, we apply fuzzy matching techniques to\ncompare the generated responses against predefined labels. See Appendix A.1 for further details of\nfew-shot exemplars, fuzzy matching rules, and examples of two templates."}, {"title": "CHROKNOWBENCH: CONSTRUCTING A BENCHMARK DATASET", "content": "In this section, we enumerate the details of constructing CHROKNOWBENCH, a chronologically\naccumulated knowledge benchmark dataset. The CHROKNOWBENCH dataset encompasses three key\naspects: time dependency (time variant and invariant), multiple domains (general, biomedical, legal,\ncommonsense, and math), and temporal state (dynamic and static). We first categorize knowledge\ninto two groups: knowledge that remains unchanged over time (time invariant) and knowledge that\nchanges over time (time variant). Additionally, we classify domains based on whether they are\ninfluenced by the flow of time, considering the specificity of each domain. Finally, we categorize\nknowledge as either changeable (dynamic) or steady (static) within the time frame we have set."}, {"title": "TASK DEFINITION", "content": "Our primary focus is a time variant knowledge across three domains (general, biomedical, and legal)\nwith comparisons to time invariant knowledge across two domains (commonsense and mathematics).\nWhat knowledge would be the difference between time variant and invariant? The time variant\nknowledge has a specific object changing across a stream of time. For example, \"Cristiano Ronaldo\n(s) was a member of sports team of (r) Manchester United F.C. (o1) in 2009 (t1) and Real Madrid CF\n(ok) in 2018 (tk)\u201d. Likewise, we identify subject and object alias for each relation, then gather yearly\nchanged objects. After accumulating object lists {o1, o2, ., om}, we de-duplicate and fill out the\nmissing data in specific years based on available data; objects between Manchester United F.C. (o1)\nin 2009 (t1) and Real Madrid CF (ok) in 2018 (tk), missing objects between 2010 (t2) to 2017 (tk\u22121)\nfilled with existing object of 2009 (t1). The statistic of CHROKNOWBENCH dataset is in Table 2."}, {"title": "DATASET GENERATION", "content": "To construct dataset, we select annual knowledge sources for each domain, possible to be aligned\nwith each elements even though the corpus does not specifically mention about time stamp. For\nsources with structured triplets, we identify temporal affect-able relations that typically change over\ntime, such as \"position held\". As time variant knowledge refers to the knowledge that has the\npotential to change over time, we divide it into two temporal states for more fine grained results:\n(1) dynamic, where the knowledge has evolved over the accumulated period. (2) static, where no\nchange occurred during the accumulated period though it has potential to be changed. Following\nthe methodology outlined in Section 3.1, we track changes in objects to build the dynamic dataset,\nemploying normalization and de-duplication for verification. Each object is checked with strict exact\nstring match, then add into objects pool. Simultaneously, we identified unchanged objects over the\nsame time frame to construct the static dataset. At the end, all data elements consist with an associated\nobject pool {o1, o2, ..., Om } over time frames {t1, t2,...,tm}."}, {"title": "TIME VARIANT & INVARIANT KNOWLEDGE", "content": "We sourced time variant knowledge from the general, biomedical, and legal domains. In general\ndomain, we utilize Wikidata (Vrande\u010di\u0107 & Kr\u00f6tzsch, 2014) dump to track object changes among"}, {"title": "EXPERIMENTAL SETUP", "content": "We conduct the experiments on a variety of proprietary and open-source LLMs. Each model utilizes\neither an instruction-tuned or chat version to enhance instruction following during sampling. We\nenumerate the eight open-source and one proprietary LLMs: Llama-3.1-8B-Instruct (Meta, 2024),\nLlama-3-8B-Instruct (Dubey et al., 2024), Llama-2-7b-chat-hf (Touvron et al., 2023b), Mistral-7B-\nInstruct-v0.3 (Jiang et al., 2023), Phi-3.5-mini-instruct (Abdin et al., 2024), SOLAR-10.7B-Instruct-\nv1.0 (Kim et al., 2023), gemma-2-9b-it (Team et al., 2024b), gemma-7b-it (Team et al., 2024a),\nand GPT-40 mini (OpenAI, 2024a). We focus on analyzing trends in the chronological knowledge\ncaptured by suggested models from various companies. These models differ in corpus coverage\nreflecting their respective cut-off dates. Details of our inference setups are in Appendix A.3."}, {"title": "CHROKNOWLEDGE: CHRONOLOGICAL CATEGORIZATION OF KNOWLEDGE", "content": "In this section, we introduce CHROKNOWLEDGE (Chronological Categorization of Knowledge), a\nsampling-based framework designed to evaluate and update LLMs' non-parametric chronological"}, {"title": "RESULTS OF REPRESENTING KNOWLEDGE FROM LARGE LANGUAGE MODELS", "content": "For testing model's knowledge within the categorization, we sample five times for each knowledge\nto elicit it as possible in dynamic and static dataset. We present our findings across three different\naspects: temporal-wise, template-wise, and domain-wise results. In Figure 2 and 3, we depict the\nresults for all domains and especially address general and biomedical domains in main section. Legal\nand invariant datasets are presented in Appendix (Figure 6 and 7)."}, {"title": "Temporal-wise Results", "content": "By comparing the left and right side would provide the tendency of\ntemporal-wise results in Figure 2 and 3. A common trend across models is a decline in recent\nknowledge, aligning with pretraining corpus cutoff dates. In dynamic datasets, models show reduced\nknowledge in multiple-choice question answering (MCQA) settings, highlighting the impact of\nformatting and prompting on temporal abilities. Static datasets exhibit less fluctuation, though\nthe result of general domain suggests domain-specific characteristics that each relations are more\ninfluenced via temporal sensitivity."}, {"title": "Template-wise Results", "content": "By comparing the top and bottom side would provide the tendency of\ntemplate-wise results in Figure 2 and 3. As we utilized two templates: generation and MCQA,\nthe result is also checked in template specificity. Generation templates reveal a greater decline\nin recent knowledge, as models rely on internal representations without predefined answers. In\ncontrast, MCQA templates help models select correct answers from options, mitigating some gaps in\nrecent knowledge. Notably, GPT-40 Mini performs best in generation tasks, while gemma-2-9b-it\nexcels in MCQA. In addition, the result of Phi-3.5-mini-instruct indicates specialized abilities can\noffset smaller model sizes, as it is more than twice small billion size compared to other models.\nThis highlights the importance of template design in eliciting temporal knowledge, with generation\nbenefiting from larger models and MCQA leveraging structured formats to enhance knowledge recall."}, {"title": "Domain-wise Results", "content": "Comparing the Figure 2 and 3 provide the tendency of domain-wise results.\nIn general domain, the models show a decline in recent knowledge, especially with generation\ntemplates. It comes from domain's nature, such as changes in relation 'position held' or 'member\nof sports team' which has sensitivity to temporal cues, leading to higher variability. However, in\nbiomedical domain, models fail to track recent updates between 2022 and 2023, particularly in\ngeneration tasks. Static datasets remain consistent, reflecting the stable nature of scientific knowledge.\nHere, MCQA setting offers some resilience though still limited to specific model's improvement. We\nprovide the overall trends of legal domain and time-invariant knowledge in Appendix A.4."}, {"title": "CHROKNOWPROMPT: CHRONOLOGICAL KNOWLEDGE PROMPTING", "content": "We represent CHROKNOWPROMPT (Chronological Knowledge Prompting), an in-depth prompting\nof non-parametric method to elicit chronological knowledge which has gap between each time stamp,\nby traversing step-by-step through the surrounding time spans."}, {"title": "CHRONOLOGICAL CATEGORIZATION", "content": "In previous section, we demonstrate whether\nopen-source and proprietary models possess spe-\ncific knowledge at various time stamps. How-\never, this does not sufficiently assess the models'\nunderstanding of knowledge within a chronolog-\nical progression. As Zhao et al. (2024) suggest,\nknowledge influenced heavily by temporal fac-\ntors such as general domain can still vary in\nmore stable situation like static dataset. To ad-\ndress this issue, we first reclassify the models'\nresponses using a refined categorization scheme,\nallowing for a more comprehensive evaluation\nof temporal knowledge across all years."}, {"title": "METHOD", "content": "We introduce a chronological prompting technique for non-parametric knowledge editing, aimed\nat bridging knowledge gaps by utilizing multiple temporal snapshots. This method enhances the"}, {"title": "EXPERIMENTAL RESULTS & ANALYSIS", "content": "We apply our method to both Incorrect and Partial Correct categories, as\nthe latter may still lack definitive answers. The test set consists of 10% of the total dataset from each\ndomain. Evaluation employs fuzzy matching with a temperature of 0 for strict assessment, classifying\nan answer as Chrono-correct only if the last candidate answer matches the object."}, {"title": "RESULTS OF CHRO KNOWPROMPT", "content": "Table 3 presents the effect of chronological prompting on knowledge elicitation across different\nmodels. Results are shown as the percentage of Known category, with parenthetical values indicating\nthe increase by Chrono-correct. Significant improvements are observed in the biomedical domain\n(average increase of 11.9%), while general and legal domains show smaller gains (2.8% and 2.7%,"}, {"title": "EFFECTS OF CHRONOLOGICAL SPAN", "content": "To elucidate the mechanisms of chronological prompting, we conduct a detailed analysis across\ndifferent settings, focusing on the impact of incorporating the next span in chronological contexts.\nTable 3 compares performance when using only the previous span versus the total span (both previous\nand next). Overall, the total span approach yield higher scores compared to using only the previous\nspan. However, the degree of improvement varies by domain. In the biomedical domain, the total\nspan results in nearly double the score increase compared to the previous span alone (11.9 vs. 6.1).\nConversely, general domain shows a modest increase (2.8 vs. 1.8).\nModel-specific temporal sensitivity also varies. For instance, Llama-2-7b-chat-hf demonstrates\nsignificant score increases by effectively utilizing temporal cues, particularly from next spans. On\nthe other hand, SOLAR-10.7B-Instruct-v1.0 shows improvements when only the previous span was\nused, suggesting differing responses to temporal information across models. These findings indicate\nthat models differ in their responsiveness to past and future temporal contexts. Another possible\nexplanation is the variation in the temporal coverage of each model's pretraining corpus, which may\ninfluence their temporal sensitivity and affect knowledge recall across different time frames."}, {"title": "RELATED WORK", "content": "Since the emergence of LMs, deriving knowledge from language model is extensively studied, such\nas probing tasks (Hewitt & Manning, 2019), LAMA (Petroni et al., 2019) and BioLAMA (Sung et al.,\n2021). Then, many subsequent studies follows to exploit, (1) how LLMs define knowledge (Yu et al.;\nZhang et al., 2023; Gottesman & Geva, 2024), (2) how these models represent it (Chen et al., 2024a;b;\nWang et al., 2024d), and (3) how manipulate misleading part (Wang et al., 2023; Guti\u00e9rrez et al.,\n2024; Wu et al., 2024a). Based on them, recent researches on investigating knowledge highlight the\ndynamic nature of evolving facts and suggests that contradictions within the training data may lead to\nknowledge conflicts (Marjanovi\u0107 et al., 2024; Chang et al., 2024; Wang et al., 2024a; Xu et al., 2024;\nJin et al., 2024). And knowledge overshadowing (Zhang et al., 2024b) reveals phenomena where\ncertain conditions overshadow other facts, leading to misleading information (i.e., hallucinations).\nIn other view point, exploring temporal knowledge starts from using Wikidata, a static format\nof knowledge in triplet: subject, relation, and object, originated from extracting literature-based"}, {"title": "PARAMETRIC & NON-PARAMETRIC KNOWLEDGE UPDATE", "content": "Considering update of LLMs are in two types, parametric and non-parametric (Wang et al., 2024c), a\nclassical way of parametric update is using fine-tuning (Ghosal et al., 2024; Mecklenburg et al., 2024;\nGe et al., 2024). While the method extended to LoRA (Hu et al.), QLORA (Dettmers et al.), Melo (Yu\net al., 2024) to continual learning (GRACE (Hartvigsen et al., 2024) and WISE (Wang et al., 2024b)),\nby using the parameter accessibility of white-box LLMs like Llama series (Touvron et al., 2023a),\nMEND (Mitchell et al.), ROME (Meng et al., 2022), MEMIT (Meng et al.) emerges. Those local\neditable methods are effective, and still try to improve specificity and generalizability.\nIn contrast, for black-box LLMs, updates rely on non-parametric knowledge methods (Onoe et al.,\n2023), such as SERAC (Mitchell et al., 2022), MeLLo (Zhong et al., 2023), and IKE (Zheng et al.,\n2023). They align with two key trends: (1) Mitigating catastrophic forgetting, where the model\nloses previous knowledge, by not directly updating parameters. (2) Exploiting abilities of prominent\nblack-box LLMs like GPT-01 (OpenAI, 2024b) and Gemini (Team et al., 2023), as we cannot access\nto parameters. Another concern in knowledge update is they often focus only structured format,\npointed out that current methods struggle to update unstrctured data effectively (Wu et al., 2024b).\nIn this paper, we focus on non-parametric knowledge updates accomodating a broad range of input\nformats (structured and unstructured) to represent knowledge across diverse domains, depending on\nthe use of various white-box and black-box LLMs."}, {"title": "CONCLUSION, LIMITATION, AND FUTURE WORK", "content": "Overall, our work highlights the critical role of temporal context in knowledge evaluation and\nintroduces a robust framework for improving the temporal capabilities of future language models.\nWe present CHROKNOWBENCH, a comprehensive benchmark designed to assess the temporal\nknowledge of large language models (LLMs) across diverse domains, time dependencies, and\ntemporal states. Through our novel CHROKNOWLEDGE framework, we systematically evaluate and\nenhance the chronological knowledge of various LLMs, revealing that the effectivenesss of knowledge\nelicitation depends on the specific training data formats of each model. Our findings indicate that\nwhile models often associate facts with specific time stamps, they frequently exhibit partial recall or\nstruggle to fully capture temporal boundaries. By implementing an in-depth chronological prompting,\nCHROKNOWPROMPT, we achieve significant improvements in knowledge recall, with the most\nnotable increase seen in the biomedical domain. This non-parametric approach proves effective\nfor both proprietary and open-source models, demonstrating its potential as a practical method for\nupdating and refining the temporal accuracy of LLMs without relying on external retrieval systems.\nWhile our approach demonstrates significant improvements in certain domains, it shows limited\ngains in general domain. This could be a limitation of our non-parametric approach to effectively\nupdate knowledge within complex and context-rich fields. Specifically, domains like general and\nlegal require more definitive updates that extend beyond the capabilities of prompt-based techniques\nalone. This underscores the need for practical knowledge updating and editing methods that are\nintrinsically aligned with temporal dynamics in line of parametric approach.\nTo address this limitation, our future work will focus on developing methods for updating the\nparametric knowledge of open-source LLMs. Current knowledge update techniques do not adequately\nhandle temporal aspects, which are essential for maintaining the accuracy and relevance of information\nover time. We aim to create comprehensive updates synchronized with various time stamps and"}]}