{"title": "BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering", "authors": ["Jin Chen", "Kaijing Ma", "Haojian Huang", "Jiayu Shen", "Han Fang", "Xianghao Zang", "Chao Ban", "Zhongjiang He", "Hao Sun", "Yanmei Kang"], "abstract": "The development of multi-modal models has been rapidly advancing, with some demonstrating remarkable capabilities. However, annotating video-text pairs remains expensive and insufficient. Take video question answering (VideoQA) tasks as an example, human annotated questions and answers often cover only part of the video, and similar semantics can also be expressed through different text forms, leading to underutilization of video. To address this, we propose BoViLA, a self-training framework that augments question samples during training through LLM-based self-questioning and answering, which help model exploit video information and the internal knowledge of LLMs more thoroughly to improve modality alignment. To filter bad self-generated questions, we introduce Evidential Deep Learning (EDL) to estimate uncertainty and assess the quality of self-generated questions by evaluating the modality alignment within the context. To the best of our knowledge, this work is the first to explore LLM-based self-training frameworks for modality alignment. We evaluate BoViLA on five strong VideoQA benchmarks, where it outperforms several state-of-the-art methods and demonstrate its effectiveness and generality. Additionally, we provide extensive analyses of the self-training framework and the EDL-based uncertainty filtering mechanism. The code will be made available at https://github.com/dunknsabsw/BoViLA.", "sections": [{"title": "Introduction", "content": "Recent advances in multimodal large models (MLLMs) have demonstrated the effectiveness of scaling laws in visual instruction fine-tuning. However, the continued advancement of MLLMs is hindered by the high cost of human annotation required for visual instruction data. In fact, this supervised fine-tuning paradigm does not fully exploit the rich information available in the visual modality data or the internal knowledge of frozen large language models (LLMs), yet merely increasing the training data without optimizing data utilization can be inefficient. For example, in conventional video question-answering (VideoQA) tasks, models typically predict answers based on a given video and its associated annotated question. This approach, however, is suboptimal for effective learning. On one hand, as noted in (McQuivey 2008) that \"A Video Is Worth 1.8 Million Words\", videos often contain extensive information that can be described in various forms of language. However, typical datasets offer text that is both limited in length and uniform in structure, which significantly underutilizes the rich information embedded in videos. This restricts the model's learning to the specific annotated question-answer pairs, limiting its ability to generalize to semantically similar questions presented in different formats. Consequently, this naive training paradigm hinders the model's capacity for analogical reasoning. On the other hand, this mechanical and passive supervised training method is notably inferior compared to human learning processes, which tends to be more active. Humans often draw upon past experiences and cognition to enrich their understanding of current events, hence proactively pose new questions and seeking answers for a more comprehensive and profound grasp of the situation. This form of learning, which integrates historical knowledge and is often abstracted as \"world model\".\nTo address this, we introduce Bootstrapping Video-Language Alignment (BoViLA) training framework via LLM-based self-questioning and answering. It further exploits internal knowledge of LLMs and the rich information in videos. BoViLA includes two roles, the questioner and the answerer, both played by the same model and improve each other alternately through self-questioning and answering, as shown in Fig 1. Questioner generates new questions for enabling itself to further extract aligned knowledge from videos and unleash power of the LLM. Answerer provides feedback to questioner in terms of the self-generated question. This framework features efficiently employment of video data and the internal historical knowledge of LLMs.\nAdditionally, we also apply EDL-estimated uncertainty to filter out low-quality questions resulting from modal un-alignment. We enhance the vanilla EDL by decoupling the direction and magnitude of evidence, as directly applying a non-negative activation function to the logits of an LLM, where most parameters are frozen and the logits have high dimensionality, can result in substantial information loss.\nWe verify the effectiveness of the BoViLA on five challenging VideoQA benchmarks: STAR(Wu et al. 2024a), How2QA(Li et al. 2020), DramaQA(Choi et al. 2021), TVQA(Lei et al. 2018), and VLEP(Lei et al. 2020), where BoViLA outperforms several strong baselines. Moreover, we present extensive ablation studies as shown in Table 3 and Appendix. To sum up, our contributions are as follows:\n\u2022 We propose a bootstrapping video-language alignment framework BoViLA, which help effectively enhance modality alignment via self-questioning and answering.\n\u2022 We first investigate the uncertainty quantification method for LLMs based on Evidential Deep Learning (EDL), improving the vanilla EDL for LLMs by decoupling the direction and magnitude of evidence vector.\n\u2022 We validate the efficacy of BoViLA on five VideoQA benchmarks by outperforming several strong baseline models with only a few trainable parameters (4.5M). We also conduct thorough and detailed experiments to demonstrate the effectiveness of each component within BoViLA."}, {"title": "Related Work", "content": "As LLMs have demonstrated impressive capabilities(Brown et al. 2020; Ouyang et al. 2022; Raffel et al. 2020; Touvron et al. 2023; Chiang et al. 2023), there has been increasing interest in exploring multi-modal language models (MLLMs)(Hu et al. 2024) with visual capabilities. Unlike the more costly joint visual-language pre-training methods, some approaches focus on training lightweight visual-language connectors that endow LLMs with visual abilities(Ma et al. 2023). These methods efficiently utilize the linguistic knowledge of LLMs and the visual knowledge of pre-trained visual encoders for cross-modal alignment.\nFor instance, Flamingo(Alayrac et al. 2022) utilize a cross-attention mechanism to inject visual knowledge into the LLM, which is the so-called Perceiver Resampler. LLaMA-Adapter(Zhang et al. 2023a) applies a linear projection along with prompt adaptation to incorporate visual information, effectively projecting visual embeddings into the input space of the LLM. Additionally, BLIP-2(Li et al. 2023) trains a module called the Q-former to bridge the modal gap between pre-trained visual encoders and LLMs, enhancing the model's multi-modal understanding."}, {"title": "Video Question-Answering", "content": "VideoQA involves answering natural language questions about a video, requiring models to understand both the video and the questions across various semantic levels due to the open-ended nature of the questions, and answer them with commonsense reasoning. This makes VideoQA one of the most typical tasks in multi-modal understanding.\nTraditional VideoQA methods relied on training separate visual and text encoders, along with temporal modeling and answering modules (Qian et al. 2023; Xiao et al. 2022; Lei et al. 2021). However, with the advent of large language models (LLMs), there is a growing trend towards using LLM-based approaches due to their advanced reasoning abilities(Yu et al. 2024; Wang et al. 2023b; Ko et al. 2023; Zhang, Li, and Bing 2023; Yu, Yoon, and Bansal 2024).\nFor instance, SeViLA(Yu et al. 2024) uses an LLM to select keyframes from a video and employs another LLM to answer questions based on these keyframes, fully leveraging LLMs' capabilities. VLAP(Wang et al. 2023b) improves on this by introducing a Frame-Prompter and QFormer-Distiller for more efficient modality alignment. CREMA(Yu, Yoon, and Bansal 2024) trains a multimodal Q-former to integrate information from different modalities to answer questions. We notice the recent work of LLaMA-VQA(Ko et al. 2023) as closest to ours, which trains only a linear layer and adaptor to enable LLMs to understand videos, and enhancing modality alignment through multi-task learning by reconstructing both questions and video. In contrast, our work prompts LLMs to simultaneously engage in self-questioning and answering. The major differences are that (i) We use self-generated questions as augmented training data and ask the model to answer them correctly. (ii) We further enhance the model's questioning ability by encouraging it to answer correctly from self-generated questions. In other words, the loss of the answerer on the self-generated questions propagates gradients back to the parameters of the questioner."}, {"title": "LLM-Based Bootstrapping Training", "content": "Early research has extensively explored the use of LLMs to generate training data for other models, which leverages the capabilities of LLMs to reduce the dependency on human labor for data collection(Lee et al. 2024; Liu et al. 2022; Meng et al. 2023; Wang et al. 2024; Mekala et al. 2022). Recently, there has been increasing interest in using LLMs to generate data for their own training(Ulmer et al. 2024; Zhao et al. 2024; Wang et al. 2022a; Huang et al. 2022; Amini et al. 2022).\nFor example, Wang et al. (2022a) enables models to generate data for instruction fine-tuning, focusing on data efficiency and general-purpose tasks. Huang et al. (2022) use Chain-of-Thought prompting and self-consistency to generate rationale-augmented answers without labeled data. Ulmer et al. (2024) focuses on a single improve step and employs a conceptually simpler supervised finetuning strategy instead of RL. Zhao et al. (2024) investigates how self-generation can further enhance an instruction-finetuned model's ability to execute task-specific instructions. Our work differs from prior efforts in several key ways: (i) We focus on multi-modal tasks rather than text-only ones; (ii) We update model parameters end-to-end rather than alternately performing these processes offline to improve modality alignment; (iii) This dual approach enhances learning efficiency, offering a faster and more convenient solution.\nFurthermore, training data generation can be seen as a form of knowledge distillation(Lei and Tao 2023; Wang et al. 2022a; Yang et al. 2024), while our self-questioning and answering method can also be seen as self-distillation."}, {"title": "Uncertainty Estimation Model", "content": "Numerous studies have explored models capable of estimating uncertainty to enhance reliability and trustworthiness(Zhang et al. 2021; Xiao et al. 2021; Li 2022; Izmailov et al. 2021; Gal and Ghahramani 2016; Amini et al. 2020; Sensoy, Kaplan, and Kandemir 2018). EDL(Sensoy, Kaplan, and Kandemir 2018), as one of these approaches, models \"second-order probabilities\" over logits based on Dempster-Shafer Theory(Shafer 1992) and Subjective Logic(Jsang 2018) to capture uncertainty conveniently and accurately in various fields(Han et al. 2022; Huang et al. 2024a; Ma et al. 2024; Huang et al. 2024b, 2023; ?), particularly for out-of-distribution (OOD) samples.\nIn this work, we leverage EDL-estimated uncertainty to evaluate the modality alignment within the context and employ it for \"soft filtering\" of the model's self-generated questions. To the best of our knowledge, we are the first to explore the integration of EDL with LLMs."}, {"title": "Methdology", "content": "We first present the architecture of BoViLA, detailing its key components and functionalities. Then we elaborate on our novel bootstrapping training framework, which leverages self-questioning and answering to enhance learning efficacy and modality alignment. Finally, we outlines our innovative approach for filtering self-generated questions, which is based on EDL-estimated uncertainty."}, {"title": "Model Architecture", "content": "As shown in Figure 2, our model architecture consists of an LLM decoder, a learnable linear layer for mapping visual tokens to the text embedding space, a lightweight adaptor for task-specific fine-tuning, and an EDL head to estimate uncertainty. For videos, we first extract frames $v = \\{v_1, v_2, \\ldots, v_N\\}$ and use a pre-trained visual encoder $E(\\cdot)$ to extract their features $E(v) = \\{E(v_1), E(v_2), \\ldots, E(v_N)\\} \\in \\mathbb{R}^{N_v \\times D}$. These features are then mapped to the text embedding space with the learnable linear layer $f_0(\\cdot)$, and an extra learnable temporal embedding $t = \\{t_1, t_2, ..., t_{N_v}\\} \\in \\mathbb{R}^{N_v \\times D}$ is added, i.e.\n$\\begin{aligned} h &= f_0(E(v)) + t\\\\ &= \\{f_0(E(v_1)) + t_1, \\ldots, f_0(E(v_{N_v})) + t_{N_v}\\} \\in \\mathbb{R}^{N_v \\times D}.\\end{aligned}$\nFor text inputs such as task instructions, questions, or answers, we use the LLM's tokenizer and token embedding module to obtain the corresponding tokens and embedding. Specifically, for questions, we get the tokens $q = \\{q_1, q_2, \\ldots, q_{N_q}\\}$ and their embedding $h_q = \\{q_1, q_2, \\ldots, q_{N_q}\\}$. Similarly, for answers, we obtain the tokens $a = \\{a_1, a_2, \\ldots, a_{N_a}\\}$ and their embedding $h_a = \\{a_1, a_2, \\ldots a_{N_a}\\}$. Then, we concatenate the video and text tokens together as input to the LLM. As task-specific fine-tuning is necessary, we employed several prevalent PEFT methods such as LoRA(Hu et al. 2021), adapter methods(Hu et al. 2023; Zhang et al. 2023b), and prefix-tuning(Li and Liang 2021; Zhang et al. 2023b) for efficient modal alignment."}, {"title": "Bootstrapping Training Framework", "content": "In our bootstrapping training framework, the model acts as both a \"questioner\" and an \"answerer\". The questioner generates additional questions samples for the answerer, and the answerer improves its answering skills by tackling with these questions while providing feedback about quality of questions to improve the questioner's ability. The overall framework is illustrated in Fig. 1.\nWhen the model acts as the questioner, we instruct it to generate a question based on the video, answer, and seed question, as shown in Figure 2. Assuming the LLM has $l$ layers, the hidden states from the final layer are $h_q = \\{q_1, q_2, \\ldots, q_{N_q}\\}$. For gradient backpropagation, we use Gumbel-Softmax(Jang, Gu, and Poole 2016) for sampling, formulated as below:\n$\\begin{aligned}p(q|v, a, q) &= \\prod_{i=1}^{N_q}p(q_i|v, a, q_{<i})\\\\&= \\prod_{i=1}^{N_q} \\operatorname{Gumbel-Softmax}(\\operatorname{Linear}(q_i)).\\end{aligned}$\nAs Gumbel-Softmax can output one-hot probability vector, we directly multiply it with the token embedding matrix to obtain a gradient-propagatable question embedding $h_q = \\{q_1, q_2, \\ldots, q_{N_q}\\}$, which serves as the input for the answerer."}, {"title": "Answerer.", "content": "When the model acts as the answerer, we ask it to predict answers of all questions (seed questions and self-generated questions) based on the video, where the answers share those of seed questions because the questioner generates questions conditioned on these answers. To be detailed, the loss can be computed as:\n$\\begin{aligned}L_{vqa} &= - \\log p(a|v, q) = - \\sum_{i=1}^{N_a} \\log p(a_i|v, q, a_{<i}).\\\\L_{v\\tilde{q}a} &= - \\log p(a|v, \\tilde{q}) = - \\sum_{i=1}^{N_a} \\log p(a_i|v, \\tilde{q}, a_{<i}).\\end{aligned}$"}, {"title": "Regularization Based on Seed Questions.", "content": "A key point of BoViLA is the differentiability of self-generated questions. If answerer fails to predict the target answer from $\\tilde{q}$, then penalty will be applied to the questioner to improve itself. In contrast, if answerer succeed in answering $\\tilde{q}$, that means $\\tilde{q}$ is considered a \"good\" question by answerer. However, this can easily lead to 'information leakage', where the questioner creates meaningless questions but contain target answers implicitly. To address this, we apply seed-based question regularization to constrain the generation space of questioner as follows:\n$\\begin{aligned}L_{reg} &= \\mathbb{E}_{v, a, q} \\left[ \\operatorname{KL}[p(q) || p(q|v, a, q)] \\right] \\\\&= \\sum_{i=1}^{N_q} \\operatorname{KL}[p(q_i) || p(q_i|v, a, q_{<i})]\\\\&= \\sum_{i=1}^{N_q} p(q_i) \\log \\frac{p(q_i)}{p(q_i|v, a, q_{<i})}.\\end{aligned}$"}, {"title": "EDL-based Filter", "content": "Despite the effectiveness of regularization, the questioner can still generate low-quality questions due to modal unalignment (especially at the early stages of training). Since EDL excels at predicting high uncertainty for OOD samples, which lie beyond the model's knowledge, and we consider misaligned video contexts and low-quality questions can be regarded as OOD samples, we introduce EDL-estimated uncertainty to assess the degree of modality alignment and the quality of self-generated questions. We then adjust the impact on loss $L_{v\\tilde{q}a}$ based on the level of uncertainty."}, {"title": "", "content": "Vallina EDL treats transformed logits as strength parameters $\\alpha = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_K)$ of a Dirichlet distribution $\\operatorname{Dir}(p|\\alpha)$ in a $K$-way classification and samples class probabilities $p = (p_1, p_2, \\ldots, p_K)$ from this distribution as the final prediction. It is also applicable to LLMs, as they generate text via next-token prediction, which can essentially be viewed as $K$-way classification where $K$ is great.\nHowever, directly applying vanilla EDL to LLMs would fail. This is because vanilla EDL commonly applies non-negative activation functions like ReLU(Sensoy, Kaplan, and Kandemir 2018) or Softplus(Amini et al. 2020) on logits to ensure the non-negativity of evidence. This causes information loss and negative effects(Ye et al. 2024; Meinert, Gawlikowski, and Lavin 2023; Wu et al. 2024b), especially for finetuning pretrained models with a large number of categories. To overcome this and successfully apply EDL to LLMs, we propose decoupling the direction and magnitude of the evidence vector $e = (e_1, e_2, \\ldots, e_K)$ to mitigate information loss. To be detailed, assume the logits output by the model are $z = (z_1, z_2, \\ldots, z_K)$, vanilla EDL determines $\\alpha$ through the following transformation:\n$\\alpha_i = e_i + 1 = \\operatorname{ReLU}(z_i) + 1, \\quad i = 1, 2, \\ldots, K,$\nand the total strength $S$ can be evaluated as $S = \\sum_{i=1}^{K} \\alpha_i$. Dirichlet distribution is modeled with $\\alpha$ as:\n$\\operatorname{Dir}(p|\\alpha) = \\begin{cases}\\frac{1}{B(\\alpha)} \\prod_{i=1}^{K} p_i^{\\alpha_i - 1}, & \\text{if } p \\in S_K\\\\0, & \\text{otherwise,}\\end{cases}$"}, {"title": "Experimental Setup", "content": "We conduct all training with 8 \u00d7 80GB A800 GPUs for 10 epochs. For all the datasets, we use VIT-L/14 as the visual encoder to extract 10 frame features for each video and use LLaMA(7B) as our large language model. Regarding evaluation metrics, we use the accuracy of choosing the right answer and test on the validation split. We provide the detailed prompt template of BoViLA in Table 1. Please refer to Appendix for more training details."}, {"title": "Baselines & Benchmarks.", "content": "We compare our framework with some state-of-the-art (SOTA) baselines, especially that are LLM-based such as BLIP-2(Li et al. 2023), SeViLA(Yu et al. 2024) and LLaMA-VQA(Ko et al. 2023), on 5 challenging multi-choice VideoQA benchmarks: 1) TVQA(Lei et al. 2018), requireing answer questions based on video, dialogues and scenes, featuring 152,545 QA pairs from 21,793 video clips extracted from popular TV shows. 2) STAR(Wu et al. 2024a), which is designed for spatio-temporal and relational reasoning, containing 22,670 QA pairs based on 12,672 video clips. 3) DramaQA(Choi et al. 2021), which is tailored for emotional and social reasoning, featuring 16,191 QA pairs derived from 23,239 video clips. 4) VLEP(Lei et al. 2020), which focuses on predicting future events based on video and dialogues, consisting of 28,726 QA pairs from 10,000 video clips. 5) How2QA(Li et al. 2020), which is designed for instructional video comprehension, containing 46,467 QA pairs derived from 23,228 video clips."}, {"title": "Main Results", "content": "Table 2 shows comparison results between our BoViLA and several strong baseline methods on the VideoQA task. Our proposed BoViLA achieves superior performance across multiple VideoQA benchmark datasets, showcasing strong capabilities in 1) cross-modal understanding of descriptive questions and 2) advanced temporal causal reasoning. To begin with, take How2QA benchmark as an example, which focuses on understanding video content and tests the model's ability to perceive detailed visual information based on the given questions. We consider such ability as a fundamental capability for video-textual cross-modal understanding. Our model outperforms the state-of-the-art by more than 2.7%. For the more demanding task of temporal causal reasoning, we report the results on the STAR, DramaQA, VLEP, and TVQA datasets in Table 2. For example, our method outperforms the previous best model by 8.2% in total accuracy on the TVQA dataset and exceeds the performance of existing models by 1.1% on the DramaQA dataset.\nIt is noteworthy that, compared to many VideoQA baselines utilizing large language models (LLMs), our approach enhances reasoning capabilities on VideoQA benchmarks with only 4.5M trainable parameters. This efficiency is achieved through our Bootstrapping training method, which effectively leverages the strong priors provided by LLMs. As a result, BoViLA not only significantly reduces training costs compared to models trained from scratch (e.g., Intern-Video, 1.3B), but also outperforms most parameter-efficient fine-tuning (PEFT) paradigms."}, {"title": "Ablation Studies", "content": "As shown in Figure 3, the self-questioning and answering process can easily cause 'information leakage', where the questioner creates degenerate questions, which are meaningless and cheatingly contain target answers. Our regularization method and EDL-based filter really help solve this problem and allow the model to generate high-quality questions to boost answers. Furthermore, we provide comprehensive ablation studies on STAR validation set in Table 3 to verify the effectiveness of our method. The results clearly demonstrate: 1) Only learning from regularization about questioning allows the model to achieve a 2.2% performance boost. 2) Further learning by answering these self-generated questions contributes an additional 0.7% improvement. 3) If the answerer is allowed to backpropagate gradients to the questioner, i.e. providing feedback on question quality, performance can increase by another 0.5%. 4) Moreover, applying an EDL-based filter to progressively eliminate potential junk questions that could negatively impact the model can further enhance performance by 0.9%."}, {"title": "More Discussion", "content": "Our proposed EDL-based filter is built on the assumption that \"EDL-estimated uncertainty is able to measure the quality of self-generated questions and, is approximately negatively correlated with it\u201d. Here, we further explore and validate this assumption. We use $L_{v\\tilde{q}a}$ and $L_{reg}$ as approximate measures of the quality score for self-generated questions. $L_{v\\tilde{q}a}$ represents the similarity between the self-generated question $\\tilde{q}$ and the seed question $q$ in terms of KL divergence, where a high $L_{v\\tilde{q}a}$ indicates a completely uncontrolled self-generated question. $L_{reg}$ represents the likelihood that the answerer successfully predicts the target answer from the self-generated question, where a high $L_{reg}$ is likely to indicates that there is no meaningful semantics in the self-generated question. $L_{v\\tilde{q}a}$ and $L_{reg}$ complement and reinforce each other, so we use them together to represent the quality of self-generated questions. As shown in Figure 4, both $L_{v\\tilde{q}a}$ and $L_{reg}$ demonstrate the pattern that \"the lower the quality of the self-generated question, the higher the uncertainty\". This insight further provides a solid explanation for the effectiveness of our proposed EDL-based filter."}, {"title": "Conclusion, Limitation and Future Work", "content": "In this paper, we present BoViLA, a pioneering framework that enhances video-language alignment through self-questioning and answering. BoViLA utilizes a unique bootstrapping approach where the model alternates between the roles of questioner and answerer, enabling it to generate and answer self-created questions, thereby deepening modality alignment without reliance on extra annotated data by fully leverage thr rich information within videos and internal knowledge in LLMs. Our framework also improve the vanilla EDL method and incorporates it to assess and filter the quality of self-generated questions. BoViLA demonstrates superior performance across five VideoQA benchmarks, outperforming current state-of-the-art methods with only a few trainable parameters.\nDespite its success, the framework faces limitations in a constrained question generation space because our questioner always generates new questions by autoregressively predicting the next token based on the context of the seed question. Future work will focus on exploring how to sample more freely from the joint distribution of question samples during training, in order to generate more diverse self-generated questions."}, {"title": "Implementation Details", "content": "As reported in Table A, we provide a detailed list of experimental settings across various datasets."}, {"title": "Details of EDL Loss", "content": "Here we show the detailed derivations of $\\mathcal{L}_{vqa}^{\\text{edl}}$ and $\\mathcal{L}_{reg}^{\\text{edl}}$. The $\\mathcal{L}_{vqa}^{\\text{edl}}$, which is essentially the Bayes risk, is as follows:\n$\\begin{aligned}\\mathcal{L}_{vqa}^{\\text{edl}} &= \\mathbb{E}_{Dir} [-\\log p(a_j)] = - \\sum_{j=1}^{N_a} \\int \\log p(a_j) B(\\alpha) \\prod_i p_i^{\\alpha_i-1} d p, \\end{aligned}$\nBy the properties of the expectation of the Dirichlet distribution, we have:\n$\\mathbb{E}_{Dir} [\\log p(a_j)] = \\psi(\\alpha_{a_j}) - \\psi(S_j),$\nwhere $\\psi(\\cdot)$ is the digamma function. So the origin loss can be formulated as:\n$\\begin{aligned}\\mathcal{L}_{vqa}^{\\text{edl}} &= - \\mathbb{E}_{Dir} [\\log p(a_j)] = \\sum_{j=1}^{N_a} (\\psi(S_j) - \\psi(\\alpha_{a_j})). \\end{aligned}$\nThe $\\mathcal{L}_{reg}^{\\text{edl}}$, which is essentially the KL divergence with the zero evidence Dirichlet distribution, is as follows:\n$\\begin{aligned}\\mathcal{L}_{reg}^{\\text{edl}} &= KL[Dir(p|\\alpha) || Dir(p|(1, \\ldots, 1))] \\\\ &= \\mathbb{E}_{Dir(p|\\alpha)} \\log \\frac{Dir(p|\\alpha)}{Dir(p|(1, \\ldots, 1)))} \\\\ &= \\mathbb{E}_{Dir(p|\\alpha)} [\\log Dir(p|\\alpha) - \\log Dir(p|(1, \\ldots, 1))] \\\\ &= \\mathbb{E}_{Dir(p|\\alpha)} [\\log B(\\alpha) + \\sum_{k=1}^K(\\alpha_k - 1) \\log p_k + \\log B((1, \\ldots, 1))] \\\\ &= \\log \\frac{\\Gamma(\\sum_{k=1}^K(\\alpha_k))}{\\prod_{k=1}^K \\Gamma(\\alpha_k)} + \\mathbb{E}_{Dir(p|\\alpha)} \\sum_{k=1}^K(\\alpha_k - 1) \\log p_k \\\\ &= \\log \\frac{\\Gamma(\\sum_{k=1}^K \\alpha_k)}{\\prod_{k=1}^K \\Gamma(\\alpha_k)} + \\sum_{k=1}^K(\\alpha_k - 1) (\\psi(\\alpha_k) - \\psi(\\sum_{j=1}^K \\alpha_j)), \\end{aligned}$\nwhere $\\Gamma(\\cdot)$ is the gamma function."}, {"title": "Validity of EDL-Estimated Uncertainty", "content": "Ideally, the uncertainty estimated by the model should generally follow (though not strictly adhere to) the rule that \u201cthe more accurate the prediction, the lower the uncertainty\u201d. In the experimental section of the main text, we have validated this rule by quantifying prediction accuracy using loss $L_{v\\tilde{q}a}$. Here, we revisit this point by comparing the uncertainty distributions of correct and incorrect predictions made by the model on the STAR validation set, as shown in Figure A."}, {"title": "Adversarial Experiments.", "content": "In the methodology section of the main text, we assume that \u201cthe model will regard low-quality video representations caused by insufficient modality alignment and low-quality questions as OOD context and will output higher uncertainty when answering\u201d. Here, we simulate low-quality video representations and low-quality questions by applying varying levels of Gaussian noise to the video features and by zeroing out different proportions of the question text respectively, on the STAR validation set, and examine the resulting uncertainty in the answers."}, {"title": "Case Study about Different Forms of question", "content": "We present here the model\u2019s varying responses when confronted with semantically consistent but differently formatted questions, as shown in Figure C, which demonstrates the necessity of our approach."}]}