{"title": "BoViLA: Bootstrapping Video-Language Alignment via LLM-Based Self-Questioning and Answering", "authors": ["Jin Chen", "Kaijing Ma", "Haojian Huang", "Jiayu Shen", "Han Fang", "Xianghao Zang", "Chao Ban", "Zhongjiang He", "Hao Sun", "Yanmei Kang"], "abstract": "The development of multi-modal models has been rapidly advancing, with some demonstrating remarkable capabilities. However, annotating video-text pairs remains expensive and insufficient. Take video question answering (VideoQA) tasks as an example, human annotated questions and answers often cover only part of the video, and similar semantics can also be expressed through different text forms, leading to underuti-lization of video. To address this, we propose BoViLA, a self-training framework that augments question samples during training through LLM-based self-questioning and answering, which help model exploit video information and the internal knowledge of LLMs more thoroughly to improve modality alignment. To filter bad self-generated questions, we intro-duce Evidential Deep Learning (EDL) to estimate uncertainty and assess the quality of self-generated questions by evaluat-ing the modality alignment within the context. To the best of our knowledge, this work is the first to explore LLM-based self-training frameworks for modality alignment. We evalu-ate BoViLA on five strong VideoQA benchmarks, where it outperforms several state-of-the-art methods and demonstrate its effectiveness and generality. Additionally, we provide ex-tensive analyses of the self-training framework and the EDL-based uncertainty filtering mechanism. The code will be made available at https://github.com/dunknsabsw/BoViLA.", "sections": [{"title": "Introduction", "content": "Recent advances in multimodal large models (MLLMs) have demonstrated the effectiveness of scaling laws in visual in-struction fine-tuning. However, the continued advancement of MLLMs is hindered by the high cost of human annotation required for visual instruction data. In fact, this supervised fine-tuning paradigm does not fully exploit the rich infor-mation available in the visual modality data or the internal knowledge of frozen large language models (LLMs), yet merely increasing the training data without optimizing data utilization can be inefficient. For example, in conventional video question-answering (VideoQA) tasks, models typically predict answers based on a given video and its associated annotated question. This approach, however, is suboptimal for effective learning. On one hand, as noted in (McQuivey 2008) that \"A Video Is Worth 1.8 Million Words\", videos often contain extensive information that can be described in various forms of language. However, typical datasets offer text that is both limited in length and uniform in structure, which significantly underutilizes the rich information em-bedded in videos. This restricts the model's learning to the specific annotated question-answer pairs, limiting its ability to generalize to semantically similar questions presented in different formats. Consequently, this naive training paradigm hinders the model's capacity for analogical reasoning. On the other hand, this mechanical and passive supervised train-ing method is notably inferior compared to human learning processes, which tends to be more active. Humans often draw upon past experiences and cognition to enrich their un-derstanding of current events, hence proactively pose new questions and seeking answers for a more comprehensive and profound grasp of the situation. This form of learning, which integrates historical knowledge and is often abstracted as \"world model\".\nTo address this, we introduce Bootstrapping Video-Language Alignment (BoViLA) training framework via LLM-based self-questioning and answering. It further ex-"}, {"title": "Related Work", "content": "LLMs for multi-modal understanding\nAs LLMs have demonstrated impressive capabilities(Brown et al. 2020; Ouyang et al. 2022; Raffel et al. 2020; Tou-vron et al. 2023; Chiang et al. 2023), there has been in-creasing interest in exploring multi-modal language models (MLLMs)(Hu et al. 2024) with visual capabilities. Unlike the more costly joint visual-language pre-training methods, some approaches focus on training lightweight visual-language connectors that endow LLMs with visual abilities(Ma et al. 2023). These methods efficiently utilize the linguistic knowl-edge of LLMs and the visual knowledge of pre-trained visual encoders for cross-modal alignment.\nFor instance, Flamingo(Alayrac et al. 2022) utilize a cross-attention mechanism to inject visual knowledge into the LLM, which is the so-called Perceiver Resampler. LLaMA-Adapter(Zhang et al. 2023a) applies a linear projection along with prompt adaptation to incorporate visual information, effectively projecting visual embeddings into the input space of the LLM. Additionally, BLIP-2(Li et al. 2023) trains a module called the Q-former to bridge the modal gap between pre-trained visual encoders and LLMs, enhancing the model's multi-modal understanding.\nVideo Question-Answering\nVideoQA involves answering natural language questions about a video, requiring models to understand both the video and the questions across various semantic levels due to the open-ended nature of the questions, and answer them with commonsense reasoning. This makes VideoQA one of the most typical tasks in multi-modal understanding.\nTraditional VideoQA methods relied on training separate visual and text encoders, along with temporal modeling and answering modules (Qian et al. 2023; Xiao et al. 2022; Lei et al. 2021). However, with the advent of large language models (LLMs), there is a growing trend towards using LLM-based approaches due to their advanced reasoning abil-ities(Yu et al. 2024; Wang et al. 2023b; Ko et al. 2023; Zhang, Li, and Bing 2023; Yu, Yoon, and Bansal 2024).\nFor instance, SeViLA(Yu et al. 2024) uses an LLM to se-lect keyframes from a video and employs another LLM to answer questions based on these keyframes, fully leveraging LLMs' capabilities. VLAP(Wang et al. 2023b) improves on this by introducing a Frame-Prompter and QFormer-Distiller for more efficient modality alignment. CREMA(Yu, Yoon, and Bansal 2024) trains a multimodal Q-former to integrate information from different modalities to answer questions. We notice the recent work of LLaMA-VQA(Ko et al. 2023) as closest to ours, which trains only a linear layer and adaptor to enable LLMs to understand videos, and enhancing modality alignment through multi-task learning by reconstructing both questions and video. In contrast, our work prompts LLMs to simultaneously engage in self-questioning and answering. The major differences are that (i) We use self-generated ques-tions as augmented training data and ask the model to answer them correctly. (ii) We further enhance the model's ques-tioning ability by encouraging it to answer correctly from self-generated questions. In other words, the loss of the an-swerer on the self-generated questions propagates gradients back to the parameters of the questioner.\nLLM-Based Bootstrapping Training\nEarly research has extensively explored the use of LLMs to generate training data for other models, which leverages the capabilities of LLMs to reduce the dependency on human labor for data collection(Lee et al. 2024; Liu et al. 2022; Meng et al. 2023; Wang et al. 2024; Mekala et al. 2022). Recently, there has been increasing interest in using LLMs to generate data for their own training(Ulmer et al. 2024; Zhao et al. 2024; Wang et al. 2022a; Huang et al. 2022; Amini et al. 2022).\nFor example, Wang et al. (2022a) enables models to gen-erate data for instruction fine-tuning, focusing on data effi-ciency and general-purpose tasks. Huang et al. (2022) use Chain-of-Thought prompting and self-consistency to gener-ate rationale-augmented answers without labeled data. Ulmer et al. (2024) focuses on a single improve step and employs a conceptually simpler supervised finetuning strategy instead"}, {"title": "Methdology", "content": "We first present the architecture of BoViLA, detailing its key components and functionalities. Then we elaborate on our novel bootstrapping training framework, which leverages self-questioning and answering to enhance learning efficacy and modality alignment. Finally, we outlines our innovative ap-proach for filtering self-generated questions, which is based on EDL-estimated uncertainty."}, {"title": "Model Architecture", "content": "As shown in Figure 2, our model architecture con-sists of an LLM decoder, a learnable linear layer for mapping visual tokens to the text embedding space, a lightweight adaptor for task-specific fine-tuning, and an EDL head to estimate uncertainty. For videos, we first extract frames $v = \\{v_1, v_2, \\ldots, v_N\\}$ and use a pre-trained visual encoder $E(\\cdot)$ to extract their features $E(v) = \\{E(v_1), E(v_2), \\cdots, E(v_N)\\} \\in \\mathbb{R}^{N_v \\times D}$. These features are then mapped to the text embedding space with the learnable linear layer $f_o(\\cdot)$, and an extra learnable temporal embedding $t = \\{t_1, t_2, ..., t_v\\} \\in \\mathbb{R}^{N_v \\times D}$ is added, i.e.\n$h = f_o(E(v)) + t$\n$\\quad = \\{f_o(E(v_1)) + t_1, \\ldots, f_o(E(v_{N_v})) + t_{N_v}\\} \\in \\mathbb{R}^{N_v \\times D}$.\nFor text inputs such as task instructions, questions, or answers, we use the LLM's tokenizer and token em-bedding module to obtain the corresponding tokens and embedding. Specifically, for questions, we get the to-kens $q = \\{q_1, q_2, \\ldots, q_{N_q}\\}$ and their embedding $h_q = \\{q_1, q_2, \\ldots, q_{N_q}\\}$. Similarly, for answers, we obtain the to-kens $a = \\{a_1, a_2, \\ldots, a_{N_a}\\}$ and their embedding $h_a = \\{a_1, a_2, \\ldots, a_{N_a}\\}$. Then, we concatenate the video and text tokens together as input to the LLM. As task-specific fine-tuning is necessary, we employed several prevalent PEFT methods such as LoRA(Hu et al. 2021), adapter methods(Hu et al. 2023; Zhang et al. 2023b), and prefix-tuning(Li and Liang 2021; Zhang et al. 2023b) for efficient modal align-ment."}, {"title": "Bootstrapping Training Framework", "content": "In our bootstrapping training framework, the model acts as both a \"questioner\" and an \"answerer\". The questioner gen-erates additional questions samples for the answerer, and the answerer improves its answering skills by tackling with these questions while providing feedback about quality of questions to improve the questioner's ability. The overall framework is illustrated in Fig. 1.\nQuestioner. When the model acts as the questioner, we instruct it to generate a question based on the video, an-swer, and seed question, as shown in Figure 2. Assuming the LLM has $l$ layers, the hidden states from the final layer are $h_l = \\{q_1, q_2, \\ldots, q_{N_q}\\}$. For gradient backpropagation, we use Gumbel-Softmax(Jang, Gu, and Poole 2016) for sam-pling, formulated as below:\n$p(q|v, a, q) = \\prod_{i=1}^{N_q}p(q_i|v, a, q_{<i})$\n(3)\n$= \\prod_{i=1}^{N_q} \\text{Gumbel-Softmax}(\\text{Linear}(h_l^{(i)}))$.\n(4)\nAs Gumbel-Softmax can output one-hot probability vec-tor, we directly multiply it with the token embedding ma-trix to obtain a gradient-propagatable question embedding $h_q = \\{q_1, q_2, \\ldots, q_{N_q}\\}$, which serves as the input for the answerer.\nAnswerer. When the model acts as the answerer, we ask it to predict answers of all questions (seed questions and self-generated questions) based on the video, where the answers share those of seed questions because the questioner gener-ates questions conditioned on these answers. To be detailed, the loss can be computed as:\n$L_{vqa} = - \\log p(a|v, q) = - \\sum_{i=1}^{N_a} \\log p(a_i|v, q, a_{<i})$.\n(5)\n$L_{\\tilde{v}qa} = - \\log p(a|v, \\tilde{q}) = - \\sum_{i=1}^{N_a} \\log p(a_i|v, \\tilde{q}, a_{<i})$.\n(6)\nRegularization Based on Seed Questions. A key point of BoViLA is the differentiability of self-generated questions. If answerer fails to predict the target answer from $\\tilde{q}$, then"}, {"title": "EDL-based Filter", "content": "Despite the effectiveness of regularization, the questioner can still generate low-quality questions due to modal unalignment (especially at the early stages of training). Since EDL excels at predicting high uncertainty for OOD samples, which lie beyond the model's knowledge, and we consider misaligned video contexts and low-quality questions can be regarded as OOD samples, we introduce EDL-estimated uncertainty to assess the degree of modality alignment and the quality of self-generated questions. We then adjust the impact on loss $L_{\\tilde{v}qa}$ based on the level of uncertainty.\nVallina EDL treats transformed logits as strength param-eters $\\alpha = (\\alpha_1, \\alpha_2, \\ldots, \\alpha_K)$ of a Dirichlet distribution $\\text{Dir}(p|\\alpha)$ in a $K$-way classification and samples class prob-abilities $p = (p_1, p_2, \\ldots, p_K)$ from this distribution as the final prediction. It is also applicable to LLMs, as they gener-ate text via next-token prediction, which can essentially be viewed as $K$-way classification where $K$ is great.\nHowever, directly applying vanilla EDL to LLMs would fail. This is because vanilla EDL commonly applies non-negative activation functions like ReLU(Sensoy, Kaplan, and Kandemir 2018) or Softplus(Amini et al. 2020) on logits to ensure the non-negativity of evidence. This causes infor-mation loss and negative effects(Ye et al. 2024; Meinert, Gawlikowski, and Lavin 2023; Wu et al. 2024b), especially for finetuning pretrained models with a large number of cat-egories. To overcome this and successfully apply EDL to LLMs, we propose decoupling the direction and magnitude of the evidence vector $e = (e_1, e_2, \\ldots, e_K)$ to mitigate in-formation loss. To be detailed, assume the logits output by the model are $z = (z_1, z_2, \\ldots, z_K)$, vanilla EDL determines $\\alpha$ through the following transformation:\n$\\alpha_i = e_i + 1 = \\text{ReLU}(z_i) + 1, i = 1, 2, \\ldots, K,$\n(10)\nand the total strength $S$ can be evaluated as $S = \\sum_{i=1}^K \\alpha_i$. Dirichlet distribution is modeled with $\\alpha$ as:\n$\\text{Dir}(p|\\alpha) = \\begin{cases}\\frac{1}{B(\\alpha)} \\prod_{i=1}^K p_i^{\\alpha_i - 1} & \\text{if } p \\in S_K, \\\\ 0 & \\text{otherwise},\\end{cases}$\n(11)"}, {"title": "Experimental Setup", "content": "Implementation Details. We conduct all training with 8 \u00d7 80GB A800 GPUs for 10 epochs. For all the datasets, we use VIT-L/14 as the visual encoder to extract 10 frame features for each video and use LLaMA(7B) as our large language model. Regarding evaluation metrics, we use the accuracy of choosing the right answer and test on the validation split. We provide the detailed prompt template of BoViLA in Table 1. Please refer to Appendix for more training details."}, {"title": "More Discussion", "content": "Our proposed EDL-based filter is built on the assumption that \"EDL-estimated uncertainty is able to measure the quality of self-generated questions and, is approximately negatively correlated with it\u201d. Here, we further explore and validate this assumption. We use $L_{\\tilde{v}qa}$ and $L_{reg}$ as ap-proximate measures of the quality score for self-generated questions. $L_{\\tilde{v}qa}$ represents the similarity between the self-generated question $\\tilde{q}$ and the seed question $q$ in terms of KL divergence, where a high $L_{\\tilde{v}qa}$ indicates a completely uncon-trolled self-generated question. $L_{reg}$ represents the likelihood that the answerer successfully predicts the target answer from the self-generated question, where a high $L_{reg}$ is likely to indicates that there is no meaningful semantics in the self-generated question. $L_{\\tilde{v}qa}$ and $L_{reg}$ complement and reinforce each other, so we use them together to represent the qual-ity of self-generated questions. As shown in Figure 4, both $L_{\\tilde{v}qa}$ and $L_{reg}$ demonstrate the pattern that \"the lower the quality of the self-generated question, the higher the un-certainty\u201d. This insight further provides a solid explanation for the effectiveness of our proposed EDL-based filter."}, {"title": "Conclusion, Limitation and Future Work", "content": "In this paper, we present BoViLA, a pioneering frame-work that enhances video-language alignment through self-questioning and answering. BoViLA utilizes a unique boot-strapping approach where the model alternates between the roles of questioner and answerer, enabling it to generate and answer self-created questions, thereby deepening modal-ity alignment without reliance on extra annotated data by fully leverage thr rich information within videos and inter-nal knowledge in LLMs. Our framework also improve the vanilla EDL method and incorporates it to assess and filter the quality of self-generated questions. BoViLA demonstrates superior performance across five VideoQA benchmarks, out-performing current state-of-the-art methods with only a few trainable parameters.\nDespite its success, the framework faces limitations in a constrained question generation space because our questioner always generates new questions by autoregressively predict-ing the next token based on the context of the seed question. Future work will focus on exploring how to sample more freely from the joint distribution of question samples during training, in order to generate more diverse self-generated questions."}]}