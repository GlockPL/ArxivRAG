{"title": "Concept-Based Explainable Artificial Intelligence: Metrics and Benchmarks", "authors": ["Halil Ibrahim Aysel", "Xiaohao Cai", "Adam Prugel-Bennett"], "abstract": "Concept-based explanation methods, such as concept bottleneck models (CBMs), aim to improve the interpretability of machine learning models by linking their decisions to human-understandable concepts, under the critical assumption that such concepts can be accurately attributed to the network's feature space. However, this foundational assumption has not been rigorously validated, mainly because the field lacks standardised metrics and benchmarks to assess the existence and spatial alignment of such concepts. To address this, we propose three metrics: the concept global importance metric, the concept existence metric, and the concept location metric, including a technique for visualising concept activations, i.e., concept activation mapping. We benchmark post-hoc CBMs to illustrate their capabilities and challenges. Through qualitative and quantitative experiments, we demonstrate that, in many cases, even the most important concepts determined by post-hoc CBMs are not present in input images; moreover, when they are present, their saliency maps fail to align with the expected regions by either activating across an entire object or misidentifying relevant concept-specific regions. We analyse the root causes of these limitations, such as the natural correlation of concepts. Our findings underscore the need for more careful application of concept-based explanation techniques especially in settings where spatial interpretability is critical.", "sections": [{"title": "1. Introduction", "content": "In recent years interest in explainable artificial intelligence (XAI) methods has grown substantially because of the desire to exploit the success of newly developed machine learning methods to new areas of our lives (Goebel et al., 2018; Buhrmester et al., 2021; Van der Velden et al., 2022; Ali et al., 2023; Hassija et al., 2024). In an attempt to make XA\u0399 more understandable to the layman there has been a growing drive to develop techniques that provide explanations in terms of human-understandable concepts (Bau et al., 2017; Kim et al., 2018; Koh et al., 2020; Havasi et al., 2022; Aysel et al., 2023; Shin et al., 2023). One of the big challenges of concept-based XAI methods is that of paramount importance yet lacks a systematic study to ensure that the concepts identified as important to making a decision properly align with human understanding of the concepts.\nIn this paper, we propose three new metrics for measuring this alignment. The first is the concept global importance metric (CGIM), measuring the concept alignment for each image in a class. The second is the concept existence metric (CEM), measuring whether the concepts identified as important for making a classification exist in an image. For example, if the horn is identified as the most important concept for deciding the image is a rhinoceros then we should expect the horn to be visible in the image. The third metric is the concept location metric (CLM), measuring whether the excitable region of the feature maps used to determine an important concept is close to the location where we would expect the concept to be. In the example above, we would expect the heatmap representing the area of the feature map that corresponds to the horn concept should be located around the horn. Using these three metrics, we create a benchmark problem using the Caltech-UCSB Bird (CUB) dataset (Wah et al., 2011), and test the performance of concept-based XAI methods.\nTo illustrate the usefulness of our metrics, we examine a prominent example of a concept-based XAI system known as the post-hoc concept-bottleneck models (CBMs) (Yuksekgonul et al., 2023). This method is designed to provide explanations of classifiers based on deep neural network (DNN). The method is a synthesis of two approaches \u2013 traditional CBMs (Koh et al., 2020) and concept activation vectors (CAVs) (Kim et al., 2018) \u2013 for concept-based explanations. Traditional CBMs are a relatively straightforward approach to introducing human-understandable concepts into XAI. In traditional CBMs, we start from a network trained to classify a set of classes and replace the final few layers with a new set of layers that are trained to predict human-understandable concepts, which provides a \"concept bottleneck\". From this concept representation, a fully connected layer is trained to predict the classes. Given a new"}, {"title": "2. Preliminary", "content": "Let $\\mathcal{X}$ be the set of images, $\\mathcal{U}$ be the set of concept labels, and $\\mathcal{Y} = \\{1,2,\\ldots, K\\}$ be the set of $K$ class labels. Let $\\mathcal{S} = \\{(X_i, U_i, Y_i, A_i, P_i) \\mid X_i \\in \\mathcal{X}, U_i \\in \\mathcal{U}, Y_i \\in \\mathcal{Y}, i = 1,2,..., N\\}$ be the training set with $N$ samples, where $u_i \\in \\{0,1\\}^L$ is the concept label vector with $L$ different concepts for image $X_i \\in \\mathbb{R}^{M_1 \\times M_2 \\times M_3}$ ($M_3 = 3$ for RGB images), $Y_i \\in \\mathbb{R}^K$ (a one-hot vector) denotes the class label of image $X_i$, $A_i$ is the set containing the indexes of activated concepts for image $X_i$ (i.e., the indexes of the components in $u_i$ with value 1), and $P_i = \\{p_{i1},\\ldots,p_{iL}\\}$ is the set holding centre pixel coordinates $p_{ij}$ of concept $j$ with $j = 1,\\ldots, L$ for image $X_i$. Let $f : \\mathcal{X} \\rightarrow \\mathbb{R}^d$ be a $d$-dimensional feature extractor, which can be any trained DNN such as ResNet (He et al., 2016) or VGG (Simonyan & Zisserman, 2014). From block @ in Figure 1, we see that the feature vector $f(X_i)$ consists of the post-GAP features (i.e., the features right after the GAP layer). Let $E_i \\in \\mathbb{R}^{H\\times W\\times d}$ represent the pre-GAP feature maps (i.e., the features right before the GAP layer), where $H, W$ and $d$ denote the height, width and depth (i.e., the number of channels). The $k$-th channel of $E_i$ is represented as $E_i(:,:,k) \\in \\mathbb{R}^{H\\times W}$.\nCAVs. Following Kim et al. (2018) and Yuksekgonul et al. (2023), for $j = 1,..., L$, to generate the CAV $c_j \\in \\mathbb{R}^d$ for the $j$-th concept, two sets of image embeddings through $f$ are needed, i.e., $\\mathcal{N}_{pos}$ for positive examples and $\\mathcal{N}_{neg}$ for negative ones. In detail, set $\\mathcal{N}_{pos}$ consists of embeddings of $N_p$ images (positive examples) that contain the $j$-th concept, and set $\\mathcal{N}_{neg}$ consists of embeddings of $N_n$ randomly chosen images (negative examples) that do not contain the concept. Sets $\\mathcal{N}_{pos}$ and $\\mathcal{N}_{neg}$ are then used to train an SVM with $c_j$ being the obtained normal vector to the hyperplane separating sets $\\mathcal{N}_{pos}$ and $\\mathcal{N}_{neg}$. All together, these $L$ number of CAVs form a concept bank $\\mathcal{C} = (c_1,\\ldots, c_L)^\\top \\in \\mathbb{R}^{L\\times d}$. For an image $X_i$, the feature vector $f(X_i)$ is to be projected onto the concept space by $\\mathcal{C}$, i.e., $\\mathcal{C}f(X_i) \\in \\mathbb{R}^L$, which is the concept value vector $\\hat{u}_i$ to be fed to the classifier.\nTraditional vs. post-hoc CBMs. After the feature vector $f(X_i)$ is obtained for image $X_i$, the traditional CBMs predict concepts by the concept prediction block, while the post-hoc CBMs project the feature vector $f(X_i)$ onto the"}, {"title": "3. Proposed Methodology", "content": "There is a significant gap in the field regarding the evaluation of the explainability power of the well-known concept-based methodologies. To fill this gap and assess the existence and correctness of the concepts given as highly important by XAI techniques, we propose our CoAM (concept activation mapping) framework (see \u20ddf in Figure 1 for an overview), which allows concept visualisation. Moreover, we also propose the CGIM (concept global importance metric) to test the global concept alignment by XAI methods, the CEM (concept existence metric) to evaluate the existence of the concepts, and the CLM (concept location metric) to reveal whether the highly important concepts correspond to the correct regions in a given test image."}, {"title": "3.1. Concept Activation Mapping", "content": "We propose the CoAM framework, which generates concept activation maps revealing the parts of an image that corre spond to the concepts. As we know, for post-hoc CBMs, the pre-GAP feature maps $E_i \\in \\mathbb{R}^{H\\times W\\times d}$ (which contain spatial information) for the examined image $X_i$ become the post-GAP feature vector $f(X_i)$ after the GAP layer, which is then linked to the CAVs, $c_j = (c_{j1}, \\ldots, c_{jd})^\\top$, $j = 1, \\ldots, L$.\nOur introduced concept activation maps, say $F_{ij}$, for $X_i$ corresponding to the $j$-th concept for $j = 1, \\ldots, L, are calculated by\n$F_{ij} = \\frac{1}{d}\\sum_{k=1}^{d}c_{jk}E_i(:, :, k) \\in \\mathbb{R}^{H\\times W} ,$ \ni(2)\ni\ni.e., weighing the pre-GAP feature maps of $X_i$ by the $j$-th CAV; see block \u20ddc in Figure 1. The CoAM framework is also summarised in Algorithm 1, with $F_i$ being the output, where $F_i(:, :, j) = F_{ij}$ for $j = 1, 2, \\ldots, L. Since the size of each $F_{ij}$ is significantly smaller than that of $X_i$, to visualise the concept activation maps in a better way and for localisation assessment, we upsample them to the original image size of $X_i$, denoted by $\\bar{F}_{ij}$, and overlay them on $X_i$. This will tell us what parts of the input image contribute to the individual concepts. Algorithm 2 in the Appendix gives the details of the final feature visualisation pseudo-code."}, {"title": "3.2. Concept Global Importance Metric", "content": "We firstly introduce the global importance score of concept $j$ for class $k$ as $\\theta_{jk}$ [the $(j, k)$-th entry of $\\theta$], i.e., the weight in the classifier $h$ mapping the $j$-th concept to the $k$-th class, for $j = 1, 2, \\ldots, L$ and $k = 1, 2, \\ldots, K$. Let $V \\in"}, {"title": "3.3. Concept Existence Metric", "content": "We now define the local importance score of concept $j$ for class $k$ as $\\theta_{jk}\\hat{u}_{ij}$; note that $\\hat{u}_{ij}$ is the obtained $j$-th"}, {"title": "3.4. Concept Location Metric", "content": "After checking whether the obtained important concepts of image $X_i$ exist in the ground-truth set $A_i$ with CEM and generating concept activation maps with CoAM, we now propose CLM to assess whether the obtained concepts of image $X_i$ correspond to the correct region in $X_i$."}, {"title": "4. Experiments", "content": "In this section, we present benchmark results and evaluate the performance of the post-hoc CBMs using our proposed metrics. The benchmark fine-grained bird classification dataset, Caltech-UCSD Birds (CUB) (Wah et al., 2011), with concept annotations such as wing color, beak shape and feather pattern is employed for the experiments. It consists of 200 different classes and 112 binary concept labels for around 11, 800 images. Additionally, the central pixel locations of 12 different body parts are provided and used for concept localisation assessment by the proposed CLM. Following Yuksekgonul et al. (2023), we employ a ResNet-18 (He et al., 2016) trained on the CUB dataset\u00b9 as the feature extractor $f$. CAVs are calculated as explained in Section 3 to create a concept bank C (also see in Figure 1). Finally, a single layer $h$ with weights $\\theta\\in \\mathbb{R}^{112\\times 200}$ is trained for the classification."}, {"title": "4.1. Post-hoc CBMs Reproduction", "content": "By employing the same model as the feature extractor and following the same steps for CAVs and classifier training, we reproduce the results of post-hoc CBMs (Yuksekgonul et al., 2023) with various hyperparameter combinations. The details of the post-hoc CBMs reproduction is given in the Appendix."}, {"title": "4.2. Global Importance Evaluation", "content": "We now investigate the quality of the global explanations of the post-hoc CBMs. Recall that the entries of $\\theta \\in \\mathbb{R}^{112\\times 200}$ are considered as the global importance scores, determining the importance of a concept for an examined class. Ideally, these weights should closely align with human annotations in $V\\in \\mathbb{R}^{112\\times 200}$, i.e., the so-called ground truth. Intuitively, we expect the CGIM scores $\\rho^{\\text{CGIM1}}_j$, $\\rho^{\\text{CGIM2}}_j$, and $\\rho^{\\text{CGIM3}}_j$ of $\\Theta$, $U^*$, and $\\hat{U}$ corresponding to $V$ for each concept $1 \\leq j \\leq 112$ (and analogously for each class $1 \\leq k \\leq 200$) to be close to 1 if the obtained $\\Theta$, $U^*$, and $\\hat{U}$ are meaningful."}, {"title": "4.3. Concept Existence Evaluation", "content": "After analysing the global importance evaluation based on classifier's weights and average concept predictions, we, in this section, focus on the local importance analysis. The first step in this regard is to assess the concept existence qualitatively and quantitatively."}, {"title": "4.3.1. QUALITATIVE OBSERVATIONS", "content": "When a set of concepts is presented as highly important for a prediction by a trained model, it is essential to qualitatively verify whether these concepts really exist in the image. In Figure 3, we present random images from the test set with the top 5 most important concepts for their prediction outputted by the reproduced post-hoc CBMs. As shown in Figure 3, many of those highly important concepts do not actually exist in the given images. For instance, for an American Redstart in the first column, the most important"}, {"title": "4.3.2. QUANTITATIVE TEST BY CEM", "content": "We calculate the CEM score over the entire test set. The full results are presented in Table 1 in terms of ranking the importance of the concepts based on i) the weights of the classifier $\\theta_{jk}$, ii) the projected concept values $\\hat{u}_{ij}$, and iii) their combination $\\theta_{jk}\\hat{u}_{ij}$, for the top $l$ most important concepts with $l$ set to 1, 3, and 5. The results show that the CEM score based on $\\hat{u}_{ij}$ is significantly higher than the others, which is intuitive at first glance as the highest values after concept projection are highly likely to be present in the ground-truth label. However, as detailed in Section 3, the concept values in $\\hat{u}$ do not independently determine the final class prediction; instead, these values are weighted by their respective weights in $\\theta$, which can significantly alter their overall impact. Relying solely on the projected concept values in $\\hat{u}$ may therefore lead to misleading conclusions. Hence, we build our argument based on $\\theta_{jk}\\hat{u}_{ij}$ rather than solely on $\\hat{u}_{ij}$ or $\\theta_{jk}$. Strikingly, as shown in Table 1, the single most important concept (i.e., when $l$ = 1) only exists in the images around 55% of the times when the image is correctly classified. This score drops to 49% when the test is done on the entire test set. Moreover, the CEM score is even lower when $l$ is set to 3 and 5."}, {"title": "4.4. Concept Localisation Evaluation", "content": "4.  4.  1. Qualitative Observations\nBy visualizing concept heatmaps for different concepts using our proposed CoAM, we identified several recurring patterns. Figure 4 in the Appendix presents some examples of class and concept visualisation by using our CoAM. In many cases, the concept activation maps cover broad image regions, often extending beyond the expected concept areas. For instance, when detecting the concept grey leg in a white breast nuthatch image, the concept map covers the entire body of the bird rather than focusing on the specific region around the leg, as shown in the first row in Figure"}, {"title": "4.4.2. QUANTITATIVE TEST BY CLM", "content": "To be able to calculate the CLM score, the centre pixel coordinates for individual concepts are needed. In the CUB dataset, the centre pixel coordinates are only available for 12 broader body parts such as beak, throat, and leg. Fortunately, most of the 112 concepts are related to one of the 12 body parts, allowing us to match each concept to its closest body part and hence exploit the corresponding body-part coordinates for concepts. For instance, we match the hooked seabird beak concept with the beak part and the solid wing concept with the wing; see Tables 4 and 5 in the Appendix for details. We ignore concepts that are not related to a specific body part such as overall size, shape and colour information, which leaves us with 89 out of 112 concepts for the CLM evaluation.\nRecall that after obtaining the activation map for the $j$-th concept $F_{ij}$ of $X_i$, CLM checks if the centre pixel location $p_{ij}$ falls into the highest activated region $\\mathcal{N}_{ij}$. Here $\\mathcal{N}_{ij}$ is formed by the $\\alpha(M_1M_2)/12$ number of pixels in terms of the largest pixel intensities in $F_{ij}$, where $\\alpha$ is a hyperparameter that allows changing the region's size. For example, $\\alpha$ = 1 means the 1/12 of the image is scanned, which is the size of a rough area for each of the 12 body parts such as beak, back and throat as given in Tables 4 and 5."}, {"title": "5. Discussion", "content": "Learning human-understandable concepts is a challenging task. Often concepts are highly correlated with other features. For example, although hooves clearly relate to the feet"}, {"title": "6. Conclusion", "content": "In this paper, we proposed three novel metrics, i.e., CGIM, CEM and CLM, for concept-based XAI systems. CGIM provides a way to measure the global concept alignment ability of concept-based XAI techniques. CEM and CLM are introduced for local importance evaluation, testing if highly important concepts proposed by XAI techniques exist and can be correctly localised in a given test image, respectively. Employing these three metrics, we benchmarked post-hoc CBMs on the CUB dataset. Our experiments demonstrated significant limitations in current post-hoc methods, with many concepts and classes found to be weakly or even negatively correlated with their ground-truth labels by CGIM. Moreover, many concepts presented as highly important are not found to be present in test images by CEM, and their concept activations fail to align with the expected regions of the input images by CLM. As the field of XAI continues to evolve, it is essential to ensure that methods not only provide understandable concepts but also accurately predict"}, {"title": "7. Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Related Work", "content": "This section recalls concept-based methodologies for XAI and examines existing variants of class activation mapping (CAM) highlighting the need for a dedicated approach to concept visualisation (which can be addressed by our CoAM).\nNetwork dissection (Bau et al., 2017) is one of the well-known concept-based approaches, where individual neurons in a network are examined to identify their correspondence to human-understandable concepts like edges, textures, or objects. By aligning neuron activations with segmentation-annotated images, network dissection quantifies how well a model's internal representations map to meaningful concepts. However, this method is computationally expensive and data-intensive, requiring large and richly labelled datasets to accurately associate neurons with interpretable concepts. Despite its valuable insights, these limitations have prompted the development of more efficient and flexible methods, such as CAVs and CBMs.\nTesting with CAVs (TCAV) framework (Kim et al., 2018) introduced CAVs to explain model predictions based on high-level human-interpretable concepts. CAVs represent directions in the latent space of a model corresponding to specific concepts, allowing for sensitivity analysis. By perturbing an input in the direction of a concept vector, TCAV measures how much the model's prediction depends on that specific concept, offering quantitative insights into the reliance on different concepts for a given task. TCAV has been applied in several fields to assess whether models depend on sensitive attributes like gender or race when making decisions. Recent adaptations have improved the computational efficiency and robustness of CAVs when applied to large-scale models (Ghorbani et al., 2019). However, TCAV can only unveil the global effect of concepts on examined classes and not on individual samples. Therefore, it is unable to directly assess the concept predictions or provide spatial concept localisation for individual images.\nCBMs (Koh et al., 2020) offers a significantly different approach to interpretability. They enforce that intermediate representations of the model correspond to human-understandable concepts, such as attributes (e.g., colour, shape, part) of objects in an image. By constraining the model to predict based on these explicit concepts, CBMs inherently provide an interpretable mechanism for understanding decisions. This makes it easier to debug and correct errors by diagnosing the model's performance on individual concepts. Recent work in CBMs has focused on improving robustness, especially when concept labels are noisy or incomplete. For instance, in Label-free CBMs (Oikarinen et al., 2023), a method was proposed using unsupervised techniques to learn concept bottlenecks, thereby extending the applicability of CBMs to scenarios where manual labelling is expensive or impractical. Despite their interpretability, CBMs typically lack the ability to provide spatial visualisations, limiting their usefulness in tasks that require precise localisation of important concepts.\nThe multilevel XAI method in (Aysel et al., 2023) offers solutions for both expensive annotation needs and single-level output drawbacks of CBMs. The cost-effective solution to CBMs is achieved by only requiring class-wise concept annotations rather than per-image. Moreover, the multilevel XAI method provides concept-wise heatmaps by-product handling the single-level limitation of CBMs. To be more precise, different from other CBM approaches, the explanations by the multilevel XAI method are not only raw concept values, but also each concept comes with its saliency map that highlights the region in the image activated by that concept. The authors in Aysel et al. (2023) have also shown the possibility of concept intervention on the input dimension, which is much more intuitive than the concept dimension. To give an example, in other CBMs, one may tweak the concept value, say, \u201cwhite\u201d at the bottleneck layer to flip the prediction, say, from polar bear to grizzly bear. In the multilevel XAI method, one can convert the white colour region in the image to brown to achieve the same flipping, which is more intuitive and reliable.\nA breakthrough in visual explanations came with the introduction of CAM (Zhou et al., 2016), which provides spatial localisation by computing class-specific activation maps that highlight the regions of an image most relevant for a given prediction. CAM operates by utilising the output of GAP layers in CNNs, enabling the generation of heatmaps that represent regions crucial for the final classification. This approach was generalised in Grad-CAM (Selvaraju et al., 2017), which makes use of the gradients flowing into the final convolutional layer to visualise where the model \u201clooks\u201d when making a decision. Grad-CAM extends CAM to more general architectures without requiring specific layers like GAP. However, Grad-CAM does not always provide sharp localisation, especially when multiple objects are present in the image. Grad-CAM++ (Chattopadhay et al., 2018) addresses this limitation by refining the localisation to better handle multiple instances of objects, offering a more fine-grained interpretation. Further extensions include Score-CAM (Wang et al., 2020), which eliminates the dependency on gradients, instead using the activations themselves to weigh different regions of the input. This addresses some of the instability associated with gradient-based methods but comes with increased computational overhead. Other advancements like Ablation-CAM (Ramaswamy et al., 2020) explore removing parts of the model and input to measure their impact on predictions, thus improving interpretability."}, {"title": "B. Limitations", "content": "There are drawbacks to the metrics CEM and CLM that we propose. The CEM can only be used on datasets where we have per-image annotations of the concepts for a test set (note that for our case, only a small number of concepts like the top $l < L$ are required per-image, and therefore is cheap). This limits its use to a very small number of datasets. Having a metric limited to one (or a small number of datasets) runs the risk that models are developed that overfit to that particular dataset. The CLM requires knowledge of the location of the concepts. In fact, the concept locations were not given and we had to do a \"best guess\" approximation of whether the concepts found in the \u201csaliency maps\u201d overlap with the real concept locations. It is also debatable whether the heatmaps we obtained by weighting the feature maps before doing GAP correctly capture the location of the concepts. In our judgment, this seems as fair an estimate of the position as we can make. We feel there is considerable value in visualising the location of a concept through the use of heatmaps. In Aysel et al. (2023), the authors built saliency maps for each concept, but there they aligned each feature map to a concept which prevented cross-contamination between concept locations. By providing visualisations of the parts of the image that activated the concept, it made it much easier to assess the alignment of concepts in that model. We have attempted to provide a similar visualisation for the post-hoc CBMs (Yuksekgonul et al., 2023), although as this is not part of the design of that model the visualisation may not be perfect. Finally, reducing the assessment of alignment to a couple of numbers loses a lot of fine-grain detail. As we illustrated, we can get a better understanding of the failure of the network by examining the performance in more detail, for example, by plotting histograms of the CBMs results to identify particularly poor concepts, or by visualising the locations of the features to understand what concepts might be being learnt.\nDespite those drawbacks, we believe that proposing a new benchmark for assessing concept alignment has the potential to concentrate the effort of researchers on improving the performance of concept-based XAI systems. As we have illustrated, the performance of post-hoc CBMs is surprisingly poor. Without doing a systematic analysis of this alignment, it is easy to overlook this problem and believe that an XAI system is more powerful than it actually is. Our hope is that by introducing new metrics and benchmarks we can improve the accuracy of future concept-based XAI systems."}, {"title": "C. Post-hoc CBMs Reproduction \u2013 Details", "content": "By employing the same model as the feature extractor and following the same steps for CAVs and classifier training, we reproduce the results of post-hoc CBMs (Yuksekgonul et al., 2023) with various hyperparameter combinations. There are two hyperparameters to tune during the SVM training for CAV learning, i.e., $N_p$ and $N_n$ (the number of positive and negative images per concept), which we set to 50 and 100, respectively. The other hyperparameter is the regularisation parameter $\\lambda$ in SVM, which controls the trade-off between maximising the margin that separates classes and minimising classification errors on the training data. A low $\\lambda$ value allows the model to prioritise a wider margin, even if some data points are misclassified, making the model more robust to noise and potentially improves its generalisation of new data. In contrast, a high $\\lambda$ value forces the SVM to minimise the training error, making it less tolerant of misclassifications and resulting in a narrower margin. While a high $\\lambda$ can lead to more accurate training performance, it may also increase the risk of overfitting, as the model becomes more sensitive to individual data points. Thus, $\\lambda$ helps balance the SVM's complexity and flexibility, impacting its ability to generalise well.\nWe train SVM with $\\lambda$ values ranging from 0.001 to 10. Table 3 shows the classification accuracy of the classifier $h$ with various concept banks obtained by these hyperparameter combinations. For the experiments in the main paper, we employ the model with the best classification accuracy 59.1%, which is achieved when $N_p$ = $N_n$ = 100 and $\\lambda$ = 1. This result is very close to the accuracy 58.8% reported in the seminal work (Yuksekgonul et al., 2023). Note that there is more than 15% accuracy loss in comparison to the traditional model, i.e., the one without a concept bottleneck (i.e., $\\Theta$ + $\\mathcal{g}$ in Figure 1), for"}, {"title": "E. Concept Visualisation Algorithm", "content": "Concept activation maps in $F_i$ for image $X_i$ are obtained as summarised in Algorithm 1. These maps are at smaller resolutions since they are at a late layer of the trained DNN. Algorithm 2 below demonstrates the steps to upsample these maps to the input size and we use them to mask the input image $X_i$."}]}