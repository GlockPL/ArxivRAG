{"title": "CMU'S IWSLT 2024 Simultaneous Speech Translation System", "authors": ["Xi Xu", "Siqi Ouyang", "Brian Yan", "Patrick Fernandes", "William Chen", "Lei Li", "Graham Neubig", "Shinji Watanabe"], "abstract": "This paper describes CMU's submission to the IWSLT 2024 Simultaneous Speech Translation (SST) task for translating English speech to German text in a streaming manner. Our end-to-end speech-to-text (ST) system integrates the WavLM speech encoder, a modality adapter, and the Llama2-7B-Base model as the decoder. We employ a two-stage training approach: initially, we align the representations of speech and text, followed by full fine-tuning. Both stages are trained on MuST-c v2 data with cross-entropy loss. We adapt our offline ST model for SST using a simple fixed hold-n policy. Experiments show that our model obtains an offline BLEU score of 31.1 and a BLEU score of 29.5 under 2 seconds latency on the MuST-C-v2 tst-COM\u039c\u039f\u039d.", "sections": [{"title": "Introduction", "content": "This paper presents CMU's submission to the IWSLT 2024 (Carpuat et al., 2024) Simultaneous Speech Translation (SST) task, focusing on streaming English speech to German text translation. Recent advancements in large language models (LLMs) have demonstrated their potential to be a strong backbone for offline ST (Huang et al., 2023; Zhang et al., 2023). In this year's submission, we build an end-to-end offline ST model with WavLM (Chen et al., 2022) and Llama2-7B-Base (Touvron et al., 2023) following the practice of LST (Zhang et al., 2023). Then we adapt the offline model for simultaneous translation.\nWe prepare our end-to-end ST model in the following steps:\n1. Offline ST with WavLM and Llama2-7B-base.\n2. Online adaptation of offline model via hold-n policy and incremental beam search."}, {"title": "Task Description", "content": "The IWSLT 2024 SST track\u00b9 English-German direction is a shared task for streaming speech-to-text translation of English TED talks. The task requires the system to generate the translation without modifying its previous outputs. The average lagging (AL) (Ma et al., 2019) of SST systems must be below 2 seconds on MuST-C v2.0 tst-COMMON set (Di Gangi et al., 2019). Note that AL has been modified from its original definition (Ma et al., 2020a). Following the constraint of data and pretrained weights, we use MuST-C v2.0 as the only training set and leverage pretrained models of WavLM and Llama2-7B-Base."}, {"title": "System Description", "content": "As shown in Figure 1, our offline ST models consists of three primary components: a speech encoder, an adapter, and a LLM decoder.\nFor the speech encoder, we employ the WavLM model 2, which has been pre-trained on 94,000 hours data including LibriLight (Kahn et al., 2020), VoxPopuli (Wang et al., 2021) and GigaSpeech (Chen et al., 2021). We use the output of last encoder layer as the speech representation.\nThe modality adapter consists of two components: a length adapter and a modality adapter. The length adapter consists of two 1-dimensional convolutional layers with a kernel size of 5, a stride size of 2, padding of 2, and a hidden size of 1024. The modality adapter is a linear layer that projects the output of the length adapter to the embedding space of LLM.\nWe use Llama2-7B-Base as the LLM decoder. The LLM decoder takes the output of the modality adapter and autoregressively generate the target text translation."}, {"title": "Offline Speech Translation (ST)", "content": "For each sample, given speech XS, the reference translation XT, and the prompt XP, we initially transform the speech signal into a feature representation via the speech encoder:\n\\(HS = Encoder(XS),\\) (1)\nwhere \\(HS = [h_1^S,...,h_T^S]\\) with T denoting the sequence length of the feature representation. To reconcile the length difference between the speech feature sequence HS and its corresponding text, we downsample the speech with a length adapter. To clarify further, the length adapter transforms HS using a pair of 1-dimensional convolutional layers, which can be represented as:\n\\(ZS = Length adapter(HS; k, s,p, h),\\) (2)\nwhere k is the kernel size, s is the stride, p is the padding, and h denotes the number of convolutional filters. The reduced temporal dimension is ZS = \\([z_1^S, ..., z_N^S]\\), where\n\\(N = \\frac{T - k + 2P + 1}{S},\\) (3)\nNext, a projector is applied to transform the speech features ZS into ES with the same dimension as the LLM input embedding. We use a single hidden layer as the projector,\n\\(ES = Linear(ZS).\\) (4)\nFinally, we feed the speech embedding ES, translation embedding ET, and prompt embedding EP into the template to compose the final input E of LLM,\n\\(ET = Emb(Tokenizer(XT)),\\) (5)\n\\(EP = Emb(Tokenizer(XP)),\\) (6)\n\\(E =\n\\begin{cases}\nTemplate(ES, EP, ET) & \\text{if training,}\\\\\nTemplate(ES, EP, ET) & \\text{if inference,}\n\\end{cases}\\) (7)\nwhere Emb is the LLM embedding layer, ET is the embedding of model's previously generated tokens. The template is formatted as:\n<P> USER: <S> ASSISTANT:<T>\nwhere <P> represents the system prompt\u00b3, <S> denotes the speech embedding, and <T> is the target reference or generated translation.\nWe finetune our offline ST model following a 2-stage strategy. In the first stage, we finetune the speech encoder together with the adapters, while keeping the LLM frozen. In the second stage, we finetune the entire model. We employ cross entropy loss in both stages. In addition, we apply rule-based filtering (Ouyang et al., 2022) of the dataset"}, {"title": "Simultaneous Speech Translation (SST)", "content": "We adapt our offline ST model for streaming inference using hold-n policy. Our scheme uses a fixed duration (e.g. 2 seconds) to compute the encoder representations on chunks of input speech. With each new chunk, we re-compute the encoder representations using the entire given input speech.\nAs shown in Algorithm 1, for each chunk c, we obtain the corresponding hypotheses W(c) using beam search given partial speech input. We then determine the number of tokens n' to withhold based on the minimum of the predefined value n and the length of the current chunk's hypotheses l. The prefix W0:l-n' is obtained by selecting the tokens from index 0 to l -n'."}, {"title": "Experimental Setup", "content": "We use the AdamW optimizer with a cosine learning rate decay and a warmup ratio of 0.2. The learning rate commences at 2e-4 for the first training stage and is reduced to 2e-5 for the second stage. We train the first stage for 6 epochs and train the second stage for 1 epoch.\nWe employ an early stopping strategy with a patience of 6 epochs, evaluating every 1000 steps in Stage 1 and every 200 steps in Stage 2. The batch size is set to 128 for both stages. All models are trained on 4 Nividia A6000 GPUs with Deepspeed's ZERO training strategy. The training times for the first and second stages are approximately 29 hours and 9 hours, respectively. We select the checkpoints with the lowest dev loss for testing.\nFor offline testing, we use a beam size of 4 to generate translations. In the simultaneous testing scenario, we set the start seconds to 2, indicating the initial wait time before processing speech chunks. We employ a hold-n strategy with n set to 7, meaning that the last 7 tokens of each chunk are withheld until more context is available. The beam size is set to 4, and the chunk size is set to 2500ms.\nWe evaluate translation quality using SacreBLEU (Post, 2018). We evaluate translation latency for SST with average lagging (AL) (Ma et al., 2020b) and length-adaptive average lagging (LAAL) (Papi et al., 2022) using SimulEval toolkit (Ma et al., 2020b)."}, {"title": "Results", "content": "Table 1 shows the quality and latency of our SST system as measured on En-De tst-COMMON. We also include the offline ST performance of our model for reference. We implement the Alignatt policy (Papi et al., 2023) as a baseline for our model, we set start seconds to 2, speech segment size to 1000ms. We set number of frames to 20 and use attention from all layers of the LLM decoder with greedy decoding.\nFrom ST to SST, we observe a 5% quality degradation (31.1 to 29.5 SacreBLEU). However, this comes with significant latency improvements. The Average Lagging (AL) decreases from 5.85 to 1.96 seconds, a 66.5% reduction. The Length Adaptive Average Lagging (LAAL) improves from 5.85 to 2.22 seconds, a 62.1% decrease.\nWe also investigate the impacts of different LLMs and speech encoders, as shown in Table 2. We compare WavLM with a CTC fine-tuned Wav2vec 2.0 large model4. This Wav2vec model was pre-trained on 53.2k hours of untranscribed speech from LibriVox and fine-tuned on 960 hours of transcribed speech from Librispeech, as well as on pseudo-labels. Our results show that replacing Wav2vec with WavLM yields a significant improvement: a 1.1 BLEU score increase when using the Tower LLM (Alves et al., 2024) as the decoder, and a 0.3 BLEU score increase with LLaMA2 as the decoder. This suggests that the performance gains from a well-pretrained speech encoder are more pronounced when coupled with LLMs of higher translation capability.\nOur analysis of the performance between different LLMs used as decoders shows that the Tower LLM5, subjected to continued pre-training on a curated multilingual dataset of 20 billion high-quality tokens, exhibits a marked performance advantage over LLaMA2 in the initial stage of training. However, during the second stage, when the LLM backend is trainable, Tower quickly overfits, implying potential overlap between the MuST-C corpus and the data involved in Tower's pretraining. Tower Instruct, which undergoes supervised fine-tuning (SFT) on instruction dataset for various translation-related tasks, achieves a slightly lower BLEU score compared to the base model. To mitigate overfitting during the second stage of training with Tower, a reduced learning rate of 7e-6 is used, compared to the 2e-5 learning rate applied to LLaMA2 training."}, {"title": "Conclusion", "content": "In this paper, we describe the submission of CMU's English to German simultaneous speech-to-text translation systems for the IWSLT 2024 Simultaneous track. We start by building a offline speech-to-text system which leverages self-supervised speech and text foundation models. We then adapt this offline model for streaming inference, enabling simultaneous speech-to-text translation."}]}