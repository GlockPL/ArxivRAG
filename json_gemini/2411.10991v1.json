{"title": "MODULATING RESERVOIR DYNAMICS VIA REINFORCEMENT LEARNING FOR EFFICIENT ROBOT SKILL SYNTHESIS", "authors": ["Zahra Koulaeizadeh", "Erhan Oztop"], "abstract": "A random recurrent neural network, called a reservoir, can be used to learn robot movements conditioned on context inputs that encode task goals. The Learning is achieved by mapping the random dynamics of the reservoir modulated by context to desired trajectories via linear regression. This makes the reservoir computing (RC) approach computationally efficient as no iterative gradient descent learning is needed. In this work, we propose a novel RC-based Learning from Demonstration (LfD) framework that not only learns to generate the demonstrated movements but also allows online modulation of the reservoir dynamics to generate movement trajectories that are not covered by the initial demonstration set. This is made possible by using a Reinforcement Learning (RL) module that learns a policy to output context as its actions based on the robot state. Considering that the context dimension is typically low, learning with the RL module is very efficient. We show the validity of the proposed model with systematic experiments on a 2 degrees-of-freedom (DOF) simulated robot that is taught to reach targets, encoded as context, with and without obstacle avoidance constraint. The initial data set includes a set of reaching demonstrations which are learned by the reservoir system. To enable reaching out-of-distribution targets, the RL module is engaged in learning a policy to generate dynamic contexts so that the generated trajectory achieves the desired goal without any learning in the reservoir system. Overall, the proposed model uses an initial learned motor primitive set to efficiently generate diverse motor behaviors guided by the designed reward function. Thus the model can be used as a flexible and effective LfD system where the action repertoire can be extended without new data collection.", "sections": [{"title": "1 Introduction", "content": "Learning-from-demonstration (LfD) has become an effective method to equip robots with movement skills in diverse tasks, from basic manipulation to complex navigation Schaal [1999], Argall et al. [2009], Chernova and Thomaz [2014]. By generalizing from human demonstrations, LfD enables robots to replicate intricate behaviors with minimal manual programming. This capability has made LfD an attractive approach in robotics, especially for applications requiring rapid learning and adaptation Urain et al. [2024], Ravichandar et al. [2020]. In behavior cloning (BC)Pomerleau [1988], a common LfD method, the robot learns to map observed states to demonstrated actions, enabling it to achieve task goals based on previous examples.\nHowever, Behavior Cloning (BC) methods are highly dependent on the training data distribution, making them susceptible to covariate shift problemPomerleau [1988], where discrepancies between the training and deployment data lead to compounding errors over timeRoss and Bagnell [2010], thereby limiting their ability to generalize to"}, {"title": "2 Related Work", "content": "Reservoir Computing (RC) Nakajima and Fischer [2021] is a learning framework that utilizes a fixed random recurrent neural network(RNN)Medsker et al. [2001], called the reservoir, to transform a given input or input sequence into a high-dimensional dynamical system, which then can be mapped to a desired temporal sequence via linear regression. Since only the output layer is trained, RC is adopted as a lightweight learning system applicable to time series prediction and control tasks, where temporal dependencies are criticalBaldini [2022]. One of the most commonly used implementations of Reservoir Computing (RC) is the Echo State Network (ESN), introduced in Jaeger [2007]. The reservoir of an ESN is required to have the so called echo state property, which ensures that the internal states respond transiently to inputs and gradually fade, providing temporal stability and short-term memory. This property has proven effective not only for time-series tasks, such as those in Xu and Han [2016], Rodan and Tino [2010], Li et al. [2012], but also for robotic applications.\nThe memory capacity and simplicity of ESNs have been effectively utilized in robotic control tasks, as demonstrated in Hartland et al. [2009], Hartland and Bredeche [2007], where ESNs serve as controllers, and in Annabi et al. [2020], where an ESN is used to generate motor trajectories. In this latter application, the ESN is combined with a perception module and a controller to achieve adaptive trajectory generation. A recent extension of ESNs, known as Context-based Echo State Networks (CESNs), has an additional context input, that may encode target position, obstacle size, etc. so that trajectories are generated conditioned on the context.\nRecent works, such as Kawai et al. Kawai et al. [2023] and Kawai et al. Kawai et al. [2024], have attempted to address instability and generalization issues in reservoir computing. In the first study [20], the authors proposed the reBASICS (reservoir of basal dynamics) model to handle tasks with long-term intervals. reBASICS consists of multiple small reservoirs, where the reduced network size helps mitigate instability by minimizing the impact of chaotic dynamics. This approach effectively stabilizes the system, allowing it to reproduce learned time-series data with greater consistency. However, despite this improvement, the model show difficulties in generalizing to new time-series patterns, limiting its capacity to handle unfamiliar scenariosKawai et al. [2023].\nIn the second study Kawai et al. [2024], the authors introduced random sinusoidal oscillations as input signals during task execution to address both instability and generalization issues. This method was effective for the specific tasks"}, {"title": "3 Methodology", "content": "Our motivation is to use a high-dimensional dynamical system that is open to modulation via reinforcement learning in order to expand the trajectory generation capacity of the original system. For this, as the basis network, Context-based Echo State Network (CESN) model [Amirshirzad et al., 2023] is used as the LfD model. We call the LfD learning by CENS as the stage-1 learning. Stage-2 learning involves RL, where stage-1 learning weights are kept fixed and RL module learns to modulate context inputs of CESN to achieve a desired novel task or novel task goal. Stage-2 learning is explained in subsection 3.2."}, {"title": "3.1 Context based Echo State Networks(CESN)", "content": "A CESN consists of an input layer, a reservoir, and an output layer. The input layer may contain an input time series, feedback from the from the, possibly controlled, environment, and contextual information capturing global task goals. The reservoir is a recurrent neural network with fixed weight which is connected to the input layer through fixed weights. The output layer includes linear read-out layer that is trained to generate desired target trajectories for the given inputs and context conditions. We extend the CESN model by allowing the context to be a dynamical variable that can depend on the reservoir and/or environment state, and use a slightly different formalization by having separate input layer weights. We denote the reservoir state (unit activities) at time t with x(t), the control input with u(t), the feedback with f(t), and the context with c(t). Then the reservoir is updated at discrete time steps by using the following equations.\nXnet = Wxx(t \u2212 1) + Wcc(t) + W\u00a3f(t) + Wuu(t)  (1)\nx(t) = (1 - \u03b1)x(t-1) + \u03b1tanh(xnet)  (2)\nWhere $W_x \\in \\mathbb{R}^{N_r\\times N_r}, W_c\\in \\mathbb{R}^{N_r\\times N_c}, W_\u00a3 \\in \\mathbb{R}^{N_r\\times N_y}$, and $W_{in} \\in \\mathbb{R}^{N_r\\times N_u}$ are the reservoir, context, feedback, and the input weight matrices respectively. \u03b1 \u2208 (0, 1] is a smoothing parameter softening the change in reservoir state at each time step. Nr is the reservoir size determined by the designer. Nu, Nc, and Ny are determined by the learning task and indicates the input command (u), context c, and the output (y) dimensions respectively.\nGiven a desired output time series y(t) with corresponding input u(t) and context c(t), one can set up a linear regression problem to map reservoir states x(t) to output y(t), through a linear read-out weight matrix Wout, which can be approximately solved with ridge regression with A as the ridge parameter:\nW_{out} = YX^T (XX^T + \\lambda I_m)^{-1}  (3)\nwhere the rows of Y are made up of $y^T(t)$ and the rows of X are constructed from the reservoir state as [1, x(t)], for t = 1, 2, ..., T, where T is the end time. Once having computed the $N_y \\times (N_r + 1)$ output matrix Wout, one can do predictions given a reservoir state x(t) with\ny (t) = Wout [1, x(t)]  (4)\nAfter this first stage, the computed the Wout matrix together with other fixed random weights are saved to be used in RL-based training in stage-11."}, {"title": "3.2 Dynamic context with Reinforcement Learning", "content": "In the second stage of DARC training (see Figure 1), the purpose is to train the RL module to generate modulated contexts for the extrapolated task. The problem is formalized as a Markov Decision Process (MDP) represented as a tuple < S, A, T, R >, where S is the state space, A is the action space, defined here as a subset of $\\mathbb{R}^{N_c}$, with Nc being the context dimension. The state s \u2208 S can be taken as the state of the controlled system and/or reservoir state x (Equation 2). T represents the transition function or the transition probability distribution that describes how states evolve. In our case, it is directly determined by the reservoir dynamics given in Equation 2. Finally, R defines the reward function, which in our setting is designed to reward the generation of a desired new behavior, which may be an improvement in accuracy over the extrapolated version of the original task or even the generation of a completely novel task behavoir. As usual, the RL objective is to obtain a policy $\u03c0_\u03b8 : S \u00d7 A \u2192 \\mathbb{R}^{N_c}$ that learns how to modulate"}, {"title": "4 Test Platform and Tasks Addressed", "content": ""}, {"title": "4.1 Environment", "content": "The experiments were carried out in the Reacher environment of Gymnasium OpenAI [Towers et al., 2024], which is widely used as a benchmark for reinforcement learning tasks involving robotic arms. The environment features a 2 degrees-of-freedom (DOF) robotic arm attached to a fixed base (see Figure 2b). The objective is to generate joint torques to make the arm reach the desired target points in the 2D plane (see Figure 2a)."}, {"title": "4.2 Evaluation Tasks", "content": "Since DARC is a hybrid model that combines Learning from Demonstration (LfD) and Reinforcement Learning (RL), we evaluate its performance against two component baselines to understand the contributions of each core element: the CESN[Amirshirzad et al., 2023] model as an LfD-only approach, and a standalone Proximal Policy Optimization (PPO) [Schulman et al., 2017] agent as an RL-only approach. The CESN model used here is identical to the one integrated into our DARC model, allowing us to evaluate the effectiveness of dynamic context generation provided by the RL module. Similarly, we trained the standalone PPO agent under the same conditions as our RL module within DARC, including the action generation frequency, reward structure, and number of training episodes. By analyzing DARC alongside these baselines, we can assess the contributions of each component and demonstrate the added value of combining LfD with RL for enhanced generalization and adaptability. For evaluations we designed three experiments as detailed next."}, {"title": "4.2.1 Reaching task", "content": "In the first task, the objective is to control the arm to reach a series of predefined target points scattered throughout the workspace. These target points differ from those used to train the reservoir, challenging DARC to generate appropriate trajectories for unseen locations. Performance is evaluated based on how closely the arm approaches each target and the efficiency of task completion."}, {"title": "4.2.2 Reaching while avoiding obstacle task", "content": "In the second task, we increased the difficulty of the task by introducing an obstacle among the target points as shown in Figure 2.a. In this task, the arm has to follow the trajectory without colliding with the obstacle. The goal is to assess the model's capacity for obstacle-aware trajectory generation, requiring the agent to modulate its path to avoid collisions while still reaching the target."}, {"title": "4.2.3 Novel Task of circular path tracking", "content": "In this third task, the goal is to evaluate DARC's ability to perform transfer learning. Specifically, we use the reservoir model trained in the second task (reaching with an obstacle) but now train the RL module to track a moving target along a circular path. This setup tests whether DARC can adapt to a new, dynamic task by leveraging the knowledge gained from the previous task with static targets. The DARC model's performance is evaluated based on its ability to generate accurate trajectories that follow the path while avoiding the obstacle."}, {"title": "4.3 Data Collection", "content": "In the first stage of training the DARC model, we utilized a feedback controller to collect the dataset D = {$D^{(e)}$ |\nt = 1,..., T, e = 1, ..., E}, where each episode e contains a sequence of data points $D^{(e)}_t$\n= $(c^{(e)}_t, T^{(e)}_t,  f^{(e)}_t)$ over T timesteps. Here, $c^{e}$ is the context data, specifically the target position $P_{target}(t)$ with Cartesian coordinates [$\\Pt, Pt\u2084$ ]; $Te) = [P_{end effector}(t), u\u2081 (t), u2(t)]$ represents the trajectory data, including the end effector position\n$P_{end effector}(t) = [p_{ex}, p_{ey}]$ and joint torques $u\u2081 (t)$ and $u2(t)$; and $f^{(e)} = [q1 (t), q2 (t), q1 (t), q2 (t)]$ provides the feedback data, comprising joint angles and velocities. This dataset D provides the structured inputs required for training the reservoir model. Training data points are shown in Figure 2 by green dots."}, {"title": "5 Implementation Details", "content": ""}, {"title": "5.1 Stage-1: Learning an initial set of movement with LfD", "content": "We adopt the core framework of the CESN model to perform initial training on a set of demonstrated task trajectories coupled with their task parameters, i.e., contexts. To ensure the echo state property, the reservoir weights $W_x$ are scaled by an appropriate factor[Luko\u0161evi\u010dius, 2012]. Additionally, the feedback weights Wf and context weights W are scaled to fine-tune the reservoir's performance[Luko\u0161evi\u010dius, 2012].\nIn this implementation, the input to the reservoir at each timestep t consists of the task-specific context $c_t =\n[X_{target}, Y_{target}]$. For the first task (reaching task), which is relatively simple, we only use the context $c_t$ without feedback. However, for the more complex tasks (reaching with obstacle avoidance and circular trajectory generation), we incorporate the feedback signal $f_t = [q_1(t), q_2(t), \u0121_1 (t), \u0121_2(t)]$, representing joint angles and velocities, to improve performance. For further increasing the robustness of the system, the feedback signal ft is augmented with noise(see AppendixA for details).\nThe context and feedback data collected in Section 4.3 are utilized to generate the reservoir states using Equations 1,2, and subsequently, the trajectory data from the dataset is employed to compute the output weights as described in Equation 4. Once the output weights matrix Wout is computed by Equation 4, trajectories can be generated at each time stept as $T_t = [px(t), py(t), u\u2081(t), u2(t)]$ through matrix multiplication with the learned output weight (Equation 4). Thus the system outputs the torques u1 (t), u2(t) to drive the robot arm as well as its prediction for the endeffector position, $(px(t), py(t))$. So it can be used for torque control or PD-control of the robot. In the current implementation only torque outputs are used to drive the robot."}, {"title": "5.2 Stage-2: Dynamic context generation with RL", "content": "In the second stage, the RL module is trained to solve the MDP task (S, A, R, T), in which the state st \u2208 S is defined as $s_t$ = [cos(q), sin(q), q, q, Pt, Pe] where q and q represent the angles and angular velocities of the joints of the robot respectively, and pt represents the position of the target and pe robot end effector position.\nThe action space A consists of continuous actions $a_t \u2208 [-1, 1] \u2208 \\mathbb{R}^N$, matching the dimensionality of the context input to the CESN reservoir (Vc). For this study, Proximal Policy Optimization (PPO)[Schulman et al., 2017] is chosen as the"}, {"title": "5.3 Reward Function", "content": "To accelerate policy learning, we used reward shaping in the experiments reported in this study. A running reward function Rrun(t) is designed to guide the learning towards the desired solution, while a terminal reward Rterm(t) is used to indicate the level of completion of the task. A composite running reward function is used:\nR_{run}(t) = \u03b1r_d(t) + \u03b2r_s(t) + ro  (5)\nwhere ra(t) is the Euclidean distance to the specified task goal in the current step, rs(t) is a bonus reward for staying close to the target defined by\nr_s(t) =\n\\begin{cases}\n+1, & \\text{if } ||P(t) \u2013 P_{desired}(t)|| < 5 mm \\\\\n0, & \\text{otherwise}\n\\end{cases}\n\nand ro(t) = -10 when the arm is in collision with the obstacle otherwise it is zero.\nThe terminal reward rf differs between the first two tasks and the third task. In the first and second tasks, if the task is successfully completed in the final step (i.e., the end effector reaches within 1 cm of the target), the terminal reward is defined as $r_f = c\u00b7Y^{-L}$ where c is a positive constant and L is the path length computed by $L = \\sum_{t=1}^{T} ||P(t)-P(t-1)||$ where P is the end effector positions, and T is the end time step. This term encourages the model to minimize the path-length while reaching the target.\nPath-length minimization is not necessary for circular trajectory generation, as the goal is to follow a circular trajectory which the running reward should take care of. Instead, the terminal reward is defined as $r_f = c\u00b7 \u03b7\u00b7 p_e$ where \u03b8 is a coefficient to weight the trajectory error, and pe is the average Euclidean distance error between the followed trajectory and the desired circular trajectory, computed as $p_e = \\frac{1}{T} \\sum_{t=1}^{T} || P(t) - P_{desired}(t) ||$, where pdesired indicates desired end effector position. The coefficients \u03b1, \u03b2, \u03b3, \u03b8 and c are tuned based on task requirements to balance each component's influence on the agent's behavior. In the experiments presented in this study they are chosen empirically as shown in table 1."}, {"title": "6 Results", "content": "In the experiments conducted in this study, the CESN model was trained on 50 demonstrated target points for the reaching task. For the reaching task with an obstacle, target points within the obstacle region were excluded. However, additional points were included on the left side of the obstacle, as reaching this area is more challenging. Consequently, a total of 80 points were used for the second and third tasks. The model's generalization was then evaluated on 64 extrapolated target points that were not included in the training set(see Figure 2a). For DARC, the same CESN model was further extended with an RL module, trained to adapt to these 64 new targets. Lastly, we evaluate a standalone RL model trained and tested directly on the 64 extrapolated points. For reaching tasks, the model performances are assessed with Final Distance to Target and Path Length measures. A successful reaching is expected to have a low endpoint error avoiding convoluted trajectories. See Table 2 for setup details of each experiment."}, {"title": "6.1 Reaching task", "content": "This section first establishes how CESN performs on the novel test targets that are outside its training range. Then, it evaluates how successful our DARC model is in leveraging CESN in reaching those points with additional RL. In this experiment the RL module of DARC is allowed to change context input only twice (i.e., k=25, where full trajectory length is 50). To assess the baseline difficulty of the task, we also train a PPO model to learn a policy to drive the robot endeffector to these test targets. Each model is trained twice with different random seeds. As each run includes 64 points, the models are evaluated on a total of 128 reaching episodes."}, {"title": "6.2 Reaching with obstacle avoidance task", "content": "In this section, we assess the performance of CESN, DARC, and PPO models on the second task, which requires reaching target points in 100 steps while avoiding obstacles. In this experiment, each model is tested over 4 training runs on 64 points, resulting in a total of 256 test episodes.\nSince CESN is not provided with additional LfD data for the new target points, its performance on novel targets relies solely on its intrinsic extrapolation capability, which is limited to small extrapolations (Amirshirzad et al. [2023]). As a result, in this experiment, in contrast to reaching without an obstacle, CESN was unable to reach any of the extrapolated points (0/256). Curiously, its endpoint error was comparable to that of the first experiment, possibly, due to the fact in this experiment the robot state feedback was used as an input to the reservoir network. The DARC model, by learning to modify context input to CESN, is able to reach successfully to all points (256/256). Since PPO is directly trained on the novel targets, better performance than that of CESN might be expected. Indeed, PPO achieves 171 successful episodes out of 256. However, under the same training conditions as the RL module in DARC, it exhibits a higher path-length error. DARC model seems to combine the strengths of LfD and RL and give the best performances (Figure 5)."}, {"title": "6.2.1 Novel Task of circular path tracking", "content": "This experiment aims to evaluate the transfer learning capability of our proposed model DARC. First, the reservoir is trained on demonstration data to reach static targets, same as the reaching task with obstacle avoidance. Then, the RL module of DARC is trained to track a moving target along a specified circular path, while continuing to avoid obstacles accurately. Then the robot is asked to follow a circular path centered at [0,0.11] meters, with a radius of 7 cm, over a duration of 300 steps. In the case of CESN, the moving target is provided as context at each step, allowing for some tracking due on its extrapolation capability. In constrast both the PPO and DARC models, are allowed to learn and generate actions at every 5 steps."}, {"title": "7 Conclusion", "content": "In this paper, we introduced a new adaptive movement primitive approach, called DARC, which integrates Learning from Demonstration (LfD) with Reinforcement Learning (RL) to enhance the generalization capability, computational efficiency, and scalability of LfD. DARC has been shown to outperform standalone LfD and pure RL in solving unseen tasks, demonstrating superior adaptation and computational efficiency. By learning to generate dynamic context for new situations, DARC enables successful generalization. Additionally, we leveraged reservoir dynamics modulation within DARC to perform transfer learning, allowing it to solve novel tasks with little additional training. This work provides a foundation for further advancements. For instance, although the reservoir predicts the fingertip position, this prediction is not currently utilized; integrating an additional controller to backup the torque output of DARC should improve performance. Additionally, refining the reward structure by incorporating terms related to the reservoir dynamics could further optimize performance. Finally, while this study was conducted in a simulation environment, future work should focus on validating the approach through experiments on real robotic systems."}, {"title": "A Feedback Augmentation details", "content": "The feedback signal, ft, is augmented with noise such that ft = ft + e, where e is a noise vector. The noise e is drawn\nfrom a uniform distribution, with its lower and upper bounds adjusted based on the variability in the demonstration\ndataset, as shown in Table 2. The feedback was augmented with this noise for 10 times."}]}