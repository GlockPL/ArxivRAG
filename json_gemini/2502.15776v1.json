{"title": "Logic.py: Bridging the Gap between LLMs and Constraint Solvers", "authors": ["Pascal Kesseli", "Peter O'Hearn", "Ricardo Silveira Cabral"], "abstract": "We present a novel approach to formalise and solve search-based problems using large language models, which significantly improves upon previous state-of-the- art results. We demonstrate the efficacy of this approach on the logic puzzles benchmark ZebraLogicBench. Instead of letting the LLM attempt to directly solve the puz- zles, our method prompts the model to formalise the prob- lem in a logic-focused domain-specific language (DSL) called Logic.py. This formalised representation is then solved using a constraint solver, leveraging the strengths of both the language model and the solver. Our approach achieves a remarkable 65% absolute improvement over the baseline performance of Llama 3.1 70B on ZebraLog- icBench, setting a new state-of-the-art with an accuracy of over 90%. This significant advancement demonstrates the potential of combining language models with domain- specific languages and auxiliary tools on traditionally challenging tasks for LLMs.", "sections": [{"title": "Introduction", "content": "Large language models have revolutionised the field of natural language processing, achieving state-of-the-art re- sults in various tasks such as language translation, text summarisation, and question answering. However, de- spite their impressive performance, LLMs have histori- cally struggled with certain tasks that require a deeper un- derstanding of mathematical and logical concepts. For instance, Kambhampati et al. [2024] demonstrated that LLMs are unable to plan and reason about complex prob- lems, highlighting the need for further research in this area. In this paper, we focus on improving the perfor- mance of LLMs in solving Logic Grid Puzzles, also called Zebra Logic Puzzles or Einstein's Riddles, which we ex- plain in more detail in Sec. 1.2. We present the following research contributions:\n1.  Logic.py: We introduce a domain-specific language called Logic.py which facilitates expressing logic and search-based problems by LLMs.\n2.  Logic Agent: We implement an agentic solver en- gine which accepts search-based, informal problem statements, formalises them in Logic.py and solves them using a constraint sovler.\n3.  ZebraLogicBench Evaluation: We evaluate the ef- ficacy of this approach on the logic puzzle bench- mark ZebraLogicBench Lin et al. [2024]."}, {"title": "1.1 Related Work", "content": "Prior research has explored techniques to enhance the ability of LLMs in these central reasoning tasks, such as chain-of-thought prompting and introducing symbolic representations. However, according to Berman et al. [2024], these frameworks often struggle with complex logical problems like Zebra puzzles, partly due to the in- herent difficulty of translating natural language clues into logical statements. They propose integrating LLMs with theorem provers to tackle such challenges, demonstrating significant improvements in puzzle-solving capabilities."}, {"title": "1.2 Logic Grid Puzzles", "content": "We evaluate the effectiveness of our approach on the Ze- braLogicBench benchmark presented in Lin et al. [2024]. ZebraLogicBench is a dataset of 1000 Logic Grid Puz- zles, also referred to as Zebra Puzzles. These puzzles consist of a series of clues about features of entities in a described environment. In order to solve the puzzle, one has to guess the correct features of all entities, while re- specting all the information provided in the cluses."}, {"title": "1.3 CBMC - Bounded Model Checker for C and C++ programs", "content": "CBMC Clarke et al. [2004] is a static analysis tool de- signed for C and C++ programs. It operates by map- ping programs to formulas in a back-end solver, typi- cally SAT or SMT, which are satisfiable if and only if the mapped program exhibits a specific property. This capa- bility enables CBMC to effectively check programs for bugs or other properties of interest. Notably, CBMC is powered by the underlying CPROVER static analysis en- gine, which also supports other language front-ends, such as the JBMC Cordeiro et al. [2018] Java front-end.\nSince CBMC implements a mapping between pro- grams and SAT or SMT formulas, it can serve as a con- venient front-end for such constraint sovlers, exposing an API that allows expressing SAT or SMT formulas as C programs with free input variables. Expressing formulas in this fashion makes many constraint solver tasks more accessible for human developers, and it is a core hypoth- esis of this paper that it equally simplifies the use of con- straint solvers for LLMs. We describe this DSL that we expose to the LLM in more detail in Sec. 2."}, {"title": "2 The Logic.py language", "content": "Our goal in designing Logic.py was to provide a stream- lined API language that allows an LLM to efficiently express search-based problems and leverage a constraint solver. We optimised for the folowing criteria:\n1.  Robustness: The language should minimise the sur- face area for syntax errors.\n2.  Conciseness: The model should be able to express the necessary constraints to solve a search-based problem without boilerplate or needing to worry about unrelated implementation details of the pro- gramming language.\n3.  Expressiveness: While common constraints, such as uniqueness of a property, should be easy to express in our DSL, the language must not be restricted to just these common cases. Instead, it must allow the LLM to express arbitrary constraints in the underly- ing constraint solver framework if necessary.\nTo provide a good basis for our DSL in terms of ro- bustness and conciseness, we decided to not start with CBMC's native C as a base language for Logic.py but in- stead, as its name suggests, we selected Python for this purpose. We use libCST\u00b9 to transform Logic.py to C for analysis by CBMC, as explained in more detail in Sec. 4. Python allows the model to introduce new variables with- out the need for explicit, typed declarations. Similarly, these variables can be reused for other values later on without needing to worry about type compatiblity."}, {"title": "2.1 Type Decorators", "content": "In order to further improve the conciseness of Logic.py, we borrow well-known language features from existing languages and combine them in our DSL. As an exam- ple, expressing uniqueness of a property in SMT directy requries code similar to the example in Fig. 3.\n(forall ((x T) (y T))\n(=> (distinct x y)\n(not\n(= (idx) (id y))))"}, {"title": "2.2 Free Variables, Assumptions, and Assertions", "content": "A key concept in using constraint solvers are free vari- ables. By default, any variable which is not explicitly initialised in Logic.py is assumed to be a free variable. Consequently, the constraint solver is allowed to assign it any value in order to find a satisfying assignment for all the specified constraints. We also say that such a variable has a nondeterministc value.\nThe CPROVER framework also allows users to spec- ify assumptions and assertions. These two features work in tandem: Assumptions act as preconditions, usually expressed over free variables, to constrain the solver's search to valid or interesting inputs. Assertions on the other hand represent safety properties, for which the solver tries to find a falsifying assignment in order to prove the presence of a bug. Both of these features are exposed in Logic.py via the assume(...) function and the assert... statement, respectively.\nIt should be noted that in Logic.py, from the perspective of the LLM, these two become interchangeable. We inter- pret assertions specified by the model not as safety prop- erties to be checked for violations, but instead as require- ments on a valid solution. Interpreting assertions as as- sumptions and passing only a single reachability assertion to the constraint solver is a common pattern in program synthesis use cases David et al. [2018]. We will explain the structure of constraints we produce at CPROVER in- termediate representation level in more detail in Sec. 4. We summarise the nondeterminism-related Logic.py fea-"}, {"title": "3 Search Problem Formalisation", "content": "After reviewing the features of Logic.py in Sec. 2, we now review how we prompt the model to express search prob- lems in this language. We provide the full prompts in our open source agent project polymath\u00b2. Note that all our prompts are zero-shot in that we do not provide any full Zebra puzzle as example to the model. Instead, we only explain Logic.py and how to use its nondeterministic fea- tures to express generic search-based problems."}, {"title": "3.1 Describing a Result Data Structure", "content": "Before attempting to convert the required properties and constraints for the solution of a search-based problem, we first prompt the model to define the data structure that can contain such a solution. We ask the model to define this data structure in Logic.py and be as precise as possible with respect to type annotations. This constrains the space of valid solutions purely based on the domain of the prob- lem, irrespective of explicit constraints for now."}, {"title": "3.2 Constraining a Correct Solution", "content": "Once the data structure is defined, we prompt the model to generate a validation function that accepts an argument of this type and asserts that it is the correct solution we are searching for. In the case of Zebra puzzles, this leads to the model adding assertions corresponding to the clues in the puzzle. This approach transforms the challenge for the LLM fundamentally: Instead of searching the solution space for configurations that satisfy the clues stated in the puzzle, it just needs to be able to reason about the clues themselves."}, {"title": "4 Logic Agent Architecture", "content": "Fig. 7 illustrates the full implementation architecture of our solver engine, starting with the formalisation steps outlined in Sec. 3.\nOur engine converts these Logic.py constraints into an equivalent, lower-level C representation using a libCST transformer. During this process, the type decorators in- troduced in Sec. 2.1 are mapped to matching initialisation helpers in CBMC's IR. An example of this mapping is illustrated in Fig. 8.\nThe validation function containing the constraints de- rived from the Zebra Logic clues is equally converted to a C representation, and embedded into a search harness. In this harness, a nondeterministic instance of the result data structure proposed by the model is initialised using the type decorator information, then constrained using the converted validation function. All assertions in the vali- dation function are converted into assumptions, and we add a single reachabilty assertion to prompt the constraint solver to find an assignment for the nondeterministc puz- zle solution such that it satisifes all constraints. An exam- ple harness is shown in Fig. 9."}, {"title": "4.1 Error Recovery", "content": "Due to the heuristic nature of the formalisation and con- straint solver steps, our solver engine can fail at various steps along the pipeline. The model might produce invalid Logic.py code to begin with, leading to syntax errors in libCST. These errors are caught during the C harness gen- eration, in which case we revert to the original data struc- ture generation step and start the process from scratch. We currently do not provide any information about the syntax error to the model or ask it to fix its prior mistakes, and such approaches could be explored in future work.\nFurthermore, if the model misinterprets constraints in the informal description of the search problem, such er- rors can lead to contradictory or otherwise unsatisfiable constraints. This is detected during the constraint solver invocation by CBMC, and if detected we again just revert to the initial step of the process. In future work we might again explore whether the model can recover from this, if provided this information, or whether it is preferable to just restart the process, as is currently the case.\nSimilar to unsatisfiable constraints, the model might also commit formalisation mistakes that loosen the re- quirements and thus allow for more solutions than should actually be the case. The constraint solver can also de- tect whether two distinct solutions are possible under the given constraints, and report this information back to the model. The model could then decide whether the prob- lem at hand might indeed allow for multiple different so- lutions, or whether a correct solution should be unique and thus attempt to correct its mistake. However, we did not implement this ambiguity detection mechanism in our prototype. Even though every task in ZebraLogicBench, which we use for our evaluation, allows for exactly one correct solution, this information is not shared with the model and is only intended to simplify benchmark result evaluation."}, {"title": "5 Experimental Evaluation", "content": "To evaluate the effectiveness of our approach, we con- ducted experiments on ZebraLogicBench, a benchmark suite consisting of 1000 logic grid tasks. In order to run the evaluation independently, researchers must request ac- cess to a private dataset hosted huggingface.co. We im- plemented a benchmark runner that accepts this dataset as input which we share in our open source project poly- math. The output result of the benchmark runner is evalu- ated using the ZeroEval\u00b3 evaluation suite provided by the"}, {"title": "6 Threats to Validty", "content": "While we believe we made a convincing case in Sec. 5 that auxiliary tools can significantly boost the performance of state-of-the-art LLMs on challenging tasks, we would like to address potential threats to the validity of our results.\nFirstly, our engine has thus far only been evaluated against a particular category of search-based problems in the form of logic grid puzzles. Our technique may prove less impactful when applied to other search-based problems formalised by an LLM. This may be due to the fact that our Logic.py prototype is currently incomplete and does not yet support some common Python language and standard library features, or more fundamentally, that"}, {"title": "7 Conclusions and Future Work", "content": "In this paper, we presented a novel approach to solv- ing search-based problems using large language models. By introducing the domain-specific language Logic.py and implementing an agentic solver engine, we demon- strated significant improvements in performance on the logic puzzle benchmark ZebraLogicBench. Our results show that combining the strengths of LLMs with those of constraint solvers can lead to remarkable advancements in solving traditionally challenging tasks.\nThe success of our approach highlights the potential for further research in this area. Some promising directions for future work include:\n1.  Extending Logic.py: Developing a more compre- hensive and expressive version of Logic.py could en- able the formalisation of a wider range of search- based problems.\n2.  Improving the Logic Agent: Enhancing the agen- tic solver engine to better handle complex problem statements and iterate on its mistakes (e.g. syntax errors) could lead to further performance gains.\n3.  Applying the approach to other domains: Explor- ing the application of our method to other, such as optimisation or mathematical rasoning, could reveal new opportunities for improvement.\n4.  Investigating the role of LLMs in problem for- malisation: Further research into the capabilities and limitations of LLMs in formalising search-based problems could provide valuable insights into the de- sign of more effective problem-solving systems.\n5.  Teaching neural nets to search like solvers: Our translations leverage powerful aspects of constraint solvers: proof checking and search. It would be possible to decompose these strengths. Solvers like Z3 have evolved subtle heuristics for approaching computationally intractable problems that are NP- hard and more, and if we can train nets in a way that learns these heuristics they might provide ben- efit more broadly than as the targets of translation- based work. In this context, work such as this could provide baselines or targets for what we would want out of the training.\nIn particular, we are currently working on a DSL suit- able for tackling first order logic problems and intend to evalute this engine against the FOLIO benchmark. Han et al. [2022]\nBuilding on this project, our broader research aims to further explore the potential of combining large language models with specialized reasoning tools. By pursuing these avenues of research, we believe that it is possible to develop even more powerful and efficient problem- solving systems."}]}