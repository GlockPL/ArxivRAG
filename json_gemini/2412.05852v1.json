{"title": "EVOLVING ALGEBRAIC MULTIGRID METHODS USING GRAMMAR-GUIDED GENETIC PROGRAMMING", "authors": ["DINESH PARTHASARATHY"], "abstract": "Multigrid methods despite being known to be asymptotically optimal algorithms, depend on the careful selection of their individual components for efficiency. Also, they are mostly restricted to standard cycle types like V-, F-, and W-cycles. We use grammar rules to generate arbitrary-shaped cycles, wherein the smoothers and their relaxation weights are chosen independently at each step within the cycle. We call this a flexible multigrid cycle. These flexible cycles are used in Algebraic Multigrid (AMG) methods with the help of grammar rules and optimized using genetic programming. The flexible AMG methods are implemented in the software library of hypre, and the programs are optimized separately for two cases: a standalone AMG solver for a 3D anisotropic problem and an AMG preconditioner with conjugate gradient for a multiphysics code. We observe that the optimized flexible cycles provide higher efficiency and better performance than the standard cycle types.", "sections": [{"title": "1. Introduction", "content": "Multigrid methods are a class of highly efficient algorithms for solving large systems of discretized partial differential equations. The selection of algorithmic components in a multigrid method plays an important role in determining its efficiency. The task of designing an efficient multigrid method is a non-trivial one. Leveraging the recent advances in artificial intelligence (AI), efforts have been made to find optimal multigrid components, such as smoothers [6], intergrid operators [5, 9, 8, 7], and coarsening schemes [14]. We take a complementary approach and construct efficient multigrid cycles from a set of available multigrid components. Traditional multigrid methods employ recursive cycle types such as V-, W-, F, or the more recent \u043a-cycles [1]. However, we use a so-called flexible multigrid cycle, which can arbitrarily cycle up or down in a non-recursive manner. Also, the choice of smoothers and their relaxation weights can be made individually for each step in the cycle (Fig 1.1b). These flexible cycles are challenging to hand-tune due to their extensive search space, but they can be formulated as a program optimization task. This way, we aim to exploit the inherent flexibility in the cycling structure, with the intent of discovering methods more efficient than the standard cycling types. Schmitt et al. use a similar approach to construct efficient grammar-guided geometric multigrid (GMG) methods [13, 12]. This involves the automatic generation of GMG programs evolved using genetic programming. We adapt this methodology to construct efficient algebraic multigrid (AMG) methods. This approach is used to optimize an AMG method as a solver for a 3D anisotropic problem and then as a preconditioner with preconditioned conjugate gradient (PCG) for a multiphysics simulation. The constructed methods are evaluated and compared to optimized reference methods with standard AMG cycling."}, {"title": "2. Background", "content": "Genetic programming (GP) is a research area in AI that deals with the evolution of computer code. It is one of the branches of Evolutionary Algorithms (EAs), following Darwin's principle of natural selection. The general approach of GP is to apply a population of programs to a given problem, and compare their performance (fitness) relative to each other. Operators inspired by genetics (cross-over, mutation) are applied to selected programs from the population (parents) such that in time better programs (offsprings) emerge by evolution. This principle is applied iteratively for multiple generations until a desirable population of programs is obtained [2, 11]. In problems where the programs are required to conform to a specific structure (for eg. multigrid methods), grammar rules can be used to constrain the evolutionary process such that only valid programs are produced. Grammar-guided genetic programming (G3P) is an extension of traditional GP systems that use context-free grammars (CFGs) to impose such constraints on the initial population and subsequent genetic operations. Exploiting domain knowledge using CFGs helps in eliminating invalid programs and speeds up the optimization process [15, 10]."}, {"title": "3. Method", "content": "Consider a system of linear equations $Au = f$, with a coarser level system $A_cu_c = f_c$, for the residual equations. Every step during the solve phase of an AMG method can be written in the form $v^{i+1} \\leftarrow v^i + \\omega B(f - Av^i)$, where $v^i$ is an approximate solution at iteration $i$, $B$ is an operator and $\\omega \\in \\mathbb{R}$ is a relaxation factor for a smoothing step or a scaling factor for a coarse-grid correction (CGC) step. For instance, a Jacobi smoothing step can be defined as $v^{i+1} \\leftarrow v^i + \\omega D^{-1}(f - Av^i)$, where $D = \\text{diag}(A)$. Similarly, the CGC step can be expressed as $v^{i+2} \\leftarrow v^{i+1} + \\omega P A_c^{-1} P^T (f - Av^{i+1})$. Here, $P$ is the interpolation operator that maps the coarse system to the original system of equations and $P^T$ is the transpose that maps from the original system to the coarser level. Substituting $v^{i+1}$ from the previous expression here, yields a single expression for a two-grid AMG method with Jacobi pre-smoothing. This approach is extensible to multiple levels, and substituting subexpressions recursively creates a single final expression that maps uniquely to an arbitrary flexible multigrid cycle (Fig. 1.1b). Schmitt et al. designed a CFG for multigrid methods defining a list of rules to generate such expressions automatically [13]. We use this CFG to generate different multigrid cycles for a given AMG setup. This lets us automatically generate an initial population of AMG programs and further discover optimal AMG cycles using G3P."}, {"title": "4. Implementation", "content": "The BoomerAMG\u00b9 implementation from the software library hypre is used to generate the AMG programs. Additional interfaces are added and implemented so that AMG methods with flexible cycling can be defined and used within the hypre framework\u00b2. The AMG expression generated from the CFG is transformed to corresponding input arguments for BoomerAMG. Executing each program returns the solve time and convergence factor as the fitness measure for the optimization. The optimization is performed using the EvoStencils framework\u00b3. This uses the CFG introduced earlier to generate high-level representations of multigrid methods, and the DEAP library for applying evolutionary algorithms. We adapt this framework to include AMG components from hypre during expression generation. To eliminate redundant AMG setup times during fitness evaluation, the program individuals are grouped into batches of size b, with one setup phase per batch, and then the solve phase for the individual solvers is executed in succession (Fig. 4.1). The batches are distributed across multiple processes using MPI bindings5."}, {"title": "5. Experimental Setup", "content": "We evaluate our approach to discover efficient AMG methods when used as solvers and preconditioners. During the optimization, the AMG setup phase is fixed. From a given choice of smoothers, a set of relaxation weights for smoothing, a set of scaling factors for CGC, and flexibility in the cycling structure, we search for optimal AMG methods (Table 5.1a). We enable cycle flexibility for the top five levels and use a V(1,1) 6 cycle on the coarser levels. This allows the optimized methods to be used for different problem sizes. The methods are optimized for two objectives-solving time per iteration and convergence factor, and a 2D Pareto front is constructed. Eventhough we finally choose methods with the fastest total time to solution, using two objectives maintains diversity in the population and avoids premature convergence. Using separate objectives ensures that both cheap methods (per iteration), and methods with a high convergence rate (but probably expensive) are found in the optimization."}, {"title": "5.1. AMG as a solver", "content": "AMG method as a standalone solver is optimized for the following 3D anisotropic problem:\n\n$\\begin{aligned}\n-a u_{x x}-b u_{y y}-c u_{z z} &=f \\text { in } \\Omega, \\\\\nu &=0 \\text { on } \\partial \\Omega\n\\end{aligned}$\n\nThe system of equations is built using a standard 7-point laplacian stencil in a 100 \u00d7 100 \u00d7 100 grid with anisotropy a = 0.001,b = 1,c = 1 and f = 0. The system is considered solved when the initial defect is reduced by a factor of 10-8."}, {"title": "5.2. AMG as a preconditioner", "content": "Here, we find optimal AMG methods as a preconditioner for PCG. The PCG method with AMG preconditioner is used to solve an application in the Ares framework, a parallel multiphysics code developed and maintained at Lawrence Livermore National Laboratory. The AMG method is optimized using the system matrix at a specific time step, and later evaluated on other time instances (with slightly different matrices than the one used for optimization). The performance is compared to a PCG method with optimized standard AMG cycling."}, {"title": "6. Evaluation", "content": "The experiments for each of the two cases mentioned are conducted multiple times to ensure the robustness and consistent convergence of the optimization process towards an optimal set of solutions. Additionally, the newly discovered optimal AMG methods are evaluated and compared to standard reference AMG methods."}, {"title": "6.1. AMG as a solver", "content": "The optimization process is executed over 100 generations, and the evolution of objective values is monitored. It is observed that the minimum values of both objectives within the population decrease over time"}, {"title": "6.2. AMG as a preconditioner", "content": "Here, we optimize AMG preconditioners for a conjugate gradient (CG) method. An AMG preconditioned CG (AMG-PCG) method is used for solving a time-stepping multiphysics simulation from the Ares framework. The equation governing the simulation is given by $\\frac{\\partial \\phi}{\\partial t}+\\nabla \\cdot F=S$, where $F$ adheres to Fick's Law, expressed as $F=-D \\nabla \\phi$, and $S$ represents the source term(s). The spatial domain is a 3D block featuring reflecting boundary conditions on the four faces (xz, xy) and open boundary conditions on the remaining yz faces. The system is stimulated by a 1 keV temperature source applied at the bottom yz face at t = 0. The problem has 64000 unknowns and the AMG method operates on a grid hierarchy of 7 levels. One cycle of AMG is used per CG iteration and the system is considered solved when the relative residual norm is less than 10-8.\nWe consider the system derived from time step t = 1 for the optimization. The objectives in the optimization are now measured for the PCG solver using different AMG"}, {"title": "7. Conclusion", "content": "We used a novel grammar-guided approach for the automated generation of AMG methods, utilizing a set of grammar rules. These generated methods are not confined to standard cycle structures but instead possess arbitrary cycling capabilities. This flexibility was harnessed to search for more efficient AMG methods using G3P. Our results indicate that AMG methods with flexible cycles specifically tailored for a given problem are superior in performance when compared to AMG methods with standard cycles. They are also seen to generalize well for different problem variants and sizes when used as a standalone solver. Furthermore, when optimized as a preconditioner in a time-stepping simulation code, they maintain their efficiency across different time instances. However, we have not delved into an in-depth analysis of the optimized solvers to gain insights into why the evolved cycle structures prove to be efficient. Also, it is not yet fully understood which components (smoothers, relaxation weights, cycle structures), exert the most influence on optimality. Another potential future direction could be formulating additional grammar rules that include the AMG setup phase within the optimization process. Furthermore, the collection of Pareto optimal solutions generated could serve as a valuable repository with a rich dataset of diverse AMG methods. One possible use of this dataset could be to train a machine learning model, which, in turn, could be used to select AMG methods tailored to specific problem instances."}]}