{"title": "FoldToken2: Learning compact, invariant and generative protein structure language", "authors": ["Zhangyang Gao", "Cheng Tan", "Stan Z. Li"], "abstract": "The equivalent nature of 3D coordinates has posed long term challenges in protein structure representation learning, alignment, and generation. Can we create a compact and invariant language that equivalently represents protein structures? Towards this goal, we propose FoldToken2 to transfer equivariant structures into discrete tokens, while maintaining the recoverability of the original structures. From FoldToken1 to FoldToken2, we improve three key components: (1) invariant structure encoder, (2) vector-quantized compressor, and (3) equivalent structure decoder. We evaluate FoldToken2 on the protein structure reconstruction task and show that it outperforms previous FoldToken1 by 20% in TMScore and 81% in RMSD. FoldToken2 probably be the first method that works well on both single-chain and multi-chain protein structures quantization. We believe that FoldToken2 will inspire further improvement in protein structure representation learning, structure alignment, and structure generation tasks.", "sections": [{"title": "Introduction", "content": "\"SE-(3) structure should not be special and difficult. Let's lower the barrier.\"\nOur Goal\nProtein structure modelling plays a foundational role in computational biology and have attracted increasing attention in machine learning. Due to the SE-(3) equivariant nature, encoding [25, 8, 11, 10, 14] and generating [18, 3, 19, 22, 1] protein structure never to be trivial, which requires the special design targeted to protein structures. For example, PiFold [11] proposes the invariant featurizer to encode structure patterns, and AlphaFold2 [18] design frame-based model to generate equivariant 3D coordinates. While nuemerious innovations have been proposed in designing the protein structure models, the structures data itself remains the SE-(3) nature. Can we transform equivariant structures into a invariant form, and then use the existing NLP/CV models to encode and generate the structures?\nWe introce FoldToken2, a novel method to transform SE-(3) structures into invariant representations. The key insight is to create a compact invariant latent space that preserve the structure information via self-reconstruction. After pretraining, the invariant latent representation can serve as prototype of the equivariant structures that is editable in the latent space. Akin to image and text, we also introduce a vector quantization module to discretize the latent space to create a SE-(3) invariant language. Taking the invariant latent embedding or language as input, we can use the existing CV or NLP models [4, 7, 9, 21] to encode and generate the protein structures. FoldToken2 contains three key components: (1) invariant encoder, (2) vector quantization module, and (3) equivariant decoder."}, {"title": "Method", "content": ""}, {"title": "Overall Framework", "content": "As shown in Fig.1, the overall frame work keeps the same as FoldToken1 [12, 13], including encoder, quantifier and decoder. FoldToken2 comprehensively improves the each module to enhance the reconstruction performance, which are summarized as follows:\n1. Data Form: Changing from the angle-based representation to coordinate-based one.\n2. Backbone: We replace the transformer backbone with new GNN, named BlockGAT.\n3. VQ: We introduce teacher-guided temperature annealing strategy.\n4. Generator: We propose novel SE-(3) layer to refine the protein structures iteratively."}, {"title": "Invariant Graph Encoder", "content": "Due to the rotation and translation equivariant nature, the same protein may have different coordinates records, posing a challenge of learning compact invariant representations for the same protein. Previous work [11, 17, 5, 15] has shown that the invariant featurizer can encode the invariant structure patterns, and we follow the same road: representing the protein structures as a graph consisting of invariant node and edge features. Then we use the BlockGAT [15] to learn high-level representations.\nFrame-based Block Graph. Given a protein $M = \\{B_i\\}_{i=1}^n$ containing n blocks, where each block represent a amino acid, we build the block graph $G(\\{B_i\\}_{i=1}^n, E)$ using kNN algorithm. In the block graph, the s-th node is represented as $B_s = (T_s, f_s)$, and the edge between (s, t) is represented as $B_{st} = (T_{st}, f_{st})$. $T_s$ and $T_{st} = T_s^{-1} \\circ T_t$ are the local frames of the s-th and the relative transform between the s-th and t-th blocks, respectively. $f_s$ and $f_{st}$ are the node and edge features."}, {"title": "Quantifier", "content": "Following FoldToken1 [12, 13], we use the SoftCVQ to quantize the invariant embeddings into discrete tokens, termed fold language. Instead of projecting continual embeddings into discrete tokens, SoftCVQ maps pre-defined binary number ($b_j$) into continuous token embeddings $v_j$ and then conduct soft alignment between the token embeddings $v_j$ and latent embeddings $h_s$.\nGiven the decimal integer z and the codebook size m, we represent z as a binary vector $b_j$ with length $\\log_2(m)$. For example, if m = 4, we have $b_1 = [0,0]$, $b_2 = [0, 1]$, $b_3 = [1,0]$, $b_4 = [1, 1]$. The quantization operation transforms continuous embeddings $h_s$ into discrete tokens z:\n$a_{sj} = \\frac{\\exp{(f_s \\cdot v_j / T)}}{\\sum_{j=1}^m \\exp{(f_s \\cdot v_j / T)}}$\n$z = \\arg \\max_j a_{sj}$\nwhere $v_j$ is the j-th token embedding, generated by a conditional network, which also server as the de-quantization operation:\n$b_s = Bit(z, \\log_2(m))$\n$f_s = ConditionNet(b_s)$\nIn Eq. 2, the \"argmax\" is non-differentiable, and we use a soft approximation during training:\n$f_i = \\sum_{j=1}^m \\alpha_{ij} v_j$\n$b_j = Bit(j, \\log_2(m))$\n$v_j = ConditionNet(b_j)$\nThe temperature parameter T controls the softness of the attention query operation. When T is large, the attention weights will be close to uniform; otherwise, the attention weights will be close to one-hot."}, {"title": "Equivariant Graph Decoder", "content": "Generating the protein structures conditioned on invariant representations poses significant challenges in computing efficiency. For example, training well-known AlphaFold2 from scratch takes 128 TPUv3 cores for 11 days [24]; OpenFold takes 50000 GPU hours for training [2]. In this work, we propose an efficiency plug-and-play SE3Layer that could be added to any GNN layer for structure prediction. Thanks to the simplified module of SE3Layer and BlockGAT with sparse attention, we can train the model over the whole PDB dataset in 1 day using 8 NVIDIA-A100s.\nSE-(3) Frame Passing Layer. We introduce frame-level message passing, which updates the local frame of the s-th block by aggregating the relative rotation R, and translation $t_s$ from its neighbors:\n$\\text{vec}(R_s) = \\sum_{j \\in N_s} a_j \\text{vec}(R_{sj})$\n$R_s \\leftarrow \\text{Quat2Rot} \\circ \\text{Norm} \\circ \\text{MLP}^{9 \\to 4}(\\text{vec}(R_s))$\n$t_s = \\sum_{j \\in N_s} a_j t_{sj}$\nwhere $a_j$ and $a'_j$ are the rotation and translation weights, and $N_s$ is the neighbors of the s-th block. $\\text{vec}(\\cdot)$ flattens 3 \u00d7 3 matrix to 9-dimensional vector. $\\text{MLP}^{9 \\to 4}(\\cdot)$ maps the 9-dim rotation matrix to 4-dim quaternion, and $\\text{Norm}(\\cdot)$ normalize the quaternion to ensure it represents a valid rotation. $\\text{Quat2Rot}(\\cdot)$ is the quaternion to rotation function. We further introduce the details as follows:\n$w_{st}, w_t = \\sigma(\\text{MLP}(f_{st}))$\n$\\text{vec}(R_{st}) \\leftarrow w_t \\text{vec}(R_{st}) + (1 - w_t) \\text{MLP}^{d \\to 9}(f_{st})$\n$t_{st} \\leftarrow w_t t_{st} + (1 - w_t) \\text{MLP}^{d \\to 3}(f_{st})$\n$a_{st}, a_t = \\text{Softmax}(\\text{MLP}^{d \\to 1}(f_{st}))$\nwhere $w_{st}$ and $w_t$ are the updating weights for rotation and translation, $a'_{st}$ and $a_t$ are the attention weights. The propose SE-(3) layer could be add to any graph neural network for local frame updating."}, {"title": "Reconstruction Loss", "content": "Inspired by Chroma [16], we use multiple losses to train the model. The overall loss is:\n$\\mathcal{L} = \\mathcal{L}_{global} + \\mathcal{L}_{fragment} + \\mathcal{L}_{pair} + \\mathcal{L}_{neighbor} + \\mathcal{L}_{distance}$\nTo illustrate the loss terms, we define the aligned RMSD loss as $\\mathcal{L}_{align} (\\mathcal{X}^{(l)}, \\mathcal{X}) = ||Align(\\mathcal{X}, \\mathcal{X}) - \\mathcal{X}||$, given the the ground truth 3D coordinates $\\mathcal{X} \\in \\mathbb{R}^{n,3}$ and the predicted 3D coordinates $\\mathcal{X} = \\{x_1,x_2,x_3,\\dots,x_n\\} \\in \\mathbb{R}^{n,3}$. The global, fragment and pair loss are defined by the aligned MSE loss, but with different input data shape:\n* Global Loss: $\\mathcal{X}$ with shape [n, 4, 3]. RMSD of the global structure.\n* Fragment Loss: $\\mathcal{X}$ with shape [n, c, 4, 3]. RMSD of c neighbors for each residue.\n* Pair Loss: $\\mathcal{X}$ with shape [n, K, c. 2, 4, 3]. RMSD of c neighbors for each kNN pair.\n* Neighbor Loss: $\\mathcal{X}$ with shape [n, K, 4, 3]. RMSD of K neighbors for each residue.\nwhere n is the number of residues, c = 7 is the number of fragments, K = 30 is the number of kNN, 4 means we consider four backbone atoms {N, CA, C, O}, and 3 means the 3D coordinates. The distance loss is defined as the MSE loss between the predicted and ground truth pairwise distances:\n$\\mathcal{L}_{distance} = ||Dist(\\mathcal{X}) - Dist(\\mathcal{X})||$\nwhere $Dist(\\mathcal{X}) \\in \\mathbb{R}^{n,n}$ is the pairwise distance matrix of the 3D coordinates $\\mathcal{X}$. We apply the loss on each decoder layer, and the final loss is the average, whcih is crucial for good performance."}, {"title": "Experiments", "content": "We conduct systematic experiments to inspire further improvement of FoldToken-V2.\n* AutoEncoder Training (Q1): How well the compact and invariant latent space learned by FoldToken2 can be used to reconstruct protein structures?\n* Single-Chain Reconstruction (Q2): Can FoldToken2 outperform FoldToken1 in single-chain protein reconstruction?\n* Multi-Chain Reconstruction (Q3): Can FoldToken2 perform well on multi-chain protein reconstruction?\nMetrics We evaluate FoldToken2 on the protein structure reconstruction task, where the TMscore and aligned RMSD are reported.\nSingle-chain Data We use the CATH4.3 dataset to train the single-chain model. The same as protein inverse folding models, CATH4.3 is split into training (16631), validation (1516), and testing (1864) sets according to the CAT code. During evaluation, we remove proteins with NaN coordinates, resulting in 493 core samples in the final testing set (T493). Due to the different data representations, backbone architecture and iterative refinement strategies, FoldToken2 trains much slower than FoldToken1. Therefore, we do not train FoldToken2 in the AF2DB dataset like FoldToken1, which can be done in the future if the computing resource is enough. In addition, we also report results over the sub-testing set (T116) of FoldToken1 for head-to-head comparision, which contains 116 proteins."}, {"title": "AutoEncoder Training (Q1)", "content": "Setting We train FoldToken2 without vector quantization to show how well the encoder-decoder can learn and generate protein structure patterns. The results could be used as the ceil performance of FoldToken series models. We train the model up to 25 epochs with a batch size of 8 and a learning rate of 0.001. The overall model contains 8 encoding layers, 8 decoding layers, and a 128 hidden dimension, total 9.2M parameters. The training process could be done in 1 days using 1 A100 GPU. We also report the structure prediction results of ESMFold [19], AlphaFold2 [18] and AlphaFold3 [1] for evaluating the structure generation capability of FoldToken2.\nResults In Table. 1, we show the reconstruction results of FoldToken2. In both T493 and T116, FoldToken2 achieves good performance in TMScore and RMSD. When compared to ESMFold, AlphaFold2, and AlphaFold3, FoldToken2 can generate more accurate protein structures from"}, {"title": "Single-Chain Reconstruction (Q2)", "content": "Setting We compare FoldToken2 with FoldToken1 on both T116 and T493 datasets. The model is trained up to 25 epochs with a batch size of 8 and leading rate of 0.001. One experiment could be done in 2 days using 1 80G A100 GPU.\nIn Table. 2 and Fig. 3, we show the reconstruction results of FoldToken2 and conclude that:\nFoldToken2 outperforms FoldToken1 by a large margin on structure reconstruction. In average TMScore, the FT2\u2020 model achieves 0.97 on T116, outperforming FT1 by 20%. In RMSD, FT2\u2020 achieves 0.63 on T116, outperforming FoldToken1 by 81%. Similer results are observed on T493. Regarding the robustness, FT2\u2020 is more reliable, with worst-case TMScores and RMSD of 0.88 and 0.42, respectively, compared to 0.39 and 24.53 for FoldToken1. These results suggest that the new encoder, quantifier, and decoder in FoldToken2 significantly enhance protein structure reconstruction.\nScalling up model does not perform better on limited data. We find that the 8-layer encoder and decoder with hidden size 128 achieve the best performance. Futher increasing the number of layers or"}, {"title": "Multi-Chain Reconstruction (Q3)", "content": "Setting We train FoldToken2 on the multi-chain protein reconstruction task using the PDB dataset. The model is trained for up to 100 epochs with a batch size of 8, a learning rate of 0.001, and a padding length of 512. There is no available benchmark for evaluating multi-chain protein reconstruction, and we provide some visual examples in Fig. 4. The comprehensive benchmark is coming soon.\nFoldToken2 generalize well to large-scale protein systems. While we crop long proteins to ensure that the maximum length is 500, FoldToken2 can generate protein complexes with low RMSD error, as shown in Fig. 4. The results suggest that FoldToken2 can be used for multiple real-world applications beyond protein monomers.\nWe are surprised by the small model size and good training efficiency. Unlike other models that use AF2 for structure generation, FoldToken2 does not copy AF2. Instead, we use a light-weight model, comprising a 4.31M encoder, 4.27M quantifier, and 4.92M decoder, achieving state-of-the-art results. When generating protein structures, FoldToken2 requires 5000 GPU hours; in contrast, FT2 only requires 40GPU hours training over the whole PDB dataset."}, {"title": "Extension (Future Work)", "content": "TokenFlow Given a discrete protein language and protein sequence, we train a flow-matching model using rectified flow [20] on the CATH4.3 training set. The flow-matching model is designed to predict protein structures from single protein sequences without using MSA information. The model is trained for up to 1000 epochs with a batch size of 128, a learning rate of 0.0005, and a padding length of 512. The overall model architecture consists of a 12-layer transformer, similar to ESM-35M. The training process can be completed in about one day using a single A100 GPU. When training over 16K data of CATH4.3, the model seems to be not work well as expected. Limited to the energe and computing resource, we plan to scalling the model in the future.\nFoldGPT Similar to FoldToken1, we train a model for generating masked structure regions based on unmasked ones. However, the model seems to be overfitting on the training data, and we need to scale up the model in the future."}, {"title": "Conclusion", "content": "This paper propose FoldToken2, probably the first multi-chain protein structure tokenization approach. With great efforts in improving the encoder, quantifier, and decoder, FoldToken2 significantly outperforms FoldToken1 by 20% in TMScore and 81% in RMSD. Additionally, the training efficiency is remarkable, requiring only 40 GPU hours to train on the entire PDB dataset. The propose methods may inspire further improvement in protein structure representation learning, structure alignment, and structure generation tasks."}]}