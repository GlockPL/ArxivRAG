{"title": "TGB-SEQ BENCHMARK: CHALLENGING TEMPORAL GNNS WITH COMPLEX SEQUENTIAL DYNAMICS", "authors": ["Lu Yi", "Jie Peng", "Yanping Zheng", "Fengran Mo", "Zhewei Wei", "Yuhang Ye", "Yue Zixuan", "Zengfeng Huang"], "abstract": "Future link prediction is a fundamental challenge in various real-world dynamic systems. To address this, numerous temporal graph neural networks (temporal GNNs) and benchmark datasets have been developed. However, these datasets often feature excessive repeated edges and lack complex sequential dynamics, a key characteristic inherent in many real-world applications such as recommender systems and \"Who-To-Follow\u201d on social networks. This oversight has led existing methods to inadvertently downplay the importance of learning sequential dynamics, focusing primarily on predicting repeated edges.\nIn this study, we demonstrate that existing methods, such as GraphMixer and DyG-Former, are inherently incapable of learning simple sequential dynamics, such as \"a user who has followed OpenAI and Anthropic is more likely to follow AI at Meta next.\" Motivated by this issue, we introduce the Temporal Graph Benchmark with Sequential Dynamics (TGB-Seq), a new benchmark carefully curated to minimize repeated edges, challenging models to learn sequential dynamics and generalize to unseen edges. TGB-Seq comprises large real-world datasets spanning diverse domains, including e-commerce interactions, movie ratings, business reviews, social networks, citation networks and web link networks. Benchmarking experiments reveal that current methods usually suffer significant performance degradation and incur substantial training costs on TGB-Seq, posing new challenges and opportunities for future research. TGB-Seq datasets, leaderboards, and example codes are available at https://tgb-seq.github.io/.", "sections": [{"title": "INTRODUCTION", "content": "Future link prediction (Divakaran & Mohan, 2020) is a fundamental challenge in various real-world dynamic systems, such as social networks (Daud et al., 2020), e-commerce (Bai et al., 2020), financial systems (Rajput & Singh, 2022). For instance, an online shopping website must decide which items to recommend to users based on their click history, while a social networking platform needs to identify which users may be interested in connecting based on their existing relationships. Among the various approaches for future link prediction, temporal Graph Neural Networks (GNNs) are particularly notable for their flexibility in modeling diverse applications and their representation learning capabilities (Zheng et al., 2024; Skarding et al., 2021; Kazemi et al., 2020). Recently, several temporal GNN methods (Yu et al., 2023) have demonstrated impressive performance in future link prediction on existing benchmarks (Poursafaei et al., 2022). However, most existing datasets are not derived from real-world recommender systems, despite recommendations being a natural and essential application of future link prediction.\nObservations. To assess the capability of current temporal GNNs in recommendation tasks, we evaluate their performance on future link prediction using two widely used recommendation datasets,"}, {"title": "TASK FORMULATION AND CURRENT PITFALLS", "content": "Temporal graphs represent entities in the dynamic systems as nodes and interactions among entities as edges. Each edge is labeled with a timestamp to indicate the time of interaction occurred. Existing studies mainly categorize temporal graphs into two types: continuous-time temporal graphs and discrete-time temporal graphs. In this paper, we focus on continuous-time temporal graphs since they better reflect how dynamic graphs form incrementally in real-world scenarios and discrete-time tempo-ral graphs can be directly converted to continuous-time temporal graphs without information loss. For-mally, a continuous-time temporal graph can be denoted as G = (V, E), where the edge set E can be represented as a stream of timestamped edges, i.e., E = {(s_0, d_0, t_0), (s_1, d_1, t_1),\u2026\u2026,(s_T,d_T,t_T)} with s_i, d_i \u2208 V representing the source and destination nodes, respectively. The t_i denotes the timestamp of the i-th edge with t_0 \u2264 t_1 <\u2026\u2026\u2264t_T."}, {"title": "FUTURE LINK PREDICTION FORMULATION AND EVALUATION", "content": "Future Link Prediction. The task of future link prediction is formulated as predicting the existence of a link between two nodes at a given timestamp in literature (Kumar et al., 2019). Specifically, given a temporal graph G, a query edge (s, d, t), and all edges appeared before time t, the model is required to predict the likelihood of the edge (s, d) appearing at time t. However, in real-world applications, the fundamental objective is to determine which entities the query entity is most likely to interact with. For instance, in the \u201cWho-To-Follow\" scenario within social networks, the task is to predict which users the query user is likely to follow next. The users with the highest predicted likelihood are then recommended to the query user. Given the high computational costs associated with calculating the likelihood of all potential entities in a large-scale graph, current literature in recommendation and knowledge graphs He et al. (2017); Kang & McAuley (2018); Teru et al. (2020) treats the future link prediction task as a ranking problem among multiple negative samples. Specifically, given a query edge (s, d, t), the model needs to rank the positive destination node d higher among the sampled k negative destinations based on the likelihood. The current temporal graph benchmark study, TGB (Huang et al., 2024b), adopts these settings and sets k to 20. We set k to 100 for a more robust evaluation in our experimental setup.\nNegative Sampling Strategies. Previous studies (Poursafaei et al., 2022; Huang et al., 2024b) leverage historical edges as negative samples to increase the difficulty for models in predicting potential links, based on the assumption that positive edges are likely repetitions of historical edges. However, this assumption does not hold in the domains covered by our TGB-Seq datasets, where historical edges are unlikely to reoccur in future time steps. Consequently, we randomly sample negative destination nodes from all possible nodes, specifically selecting from all nodes in non-bipartite datasets and all items in bipartite recommendation datasets.\nEvaluation Metrics. Most existing studies leverage Area Under the Receiver Operating Characteristic curve (AUROC) and Average Precision (AP) for link prediction evaluation with a single negative sample, while Yang et al. (2015)and Huang et al. (2024b) argue that they are not proper metrics for link prediction with multiple negative samples. Thus, we deploy the widely used ranking metric, Mean Reciprocal Rank (MRR) for evaluations, following (Cong et al., 2023; Huang et al., 2024b)."}, {"title": "CURRENT PITFALLS IN TEMPORAL GNNS", "content": "In this section, we aim to demonstrate that existing temporal GNNs are unable to capture even simple sequential dynamics. Figure 3 illustrates a toy example of sequential dynam-ics in a temporal graph. To empirically evaluate whether existing temporal GNNs can learn the simple sequential dynamics, we construct a dataset that mirrors the dynamics depicted in Figure 3. Specifically, the dataset consists of items {i_k}^8_{k=0} and multiple nodes in both group u and group v, as in the toy example. To ensure that the sequential dynamics can be effectively modeled, the number of nodes in both group u and group v is set to 500. Each u_k interacts sequentially with items {i_k}^3_{k=0}, while each v_k interacts sequentially with items {i_k}^3_{k=3}. Note that each u_k and v_k always interact at the same timestamps as stated in the caption of Figure 3. Both nodes and edges lack features. The dataset is chronologically split into training set, validation set, and test set. The training set contains the complete interactions of 70% of the users in both group u and group v. Given the four historical interactions, i.e, {i_k}^3_{k=0} or {i_k}^3_{k=3}, a temporal GNN model is required to predict the interaction likelihood of the query user with i_4 and i_9. Despite these straightforward sequential dynamics appearing commonly in the training set and thus considered as simple patterns, existing methods cannot correctly predict item i_4 instead of i_9 given that a test node has interacted with {i_k}^3_{k=0} sequentially. We use the AP metric to evaluate nine temporal GNNs and SGNN-HN. All temporal GNNs achieve an AP score of approximately 50% as shown in Table 1, indicating that they cannot distinguish between i_4 and i_9.\nThe shortcomings of current temporal GNNs in capturing sequential dynamics might relate to the functionality of their structures. Generally, the temporal GNN models can be partitioned into two components: i) a memory module to represent the interaction history of the nodes, and ii) an aggregation module to aggregate neighborhood information when predicting future interactions. Among the existing studies, the designed temporal GNN models might contain both or either of these two components. The limitations of each component in capturing sequential dynamics to distinguish items i_4 and i_9 are discussed as follows.\nNotations. We denote the node feature of u as x_u, the edge feature of (u, v) as e_{u,v}, and the time interval between the interaction (u, v) and the query time as \u2206t. mem(u) and emb(u) denote the memory and embedding of node u, respectively. N(u) denotes the set of k-hop historical neighbors of node u before time t. We use N_b(u) to denote the set of u's neighbors within a batch.\nMemory module. The memory module is designed to memorize the interaction history of nodes using a low-dimensional representation, called memory. Formally, a node s's memory is updated"}, {"title": "PROPOSED DATASETS", "content": "Our proposed TGB-Seq aims to challenge temporal GNNs with intricate sequential dynamics that are inherently exhibited in various real-world dynamic systems. TGB-Seq comprises eight temporal graph"}, {"title": "TRAINING COST", "content": "To comprehensively study the efficiency of existing temporal GNNs, we select three datasets with various sizes of edge sets and report the average training cost per epoch of the corresponding approach. Figure 4 illustrates the results on the GoogleLocal, Patent, and Yelp datasets, where the methods that cannot finish one epoch in 24 hours are omitted. We observe that methods with simpler architectures, such as JODIE, DyRep, TCL and GraphMixer, exhibit significantly shorter training time compared to the others. In contrast, TGAT and CAWN are the most inefficient methods, requiring considerable time to complete an epoch on the Patent and Yelp datasets. This inefficiency stems from the complex aggregation modules of these methods, which require multi-hop neighbor retrieval. While DyGFormer is more efficient than TGAT and CAWN on GoogleLocal, Patent, and Yelp, it still incurs higher costs. The calculation of co-occurrence frequencies for neighbors becomes particularly expensive when the temporal graph is dense. As demonstrated in Table 3, DyGFormer cannot complete an epoch within 24 hours for the ML-20M dataset, whereas other methods, including TGAT and CAWN, can.\nThese observations highlight that complex aggregation modules can significantly increase the training cost of existing temporal GNNs. As demonstrated in Table 3 and Table 4, simpler methods like TCL and GraphMixer may be more efficient in terms of training, but they fail to achieve satisfactory performance. This investigation suggests that achieving both efficiency and effectiveness in temporal GNNs simultaneously remains an open problem, further underscoring the distinctive capability of TGB-Seq for comprehensive evaluations of these models."}, {"title": "CONCLUSION", "content": "In this paper, we demonstrate that current temporal GNNs struggle to capture intricate sequential dynamics inherent in real-world dynamic systems, thereby limiting their abilities to generalize across various future link prediction applications. However, existing datasets often contain excessively repeated edges and thus are inadequate for evaluating such abilities of temporal GNNs compre-hensively. To address this gap, we propose TGB-Seq, a new challenging benchmark for temporal GNNs. TGB-Seq comprises eight datasets curated from diverse application domains characterized by complex sequential dynamics. Comprehensive evaluations on TGB-Seq reveal that existing temporal GNNs fail to achieve satisfactory performance across all datasets, underscoring the limitations of current methods and the necessity of TGB-Seq for robust temporal GNN evaluation."}, {"title": "DATASET DOCUMENTATION", "content": "The resource links for TGB-Seq benchmark suits and datasets are provided as follows.\n\u2022 The TGB-Seq website: https://tgb-seq.github.io/.\n\u2022 The TGB-Seq datasets are available at Hugging Face: https://huggingface.co/TGB-Seq.\n\u2022 The tgb-seq pip package is at https://pypi.org/project/tgb-seq/. The package will download datasets and negative samples automatically from Hugging Face.\n\u2022 The TGB-Seq datasets can also be downloaded at https://drive.google.com/drive/folders/1qoGtASTbYCO-bSWAzSqbSY2YgHr9hUhK.\n\u2022 The original datasets of TGB-Seq are available at https://drive.google.com/file/d/1-1Ndp3R2qk_Jfk2zctReZIPOOzUuAQQI."}, {"title": "FURTHER INFORMATION ON TGB-SEQ DATASETS", "content": "We provide a selected list of commonly used datasets for continuous-time temporal graph learning in Table 5 for reference. We also plot the node degree distribution of TGB-Seq datasets in Figure 5. Figure 5 demonstrates that all the TGB-Seq datasets follow a power-law distribution, a common characteristic of real-world networks (Barab\u00e1si, 2013). This power-law behavior indicates that while a few hub nodes have a high degree of connections, the majority of nodes possess significantly fewer connections. Consequently, TGB-Seq is highly sparse, exhibiting low density, as shown in Table 5.\nPreprocessing of the Patent dataset. The Patent dataset is quite special that all citations of one patent are labeled with the same timestamp, specifically the publication time of the patent. Therefore, we carefully select test samples to ensure that each patent has prior citations, so that temporal GNNs are able to leverage these historical edges for future link prediction. Specifically, we choose not to validate or test the first 50% of citations for the patents included in the validation and test sets; these citations serve solely as historical edges and are not used for model training. The remaining 50% of citations are then evenly divided into validation and test samples. Although the citations of a patent occur simultaneously at the publication time, temporal GNNs can utilize the relative publication times of these patents and their neighbors to capture inherent research trends, thereby enhancing future link prediction performance."}, {"title": "EXPERIMENTAL CONFIGURATIONS", "content": "We perform a grid search to determine the optimal settings for key hyperparameters, with the search ranges and corresponding methods detailed in Table 7. The final hyperparameter configurations identified through the grid search for various methods are summarized in Table 8 and Table 9. Regarding the configurations of neighbor sampling strategies, most methods achieve their best performance using the recent neighbor sampling strategy. However, for the ML-20M dataset, both CAWN and TCL achieve their best performance with the uniform neighbor sampling strategy.\nFor the ML-20M and the Flickr datasets, experiments are conducted on an Ubuntu machine equipped with Intel(R) Xeon(R) Gold 6240R CPU @ 2.40GHz. The GPU device is NVIDIA A100 with 80 GB memory. For the Taobao dataset, experiments are conducted on an Ubuntu machine equipped with Intel(R) Xeon(R) Gold 5218 CPU @ 2.30GHz. The GPU device is NVIDIA A100-SXM4 with 80 GB memory. For the Yelp dataset, experiments are conducted on an Ubuntu machine equipped with Intel(R) Xeon(R) Gold 6226R CPU @ 2.90GHz. The GPU device is NVIDIA RTX A6000 with 40 GB memory. For the GoogleLocal, the Patent, and the WikiLink datasets, experiments are conducted on an Ubuntu machine equipped with Hygon C86 7390 32-core Processor. The GPU device is NVIDIA A800 with 80 GB memory. For the YouTube dataset, experiments are conducted on an Ubuntu machine equipped with Intel(R) Xeon(R) Platinum 8369B CPU @ 2.90GHz. The GPU device is A100-SXM4 with 80 GB memory."}, {"title": "TEMPORAL GRAPH LEARNING METHODS", "content": "We provide a brief overview of the temporal GNNs used in our experiments as follows.\nJODIE (Kumar et al., 2019) uses two coupled recurrent neural networks to dynamically update the states of users and items during interactions. It includes a novel projection operation that predicts future representation trajectories of both users and items, allowing the model to anticipate future behaviors. This architecture not only captures the evolution of user-item interactions but also facilitates the learning of representations that can be used for downstream tasks like recommendation and link prediction.\nDyRep (Trivedi et al., 2019) introduces a dynamic representation learning framework that updates node states in real-time with each interaction. It leverages a recurrent neural network to capture node interactions and utilizes a temporal-attentive aggregation module to focus on evolving graph structures over time. DyRep is particularly effective in modeling dynamic relationships by considering both node communication and structural events, thus providing a comprehensive understanding of temporal graph changes.\nTGAT (Xu et al., 2020) incorporates self-attention mechanisms to simultaneously model both the structural and temporal properties of dynamic graphs. Its design includes a time encoding function that uniquely represents temporal information, enabling the model to handle complex, evolving interactions among nodes. This combination allows TGAT to capture intricate temporal patterns and efficiently aggregate information from temporal-topological neighbors.\nTGN (Rossi et al., 2020) introduces a memory-based approach for dynamic graph learning, where each node maintains an evolving memory that is updated through various interactions. Using a combination of message functions, aggregators, and memory updaters, TGN generates temporal node representations. The embedding module is crucial in capturing the temporal dynamics of nodes, which makes TGN adaptable for various dynamic graph tasks like link prediction and node classification.\nCAWN (Wang et al., 2021d) performs random walks on continuous-time dynamic graphs and employs an attention mechanism to selectively focus on crucial segments of these walks. This allows it to capture both temporal relationships and causal dependencies in the network. By learning these patterns, CAWN is capable of generating relative node identities, making it effective for temporal graph tasks such as anomaly detection and node classification.\nEdgeBank (Poursafaei et al., 2022) is a memory-centric approach tailored for transductive dynamic link prediction without relying on trainable parameters. It memorizes observed interactions and uses various strategies to update its memory. EdgeBank predicts future interactions based on whether the interaction is stored in its memory. Its simplicity lies in its rule-based decision-making, making it a lightweight yet competitive approach for link prediction in dynamic networks.\nTCL (Wang et al., 2021a) employs contrastive learning on temporal graphs to learn robust node embeddings. Maximizing the agreement between node pairs that are temporally similar captures both temporal dependencies and topological structures. TCL uses a graph transformer to incorporate both graph topology and temporal information, along with cross-attention mechanisms to model interactions between nodes over time.\nGraphMixer (Cong et al., 2023) focuses on enhancing node embeddings in dynamic graphs by mixing both temporal and structural features. It uses a fixed time encoding function rather than a trainable one, incorporating it into a link encoder based on MLP-Mixer to learn temporal links effectively. GraphMixer also includes a node encoder with neighbor mean-pooling to aggregate node features, offering a comprehensive method for dynamic graph analysis.\nDyGFormer (Yu et al., 2023) adopts a Transformer-based approach to capture long-term temporal dependencies in dynamic graphs. It introduces neighbor co-occurrence encoding and patching techniques, which help in modeling both the local and global structure of evolving interactions. This allows DyGFormer to effectively capture complex patterns in dynamic environments, making it suitable for various temporal graph tasks."}]}