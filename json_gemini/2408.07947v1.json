{"title": "Conditional Brownian Bridge Diffusion Model for\nVHR SAR to Optical Image Translation", "authors": ["Seon-Hoon Kim", "Dae-won Chung"], "abstract": "Abstract-Synthetic Aperture Radar (SAR) imaging technol-\nogy provides the unique advantage of being able to collect\ndata regardless of weather conditions and time. However, SAR\nimages exhibit complex backscatter patterns and speckle noise,\nwhich necessitate expertise for interpretation. To deal with this\nchallenge, research has been conducted on translating SAR\nimages into optical-like representations to aid the interpretation\nof SAR data. Nevertheless, existing studies have predominantly\nutilized low-resolution satellite imagery datasets and have largely\nbeen based on Generative Adversarial Network (GAN) which\nare known for their training instability and low fidelity. To\novercome these limitations of low-resolution data usage and GAN-\nbased approaches, this paper introduces a conditional image-to-\nimage translation approach based on Brownian Bridge Diffusion\nModel (BBDM). We conducted comprehensive experiments on\nthe MSAW dataset, a paired SAR and optical images collection\nof 0.5m Very-High-Resolution (VHR) images. The experimental\nresults indicate that our method surpasses both the Conditional\nDiffusion Model (CDM) and the GAN-based models in diverse\nperceptual quality metrics.", "sections": [{"title": "I. INTRODUCTION", "content": "NOTWITHSTANDING the all-weather, day-and-night ca-\npability of Synthetic Aperture Radar (SAR), its advan-\ntages are accompanied by notable challenges. Unintentional\nartifacts like speckle noise and geometric distortions, inherent\nto SAR images, complicate their analysis compared to optical\nimagery. VHR SAR to optical translation is essential for\ntaking advantage of VHR SAR's all-weather and fine-detail\ncapturing capabilities while enhancing image interpretability.\nRecent studies have explored translating SAR imagery into\noptical-like images using deep learning models to improve\ninterpretability. Furthermore, this translation process becomes\nsignificantly more complex at higher resolutions due to in-\ncreased detail complexity, greater computational demands,\nand a wider domain gap between SAR and optical imagery\ncharacteristics.\nHowever, research on SAR to optical translation using Very\nHigh Resolution (VHR) data with sub-meter resolution is\nextremely scarce. Most existing studies have utilized datasets\nthat fall short of sub-meter VHR standards [1]-[7]. The widely\nused SEN12 dataset [8] consists of paired SAR and optical\nimages at 10-meter resolution, captured by Sentinel-1 and\nSentinel-2 satellites respectively. While this dataset has been\nimmensely valuable for various remote sensing applications,\nits relatively coarse resolution limits its utility for VHR SAR\nto optical translation tasks. Other datasets such as WHU-OPT-\nSAR [9] and SARptical [10], while offering improvements in\nresolution, still do not meet the sub-meter resolution criteria\nfor true VHR data.\nYet, even with this inherent advantage of utilizing these\nlow-resolution images, existing GAN-based approaches for\ntranslating SAR-to-optical have struggled to achieve practi-\ncal performance, facing issues like training instability, mode\ncollapse, and geometric loss with complex scenes [1]\u2013[5]\n[11]. Only a few recent studies [6], [7] have explored Con-\nditional Diffusion Model (CDM) to overcome the limitations\nof GAN-based models, which currently dominate the image\nsynthesis field [12]. However, while promising, CDM face\nchallenges in terms of limited model generalization. Moreover,\nthey lack of robust theoretical foundations to ensure that\nthe final diffusion outcome accurately represents the intended\nconditional distribution. Additionally, while conditional Latent\nDiffusion Model (LDM) improved generalization to a degree,\nit still experiences performance degradation when translating\nbetween significantly disparate domains.\nTo alleviate the limitations in existing research, we used the\nMSAW dataset [13], which provides overlapped pairs of 0.5m\nVHR SAR and optical imagery. Unlike a previous study [11]\nthat used random train-validation splits on overlapping areas,\nwe split the dataset based on longitude to ensure truly unseen\nvalidation data. We adopted an innovative image translation\nframework, the Brownian Bridge Diffusion Model (BBDM)\n[14], utilizing the carefully partitioned MSAW dataset. BBDM\nestablishes a mathematical foundation for diffusion-based\nimage translation. BBDM utilizes a bidirectional diffusion\nprocess based on stochastic Brownian bridge to directly learn\nthe mapping between domains.\nInspired by CDM [12], we adjusted the BBDM framework\nfor VHR SAR to optical image translation. Our approach\nincorporates bilinearly interpolated information from the pixel\nspace as a condition. This ensures that the model receives\nintact spatial information to guide the translation process.\nWe perform the diffusion process in a compressed latent\nspace to operate it more efficiently by leveraging compact yet\ninformative latent representations. This combination of latent\nrepresentations and interpolated information as a condition\nstrikes a balance between computational efficiency and the\npreservation of essential spatial details from the SAR image."}, {"title": "II. METHODOLOGY", "content": "Recent progress in generative modeling has been driven\nby diffusion-based methods. Ho et al. [16] effectively imple-\nmented the diffusion framework for image synthesis. Rombach\net al. [15] proposed conditional LDM which performs the\ndiffusion process in the compressed latent space as illustrated\nin Fig. 1. Forward process gradually transforms an original\nlatent feature xo into Gaussian noise x\u0442 ~ N(0, 1) through T\niterative noise addition steps:\nq(xt|xt\u22121) := N(xt; \\sqrt{1 \u2013 \u03b2t}xt\u22121, \u03b2tI)\nwhere \u1e9et is a linearly scheduled variance. As shown in\nFig. 1, the reverse process reconstructs the original image by\nestimating noise at each step:\nPo(Xt-1|Xt) := N(xt\u22121; \u03bc\u00f8(xt, t), \u03c3I)\nwhere \u03c3\u03c4\u03b9 represents the time-dependent constants which\nare not trainable. For image-to-image translation, conditional"}, {"title": "B. Brownian Bridge Diffusion Model", "content": "Li et al. [14] proposed Brownian Bridge Diffusion Model\n(BBDM) which is a novel approach for image translation\ngrounded in concepts from Brownian bridge process. A Brow-\nnian bridge represents a continuous-time stochastic process\nwith fixed start and end points. BBDM applies this concept\nto represent the translation as a probabilistic transformation\nbetween two fixed states, as shown in Fig. 1. For a given\nstarting point 20 and ending point x, the probability distri-\nbution of intermediate states xt in a Brownian bridge process\nis formulated as:\nP(xt|x0, xT) = N(xt; (1-\\frac{t}{T})x_0+\\frac{t}{T}x_T, \\frac{t}{T}(1-\\frac{t}{T})I)\nTo downscale the maximum variance that peaks at the\nmidpoint, d\n7, [14] formulate the Brownian bridge\nforward diffusion process as:\nqBB(xt|x0,y) = N(xt; (1 \u2013 mt)xo + mty, d\u1e6dI)\nwhere xo represents the initial image, y denotes the target\ndomain image, mt = t/T, and dt = 2(mt \u2013 m\u00b2) which is the\nscheduled variance. In this formulation, the maximum variance\n\u0431\u0442\u0430\u0445 = 87 equals \\frac{1}{2}. When t = 0, mo is equal to 0, the mean\nand variance becomes 20 and 0, respectively. When t = T,\nmr is equal to 1, the mean and variance becomes y and 0\nrespectively. This formulation is analogous to the Brownian\nbridge, with x and y serving as the fixed endpoints.\nBy combining two equations of xt and Xt-1 defined from\nEq.(5), the transition probability can be derived as proposed\nin [14].\nqBB(xt|xt\u22121,Y) = N(xt; \\frac{1}{1-m_{t-1}}-\\frac{m_t}{m_t-1}Xt-1+\\frac{m_t}{m_t-1}y, d_{t|t-1}I)\nwhere Stt-1 is derived as:\n\u03b4t|t-1 = \u03b4\u03b5 - \u03b4\u03b5\u22121\n(1 - mt)2\n(1-mt-1)2\n(7)\nThe transition probability given by Eq.(6) still describes a\ndirect mapping between two fixed points.\nThe reverse process aims to estimate the mean of the noise\nat each step:\nPo(Xt\u22121|Xt, y) = N(xt\u22121; \u03bc\u04e9(xt, t), \u03b4\u03b5I)"}, {"title": "C. Conditional BBDM for SAR to optical", "content": "To address the limitations of the original BBDM and\nadapt it for SAR to optical image translation, we propose a\nconditional BBDM (cBBDM). This modification allows for\nthe incorporation of additional guiding information during the\ntranslation process, leading to more refined results.\nAs with the original BBDM, CBBDM mainly operates\nin the compressed latent space of images, which reduces\ncomputational complexity while preserving essential features.\nAs illustrated in Fig. 1, we introduce a conditioning variable\nc, derived from the pixel space of given SAR images to guide\nthe translation process. This condition is obtained by applying\nbilinear interpolation to the SAR images to align with the 16-\nfold compressed latent space dimensions, followed by a 1x1\nconvolution. The condition c is incorporated into the reverse\ndiffusion process through channel-wise concatenation ([,]):\nPo(Xt-1|Xt, XT, c) = N(xt\u22121; \u03bc\u00f8 ([xt, c], t), \u03b4tl)\nThe training objective of cBBDM is modified to include\nthis conditioning information:\nEx0, XT, C, \u20ac[Cet||mt(xT - xo) + \\sqrt{\u03b4\u03b9\u03b5} - \u20ac0 ([xt, c], t)||2]\nwhere 60 ([xt, c], t) is the noise prediction network that takes\nthe concatenated representation of latent features and condition\nas input."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "We conducted experiments on the MSAW dataset [13]\nwhich consists of 0.5m VHR SAR and optical images captured\nby Capella Space X-band quad-pol SAR and WorldView-2\noptical satellite, respectively. This dataset offers 3,401 geo-\nreferrenced SAR and optical imagery paired sets, tiled to\n900x900 pixels (450m x 450m) including zero-intensity back-\nground.\nOne or more 512x512 images were extracted from within\neach tile removing zero-intensity background. Since the paired\nsets overlap with each other, we split them into train and\nvalidation sets based on longitude to ensure that validation\nis conducted on unseen data. We created 3-channel false\ncolor composite with VV, VH, HH polarization and applied\nhistogram equalization using CLAHE algorithm. We did not\napply any noise reduction algorithm."}, {"title": "B. Evaluation Metrics", "content": "We used several metrics to assess image translation perfor-\nmance of our proposed method compared to conditional LDM\nand GAN-based models. Three Learned Perceptual Image\nPatch Similarity (LPIPS) [17] scores are selected to evaluate\nhow effectively the translated SAR imagery improves SAR\nimage interpretability. These scores, based on three different\nneural networks (AlexNet, VGG-16, and SqueezeNet), are\nwidely used to measure perceptual quality. We also used\nFr\u00e9chet Inception Distance (FID) [18] to evaluate distribution\nsimilarity between generated and authentic optical images.\nLower scores indicate better performance for both metrics."}, {"title": "C. Implementation Details", "content": "We adopted a pre-trained VQ-VAE which reduces pixel\nspace dimensions by a factor of 4. We trained the proposed\nmodel for 48 epochs using an NVIDIA A6000 GPU, with\na minibatch size of 1 and Adam optimizer of le-4 learning\nrate. Data augmentation was implemented using horizontal\nflips during training. To ensure fair comparison, we applied\nsame optimizer and augmentation techniques when training the\nconditional LDM and GAN-based models. These comparative\nmodels were trained using their default minibatch size and\nlearning rates, which are widely adopted in the literature. The\nconditional LDM was fine-tuned from the pre-trained model\nby [15]. For evaluation, we selected the checkpoint of each\nmodel that recorded the minimal loss on the validation dataset."}, {"title": "D. Results and Analysis", "content": "To evaluate our proposed method, we conducted compar-\nisons with widely-used GAN-based models in the field of\nSAR to optical image translation, specifically Pix2Pix [19]\nand CycleGAN [20]. Additionally, we tested a representative\nconditional Diffusion Model, conditional LDM [15], to assess\nthe effectiveness of the BBDM framework based on Brownian\nbridge for image translation. Furthermore, we also tested\noriginal BBDM without any condition for ablation study."}, {"title": "IV. CONCLUSION", "content": "In this paper, we have proposed conditional BBDM, an\ninnovative framework for VHR SAR-to-optical image trans-\nlation. By combining conditional information with Brownian\nbridge mapping in latent space, cBBDM achieves high-quality,\ncost-effective translation for large VHR images. Quantitative\nand qualitative assessments indicate that cBBDM generates\nimpressive optical-like images and reveals its practical po-\ntential for SAR imagery interpretation. As an alternative to\nGAN-based approaches, our work contributes to improving\nSAR image interpretation and narrows the gap between SAR\nand optical remote sensing modalities. This development has\nthe potential to open new research avenues in this field."}]}