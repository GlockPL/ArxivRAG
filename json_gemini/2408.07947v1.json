{"title": "Conditional Brownian Bridge Diffusion Model for VHR SAR to Optical Image Translation", "authors": ["Seon-Hoon Kim", "Dae-won Chung"], "abstract": "Synthetic Aperture Radar (SAR) imaging technology provides the unique advantage of being able to collect data regardless of weather conditions and time. However, SAR images exhibit complex backscatter patterns and speckle noise, which necessitate expertise for interpretation. To deal with this challenge, research has been conducted on translating SAR images into optical-like representations to aid the interpretation of SAR data. Nevertheless, existing studies have predominantly utilized low-resolution satellite imagery datasets and have largely been based on Generative Adversarial Network (GAN) which are known for their training instability and low fidelity. To overcome these limitations of low-resolution data usage and GAN-based approaches, this paper introduces a conditional image-to-image translation approach based on Brownian Bridge Diffusion Model (BBDM). We conducted comprehensive experiments on the MSAW dataset, a paired SAR and optical images collection of 0.5m Very-High-Resolution (VHR) images. The experimental results indicate that our method surpasses both the Conditional Diffusion Model (CDM) and the GAN-based models in diverse perceptual quality metrics.", "sections": [{"title": "I. INTRODUCTION", "content": "NOTWITHSTANDING the all-weather, day-and-night ca-\npability of Synthetic Aperture Radar (SAR), its advan-\ntages are accompanied by notable challenges. Unintentional\nartifacts like speckle noise and geometric distortions, inherent\nto SAR images, complicate their analysis compared to optical\nimagery. VHR SAR to optical translation is essential for\ntaking advantage of VHR SAR's all-weather and fine-detail\ncapturing capabilities while enhancing image interpretability.\nRecent studies have explored translating SAR imagery into\noptical-like images using deep learning models to improve\ninterpretability. Furthermore, this translation process becomes\nsignificantly more complex at higher resolutions due to in-\ncreased detail complexity, greater computational demands,\nand a wider domain gap between SAR and optical imagery\ncharacteristics.\nHowever, research on SAR to optical translation using Very\nHigh Resolution (VHR) data with sub-meter resolution is\nextremely scarce. Most existing studies have utilized datasets\nthat fall short of sub-meter VHR standards [1]-[7]. The widely\nused SEN12 dataset [8] consists of paired SAR and optical\nimages at 10-meter resolution, captured by Sentinel-1 and\nSentinel-2 satellites respectively. While this dataset has been\nimmensely valuable for various remote sensing applications,\nits relatively coarse resolution limits its utility for VHR SAR\nto optical translation tasks. Other datasets such as WHU-OPT-\nSAR [9] and SARptical [10], while offering improvements in\nresolution, still do not meet the sub-meter resolution criteria\nfor true VHR data.\nYet, even with this inherent advantage of utilizing these\nlow-resolution images, existing GAN-based approaches for\ntranslating SAR-to-optical have struggled to achieve practi-\ncal performance, facing issues like training instability, mode\ncollapse, and geometric loss with complex scenes [1]\u2013[5]\n[11]. Only a few recent studies [6], [7] have explored Con-\nditional Diffusion Model (CDM) to overcome the limitations\nof GAN-based models, which currently dominate the image\nsynthesis field [12]. However, while promising, CDM face\nchallenges in terms of limited model generalization. Moreover,\nthey lack of robust theoretical foundations to ensure that\nthe final diffusion outcome accurately represents the intended\nconditional distribution. Additionally, while conditional Latent\nDiffusion Model (LDM) improved generalization to a degree,\nit still experiences performance degradation when translating\nbetween significantly disparate domains.\nTo alleviate the limitations in existing research, we used the\nMSAW dataset [13], which provides overlapped pairs of 0.5m\nVHR SAR and optical imagery. Unlike a previous study [11]\nthat used random train-validation splits on overlapping areas,\nwe split the dataset based on longitude to ensure truly unseen\nvalidation data. We adopted an innovative image translation\nframework, the Brownian Bridge Diffusion Model (BBDM)\n[14], utilizing the carefully partitioned MSAW dataset. BBDM\nestablishes a mathematical foundation for diffusion-based\nimage translation. BBDM utilizes a bidirectional diffusion\nprocess based on stochastic Brownian bridge to directly learn\nthe mapping between domains.\nInspired by CDM [12], we adjusted the BBDM framework\nfor VHR SAR to optical image translation. Our approach\nincorporates bilinearly interpolated information from the pixel\nspace as a condition. This ensures that the model receives\nintact spatial information to guide the translation process.\nWe perform the diffusion process in a compressed latent\nspace to operate it more efficiently by leveraging compact yet\ninformative latent representations. This combination of latent\nrepresentations and interpolated information as a condition\nstrikes a balance between computational efficiency and the\npreservation of essential spatial details from the SAR image."}, {"title": "II. METHODOLOGY", "content": "Recent progress in generative modeling has been driven\nby diffusion-based methods. Ho et al. [16] effectively imple-\nmented the diffusion framework for image synthesis. Rombach\net al. [15] proposed conditional LDM which performs the\ndiffusion process in the compressed latent space as illustrated\nin Fig. 1. Forward process gradually transforms an original\nlatent feature xo into Gaussian noise x\u0442 ~ N(0, 1) through T\niterative noise addition steps:\nq(xt|xt\u22121) := N(xt; \u221a\u221a1 \u2013 Btxt\u22121, \u03b2tI)\nwhere \u1e9et is a linearly scheduled variance. As shown in\nFig. 1, the reverse process reconstructs the original image by\nestimating noise at each step:\nPo(Xt-1|Xt) := N(xt\u22121; \u03bc\u00f8(xt, t), \u03c3\u0399)\nwhere \u03c3\u03c4\u03b9 represents the time-dependent constants which\nare not trainable. For image-to-image translation, conditional"}, {"title": "B. Brownian Bridge Diffusion Model", "content": "Li et al. [14] proposed Brownian Bridge Diffusion Model\n(BBDM) which is a novel approach for image translation\ngrounded in concepts from Brownian bridge process. A Brow-\nnian bridge represents a continuous-time stochastic process\nwith fixed start and end points. BBDM applies this concept\nto represent the translation as a probabilistic transformation\nbetween two fixed states, as shown in Fig. 1. For a given\nstarting point 20 and ending point x, the probability distri-\nbution of intermediate states xt in a Brownian bridge process\nis formulated as:\nP(xt|x0, XT) = N(x\u2081; (1-1)x+x, (1-1)\nTo downscale the maximum variance that peaks at the\nmidpoint, d\n7, [14] formulate the Brownian bridge\nforward diffusion process as:\nqBB(xt|x0,y) = N(xt; (1 \u2013 mt)xo + mty, d\u1e6dI)\nwhere xo represents the initial image, y denotes the target\ndomain image, mt = t/T, and dt = 2(mt \u2013 m\u00b2) which is the\nscheduled variance. In this formulation, the maximum variance\n\u0431\u0442\u0430\u0445 = 87 equals. When t = 0, mo is equal to 0, the mean\nand variance becomes 20 and 0, respectively. When t = T,\nmr is equal to 1, the mean and variance becomes y and 0,\nrespectively. This formulation is analogous to the Brownian\nbridge, with x and y serving as the fixed endpoints.\nBy combining two equations of xt and Xt-1 defined from\nEq.(5), the transition probability can be derived as proposed\nin [14].\nqBB(xt|xt\u22121,Y) = N(xt; 1-mt-1\nwhere Stt-1 is derived as:\nStt-1 = \u03b4\u03b5 - \u03b4\u03b5-\u22121\nThe transition probability given by Eq.(6) still describes a\ndirect mapping between two fixed points.\nThe reverse process aims to estimate the mean of the noise\nat each step:\nPo(Xt\u22121|Xt, y) = N(xt\u22121; \u03bc\u04e9(xt, t), \u03b4\u03b5\u0399)\nwhere \u03b4t represents the variance of the noise at each step:\nt =\nWe can express the training objective of BBDM, derived\nfrom the Evidence Lower Bound (ELBO), in a simplified form\nas proposed in [14]:\nExo, y, \u20ac[Cet||mt(y \u2013 xo) + \u221a \u03b4\u03b9\u03b5 \u2013 \u03b5\u03b8(xt,t)||2]\nwhere Cet is defined as:\nCet = (1-mt-1)\nThe formulation enables BBDM to learn a smooth, contin-\nuous mapping between image domains, potentially resulting\nin higher quality translations. The application of the Brownian\nbridge concept ensures that the translation process is anchored\nat both the source and target images. It provides a natural\nframework for image-to-image translation.\nDespite its advantages, including direct modeling of domain\ntransitions and efficient handling of high-resolution images\nthrough latent space operations, BBDM presents certain lim-\nitations. The primary constraint is the absence of explicit\nconditioning mechanisms. BBDM does not incorporate addi-\ntional guiding information during the translation process. This\nlimitation may potentially restrict its utility in applications\nrequiring fine-grained control over the translation process."}, {"title": "C. Conditional BBDM for SAR to optical", "content": "To address the limitations of the original BBDM and\nadapt it for SAR to optical image translation, we propose a\nconditional BBDM (cBBDM). This modification allows for\nthe incorporation of additional guiding information during the\ntranslation process, leading to more refined results.\nAs with the original BBDM, CBBDM mainly operates\nin the compressed latent space of images, which reduces\ncomputational complexity while preserving essential features.\nAs illustrated in Fig. 1, we introduce a conditioning variable\nc, derived from the pixel space of given SAR images to guide\nthe translation process. This condition is obtained by applying\nbilinear interpolation to the SAR images to align with the 16-\nfold compressed latent space dimensions, followed by a 1x1\nconvolution. The condition c is incorporated into the reverse\ndiffusion process through channel-wise concatenation ([,]):\nPo(Xt-1|Xt, XT, c) = N(xt\u22121; \u03bc\u00f8 ([xt, c], t), \u03b4tl)\nThe training objective of cBBDM is modified to include\nthis conditioning information:\nEx0, XT, C, \u20ac[Cet||mt(xT - xo) + \u221a \u03b4\u03b9\u03b5 - \u20ac0 ([xt, c], t)||2]\nwhere 60 ([xt, c], t) is the noise prediction network that takes\nthe concatenated representation of latent features and condition\nas input."}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "We conducted experiments on the MSAW dataset [13]\nwhich consists of 0.5m VHR SAR and optical images captured\nby Capella Space X-band quad-pol SAR and WorldView-2\noptical satellite, respectively. This dataset offers 3,401 geo-\nreferrenced SAR and optical imagery paired sets, tiled to\n900x900 pixels (450m x 450m) including zero-intensity back-\nground.\nOne or more 512x512 images were extracted from within\neach tile removing zero-intensity background. Since the paired\nsets overlap with each other, we split them into train and\nvalidation sets based on longitude to ensure that validation\nis conducted on unseen data. We created 3-channel false\ncolor composite with VV, VH, HH polarization and applied\nhistogram equalization using CLAHE algorithm. We did not\napply any noise reduction algorithm.\nWe used several metrics to assess image translation perfor-\nmance of our proposed method compared to conditional LDM\nand GAN-based models. Three Learned Perceptual Image\nPatch Similarity (LPIPS) [17] scores are selected to evaluate\nhow effectively the translated SAR imagery improves SAR\nimage interpretability. These scores, based on three different\nneural networks (AlexNet, VGG-16, and SqueezeNet), are\nwidely used to measure perceptual quality. We also used\nFr\u00e9chet Inception Distance (FID) [18] to evaluate distribution\nsimilarity between generated and authentic optical images.\nLower scores indicate better performance for both metrics."}, {"title": "C. Implementation Details", "content": "We adopted a pre-trained VQ-VAE which reduces pixel\nspace dimensions by a factor of 4. We trained the proposed\nmodel for 48 epochs using an NVIDIA A6000 GPU, with\na minibatch size of 1 and Adam optimizer of le-4 learning\nrate. Data augmentation was implemented using horizontal\nflips during training. To ensure fair comparison, we applied\nsame optimizer and augmentation techniques when training the\nconditional LDM and GAN-based models. These comparative\nmodels were trained using their default minibatch size and\nlearning rates, which are widely adopted in the literature. The\nconditional LDM was fine-tuned from the pre-trained model\nby [15]. For evaluation, we selected the checkpoint of each\nmodel that recorded the minimal loss on the validation dataset."}, {"title": "D. Results and Analysis", "content": "To evaluate our proposed method, we conducted compar-\nisons with widely-used GAN-based models in the field of\nSAR to optical image translation, specifically Pix2Pix [19]\nand CycleGAN [20]. Additionally, we tested a representative\nconditional Diffusion Model, conditional LDM [15], to assess\nthe effectiveness of the BBDM framework based on Brownian\nbridge for image translation. Furthermore, we also tested\noriginal BBDM without any condition for ablation study."}, {"title": "1) Quantitative Results", "content": "Table I presents the quantitative\ncomparison between ground truth optical images and trans-\nlated optical-like images. The top score is marked in red,\nwhile the second-highest is shown in green. Our proposed\nmodel significantly outperforms other models in three different\nLPIPS scores and FID. The superior LPIPS scores indicate ex-\ncellent perceptual quality in our results. These scores highlight\nour method's potential as a robust SAR-to-optical translation\nframework. The best FID score suggests our model most\naccurately reconstructs the target distribution. These strong\nresults on deep learning-based metrics highlight the potential\nof our generated optical-like images for deep learning-based\ndownstream applications, such as synthetic pretraining."}, {"title": "2) Qualitative Results", "content": "Fig. 2 illustrates translated optical-\nlike images from various methods. The first row depicts\na complex urban scene. Pix2Pix, while initially appearing\nsatisfactory, upon closer inspection reveals significant loss of\nstructural detail. CycleGAN failed to preserve surface bound-\naries and over-emphasized building side walls, a common\nartifact in CycleGAN's style-focused approach. Conditional\nLDM, while distinguishing surfaces well, produced hazy im-\nges due to over-reliance on the given condition. In contrast,\nour conditional BBDM demonstrates superior reconstruction\nproducing clearly discernible surfaces. It outperformed the\nbasic BBDM which missed some detailed information.\nThe second row presents a scene with trees and bare land.\nOur conditional BBDM successfully translated this challeng-\ning scene and clearly distinguished land and trees despite the\npotential confusion of low-intensity bare land with speckle\nnoise. This demonstrates CBBDM's robustness against speckle\nnoise, generating a distinguishable optical-like image from\nSAR image without any prior noise reduction. While the\ncolorization leans towards green tones, it outperforms other\nmethods which produced indistinguishable surfaces with nu-\nmerous artifacts. These results highlight the effectiveness of\nconditional BBDM in generating high-quality, detailed optical-\nlike images from SAR inputs across various scene types."}, {"title": "IV. CONCLUSION", "content": "In this paper, we have proposed conditional BBDM, an\ninnovative framework for VHR SAR-to-optical image trans-\nlation. By combining conditional information with Brownian\nbridge mapping in latent space, cBBDM achieves high-quality,\ncost-effective translation for large VHR images. Quantitative\nand qualitative assessments indicate that cBBDM generates\nimpressive optical-like images and reveals its practical po-\ntential for SAR imagery interpretation. As an alternative to\nGAN-based approaches, our work contributes to improving\nSAR image interpretation and narrows the gap between SAR\nand optical remote sensing modalities. This development has\nthe potential to open new research avenues in this field."}]}