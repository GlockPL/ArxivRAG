{"title": "MpoxVLM: A Vision-Language Model for Diagnosing Skin Lesions from Mpox Virus Infection", "authors": ["Xu Cao", "Wenqian Ye", "Kenny Moise", "Megan Coffee"], "abstract": "In the aftermath of the COVID-19 pandemic and amid accelerating climate change, emerging infectious diseases, particularly those arising from zoonotic spillover, remain a global threat. Mpox (caused by the monkeypox virus) is a notable example of a zoonotic infection that often goes undiagnosed, especially as its rash progresses through stages, complicating detection across diverse populations with different presentations. In August 2024, the WHO Director-General declared the mpox outbreak a public health emergency of international concern for a second time. Despite the deployment of deep learning techniques for detecting diseases from skin lesion images, a robust and publicly accessible foundation model for mpox diagnosis is still lacking due to the unavailability of open-source mpox skin lesion images, multimodal clinical data, and specialized training pipelines. To address this gap, we propose MpoxVLM, a vision-language model (VLM) designed to detect mpox by analyzing both skin lesion images and patient clinical information. MpoxVLM integrates the CLIP visual encoder, an enhanced Vision Transformer (ViT) classifier for skin lesions, and LLaMA-2-7B models, pre-trained and fine-tuned on visual instruction-following question-answer pairs from our newly released mpox skin lesion dataset. Our work achieves 90.38% accuracy for mpox detection, offering a promising pathway to improve early diagnostic accuracy in combating mpox.", "sections": [{"title": "1. Introduction", "content": "Mpox is a zoonotic disease caused by the orthopoxvirus monkeypox virus (MPXV) (Mitj\u00e0 et al., 2023a,b; Siegrist and Sassine, 2023; Laurenson-Schafer et al., 2023; Lu et al., 2023) that has affected over 99,518 individuals in an outbreak across 122 countries as of Sep 7, 2024. In the USA, there are 32,063 mpox cases with 58 associated deaths. Its spread across all inhabited continents has demonstrated sustained and prolonged human-to-human transmission, which was not previously recognized before 2022. The World Health Organization (WHO) declared its global spread a Public Health Emergency of International Concern (PHEIC) in 2022. Undetected infections delayed isolation and helped further transmission (Bragazzi et al., 2022). In July 2024, more than 100 laboratory-confirmed cases of clade 1b mpox were reported in\nbased models developed for skin lesion analysis are trained on small, imbalanced dermatological datasets, often of low quality, including images of monkeys, drawings, and microscopic virus images (Islam et al., 2022).\nAfter the blooming of large language models (LLMs) and vision-language models (VLMs) (Omiye et al., 2024), it becomes a wide trend to develop foundation models for different medical domains (Li et al., 2023b; Huix et al., 2024). Recent studies showed that VLMs finetuned with large-scale, broad-coverage biomedical figure-caption and visual question-answer datasets can effectively learn semantic characteristics via visual instruction tuning (Liu et al., 2024; Li et al., 2024). Thus, connecting visual representations with multimodal text information from biomedical language models becomes increasingly critical to adapting foundation models for medical image classification, particularly in the challenging setting of multimodal data deficiency like mpox (Cheng et al., 2023) and other skin lesion imaging tasks (Zhou and Gao, 2023).\nIn our study, we develop a VLM-based mpox diagnosis algorithm, MpoxVLM, designed specifically for the identification of mpox virus skin lesions in photographic skin lesion images together with easily accessible clinical information. Our algorithm's evaluation across our newly collected mpox dataset with multimodal information aims to ensure its effectiveness for individuals of different skin types, genders, ages, and lesion sites, thus facilitating timely and accurate diagnosis of mpox infections. The contributions are summarized as follows:\n1. We propose the first Vision Language Model (VLM) framework MpoxVLM for mpox detection and its training pipeline. This is the first foundation model diagnosing skin lesions from mpox virus infection.\n2. We collected a new multimodal mpox diagnosis dataset from publicly available information including skin lesion images and clinical record from 2,914 samples.\n3. The experiment results on our newly collected dataset show that the proposed MpoxVLM achieves the best performance results than the previous state-of-the-art methods."}, {"title": "2. Related Works", "content": "Mpox History and How it spread. Historically, the mpox was predominantly in children exposed to host animals, in outbreaks in Central and West Africa since the 1970s and later due to imported pets in the Midwestern United States in 2003, but the outbreak in Nigeria in 2017 and the later global outbreak since 2022 has been predominantly in adults (Breman et al., 1980; Zinnah et al., 2024; Luna et al., 2022; Guarner et al., 2022). The role of sexual networks was under-appreciated until 2022 when transmission was largely among men who have sex with men (MSM) (Allan-Blitz and Klausner, 2024; Pekar et al., 2024). Given the changing epidemiology and the associated stigma and denial, cases, even in a known outbreak, were often missed; in one study, only 23% of healthcare workers used appropriate personal protective equipment for all encounters with patients later confirmed to have mpox (Marshall et al., 2022). Mpox is often mistaken for another infection, such as sexually transmitted infections and viral exanthems, but also less common autoimmune and infectious causes. The lesions develop from unremarkable pimples over days to weeks into pathognomonic, umbilicated pustular lesions, but are infectious throughout and transmission often occurs before diagnosis is suspected. Associated symptoms (fever, sore throat, lymphadenopathy, and pain) vary between patients and resemble other diseases, so clinical suspicion usually focuses on the rash. Given a missed infection can lead to the further community - or less commonly, healthcare clinic transmission it is important to have a screening test and clinical support for clinicians with little experience diagnosing mpox. Thus, it is very meaningful to develop and deploy computer-aided diagnosis solutions for those countries influenced by mpox (Bleichrodt et al., 2024).\nMpox Diagnosis with Deep Learning. Deep learning has shown promise in classifying skin lesions in dermatology (Barata et al., 2019; Liu et al., 2020; Gr\u00f6ger et al., 2023). For mpox, Convolutional Neural Networks (CNNs) have demonstrated effectiveness in identifying disease through the analysis of skin lesion images (Thieme et al., 2023). Initial studies employed CNN models such as MobileNet (Jaradat et al., 2023; Altun et al., 2023), VGG Net (Ahsan et al., 2022a) that were pre-trained using the ImageNet database, and subsequently fine-tuned with either publicly or privately sourced mpox skin lesion image datasets. Sitaula et al. (Sitaula and Shahi, 2022) further pro-posed an ensemble learning method to combine Xception and DenseNet to predict mpox. Bala et al. (Bala et al., 2023) amassed a dataset comprising 770 skin lesion images, including 279 mpox cases, and introduced MonkeyNet, a model that leverages the architecture of DenseNet alongside multiple data augmentation techniques. Subsequent research has made the Mpox Skin Lesion Dataset (MSLDv2.0) (Ali et al., 2023) publicly accessible, which includes 284 Mpox image samples. However, these datasets are still too small to train reliable models.\nVision-Language Models (VLMs). In computer vision, the exploration of VLMs for handling tasks involving multiple modes of data has developed very fast. This interest has sparked new developments in multi-modal large language models (MLLMs), leading to the creation of new innovative tools like GPT-4V (Wu et al., 2023) and Gemini (Team et al., 2023). A key strategy to embed visual information into language models involves the fine-tuning or instruction tuning of large-scale foundational VLMs, such as Flamingo (Alayrac et al., 2022) and MiniGPT4 (Zhu et al., 2023). This technique has been further refined by recent developments like visual prompt tuning (Jia et al., 2022), LLaVA (Liu et al., 2024; Li et al., 2024), InternVL Chen et al. (2024), LLaMA-Adapter (Zhang et al., 2023), and BLIP-2(Li et al., 2023a), which employ instruction-following LLMs trained on VQA datasets specifically designed for image instruction tuning. The effectiveness of instruction tuning in improving the performance of VLMs on multi-modal vision-language tasks has been demonstrated, highlighting its potential to significantly advance the field."}, {"title": "3. Methodology", "content": "Overall Architecture. Fig 4 illustrates the main workflow of the proposed MpoxVLM framework. It included two encoders and a LLM decoder. The first encoder is a Contrastive Language-Image Pretraining (CLIP) visual encoder and it is frozen during our model training. The second encoder is a Vision Transformer (ViT) classifier encoder self-supervised learning pretrained with our proposed mpox dataset in using masked autoencoder (He et al., 2022). The LLM decoder used in our framework is LLaMA-2-7B Touvron et al. (2023) with 7 billion parameters. Each encoder is connected with the LLM decoder by a two-layer multilayer perceptron (MLP) module as\nthe alignment between visual features and language that is specialized in skin lesion image analysis. This encoder focuses on deriving high-level classification insights from the visual data. The output from this visual encoder, particularly the classification token, is directed through another trainable projection alignment module. The purpose of this alignment module is to project the classification token accurately into the LLM's text embedding space. These dual encoders are important for translating complex visual features into a format that the LLM can seamlessly process and interpret, enhancing the overall visual understanding capability of the MpoxVLM.\nInspired by LLaVA (Liu et al., 2023) and MiniGPT-4 (Zhu et al., 2023), our framework aims to integrate the visual features of skin lesion images and the clinical records of patients into a unified representation, leveraging the LLM to facilitate the diagnosis of mpox. The input of the task comprises a skin lesion image, denoted as Xv, alongside context-specific prior knowledge, represented as Xc.\nTo train the MpoxVLM, we also design question-answer pair from our proposed dataset, (Xq, Xo, Y), to guide the model's learning. The aim of using question-answer pairs as input and output structures is to streamline the integration of visual and textual data. Here, Xq poses a critical question based on the evaluation of the skin lesion image: \"After reviewing this skin lesion image, do you think the patient has mpox?\". Xo is a group of options denoting mpox or non-mpox. In our original dataset, Xo can be multiple choices including > 30 common skin diseases. In order to let model only focus on the diagnosis of mpox, we set the number of options to two in the training phase. The task of MpoxVLM and other baseline models is to predict an answer \u0176 from the options, which represents the model's assessment of whether the patient is likely to have mpox.\nThe proposed MpoxVLM Fe can be formulated as:\nY = Fe(Xv, Xc, Xq, Xo) \\tag{1}\nIn the following subsections, we will introduce the structure of MpoxVLM and our training procedure.\n3.1. Model Design\nVisual Encoder. Different to previous VLM solutions in medical imaging such as Med-LLaVA, the MpoxVLM framework incorporates dual encoders to process and analyze input image data effectively. The first encoder fCLIP is a visual-language encoder, specifically a freezed pre-trained CLIP visual encoder. Within the MpoxVLM framework, this encoder's role is to extract general visual features from the input mpox image, enabling a detailed visual-language representation of the skin lesion image. The extracted features are then passed through a trainable projection alignment module (a 2-layer MLP). This alignment module is crucial for mapping the visual tokens into text embedding space of the LLM. The second encoder fv in the MpoxVLM system is a visual encoder, built upon a pre-trained vision transformer (ViT) classifier\nVisual Instruction Tuning for MpoxVLM. To finetune the LLM within the MpoxVLM framework, our approach aims to synergistically harness the strengths of the pretrained LLM (LLaMA-2) along with the integrated dual visual encoders. For input skin lesion image Xv, we use the tokenizer from CLIP visual encoder (ViT-L/14-336) to embed the image into tokens. The visual feature is extracted from CLIP's visual encoder and then the adapter layer to map image features into the LLM's word embedding space:\nZCLIP = WCLIP \\cdot fCLIP (Tokenizer(Xv)), \\tag{2}\nZCLIP \u2208 \\mathbb{R}^{d_h \\times k},\nwhere fCLIP is the CLIP encoder. WCLIP is the weight of the linear projection layer. dh is the dimension of the LLM embedding. k is the number of visual tokens.\nIn the classification path, the input skin lesion image Xv is tokenized by the same CLIP's patch embedding layer and then extracted visual tokens by a pretrained ViT for skin lesion image classification. The classification token of the last layer is then fed to an adapter layer to map the classification features into the LLM's word embedding space.\nZv = Wv \\cdot [fv(Tokenizer(Xv))]CLS, \\tag{3}\nZv \u2208 \\mathbb{R}^{d_h \\times 1},\nwhere fv is the ViT encoder for classification. Wv is the weight of the linear projection layer. dh is the dimension of the LLM embedding. CLS means the classification token from the ViT's output.\nFor a sequence of length L, the autoregressive encoder in the LLM for generation mpox diagnosis answer is as follows:"}, {"title": "3.2. Training Pipeline", "content": "p(\\hat{Y}|ZCLIP, Zv, Xc, Xq) \\\\\n= \\prod_{i=1}^L Po(y_i|(ZCLIP \\oplus Zv), Xc, Xq, Xo, \\hat{Y} _{<i}) \\tag{4}\n, where Xq is all of the question tokens (the whole question). Xo is the option tokens. \u2295 is the concatenation operation for latent features. Y<i is all answer tokens before yi.\n\\hat{Y} = fLLM(ZCLIP, ZV, Xc, Xq, Xo) \\tag{5}\n, where fLLM is the LLM (LLaMA-2-7B).\nFirstly, we pretrained a ViT encoder (ViT-L-14-336) with the proposed mpox skin lesion dataset. Then, we build the MpoxVLM model with the pretrained ViT encoder as the classification visual encoder and the CLIP encoder (clip-vit-large-patch14) (Radford et al., 2021) as the visual-language encoder. Then, in the following experiment, we freeze the weights of both the CLIP visual encoder and classification visual encoder. The LLM used in our task is LLaMA-2 7B (Touvron et al., 2023). To train the two linear adaptors, we keep"}, {"title": "4. Experiments and Results", "content": "4.1. New Mpox Dataset and Demographic Analysis.\nPrior to our work, there are some mpox skin lesion image datasets available (Ahsan et al., 2022b; Jaradat et al., 2023; Bala et al., 2023; Ali et al., 2023). However, most of the datasets are relatively small and can not be used to train large models. To solve this issue, we proposed a new mpox dataset including both mpox skin lesion image and multimodal clinical information such as patients' medical history. The comparison of different mpox datasets is presented in Table 1. All of the data in our new dataset is collected and curated by doctors from public sources (medical and news journals, public health websites, and social media). It comprises 1,057 Mpox-positive samples and 1,857 samples of other diseases, totaling 2,914 samples. We have meticulously cropped each image to remove surrounding clothing or backgrounds and to\n4.2. Evaluation Metrics.\nTo evaluate the performance of our model, we use several standard metrics: Accuracy: measures the proportion of correctly predicted instances out of all predictions. Precision: known as positive predictive value. It evaluates the proportion of true positive predictions to the total number of positive predictions. It is useful for understanding the reliability of\npositive classifications made by the model. Recall: known as sensitivity, it assesses the proportion of actual positives correctly identified by the model, which is crucial in medicine for designing a screening test. F1 Score: the harmonic mean of precision and recall. It considers both false positives and false negatives. AUROC: The Area Under the Receiver Operating Characteristic curve represents the likelihood of the model distinguishing between classes. A higher AU-ROC score indicates better model performance.\n4.3. Training Details.\nAs most prior papers did not provide open-sourced code, therefore, we reproduce their methods carefully and trained them on our dataset. We adopt a dataset split of 5:1:1 (2000:398:398) for training, validation, and testing set, respectively. The split was done randomly based on patients' ID. The ratio of positive (mpox) and negative (non-mpox) is 4:7 in the test set. The training setting for all baselines is the same. To preprocess the images, we also center-crop and resize the input images to 336 \u00d7 336 resolution. The AdamW optimizer (Loshchilov and Hutter, 2017) with a weight decay set at 0.01 is also used to train all baselines and"}, {"title": "4.4. Compare with Previous Methods.", "content": "MpoxVLM. Additionally, we employ a cosine annealing learning rate scheduler (Loshchilov and Hutter, 2016), with the base learning rate set at 5 \u00d7 10-5. All baseline models are trained for 100 epochs and use early stops. The MpoxVLM is finetuned for 10,000 steps on 2 NVIDIA RTX 4090 GPUs using LORA (Hu et al., 2021), with each GPU handling a batch size of 2. All codes and dataset details will be available upon acceptance.\nThis section provides a comparative analysis of our method against existing approaches in the field. Previous studies (Bala et al., 2023; Thieme et al., 2023; Ahsan et al., 2022a; Jaradat et al., 2023; Altun et al., 2023) have predominantly employed CNN-based single network architectures. Our method demonstrates a significant improvement in performance over these CNN-based approaches. Conversely, another line of research, exemplified by Sitaula et al. (Sitaula and Shahi, 2022), utilizes an ensemble of multiple networks. Different from these methods, our method adopts the idea that LLM can enhance the performance of ViT in classification tasks (Chu et al., 2024) and uniquely leverages visual-language features extracted with general domain to enhance specific domain knowledge in the mpox diagnosis area."}, {"title": "4.5. Compare with Multimodal LLMs.", "content": "As multimodal LLMs have been applied to various medical imaging tasks (Wu et al., 2023; Panagoulias et al., 2024) and are proved having basic understanding of mpox (Cheng et al., 2023), we also evaluated their ability to diagnose skin lesions caused by mpox virus infection. The input to these models included a system prompt: \u201cMpox is a zoonotic disease caused by the orthopoxvirus monkeypox virus. The example image is a patient with mpox. You are a helpful visual reasoning assistant that can distinguish mpox in new skin lesion images,\" along with an example mpox skin image. The experimental results, shown in Table 3, reveal that state-of-the-art multimodal LLMs, such as Claude 3 variants (Anthropic, 2024) and GPT-40 (Achiam et al., 2023), perform only marginally better than random guessing. Despite advancements, these models still struggle with fine-grained clinical tasks, suggesting that in their current unprimed state, they are not yet suitable for complex clinical applications. In contrast, the MpoxVLM framework achieves a significantly higher accuracy of 90.38%, underscoring"}, {"title": "4.6. Ablation Study.", "content": "To validate the effectiveness of our model design, we conduct a detailed analysis of the individual contributions of various modules within our framework. We observe that even by freezing the weight of the CLIP model and only fine-tuning the LLM (LLaMA-2 7B), the performance can achieve relatively high accuracy similar to the previous SOTA method (line 2). Another key observation is the significant impact of the classification visual encoder on our framework's performance. This is obvious in the marked improvement in the AUROC metric."}, {"title": "5. Discussion", "content": "Mpox exemplifies the domain gap issue expected in emerging diseases. Tools trained on initial, small datasets do not capture the diversity in a mature epidemic. Here an outbreak in a specific population spreads to a broader population requiring training of images of different lesion stages on a broad range of skin types and body parts. To ensure this, we use a significantly larger dataset than prior mpox computer vision approaches. This dataset encompasses a diverse demographic, mirroring those affected by mpox as the epidemiology shifted from children in West and Central Africa to the Midwestern United States, to adults in Nigeria, and more broadly in MSM globally, with people of color disproportionately affected. The current mpox outbreak with new variant clade Ib has been affecting both adults and children in Burundi and the DRC, but with lower case positivity rates in children tested, we need to ensure accuracy in diagnosis, given the higher mortality rates of mpox in young children, but also the risks of isolating a young child and of not treating other causes of rash and illness. Images also reflect the range of mpox stages; many available images reflect the more easily identified, later images (such as the crusting umbilicated pustules in Fig 1) but fail to include earlier, less recognizable stages such as papules, whose identification would reduce transmission. Comparator diseases reflect both demographics and the global disease landscape against which mpox is identified. In addition, the images focus on the lesion alone, to avoid spurious correlations with clothing, backdrops, or even image quality, which may hint at disease geographies.\nThe scarcity of mpox images in the existing literature underscores the neglect of a disease known for 50 years to be endemic in West and Central Africa. In 2023, at least 581 people, mostly young children, likely died from mpox, with the case-fatality ratio attaining 8% in highly affected areas (Organization, 2024). However, many past dermatologic tools have included few images of persons of color or, as seen here, do not describe the demographics of the dataset. Such a tool will also need to be practical and achieve high accuracy, here 90%, and a high recall (sensitivity) for screening, and precision (positive predictive value) to assist in diagnosis where laboratory capacity is limited. Given the expected rise in novel and emerging diseases due to climate change and increased global movement, such an approach leads the way for further tools to address novel diagnostic challenges (Baker et al., 2022)."}, {"title": "6. Conclusion", "content": "Mpox has been historically neglected - first in Central and West Africa - and then in vulnerable populations globally including the LGBTQ community and disproportionately among people of color. The lack of attention to mpox has resulted in gaps in research and resources, particularly in dermatologic imaging and diagnosis. This paper outlined the first application of a vision-language model specifically tailored for dermatologic mpox images, alongside the largest fine-grained diagnostic dataset/benchmark for skin lesions caused by mpox. Our framework integrates clinical information such as mpox stages, affected body parts, gender, age, and Fitzpatrick skin types, making it the most comprehensive dataset for both AI researchers and clinicians. By developing this framework, we aim to provide a critical tool that supports clinicians in diagnosing mpox and, in the future, other emerging infectious diseases from skin lesion images. In future work, we aim to publish larger datasets containing more samples for each class and design the vision foundation model for digital dermatology."}]}