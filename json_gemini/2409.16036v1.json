{"title": "Grounded Computation & Consciousness", "authors": ["Ryan Williams"], "abstract": "Computational modeling is a critical tool for understanding consciousness, but is\nit enough on its own? This paper discusses the necessity for an ontological basis of\nconsciousness, and introduces a formal framework for grounding computational de-\nscriptions into an ontological substrate. Utilizing this technique, a method is demon-\nstrated for estimating the difference in qualitative experience between two systems.\nThis framework has wide applicability to computational theories of consciousness.", "sections": [{"title": "Computational Accounts of Consciousness", "content": "The common feature of all computational accounts of consciousness is what I call The\nComputational Thesis:\nThe Computational Thesis: Any system that can be interpreted as being\ndescribed by algorithm X is conscious.\nThe specific algorithm differs from theory to theory, with some overlap, and is in most cases\nonly loosely specified. But the key feature is that they give an abstract description of a\nsystem, to varying degrees of specificity, without grounding in any other ontology. The\nonly criteria necessary is that the system must be able to be interpreted as following the\nprescribed computation, and the conclusion regarding the consciousness or unconsciousness\nof the system follows.\nBelow is a quick survey of existing computational accounts of consciousness. Although my\nperspective is not convinced of these theories as full accounts of conscious pheonomena, I\nbelieve there are key lessons to learn from each of these systems:\n1. G\u00f6del, Escher, Bach (GEB): In the his classic book\u00b9, Douglas Hofstadter specu-\nlates that consciousness is an emergent property that arises in complex, self-referential\nsystems.\n2. Global Workspace Theory (GWT): In GWT5, it is proposed that the brain pro-\ncesses information in separate modules; when these modules broadcast and integrate\ntheir information in the \"global workspace\", the system becomes conscious.6"}, {"title": "The Ontology of Conscious Experience", "content": "The thing we can be most sure of is the existence of our own subjective experience.9 What-\never ways we may otherwise be deceived, the state of being deceived itself requires it. In this\npaper, I take it that our subjective experience exists, and that it holds qualitative properties\n(what philosophers often call qualia).\nI call them \"qualitative properties\" (as opposed to qualia) because it comes without addi-\ntional connotations. In absence of clear evidence, I prefer to avoid terms that might convey\nspecific assumptions about the nature of these qualitative phenomena. These qualitative\nproperties consist in the ineffable character of experiences themselves the smell of coffee\nor sensation of pain.\nRoughly speaking, the qualitative properties seem to hold close ties to relational and struc-\ntural properties, as they themselves may hold relations and can be structured. But their\nqualitative nature itself is not describable only in terms of relationships and structures.\nNo description will ever convey an experience that has no analogue in the reader's previous\nexperience. To illustrate, if I tell a normal-sighted person to imagine a color between two\nshades of blue they are already familiar with-say cobalt blue and royal blue they will have\nno trouble imagining the color.10 In their mind, it will be almost as if they were presently\nperceiving the new shade itself. Cued by the prompts \"cobalt\" and \"royal\", they draw upon\ntheir previous perceptions to produce new images. But a person blind from birth is unable\nnot only to imagine any shade of the color blue, but neither the concept of color itself, nor\neven of visual extension.\nWe might pause to ask the question: Why this limitation? What is it about a linguistic\ndescription that is unable to capture the qualitative properties of an experience?\nOne difficulty is due to the abstract nature of language. Words can refer indexically to\nconcepts already held. They can spell out new relationships between existing concepts, which\nnew relationships may then become novel concepts themselves. These concepts conjure\nmental images or sensations based on perceptions we associate with them. But they cannot\ncreate new qualitative categories of experience that are not already available for the mind\nto manipulate.\nQualitative experiential properties are ontological in nature. This is important: Qualitative\nproperties are fixed to the systems they are embodied by. Any qualitative property is available\nonly to the thing which it is."}, {"title": "The Epistemology of Conscious Experience", "content": "My experiences are my own. I can never share your exact experiences without being you.\nThough I may sometimes be capable of mimicking some of your relevant state within my-\nself. We can do this imperfectly by manipulating our experience, say by looking at the\nsame painting. We can induce similar physical changes to the systems which embody our\nexperience themselves (most saliently our brains), for instance by sharing in the same psy-\nchoactive substances. We can also communicate linguistic descriptions about items in our\ncommon experience, which are then reconsituted in the mind of the receiver, as we have\nbriefly discussed."}, {"title": "The Limitations of Abstractions", "content": "Computations are abstractions. They express patterns of representations and transforma-\ntions independent of their implementation within some substrate. Abstract and functional\nthinking are critical to our ability to operate usefully within the world. They allow us to\npartition out representations from the world and manipulate them in ways that are useful\nto us, without needing to grasp an intractable set of details.\nThe essential question is this:\nDoes the substantive implementation of a system matter with respect to conscious\nexperience? Or can you dispose of the implementation, choose an arbitrary level\nof description of a system, and give a full ontological account of conscious phe-\nnomena without reference to its substrate?\nIn my mind, the evidence we have points firmly in the direction that the implementation\nmatters. We can see at least two fundamental limitations of any abstract description,\nincluding computational, functional, formal, and linguistic descriptions.\nFirst, any abstract description of a system is devoid of qualitative properties. It may ref-\nerence qualitative properties, but we have already examined that those references can only\nserve to spur an existing sytem to reconsistitute qualitiative properties that are already\navailable to it. A description is not the same as the thing described. Some specified compu-\ntation, an algorithm X, is a description of the systems which implement it. It's important\nthat we don't conflate between the map and the territory.\nSecond, to manifest the qualitative properties of a system, it appears to be a requirement\nthat one must be that system (or at least mimic its substance and structure closely). But\nconcrete existence is inherently unavailable to abstractions, which by their nature, discard\ntheir implementation. Even if two concrete systems have identical substance and structure,\nthey still implement two separate qualitative experiences. These properties are not abstract,\nby the very nature of what it means to be abstract.\nWe will see in the following sections some additional formal and conceptual challenges of\ndescribing concrete systems computationally. The purpose of outlining these challenges is\nso that we can approach them with the goal to meet them.\nOne of the key challenges is how we can determine which computations are valid descriptions\nof which systems. Multiple functional descriptions can be provided for any concrete system,\nand multiple concrete implementations can be provided for any functional description.\nThe sections below will allow us to build a framework for expressing these challenges. We\nwill put the same framework to use later on to make progress in overcoming them."}, {"title": "Describing Systems Computationally", "content": "Before we proceed further, we will profit to explore what it means to describe a system\ncomputationally. We consider the set of systems S and the set of computational descriptions\nC. System $S \\in S$ implements a computational description $C \\in C$ if and only if there exist\nmappings $\\phi$ and $\\gamma$ such that:\n1. $\\phi : S \\rightarrow \\{s_0,s_1,...\\}$ is a mapping from S to a discrete set of representations\n$\\{s_0, s_1,...\\}$. It can be useful to denote $\\phi(S) \\subseteq \\{s_0, s_1,...\\}$ as the subset of represen-\ntations for all valid states of the ontological system S. The states $\\{c_0, c_1,...\\}$ of C\nare given within its description.\n2. $\\gamma: \\{s_0, s_1,...\\} \\rightarrow \\{c_0, c_1, ...\\}$ is a structure-preserving map from the representations\nof S to the states of C.\n3. For every possible state $c_i$ of C, there exists a corresponding representation $s_i$ of $\\phi(S)$\nsuch that $\\gamma(s_i) = c_i$.\n4. For every state transition $(c_i \\rightarrow c_j)$ in C, if $c_i = \\gamma(s_i)$, then there exists a valid\ntransition between representations $(s_i \\rightarrow s_j)$ such that $c_j = \\gamma(s_j)$.\n5. $\\gamma$ is compatible with $\\phi$, meaning that for any two systems S and S', if $\\phi(S) = \\phi(S')$, then $[\\gamma \\circ \\phi](S) = [\\gamma \\circ \\phi](S')$.\n6. Pragmatically, the mappings $\\phi$ and $\\gamma$ must be robust under small perturbations of S\nin some sense we'll leave only intuitively defined, ensuring that the implementation is\nstable and not overly sensitive to minor fluctuations.\nIn this formulation, $\\phi$ can be seen as the system's structure, which produces a state represen-\ntation $s = \\phi(S)$. The ability for the system to assume these representations are what make\nthe system amenable to computation, whose transitions $(s_i \\rightarrow s_j)$ are the representation's\ndynamics.\n$\\gamma$ is the system's semiotics, the assignment of a symbolic state $c = \\gamma(s)$ to the representation\nof the system. The symbolic states evolve according to state transitions $(c_i \\rightarrow c_j)$ which\nare its syntax. The evolution of the system's dynamics must match the computational\ndescription's syntax in order for S to implement C via $[\\gamma \\circ \\phi]$.\nThe set of state transitions in C can be represented as a matrix $T_C$ where each $t_{ij} \\in T_C$ is\n1 when $(c_i \\rightarrow c_j)$ is a valid transition and 0 otherwise. A corresponding matrix $T_{\\phi(S)}$ exists\nfor the representations $\\phi(S)$."}, {"title": "Computational Ambiguity", "content": "$\\Theta: S \\rightarrow 2^C$ is a map of interpretations from each system $S \\in S$ into its set of possible\ncomputational descriptions, a subset of C. Each $\\Theta(S)$ can be seen as a composition $[\\gamma \\circ \\phi]$\nof some chosen $\\gamma$ and $\\phi$, which must meet the compatibility condition described above.\nConversely, $\\Omega: C \\rightarrow 2^S$ is a map of implementations from each computational description\n$C \\in C$ into its set of possible realized systems, a subset of S. Each $\\Omega(C)$ can be seen as a\ncomposition $[\\phi^{-1} \\circ \\gamma^{-1}]$ (note the reversal of order), the inverse relations of some chosen $\\gamma$\nand $\\phi$. Note that each $\\phi^{-1}$ and $\\gamma^{-1}$ may be nondeterministic mappings (multivalued inverse\nfunctions) which may assign multiple possible system states to a given computational state.\nThe nature of an abstraction is to be ambiguous.11 Abstractions generalize over concretions\nto extract meaningful common structures, relationships, and behaviors between them.\nThere are two key distinctions we will observe in the types of ambiguities we find between\nsystems and computational descriptions.\nThe first is a distinction that relates to the direction of the ambiguity. We refer to the\ncase that $\\Theta(S)$ of some system S has multiple computational descriptions $C_1, C_2,...$ as\nimplementational ambiguity. We refer to the alternate case, where $\\Omega(C)$ of some computa-\ntional description C may be implemented as multiple systems $S_1, S_2, ...$, as interpretational\nambiguity.\nThe second distinction is between structural and semiotic ambiguity, which result from the\nmaps $\\phi$ and $\\gamma$, respectively."}, {"title": "Implementational Ambiguity", "content": "Implementational ambiguity occurs between a computational description C and multiple\nconcrete implementations $S_1, S_2,....$ This is commonplace, and can be seen in our ability\nto run the same computer programs on multiple machines. Implementational ambiguity\nallows us to describe disparate physical systems in terms of the same functional account,\nand therefore highlight functional similarities between those systems.\nImplementational ambiguity is not problematic for a computational ontology of conscious\nexperience because of the ambiguity on its own. However, when the makeup of those systems\nis vastly different, it could be a vulnerable assumption that such disparate systems share\nqualitative properties without sharing similarity in substance or structure."}, {"title": "Interpretational Ambiguity", "content": "Interpretational ambiguity runs in the opposite direction, from a physical implementation\nto multiple computational descriptions. But if there are multiple computations that serve as\ndescriptions of the same concrete system, how do we know which is the correct one? We'll\nconsider questions surrounding the mapping between systems and computational descrip-\ntions below."}, {"title": "Semiotic Ambiguity", "content": "Semiotic ambiguity occurs in the relation specified by $\\gamma$, the mapping between a system\nstate $s = \\phi(S)$ and its corresponding computational state c. There is a set of relations V\nwhere each $\\gamma \\in V$ is a distinct mapping between structural representations and symbolic\nstates. Our choice between potentially multiple possible $\\gamma \\in V$ produces the ambiguity.\nLet's illustrate the ambiguity with a simple example that frequently occurs in your own\ncomputer hardware. In the example provides below we take our system S, with each state\n$\\phi(S)$ consisting of 6 bytes $w_0,..., w_5$, and each byte containing 7 bits $x_0,...,x_6$. We will\nshow that a single initial state $s_0$ and transition $\\delta$ to the resulting state $s_1 = \\delta(s_0)$ of this\nsystem can be mapped to a computational description in no less than four intuitive (and\nlargely realistic) ways."}, {"title": "Pixel Semiotics", "content": "The first interpretation of the system will be a visual one, with each bit $x_i$ representing a\npixel to be drawn to the screen, and each byte $w_i$ representing the next row. Our example\nshows a pattern of pixels that could be interpreted as the letter 'N'. In this case, the\ntransition $\\delta$ represents an operation to draw a vertical line in the far left column, making\nthe letter bold.\nWe might express this transformation in some computational description C as\n$N \\xrightarrow{\\delta_{bold}} N$,\nwhere N and N are states of C and $\\xrightarrow{\\delta_{bold}}$ is $\\delta$, the state transition."}, {"title": "Alphabetic Semiotics", "content": "We may also take an alphabetic interpretation, in which each byte $w_i$ represents a letter of\nthe English alphabet. In this example, we're used the ASCII encoding, which conveniently\nis defined so that the number that encodes any uppercase letter plus 32 becomes the ASCII\nencoding of the same letter in lowercase. In other words, the application of $\\delta$ makes the\nletters lowercase.\n$AQIECA \\xrightarrow{lowercase} aquieca$\nThere is a further ambiguity in this example in that if we used a different encoding, each\nnumber would represent a different character, and the addition of 32 to each encoded char-\nacter is not likely to have any neat semiotic interpretation."}, {"title": "Numeric Semiotics", "content": "We can also interpret the system numerically, with each byte $w_i$ representing a binary\ninteger. In this case, the application of $\\delta$ increments each number in the sequence by 32."}, {"title": "Bitmask Semiotics", "content": "A bitmask takes a binary representation at face value, i.e. as a sequence of independent\nbits. They are often used to represent a series of boolean (i.e. true=1/false=0) variables\nwithin a single byte. In our example, each $x_i$ would represent some true/false value. The\ncomputation expressed by $\\delta$ can then be interpreted as a toggling on of the $x_6$ bit in each\nbyte. In this example we'll examine the operation $\\delta$ on $w_0$, which can be seen in boolean\ncontext as\n$FTFFFFT \\xrightarrow{x_6=T} TTFFFFT$,"}, {"title": "Structural Ambiguity", "content": "Structural ambiguity is found in the mapping $\\phi(S)$ which \u201cslices up\u201d a system S into\ndiscrete representations and state transitions. The property of a system having a set of\n$U = \\{\\phi_a, \\phi_b, ...\\}$ where some $\\phi \\in U$ are useful implementations of computational descrip-\ntions is what makes them amenable as computing machines.\nFor instance, the implementation of a computer's memory as an array of binary (off or on)\ndigits is itself an abstraction over the actual physical system. In an actual computer S, each\nbit in memory is represented by an electric potential. $\\phi$ maps the system S into a useful\nstructural representation via $\\phi(S)$.\nTo illustrate, let's say that 0V represents the 0 or \"off\" state and 1V represents the 1 or\n\"on\" state. At any point in time, the likelihood of the component that represents a given\nbit actually being equal to 0V or 1V is extremely small. Instead, the computer is engineered\nto behave as \u201con\u201d if the voltage is above a certain threshold $\\tau$ (say $\\tau = .5V$) and \u201coff\u201d if the\nvoltage falls below the threshold. Given our computer S, we can isolate some component x\nas a subsystem.\n$\\phi(x < \\tau) = 0 and \\phi(x > \\tau) = 1$\nWhen $\\tau = .5V$, the subsystem will have an identical interpretation if\n$\\phi(x = .51V) = \\phi(x = .99V) = 1$.\nThis example of implementational ambiguity can seem fairly mild in the case of systems\nwith similar architecture, but result in comparisons of vastly different systems in actual\npractice (comparisons between engineered and biogical systems, for instance).\nEven the binary interpretation we require for the computer to be useful needs to be engi-\nneered onto and read out from the physical system. Without changing the engineering of\nthe memory or logical units of a machine (though to be practical you'd need to update the\ninput-output systems), you could alter the threshold to some arbitrary other voltage (say\n$\\tau = .25V$) and have a vastly different interpretation of the computations that have occured\nin the system. Even further, without making any changes to the the threshold, you could\nsimply reverse the interpretation as $\\phi'(x) = \\neg \\phi(x)$, with untold havoc in the results.\n$\\phi'(x < \\tau) = 1 and \\phi'(x > \\tau) = 0$"}, {"title": "Ontological Inconsistency", "content": "While we've already viewed the results of computational abiguity in more relatable settings\nin the examples above, this section will take the concept further.\nWe will assume that any closed physical system can be approximated to arbitrary accuracy\nas a finite automata. This assumption is evidenced by various known methods for approx-\nimating continuous phenomena with discrete methods such as Taylor's Theorem, Fourier\nSeries, and the Shannon-Nyquist Sampling Theorem,12 as well as techniques of art in com-\nputational physics. Given that assumption, we can demonstrate that:\n1. There always exist multiple computational descriptions consistent with any closed\nphysical system; and that\n2. A pair of those descriptions will be mutually inconsistent.\nWe will consider a physical system described by a finite automata Q, where the state of\nthe machine consists of a sequence of symbols s of finite length N, where the i-th symbol,\ni < N, in s is represented $s_i \\in \\{0,1\\})$ and a set of transition rules R, where each r$\\in$ R is a\nrule\n$r(s) = s'$,\nwhich determines a subsequent state s' at time t +1 according to the current states at time\nt. Each r is composed of a series of transitions of individual symbols within the state\n$r(s_i) = s_i$.\nAny such system Q is consistent with a more general system Q' where each r(s) is defined\n$r'(s_{i+kN}) = r(s_i)$,\nwhere k is an integer k > 0 and i < N.13 This generalizes r outside the range 0 < i < N.\nThis property guarantees that all such finite systems will be interpretationally and semiot-\nically ambiguous, since both Q and its extended Q' are correct models of the system.\nHowever, any such system Q is also consistent with a system Q\" where each r\"(s;) is defined\n$r''(s_j)=\\begin{cases}\nr'(s_j) & \\text{if } j < N\\\\\n1 & \\text{if } j > N \\text{ and } r'(s_j) = 0\\\\\n0 & \\text{if } j > N \\text{ and } r'(s_j) = 1.\\\\\n\\end{cases}$\nNote that this is a form of diagonalization, where r\"(sj) just produces the opposite of r'(sj)\nwhen j > N, thus ensuring its inconsistency. This argument bears no accidental relation\nto Cantor's diagonal argument, Russell's paradox, G\u00f6del's incompleteness theorems, and\nTuring's halting problem.14 It also is due some credit to an argument made by Wittgenstein\nregarding arithmetical sequences.15\nWhile Q is consistent with both Q' and Q\", Q' and Q\" are mutually inconsistent with each\nother when j > N Expressed formally,\n$Q' \\supseteq Q \\text{ and } Q''=Q$,\nbut\n$Q' = \\neg Q''\\text{ and }Q''= \\neg Q'.$"}, {"title": "Mapping Computational Systems", "content": "In this section, we'll examine some of the questions that come about when trying to map\nsystems to computational descriptions, and how we might address them.\nIf there are multiple valid but inconsistent computational descriptions\nof any system, how do we determine which description of a system is\nthe correct one?\nWe might observe that there is no proposition to admit arbitrary computational descriptions\ninto our ontology; we are only examining systems for the presence of a specific computational\ndescription. We will denote this proposition $R : S \\times C \\rightarrow \\{0,1\\}$, where R(S, C) is 1 when\na system S implements the specific computational description C according to the criteria\nfrom the section Describing Systems Computationally, and 0 when it does not.\nDoes the proposition R require that R itself is computational?\nIf R(S, C) is taken as a statement that the ontology of consciousness is fully computational,\nthen it seems to require a computational procedure that will determine whether the system\nS matches the proposed computation C.\nEven the weaker epistemological position that R(S, C) can be known solely through com-\nputational means would require by definition that R(S, C) is computational.\nCan R(S,C) be computational, if its input S is a concrete system (as\nopposed to an abstract one)?\nComputation can only operate over abstract inputs to produce abstract outputs. If R(S, C)\nis supposed to be determined solely through computational means, we require an abstract\nrepresentation of the system $s \\phi(S)$ that is amenable to being interpreted computationally\nin the first place.\nThis process can't bootstrap itself; if you suppose that $\\phi$ itself is only a computation,\nthen S is already a computational state. We've only pushed back the question to some\n$S = \\phi'(S')$. Ultimately some final reference to the concrete (i.e. non-abstract) system $S^*$\nmust be reached. Any mapping $\\phi^*$ from the concrete system $S^*$ to a representation $s^*$\ncannot be solely computational.\nIf it's not computational, what is it?\n$\\phi(S)$ is a structural interpretation of the system S. It depends on an interpreting sys-\ntem I, an observer, which is capable of taking concrete percepts and producing abstract\nrepresentations. I is a concrete system.\nNote that this is not a statement about the Church-Turing-Deutsch principle, which states\nthat a universal computing device can simulate every physical process. The interpreting\nsystem I may be itself simulable as $\\phi(I)$, produced by an observer I', which interestingly\nmay be I itself, in which case I' = I.16 But the simulation $\\phi(I)$ cannot perform $\\phi$, as the\nsimulation is already abstract; it has no concrete input available to it."}, {"title": "Ontological Context", "content": "Doesn't that introduce a significant influence of arbitrary interpre-\ntation into our ontology, if the determination is dependent on an\nobserver?\nYes, I would say that any computational account of the ontology of consciousness does admit\ninterpretation into our ontology. Computational ontologies systems fix our choice of $\\gamma$ but\nleave $\\phi$ open.\nAn alternative, grounding computation to a specific ontology by also fixing $\\phi$, is explored in\nthe following sections. Note that grounding doesn't settle the choice of $\\phi$ epistemologically.\nThat's an empirical question that may not be fully decidable (though we will show methods\nfor chosing optimal choices of $\\phi$ based on different criteria). But the stipulation of having a\nfixed $\\phi$ does remove the ambiguity from our ontology.\nWe have already seen that R cannot be computational. But it's interesting to consider if\nit were. If R(S, C) were computational, it would require a representation $\\phi(S)$ to act on\nwhich was computational.\nSupposing that $\\phi$ were computational, it must be interpretationally ambiguous, as all finite\nautomata were shown to be above in the section Ontological Inconsistency. There will be\nmore than one $\\phi$ such that $\\phi(S) = s$, and that some of those $\\phi$ will be inconsistent with\neach other in other contexts.\nConsider what happens if we were to claim that there exists some computational procedure\n$\\mu(\\{\\phi_a, \\phi_b,...\\}) \\rightarrow \\phi$\nwhich will determine a unique $\\phi$, based on its computational properties alone (i.e. without\nreference to any substrate). Such a procedure cannot distinguish between two inconsistent\nmappings $\\phi_a$ and $\\phi_b$ which are isomorphic but whose symbols are ungrounded.\nRecall the structural ambiguity we can introduce simply by inverting the representation17\n$\\phi_a(S) = \\neg \\phi_b(S)$, i.e. $\\phi_a(S) = 0 \\text{ and } \\phi_b(S) = 1$,\nwhere 1 and 0 are arbitrary new ungrounded symbols. There is no way for u to distinguish com-\nputationally between $\\phi_a$ and $\\phi_b$, unless the inputs and outputs are grounded with reference\nto some context.\nAnd yet we know what kind of effect a reversal in representation could have on the operation\nof a system. An example was given above, in which we fixed each representation to a set of\nphysical facts about voltages, eliminating the ambiguity, as in\n$\\Phi_\\alpha(x < .5V) = 0 \\text{ and } \\Phi_\\alpha(x > .5V) = 1$\nwhile\n$\\phi_b(x < .5V) = 1 \\text{ and }\\phi_b(x > .5V) = 0$."}, {"title": "Grounded Computation", "content": "We could counter by claiming that the isomorphism between the systems allows them to be\nconsidered identically. But both $\\phi_a$ and $\\phi_b$ could in fact be used by different components a\nand b within the same system Q, in which case we'd need to differentiate between them.\n$\\phi(Q) = \\{\\phi_a(a), \\phi_b(b)\\}$\nFor instance, if we take a = b = .7V,\n$\\phi(Q) = \\{\\phi_a(.7V) = 1, \\phi_b(.7V) = 0\\}$.\nAt this point, we can appeal back to the same claim of isomorphism that within the system\nQ, $\\phi_a$ and $\\phi_b$ are distinct, but without the larger context, they are not. This seems like a\nreasonable statement from an epistemological perspective, but more difficult ontologically,\nwhere questions about context-dependence come into play. What would it mean for the\nintrinsic properties of a system to be context-dependent?\nIn any case, the question can be resolved straightforwardly with reference to an ontological\nsubstrate, as with the introduction of the inputs.7V, which create an invariance between\nthe systems and allows them to be compared.\nWe've now seen several lines of motivation for grounding our computations within some\nontological context. Most significant are the principled reasons we outlined in the first few\nsections of the paper, but we've seen that ontological context is also adept at removing\nambiguities which otherwise provoke questions.\nThere are two straightforward steps towards the removal of many ontological ambiguities\nthat arise in mapping substantial systems to computational descriptions. We can:\n1. Choose specific mappings $\\phi$ and $\\gamma$, instead of leaving their choice open; and we can\n2. Explicitly ground each representation: each $s = \\phi(S)$ must be identified with an\nontological entity or state.\nNote that this resolution in no way ensures any particular scheme's correctness; these are\nonly measures to remove ambiguity. We will also see that there is some ambiguity that is\nunavoidable in principle due to epistemological considerations (as opposed to ontological\nones). We'll demonstrate how the remaining ambiguity arises, and propose methods for\nmeasuring and minimizing it.\nBecause $[\\gamma \\circ \\phi]$ yields a single computational state c, no further consideration is needed\nto resolve computational ambiguities once $\\gamma$ and $\\phi$ are specifically chosen. However, the\ngrounding relation between states of $\\phi(S)$ and ontological states needs further explication.\nUltimately, we want to anchor our descriptions to the concrete system by identifying rep-\nresentations with the substantial states of the system. This process isn't new, and occurs\nimplicitly in many physical theories. The level the ontological identity will occur at will\ndepend on the representation of the ontological system $\\phi(S)$.\nWe want our states $\\phi(S)$ to follow natural representations of the ontological state of S.\nThis essentially amounts to a coarse-graining procedure in which $\\phi(S)$ can be interpreted\nas a set of equivalence classes over the ontological states of S. One method of determining"}, {"title": "Generalizing Our Model of Computation", "content": "Before we consider specific metrics, it makes sense to first generalize our model of compu-\ntation slightly. In the real world, there will always be epistemological uncertainties in our\nrepresentations and ontological stochasticity in our implementation of any idealized com-\nputation. We may also concern ourselves with inherently probablistic computations. \u03a4\u03bf\ncapture this uncertainty, we will generalize our transitions in both the computational and\nsubstantial systems to allow for probablistic transition between states.\nEach transition matrix $T_{\\phi(S)}$ and $T_C$ will take the form of a transition probability matrix\n(TPM) where each $T_{ij}$ is the probability that the system will transition from the i-th state\nto the j-th state. Accordingly, each probability p in the matrix must fall in the interval\n0 <p< 1 and must follow the normal rules for probabilties wherein each row sums to 1.\nThe following is an example TPM for a system X with only two states:\n$T_X =\\begin{bmatrix}\nP(s_0|s_0) & P(s_1 | s_0)\\\\\nP(s_0|s_1) & P(s_1|s_1)\\\\\n\\end{bmatrix} =\\begin{bmatrix}\n.1 & .9\\\\\n.2 & .8\\\\\n\\end{bmatrix}$\nThis system indicates that the 0 state has a 10% probability of remaining in the 0 state,\nand a 90% probability of transitioning to the 1 state. The 1 state has an 80% chance of\nremaining in the 1 state and a 20% chance of transitioning to the 0 state."}, {"title": "Divergence & Optimization", "content": "There are a few choices of divergence measures that seem to present themselves fairly nat-\nurally to me that I will outline below", "s_b$": "n$D_s [s_b||s_a", "s": "n$D_{T_s"}, ["s_b||s_a"], "D_s [s_b||s_a"], "can": "n$D_s [\\phi_b(S)||\\phi_a(S)] = \\sum\\limits_i P(s_a, s_b) D_{T_s} [s_b||s_a]$\nwhere $s_a = \\phi_a(S)_i"}