{"title": "Tighter Value-Function Approximations for POMDPs", "authors": ["Merlijn Krale", "Wietze Koops", "Sebastian Junges", "Thiago D. Sim\u00e3o", "Nils Jansen"], "abstract": "Solving partially observable Markov decision processes (POMDPS) typically requires reasoning about the values of exponentially many state beliefs. Towards practical performance, state-of-the-art solvers use value bounds to guide this reasoning. However, sound upper value bounds are often computationally expensive to compute, and there is a tradeoff between the tightness of such bounds and their computational cost. This paper introduces new and provably tighter upper value bounds than the commonly used fast informed bound. Our empirical evaluation shows that, despite their additional computational overhead, the new upper bounds accelerate state-of-the-art POMDP solvers on a wide range of benchmarks.", "sections": [{"title": "1 INTRODUCTION", "content": "Partially observable Markov decision processes (POMDPs) are a versatile modeling framework for stochastic environments where the decision maker (the agent) cannot fully observe the current state of its environment [26]. Finding optimal policies for POMDPs is generally undecidable [36]. Yet, in recent years, methods like POMCP [43], DESPOT [56], and AdaOPS [55] have been able to find policies for increasingly large POMDPs.\nAlthough such methods often provide a (statistical) lower bound on the value of the policy, they are typically unable to find an upper bound on the optimal value. Such further certification of the quality of a policy may be essential for safety-critical problems. For example, planning medical treatments [24], scheduling infrastructure maintenance [37, 38] or computing safe flight paths [50] require us not only to know how well our policy will perform, but also that we cannot (reasonably) do any better.\nSo-called \u03f5-optimal solvers such as SARSOP [31] and HSVI [47] compute both a policy and an upper bound. These algorithms make use of heuristic search to find good policies quickly. However, they often struggle to find upper bounds that are reasonably tight, since this requires reasoning over all possible policies.\nBoth HSVI and SARSOP use the fast informed bound, or FIB [23], to initialize their upper bound computations. Intuitively, FIB computes values in a simplified POMDP, where the agent fully observes the state of the environment with a delay of one time step. However, these bounds are often loose in practice, while tighter upper bounds could improve the performance of \u03f5-optimal solvers.\nWe contribute three different methods to obtain bounds that exhibit varying levels of tightness and computational overhead. We first introduce the tighter informed bound (TIB) as an alternative for FIB. Intuitively, TIB uses a delay of two time steps rather than one time step. TIB can be computed using value iteration, as employed by [8, 42], on all one-step beliefs, that is, beliefs the agent can have one time step after knowing the state. These precomputations are more expensive than for FIB, but allow to compute a bound for any belief at the same computational cost as FIB. However, we show that increasing the delay further would significantly increase these computational costs.\nCloser inspection of TIB shows that it expresses posterior beliefs of the agent as a convex combination of one-step beliefs. However, choosing different combinations may further tighten the bound. The optimized tighter informed bound (OTIB) uses the convex combinations that yield the tightest possible bound. However, finding this convex combination requires solving a linear program for each posterior belief in each iteration step, which is usually too expensive. Instead, the entropy-based tighter informed bound (ETIB) heuristically chooses a single combination for each posterior belief by maximizing the weighted entropy of the chosen one-step beliefs. This combination is reused for each iteration, thus greatly reducing computational cost.\nEmpirically, TIB and ETIB provide better bounds than FIB on a large range of benchmarks with reasonable computational cost. To test the practical relevance of our bounds, we adapt the offline state-of-the-art solver SARSOP [31] to use our upper bounds as initialization. With this alteration, SARSOP finds tighter optimality bounds more quickly on a wide range of benchmarks, which means the additional computational overhead of our bounds is compensated by a speedup in convergence. Moreover, this positive effect grows as the discount factor increases.\nContributions. To summarize, our main contributions are introducing three novel bounds for POMDPs, namely TIB, ETIB, and OTIB. These bounds both theoretically and empirically improve prior methods. Moreover, integrating these novel bounds with the state-of-the-art \u03f5-optimal solver SARSOP [31] leads to significant speedups and smaller optimality gaps."}, {"title": "2 PROBLEM SETTING", "content": "In this section, we provide a formal definition of our problem setting in order to formalize our problem statement. We first introduce some basic notation: \u25b3(X) denotes the set of probability distributions over a finite set X. Given a function F: X \u2192 \u25b3(Y) and elements x \u2208 X, y \u2208 Y, F(. | x) denotes the conditional probability distribution over Y given x, F(y | x) the probability of element y given x, and y~ F(x) an element y randomly sampled from F(x).\nPOMDPs. An (infinite-horizon, discounted) partially observable Markov decision process (POMDP) [26, 49] is defined as a tuple M = (S, A, T, O, O, R, \u03b3), with (S, A, T, R, \u03b3) an MDP [42] with a finite set of states S, a finite set of actions A, a transition function T: S \u00d7 A \u2192 \u25b3(S), a reward function R: S \u00d7 A \u2192 R, and a discount factor \u03b3 \u2208 (0, 1). Additionally, O is a finite set of observations and O: A \u00d7 S \u2192 \u0394(O) is the observation function.\nA POMDP models the interaction between a stochastic environment and an agent. Let b\u2080 \u2208 \u25b3(S) be the fixed initial distribution (aka initial belief). The initial state s\u2080 of the environment is sampled from b\u2080. At each time step t, the agent picks an action a\u209c \u2208 A. As a result, the environment transitions to a new state s\u209c\u208a\u2081 ~ T(. | s\u209c, a\u209c) and returns a reward r\u209c = R(s\u209c, a\u209c). However, unlike for MDPs, the agent does not observe the state s\u209c\u208a\u2081, but instead receives an observation o\u209c\u208a\u2081 ~ O(. | a\u209c, s\u209c\u208a\u2081). In general, agents make decisions based on their history (b\u2080, a\u2080, o\u2081, . . ., a\u209c, o\u209c\u208a\u2081). As shown by \u00c5str\u00f6m [4], this history can be summarized by a belief b\u209c \u2208 \u25b3(S). Therefore, we can assume that the agent chooses actions according to a (deterministic) belief-based policy \u03c0: \u25b3(S) \u2192 A. Given a policy \u03c0 and an initial belief b, we define the value as the expected discounted return over an infinite horizon:\n$E_{\u03c0}[\\sum_{t=0}^{\u221e} \u03b3^{t}r_{t} | s\u2080 ~ b]$\nThe agent aims to maximize the value for the initial belief b\u2080.\nProbabilities. We now introduce additional notation that will be used throughout this paper. For any belief b \u2208 \u25b3(S), let R(b, a) = \u2211\u209b\u2208\u209b b(s)R(s, a) be the expected reward of action a in belief b and let T(s' | b,a) = \u2211\u209b\u2208\u209b b(s)T(s' | s, a) be the probability of transitioning to state s' when taking action a in belief b.\nWe define a shorthand for four probabilities. Given a state s and an action a, the probability of transitioning to state s' and observing o is denoted by\n$Pr(s', o | s, a) = O(o | a, s')T(s' | s, a)$,\nwhile the probability of observing o is denoted by\n$Pr(o | s, a) = \\sum_{s' \\in S} Pr(s', o | s, a)$.\nGiven a belief b and action a, we denote the probability of transitioning to s' and observing o by\n$Pr(s', o | b, a) = \\sum_{s \\in S} [b(s) Pr(s', o | s, a)]$;\nwhile the probability of observing o is given by\n$Pr(o | b, a) = \\sum_{s' \\in S} Pr(s', o | b, a)$.\nBeliefs. We also define notation for specific beliefs. For any s \u2208 S, let the unit belief b\u209b be the belief such that b\u209b(s) = 1 (and hence b\u209b(s') = 0 for s' \u2260 s). Let B\u209b = {b\u209b | s \u2208 S} be the set of all unit beliefs. If Pr(o | b,a) > 0, b_{b,a,o} is the belief after taking action a and observing o from belief b, i.e.:\u00b9\n$b_{b,a,o} (s') = Pr(s' | b, a, o) = \\frac{\\sum_{s \\in S} b(s) Pr(s', o | s, a)}{Pr(o | b, a)}$        (1)\nFurther, we denote a one-step beliefs as b\u209b,\u2090,\u2092 = b_{b\u209b,a,o}, which denotes a belief reached from a unit belief b\u209b (i.e., a belief where the agent knows the state) in a single time step after executing a and observing o. We define B\u2081 as the (finite) set containing all one-step beliefs and the initial belief b\u2080, i.e.,\n$B\u2081 = {b\u209b,\u2090,\u2092 | s \u2208 S, a \u2208 A, o \u2208 O, Pr(o | s, a) > 0} \u222a {b\u2080}$.        (2)\nWe note that every reachable belief (except possibly b\u2080) can be written as a convex combination of one-step beliefs. Hence, all reachable beliefs can be written as a convex combination of beliefs in B\u2081.\nQ-values. Lastly, to reason about the decision-making process of an agent, we define the Q-value function Q: \u25b3(S) \u00d7 A \u2192 R as the value for a given belief-action pair. Let Q be the set of all functions Q: \u25b3(S) \u00d7 A \u2192 R. The Q-value function corresponding to an optimal policy can be given as the (unique) fixed point of the Bellman operator H_{POMDP}: Q \u2192 Q [48]:\n$H_{POMDP}Q(b, a) = R(b, a) + \u03b3 \\sum_{o \\in O} Pr(o | b, a) \\max_{a' \\in A} Q(b_{b,a,o}, a').$        (3)\nWith our problem setting defined, we formalize our problem statement as follows:\nPROBLEM STATEMENT. Find tractable methods of computing tight overapproximations (or bounds) of the Q-value function for POMDPs to improve the performance of \u03f5-optimal solvers."}, {"title": "3 PRIOR METHODS", "content": "In this section, we describe the baseline methods of finding upper bounds for POMDPs using the notation introduced in Sect. 2. We discuss the fast informed bound (FIB) [23], but define it using Q-functions. Then, we recall point set bounds [41] and show how FIB can be interpreted as a point set bound. Finally, we briefly review how upper bounds are used in state-of-the-art solver SARSOP.\nTo further tighten this bound, the fast informed bound (FIB) [23] assumes an agent fully observes the current and future states with a delay of 1 time step. More precisely, we define Q_{FIB}, the Q-value function for this bound, as follows:\n$H_{FIB}Q(b, a) = R(b, a) + \u03b3 \\sum_{o \\in O} \\max_{a' \\in A} \\sum_{s' \\in S} [Pr(o, s' | b, a)Q(b_{s'}, a')] $.       (5)\nRecall the GUESSING POMDP. Under the QMDP assumption, taking action w would fully reveal the agent's state. In that case, an agent can always guess correctly after taking action w, which yields an expected value of \u03b3. Similarly, under the FIB assumption, taking action w would fully reveal the agent's previous state. The probability of still being in this state after this action is 0.8. Thus, taking action w and guessing the revealed initial state yields an expected return of 0.8\u03b3. Both are strict overapproximations of the optimal value 0.5 of the POMDP, and both incorrectly give higher Q-values for action w than for x or y."}, {"title": "3.2 Point Set Bounds", "content": "To compute tighter approximations than FIB, we consider a general value bound that uses point sets [41]: sets of beliefs with known upper bounds. To make the connection with our own method more clear, we define them using our own (non-standard) notation. We start by defining a weight function as follows:\nDEFINITION 3.4. Let b \u2208 \u25b3(S) be a belief and let B \u2282 A(S) be a point set. A weight function w: B \u2192 R\u2265\u2080 is any function satisfying b(s) = \u2211_{b' \u2208 B}w(b')b'(s) for all s \u2208 S. WB,b denotes the set of all possible weight functions for belief b given point set B.\nTHEOREM 3.5 (POINT SET BOUND). Given a belief b, a point set B, and a function Q: B \u00d7 A \u2192 R which over-approximates the QP\u2092\u2098DP-values of all beliefs-action pairs (b', a) \u2208 B \u00d7 A. Then, any weight function w \u2208 WB,b gives an upper bound on the value of b:\n$Q_{POMDP}(b, a) \u2264 \\hat{Q} (w, a) := \\sum_{b'e B} w(b')Q(b', a)$. (6)\nThis theorem follows directly from the convexity of the value function for POMDPS [48]. To understand how Theorem 3.5 is used implicitly by FIB, consider using point set B\u209b and the weight functions $w_{b,a,o} (b_{s'}) =  \\frac{Pr(o,s' | b,a)}{Pr(ob,a)}$. In that case, we find:\n$\nH_{FIB}Q(b, a) = R(b,a)+\u03b3\\sum_{o \u2208 O}\\max_{a' \u2208 A}  \\sum_{o \u2208 O} [Pr(o | b,a) \\hat{Q} (w_{b,a,o}, a')], (7)$\nGiven a point set B and belief b, the tightest upper bound we can compute using Theorem 3.5 is found using the linear program (LP):"}, {"title": "3.3 Using Bounds in Point-Based Solvers", "content": "Point set bounds are an important component of point-based solvers, a type of algorithm that uses a finite set of beliefs to compute both upper- and lower bounds on the value of a POMDP. Early methods use predefined sets of beliefs to cover the entire belief space evenly [9, 34, 58], but these methods typically scale poorly to large POMDPs. Instead, state-of-the-art algorithms such as HSVI [46, 47] and SARSOP [31] use heuristic search to find beliefs that closely resemble those encountered by an optimal policy, which is sufficient for finding \u03f5-optimal solutions [31].\nSARSOP [31] is a state-of-the-art point-based solver that uses a variant of value iteration [45] to compute lower bounds and the sawtooth bound for upper bounds. The latter requires precomputing value bounds for the set of unit beliefs B\u209b, which is traditionally done using FIB. The next section proposes methods of computing tighter bounds for this set in tractable time."}, {"title": "4 INTRODUCING TIGHTER BOUNDS", "content": "In this section, we introduce three novel bounds on the value function QP\u2092\u2098DP, which are tighter than FIB."}, {"title": "4.1 Tighter Informed Bound (TIB)", "content": "Firstly, we propose an extension of FIB that extends the delay at which the full state is observed. More precisely, we define the tighter informed bound (TIB), which assumes an agent fully observes the current and future states with a delay of 2 time steps. We define the corresponding Q-value function, Q_{TIB}, as:\nDEFINITION 4.1. Q_{TIB} is the fixed point of the operator H_{TIB}:\n$H_{TIB} Q(b, a) = R(b, a) + \u03b3 \\sum_{o \\in O} \\max_{a' \\in A} [\\sum_{s \\in S} b(s) Pr(o | s, a)Q(b_{s,a,o}, a')] $ . (8)\nRunning example. To provide some intuition on the tightness of TIB, we recall the GUESSING POMDP. Under the TIB assumption, taking action w twice lets an agent observe its initial state. The probability of still being in this state after these actions is 0.8\u00b2 + 0.2\u00b2 = 0.68. Thus, taking action w twice and guessing the revealed initial state yields an expected return of 0.68\u03b3\u00b2. This is a strict overapproximation of the optimal value of the POMDP, which is 0.5, but significantly tighter than the bound of 0.8\u03b3 found by FIB."}, {"title": "4.2 Optimized Tighter Informed Bound (OTIB)", "content": "Like for FIB, we notice TIB can be rewritten using Theorem 3.5 with point set B\u2081, as follows:\n$H_{TIB}Q(b,a) = R(b,a) + \u03b3 \\sum_{o \u2208 O} \\max_{a' \u2208 A}  [Pr(o | b,a) \\hat{Q} (\\hat{w}_{b,a,o}, a')],$\nwhere we use the following weight function:\n$\\hat{w}_{b,a,o} (b_{s,a,o}) = \\frac{b(s) Pr(o | s, a)}{Pr(o | b, a)}$.\nIn contrast to FIB, however, these weights are not necessarily unique, and Theorem 3.5 tells us any weight that represents our belief gives a viable upper bound. Thus, we define the optimized tighter informed bound (OTIB), which assumes the value for future beliefs is equal to the minimal point set bound (Theorem 3.5) using point set B\u2081. We define the corresponding Q-value function as follows:\nDEFINITION 4.3. Write $W_{b,a,o} = W_{B\u2081,\\hat{b}_{b,a,o}}$. Then, Q_{OTIB} is the fixed point of the operator H_{OTIB}:\n$H_{OTIB}Q(b, a) = R(b, a) + \u03b3 \\sum_{o \u2208 O} \\max_{a' \u2208 A} [Pr(o | b,a)\\min_{w\u2208W_{b,a,o}} \\hat{Q} (w,a')]$. (9)\nRunning example. We consider the GUESSING POMDP (Example 3.1). Under the OTIB assumption, the belief after taking action w can be expressed using any weight function in $W_{b_{o}, w,1}$. In particular, since b\u2080 \u2208 B\u2081, one valid choice uses weight 1 for b\u2080 and 0 for all others. In that case, the action is suboptimal (with value 0.5\u03b3), and the OTIB bound corresponds with the real value 0.5."}, {"title": "4.3 Entropy-based Tighter Informed Bound (ETIB): A Heuristic Approach", "content": "To reduce the complexity of the precomputations of OTIB, we consider using a single weight for each belief that we reuse for all iterations. More precisely, we approximate the worst-case weights by those that maximize the weighted entropy. This gives higher weights to more uncertain beliefs, which should intuitively give a tighter bound. To formalize this, we first define the maximal entropy weight function for a belief b_{b,a,o} as follows:\n$\\hat{w}_{b,a,0} \u2208 arg \\max_{w \u2208 W_{W_{B,a,o}}} \\sum_{b' \u2208 B_{S}} H(b')w(b').$           (10)\nThen, the entropy-based tighter informed bound (ETIB) assumes the value for future beliefs is equal to the point set bound (Theorem 3.5) using point set B\u2081 and maximal entropy weight functions (Eq. (10)). We define the corresponding Q-value function as follows:\nDEFINITION 4.5. Q_{ETIB} is the fixed point of the operator H_{ETIB}:\n$H_{ETIB} Q(b, a) = R(b, a) + \u03b3 \\sum_{o \u2208 O} \\max_{a' \u2208 A} [Pr(o | b, a) \\hat{Q} (\\hat{w}_{b,a,o}, a')].$\nRunning example. Consider the GUESSING environment. Under the ETIB assumption, the value of taking action w is approximated using the maximal entropy weight function  $\\hat{w}_{b_{0}w, 1}$. Since b\u2080 is the belief with the largest entropy in B\u2081, this weight function is the weight function defined by w(b) = 1 if b = b\u2080, and w(b) = 0 otherwise. Thus, ETIB and OTIB find the same optimal bound."}, {"title": "5 EMPIRICAL EVALUATION", "content": "In this section, we empirically evaluate the proposed bounds: TIB, ETIB, and OTIB. We address the following questions:\n(Q1) Bounds tightness. How do the proposed bounds compare to each other and prior bounds such as FIB? How close are these bounds to the optimal value?\n(Q2) Computational cost. What is the computational cost of these bounds? How do they scale with the POMDP size?\n(Q3) Benefits for SARSOP. Can these bounds improve the performance of POMDP solvers such as SARSOP?\n(Q4) Discount dependency. How does the effect of using these bounds in SARSOP depend on the discount factor?\nImplementation & Baselines. We implement Algorithm 1 within the POMDPs.jl framework [16], and extend the native Julia implementation of SARSOP [31] to use our bounds as initialization. As discussed in Sect. 3.3, we replace the bounds for B\u209b with those computed by our methods. Unless stated otherwise, we use discount factor \u03b3 = 0.95 and compute bounds using relative precision \u03f5 = 10\u207b\u00b3 and h = 250 maximum iterations. By initializing the bound calculations with a (looser) upper bound, we ensure the result is valid despite the finite number of iterations.\nEnvironments. For our experiments, we use several standard POMDP benchmarks: TIGER [12], ROCKSAMPLE [46], ALOHA [25], TAG [41], TIGERGRID [33], HALLWAY1 [33], HALLWAY2 [33], PENTAGON [13] and FOURTH [13]. These environments have diverse characteristics, varying from 2 to 12545 states, from 3 to 29 actions, and from 1 to 1052 observations. Additionally, we consider the GUESSING environment (Fig. 1) and two new environments inspired by POMDPs in the literature. Firstly, we consider a 6 \u00d7 6 grid where an agent needs to navigate from the bottom left to the top right corner. The observation function is as in Amato et al. [3]: the agent observes in which column it is, but not in which row. Secondly, we consider a maintenance environment called K-OUT-OF-N with the goal of keeping a number of components from breaking down [27]. For the latter, we add partial observability using measuring action, which gives a negative reward but reveals the current state, and assume the agent gets no observations otherwise (similar to, e.g., [7, 30, 39]). App. A provides a complete description of both new environments."}, {"title": "5.1 Bound Tightness and Computational Cost", "content": "To address questions (Q1) and (Q2), we compare the upper bounds for the initial beliefs of all environments. In addition, we show the bounds computed by FIB and the best lower bound found by SARSOP within 1200s, which we consider as the closest proxy for the optimal value when evaluating the tightness of the bounds.\nOTIB is tight but computationally intractable. As shown in Table 2, OTIB is always the tightest upper bound for smaller environments. However, its computation times are significantly higher than of the other tested bounds, and for larger environment it often does not converge within the given time.\nTIB and ETIB are tighter than FIB, with tractable overhead. TIB and ETIB are tighter than FIB in all environments at the cost of longer, but mostly tractable, computation times. The differences in the bounds are the largest for GUESSING, TIGER, and GRID6X6, where ETIB performs significantly better than TIB and about on par with OTIB. However, for most other environment the difference between TIB and ETIB is minimal. The difference between FIB and our proposed bound is small in environments where all uncertainty is contained in the initial state, as is the case for ROCKSAMPLE."}, {"title": "5.2 Improvement of SARSOP", "content": "Next, we investigate question (Q3) by comparing the performance of SARSOP when using FIB, TIB, and ETIB as initialization. For our evaluation, we split the environments into two groups. For the smaller environments where SARSOP finds an \u03f5-optimal policy within one hour, we consider convergence times. For the larger environments, where SARSOP does not converge within an hour, we instead consider the relative value gap\n$V_{gap} =  \\frac{\\hat{V}(b_{0}) - V(b_{0})}{\\hat{V}(b_{0})}$,\nafter 600s, 1200s and 3600s. We also provide the upper- and lower bounds in App. A. All running times include the precomputation times of the bounds.\nIn smaller environments, using TIB or ETIB to initialize SARSOP yields mixed results. As shown in Table 4, for most small environments, SARSOP is already sufficiently fast that the initialization has little effect on computation times. The exceptions are K-OUT-OF-N (2), where using TIB and ETIB is significantly quicker than using FIB, and ALOHA (30), where ETIB is significantly slower. In larger environments, using TIB improves the bounds computed by SARSOP. Table 3 shows the tightest relative value gap found with different computational budgets for our larger environments. We see that using TIB typically improves the performance of SARSOP given a sufficiently large computational budget. However, for environments where TIB is computationally expensive (such as ROCKSAMPLE (7,8) and FOURTH), we find that using FIB (initially) yields better results. In contrast, using ETIB yields better results for GRID6X6, but otherwise performs similar or worse then TIB due to its higher computational cost."}, {"title": "5.3 Discount dependency", "content": "Lastly, we investigate question (Q4) by testing how the discount factor affects the computation times of SARSOP, given different initialization bounds. Due to the computational cost, we only test on smaller environments, but we expect this behavior to translate to larger environments as well.\nSARSOP profits more from tighter initial bounds for high discount factors. As shown in Fig. 3, the effect of different initialization is minimal for goal-oriented environments, such as ROCKSAMPLE (5). However, for non-goal-oriented problems (such as TIGER and K-OUT-OF-N), we find that the absolute speedup of using TIB and ETIB increases with the discount factor."}, {"title": "6 RELATED WORK", "content": "Besides QMDP [33], FIB [23] and point-based methods [9, 34, 41, 46, 47, 58] which we introduced in Sect. 2, we mention a number of other methods used for computing upper bounds. Firstly, a number of works consider simplifying the set of reachable beliefs by discretizing the belief space in a similar style as point-based solvers [10, 11, 18, 40, 54]. Next, Yoon et al. [57] introduce 'hindsight optimization', which uses deterministic planning in a number of sampled 'situations' to approximate the value of a (PO)MDP. Haugh and Lacedelli [21] use 'information relaxation' in a similar way. Barenboim and Indelman [6] consider only a subset of possible outcomes of the transition- and observation function to compute upper bounds in online solvers. However, all these methods are typically less tight (though computationally cheaper) than our proposed bounds. Lastly, some bounds are based on the properties of a particular type of POMDP. For example, Sinuany-Stern et al. [44] consider POMDPs that model maintenance, while Krale et al. [30] consider POMDPs where agents have explicit measuring actions.\nOur empirical analysis focuses on SARSOP [31], but we mention a few related state-of-the-art POMDP solvers. Firstly, POMCP [43], and AdaOPS [55] are both variants of Monte Carlo tree search (MCTS) adapted for POMDPs. DESPOT [56] is also based on tree search but uses a method based on hindsight optimization to increase tractability. Lastly, many methods make use of (deep) reinforcement learning to find approximate solutions to POMDPs [19, 22, 32]. However, all these methods focus on large (continuous) state-, action- and observation spaces where our bounds are computationally intractable.\nDeterministic Delay MDPS (DDMDPs) [2, 28, 52] are MDPs where the agent can fully observe its state with some constant delay, which is conceptually similar to FIB and TIB. Finding exact solutions to DDMDPs is NP-hard [52], but efficient approximate solvers exist [1, 14]. However, FIB and TIB take into account (partial) observations occurring before the state is fully revealed, while such observations do not exist in DDMDPs. This means that in POMDPs with no observations, FIB and TIB correspond to the solutions of DDMDPS with delays 1 and 2, respectively. However, solutions for DDMDPS are not sound upper bounds for POMDPs in general, so we do not compare our method with DDMDP solvers.\nLastly, we mention a number of other works related to \u03f5-optimal POMDP solving. Walraven and Spaan [51] and Hansen and Bowman [20] propose methods to speed up the incremental pruning of \u03b1-vectors, which constitutes a considerable amount of the computation time of SARSOP. Relatedly, Dujardin et al. [15] propose a method that uses less \u03b1-vectors instead. Wang et al. [53] proposes to use quadratic functions instead of piecewise-linear functions to represent the upper bound."}, {"title": "7 CONCLUSION", "content": "To improve the performance of \u03f5-optimal solvers, we introduced three novel bounds for POMDPs (TIB, OTIB, and ETIB). We prove these bounds are tighter than the commonly used FIB, and show they can be computed using value iteration. Empirically, both TIB and ETIB are computationally tractable on a large range of benchmarks. Moreover, using these bounds to initialize state-of-the-art solver SARSOP improves its performance.\nFuture work may focus on increasing the tractability of our bounds. For example, instead of computing bounds for all beliefs b \u2208 B\u2081, it may be quicker to use FIB for those that have a low probability of being reached. Alternatively, more research could be done on different heuristic choices for weights, particularly choices that do not require solving LPs. Lastly, future work could consider how our bounds can be applied to other settings, such as finite- or indefinite horizon problems."}, {"title": "A TECHNICAL DETAILS EXPERIMENTS", "content": "In this section, we provide additional details about our experimental evaluation that did not fit in the main paper. All code can be found in the supplementary materials and will be made publicly available after publication. All experiments are performed on a 3.7GHz Intel Core i9 CPU running Ubuntu 22.04.1 LTS."}, {"title": "A.1 Implementation details", "content": "In this section, we give a high-level overview of how we implemented the precomputation of our bounds."}, {"title": "A.1.1 FIB and QMDP", "content": "We initially used the FIB.jl framework for computing FIB, but found that it did not perform well on larger environments. Thus, we used a custom implementation to precompute FIB using value iteration. To speed up convergence, we additionally wrote a custom implementation for QMDP (using value iteration) that we use as an initialization for FIB. To guarantee we obtain sound upper bounds, we initialize QMDP using $max_{(s,a) \u2208 S x A} R(s, a) )$T for all state-action pairs."}, {"title": "A.1.2 TIB, ETIB and OTIB", "content": "The precomputation of TIB, ETIB, and OTIB essentially follows Algorithm 1. To improve computational speeds, we cache transition probabilities and reachable beliefs for all beliefs in B\u2081. For solving LPs, we use the JuMP.jl framework [35] and the Clp optimizer [17], which we found to perform best in practice. We initialize our values using FIB, which guarantees soundness."}, {"title": "A.1.3 Implementation SARSOP", "content": "Our experiments use the native Julia SARSOP solver (https://github.com/JuliaPOMDP/NativeSARSOP.jl), which we alter in two ways:\nFirstly, to be able to use our bounds, we altered SARSOP such that it can use any solver to compute an initial upper bound. More precisely, the Julia implementation of SARSOP uses a vector cornervalues to represent the values of beliefs B\u209b, which are initialized using FIB by default. We alter the code so that it can be initialized using any solver. We additionally add two vectors B_heuristic and V_heuristic, which get treated as part of the point set when computing upper bounds.\nSecondly, by default, SARSOP uses an absolute target precision \u03f5 as a stopping condition and a planning precision \u03f5\u209a to determine the sampling depth. To allow for better comparison between benchmarks, we alter SARSOP such that the precision is relative, i.e. the algorithm terminates when $\\frac{\\hat{V}(b_{0}) - V(b_{0})}{\\hat{V}(b_{0})} < \u03f5.$ Prior works have found that picking \u03f5\u209a > \u03f5 often gives better performance, but then there is no guarantee that an \u03f5-optimal policy can ever be found."}, {"title": "A.2 Environment Descriptions", "content": "Next, we give a short overview of our novel benchmarks environments. Both are implemented using the POMDPs.jl framework and can thus be used by others."}, {"title": "Grid", "content": "We consider a 6 \u00d7 6 Grid POMDP. This POMDP has 36 states, each representing that the agent is in a particular cell in the"}]}