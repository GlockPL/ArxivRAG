{"title": "An End-to-End Two-Stream Network Based on RGB Flow and Representation Flow for Human Action Recognition", "authors": ["Song-Jiang Lai", "Tsun-Hin Cheung", "Ka-Chun Fung", "Tian-Shan Liu", "Kin-Man Lam"], "abstract": "With the rapid advancements in deep learning, computer vision tasks have seen significant improvements, making two-stream neural networks a popular focus for video-based action recognition. Traditional models using RGB and optical flow streams achieve strong performance but at a high computational cost. To address this, we introduce a representation flow algorithm to replace the optical flow branch in the egocentric action recognition model, enabling end-to-end training while reducing computational cost and prediction time. Our model, designed for egocentric action recognition, uses class activation maps (CAMs) to improve accuracy and ConvLSTM for spatio-temporal encoding with spatial attention. When evaluated on the GTEA61, EGTEA GAZE+, and HMDB datasets, our model matches the accuracy of the original model on GTEA61 and exceeds it by 0.65% and 0.84% on EGTEA GAZE+ and HMDB, respectively. Prediction runtimes are significantly reduced to 0.1881s, 0.1503s, and 0.1459s, compared to the original model's 101.6795s, 25.3799s, and 203.9958s. Ablation studies were also conducted to study the impact of different parameters on model performance.", "sections": [{"title": "INTRODUCTION", "content": "Early action recognition techniques primarily focused on detecting and representing interest points using methods such as spatio-temporal point detection [1] and optical flow histograms [2]. Although these methods are effective for feature extraction, they struggle with large datasets due to their lack of scalability. Heng et al. [3] introduced dense trajectory features, which improve motion representation but still face scalability issues. Recent advances in deep learning have surpassed traditional methods. Karpathy et al. [4] developed deep models that process temporal information and combine dense optical flow with deep features. Despite these advancements, limitations persisted, including challenges with long videos and pre-computed optical flow. Innovations, such as spatial-temporal CNN integration [5] and LSTM-based time fusion [6], further advanced the research field of action recognition.\nZhou et al. [7] improved action recognition accuracy through weighted fusion and sequential reasoning. Temporal attention models and ConvLSTM variants [8] have enhanced the focus on important frames and preserved spatial structures, proving valuable in applications like anomaly and violence detection. Concurrently, research on optical flow has evolved, with Fan et al. [9] enhancing accuracy with the TV-L1 method, and Sun et al. [10] introducing the optical flow-guided feature (OFF). Piergiovanni et al. [11] advanced this with the representation flow algorithm, offering a faster and more accurate alternative to traditional optical flow.\nOur model combines the strengths of the egocentric activity recognition model proposed by Swathikiran et al. [12] with the representation flow algorithm, yielding an end-to-end trainable system. By fusing input RGB frames with representation flow, our method not only reduces prediction time but also enhances accuracy. This is achieved by leveraging the combined benefits of both the original EgoRCNN model [12] and the advanced representation flow model."}, {"title": "METHODOLOGY", "content": "The proposed activity recognition method comprises two components: deep feature encoding for the RGB stream employing spatial attention and class activation maps (CAMs), and a fully differentiable convolutional layer for the representation stream, which is developed from optical flow methods. The entire framework is designed to be trainable end-to-end, allowing all parameters to be optimized jointly to enhance model performance."}, {"title": "Class Activation Maps", "content": "A novel mechanism known as Class Activation Mapping (CAM) is introduced by Zhou et al. [13] to produce the class-specific saliency maps by leveraging the average pooling layer in contemporary deep CNN architectures, such as ResNet. Let $w_i$ represent the weight for unit $l$ corresponding to class $c$, and $a_l(i)$ denote the activation values at spatial location $i$ of unit $l$ in the last convolutional layer. Furthermore, the CAM for class $c$, $F_c(i)$, can be calculated as follows:\n$F_c(i) = \\sum_l w_l^c a_l(i)$.\nTherefore, the CAM, $F_c$, can be used to identify the image regions that the CNN utilized to recognize the class was being considered (i.e., class $c$). Subsequently, we select the class that exhibits the highest probability, referred to as the winning class. This allows us to produce a saliency map using the CAM."}, {"title": "ConvLSTM Module", "content": "The structural block diagram of the RGB stream shows how RGB frames are encoded, is shown in Figure 1. ResNet-34 serves as the backbone, responsible for generating spatial attention maps, extracting relevant features, and predicting class categories. The training is conducted in two stages: Stage 1 focuses on training only the classifier and ConvLSTM layer, while Stage 2 involves training the additional convolutional and fully connection layers of the ResNet-34 network. The pre-trained weights based on the ImageNet dataset are utilized by the rest parts of the model. ConvLSTM is used for temporal aggregation, and Equation (1) is employed to compute the CAM for each RGB frame. The CAM is converted into an attention map via the softmax function along the spatial dimension, which is then multiplied with the output of the last convolutional layer. The attention map is computed as follows:\n$f_{SA}(i) = f_i \\frac{e^{M_c(i)}}{\\sum_{i'} e^{M_c(i')}}$.\nWhere $f_i$ represents the output feature for the final layer of ResNet-34 at the spatial location $i$. $f_{SA}(i)$ denotes the Hadamard product of $f_i$ and the softmax output, which indicates the spatial attention map applied to image features. $M_c(i)$ indicates the use of the winning category $c$ to obtain the corresponding CAM. The symbol $\\odot$ is used to denote the Hadamard product. After obtaining visual features with spatial attention approximated by Equation (2), temporal encoding for frame-level information is required.\nConvLSTM works as shown by these following equations:\n$I_t = \\sigma(W_{ix} * f_{sa} + W_{ih} * h_{t-1} + b^i)$,\n$f_t = (W_{fx} * f_{sa} + W_{fh} * h_{t-1} + b^f)$,\n$C = tanh(W_{cx} * f_{SA} + W_{ch} * h_{t-1} + b^c)$,\n$C_t = C_t \\odot f_{SA} + C_{t-1} \\odot f_t$,\n$O_t = \\sigma(W_{ox} * f_{Sa} + W_{oh} * h_{t-1} + b^o)$,\n$h_t = O_t \\odot tanh(C_t)$,\nWhere $\\sigma$ denotes the sigmoid function, $I_t$, $f_t$, and $O_t$ are the input state, forget state, and hidden state, respectively, $w$ and $b$ represent the learnable weights and biases of the ConvLSTM."}, {"title": "The Representation Flow Algorithm", "content": "The representation flow algorithm shares many similarities but also has notable differences with the traditional optical flow algorithm. As detailed by Piergiovanni et al. [11], while both algorithms aim to capture motion information between frames, the representation flow layer is designed to extract features from deep feature maps, whereas optical flow is applied to video frames in the spatial domain. Additionally, the representation flow algorithm differs in its learning parameters: optical flow involves fixed hyper-parameters, such as $\\tau$, $\\theta$, and $\\lambda$, which are manually set, whereas the representation flow algorithm utilizes trainable parameters that are updated during model training.\nDenote $F_1$ and $F_2$ as the feature maps from two consecutive frames, with a single channel. These feature maps are then convolved with the Sobel operators, as shown in Equation (9), to compute the gradients of the feature maps.\n$F_{i,x} = \\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix} * F_i, F_{i,y} = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix} * F_i$.\nFirstly, we introduce the flow field: $u$, and the dual vector fields: $p$, which are used to minimize the energy, and set them to zero. Then, Algorithm 1 is applied iteratively until the minimum total variational energy is achieved."}, {"title": "Algorithm 1: Method for the representation flow layer [11]", "content": "Initialize $u$ and $p$, as follows:\n$u = 0, p = 0$\nCompute the feature gradients using Equation (9), and then compute the difference as follows:\n$P_c = F_2 - F_1$\nPerform the following operations, with $n$ iterations:\n$p = p_c + \\nabla_x F_2 \\cdot U_x + \\nabla_y F_2 \\cdot U_y$\n$v = \\begin{cases} u + \\lambda \\theta \\nabla F_2, & \\rho < -\\lambda \\theta |\\nabla F_2|^2 \\\\ u - \\lambda \\theta \\nabla F_2, & \\rho > \\lambda \\theta |\\nabla F_2|^2 \\\\ u - \\frac{u-\\rho}{\\theta |\\nabla F_2|^2} \\nabla F_2, & otherwise \\end{cases}$.\n$u = v + \\theta \\cdot divergence(p)$\n$p = \\frac{p + \\tau \\nabla u}{1 + \\tau |u|}$\nFinally, after the loop, the flow $u$ is obtained.\nFollowing Algorithm 1, zero-pad $p$ in the $x$ and $y$ directions and convolute them with the weights $W_x$ and $W_y$ to calculate the divergence:\n$divergence(p) = p_x * W_x + p_y * W_y$,\n$\\nabla u_x$ and $\\nabla u_y$ are computed as follows:\n$\\nabla_{ux} = \\begin{bmatrix} 1 & 0 & -1 \\\\ 2 & 0 & -2 \\\\ 1 & 0 & -1 \\end{bmatrix} * U_x, \\nabla_{uy} = \\begin{bmatrix} 1 & 2 & 1 \\\\ 0 & 0 & 0 \\\\ -1 & -2 & -1 \\end{bmatrix} * U_y$."}, {"title": "Computing Flow-of-Flow", "content": "In principle, continuously stacking representation flow layers can enhance motion information extraction and representation, as more flow layers can more comprehensively capture the direction and magnitude of motion. However, a greater number of flow layers often results in more intricate motion features and textures. Applying the representation flow technique to sequential flow images may lead to inconsistent optical flow and non-rigid movements, hence impairing model performance. Inserting standard convolution layers between representation flow layers aids in preserving optical flow consistency and enhancing motion representation to resolve this issue. The comprehensive architecture of the video-CNN, integrated with the representation flow, which is demonstrated in Figure 2."}, {"title": "Applying Representation Flow Layer for Action Recognition", "content": "In our proposed model, as shown in Figure 3, the representation flow layer replaces the original optical flow branch in a CNN model, integrating into the EgoRCNN framework. This refined model combines the RGB and representation flow branches, employing two classifier layers for joint training. Both branches take RGB frames as input. The CNN produces predictions for each time step, which are then averaged throughout the time dimension for generating the category probabilities. The hybrid network is trained by reducing the cross-entropy loss, as outlined below:\n$L(v, c) = - \\sum_i^k 1_{v(c) == i} log(p_i)$,\nWhere M is the classification model, $v$ represents the video, $c$ denotes a specific category among all categories, $L$ refers to the value of the cross-entropy loss function, and $p = M(v, c)$."}, {"title": "EXPERIMENT RESULTS", "content": "In this study, we employ RGB features and motion features, based on representation flow, for action recognition. The ConvLSTM with 512 hidden units is utilized by the RGB branch of our model for spatio-temporal coding and ResNet-34 as the backbone for extracting frame-level image features and spatial attention maps. Training is performed in two stages: Stage 1 spans 200 epochs with a learning rate of 10-3, adjusted at the 25, 75, and 150 epochs, while Stage 2 involves 150 epochs with a learning rate of 10-4, adjusted at the 25 and 75 epochs, utilizing the ADAM optimizer and a batch size of 32 with 25 frames sampled per video.\nThe representation flow branch utilizes ResNet-34 as the backbone and consists of two representation flow layers, separated by an intermediate convolutional layer to smooth the flow. The input video clip length is set to 16 frames for optimal performance. The number of channels is reduced to 32 by a 1 \u00d7 1 convolutional layer, balancing speed and performance, while a 3\u00d73 convolutional layer restores the original channel count. The flow branch is trained for 750 epochs with"}, {"title": "GTEA61", "content": "Initially, we assessed the efficacy of our model using split2 of the GTEA61 dataset and contrasted it with the original EgoRCNN model that included the optical flow branch. The findings shown in Table 1 indicate that the representation flow model's accuracy on split2 is 37.74, while the optical flow component of the original EgoRCNN model attains 39.42. Following the collaborative training of the two streams, both our proposed model and the original EgoRCNN model attained an accuracy of 69.84. Since the size of GTEA 61 dataset is too small, the superiority of our model cannot be reflected."}, {"title": "EGTEA GAZE+", "content": "We trained and assessed the proposed model on the larger EGTEA GAZE+ dataset. This dataset is significantly larger than the GTEA61 dataset, providing a more comprehensive basis for our analysis. The prediction accuracies of the two models are tabulated in Table 2. The assessment was conducted across three splits of the EGTEA GAZE+ dataset. The results illustrate the proposed model consistently outperforms the original model in terms of accuracy."}, {"title": "HMDB", "content": "The superiority of our proposed model is previously assessed using the GTEA61 and EGTEA GAZE+ datasets, both of which are first-person perspective datasets. To further illustrate the generalizability and robustness of our proposed model, we expanded our experiments to the HMDB dataset, which provides a third-person perspective and has a modest sample size. The experimental findings on the HMDB dataset are shown in Table 4, illustrating the performance of our proposed model across various perspectives and sample sizes.\nOur proposed model outperforms the original model can also be demonstrated by the prediction results on the HMDB dataset. Specifically, while the original model achieved an accuracy of 22.89, our proposed model reached 24.34. With joint training of both two streams, the accuracy for the original model improves to 49.87, compared to 50.71 for the proposed model. In terms of runtime, as shown in Table 4, the original model takes an average runtime of 203.9958 seconds, including 0.1058 seconds for classification and 203.89 seconds for optical flow extraction. In contrast, the proposed model significantly reduces the average runtime to only 0.1459 seconds."}, {"title": "Effects of backbone size", "content": "The depth of a deep model can influence its tendency to overfit, thus affecting its accuracy and generalization. In order to investigate this, we implemented experiments that involved altering the size of backbone for the representation flow branch from ResNet-18 to ResNet-101. The performance is influenced by the model depth, as demonstrated in Table 7. It is evident the accuracy of the proposed model is at its highest when the backbone of our model is configured to ResNet-34."}, {"title": "Comparison to other state-of-art methods", "content": "Table 8 and Table 9 summarize the results of numerous experiments conducted to assess the efficacy of our proposed model and compare it to state-of-the-art methods. The methods in Table 8 focus on egocentric activity recognition tasks, while Table 9 addresses the third-person activity recognition tasks. The experiment results in Table 8 illustrate the accuracies on the fixed split2 of GTEA61, and for Table 9, it shows the prediction accuracies on each split of EGTEA GAZE+."}, {"title": "Ablation study", "content": ""}, {"title": "Optimal number of representation flow layers", "content": "The number of representation flow layers incorporated into a CNN model significantly impacts its performance. We investigated the effects of setting different number of representation flow layers to 1, 2, 3 and none on our proposed model. Evaluations were conducted on the HMDB dataset using consistent experimental settings, and the results are tabulated in Table 5. Our findings indicate the model with two representation flow layers achieved the highest accuracy compared to the configurations with 1, 3 or no flow layers."}, {"title": "Optimal number of iterations for training", "content": "The performance of our model highly relies on the number of iterations used to compute the flow in the representation flow layer. We conducted experiments using 10, 20, 30, and 50 iterations, and the results are shown in Table 6. It is clear our model achieves the highest accuracy when 20 iterations are adopted for computing representation flow."}, {"title": "CONCLUSIONS", "content": "In this study, we have proposed a learnable representation flow layer as a replacement for the traditional optical flow branch in the original EgoRCNN model. Our model leverages the class attention map (CAM) of the RGB stream, thereby enhancing its ability to focus on activity-relevant regions. We further incorporate Conv-LSTM for spatio-temporal encoding with spatial attention. This has resulted in an improved performance over the original EgoRCNN model in terms of both accuracy and prediction runtime. This improvement is evident across three standard datasets: two of which are from the first-person perspective and the remaining one from the third-person perspective."}]}