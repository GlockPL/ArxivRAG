{"title": "UNDERSTANDING REASONING IN CHAIN-OF-THOUGHT FROM THE HOPFIELDIAN VIEW", "authors": ["Lijie Hu", "Liang Liu", "Shu Yang", "Xin Chen", "Zhen Tan", "Muhammad Asif Ali", "Mengdi Li", "Di Wang"], "abstract": "Large Language Models have demonstrated remarkable abilities across various tasks, with Chain-of-Thought (CoT) prompting emerging as a key technique to enhance reasoning capabilities. However, existing research primarily focuses on improving performance, lacking a comprehensive framework to explain and understand the fundamental factors behind CoT's success. To bridge this gap, we introduce a novel perspective grounded in the Hopfieldian view of cognition in cognitive neuroscience. We establish a connection between CoT reasoning and key cognitive elements such as stimuli, actions, neural populations, and representation spaces. From our view, we can understand the reasoning process as the movement between these representation spaces. Building on this insight, we develop a method for localizing reasoning errors in the response of CoTs. Moreover, we propose the Representation-of-Thought (RoT) framework, which leverages the robustness of low-dimensional representation spaces to enhance the robustness of the reasoning process in CoTs. Experimental results demonstrate that RoT improves the robustness and interpretability of CoT reasoning while offering fine-grained control over the reasoning process.", "sections": [{"title": "1 INTRODUCTION", "content": "Large Language Models (LLMs) have demonstrated exceptional capabilities in following the nat-ural language instructions (Ouyang et al., 2022; Jin et al., 2024) and excelling across a variety ofdownstream tasks (Hu et al., 2023a; Zhang et al., 2023; Yang et al., 2024a;c;b). As reasoning skillsare crucial for tasks such as commonsense and mathematical reasoning (Rae et al., 2021), there is agrowing focus on enhancing these capabilities. One prominent approach is Chain-of-Thought (CoT)prompting (Wei et al., 2022; Kojima et al., 2022), a simple yet highly effective technique to unleashthe reasoning capability of LLMs. However, despite its success, a natural and fundamental researchquestion remains: How does the reasoning capability emerge through CoT prompting?\nNumerous studies have sought to identify the key factors or elements that enable CoT to enhance thereasoning capabilities of LLMs (Kojima et al., 2022; Wang et al., 2023a; Tang et al., 2023; Merrill &Sabharwal, 2023). Some works focus on improving CoT reasoning through query-based corrections(Kim et al., 2023), knowledge-enhanced frameworks (Zhao et al., 2023), and symbolic reasoningchains for faithful CoT (Lyu et al., 2023; Lanham et al., 2023). Other research has examined howthe sequence of demonstrations, random labels (Min et al., 2022), or even meaningless tokens (Pfauet al., 2024) can positively influence reasoning performance. However, these works primarily fo-cus on improving the model's reasoning performance, and they do not provide a comprehensiveframework to explain the underlying factors driving CoT's success.\nTo understand the reasoning process in CoTs more deeply, we draw inspiration from cognitive neuro-science, specifically the relationship between cognition and brain function. In this field, the Hopfiel-dian view (Hopfield, 1982) and the Sherringtonian view (Sherrington, 1906) represent two differentways of understanding neural computational models and cognitive mechanisms. While the Sherring-tonian view of cognitive explanation focuses on specific connections between neurons in the brain,"}, {"title": "2 RELATED WORK", "content": "Chain-of-Thought (CoT). The CoT is a prompting technique that engages LLMs in step-by-stepreasoning rather than directly providing the answers (Nye et al., 2021). Studies have shown thatintroducing intermediate steps or learning from demonstrations can significantly improve the rea-soning performance of LLMs (Wei et al., 2022; Kojima et al., 2022). Given the success of CoT,numerous studies have explored its application to a variety of complex problems, including arith-metic, commonsense, symbolic reasoning (Wang et al., 2023c; Zhou et al., 2023; Wang & Zhou,2024), and logic tasks (Creswell & Shanahan, 2022; Pan et al., 2023; Weng et al., 2023). Recently,numerous endeavors have been made to enhance the reasoning capabilities in LLMs (Wang et al.,2023a; Dutta et al., 2024). For example, Kim et al. (2023) proposed a query-based approach to cor-rect erroneous reasoning steps within a CoT. Zhao et al. (2023) introduced a knowledge-enhancedmethod to improve the factual correctness for multi-pole open-domain QA tasks. Lyu et al. (2023)developed \"faithful CoT\", i.e., a framework that first translates natural language queries into sym-bolic reasoning chains and then solves the problem using CoT. Additionally, several studies havealso focused on the sequence and quantity of demonstrations within the context, investigating theircontributions to the final reasoning performance. For this, Min et al. (2022) discovered that evenrandom labels or ineffective reasoning steps can still improve the model's reasoning performance.Lanham et al. (2023) demonstrated the impact of intervening in the CoT process by adding mistakesor paraphrases. Pfau et al. (2024) showed that using meaningless filler tokens in place of a chain-of-thought can surprisingly boost reasoning performance. However, these studies primarily focused onhow to improve the CoT's reasoning performance and do not provide a framework to analyze the fun-damental reasons, i.e., how does the reasoning capability emerge through CoT? Dutta et al. (2024)investigates the neural sub-structures within LLMs that manifest Chain-of-Thought (CoT) reasoningon the Llama-2-7B model. Similarly, Rai & Yao (2024) explores neurons in the feed-forward layersof LLMs to analyze their arithmetic reasoning capabilities on the Llama-2-7B model. Both studiesare grounded in the Sherringtonian view of neural activity. In contrast, we adopt the Hopfieldianperspective to bridge this gap, focusing on representations rather than individual neurons. We applyour approach across three different downstream tasks and can further extend our analysis to largermodels like Llama-2-70B.\nInterpretability of LLMs. Interpretability plays a key role in a deeper understanding of LLMsto identify potential risks and better meet human requirements (Zou et al., 2023). Common inter-pretability strategies include (i) Salience maps, which rely on highlighting the regions in the inputthat are attended by the model (Simonyan et al., 2014; Smilkov et al., 2017; Clark et al., 2019;Hu et al., 2023c;b; Lai et al., 2024); (ii) Feature visualization, which creates representative inputsindicative of particular neurons' activations (Szegedy et al., 2014; Nguyen et al., 2016; Fong &Vedaldi, 2018; Nguyen et al., 2019); and (iii) Mechanistic interpretability, which employs reverse-engineering tools to explain networks based on circuits and node-to-node connections (Olah et al.,2020; Olsson et al., 2022; Wang et al., 2023b). However, these methods often require substantialhuman intervention and are limited in terms of scalability or interpretability, especially for the largelanguage models (Fong & Vedaldi, 2018; Jain & Wallace, 2019; Hu et al., 2024). Thus, these meth-ods cannot be directly used to interpret CoT reasoning. Additionally, most current approaches focuson representation-level analysis without considering how these representations connect to conceptslearned during pre-training (Bricken et al., 2023; Templeton et al., 2024). Other works investigatethe localization and representation of concepts in the network (Kim et al., 2018; Li et al., 2024),linear classifier probing to uncover input properties (Belinkov, 2022), fact localization and edit-ing (Meng et al., 2022; Zhong et al., 2023; Cheng et al., 2024a;b), concept erasure (Shao et al.,2023; Gandikota et al., 2023), and corrective analysis (Burns et al., 2023), etc. These observationsare aligned with RepE (Zou et al., 2023), which emphasized the nearly linear nature of LLM repre-sentations (Park et al., 2024). However, none of these approaches directly address the inner workingsof CoT reasoning. While recent work has begun exploring connections between LLM interpretabil-ity and cognitive neuroscience (Vilas et al., 2024). However, it does not discuss the Hopfieldian view"}, {"title": "3 PRELIMINARIES", "content": "Large Language Models and Prompting. Prompts can take various forms, such as a single sen-tence or longer paragraphs, and may include additional information or constraints to guide themodel's behavior. Let $M : X \\rightarrow Y$ be an LLM that takes an input sequence $x = (x_1,x_2,...,x_q) \\inX$ and produces an output sequence $y = (Y_1, Y_2,..., Y_m) \\in Y$. The model is typically trained tooptimize the conditional probability distribution $pr(y|x)$, which assigns a probability to each possibleoutput sequence y given x. To incorporate a prompt w with the input sequence x, we can concate-nate them into a new sequence $\\hat{x} = (w,x_1,x_2,...,x_q)$. The conditional probability distribution$pr(y|\\hat{x})$ is then computed using $\\hat{x}$. Formally, the probability of the output sequence \u0177 given \u00ee is:\n$pr(\\hat{y}|\\hat{x}) = \\prod_{i=1}^{m} pr(y_i|y_{<i}, \\hat{x})$,where $y_{<i}$ represents the prefix of the sequence y up to position i \u2013 1, and $pr(y_i|y_{<i}, \\hat{x})$ denotes theprobability of generating yi given $y_{<i}$ and $\\hat{x}$.\nThe Hopfieldian View. In cognitive neuroscience, two prominent perspectives aim to explain cog-nition: the Sherringtonian view and the Hopfieldian view.\u00b9 The Hopfieldian view focuses on un-derstanding behavior through computation and representation within neural spaces, rather than thespecific biological details of neurons, ion flows, or molecular interactions (Hopfield, 1982; 1984;Hopfield & Tank, 1986). It operates at a higher level of abstraction, emphasizing the role of repre-sentations and the computations performed on them.\nThis approach conceptualizes cognition as transformations between representation spaces. At theimplementation level, the collective activity of neurons is mapped onto a representation space, whichcontains a low-dimensional representational manifold. Algorithmically, Hopfieldian computationviews these representation spaces as fundamental entities, with movements within or transformationsbetween them as the central operations. The representations themselves are structured as basinsof attraction within a state space, and while they are implemented by neural structures (whetherindividual neurons, neural populations, or other components), the focus is on the dynamics ofthe system rather than its specific biological mechanisms. Most Hopfieldian models, in practice, centeron the activity of neural populations.\nA parameter space defines the dimensions of variation within these representational spaces, aligningwith quality-space approaches from philosophy, where content is similarly structured. Computa-tions over these representations are understood as dynamic transformations between spaces or shiftswithin them, characterized by features like attractors, bifurcations, limit cycles, and trajectories.Ultimately, cognitive functions are realized through these dynamic movements within or betweenrepresentational spaces.\nLinear Representations in Language Models. Recent investigations into the internal mechanicsof LLMs have revealed intriguing properties of their learned representations. Park et al. (2024)posited that high-level semantic features such as gender or honesty could be linearly representedas directions within the model's representation space. This can be illustrated by the well-knownword analogy task using a word embedding model (Mikolov et al., 2013). By defining $M(\\cdot)$ as afuntion of extracting the representations of a given word by a word embedding model, the oper-$M(Spain) - M(Madrid) + M(Paris)$ often results in an output close to $M(France)$, where$M(Spain) - M(Madrid)$ can be considered as the representation vector of the abstract \u201ccapital of\u201dfeature in the embedding space. Concurrently, research on interpretable neurons (Dale et al., 2023;Ortiz-Jim\u00e9nez et al., 2023; Voita et al., 2024) has identified neurons that consistently activate forspecific input features or tasks, suggesting that these features may also be represented as directionsin the LLMs\u2019 neuron space. For instance, Tigges et al. (2023) use the PCA vector between LLMs\u2019hidden states on instructions \u201cpositive\u201d and \u201cnegative\u201d to find the sentiment direction in LLMs. Ad-ditionally, recent works (Zou et al., 2023; Arditi et al., 2024) show the effectiveness of engineeringon language models using these directions. For example, adding multiples of the \u201chonesty\u201d directionto some hidden states has been sufficient to make the model more honest and reduce hallucinations.See Appendix A for an introduction to the Sherringtonian view. For a detailed comparison between these"}, {"title": "4 BRIDGING REASONING IN COTS AND THE HOPFIELDIAN VIEW", "content": "In this section, we aim to build a bridge between the reasoning process in CoTs and the cogni-tive brain from the Hopfieldian view. We will particularly associate the main elements (stimuli,neural populations, and representation spaces) in the Hopfieldian view. After understanding theseelements, we can leverage the strength of the Hopfieldian view to deepen our understanding of thereasoning process in current CoTs and further improve it. Note that we will leave other elements inthe Hopfieldian view, such as attractors and state space, as future work.\nStimuli and Actions. Stimuli and actions are key components of how the brain processes informa-tion and interacts with the environment. Actions refer to the motor responses or behaviors that resultfrom cognitive processing, which are responses given by LLMs through CoTs.\nStimuli refer to external or internal events, objects, or changes in the environment that are detectedby the sensory systems and can influence cognitive processes and behavior. Based on this, we canadopt the term \"stimuli\" from cognitive science in the context of CoTs to refer to specific prompttext or instructions that trigger CoT reasoning. Specifically, in the zero-shot setting, we define thestimulus as $S_{zero}$ to represent a set of supplementary instructions in the prompt that encourage themodel to provide more intermediate reasoning steps before arriving at a final answer. For example,it can be \u201clet\u2019s think step by step\" or \u201cmake sure to give steps before your answer\". In the few-shotsetting, the stimulus $s_{few}$ is defined as the sequence of demonstrations $D = \\{(\\bar{q}_1, \\bar{a}_1), (\\bar{q}_2, \\bar{a}_2),...\\}$in the prompt, where $\\bar{q}_i$ represents the query and $\\bar{a}_i$ is the corresponding response. In the followingdiscussion, we use $s^+$ to indicate that stimuli are included in the model\u2019s input and $s^-$ to indicatethat no stimuli are added. Note that we avoid using explicitly negative stimuli, such as \u201cplease becareless and answer the following question\u201d, because a well-aligned model would likely refuse tobehave in such a manner (Ouyang et al., 2022).\nNeural Populations. As we mentioned, in the Hopfieldian view, representations are realized byvarious forms of neural organization, especially populations. Identifying these \u201cneural populations\u201din CoTs is especially important. In our framework, there are two steps for finding them.\n(i) Stimulus Set Designing. Here our goal is to elucidate the sensitivity of LLMs to different CoTprompts with stimuli. Understanding such sensitivity could help us know the neural populationsraised from the stimuli. In detail, we construct a prompt set. For each query q, we considertwo forms of prompts: positive one (with stimuli) as $p^+ = T(s^+,q)$ and negative one (with-out stimuli) as $p^- = T(s^-,q)$, where T is the prompt template. Specifically, for each query $q_i$,we construct M number of prompts for both of them with different stimuli, which is denoted as$P_i = \\{p_{i_1}^-, p_{i_1}^+, p_{i_2}^-, p_{i_2}^+, ..., p_{i_M}^-, p_{i_M}^+\\}$. Such construction is to make our following neuralpopulations less dependent on the specific template form. Thus, in total, we have a stimulus set$P^* = \\{P_1, P_2, \\cdots, P_N\\}$, where N is the number of queries. These contrastive pairs of promptswill be used to identify neural populations given by these stimuli.\n(ii) Identifying Neural Populations. Intuitively, the neural populations should be the most influ-ential activation vectors of these prompts or stimuli. In detail, for each prompt in $P^*$, the nextstep is capturing the network architecture\u2019s corresponding neural populations. Since LLMs rely ontransformer-based architecture to store distinct representations intended for different purposes,it is crucial to design the extraction process to capture task-specific representations carefully. For agiven prompt $p^+$ or $p^-$, we will find the \u201cmost representative token\u201d, which encapsulates rich andhighly generalizable representations of the stimuli. Here we select the last token after tokenizing theprompt, which is based on the observation in Zou et al. (2023) that it is the most informative tokenfor decoder-only or auto-regressive architecture models.\nOnce the last token position is identified, we can naturally select some of its activations (hiddenstate) in hidden layers. Previous studies (Fan et al., 2024; Cosentino & Shekkizhar, 2024) haveshown that not all layers store important information about reasoning; thus we focus on a subset ofthem to reduce the computation cost, whose indices are denoted as a set K (in practice, K is alwaysthe last several layers). Thus, we have a collection of activation vectors. However, since we arefocusing on the reasoning of CoT, studying the neural populations raised from the stimuli ratherthan the whole prompt is more important. Thus, we consider the difference in the activations ofpairs of prompts. Specifically, for a pair $(p^+, p^-)$, we can get their activations for all selected layersK: $\\{h_k(p^+)\\}_{k\\in K}$ and $\\{h_k(p^-)\\}_{k\\in K}$, where $h_k(p)$ refers to the activation vector of the k-th layer foragiven input prompt p. Then the differences of activations $\\{h_k(p)\\}_{k\\in K}$ are the neural populations"}, {"title": "5 APPLICATIONS OF HOPFIELDIAN VIEW TO COTS", "content": "In the previous section, we mainly discussed how each element in the Hopfieldian view correspondsto the reasoning in CoTs. From our previous view, we can understand the reasoning process as themovement between these representation spaces. Based on these connections, we can leverage thestrength of the Hopfieldian view to improve or further understand CoTs. In this section, we firstconsider how to localize the reasoning error based on the low dimensional representation spaces.Then, by leveraging the robustness of the Hopfieldian view, we propose a new framework, namelyRepresentation of Thought, that enhances the performance robustness of CoTs.\n5.1 REASONING ERROR LOCALIZATION\nIn this task, for a given query, we want to check if there are some reasoning errors in the responseby CoTs. If so, we aim to localize these errors. As in the Hopfieldian view, cognition occurswithin low-dimensional representation spaces. Reasoning errors can be identified by analyzing thestructure of these spaces, such as when certain directions $R_k$ (representing specific cognitive factors)are disproportionately activated or suppressed. This can help localize the source of the error withinthe cognitive process. Motivated by this, we can leverage the internal structure of spaces we havelearned via PCA to locate the reasoning error for a given query in CoTs.\nIntuitively, since the reasoning occurs within these representation spaces, if there is a reasoning errorin the response, then during the reasoning process, some tokens make the activations (hidden states)of the response far from the corresponding representation spaces. This is because if these activationsare far from the spaces, CoTs do not reason the corresponding \"concepts\" in the response. Motivatedby this, our idea is to iteratively check the tokens in the response to see whether they are far fromthe representation spaces.\nMathematically, for a given prompt T via CoT of query x with its response y = ($y_1, y_2,\\cdots, y_m$),we will iteratively feed the prompt with a part of the response, i.e., $T_i = T + y_{<i}$, where + is thestring concatenation. If the activations of $T_{i-1}$ are close to while those of $T_i$ are very far from therepresentation spaces $\\{R_k\\}_{k\\in K}$ in (2), then we can think the i-th token $y_i$ makes an reasoning error.We use the following criterion to access and/or evaluate the quality of the rationale for $T_i$:\n$scores(T_i) = Mean(\\{scores_k(T_i)\\}_{k\\in K})$, where $scores_k(T_i) = h_k(T_i)^TR_k - \\delta$.Here \u03b4 is the threshold, scoresk (T\u1d62) is the rationale for the k-th representation space, and scores(T\u1d62)is the average score across all layers in K. When the score is less than 0, it indicates that theactivations of prompt T\u1d62 are far from the representation spaces. See Algorithm 1 for details.\n5.2 REPRESENTATION OF THOUGHT\nThe Hopfieldian view of cognition offers a framework that can potentially be used to control orinfluence cognitive processes. Specifically, influencing neural populations directly offers a more"}, {"title": "6 EXPERIMENTS", "content": "In this section, we will perform experimental studies on the above two applications to verify thecorrectness of our understanding from the Hopfieldian view.\n6.1 EXPERIMENTAL SETUP\nDatasets. Our experiments are performed on benchmark datasets for diverse reasoning problems.We consider 6 datasets for 3 different tasks: Arithmetic Reasoning, Commonsense Reasoning, andSymbolic Reasoning. Specifically, for Arithmetic Reasoning, we select GSM8K (Cobbe et al., 2021)and SVAMP (Patel et al., 2021); we study StrategyQA (Geva et al., 2021) and CommonsenseQA(CSQA) (Talmor et al., 2019) for Commonsense Reasoning; lastly, for Symbolic Reasoning, wechoose the Coin Flip (Wei et al., 2022) and Random Letter datasets, where the latter one is con-structed from the Last Letter dataset (Wei et al., 2022). More details and statistics of the datasets areprovided in Appendix B.1.\nLLMs. We employ Llama-2-7B-Chat (Touvron et al., 2023) and Llama-3-8B-Instruct (Meta, 2024)to evaluate their precision performance (accuracy) both before and after applying RoT to differ-"}, {"title": "7 CONCLUSION", "content": "In this paper, we proposed a novel framework to explain and understand the fundamental factorsbehind CoT's success. Specifically, we first connected CoT reasoning and the Hopfieldian viewof cognition in cognitive neuroscience. Then, we developed a method for localizing reasoning er-rors and proposed the RoT framework to enhance the robustness of the reasoning process in CoTs.Experimental results demonstrate that RoT improves the robustness and interpretability of CoT rea-soning while offering fine-grained control over the reasoning process."}, {"title": "A ADDITIONAL PRELIMINARIES", "content": "The Sherringtonian View. Unlike the Hopfieldian perspective, the Sherringtonian view (Sherring-ton, 1906; Barlow, 1953) of cognitive explanation emphasizes the importance of direct neuron-to-neuron connections in the brain. This view posits that the primary explanation for cognition liesin the specific interactions between neurons and the computations these neurons perform withinwell-defined circuits (Mogenson, 2018).\nAt an algorithmic level, the Sherringtonian view conceptualizes cognition as networks of nodes(neurons) with weighted connections (synapses) between them. In this framework, neurons performdistinct computational transformations on the signals they receive from other neurons in the network.Cognitive processes are described by how individual neurons receive inputs, process these inputsthrough neural transfer functions, and transmit the resulting signals to connected neurons. Thus,cognition is explained through the computations occurring at the level of individual neurons and thesignal flow across their connections.\nZero-shot CoT. Zero-shot CoT is a simple but effective chain of thought (CoT) prompting ap-proach proposed by Kojima et al. (2022). It allows language models to generate a step-by-stepexplanation or thought process to solve problems without requiring prior demonstrations or specifictraining by simply adding \u201cLet\u2019s think step by step\u201d before each answer. Specifically, given a queryq and a model M with weights \u03b8, the generation process can be defined as:\n$R = arg\\ max\\ pr(R|q, t; \u03b8)$where R is the response text of the model, which is generated from all potential responses R, and tis a prompt text like \u201cLet\u2019s think sttep by step.\u201d\nFew-shot CoT. Similar to zero-shot CoT, few-shot CoT (Wei et al., 2022) is also a prompting tech-nique that gives a few examples with step-by-step reasoning processes to stimulate the model\u2019s rea-soning ability. Formally, given the query q and a set of demonstrations D = {($\\bar{q}_1, \\bar{a}_1$), ($\\bar{q}_2, \\bar{a}_2$)},our aim is to generate a response with intermediate reasoning steps:\n$R = arg\\ max\\ pr(R|D, q; \u03b8)$"}, {"title": "B OTHER EXPERIMENTAL DETAILS", "content": "B.1 DATASET\nThe statistics of the data is shown in Table 4. The details about each data set are as follows:\nArithmetic Reasoning. The arithmetic reasoning benchmarks aim to analyze and/or understandthe model\u2019s mathematical reasoning skills. These include: (i) GSM8K Cobbe et al. (2021), a mathword problems benchmark encompassing a variety of reasoning steps; (ii) SVAMP Patel et al.(2021), containing math word problems with multiple structures.\nCommonsense Reasoning. These data sets aim to analyze the ability of the model on common-sense reasoning tasks. These include: (i) StrategyQA Geva et al. (2021), a commonsense benchmarkrequiring multi-level strategy to answer the question; (ii) CSQA Talmor et al. (2019) benchmarkdataset of multiple-choice questions that require different types of commonsense knowledge to pre-dict the correct answers."}]}