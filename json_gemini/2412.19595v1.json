{"title": "SocRATES: Towards Automated Scenario-based Testing of Social Navigation Algorithms", "authors": ["Shashank Rao Marpally", "Pranav Goyal", "Harold Soh"], "abstract": "Current social navigation methods and benchmarks primarily focus on proxemics and task efficiency. While these factors are important, qualitative aspects such as perceptions of a robot's social competence are equally crucial for successful adoption and integration into human environments. We propose a more comprehensive evaluation of social navigation through scenario-based testing, where specific human-robot interaction scenarios can reveal key robot behaviors. However, creating such scenarios is often labor-intensive and complex. In this work, we address this challenge by introducing a pipeline that automates the generation of context-, and location-appropriate social navigation scenarios, ready for simulation. Our pipeline transforms simple scenario metadata into detailed textual scenarios, infers pedestrian and robot trajectories, and simulates pedestrian behaviors, which enables more controlled evaluation. We leverage the social reasoning and code-generation capabilities of Large Language Models (LLMs) to streamline scenario generation and translation. Our experiments show that our pipeline produces realistic scenarios and significantly improves scenario translation over naive LLM prompting. Additionally, we present initial feedback from a usability study with social navigation experts and a case-study demonstrating a scenario-based evaluation of three navigation algorithms.", "sections": [{"title": "I. INTRODUCTION", "content": "In social navigation, robots must navigate through dy-namic human environments while adhering to social norms. This presents a dual challenge: ensuring both competent nav-igation and socially appropriate behavior. Evaluating these aspects is difficult, as social appropriateness is subjective and context-dependent, often requiring human judgment. Current evaluation methods tend to focus on quantifiable metrics like safety and comfort, but these fail to capture the full spectrum of social interactions. If we could automate the generation of socially relevant scenarios, grounded in both location and task, we would significantly enhance the ability to test and refine social navigation algorithms. This would lead to more reliable and robust robots that can function effectively in real-world human environments.\nIn this work, we propose SocRATES (Social Robot Assessment Through Scenario Evaluation) (Fig. 1), a system designed to automate the generation of context-specific sce-narios for evaluating social navigation algorithms. The key insight behind SocRATES is the use of large vision-language models (VLMs) to translate high-level, potentially ambigu-ous inputs into detailed simulation scenarios. By leverag-ing the commonsense reasoning and context-understanding capabilities of these models, SocRATES generates realistic and varied scenarios that capture both the physical layout of the environment and the social interactions that occur within it. We believe this combination of AI-driven scenario generation with simulation makes SocRATES a significant first-step towards testing both the navigation and social appropriateness of robots.\nOur work builds on prior research in scenario-based testing for autonomous systems and social navigation. Tools like SEAN 2.0 [1] and HuNavSim [2] have explored human behavior simulation in social settings, but they remain limited by a narrow set of predefined scenarios and/or lack flexibility or control over scenario generation. In contrast, SocRATES allows users to define scenarios based on textual and image-based inputs, providing much greater control over the evaluation process. Additionally, while prior tools focus primarily on the fidelity of human simulation, SocRATES is designed to evaluate the robot's behavior in relation to both the task and social context. To our knowledge, SocRATES is the first system to integrate LLM-driven scenario generation with simulation for the comprehensive testing of social navigation algorithms.\nOur experimental results demonstrate the effectiveness of SocRATES in generating diverse, contextually relevant sce-narios. We conducted a design analysis that shows the system is fast and cost-effective, producing scenarios in under a minute with minimal cost. A user study with researchers in social navigation validated the system's usability, with all participants preferring SocRATES over manual scenario"}, {"title": "II. BACKGROUND", "content": "A. Evaluating Social Navigation\nEvaluating social navigation is complex because it in-volves two interlinked components: social appropriateness and competent navigation. Comprehensive evaluation re-quires assessing both, yet the subjective nature of social appropriateness makes it particularly challenging [3]. For example, [4] outlines seven key principles that robots should follow to be considered socially appropriate. Most current evaluation methods primarily test two of these principles: safety (P1) and comfort (P2), often using proxemics-based metrics that measure the robot's and human's relative po-sitions and speeds [5]. However, principles like contextual appropriateness (P8) depend on the robot's social context and task, which are often overlooked due to their qualitative nature.\nTo address these challenges, recent approaches have emerged in two main directions. The first uses proxy metrics that model specific aspects of a robot's social awareness. For instance, [6] introduces personal and group space in-trusion metrics, which are subsequently adopted in other algorithms [7] and evaluation frameworks [2]. The second approach involves using human studies to assess social behaviors. For example, legibility has been modeled [8] and used to improve social navigation. Metrics like Envelope of Readiness, Clarity, and Moments of Confusion [9] rely on continuous observer data to quantify legibility. Additionally, prior studies [10] collect qualitative perceptions of social navigation using validated scales, e.g., PSI [11].\nHowever, collecting scalable and informative human study data remains challenging, and existing benchmarks provide limited coverage of edge cases and special scenarios [4]. Crafting specific simulations for testing edge cases is cum-bersome, contributing to the difficulty in gathering useful hu-man evaluation data. These limitations highlight the need for more comprehensive scenario-based testing, where specific behavioral aspects of the robot can be evaluated in controlled settings. Our work addresses this gap by automating the proposal and generation of context- and location-specific scenarios from simple textual and image-based inputs.\nB. Scenario-based Testing for Social Navigation\nAutomated scenario generation for testing has a long history in Advanced Driver Assistance Systems (ADAS) [12]\u2013[14], with recent efforts extending into Human-Robot Interaction (HRI) policy evaluation [15], [16]. However, most current social navigation evaluation tools, such as HuNavSim [2], Arena [17], and SocNavBench [18], focus primarily on the fidelity of human simulation rather than generating specific scenarios that test a robot's navigation and reactive capabilities.\nSEAN 2.0 [1] is one exception that seeks to expose the robot to various social situations. However, SEAN 2.0 models only a limited set of scenarios and environments, offering little control over the scenario itself, which increases the uncertainty of whether a specific user-defined scenario will occur during evaluation. To the best of our knowledge, none of the current tools provide comprehensive simulation of human-robot interactions.\nIn contrast, SocRATES leverages the rich context and commonsense reasoning capabilities of large language mod-els (LLMs), along with a flexible scenario and map param-eterization, to generate a diverse set of scenarios, including human-robot interaction scenarios, in any user-defined loca-tion. This approach enables more comprehensive testing by allowing the user to specify detailed parameters, ensuring that specific scenarios of interest can be generated and evaluated."}, {"title": "III. METHODOLOGY", "content": "Our objective is to develop an easy-to-use and reliable system for automating scenario proposal and generation in social navigation testing. Users provide simple textual and image-based inputs to guide the scenario proposal, which our system then converts into a simulation. Once the simulation is generated, any robot-planner combination based on the ROS2 Navigation stack\u00b9 can be used for evaluation.\nFigure 2 provides an overview of our framework. We structure our system by deconstructing a scenario into five main components: (1) the location where the scenario takes place (Sec. III-A), (2) a detailed description of the scenario (Sec. III-B), (3) the paths for pedestrians and the robot (Sec. III-C), (4) the behavior of the pedestrians (Sec. III-D), and (5) the simulation that realizes these components (Sec. III-E). Each module of SocRATES interactively queries a Vision-Language Model (VLM) to generate elements of the scenario content. The simulation module then executes the scenario using these generated components and the specified navigation algorithm on the robot.\nA. Map Annotation\nSocRATES includes a simple map annotation tool that allows users to provide the necessary contextual information about their desired location. This enables users to generate scenarios in custom simulated environments.\nWe represent a location as a 2D semantic scene graph, which contains both semantic and spatial information. An example scene graph overlaid on the map image for the Small Amazon Warehouse Gazebo world is shown in Fig. 2. Our annotation tool guides users in annotating a location map by adding nodes and edges. The schema for this graph can be defined by the user, with the requirement being that the node and edge types are self-explanatory. For example, edges are associated with semantic types (e.g., 'intersection' and 'hallway').\nThe scene graph serves two main purposes: (a) it provides both semantic and spatial context regarding different areas in the map and how they are connected, and (b) the structure of the scene graph helps to identify and validate paths for pedestrians and the robot, as explained further in Sec. III-C.\nB. Scenario Proposal\nThe scenario proposal module is responsible for translating the user's potentially ambiguous textual inputs into a detailed and precise characterization of the scenario. Inspired by the concept of scenario-cards [4], we ask users to input scenario metadata: (a) the social context in which the robot operates (e.g., \"A quiet old-age home\"), (b) the robot's intended task (e.g., \"Deliver coffee from the kitchen to the Hall\"), (c) an optional rough scenario where the user can describe more scenario details to constrain the system's outputs, and (d) a description of the location.\nWe generate a detailed scenario description grounded in the user's provided location by prompting a Large Language Model (LLM) with the scenario metadata, context about the social navigation task, and the capabilities of the pedestrians in the simulator (e.g., they cannot manipulate objects, only navigate).\nFrom the Vision-Language Model (VLM), we extract the following outputs: a precise scenario description to condition path and behavior generation, a description of the behaviors of humans in the scenario, and the expected behavior of the robot when navigating the scenario, which serves as ground truth for evaluation. We observe that providing example in-puts and corresponding scenarios in the prompt significantly improves the quality of the LLM's responses. Therefore, we include a set of handcrafted examples in the prompt to guide the model's output.\nC. Pedestrian and Robot Path Generation\nThis module generates paths for both pedestrians (with group assignments, if applicable) and the robot, orchestrating their movements to align with the scenario. To ensure consis-tency with the user's provided location, the path generation is conditioned on the scene graph from Sec. III-A. Human and robot paths are represented as trajectories on this graph. The VLM is queried with a structured prompt, which includes: the scene graph and the scene-graph annotated map image, pedagogical examples on interpreting scene graphs and path generation, and the scenario description (from Sec. III-B).\nWe extract paths for each pedestrian and the robot simulta-neously, along with group assignments. We identify specific nodes where the robot encounters each pedestrian, which is useful for timing pedestrian motion in the simulation to ensure the scenario unfolds as intended. A common error by the VLM is the assignment of discontinuous paths on the scene graph. We detect such errors and re-query the LLM when necessary. Despite structured prompts, the VLM may still generate a valid but unsatisfactory path. To address this, this module is interactive, allowing the user to accept, reject, or edit paths using natural language commands (e.g., \"Make the robot's path longer\"), which re-queries the LLM for updates."}, {"title": "D. Pedestrian Behavior Generation", "content": "The behavior generation module translates the behav-ior descriptions from the scenario proposal into encodings compatible with the simulator. We model pedestrians using HuNavSim [2], which supports rich, programmable reactive behaviors through behavior trees (BTs). For each pedestrian, we construct an LLM prompt that includes the BT node library (a list of available behavior tree nodes), the required human behaviors (from Sec. III-B), and pedagogical exam-ples on BT syntax and design rules.\nThe LLM outputs a behavior tree in XML for each pedestrian, which can be directly imported into HuNavSim to control their actions. However, the default behavior trees in HuNavSim do not support interaction between humans and the robot or complex behaviors. To enable interactive scenarios, we implemented additional behaviors allowing humans to gesture towards other agents, as well as recognize gestures. For example, a simulated human can wait until the robot makes a specific gesture or until another pedestrian performs an action.\nE. Simulation\nThis module integrates the generated behaviors and tra-jectories into a simulation environment. The scene graph paths for each pedestrian are transformed into simulator world frame paths using the map parameters and are used as navigation goals for pedestrians and waypoints for the robot planner. Running the simulation through HuNavSim using ROS2 creates a Gazebo instance where the pedestrians follow the specified paths and act according to the behavior trees.\nFor interactions (e.g., gesturing), pedestrians and the robot publish integer-coded gestures to specific ROS2 topics. In our experiments, we found that timing the arrival of pedes-trians and the robot at their interaction points is critical for orchestrating the scenario. Since the robot's motion is fully controlled by its onboard navigation algorithm, we introduce a scenario manager ROS2 node that synchronizes pedestrian movements with the robot's location, ensuring their timely arrival at interaction points, thus increasing the likelihood that the desired scenario plays out correctly."}, {"title": "IV. EXPERIMENTS AND RESULTS", "content": "We evaluated the effectiveness of SocRATES, by analyz-ing its reliability, cost, and design choices. We evaluated its utility as a tool through a usability study with academic researchers and separately with a persona-based assessment.\nA. Design Analysis\nWe assessed our system for reliability, cost and speed. In addition, we conducted an analysis to validate SocRATES structured design:\n\u2022 Reliability. We assessed the reliability of SocRATES for both unguided (no rough scenario) and guided (with rough scenario) generation across two different maps. For unguided generation, we created 20 scenarios in each location, while for guided generation, we generated\n\u2022 Cost. Given that closed-source large models charge based on token usage, we evaluated the total cost of running our system by measuring the combined token length of all prompts, including typical map image inputs (we used the warehouse map). Assuming we accepted the first response for each module, the total input tokens amounted to approximately 15,000 tokens, which costs less than 30 cents per trial using GPT-40.\n\u2022 Speed. To assess the time required to generate a typ-ical scenario, we created 20 scenarios in one location (without interactivity). We measured the wall-clock time needed to generate a scenario. On average, it took 43 seconds to generate a scenario, which is considerably faster than manually-crafting and coding scenarios.\n\u2022 Structured Prompting. To evaluate the importance of our structured prompts and error-handling mechanisms, we compared our system to a simpler, naive system for path and behavior generation. The naive system lacked our context-rich prompts and directly queried a VLM for path and behavior proposals. As in the reliability test, we again generated and evaluated 100 guided scenarios and 20 unguided scenarios with the naive approach (with the same social context, task and scenario inputs as SocRATES). We observed a 30% overall success rate for guided generation and 10% success rate for unguided generation. This result highlights the critical role of structured prompts and error-handling mechanisms in improving the reliability and quality of scenario generation.\nIn summary, SocRATES is cost-effective and efficient, generating scenarios in under a minute with minimal cost. The system achieves a 73% success rate for first-cut gen-eration, with improvement through interactive adjustments. The comparative study with the naive system confirms that structured prompts and error-handling are key to improving accuracy.\nB. Usability Study with Academic Researchers\nWe conducted a user study to better understand the useful-ness of SocRATES as an evaluation tool for social navigation researchers. We engaged five researchers from the National University of Singapore, Yale University, Miraikan, and\n5 scenarios, 10 times each, in two locations (100 total). Each scenario was manually evaluated for simulability, contextual appropriateness, and alignment with the input rough scenario. We observed a 55% success rate for unguided generation and a 73% success rate for guided generation. Most failures were due to incorrect behavior often caused by the LLM using incorrect node selection or ordering and poor trajectories that were too short or inaccurate for the proposed scenario. Note that in this experiment, we did not use interactivity and accepted the first syntactically valid response. It was possible to correct the trajectory-related errors by re-querying the LLM with specific instructions."}, {"title": "C. Case Study via Persona-based Assesssment", "content": "In this section, we demonstrate the usability of SocRATES through a case study and aim to identify potential areas for improvement.\nWe employed a methodology similar to persona-based as-sessment [19], adopting the role of Amy, an HRI researcher. Amy works at a delivery robot company and is interested in understanding how people perceive the behavior of naviga-tion algorithms in social settings. She has a background in Human-Computer Interaction and is familiar with software development and ROS but does not program robots daily. Amy's task is to advise her development team on the social capabilities of three different navigation algorithms:\n\u2022 Model Predictive Path Integral Controller (MPPI) with the NavFn Planners;\n\u2022 Human-aware NavFn-MPPI (HA-MPPI), which aug-ments the above with costmaps [20] that penalize en-tering the social zone of nearby humans;\n\u2022 Nav2Can [21], which uses costmaps to penalize passing through groups and entering the social zone of nearby humans.\nAmy's goal is to determine if there is a socially preferred algorithm and how each performs in specific scenarios. Her task involves:\n\u2022 Using SocRATES to create four distinct scenarios and generate videos of the robot's behavior;\n\u2022 Collecting feedback from an online pool of participants;\n\u2022 Analyzing and reporting the results.\nWe generated four scenarios in which the robot had to:\n1) Enter an elevator in a multi-floor hospital;\n2) Navigate a corridor in an office environment with humans following and obstructing its path;\n3) Navigate an intersection where humans suddenly ap-pear from a room in an office;\n4) Navigate past a group of employees in a warehouse.\nAdopting the Amy persona, we found scenario generation using SocRATES to be straightforward. For example, to generate the elevator scenario, Amy used the prompt: The Robot approaches an elevator to go to a different floor to deliver supplies. The elevator opens and has a few people in it. 2 people leave the elevator while one of them is startled by the robot and keeps looking at it and doesn't leave the elevator. The complete prompts used are available in our online repository7. On average, each scenario took 10 minutes to complete. The most difficult scenario to craft was the group scenario in the warehouse, where the generated trajectories were suboptimal in that the humans were placed in an unrealistic group formation and specific behaviors of the humans were triggered too early. As such, it was necessary to modify the trajectories and behaviors manually (though, this was easier than creating the trajectories from scratch). Annotating the hospital map also required finding a balance between very specific trajectories and giving the navigation planner freedom to choose alternate paths. These observations highlighted areas for improvement in future versions of SocRATES.\nAfter generating and visualizing the scenarios in Gazebo, Amy easily recorded videos for use in a questionnaire. Snapshots of the videos for selected scenarios are shown in Fig. 4. She selected questions from the PSI [11] and added questions about social navigation principles [4], focusing on predictability, contextual appropriateness, and proactivity.\nAmy surveyed 40 participants per scenario (adults with no vision disabilities) through Prolific- note that we actually conducted this study as part of our persona-based assess-ment. The results revealed significant differences between the algorithms in the scenarios. For example, as shown in Fig. 3, the basic MPPI algorithm received higher Perceived Social Intelligence (PSI) scores than Nav2Can in the corridor scenario, while Nav2Can performed better in the warehouse scenario. This was reinforced by participants comments: \u201cI think it [MPPI] was very effective as it got through the corridor at a good speed without being too much of an issue for people passing through.\u201d, \u201c[Nav2Can] moved out of the way to go around the group of people. [MPPI] and [HA-MPPI] drove right into the group of people, which could cause accidents in the warehouse.\" This led Amy to suggest that different algorithms might be more suitable for different social contexts, or further development was needed on general social navigation.\nTo summarize, this study affirmed the findings from our design validation and usability studies. Without SocRATES, we estimate that Amy would have spent approximately a month conducting this study manually, compared to 4 days with SocRATES. The study also revealed areas for improve-ment, such as in trajectory generation and modification."}, {"title": "V. CONCLUSION", "content": "In this work, we introduced SocRATES, an automated sys-tem for generating diverse, contextually rich social naviga-tion scenarios in simulated environments. Through usability studies with researchers and a persona-based assessment, we demonstrated the system's effectiveness and ease of use. SocRATES marks an important step towards scalable, scenario-based testing and benchmarking for social naviga-tion algorithms. By incorporating human evaluations along-side scenario-based testing, we aim to complement existing proxemics-focused benchmarks and enable the evaluation of subjective, hard-to-define social metrics.\nBuilding on the feedback from our user study, we plan several improvements to enhance the system's robustness and user experience. First, LLMs do make reasoning errors and we plan to explore fine-tuning models specifically for scenario generation to improve reliability. Additionally, the current requirement for users to manually annotate maps can be time-consuming, so we plan to automate this process and include procedurally generated environments based solely on social context and location descriptions. Finally, we aim to develop a validation module to ensure that the generated scenarios accurately reflect the user's input. With these enhancements, SocRATES has the potential to significantly advance the testing and evaluation of social navigation algo-rithms, providing a scalable, flexible platform for assessing both technical performance and social behavior."}]}