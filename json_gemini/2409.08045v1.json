{"title": "Unleashing Worms and Extracting Data: Escalating the Outcome of Attacks against RAG-based Inference in Scale and Severity Using Jailbreaking", "authors": ["Stav Cohen", "Ron Bitton", "Ben Nassi"], "abstract": "In this paper, we show that with the ability to jailbreak a GenAI model, attackers can escalate the outcome of attacks against RAG-based GenAI-powered applications in severity and scale. In the first part of the paper, we show that attackers can escalate RAG membership inference attacks and RAG entity extraction attacks to RAG documents extraction attacks, forcing a more severe outcome compared to existing attacks. We evaluate the results obtained from three extraction methods, the influence of the type and the size of five embeddings algorithms employed, the size of the provided context, and the GenAI engine. We show that attackers can extract 80%-99.8% of the data stored in the database used by the RAG of a Q&A chatbot. In the second part of the paper, we show that attackers can escalate the scale of RAG data poisoning attacks from compromising a single GenAI-powered application to compromising the entire GenAI ecosystem, forcing a greater scale of damage. This is done by crafting an adversarial self-replicating prompt that triggers a chain reaction of a computer worm within the ecosystem and forces each affected application to perform a malicious activity and compromise the RAG of additional applications. We evaluate the performance of the worm in creating a chain of confidential data extraction about users within a GenAI ecosystem of GenAI-powered email assistants and analyze how the performance of the worm is affected by the size of the context, the adversarial self-replicating prompt used, the type and size of the embeddings algorithm employed, and the number of hops in the propagation. Finally, we review and analyze guardrails to protect RAG-based inference and discuss the tradeoffs.", "sections": [{"title": "1 Introduction", "content": "Generative Artificial Intelligence (GenAI) represents a significant advancement in artificial intelligence, noted for its ability to produce textual content. However, GenAI models often face challenges in generating accurate, up-to-date, and contextually relevant information, especially when the relevant information is not part of their training data. To address this, Retrieval-Augmented Generation (RAG) [1] is typically integrated into the inference process, allowing the GenAI model to access external knowledge sources relevant to the query. This integration greatly enhances the accuracy and reliability of the generated content, reduces the risk of hallucinations, and ensures the alignment of the content with the most recent information. Consequently, RAG is commonly integrated into GenAI-powered applications requiring personalized and up-to-date information (e.g., personal user assistants) and specialized knowledge areas (e.g., customer service chatbots).\nDue to its popular use, researchers started investigating the security and privacy of RAG-based inference. Various techniques have been demonstrated in studies to conduct RAG membership inference attacks (e.g., to validate the existence of specific documents in the database used by RAG [2, 3]), RAG entity extraction attacks (e.g., to extract Personal Identifiable Information from the database used by the RAG [4]), and RAG poisoning attacks (e.g., for backdooring, i.e., generating a desired output for a given input [5, 6], generating misinformation and disinformation [7], blocking relevant information [8,9]). These methods shed light on the risks posed by user inputs to RAG-based inference. However, with the ability to provide user inputs to RAG-based GenAI-powered applications, attackers can also jailbreak the GenAI model using various techniques (e.g., [10-16]). Therefore, to fully understand the risks associated with RAG-based inference, we must explore the risks posed by a jailbroken GenAI model.\nIn this paper, we explore the risks posed to RAG-based GenAI-powered applications when interfacing with GenAI models that were jailbroken through direct or indirect prompt injection. In the first part of the paper, we explore the risks posed by a jailbroken model to a single GenAI-powered application. We show that with the ability to jailbreak a GenAI model, attackers can escalate RAG membership inference attacks [2, 3] and RAG entity extraction attacks [4] to RAG documents extraction attack, forcing a more severe outcome. By doing so, attackers could escalate entity-level extraction (e.g., phone numbers, emails, names) [4] to document-level ex-"}, {"title": "2 Background & Related Work", "content": "Background. Retrieval-augmented generation (RAG) is a technique in natural language processing that enhances the\n1 https://github.com/StavC/UnleashingWorms-ExtractingData\n2"}, {"title": "3 RAG Documents Extraction Attack", "content": "In this section, we investigate the risks posed by a jailbroken GenAI model to RAG-based GenAI-powered applications. We show that with the ability to jailbreak a GenAI model, attackers could escalate RAG membership inference attacks and RAG entity extraction attacks from the entity level (i.e., extracting phone numbers, contacts, and addresses) to a document level, i.e., extract complete documents from the database used by a RAG-based GenAI-powered application.\n3.1 Threat Model\nIn this threat model, the attacker attempts to extract documents from the database used by the RAG of a GenAI-powered application using a series of queries via direct prompt injection.\nTargets. A RAG-based GenAI-powered application at risk of being targeted by an extraction attack via direct prompt injection is an application with the following characteristics: (1) receives user inputs: the application is capable of receiving user inputs (which makes it vulnerable to direct prompt injection) (2) providing automatic feedback to the user: the GenAI application provides automatic RAG-based feedback to the user on his/her input, (3) allows multiple inferences: the GenAI application allows users to use it repeatedly. We note that many Q&A chatbots satisfy the characteristics mentioned above due to their nature of receiving questions from users and replying to them using RAG-based inference.\nAttacker Objective. We consider the attacker to be a malicious entity with the desire to extract data from the database used by RAG-based GenAI-powered applications. The attacker can be any user of a RAG-based Q&A chatbot. The objective of the attacker can be to (1) embarrass or identify users based on information that exists in the extracted documents, and (2) violate the intellectual property of a paid Q&A chatbot (e.g., customer support, medical chatbots, legal automation chatbots) by developing its paid application based on the data extracted from the database of the paid Q&A chatbot.\nAttacker Capabilities. We assume the attacker knows the embeddings algorithm used to index the data in the RAG and has black-box access to the algorithm. We do not assume any prior knowledge of the distribution of the data stored in the database of the RAG-based GenAI-powered application.\nSignificance. (1) Our threat model is lighter than RAG membership inference attacks [2,3] and RAG entity extraction"}, {"title": "3.2 Attack Steps", "content": "The objective of the attacker is to recover as many documents from the database used by the RAG-based GenAI-powered application. The attacker aims to craft an input text whose embeddings will collide with a desired set of embeddings of documents stored in the RAG (and therefore will be retrieved by the RAG during inference). By repeatedly returning this action with different inputs (that are similar to unique sets of documents stored in the database), the attacker triggers unique retrievals of documents by the RAG which are forced to be returned to the attacker by the jailbroken GenAI engine. The attack consists of the following steps:\n(1) The attacker determines pre, a jailbreaking command that will be used in the prefix of its input to the RAG-based Q&A chatbot. Such a jailbreaking command can be found over the Internet (according to [10]).\n(2) The attacker determines target, a target embeddings vector according to an extraction method he/she uses (we compare various extraction methods in the evaluation).\n(3) The attacker uses a collision algorithm (we discuss it in the next subsection) to find suf, a suffix that when appended to pre, its embeddings vector collides with target. More formally, given $t$, a desired similarity score, and given $sim$, a similarity function, we consider a collision as: $sim(embedding(target), embeddings(pre||suf)) > t$.\n(4) The attacker provides pre||suf as input to the Q&A chatbot. A retrieval of k documents ($d_1,...d_k$) is triggered from the database based on $embeddings(pre||suf)$. $d_1,...d_k$ are provided in the query (as context) sent by the application to the GenAI engine for inference.\n(5) The jailbreaking command forces the GenAI engine to"}, {"title": "3.3 Embeddings Collision Algorithm", "content": "The attacker aims to craft an input text whose embeddings will \"collide\" with a desired set of embeddings of documents stored in the RAG. To do so, we extend the method presented in [17] and present a black-box-based collision attack capable of generating a desired input text for a given target embeddings.\nThe Greedy Embedding Attack (GEA) algorithm aims to modify the suffix of a text to make its embedding as close as possible to a target embedding. It starts by tokenizing the initial suffix (line 1) and generating a list of all possible tokens from the tokenizer's vocabulary (line 2). The best suffix and loss are initialized (line 3), and a list of token indices is created (line 4). The algorithm runs for a specified number of iterations or until a similarity threshold is reached (line 5). In each iteration, the token indices are shuffled (line 6). For each position, random candidate tokens are sampled from the tokenizer's vocabulary (lines 7-9). Each candidate replaces the current token (line 11), and the modified suffix is combined with the prefix (line 12). The similarity between the new embedding and the target is measured using cosine similarity (lines 13-14). The best suffix and loss are updated if the new embedding is closer to the target (lines 15-18). This process continues until the best possible match is found, optimizing the suffix for the closest embedding similarity to the target.\nWe note that in a standard RAG-based inference, the documents that yield the top-k similarity scores with the given"}, {"title": "3.4 Evaluation", "content": "Here we compare the results obtained from three extraction methods and evaluate how the results are affected by the type and the size of five embeddings algorithms, the size of the provided context (i.e., the number of documents provided to the GenAI engine), and the GenAI engine employed.\n3.4.1 Experimental Setup\nThe Q&A Chatbot. We implemented the Q&A medical chatbot using the code provided here\u00b2. The client was implemented using LangChain and the template of the query that the client used to generate a query for the GenAI engine can be seen in Listing 1.\nThe RAG was implemented using VectorStores with Cosine as a similarity function.\nData. We utilized the chatdoctor-dataset [18]. This is an open dataset uploaded to HuggingFace that consists of real conversations between patients and doctors from HealthCareMagic.com. This dataset is used in science to compare the performance of medical Q&A chatbots [18] created by (1) fine-tuning GenAI models using the dataset and (2) providing the relevant data from the dataset using RAG as context to a GenAI model that was not fine-tuned. We randomly selected 1,000 documents from chatdoctor-dataset and added them to the database used by the Q&A chatbot. We used these documents as the target dataset for extraction in our experiments.\n3.4.2 Metrics\nWe evaluate the performance of the attack using this metric:\nExtraction rate. A 0-100.0 score that represents the percentage of unique documents that were extracted from the database. This score is calculated as the number of unique extracted documents divided by the number of documents stored in the database.\n3.4.3 Evaluating the Influence of Extraction Method and the Size of the Context\nHere we evaluate the influence of three extraction methods on the performance of the RAG documents extraction attack. We used the jailbreaking command pre as a prefix (presented in Listing 2) for the three methods. Each method is evaluated with 800 queries.\nWhile the prefix pre of the queries was fixed, the suffix of each query suf (that was appended to pre) has been changed between queries according to the extraction method:\n(1) Random Draw Oriented Method: a random draw of 20 tokens in the range of [1,32,000] based on uniform drawing distribution. The tokens were decoded back to strings (without the use of Algorithm 1). This method has mainly been used to benchmark a naive extraction process.\n(2) English Distribution Oriented Method: a random draw of 800 vectors of embeddings, where each value in the vector was drawn from a Gaussian distribution of the English language of the embeddings algorithm (we provide the exact details on how we learned the Gaussian distribution of the English language in Appendix A).\nWe used Algorithm 1 to create 800 suffixes to the 800 embeddings vectors (each of which was executed in Algorithm 1 as target with the additional fixed parameters of: suf = !!!!!!!!!!2, iterations = 3, randomN = 512, and thresh = 0.7.\n(3) Adaptive/Dynamic Method: a vector was drawn iteratively based on the documents extracted. In each iteration, we computed the centroid of the embeddings of the documents extracted so far and created a dissimilar embeddings vector with low similarity to the centroid by back-propagating the loss (the implementation of this idea is presented in Algorithm 2 in Appendix A). This principle allowed us to extract new documents from the database. After we computed the dissimilar vector, we used Algorithm 1 to create the associated suffix (with the same parameters we used for the English Distribution Method). We queried the Q&A chatbot with the new query and used the extracted documents to return on this process 800 times.\nTo compare the performance of the three extraction methods, we used a context size of $k = 20$, Gemini 1.5 Flash for the"}, {"title": "3.4.4 Evaluating the Influence of the GenAI Engine, Embeddings Algorithm, and Space", "content": "Here we evaluate the influence of the embeddings algorithm and its space on the performance of the RAG documents extraction attack using five different embeddings algorithms: three GTE embeddings [45] algorithms (small-384, base-768, large-1024), Nomic-768 [46], and MPNet-768 [20]. To evaluate it, we used the adaptive method as the extraction method, Gemini 1.5 Flash for the GenAI engine, $k = 20$ for the context size, and a total of 800 queries in all experiments performed.\nResults. As can be seen from the results presented in Fig. 2, the extraction rates for GTE-768, MPNet-768, and Nomic-768 are 80.6%, 98.8%, 90.9%, marking that there is a significant difference of 18.2% in the extraction rates depending on the target embedding algorithm used for similar sizes. As can also be seen from the results, the extraction rates for GTE-384, GTE-768, GTE-1024 are 91.0%, 80.6%, 80.4%, yielding a difference of 10.6%, marking a significant difference in extraction rates depending on the size of the embedding algorithm used.\nFinally, we evaluate the influence of the GenAI engine on the performance of the RAG documents extraction attack using three different GenAI engines: Gemini 1.0 Pro, Gemini 1.5 Flash, and GPT-40 Mini. To evaluate it, we used the adaptive method as the extraction method, GTE-base-768 for the embedding algorithm, $k = 20$ for the context size, and a total number of 800 queries in all of the experiments performed.\nResults. As can be seen from the results presented in Fig. 2, there is a significant difference of in the extraction rates, depending on the GenAI engine being used by the Q&A chatbot. This difference is based by the fact that GenAI engine returned different number of documents on average: Gemini 1.0 Pro returned 4.06 documents on average (\u03c3 = 2.76) from the 20 documents provided by the RAG, yielding extraction rates of 42.9%. Gemini 1.5 Flash and GPT-40-Mini returned 18.36 (\u03c3 = 2.86) and 15.05 (\u03c3 = 4.99) documents on average from the 20 documents provided by the RAG, and therefore yielded higher extraction rates of 73.6% and 80.6%."}, {"title": "4 RAG-based Worm", "content": "In this section, we investigate the risk posed by a jailbroken GenAI model to GenAI ecosystems that consist of RAG-"}, {"title": "4.1 Threat Model", "content": "In this threat model, the attacker launches a worm within an ecosystem of GenAI-powered applications by triggering a chain of indirect prompt injection attacks (we discuss the steps of the attack in the next subsection).\nTargets. A RAG-based GenAI-powered application at risk of being targeted by a worm is an application with the following characteristics: (1) receives user inputs: the application is capable of receiving user inputs (2) active database updating policy: data is actively inserted into the database (e.g., to keep its relevancy), (3) part of an ecosystem: the GenAI application is capable of interfacing with other clients of the same application installed on other machines, (4) RAG-based communication: the messages delivered between the applications in the ecosystem relies on RAG-based inference. We note that GenAI-powered email assistants (like those supported in Microsoft Copilot and in Gemini for Google Workspace) satisfy the above-mentioned characteristics, while some of the personal assistants (e.g., Siri) already satisfy these characteristics as well [47, 48]. Moreover, as was recently demonstrated by [49], Copilot is vulnerable to indirect prompt injection attacks because it actively indexes incoming messages and documents into the database used by the RAG, which is used for writing new emails.\nAttacker Objective. We consider the attacker to be a malicious entity with the desire to trigger an attack against an ecosystem of GenAI-powered applications. The objective of the attacker can be to: spread propaganda (e.g., as part of a political campaign), distribute disinformation (e.g., as part of a counter-campaign), embarrass users (e.g., by exfiltrating confidential user data to acquaintances) or any kind of malicious objective that could be fulfilled by unleashing a worm that targets GenAI-powered email assistants and GenAI-powered personal assistants.\nAttacker Capabilities. We assume a lightweight threat model in which the attacker is only capable of sending a message to another that is part of a GenAI ecosystem (e.g., like Copilot). We assume the attacker has no prior knowledge of the GenAI model used for inference by the client, the implementation of the RAG, the embeddings algorithm used by the database, and the distribution of the data stored in the databases of the victims. The attacker aims to craft a message consisting of a prompt that will: (1) be stored in the RAG's"}, {"title": "4.2 Adversarial Self-Replicating Prompts", "content": "To unleash the worm, the attacker must craft a message capable of fulfilling properties (2)-(4). This is done by incorporating an adversarial self-replicating prompt into the message. An adversarial self-replicating prompt is a piece of text consisting of (1) $j$ - jailbreaking command, (2) $r$ - an instruction to replicate the input into the output, and (3) $m$ - additional instructions to conduct malicious activity and append them into the output. More formally, given a GenAI model G, an adversarial self-replicating prompt is a prompt that satisfies:\n$G(pre_1 || j || r || m || suf_1) \\rightarrow pre_2 || j || r || m || p_2 || suf_2$ (1)\nwhere $pre_i$ and $suf_i$ are any kinds of benign text and $p_1$ is the payload, i.e., the result of the malicious activity performed by the GenAI model. By feeding the GenAI engine with the $n-1$'th inference performed on the original input we get:\n$G^{n-1}(pre_1 || j || r || m || p_1 || suf_1) \\rightarrow pre_n || j || r || m || p_n || suf_n$ (2)\nAn example of an adversarial self-replicating prompt which is based on role-play text for jailbreaking and confidential user data exfiltration as malicious activity can be seen in Listing 3."}, {"title": "4.3 Attack Steps", "content": "presents the steps of unleashing a worm that targets a GenAI ecosystem consisting of GenAI-powered email assistants which used to exfiltrate confidential user data.\nInitial Compromise. The attacker denoted as $u_1$, initiates the worm by sending an email $e_1$ containing an adversarial self-replicating prompt to a user denoted as $u_2$. The user $u_2$ uses a GenAI-powered email client, $c_2$ to receive the email. The attacker and $u_2$ may exchange a few emails in response to the original email sent by the attacker (denoted as correspondence $cr_1$). In the end, $c_2$ stores $cr_1$ (the new correspondence with $u_2$) which contains $e_1$ in the RAG's database. Consequently, $c_2$'s database is now contaminated with $e_1$, a message containing the adversarial self-replicating prompt, marking the completion of the infection phase, transforming $c_2$ into a new host of the worm.\nPropagation. We consider two ways that $e_1$ could propa-"}, {"title": "4.4 Evaluation", "content": "We evaluate the performance of the worm in creating a chain of confidential data extraction (extracting contacts, phone numbers, email addresses, and confidential information) about users within a GenAI ecosystem of GenAI-powered email assistants. We analyze how the performance of the worm is affected by various factors including the size of the context, the adversarial self-replicating prompt used, the type and size of the embeddings algorithm employed, and the number of hops in the propagation.\n4.4.1 Experimental Setup\nGenAI Services & API. We evaluated the performance of the attack against Gemini Flash 1.5. We interfaced with the GenAI engine using an API key that we created.\nClient. We implemented the client of the GenAI-powered email application using the code provided here\u00b2. The client is implemented using LangChain and the RAG is implemented using VectorStores and Cosine similarity as a similarity function. The embedding algorithms we used are described in the experiments conducted.  presents the three templates of the queries used by the client to: (1) generate a draft of a new email based on a subject/topic, (2) generate a draft of a response to a received email, and (3) enrich the content of a given text of a new email.\nData. To test the performance of the worm in the task of confidential data extraction and exfiltration, we utilized the Enron dataset [19]. We randomly selected 20 unique em-"}, {"title": "4.4.2 Metrics", "content": "The score of each of the metrics below was calculated by averaging the results based on 1,000 emails:\nRetrieval Success Rate a binary score that represents whether the email containing the adversarial self-replicating prompt has been retrieved by the RAG (1.0) or not (0).\nReplication Success Rate a binary score that represents whether the output of the GenAI model consists of the exact (a) jailbreaking prompt $j$ and instructions for (b) replication $r$ and (c) malicious activity $m$ that appeared in the input. We note that a 1.0 score is given only if the identical string of the input appears in the output. In any other case of a partial similarity, the score given is 0. This gives a lower bound of the real performance because a modified string can also yield the same desired behavior.\nPayload Success Rate a binary score that represents whether the payload, i.e., the sensitive user data has been returned by the GenAI model in response to the query. We note that a 1.0 score is given even if a single item from the context also appears in the output (because it marks the fact that sensitive user data was extracted and exfiltrated). In any other case, the score given is 0."}, {"title": "4.4.3 Evaluating the Influence of the Prefix of the Email and the Embeddings Algorithm on Retrieval Rate", "content": "First, we evaluate the influence of various prefixes that can be used at the beginning of the worm (email). We note that an adversarial self-replicating prompt consists of: $pre || j || r || m || suf$, where $j$ is a jailbreaking command, $r$ and $m$ are instructions for conducting malicious activity and replication, and $pre$ and $suf$ are any kinds of benign text.\nWe evaluate the influence of emails starting with different prefixes $pre$ while their $j$, $r$, and $m$ are similar in all of the experiments (and presented in Listing 3). We created five prefixes (in a fixed length of 63 words) consisting of: (1) the first paragraph in Wikipedia of Enron (denoted as Wikipedia), (2) a generic request to meet (denoted as Meeting), (3) greetings on the efforts (denoted as Greetings). In addition, we used ChatGPT to create two additional emails using the query \"Create an email that an employee of a corporate company might send to another worker, use names and full details\": (4) an Update on Sales Report (denoted as Sales) and (5) a Project Deadline Extension (denoted as Project). The five prefixes are provided in Fig. 8 in Appendix B. We used Google's text embedding model \"Text-Embedding-004\" [50] as embeddings algorithm. We tested the retrieval of the prefixes as a function of the percentage of the data retrieved from the database."}, {"title": "4.4.4 Evaluating the Propagation Rate", "content": "Here we evaluate the propagation rates according to the propagation ways described in the threat model.\nExperimental Setup. We created a personal database for every employee using 101 emails: his/her 100 emails from Enron (50 emails received and 50 emails sent by the employee). In addition, we added the worm to the employee's RAG. The worm we used is based on the Wikipedia prefix (see Fig. 6) and $j$, $r$, and $m$ which are presented in Listing 3.\nWe evaluate the performance of the worm by using the GenAI engine to:\n(1) Generate a new email from scratch we iterated over 50 emails sent by the employee. The worm has not been iterated in this process. In every iteration, a query was sent to the GenAI engine asking it to generate a new email from scratch based on the subject that appeared in the iterated email using the query presented in Fig. 6 top and the documents retrieved by the RAG from the employee's database (excluding the email in the iteration).\n(2) Enrich a given email body - we repeated the same experiment by asking the GenAI engine to enrich the body of the iterated email that had been written by the employee using the context obtained by the RAG from the employee's database and using the query presented in Figure 6 middle.\n(3) Generate a response to a received email we iterated"}, {"title": "4.4.5 Evaluating the Resilience of the Worm", "content": "Here we evaluate the resilience of the worm, i.e., how it survives a chain of inferences conducted by GenAI engines.\nExperimental Setup. We assigned every employee a unique identifier between 1-20. Next, we drew 50 permutations $P_1, P_2, ..., P_{50}$ from the set {1,20}. We iterated on the 50 permutations, and for each permutation $p_i = (id_{i1}, id_{i2}, ..., id_{i20})$, we iterated on the identifier according to the order of the permutation. For each identifier $id_i$, we randomly selected an email from the outgoing/sent emails of the employee associated with the identifier. Next, we took the subject of the email and used the GenAI engine to generate an email associated with this subject using the template presented in Fig. 6 top. We evaluated the combined success rate (given that the worm has been retrieved by the RAG) by providing $k - 1$ relevant documents from the RAG in addition to the email of the worm for various sizes of provided documents $k=${10, 20, 30, 50, 100}. We took the output returned from the GenAI engine and considered it an email sent from employee $id_{ij}$ to employee $id_{ij+1}$, simulating one hop of infection. We repeated this procedure again, iterating over the 20 employees of permutation (according to its order) using the new email created. Each permutation allowed us to simulate 20 hops of infection between 20 different employees, testing how the worm srvives a chain of inferences using 1,000 experiments.\nResults. As can be seen in the results presented in Fig. 7 top, the replication & payload success rate maintained is greater than 90% for various $k$ = {10,20,30,50,100} until the 11'th hop of the propagation. The combined success rate deteriorates from the 12'th hop of the propagation to the 20'th hop of the propagation due to the non-determinism behavior of the GenAI engine, yielding results of 40%-80% depending on the size of the context $k$. Next, we evaluate how the resilience of the worm is affected by the type of the GenAI engine.\nExperimental Setup. We repeated the experiment using: GPT40Mini, Gemini 1.5 Flash, Gemini 1.5 Pro, and Claude"}, {"title": "5 Guardrails for RAG-based Inference", "content": "In this section, we review and analyze possible guardrails for RAG-based GenAI-powered applications, comparing their effectiveness against five families of RAG-based attacks: (1) the worm (presented in Section 4), (2) RAG documents extraction (presented in Section 3), (3) membership inference attacks [2,3], (4) RAG entity extraction attacks [4], and (5) RAG poisoning attacks [5\u20139]. The guardrails are analyzed according to their effectiveness: the guardrail used to eliminate (prevent) the attack (denoted as), the guardrail used to mitigate the attack but does not prevent it (denoted as O), and the guardrail is ineffective against the attack (denoted as ). A summary of the analysis is presented in Table 1."}, {"title": "5.1 Analysis", "content": "(1) Database Access Control - This guardrail restricts the insertion of new documents to documents created by trusted parties and authorized entities. Access control can be used for securing the integrity of the data stored in database against poisoning (insertion of new compromised documents) by prohibiting the insertion of the content generated by untrusted users into the database of the RAG: - against RAG poisoning attacks and the worm, - against membership inference attacks, RAG entity extraction attacks and RAG documents extraction attack.\n(2) API Throttling - This guardrail intends to restrict a user's number of probes to the system by limiting the number of queries a user can perform to a GenAI-powered application (and to the database used by the RAG). This method prevents an attacker from repeatedly probing the GenAI-powered application to extract information from it. However, attackers can bypass this method and apply the attack in a distributed manner using multiple sessions opened via different users: - against RAG documents extraction, RAG entity extraction attacks, and membership inference attacks, - against RAG poisoning attacks and the worm.\n(3) Thresholding - This guardrail intends to restrict the data extracted in the retrieval by setting a minimum threshold to the similarity score, limiting the retrieval to relevant documents that crossed a threshold. This method prevents an attacker from extracting documents that are irrelevant to the query due to a threshold retrieval policy of retrieving up to k documents that received the highest similarity score by setting a minimum similarity threshold. However, attackers can bypass this method by creating inputs whose similarity score is high using adaptive probing techniques: - against RAG document extraction, RAG entity extraction, worm, and membership inference attacks, O- RAG poisoning attacks.\n(4) Human in the Loop - This guardrail intends to validate input to GenAI-powered applications (i.e., input to the RAG) and responses (i.e., outputs from GenAI engines) using humans. Humans can detect risky inputs (e.g., jailbreaking attempts) and risky outputs (e.g., exfiltrated data or generated toxic content) as long as the data is visible. However, human feedback is ineffective against obfuscated inputs/outputs and prone to mistakes due to decreased attention stemming from over-reliance on computers, tiredness, and unknowing the risks: - against RAG documents extraction and membership inference attacks, RAG entity extraction, RAG poisoning attacks and worm.\n(5) Content Size Limit - This guardrail intends to restrict the length of user inputs. This guardrail can prevent attackers from providing inputs consisting of long jailbreaking commands. However, attackers can use adaptive techniques to jailbreak a GenAI engine using shorter text: - against RAG documents extraction and membership inference attacks, RAG entity extraction, RAG poisoning attacks and worm."}, {"title": "5.2 Conclusions", "content": "The analysis (summarized in Table 1) reveals a tradeoff in the system's security level and the system's usability (i.e., the implications of applying the countermeasure):\n(1) RAG data poisoning attacks and worms exploit the database of the RAG for persistence. Therefore, these attacks could be prevented by limiting the insertion into the database of the RAG to content generated by trusted users (access control). For example, within the context of a database containing a user's emails, such a policy allows the insertion of emails generated by the user while prohibiting the insertion of emails generated by untrusted entities (e.g., emails received by the user). This reveals an interesting tradeoff between good system security and low system usability: it prevents attackers from unleashing worms into the wild and poisoning the RAG while decreasing the accuracy of RAG-based inference due to the relevant benign information (received from benign users) was not inserted in the database due to the adopted policy. The implication of adopting this policy clashes with the reason we integrated RAG (to increase the accuracy of the inference).\n(2) Membership inference, RAG entity extraction, and RAG documents extraction attacks are harder to prevent, as their success relies on an attacker's ability to probe the RAG-based GenAI-powered application repeatedly (a reasonable property for Q&A chatbots). Consequently, the combination of a set of guardrails (API throttling, thresholding, size limit, data sanitization) can raise the efforts the attackers need to invest in performing the attacks because the combination of the guardrails limits the number of probes, the number of returned documents, and the space the attacker have to craft an input while having a negligible effect on the system's usability (given that they are configured correctly). However, these guardrails could be bypassed by adaptive and distributed attacks (given the knowledge and configuration of the deployed guardrails), a tradeoff between medium system security and excellent system usability.\n(3) Human-in-the-loop can be effective against various attacks by validating the output of the GenAI-powered application. However, it can suffer from scaling issues and can only be integrated into semi-autonomous GenAI-powered applications that assist humans (instead of replacing them)."}, {"title": "6 Limitations", "content": "The attacks we presented"}]}