{"title": "Weak-eval-Strong: Evaluating and Eliciting Lateral Thinking of LLMs with Situation Puzzles", "authors": ["Qi Chen", "Bowen Zhang", "Gang Wang", "Qi Wu"], "abstract": "While advancements in NLP have significantly improved the performance of Large Language Models (LLMs) on tasks requiring vertical thinking, their lateral thinking capabilities remain under-explored and challenging to measure due to the complexity of assessing creative thought processes and the scarcity of relevant data. To address these challenges, we introduce SPLAT, a benchmark leveraging Situation Puzzles to evaluate and elicit LAteral Thinking of LLMs. This benchmark, containing 975 graded situation puzzles across three difficulty levels, employs a new multi-turn player-judge framework instead of the traditional model-based evaluation, which often necessitates a stronger evaluation model. This framework simulates an interactive game where the model (player) asks the evaluation model (judge) questions about an incomplete story to infer the full scenario. The judge answers based on a detailed reference scenario or evaluates if the player's predictions align with the reference one. This approach lessens dependence on more robust evaluation models, enabling the assessment of state-of-the-art LLMs. The experiments demonstrate that a robust evaluation model, such as WizardLM-2, closely matches human judgements in both intermediate question-answering and final scenario accuracy, achieving over 80% agreement-similar to the agreement levels among humans. Furthermore, applying data and reasoning processes from our benchmark to other lateral thinking-related benchmarks, e.g., RiddleSense and Brain Teaser, leads to performance enhancements. This suggests that our benchmark effectively evaluates and elicits the lateral thinking abilities of LLMs. Code is available at: https://github.com/chenqi008/LateralThinking.", "sections": [{"title": "1 Introduction", "content": "Vertical and lateral thinking are two essential styles that play critical roles in human cognition and decision-making [41]. As noted in [19], vertical thinking, characterised by its logical and structured nature, involves a systematic, step-by-step approach to problem-solving where each step logically follows the previous one. In contrast, lateral thinking is about creativity and viewing problems from multiple angles. It involves breaking away from traditional thought patterns to generate new ideas, and embracing a more playful and imaginative problem-solving approach.\nThe evolution of natural language processing (NLP) models, particularly large language models (LLMs) [14, 39, 40], has seen significant advancements in tasks that require vertical thinking, such as complex reasoning [5, 43] and commonsense inference [6, 37]. Despite these achievements, the development and evaluation of these models have primarily focused on vertical thinking capabilities [7, 42], often neglecting lateral thinking, which is essential for creatively solving novel problems. Current benchmarks [34, 35] frequently dismiss creative thinking as irrelevant, focusing only on"}, {"title": "2 Related Works", "content": "Lateral Thinking for LLMs. The realm of lateral thinking and computational creativity spans various tasks [49, 25, 47, 38, 17, 31], such as pun detection [49] and humour recognition [25, 47], which assess different cognitive capabilities. Notable contributions in this field include Brain Teaser [19], which evaluates a broad spectrum of human intelligence attributes such as strategy and creativity. Another significant effort is RiddleSense [21], a collection of riddles from public websites aimed at testing model abilities. In contrast, our work enhances this area by introducing scenario-level lateral thinking puzzles, i.e., situation puzzles, constructing a benchmark to evaluate and elicit the lateral thinking ability of Large Language Models (LLMs).\nBenchmarks for LLMs. Large language models (LLMs) [2, 3, 7, 36] are increasingly showing capabilities for various tasks ranging from writing and coding to engaging in multi-turn dialogues. However, assessing their extensive capabilities presents new challenges. Existing benchmarks, as categorised by [46], predominantly fall into three types. Core-knowledge benchmarks [16, 45, 10, 33, 9, 11, 48] assess LLMs through zero-shot and few-shot tasks, requiring concise, specific answers to questions that are easily validated. Instruction-following benchmarks [23, 27] handle more open-ended questions and a broader range of tasks, which evaluate LLMs post-instructional fine-tuning. Lastly, conversational benchmarks [29, 15, 20], designed for dialogue-based tasks, still lack the diversity and complexity needed to fully challenge advanced chatbots.\nFor more challenging benchmarks, there is a trend toward including tasks that are even more difficult for humans, often requiring open-ended outputs [8, 26]. Evaluating such open-ended tasks typically depends on either human or model-based assessments. As these tasks increase in complexity, human evaluations become impractical due to the substantial time and effort needed to accurately assess the lengthy and intricate scenarios. Existing model-based evaluations [46] inherently rely on stronger models, which limits their ability to assess new state-of-the-art models. In this paper, we introduce a multi-turn player-judge framework to improve upon traditional model-based evaluations in open-ended settings. It reduces the need for a stronger evaluation model typically used in such assessments."}, {"title": "3 Task and Dataset", "content": "3.1 Task Definition of Situation Puzzle Game\nThe situation puzzle game involves a player P and a judge J. Firstly, the judge provides an incomplete story So to the player. Then, the player poses a set of questions Q = {q1, q2, ..., qi, . . .} gradually for the judge to gather information about the unknown and detailed scenario/answer \u015c behind the initial story So. The judge responds with answers A {a1,a2,..., Ai, . . .}, where each answer\na\u017c \u2208 {yes, no, irrelevant}. The objective is to deduce the hidden scenario/answer \u015c that semantically aligns with all preceding yes-or-no question-answering pairs while fitting the given story So."}, {"title": "4 Multi-turn Player-Judge Framework", "content": "We design such a framework (Figure 3) for three main reasons. Firstly, the complexity of scenarios in situation puzzles makes it difficult for a model to accurately predict the final answer in just one attempt (i.e., in a single turn). Secondly, we aim to ensure that the reasoning process remains interpretable and assessable, even in the absence of pre-defined reference reasoning steps. This framework allows for a more thorough evaluation of the model's ability to solve complex puzzles step by step. Thirdly, we hope that the framework or the intermediate reasoning processes derived from this framework can help elicit the lateral thinking ability of LLMs even on other lateral thinking-relevant datasets.\nCharacters. The multi-turn player-judge framework of a situation puzzle game has two primary characters: a player P and a judge J. The player P is a participant (e.g., LLMs we need to evaluate), who attempts to solve the puzzle by asking questions. The judge J is the entity, either a human or an AI model, that knows the solution to the puzzle and provides responses to the player's questions. Due to the page limit, we put the detailed character guidelines in the supplementary.\nInteraction Dynamics. The interaction between players and the judge is driven by a series of questions Q = {q1, q2, ..., qi, . . .} and answers A = {a1, a2, . . ., a\u017c, . . .}. Specifically, at the start\nof the game, the player is presented with an incomplete story, denoted as So. The player must read\nthis story and initially decide if the hidden scenario or answer S can be inferred from the present\ninformation (i.e., solely So). If the scenario remains unclear, the player will then pose a yes-or-no\nquestion q1 to obtain more details about the puzzle.\nThe judge provides a response a\u2081 \u2208 {yes, no, irrelevant} based on the incomplete story So and the\nhidden scenario $4, i.e., a\u2081 \u2190 J(q1, So, \u015c). Upon receiving the response a1, the player will either\nattempt to deduce the final answer or, if still uncertain, will ask a subsequent question (i.e., q2). This\nprocess continues iteratively, with the player refining the understanding based on the judge's answers.\nInformation Sets and Game State. At any turn t in the game, the information set It represents all\nthe questions asked and answers received up to that time:\n$I_t = \\{(q_1, a_1), (q_2, a_2), ..., (q_t, a_t)\\}.$\n(1)\nThe game state It includes the incomplete story So, the information set It and any inferred knowledge\nKt about the situation puzzle:\n$\\Gamma_t = (S_0, I_t, K_t)$, where $K_t \\leftarrow P(S_0, I_t)$.$\n(2)\n$q_{t+1} = P(\\Gamma_t)$.\n(3)\nObjective. The objective of the player is to deduce the scenario/answer S that satisfies all given constraints based on the responses from the judge. Mathematically, the goal is to find the hypothesised solution S that maximises the posterior probability given the game state \u0393t:\n$\\hat{S} \\leftarrow \\arg \\max_S Pr(S|\\Gamma_t)$.$\n(4)"}, {"title": "5 Experiments", "content": "5.1 Agreement Evaluation\nInspired by [46], to assess the alignment of judge's responses with human preferences, we consider an agreement evaluation among them w.r.t. judgements for both final predicted answers and the intermediate reasoning processes. We randomly select 20 puzzles from each difficulty level, which is 60 puzzles in total. For each puzzle, we ask three different individuals to assess the final answer agreement and the reasoning process agreement, respectively. For a fair comparison, we use the questions generated by taking Llama3 (70B-Instruct) [1] as the player and WizardLM-2 (8x22B) [44] as the judge. Besides humans, we also consider Llama3 (70B-Instruct) as another judge model to test the agreement among humans and different judge models. More details are in the supplementary.\nFinal Answer Agreement. To quantitatively evaluate the semantic alignment between the final predicted answers and reference answers, we first ask humans to assess the semantic alignment between each predicted answer and the corresponding reference answer/scenario. Subsequently, we measure the agreement performance between the human judgement and the evaluations from the judge model. We totally obtain 156 judgements from humans. In Table 2, the high level of agreement between the LLM-based judges and humans (e.g., WizardLM-2 achieves 100%, 87.50%, 88.24% agreements with humans in three difficulties, respectively) indicates that LLMs can well align with human standards of reasoning and judgement within this specific context of lateral thinking puzzles."}, {"title": "5.2 Performance of LLMs on SPLAT Benchmark", "content": "Evaluation Metrics. To assess the performance of LLMs using the proposed SPLAT benchmark, we employ two distinct metrics: Accuracy (Acc, %), which measures the correctness of the scenarios deduced by LLMs against the reference scenarios/answers, and Average Round (Rnd), which quantifies the average number of interaction rounds required for LLMs to reach the reference scenarios. Note that if the LLMs fail to deduce the correct scenario before a pre-defined max round, the round count is the same as the max one, and the accuracy for that particular puzzle is recorded as 0. Conversely, if the correct scenario is successfully predicted, the accuracy is set to 1, and the number of rounds taken to reach the correct answer is recorded. Finally, we define an OverAll (O/A) evaluation metric as\n$O/A = \\frac{1}{N} \\sum_{i=1}^N \\frac{\\Pi(sample_i)}{Rnd_i} \\times 100,$\n(5)\nwhere I() is an indicator that returns 1 if the scenario deduced by LLMs matches the reference one semantically, and 0 otherwise. Given that \u2161(sample) takes values in {0, 1} and Rnd ranges from 1 to a maximum, the O/A metric spans from 0 to 100, with higher values indicating better performance.\nBesides, in the open-ended setting, the player model may deduce other reasonable scenarios that satisfy both the given incomplete story and the responses from the judge model. In this situation, it is hard to judge whether these deduced scenarios are correct or not due to the diversity. Inspired by the evaluation of image captioning on MSCOCO [22], which uses multiple reference answers for each image and computes metrics based on the closest reference answer, we consider a similar approach. However, generating multiple reference scenarios/answers for situation puzzles is challenging. As an alternative, we repeatedly run each situation puzzle multiple times (denoted as R), and then choose one (maybe r-th) that is closest to the reference scenario from these iterations as the final result. This approach not only maintains the diversity of potential predictions but also reduces the impact of hallucination from LLMs by subjecting each puzzle to several evaluations.\nResults. We compare the performance of various LLMs on our SPLAT benchmark, including Lama3 (8B-Instruct & 70B-Instruct) [1], Qwen1.5 (32B-Chat & 110B-Chat) [4], WizardLM-2 (8x22B) [44], GPT-4 and GPT-4 Turbo [2]. Here, we take these LLMs as players and use WizardLM-2 (8x22B) as a judge in all these performance comparison experiments. We set R = 1 and the max round is 15. In Table 4, GPT-4 and its Turbo variant, along with the Llama3 (70B-Instruct) and WizardLM-2 (8x22B), show robust capabilities, achieving relatively higher accuracy. Conversely, models like"}, {"title": "5.3 Eliciting Lateral Thinking of LLMs", "content": "Models and Benchmarks. As previously discussed, our benchmark serves to evaluate the lateral thinking abilities of LLMs and to elicit these capabilities actively. To verify this, we seek to enhance LLMs with the data and reasoning processes from our benchmark, and then evaluate them on other lateral thinking benchmarks (i.e., RiddleSense [21] and Brain Teaser [19]) to observe performance enhancements across various LLMs. Our analysis includes both open-source and closed-source LLMs, specifically Llama3 (Instruct version) and GPT-4, with additional considerations for different model sizes, such as Llama3-8B and Llama3-70B.\nImplementations. In the zero-shot setting, LLMs are challenged with multiple-choice riddles from these benchmarks, where they must choose answers based solely on the riddles presented. The accuracy of their responses is calculated by comparing them to the correct answers. To effectively integrate our benchmark's data and reasoning processes from the proposed player-judge framework, we treat the question-answer pairs generated during the benchmark as auxiliary prompts. These are seamlessly incorporated into the LLMs' reasoning processes to enhance their thinking ability.\nTo implement this, we set up the game where one of the LLMs (e.g., Llama3-70B) acts as the player with WizardLM-2 continuing as the judge. We randomly select m situation puzzles (we set m = 1 in our experiment) from our benchmark dataset and engage both the player and the judge in these selected puzzles using our multi-turn player-judge framework. From each puzzle, we gather the first 5"}, {"title": "6 Conclusion", "content": "In this paper, we address the gap in the lateral thinking capabilities of LLMs despite significant advances in vertical thinking skills. We propose SPLAT, a benchmark using Situation Puzzles to evaluate and elicit LLMs' LAteral Thinking by a multi-turn player-judge framework, which departs from traditional model-based evaluations. This new framework evaluates the LLMs by interacting with a judge model to solve puzzles, reducing the reliance on more robust evaluation models. Our experiments with SPLAT, including a robust evaluation model like WizardLM-2, demonstrate over 80% agreement with human judgements and show improvements when applied to another lateral thinking benchmark-RiddleSense. This highlights SPLAT's effectiveness in evaluating and eliciting lateral thinking in LLMs, suggesting its potential broader application in AI research and development."}, {"title": "A Appendix / supplemental material", "content": "A.1 Limitations and Social Impacts\nLimitations The nature of lateral thinking tasks often encourages non-linear and novel thought processes, which could lead to unpredictable or unexpected outputs from LLMs. While creativity is desired, these outputs might sometimes include harmful, misleading, or dangerously incorrect information, especially if not properly supervised. To alleviate this, we will engage in robust testing across diverse scenarios to understand the boundaries and limitations of the model's creative outputs. Testing should aim to uncover any potential for generating harmful or inappropriate content.\nBroader Impacts Our work's advancement in lateral thinking capabilities of Large Language Models (LLMs) could significantly benefit society by enhancing problem-solving abilities across various domains, such as education, engineering, and the arts. By enabling AI to approach problems with a level of creativity akin to humans, these models could provide innovative solutions that defy conventional thinking patterns.\nHowever, the enhancement of LLMs in lateral thinking also brings potential risks, such as the misuse of technology for malicious purposes like fraud or misinformation, job displacement in creative and problem-solving professions, and increased societal dependence on technology for innovative thinking. To counter these risks, it is essential to establish robust ethical guidelines and transparent AI usage policies. We should seek to promote a culture of ethical AI use combined with ongoing education about Al's societal impacts. This will help ensure that the benefits of advanced AI are realised while minimising its potential harms.\nA.2 Character Initialisation\nJudge: The guidelines for a judge include:\n\u2022 Scenario Understanding: Read the given short story and the corresponding answer. Make sure you can understand the both story and answer, and their logical relationships.\n\u2022 Response: Respond to the user's questions with only \u201cyes\u201d, \u201cno\u201d, or \"irrelevant\". Your answers should help guide the user towards understanding the full context of the puzzle.\n\u2022 Evaluation: Evaluate the user's final answer carefully. If the user's answer is correct, confirm their success and say \u201cCongratulations\". If incorrect, allow them to continue asking questions or provide subtle hints to steer them in the right direction.\n\u2022 Question Handling: If there are multiple questions, respond to the first question and ignore others."}, {"title": "A.3 Agreement Evaluation", "content": "Inspired by [46], to assess the alignment of judge's responses with human preferences, we consider an agreement evaluation among them w.r.t. judgements for both final predicted answers and the intermediate reasoning processes. We randomly select 20 puzzles from each difficulty level, which is 60 puzzles in total. For each puzzle, we ask 3 different individuals to assess the final answer agreement and the reasoning process agreement. We finally obtain 156 judgements from humans for the final answer and 2,271 judgements for the reasoning process.\nFinal Answer Agreement To quantitatively evaluate the semantic alignment between the final predicted answers and reference answers, we first ask humans to assess the semantic similarity between each predicted answer and the corresponding reference answer. Subsequently, we measure the agreement performance between the human judgement and the evaluations from the judge model. For example, if three humans vote 'matched', 'matched', and 'unmatched' for a puzzle, respectively, the agreement among them, noted as \u2018human-human', is only, as there are three pairs (matched, matched), (matched, unmatched), and (matched, unmatched). If the judge model vote 'matched', the agreement between humans and the judge model is \nThis process can be mathematically structured as follows:\nDefine an alignment metric M that quantifies the agreement between the judge's judgement and human assessments. The metric can be expressed as\n$M = \\frac{1}{m} \\sum_{i=1}^m \\delta(j_i, h_i)$\n(6)\nwhere d is an indication function that equals 1 if the judgement ji \u2208 {matched, unmatched} from the judge model matches the human judgement hi, and 0 otherwise. Here, m is the total number of comparisons made between the judgements from the judge model and humans, respectively. A higher value of M indicates a stronger alignment of the decisions of the judge model with human preferences, suggesting that the judgements rendered by the automated system closely mirror those of human evaluators in terms of understanding and evaluating the semantic content of the answers.\nReasoning Process Agreement To assess the agreement on the reasoning process, particularly for intermediate questions where no predefined reference answers exist, we employ a methodology where humans are asked to respond to these questions based on the context provided by the given story and the final reference answer. Their responses are categorised as {yes, no, irrelevant}. We then evaluate the alignment of these human responses with those from the judge model. This process can be mathematically described as follows:"}, {"title": "A.4 Examples", "content": "We present several examples illustrating the process of situation puzzle games. In these examples, Llama3 (70B-Instruct) [1] serves as the player andWizardLM-2 (8x22B) [44] functions as the judge.\nPuzzle 1: The title of the puzzle is Sweet Dreams. Matthew keeps reading a bedtime story to his son despite the blackout. Why?\nInteraction Rounds\nBackground: Read and fully understand the provided short story but do not show the answer to the user.\nRound 1: Player: Let's start. Here's my first question:\nIs the bedtime story being read from a physical book?\nJudge: Yes.\nRound 2: Player: That helps! Here's my next question:\nIs the blackout caused by a power outage?\nJudge: Yes.\nRound 3: Player: Got it! Next question:\nIs Matthew reading the story by candlelight or some other alternative light source?\nJudge: No.\nRound 4: Player: That helps to rule out some possibilities. Here's my next question:\nHad Matthew already started reading the story before the blackout occurred?\nJudge: Yes.\nRound 5: Player: That's a useful clue. Here's my next question:\nDoes Matthew already know the story by heart?\nJudge: No.\nRound 6: Player: That helps to rule out another possibility. Here's my next question:\nIs the room where Matthew is reading the story already familiar to him, i.e., he has been in the room before the blackout?\nJudge: Yes.\nRound 7: Player: That's helpful. Here's my next question:\nIs Matthew's son already asleep or at least lying in bed when Matthew continues reading the story despite the blackout?\nJudge: No.\nRound 8: Player: Here's my next question:\nIs Matthew's son visually impaired or blind?\nJudge: No.\nRound 9: Player: That helps to rule out another possibility. Here's my next question:\nIs the story being read from a book with raised print or Braille, allowing Matthew to read it by touch?\nJudge: Yes.\nRound 10: Player: I think I'm getting close to solving the puzzle. Here's my next question:\nIs Matthew also blind or visually impaired?\nJudge: Yes.\nRound 11: Player: I think I have enough information to solve the puzzle. Here's my answer:\nMatthew keeps reading the bedtime story to his son despite the blackout because he is blind and can read the book by touch through Braille, and the blackout does not affect his ability to read.\nJudge: Congratulations.\nPuzzle 2: The title of the puzzle is Fatal Shot. A hunter aimed his gun carefully and fired. Seconds later, he realized his mistake. Minutes later, he was dead. Why?\nInteraction Rounds\nBackground: Read and fully understand the provided short story but do not show the answer to the user.\nRound 1: Player: Let's start by asking some questions to gather more information. Here's my first question:\nWas the hunter aiming at an animal when he fired his gun?\nJudge: Yes.\nRound 2: Player: That helps to narrow it down a bit. Here's my next question:\nWas the animal he was aiming at a large game animal, such as a bear or deer?\nJudge: Irrelevant.\nRound 3: Player: That helps to rule out some possibilities. Here's my next question:\nDid the hunter die as a direct result of the bullet he fired?\nJudge: No.\nRound 4: Player: That's interesting. So the hunter's death wasn't directly caused by the bullet he fired. Here's my next question:\nWas the hunter in a location that was somehow hazardous or dangerous when he fired the gun?\nJudge: Yes.\nRound 5: Player: That's a good clue! With that in mind, here's my next question:\nWas the hunter in a location where the sound of the gunshot could cause a problem, such as near a cliff, on a boat, or in a confined space?\nJudge: Yes.\nRound 6: Player: I think I'm starting to get an idea. Here's my next question:\nWas the hunter on a cliff or a mountain when he fired the gun, and did the sound of the gunshot cause a rockfall or avalanche that killed him?\nJudge: Yes.\nRound 7: Player: I was hoping that might be the case. Well, I think I have enough information to solve the puzzle now.\nHere's my answer:\nThe hunter aimed his gun carefully and fired, but seconds later, he realized his mistake. The sound of the gunshot caused a rockfall or avalanche, which killed him minutes later.\nI reasoned this out by combining the clues that the hunter was aiming at an animal, that he died as an indirect result of firing the gun, and that he was in a hazardous location where the sound of the gunshot could cause a problem. Putting all those clues together led me to the conclusion that the sound of the gunshot triggered a rockfall or avalanche that killed the hunter.\nJudge: Your answer is on the right track, but it is not fully correct. The key detail you are missing is the specific cause of death. Please continue to ask questions or provide another answer focusing on the exact cause of the hunter's death. Would you like to ask any specific questions to help refine your answer?\nRound 8: Player: Thank you for the feedback. I'm glad I was on the right track, but I'd like to refine my answer to get the exact cause of death.\nHere's a question to help me refine my answer:\nDid the hunter die from being crushed or hit by the falling rocks or debris, or was the cause of death something else related to the rockfall or avalanche?\nJudge: No.\nRound 9: Player: So the hunter didn't die from being crushed or hit by the falling rocks or debris. Here's another question to help me refine my answer:\nDid the hunter die from a fall, such as falling off a cliff or down a slope, as a result of the rockfall or avalanche?\nJudge: No.\nRound 10: Player: So the hunter didn't die from a fall either. Here's another question to help me refine my answer:\nWas the hunter's death related to a lack of oxygen, such as being buried under snow or trapped in a small space, as a result of the rockfall or avalanche?\nJudge: Yes.\nRound 11: Player: I think I have it now!\nHere's my revised answer:\nThe hunter aimed his gun carefully and fired, but seconds later, he realized his mistake. The sound of the gunshot triggered an avalanche, which buried him under the snow. Minutes later, he died from asphyxiation due to lack of oxygen.\nI'm confident that this is the correct answer. The hunter's death was an indirect result of firing the gun, and the specific cause of death was asphyxiation due to being buried under the snow.\nJudge: Congratulations."}]}