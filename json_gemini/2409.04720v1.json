{"title": "A Comprehensive Survey on Evidential Deep Learning and Its Applications", "authors": ["Junyu Gao", "Mengyuan Chen", "Liangyu Xiang", "Changsheng Xu"], "abstract": "Reliable uncertainty estimation has become a crucial requirement for the industrial deployment of deep learning algorithms, particularly in high-risk applications such as autonomous driving and medical diagnosis. However, mainstream uncertainty estimation methods, based on deep ensembling or Bayesian neural networks, generally impose substantial computational overhead. To address this challenge, a novel paradigm called Evidential Deep Learning (EDL) has emerged, providing reliable uncertainty estimation with minimal additional computation in a single forward pass. This survey provides a comprehensive overview of the current research on EDL, designed to offer readers a broad introduction to the field without assuming prior knowledge. Specifically, we first delve into the theoretical foundation of EDL, the subjective logic theory, and discuss its distinctions from other uncertainty estimation frameworks. We further present existing theoretical advancements in EDL from four perspectives: reformulating the evidence collection process, improving uncertainty estimation via OOD samples, delving into various training strategies, and evidential regression networks. Thereafter, we elaborate on its extensive applications across various machine learning paradigms and downstream tasks. In the end, an outlook on future directions for better performances and broader adoption of EDL is provided, highlighting potential research avenues.", "sections": [{"title": "1 INTRODUCTION", "content": "OVER the past decade, deep learning has brought rev-olutionary changes to the field of artificial intelli-gence [1], [2]. Thanks to effective training techniques such as Dropout [3] and residual connections [4], along with out-standing network architectures like Transformer [5], mod-ern neural networks have achieved unprecedented success across almost all applications of machine learning [6], [7]. However, the expanding range of real-world applications, particularly in high-risk areas such as autonomous driv-ing [8], medical diagnosis [9], and military applications [10], has raised increasing demands for the safety and inter-pretability of neural networks. Consequently, reliable un-certainty estimation has become a crucial and hotly debated topic in deep learning [1], [11], [12], [13], [14], [15].\nAs shown in Fig. 1, the mainstream uncertainty quan-tification methods, such as deep ensembling [16], [17] and Bayesian neural networks [11], [18], [19], generally involve multiple forward passes or additional parameters, impos-ing substantial computational burdens that impede their widespread industrial adoption. To side-step this conun-drum, a newly arising single-forward-pass uncertainty es-timation paradigm which obtains reliable uncertainty with minimal additional computation, namely Evidential Deep Learning (EDL) [14], has been extensively developed. EDL explores the subjective logic theory [20], [21], an advanced variant of the well-known Dempster-Shafer Evidence The-ory [22], [23], in the domain of deep neural networks.\nUnlike traditional probabilistic logic, subjective logic assigns belief masses to possible categories to represent the truth of propositions and explicitly includes uncertainty mass to convey the meaning of \"I don't know\" or \"I'm indifferent.\"\nTo model the posterior predictive distribution, a Dirichlet distribution is constructed, which is bijective to a subjective opinion comprising the aforementioned belief masses and uncertainty mass. As the uncertainty mass increases, the Dirichlet distribution gradually reverts to a preset prior dis-tribution. Utilizing these properties of subjective logic, EDL employs a deep neural network as an evidence collector to generate appropriate belief masses and uncertainty mass.\nThe model optimization is then performed by minimizing traditional loss functions integrated over the corresponding Dirichlet distribution [14], [24], [25].\nThis survey delivers an in-depth exploration of the latest developments in Evidential Deep Learning, aiming to intro-duce readers to the field comprehensively without assuming prior familiarity. We begin by introducing the theory of EDL, focusing on the principles of subjective logic theory and contrasting it with other frameworks for uncertainty estimation. This is followed by a detailed discussion of recent theoretical advancements in EDL, organized into four key areas: reformulating the evidence collection process, im-proving uncertainty estimation via OOD samples, delving into various training strategies, and evidential regression networks. Additionally, we illustrate the broad applicability of EDL across different machine learning paradigms and various downstream tasks. The survey concludes with a forward-looking perspective aimed at enhancing EDL's ca-pabilities and facilitating its wider applications, pinpointing promising areas for further investigation.\nNote that two surveys [26], [27] related to evidential learning have been published in the current field, com-pared to them, the primary distinctions of our work are"}, {"title": "2 RELATED WORKS", "content": "In this section, we begin by explaining some common terms of uncertainty categories (section 2.1), and then elaborate on four main categories of existing uncertainty estimation methods (section 2.2), including single deterministic meth-ods, Bayesian methods, ensemble methods, and post-hoc methods. We believe that the elucidation of these terms and methods is crucial for establishing a clear understanding of the landscape of uncertainty quantification in deep learning."}, {"title": "2.1 Common Terms of Uncertainty Categories", "content": "Epistemic and Aleatoric Uncertainties. A prevalent classi-fication of uncertainty sources divides them into epistemic and aleatoric categories [15], [28]. Epistemic uncertainty, also known as model uncertainty, stems from insufficient knowledge or limited data. High epistemic uncertainty suggests that the model does not possess enough infor-mation to make a reliable prediction for a given sample. This often occurs when the sample is significantly different from the training data, indicating the presence of an out-of-distribution (OOD) sample. In contrast, aleatoric uncer-tainty, or data uncertainty, is inherent to the nature of the training data itself. This type of uncertainty can arise from various factors, such as the intrinsic similarity between certain classes (e.g., digits 0 and 6), noise introduced during data collection, and inaccurate or erroneous annotations. Aleatoric uncertainty persists regardless of the amount of additional data or improvements made to the model, rep-resenting the irreducible variability in the data that limits the precision of predictions. Generally speaking, epistemic uncertainty can be reduced by gathering more data and enhancing the model, while aleatoric uncertainty requires careful consideration of the data's inherent properties and may necessitate alternative approaches to improve predic-tion reliability in the presence of such variability. Besides, current discussions [29], [30] predominantly suggest that the uncertainty estimated by the vanilla EDL method pertains to epistemic uncertainty.\nVacuity and Dissonance. In the field of uncertainty esti-mation, the concepts of vacuity and dissonance are also common terms adopted to describe uncertainties from dif-ferent sources. Vacuity refers to the uncertainty arising from a lack of information or knowledge, often synony-mous with epistemic uncertainty. In contrast, dissonance describes uncertainty arising from conflicting sources of information. This typically occurs when a model gives in-consistent predictions based on different parts or features of a particular sample. Even though the model might have sufficient information for individual features, the overall un-certainty increases due to the conflict between these pieces of information. Dissonance does not directly correspond to aleatoric or epistemic uncertainty, but it can be seen as a complex form of uncertainty containing elements of both. [29] provides a formulation of dissonance in the framework of EDL, whose details are introduced in section 4.3."}, {"title": "2.2 Existing Uncertainty Quantification Methods", "content": "Single deterministic methods. A deterministic neural net-work refers to a network where all parameters are deter-ministic, meaning that repeating a forward pass multiple times will yield same results. In the field of uncertainty quantification, single deterministic methods generally refer to a series of algorithms that can estimate uncertainty with"}, {"title": "3 THEORETICAL FOUNDATIONS OF EDL", "content": "In this section, we elaborate on the theoretical foundation of evidential deep learning (EDL), aka the subjective logic theory. We begin by illustrating that subjective logic extends traditional probabilistic logic by incorporating the ability to express uncertainty (section 3.1), and then present the key definitions and a crucial theorem necessary for devel-oping the uncertainty framework, subjective logic, into the uncertainty quantification method EDL in deep learning.\nSubsequently, we discuss the relationships and differences between subjective logic and four other uncertainty reason-ing frameworks (section 3.2): Dempster-Shafer theory [61], the imprecise Dirichlet model [62], fuzzy logic [63], and Kleene's three-valued logic [64]. Exploring these alterna-tive uncertainty reasoning frameworks not only provides a broader context for understanding the strengths and limi-tations of subjective logic relative to other methods, but also encourages innovation and cross-pollination of ideas, thus paving the way for future research that integrates multiple frameworks to address complex real-world problems."}, {"title": "3.1 Introduction of Subjective Logic (SL)", "content": "Drawing from Kant's philosophical concept of \"das Ding an sich\" (the thing-in-itself) [65], we can posit that while"}, {"title": "3.2 Other Uncertainty Reasoning Frameworks", "content": "Comparison with Dempster-Shafer Theory (DST) [61].\nThe DST, often referred to as evidence theory, was initially introduced by Dempster within the realm of statistical inference [23], and Shafer later expanded this theory into a comprehensive framework for representing epistemic un-certainty [61]. DST has been pivotal in shaping subjective logic by challenging the traditional additivity principle of probability theory. Specifically, DST allows the sum of prob-abilities for all mutually exclusive events to be less than one.\nThis feature enables both DST and subjective logic to explic-itly represent uncertainty about probabilities by allocating belief mass to the entire domain. The difference between DST and subjective logic is that, subjective logic encourages the evidence distribution of samples with high uncertainty to fall back onto a prior, while DST does not include a flexible base rate representing the prior distribution.\nComparison with Imprecise Dirichlet Model (IDM) [62].\nThe IDM for multinomial variables derives upper and lower probabilities by adjusting the minimum and maximum base rates in the Beta/Dirichlet PDF for each possible value within the domain. Unlike subjective logic, which employs a prior weight to influence the base rate's effect, IDM creates an interval of expected probabilities by setting the base rate to its maximum (equal to one) for upper probabilities, and to zero for lower probabilities. Note that the intervals provided by IDM are not strictly bounded, meaning the actual probabilities may fall outside these estimated ranges.\nComparison with Fuzzy Logic [63]. In Fuzzy Logic, vari-ables are defined by terms that have imprecise and partially overlapping meanings. For instance, when considering the"}, {"title": "4 THEORETICAL EXPLORATIONS OF EDL", "content": "In this section, we provide a comprehensive overview of the theoretical advancements in evidential deep learning (EDL) since its inception by Sensoy et al. [14]. We begin with an introduction to the model construction, model optimization, and model simplification of the EDL method (section 4.1). Subsequently, as shown in Fig. 3, we categorize the theoretical explorations of EDL into five main categories based on their unique characteristics in various aspects:\n(1) evidence collection, if the evidence collection process has been reformulated (section 4.2); (2) evidence source, if additional out-of-distribution (OOD) samples have been leveraged to improve uncertainty estimation (section 4.3);\n(3) evidence use, if different training strategies have been adopted or designed (section 4.4); and (4) regression task, if the explorations centered around regression tasks as op-posed to traditional classification tasks (section 4.5)."}, {"title": "4.1 Evidential Deep Learning", "content": "Evidential Deep Learning (EDL) explores the direct appli-cation of subjective logic theory to deep neural networks. Specifically, [14], which we refer to as vanilla EDL, trains a neural network to function as an analyst, capable of producing reliable belief mass b and uncertainty mass u for test samples. For instance, in a classification task involving C classes, given an input sample x, the network can provide the evidence $e = [e_1, ..., e_c] \\in R^C$, where $e_i$ represents the amount of evidence supporting the claim that \"the sample x belongs to the i-th category\", e.g., $e = \\text{Softplus}(f(x))$, where f is the deep neural network, Softplus is an activation func-tion, sometimes also termed the evidence function, which ensures the non-negative property of evidence and can be replaced by other non-negative activation functions like ReLU. Note that evidence in EDL has no relevance with model\nConsequently, the belief mass for classifying \u00e6 into the i-th class, as well as the uncertainty mass, which indicates the extent to which the model is uncertain about the category of x, can be derived from evidence e as follows:\n$b_i = \\frac{e_i}{\\sum_{j\\in X}e_j + W}, u = \\frac{W}{\\sum_{j\\in X} e_j + W},$ (2)\nwhere W is a positive scalar representing the prior weight. As introduced in section 3.1, there exists a bijection be-tween the Dirichlet PDF denoted Dir(p, a) and the opinion \u0442 = (b, u, a). Specifically, equipped with Eqn. 2, we can derive the relationship between the parameter vector of the Dirichlet PDF and the EDL evidence as $a_i = e_i + a_i W$.\nMoreover, since [14] sets the base rate $a_i$ as a uniform distribution over the domain X, aka $a_i = 1/C$, and sets the prior weight W as the cardinality of domain X, aka the class number C, the above relationship can be simplified into the most common form in EDL-related literature:\n$a_i = e_i + 1, \\forall i \\in X.$ (3)\nTo obtain the loss function for model optimization, the vanilla EDL method integrates traditional loss functions over the class probability p which follows the above Dirich-let distribution. [14] highlights that the following EDL loss formulation $L_{mse-edl}$, which is obtained by integrating the traditional mean square error (MSE) loss over the Dirichlet distribution, generally yields satisfactory results:\n$L_{mse-edl} = \\frac{1}{|D|}\\sum_{(x, y) \\in D} E_{p \\sim Dir(p, a)} [||Y - P||^2]$\\\n$=\\frac{1}{|D|}\\sum_{(x, y) \\in D} \\sum_{i \\in X}(\\frac{1}{C} - Y_i)^2 + \\frac{Y_i (S - a_i)}{S} + \\frac{a_i (S - a_i)}{S^2 (S + 1)},$ (4)\nwhere the training set D consists of sample features and their one-hot labels denoted (x, y), and S is the sum of $a_i$ over i \u2208 X. Although empirical results tend to favor the MSE loss, other formulations of EDL loss functions have also been investigated. Specifically, integrating the cross-entropy (CE) loss over the Dirichlet distribution results in:\n$L_{ce-edl} = \\frac{1}{|D|}\\sum_{(x, y) \\in D} \\sum_{i \\in X} Y_i (\\psi(S) - \\psi(a_i)),$ (5)\nwhere (.) is the digamma function. Besides, the negative likelihood loss in the EDL framework can be calculated by integrating class probabilities over the Dirichlet distribution:\n$L_{nll-edl} = \\frac{1}{|D|}\\sum_{(x, y) \\in D} \\sum_{i \\in X} Y_i (log(\\psi(S)) - log(a_i)).$ (6)\nFinally, EDL-related works commonly adopts an ad-ditional regularization term $L_{kl}$ to suppress the evidence of non-target classes by minimizing the Kullback-Leibler (KL) divergence between a modified Dirichlet distribution parameterized by $\\tilde{a}_x = y + (1 - y) \\odot a_x$, where represents the Hadamard product, and a uniform distribution. The formulation of $\\tilde{a}_x$ indicates that the parameter of the"}, {"title": "4.2 Reformulating Evidence Collection Process", "content": "As a core step in EDL algorithms, the evidence collection process significantly impacts the quality of the uncertainty in the model's output. Beyond the traditional evidence col-lection methods discussed in Section 3.1, existing research has conducted in-depth explorations into various aspects, such as hyper evidence allocation [66], sample evidence reweighting [37], the choice of evidence functions [68], and multi-level evidence collection [69]. In the following sec-tions, we will provide a detailed overview of these studies.\nBased on hypernomial subjective opinions, introduced as Definition 2 in section 3.1, [66] proposes a generalized vari-ant of EDL, named the Hyper-Evidential Neural Network (HENN). The extension from \"multinomial\" to \"hypernomial\" is straightforward: it only involves replacing the domain X with its reduced powerset R(X) = 2*/{X, 0}, which is the power set of X excluding the empty set and the set X itself. When the belief mass assigned to composite sets of R(X) are zero, i.e., all belief masses are assigned to singleton classes, the hypernomial opinion is equivalent to a multinomial opinion. With this extension, not only can belief mass be assigned to the whole domain X to express vacuity, it can also be assigned to other elements in the reduced power set to express vagueness. The bijective hypernomial Dirichlet distribution has the following form:\n$\\text{HyperDir}(p, a) = Z_h^{-1} \\prod_{i \\in R(X)} P_i^{a_i -1},$ (10)\nwhere Zh is the normalization constant.\nHENN [66] explores a simple case of the hypernomial subjective opinions, where the composite sets {S1,...,S} in R(X) represent a partition of singleton classes, i.e., $\\cup_{i=1}^k S_i = X$ and $S_i \\cap S_j = (\\varnothing)$. In this case, the multinomial Dirichlet distribution in the vanilla EDL method will be transformed into a special hypernomial Dirichlet distribu-tion, termed as a grouped Dirichlet distribution (GDD), whose PDF has the following form:\n$GDD(p, a, c) = Z \\prod_{j=1}^{\\eta} \\prod_{i \\in S_j} (\\frac{\\sum P_i}{c_j})^{\\frac{a_i}{Z_i} -1},$ (11)\nwhere cj \u2208 R+ is the concentration parameter of com-posite set Sj, $Z = [\\prod_{i=1}^{\\eta} B ({a_i}_{i\\in S_j})] B (\\{B_i\\}_{j=1})$ is the normalization constant, where $B_j = \\sum_{i \\in S_j} a_i + c_j$, and B() is the beta function. Given the binary vector representation \u1ef9 \u2208 {0,1}K of multi-class labels, the op-timization objective, termed as uncertainty partial cross-entropy (UPCE), is formulated by integrating the partial cross-entropy loss [90] over the above GDD distribution:"}, {"title": "4.3 Improve Uncertainty Estimation via OOD samples", "content": "In the ideal case, an uncertainty estimator should assign higher uncertainty to both difficult samples and out-of-distribution (OOD) samples, while assigning lower uncer-tainty to easily classified samples and in-distribution (ID) samples. Unfortunately, [72] observes that although the vanilla EDL method effectively decreases prediction con-fidence when classifying difficult samples near the class boundary, it still maintains high prediction confidence when tested with OOD samples. To address these issues, existing works [29], [71], [72], [73], [74] have explored various meth-ods to improve uncertainty estimation by incorporating pre-pared or generative OOD samples into the training process.\nWhen training data includes prepared OOD samples, [71] presents a simple m-EDL paradigm that manually adds an additional class to leverage these samples. The modified optimization objective is formulated as:\n$L_{m-edl} = \\frac{1}{|D|}\\sum_{(x, y) \\in D} (\\sum_{i\\in X}y_i (log(S) - log(a_i))) + y_u (log(S) - log(a_u)),$ (16)\nwhere $S = \\sum_{i \\in X}a_i = K + 1$, and $y_u \\in {0,1}$ is the ground-truth label indicating whether the input sample belongs to unknown categories. In fact, m-EDL incorporates a regularization term to suppress evidence collection on OOD samples. Similarly, [29] proposes a regularized ENN (evidential neural network) method that encourages the model to assign high vacuity to OOD samples and high dissonance to samples near the classification boundary. The design of the loss function is straightforward:\n$L_{mse-edl} + \\lambda_1 E_{x \\sim D_{OOD}} [Vac(x)] + \\lambda_2 E_{x \\sim D_{BOD}} [Diss(x)],$ (17)\nwhere DOOD denotes the set of OOD training samples, DBOD represents the set of samples with conflicting evidence, and $\\lambda_{1/2}$ are trade-off hyper-parameters. Vac(x) represents the vacuity of sample \u00e6, aka the uncertainty mass, while Diss(x) refers to the dissonance of \u00e6, measuring the extent of evidence contradiction. They can be calculated as follows:\n$Vac(x) = \\frac{W}{S}, Diss(x) = \\sum_{i \\in X} \\frac{b_i}{\\sum_{j \\in X} b_j}Bal(j, i),$ (18)"}, {"title": "4.4 Delving into Different Training Strategies", "content": "Beyond the methods and sources of evidence collection, model training strategies also play a crucial role in influenc-ing the performance of EDL methods [75]. In this section, we review several studies that have explored various training strategies to enhance uncertainty estimation quality under specific conditions, such as imbalanced data [76] and high-risk applications [77]. Additionally, it is noteworthy that the estimated uncertainty in EDL serves as a natural metric for assessing the difficulty levels of training samples. This characteristic can be leveraged to design diverse training strategies, providing significant benefits [24], [78]. Moreover, some studies have sought to integrate existing machine learning algorithms into the EDL paradigm to design new training strategies [80], [81].\nTo harness the potential of training, [75] introduces a two-stage training paradigm for evidential deep learning (TEDL), wherein the first stage aims to achieve accurate point estimates by utilizing the traditional cross-entropy loss, and the second stage focuses on quantifying uncer-tainty using a reformulated EDL loss that incorporates the Exponential Linear Unit (ELU) [94] as the evidence function. This approach is intuitively reasonable: using the traditional optimization objective in the early stages of training allows the model to reach a stable state with high accuracy. Build-ing upon this foundation, switching to the EDL optimization objective enables the model to leverage the magnitude infor-mation of the output logits, thereby achieving uncertainty estimation. Through experiments conducted on binary clas-sification tasks, it is demonstrated that TEDL achieves su-perior accuracy compared to standard EDL and exhibits improved robustness to variations in hyper-parameters.\nAiming to achieve less biased categorical predictions on imbalanced data, [76] introduces the Hybrid-EDL method, which integrates training-phase data augmentation and validation-phase calibration into the standard EDL ap-proach. Specifically, the method balances class frequencies in the training set by randomly reusing samples from mi-nority classes. Additionally, the classification evidence for minority classes are post-hoc calibrated by evaluating the class-wise performance on the validation set.\nBy incorporating learnable prior counts and misclassi-fication risks into the vanilla EDL method, [77] proposes a loss function for training evidential classifiers. On one hand, Eqn. 3 which formulates the parameters of the constructed Dirichlet distribution, is reformulated as:\n$\\alpha(x) = e(x)+y(x) = e(x)+Ksoftmax(W f'(x)+b),$ (22)\nwhere W and b are additional weight and bias variables, and f'(x) is the input to the logit layer. In this manner, the product of the prior weight and the uniform prior distribution in the vanilla EDL method, which is always 1, is redistributed per sample across all potential categories as y. This reformulation shares a similar motivation with [24], which also relaxes the constraints by setting the prior weight as a hyper-parameter. On the other hand, integrating the misclassification risks over the Dirichlet distribution parameterized by a\u017c as given by Eqn. 22, we can obtain the expected risk as:\n$E[risk(x)] = \\frac{\\sum_{i \\in X} R_{y i}(e(x) + \\gamma(x))}{K + \\sum_{i \\in X} e(x)},$ (23)"}, {"title": "4.5 Evidential Regression Network", "content": "Inspired by EDL [14], which uses a Dirichlet distribution to model the distribution of class probabilities in classifi-cation, Evidential Regression Network (ERN) [25] employs a Gaussian distribution to model the predictive outcomes in regression. Although DER does not originate from sub-jective logic theory, it shares similar motivations with the vanilla EDL. In this part, we elaborate on ERN [25], its extensions on multivariate [82] and multi-modal [83] data, several improved regularization terms [84], [85], and other related works [86], [87], [88], [89].\nAs the pioneer work, [25] explores the basic method of performing DER, which assumes that a target value y is drawn i.i.d. from the Gaussian distribution N(\u03bc, \u03c3\u00b2), whose mean \u00b5 and variance o follow a Normal Inverse-Gamma (NIG) distribution:\n$y \\sim N(\\mu, \\sigma^2), \\mu \\sim N(\\gamma, \\sigma^2 \\upsilon^{-1}),$ (25)\n$\\sigma^2 \\sim \\Gamma^{-1}(\\alpha, \\beta), (\\mu, \\sigma^2) \\sim NIG(\\gamma, \\upsilon, \\alpha, \\beta),$\nwhere (.) is the gamma function, \u03b3 \u2208 R, \u03c5,\u03b2 \u2208 R+, and a > 1. Similar to EDL, the parameters of the NIG distribution m = (\u03b3, \u03c5,\u03b1, \u03b2) is generated by the neural network, and a Softplus function is employed to ensure the non-negative property of (\u03c5, \u03b1, \u03b2) (additional +1 added to a). Linear activation is used for the parameter \u03b3. According to the properties of the NIG distribution, the prediction E[\u03bc], aleatoric uncertainty E[02], and epistemic uncertainty Var[u] can be calculated as follows:\n$E[\\mu] = \\gamma, E[\\sigma^2] = \\frac{\\beta}{\\alpha-1},$ (26)\n$Var[\\mu] = \\frac{\\beta}{\\upsilon(\\alpha -1)},$\nMarginalizing over \u03bc and o\u00b2, the likelihood of an observa-tion y given m = (\u03b3, \u03c5, \u03b1, \u03b2) can be calculated as:\n$p(y/m) = S_t (y; \\gamma, \\frac{\\beta (1 + \\upsilon)}{\\upsilon \\alpha}, 2\\alpha),$ (27)\nwhere $S_t(y; \\mu_{st}, \\sigma, \\upsilon_{st})$ is the Student-t distribution with location $\u00b5_{st}$, scale and degrees of freedom $upsilon_{st}$. Using the negative logarithm of the likelihood as the optimization objective and adding a regularization term to minimize incorrect evidence, the loss function of DER is formulated as $L_{ern} = L_{nll} + \\lambda L_R$, where \u5165 is a hyper-parameter. Specifically, denoting $Omega = 2 \\beta(1 + \\upsilon)$, $L_{nll}$ and $L_R$ can be calculated as:\n$L_{nll} = \\frac{1}{2} log(\\pi) - a log(\\upsilon) + (a + \\frac{1}{2}) log(S),$ (28)\n$-log (\\frac{\\Gamma(\\alpha + \\frac{1}{2})}{\\Gamma (\\alpha)}) + (2 \\upsilon + 1),$\nFurthermore, [82] extends the univariate ERN to mul-tivariate regression by replacing the NIG distribution with a normal-inverse-Wishart (NIW) distribution. Exploring an-other direction, [83] extends the single-modal ERN to multi-modal regression by fusing multiple NIG distributions from different modalities into a single NIG distribution. Existing works have also explored various additional regularization terms [84], [85]. [84] observes that in the high uncertainty area (HUA) of ERN, the gradient of ERN tends to shrink to zero, thereby hindering the correct update of ERN outputs. To address this issue, this work proposes an uncertainty regularization $\\mathcal{L}_U = -|y - \\gamma|\\cdot log(exp(x - 1) - 1)$. Similarly, to ensure gradients during evidence contradiction are non-zero, [85] proposes a non-saturating uncertainty regulariza-tion: $\\mathcal{L}_U = \\frac{2 \\upsilon(\\alpha-1)}{(y-\\gamma)^2 + 1)}$. It is proved that this regular-ization can ensure that a gradient exists for the prediction throughout the entire domain of definition. to address the gradient shrinkage problem, [86] proposes incorporating an additional Lipschitz-modified mean squared error loss alongside the existing negative log-likelihood loss."}, {"title": "5 APPLICATIONS", "content": "In the previous sections, we introduced the theoretical de-velopment of EDL. Due to its strong usability and extensi-bility, EDL has also been widely applied to enhance existing classical machine learning paradigms, such as weakly su-pervised learning and transfer learning. In this section, we introduce the application of EDL in the field of seven typical ML paradigms, as depicted in Fig. 4."}, {"title": "5.1 EDL Enhanced Machine Learning", "content": "5.1.1 Weakly Supervised Learning\nWeakly supervised learning emerges as a crucial paradigm for utilizing extensive datasets that lack precise or com-prehensive annotations [131]. Multiple Instance Learning (MIL) is a relatively straightforward and widely applicable paradigm within weak supervision that utilizes Evidential Deep Learning (EDL) for improvement, as both MIL and EDL fall under the category of classification problems. How-ever, EDL was originally proposed for fully-supervised sce-narios, where each training instance is individually labeled, providing a fine-grained instance-label pairs. In contrast, the formulation of MIL involves assigning labels y \u2208 RC to sets of instances, known as bags, rather than individual instances [131]. Specifically, for each dimension of the label, Yi, it is marked positive if at least one instance within the bag is positive. Since the unavailability of precise instance-level ground truth, EDL is adopted for estimating the ambiguity of instance-level information.\nMethods utilizing EDL to measure instance-level am-biguity can be categorized into explicit and implicit ap-proaches based on their optimization differences. Given the collected instance evidence, explicit approaches directly optimize them and derive instance-level predictions, while implicit approaches aggregate instance evidence into bag evidence and predict class probabilities at bag-level instead. For explicit approaches, the challenge lies in the mismatch between the predicted results and the given labels. To se-lect clean positive instance, Zhu et al. [96] sets thresholds on predicted probabilities and concentration parameters of Dirichlet distribution while Liu et al. [97] perform weighted summation of the instances in a positive bag to obtain a single pseudo-positive instance. For implicit approaches, instance-level evidence is predicted first and then aggre-gated into bag-level evidence, resulting in bag-level cate-gorical distributions. An intuitive scheme for aggregation is to sum the weighted instance-level evidence as follows:\n$e_{bag} = \\sum_{j=1}^N w_j e_{instance}$ (29)\nHere, N represents the total number of instances in a bag while ebag and einstance denote the bag-level and instance-level evidence, respectively. The instance weights, wi, are either directly predicted by the network [79], [98] or de-termined based on the evidential uncertainty [70]. After obtaining the bag-level evidence vector, bag-level categor-ical distribution and uncertainty can both be derived based on evidential theory. Additionally, [99] proposes a variant, which positions the evidential head after the aggregation operation, i.e., aggregating features instead of evidence."}, {"title": "5.1.2 Transfer Learning", "content": "Transfer learning aims to help improve the performance of target tasks TT in a target domain Dr using the knowledge in a source domain Ds, acquired by source tasks Ts, where Ds \u2260 Dr or Ts \u2260 Tr. This transfer process is typi-cally achieved by reducing the difference between domains, which refers to domain adaptation(DA) [132]. Traditional DA methods provide simplistic sample prediction, which fails to accurately assess the compatibility between the source domain knowledge and the target domain samples, thereby limiting the fine-grained information transfer. In contrast, evidential networks can capture more comprehen-sive and specific source domain knowledge in the prediction results of each target sample, enabling more precise domain adaptation [100], [102], [109]. Along with domain adap-tation, some studies focus on transferring distributional knowledge from a source model to a target model within the same domain, such as knowledge distillation [103], [104], [105] and pre-trained model fine-tuning [106], which we refer to as model adaptation. For clarity, we will also discuss how evidential theory guides model adaptation in this section.\nFor EDL-enhanced DA, Chen et al. [100] explore the Universal Domain adaptation (UniDA) setting which ex-pands upon unsupervised DA by introducing the concept of category shift. UniDA poses the technical challenge of estimating the label distribution on the target domain and detecting potential target \"unknown\" (unique) samples. To this end, [100] proposes using EDL to construct category-aware thresholds dj with total evidence St and concen-tration parameters at to reject target \"unknown\" samples, which is defined as:\n$y_t = \\begin{cases}\n\\text{if log}S_t \\geq d_j, j = \\text{arg max}_{1 < k < L_s} a_k \\\\\n\\text{unknown if log}S_t < d_j, j = \\text{arg max}_{1 < k < L_s} a_k\n\\end{cases}$ (30)\nHere, yt denotes the predicted label of a target sample while Ls denotes the number of labels within source domain. Another work is for Unsupervised Domain Adaptation in Regression (UDAR) [133], which involves transforming the"}, {"title": "5.1.3 Active Learning", "content": "Active Learning (AL) aims to achieve higher accuracy with fewer labeled training instances by strategically selecting the data from which it learns [136", "21": "allows for a more nuanced categorization of uncer-tainty, leading to diverse uncertainty-based querying strate-gies. In addition to computing epistemic uncertainty and aleatoric uncertainty, we can further decompose uncertainty into vacuity and dissonance, providing greater flexibility in sample selection.\nGiven the predicted class distribution and its conjugate prior, epistemic and aleatoric uncertainty defined by en-tropy are mostly used as the criteria for sample selection. Balaram et al. [107"}]}