{"title": "Multi-language Video Subtitle Dataset for Image-based Text Recognition", "authors": ["Thanadol Singkhornart", "Olarik Surinta"], "abstract": "The Multi-language Video Subtitle Dataset is a comprehensive collection designed to support research in text recognition across multiple languages. This dataset includes 4,224 subtitle images extracted from 24 videos sourced from online platforms. It features a wide variety of characters, including Thai consonants, vowels, tone marks, punctuation marks, numerals, Roman characters, and Arabic numerals. With 157 unique characters, the dataset provides a resource for addressing challenges in text recognition within complex backgrounds. It addresses the growing need for high-quality, multilingual text recognition data, particularly as videos with embedded subtitles become increasingly dominant on platforms like YouTube and Facebook. The variability in text length, font, and placement within these images adds complexity, offering a valuable resource for developing and evaluating deep learning models. The dataset facilitates accurate text transcription from video content while providing a foundation for improving computational efficiency in text recognition systems. As a result, it holds significant potential to drive advancements in research and innovation across various computer science disciplines, including artificial intelligence, deep learning, computer vision, and pattern recognition.", "sections": [{"title": "BACKGROUND", "content": "Massive numbers of videos are uploaded to online platforms like YouTube and Facebook, making them mainstream channels, particularly among teenagers. Many popular TV channels worldwide have embraced this trend, offering high-quality content through these platforms. Numerous videos also provide subtitles, which are necessary for individuals with hearing impairments, enabling them to comprehend the video content. Subtitles further assist audiences in learning the spelling of names, brands, acronyms, and abbreviations and understanding various accents. Additionally, subtitles bridge the accessibility gap between content and audiences. Further supporting non-native speakers, subtitles serve as essential tools for individuals with hearing impairments, offering a textual representation of spoken dialogue that transcends language barriers and audio limitations.\nIn this dataset, the research team focused on capturing subtitles from online platforms like YouTube and Facebook that feature multiple languages, including Thai and English. The text within these images consists of a mix of Thai, English, and numerals, often displayed against complex backgrounds, making recognition particularly challenging. It emphasizes the requirement for high-quality datasets that provide comprehensive data for developing deep learning models and enhancing text recognition systems. Advanced recognition systems rely on robust models capable of accurately transcribing text from videos, even when encountered with font, size, and language variations."}, {"title": "DATA DESCRIPTION", "content": "The Multi-language Video Subtitle Dataset is a comprehensive collection of images containing text in multiple languages, referred to as subtitle images. These images were extracted from 24 videos sourced from online platforms. The details of the dataset are presented as follows."}, {"title": "Character Collection", "content": "The subtitle images were extracted from 24 videos containing a diverse range of characters across multiple languages. These include Thai consonants, vowels, tone marks, punctuation marks, and numerals, as well as Roman characters and Arabic numerals. The Multi-language Video Subtitle Dataset comprises 157 unique characters, as detailed in Table 1."}, {"title": "Labelling", "content": "The research team split the 24 videos into frames, resulting in 4,224 images with a resolution of 1,280\u00d7720 pixels per frame. Out of the total, 2,700 images containing subtitles were selected and annotated. However, the dataset does not include annotation files (XML); only the image files (JPG) are freely accessible via the Mendeley Data repository. Examples of subtitle images and their labels are illustrated in Table 2."}, {"title": "File format and conversion", "content": "After the annotation and labelling process, a Python program was developed to extract the subtitle images based on the coordinates provided in the XML files. The extracted subtitle images are stored in format with RGB channels, ensuring compatibility with widely used image processing libraries and tools.\nFurthermore, the filenames of each subtitle image follow a consistent and standardized naming convention, embedding both a unique identifier and a label corresponding to the subtitle content. Examples of subtitle images and their labels are presented in Table 3."}, {"title": "Distribution of characters and subtitle images", "content": "After generating the subtitle images and labels, we calculated basic statistics for the characters, as presented in Fig. 1 and Fig. 2.\nAs depicted in Fig. 1, the distribution of characters shows the number of subtitle images relative to the number of characters appearing in each image. It reveals that more than 80 subtitle images contain between 10 and 40 characters, which represents the standard length for subtitles. However, a small number of images contain fewer than 10 characters or more than 40 characters, underscoring the importance of balanced training to ensure models can effectively handle both shorter and longer subtitles."}, {"title": "EXPERIMENTAL DESIGN, MATERIALS AND METHODS", "content": null}, {"title": "Materials", "content": "The subtitle images were collected from YouTube and Facebook between October 2020 and January 2021 by selecting videos that featured embedded subtitles. In the initial phase of data collection, the research team focused solely on subtitles appearing at the bottom of the video. However, it was later observed that many videos, particularly those from news channels, also show graphical text on the screen. Consequently, lyric videos featuring multiple lines of text in the middle of the screen were also included.\nThe video sources were gathered from the following Facebook pages: \u0e44\u0e17\u0e22\u0e23\u0e31\u0e10\u0e2d\u0e2d\u0e19\u0e44\u0e25\u0e19\u0e4c (3\u0e19\u0e32\u0e17\u0e35\u0e04\u0e14\u0e35\u0e14\u0e31\u0e07) and TEP - Thailand Education Partnership \u0e20\u0e32\u0e04\u0e35\u0e40\u0e1e\u0e37\u0e48\u0e2d\u0e01\u0e32\u0e23\u0e28\u0e36\u0e01\u0e29\u0e32\u0e44\u0e17\u0e22. The YouTube channels included Bearhug, KLUAYTHAI, KRIT Eighth Floor, Genierock, Taj Tracks, Cakes & Eclairs, 7clouds, DopeLyrics, Equilanora, JEMIN Apollo, San Ko, Tangerine JJY, SNH48 Lyrics, and Lemoring. After downloading the videos, they were split into frames, with a frame captured every 5 seconds, providing a comprehensive snapshot of the video content. An example of images with subtitle labels is shown in Fig. 4."}, {"title": "Experimental design", "content": "Deep learning methods, such as convolutional neural networks (CNNs) and long short-term memory (LSTM) networks, have been proposed to evaluate model performance using the Multi-language Video Subtitle Dataset. In 2022, Singkhornart and Surinta [1] introduced a CNN-LSTM architecture where the output utilized connectionist temporal classification (CTC) as the loss function to compute the loss value and decode the predictions. The modified VGG19 was employed as the CNN backbone, resulting in a character error rate (CER) of 9.36%."}, {"title": "LIMITATIONS", "content": "Not applicable"}, {"title": "ETHICS STATEMENT", "content": "The authors have read and follow the ethical requirements and confirming that the current work does not involve human subjects, animal experiments, or any data collected from social media platforms."}, {"title": "CRediT AUTHOR STATEMENT", "content": "Thanadol Singkhornart: Conceptualization, Data Curation, Investigation, Methodology, Resources, Validation, Writing - Original Draft; Olarik Surinta: Supervision, Conceptualization, Experimental Design, Writing \u2013 Review & Editing, Funding Acquisition."}, {"title": "DECLARATION OF COMPETING INTERESTS", "content": "The authors declare that they have no known competing financial interests or personal relationships that could have appeared to influence the work reported in this paper."}]}