{"title": "A Survey on Human-Centric LLMs", "authors": ["JING YI WANG", "NICHOLAS SUKIENNIK", "TONG LI", "WEIKANG SU", "QIANYUE HAO", "JINGBO XU", "ZIHAN HUANG", "FENGLI XU", "YONG LI"], "abstract": "The rapid evolution of large language models (LLMs) and their capacity to simulate human cognition and behavior has given rise to LLM-based frameworks and tools that are evaluated and applied based on their ability to perform tasks traditionally performed by humans, namely those involving cognition, decision-making, and social interaction. This survey provides a comprehensive examination of such human-centric LLM capabilities, focusing on their performance in both individual tasks (where an LLM acts as a stand-in for a single human) and collective tasks (where multiple LLMs coordinate to mimic group dynamics). We first evaluate LLM competencies across key areas including reasoning, perception, and social cognition, comparing their abilities to human-like skills. Then, we explore real-world applications of LLMs in human-centric domains such as behavioral science, political science, and sociology, assessing their effectiveness in replicating human behaviors and interactions. Finally, we identify challenges and future research directions, such as improving LLM adaptability, emotional intelligence, and cultural sensitivity, while addressing inherent biases and enhancing frameworks for human-AI collaboration. This survey aims to provide a foundational understanding of LLMs from a human-centric perspective, offering insights into their current capabilities and potential for future development.", "sections": [{"title": "1 INTRODUCTION", "content": "As large language models (LLMs) [1, 2], such as OpenAI's GPT family [3, 4] and Meta's LLaMA [5, 6], continue to evolve, their ability to simulate, analyze, and influence human behavior is growing at an unprecedented rate. These models can now process and generate human-like text and perform cognitive tasks at levels comparable to humans in many situations, providing new tools for understanding human cognition, decision-making, and social dynamics.\nAs such, this survey aims to provide a comprehensive evaluation of LLMs from a human-centric perspective, focusing on their ability to simulate, complement, and enhance human cognition and behavior, both on an individual and collective level. While LLMs have traditionally been rooted in computer science and engineering [7, 8], their increasing sophistication in replicating human-like reasoning, decision-making, and social interactions has expanded their use into domains where humans are the focal point. This has allowed researchers to address questions that were once too intricate or abstract for computational analysis. For example, in political science, LLMs are used to analyze political discourse, detect biases, and model election outcomes [9]; in sociology, they assist in understanding social media conversations, public sentiment, and group behaviors [10];"}, {"title": "2 OVERVIEW", "content": null}, {"title": "2.1 Human-Centric Artifical Intelligence", "content": null}, {"title": "2.1.1 Traditional Al Approaches in Human-Centric Studies", "content": "The application of AI in various human-centered fields has undergone a long progression, now reaching a pinnacle with the rise of generative models, with AI methods to being used investigate various human phenomena. however, despite their relative naivety compared to LLMs, those traditional methods have nonetheless enabled researchers to address complex social phenomena through computation.\nFor almost as long as it has been investigated, AI has been used in areas that are highly im-pactful on society [14]. Since then researchers have evaluated the many ways in which AI could emulate human behavior and thought procession, for example in cognition[15], perception [16], and executive function [17]. More recently, though, with the rise of the web and social media, AI's uses come closer to our day-to-day lives. For example, in political communication research, the detection of political bias in news articles has emerged as a critical area of study, particularly given the increasing polarization in media and online spaces. Traditional methods for predicting political ideology, based on statistical modeling and network analysis, have become an urgent task due to the vast amount of content produced daily. For instance, research by [18] employed network analysis to estimate ideological preferences of social media users. Moreover, techniques like topic modeling and content analysis have been widely used to identify bias and misinformation in news articles using data-mining methods [19, 20], highlighting the use of traditional AI techniques in understanding political discourse. Other works tackled the task of stance detection using methods like recursive neural networks [21] and clustering algorithms [22]. Furthermore, Dezfouli et al.[23] explore adversarial vulnerabilities in decision-making models, which is crucial when considering the robustness of traditional bias detection systems under adversarial conditions. Furthermore, Dafoe et al.[24] emphasize the importance of systems designed to navigate social environments,"}, {"title": "2.1.2 A Paradigm Shift from Traditional Al to LLMs", "content": "The rise of LLMs has transformed natural language processing (NLP) and artificial intelligence in general through key breakthroughs in model architecture, scale, and capabilities. Early models like Word2Vec and GloVe used word embeddings, but the introduction of the Transformer in 2017 [26], with its self-attention mechanism, enabled deeper contextual understanding and marked a turning point. OpenAI's GPT series, beginning in 2018 with GPT [3], capitalized on this, culminating in GPT-3 [27] and GPT-4 [28], which demonstrated unprecedented capabilities in reasoning, text generation, and multimodal tasks. Meanwhile, Google's PaLM 2 [29] advanced multilingualism and efficiency, and open-source models like Falcon [30] and Baidu's ERNIE Bot [31] broadened access and specialization. These developments reflect the growing impact of LLMs across diverse domains, from interdisciplinary research to ethical AI applications.\nThe rapid adoption of LLMs across academic disciplines has led to varying predictions about whether these systems will eventually match human cognitive abilities. While some experts foresee AI achieving human-like general intelligence in the near future, others remain more cautious, doubting whether AI can fully replicate the complex, abstract reasoning and creativity that define human cognition [32]. Despite these differing viewpoints, AI is already a significant force in everyday life, influencing decision-making and information processing across numerous domains. However, a key distinction remains: human cognition is driven by forward-thinking, theory-based reasoning, while AI operates on patterns derived from vast datasets, often relying on probability and past data [33]. This difference underscores the complementary nature of human and AI systems, with each excelling in distinct aspects of cognitive processing.\nUnlike human intelligence, LLMs operate without inherent goals, values, or emotional experiences. Human cognition, driven by survival, social interaction, and creativity, is deeply connected to our physical and social environments. Even embodied AI, while capable of interacting with its surroundings, lacks the nuanced, purpose-driven intelligence that defines human thought. In contrast, LLMs generate responses based on probabilistic models derived from large datasets, without the lived experiences that inform human decision-making. Though LLMs can simulate certain human-like behaviors, they still fall short of the embodied understanding humans possess.\nThese distinctions raise critical questions about the limitations and potentials of AI, especially as we consider the diverse capabilities explored in Section 3, which discusses the capabilities of LLMs including cognitive, perceptual, social, analytical, executive, cultural, moral, and collaborative skills. Section 4 delves into how interdisciplinary fields, such as political science, economics, sociology, behavioral science, psychology, and linguistics, contribute to LLM development, offering insights into how human intelligence informs and shapes the evolution of artificial systems. This exploration emphasizes the importance of leveraging LLM strengths while recognizing the fundamental differences between human and artificial cognition."}, {"title": "3 EVALUATION OF HUMAN-CENTRIC LLMS", "content": "To evaluate human-centric LLMs, we showcase a holistic representation of LLM competencies, categorized into two domains: individual (e.g., cognitive, perceptual, analytical, executive functioning skills) and collective (e.g., social skills), as shown in Figure 2. This representation includes various key LLM skills, such as reasoning, pattern recognition, spatial awareness, adaptability, decision-making, interpersonal communication, and cultural competency. Following this, Figure 4 outlines the evaluation approaches used to assess LLMs, including benchmark and dataset testing, human-centric evaluations, interactive and simulation-based evaluations, ethical and bias assessments, and lastly, explainability and interpretability evaluations. Table 1 highlights both the strengths and areas for improvement in these domains. By outlining these abilities, we provide a comprehensive comparison of human-like skills, using benchmarks to assess their strengths and limitations. Additionally, Appendix Tables 2 and 3 provide a comprehensive overview of key papers, highlighting their contributions, the LLMs assessed, and comparisons to human performance. The subsequent section delves into each category, providing an in-depth exploration of the skills and benchmarks that define LLM performance across these domains."}, {"title": "3.1 Cognitive Skills", "content": "LLMs demonstrate cognitive competencies that mirror key elements of human intelligence, primarily through reasoning and learning. While LLMs show remarkable ability in processing vast amounts of information and generating coherent responses, their proficiency varies when it comes to complex cognitive tasks. These models showcase evolved abilities in structured reasoning and generalization but encounter challenges when faced with intricate logic or learning from real-time interactions."}, {"title": "3.1.1 Reasoning", "content": "Logical reasoning, a core element of human cognition and essential for daily functioning, consists of various types of reasoning, including deductive, inductive, and causal reasoning, each contributing to how we process information and make decisions. Deductive reasoning applies general principles to obtain specific conclusions, while inductive reasoning draws generalizations from specific observations [34], and causal reasoning helps to understand cause-and-effect relationships [35, 36].\nSeveral benchmark datasets have been developed to assess these reasoning capabilities in LLMs. For deductive reasoning, the LogiQA 2.0 dataset [37] is a notable resource, focusing on five types of reasoning, including categorical, necessary conditional, sufficient conditional, conjunctive, and disjunctive reasoning. PrOntoQA [38] also evaluates deductive reasoning through first-order logic tasks where LLMs derive specific conclusions from logical premises. For inductive reasoning, CommonsenseQA 2.0 [39] requires generalization from everyday facts and commonsense knowledge, whereas the Creak dataset [40] further tests LLMs' ability to generalize from commonsense knowledge to identify inconsistencies. In turn, causal reasoning is assessed using CausalBench [41], which evaluates LLMs' ability to reason about cause-and-effect relationships across diverse domains. ContextHub [42], on the other hand, serves as another benchmark focusing on LLMs' causal reasoning in both abstract and contextualized tasks. Additional datasets like GSM8K [43] and BIG-Bench-Hard [44] are furthermore employed for mathematical reasoning and evaluating LLM performance across various reasoning domains, respectively.\nAnalyzing LLM performance with these datasets has revealed significant insights into their reasoning abilities and limitations. For deductive reasoning, although LLMs like GPT-3 have made progress, their accuracy remains at 68.65% in tasks involving logical inference, which is significantly below the 90% human benchmark [37]. This gap indicates ongoing challenges in mastering complex logical structures, especially when multiple logical steps or intricate reasoning processes are required. LLMs like GPT-3.5, PaLM, and LLaMA perform well on simpler deductive reasoning tasks but struggle with more complex scenarios that involve chaining multiple logical premises together [45]. For inductive reasoning, on the other hand, GPT-4 shows improvements in rule application with up to 99.5% partial accuracy [46], yet struggles with larger problems and minimal examples. Even with Chain-of-Thought (CoT) prompting, GPT-4 and Davinci face difficulties in"}, {"title": "3.1.2 Learning", "content": "LLMs' learning ability encompasses their capacity to adapt, generalize, and improve performance based on pre-existing training data and interactions with users or environments. Unlike traditional learning models, LLMs do not update their parameters during inference. Instead, they rely on pre-trained knowledge to perform few-shot or zero-shot tasks, highlighting their generalization capabilities. However, this comes with significant limitations when faced with evolving, real-world data.\nRecent efforts have aimed at improving LLM adaptability through various strategies. For instance, the RL with Guided Feedback (RLGF) framework [49] optimizes learning from feedback, showing that guided strategies can significantly improve text generation in dynamic conditions. Similarly, error-driven learning approaches, like LEMA (Learning from MistAKes) [50], allow models like GPT-4 to refine reasoning by identifying and correcting errors. These approaches highlight the potential of leveraging feedback and error correction to boost adaptability, yet they still rely on static data at inference.\nWhile these feedback-based methods provide immediate gains in performance, they do not address the broader challenge of continual learning in real-time. Jovanovic et al. [51] offer a critical review of incremental learning paradigms, noting that current models, including GPT-4 and GPT-3.5, struggle with dynamic, real-time updates. Though these models excel in handling pre-defined tasks, their performance declines on more complex, evolving instances, reflects a broader limitation of inability to adapt to real-time adaptability.\nThe fine-tuning approaches explored by Ren et al. [52], which aim to clarify instruction-based and preference-based learning, also highlight challenges in maintaining model alignment without overfitting. Although models show improvement through fine-tuning, they still fall short in dynamic environments where adaptability and continual learning are crucial.\nIn general, these studies reveal both the strengths and limitations of LLMs in learning. While frameworks like RLGF [49] and LEMA [50] demonstrate short-term improvements in adaptability and error correction, and fine-tuning helps refine models, the inability to learn incrementally and in real-time remains a critical obstacle. Future research will need to focus on overcoming this gap to enable LLMs to fully adapt to real-world, evolving tasks, which is why several online-deployed models have begun to include a web search component to collect more recent relevant information [53, 54]."}, {"title": "3.2 Perceptual Skills", "content": "Perceptual skills in LLMs involve their ability to interpret, organize, and process complex information from their environment, which serves as a foundation for navigating and interacting within both virtual and real-world contexts. These skills are broadly divided into two critical areas: pattern recognition and spatial awareness. Pattern recognition focuses on the LLM's capability to identify and understand recurring structures, semantic relationships, and trends within data, which are essential for tasks involving prediction and interpretation. Spatial awareness, on the other hand, relates to the LLM's ability to comprehend spatial configurations, positions, and relationships between objects within defined environments, crucial for navigation and interaction tasks. Together, these perceptual skills lay the foundation for LLMs to engage effectively in complex scenarios, although challenges remain in achieving human-level proficiency, particularly in dynamic and unpredictable settings."}, {"title": "3.2.1 Pattern Recognition", "content": "Pattern recognition refers to a model's ability to identify, interpret, and categorize recurring patterns within data [78]. In the context of LLMs, this capability is crucial for recognizing trends, making predictions, and understanding complex multimodal inputs that combine language, vision, and other sensory information. Evaluating pattern recognition involves assessing a model's proficiency in object recognition, spatial inference, and semantic understanding-core skills required for interpreting both real-world and simulated environments [78, 79].\nThis ability can be analyzed through three critical aspects. Object identification and classification refers to the process where models categorize items based on their distinct features, enabling accurate recognition [80]. Semantic and spatial relationship mapping involves understanding the relationships between objects and their spatial arrangements, which is essential for contextually accurate reasoning and interaction [81]. Finally, temporal dynamics focuses on recognizing sequences of events and their interdependencies, allowing models to grasp patterns over time [81]. These aspects highlight the complexity of pattern recognition in LLMs, showcasing their potential while emphasizing the challenges in achieving human-like perception and reasoning.\nTo evaluate these capabilities, several works have employed unique methods to help models generalize and recognize patterns in complex environments. For example, the R3M framework by"}, {"title": "3.2.2 Spatial Awareness", "content": "Spatial awareness in LLMs refers to their ability to interpret, analyze, and understand spatial configurations, positions, and relationships between objects within defined environments. This skill is essential for tasks involving precise navigation, object identification, and interaction, both in virtual and physical contexts.\nLLMs have shown remarkable capabilities in structured virtual environments. For instance, Zhao et al. [87] develop the STEVE framework within Minecraft, demonstrating LLMs' effectiveness in tasks like block search and tech tree mastery. This highlights LLMs' ability to understand spatial configurations and object relationships in virtual spaces, enabling efficient interaction. Building on these capabilities, Tan et al. [56] introduce the CRADLE framework, where LLMs interpret visual cues from game screenshots to perform context-aware actions in games like Red Dead Redemption 2. These advancements illustrate significant progress in spatial reasoning, especially in digital environments.\nDespite these advances, LLMs face challenges when transitioning from controlled virtual settings to dynamic real-world contexts. Wang et al. [88] develop the RoboGen system to evaluate spatial reasoning in physical environments, where LLMs need to adapt to unpredictable scenarios. While they show autonomy in generating robotic tasks and adapting to changes, their performances lack the precision and adaptability of human spatial awareness, especially in real-time adjustments. Liu et al. [89] explore these challenges further with the AerialVLN framework for UAV navigation in urban environments. Although LLMs effectively follow natural language instructions and recognize spatial landmarks, their handling of complex, real-world navigation remains limited. This emphasizes the gap in LLMs' ability to generalize across diverse and dynamic environments. Schumann et al. [90] and Zhang et al. [91] provide further evidence of these limitations in real-world navigation, showing that while LLMs can interpret spatial cues and adapt to changing conditions, they struggle with continuous decision-making and situational awareness.\nThe findings from the works about show that LLMs have made important processes in spatial awareness, particularly in structured virtual environments. However, their transition to real-world applications reveals substantial gaps in adaptability and decision-making. They perform well in tasks with predefined rules and clear spatial cues but encounter difficulties in unpredictable environments requiring flexible thinking and real-time processing. Bridging these gaps requires"}, {"title": "3.3 Analytical Skills", "content": "LLMs demonstrate strong analytical skills in areas such as data interpretation, creativity, and information processing. Below we outline notable works that evaluate LLMs' abilities in each of these areas."}, {"title": "3.3.1 Data Interpretation", "content": "Data interpretation refers to the ability of LLMs to analyze, interpret, and extract meaningful insights from structured and unstructured data. This capability is crucial in fields such as scientific research, where data-driven decision-making relies on accurately understanding and processing complex datasets [59]. Data interpretation in LLMs can be measured through aspects like structured data extraction, data visualization, and reasoning over data to generate actionable insights.\nThe assessment of data interpretation typically involves evaluating LLMs' ability to handle different data formats, understand domain-specific contexts, and perform accurate computations or visualizations based on the data. Benchmarks such as scientific papers, domain-specific datasets (e.g., chemistry, biology), and tools like MatPlotBench [92] are commonly used to assess these abilities, focusing particularly on visualization and data comprehension.\nA recent development in this field is the Data Interpreter model by Hong et al. [93], an LLM-based agent designed to handle complex data science tasks through hierarchical dynamic planning and real-time data adaptation. The model demonstrates significant improvements in managing data dependencies and optimizing machine learning workflows, achieving a 26% enhancement in mathematical problem-solving and a 112% improvement in open-ended tasks. These results highlight the possible enhancement for LLMs to adapt to dynamic data relationships, a key challenge in data interpretation.\nData visualization, another aspect of data interpretation, has also been addressed using LLMs. MatPlotAgent [92] automates the generation of complex visualizations from user queries and raw data, iteratively refining output through feedback. This framework demonstrates the potential of LLMs to interpret data and produce high-quality visual representations, further aiding in data comprehension and analysis.\nDagdelen et al. [58] explore structured information extraction using LLMs to create knowledge bases from scientific texts. By combining named entity recognition with relation extraction, they successfully transform unstructured research papers into structured data that can be used in downstream applications like machine learning. This contributes significantly to automating the research process, making vast amounts of scientific literature accessible in structured formats.\nFrom the above findings, it is clear that data interpretation is a key capability of LLMs, empowering them to process, analyze, and visualize complex datasets across domains. The assessments conducted in these works highlight LLMs' growing proficiency in extracting and leveraging insights from data, proving their value as helpful assistants to humans in various aspects of data-driven scientific research."}, {"title": "3.3.2 Information Processing", "content": "LLMs demonstrate advanced analytical capabilities in information processing, distinguishing themselves through their ability to handle data integration, synthesis, and filtering. Unlike previously discussed perceptual skills, which primarily involve interpreting sensory inputs, these analytical functions rely on deeper cognitive processing, leveraging natural language understanding and generation to manage data in sophisticated ways beyond traditional neural networks."}, {"title": "3.3.3 Creativity", "content": "Large, pre-trained models exhibit remarkable creativity, generating fresh ideas and unique outputs across various domains from art to science, fundamentally transforming the creative industries and pushing the boundaries of human imagination. Girotra et al.[99] find that LLMs excel in generating innovative ideas, surpassing humans in both speed and quality, even outperforming students from elite universities in creating high-purchase-intent ideas. Similarly, Xu et al.[62] propose an LLM-enhanced design template that promotes critical thinking and idea iteration during the creative process, offering a template to guide users to analyze problems deeply and improve their creativity.\nAfter reviewing 110 studies, Li et al.[100] discover that LLMs are increasingly used for advanced tasks like software development and creative content generation, lowering barriers to human-AI collaboration, particularly in industries like scriptwriting and advertising. In a related effort to"}, {"title": "3.4 Executive Functioning", "content": "Executive functioning in LLMs involves skills like adaptation, planning, and decision-making, enabling models to manage complex tasks and respond to changing conditions. This section examines these aspects of executive functioning, highlighting both strengths and areas for improvement."}, {"title": "3.4.1 Adaptability and Flexibility", "content": "Adaptability and flexibility are essential components of executive functioning in LLMs, reflecting their ability to adjust actions and strategies as task demands change. Contextual Adaptation refers to how well LLMs modify their behavior in response to evolving contexts. Luu et al.[102] demonstrate that LLMs can leverage past experiences to break tasks into context-sensitive components, achieving promising adaptability in controlled settings. However, these models falter in more dynamic, unstructured environments. Similarly, Kim et al.[103] show that LLMs guiding robotic agents through changing environments in the Dynacon system, adjusting to real-time inputs without explicit instructions. Despite this progress, LLMs still struggle to match the nuanced, context-aware decision-making humans exhibit when fast, flexible responses are needed.\nTo benchmark these capabilities, researchers have employed frameworks like LLM-CI [63], which evaluates how well LLMs align with societal norms and navigate human interactions. Although LLMs can partially understand the rules guiding human behavior, they lack depth in contextual understanding, particularly in social settings where subtle cues and ethical considerations are involved. This gap underscores LLMs' broader limitations in adapting to complex, real-world scenarios.\nAdaptability also involves handling uncertainty and overcoming obstacles in decision-making. Eigner et al.[98] evaluate LLMs across factors like task difficulty and psychological influences, finding that models perform well in structured contexts but struggle with complexity and unpredictability. Zhang et al.[104] expose weaknesses by showing how interrogation techniques can force LLMs to produce harmful content even after alignment training, highlighting limitations in adapting to malicious manipulation. On the technical side, the Ctrl-G framework [105] improves adaptability by combining LLMs with Hidden Markov Models to generate constrained text output. This enhances LLM performance in specific tasks, though their ability to adapt to more open-ended scenarios remains limited.\nWhile LLMs demonstrate increasing adaptability in structured tasks, significant challenges remain in replicating human-level flexibility in real-world situations. Benchmarks like those developed by Luu et al. and Kim et al. show that LLMs can adjust well in predictable, structured environments, but their limitations become clear in dynamic, uncertain contexts. These findings indicate that while LLMs show potential, their adaptability is still far from human capabilities, particularly in managing complexity and ambiguity. Future research should focus on improving learning mechanisms and algorithms that allow LLMs to better handle uncertainty and mimic human cognitive flexibility."}, {"title": "3.4.2 Planning and Organization", "content": "LLMs possess advanced planning and organizational capabilities, allowing them to break down complex tasks into smaller, manageable steps, arrange information logically, and generate coherent plans, fostering efficient decision-making and execution. In the context of LLMs, planning and organization refer to different cognitive-like processes that the model can simulate when generating text or solving complex tasks. In particular, planning refers to the model's ability to structure its outputs in a coherent and logical way over multiple steps or turns. Organization refers to how well the model arranges and structures information within a single response or across multiple interactions.\nSong et al. [64] introduce the LLM Planner, a method that uses LLMs to empower intelligent agents to follow natural language instructions and complete complex tasks in visually perceived environments. By learning from a small number of samples, LLM Planner significantly reduces the amount of annotated data needed to train embodied agents. The LLM Planner excels in breaking down complex tasks into smaller, actionable steps for the agent to execute while adapting to real-world challenges. In contrast, Kambhampati et al.[106] argue that while LLMs are strong in language comprehension and generation, they are not fully capable of effective planning or self-validation. To address these limitations, they propose the LLM Modulo framework, a bidirectional interaction system combining LLMs with external model-based validators. Their roles include generating candidate plans, refining problem specifications, and translating domain models. This framework positions LLMs as incomplete planners that contribute to the broader planning process but depend on external validation for accuracy and reliability.\nGundawar et al. [107] apply the LLM Modulo framework to travel planning, showing how LLMs can generate itineraries from natural language queries and use external critics for feedback. Their results demonstrated that GPT-4-Turbo improves planning success rates, outperforming traditional methods. Similarly, Sharan et al. [108] introduce LLM-ASSIST, which combines rule-based planners with LLMs for autonomous driving. The system leverages LLMs' reasoning to generate safe driving plans for complex scenarios beyond the scope of rule-based systems.\nOverall, these studies emphasize the role of LLMs in planning, focusing on dynamic adaptability as well as the importance of external validation to ensure plan feasibility."}, {"title": "3.4.3 Decision-Making", "content": "In the domain of decision-making, LLMs have both strengths and inherent limitations, particularly in risk assessment, managing uncertainty, and addressing cognitive biases. As LLMs become more widely deployed, their role in supporting decision-making processes must be rigorously evaluated.\nIn risk assessment, LLMs are expected to accurately identify and manage the risks tied to their integration. Given their growing use in various sectors, vulnerabilities must be carefully addressed. Pankajakshan et al.[109] develop a systematic framework using scenario analysis and dependency mapping to prioritize risks in LLM integration. GUARD-D-LLM [110] further assesses threats tied to specific use cases, highlighting the need for ongoing improvements in risk mitigation frameworks.\nEvaluating LLMs' decision-making through the lens of game theory, Huang et al. [65] introduce GAMA-Bench ($\\gamma$-Bench), a framework that assesses LLMs' decision-making abilities in multi-agent environments using classic game theory scenarios like the Public Goods Game and Sealed-Bid Auction. This benchmark evaluates LLMs on cooperative, betraying, and sequential decision-making, highlighting strengths in robustness but revealing limitations in generalizability compared to human decision-makers. While LLMs like GPT-3.5 show promising robustness, their performance in complex, multi-agent settings still lags behind humans, suggesting the need for approaches like CoT to improve their strategic capabilities.\nWhen it comes to decision-making under uncertainty, LLMs are increasingly used by decision-makers who may not fully grasp the nuances of these complex models. As LLM capabilities evolve, there is a growing temptation to rely on them for solving ambiguous and uncertain problems. However, this introduces risks, including the awareness and potential misuse of LLMs and the systemic risks of relying on AI for critical decisions calling for a dynamic, adaptive approach in AI development, especially when LLMs are used in high-stakes scenarios [111].\nA key challenge in LLM-based decision-making is the presence of cognitive biases. Since LLMs are trained on large datasets that often contain societal biases, they can generate biased responses [112]. Similarly, Schramowski et al. [113] note that these biases resemble the cognitive biases seen in human behavior. Such biases can distort decisions and undermine fairness and objectivity. To address this issue, Echterhoff et al. propose a self-debiasing technique that enables LLMs to automatically adjust prompts, removing bias-inducing elements and enhancing the fairness and consistency of their decision-making processes [66].\nOverall, while LLMs show potential in risk assessment, uncertainty management, and decision-making, they still require significant improvements to handle complex scenarios, cognitive biases, and high-stakes environments effectively in order to assist humans effectively."}, {"title": "3.5 Social Skills", "content": "Social skills in LLMs are essential for enabling effective human-like interactions, emotional understanding, collaboration, and cultural awareness. Key areas such as interpersonal communication, emotional intelligence, collaboration, and cultural competency define how well LLMs navigate social dynamics. The works described below give us an overview of the strengths and limitations of LLMs is such scenarios."}, {"title": "3.5.1 Interpersonal Communication", "content": "Interpersonal communication in LLMs refers to their ability to engage in human-like social behaviors, which is crucial for effective information exchange, understanding social dynamics, and participating in complex interactions [68, 114, 115].\nTo comprehensively assess LLMs' interpersonal communication abilities, the focus is often on two primary dimensions: communication skills and social reasoning. Communication skills are assessed by examining the LLM's ability to produce coherent and contextually relevant responses, while also understanding language subtleties like tone, humor, and implied meanings [116]. Social reasoning involves the model's ability to interpret and predict the thoughts, intentions, and emotions of others [117]. A critical aspect of social reasoning is Theory of Mind (ToM)\u2014the capacity to recognize that others may hold beliefs, desires, or knowledge different from one's own [67]. This capability is essential for predicting behavior and responding appropriately in dynamic social interactions.\nVarious experimental setups and datasets have been developed to test these aspects of interpersonal communication in LLMs. For instance, the Generative Agent Environment [118] provides a controlled virtual setting where LLM-based agents simulate everyday social behaviors, like forming relationships and making collective decisions. These agents operate in environments modeled after games like The Sims, demonstrating their ability to plan activities, organize events, and engage in realistic social dynamics, which showcases their evolving communication and coordination capabilities.\nRole-playing has emerged as another significant method for assessing LLMs' interpersonal skills. Shanahan et al. [119] investigate how LLMs can take on different personas to simulate complex social scenarios, including deception, persuasion, and conflict resolution. This approach highlights the potential of LLMs to be utilized in training environments where realistic social interactions are required, such as in negotiation exercises or interview preparations, further proving their relevance in human-like simulations. Furthermore, Strachan et al. [67] explore LLMs' social reasoning capabilities by focusing on ToM tasks. Their research demonstrates that models like GPT-4 could perform well in understanding indirect requests and predicting the beliefs and emotions of others. However, these models still face challenges when dealing with more nuanced social situations, such as identifying faux pas or subtle social cues, indicating areas for further improvement."}, {"title": "3.5.2 Emotional Intelligence", "content": "Emotional intelligence (EI) in LLMs is a crucial aspect of their social skills, enabling them to recognize, understand, and respond to users' emotions. In the field of \"machine psychology,\" which integrates human psychology principles into AI systems [120], EI enhances human-computer interactions by fostering empathetic communication and more engaging responses.\nEI in LLMs includes emotion perception, cognition, and expression, all vital for interpreting emotional cues and navigating social contexts. Researchers use psychometric tools like EIBENCH [70] and EmoBench [71] to evaluate these abilities. EIBENCH focuses on emotion recognition, causal reasoning, and generating empathetic responses, while EmoBench tests emotional understanding and reasoning in complex social scenarios.\nZhao et al.[70] develop the Modular Emotional Intelligence enhancement method (MoEI), improving models' EI without reducing their general intelligence (GI). Experiments show that models like Flan-T5 and LLaMA-2-Chat effectively balanced EI and GI, displaying enhanced empathy and emotional reasoning. Wang et al.[72] introduce the SECEU test, revealing that models like GPT-4 can outperform human participants in emotion understanding. Elyoseph et al. [69] use the Levels of Emotional Awareness Scale (LEAS) to find that ChatGPT exceeds average human performance in emotional awareness, showing improvements through iterative learning. Despite advancements in emotion recognition and regulation, LLMs still lag behind human-level social reasoning in complex scenarios. Frameworks like EmoBench [71] highlight that while models like GPT-4 perform well in emotional tasks, gaps remain in their ability to handle nuanced social communication.\nOVerall, the above works imply that while LLMs demonstrate significant progress in emotional intelligence, especially in perception and empathy, they struggle to fully apply this understanding in real-world social interactions."}, {"title": "3.5.3 Collaboration", "content": "LLM collaboration capabilities span two critical domains: working alongside Al agents and collaborating with human partners. These areas represent different aspects of how LLMs contribute to teamwork, strategic planning, and task execution, yet each has distinct challenges and benchmarks for evaluation.\nIn multi-agent systems (MAS), LLMs like GPT-3.5, GPT-4, and Claude-3 have demonstrated improvements in task execution and strategic decision-making [121]. Frameworks such as the Dynamic LLM-Agent Network (DyLAN) allow multiple LLM agents to share information and coordinate strategies, improving reasoning and code generation tasks by up to 13% [121]. However, they also show that while GPT-4 exhibits greater adaptability in strategy formation, models like Claude-3 tend to struggle more with maintaining complex collaborative dynamics, especially in scenarios requiring continuous adjustments.\nZhang et al. [74] introduce the Reinforced Advantage (ReAd) framework, enhancing collaboration in simulated environments through a feedback loop that refines actions based on learned advantage functions. Though ReAd improves strategic planning efficiency, LLMs still exhibit rigidity when faced with unpredictable and dynamic conditions, where adaptive collaboration is crucial.\nMeanwhile, benchmarks such as LLMARENA [73], which assesses LLM collaboration in multi-agent environments through TrueSkill\u2122 scoring, illustrate that despite models like GPT-4 showing improvements in structured environments, they struggle to match the real-time adaptability and"}, {"title": "3.5.4 Cultural Competency", "content": "LLMs have shown significant potential in understanding and simulating cultural behaviors, offering promising applications in diverse contexts. However, they face notable challenges in accurately applying this knowledge across cultures, particularly for minority or underrepresented groups. These challenges often lead to biased or stereotypical outputs, undermining cultural sensitivity and inclusivity.\nFor example, LLMs like GPT-4 Turbo have been tested for generating culturally relevant content in languages such as Indonesian and Sundanese [122", "123": "leading to cultural insensitivity and oversimplified stereotypes.\nTo assess cultural competency, researchers have developed benchmarks like BLEND, which includes 52.6k question-answer pairs spanning 16 countries and 13 languages, including low-resource languages such as Amhar"}]}