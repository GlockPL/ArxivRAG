{"title": "Schema Matching with Large Language Models: an Experimental Study", "authors": ["Marcel Parciak", "Brecht Vandevoort", "Frank Neven", "Liesbet M. Peeters", "Stijn Vansummeren"], "abstract": "Large Language Models (LLMs) have shown useful applications in a variety of tasks, including data wrangling. In this paper, we investigate the use of an off-the-shelf LLM for schema matching. Our objective is to identify semantic correspondences between elements of two relational schemas using only names and descriptions. Using a newly created benchmark from the health domain, we propose different so-called task scopes. These are methods for prompting the LLM to do schema matching, which vary in the amount of context information contained in the prompt. Using these task scopes we compare LLM-based schema matching against a string similarity baseline, investigating matching quality, verification effort, decisiveness, and complementarity of the approaches. We find that matching quality suffers from a lack of context information, but also from providing too much context information. In general, using newer LLM versions increases decisiveness. We identify task scopes that have acceptable verification effort and succeed in identifying a significant number of true semantic matches. Our study shows that LLMs have potential in bootstrapping the schema matching process and are able to assist data engineers in speeding up this task solely based on schema element names and descriptions without the need for data instances.", "sections": [{"title": "1. Introduction", "content": "Schema matching [1] constitutes a core task in data inte- gration [2]. It refers to the problem of identifying seman- tic correspondences between elements of two relational schemas that represent the same real-world concept. For example, a schema matching system may conclude that an attribute admittime from one table in a medical infor- mation system semantically corresponds to an attribute visit_start_date in another table. Once correspon- dences are identified, they can be used to translate data from the source schema into data conforming to the tar- get schema [2], a process known as schema mapping. In this paper, we focus on schema matching.\nSchema matching systems are software systems that help data engineers perform schema matching. They generate a set of match candidates (i.e., candidate cor- respondences) which the data engineer can accept, re- ject or edit in order to obtain a final set of matches [1].\nTo generate match candidates, a wide variety of signals that hint at element correspondence have been consid- ered in the research literature. These include syntac- tic similarity of attribute names; consulting thesauri; looking at data values and their distributions in con- crete database instances; and exploiting database con- straints [1, 3, 4, 5]. Unfortunately, many such signals remain unavailable in real-world schemas [6]: attribute names are often cryptic and involve domain-specific abbreviations not occurring in thesauri. Use of actual data values and concrete database instances may be re- stricted for legal reasons; e.g., this is the case in the health domain where real database instances are problematic to obtain due to privacy constraints. In the absence of available real instances, one may consider leverag- ing synthetically-generated instances to aid in matching. However, accurately replicating the complexity and sub- tle patterns of real medical data is highly challenging and time-consuming, and rigorous validation of gener- ated schema matches is hindered by the lack of a true ground truth. In the health domain setting, it is hence vital to be able to generate match candidates with as little information as possible.\nIn the healthcare data integration context, we found that, despite its restrictions, we often have schema docu- mentation in the form of data dictionaries available, as well as natural-language descriptions of some schema elements. In particular, target schemas are often common data models: data schemas designed by community con- sensus that harmonize healthcare data [7]. These data models are well documented, explaining the semantics in the setting where, except for schema documentation,\n    such signals are unavailable.\nLLMs are general machine learning models trained on large and generic natural text data, such as the web. They are able to solve a variety of tasks with no or minimal fine-tuning effort [10]. In the field of data management, LLMs have shown promising results for data wrangling tasks such as error detection and data imputation [11]. However, except for [12], they have not been widely applied to schema matching, yet.\nZhang et al [12] also use language models for instance- free schema matching, but employ and fine-tune an encoder-only model (BERT). By contrast, we use an off- the-shelf generative decoder-only model (GPT) without any need for fine-tuning.\nAlso related is SMAT [13] which uses an attention- based neural network to match GLoVe embeddings [14] of schema elements, but requires a majority of the data to be labelled: 80% of the data that needs to be matched is used for training, and subsequently an additional 10% is used for tuning weights, leaving only 10% to evaluate the model. For practical applications, this presents a sig- nificant limitation, as requiring 90% of the input schemas to be labeled, amounts to almost completely matching the schemas by hand. Our approach, however, does not require any labelled data, allowing an off-the-shelf usage.\nAdnEV [15] proposes a methodology based on deep learning and weak supervision to adjust and combine different schema matching algorithms. In this work, we observe that it makes sense to combine different task scopes to achieve the greatest effectiveness. It is an in- teresting direction for future work whether approaches such as AdnEV can be used to make this combination even more effective.", "content_type": "text"}, {"title": "2. Methods", "content": "Schema Matching. For the purpose of this paper, a schema refers to a relational schema, i.e., a finite set of at- tributes. A 1:1 match between two schemas $S_1$ and $S_2$ is a pair (a, b) \u2208 $S_1$ \u00d7 $S_2$ that is meant to indicate that there is a semantic correspondence between attribute a \u2208 $S_1$ and b \u2208 $S_2$. Because in the schema mapping phase we should be able to unambiguously map data values of at- tribute a into data values of attribute b (and vice versa) we say that (a, b) is a (semantically) valid 1:1 match if there exists an invertible function mapping values of a into values of b. We define schema matching to be the problem of deriving a set of valid 1:1 matches between two given schemas. We note that in the literature, also matches of kind 1 : m, n : 1 and n : m exist. For example, in\n\u00b9In this paper, we assume that the source and target table are already provided, the table matching step, i.e., identifying corresponding\ntables, has thus already been completed.", "content_type": "text"}, {"title": "String Similarity Baseline", "content": "We aim to compare the performance of the LLM-based approaches against a baseline based on a string similarity measure, a well- established baseline approach in the field of schema map- ping and ontology alignment [21, 22]. To do so, we have selected edit distance-based metrics investigated by [21] and [22] and checked for their availability in the common Python library textdistance. We aim to find com- monly used similarity metrics that are readily available and identified four metrics: Jaro Winkler, Levenshtein, Monge Elkan and N-gram. We evaluated these metrics on our benchmark by calculating the similarities between the attribute names for each attribute combination in the benchmark. It is important to note that these attribute pairs are the same as those used in our results, although we do not report dataset-specific values here. We then generate a ranking of all attribute pairs and calculate the precision and recall for each threshold per similarity mea- sure. Figure 2 displays the corresponding precision-recall curve and reveals that N-gram with n = 3 is the best per- forming metric (w.r.t. the area under curve). This string similarity metric will therefore be used in the following as a baseline.\nSpecifically, to obtain the baseline we calculate the N- gram string similarity $sim_{NG}(a, b)$ between all possible attribute pairs in a dataset. For each attribute name a' we obtain the set of its 3-grams a after padding with special characters as described by Sun et al. [21]. For example, the name admittime is transformed into the set {##a, #ad, adm, dmi, min, int, ntt, tti, tim, ime, me%, e%%}. For two sets a and b, we then calculate the Dice similarity:\n$sim_{NG}(a, b) := \\frac{2 \\times |a \\cap b|}{|a| + |b|}$\nSince each $sim_{NG}(a, b)$ lies in the range [0; 1], this de- fines an order on match candidates, with highest val- ues appearing first. One can either set a threshold 0 to decide which similarity value is sufficient for a match or or limit the number of matches to the top k ranked ones. Using thresholding, all pairs $sim_{NG}(a, b) \\geq 0$ will then be output as a match, and using ranking, all pairs\n\u00b2https://pypi.org/project/textdistance\n${(a_0, b_0),..., (a_n, b_n)}[1 : k]$ where $sim_{NG}(a_i, b_i) \\geq sim_{NG}(a_j, b_j)$ for all i < j will be output as a match. We choose the former and determine a separate threshold per dataset as follows: we consider all calculated similarity values as thresholds and pick the threshold that achieves the best F1-score on the dataset. We then choose $sim_{NG}$ with this threshold as the baseline for the considered dataset. This approach favors the baseline, as it overes- timates the capabilities of the N-gram string similarity for schema matching. In practice, a data engineer cannot know which threshold to use.\nThe choice of tresholding over ranking is motivated by the fact that the output of the LLM does not imply any ordering, we ask for a simple yes, no or unknown decision instead. Hence, common ranking metrics such as recall@k or mean reciprocal rank cannot be applied to the LLM results. Furthermore, we note that our approach to determine dataset-specific thresholds is equivalent to choosing a dataset-specific k that maximizes the F1-score when interpreting the simNG as a ranking.", "content_type": "text"}, {"title": "Experimental Setup", "content": "For a fixed dataset and fixed task scope, an experiment consists of sending the cor- respoding prompt three times to the LLM. We extract three votes from the responses and use majority vot- ing to minimize the effect of hallucinations. If an at- tribute pair is missing or there is a split decision, this pair is considered unknown. Each experiment is re- peated five times. The results are compared against our benchmark by means of (i) the F1-score, the har- monic mean between precision and recall w.r.t. the ground-truth semantically valid matches, and (ii) a deci- siveness-score, indicating the fraction of non-unknown votes. We use OpenAI's gpt-3.5-turbo-0125 and gpt-4-0125-preview models with default settings, ac- knowledging the fact that performance could be im- proved with tuning the settings. Jupyter notebooks that we used to obtain the results can be found in our artefacts repository [18].", "content_type": "text"}, {"title": "3. Results", "content": "We next present the findings of our experimental study on schema matching using LLMs. Section 3.1 focuses on the quality of the schema matching results generated by the different separate task scopes, whereas Section 3.2 addresses their complementarity and the benefits of com- bining task scopes.", "content_type": "text"}, {"title": "3.1. Quality of schema matching", "content": "We begin by evaluating the quality of schema matching results produced by the different task scopes, using F1- scores for comparison both among the LLMs and against the baseline (Section 3.1.1). Next, we assess the decisive- ness of the LLMs in their opinions on attribute pairs in Section 3.1.2. Finally, we analyze the consistency of our experiments across various task scopes and datasets in Section 3.1.3, reporting the standard deviation of F1-score, precision, and recall to illustrate the expected variance when using LLMs for schema matching.", "content_type": "text"}, {"title": "3.1.2. Decisiveness", "content": "In the course of our experiments, we observed that the LLM often fails to express an opinion on all attribute pairs requested. We summarize this behavior in the deci- siveness score shown in Table 3. This score captures the ratio of attribute pairs that received a yes or no vote-so not an unknown-to all attribute pairs per dataset. As the name already indicates it measures how decisive the model is. On most datasets, the following inequality holds: 1-to-1 > N-to-1 > 1-to-N > N-to-M. We clearly see that increasing the amount of information per prompt decreases the decisiveness. With GPT-3.5, the N-to-1 task scope remains in an acceptable range, 1-to-Nfluctuates between datasets while N-to-Mis con- sistently in an unacceptable range. The use of GPT-4 improves the decisiveness considerably for N-to-1 and 1-to-N. Interestingly, the decisiveness of N-to-M does not profit from the larger model.\nGiven the low quality of results for 1-to-1, the high decisiveness indicates that using the 1-to-1 task scope makes the wrong decision most of the time. This supports our decision to exclude 1-to-1 from further experiments with GPT-4. The extremely low decisiveness of N-to-M, however, may indicate that the complexity of the output could play a major role in the low quality of its results. As previously mentioned, the output of N-to-M is a list of tuples of attribute names while it is sufficient to simply list attribute names for 1-to-N and N-to-1.\nConclusion. An increase of context information per prompt decreases the number of attribute pairs an LLM expresses an opinion on. While this effect can be miti- gated using GPT-4 for 1-to-N and N-to-1, this is not the case for N-to-M.", "content_type": "text"}, {"title": "3.1.3. Consistency", "content": "We have been reporting results with respect to the me- dian. Given that we conducted each experiment five times, it is interesting to investigate the consistency of the experiment results. We do so by reporting the standard deviation of F1-scores, precision and recall in Table 4. We see that, on average, 1-to-N and N-to-1 have low stan- dard deviations with 0.074 and 0.062, respectively. Both 1-to-1 (0.141) and N-to-M (0.160) have higher stan- dard deviations, N-to-M reaching the maximum across the whole table. Using GPT-4, the results increase in consistency. N-to-1 reaching the overall minimum with 0.031 followed by 1-to-N with 0.037. N-to-M remains the least consistent but improves to 0.094.\nConclusion. We find that the standard deviation of the F1-scores remains in acceptable ranges (< 0.1) for 1-to-N and N-to-1 on both models. With GPT-4, all standard deviations improve further. We conclude that LLMs are consistent enough to be used in practice for schema matching.", "content_type": "text"}, {"title": "3.2. Complementarity", "content": "It is rare to find matching methods that combine high recall with high precision. Since in practical data integra- tion scenarios one needs to manually verify the match candidates that are proposed by an automated matching algorithm, its preferable from a practical viewpoint to use a matching algorithm that has very high recall (to ensure that no candidates are missed) while featuring a decent precision (to ensure that the verification effort remains manageable). From Table 2, we observe that using LLMs often enhances recall compared to the base- line, with this improvement being more pronounced for GPT-4 than for GPT-3.5. Given this observation, we investigate in this section how complementary the dif- ferent tasks scopes are with the baseline and each other.\nFor, if the sets of matching candidates returned by dis- tinct methods A and B are largely complementary (in the sense that there is little overlap between the returned sets), we could further increase recall by combining the methods A and B into a method A&B: the combined method simply returns the union of the matches of A and B. We must take care, however, as while the recall of A&B may increase compared to A and B alone, its precision will almost certainly decrease. As such, we are also interested in quantifying whether the verification effort for A&B remains reasonable.\nOur results in this section are computed using the fol- lowing methodology. We refer to an element of {simNG, 1-1, 1-N,N-1, N-M } as a method. Remember from Sec- tion 2 that per method we have repeated each experiment five times. Consequently per pair (S1, S2) of distinct methods we have 25 experiment pairs (E1, E2). We take the union of the matches resulting from E\u2081 and E2 and analyze this combined match w.r.t. the number of true positives, the recall, precision, etc. Per pair of methods (S1, S2) we may compute a dataset-specific average of these methods by summing the metric result over all 25 experiment pairs, and taking the average. Importantly, we only combine methods using the same LLM model (i.e. both use GPT-3.5 or both use GPT-4). Concretely, in Section 3.2.1 we analyze complementarity of matches by investigating how many additional true positive semantic matches may be recovered when combining methods. In Section 3.2.2 we offset study by the verification effort re- quired when combining methods. Finally, in Section 3.2.3, we analyze the F1-scores for every method combination.", "content_type": "text"}, {"title": "4. Conclusion", "content": "In this study, we took an initial step towards utilizing LLMs for schema matching. We found that matching quality diminishes when there is insufficient context in- formation (i.e., task scope 1-to-1) and when there is an excess of context information (i.e., task scope N-to-M). The latter is likely hindered by the more complex output format and the larger number of pairs requiring deci- sions. The 1-to-N and N-to-1 task scopes effectively provide sufficient context to make accurate matches with- out overwhelming the decision-making process. This balance results in a better overall performance of which the recall can be even further enhanced by adopting a combined approach using both task scopes in tandem. This combined method successfully identifies a signifi- cant number of true semantic matches with an accept- able verification effort. As such, we recommend using the combined (1-to-N, N-to-1) method in practice. We also found that using GPT-4 over GPT-3.5 improves matching quality and consistency over all task scopes tested on both models, and (except for N-to-M) increases decisive- ness and reduces the verification effort. The results in this paper demonstrate that LLMs have the potential to bootstrap the schema matching process and assist data engineers in speeding this task solely based on schema element names and descriptions, without the need for data instances and improving over attribute-name-based matching alone.\nWe outline some directions for future work that seem promising.\nA benefit of LLMs over the string similarity baseline is that they can be instructed to provide an explanation as to why they identify a certain attribute pair as a match or a non-match. We believe that such explanations can be a valuable instrument for a data engineer tasked to construct a schema mapping, to identify and rectify mis- classifications. Through initial experiments, we have observed that the LLM sometimes jumps to conclusions as it overemphasizes similarity of attribute names while disregarding the intent of the attributes as described in the provided documentation. For instance, we noticed that the LLM is eager to match two attributes solely based on the fact that they both refer to the time dimension of an event even when those events are clearly different. We are currently working on a tool that facilitates refining schema matchings via natural language feedback in a pragmatic and user-friendly way.\nOur benchmark consists of publicly available schemas. In future experiments, we will apply our approach on pro- prietary schemas, aiming to illustrate the usefulness of using LLMs for schema matching in real-world scenarios.", "content_type": "text"}]}