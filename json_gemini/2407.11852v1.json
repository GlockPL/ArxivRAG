{"title": "Schema Matching with Large Language Models: an Experimental Study", "authors": ["Marcel Parciak", "Brecht Vandevoort", "Frank Neven", "Liesbet M. Peeters", "Stijn Vansummeren"], "abstract": "Large Language Models (LLMs) have shown useful applications in a variety of tasks, including data wrangling. In this paper, we investigate the use of an off-the-shelf LLM for schema matching. Our objective is to identify semantic correspondences between elements of two relational schemas using only names and descriptions. Using a newly created benchmark from the health domain, we propose different so-called task scopes. These are methods for prompting the LLM to do schema matching, which vary in the amount of context information contained in the prompt. Using these task scopes we compare LLM-based schema matching against a string similarity baseline, investigating matching quality, verification effort, decisiveness, and complementarity of the approaches. We find that matching quality suffers from a lack of context information, but also from providing too much context information. In general, using newer LLM versions increases decisiveness. We identify task scopes that have acceptable verification effort and succeed in identifying a significant number of true semantic matches. Our study shows that LLMs have potential in bootstrapping the schema matching process and are able to assist data engineers in speeding up this task solely based on schema element names and descriptions without the need for data instances.", "sections": [{"title": "1. Introduction", "content": "Schema matching [1] constitutes a core task in data inte- gration [2]. It refers to the problem of identifying seman- tic correspondences between elements of two relational schemas that represent the same real-world concept. For example, a schema matching system may conclude that an attribute admitt ime from one table in a medical infor- mation system semantically corresponds to an attribute visit_start_date in another table. Once correspon- dences are identified, they can be used to translate data from the source schema into data conforming to the tar- get schema [2], a process known as schema mapping. In this paper, we focus on schema matching.\nSchema matching systems are software systems that help data engineers perform schema matching. They generate a set of match candidates (i.e., candidate cor- respondences) which the data engineer can accept, re- ject or edit in order to obtain a final set of matches [1].\nTo generate match candidates, a wide variety of signals that hint at element correspondence have been consid- ered in the research literature. These include syntac- tic similarity of attribute names; consulting thesauri; looking at data values and their distributions in con- crete database instances; and exploiting database con- straints [1, 3, 4, 5]. Unfortunately, many such signals remain unavailable in real-world schemas [6]: attribute names are often cryptic and involve domain-specific abbreviations not occurring in thesauri. Use of actual data values and concrete database instances may be re- stricted for legal reasons; e.g., this is the case in the health domain where real database instances are problematic to obtain due to privacy constraints. In the absence of available real instances, one may consider leverag- ing synthetically-generated instances to aid in matching. However, accurately replicating the complexity and sub- tle patterns of real medical data is highly challenging and time-consuming, and rigorous validation of gener- ated schema matches is hindered by the lack of a true ground truth. In the health domain setting, it is hence vital to be able to generate match candidates with as little information as possible.\nIn the healthcare data integration context, we found that, despite its restrictions, we often have schema docu- mentation in the form of data dictionaries available, as well as natural-language descriptions of some schema elements. In particular, target schemas are often common data models: data schemas designed by community con- sensus that harmonize healthcare data [7]. These data models are well documented, explaining the semantics in the setting where, except for schema documentation, such signals are unavailable.\nIn this paper, we aim to exploit this information and present an experimental study on schema matching using an off-the-shelf generative Large Language Model (LLM). We investigate how LLMs can be prompted to generate a set of match candidates. We focus on the use of schema documentation as the sole signal and evaluate the perfor- mance against a newly defined real-world benchmark. We define different task scopes for doing LLM-based schema matching. Task scopes are prompting methods they vary primarily in the amount of context information contained in the prompt. Using these task scopes, we aim to answer the following research questions:\n1. How does the quality of LLM-based schema matching vary among different task scopes and LLM models, and how does it compare to a string- similarity-based baseline?\n2. How decisive are LLMs in expressing their opin- ions on attribute pairs, and how does this affect their reliability and consistency?\n3. What is the extent of the complementarity be- tween the match results for different task scopes and the baseline?\n4. Is it useful and practical to combine different LLM- based and/or string-similarity-based matchings?\nTo answer these questions, we introduce the schema matching task and experimental setup in Section 2. There, we also introduce the different task scopes. We then present and discuss our findings w.r.t. the first two re- search questions in Section 3.1 and investigate the last two questions in Section 3.2. We conclude in Section 4.\nLLMs are general machine learning models trained on large and generic natural text data, such as the web. They are able to solve a variety of tasks with no or minimal fine-tuning effort [10]. In the field of data management, LLMs have shown promising results for data wrangling tasks such as error detection and data imputation [11]. However, except for [12], they have not been widely applied to schema matching, yet.\nZhang et al [12] also use language models for instance-free schema matching, but employ and fine-tune an encoder-only model (BERT). By contrast, we use an off- the-shelf generative decoder-only model (GPT) without any need for fine-tuning.\nAlso related is SMAT [13] which uses an attention- based neural network to match GLoVe embeddings [14] of schema elements, but requires a majority of the data to be labelled: 80% of the data that needs to be matched is used for training, and subsequently an additional 10% is used for tuning weights, leaving only 10% to evaluate the model. For practical applications, this presents a sig- nificant limitation, as requiring 90% of the input schemas to be labeled, amounts to almost completely matching the schemas by hand. Our approach, however, does not require any labelled data, allowing an off-the-shelf usage.\nAdnEV [15] proposes a methodology based on deep learning and weak supervision to adjust and combine different schema matching algorithms. In this work, we observe that it makes sense to combine different task scopes to achieve the greatest effectiveness. It is an in- teresting direction for future work whether approaches such as AdnEV can be used to make this combination even more effective."}, {"title": "2. Methods", "content": "Schema Matching. For the purpose of this paper, a schema refers to a relational schema, i.e., a finite set of at- tributes. A 1:1 match between two schemas S1 and S2 is a pair (a, b) \u2208 S1 \u00d7 S2 that is meant to indicate that there is a semantic correspondence between attribute a \u2208 S1 and b \u2208 S2. Because in the schema mapping phase we should be able to unambiguously map data values of at- tribute a into data values of attribute b (and vice versa) we say that (a, b) is a (semantically) valid 1:1 match if there exists an invertible function mapping values of a into values of b. We define schema matching to be the problem of deriving a set of valid 1:1 matches between two given schemas.\u00b9 We note that in the literature, also matches of kind 1 : m, n : 1 and n : m exist. For example, in\n\u00b9In this paper, we assume that the source and target table are already provided, the table matching step, i.e., identifying corresponding tables, has thus already been completed.\na match of kind 1: m we may relate a single attribute a in S\u2081 to a set of attributes BC S2, meaning that the information of a-values in S1 will be \"distributed\" among all the attributes in B and that we need all attributes in B to recover the a-value. A typical example is relating Name in S\u2081 to B = {First name, Last name}. In this paper, we restrict ourselves to 1:1 matches for two rea- sons. First, this shrinks the search space significantly for possible matches, making our experimental approach feasible even for larger schemas. Second, it allows us to compare our results to a baseline using string similarity measures, which are difficult to extend to 1: m, n : 1 or n: m matches.\nBenchmark. In order to gauge the suitability of LLMs for schema matching we have created a new benchmark, situated in the healthcare domain. We draw source schemas from the MIMIC-IV dataset [16] and target schemas from OHDSI OMOP Common Data Model [7]. Both are public, well-known data models in the medical informatics community. The OHDSI community main- tains an ETL process to transform data from MIMIC-IV to OMOP [17]. We use this ETL specification to manually identify all semantically valid 1:1 matches that will serve as the ground truth. That is, we manually inspect all ap- plied ETL transformations and derive each attribute com- bination (a, b) where a single value from the source at- tribute a is sufficient to determine the value from the tar- get attribute b and vice versa. For example, the attribute gender from MIMICs Patients table is mapped to both gender_concept_id and gender_source_value of OMOPs Person table. Both mappings are valid 1:1 matches, as the value in gender can be mapped to a valid value fit for either attribute and vice versa. In con- trast, the attribute admittime of MIMICs Admissions table is not a valid match for visit_start_datetime of OMOPs Visit_Occurrence table, as the ETL spec- ification needs to combine it with another attribute to determine the value of visit_start_datetime.\nWe have extracted a total of 49 valid 1:1 matches be- tween 7 relations from MIMIC-IV and 6 relations from OMOP. In total, there are 9 relation pairs that contain at least one semantic match. We will refer to each of these relation pairs as a dataset in our benchmark. Our 9 datasets create a search space of 1839 attribute pairs that contain 49 true semantic 1:1 matches as summarized in Table 1. We consider all other attribute pairs as non- matches. The schema matching problem is hence highly imbalanced. For each (source or target) table and each attribute we extract the name as well as a natural lan- guage description from respective documentations. Our benchmark is publicly available in our artefacts reposi- tory [18].\nWe acknowledge that a benchmark consisting of pub- lic datasets is probably contained in the training data of an LLM trained on the web. As an example, when asking ChatGPT to give a description of the attribute dischtime from the admissions table in MIMIC-IV, the answer returned from the model fits the description given in the official documentation of MIMIC-IV well. This represents a limitation of our experimental setup. We argue that although the datasets are known to the LLM, the true semantic matches to transform data from MIMIC-IV to OMOP are not readily explicitly available: significant effort is required to extract them from the ETL scripts.\nPrompt Engineering. Generative LLMs are trained to answer questions in natural language. As such, we need to interface with the LLM via prompts that describe the task to be performed by the LLM as well as the table and attribute names and descriptions. Previous research into prompt engineering has proposed a number of prompt en- gineering patterns that positively influence answer qual- ity [11]. We next discuss how we have applied these common practices in our prompt design by means of the visual representation in Figure 1. Each prompt is always applied to a single source schema and a single tar- get schema (plus their descriptions), and consists of four sections referred to as Introduction, Source Information, Target Information, and Task Description.\nFirst, we introduce the schema matching problem to the LLM by utilizing the Persona Pattern to let the LLM act as a schema matcher [19]. We then introduce our definition of a valid 1:1 match using the Meta Language Creation pattern [19]. Both patterns are illustrated in the Introduction section in Figure 1.\nSubsequently, we serialize the schema information, including table and attribute descriptions, using a serial- ization inspired by [11]. Concretely, we first serialize the source information, followed by the target information. An example of this can be viewed in Figure 1 in the Source Information and Target Information sections.\nWe finalize our prompts with the task description that utilizes the phrase \"Lets think step by step\" which has been shown to increase performance by instructing the LLM to build up a step-by-step argument in the out- put [20] and to which we refer as the Chain of Thought Pattern in Figure 1. We end the task description with the Output Automater pattern to instruct the model to output structured data (in particular: JSON) for further processing [19]. Here, we ask the LLM to generate a struc- tured output such that we can extract (a, b, out) triples, where a and b are attributes from the source and target schema, respectively, and out (discussed further below) is the LLM's opinion of whether (a, b) is a semantically meaningful 1:1 match. Both patterns are illustrated in the example prompt in Figure 1 in section Task Description.\nDuring our experiments, we found that using a three- step scale for out works best. We ask the LLM to use yes for a match, no for a non-match, and unknown if there is not enough information to decide. We have also exper- imented with numerical scores, which were difficult to interpret, and five-step scales, which were prone to hal- lucinations. For example, asking for a five-step scale of no correspondence, low correspondence, medium correspon- dence, high correspondence and very high correspondence frequently resulted in opinions such as low to medium correspondence, making a reliable interpretation highly difficult. We note that LLM output is not necessarily com- plete: there may be attribute pairs (a, b) for which the LLM does not give its opinion; we treat this as unknown.\nTask Scopes. In this paper, we focus on a comparison of task scopes, which we define as the amount of schema information contained in a single prompt. We define four different scopes:\n1-to-1 Each prompt contains exactly one attribute from source and one from target.\n1-to-N Each prompt contains a single attribute from the source schema and N attributes of the target schema, where N refers to the total number of attributes in the target schema.\nN-to-1 Each prompt contains N attributes from the source schema and a single attribute from the target schema, where N refers to the total num- ber of attributes in the source schema.\nN-to-M Each prompt contains N attributes from the source schema and M attributes from the target schema, where N and M refer to the total num- ber of attributes in the source and target schema, respectively.\nIt is worth noting that the task scope choice has implica- tions on the complexity to parse structured votes from the LLM output. While we expect a single vote (e.g. yes\nor no) in the 1-to-1 case, an output to the N-to-M task scope potentially contains N \u00d7 M votes, one for each attribute pair.\nWe investigate both 1-to-N and N-to-1 as both scopes present very different contexts to the LLM. In the former, the LLM is presented with all available in- formation about the target relation while limiting the information of the source information, and vice versa in the latter. We found that this difference impacts the quality of the matches.\nString Similarity Baseline. We aim to compare the performance of the LLM-based approaches against a baseline based on a string similarity measure, a well- established baseline approach in the field of schema map- ping and ontology alignment [21, 22]. To do so, we have selected edit distance-based metrics investigated by [21] and [22] and checked for their availability in the common Python library textdistance. We aim to find com- monly used similarity metrics that are readily available and identified four metrics: Jaro Winkler, Levenshtein, Monge Elkan and N-gram. We evaluated these metrics on our benchmark by calculating the similarities between the attribute names for each attribute combination in the benchmark. It is important to note that these attribute pairs are the same as those used in our results, although we do not report dataset-specific values here. We then generate a ranking of all attribute pairs and calculate the precision and recall for each threshold per similarity mea- sure. Figure 2 displays the corresponding precision-recall curve and reveals that N-gram with n = 3 is the best per- forming metric (w.r.t. the area under curve). This string similarity metric will therefore be used in the following as a baseline.\nSpecifically, to obtain the baseline we calculate the N- gram string similarity $sim_{NG}(a, b)$ between all possible attribute pairs in a dataset. For each attribute name a' we obtain the set of its 3-grams a after padding with special characters as described by Sun et al. [21]. For example, the name admittime is transformed into the set {##a, #ad, adm, dmi, min, int, ntt, tti, tim, ime, me%, e%%}. For two sets a and b, we then calculate the Dice similarity:\n$sim_{NG}(a, b) := \\frac{2 \\times |a \\cap b|}{|a|+|b|}$\nSince each $sim_{NG}(a, b)$ lies in the range [0; 1], this de- fines an order on match candidates, with highest val- ues appearing first. One can either set a threshold 0 to decide which similarity value is sufficient for a match or or limit the number of matches to the top k ranked ones. Using thresholding, all pairs $sim_{NG}(a, b) \\geq 0$ will then be output as a match, and using ranking, all pairs {(ao, bo),..., (an, bn)}[1 : k] where $sim_{NG}(ai, bi) \\geq sim_{NG}(aj, bj)$ for all i < j will be output as a match. We choose the former and determine a separate threshold per dataset as follows: we consider all calculated similarity values as thresholds and pick the threshold that achieves the best F1-score on the dataset. We then choose simNG with this threshold as the baseline for the considered dataset. This approach favors the baseline, as it overes- timates the capabilities of the N-gram string similarity for schema matching. In practice, a data engineer cannot know which threshold to use.\nThe choice of tresholding over ranking is motivated by the fact that the output of the LLM does not imply any ordering, we ask for a simple yes, no or unknown decision instead. Hence, common ranking metrics such as recall@k or mean reciprocal rank cannot be applied to the LLM results. Furthermore, we note that our approach to determine dataset-specific thresholds is equivalent to choosing a dataset-specific k that maximizes the F1-score when interpreting the simNG as a ranking."}, {"title": "3. Results", "content": "We next present the findings of our experimental study on schema matching using LLMs. Section 3.1 focuses on the quality of the schema matching results generated by the different separate task scopes", "scope": "it fails to achieve the maximal F1-score on any dataset and is worse than (or on par with) the N-gram baseline", "1-to-N": "adding context information improves matching quality. Using GPT-4", "holds": 1, "A&B": "the combined method simply returns the union of the matches of A and B. We must take care"}, 2, 5, 2, ".", 2, ".", 2, 5, ".2.1", ".2.2", ".2.3", 1, 40, 9, ".", 35.4, 5, "2.4.", 10, 136.2, 104.6, 30.0, ".", 83.6, 0, 83.6, 0, 0, 10, 1, ".", 1, 1, 0.384, ".344.", 1, 1, 1, 0.456, 1, "ions", 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, 1, "o"]}