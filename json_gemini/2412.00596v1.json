{"title": "PhyT2V: LLM-Guided Iterative Self-Refinement for Physics-Grounded Text-to-Video Generation", "authors": ["Qiyao Xue", "Xiangyu Yin", "Boyuan Yang", "Wei Gao"], "abstract": "Text-to-video (T2V) generation has been recently enabled by transformer-based diffusion models, but current T2V models lack capabilities in adhering to the real-world common knowledge and physical rules, due to their limited understanding of physical realism and deficiency in temporal modeling. Existing solutions are either data-driven or require extra model inputs, but cannot be generalizable to out-of-distribution domains. In this paper, we present PhyT2V, a new data-independent T2V technique that expands the current T2V model's capability of video generation to out-of-distribution domains, by enabling chain-of-thought and step-back reasoning in T2V prompting. Our experiments show that PhyT2V improves existing T2V models' adherence to real-world physical rules by 2.3x, and achieves 35% improvement compared to T2V prompt enhancers. The source codes are available at: https://github.com/pittisl/PhyT2V.", "sections": [{"title": "1 Introduction", "content": "Text-to-video (T2V) generation has recently marked a significant breakthrough of generative AI, with the advent of transformer-based diffusion models such as Sora [3], Pika [13] and CogVideoX [47] that can produce videos conditioned on textual prompts. These models demonstrate astonishing capabilities of generating complex and photorealistic scenes, and could even make it difficult for humans to distinguish between real-world and AI-generated videos, in the aspect of individual video frames' quality [33, 1].\nOn the other hand, as shown in Figure 1 - Left, current T2V models still have significant drawbacks in adhering to the real-world common knowledge and physical rules, such as quantity, material, fluid dynamics, gravity, motion, collision and causality, and such limitations fundamentally prevent current T2V models from being used for real-world simulation [7, 27, 15]. Enforcement of real-world knowledge and physical rules in T2V generation, however, is challenging because it requires the models' understandings of not only individual objects but also how these objects move and interact with each other. Further, unlike generating static images, T2V generation requires frame-to-frame consistency in object appearance, shape, motion, lighting and other dynamics [11]. Current T2V models often lack such temporal modeling, especially over long sequences [16], and the generated videos often contain flickering, inconsistent motion and object deformations across frames [22].\nMost of the existing solutions to these challenges are data-driven, by using large multimodal T2V datasets that cover different real-world domains to train the diffusion model [45, 10, 37]. However, these solutions heavily rely on the volume, quality and diversity of datasets [47, 38]. Since real-world common knowledge and physical rules are not explicitly embedded in the T2V generation process, the quality of video generation would largely drop in out-of-distribution domains that are not covered by the training dataset, and the generalizability of T2V models is limited due to the vast diversity of real-world scenario domains. Alternatively, researchers also use the existing 3D engines (e.g, Blender [8], Unity3D [32] and Unreal [12]) or mathematical models of edge and depth maps [23, 24, 22] to inject real-world physical knowledge into the T2V model, but these approaches are limited to fixed physical categories and patterns such as predefined objects and movements [45, 22], similarly lacking generalizability.\nTo achieve generalizable enforcement of physics-grounded T2V generation, we propose a fundamentally different approach: instead of expanding the training dataset or further complicating the T2V model architecture, we aim to expand the current T2V model's capability of video generation from in-distribution to out-of-distribution domains, by embedding real-world knowledge and physical rules into the text prompts with sufficient and appropriate contexts. To avoid ambiguous and unexplainable prompt engineering [9, 29, 28], our basic idea is to enable chain-of-thought (CoT) and step-back reasoning in T2V generation prompting, to ensure that T2V models follow correct physical dynamics and inter-frame consistency by applying step-by-step guidance and iterative refinement.\nBased on this idea, this paper presents Physcial-grounded Text-to-Video (PhyT2V), a new T2V technique that harnesses the natural language reasoning capabilities of well-trained LLMs (e.g, ChatGPT-40), to facilitate CoT and step-back reasoning as described above. As shown in Figure 2, such reasoning is iteratively conducted in PhyT2V, and each iteration autonomously refines both the T2V prompt and generated video in three steps. In Step 1, the LLM analyzes the T2V prompt to extract objects to be shown and physical rules to follow in the video via in-context learning. In Step 2, we first use a video captioning model to translate the video's semantic contents into texts according to the list of"}, {"title": "2 Related Work and Motivation", "content": ""}, {"title": "2.1 T2V Generation Models", "content": "Early T2V techniques generate video frames from text-to-image model outputs with temporal extensions [31], but cannot maintain temporal consistency and coherence over time, often producing visually appealing but temporally disconnected outputs. Diffusion Transformers (DiT) [26] improved such consistency with a transformer backbone capable of capturing more complex temporal dynamics and relationships across frames through attention mechanism and long-range dependency modeling [47, 38]. Based on the DiT architecture, recent T2V models, such as OpenSora [49] and VideoCrafter [4], demonstrated that T2V generation can be further improved by in-context learning when provided with sufficient contextual information [40]."}, {"title": "2.2 Using LLM in T2V Generation", "content": "LLMs with strong capabilities in natural language processing (NLP) have become a natural choice for prompt refinement in text-to-image and text-to-video generation, and existing work has utilized LLMs to interpret text prompts and orchestrate the initial layout configurations [19, 20, 21, 50, 42, 44]. However, since current LLMs generally lack inherent understandings of the real-world physical laws, using LLMs with simple instructions usually result in videos that appear visually coherent but lack accurate physical realism, particularly when generating scenes with complex object interactions. Furthermore, these approaches frequently rely on static prompts or simple iterative refinements based on bounding box and segmentation map, which may capture basic visual attributes but fail to adapt to nuanced changes that require continuous physical modeling and adjustment.\nAn effective approach to addressing these limitations and providing effective feedback for prompt refinement is to explicitly trigger in-context learning and reasoning in LLM. For example, as shown in Figure 5, CoT reasoning deconstructs complex prompts into stepwise logical tasks, and hence ensures a precise scheduling path to align generated content with the input prompt. However, CoT reasoning, in some cases, could make errors in some intermediate steps, and step-back prompting can address this limitation by further deriving the step-back question at a higher level of abstraction and hence avoiding confusions and vagueness. In our design of PhyT2V, we will utilize such LLM reasoning to analyze the inconsistency of the generated video to real-world common knowledge and physical rules, and use the reasoning outcome as feedback for T2V prompt refinement."}, {"title": "3 Method", "content": "In this section, we present details of our PhyT2V design. In principal, PhyT2V's refinement of T2V generation is an iterative process consisting of multiple rounds. In each round, as shown in Figure 6, the primary objective of our PhyT2V design is to guide a well-trained LLM (e.g., ChatGPT-40) to generate a refined prompt that enables the pre-trained T2V model to generate videos that better match the given user prompt and real-world physical rules, and the refined prompt will be iteratively used as the new user prompt in the next round of refinement.\nEach round of refinement is structured around decomposing the complex refinement problem into a series of simpler subproblems, more specifically, two parallel subproblems and one final subproblem. The two parallel subproblems are: Step 1) identifying the relevant physical rules that the generated video should follow based on the user prompt, and Step 2) identifying semantic mismatches between the user prompt and the generated video. Based on the knowledge about physical rules and semantic mismatches, the final subproblem (Step 3) generates the refined prompt to better adhere to the physical rules and resolve the mismatches.\nTo ensure proper identification in the parallel subproblems and prompt generation in the final sub-problem, the core of PhyT2V design is two types of LLM reasoning processes within the prompt enhancement loop: the local CoT reasoning for individual subproblems and global step-back reasoning for the overall prompt refinement problem.\nLocal CoT reasoning is executed within the prompt for each subproblem, to prompt the LLM to generate a detailed reasoning chain in its latent embedding space [34]. Addressing the parallel subproblems facilitates LLM with a more concentrated attention on prerequisites of prompt refinement, enabling a deeper comprehension of the physical laws that govern the video content as well as the identification of discrepancies between the generated video and the user prompt. The outcomes derived from these parallel subproblems reflect the language model's abstraction in step-back reasoning on the overarching prompt refinement.\nGlobal step-back reasoning: To integrate various subproblems into a coherent framework for prompt and video refinement, one intuitive approach involves employing CoT reasoning across these subprob-lems, allowing the LLM to engage in self-questioning. However, this method may lead to the risk of traversing incorrect reasoning pathways. Instead, we apply global step-back reasoning across subprob-lems, by using a self-augmented prompt to incorporate the LLM-generated responses to high-level questions about physical rules and semantic mismatches in earlier parallel problems, when generating"}, {"title": "3.1 Prompting in Parallel Subproblems for Local CoT Reasoning", "content": "In both Step 1 and Step 2, the first part of prompt is a task instruction prompt [I] to instruct the LLM to understand the task in the subproblem. [I] is designed with multiple components, each of which corresponds to different functions. In the first sentence, it provides general guidance to relate the current subproblem to the entire refinement problem, to better condition the subproblem answer. Afterwards, it will include detailed descriptions of the task: identifying the physical rule and main object in Step 1, and identifying the semantic mismatch between the user prompt and caption of the generated video (generated by the video captioning model) in Step 2. It will also contain the requirements about the expected information in LLM's output. For example, in Step 1, the LLM's output about the physical rule should be in a descriptive way without giving formulas.\nBesides, to ensure proper CoT reasoning, we follow the existing work [36, 18] and provide in-context examples [E] about tasks. To facilitate LLM's in-context learning [5, 6], [E] is given in the format of QA pairs. That is, instead of fine-tuning a separate LLM checkpoint for each new task, we prompt the LLM with a few input-output exemplars, to demonstrate the task and condition the task's input-output format to the LLM, to guide the LLM's reasoning process.\nThen, the final part of the prompt, denoted as [T], is the information of the current instance of the task, usually with the current user prompt $(P_i)$ being embedded. As a common practice of CoT reasoning, it also contains the hand-crafted trigger phrase (t), \u201cLet's think step by step\", to activate the local CoT reasoning in LLM.\""}, {"title": "3.2 Prompting in the Final Subproblem for Global Step-Back Reasoning", "content": "In the final subproblem, we enforce global step-back reasoning, by using the outputs of the two parallel subproblems above, i.e., knowledge about the physical rules and the prompt-video mismatch, as the high-level concepts and facts. Grounded on such high-level abstractions, we can make sure to improve the LLM's ability in following the correct reasoning path of generating the refined prompt.\nBeing similar to the prompts used in the two parallel subproblems above, the prompt structure in the final subproblem also contains [I], [E] and [T]. Furthermore, to ensure the correct reasoning path, we also provide quantitative feedback to the LLM about the effectiveness of previous round's prompt refinement. Such effectiveness could be measured by the existing T2V evaluators, which judge the semantic alignment and quality of physical common sense of the currently generated video. For example, the VideoCon-Physics evaluator [2] gives a score ([S]) between 0 and 1. If [S] is <0.5, it indicates that the refined prompt produced in the previous round is ineffective, hence guiding the LLM to take another alternative reasoning path.\nSince the prompt in the final subproblem is rich with reasoning and inherently very long-tailed, we removed the trigger prompt [t], to prevent incorporating the information in the final answer unrelated to the user's initial input prompt."}, {"title": "3.3 The Stopping Condition", "content": "The process of iterative refinement normally continues until the quality of the generated video is satisfactory, measured by the T2V evaluator as described above. Furthermore, the current T2V models naturally have limitations in generating some complicated or subtle scenes. In these cases, it would be difficult, even for PhyT2V, to reach physical realism after multiple rounds of iterations, and PhyT2V's refinement would stop when the iterations converge, i.e., the improvement of video quality becomes little over rounds."}, {"title": "4 Experiments", "content": "Models & Datasets: We applied PhyT2V on several DiT-based open-source T2V models, as listed below, and evaluated how PhyT2V improves the generated videos' adherence to real-world knowledge and physical rules. We use ChatGPT4 01-preview [14] as the LLM for reasoning, and Tarsier [35] as the video captioning model. All generated videos last 6 seconds with 10 FPS and resolution of 720\u00d7480. Details of evaluation setup are in Appendix A.\nSince we target enhancing the T2V models' capability of generating physics-grounded video contents, we use the following two prompt benchmarks that emphasize physical laws and adherence as the text prompts for T2V:"}, {"title": "4.1 Improvement of the Generated Video Quality", "content": "As shown in Table 1 and 2, when PhyT2V is applied to different T2V models, it can significantly improve the generated video's adherence to both the text prompt itself and the real-world physical rules, compared to the videos generated by vanilla T2V models (i.e., in Round 1 of PhyT2V's refinement). In particular, such improvement is the most significant on the CogVideoX-2B model, where PC improvement can be up to 2.2x and SA improvement can be up to 2.3x. On all the other models, PhyT2V can also reach noticeable improvement, ranging from 1.3x to 1.9x."}, {"title": "4.2 Different domains of Physical Rules", "content": "We also conducted in-depth analysis on PhyT2V's performance on improving the generated video's quality in different domains of real-world physical rules, using the CogVideoX-5B as the T2V model and ChatGPT 4 as the prompt enhancer. As shown in Table 5 and 6, PhyT2V achieves large improvements in most domains of physical rules. Especially in domains where physical interaction between objects are more subtle and difficult to be precisely captured, such as interaction with fluids and thermal-related scene changes, such improvements will be even higher.\nThese improvements are also exemplified with sample videos and their related input prompts in Figure 7 and Figure 8. With LLM reasoning and iterative refinement, PhyT2V can largely enhance the T2V model's capability when encountering out-of-distribution prompts, by providing correct and sufficient contexts to ensure that the T2V model's video generation correctly capture the key objects and interaction between objects. For example, when the prompt of \"juice dropping from a bottle onto the counter\", PhyT2V correctly depicts the juice's slow diffusion on the counter. More examples can be found in Appendix B."}, {"title": "4.3 Ablation Study", "content": "We conduct an ablation study to evaluate the necessity of both the physical rule reasoning (Step 1) and the mismatch reasoning (Step 2) within our PhyT2V workflow, by removing one of these steps from the refinement process to assess its impact on the quality of video generation.\nPhysical rule reasoning (Step 1). As shown in Figure 9, the Step 1 of physical rule reasoning significantly enhances the T2V process by providing a more detailed and coherent description of the principal object's physical status, such as motion, states and deformation (red texts in Figure 9), all grounded in relevant physical laws. By anchoring the prompt in established physical rules, this step also help avoid unnecessary texts (brown texts in Figure 9) and vague physical rule descriptions (purple texts in Figure 9), hence achieving a higher PC score.\nMismatch reasoning (Step 2). The Step 2 of mismatch reasoning addresses details that may have been overlooked in the previous iteration of the generated video as shown in Figure 10. This step plays a critical role in the iterative refinement process by identifying and correcting discrepancies between expected and observed outputs. By enhancing the model's focus on the principal object, the mismatch reasoning step reduces the likelihood of losing attention to important features (brown and purple texts in Figure 10), improving the fidelity and relevance of generated video content (red texts in Figure 10) towards a higher SA score.\nOverall, our study shows that both reasoning steps are integral to the PhyT2V workflow, contributing to a more robust and semantically-aligned generation of refined prompts in Step 3. Detailed ablation studies are in Appendix C."}, {"title": "5 Conclusion", "content": "In this paper, we present PhyT2V, a new T2V technique that expands the existing T2V model's capability to out-of-distribution domains via LLM reasoning. Experiment results show that PhyT2V can improve the generated video's adherence to real-world physical rules by up to 2.3x."}, {"title": "A Details of Evaluation Setup", "content": "Since our proposed technique of PhyT2V does not involve any efforts of retraining the T2V model, in this section we describe details about our evaluation setup of the LLM inference for CoT and step-back reasoning.\nIn our evaluations, we use 4 T2V generation models, including CogVideoX-5B [46], CogVideoX-2B [46], OpenSora [49] and VideoCrafter [4]. We choose to use these models because they are all built with transformer-based diffusion models, which enhance the semantic adherence by using the cross-attention mechanism, and were ranked top on the leaderboards of both the VideoPhy and PhyGenBench benchmarks. We use Tarsier [35] as the video captioning model, because it achieves SOTA performance on multiple video question answering (VQA) datasets, hence ensuring precise and detailed video captioning in our approach.\nSince PhyT2V improves the quality of generated videos through text prompt refinement, we use the Promptist [17] and GPT-40 [43] as the prompt enhancers, with the same model hyper-parameter settings as used in PhyT2V, to maintain the consistency between these baselines and PhyT2V.\nTo fit the maximum token input length of the T2V model, we limit the word length of the refined prompts to 120, by instructing the ChatGPT4 01-preview model that is used as the LLM for reasoning. To formatting the output and storage in our approach, the ChatGPT4 01-preview model are instructed to output in JSON format and output results in each step are saved in a CSV file by row. More specifically, in our implementation, we invoke the ChatGPT4 model by using the OpenAI 01 APIs, with our constructed prompts as described in Section 3. In each round of the refinement process, after Step 1 and Step 2 finish, their outputs are embedded to the prompt of Step 3 by replacing the pre-defined place holder, and we then use the generated prompt as the input to LLM again to generated the refined prompt for this round."}, {"title": "B More Examples of Physics-Grounded Videos Generated by PhyT2V", "content": "In this section, we extend Section 4 in the main text of the our paper, and provide more examples of the physics-grounded videos generated by PhyT2V. We will first show more videos generated in different categories of physical rules, and then show more details about the iterative process of prompt refinement."}, {"title": "B.1 Examples of Generated Videos in Different Categories of Physical Rules", "content": "In this subsection, we present more examples of generated videos, with the CogVideoX-5B T2V model on both the VideoPhy and PhyGenBench prompt datasets. Fig 11, Fig 12 & 13, and Fig 14 & Figure 15 show the generated videos in categories of fluid-to-fluid, solid-to-fluid, and solid-to-solid for the VideoPhy dataset. Fig 16, 17 and 18 show the generated videos in categories of mechanics, optics and thermal on the PhyGenBench dataset."}, {"title": "B.2 Details of Prompt Refinement in PhyT2V", "content": "In this subsection, we provide more detailed examples of the process of prompt refinement in PhyT2V, with the CogVideoX-5B T2V model on the VideoPhy and PhyGenBench prompt datasets. Fig 19, 20, and 21 show the generated videos in categories of solid-to-solid, solid-to-fluid, and fluid-to-fluid for the VideoPhy dataset. Fig 22, 23 and 24 show the generated videos in categories of mechanics, optics and thermal on the PhyGenBench dataset."}, {"title": "C Details of the Ablation Study", "content": ""}, {"title": "C.1 The Impact of T2V Model size", "content": "By comparing the videos generated by the CogVideoX-2B and CogVideoX-5B models with the same text prompt, we found that the PhyT2V approach can unleash more power of physical-grounded video generation and achieve better quality of the generated video, with a T2V model with larger parameter sizes. Results are show in Figure 25."}, {"title": "C.2 Components in Prompts", "content": "In this section, we remove some components from the prompts being used in two parallel subproblems and the final subproblem described in Section 3.1 and 3.2 of the main texts of the paper, and investigate how such removal affects the video generation. Results are shown in Figure 26, 27 and 28, respectively. These results show that, without the first sentence of role indicator (\"You are a physics expert.\"), the generated output content is lake of precise information, On the other hand, when the in-context examples ([E]) are missing, the LLM can not generate the output in the expected format."}, {"title": "D Difficult T2V Cases", "content": "Finally, we also found that, even when using the PhyT2V to refine the T2V process for multiple rounds, the generated video still failed to correctly reflect some real-world common knowledge. For example, Figure 29 shows that the generated video would fail to precisely generate the human body, especially the human hands. We believe that PhyT2V's difficulty in these cases is consistent with the limitations of the existing T2V models, and can only be addressed by further improving the design of T2V model architectures."}]}