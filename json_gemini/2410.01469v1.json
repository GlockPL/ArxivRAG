{"title": "TIGER: TIME-FREQUENCY INTERLEAVED GAIN\nEXTRACTION AND RECONSTRUCTION FOR EFFICIENT\nSPEECH SEPARATION", "authors": ["Mohan Xu", "Kai Li", "Guo Chen", "Xiaolin Hu"], "abstract": "In recent years, much speech separation research has focused primarily on im-\nproving model performance. However, for low-latency speech processing sys-\ntems, high efficiency is equally important. Therefore, we propose a speech separa-\ntion model with significantly reduced parameters and computational costs: Time-\nfrequency Interleaved Gain Extraction and Reconstruction network (TIGER).\nTIGER leverages prior knowledge to divide frequency bands and compresses fre-\nquency information. We employ a multi-scale selective attention module to extract\ncontextual features, while introducing a full-frequency-frame attention module\nto capture both temporal and frequency contextual information. Additionally, to\nmore realistically evaluate the performance of speech separation models in com-\nplex acoustic environments, we introduce a dataset called EchoSet. This dataset\nincludes noise and more realistic reverberation (e.g., considering object occlusions\nand material properties), with speech from two speakers overlapping at random\nproportions. Experimental results showed that models trained on EchoSet had\nbetter generalization ability than those trained on other datasets to the data col-\nlected in the physical world, which validated the practical value of the EchoSet.\nOn EchoSet and real-world data, TIGER significantly reduces the number of pa-\nrameters by 94.3% and the MACs by 95.3% while achieving performance surpass-\ning state-of-the-art (SOTA) model TF-GridNet. This is the first speech separation\nmodel with fewer than 1 million parameters that achieves performance compa-\nrable to the SOTA model.", "sections": [{"title": "INTRODUCTION", "content": "Humans possess the ability to focus on a specific speech signal in noisy environments, a phenomenon\nknown as the \"cocktail party effect\u201d (Cherry, 1953). In speech processing, the corresponding chal-\nlenge is accurately separating different sound sources from mixed audio signals, a task referred to as\nspeech separation. Speech separation is typically used as a preprocessing step for speech recogni-\ntion, as it helps enhance recognition accuracy (Haykin & Chen, 2005). Consequently, it is crucial to\nensure that speech separation not only produces clear and distinct outputs on real-world audios but\nalso meets the demands of low latency and computational efficiency (Divenyi, 2004). Therefore, in\npractical applications, optimizing the parameters and computational costs of separation models to\nreduce resource consumption becomes particularly important.\nIn recent years, the application of deep learning methods to the speech separation task has received\nwidespread attention (Wang et al., 2023; Li et al., 2023; 2022; Li & Luo, 2023; Subakan et al.,"}, {"title": "RELATED WORK", "content": "Speech separation. Speech separation methods can be divided into time domain and time-frequency\ndomain. Time domain methods directly process the original audio signal. Conv-TasNet (Luo &\nMesgarani, 2019) is the cornerstone of time domain separation models. It extracts features by tem-\nporal convolutional network (Lea et al., 2016). To improve the performance on long sequence data,\nDPRNN (Luo et al., 2020) divides the temporal sequence into small blocks and alternately per-\nforms intra-block and inter-block modeling, which becomes a common paradigm for many follow-\ning works (Wang et al., 2023; Subakan et al., 2021). The time-frequency domain methods apply\na Short-Time Fourier Transform (STFT) to transform the waveform into a joint representation of\ntime and frequency. TF-GridNet (Wang et al., 2023) enhances the temporal context information by\na full-band self-attention module. Although TF-GridNet achieved SOTA performance, it involves\nhuge computational costs.\nLightweight models. Some models (Wang et al., 2023; Yang et al., 2022; Subakan et al., 2021) with\nhigh computational complexity are difficult to be applied to real-time speech processing on edge de-\nvices. To reduce computational costs, TDANet (Li et al., 2023) draws on the attention mechanism\nof human brains and designs a lightweight structure. TDANet achieved the same performance with\nonly 5% MACs of SepFormer (Subakan et al., 2021), inspiring the design of lightweight separa-\ntion models. In music separation, BSRNN (Luo & Yu, 2023) uses prior knowledge to split band,\nperforming band merging on less important bands to compress the feature while retaining key fre-\nquency band information. BSRNN surpasses the previous music separation model in performance.\nThe band-split strategy is also inspiring for exploring lightweight speech separation models.\nDatasets for speech separation. WSJ0-2mix (Hershey et al., 2016) is an early and commonly used\nspeech separation dataset. However, there are two main problems with WSJ0-2mix. First, utterances\nof different speakers fully overlap during the conversation, which is not the case in real world.\nSecond, audio in WSJ0-2mix contain no noise or reverberation. WHAM! (Wichern et al., 2019)\nadded environmental noises to WSJ0-2mix. To take a step further, WHAMR! (Maciejewski et al.,\n2020) added reverberation to the audio. However, it only considered rooms with rectangular shapes\nand ignored the influence of materials and other non-rectangular shapes in the room on reverberation.\nLibri2Mix (Cosentino et al., 2020) was proposed based on the observation (Kad\u0131o\u011flu et al., 2020)\nthat the test performance of Conv-TasNet trained on WSJ0-2mix dropped sharply on other separation\ndatasets. The utterances in Libri2Mix were mixed with sparse overlap, and noises were added to the\nmixed audio, but reverberation was not considered in Libri2Mix. LRS2-2Mix (Li et al., 2023) was\nmixed by video clips acquired through BBC. The audio was recorded in real acoustic scenarios, thus\ncontaining much noise and reverberation. However, due to the different recording environments of\nthe clips, such as the shapes and materials of the room and objects, the reverberation obtained when\nthe clips were directly mixed is still unrealistic."}, {"title": "TIGER", "content": null}, {"title": "OVERALL PIPELINE", "content": "Let L be the sequence length of an audio. Given a monaural mixture audio $S \\in \\mathbb{R}^{1\\times L}$ containing\nutterances of C speakers and noise $n \\in \\mathbb{R}^{1\\times L}$:\n$S = \\sum_i^C P_i + \\eta,$\nthe speech separation task is to recover the clean speech of each speaker $P_i \\in \\mathbb{R}^{1\\times L}$.\nThe TIGER system (Figure 1) can be divided into five main components: the encoder, the band-split\nmodule, the separator, the band-restoration module, and the decoder. Specifically, we first use STFT\nas the encoder to convert the mixed audio signal $S \\in \\mathbb{R}^{1\\times L}$ into its time-frequency representation\n$X \\in \\mathbb{C}^{F\\times T}$, where F and T represent the number of frequency bins and time frames, respectively.\nNext, we apply a frequency band-split strategy, dividing the frequency bands into K sub-bands of\nvarying widths based on their importance. Each sub-band is transformed into a uniform channel size\nN using 1D convolutions, and these are then stacked along the frequency dimension to produce the\nfeature representation $Z \\in \\mathbb{R}^{N\\times K\\times T}$. Thirdly, Z serves as the input to the separator, which uses FFI\nblocks with shared parameters to model the acoustic characteristics of each speaker. Subsequently,\nthe band-restoration module restores the sub-bands to the full frequency range using separator output\n$J\\in \\mathbb{R}^{N\\times K\\times T}$, and the mask for each speaker $M_i \\in \\mathbb{C}^{F\\times T}$ is applied element-wise product to X,\nproducing the separated representation for each speaker $H_i \\in \\mathbb{C}^{F\\times T}$. Finally, the inverse STFT is\nused to generate the clean speech signal $P_i \\in \\mathbb{R}^{1\\times L}$ for each speaker."}, {"title": "BAND-SPLIT MODULE", "content": "Given a time-frequency representation X, we first apply a frequency band-split strategy to divide\nthe frequency dimension into K frequency sub-bands {$B_k \\in \\mathbb{C}^{G_k\\times T}|k = [1,K]$}:\n$F = \\sum_{k=1}^K G_k.$\nThe widths of the sub-bands $G_k$ are not necessarily the same. For each frequency sub-band $B_k$,\nwe merge its real $Re(\\cdot)$ and imaginary $Im(\\cdot)$ parts into the frequency dimension to generate $B_k \\in\n\\mathbb{R}^{2G_k\\times T}$. We denote the concatenation operation as $\\|$, then:\n$B_k = Re(B_k)\\|Im(B_k).$\nNext, we transform the frequency dimension $2G_k$ of $B_k$ to the feature dimension N using a group\nnormalization layer followed by a 1D convolution, which utilizes a kernel size of 1 and does not\nshare parameters across different $B_k$. In this way, we obtain feature $Z_k \\in \\mathbb{R}^{N\\times T}$ of the same\nshape for each sub-band. We then stack the features $Z_k$ from the K frequency sub-bands along the\nfrequency dimension to yield the input feature $Z \\in \\mathbb{R}^{N\\times K\\times T}$ for the separator."}, {"title": "SEPARATOR", "content": "In the separator, the input feature Z passes sequentially through B frequency-frame interleaved (FFI)\nblocks with shared parameters, as shown in Figure 2. In each FFI block, the frequency path is first\nused to extract contextual information between different sub-bands, producing $Z_{b,f} \\in \\mathbb{R}^{N\\times K\\times T}$.\nNext, we feed $Z_{b, f}$ into the frame path to further model the contextual information between different\ntime frames, generating $Z_{b+1} \\in \\mathbb{R}^{N\\times K\\times T}$.\nThe structures are identical in both the frequency path and the frame path, modeling along the fre-\nquency dimension and the time dimension respectively. Each path consists of two main modules: the\nmulti-scale selective attention (MSA) module and the full-frequency-frame attention (F3A) module.\nAs illustrated in Figure 3, taking the frequency path as an example, we first apply the MSA module\nalong the frequency dimension K to selectively extract features from $Z_b$, which results in enhanced\nfrequency features $\\tilde{Z_b} \\in \\mathbb{R}^{N\\times K\\times T}$. Then, the F3A module is used to integrate information across\ndifferent sub-bands of $\\tilde{Z_b}$, followed by layer normalization, to produce the output feature of the\nfrequency path $Z_{b, f}$."}, {"title": "MSA MODULE", "content": "The MSA module enhances important features through a selective attention mechanism and is di-\nvided into three stages: encoding, fusion, and decoding, as shown in Figure 3(a). Taking the MSA\nmodule in the frequency path as an example, the input to the module is $Z_b$.\nThe encoding stage. This stage aims to capture multi-scale acoustic features. Specifically, we\nfirst use multiple 1D convolutional layers (with a stride of 2 and channel of H) to progressively\ndownsample the frequency dimension to $\\frac{K}{2^D}$, resulting in a set of multi-scale acoustic features\n{$E_d \\in \\mathbb{R}^{H\\times \\frac{K}{2^d}\\times T}|d = [0, D]$}, where d denotes the d-th layer of downsampling. Next, we ap-\nply average pooling layers, denoted as $A(\\cdot)$, to downsample all $E_d$ to the same frequency resolution\n$\\frac{K}{2^D}$. Subsequently, the features with different frequency resolutions are fused into global features\n$G = \\sum_{d=0}^D A(E_d), G \\in \\mathbb{R}^{H\\times \\frac{K}{2^D} \\times T}$ by summing. Finally, a multi-layer convolutional (MLC) net-\nwork is used to transform G into $G' \\in \\mathbb{R}^{H \\times \\frac{K}{2^D} \\times T}$.\nThe fusing stage. In this stage, we fuse the local $E_d$ and global $G'$ information using the selective\nattention (SA) module. Specifically, for the d-th layer, we first use two 1D convolutions to map $G'$\ninto $\\tau \\in \\mathbb{R}^{H\\times \\frac{K}{2^D} \\times T}$ and $\\rho \\in \\mathbb{R}^{H \\times \\frac{K}{2^D} \\times T}$, respectively. Then, we also use one 1D convolution to"}, {"title": "FULL-FREQUENCY-FRAME ATTENTION MODULE", "content": "In the frequency path, the F3A module is used to aggregate features across different sub-bands,\nas shown in Figure 3(b). Given the input $Z_b$ and the number of attention heads A, we first use\nseparate 1 \u00d7 1 2D convolutional layers with distinct parameters to transform $Z_b$ into query $Q\\in\n\\mathbb{R}^{(A\\times E)\\times K\\times T}$, key $K \\in \\mathbb{R}^{(A\\times E)\\times K\\times T}$, and value $V \\in \\mathbb{R}^{(A\\times E)\\times K\\times T}$.\nTo obtain the information of full time length on each sub-band and apply self-attention mechanism,\nframe dimension T and the channel dimension E are merged in order of time step, so we get query\n$Q_i \\in \\mathbb{R}^{K\\times (E\\times T)}$ and key $K_i \\in \\mathbb{R}^{K\\times (E\\times T)}$ for the i-th attention head. Similarly, we get value\n$V_i \\in \\mathbb{R}^{K\\times (E\\times T)}$. $K_i$ is transposed and then multiplied with $Q_i$ to calculate the attention map of\nsize K \u00d7 K, which indicates the similarity between each sub-band and acts as the weight information\nof the frequency context. Then the attention map is multiplied with $V_i$ to obtain the output matrix.\nFor the i-th attention head, the output $O_i \\in \\mathbb{R}^{K\\times (N\\times T)}$ is calculated as follows:\n$O_i = Softmax(\\frac{Q_i K_i^T}{\\sqrt{E\\times T}}) V_i,$\nThe output matrix of each attention head is concatenated to get $O \\in \\mathbb{R}^{K\\times (N\\times T)}$, and the full-time\nlength is split into T time steps and transformed by 2D convolutional layer, generating the output\n$Z_{b,f} \\in \\mathbb{R}^{N\\times K\\times T}$. The process of the F\u00b3 A module in the frame path is similar."}, {"title": "BAND RESTORATION MODULE", "content": "After going through the separator, the sub-bands need to be converted back to their original width\nduring mask estimation. Specifically, $J \\in \\mathbb{R}^{N\\times K\\times T}$ denotes the output of the separator. For the k-th\nsub-band feature $J_k \\in \\mathbb{R}^{N\\times T}$ (k \u2208 [1, K]), the PReLU activation function and 1D convolutions are\nused to transform the number of channels to twice the original dimension $2G_k$, corresponding to the\nreal and the imaginary part. The complex feature is restored to generate a mask for each sub-band\n$M_k \\in \\mathbb{C}^{G_k\\times T}$ using the ReLU activation function. Then they are merged on the frequency dimen-\nsion to get the mask for the whole band $M \\in \\mathbb{C}^{F\\times T}$. Similar to band-split, the 1D convolutions of\ndifferent sub-bands do not share parameters."}, {"title": "ECHOSET", "content": "To develop models that perform better in daily scenarios, we need a dataset close to the real world.\nWe create EchoSet, a speech separation dataset with various noise and realistic reverberation, based\non SoundSpaces 2.0 (Chen et al., 2022) and Matterport3D (Chang et al., 2017). An analysis of the\ndataset is shown in Table 1.\nSoundSpaces 2.0 is an audio rendering platform in 3D environments. Given the mesh of a 3D sce-\nnario, it can simulate the acoustic effects of any sound captured from microphones. We followed the\nsteps below to generate mixed speech. (1) Choose the scenario. We selected rooms where daily con-\nversations often occur (such as office, living room, bedroom, dining room, etc.) from Matterport3D,\na large RGB-D dataset containing 90 diverse multi-floor and multi-room indoor scenes. (2) Define\nor sample the position. We defined a microphone at a suitable position, like next to a table or sofa,\nand sampled two sound sources in the same room. (3) Sample the direction. The angle between\nthe microphone and the sound source must be obtuse, meaning that the speaker and listener face\neach other. (4) Sample the height. The microphone and sound sources were randomly generated at\na vertical height of 1.5 m to 1.9 m from the floor, which is about a person's height. (5) Generate the\naudio. With SoundSpaces 2.0, mixed audio files were generated based on bidirectional path tracking\nalgorithm (Cao et al., 2016), which can simulate various effects in the sound propagation process,\nincluding reverberation, diffraction, and absorption. Materials of the room wall and the objects were\nannotated by Matterport3D and considered during the generation of the audio mixture.\nBased on the SoundSpaces 2.0 platform and the Matterport 3D scene dataset, we can simulate rever-\nberant audio from different speakers in LibriSpeech (Panayotov et al., 2015) to build a new dataset,\nEchoSet. In total, EchoSet includes 20,268 training utterances, 4,604 validation utterances, and\n2,650 test utterances. Each utterance lasts for 6 seconds. We mixed the speech of the two speakers\nat a random overlap ratio and added some noises from WHAM! noise (Wichern et al., 2019). The\ntwo different speakers were mixed with signal-to-distortion ratio (SDR) sampled between -5 dB and\n5 dB. The noises were mixed with SDR sampled between -10 dB and 10 dB."}, {"title": "EXPERIMENTAL SETUP", "content": "Dataset. We report the performance of TIGER on EchoSet. For fair comparison with previous\nspeech separation methods (Li et al., 2023; Wang et al., 2023; Hu et al., 2021), we also used two\nbenchmark datasets LRS2-2Mix (Li et al., 2023) and Libri2Mix (Cosentino et al., 2020).\nTo validate the gap between EchoSet and real-world environments, we constructed real-world data\nby selecting 10 real-world environments and recording audio from 40 speakers from the LibriSpeech\n(Panayotov et al., 2015) test set. The two audio used for mixing were recorded in the same acoustic\nscene (e.g., the shape and material of the walls and objects in the room) and followed the same\nmixing method as LRS2-2Mix (Li et al., 2023). For more details of these datasets, please refer to\nAppendix A.\nTraining and evaluation. For training, adam optimizer (Kingma & Ba, 2014) was employed with\nan initial learning rate of 0.001, adjusted based on validation performance. Evaluation metrics in-\ncluded SDRi and SI-SDRi (Vincent et al., 2006), with higher values indicating better performance.\nWe report parameters and MAC operations for complexity, which are calculated for one second of\naudio at 16 kHz. Inference speed was measured on NVIDIA RTX 4090 and Intel Xeon Gold 6326.\nDetailed training and evaluation configurations can be found in Appendix B and Appendix C. The\ncode will be made publicly available after the paper is accepted."}, {"title": "RESULTS AND DISCUSSION", "content": "ECHOSET IS MORE CLOSE TO THE REAL-WORLD DATA\nWe trained different models on Libri2Mix, LRS2-2Mix and EchoSet, and then tested them on the\ndata collected in the real world. The results are presented in Figure 4. Compared to models trained\non Libri2Mix and LRS2-2Mix, the models trained on EchoSet produced higher-quality separated\nspeech, confirming that the gap between EchoSet and real-world audio is relatively small."}, {"title": "COMPARISONS WITH STATE-OF-THE-ART METHODS", "content": "We compared TIGER with previous SOTA models including Conv-TasNet (Luo & Mesgarani,\n2019), DualPathRNN (Luo et al., 2020), SudoRM-RF1.0x (Tzinis et al., 2020), A-FRCNN-16 (Hu\net al., 2021), TDANet Large (Li et al., 2023), BSRNN (Luo & Yu, 2023) and TF-GridNet (Wang"}, {"title": "ABLATION STUDY", "content": "We adopted the small version of TIGER (B = 4) in the ablation studies. All the models were trained\nand tested on EchoSet. The training configuration of TIGER and other models was the same."}, {"title": "MSA MODULE", "content": "The MSA module enhances important features through a selective attention mechanism and is di-\nvided into three stages: encoding, fusion, and decoding, as shown in Figure 3(a). Taking the MSA\nmodule in the frequency path as an example, the input to the module is $Z_b$.\nThe encoding stage. This stage aims to capture multi-scale acoustic features. Specifically, we\nfirst use multiple 1D convolutional layers (with a stride of 2 and channel of H) to progressively\ndownsample the frequency dimension to $\\frac{K}{2^D}$, resulting in a set of multi-scale acoustic features\n{$E_d \\in \\mathbb{R}^{H\\times \\frac{K}{2^d}\\times T}|d = [0, D]$}, where d denotes the d-th layer of downsampling. Next, we ap-\nply average pooling layers, denoted as $A(\\cdot)$, to downsample all $E_d$ to the same frequency resolution\n$\\frac{K}{2^D}$. Subsequently, the features with different frequency resolutions are fused into global features\n$G = \\sum_{d=0}^D A(E_d), G \\in \\mathbb{R}^{H\\times \\frac{K}{2^D} \\times T}$ by summing. Finally, a multi-layer convolutional (MLC) net-\nwork is used to transform G into $G' \\in \\mathbb{R}^{H \\times \\frac{K}{2^D} \\times T}$.\nThe fusing stage. In this stage, we fuse the local $E_d$ and global $G'$ information using the selective\nattention (SA) module. Specifically, for the d-th layer, we first use two 1D convolutions to map $G'$\ninto $\\tau \\in \\mathbb{R}^{H\\times \\frac{K}{2^D} \\times T}$ and $\\rho \\in \\mathbb{R}^{H \\times \\frac{K}{2^D} \\times T}$, respectively. Then, we also use one 1D convolution to"}, {"title": "ABLATION STUDY: TIME-FREQUENCY INTERLEAVING", "content": "In the separator of TIGER, we model time and frequency features of the mixed audio alternately. To\ndemonstrate the effect of time-frequency interleaved structure, we tested the performance of F-F and\nT-T structures. For F-F, we replace the frame path with the frequency path in the FFI blocks. In other\nwords, each FFI block only includes two frequency paths which process the input $Z_b \\in \\mathbb{R}^{N\\times K\\times T}$\nand $Z_{b,f} \\in \\mathbb{R}^{N\\times K\\times T}$ in the same way but don't share parameters. All FFI blocks still share\nparameters. The implementation is similar for T-T.\nAccording to the result shown in Table 11, compared with only modeling time or only modeling fre-\nquency, the time-frequency interleaved structure can better capture time and frequency information\nof audio, which facilitates improving performance while keeping the model lightweight."}, {"title": "VISUALIZATION", "content": "In order to intuitively demonstrate the separation performance of TIGER, we provide some examples\nfor visualization, as shown in Figure 5. The following spectrograms show the inference results of\nTIGER (large) and TF-GridNet on the same audio, and the ground truth. Sample I and II show\nthat TIGER produces finer reconstruction results at high frequencies compared with TF-GridNet.\nTIGER also has better effects in noise reduction and spectrum leakage prevention, as illustrated in\nSample III and IV."}, {"title": "CONCLUSION", "content": "In this paper, we present TIGER, an efficient time-frequency domain speech separation model with\nsignificantly reduced parameters and computational costs. TIGER effectively extracts key acous-\ntic features through frequency band-split, multi-scale and full-frequency-frame modeling. We also\nintroduce the EchoSet dataset that simulates realistic acoustic scenarios. Experiments showed that\nTIGER outperformed existing SOTA models in complex acoustic environments, with 94.3% fewer\nparameters and 95.3% less computational costs, and demonstrated good generalization ability in\nthe task of movie audio separation. TIGER provides new ideas for designing lightweight speech\nseparation models suitable for devices with limited resources."}, {"title": "DATASET DETAILS", "content": "EchoSet. This dataset includes 20268 training utterances, 4604 validation and 2650 test utterances.\nThe length of each audio is 6 seconds. The target speech was selected from LibriSpeech (Panayotov\net al., 2015), mixed with SDR ranging from -5 dB to 5 dB. The speech and noise which was sampled\nfrom WHAM! were mixed with SDR sampled between -10 dB and 10 dB. This dataset contains\nrealistic reverberation.\nLRS2-2Mix (Li et al., 2023). Each audio in this dataset lasts for 2 seconds, and the training set,\nvalidation set and test set are about 11.1, 2.8 and 1.7 hours, respectively. The utterances were\nselected from the LRS2 (Afouras et al., 2018) corpus, which consists of video clips acquired through\nBBC, and were mixed with SDR sampled between -5 dB and 5 dB. Since the audio files were\nrecorded in real acoustic scenarios, LRS2-2Mix contains much noise and reverberation.\nLibri2Mix (Cosentino et al., 2020). Each audio in this dataset lasts for 3 seconds. The target speech\nfor each audio mixture was randomly chosen from LibriSpeech (Panayotov et al., 2015) (train-\n100) and combined with a uniformly sampled Loudness Units relative to Full Scale (Series, 2011)\nbetween -25 and -33 dB. This dataset contains no noise or reverberation.\nReal-world data. We collected a smaller-scale real-world dataset to test the performance of models\ntrained on different datasets. First, we randomly selected audio from various speakers in the Lib-\nriSpeech (Panayotov et al., 2015) test set, totaling 1.5 hours. All of this audio was clean, without\nreverberation or noise. The noise data were selected from the WHAM! noise dataset (Wichern et al.,\n2019). Next, we recorded the selected audio in 10 real-world environments, primarily consisting of\nrectangular conference rooms of various sizes and rooms with different shapes (such as fan-shaped\nand circular conference rooms). Finally, we mixed the audio from different speakers, which moved\nin different trajectories using SDRs sampled between -5 dB and 5 dB, while the noise was mixed\nusing SDRs sampled between -10 dB and 10 dB."}, {"title": "TRAINING CONFIGURATION", "content": "In the encoder and decoder, the window and hop size of STFT and iSTFT were set to 640 (40 ms)\nand 160 (10 ms). We use the Hanning window to mitigate spectrum leakage. According to the\nNyquist sampling theorem, the frequency range represented was 0-8 kHz for audio with a sampling\nrate of 16 kHz. In this way, each frame was represented by 321-dimensional complex spectra, and\nthe frequency resolution was 25 Hz. We adopt the band-split scheme LowFreqNarrowSplit in Table\n10. The number of total sub-bands K was 67. For each sub-band, the bandwidth was uniformly\ntransformed into N = 128. In the separator, the FFI blocks which share parameters were repeated"}, {"title": "EVALUATION CONFIGURATION", "content": "In all experiments, we reported the quality of separated audio on SDRi (Vincent et al., 2006) and\nSI-SDRi (Le Roux et al., 2019):\n$SDRi = SDR(\\hat{P_i}, P_i) \u2013 SDR(S, P_i),$\n$SI-SDRi = SI-SDR(\\hat{P_i}, P_i) \u2013 SI-SDR(S, P_i),$\nTo measure the complexity of the model, we used parameters and multiply-accumulate operations\n(MACs) for theoretical analysis. In the speech separation task, since the audio length is not fixed,\nwe used MACs for separating one-second audio as an indicator for complexity evaluation. We used\nptflops 0.7.32 to calculate parameters and MACs. For actual evaluation, we performed the backward\nprocess (training) and forward process (inference) 1000 times, respectively, on one second of audio\nat a 16 kHz sampling rate, then took the average to indicate the training and inference speed. We\nreported the GPU time and GPU memory usage during the training process, as well as the CPU\ntime, GPU time, and GPU memory usage during the inference process. To simulate the limited\ncomputational conditions of mobile devices on which the speech separation model is deployed in\nreal-world situations, we fixed the number of threads to 1 when calculating CPU (Intel(R) Xeon(R)\nGold 6326) time and only used a single card when calculating GPU (GeForce RTX 4090) time."}, {"title": "CINEMATIC SOUND SEPARATION TASK", "content": "The cinematic sound separation task (Uhlich et al., 2024) is to separate different signals from mixed\naudio, including speech, music and sound effects. We migrated TIGER to cinematic sound separa-\ntion to test the generalization ability of the model on similar tasks.\nWe tested TIGER's performance on the DnR dataset, which consists of three tracks: speech, music,\nand sound effects. The length of each audio is 60 seconds. Each track does not completely overlap,\nand the sampling rate is 44.1 kHz. The dataset is composed of 3295 training audio, 440 validation\naudio, and 652 test audio."}, {"title": "DETAILS OF DIFFERENT BAND-SPLIT SCHEMES", "content": "In Table 10, we list several band-split schemes. For datasets of 16 kHz, the full band ranges from 0-8\nkHz. Because real-to-complex STFT satisfies the conjugate symmetry, the result can be expressed\nusing only one side. According to the implementation of the torch.stft\u00b3, when the window size was\nset to 640, the encoding dimension was [640/2] + 1 = 321.\nFor the NonSplit scheme, we didn't apply band-split and kept the original frequency samples 321.\nThe width of each sub-band was 25 Hz. The total sub-band number was 321. We write the mixed\naudio after STFT as $X \\in \\mathbb{C}^{F\\times T}$. The real and imaginary part of X were treated as two channels and\nstacked on the channel dimension to obtain feature $X \\in \\mathbb{R}^{2\\times F\\times T}$. Then a 2D convolutional layer\nwas applied to X to expand the channel dimension to N. In this way, we got the input $Z \\in \\mathbb{R}^{N\\times K\\times T}$\nfor the separator (K = F = 321 in this case).\nFor the NormalSplit scheme, we split finer in the low-frequency part. Specifically, we split 0-1000\nHz by a 50 Hz bandwidth. Since the resolution was 25Hz, 2 frequency samples were treated as one"}, {"title": "DIFFERENT STRUCTURES IN MSA AND F\u00b3A MODULES", "content": "In the experiments where we replaced the MSA and F3A modules, we used LSTM (Graves &\nGraves, 2012), SRU (Lei et al., 2018), and Mamba (Gu & Dao, 2023) as the alternative model struc-\ntures. When we substituted LSTM for the"}]}