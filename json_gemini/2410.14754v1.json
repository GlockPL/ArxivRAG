{"title": "On the Sparsity of the Strong Lottery Ticket Hypothesis", "authors": ["Emanuele Natale", "Davide Ferr\u00e9", "Fr\u00e9d\u00e9ric Giroire", "Giordano Giambartolomei", "Frederik Mallmann-Trenn"], "abstract": "Considerable research efforts have recently been made to show that a random neural network N contains subnetworks capable of accurately approximating any given neural network that is sufficiently smaller than N, without any training. This line of research, known as the Strong Lottery Ticket Hypothesis (SLTH), was originally motivated by the weaker Lottery Ticket Hypothesis, which states that a sufficiently large random neural network N contains sparse subnetworks that can be trained efficiently to achieve performance comparable to that of training the entire network N. Despite its original motivation, results on the SLTH have so far not provided any guarantee on the size of subnetworks. Such limitation is due to the nature of the main technical tool leveraged by these results, the Random Subset Sum (RSS) Problem. Informally, the RSS Problem asks how large a random i.i.d. sample \u03a9 should be so that we are able to approximate any number in [-1, 1], up to an error of \u025b, as the sum of a suitable subset of \u03a9.\nWe provide the first proof of the SLTH in classical settings, such as dense and equivariant networks, with guarantees on the sparsity of the subnetworks. Central to our results, is the proof of an essentially tight bound on the Random Fixed-Size Subset Sum Problem (RFSS), a variant of the RSS Problem in which we only ask for subsets of a given size, which is of independent interest.", "sections": [{"title": "Introduction", "content": "The Lottery Ticket Hypothesis (LTH) is a research direction that has attracted considerable atten-\ntion over the years, stemming from the empirical contrast between the fact that, while large neural\nnetworks can be successfully trained to achieve good performance on a given task and successively\npruned to a great level of sparsity without compromising their performance, researchers have strug-\ngled to train sparse neural networks from scratch. The authors of [13] observed that, using a simple\npruning strategy (namely Iterative Magnitude Pruning while rewinding the original weights of the\nremaining edges to their value at initialization), starting from a sufficiently large random neural net-\nworks, it is possible to identify sparse subnetworks that can be trained to achieve the performance\nachievable by the starting network (see Figure 2 in the appendix for an illustration). The previous\nstatement, namely the LTH, soon gave rise to an even stronger one, corroborated by empirical works\n[30, 27] which proposed \u201ctraining-by-pruning\u201d algorithms (see Section 2 for details), providing evi-\ndence that starting from a sufficiently large random neural networks, it is possible to identify sparse\nsubnetworks that exhibit good performance as they are, without changing the original weights (see\nFigure 3 in the appendix for an illustration). By removing the need to analyze the dynamics of"}, {"title": null, "content": "training, the last statement, namely the Strong Lottery Ticket Hypothesis (SLTH), allowed a fruitful\nseries of rigorous proofs for increasingly more general architectures (see Section 2 for an overview).\nSuch rigorous results can informally be stated as follows:\nTheorem 1 (Informal statement of previous SLTH results). With high probability, a random arti-\nficial neural network \u039d\u03a9 with m parameters can be pruned so that the resulting subnetwork Ns\n\u025b-approximates (i.e., approximates up to an error \u025b) any target artificial neural network Nt with\nO (m/log2(1/8)) parameters.\nIt is important to note that, to this day, we only have proofs on the existence of such subnetworks,\nalso called winning tickets, but it remains an open question how to find them reliably.\nAll theoretical results on the SLTH however have so far provided no guarantee on the number of\nparameters of the winning ticket Ns. This is in contrast to the original motivation of the LTH and\nto the practical application of the aforementioned training-by-pruning algorithms that motivated the\nSLTH, such as [16, 15]. In fact, to approximate target networks with O (m/log2(1/8)) parameters,\nessentially all winning tickets Ns have \u0398(m) parameters (see Appendix A), thus being roughly of\nthe same size of the original network \u039d\u03a9. We thus ask the following natural question:\nIf we want to \u025b-approximate a family of target artificial neural networks with\nmt parameters by pruning a fraction a, called sparsity, of the m parameters of\na random artificial neural network No, how big should m be?\nWe are particularly interested in the regime in which the density parameter \u03b3 = 1 a vanishes as\nthe size of the network increases, so that the size of the winning ticket Ns is ym = \u043e(\u0442).\nThe above question has so far remained unanswered as a consequence of the limitation inherited\nfrom the core technical tool that has been leveraged so far to prove SLTH results, namely the Random\nSubset Sum (RSS) Problem [18]. Informally, the RSS asks how large a random i.i.d. sample \u03a9\nshould be so that we are able to approximate any number in [-1,1] as the sum of a suitable subset\nof \u03a9. The applicability of RSS to the SLTH was first recognized by [25] within the proof strategy\npreviously developed in [21]."}, {"title": "Our Contribution", "content": "We answer the aforementioned question by introducing and proving a refined variant of the RSS\nProblem, namely the Random Fixed-Size Subset Sum Problem (RFSS), in which the approximation\nof the target values should be achieved by only considering subsets of fixed size k from a set of n\nsamples (Theorem 2). We focus on subsets of fixed size k rather than subsets of size up to k for two\nmain reasons. From a theoretical point of view, it is a stronger requirement, and practically speaking,\nusing fixed-size subsets enables us to achieve SLTH results where the layers of the lottery ticket\nexhibit a uniform structure, potentially offering a computational advantage in their implementation.\nIn Section 4, we show how the density y impacts the overparameterization, i.e., the ratio (m/mt)\nbetween the number of parameters of the original network No and that of the class of target networks\nNt that can be \u025b-approximated by pruning No down to a subnetwork Ns with ym parameters. In\nour analysis, we also compare and recover as special cases previous SLTH results such as [25, 21,\n7, 3, 10]. For instance, when ym = \u0398(m), we recover up to a logarithmic factor the result of\n[25], which states that the overparameterization needed is O(log2 (m\u00b2/s\u00b2)). In the case of Dense\nNeural Networks, Theorem 3 thus bridges the gap between the two extreme cases of ym = \u0398(mt)\nand ym = (m) considered in [21] and [25], respectively. It is worth noting that [25] is often\nconsidered an improvement over [21], as it exponentially reduces the overparameterization, albeit at\nthe cost of a trivial sparsity level. Finally, we prove that our bounds on the overparameterization as\na function of the subnetwork sparsity are essentially tight.\nOrganization of the paper. After reviewing the literature on the SLTH in Section 2, we introduce\nthe Random Fixed-Size Subset Sum Problem in Section 3. In Section 4, we explore some appli-\ncations of the RFSS Problem to the SLTH, and finally draw our conclusions in Section 5. Some\nlimitations of our work, along with its potential impact, are discussed in Section 6."}, {"title": "Related Work", "content": "The SLTH is named after the LTH, which was introduced by Frankle and Carbin in [13]. At the time\nof writing, this paper has received over 3,300 citations, attesting to the significance and impact of\nthe research topic. Surveying the LTH is thus besides the scope of this work, and we defer the reader\nto dedicated surveys such as [17].\nThe SLTH was empirically motivated by work investigating training-by-pruning algorithms such as\n[30, 27], namely algorithms that leverage the gradient of the network parameters to learn a good\nmask of the edges to be retained (i.e., a good subnetwork, called the winning ticket). [30] achieves\nthis by learning a probability associated to each edge, which is then used to sample the edges that\nshould be included in the subnetwork. [27] gets rid of the stochasticity involved in the aforemen-\ntioned strategies by learning a score associated to each edge; the subnetwork is then determined by\nincluding the edges with the highest score. Such strategies are leveraged in [16, 15] in a federated\nlearning setting, in order to improve the communication cost of distributed training by communi-\ncating the sampled masks of a fixed shared network, rather than the entire weights. However, these\ntraining-by-pruning algorithms are generally not computationally less expensive than classical train-\ning, since they also make use of backpropagation to update scores and are applied to a sufficiently\nlarge network to find a winning ticket. To reduce the computational cost of finding a good sub-\nnetwork, [14] shows, both theoretically and experimentally, that randomly pre-pruning the source\nnetwork before looking for a winning ticket can be an effective approach. In [24], on top of ran-\ndomly pruning the source network, some parameters are also frozen. Frozen parameters are forced\nto be part of the winning ticket and they do not have an associated score, which effectively reduces\nthe search space for the training-by-pruning algorithms.\nThe first rigorous proof of the SLTH in the case of dense neural networks has been provided by [21],\nwhich establishes a framework that was inherited by the subsequent works. [25] crucially shows\nthat the framework in [21] allows the application of the RSS analysis in [18], proving that, with\nno constraint on the size of the subnetworks, a random network with m-parameters can be pruned\nto approximate target networks with m/log(1/8) parameters (we defer the reader to Theorem 3\nfor details on further constraints on the parameters). An alternative proof of the result in [25] was\nsimultaneously shown in [23]. [8] and [3] successively extended [25] and [23] to convolutional\nneural networks (CNNs). By leveraging multidimensional generalizations of RSS [9, 2], [6] further\nextended the SLTH to structured pruning of CNNs and, as a special case, dense networks. Finally,\n[10] provided a general framework that proves the SLTH for equivariant networks.\nAs for refinements and generalizations of the above results, [4] shows that, at the cost of a quadratic\noverhead in the overparameterization w.r.t. [25], the number of layers of the random network \u039d\u03a9\ncan be reduced to l + 1, where l is the number of layers of the target networks Nz; furthermore,\nwhile previous results only considered networks with ReLU activation, [4] shows how to extend the\nproof in [25] to a more general class of activations functions. [5] introduces the notion of universal\nlottery ticket, and show that it is possible to prune a sufficiently overparameterized random network\nso that the resulting subnetwork (the lottery ticket) can approximate certain class of functions up\nto an affine transformation of the output of the subnetwork (in this sense being universal). [12]\nshows how to extend the proof in [25] when neurons have random biases, and adapts the training-\nby-pruning algorithm of [27] to find a strong lottery ticket with a desired sparsity level. Motivated\nby theoretical insights on the existence of sparse strong lottery tickets, [11] develops a framework to\nplant the latter in large random network and investigates training-by-pruning algorithms, providing\nevidence that sparse strong lottery tickets typically exists for common machine learning tasks, and\nthe difficulty to find them is of algorithmic nature.\nOur proof of the RFSS Problem in Section 3 is based on the second moment method approach first\nexplored by [19], and which has recently been refined to prove multidimensional generalizations of\nRSS by [9] and [2]."}, {"title": "Fixed-Size Random Subset Sum", "content": "In this section we present our technical contributions on the RFSS, which are the foundation of our\nproofs regarding the sparsity of the SLTH."}, {"title": null, "content": "Let us start by introducing some notation. We denote by [n] the set {1, . . ., n}, for n \u2208 N. Given a\nset \u03a9 = {X1, ..., Xn} and a set of indices S \u2286 [n] we define \u03a3s = \u2211i\u2208S Xi, and we omit n when\nclear from the context. We now define a class of distributions for which our RFSS result holds.\nDefinition 1 (sum-bounded). We say that a probability density function f is sum-bounded if there\nexist positive constants c\u2081 and cu such that, for all k \u2208 N, given k independent samples X1, ..., Xk\nwith density f, the density of their sum f\u2211[k] satisfies\n$\\frac{c_l}{\\sqrt{k}} \\leq f_{\\sum[k]}(x) \\leq \\frac{c_u}{\\sqrt{k}}$\nwith the lower bound holding for all x \u2208 [-\u221ak, \u221ak] and the upper bound holding for all x \u2208 R.\nAt first, our definition of sum-bounded could look as a weaker version of a classical local limit\ntheorem on the sum of random variables (e.g., see [26, Chapter VII, Theorem 7]). However, that is\nnot the case, since we require a lower bound on the sum for any k, which is needed to prove our\nmain result.\nDenote, for all x \u2208 [0, 1], the binary entropy as\nH2(x) = -x log2 x \u2212 (1 \u2212 x) log2(1 \u2212 x).\nOur main technical result is the following proof of a fixed-size subset variant of the RSS Problem.\nTheorem 2. Let 0 < \u025b < 1, Chyp \u2265 1, k, n be integers with 1 \u2264 k \u2264 n/2, and let \u03a9 = {X1, ..., Xn}\nwhere the Xi's are i.i.d. random variables with sum-bounded density. There exists a constant Cthm\nsuch that, if\nn\u2265 Chyp $\\frac{\\log_2 \\frac{k}{\\epsilon}}{H_2(\\frac{k}{n})}$,\n(1)\nthen for every fixed z \u2208 [-\u221ak, \u221ak] it holds that\nPr (S\u2282 [n], |S| = k : |\u2211s \u2212 z| < \u025b) \u2265 Cthm\nRemark. The proof of Theorem 2 is given in Section 3.1, and it actually holds for any 1 \u2264 k \u2264 \u03bb\u03b7,\nfor an arbitrary \u03bb\u2208 [1/n, 1). We state the theorem this way for readability and because we are\nprimarily interested in high-sparsity settings (i.e., small size k of the subsets), so considering values\nof k\u2265 does not add much to our analysis. The same remark also holds for Corollary 1.\nThe sum-bounded condition of Definition 1 is easily verified for distributions such as the Gaussian\ndistribution. Previous SLTH results rely on a classical resampling argument by [18, Corollary 3.3],\nwhich shows how RSS results for Uniform[-1,1] independent random variables naturally extend\nto independent random variables that contains a uniform distribution, in the sense that they can be\nexpressed as the mixture of distributions one of which is Uniform[-1,1] with constant probability.\nThe next lemma thus proves that the Uniform[-1,1] distribution is sum-bounded\u00b2. A detailed proof\nis provided in Appendix C.\nLemma 1. The Uniform[-1,1] probability density function is sum-bounded, i.e., given a set\nUn = {Ui}i\u2208[n] of i.i.d. variables Ui with Uniform[-1,1] probability density function, there ex-\nist constants c\u2081 and cu such that the probability density function f(x,n) of the sum un of these\nvariables, for all n \u2208 N,\n$\\frac{c_l}{\\sqrt{n}} \\leq f(x, n) \\leq \\frac{c_u}{\\sqrt{n}}$,\n(2)\nwith the lower bound holding for all x \u2208 [\u2212\u221an, \u221an), and the upper bound holding for all x \u2208 R.\nFinally, in our proofs on the Sparse SLTH in Section 4, we make use of the following corollary\nof Theorem 2, which ensures a uniform high probability of hitting any target z \u2208 [-\u221ak, \u221ak],\nconsidering independent random variables that contain a uniform distribution."}, {"title": null, "content": "Corollary 1. Let 0 < p < 1 and \u025b \u2208 (0,1/2) be constants, k,n with 1 \u2264 k \u2264 n/2, and let\n\u03a9 = {X1,..., Xn} be i.i.d. random variables whose density is a mixture of a Uniform([-1,1])\nwith probability p, and some other density otherwise. There exists a positive constant Camp that only\ndepends on p such that, if\nn\u2265 Camp $\\frac{\\log_2 \\frac{k}{\\epsilon}}{H_2(\\frac{k}{n})}$,\n(3)\nthen\nPr (\u2200z\u2208 [-\u221ak, \u221ak], \u2203S\u2082C [n], |S\u2082| = k : |\u2211sz \u2212 z| < \u025b) \u2265 1 \u2212 \u025b.\nProof Idea. The corollary follows from three arguments. First, by a standard sampling argument,\nwe can assume that a constant fraction of the sample follows a Uniform[-1,1] distribution. Sec-\nondly, by Lemma 1, the uniform probability density function is sum-bounded. We can thus apply\nTheorem 2, which guarantees a success probability of Cthm for approximating a given target. Fi-\nnally, by a standard probability amplification argument and a union bound applied to Theorem 2, by\npaying an extra factor log2(k/\u025b) in Eq. 1, the constant Cthm can be assumed to be 1 \u025b, and the\nexistence of a suitable subset Sz holds simultaneously for all z \u2208 [\u2212\u221ak, \u221ak]. Details are given in\nAppendix D.\nFor k big enough, we can get rid of the squared logarithmic dependency on k in the right hand side\nof Equation 3, as shown in the following Corollary, whose proof can be found in Appendix E.\nCorollary 2. Let 0 < p < 1 and \u025b \u2208 (0,1/2) be constants, k,n be integers with 1 \u2264 k \u2264 n/2\nand k \u2265 2Camp (log2 k + 2log2k \u00b7 log2+). Let \u03a9 = {X1, ..., Xn} be i.i.d. random variables whose\ndensity is a mixture of a Uniform([-1,1]) with probability p, and some other density otherwise.\nThere exists a positive constant Camp that only depends on p such that, if\nn\u2265 2Camp $\\frac{\\log_2 \\frac{k}{\\epsilon}}{H_2(\\frac{k}{n})}$,\n(4)\nthen\nPr (\u2200z\u2208 [-\u221ak, \u221ak], \u2203S\u2082C [n], |S\u2082| = k : |\u2211sz \u2212 z| < \u025b) \u2265 1 \u2212 \u025b.\nAs customary in conference versions of papers, our proofs adopt the convention of taking ceilings\nand floors as suitable for non integer fractional terms. This is done in the interest of the reader (and\nours), and does not impact the results in any significant way."}, {"title": "Proof of Theorem 2", "content": "Proof of Theorem 2. For simplicity, throughout the proof we will often use c to denote any positive\nconstant. Let Sk = {S C [n] ||S| = k} and define, for a fixed z \u2208 [\u2212\u221ak, \u221ak],\nY = Y(z) = \u2211 Zs\nSESk\nwhere Zs = Zs(z) = 1{|\u2211s-z|<e}. Following [19], we exploit the second moment method for\nRFSS, generalising it to arbitrary k.\n$\\Pr (Y > 0) \\geq \\frac{(E [Y])^2}{E [Y^2]}$,\n(5)\nit thus suffices to prove that\nE [Y\u00b2] \u2264 c (E [Y])\u00b2 .\n(6)\nWe first rewrite Eq. 5 in a more convenient form. Let S and \u0160' be two independently and uniformly\nat random chosen subsets of [n] of size k, and denote Hs(z) as the event that \u2211\u03c2 \u03b5-approximates z,\nnamely\nHs = Hs(z) = {|\u2211s \u2013 z| < \u03b5} ."}, {"title": null, "content": "We have\nE[Y] = \u2211E[Zs] = \u2211 Pr(Hs) = $\\binom{n}{k}$ Pr (Hs) (7)\nSESk SESk\nand\nE[Y\u00b2] = E [ ( \u03a3 Zs)( \u03a3 Zs\u0131 )] = \u03a3 E [ZsZs\u0131 ]\nSESk S'ESK S,S'ESk\n= \u2211 Pr (Hs\u2227 H\u011f\u0131) = $\\binom{n}{k}$\u00b2 Pr (H\u011f, \u2227 H\u011f\u0131). (8)\nS, S'ESk\nUsing Eqs. 7 and 8 we can rewrite the r.h.s. of Eq. 5 as follows\n$\\frac{(E [Y])^2}{E [Y^2]} = \\frac{[Pr (H_\\bar{s})]^2}{Pr (H_\\bar{s}, H_{\\bar{s}'})}} = \\frac{Pr (H_{\\bar{s}})}{Pr(H_{\\bar{s}'}|H_{\\bar{s}})}$\nEq. 6 thus becomes\nPr (H5, H5) \u2264 cPr (Hg). (9)\nLet I\u2081 denote the event {|\u0160\u2229 \u0160'| = i} and Ia,b the event Ua<i<b Ii. Fix \u03bc \u2208 (\u03bb, 1). By the law of\ntotal probability and independence of I\u00bf and H\u011f, we rewrite the l.h.s. of Eq. 9 as follows:\nPr (H\u011f, | H5)\n= Pr (H\u011f, \u2227 Ik | H\u011d) + Pr (H\u011f, \u039b \u0399\u03bck,k\u22121 | H\u011d) + Pr (H\u011f, \u2227 I0,\u00b5k\u22121|\u03975) (10)\n= Pr (Ik) \u00b7 Pr (H\u011f\u0131 | H\u011f, Ik) (11)\n+ Pr (I\u00b5k,k\u22121)\u00b7 Pr (H\u011f\u0131 | H\u1ef9, I\u00b5k,k\u22121)\n\u03bc\u03ba-1\n+ \u2211 (Pr (Ii). Pr (H\u011f\u0131 | H\u011f, Ii)) . (12)\ni=0\nTo conclude the proof, it suffices to show that each addendum in Eqs. 10, 11 and 12 are upper-\nbounded by some constant multiple of \u03b5/\u221ak, since the lower bound in Definition 1 ensures that\n\u03b5\n<cPr (Hg). (13)\n\u221ak\nAs for the first addendum (Eq. 10), since Pr (H\u011f, | H\u011f, Ik) = 1, then\n$\\frac{c \\epsilon}{\\sqrt{k}}$"}, {"title": null, "content": "Pr (Ik) \u00b7 Pr (H5, | H\u011f, Ik) = Pr (Ik) = $\\frac{\\binom{k}{k} \\binom{n-k}{0}}{\\binom{n}{k}} = \\frac{1}{\\binom{n}{k}}$\n$\\leq \\frac{1}{\\binom{n}{k}} \\stackrel{(a)}{<} \\frac{8k(n-k)}{n} 2^{- n H_2(\\frac{k}{n})} \\stackrel{(b)}{<} \\frac{\\epsilon}{\\sqrt{k}} \\frac{8k(n-k)}{n} 2^{-\\text{Chyp} \\log_2(\\frac{k}{\\epsilon})} \\stackrel{(c)}{\\leq} c\\frac{\\epsilon}{\\sqrt{k}}$\n(14)\nwhere inequality (a) in Eq. 14 is a standard lower bound on (2) holding for all k \u2264 n 1; in\ninequality (b) in Eq. 14 we used Eq. 1, namely nH2 (1) \u2265 Chyp log2; in inequality (c) in Eq. 14\nwe used that Chyp \u2265 1.\nAs for the second addendum (Eq. 11), we next show that\nPr (I\u00b5k,k\u22121) Pr (H\u011f\u0131 | H\u011f, I\u00b5k,k\u22121) \u2264 c $\\frac{\\epsilon}{\\sqrt{k}}$\n(15)\nby proving that\nC\nPr (Iuk,k-1) \u2264 k (16)\n\u221ak\nand\nPr (H\u011f\u0131 | H, I\u00b5k,k\u22121) \u2264 c\u03b5. (17)"}, {"title": null, "content": "First, observe that I = |\u0160\u2229\u0160\u02b9| follows a Hypergeometric(n, k, k) distribution, thus by Chebyshev's\ninequality\nPr (I\u00b5k,k\u22121) \u2264 Pr (I > \u00b5k) = Pr $\\left( I - \\frac{k^2}{n} > \\mu k - \\frac{k^2}{n} \\right) \\leq \\frac{Var[I]}{(\\mu k - \\frac{k^2}{n})^2}$\n\u2264 $\\frac{c' n \\frac{k^2}{n} \\frac{n-k}{n} \\frac{n-k}{n-1}}{\\mu^2k^2(1 - \\frac{\\lambda}{\\mu})^2}$ \u2264 $\\frac{c' n \\frac{k^2}{n} \\frac{n-k}{n-k}}{\\mu^2k^2(1 - \\frac{\\lambda}{\\mu})^2}$ $\\leq \\frac{c \\epsilon}{\\sqrt{k}}$,\n(18)\nhaving set c' = \u03bc\u00b2(1 \u2212 \u03bb/\u03bc)\u00b2 > 0, thus proving Eq. 16. Secondly, define A = \u0160'\\\u0160 and observe\nthat\nPr (H5, | H\u014d, I\u00b5k,k\u22121)\nk-1\n= \u2211 Pr (H51 | H5, Ii) Pr (Ii | H5, I\u00b5k,k\u22121) (19)\nS\ni=\u03bc\u03ba\nk-1\n= \u2211\u222b Pr (\u03a3\u0391 - (z - y)| < \u03b5 | \u03a3\u2081 = y, I\u2081, H\u011d) Pr (\u03a3\u2081 = y | H\u011f, Ii) dy\n18\ni=\u03bc\u03ba\n\u00b7 Pr (Ii | H\u1ef9, I\u00b5k,k\u22121)\nk-1\n= \u2211\u222b Pr (|\u03a3\u0391 - (z - y)| < \u03b5| \u03a3\u2081 = y, I\u2081) Pr (\u03a3\u2081 = y | H\u011f, Ii) dy\n-\u221e\ni=\u03bc\u03ba\n\u00b7 Pr (Ii | H5, I\u00b5k,k\u22121)\nk-1\n\u2264 c\u03b5 \u2211\u222b Pr (\u03a3\u2081 = y | H 5, Ii) dy Pr (Ii | H\u1fb6, I\u00b5k,k\u22121) \u2264 c\u03b5,\n(20)\n(21)\n(22)\n(23)\n81\ni=\u03bc\u03ba\nwhere from Eq. 19 to Eq. 20 and from Eq. 20 to Eq. 21 we used the law of total probability;\nfrom Eq. 21 to Eq. 22 we dropped the redundant event H in the conditioning, due to conditional\nindependence; finally, from Eq. 22 to Eq. 23 we used Definition 1 which implies that for any\ni \u2208 {\u03bc\u03ba, ..., k \u2212 1} it holds\nPr (|\u03a3 - (z - y)| < \u03b5|\u03a3\u2081 = y, I\u00bf) = Pr (|\u03a3[k\u2212i] - (z - y)| < \u025b) \u2264 ce.\nThis concludes the proof of Eq. 17.\nAs for the third addendum (Eq. 12), analogously to the calculations from Eq. 20 to Eq. 22, by the\nlaw of total probability we have\n\u03bc\u03ba-1\n\u2211 Pr (Ii) Pr (H5, | H5, Ii)\ni=0\n\u03bc\u03ba-1\n= \u2211Pr (1) Pr(A -(-8)<\u00ab| \u03a3\u2081 = y, \u2081), r(\u03a31 = y | 2, 1.) dy\nPr \u03a3\u0391 \u00b7 + \u03b5\ni=0\n\u03bc\u03ba-1\n= \u2211Pr (1) | P([k-1] - (2 - y)| < \u03b5) Pr (\u03a31 = y | 2, 14) dy\nni=0\nPr + + \u03b5 \u2264\n- 1\n\u2264 (Pr (1) P(,y 2,1) dy\nPr Pr y | #1) dy\ni=0\n81\ni=0\n\u03bc\u03ba-1\n\u2211 dy \u2264\nPr 1\nni=0\n\u03bc\u03ba-1\n\u03b5\n<c\u2211 Pr (Ii) = (24)\n(25)\n(26)\nwhere from Eq. 24 to Eq. 25 we used Definition 1, which implies that for any i \u2208 {0, ..., k \u2212 1}\nit holds\nPr (|\u03a3[k-i] - (z - y)| < \u03b5) \u2264 c \u2264 c \u03b5\n\u221ak-i \u221ak"}, {"title": null, "content": "The three bounds on the addenda in Eqs. 10, 11 and 12 (respectively Eqs. 14, 15 and 26), combined\nwith Eq. 13, conclude the proof."}, {"title": "Sparse Strong Lottery Ticket Hypothesis (SSLTH)", "content": "We now apply our results on the RFSS problem to the SLTH and obtain guarantees on the sparsity of\nwinning tickets for Dense Neural Networks (DNNs, Theorem 3) and Equivariant NNs (Theorem 4).\nThe next theorem essentially interpolates between the two extremes of [21][Theorem 2.1] (where\n\u03b3m = (mt)) and [25][Theorem 1] (where ym = \u0398(m)), where we recall that m and mt represent\nthe number of parameters of the overparameterized and the target networks, respectively, and y is\nthe density of the winning ticket.\nWe use \u03c3(\u00b7) to denote the ReLU activation function, i.e., \u03c3(x) = x1x\u22650, and ||W|| to denote the\nspectral norm of the matrix W. Let F to be a set of target ReLU neural networks f : Rdo \u2192 Rd\u0131\nof depth l such that\nF = {f : f(x) = W\u03b9\u03c3(W1-1 ... \u03c3(W\u2081x)), Vi W\u2081 \u2208 Rdixdi-1 and ||W|| \u2264 1} (27)\nFor a given f \u2208 F, for all i \u2208 [l], let pi = max{di\u22121/di, di/di-1}, and p = maxi pi. Then, recalling\nthat Camp is the constant defined in Corollary 1, we have the following result.\nTheorem 3 (SSLTH for DNNs). Let g be a randomly initialized feed-forward 2l-layer neural net-\nwork, in which each weight is drawn from a Uniform[-1,1] distribution, of the following form:\ng(x) = \u039c2\u03b9\u03c3(M21\u22121... \u03c3(M1x)).\nLet \u03b3' = \u03b3'(\u03b5) \u2208 (0,1), M2i \u2208 Rdi\u00d72di\u22121n and M2i\u22121 \u2208 IR2di-1n \u00d7di-1, with n satisfying\nn = Camp $\\frac{\\log_2(\\frac{2ldi-1d_i}{\\epsilon})}{H_2(\\gamma')}$ (28)\nWith probability at least 1 \u025b, for every f \u2208 F, where F is defined as in Eq. 27, g can be pruned\nto obtain a subnetwork of sparsity at least a = 1 \u2212 \u03b3 that approximates f up to an error \u025b, having\ndefined y = \u03c1\u03b3'.\nProof Idea. The theorem follows from a slight variation of the same approach detailed in [25], in\nwhich we use our Corollary 1 instead of [18][Corollary 2.5] when pruning g, allowing us to have\ncontrol over the size of the pruned network. A detailed proof is provided in Appendix F.\nTo illustrate a simple example of how Theorem 3 addresses the main question asked in the intro-\nduction, consider the case where we want to approximate a target network with mt parameters and\nl layers, each of width d (so p = 1 and \u03b3' = \u03b3), by pruning an overparameterized network to\nachieve a desired sparsity level of a = 1 \u2212 \u03b3. The condition expressed by Equation 28 in Theorem 3\ncomes from the use of Corollary 1 when pruning network g, as shown in the proof. If, instead of\nCorollary 1, we use its simplified variant Corollary 2, it is easy to observe that Equation 28 would\nbecome\nn = Camp $\\frac{\\log_2(\\frac{2ldi-1di}{\\epsilon})}{H_2(\\gamma')}$ (29)\nUsing this condition, Theorem 3 then tells us that we need to prune a randomly initialized network\nwith twice as many layers and a number of parameters of the order of d2 $\\frac{\\log_2(\\frac{d^2}{\\epsilon})}{H_2(\\frac{d}{n})}$.\nWe will now clarify the connection between Theorem 3 and the earlier results from [21] and [25].\nFigure 1 provides a quick visual comparison.\nMalach et al.[21]. When all layers have the same width d, [21] showed that any target network\nwith l layers and a total of m\u2081 = d\u00b2l parameters can be \u025b-approximated by pruning a randomly\ninitialized network with 2l layers. The overparameterization of this network, relative to the target\nnetwork, is O($\\frac{\\log_2(\\frac{d^2}{\\epsilon})}{\\frac{d}{n}}$) = \u00d5($\\frac{\\log_2 m_t}{\\epsilon}$). More specifically, the winning ticket found after pruning"}, {"title": null, "content": "has a parameter count of the same order as the target network, resulting in a density of y = \u00d5($\\frac{\\epsilon}{\\log_2 m_t}$).\nNotably, this density y is the inverse of the overparameterization, as the size of the winning ticket\nmatches that of the target network.\nNext, we"}]}