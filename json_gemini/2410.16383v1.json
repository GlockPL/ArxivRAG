{"title": "Designing Robust Cyber-Defense Agents with Evolving Behavior Trees", "authors": ["Nicholas Potteiger", "Ankita Samaddar", "Hunter Bergstrom", "Xenofon Koutsoukos"], "abstract": "Modern network defense can benefit from the use of autonomous systems, offloading tedious and time-consuming work to agents with standard and learning-enabled components. These agents, operating on critical network infrastructure, need to be robust and trustworthy to ensure defense against adaptive cyber-attackers and, simultaneously, provide explanations for their actions and network activity. However, learning-enabled components typically use models, such as deep neural networks, that are not transparent in their high-level decision-making lead-ing to assurance challenges. Additionally, cyber-defense agents must execute complex long-term defense tasks in a reactive manner that involve coordination of multiple interdependent subtasks. Behavior trees are known to be successful in modelling interpretable, reactive, and modular agent policies with learning-enabled components. In this paper, we develop an approach to design autonomous cyber defense agents using behavior trees with learning-enabled components, which we refer to as Evolving Behavior Trees (EBTs). We learn the structure of an EBT with a novel abstract cyber environment and optimize learning-enabled components for deployment. The learning-enabled components are optimized for adapting to various cyber-attacks and deploying security mechanisms. The learned EBT structure is evaluated in a simulated cyber environment, where it effectively mitigates threats and enhances network visibility. For deployment, we develop a software architecture for evaluating EBT-based agents in computer network defense scenarios. Our results demonstrate that the EBT-based agent is robust to adaptive cyber-attacks and provides high-level explanations for interpreting its decisions and actions.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern network defense is an increasingly difficult task due to a multitude of novel and diverse cyber-attacks and the scale of systems. Developers have crafted solutions such as alert monitors and decision logic rules to alleviate human cognitive fatigue, but this is not viable or scalable as new attacks are discovered. The capabilities and strategies of cyber-defense continue to grow and, as such, fully autonomous cyber defense agents are being considered as an alternative to optimally utilize the resources needed to mitigate adversaries. However, there are challenges regarding the transparency of these agents and their robustness.\nAutonomous agents contain a mix of standard and learning-enabled components (LECs) trained with machine learning (ML) and reinforcement learning (RL). These LECs are nor-mally modeled with neural networks that lack the ability to provide high-level explanations and struggle to complete long-term objectives with multiple subtasks. Cyber-defense breaks down into a variety of subroutines (analysis, monitoring, restoring, deploying decoys, etc.) that a single component will struggle to represent and optimize. Additionally, it is unclear how autonomous agents will adapt to multiple varieties of cyber-attacks. Adversaries are versatile, they may use one or more attacks on the system targeting multiple components. The adversary may use one strategy at the beginning to explore the system in a low-detection manner and then launch a full-scale attack after they have discovered enough knowledge. The autonomous agent will need to be robust to these scenarios and explain its current perspective of the adversary's behavior.\nAn effective way of representing a reactive control policy between subtasks in the hierarchy is using Behavior Trees (BTs) [2]. BTs are structures for modeling complex control policies with advantages of modularity, reactivity, and ex-plainability. The reactive nature of the BT allows for explicit switching between multiple behaviors in quick succession depending on environment changes. The LECs and other components can be coordinated and modelled as a policy we refer to as an Evolving Behavior Tree (EBT). An EBT can jointly optimize and model the control flow of optimized components.\nThe structure of the control of behaviors in an EBT is typically manually developed, but due to the complexity of multiple subtasks and their dependencies, construction can prove tedious and potentially infeasible. Recent work has focused on automatic construction of BTs using genetic pro-gramming (GP) [3], [4] or large-language models [5], [6]. In particular, the works in GP develop abstract environments for computational efficiency that map to a realistic simulation.\nIn this paper, we develop an autonomous cyber-defense agent that leverages the hierarchical structure of EBTs for robustness against dynamic cyber-attacks. We design the agent in three stages: (1) learning the high-level control structure of the EBT, (2) optimization of LECs for robustness, and (3) integration and deployment to a realistic cyber environment. The research objective of (1) is to enhance scalability by generating control structures without requiring detailed knowl-edge of network components. This approach enables learning"}, {"title": "II. RELATED WORKS", "content": "With the onset of sophisticated cyber-attacks, traditional se-curity measures often fall short. Thus, there is a need for more advanced and interpretable solutions. Nowadays, differ-ent RL methods are adopted to develop learning enabled cyber-defense policies in autonomous networks [8]. Different RL-based cyber-defense policies have used CybORG to enact their approaches. Foley et al. utilized a goal-conditioned hierarchi-cal RL (HRL) to select trained defense strategies in CybORG CAGE Challenge Scenario 2 [9]. The defense strategy in [9] is optimized via RL for each attacker strategy, following which a meta-policy selects between the defense strategies based on the attacker behavior at the beginning of a simulation. The winners of the CAGE Challenge Scenario 2, the CardiffUni, used a similar goal-conditioned approach but with an added focus on reducing action space and deploying decoys [9]. Wolk et al. presented an alternative where an ensemble approach aggregates the policy output [10]. Towards emulation, Molina-Markham et al. developed a novel tool, FARLAND, with a focus on realistic cyber-defense environments and curriculum learning for cyber-defense agents [11]. Unlike classic RL tasks where the agents are regularly rewarded for progress, the reward signals assigned to cyber-security tasks are distributed sparsely across each defense episode in the form of penalties. To overcome this gap, Elizabeth et al. presented a reward shaping policy in deep RL and evaluated the policy in Cy-bORG [12].\nAnother line of study is motivated by neurosymbolic AI ap-proaches which combine the pattern recognition capabilities of neural networks along with the explicit reasoning of symbolic"}, {"title": "III. AUTONOMOUS CYBER-DEFENSE", "content": "Autonomous cyber-defense is the long-term task of fortifying and mitigating a system against cyber-attackers without human intervention. It is a task that constantly needs to be iterated on as long as the system is operating, otherwise novel attacks could breach and disrupt critical resources. Cyber-defense agents can utilize a mix of security standards and LECs to successfully adapt and defend a system.\nLECs typically rely on function approximators, such as neural networks, trained with ML or RL to compute optimal actions. Existing algorithms work well for short-term tasks. However, as the task increases in complexity and longevity, new subtasks and capabilities are required that a single component struggles to optimize. Also, the components are not transparent, leading to challenges with assurance and trustworthiness. In a system with safety-critical resources, it is essential that we understand the behavior of an agent, or else there can be unintended consequences due to agent error.\nIn general, autonomous cyber-defenders need to be prepared for a diversity of potential cyber-attacks. These attacks can derive from one or multiple actors. During an attack, an actor may decide that it should switch to another attack strategy based on information it has observed in the system. If the cyber-defender is not aware of this switch, the system may be vulnerable due to a lack of knowledge of defense against the new strategy.\nIn the face of these challenges, the objective of this paper is to develop a neurosymbolic model representation of an autonomous cyber-defense agent that (1) captures an explicit hierarchy of subtasks and the control flow required to execute subtasks, (2) employs LECs for specific subtasks, and (3) adapts to multiple dynamic cyber-attacks. The model must allow optimization of the LECs and generalize to multiple"}, {"title": "IV. OPTIMAL BT STRUCTURE FOR CYBER-DEFENSE", "content": "We present a neurosymbolic approach to autonomous cyber-defense that is robust against dynamic cyber-attacks. The agent constructed interacts with the environment using cyber-agent actions from a set of capabilities to mitigate adversarial red agents. Fig. 2 presents the control flow of the autonomous cyber-defense agent. Given a goal or specification, a symbolic structure acts as a model to interact with the environment. The symbolic structure utilizes cyber actions and sensor informa-tion to take effective action(s) via effectors against adversarial agents in the environment.\nThe symbolic structure in our autonomous cyber-defense agent is a BT. The BT allows us to reason about cyber-defense control at a high level and provides reactive switching to adapt based on environmental shifts. Additionally, BTs are modular, allowing new capabilities to be seamlessly integrated.\nIn this section, we describe the optimization method uti-lizing GP to compute a BT structure for cyber-defense con-trol. The first stage of the neurosymbolic design approach, in Fig. 3 focuses on learning the structure and high-level control of behaviors in the BT. A key contribution of learning the structure is the development of a novel abstract cyber environment, Cyber-Firefighter, to capture high-level computer network defense. Another benefit of the abstract environment is efficient simulation for evaluation of the structure of a BT over a more computationally heavy realistic simulator.\nWe design an abstract environment, Cyber-Firefighter, a partially-observable pursuit-evasion game, where a defender must contain an attacker from spreading over a network. The pursuit-evasion game is inspired by the Firefighter [19], a game where a \u201cfirefighter\u201d (defender) must optimally contain a spreading \"fire\u201d (attacker) in a network of \"trees\" (nodes) using full knowledge of the \u201cfire\u201d movements (attacker's movements) and a \"fire retardant\" (mitigation action) is ex-ecuted to block the spread. The game terminates when the \"fire\" can no longer spread to new \"trees\".\nIn the Cyber-Firefighter game, the objective remains the same. However, to make the game similar to cyber-defense on a computer network, the \"firefighter\" does not have full knowledge of the \"fire\" movements (attacker's movements). Therefore, we re-frame the problem as a partially observable environment and include two new actions to reveal information in the environment. 1) The \u201cfirefighter\u201d can deploy \u201cdrones\u201d (detectors) to detect the \"fire\" if it spreads into a \u201cdrone\u201d detection zone. 2) The \"firefighter\" can activate a \u201cdrone\u201d (per-form analysis) to increase the detection zone and gain further information from the environment. Once the information, {the \"fire\" has spread to} is revealed to the \u201ctrees\u201d (nodes), the \u201cfire retardant\" (mitigation action) is placed (executed) to block the spread.\nFormally, the Cyber-Firefighter environment can be repre-sented as a network graph, G: (V, E), where V denotes the set of \"trees\" or nodes and E denotes the set of edges or connections through which the \"fire\u201d can spread. The neighbor of a node v is denoted by N(v) = {u | (v, u) \u2208 \u0395; v, u \u2208 V}. Nr(v) denotes the nodes of the induced neighbor subgraph up to a radius r from node v."}, {"title": "B. Cyber BT Behaviors", "content": "The construction of a BT requires a set of behaviors as building blocks. The behaviors are generalizable, i.e., they can be mapped to our abstract environment as well as our realistic cyber-security environment, CybORG. There are a variety of behaviors that are used to construct the BTs [2]. In the construction of a BT, each timestep is known as a tick. A BT tick starts from the root behavior and follows a Depth-First Traversal. This traversal can shift depending on the returned status of child behaviors. The behavior(s) can return a status of Failure, Running or Success altering the execution traversal of the BT.\nBehaviors can be broadly classified into two groups: control behaviors and execution behaviors. Control behaviors are the internal behaviors in a BT that control the logical flow of switching between the behaviors. Execution behaviors are the leaf behaviors in a BT that execute the selected actions in the environment. The control behaviors in the BT can be either Sequences or Fallbacks. Sequences execute a set of child behaviors sequentially until all child behaviors return Success, return Failure otherwise. Fallbacks execute child behaviors until one child behavior returns Success, return Failure otherwise. The execution behaviors in the BT can be either Condition or Action behaviors. The return status of the execution behavior is determined based on its intended logical condition or user-defined functionality. After the status is returned, it propagates back up to the root, recursively updating the status of the parent control behaviors.\nWe define five Action behaviors, to allow the BT to defend against an adversary:\n1) SelectStrategy! selects a defense strategy based on an adversarial movement.\n2) GetMetaAction! selects one of three defense behaviors using the selected defense strategy.\n3) GetDetectorAction! deploys a detection mechanism in the environment to alert a local adversarial activity.\n4) GetMitigateAction! prevents an adversary from achiev-ing their objective, such as blocking adversarial move-ment or restoring a network node to a previously \"good\" state.\n5) GetAnalysisAction! monitors or analyzes the environ-ment retrieving new information that is unknown to the agent.\nFurthermore, there are four condition behaviors. There is one condition behavior for each of the three defense operation behaviors, to ensure a behavior is only enacted when chosen by GetMetaAction!. Additionally, there is a condition behavior for SelectStrategy! to ensure the strategy is only selected or shifted when necessary.\nTo construct BTs for execution in the Cyber-Firefighter environment, we map the defined BT behaviors to actionable behaviors in the environment. The three defense behaviors map to the three \"firefighter\" actions. Each defense behavior deter-mines a node to deploy a drone (GetDetectorAction!), activate a drone for further visibility analysis (GetAnalysisAction!),"}, {"title": "C. Learning BT Structure using Genetic Programming", "content": "To learn the optimal structure of the EBT in Fig. 4, we use GP combined with suboptimal baseline BTs. The inputs to the GP are the pre-defined BT nodes for the Cyber-Firefighter environment and a baseline BT that only considers strategy selection. The abstract environment will be used for evaluation and computation of a fitness value for optimizing towards an efficient BT that mitigates the attacker spread on the network.\nMore specifically, the GP algorithm is initialized with the set of execution behavior nodes and control behavior nodes. The BT is defined and generated randomly from these set of behaviors. The intention of the baseline BT is to provide a starting representation to encourage expansion and enhance training efficiency. After initialization, the GP algorithm selects, breeds, and evaluates BTs for a fixed number of generations using a fitness function, ultimately computing a solution that maximizes the fitness.\nThe fitness function, F(x), is the primary component in GP that drives the definition of the optimal BT. We consider the \u201cfire\u201d impact, the visibility of the network, and the size of BT nodes as a fitness function for the Cyber-Firefighter environment. We define the fitness function as\n$F(x) = c_v|I| - c_r|x| - c_f \\sum_{t=0}^{T} |\\{v \\in V | T_{burn}^t (v) = T_{max}\\}|$"}, {"title": "V. ROBUST BTS WITH LECS", "content": "Behaviors from the abstract cyber environment must be trans-ferred and implemented for the realistic environment, Cy-bORG CAGE Challenge Scenario 2, as shown in Fig. 3. Behaviors that require long-term objectives, such as cyber-defense against unknown attackers over an extended period, benefit from the integration of optimized LECs for robustness and generalizability. There are two LECs for our approach that focus on decision-making for selecting a cyber action behavior and switching strategies. Optimal LECs are then connected to behaviors in the BT structure for deployment in a new structure that we refer to as an Evolving Behavior Tree (EBT).\nIn this section, we first describe the transfer of behaviors to the realistic cyber environment, then derive the optimization techniques and policies for the LECs. At the end of the section we integrate the LECs with the BT structure to form the EBT for deployment."}, {"title": "A. Behavior Transfer in Realistic Cyber Environment", "content": "An optimal BT structure learned in an abstract environment must have its behaviors mapped to the realistic cyber envi-ronment for optimization and deployment. The realistic cyber environment employed in this paper is the CybORG CAGE Challenge Scenario 2.\nThe cyber-defense actions are mapped to their appropriate behaviors. GetAnalysisAction! maps to Analyze and Monitor as they reveal information about the computer network state. GetDetectorAction! maps to a greedy deterministic policy that selects a Deploy Decoy action conditioned on a host node. Furthermore, GetMitigateAction! maps to Remove and Restore because the actions prevent further damage caused by the attacker. To coordinate which of the above behaviors are selected, a cyber-agent controller policy is used that is executed by GetMetaAction! bahavior. Finally, the type of controller policy used is determined using the SelectStrategy! behavior. A strategy switching policy captured by NotSelect-Strategy? behavior determines if the controller policy needs to be switched."}, {"title": "B. Optimizing Behaviors", "content": "Components in the EBT with complex decision-making benefit from optimization techniques for generalizability and adaptation to novel scenarios. Both the cyber-agent controller and the strategy switching will need to be optimized to mitigate complex red agent behavior.\nThe cyber-agent controller contains a set of N learnable policies, [\u03c01, \u03c02, \u2026\u03c0\u03bd], for N red agent strategies. Each policy \u03c0i has an associated reward ri that maximizes with standard"}, {"title": "VI. EVALUATION", "content": "We perform an evaluation of optimizing the BT structure and LECs of the EBT. We determine if the optimal structure can appropriately mitigate an adversary and gain visibility of the network. For the EBT we determine if it is robust to multiple cyber-attacks compared to a state-of-the-art solution. We track the performance of simulations of Cyber-Firefighter and CybORG CAGE Challenge Scenario 2. We also discuss the explainable nature of the EBT and its role in monitoring the program flow."}, {"title": "A. Software Architecture", "content": "For evaluation, we developed a software architecture, shown in Fig. 5, that constructs and executes an EBT with a computer network simulator. A communication mechanism known as a blackboard [20] is instantiated between the the EBT and the simulator to maintain and update shared data. Similar to a publish-subscribe framework, the EBT and simulator can read or write particular data values based on access permissions. Behaviors in the EBT are restricted to access only variables on the blackboard specific to their functionality to prevent data leaking. To develop the EBT that are used for high-level decision making, we use the PyTrees library [20]. The tree that we construct contains actions for both the switching mechanism and the functionality to take specific actions. For training our autonomous agents, we use the PyTorch framework. We use this to decide which action should be taken based on the observation."}, {"title": "B. Genetic Programming Setup", "content": "For evaluation of learning the structure of the EBT, we consider a comparison of multiple BT structures and their respective performance in the abstract environment. The GP algorithm is initialized with 16 BTs and a baseline BT in Fig. 7. The baseline BT is boosted through the inclusion in each generation and preference towards crossover and mutation. In each generation, a BT is represented using a hierarchical list of strings representation, which can then be directly converted to an executable BT for evaluation. The string representation allows for simple operations to update and add new behaviors before execution. The algorithm is executed for 5 trials of 200 generations with approximately 5000 simulation runs (episodes) recorded with the best fitness for each generation stored. We compare the fitness of the GP BTs, the baseline BT, and an expert manually crafted BT."}, {"title": "C. GP Performance", "content": "Fig. 6a shows the GP fitness over 5 training runs of 4000 episodes for the Expert, Baseline, and GPBT. The GPBT fitness (plot in blue) improves from the baseline (plot in green) up to the Expert BT (plot in black). The learned BT structure, computed from the trials, achieves the best fitness of -19.45 (shown in Fig. 4). This is the same fitness as the Expert BT. Therefore, we can conclude that we can learn the structure of a BT to achieve an abstract cyber-security task with performance similar to a BT constructed with expert knowledge."}, {"title": "D. Optimizing Behaviors Setup", "content": "Opposed to the standard scenario, in which the red agent selects one strategy for the entirety of an episode, our scenario sees the RedSwitch agent that changes from a Meander to a BLine strategy. This strategy switching is set to randomly occur between timesteps 10 - 30, simulating how an attacker may change their strategy based on information of the network structure that they can quickly obtain. The defender (EBT) interacts with the environment in the architecture in Fig. 5 through the blackboard to determine and adapt from a red strategy change.\nTwo cyber-agent controller policies were trained based on the Meander and BLine strategies using a PPO policy. The policies consist of actor-critic neural networks with the actor and critic networks containing 1 layer of size 64. Both of these policies were trained for 100K observations with 1000 episodes each in a non-deterministic manner. This was done using a multi-layered perceptron developed by the CardiffUni team. Training was done in order to minimize the negative reward received by the blue agent from allowing the red agent to take certain actions and reach various hosts. The blue agent receives a reward of -0.1 per turn for each user and operational host (excluding user host 0) the red agent has access to. A reward of -1 per turn is received for each enterprise and operational server the red agent has access to, or if a blue agent performs a restore operation. If the red agent performs an impact action on the operational server, the blue agent receives a reward of -10 per turn. Since these rewards are negative, an optimal solution should be closer to 0, indicating better success at mitigating a red agent attack. Training rewards over 100K timesteps for the BLine and Meander cyber-agent controller policies are shown in Fig. 6b. The reward is maximized, asymptotically approaching 0, indi-cating the training was successful. Therefore, the policies are prepared for deployment evaluation.\nAn LSTM model was trained in order to carry out the strategy switching. This model was trained successfully with a window size of 5 over 5 epochs (3000 optimization iterations) with 100k observations (1000 episodes) using supervised Binary Cross Entropy (BCE) loss. Observations are collected using an oracle that correctly labels each strategy, Meander or BLine. The supervised loss is used to optimize the predicted label of the strategy from the LSTM with the correct labelling from the oracle using a window of observations. The LSTM model consists of a neural network with 2 layers of size 100. The learning rate used was le 3. BCE training loss for 3000 iterations over 5 training runs is shown in Fig. 6c. A supervised approach was taken in which the model was aware when the red agent switched strategies in order to maximize the efficacy of the training. Similar to training the PPO models, these agents were trained to minimize the reward received."}, {"title": "E. Attacker Strategy Robustness", "content": "In Figs. 8a and 8b, the cumulative reward over is plotted for CardiffUni and the EBT, recorded for 1000 episodes. The EBT (blue) achieves consistent results with the original CardiffUni approach (red). The EBT (blue) performs slightly below CardiffUni (red) on average over 1000 episodes. This is likely attributed to minimal overhead by BT execution. Therefore, integrating LECs into the GPBT structure to develop an EBT still achieves consistent performance with the state-of-the-art CardiffUni approach.\nFig. 8c, displays the cumulative reward over time recorded for 1000 episodes, comparing CardiffUni, OracleSwitch, and LearnedSwitch GPBTs. We found that our LearnedSwitch solution (green) shows a significant improvement of around"}, {"title": "F. Explainability", "content": "The CardiffUni solution used a simple sequential approach in carrying out the actions during each episode. However, we employed an EBT in our solutions, as we believe that it im-proves evaluation. The EBT precisely explains the operations of the simulation during each episode and acts as a runtime monitor for high-level behaviors. In particular, it allows us to visually determine when certain events are occurring, such as adapting to a new strategy or deploying a decoy. Visualization is achieved through the built-in functionality of PyTrees to render BT execution [20]. This provides us with an interface for following the execution of the components throughout the runtime of the EBT. Also, the EBT simplifies the task of performing the strategy switch during an episode. It includes designated nodes to evaluate the current red agent strategy and then switch its own strategy, if needed. These actions are explicitly independent as illustrated in the EBT, both from each other and from other high-level actions. This overall assists in improving control flow of the program."}, {"title": "VII. CONCLUSIONS & FUTURE WORK", "content": "In this work, we describe an approach for designing long-term autonomous cyber-defense agents enabled by Evolving Behavior Trees (EBTs). We utilize a GP algorithm and develop a novel abstract cyber environment to learn the high-level structure of the EBT. Then, we optimize the EBT in a realistic cyber environment with LECs for reactive strategy switching to mitigate complex and dynamic cyber-attacks. The structure of the EBT is modular and generalizable to autonomous cyber-defense in a network. The performance of learning the struc-ture is evaluated in the abstract cyber environment where we demonstrate how the structure learned will promote visibility and mitigate an attack. We then evaluate the integration of LECs in the EBT in the CybORG simulation environment using CAGE Challenge Scenario 2. Furthermore, we develop a software architecture to integrate the EBT with CybORG.\nOur results demonstrate that our proposed approach against the attacker strategy switching shows a 39% improvement in average reward compared to a state-of-the-art approach, and provides explainability for use in runtime monitoring of key events.\nOur future works will focus on scalability, emulation, and expansion to new attack scenarios. We aim to expand our set of available defense strategies to cover a multitude of attacks. We will also research methods to train individual policies that can generalize to a subset of red agent strategies. Additionally, we seek to evaluate our approach in an emulation environment to demonstrate the ability to deploy our solution in real network systems."}]}