{"title": "Transfer Learning of Tabular Data by Finetuning Large Language Models", "authors": ["Shourav B. Rabbani", "Ibna Kowsar", "Manar D. Samad"], "abstract": "Despite the artificial intelligence (AI) revolution, deep learning has yet to achieve much success with tabular data due to heterogeneous feature space and limited sample sizes without viable transfer learning. The new era of generative AI, powered by large language models (LLM), brings unprecedented learning opportunities to diverse data and domains. This paper investigates the effectiveness of an LLM application programming interface (API) and transfer learning of LLM in tabular data classification. LLM APIs respond to input text prompts with tokenized data and instructions, whereas transfer learning finetunes an LLM for a target classification task. This paper proposes an end-to-end finetuning of LLM to demonstrate cross-data transfer learning on ten benchmark data sets when large pre-trained tabular data models do not exist to facilitate transfer learning. The proposed LLM finetuning method outperforms state-of-the-art machine and deep learning methods on tabular data with less than ten features - a standard feature size for tabular data sets. The transfer learning approach uses a fraction of the computational cost of other deep learning or API-based solutions while ensuring competitive or superior classification performance.", "sections": [{"title": "1 Introduction", "content": "In the past decade, the field of artificial intelligence (AI) has advanced to new eras with the advent of deep learning (DL) and large language models (LLMs). While DL methods have revolutionized image learning and computer vision applications [1], pre-trained LLMs have shown unprecedented opportunities for generative AI. Image and text are primary subjects of AI because vision and language are two main modes of human communication. In contrast, tabular data structured in rows and columns are ubiquitous in science, engineering, and industrial applications, which are yet to utilize the strengths of deep learning [2, 3]. Despite unprecedented achievements of deep learning, traditional machine learning remains de facto for tabular data. Recent literature has concluded that deep learning methods are yet to \"conquer the castle\" of tabular data [4] or deep learning is \"not all we need\" for tabular data [5]. With only a fraction of samples and features of image or text data sets, most tabular data sets are often not considered for data-hungry deep learning.\n\nThe limited sample size problem in data-hungry deep learning is investigated by zero-shot or few-shot learning methods [6, 7]. Recent studies show promising performance of LLMs in zero or few shot learning of tabular data on tasks like classification [8, 9, 10], reasoning [11, 12], and text generation [11, 13]. These recent studies show that a pre-trained LLM can perform diverse tasks beyond text generation using few or no training samples. One key advantage of an LLM is its ability to learn from text semantics. While deep and machine learning techniques are designed to learn from numerical or encoded categorical features, LLMs can additionally learn semantics from feature names and descriptions in data sets or human instructions. Emerging studies on tabular data show that the efficacy of LLM-based application programmable interface (API) is limited to zero-shot or few-shot learning [8]. APIs are"}, {"title": "2 Background", "content": "Data structured in tables, namely tabular data, appear as spreadsheets or petabytes of data stored in relational databases in science, business, industry, and medicine. The feature space of tabular data is heterogeneous (e.g., salary, age) and permutation invariant, which fundamentally differs from homogeneous (e.g., pixel) image or text data where word or pixel positions are significant. The effectiveness of deep learning methods in AI can be attributed to homogeneous features sequentially positioned in pixel space or sentences. In contrast, recent studies reveal that deep learning methods are often less effective than traditional machine learning on tabular data with heterogeneous feature space [14, 5, 15].\n\nA large sample size is an important requirement of deep learning. The 100 most downloaded benchmark tabular data sets from the popular University of California Irvine (UCI) machine learning data repository have a median sample size of 660 and feature size of 18 [16]. Therefore, deep learning studies on tabular data selectively choose data sets with large sample sizes (>10,000) [15, 17, 18]. Alternatively, zero-shot and few-shot deep learning methods have been proposed to demonstrate the economical use of limited data samples when a large pre-trained model is available. An LLM has enabled no or few shot learning across various data domains, including tabular data.\n\nThe LLM literature on tabular data can be broadly grouped as in-context learning and fine-tuning [19]. In-context learning uses prompt engineering by sending custom prompts to guide the LLM in generating the desired response [20]. For example, the prompt for tabular classification includes data samples inscribed in text with tasks and additional descriptions [8, 10]. Prompt engineering aims at effective user interaction with the API at a higher abstraction level, avoiding the need to access or modify the underlying model. For example, the FeatLLM method prompts an LLM API to create rules or conditions on important features to facilitate the tabular data classification task. Unfortunately, the API has an upper limit on the number of tokens in the text prompt. For example, the gpt-3.5-turbo-0613 model from OpenAI can accept text prompts with a maximum of 16,385 tokens, which limits the number of input samples. Therefore, the API can process a fraction of available samples, which may not fully utilize data with a larger sample size. In contrast, the fine-tuning approach learns from the existing knowledge of an LLM by appending a classifier head to perform a custom classification task [10, 21]. The model weights can be finetuned via backpropagation following the loss function specific to a learning task.\n\nThe current literature is unclear on how the in-context solutions via an API compare against the LLM finetuning approaches in tabular data classification. The limit on token sizes has not shown the full capacity of LLM beyond no and few-shot learning. This paper proposes an end-to-end finetuning of an LLM to perform transfer learning of tabular data. The classification performance and computational costs are compared against traditional machine and deep"}, {"title": "3 Methods", "content": "This section provides the computational steps in preparing tabular data for classification using LLMs in addition to the baseline methods and experimental steps."}, {"title": "3.1 LLM for tabular data classification", "content": "A tabular data set is structured as $n \\times m$ dimensional matrix where n and m are sample and feature dimensions, respectively. LLMs can not directly process tabular data structured in rows and columns. Therefore, each row sample of a table must be converted to a text prompt. The steps to perform tabular data classification using an LLM are provided below."}, {"title": "3.1.1 Data serialization", "content": "Data serialization uses a computer program to textualize each sample with feature names and values in a text prompt. For example, a data sample with age and sex features is converted to \"Age is 25. Sex is male.\u201d Additional descriptions of features and tasks can be included in the text prompt as metadata, which is known as prompt engineering. Metadata provides important task-specific contexts to enhance the in-context learning of a pre-trained LLM. We perform data serialization in two steps. First, the feature and value pairs of each sample are separated into n serialized tokens. Second, additional information in the text prompt, including feature-specific metadata and a task, is included, as shown in Figure 1."}, {"title": "3.1.2 Data tokenization", "content": "After data serialization, text samples are tokenized by splitting into smaller components (e.g., words, subwords, or characters). Each unique token is a part of the model's vocabulary and mapped to a learnable d-dimensional embedding. Input tokens of each sample are padded or truncated to maintain a constant sequence length in line with the LLM's input shape requirements. In-context learning using an API limits the number of input tokens due to memory constraints. However, DistilGPT2 can take up to 1024 tokens per sample without restrictions on the sample size."}, {"title": "3.1.3 Large language models", "content": "We use DistilGPT2 [22] for transfer learning, which is a lightweight and open-source large language model obtained from the HuggingFace project [23]. DistilGPT2 is a knowledge distillation version of GPT-2 with 82 million parameters. The DistilGPT2 model is trained using the English Wikipedia and Toronto Book Corpus [24]. The authors of DistilGPT2 show that it retains 97% of full GPT-2 performance with 40% fewer parameters. DistilGPT2 has six transformers designed to take inputs from a token encoder. The token encoder converts input tokens to continuous vector representations in high-dimensional space. For in-context learning, we use OpenAI's gpt-3.5-turbo-0125 model to evaluate its performance in a recently proposed method for tabular data, FeatLLM [8]."}, {"title": "3.1.4 Modeling for classification", "content": "An LLM converts a batch of tokenized sequences into a new feature representation. A classifier head transforms the LLM feature representation into a logit vector to show the class correspondence. The LLM with a classifier head is finetuned in one of two ways: 1) LLM with all frozen weights and 2) end-to-end learning involving all trainable LLM weights. The classification task in finetuning minimizes a cross-entropy loss, as shown in Equation 1.\n\n$L_{CE} = \\frac{1}{n} \\sum_{i=1}^{n} \\sum_{c=1}^{C} Y_{ic} \\log P_{ic}$\n\nHere, n is the number of samples in a batch, C is the number of classes, $Y_{ic}$ is the true label for the i-th sample and class c, and $p_{ic}$ is the predicted likelihood of class c for the i-th sample.\n\nIn contrast, in-context learning involves no task-specific classifier head or learning steps. For example, FeatLLM prompts the LLM to output decision rules related to the classification task on important features, which are used to create a binary data matrix for standalone classification."}, {"title": "3.2 Baseline models", "content": "The baseline classification methods include the GBT classifier, an MLP, and a state-of-the-art deep learning method proposed for tabular data, self-supervised contrastive learning using random feature corruption (SCARF) [25] implemented in [26]. The GBT model is implemented using the Scikit-learn package [27]. The MLP network comprises three fully-connected hidden layers with 128, 64, and 32 neurons. We have updated the prompt generator of the FeatLLM method to include as many samples as possible under the constraint on token size."}, {"title": "3.3 Experimental setup", "content": "The experiments are performed on an Ubuntu 20.04 machine with an Intel(R) Core (TM) i9-13900F CPU @ 5.60GHz with 32 cores, 64GB RAM, and an RTX 4090 24GB GPU. Since we utilize PyTorch modules to build and train our model for deep learning methods, it automatically uses multiple CPUs for parallel processing. A five-fold cross-validation scheme is used for all experiments, where four folds are used to train a model and test using the left-out fold. One-eighth of the training data is used to validate the model against overfitting. We follow the experimental setup outlined in [26] while running GPT, MLP, and SCARF methods.\n\nDistilGPT2 is trained for up to 100 epochs with an early-stopping criterion and patience 10 using tokenized inputs. The model training uses a weight decay of 0.01 and a learning rate of 0.00005. The input batch size is 16. The final LLM embedding of size 768 is mapped to class logits for classification. The best classification model on the validation loss is saved for subsequent evaluation on test data folds. We report the average area under the operating characteristic curve (AUC) across five folds to report the final classification performance."}, {"title": "4 Results", "content": "The findings related to model design and experiments are discussed below."}, {"title": "4.1 Tabular data sets", "content": "The proposed LLM-based solution, deep and machine learning methods are evaluated using ten tabular data sets of diverse domains and sample sizes, including an electronic health record (EHR) data set, as shown in Table 1. We select an equal number of hard and easy-to-classify data sets based on a data selection criterion used in [26]. The ten tabular data sets are obtained from the UCI machine learning [28] and OpenML [29] repositories. The EHR data set is sourced from the Health Gym project, which is curated from the MIMIC-III database [30]. The metadata of each data set is used to complete a prompt template with the classification tasks and feature description as required by FeatLLM. The maximum number of samples (shots) for in-context learning of FeatLLM, due to the 15000 limit on token size, is shown in Table 1."}, {"title": "4.2 Prompt engineering", "content": "In-context learning has a limit on the number of tokens in input prompts. Therefore, the size of FeatLLM's output is limited to 1385 to reserve 15000 tokens for the input prompt. The FeatLLM prompt template also includes reasoning instructions, response instructions, feature metadata, and serialized examples, as shown in Figure 2. In contrast, transfer learning of LLM has a 1024-token limit per sample and can process data in batches to train on the entire data set. The transfer learning method uses a straightforward feature-to-text serialization method, avoiding the need for a complex text prompt, as presented in Figure 3."}, {"title": "4.3 Model training", "content": "In transfer learning, an LLM with all frozen weights is more rigid than the model with all trainable weights. The loss curves of the LLM with all frozen weighs reach a plateau above the loss value of 0.5, indicating underfitting. In contrast, LLM with all trainable weights yields a training loss closer to zero but starts to overfit after 30 epochs, as shown in Figure 4. Our early stopping criterion selects the model with the lowest validation loss for subsequent testing. Therefore, the proposed LLM-based transfer learning needs a proper balance of frozen and trainable weights to optimize the classification performance."}, {"title": "4.4 Model performance", "content": "Table 2 presents the classification performances of the proposed LLM-based transfer learning along with machine and deep learning methods. In terms of rank order, the traditional machine learning method (e.g., GBT) appears to be the best performing method, which is also supported by the tabular data classification literature."}, {"title": "4.5 Computational costs", "content": "Table 3 presents the average time in seconds to complete four-fold training and one-fold testing. As expected, GBT and MLP are the most efficient methods for completing these tasks within ten seconds. The in-context learning method (FeatLLM) suffers from long execution times of up to five minutes due to network latency from API access. Transfer learning using LLM, either with frozen or fully trainable model weights, takes 10% to 50% of the computational cost required in in-context learning or deep learning methods for tabular data. Considering the trade-off between computational costs and classification accuracy, transfer learning of tabular data using LLM stands out as the best learning approach for tabular data."}, {"title": "5 Conclusions", "content": "This paper investigates the performance of in-context learning and transfer learning of LLM in tabular data classification tasks. Our results reveal that transfer learning using LLM is superior to in-context LLM learning and deep learning methods proposed for tabular data in terms of performance and computational costs. This paper shows one of the unique strengths of LLMs in text-to-tabular transfer learning, which may advance computing in data domains with"}, {"title": "6 Acknowledgement", "content": "The research reported in this publication was partially supported by the US National Science Foundation (NSF) award # 2431058 and received support from the Air Force Office of Scientific Research under Grant Number W911NF-23-1-0170. The content is solely the responsibility of the authors and should not be interpreted as representing the official policies, either expressed or implied, of the Army Research Office or the U.S. Government."}]}