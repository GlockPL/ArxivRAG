{"title": "Developmental Predictive Coding Model for Early Infancy Mono- and Bilingual Vocal Continual Learning", "authors": ["Xiaodan Chen", "Alexandre Pitti", "Mathias Quoy", "Nancy F. Chen"], "abstract": "Understanding how infants perceive speech sounds and language structures is still an open problem. Previous research in artificial neural networks has mainly focused on large dataset-dependent generative models, aiming to replicate language-related phenomena such as \"perceptual narrowing\". In this paper, we propose a novel approach using a small-sized generative neural network equipped with a continual learning mechanism based on predictive coding for mono- and bilingual speech sound learning (referred to as language sound acquisition during \"critical period\") and a compositional optimization mechanism for generation where no learning is involved (later infancy sound imitation). Our model prioritizes interpretability and demonstrates the advantages of online learning: Unlike deep networks requiring substantial offline training, our model continuously updates with new data, making it adaptable and responsive to changing inputs. Through experiments, we demonstrate that if second language acquisition occurs during later infancy, the challenges associated with learning a foreign language after the critical period amplify, replicating the perceptual narrowing effect.", "sections": [{"title": "1 Introduction", "content": "The discourse surrounding the origin of human speech spans across an extensive historical timeline. [28] suggests that the mimetic capacity inherent in brain areas like F5 and Broca's area played a crucial role in the transition from gestural communication to speech [30]. As such, vocalization became more than just emotional reinforcement for facial expressions [1]. Instead, sounds acquired a descriptive value and needed to remain consistent in identical situations. This required not only the ability to produce specific sounds but also the ability to imitate sounds emitted by others, which likely contributed to the emergence of human Broca's area from a precursor with mirror properties. This area, associated with speech production and language processing, became crucial for the control and coordination of vocalization, especially in terms of imitation and precise execution of sounds [28].\nOn the other hand, studies [5] suggest that listeners can quickly mimic speech, demonstrating faster reactions to syllables or gestures that align with those of the speaker. This tendency to imitate may originate from early infancy, where it likely serves a crucial developmental role. Infants must decipher the phonetic nuances of speech signals from adults to learn how to map their own motor commands for speech reproduction. In humans, vocal imitation is a pivotal behavior that lays the foundation for language development. Studies have noted the mimicry of sounds emerge as early as 2 months of age [24]. This early vocal imitation has been found to contribute to later lexical development, and more importantly, to the acquisition of second language (L2), reflecting on the later ability for reproduction of foreign speech sounds [27]. As such, these speech sound imitation variations among individuals can lead to noticeable foreign accent disparities among late L2 learners [23].\nHowever, individual variances of capacity in L1 learning is not the only influence on L2 learning. A phenomenon called perceptual narrowing witnessed across various domains within the first year of age may account for that. The term perceptual narrowing [32] initially served as a descriptive phrase highlighting findings that infants exhibit greater sensitivity to various social signals early in life, including non-native speech sounds, and this sensitivity decreases over time as they become more attuned to their surrounding environments [13]. Though lost discrimination can be regained outside the critical period with extensive and substantial training, indicating perceptual narrowing involves attenuation or reorganization rather than complete loss, some phonetic distinctions continue to be challenging compared to face discrimination even with significant training [22].\nSuch a phenomena is closely related to two key factors. The first one is that \"minimal input\" [22], namely early foreign-language exposure [20] is needed to maintain these initial sensitivities. A famous example is Japanese children's deficit in distinguishing difference between English -r and -1 as the absence to the sounds during their early infancy due to the linguistic environment of their parents [19]. The second one is 'critical period', a distinct subset within the concept of 'sensitive periods' [13] that refers to a specific time window during which experience is essential for learning to take place, and the knowledge acquired during this period leads to lasting effects that are difficult to reverse [17]. After this period, the system will not be open to, or at least will be more resistant to, reorganization or retuning at a older age. [31] shows that adults' perception of speech sounds becomes constrained compared to that of newborns, as influenced"}, {"title": "2 Related works", "content": ""}, {"title": "2.1 Perception and production reinforce imitation", "content": "In Kuhl et al. [18], the role of imitation in vocal learning is emphasized as crucial for establishing the early connection between perception and production. In line with this, studies have shown that motor and sensorimotor systems can impact speech perception, even in infants who are too young to produce speech-like vocalizations [2]. [11] examine studies revealing that sensorimotor information regarding speech emerges prior to the developmental onset of speech-like vocalizations, as inferior frontal cortex (IFC) including Broca's area, exhibits activation during auditory-only speech perception even in very young infants who have not yet initiated speech-like vocalizations. Concurrently, Zhao et al. in [33] illustrate that activation patterns of the IFC exhibit correlations with speech sound perception in infancy: similar activation patterns are observed when neonates and early infants perceive both native and non-native speech sounds, with a decrease in activation and perception of non-native speech sounds occurring by the end of infancy."}, {"title": "2.2 Speech sound learning neural networks", "content": "In the following part, we discuss some neural networks for speech sound learning."}, {"title": "Perceptual narrowing", "content": "Schatz et al. [29] came up with a Gaussian mixture-based model that is able to reproduce perceptual narrowing. They demonstrated that merely 1 hour of input is sufficient for the emergence of distinct characteristics between Japanese and English models, with this gap amplifying with exposure to the native language. While the observation of intensified cross-linguistic contrast with increased data aligns with empirical findings in infants, it remains challenging to determine whether this correlation is attributable to dataset augmentation leading to overfitting or enhanced learning of linguistic features, as they did not involve L2 learning for further comparison and analysis based on the L1 learning model."}, {"title": "Sensori-motor learning", "content": "Georges et al. [8] introduced a computational model of speech production featuring forward and inverse models that can be jointly trained. It encompasses several components, such as a pre-trained neural articulatory synthesizer responsible for converting articulatory parameters into speech stimuli, a DNN-based internal forward model for predicting sensory outcomes, and an internal recurrent neural network designed to recover articulatory commands from acoustic speech input. While this model has shown the capability of reproducing speech sounds trained in a self-supervised manner using raw acoustic-only speech data, limited exploration has been conducted on linguistic-related features."}, {"title": "Other sound learning models", "content": "The Zero Resource Challenge (ZRC) [4] introduced the concept of replacing textual language databases with raw audio databases, drawing inspiration from the fact that infants acquire speech skills long before they can read or write. One of the four tasks proposed in the challenge, closely related to this paper, is acoustic unit discovery. Similarly, they argue that the units discovered should serve a fundamental linguistic function: distinguishing linguistic contrasts. However, most neural networks in this task employ large-scale architectures, with few exploring bilingualism."}, {"title": "3 Methods", "content": "In this section, we will introduce the encoder-decoder architecture of our generative network along with the schema of the proposed models."}, {"title": "3.1 Network architecture", "content": "The network architecture, depicted in Fig. 1, comprises an encoder and a decoder. The input to the neural network is a 20-dimensional MFCC (Mel-frequency cepstral coefficients) vector, corresponding to a 23ms sound segment. Layer Y is a one-dimensional self-organizing map (SOM) [14] consisting of a fixed number of 2000 neurons. The weight update rule is described in Equation 1 and the output of SOM layer Y in Equation 2, both adhere to the standard SOM formulation"}, {"title": "3.2 Proposed reconstruction modes", "content": "Continual learning mode The process of predictive coding based continual learning occurs subsequently to the training of layer Y. Predictive coding involves a hierarchical structure, where higher levels generate predictions for lower levels, and prediction similarities are propagated back up the hierarchy to refine those predictions. Here, the self-organised map becomes a static mapper or a priori structure as top layer, used to transform causes (input) to some internal representation [7].\nConsequently, the weights W2 between layers Y and Z represent the pattern (internal representation) transformed from the heard sound (cause) as shown in Equation 3, modulating the activities of Y, which in turn represents the pattern of the produced sound (predictions about the states of lower layers), as shown in Equation 4.\nThe similarity maximization in predictive coding is realised through multiplication as shown in Equation 5, where only the winning neuron in Z becomes activated, facilitating the continual generation of new sounds to approximate the heard sound.\nThe equation governing the update rule for the weights W3 connecting layer Z to the output layer is detailed in Equation 6. Here, io indicates that only the weight associated with the winning neuron in layer Z and the output layer is reinforced. This reinforcement is realized in the form of adding a randomly generated Gaussian-type input to itself if this Gaussian-type input amplifies the activity of the winning neuron within layer Z, minimizing prediction error.\nAs a result, X' represents the newly generated input for the next iteration, namely the reconstructed output of the current iteration, which is adjusted based on the input. In the hierarchical predictive coding structure, X' is considered as the lower layer, which is then looped back to Layer Y, serving as a prediction that is updated to minimize the discrepancy with the actual input.\nIn this setup, the SOM layer Y functions as a predictor, utilizing its learned mapping to generate predictions. Meanwhile, the reconstructed output X' undergoes adjustments contingent upon sensory input. This process mirrors the principles of predictive coding, where continual learning is facilitated by iteratively updating the SOM's internal representation with fresh input data X'. This adaptability empowers the system to evolve its predictions in response to environmental changes, thus enhancing prediction accuracy over time. Through this continual learning mechanism, the model exhibits a simplified hierarchical structure within the predictive coding framework. By leveraging similarity to fine-tune predictions, it refines its predictive capabilities and adapts to dynamic environmental cues.\nCompositional optimization mode The CO mode shown on the right side of Fig. 2 utilizes the combination coefficient W4, which is optimized through backpropagation as described in Equation 8, without involving any learning.\nAs previously discussed, the replication of later-acquired sounds relies on the accumulation of various sound elements obtained during a crucial developmental period in early life, achieved through the incorporation of multiple acquired sounds, which we term \"constituents\". In the subsequent experiment, we set the number of constituents N to 10. It's noteworthy that performance tends to enhance with a greater number of constituents (N \u2264 |Z|)."}, {"title": "4 Experiments", "content": "In this section, we will detail the learning process of the models, along with the design and results of experiments."}, {"title": "4.1 Datasets", "content": "As previously mentioned, our objective is to investigate the impact of the sound learning timing on second language sound acquisition. Accordingly, we utilize three distinct language datasets: English (sourced from [25]), French (recorded by French speakers, encompassing both male and female voices), and Chinese (sourced from [21]). Here, English serves primarily as the training dataset, thus regarded as L1, while the other two languages serve as L2."}, {"title": "4.2 Training details", "content": "The English training dataset is a 16 minutes' 16kHz multi-person read English speech sounds, randomly chosen among the original 1000 hours corpus of read English speech [25]. They are transformed into more than 41000 20-dimensional MFCC using Librosa Python library by setting hop-length = 512 (number of samples between successive frames), and n_fft = 1024 (length of the FFT window). These MFCCs are then fed into layer Y (SOM). We trained the layer Y for 100000 iterations. It is noticeable that the radius of the neighborhood of BMU changed along time. Specifically, here we apply Mexican hat function as the neighborhood function.\nAs for the generative model, we initially configure the size of layer Z to be 5000, while setting the threshold value at 0.00002 for adapting to an empty state or adding a new neuron, which is subsequently identified as the winner neuron io. This mechanism is based on the hypothesis that the variability of activity within a Self-Organizing Map (SOM) can serve as an indicator to discern whether the observed sound is novel or previously learned. In this hypothesis, when encountering a new sound, the activity pattern within the SOM would differ significantly, potentially necessitating the involvement of different neurons in its reconstruction compared to a familiar sound. Essentially, the SOM's ability to adapt its neural activations in response to new stimuli allows it to distinguish between learned and novel sounds based on the variance of this activity. It is crucial to acknowledge that increasing the threshold value enhances the dynamism of layer Z, but simultaneously, it exacerbates the problem of poor generalization."}, {"title": "4.3 Results", "content": "In this section, we will represent and analyse experimental results obtained from the training and the designed experiments talked previously."}, {"title": "Self-organised map", "content": "A fundamental characteristic of SOM is that neurons with geographic proximity encode similar inputs. As such, in Fig. 3a, high-dimensional neurons are projected into a 2-dimensional space using PCA (Principal Components Analysis) for visualization, with each dot representing a neuron. It's noteworthy that only neurons coding for at least one input X are depicted (1603 out of 2000). Dots are displayed in various colors, and colors that are closely aligned according to the colorbar next to the figure signify geographic proximity. Similarly, Fig. 3b encodes inputs into a 2-dimensional space using PCA. However this time, each dot represents one input MFCC data (totaling more than 41000), with the color of each dot corresponding to the color of the winning neuron. Thus, inputs sharing the same neuron are depicted in the same color. Additionally, inputs whose winner neurons are neighbors, though not shared, should exhibit similar colors based on the colorbar next to the figure. The analysis of Fig. 3 yields two notable observations. Firstly, in Fig. 3a, closely located dots exhibit similar colors as indicated by the colorbar, suggesting that neighboring neurons are indeed close to each other in the original 20-dimensional space. Additionally, the color variations of the dots align with those of the colorbar, reinforcing the proximity of neighboring neurons. Secondly, in Fig. 3b, inputs belonging to the same clustering are encoded by the same neuron or neighboring neurons, evident from the shared or similar colors among the dots. This correspondence between input clusters and neuron encoding further validates the effectiveness of the SOM learning process. Therefore, we can confidently conclude that the SOM has learned effectively to discriminate the input MFCC."}, {"title": "Pattern similarity and sound reconstruction", "content": "In the CL mode depicted in Fig. 2, symbolizing the speech sound learning of newborns during early infancy, it is imperative to evaluate its ability to reproduce input sounds. Therefore, it is essential to assess the efficiency and effectiveness of the model in reproducing MFCC inputs. Fig. 4a illustrates the trend of error between the input MFCC in the training dataset and the reconstructed MFCC. Here, we observe a decrease in error over time, with the mean error among 41000 inputs approaching 0. A similar trend is evident in the test dataset, as depicted in Fig. 4b. Additionally, as mentioned earlier, a randomly generated Gaussian-type input is added to itself if this input amplifies the activity of the winner neuron, as described in Equation 6. This mechanism aims at approximating the pattern of the reproduced sound to that of the heard sound. Consequently, if the error of reproduction decreases, so should the error between these two patterns. This error is visualized in Fig. 4c, indicating a decrease over time, eventually approximating 0 by the end of 1000 iterations."}, {"title": "Mono/bilinguality and reconstruction mode comparison", "content": "In addition to its ability to imitate sounds regardless of language, the model should excel at capturing the inherent differences between languages. This capability is crucial for exploring L1 or L2 language learning during different age of development. In these experiments, we investigate the differences between models that learn solely one language and those that learn two languages.\nAs mentioned earlier, we propose two modes, CO and CL, representing sound imitation during different developmental stages: early infancy (before one year of age) versus later infancy or adulthood. Consequently, we have designed four conditions to independently investigate the effects of minimal input and critical period. Specifically, for the minimal input experiment, we compare the ability to imitate L2 sounds between models trained on both L1 and L2 and models trained solely on L1. For the critical time experiment, we further compare the imitation capability of L2 sounds separately under the CO and CL modes, which correspond to later infancy and early infancy respectively.\nIn the following experiment, we test the mode of reconstruction with models that learn English as L1 and French or Chinese as L2. As we can see in Fig. 5, there are 2 modes of reconstruction, and on the left side of each mode is the model that has learned only English whereas on the right side has learned 8min L2 French or Chinese in addition to the 16min English. The blue boxes reconstruct MFCC outputs based on compositional optimization (constituents number N = 10) whereas the orange ones based on continual learning. Please note that the test input language in this experiment is L2.\nFocusing on the left bars within each group in Fig. 5b and Fig. 5a, which corresponds to the model that learned English solely, we observe the following performance trend across modes of reconstruction: CL > CO. This can be interpreted as follows: when the acquisition of L2 occurs after the critical period (CO mode), it is more challenging to achieve performance comparable to learning during this period (CL mode). This result aligns with the perceptual narrowing phenomenon mentioned earlier.\nIf we compare the results within the group of blue boxes, we notice that errors of reconstruction decrease when L2 is learned, while errors in CL groups remain the same, as L2 has already been learned in both cases in the orange box group (L2 continual learning on L1 learned model (left orange box) is the same as L1+L2 learned model (right orange box)). It is worth noting that even after learning L2 in the CO mode, the error is still higher than in the CL mode (cn: 0.38 > 0.21; en: 0.47 > 0.22). This is due to the number of constituents not being optimized. However, the main focus here is on the decrease between the two blue boxes, where the minimal input of them is L1 only (left blue box) versus L1+L2 (right blue box), thus further supporting the perceptual narrowing hypothesis.\nOne may question whether the reduction in error within the CO group in Fig. 5b and Fig. 5a is attributed to the increased size of the learning dataset rather than linguistic features alone. To address this, we introduced a control group by considering L1 English as the L2, as depicted on the left of each group with the horizontal axis labeled 'en+en'. The disparities in reconstruction error under CO mode among the L2 languages are as follows: Chinese \u2248 French >> English. This observation suggests that the reduction in error is more likely due to the quality rather than the quantity of the dataset."}, {"title": "Catastrophic forgetting", "content": "Catastrophic forgetting poses a significant challenge in neural networks and algorithms [9]. It refers to the phenomenon where a model tends to erase previously acquired knowledge when learning new information. In this experiment, we aim to determine if our proposed neural network is susceptible to this issue. We compare the L1 reconstruction capacity between models trained solely on L1 (with X axis value 'En') and those trained on first L1 and then on L2 (with X axis value 'En+Fr' or 'En+Cn'), irrespective of reconstruction mode. Theoretically, if the proposed neural networks suffer from catastrophic forgetting, L1 reconstruction errors of models that learned L1 for 16 minutes should be much lower than those of models that first learned L1 for 16 minutes and then 8 minutes of L2, regardless of the mode group. However, as depicted in Fig. 7, only subtle differences are observed across all reconstruction modes, suggesting that our model may not suffer from catastrophic forgetting.\nOne potential explanation for the resilience of our neural network against catastrophic forgetting could be attributed to the mechanism where empty (or new) neurons are activated (or added) based on the variance of neuron activity in the self-organizing map."}, {"title": "5 Discussion", "content": "In this paper, we introduced a simple neural network trained on a limited dataset comprising 16 minutes of L1 data and 8 minutes of L2 data. Our primary objective was to investigate two hypotheses. Firstly, we sought to determine whether early infancy imitation of speech sounds contributes to later foreign language development. To explore this, we designed an encoder-decoder neural network capable of reconstructing speech sound inputs represented as MFCC. In addition to being able to reconstruct sound fed from input, to avoid the possibility that such performance was due to overfitting, we introduced a mono-bilingual model and compared its performance. Our results showed that, on one hand, models that learned L2 additionally based on the L1 baseline model outperformed L1 models in L2 performance. We introduced a control group where L1 played the L2 role to eliminate the possibility that such better performance was due to the increased size of the learning dataset. On the other hand, we also showed through experiments that having learned L2 did not lead to catastrophic forgetting in L1. These results indicate that our neural networks excel at capturing the inherent differences between languages to perform imitation. Secondly, we investigated whether the concepts of minimal input and critical period, closely associated with the perceptual narrowing phenomenon, influence infancy sound imitation. We designed two modes of reconstruction to simulate learning in different periods: Continual learning mode for language sound acquisition during the critical time window versus compositional optimization mode for language sound acquisition after the critical period. Our experiments were able to reveal that if second language acquisition occurs during later childhood or adulthood, learning foreign languages becomes more challenging.\nAs discussed previously, the neural networks we proposed are small-sized neural networks, where both the number of neurons and layers are much smaller than deep neural networks, which typically require substantial offline training periods. Differently, our proposed predictive coding-based continuous learning mode is a form of online learning, and its ability to incorporate new information in real-time without necessitating a complete retraining of the model ensures that online learning systems remain relevant and accurate even as the data evolves, which can be challenging for pre-trained deep networks to handle effectively.\nIndeed, one may notice that we did not compare the imitation performance between deep networks and our proposed neural networks. However, the objective of our study is to develop a simple, explanatory model rather than to achieve the highest performance metrics. Our focus is on creating a model that is sufficiently straightforward to elucidate the underlying infant sound learning mechanism. Unlike complex deep learning models that prioritize performance at the expense of transparency, our approach aims to strike a balance between simplicity and functionality. By doing so, we enable a clearer understanding of how the model processes and interprets data, which is crucial for applications where interpretability and insight into the learning process are as important as the outcomes themselves. This emphasis on simplicity and explanatory power ensures that the model is not only effective but also accessible and informative.\nIn the future, we intend to further develop this neural network for language sound sequence (phonotactic patterns) learning, focusing on addressing the varying phoneme-sets between L1 and L2 languages, as well as the complexity introduced by tonal variations, especially in languages like Chinese."}], "equations": ["w_{ij}(t + 1) = w_{ij}(t) + \\Theta(i_0, i, t) \\cdot \\alpha(t) \\cdot (x_j \u2013 w_{ij}(t))", "Y = \\frac{1}{1 + ||X - W_1||^3}", "W_2 = Y", "Y = \\frac{1}{1 + ||X' - W_1||^3}", "Z = W_2Y", "w_{i_0}^{z}(t + 1) = w_{i_0}^{z}(t) + I_G \tewhere I_G \\sim N(0, 0.01)", "X' = \\delta_{i_0}(Z) \\cdot W_3 \\tewhere \\delta_{i_0}(Z) = \\begin{cases}1, & \\text{if } i = i_0\\\\0, & \\text{otherwise}\\end{cases}", "w^{(x)}(t + 1) = w^{(x)}(t) \u2013 \\eta ||x - x''||^2", "X' = \\delta_{in}(Z) \\cdot W_3 \\tewhere \\delta_{in}(Z) = \\begin{cases}1, & \\text{for the indices of N highest values in Z}\\\\0, & \\text{otherwise}\\end{cases}", "X'' = W_4 \\cdot X'"], "keywords": ["Speech sound learning", "Continual learning", "Compositional optimization"]}