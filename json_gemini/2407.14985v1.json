{"title": "Generalization v.s. Memorization:\nTracing Language Models' Capabilities Back to Pretraining Data", "authors": ["Antonis Antoniades", "Xinyi Wang", "Yanai Elazar", "Alfonso Amayuelas", "Alon Albalak", "Kexun Zhang", "William Yang Wang"], "abstract": "Despite the proven utility of large language mod-\nels (LLMs) in real-world applications, there re-\nmains a lack of understanding regarding how they\nleverage their large-scale pretraining text corpora\nto achieve such capabilities. In this work, we\ninvestigate the interplay between generalization\nand memorization in pretrained LLMs at scale,\nthrough a comprehensive n-gram analysis of their\ntraining data. Our experiments focus on three gen-\neral task types: translation, question-answering,\nand multiple-choice reasoning. With various sizes\nof open-source LLMs and their pretraining cor-\npora, we observe that as the model size increases,\nthe task-relevant n-gram pair data becomes in-\ncreasingly important, leading to improved task\nperformance, decreased memorization, stronger\ngeneralization, and emergent abilities. Our results\nsupport the hypothesis that LLMs' capabilities\nemerge from a delicate balance of memorization\nand generalization with sufficient task-related pre-\ntraining data, and point the way to larger-scale\nanalyses that could further improve our under-\nstanding to these models.", "sections": [{"title": "1. Introduction", "content": "Pretrained large language models (LLMs) have shown im-\npressive performance on many text-based tasks, but there\nis a debate about whether they are generalizing on unseen\ntest cases or simply memorizing from their vast training\ndata (Magar and Schwartz, 2022; Srivastava et al., 2024;\nBender et al., 2021). Previous works have studied LLM\nmemorization as exactly recalling training examples (Zhang\net al., 2023; Jiang et al., 2024; Carlini et al., 2022). However,\nusually, we aim to utilize more sophisticated LLM capabili-\nties, which cannot be explained by copying training data. In\nthis paper, we extend the definition of memorization beyond\nexact copying and study how pretraining data contributes to\nhigher LLM capabilities. We define memorization as the\ndegree of similarity between LLM generations and training\ndata, and generalization as how different the generated con-\ntent is from training data. Several papers have studied the\ninterplay between memorization and generalization (Feld-\nman, 2020; Feldman and Zhang, 2020; Zhang et al., 2023),\nbut analyzing pretrained LLMs at scale remains challenging.\n1 We propose to estimate the pretraining data distribution by\nn-gram pairs mined from task data. The appearance of such\npairs in training data can be regarded as weak supervision\nof the testing task. We conduct experiments with the Pythia\n(Biderman et al., 2023) and OLMO-7B (Groeneveld et al.,\n2024) models on translation, factual question answering,\nand reasoning tasks. Our findings show that:\n1. Task-relevant n-gram pairs are better representative of\ntask-related data than single n-grams.\n2. Task performance is positively related to n-gram pair\nfrequency.\n3. The phenomenon of emergent abilities can be viewed as\na mismatch between adequate task-related pretraining data\nand inadequate model size.\n4. Small LMs memorize while large LMs generalize.\n5. Instruction tuning helps LM make better use of pretrain-\ning data.\nTo the best of our knowledge, this is the first effort to analyze\nthe origin of LLM capabilities on full pretraining corpora."}, {"title": "2. Problem Setting", "content": "The pretraining corpus of LLMs is usually huge, with bil-\nlions even trillions of tokens. It is hard to directly analyze\nit without aggressive down-sampling (Kirchenbauer et al.,\n2024). In order to model the whole pretraining corpus, we"}, {"title": "3. Experiment Setting", "content": "In this section, we introduce the datasets and models we use\nfor analyzing the memorization and generalization behav-\niors of LLMs.\nModels and Pretraining Corpus We utilize two families\nof fully open-sourced LMs: Pythia (Biderman et al., 2023)\nand OLMO (Groeneveld et al., 2024). Both of them are\nautoregressive Transformer-decoder-based LMs. Pythia (Bi-\nderman et al., 2023) is a classic suite of fully open-sourced\nLMs with a wide range of model sizes ranging from 13M\nto 12B parameters. All Pythia models are trained on Pile\n(Gao et al., 2020), a diverse pretraining corpus consisting\nof approximately 207B tokens. OLMo (Groeneveld et al.,\n2024) is a more recent, more performant suite of fully open-\nsourced LMs, pretrained on a larger corpus Dolma (Soldaini\net al., 2024) with approximately 3T tokens, and instruction-\ntuned on Tulu (Wang et al., 2023b; Ivison et al., 2023).\nDownstream Tasks We use three types of tasks: transla-\ntion, factual question answering, and reasoning with multi-\nple choices.\nFor translation, we use the WMT-09 dataset with a 2.5K\ntesting set, consisting of European languages aligned with"}, {"title": "4. Quantify n-gram Contribution by Gradient", "content": "To justify the usage of n-gram pairs and understand\nthe contribution of the task-relevant n-gram pairs to\nLLM's capability of solving the task, we propose a\ngradient-based analysis inspired by Han et al. (2023). More\nspecifically, we quantify the contribution of a n-gram by\nthe similarity between its pretraining loss gradient and\nthe task example gradient. For a task-relevant n-gram\npair $(s_x, s_y)$, with testing task example (x, y), and $s_x \\in x$,\n$s_y \\in y$, the task gradient $g_T(s)$ is defined as: $g_T(s) =$\n$\\frac{\\partial}{\\partial \\theta} [-log P_{LM}(Y_{[i:i+n-1]}|U, X, Y_{[1:i-1]}) | Y_{[i:i+n-1]} = s\"].$\nHere $\\theta$ denotes all parameters of the LM. To com-\npute the gradient of the n-gram pair $(s_x,s_y)$ in the\npretraining corpus D at the pretraining time, we first\nretrieve K documents containing the n-gram pair from\npretraining corpus D. Here a maximum number of\nretrieved documents is set for computing efficiency,\nas back-propagating through a huge number of docu-\nments for each n-gram pair is infeasible for our study.\nDenoting the retrieved documents by ${d_1,d_2,...,d_K}$,\nthe pretraining gradient $g_D(s)$ is defined as: $g_D(s) =$\n$\\frac{1}{K} \\Sigma_{k=1}^{K} \\frac{\\partial}{\\partial \\theta} \\Sigma_{i=1}^{L-n} [-log P_{LM}(d^k_{[i:i+n-1]} | d^k_{[1:i-1]}) | d^k_{[i:i+n-1]} = s_Y ].$\nIn practice, we clip long documents to a maximum length to\nfit into the GPU memory. Then we compute the contribution\nof the pretraining data containing task-related n-gram pairs\nto the task example (x, y) by cosine similarity between the\ntask gradient and pretraining gradient:\n$\\beta_p(x, y, D) = \\Sigma_{(s_x,s_y)\\in H_n(T)} C((s_x, s_y), D)cos(g_T(s), g_D(s'))$"}, {"title": "5. Estimating Data Distribution", "content": "In this section, we model the data distribution with the fre-\nquency of the previously defined n-gram pairs. Denote\n$C((s_x, s_y), (x, y)) = min{C(s^*,x), C(s_y, y)}$ to be the\nnumber of occurrences of the n-gram pair $(s^x, s^y)$ in a\ntask example (x, y). We can also define the n-gram paral-\nlel pair count in a document string d by $C((s_x, s_y), d) =$\n$\\Sigma_{s_i \\in G_n(d)} \\Sigma_{s_j \\in G_n(d)} 1_{s_i=s_x} 1_{s_j=s_y}$. If we define the pre-\ntraining data distribution to be over all the possible n-\ngrams, then the empirical data probability of a n-gram\npair $(s_x, s_y)$ would be $P_D(s_x, s_y) \\propto C((s_x, s_y), D) =$\n$\\Sigma_{d\\in D} C'((s_x, s_y), d)$."}, {"title": "5.1. Task-related Data Frequency v.s. Task Performance", "content": "In this section, we show a strong positive correlation be-\ntween the frequency of task-related data in the pretraining\ncorpus and LM's task performance. We also show that some\nabilities appear to be emergence because of the mismatch\nbetween data and model size.\nWe can estimate the probability of task T related data ap-\npearing in the pretraining corpus as the probability of any"}, {"title": "5.2. Data Distribution v.s. Language Model Distribution", "content": "In this section, we measure the memorization of LMs by the\nsimilarity between the LM distribution and the pretraining\ndata distribution, while we measure the generalization of\nLMs by the novelty of the generation, in terms of distribu-\ntion difference or the amount of novel n-grams.\nWe can decompose a pair of input-output text (x, y) into\nmany such n-gram pairs. Then suppose the n-gram pairs are\nmutually independent, the empirical data probability of a\npair of translated sentences (x, y) can be decomposed into:\n$P_{D,n}(x, y) = \\Pi_{s_i \\in G_n(x)} \\Pi_{s_j \\in G_n(y)} P_D(s_i, s_j)^{1_{(s_i,j)\\in H_n (T)}}$\n$\\propto exp(\\Sigma_{(S_i,S_j)\\in H_n(T)} C((s, s_y), (x, y)) log C'((s^*, s_y), D))$ (1)\nTo avoid excessive searches over the pretraining corpus, we\nsuppose the marginal distribution $P_{D,n}(x)$ is similar for all\ninput x's from task T. Then we approximate the conditional\ndistribution as $P_{D,n}(y|x) \\propto P_{D,n}(x, y)$. Similarly, we can\ndecompose the LM probability $P_{LM}(\\hat{y}|u, x)$ into n-grams\nas follows:\n$P_{LM,n}(\\hat{y}|u, x) = exp (\\Sigma_{i=1}^{L-n} [log P_{LM}(\\hat{y}_{[1:i+n-1]}|U, X, \\hat{Y}_{[1:i-1]})$\n$\\Sigma_{s_x \\in G_n (x)} 1_{(s_x, \\hat{y}_{[i:i+n-1]}) \\in H_n(T)}])$ (2)\nThen we can compare the (empirical) data distribution and\nthe LM distribution. Since the sample space of text se-\nquence $\\hat{y}$ is too large, even after decomposed into n-grams,\nthe common distribution similarity measure requiring the\nwhole distribution over the sample space would be infeasi-\nble. So we choose to fit a linear regression instead from the\nlog data probability log $P_{D,n}(\\hat{y}|x)$ to the log LM probabil-\nity log $P_{LM,n} (\\hat{y}|u, x)$ for each training example (x, y). In\nthis way, we take into consideration the possible mismatch\nin scale between these two distributions, but reserve the\ndistribution shape. We measure the closeness of these two\ndistributions by the R2 score of the regression."}, {"title": "6. Conclusion", "content": "In this paper, we propose a scalable method to trace LLMs'\ncapabilities back to the pretraining data by searching for\npretraining data at the n-gram level and enforcing seman-\ntic similarity within n-gram pairs using embedding models.\nThis approach enables an extensive search across the pre-\ntraining corpus while allowing direct interpretation of the\nn-grams. Experiments with Pythia and OLMO models on\nvarious tasks reveal that task-relevant n-gram pairs play a\ncrucial role in model performance, with small models tend-\ning to memorize and larger models demonstrating enhanced\ngeneralization. This analysis is a first step in comprehen-\nsively analyzing the origins of LLM capabilities."}, {"title": "Appendix", "content": "While this paper provides valuable insights into how large-scale pretraining corpus contributes to the emergent abilities\nof LLMs through n-gram search, there are a few limita-tions that we want to list out. First, the main model we use,Pythia, and the main pretraining corpus, Pile, is slightly out-dated, and has been outperformed by many new open-sourceLLMs. Most open-source LLMs lack corresponding pre-training data and have limited model sizes, hindering scalingeffect studies. The recently released Neo LLMs and Matrixpretraining corpus offer better experimental opportunitiesfor future work. The current WIMBD system has limita-tions in searching larger corpora like Dolma (3T tokens)and Matrix (4.7T tokens), requiring improved searchingand retrieval methods. The quality of task-relevant n-grampairs is highly sensitive to the filtering method, and whilethe current embedding similarity-based approach is effec-tive, better filtering methods could significantly enhance theanalysis, which is left for future research."}, {"title": "B. Broader Impacts", "content": "The insights and methodologies developed in this paperhave several significant implications for the broader field ofartificial intelligence, particularly in the development anddeployment of large language models (LLMs). Understanding the balance between memorization and generalizationwithin LLMs is crucial for both advancing the theoreticalfoundation of machine learning and addressing practicalconcerns related to their use.\nEnhanced Model Interpretability: By extending the def-inition of memorization and examining how LLMs utilizetheir pretraining data, our research contributes to a deeperunderstanding of the internal mechanics of these models.This improved interpretability can help researchers and prac-titioners diagnose and mitigate issues related to data bias,model robustness, and unexpected behaviors in AI systems.\nPri vacy and Security Considerations: Our findings havedirect implications for privacy and security in AI. Demon-strating how LLMs memorize and potentially recall train-ing data underscores the need for rigorous data handlingand anonymization techniques. It raises awareness aboutthe risks of inadvertent leakage of sensitive information,thereby informing policy and best practices for data usagein training large models.\nEconomic and Societal Impact: As LLMs become more in-tegral to various industries, understanding their capabilitiesand limitations can have significant economic and societalimplications. Our research can help businesses and poli-cymakers make informed decisions about deploying thesemodels, ensuring they are used ethically and effectively.This, in turn, can lead to more reliable and trustworthy AIsystems, fostering greater public trust and acceptance."}, {"title": "C. Related Work", "content": "Understanding LLMs' capabilities from training data\nBecause of the scale of the data and model sizes, mostwork on understanding LLMs attempts to examine howLLMs gain their capabilities from synthetic experimentsor on a small scale (Arora and Goyal, 2023). Prystawskiet al. (2023) and Wang et al. (2024) discuss how the reasoning ability of language models is a consequence of theirpretraining data. Prystawski et al. (2023) discusses howchain-of-thought reasoning is effective in autoregressive lan-guage models because of local structure within pretrainingdata, and Wang et al. (2024) derives novel conclusions fromknown facts by aggregating reasoning paths seen in pretrain-ing data. On the other hand, Xie et al. (2022) and Wang et al.(2023a) discuss how in-context learning is a by-product oflearning the pretraining data distribution. They both sug-gest that language models learn to implicitly infer a latentvariable from the given prompt, as the pretraining data isgenerated from some unknown latent variable. Additionally,Chan et al. (2022) propose that the distributional propertiesof training data drive emergent in-context learning behaviorsin large language models. Chen et al. (2024) also highlightsthe significance of parallel structures in pretraining data forthe emergence of in-context learning.\nHowever, the small-scale nature of such analysis is antithet-ical to the commonly believed main driving factor behindthe performance of LLMs: scaling. Recently, Kirchen-bauer et al. (2024) proposes to provide statistical evidenceof the dependence of a target model capabilities on sub-sets of its training data, by estimating the data distributionwith an embedding-induced kernel. However, their esti-mation is based on a very small portion of the pretrainingdata (around 0.3%) as computing the embeddings of a hugedataset is very non-trivial. To get a better estimation ofthe whole distribution of the pretraining data, Elazar et al.(2024) construct a retrieval system, WIMBD, that can effi-ciently search n-gram phrases over hundreds and thousandsof GBs of pretraining data. However, it is unclear what in-sights of the LLMs trained on these datasets can be obtainedfrom such searches.\nNew methods and analysis to investigate these capabilitiesat scale and to understand the role of scaling are needed toobtain useful insights into real-world LLMs. In this work,we aim to provide an in-depth analysis of the origin ofthe general zero-shot capabilities of LLMs, by performingfull searches across the whole pretraining corpus with theWIMBD framework."}, {"title": "Memorization v.s. generalization", "content": "The phenomenon ofmachine learning models being able to perfectly memorizethe training data has been studied in many previous works.Most of them define LLM memorization as exactly recallingthe training examples by designed prompting, including thememorization of rare long-tail data, like private information(Zhang et al., 2023), and the contamination of testing sets(Jiang et al., 2024). Carlini et al. (2022) found that the exactcopy and pasting behaviors are more prevalent in largerLMs.\nSeveral papers have studied the interplay between memo-rization and generalization of training data. Feldman (2020)prove that memorizing the training data is in fact required foroptimal generalization on testing data. Many works alongthis line (Feldman and Zhang, 2020; Zhang et al., 2023)extend the original definition of memorization by quantify-ing the extent of memorizing a training example with theperformance difference when including and excluding thisspecific example in training data. However, this definition isimpractical for large-scale analysis of pretrained LLMs asit would require retraining an LLM from scratch to analyzeone data point. In this paper, we propose a new definitionof memorization by using n-gram counts, which is moresuitable for large-scale analysis with LLMs."}, {"title": "D. Experiment Details", "content": "We perform our experiments on 8 GPU 40G A100 workingstations. Below is the license information for the datasetswe used:\n\u2022 Pile: MIT license. URL: https://github.com/EleutherAI/the-pile/tree/master\n\u2022 Tulu: ODC-BY license. URL: https://huggingface.co/datasets/allenai/tulu-v2-sft-mixture\n\u2022 WMT-09: published with the WMT workshop. URL: https://www.statmt.org/wmt09/translation-task.html\n\u2022 TriviaQA: Apache License 2.0. URL: https://nlp.cs.washington.edu/triviaqa/\n\u2022 MMLU: MIT license. URL: https://github.com/hendrycks/test"}, {"title": "E. n-gram Pair Examples", "content": "In this section, we present some representative examplescollected from the analysis for the different tasks evaluating,including Translation and Question-Answering (MMLU,TriviaQA). In order to show examples from the differentexperiments, we show examples with different model sizesand number of n-grams."}]}