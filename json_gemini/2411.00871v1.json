{"title": "LLaMo: Large Language Model-based Molecular Graph Assistant", "authors": ["Jinyoung Park", "Minseong Bae", "Dohwan Ko", "Hyunwoo J. Kim"], "abstract": "Large Language Models (LLMs) have demonstrated remarkable generalization and\ninstruction-following capabilities with instruction tuning. The advancements in\nLLMs and instruction tuning have led to the development of Large Vision-Language\nModels (LVLMs). However, the competency of the LLMs and instruction tuning\nhave been less explored in the molecular domain. Thus, we propose LLaMo:\nLarge Language Model-based Molecular graph assistant, which is an end-to-\nend trained large molecular graph-language model. To bridge the discrepancy\nbetween the language and graph modalities, we present the multi-level graph\nprojector that transforms graph representations into graph tokens by abstracting\nthe output representations of each GNN layer and motif representations with the\ncross-attention mechanism. We also introduce machine-generated molecular graph\ninstruction data to instruction-tune the large molecular graph-language model for\ngeneral-purpose molecule and language understanding. Our extensive experiments\ndemonstrate that LLaMo shows the best performance on diverse tasks, such as\nmolecular description generation, property prediction, and IUPAC name prediction.\nThe code of LLaMo is available at https://github.com/mlvlab/LLaMo.", "sections": [{"title": "1 Introduction", "content": "In recent years, molecular machine learning [1, 2, 3, 4] has received significant attention, addressing\ndiverse tasks in the chemical domain. The predominant approach for molecular tasks is graph machine\nlearning [5, 6, 7] that leverages the molecular graph structure, which is a natural and expressive\nrepresentation of molecules. Although graph-based methods have successfully represented molecules,\nthey have limited interpretability and incompatibility to solve multi-modal molecular tasks dealing\nwith pairs of texts and molecules. To address these issues, recent works [4, 8] train both a language\nmodel and a graph encoder with cross-modal contrastive learning. However, the models trained with\ncross-modal contrastive learning are insufficient to perform open-ended molecule-to-text generation\ntasks [3], which are more applicable to practical use.\nLarge Language Models (LLMs) [2, 9, 10, 11] have shown impressive progress and accomplished\nhuman-like open-ended text generation with the power of billions of parameters. To leverage the\ninstruction-following capability of LLMs, many works employ instruction-tuning approaches [12, 13,\n14] for general-purpose language models. Motivated by the development of LLMs and instruction\ntuning, Large Vision-Language Models (LVLMs) have recently been explored and achieved success\non image comprehension and image-to-text generation tasks [10, 11, 15, 16, 17, 18]. Despite the\nsuccess of LLM-based approaches on natural language processing and machine vision domains,\nthe research on the integration of language models and molecular graphs has been less studied\ndue to the lack of consideration of the architecture design of Large Molecular Graph-Language\nModels (LMGLMs) and the molecular graph instruction data."}, {"title": "2 Related works", "content": "Molecular graph modeling. Molecular graphs serve as a natural and expressive representation of\nmolecules, effectively capturing the structural information. Graph neural networks [19, 20, 21, 22] are\ncommonly utilized architectures for molecular graph representations. To learn graph neural networks\nwith the limited molecular graph data [23], self-supervised learning has been explored. For example,\nvarious approaches [24, 25] have been developed to capture multi-level features of molecular graphs,\nsuch as node-level masked atom modeling [24], motif-based self-supervised learning [25, 26], and\ngraph-level contrastive learning [27, 28]. With the advance of multi-modal large language models,\nmolecule-language tasks such as molecule-text retrieval [8] or molecule captioning [29] have recently\ndrawn significant attention. Recent works [3, 4, 30] have attempted to enable language models to\nunderstand molecular graphs. [30] treated nodes of molecular graphs as tokens of language models.\nSome works have adopted GNN-based encoders, either by propagating their outputs to language\nmodels through MLP [4] or employing cross-modal projectors [3]. However, these methods fail\nto consider molecular graphs at multiple levels and are hindered by inherent limitations of graph\nencoders, such as the over-smoothing problem [31]. To address these challenges, we propose a novel\narchitecture, LLaMo, which effectively propagates multi-level information of molecular graphs to\nlanguage models.\nInstruction tuning. Recent advancements of LLMs lead to extensive research on instruction tuning,\naimed at improving the model's capability to follow human instructions [12, 32, 33, 34, 35]. \u03a4\u03bf\nconstruct high-quality instruction tuning data, a line of previous approaches [34, 35] has adopted\nexisting human-annotated datasets and integrated them with a new structure and template. On the\nother hand, recent studies [12, 36, 37] on instruction tuning have collected data samples from strong\nLLMs like GPT-4 [10]. These works first manually construct annotated seed instruction samples and\nexpand them by prompting LLMs. As a result, several instruction-tuned LLMs [14, 16, 37] have\nbeen proposed from the open-source LLMs, e.g., LLaMA [9] and shown generalizability across a\nwide range of instructions. More recently, those studies on instruction tuning have been expanded\nto visual instruction tuning in image [15, 17, 38] and video [39, 40] domain to enable the model to\nunderstand the visual contents. Inspired by the instruction tuning for multi-modal LLMs in other\ndomains, in this work, we study instruction tuning specifically for molecule graphs, which has been\nunderexplored in the literature."}, {"title": "3 LLaMo: Large Language Model-based Molecular Graph Assistant", "content": "The primary goal is to seamlessly integrate a molecular graph encoder and a Large Language\nModel (LLM) to generate instruction-following responses to the input texts and molecules. To\nachieve it, we propose LLaMo: Large Language Model-based Molecular graph assistant, a general-\npurpose Large Molecular Graph-Language Model (LMGLM) equipped with a multi-level graph\nprojector. Specifically, the proposed framework utilizes three input modalities: 1D SMILES [41], 2D\nmolecular graph, and text (instruction). SMILES [41] is a 1D representation of a molecule, and a 2D\nmolecular graph is processed by a GNN. The three input modalities are fed as a sequence of tokens\nand our LLaMo autoregressively generates text responses. Formally, given SMILES $S$, molecular\ngraph tokens $G$, and text (instruction) $T$, the proposed method renders the response $Y = \\{y_i\\}_{i=1}^K$ as:\n$$p(Y|S, G, T) = \\prod_{i=1}^K p (y_i|S, G, T, y_{<i}),$$", "Where": "y<i indicates generated token sequences until i-th token."}, {"title": "3.1 Model Architecture", "content": "The overall architecture of LLaMo is illustrated in Figure 1. LLaMo consists of a graph encoder, a\nmulti-level graph projector, and a backbone large language model. The graph encoder $g(\u00b7)$ takes a\n2D molecule graph as an input and outputs their node representations as a sequence of tokens. The\nmulti-level graph projector $Proj_{MG}(\u00b7)$ transforms the sequence of node representations into molecular\ntokens to align them with the LLM. Then, the LLM $f(\u00b7)$ processes molecular and text tokens and\nprovides a response in an autoregressive manner.\nGraph encoder. We adopt Graph Neural Networks (GNNs) as a molecular graph encoder. Given\nthe graph $G$, graph neural networks $g (\u00b7)$ iteratively update node representation $z_v^{(l)} \\in \\mathbb{R}^{d(l)}$ via the\nmessage-passing framework. With the message-passing, L-layer GNN provides node representations\n$z_v^{(L)}$ that express an L-hop ego-graph given the node $v$ as a center node. More details about graph\nneural networks are in the Appendix C."}, {"title": "Multi-level graph projector", "content": "The goal of a multi-level graph projector is to align the graph encoder\nwith the LLM by transforming a set of node representations $Z_{graph}$ into a sequence of molecular graph\ntokens $H_{graph}$. It enables the language model to utilize graph information. In the literature, projectors\nhave been proposed mainly for Large Vision-Language Models (LVLMs) [17, 18, 38, 42, 43]. They\nare usually implemented using a linear projection [38] or an abstraction of visual features [17, 42],\nwhich are outputs of the final layer of a visual encoder given input image. Analogously, we can design\nthe projector for large molecular graph-language models with a linear projection or an abstraction of\nhigh-level node representations from the pre-trained graph encoder, which is formulated as:\n$$H_{graph} = Proj (Z_{graph}), Z_{graph}^{(L)}), where Z_{graph} = g (G),$$\nwhere $Z_{graph}^{(L)} = [z_0^{(L)},..., z_v^{(L)}]$ \\in \\mathbb{R}^{|V|\\times d(L)} is the concatenation of node representation $z_v^{(L)} \\in \\mathbb{R}^{d(L)}$ from L-th layer GNN and $Proj (\u00b7)$ is the projector.\nHowever, we observe that the high-level representation is not effective in capturing the local infor-\nmation due to the over-smoothing problem [31], which means that the node representations become\nindistinguishable, as the number of layers in the GNN increases. Figure 2 depicts node represen-\ntations (yellow dots) of graph encoder with 1,2,4,5 layers on one molecular graph sample. (More\nsamples are in Appendix I.) As mentioned above, node representations become over-smoothed as\nthe number of layers increases, leading to nearly identical node representations in the final layer.\nConsequently, conventional projectors relying on high-level node representations have a limited\ncapability to preserve the detailed or local information of molecular graphs. Moreover, many tasks\nrequire multi-scale information, including atom, atomic group, and molecule levels. Hence, the\nprojector that solely utilizes features from the top layer is suboptimal for the tasks.\nMotivated by the observations, we propose a novel multi-level graph projector to generate graph\ntokens that contain richer information reflecting the graph structure at multiple levels. The multi-level\ngraph projector $Proj_{MG} (\u00b7)$ is formulated as\n$$H_{graph} = Proj_{MG} (\\{Z_{graph}^{(l)}\\}_{l=0}^L), where \\{Z_{graph}^{(l)}\\}_{l=0}^L = g (G).$$"}, {"title": null, "content": "The method captures multi-hop graph information by leveraging node representations from all layers\nof a GNN. To handle an arbitrary number of nodes, yielding a variable length $|V| \\times L$ features, we\nadopt the cross-attention with learnable tokens $P^{(l)} = [p_1^{(l)},...,P_b^{(l)}] \\in \\mathbb{R}^{b \\times d}$ for $l = 0, ..., L$,\nwhere $b$ is the number of learnable prompts. Here, $[\u00b7, \u00b7]$ indicates the concatenation operation. The\nlearnable tokens aggregate l-th layer GNN representations into a fixed number of tokens as:\n$$P^{(l)} = Attn(\\mathbb{Z}^{(l)} (P^{(l)}, Z_{graph}^{(l)}, Z_{graph}^{(l)}) \\in \\mathbb{R}^{b \\times d}$$\nwhere $Attn (Q, K, V)$ is the attention operation with query $Q$, key $K$, and value $V$.\nFor more detailed representations of the input molecule, LLaMo also has learnable tokens $P^{(motif)}$\nfor motif-level representations. We use the functional groups as motifs, which are the statistically\nimportant subgraphs in the molecular graphs. To construct functional group representations $Z_{FG}$, We\ninitially identify functional groups, following [23]. Then, we vectorize the main characteristics of each\nfunctional group, which is represented as $Z_{FG,i}$. Finally, the functional group representations $Z_{FG}$"}, {"title": "3.2 Training LLaMo", "content": "Similar to most LVLMs [18, 38, 42], we train LLaMo in the two-stage pipeline: (1) pre-training for\nmolecular graph-language alignment and (2) instruction-tuning end-to-end as in Figure 3.\nStage 1. Pre-training for molecular graph-language alignment. The first stage focuses on the\nalignment between the graph encoder and a large language model by learning our multi-level graph\nprojector. In this stage, with the LLM frozen, we train the multi-level graph projector and the\ngraph encoder by generating molecule descriptions. For training, we use a molecule-description pair\ndataset (e.g., PubChem [44]) consisting of a 1D SMILES representation of molecule and molecular\ngraph and its corresponding description.\nStage 2. Instruction-tuning end-to-end. In the second stage, we train the LLM to enhance the\ninstruction-following capabilities and enable a deeper understanding of molecular graphs. In this\nstage, we freeze the graph encoder and train both the multi-level graph projector and the LLM. Since it\nis too expensive to train the full LLM, we employ LoRA [45] to adapt LLM to the data. For instruction-\nfollowing, we use the GPT-generated instruction-following multi-turn conversation dataset, which\nwill be introduced in Section 4. In addition to our generated instruction-following dataset, we use a\ndiverse set of datasets with various instructions: molecule description generation, molecular property\nprediction, IUPAC name generation, forward reaction prediction, and retrosynthesis datasets."}, {"title": "4 GPT-assisted Molecular Graph Instruction Data Generation", "content": "Instruction data are essential for improving the instruction-following capabilities of LLM-based\nmodels. Despite active research on instruction-tuning, the instruction-following data for molecular\ngraphs have been less explored in the literature since annotations require expertise. To alleviate the"}, {"title": "5 Experiments", "content": "5.1 Experimental Settings\nBenchmarks. To evaluate the efficacy of the proposed method, we evaluate the model for three tasks\nsuch as 1) molecule description generation, 2) IUPAC name prediction, 3) property prediction (re-\ngression). We conducted experiments under two major settings: generalist and specialist models. In\nthe generalist setting, one model handles all three tasks, whereas in the specialist setting, we train a\nmodel for each downstream task. More details about benchmarks are in Appendix G.\nImplementation details. For the generalist models, we train our LLaMo based on\nLlama-2-7b-chat [9] for a fair comparison with Mol-Instructions [48]. For the specialist models,\nwe train our LLaMo with Galactica 1.3B [2] for a fair comparison with MolCA [3]. To train the\ngeneralist variant of LLaMo, we use a training split of molecular description generation dataset of\nMol-Instruction [48] in stage 1. In stage 2, the model is instruction-tuned with a training split of\ndescription generation, property prediction, forward reaction, and retrosynthesis instruction dataset of\nMol-Instruction [48], IUPAC name prediction from [3], and our GPT-generated instruction-following\ndata. To train the specialist variant of LLaMo, we follow MolCA [3] to train the model with a\npretraining split of PubChem324kV2 in the stage 1 phase and fine-tune the model for each specific\ndownstream task in the stage 2. We adopt a long training schedule (epoch 1 pre-training, epoch\n3 instruction tuning) for the final models. For analysis, we use a short training schedule (epoch 1\npre-training, epoch 1 instruction tuning). For further implementation details, refer to Appendix E.1.\nBaselines. For the generalist models, we compare our LLaMo with (1) LLM-based generalist\nmodels including Galactica [2], LLaMA2-7B [9], GPT-3.5, and GPT-4, (2) Molecule-specialized\nLLM such as Text+Chem T5 [49], and (3) Molecule instruction-tuned generalist model such as Mol-\nInstructions [48]. Since GPT-3.5 and GPT-4 have difficulty in solving the tasks without in-context\nlearning, we additionally measure the performance of GPT-3.5 and GPT-4 with 4-shot in-context\nlearning, which are GPT-3.5 (ICL) and GPT-4 (ICL). For the specialist models, we use single-task\nspecialist molecule-language models as baselines, including MolT5 [29], MoMu [4], and MolCA [3]."}, {"title": "5.2 Experimental Results", "content": "Generalist models. We provide the experimental results of generalist models in molecular description\ngeneration, IUPAC name generation, and property prediction tasks. Our LLaMo is built on LLaMA-\n7B and it is fine-tuned by our instruction-tuning method. Table 2 shows that our LLaMo achieves the\nbest performance in all three tasks. In comparison to GPT-4 (ICL), which is GPT-4 with in-context-\nlearning, LLaMo shows a performance improvement of 11.9 in BLEU-4 and 14.9 in METEOR for\nmolecular description generation. Furthermore, LLaMo outperforms Mol-Instructions, an instruction-\ntuned model with molecular data, by a substantial performance gain of 41.7 in METEOR for molecular\ndescription generation and a 0.007 performance gain in MAE on the property prediction task. More\nexperimental results on forward reaction prediction and retrosynthesis are in Appendix D."}, {"title": "5.3 Analysis", "content": "Impact of multi-level graph projector. To validate the effectiveness of our multi-level graph\nprojector, we compare the performance of the multi-level graph projectors (denoted by MGProj)\nwith other projectors in Table 4, including two widely-used projectors such as MLPs and resamplers.\nAdditionally, we measure the performance of the base model without a graph (and a projector) denoted\nas w/o Graph for the ablation study. MLP (w/ low-level) and MLP (w/ high-level) denote the MLP\nprojectors where the input is low-level representation $Z_{graph}^{(1)}$ and high-level representation $Z_{graph}^{(L)}$\nrespectively. MLP (w/ concat) indicates the MLP projector with the concatenated representations\nof all GNN layers as an input. Resampler denotes the cross-attention based resampler projector\ndesigned in Qwen-VL [50]. MGProj (w/o motif) and MGProj are our multi-level graph projector\nwithout and with motif tokens (motif).\nTable 4 shows that our multi-level graph projector (MGProj) achieves the best performance across\nall three tasks. Specifically, the multi-level graph projector achieves 49.6 BLEU and 70.9 METEOR\nscores with a significant improvement compared to MLP projectors in the IUPAC prediction task.\nThese experimental results demonstrate that our multi-level graph projector is more effective than\nconventional projectors by capturing multi-scale information, including atom, atomic group, and\nmolecule-level information."}, {"title": "6 Conclusion", "content": "We propose LLaMo: Large Language Model-based Molecular graph assistant, an end-to-end trained\nlarge molecular graph-language model, to perform various molecule-related tasks with a single\nmodel. For the projector, we newly introduce a multi-level graph projector, which addresses the\nover-smoothing problem of the graph encoder and captures multi-hop graph information. We also\npresent machine-generated instruction-following data in the form of multi-turn conversations to\nimprove the instruction-following capabilities of the large language model."}, {"title": "A Limitations", "content": "Our LLaMo is built upon an LLM, e.g., LLaMA and Galactica, and is fine-tuned on molecule\nbenchmark datasets by leveraging its pretrained knowledge. Given that LLMs are pretrained using\nextensive web-crawled corpora, it is uncertain whether the data used for LLMs' pretraining and the\ntest samples in molecule benchmark datasets are mutually exclusive. This results in implicit data\nleakage when fine-tuning and evaluating LLaMo on molecule benchmark datasets. Furthermore,\nLLMs inherently require large memory and computational costs and cause hallucination problems\nwhere the model generates incorrect but plausible text. Our LLaMo may inherit these LLMs' problems\ndue to LLMs' powerful pre-trained knowledge."}, {"title": "B Broader Impacts", "content": "We proposed the first molecular graph-based general-purpose model, LLaMo, which is widely\napplicable to various molecule tasks such as molecule captioning, property prediction, and IUPAC\nnaming. Our LLaMo itself does not have negative societal impacts. However, as discussed above,\nsince our model is built upon an LLM, the model sometimes generates biased output concerning race,\nreligion, culture, and gender, resulting in the misusage of our model. Also, training LLMs requires\nmassive amounts of CO2 emission promoting global warming."}, {"title": "C Explanation on Graph Neural Networks", "content": "Let $G = (V, E, X)$ denote the input graph, where $V, E$ are a set of nodes and edges, respectively, and\n$X$ indicates a set of input node features. The input feature of node $v \\in V$ is defined as $x_v^{(0)}$ and the\nedge between node $u$ and $v$ is represented with $(u, v) \\in V \\times V$. The neighbor set of node $v$ on the\ninput graph is denoted by $N_v = \\{u| (u, v) \\in E\\}$. Given the graph, graph neural networks iteratively\nupdate node representation $z_v^{(l)} \\in \\mathbb{R}^{d(l)}$ via the following message-passing framework:\n$$z_v^{(l)} = UPDATE^{(l)} (z_v^{(l-1)}, AGGREGATE^{(l)} (\\{z_u^{(l-1)} : u \\in N_v\\})), l = 1, ... L$$\nwhere $z_u^{(0)} = x_v^{(0)}$, in that the node representation of 0-th layer is input node features. $AGGREGATE^{(l)}(\u00b7)$\nfunction aggregates the representations of the neighbor set with a particular function. $UPDATE^{(l)}(\u00b7)$\nfunction is designed to update a node representation $z_v^{(l-1)}$ with the aggregated information produced\nby $AGGREGATE^{(l)}(\u00b7)$. With the message-passing, L-layer GNN provides node representations $z_v^{(L)}$\nthat express an L-hop egograph given the node $v$ as a center node. In this paper, we use a pre-trained"}, {"title": "E Detailed Experimental Settings", "content": "E.1 Implementation Details\nOur code is implemented based on PyTorch [55] library. Also, we adopt PyTorch Geometric\n(PyG) [56], and Huggingface transformers [57] to utilize the graph architectures and Large Language\nModels (LLMs). PEFT [58] and OpenDelta [59] libraries are used for parameter-efficient fine-tuning\nof LLMs, i.e., LoRA. We use LLaMA 2 chat 7B model [9] and Galactica 1.3B [2] as our base\nlanguage model. We leverage GIN [21] with five layers initialized based on the MoleculeSTM graph\nencoder [60], which is pre-trained with text-graph contrastive learning [61]. We use LoRA to train\nthe large language model in stage 2. We use OGB [62], a smiles2graph function, to convert SMILES\nrepresentations to 2D graphs. Our experiments are run on 4 \\times A6000 GPUs or 4 \\times V100 GPUs and\n2 \\times A6000 GPUs for LLaMA2 and Galactica, respectively. In stage 1, the AdamW [63] optimizer is\nadapted with an initial learning rate of 1e-4 (minimum learning rate is 1e-5 and warmup learning rate"}, {"title": "E.2 Metrics", "content": "We report BLEU [65] and METEOR [66] for the molecule description generation and IUPAC name\nprediction tasks. MAE is reported for property QA.\nBLEU. The BLEU metric measures the quality of generated text by comparing n-gram sequence\nbetween the generated text and the reference text, which can be formulated as:\n$$BLEU = BP \\times exp(\\sum_{n=1}^N \\frac{1}{N} log p_n),$$\nwhere $N$ is the number of n-grams and $p_n$ is the precision, i.e., the ratio of the number of n-grams in\nthe generated text appearing in the reference text. The BLEU score also takes into account sequence\nlength with Brevity Penalty (BP) as:\n$$BP = \\begin{cases}\n1 & if c > r \\\\\ne^{(1-r/c)} & if c \\le r\n\\end{cases}$$\nwhere $c$ and $r$ are the lengths of generated and reference texts, respectively. This encourages the\nmodel to avoid generating short sequences. In our experiments, we use BLEU-4 as the default BLEU\nmetric.\nMETEOR. The METEOR metric is proposed to consider both precision and recall between the\ngenerated text and the reference text, which is as follows:\n$$P = \\frac{number\\ of\\ matched\\ words}{number\\ of\\ words\\ in\\ generated\\ text},$$\n$$R = \\frac{number\\ of\\ matched\\ words}{number\\ of\\ words\\ in\\ reference\\ text},$$\n$$F = \\frac{10PR}{9P + R},$$\n$$Penalty = 0.5 \\cdot (\\frac{number\\ of\\ chunks}{number\\ of\\ matched\\ words}),$$\n$$METEOR = F \\cdot (1 \u2013 Penalty),$$"}]}