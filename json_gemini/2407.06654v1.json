{"title": "SoftDedup: an Efficient Data Reweighting Method for Speeding Up Language Model Pre-training", "authors": ["Nan He", "Weichen Xiong", "Hanwen Liu", "Yi Liao", "Lei Ding", "Kai Zhang", "Guohua Tang", "Xiao Han", "Wei Yang"], "abstract": "The effectiveness of large language models (LLMs) is often hindered by duplicated data in their extensive pre-training datasets. Current approaches primarily focus on detecting and removing duplicates, which risks the loss of valuable information and neglects the varying degrees of duplication. To address this, we propose a soft deduplication method that maintains dataset integrity while selectively reducing the sampling weight of data with high commonness. Central to our approach is the concept of \"data commonness\", a metric we introduce to quantify the degree of duplication by measuring the occurrence probabilities of samples using an n-gram model. Empirical analysis shows that this method significantly improves training efficiency, achieving comparable perplexity scores with at least a 26% reduction in required training steps. Additionally, it enhances average few-shot downstream accuracy by 1.77% when trained for an equivalent duration. Importantly, this approach consistently improves performance, even on rigorously deduplicated datasets, indicating its potential to complement existing methods and become a standard pre-training process for LLMs.", "sections": [{"title": "1 Introduction", "content": "In recent years, the expansion of pre-training datasets has played a pivotal role in advancing LLMs (Raffel et al., 2023; Gao et al., 2020; Penedo et al., 2023). However, a large fraction of these datasets is derived from uncurated snapshots of the internet, resulting in a significant amount of duplication. Such redundancy, particularly beyond certain levels, can severely impair the performance of LLMs (Hernandez et al., 2022). While repetition under specific conditions may be beneficial, the marginal gains from additional computation diminish to zero over time (Muennighoff et al., 2023). Thus, it is imperative to ensure that data repetition is a deliberate choice rather than an unintentional consequence. In light of this, data deduplication has emerged as a critical procedure in the management of pre-training datasets.\nMost current data deduplication strategies can be classified as hard deduplication methods, focusing on identifying and removing redundant data. For example, MinHashLSH (Leskovec et al., 2020), a widely utilized method (Soboleva et al., 2023; Penedo et al., 2023), approximates Jaccard similarity among samples by generating MinHash (Broder, 1997) signatures and using locality sensitive hashing to map these signatures into multiple buckets. Samples are considered duplicates if their MinHash values exactly match in at least one bucket, indicating they exceed a predefined similarity threshold. In the subsequent removal stage, a common practice involves clustering samples across all buckets (for instance, if samples A and B match in one bucket, and B and C in another, then A, B, and C are considered a cluster) (Penedo et al., 2023). Within each cluster, only one sample is preserved.\nThese methods face several principal limitations. First, the concept of duplicates within a set of samples is symmetric. Randomly retaining one sample while discarding the others may introduce bias by ignoring the differences among them. Second, setting a specific threshold for duplication presents a challenge since the degree of duplication is continuous. A high threshold might overlook near-duplicates that bear significant similarities, whereas a low threshold could result in the exclusion of valuable data. Moreover, data categorized as non-duplicates according to these thresholds are uniformly treated, despite the variations in the degree of duplication among them.\nTo address these limitations, we introduce a soft deduplication method (Figure 1). This method diverges from traditional practices by preserving the entirety of the dataset and avoids the need for setting thresholds to determine duplicates. We introduce the concept of \"data commonness\", a metric that quantifies the degree of duplication by measuring the occurrence probabilities of samples using an n-gram model. Samples with high commonness are assigned a lower sampling weight, while those with low commonness receive a higher weight. This method reduces the risk of inadvertently discarding valuable data and leverages the spectrum of data duplication, offering a refined and comprehensive perspective on data deduplication.\nOur empirical analysis reveals that the proposed method enables language models to achieve baseline performance with at least 26% fewer training steps, ultimately leading to improved performance on downstream tasks. It exhibits superior temporal efficiency and outperforms existing methods in terms of effectiveness. Significantly, even when applied to rigorously deduplicated datasets, our method still delivers substantial improvements. These results suggest that our approach can complement existing methods and can be adopted as a standard procedure in the pre-training of LLMs."}, {"title": "2 Related Work", "content": "2.1 Data deduplication\nResearch has revealed that many existing pre-training datasets contain a substantial number of duplicate samples (Lopes et al., 2017; Bandy and Vincent, 2021; Penedo et al., 2023). To explore the impact of duplicate data on model performance, numerous studies have been conducted on both general and domain-specific datasets (Allamanis, 2019; Lee et al., 2022; Biderman et al., 2023; Xue et al., 2023). The results indicate that repetition at certain frequencies can significantly harm model performance (Hernandez et al., 2022). Although appropriate repetition under specific circumstances can be beneficial (Muennighoff et al., 2023), this should result from careful selection rather than being an unintended consequence of data duplication.\nTherefore, data deduplication is crucial for pre-training large language models. Exact deduplication is typically achieved through suffix arrays (Manber and Myers, 1993). MinHash (Broder, 1997) and SimHash (Charikar, 2002) are widely used fuzzy deduplication methods. In recent research, some studies have shifted towards semantic-based deduplication. Abbas et al. (2023) and Sorscher et al. (2023) utilize pre-trained embeddings for clustering to remove semantically redundant data. Tirumala et al. (2023) combines both methods.\n2.2 Data reweighting\nAdjusting the significance of training samples through data reweighting has proven to be an effective strategy for enhancing model performance, either through modifying loss function weights or changing the sampling probabilities. Focal Loss, as introduced by Lin et al. (2018), employs a soft weighting scheme to allocate higher weights to more challenging samples. Ren et al. (2019) assign weights to training samples based on the direction of their gradients. In DSIR (Xie et al., 2023b), sampling based on importance weights is utilized, allowing the training data to align with the distribution of high-quality corpora such as Wikipedia. DoReMi (Xie et al., 2023a) explores an automated scheme for determining the sampling weights of different data sources."}, {"title": "3 Method", "content": "3.1 Hard deduplication\nHard deduplication methods identify and remove duplicate samples. This process can be seen as partitioning the dataset D into numerous distinct subsets \\(D_i\\), such that \\(D = \\bigcup_{i=1}^{k} D_i\\). Each of these subsets contains samples deemed to be duplicates based on a specific similarity threshold. Within each subset \\(D_i\\), only one sample, denoted as \\(x_i\\), is retained, while the rest are discarded. If a subset consists of only one sample, it indicates that this sample has no duplicates within the dataset.\nIn the context of pre-training LLMs, the fundamental training goal is to maximize the log likelihood of the training data. Incorporating hard deduplication into this process can be formulated as:\n\\[\nL = \\sum_{x \\in D} I(x) \\log P(x|\\Theta),\n\\]\n\\[\nI(x) =\\begin{cases}\n1, & x \\in \\{X_1, X_2, \\dots, X_k\\}\\\\\n0, & \\text{otherwise}\n\\end{cases}\n\\tag{1}\n\\]\nwhere \\(L\\) denotes the log likelihood function, \\(\\Theta\\) represents the model parameters. Despite its utility, hard deduplication may inadvertently omit valuable data and fail to adequately consider the degree of redundancy.\n3.2 Soft deduplication\nTo address the limitations of hard deduplication, we propose a soft deduplication method. This method employs sampling weights \\(W(x)\\), allowing for a nuanced handling of data redundancy by adjusting the influence of each sample on the model based on its commonness:\n\\[\nL = \\sum_{x \\in D} W(x) \\cdot \\log P(x|\\Theta), \\quad W(x) \\in (0, 1). \\tag{2}\n\\]\nWe assume that the sampling weight of sample x can be represented as follows:\n\\[\nW(x) \\propto \\frac{1}{p(x)} \\tag{3}\n\\]\nHere, \\(p(x)\\) denotes the occurrence probability of sample x, serving as a direct measure of its commonness. This probability-based measure effectively captures the degree of duplication of each sample. This approach ensures that samples with higher commonness are assigned lower weights, thus mitigating the impact of duplicates without discarding potentially valuable information."}, {"title": "3.3 Implementation of commonness calculation", "content": "In practical implementation, we leverage an n-gram model to process data, achieving high temporal efficiency in calculating the commonness of each data sample (Figure 2). This process consists of three steps.\n1. Tokenization. The n-gram model assumes that the appearance of a word is determined by the previous n \u2212 1 words. The first step is to tokenize the original corpus. We use the same tokenizer as the pre-training models for consistency.\n2. Train n-gram model. In the training process of an n-gram model (where n = 4), maximum likelihood estimation is used to calculate the probability of each n-gram. We empirically choose n = 4 after conducting early experiments. To alleviate the issue of data sparsity, we employ the Kneser-Ney smoothing technique (Ney et al., 1994). We utilize the KenLM toolkit* to accomplish this step.\n3. Calculate commonness. We utilize the obtained n-gram model to compute the commonness (measured by the occurrence probability) for each data sample. For a given \\(x\\) containing N tokens,\n\\[\np(x) = \\prod_{i=1}^{N} P(W_i|W_{i-1},..., W_{i-n+1})  \\tag{4}\n\\]\nBy employing the geometric mean, the influence of sample length can be eliminated.\n3.4 Approximate sampling for large-scale data\nDue to the vast volume of data, directly assigning individual sampling weights to each data point is impractical. To overcome this, we introduce an approximate method for data sampling that segments M samples into K categories. This process initiates by sorting the M samples in ascending order of data commonness, followed by dividing the dataset into K distinct segments according to K quantiles. For the k-th segment, the sampling weight \\(W_k\\) is determined by the k-th quantile, \\(p_k\\), as follows:\n\\[\nW_k = C \\cdot  \\left( \\frac{T}{p_k} \\right) \\tag{5}\n\\]\nwhere T is a hyperparameter that adjusts the sampling weight and C is a normalization constant, which ensures that the sum of the weights across all segments equals one."}, {"title": "4 Experimental Setup", "content": "4.1 Datasets\nWe conduct experiments on different versions of the Common Crawl dataset, which is a comprehensive and publicly accessible collection of data obtained through web crawling.\nRedPajama CommonCrawl is a subset of the RedPajama dataset (Computer, 2023). It involves the original Common Crawl data undergoing processing through the CCNet pipeline (Wenzek et al., 2019). This dataset has been subjected to paragraph-level deduplication; however, it has not undergone rigorous deduplication procedures.\nSlimPajama CommonCrawl is a subset of the SlimPajama dataset (Soboleva et al., 2023). The SlimPajama dataset represents a further refined iteration of the RedPajama corpus, boasting enhanced data cleansing procedures and the implementation of MinHashLSH (Leskovec et al., 2020) for more effective deduplication.\nFalcon RefinedWeb is introduced as a pre-training dataset for the Falcon series (Penedo et al., 2023; Almazrouei et al., 2023). It undergoes rigorous deduplication processes using exact matching and MinHashLSH.\n4.2 Model training\nIn the experiments, we employ the same model architecture as the LLaMA (Touvron et al., 2023) series. Our models are configured with 1.3B parameters, incorporating 16 attention heads and 24 layers. The hidden size is set to 2048, and the dimension of feed-forward network is 5504. Previous research has demonstrated the feasibility of pre-training validation on models of this scale (Tirumala et al., 2023; Xie et al., 2023a). All models are trained from scratch to 40B tokens. The batch size is 512, and the training sequence length is 1024. The learning rate is decayed from 2e-4 to 2e-5.\n4.3 Baselines\nOur primary baseline is defined by directly training on a dataset that has been randomly sampled to encompass 40B tokens. In our study, we implement the SoftDedup method across all three datasets,"}, {"title": "5 Results", "content": "In this section, we provide a detailed report of the results under various experimental settings.\n5.1 Enhanced performance and efficiency in language model pre-training\nTo verify the effectiveness of our soft deduplication method, we conduct experiments on the RedPajama CommonCrawl dataset, which has not subjected to meticulous deduplication. Our findings indicate a significant improvement with our method compared to the direct training baseline, as illustrated in Figure 3.\nOur approach consistently outperforms the baseline in terms of average perplexity across all evaluated datasets. Specifically, on the Pile test set, our method enables models to achieve baseline perplexity within 50,000 iterations, saving nearly 30,000 training steps. Furthermore, models continue to improve, ultimately reaching a lower perplexity, as shown in Figure 3a. Similar advancements are observed in the SlimPajama test set, confirming our method's effectiveness (Figure 3b). Additionally, we report the average perplexity for each subset upon completion of training (Appendices A.1 and A.2). Our method enables the models to yield performance improvements across the majority of the test subsets.\nIn our evaluation of downstream tasks, our method outperforms the baseline in accuracy. It accelerates learning on the RedPajama dataset, achieving baseline performance nearly 30,000 steps sooner and improving average accuracy by 1.77% at the end of training, as shown in Figure 3c. Detailed scores for each individual task at the final training checkpoint are delineated in Table 1. Our approach yields improvements in all evaluated tasks.\nIn summary, our experiments on the RedPajama Common Crawl dataset substantiate that the soft deduplication method is capable of reducing perplexity and enhancing the accuracy of downstream tasks more efficiently compared to the baseline model. Such accelerated convergence is crucial for pre-training large language models, considering the significant costs associated with training duration and resource utilization.\n5.2 Surpassing hard deduplication in effectiveness\nIn the experiments carried out using the RedPajama CommonCrawl dataset, we also contrast the SoftDedup method against traditional hard deduplication techniques (refer to Table 1). Considering that the SlimPajama dataset originates from the RedPajama dataset, refined through MinHashLSH deduplication, we employ models trained on the SlimPajama CommonCrawl dataset as the hard deduplication baseline.\nThe evaluation results of models on various downstream tasks reveal our method's superior performance over both the hard deduplication technique and the direct training baseline. In detail, while the hard deduplication method surpasses the direct training baseline in nine out of twelve tasks, showing an average increase in accuracy of 0.6%, our SoftDedup method demonstrates more consistent and significant improvements. It outperforms the direct training baseline across all evaluated tasks, achieving an average accuracy enhancement of 1.77%. These findings underscore the advantages over conventional deduplication methods in enhancing downstream task performance.\n5.3 A powerful complement to existing techniques\nTo further assess the effectiveness of our method when applied in sequence with extant hard deduplication processes, we conduct experiments on the SlimPajama CommonCrawl and Falcon RefinedWeb datasets, which have undergone stringent deduplication processes (as shown in Figures 4 and 5).\nIn the evaluations conducted on the Pile and SlimPajama test sets, our method exhibits consistent superiority over the baseline models. Notably, our models achieve equivalent baselines in perplexity with a reduction of 26% to 39% in the number of required training steps. Additionally, the ultimate performance of the models demonstrates a tangible enhancement, as evidenced by the results displayed in Figures 4a, 4b, 5a, and 5b. In terms of accuracy on downstream tasks, Figures 4c and 5c highlight the training efficiency achieved by our models. It is particularly noteworthy that our method reaches baseline performance with around 20,000 fewer training steps. We report the detailed scores in Appendices A.1, A.2, and A.3.\nIn summary, even when applied to already deduplicated datasets, our method significantly enhances training efficiency and effectiveness. This underscores its capability to address the shortcomings of current deduplication techniques. Specifically, our approach reweights the data to reflect varying levels of duplication, thus avoiding one-size-fits-all solutions. This integration has the potential to become a standard practice in the pre-training of large language models.\n5.4 Finer data partitioning for improved downstream task performance\nIn Figure 6, we illustrate the impact of different numbers of data partitions on model performance. We argue that investigating methods to further enhance the training effectiveness of higher-quality data is a more critical concern. Therefore, our experiments are conducted on the Falcon RefinedWeb dataset.\nIn evaluations conducted on both the Pile and SlimPajama test sets, models exhibit negligible variations in average perplexity across a range of data partition counts, specifically 10, 20, 50, and 100. This observation indicates that perplexity, as a metric, demonstrates relatively low sensitivity to changes in the granularity of data partitioning.\nIn contrast, we observe that as the granularity of data partitioning increases, the accuracy of the language model in downstream tasks also improves. As demonstrated in Figure 6c, there is a clear correlation between the number of data partitions and the model's accuracy. This indicates that finer-grained data partitioning can make the training data more balanced, thereby enhancing performance in downstream tasks.\n5.5 Effects of sampling weight disparities on model performance\nFigure 7 presents the outcomes of experimental investigations into the effects of varying disparities between maximum and minimum weights assigned to different data partitions. The methodology employed ensures a consistent ascending order in the allocation of weights, with greater disparities indicating a more pronounced suppression of data with high commonness.\nExperiments conducted utilizing disparities in the maximum to minimum weight ratios of 2-fold, 5-fold, and 10-fold reveal a consistent trend: increased disparities between the maximum and minimum weights lead to a reduction in average model perplexity. Although slight variations are observed in the performance outcomes for downstream tasks, the experiments demonstrate that the largest weight disparity consistently facilitates the most optimal model performance.\n5.6 Cost of data reweighting\nThe computational processes of n-gram training and commonness calculation are executed solely on CPU resources. For a 40B token corpus, the n-gram training procedure (with n = 4) requires 4 CPU cores for 5 hours, followed by computing data commonness using 4 CPU cores in 2 hours. Compared to the substantial costs of GPU conservation (at least 930 V100 GPU hours in our experiments), these expenses can be considered negligible. This underscores the efficiency of SoftDedup and the feasibility of its implementation in resource-constrained environments."}, {"title": "6 Conclusion", "content": "In this study, we introduce a soft deduplication method that effectively addresses the primary limitations associated with traditional hard deduplication methods. Unlike its predecessors, this approach retains all samples of data while reallocating sampling weights according to data commonness. Experimental analyses demonstrate that this technique can significantly expedite the training process for large language models, evidenced by a reduction of over 26% in the number of training steps required. The proposed method surpasses existing deduplication techniques in effectiveness and can serve as a valuable complement to these methods. Due to its low operational cost and superior efficiency, we advocate for the integration of this soft deduplication approach with traditional hard deduplication methods as a standard practice in the pre-training phase of large language models."}, {"title": "Limitations", "content": "Due to current limitations in computational resources, the extension of SoftDedup to larger-scale models will be deferred to future research endeavors. Moreover, future studies will seek to conduct a more comprehensive evaluation of the method's effectiveness across various mixed data sources."}, {"title": "A Appendix", "content": "A.1 Average perplexity for each subset in the Pile test set\nIn Table 2, we provide a detailed report on the average perplexity for each subset within the Pile test set. For models trained on the RedPajama Common Crawl dataset, our method results in improvements across 18 out of 22 subsets. For models trained on the SlimPajama CommonCrawl dataset, our method leads to improvements in 17 subsets. For models trained on the Falcon RefinedWeb, improvements are observed in 19 subsets. Due to the exceedingly small number of documents in the Ubuntu IRC subset, we exclude it from the calculation of the average perplexity on the Pile test set.\nA.2 Average perplexity for each subset in the SlimPajama test set\nIn Table 3, we provide a detailed report on the average perplexity for each subset within the SlimPajama test set. Our method has led to improvements across nearly all subsets.\nA.3 Accuracy for each downstream task\nIn Table 4, we provide a detailed report on the accuracy for each downstream task. For models trained on the RedPajama CommonCrawl dataset, our method has led to improvements across all tasks. For models trained on the SlimPajama CommonCrawl and Falcon RefinedWeb datasets, our approach has also resulted in accuracy improvements on the majority of tasks."}]}