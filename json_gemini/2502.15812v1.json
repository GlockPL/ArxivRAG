{"title": "Insight Vision: A Comprehensive, Multi-Level Chinese-based Benchmark for Evaluating Implicit Visual Semantics in Large Vision Language Models", "authors": ["Xiaofei Yin*", "Yijie Hong", "Ya Guo", "Yi Tu", "Weiqiang Wang", "Gongshen Liu", "Huijia zhu"], "abstract": "In the evolving landscape of multimodal language models, understanding the nuanced meanings conveyed through visual cues-such as satire, insult, or critique-remains a significant challenge. Existing evaluation benchmarks primarily focus on direct tasks like image captioning or are limited to a narrow set of categories, such as humor or satire, for deep semantic understanding. To address this gap, we introduce, for the first time, a comprehensive, multi-level Chinese-based benchmark designed specifically for evaluating the understanding of implicit meanings in images. This benchmark is systematically categorized into four sub-tasks: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension. We propose an innovative semi-automatic method for constructing datasets, adhering to established construction protocols. Using this benchmark, we evaluate 15 open-source large vision language models (LVLMs) and GPT-40, revealing that even the best-performing model lags behind human performance by nearly 14% in understanding implicit meaning. Our findings underscore the intrinsic challenges current LVLMs face in grasping nuanced visual semantics, highlighting significant opportunities for future research and development in this domain.", "sections": [{"title": "1. Introduction", "content": "In the domain of multimodal language models[1, 21, 42], grasping the subtle meanings conveyed through visual cues such as sarcasm, insult, or criticism-remains a substantial challenge. Understanding the nuanced implications of images is indicative of advanced human intelligence, serving as a vital bridge between perceptual and cognitive intelligence[12, 15]. Many images cannot be fully comprehended by merely examining their surface content; instead, a genuine understanding requires integrating background knowledge and symbolic cues to discern the true intentions of the image's creator[14, 39].\nWhile visual perception entails transforming visual signals into insightful conclusions, such as profound image semantics or subtle narrative tones, existing evaluation benchmarks often fall short of assessing these deeper levels of understanding [16, 18]. These benchmarks primarily emphasize superficial tasks, such as image captioning, with datasets like COCO and ImageNet[6, 19]. Such efforts inadequately capture the intricacies of symbolic meanings and implicit interpretations. Furthermore, comprehensive visual perception demands both high- and low-level understanding, whereby humans employ common-sense knowledge to interpret broad concepts before honing in on the details [9, 39]. Current large vision-language models (LVLMs), however, often show limitations in articulating this hierarchical understanding.\nTo address these challenges and bridge the gaps in existing research, we introduce InsightVision, a comprehensive Chinese-based benchmark designed for nuanced, multi-level image evaluation. The InsightVision is systematically divided into four subtasks: surface-level content understanding, background knowledge comprehension, symbolic meaning interpretation, and implicit meaning comprehension. Unlike traditional datasets, it aims to provide a more thorough evaluation of multimodal language models' ability to grasp the deep semantics underlying images. The dataset comprises over 2,500 samples, each consisting of an image accompanied by questions spanning the four dimensions. Additionally, we have developed a semi-automatic pipeline to construct high-quality dataset. Utilizing Insight Vision, we evaluate the implicit understanding capabilities of 15 open-source LVLMs and GPT-40. Our assessment reveals a substantial gap between existing LVLMs and human performance in comprehending implicit meanings. For instance, even the best-performing model lags behind humans by nearly 14% in terms of understanding implicit implications. These findings highlight the significant challenges in this domain and underscore the substantial opportunity for improvement in developing models capable of deeply understanding visual semantics. We have publicly released our annotations, code, and model results. We will publicly release our InsightVision dataset, code upon acceptance of the paper."}, {"title": "2. Related Work", "content": null}, {"title": "2.1. Large vision language model", "content": "Vision-language models [2, 5, 21, 23, 28, 34]have achieved remarkable advancements within the realm of multimodal intelligence. By amalgamating large language models [1, 3, 32, 36, 37] with visual content, LVLMs effectively manage intricate visual and linguistic inputs, thereby executing a variety of tasks ranging from visual description to logical reasoning. Flamingo[2] and OpenFlamingo[4] models incorporate visual feature processing modules into the internal strata of language models using gated cross-attention, thereby propelling the profound integration of visual data within LLMs. CLIP[31, 33] utilizes contrastive learning to harmonize image and text modalities and is trained on extensive, noisy web-derived image-text pairs. By integrating modules such as QFormer[23] and MLP[25], previous works[5, 11, 24] facilitate a collaborative comprehension between visual encoders and large language models (LLMs) of multimodal inputs. LLaVA[25] stands out for its pioneering use of GPT-generated instruction-following data to amplify LVLMs' responsiveness to visual instructions. A plethora of powerful LVLM APIs, including GPT-40[1] and Qwen-VL-max[5], are now available. Through a rigorous evaluation of these models based on our proposed benchmark, we offer insightful perspectives into the ongoing research surrounding LVLMs."}, {"title": "2.2. Vision Language Benchmarks", "content": "A rapidly expanding suite of multimodal benchmarks now rigorously evaluates the capabilities of LVLMs. Established benchmarks, including COCO Caption [7], VQAv2 [16], and GQA [19], predominantly center on image description and question-answering tasks, employing metrics such as BLEU, CIDEr, and accuracy to gauge performance. Yet, as LVLMs advance, these traditional datasets have become insufficient for fully capturing the breadth of model capabilities. In response, researchers have developed more comprehensive evaluation frameworks that test a wider range of competencies, encompassing perceptual and cognitive skills [13], spatial-temporal reasoning [20], and relational understanding [26]. For instance, MMMU [43] curates data from college-level textbooks and lecture materials, challenging models to demonstrate expertise across six academic disciplines. Similarly, CMMU [17] gathers questions from primary through high school curricula to assess foundational knowledge within the Chinese educational context. Nevertheless, these benchmarks largely remain focused on basic visual tasks, without adequately addressing the complexity of multimodal understanding. This paper introduces a benchmark tailored to evaluate deep semantic comprehension of images, specifically within a Chinese cultural framework."}, {"title": "2.3. Image implicit meaning comprehension", "content": "Image implicit meaning comprehension has become an important research focus for contemporary LVLMs, especially in handling images that convey complex emotions, cultural symbolism, and social critique. Existing evaluation datasets primarily test the models' linear visual reasoning abilities, such as visual question answering for surface-level content[19]. However, several works [6, 29] have demonstrated that LVLMs' capabilities go beyond understanding surface-level meanings. Recent works[27, 40] highlight the limitations of current models when it comes to processing nonlinear narratives and understanding cultural contexts. For example, the most relevant prior work, DEEPEVAL[40], introduces three core tasks and shows that while the most advanced models achieve near-human performance on basic visual description tasks, they still perform poorly on tasks that involve understanding implicit semantics such as social background and satire. This paper provides a more comprehensive Chinese understanding benchmark, which, compared to the six categories in Deep-Eval, expands to include more thematic categories, with a total of 13 major categories and 41 subcategories and offers more detailed testing across four dimensions of model performance."}, {"title": "3. Dataset and task overview", "content": "Insight Vision, a comprehensive Chinese dataset, has been meticulously developed to assess the proficiency of LVLMs in deciphering nuanced and implicit meanings within visual content. This dataset encompasses 2,500 carefully curated samples, each comprising an image coupled with a set of choice questions. These questions are strategically designed to evaluate four distinct dimensions: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension.\nThe structure of Insight Vision reflects the complex cognitive process involved in image interpretation, where models are required to first comprehend the surface visual content, then integrate extensive background knowledge and symbolic interpretations to ultimately infer the implicit meaning. To facilitate quantitative evaluation, we have crafted one or more single-choice questions for each dimension, testing the model's understanding across various levels of complexity. Each question presents an image, a query, and four answer options, with only one correct answer and three carefully designed distractors.\nThis holistic design in dataset construction allows for a robust evaluation of LVLMs' capabilities in processing visual information beyond mere surface-level recognition, delving into deeper levels of contextual and cultural understanding.\nThe four primary subtasks in our evaluation framework are:\nSurface-level content understanding: This subtask assesses the model's ability to accurately identify and describe visual details present in the image. It serves as a foundation for more complex interpretations and ensures that the model can process basic visual information effectively.\nSymbolic meaning interpretation. This subtask evaluates the model's capacity to understand the symbolic or metaphorical meanings conveyed by the image content. It tests the model's ability to move beyond literal interpretation and grasp deeper, culturally-informed meanings.\nBackground knowledge comprehension. This subtask evaluates the model's ability to leverage relevant background knowledge necessary for understanding the image content. It examines the model's capacity to integrate external information and context with visual cues.\nImplicit meaning comprehension. The final subtask examines the model's proficiency in grasping the overall implicit message or subtle connotations conveyed by the image. This challenges the model to synthesize information from multiple sources and levels of interpretation to arrive at a holistic understanding.\nThe rationale for selecting these four tasks is to provide a comprehensive assessment of LVLMs' strengths and weaknesses in interpreting implicit visual meanings. This approach evaluates models across a range of cognitive processes, from basic perception to high-level reasoning and cultural understanding. By structuring the evaluation this way, we gain insights into how well LVLMs mimic human understanding of complex visual stimuli, identify areas for improvement, and guide future research in developing more sophisticated multimodal AI systems capable of nuanced interpretation."}, {"title": "4. Dataset construction", "content": "Constructing datasets that cover a broad range of knowledge typically requires highly educated annotators, but this approach is time-consuming and costly. To address these challenges, we developed a semi-automatic pipeline for creating the InsightVision dataset, focused on images with implicit meanings. The pipeline includes the following steps (as shown in Figure 3): 1) Image collection, 2) Data annotation, 3) Keypoint extraction, 4) Question and option generation, and 5) Quality control."}, {"title": "4.1. Image collection", "content": "The Insight Vision dataset was constructed through a comprehensive web crawling process. We systematically collected approximately 100,000 images from Cartoon Movement[30], a reputable online platform for editorial cartoons and comics. Each image was accompanied by its associated metadata, including titles, detailed textual descriptions, and relevant keywords. Following the collection phase, we conducted a manual curation process to eliminate duplicates and images lacking implicit meanings. Unlike previous studies, which typically categorize images into a limited set of themes such as humor or satire, we aimed to design a more comprehensive classification system. Therefore, we developed a hierarchical classification system to categorize the curated images based on their primary thematic content. This classification resulted in 13 major categories, including, but not limited to: Arts and Cultural Expression, Economic Development, Social and Cultural Issues, Politics and Power Dynamics, Health and Safety Concerns, and more. These major categories were further subdivided into 41 specific subcategories, providing a granular approach to image classification (Figure 2). From these categories, we selected 2,500 images to proceed to the next phase of annotation tasks."}, {"title": "4.2. Data pre-annotation", "content": "To obtain high-quality image annotation data, we implemented a novel approach combining LVLM pre-annotation with human expert verification. This method ensures comprehensive and accurate image understanding, encompassing both explicit visual content and implicit meanings.\nPre-annotation model and human annotator selection. After extensive comparative analysis, we identified GPT-40 as the optimal pre-annotation model. GPT-40 demonstrated superior performance in interpreting nuanced image meanings when provided with textual prompts. To maintain annotation quality, we employed a dual-review process involving two postgraduate-level experts independently verifying each pre-annotation, thus minimizing potential biases and errors.\nComprehensive image description generation. To generate high-quality image understanding data encompassing surface-level content, background knowledge, symbolic meanings, and implicit connotations, we input the crawled images along with their corresponding titles, textual descriptions, and keywords into GPT-40. Guided by these textual prompts, we instruct GPT-40 to provide a comprehensive description of the image, including: a) Detailed surface-level visual content; b) Implicit meanings and connotations; c) Requisite background knowledge for understanding these implicit meanings; d) Explanation of symbolic representations and connotations. This approach results in high-quality image-description pairs, each containing a rich, multi-layered interpretation of the visual content."}, {"title": "4.3. Keypoint extraction", "content": "After providing a complete description for each image, we extracted key points corresponding to four distinct tasks from these complete descriptions."}, {"title": "4.4. Questions and options generation", "content": "After obtaining image annotations, we utilize the annotated keypoints to generate questions and four answer options. Due to the high manual cost, we utilize the complete image descriptions from Section 4.2 and Qwen2-72B to assist in generating questions and options. Qwen2-72B, with 72 billion parameters, is chosen for its capability in natural language generation.\nFor surface-level understanding, symbolic meaning comprehension, and background knowledge tasks, multiple questions are generated based on keypoints, each with four answer choices. For implicit meaning understanding, which primarily evaluates the model's ability to grasp implicit meanings through reasoning that involves surface-level content, background knowledge, and symbolic interpretation, the answer options tend to be lengthier."}, {"title": "4.5. Dataset quality", "content": "To ensure dataset quality, we developed a comprehensive set of quality generation criteria and filtering procedures (detailed prompts are provided in the Appendix C).\nGeneration criteria:\n1. Consistency. All options should have roughly the same word count, avoiding obvious length discrepancies. Ensure all options maintain consistency in tone, professionalism, and vocabulary style to prevent the correct answer from being identified through stylistic differences.\n2. Distractibility. Wrong options should be designed to be misleading and seemingly reasonable, making them difficult to eliminate by common sense alone. Ensure incorrect options have a certain persuasiveness, rather than being mere assumptions or obvious errors.\n3. Avoiding image element misguiding. Ensure that any image elements mentioned in the options match or are similar to the actual content, avoiding easy elimination of incorrect options due to incorrect image details.\n4. Preventing keyword and pattern recognition. Avoid obvious keyword matches between the question and options to prevent easy inference.\n5. Unique correct answer. Ensure only one correct answer, avoiding ambiguity and ensuring clarity in each option.\n6. Core assessment. The design of the question and answer must focus on the key information in the assessment point, which refers to the information related to understanding the deeper meaning.\nFiltering procedures:\n1. Initial filtering. We employ Qwen2-72B to verify whether generated questions and options fully comply with the six criteria across different understanding levels (surface content, symbolic meaning, background knowledge, and implicit meaning). Questions meeting all criteria are retained; others are regenerated.\n2. Advanced filtering. Research suggests that some benchmarks are less reliant on visual input.[35] To ensure true visual dependency and avoid reliance on keyword or pattern recognition, we developed an innovative screening method. Questions are initially input to Qwen2-72B without accompanying images. If the model answers correctly without visual context, the question is discarded and regenerated until it genuinely requires visual input.\n3. Difficulty control. We implemented a model voting system using 16 different models to evaluate question difficulty. The difficulty of each question is determined by the proportion of models that answer it correctly. Questions are categorized based on their correct rate (e.g., 100% correct rate is classified as easy, 10% as difficult) and are equally distributed across difficulty levels in the final dataset, excluding the easy level.\n4. Human evaluation. Final quality assurance involves a three-person voting system, wherein questions are retained only if all three annotators unanimously agree on their validity and appropriateness. In our study, we recruited a total of nine annotators, who were grouped into teams of three to annotate the same set of questions. The educational backgrounds of the annotators were diverse, comprising two with undergraduate degrees and seven with associate degrees. The questions were broken down into highly specific components, so a high level of academic qualification was not required for annotation.\nThis methodology ensures a high-quality, visually-dependent dataset with controlled difficulty levels and verified accuracy. Regarding the quality assessment of automatically generated questions, the error rate for pre-annotation using GPT-4 (image pre-annotation) was found to be 2%, while the final error rate for the generated questions was 5%. These results suggest that the quality of the automatically generated questions was generally high, demonstrating the effectiveness of the automated process."}, {"title": "4.6. License and copyright", "content": "Ethics Statement: All data samples for this project are sourced from publicly accessible content on social media platforms. To ensure copyright compliance, we use direct links to the original comics to avoid any infringement. Our annotated benchmark will be open-sourced, with links provided for each comic image. We carefully review samples to exclude any content that might be offensive or harmful. Additionally, we have obtained permission from the creators to use these public images within our benchmark.\nData Annotation: Our annotators voluntarily participated in the annotation process and were fairly compensated."}, {"title": "5. Experiments", "content": "Given the impressive performance of LVLMs in tackling image understanding challenges, we evaluated the following LVLMs: InternVL2[8], Qwen2-VL[38], MiniCPM-V-2_6[41], DeepSeek-VL[28], LLaVA-OneVision[22], and GPT40[1]. These models were selected based on their top-ranking performance in the OpenCompass leaderboard[10]. Notably, Qwen2-VL-72B[38] stands out as the leading open-source LVLMs, while GPT-40[1] is widely regarded as one of the excellent closed-source LVLM."}, {"title": "5.1. Evaluation", "content": "For evaluating task performance, accuracy was considered the primary metric. A model's answer was deemed correct if it matched the ground truth. Accuracy was computed as the ratio of the number of correct answers (N) to the total number of questions (N), i.e.,Nr/N].\nOur task prompts were determined based on each image and task type (referring to the four tasks), followed by choice options: A, B, C, D. The specific parameter settings, including temperature and top-k values, used for each model in the experiments are detailed in the Appendix E. Furthermore, to assess human performance on these tasks, we randomly selected 100 questions from each task in the dataset and had human evaluators provide answers. This allowed us to benchmark human participants' performance against our models, providing a comprehensive comparison of human and machine capabilities on these specific tasks."}, {"title": "5.2. Main results", "content": "Surface-level content understanding. Among the open-source models, Qwen2-VL-72B-Instruct and InternVL2-40B performed best on the surface-level content understanding task, with accuracies of 79.3% and 79.5%, respectively, close to GPT-40 (82.0%). Performance generally correlated with model size, ranging from 44.4% for the 0.5B llava-onevision-qwen2 to 79.3% for the 72B Qwen2-VL. However, all models showed a substantial gap compared to human performance (98.0%), highlighting room for improvement.\nSymbolic meaning interpretation. Qwen2-VL-72B-Instruct performed optimally, achieving an accuracy of 82.6%, slightly surpassing GPT40's 80.8%. Smaller models like llava-onevision-qwen2-0.5b-ov-hf achieved only 45.0%, suggesting that model scale significantly impacts symbolic understanding capabilities. Most models' performance on this task was similar to the surface-level content understanding task, indicating comparable difficulty levels for symbolic meaning interpretation and surface-level content understanding.\nBackground knowledge comprehension. InternVL2-40B and Qwen2-VL-72B-Instruct exhibited the best performance, with accuracies of 80.7% and 81.6%, respectively. The relatively small gap compared to human performance (86.0%) indicates that models have made significant progress in background understanding.\nImplicit meaning comprehension. All models performed significantly worse on the implicit meaning comprehension task compared to the other tasks. The best performance was achieved by Qwen2-VL-72B-Instruct at 60.1%, comparable to GPT40 (59.3%). Smaller models like llava-onevision-qwen2-0.5b-ov-hf achieved only 23%, revealing a substantial gap compared to human performance (74.0%). This task appears to be the most challenging for current LVLMs."}, {"title": "6. Analysis", "content": null}, {"title": "6.1. How do the models perform across different categories of visual perception?", "content": "Figure 4 illustrates model performance across four key tasks: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension, spanning various categories. Accuracy varies significantly across categories. In simpler categories like history and environment, models achieve higher accuracy by effectively capturing direct information. However, performance drops in categories involving deep cultural symbols or metaphors, such as philosophy and personal growth, highlighting current models' limitations in handling complex semantics and cultural nuances.\nLarger models (40B+) consistently outperform smaller ones, especially on complex tasks. For simpler tasks like surface-level content, all models perform well, though larger models still have an edge. As task complexity increases, performance gaps widen, with top models significantly surpassing smaller ones but still facing challenges. The Qwen2-VL and InternVL2 series excel in symbolic meaning and background knowledge but show varying stability in implicit meaning comprehension, highlighting ongoing challenges in complex semantic interpretation. These results suggest that while scaling improves performance, implicit meaning comprehension requires further architectural or training optimizations for substantial progress."}, {"title": "6.2. Can Image Descriptions Help the Model Understand Implicit Meaning?", "content": "We believe that, like humans, models need to combine surface-level content, symbolic meaning, and background knowledge to understand implicit meanings. Figure 5 shows that performance in implicit meaning comprehension is closely related to the first three tasks. To further validate this, we added key information from these tasks to the reasoning prompts. Experimental results (see Appendix F) show significant improvement, with the optimal model's accuracy surpassing human performance. We reasonably assume that adding this information enables the model to capture most of the foundational content and background knowledge required for implicit meaning comprehension. However, despite these benefits, there remains room for improvement, suggesting that capturing key information alone is insufficient for fully understanding implicit meanings. To achieve human-like comprehension, models need not only the ability to capture key information but also the reasoning ability to process it effectively."}, {"title": "6.3. How Does Model Parameter Scale Affect Implicit Meaning Comprehension?", "content": "According to scaling laws, increasing model parameters generally improves performance. To evaluate this relationship, we selected models of different scales from two distinct series, InternVL2 and Qwen2-VL. Within each series, the models share the same architecture but differ in scale. InternVL2 and Qwen2-VL, which share architecture but vary in size. Figure 6 shows that larger models perform better across all four tasks, with models in the 40B-72B range balancing performance and computational cost. However, deeper semantic tasks may need further architectural optimizations, indicating that enhancing deep semantic comprehension requires more than scaling-it also needs specialized strategies."}, {"title": "7. Conclusion", "content": "We introduce InsightVision, a comprehensive, multi-level Chinese-based benchmark designed to evaluate the understanding of implicit visual semantics in LVLMs. The benchmark comprises over 2,500 carefully curated images, each paired with questions that assess four levels of comprehension: surface-level content understanding, symbolic meaning interpretation, background knowledge comprehension, and implicit meaning comprehension. Our evaluations demonstrate a considerable gap between current LVLMs and human performance, particularly in understanding implicit meanings. We suggest that enhancing model parameters or integrating detailed image descriptions during reasoning may help improve the model's ability to capture and interpret deeper semantic content. This work underscores the need for more advanced multimodal models capable of nuanced visual semantic understanding. We hope Insight Vision will serve as a valuable resource for advancing research aimed at bridging the gap between perceptual recognition and cognitive understanding of visual content."}, {"title": "Limitations", "content": "The InsightVision dataset currently focuses on comic images, which effectively convey implicit meanings but lack visual diversity. Future expansions will include other media, such as photography and video, to enhance diversity and applicability. Additionally, the dataset is based on Chinese cultural contexts, which may limit generalizability; broader cultural inclusion is planned. Lastly, despite using GPT-40 and human review for annotation, biases and errors may still exist, and improvements to the generation pipeline are needed to address these issues."}]}