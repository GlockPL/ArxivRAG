{"title": "REALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments", "authors": ["Taewoong Kim", "Cheolhong Min", "Byeonghwi Kim", "Jinyeon Kim", "Wonje Jeung", "Jonghyun Choi"], "abstract": "Simulated virtual environments have been widely used to learn robotic agents that perform daily household tasks. These environments encourage research progress by far, but often provide limited object interactability, visual appearance different from real-world environments, or relatively smaller environment sizes. This prevents the learned models in the virtual scenes from being readily deployable. To bridge the gap between these learning environments and deploying (i.e., real) environments, we propose the REALFRED benchmark that employs real-world scenes, objects, and room layouts to learn agents to complete household tasks by understanding free-form language instructions and interacting with objects in large, multi-room and 3D-captured scenes. Specifically, we extend the ALFRED benchmark with updates for larger environmental spaces with smaller visual domain gaps. With REALFRED, we analyze previously crafted methods for the ALFRED benchmark and observe that they consistently yield lower performance in all metrics, encouraging the community to develop methods in more realistic environments. Our code and data are publicly available\u00b3.", "sections": [{"title": "1 Introduction", "content": "Building autonomous robotic assistants that can perform everyday household tasks has been an elusive aspiration within the research community for decades. To let them learn these intricate tasks, we may provide them with interactive environments where agents can learn task completion skills with numerous interactions with environments. A straightforward approach to train such agents that can carry out real-world activities is to directly deploy robots in real-world environments and let them learn to complete desired tasks. However, this often faces several practical challenges, including cost, time, or safety concerns [3,14,39,68]."}, {"title": "3 The REALFRED Benchmark", "content": "To develop agents capable of performing household tasks, substantial progress has been achieved in various domains, including navigation [1,38], rearrange-"}, {"title": "3.1 Object-Interactable 3D-Captured Scenes", "content": "To reduce the visual domain gap, a straightforward approach is to use 3D scans of real world environments. However, the captured 3D scans (i.e., meshes) remain static and thus, agents cannot interact with objects in the captured scenes. For object interaction, we manually replace object parts with 3D object assets to support object interaction. For photorealism comparison with previous environments using FID [27] and KID [4] metrics, please refer to Sec A.3.\nWe detail the process of collecting object-interactable 3D-captured environments and highlight key differences from previously proposed benchmarks below.\nData acquisition process. To collect 3D scans of real-world environments, we visit residential properties and employ scanners. Inspired by recent work [2], we collect scans outside of the US to add the diversity with public scanned environments. We utilize the same 3D scanner as [7], equipped with three RGB cameras"}, {"title": "Object-interactable environments.", "content": "To construct an environment with numerous interactable objects, separating each objects should be preceded. In other words, objects in the scans are initially merged into the background and therefore they remain as the background. Constructing an environment where objects can be interacted with requires the separation of objects from the background and from each other. Therefore, we manually separate the 3D scans into background elements and interactive objects. Furthermore, each object can exist in various states. To visualize changes in an object's state, we add state-relevant textures on objects. For example, we add a stain texture to a clean object when it becomes dirty. Finally, we reconstruct these individual object meshes within the Unity editor, making them compatible with the AI2-THOR simulator [35]."}, {"title": "Comparison with previous benchmarks.", "content": "To investigate the spatial characteristics of scenes in the REALFRED benchmark, we compare ours with other benchmarks that support interaction with objects [16, 61, 67]. We observe enhancements in our dataset in both: 1) spatial sizes and 2) spatial complexity.\nSpatial size. We compare the REALFRED benchmark with other benchmarks in terms of spatial size [56] by measuring 'Floor area' and 'Navigable area' and provide the result in Fig. 4. 'Floor area' represents the total spatial size (m\u00b2) of a scene, defined by the floor projection. 'Navigable area' measures the spatial size (m\u00b2) of the space in which an agent can actually navigate. 'Navigable area' is smaller than or equal to 'Floor area' since 'Navigable area' excludes areas where the agent collides with any components in the scene from 'Floor area.'\nWe observe that the REALFRED benchmark yields a diverse distribution of floor areas, compared to previous work [61], with a larger average per scene. Furthermore, the REALFRED benchmark provides a broader range of navigable"}, {"title": "Spatial complexity.", "content": "We now investigate the complexity of spatial structures in the REALFRED benchmark using several metrics. To ensure a fair comparison with other datasets [16,61,67], we employ the navigation complexity introduced by [74] and the scene clutter measurement from [56]. A higher navigation complexity indicates an increased difficulty in navigating through the space, while a higher scene clutter implies the presence of more obstacles in the environment. By utilizing these metrics, we conduct a comparative analysis with object-interactable benchmarks [16,61] and provide the result in Table 2.\nWe observe that the REALFRED benchmark provides the environment with higher navigation complexity and scene clutter compared to other benchmarks [61,67]. The high navigation complexity in our scenes stems from their multi-room composition. This setup requires the agent to execute more intricate navigation when moving from one room to another, in comparison to scenarios where the agent operates within a single room. We observe that ours poses the second-highest scene clutter with a marginal gap from [16]. This is because the spaces in [16] are relatively confined with a high density of furniture. We observe that ours has a similar value to [16], meaning that our scenes have a large amount of obstacles with a similar portion of [16] since our scenes' average size is larger than that of [16]. This implies that the REALFRED benchmark provides a complex and challenging space for the agent to explore the environment."}, {"title": "3.2 Expert Demonstration Generation", "content": "Each expert demonstration includes a set of an egocentric RGB view and action information with an interaction mask if exists at each time step. Expert demonstrations for each task are generated by a planner [29] with encoded state spaces into Planning Domain Definition Language (PDDL) rules [22]. To generate household tasks, we utilize seven task types introduced in [61].\nData splits. We split the generated demonstrations into train, validation, and test folds. Specifically, we designate 135 scenes for seen and 15 scenes for unseen fold. Comparison of the amount of each task and the distribution across training and validation folds with previous work [61] is shown in Fig. 5.\nCurating free-form language instructions. Detailed language instructions describe a task that involves a sequence of actions, a point of interest in robotics. The REALFRED benchmark offers 30,696 language directives, each comprising a human-annotated high-level goal and a set of step-by-step instructions. These directives are collected from 93 Amazon Mechanical Turk workers with a 'Master' qualification, ensuring high-quality. Collected annotations are validated through an additional voting survey, and invalid instructions are replaced with newly collected instructions. The distribution of language instructions by their first four words is presented in Fig. 6. We provide a detailed annotation process and examples of an expert demonstration with the instruction in Sec. A.1 and A.4 in the supplementary material."}, {"title": "4 Experiments", "content": "Metrics. We follow the same evaluation protocol of the ALFRED benchmark [61]. The primary metric is 'Success Rate (SR)' which measures the percentage of completed tasks. \u2018Goal-Condition Success Rate (GC)' measures the percentage of achieved goal conditions. We also use path-length-weighted metrics to measure how efficiently an agent completes tasks. For more details, kindly refer to [61].\nBaselines. We evaluate several recent state-of-the-art methods [5,33,34,46,61-63] with competitive results in [61]. We provide more details in Sec. B."}, {"title": "4.1 Comparison of the State of the Arts", "content": "We evaluate the baselines in the proposed REALFRED benchmark over multiple runs and present the average result in Table 3. We report extended results with path-length-weighted metrics in Sec. C in supplementary.\nFor a fair comparison, we separate the baseline into two groups based on the use of additional depth supervision for semantic map reconstruction: 'Imitation Learning' where agents learn direct mapping from visual observations and language instructions to action sequences and 'Spatial Map Reconst.' where agents plan action sequences based on reconstructed semantic spatial representations."}, {"title": "4.2 Comparison to the agents with sim-to-real adaptation", "content": "We investigate the transfer from simulation training to real-world scan evaluation (sim-to-real) and from real-world scan training to real-world scan evaluation (real-to-real) with [33]. We train a sim-to-real agent with synthetic visual data and a real-to-real agent with real scanned visual data, respectively. During inference, both agents predict an action and an object mask based on the scanned visual input frame and the given language instruction at every time step."}, {"title": "4.3 Challenges in REALFRED", "content": "We propose two hypotheses for the low performance observed with state-of-the-art methods on the REALFRED benchmark: (1) navigating within larger scenes and (2) overcoming narrow pathways between rooms. These elements represent challenges of completing tasks within multi-room, household-scale environments."}, {"title": "5 Conclusion", "content": "We present REALFRED, a new dataset and benchmark for embodied instruction following task on 3D-captured environments. We capture 150 indoor houses in 3D with interactable objects to enable complex household tasks. The reconstructed indoor scenes provide a larger spatial area and complex multi-room environments that are close to the real-world scenario and challenging for an agent to successfully complete a task. Expert demonstrations are also provided along with free-form human-language instructions.\nIn our empirical evaluations, we show that state-of-the-art methods struggle in large multi-room environments, provide analyses of our newly proposed benchmark, and perform Sim2Real transfer experiments. We have released our Embodied AI research data and code for reproducibility. We expect that the REALFRED benchmark will encourage further research on developing robotic agents that execute household tasks by language instructions in the real world.\nLimitation and future work. Although we support a large number of interactable objects, the types of tasks to be completed are rather limited, considering more complex real-world scenarios. In addition, we currently address natural language in English but users may come from different regions with different languages. We can think of two future research avenues as follows. (1) adding additional complicated types of task that require both hands to complete. (2) supporting a multi-lingual interface for users from different regions."}, {"title": "A Benchmark Details", "content": "We provide all 114 types of objects in Fig. 9. The bold text denotes the uniquely introduced object in ours."}, {"title": "A.1 Annotation Interface", "content": "In this section, we describe the overall process of acquiring language annotations. Fig. 10a illustrates the interface of the Mechanical Turk used to collect human annotations from Mechanical Turk workers. We provided workers with an expert demonstration video and divided the timeline segments that have the intended subgoal (e.g., 'pick up the pencil case,' 'go to the sofa'). Workers were asked to fill each segment with their own words (e.g., 'Pick up the pencil case from the coffee table,' 'Walk around the table to get closer to the sofa'). Workers were paid $0.7 per annotation as following the previous work [61]. Moreover, we adopted a voting survey to filter out inappropriate annotations. Fig. 10b illustrates the interface of getting votes from workers. We conducted the voting with a minimum of 2 and up to 5 reviewers per annotation. Only annotations that received more than a majority of accepts in all cases were included in our set of annotations. For annotations that did not achieve a majority of accepts, we re-collected annotations and implemented a voting system to prevent the inclusion of low-quality annotations. We paid workers $0.35 to compare 5 sets of annotations following [61]."}, {"title": "A.2 Vocabulary Distribution", "content": "We provide vocabulary statistics for the language instructions in the REAL-FRED benchmark in Fig. 12."}, {"title": "A.3 Scanned Indoor Houses", "content": "Among collected 150 scenes, we split scenes into 135 seen and 15 unseen environments. Then we further split into validation (both seen and unseen) and test (both seen and unseen) folds. The details are presented in Table 5. Note that validation and test unseen scenes are exclusive.\nDegree of photorealism. We compare a degree of photorealism by measuring FID [27] and KID [4] scores following Ramakrishnan et al. [56]. We use rendederd RGB images from each synthetic environments [20, 41]. We compare the image quality with a set of RGB images rendered from previous dataset, HM3D [56] and Gibson [73]. To compare with previous scanned environments, we acquire a collection of real RGB images derived from high-resolution raw panoramas"}, {"title": "B.1 Imitation Learning", "content": "The Seq2Seq [61] model encodes the visual input with the frozen backbone visual encoder. The natural language goal and instructions are encoded with a bidirectional LSTM encoder to produce an embedding for each word. Alongside the previous action, embeddings are passed as input to an LSTM cell to produce the current hidden state. The action and corresponding mask are finally predicted"}, {"title": "B.2 Spatial Map Reconstruction", "content": "HLSM [5] uses a hierarchical controller to bridge the gap between natural language instructions and agent executable actions. The high-level controller predicts the next subgoal given the instruction and the map, and then the low-level controller outputs a sequence of actions to achieve the subgoal. FILM [46] utilizes a pre-designed template as a high-level action sequence. It uses two submodules of BERT classifiers to predict the type of instruction and the arguments to fill in the template. Finally, it uses a deterministic algorithm [60] for obstacle-free path planning. LLM-Planner [63] leverages large language model to generate subgoal sequence with a few examples. To enhance LLMs planning accuracy, it updates plans that are physically grounded in the environment. CAPEAM [34] uses context-aware planning to plan a subgoal sequence and conduct the respective subgoal with the corresponding detailed planners. It also uses additional memory to prevent the interaction of inappropriate objects."}, {"title": "C Extended Quantitative Results", "content": "We present experiment results with path-length-weighted success rate and goal condition (i.e., PLWSR and PLWGC) over multiple runs in Table 7 and 8."}, {"title": "D Map Reconstruction Strategy", "content": "We provide a more detailed analysis of the challenges in recognizing narrow passages (e.g., doors, aisles, etc.). Our observation is as follows: failure to recognize narrow navigable pathways leads an agent to stuck within the initial room.\nTo quantify this challenge, we establish a criterion for leaving a room by taking 1 step (i.e. 0.25 meter) further from the entrance of the room where the"}, {"title": "E Qualitative examples for domain adaptation", "content": "We provide qualitative examples of real-to-sim domain adaptation in Fig. 15. 'Source' column denotes a visual frame from the REALFRED benchmark, 'CycleGan' column denotes an adpated visual frame with CycleGan [79], 'UVCGAN-v2' column denotes an adapted visual frame with UVCGAN-v2 [70], and 'Target' column denotes an image from ALFRED target domain image. The advantage of real-to-sim domain adaptation is to make an agent feel at home, reducing the visual domain gap. We expect a domain adapted image to resemble some characteristics that are well represented in the target domain, where sim2real agent is trained.\nWe begin our examination with an example in the first row. We observe that the flooring, dominating the image frame, is adapted to resemble the flooring that frequently appears in the target domain (highlighted with \u2610). We also notice that the image generated with CycleGan adapts the color of the wall to brown, while the image generated with UVCGAN-v2 adapts the wall to white tone (highlighted with). \nWe now examine an example in the second row. We observe that the flooring, dominating the image frame as in the first example, is generated to resemble checkerboard tiles which are represented in target domain (highlighted with \u2610"}]}