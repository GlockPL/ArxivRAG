{"title": "REALFRED: An Embodied Instruction Following Benchmark in Photo-Realistic Environments", "authors": ["Taewoong Kim", "Cheolhong Min", "Byeonghwi Kim", "Jinyeon Kim", "Wonje Jeung", "Jonghyun Choi"], "abstract": "Simulated virtual environments have been widely used to learn robotic agents that perform daily household tasks. These environments encourage research progress by far, but often provide limited object interactability, visual appearance different from real-world environments, or relatively smaller environment sizes. This prevents the learned models in the virtual scenes from being readily deployable. To bridge the gap between these learning environments and deploying (i.e., real) environments, we propose the REALFRED benchmark that employs real-world scenes, objects, and room layouts to learn agents to complete household tasks by understanding free-form language instructions and interacting with objects in large, multi-room and 3D-captured scenes. Specifically, we extend the ALFRED benchmark with updates for larger environmental spaces with smaller visual domain gaps. With REALFRED, we analyze previously crafted methods for the ALFRED benchmark and observe that they consistently yield lower performance in all metrics, encouraging the community to develop methods in more realistic environments. Our code and data are publicly available.", "sections": [{"title": "1 Introduction", "content": "Building autonomous robotic assistants that can perform everyday household tasks has been an elusive aspiration within the research community for decades. To let them learn these intricate tasks, we may provide them with interactive environments where agents can learn task completion skills with numerous interactions with environments. A straightforward approach to train such agents that can carry out real-world activities is to directly deploy robots in real-world environments and let them learn to complete desired tasks. However, this often faces several practical challenges, including cost, time, or safety concerns."}, {"title": "3 The REALFRED Benchmark", "content": "To develop agents capable of performing household tasks, substantial progress has been achieved in various domains, including navigation , rearrange-"}, {"title": "4 Experiments", "content": "Metrics. We follow the same evaluation protocol of the ALFRED benchmark . The primary metric is 'Success Rate (SR)' which measures the percentage of completed tasks. \u2018Goal-Condition Success Rate (GC)' measures the percentage of achieved goal conditions. We also use path-length-weighted metrics to measure how efficiently an agent completes tasks. For more details, kindly refer to . Baselines. We evaluate several recent state-of-the-art methods  with competitive results in . We provide more details in Sec. B.\n4.1 Comparison of the State of the Arts\nWe evaluate the baselines in the proposed REALFRED benchmark over multiple runs and present the average result in Table 3. We report extended results with path-length-weighted metrics in Sec. C in supplementary.\nFor a fair comparison, we separate the baseline into two groups based on the use of additional depth supervision for semantic map reconstruction: 'Imitation Learning' where agents learn direct mapping from visual observations and language instructions to action sequences and 'Spatial Map Reconst.' where agents plan action sequences based on reconstructed semantic spatial representations."}, {"title": "4.3 Challenges in REALFRED", "content": "We propose two hypotheses for the low performance observed with state-of-the-art methods on the REALFRED benchmark: (1) navigating within larger scenes and (2) overcoming narrow pathways between rooms. These elements represent challenges of completing tasks within multi-room, household-scale environments."}, {"title": "5 Conclusion", "content": "We present REALFRED, a new dataset and benchmark for embodied instruction following task on 3D-captured environments. We capture 150 indoor houses in 3D with interactable objects to enable complex household tasks. The reconstructed indoor scenes provide a larger spatial area and complex multi-room environments that are close to the real-world scenario and challenging for an agent to successfully complete a task. Expert demonstrations are also provided along with free-form human-language instructions.\nIn our empirical evaluations, we show that state-of-the-art methods struggle in large multi-room environments, provide analyses of our newly proposed benchmark, and perform Sim2Real transfer experiments. We have released our Embodied AI research data and code for reproducibility. We expect that the REALFRED benchmark will encourage further research on developing robotic agents that execute household tasks by language instructions in the real world. Limitation and future work. Although we support a large number of interactable objects, the types of tasks to be completed are rather limited, considering more complex real-world scenarios. In addition, we currently address natural language in English but users may come from different regions with different languages. We can think of two future research avenues as follows. (1) adding additional complicated types of task that require both hands to complete. (2) supporting a multi-lingual interface for users from different regions."}, {"title": "A Benchmark Details", "content": "We provide all 114 types of objects in Fig. 9. The bold text denotes the uniquely introduced object in ours.\nA.1 Annotation Interface\nIn this section, we describe the overall process of acquiring language annotations. Fig. 10a illustrates the interface of the Mechanical Turk used to collect human annotations from Mechanical Turk workers. We provided workers with an expert demonstration video and divided the timeline segments that have the intended subgoal (e.g., 'pick up the pencil case,' 'go to the sofa'). Workers were asked to fill each segment with their own words (e.g., 'Pick up the pencil case from the coffee table,' 'Walk around the table to get closer to the sofa'). Workers were paid $0.7 per annotation as following the previous work . Moreover, we adopted a voting survey to filter out inappropriate annotations. Fig. 10b illustrates the interface of getting votes from workers. We conducted the voting with a minimum of 2 and up to 5 reviewers per annotation. Only annotations that received more than a majority of accepts in all cases were included in our set of annotations. For annotations that did not achieve a majority of accepts, we re-collected annotations and implemented a voting system to prevent the inclusion of low-quality annotations. We paid workers $0.35 to compare 5 sets of annotations following .   \nA.2 Vocabulary Distribution\nWe provide vocabulary statistics for the language instructions in the REAL- FRED benchmark in Fig. 12.\nA.3 Scanned Indoor Houses\nAmong collected 150 scenes, we split scenes into 135 seen and 15 unseen envi- ronments. Then we further split into validation (both seen and unseen) and test (both seen and unseen) folds. The details are presented in Table 5. Note that validation and test unseen scenes are exclusive.\nDegree of photorealism. We compare a degree of photorealism by measuring FID  and KID following Ramakrishnan et al. . We use rendederd RGB images from each synthetic environments . We compare the image quality with a set of RGB images rendered from previous dataset, HM3D  and Gibson . To compare with previous scanned environments, we acquire a collection of real RGB images derived from high-resolution raw panoramas"}, {"title": "A.4 Examples of Expert Demonstration", "content": "Fig. 18-21 illustrate the examples of expert demonstrations for 7 task types. The agent has to solve the task in interactive environments by understanding the language instructions and planning the sequential and executable actions.\nA.5 Diverse Episodes with More Objects\nEach episode is generated based on the combination of task-relevant objects (e.g., put a knife' on the 'table.') and this indicates that more object classes can result in more object-diverse episodes (e.g., put a 'potato' in a 'fridge.'). We observe that our REALFRED provides more diverse episodes compared to the ALFRED benchmark  by enriching the number of object classes. Here, we denote an episode whose combination of task-relevant objects does not overlap with the others by a unique episode. We observe that our REALFRED benchmark provides the 4,649 unique episodes while ALFRED provides 2,522 ones in the combined train and valid splits. In addition, the ratio of the unique episodes among the total ones is 53.3% in REALFRED while it is 35.6% in ALFRED. This indicates that our REALFRED benchmark provides not only a larger number of episodes but also more diverse combinations of episodes.\nA.6 Qualitative Comparison of Indoor Houses\nWe provide a qualitative comparison of the indoor houses used in our REAL- FRED and ALFRED  in the attached video files (video1.mp4, video2.mp4, and video3.mp4). We provide the agent's egocentric view on the left-hand side of a video and a top-down view with a red circle denoting the agent's corresponding current location. While ALFRED's indoor house environments consist of single room types on a room scale, ours are provided on a house scale, featuring multiple rooms within a single house. This implies that agents developed with our environments are enabled to perform instruction-following tasks that require navigating through multiple rooms.\nB Details of State of the Art Models\nWe provide details of state-of-the-art models in imitation learning and spatial map reconstruction, respectively.\nB.1 Imitation Learning\nThe Seq2Seq model encodes the visual input with the frozen backbone visual encoder. The natural language goal and instructions are encoded with a bidi- rectional LSTM encoder to produce an embedding for each word. Alongside the previous action, embeddings are passed as input to an LSTM cell to produce the current hidden state. The action and corresponding mask are finally predicted"}, {"title": "B.2 Spatial Map Reconstruction", "content": "HLSM  uses a hierarchical controller to bridge the gap between natural lan- guage instructions and agent executable actions. The high-level controller pre- dicts the next subgoal given the instruction and the map, and then the low-level controller outputs a sequence of actions to achieve the subgoal. FILM  utilizes a pre-designed template as a high-level action sequence. It uses two submodules of BERT classifiers to predict the type of instruction and the arguments to fill in the template. Finally, it uses a deterministic algorithm  for obstacle-free path planning. LLM-Planner  leverages large language model to generate subgoal sequence with a few examples. To enhance LLMs planning accuracy, it updates plans that are physically grounded in the environment. CAPEAM  uses context-aware planning to plan a subgoal sequence and conduct the respec- tive subgoal with the corresponding detailed planners. It also uses additional memory to prevent the interaction of inappropriate objects.\nC Extended Quantitative Results\nWe present experiment results with path-length-weighted success rate and goal condition (i.e., PLWSR and PLWGC) over multiple runs in Table 7 and 8.\nD Map Reconstruction Strategy\nWe provide a more detailed analysis of the challenges in recognizing narrow pas- sages (e.g., doors, aisles, etc.). Our observation is as follows: failure to recognize narrow navigable pathways leads an agent to stuck within the initial room.\nTo quantify this challenge, we establish a criterion for leaving a room by taking 1 step (i.e. 0.25 meter) further from the entrance of the room where the"}]}