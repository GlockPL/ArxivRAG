{"title": "Unsupervised Skill Discovery for Robotic Manipulation through Automatic Task Generation", "authors": ["Paul Jansonnie", "Bingbing Wu", "Julien Perez", "Jan Peters"], "abstract": "Abstract-Learning skills that interact with objects is of major importance for robotic manipulation. These skills can indeed serve as an efficient prior for solving various manipu-lation tasks. We propose a novel Skill Learning approach that discovers composable behaviors by solving a large and diverse number of autonomously generated tasks. Our method learns skills allowing the robot to consistently and robustly interact with objects in its environment. The discovered behaviors are embedded in primitives which can be composed with Hierarchi-cal Reinforcement Learning to solve unseen manipulation tasks. In particular, we leverage Asymmetric Self-Play to discover behaviors and Multiplicative Compositional Policies to embed them. We compare our method to Skill Learning baselines and find that our skills are more interactive. Furthermore, the learned skills can be used to solve a set of unseen manipulation tasks, in simulation as well as on a real robotic platform.", "sections": [{"title": "I. INTRODUCTION", "content": "Robotic manipulation is notorious for being a challeng-ing problem for Reinforcement Learning (RL), especially for Goal-Conditioned RL (GCRL). Manipulation tasks are indeed often associated with sparse rewards [1], [2], which dramatically increases the complexity of learning a suc-cessful policy. One classic example is the task of moving an object from an initial position to a target position, also known as a pick-and-place task. This task involves complex contact reasoning, fine control of the robotic manipulator,\nand possibly safety constraints. Learning to solve such a task usually requires reward shaping [3], [4], hindsight relabeling [5] or human demonstrations [6]. An alternative is the use of pre-trained behaviors, also known as skills or prim-itives. To cope with the complexity of training manipulation policies from scratch, Hierarchical Reinforcement Learning (HRL) proposes orchestrating pre-trained behaviors. Reusing a repertoire of pre-trained skills aims at increasing the probability of success, e.g., by making interactions with objects more frequent and consistent. In HRL, an orchestrator policy is trained to reuse pre-trained skills, which allows for maintaining a usable reward signal from the environment.\nSkill Discovery, or Skill Learning, is a family of methods that focus on autonomously discovering behaviors such that they can then be reused, e.g., with HRL. The most popular approaches rely on task-agnostic intrinsic rewards based on the mutual information between a skill descriptor and a transformation of the state of the environment [7], [8], [9], [10], [11], [12], [13], [14], [15], [16]. While these methods usually produce reusable behaviors in locomotion settings, they often fail to produce useful skills for manipulation, i.e., skills that yield interactions between the robot and objects in the scene. This property limits their reusability for learning downstream manipulation tasks with HRL. Furthermore, the discovered skills must not only yield interactions with objects, but these interactions must be diverse for reuse in a large set of downstream tasks.\nThis lack of meaningful interactions is the main struggle in Skill Learning for manipulation and is a direct consequence of the sparsity of the intrinsic reward based on mutual information.\nIn this work, we propose a Skill Learning method for robotic manipulation that discovers behaviors enabling the"}, {"title": "II. RELATED WORK", "content": "a) Skill Learning: A large collection of work studies the problem of learning reusable skills. Latent variable methods [7], [8], [9], [10], [11], [12], [13], [14], [15], [16] learn skills by maximizing the mutual information between a transformation of the environment state s and a latent skill descriptor z. Option architectures [21], [22], [23] learn to decompose tasks into a set of sub-goals, or options. Multiplicative Compositional Policies [20] learn to decompose behaviors in distinct Gaussian distributions that can be re-composed to yield more complicated behaviors.\nOur approach is similar to latent variable methods as we learn skills in an unsupervised manner. However, our training objective is not based on mutual information but rather on the usability of our skills in a manipulation scenario. Fur-thermore, contrary to latent variable methods, our approach explicitly learns skills that are composable to solve manipu-lation tasks. Our method is most similar to the latter family of approaches as we also rely on an MCP. However, our work differs from the pre-training objective of the MCP. While [20] pre-trains primitives from cherry-picked demonstrations\nand [24] learns them using an objective based on mutual information, we pre-train our primitives to maximize their usability on automatically generated tasks.\nb) Pre-Training and Automatic Task Generation: Many works show the difficulty of learning complex tasks with RL and propose automated curricula [5], [25], [26], [27], [28], [29] or auxiliary exploration objectives [30], [31], [32], [33], [34], [35] to learn predefined tasks. When training goal-conditioned policies, relabeling or reversing trajectories [5], [25], [26] or imitating successful demonstrations [36], [34], [35] naturally reduces the task complexity. Open-ended methods [37], [38], [39], [40], [41], [42], [43], [44] auto-matically generate increasingly complicated tasks to train an agent that discovers behaviors in an environment. Among these methods is Asymmetric Self-Play [17], [18], [19], which was shown to produce diverse tasks and solutions in the context of robotic manipulation.\nA limitation of [19] is that the trained agent uses a mono-lithic policy, which reduces the reusability of the learned behaviors in new scenarios. Our approach is most similar to [19] but differs in the fact we only use ASP as a pre-training of skills that can then be reused with HRL on novel tasks."}, {"title": "III. PRELIMINARIES", "content": "A. Reinforcement Learning and Goal-Conditioning\nWe consider an environment with a state space S, an action space A, a state transition probability p(St+1 St, at), where st \u2208 S, St+1 \u2208 S, at \u2208 A and t denotes time, and a reward function R:S\u00d7AR. It formulates a Markov Decision Process (MDP), represented as a tuple (S, A, p, R). A solution to an MDP is a policy which maps a state st \u2208 S to a probability distribution over actions, i.e. at ~ \u03c0(\u00b7st). The objective of Reinforcement Learning is to find an optimal policy \u03c0* that maximizes the expected discounted sum of rewards over a potentially infinite horizon, \u0395 [\u03a3t=0 \u03b3t R(st, at)], where \u03b3 is a discount factor.\nWhen the task is parameterized by a goal, the goal is given as input to the policy. Goal-conditioned policies, therefore, map from state st \u2208 S and goal gt \u2208 G, with the goal space G, to a probability distribution over actions, i.e. at ~ \u03c0(St, gt). In a goal-conditioned formulation, the reward function is also goal-dependent, i.e. R : S \u00d7 A \u00d7 G \u2192 R.\nB. Multiplicative Compositional Policy\nA Multiplicative Compositional Policy [20] is a policy architecture that enables an agent to activate multiple prim-itives simultaneously, where each primitive specializes in a distinct behavior. These behaviors can be composed to pro-duce a continuous spectrum of skills. The policy enables that by treating K primitives \u03c01, ..., \u03c0\u03ba as independent Gaussian distributions over actions. The composite policy is obtained by a multiplicative composition of these distributions,\n\u03c0(\u03b1\u03c2, g) = 1 / Z(8,9) * \u03a0(\u03c0i (as)wi(8,9)), wi(s,g) \u2265 0.  (1)"}, {"title": "IV. METHOD", "content": "We propose discovering diverse and composable primi-tives by pre-training an MCP to solve a large set of tasks\ngenerated through Asymmetry Self-Play. This pre-training results in a collection of primitives that capture a range of useful behaviors explicitly learned to be composable in a manipulation setting. The discovered behaviors can then be orchestrated to solve various (unseen) tasks in the environment using HRL. We present an overview of our method in Figure 1.\nA. Discovering Behaviors through Tasks Diversity\nThe core of our method is the pre-training part. It requires a task generator and a task solver. The role of the generator is to propose tasks that the solver must solve. Our method relies on the hypothesis that if the set of tasks proposed by the generator is large, diverse, and complex, then it induces the discovery of diverse and complex behaviors by the solver. While this idea is compatible with any automatic task generation framework that learns both a generator and a solver, we use Asymmetric Self-Play to discover a complex distribution of tasks.\nWe choose ASP for two main reasons. Firstly, ASP is one of the few approaches that was shown to generate a diversity of complex tasks in a robotic manipulation setting [19]. This consequence of the adversarial structure of ASP incites the generator, Alice, to seek novel and complex goals. Secondly, due to the structure of ASP, the tasks proposed by the generator are guaranteed to be feasible by the solver, Bob. More importantly, each proposed task comes with a successful demonstration, which can be used to train the solver, when it cannot succeed independently. It is called Alice Behavioral Cloning (ABC). It is of major importance in robotic manipulation to encourage proper interactions with objects. Hence, we use it during pre-training.\nOnce the automatic task generation process terminates, i.e. when the generator does not propose novel tasks anymore and the solver can solve all proposed tasks, we keep the solver for reuse with HRL on unseen downstream tasks. We hypothesize that, as the solver is exposed to a large variety of tasks, its policy embeds a diversity of behaviors necessary to solve all seen tasks. We claim that these behaviors can be reused.\nB. Embedding Behaviors in Composable Primitives\nWe learn the solver as a Multiplicative Compositional Policy to embed the whole range of discovered behaviors in primitives. We parameterize the MCP with neural networks. During pre-training, primitives learn to embed the discovered behaviors in their parameters while the orchestrator learns to compose the primitives to solve the proposed tasks. Learning the solver's policy as a monolithic policy limits the range of options for downstream tasks.\nFirstly, reusing a monolithic policy limits the range of downstream tasks to tasks with the same input space as in the pre-training phase. For instance, if downstream tasks require input information that is not part of the input space of the solver during pre-training, then the skills acquired during pre-training cannot be leveraged on downstream tasks. MCPs address this problem by separately learning an orchestrator"}, {"title": "V. EXPERIMENTAL SETUP", "content": "A. Tasks\nWe apply our method in a customized version of panda-gym [45] for pre-training and downstream task training. The environment consists of a robotic arm placed in a table-top setting where an object can be manipulated.\nSimilar environments are commonly used to evaluate Skill Learning methods for Robotic Manipulation [13], [14]. We use a Franka Emika Panda with a parallel gripper in all exper-iments. We focus on diverse manipulation tasks involving a single rigid object. Pre-training and downstream training are done in simulation only but we evaluate the transferability of our primitives to a real robotic platform on two downstream tasks. The action space corresponds to the displacement of the gripper and its finger. The state space is composed of the state of the robot and of the object. The reward function is a sparse success-based reward. The tasks consist of moving an object to a goal position. The task is considered solved when the object has achieved its target position. We consider an object has achieved its goal position if it is within a distance dthreshold = 5 cm from it. While [19] showed that ASP could be extended to tasks involving both the position and orientation of objects, we limit our study to position only as it is enough to show the benefits of our method.\nDuring pre-training, Alice and Bob manipulate a 5 cm cube. Alice proposes goals in the workspace and is penal-ized otherwise. We define the workspace as a volume of dimensions 35 x 35 x 35 cm, on the table. The initial object position is uniformly sampled in the workspace on the table.\nDownstream tasks are variations of pick-and-place tasks. We use the same workspace as in pre-training and sample initial object position similarly. Goal positions are uniformly sampled anywhere in the workspace with a probability p and otherwise on the table. The manipulated object is a cube of size 5 cm unless specified otherwise. Each downstream task perturbs the dynamics of the environment or the task distribution w.r.t. to pre-training to evaluate how a composi-tion of pre-trained primitives can adapt. In Larger, the area where initial and target object positions are sampled is of size 45 x 45 x 40 cm and p = 0.7. In In Air, target positions are always sampled in the air, i.e. p = 1.0, For Push, the gripper is blocked in a closed position, and targets are always on the table, i.e., p = 0.0, In Sphere Sphere, the object is a sphere of diameter D = 5 cm and p = 0.7, For Wall, the initial and target object positions are sampled from disjoint areas of size 15 x 35 \u00d7 0 cm separated by a fixed wall, and targets are always on the table, i.e. p = 0.7, In Box, a rigid open box of size 10 \u00d7 10 \u00d7 5 cm is welded at a random position to the table, and the goal is inside the box (p = 0.0)."}, {"title": "VI. EVALUATION", "content": "B. Baselines\nWe compare our approach to the following baselines: (a) LSD [13] with object position prior: we train the state representation function to transform the position of the object such that its transition is aligned with a latent descriptor z; (b) DADS [8] with object position prior: we train the dynamics predictor to predict the next position of the object given the current position and a continuous skill latent descriptor z; (c) DIAYN [7] with object position prior: we train the skill predictor to predict the continuous skill latent descriptor z given the position of the object; (d) ASP [19] only: we use Bob's policy from ASP pre-training only, i.e., without any training on downstream tasks; (e) Scratch MCP with Hindsight Experience Replay (HER) [5]: we train an MCP from scratch with HER; (f) Scratch MCP without HER: we train an MCP from scratch without HER; (g) Scratch Monolithic with HER: we train a monolithic policy from scratch with HER; (h) Scratch Monolithic without HER: we train a monolithic policy from scratch without HER. On downstream tasks, skills pre-trained with LSD, DADS, DIAYN, and our approach are reused with HRL. Scratch baselines are trained end-to-end and do not benefit from pre-training.\nVI. EVALUATION\nWe want to answer the following questions: (1) How do our primitives interact with objects? (2) Can our primitives be efficiently repurposed for new tasks? (3) What are the benefits of our approach compared to ASP and MCP used in isolation? (4) Can our primitives be reused in a real-world setting?\nA. Primitives Progression through Pre-Training\nWe examine the progression of our learned primitives throughout pre-training and evaluate their coverage in terms of object positions. We believe that analyzing compositions of primitives provides more insights into their behaviors than analyzing individual primitives alone. During pre-training, we qualitatively assess the evolution of our primitives by keeping checkpoints at three stages. We sample Nskill random values for the gate w \u2208 RK, which determines a fixed composition of primitives.\nAs shown in Figure 3, the primitives do not exhibit mean-ingful behaviors in early pre-training, resulting in minimal object movement. However, as pre-training progresses and proposed tasks require moving the object on the table, the primitives learn to interact with the object. Subsequently, as tasks requiring object lifting are introduced, the primitives learn to exhibit lifting behaviors. Towards the end of pre-training, after the robot has been exposed to all tasks, the learned primitives demonstrate the ability to move the object to arbitrary positions in the workspace. This observation suggests that task diversity plays a crucial role in discovering new behaviors, and these behaviors are effectively embedded in the primitives.\nTo evaluate the coverage of our primitives, we compare them to skills learned with LSD, DADS, and DIAYN. Following a similar protocol, we save the skillset only at the end of pre-training. For baselines, we sample Nskill fixed random values of the latent skill descriptor z, which are then given as fixed inputs to the skill encoder. The results of our evaluation, depicted in Figure 3, reveal that our method achieves full coverage of reachable object positions. Compositions of our primitives indeed span all reachable object positions, ensuring the ability to move the object anywhere within the workspace. Meanwhile, LSD learns skills that partially cover the space of object positions but also push and throw the object in unreachable areas, such as off the table. We believe this is a direct consequence of the intrinsic objective of LSD, which mainly encourages the discovery of dynamic behaviors. This lack of coverage and tendency to move objects off the table can be detrimental to solving downstream tasks in a table-top setting. Our method learns behaviors that are useful for tasks in a table-top setting, which encourages coverage while discouraging moving the object to irrecoverable areas. The skills learned with DADS and DIAYN exhibit few interactions with the object. In particular, DIAYN learns skills that achieve a low coverage of object positions while DADS skillset collapses to either ignoring the object or moving it along a fixed path. These limitations are likely due to the sparsity and noise of their intrinsic rewards in robotic manipulation. Attempts to address this by adding bonuses to the intrinsic rewards did not result in the learning of interactive skills. The limited coverage of object positions achieved by DADS and DIAYN"}, {"title": "VII. CONCLUSION", "content": "restricts the reusability of these skills in downstream tasks. In contrast, our method overcomes this limitation thanks to the curriculum induced by ASP and ABC, and the explicit design to promote interactions with objects.\nB. Primitives Orchestration on Downstream Tasks\nWe train orchestrators from scratch in simulation and evaluate how they learn to compose our pre-trained prim-itives to solve unseen tasks. For each task, we train a different orchestrator while using the same set of pre-trained primitives with frozen parameters. In Figure 4, we show the evolution of the success rate on these tasks for our method and baselines averaged over three random seeds. In our accompanying video, we additionally provide visualizations of each Skill Learning agent on downstream tasks. Table I reports the final achieved success rate. Most Skill Learning baselines and agents trained from scratch without HER fail on all tasks. On Larger, In Air, and Sphere, our orchestrators learn faster than all baselines, including agents benefiting from HER, and reach the highest success rates. This suggests that our primitives can be reused on tasks they were not explicitly pre-trained to solve and that they can achieve high performance with high sample efficiency. We find that LSD achieves limited performance on these three tasks, and we qualitatively observe that agents reusing skills learned with LSD can only reach goals in some areas of the workspace and do not stabilize to the target position but rather oscillate around it. In Sphere, LSD skills often fail to grasp the object stably. We suggest that this is a consequence of LSD encouraging the discovery of dynamic skills, which makes it difficult to maintain the object at a static position. DADS\nand DIAYN also achieve low success rates on these tasks. Despite the changes in dynamics induced by a new object in the environment, our approach quickly achieves high success rates on Wall and Box. Surprisingly, LSD skills achieve reasonable performance on Box despite their limited results on other tasks. We find that the oscillations of the end-effector around the target position that we observed in other tasks are alleviated by the box, which constrains the motion of the end-effector to remain inside the box. Hence, in this very specific scenario, part of the issues induced by dynamic skills are addressed by the structure of the environment. On Wall, LSD skills fail to solve the tasks, and we believe that this is due to the lack of coverage of the skillset. On Box, both DIAYN and DADS fail. However, on Wall, DADS obtains a surprisingly high score. We qualitatively observe that DADS always solves the task in a suboptimal manner by using the unique interactive behavior it discovered, by following a long trajectory to avoid the obstacle. Meanwhile, all other approaches get low success rates, if any. This shows that our primitives can be composed and used in settings with different dynamics they were pre-trained on. Push is where our approach struggles the most, while HER performs well. As the pre-trained primitives favor grasping, a blocked gripper makes them significantly less efficient in manipulating the object. LSD skills seem to suffer even more from this issue as its success rate on Push is significantly lower. This illustrates a change in dynamics where no Skill Learning method can adapt. From Table I, we observe that ASP in isolation does not achieve as good results on most tasks, especially with perturbations to the dynamics. We also observe that MCP trained from scratch in isolation performs similarly to its monolithic counterpart, if not worse. We think this is due to the more unstable nature of MCP. Overall, these results suggest that the benefits of our method do not come from ASP or MCP in isolation but from their combination, yielding the discovery of adaptable behaviors.\nWe evaluate the primitives in the real world and show the real robot using our primitives in our accompanying video. We track the position of the object with a motion capture system. We test the models trained in simulation to solve Large Area and Wall, without additional training. As we found that the learned policies tend to push the gripper against the table, we clip the action dimension of the gripper such that its position always remains slightly above the level of the table, hence avoiding collisions. We found this to be\nThis paper introduces a novel approach to learning com-posable and reusable skills in robotic manipulation. We propose to leverage Asymmetric Self-Play to generate di-verse and complex pre-training tasks and Multiplicative Compositional Policies to obtain a skill repertoire for diverse object manipulation tasks. We show the effectiveness of our method in simulated and real-world robotic manipulation scenarios on a set of downstream tasks with Hierarchical Reinforcement Learning. While showing promising results, some limitations still need to be addressed. A limitation of our approach is the object-centric observations and we would like to tackle Skill Learning from visual inputs in future work. Other future directions lie in finding better model architectures to embed and reuse the discovered behaviors."}, {"title": "VIII. APPENDIX", "content": "strictly necessary to run our trained policies on the robot safely. We diversify the initial and target positions such that the object must move in all regions of the workspace. We run 103 trials and 36 for Larger and Wall, respectively. While the wall is solid and fixed in simulation, it is made of cardboard in our real-world setup to avoid damaging the robot in case of collisions. We report the average success rate of both tasks in Table I. On the one hand, the model trained to solve Larger performs as well as in simulation. Most failures come from target positions that are at the edge of the reachable area of the end-effector, which triggers failures of the controller. This suggests that the primitives learned robust behaviors that can be transferred to real-world settings. On the other hand, the model trained to solve Wall does not perform as well. Most failures occur when the target is at the edge of the goal area or when the end-effector collides with the wall. This is a consequence of the sparse reward, which only encourages success without safety considerations. Overall, these results are encouraging as they show our primitives can be used on a real robot to solve manipulation tasks.\nVII. CONCLUSION\nA. Environment\n1) State: The state of the environment consists of the end-effector position and linear velocity, the finger positions and linear velocities, the object absolute positions, the object position relative to the end-effector, the object linear velocity, binary contact information between fingers and the object, and the number of steps taken since the beginning of the episode. Alice's policy receives the full state as input. We find that including contact information and time in Alice's inputs facilitates the discovery of novel tasks and behaviors, such as grasping objects. Since Alice is only a proxy to train Bob and is never deployed on the real robot, she can benefit from privileged information as input. As contact and velocity information would be too different between simulation and reality, we do not provide these to Bob. Bob's primitives and its orchestrator only receive positions as inputs. In addition, as in Eq. 1, the orchestrator also takes task-specific information as input, which consists of the target object position in our experiments. It ensures that\nprimitives only depend on reliable information in the real robotic platform, making the primitives more transferable to the real world. During downstream training, all agents receive only positions and goals as input.\n2) Action: The action is four-dimensional, comprising both the desired displacement of the end-effector and the change in width between the fingers. Actions are scaled from [-1,+1] to [-5, +5] cm for the end-effector and to [-10,+10] cm for finger displacements. Reference end-effector positions are obtained by applying the displacements to the current positions. During downstream training, the size of the action space of the orchestrator is the number of primitives or the size of the latent space for Skill Learning baselines.\n3) Reward: We use sparse rewards. During pre-training, we use a comparable adversarial reward structure as used in [19]. On the one hand, Bob is rewarded for solving the task at the moment the task is solved. On the other hand, Alice is rewarded at the end of each episode and gets a reward valid = +1 if it proposes a valid goal. A goal position is valid if it is defined in the workspace, and if such position is different from the initial position of the object. If the goal is valid, Alice gets an additional reward difficult = 5 if Bob fails to solve the task or difficult 0 if Bob succeeds. During downstream training, the agent is rewarded at every step where an object is on its target.\nB. Implementation Details\nFor pre-training, we use Proximal Policy Optimization (PPO) [46] to update Alice's and Bob's policies as in [19]. We re-implemented PPO as we needed to articulate it with ABC for Bob. For downstream training, we use the im-plementation of Soft Actor-Critic [47] from Stable Base-lines3 [48], for a higher sample efficiency, and compatibility with HER.\nTo ensure a fair comparison to Skill Learning methods, we set the dimensionality of their skill latent space equal to the number of primitives K. We use K = 4 for all experiments as we did not find that a higher number helped significantly on the evaluated tasks."}]}