{"title": "The Factuality Tax of Diversity-Intervened Text-to-Image Generation: Benchmark and Fact-Augmented Intervention", "authors": ["Yixin Wan", "Di Wu", "Haoran Wang", "Kai-Wei Chang"], "abstract": "Prompt-based \"diversity interventions\" are commonly adopted to improve the diversity of Text-to-Image (T2I) models depicting individuals with various racial or gender traits. However, will this strategy result in nonfactual demographic distribution, especially when generating real historical figures? In this work, we propose DemOgraphic FActualIty Representation (DoFaiR), a benchmark to systematically quantify the trade-off between using diversity interventions and preserving demographic factuality in T2I models. DoFaiR consists of 756 meticulously fact-checked test instances to reveal the factuality tax of various diversity prompts through an automated evidence-supported evaluation pipeline. Experiments on DoFaiR unveil that diversity-oriented instructions increase the number of different gender and racial groups in DALLE-3's generations at the cost of historically inaccurate demographic distributions. To resolve this issue, we propose Fact-Augmented Intervention (FAI), which instructs a Large Language Model (LLM) to reflect on verbalized or retrieved factual information about gender and racial compositions of generation subjects in history, and incorporate it into the generation context of T2I models. By orienting model generations using the reflected historical truths, FAI significantly improves the demographic factuality under diversity interventions while preserving diversity.", "sections": [{"title": "Introduction", "content": "A large body of previous works has explored social biases in Text-to-Image (T2I) models-for instance, models could follow social stereotypes and tend to generate male \u201cdoctors\u201d and female \"nurses\" (Bansal et al., 2022a; Naik and Nushi, 2023; Bianchi et al., 2023; Wan and Chang, 2024; Wan et al., 2024). To resolve this issue, several studies propose prompt-based \u201cdiversity interventions\" that effectively instruct T2I models to generate images with gender and racial diversity (Bansal et al., Here, we define \u201cdemographic factuality\" as the faithfulness to the real racial or gender distribution among individuals in historical events. Despite the rising popularity of T2I models and diversity interventions, systematic research on the factuality of diversity-intervened T2I generations is still preliminary: (1) there lacks an evaluation benchmark to quantify the severity of nonfactual generations, and (2) no previous work proposed effective solutions to strike a balance between diversity and factuality.\nTo bridge this gap, we construct DemOgraphic FActuality Representation (DoFaiR), a novel benchmark to measure the trade-off between demographic diversity and historical factuality of T2I models' depictions of individuals in historical events. As shown in Figure 2, DoFaiR first prompts models to generate images containing a representative participant class in real historical events. Then, an automated pipeline is used to obtain the demographic distribution in generated images. Finally, the generated demographic distribution is evaluated against the ground truth demographic distribution to determine the generation's factuality level. To construct the ground truth labels in the form (historical event, participant class, demographic distribution), we design an innovative data construction pipeline incorporating knowledge-enhanced fact-labeling to extract verifiable event-specific and participant-specific demographic information from Wikipedia documents (Figure 3).\nThe finalized DoFaiR benchmark consists of 756 records with information on different historical events, representative participant classes involved, and corresponding ground-truth demographic information, including (1) dominant race/gender and (2) involved racial/gender groups. Utilizing DoFaiR, we thoroughly evaluated two recent T2I models: DALLE-3 (OpenAI, 2023) and Stable Diffusion (SD) (Rombach et al., 2021). Surprisingly, the results revealed a remarkable factuality tax: on average, two previous diversity-oriented intervention methods result in a 181.66% increase in the divergence from real diversity levels in historical event participants, at the cost of decreasing the factuality accuracy by 11.03%.\nIn response, we propose Fact-Augmented Intervention (FAI), which guides T2I models for demographic factuality by synergizing knowledge sources with relevant historical information. We consider two types of factual knowledge sources: verbalized knowledge from a strong LLM, and retrieved knowledge from Wikipedia. During inference, the knowledge is incorporated to construct factuality-enriched image generation instructions. Experiments show that FAI significantly improves demographic factuality: compared with un-augmented diversity intervention outcomes, FAI-RK achieves over 22% improvement in factuality correctness of involved racial groups, and over 10% improvement in dominant race factuality,\nOur DoFaiR benchmark pioneers systematic investigation of demographic factuality in T2I models, and provides valuable resources for future studies on evaluating and mitigating the factuality \"tax\" paid by diversity interventions. We will publicly release the code and benchmarking data at https://github.com/uclanlp/diverse-factual.git."}, {"title": "The DoFaiR Benchmark", "content": "We propose the first-of-its-kind DemOgraphic FActuallty Representation (DoFaiR) benchmark to measure the critical trade-off between demographic diversity and factuality in T2I model generations. DoFaiR consists of 756 major participant classes involved in real historical events, as well as the corresponding demographic distribution for each participant class. We further divide DoFaiR into 2"}, {"title": "Dataset Construction", "content": "We design an automated and systematic data construction pipeline with retrieval-based fact labeling. An illustration of the data construction framework is demonstrated in Figure 3. Full prompt templates used are shown in Appendix A Table 4."}, {"title": "Event and Participant Class Sampling", "content": "Raw Data Generation with Descriptor-Based Seed Prompts. To begin with, we sample historical events and the corresponding participant class. To ensure data balance, we use template-based prompting to iterate through seed descriptors specifying different time periods, cultures, and dominant demographic groups involved:\nEvent: Generate 10 famous historical events during {time period} in {culture}, in which the majority people involved are of the {race/gender} group.\nGroup: For each event, also generate 3 participant class in the event.\"\nUsing verbalized prompts with different combinations of seed descriptors in Appendix A Table 3, we query the gpt-4o-2024-05-13 model to generate historical events and corresponding roles. After cleaning and re-sampling, we obtain 848 race-related entries and 262 gender-related entries. Specific prompting and information extraction strategies are presented in Appendix A, Table 4."}, {"title": "Demographic Fact Retrieval", "content": "Next, we determine the ground truth demographic distributions among involved participants in the historical events in generated entries. We adopt a retrieval-based automated pipeline to obtain demographic ground truths. We decompose the demographic labeling process into (1) query construction, (2) Wikipedia search, and (3) dominant and involved demographic groups labeling for different roles associated with different events. We begin with constructing both heuristic-based queries and additional LLM-generated queries about the participants' demographic information. Next, we used each query to independently retrieve the top 5 chunks of supporting documents from the top 10 Wikipedia passages on the dominant demographic groups and involved demographic groups for all events. Details on retrieval query construction and the retrieving process are provided in Appendix A."}, {"title": "Demographic Fact Labeling", "content": "We utilize the retrieved documents to conduct factual knowledge-augmented labeling on demographic information. Specifically, we employed gpt-4o-2024-05-13 model to use retrieved documents for labeling factual ground truths of (1) the dominant demographics (race/gender) and (2) involved demographics (race/gender) among the corresponding roles in the historical event. Details of labeling strategies are provided in Appendix A."}, {"title": "Final Dataset Statistics", "content": "We take further measures after the fact labeling loop to clean and re-sample the constructed data to ensure balance and quality, as detailed in Appendix A.3. The final dataset consists of 756 entries, with"}, {"title": "Human Verification", "content": "To further validate the factuality of the constructed dataset, we invited two volunteer expert annotators to manually verify the dominant and involved demographic groups in 100 generated entries for DoFaiR-Race and 30 for DoFaiR-Gender. Human-verified correctness of the constructed data and Inter-Annotator Agreement scores are reported in Table 1. Annotator details and instructions provided to the two annotators are in Appendix B.\nFactual Correctness The overall average factual correctness of the constructed dataset across the 2 annotators is 92.92%, proving the high quality of collected data. For gender-related data, the factual correctness is 93.33% for annotator 1 and 95.00% for annotator 2. For race-related data, the factual correctness is 92.50% for both annotators.\nInter-Annotator Agreement We calculate and report the Inter-Annotator Agreement (IAA) between the two annotators. Cohen's Kappa Score reports 1.00 for annotations of both the dominant racial groups and the involved racial groups in DoFaiR-Race entries, showing a perfect agreement between annotators. For dominant gender groups, Cohen's Kappa Score is approximately 0.84, indicating substantial agreement. For human verification on the lists of involved gender groups in DoFaiR-Gender, both annotators labeled 100% of the entries as \"factual\", resulting in a lack of variance in annotations and therefore prohibiting the calculation of meaningful IAA scores. However, the perfect agreement between the annotators indicates the high quality of the constructed data in this dimension as well."}, {"title": "Evaluating the Factuality Tax of Diversity", "content": "Using the constructed dataset, DoFaiR evaluates the demographic factuality and diversity of a T2I system's output simultaneously. Specifically, we introduce a 3-step evaluation pipeline: classifying demographic traits for each face, aggregating demographic distributions in each image, and comparing with the ground truth fact-labeled distribution."}, {"title": "Gender and Racial Trait Classification", "content": "We follow previous studies that measure gender and racial diversity on T2I models (Friedrich et al., 2023, 2024; Naik and Nushi, 2023) to use the pre-trained FairFace classifier (K\u00e4rkk\u00e4inen and Joo, 2019) for identifying demographic traits. The FairFace framework first detects human faces from generated images, and then annotates the race and gender characteristics of each face. There are 7 racial groups (White, Black, Indian, East Asian, Southeast Asian, Middle Eastern, Latino) and 2 genders (Male, Female) in FairFace's label space. After aggregating FairFace results, we can obtain the racial and gender distribution of all faces detected in model images, which we use to compare with ground truth demographic distributions."}, {"title": "Evaluation Metrics", "content": "DoFaiR features four metrics to comprehensively evaluate both demographic factuality and diversity level in model generations. To allow for easier illustration of metric calculations, we introduce the following notations:\n\u2022 Dominant Demographic (DD): the most-occurring race or gender, as identified by FairFace, in a generated image.\n\u2022 Non-dominant Demographic (ND): other races or gender, besides the most-occurring one in a generated image.\n\u2022 Involved Demographic (ID): all races or genders that are present in a generated image.\n\u2022 Uninvolved Demographic (UD): other races or genders, besides the present ones in a generated image."}, {"title": null, "content": "\u2022 All Possible Demographic (APD): the label space of all possible races or genders.\nDominant Demographic Accuracy (DDA) is defined as the accuracy of the dominant demographic group(s) in generated images, compared with the ground truth:\n$DDA = Avg_{imgs}(\\frac{\\text{#True DD + # True ND}}{\\text{# APD}})$\nA higher DDA score indicates more factual depictions of dominant demographics in model-generated images.\nInvolved Demographic Accuracy (IDA) Similar to DDA, We define IDA as the accuracy of the depicted demographic groups in generated images:\n$IDA = Avg_{imgs}(\\frac{\\text{# True ID + # True UD}}{\\text{# APD}})$\nA higher IDA score indicates more factual depictions of involved demographics in generations.\nInvolved Demographic F-1 We introduce the Involved Demographic F-1 Score (IDF) metric as the weighted F-1 score for involved and non-involved demographic groups:\n$IDF = Avg_{imgs} (\\frac{2 \\cdot \\text{#True ID}}{(2\\cdot \\text{#True ID}+ \\text{#False ID}+ \\text{#Missing ID})} + \\frac{2 \\cdot \\text{#True UD}}{(2\\cdot \\text{#True UD}+ \\text{#False UD}+ \\text{#Missing UD})})$\nA higher IDF score indicates better adherence to ground-truth demographic distributions, and thus more factual generations.\nFactual Diversity Divergence (FDD) We define the FDD metric, which quantifies the divergence in the level of demographic diversity in model generations compared with the factual ground truth. We calculate diversity as the proportion of represented demographic groups in an image relative to the total number of conceivable groups (e.g., 7 racial categories, 2 gender categories). Then, the FDD score can be calculated as:\n$FDD = Avg_{imgs} (\\frac{\\text{# Image ID - #Ground Truth ID}}{\\text{# APD}})$\nAn FDD score that is closer to O indicates better adherence to the level of diversity in ground-truth demographic distributions, and higher factuality."}, {"title": "The Factuality Tax of Diversity Interventions", "content": "Using the DoFaiR benchmark, we conducted experiments to evaluate the trade-off between demographic diversity and factuality in T2I models."}, {"title": "Experimental Setup", "content": "Models We evaluate two popular T2I systems: DALLE-3 (OpenAI, 2023) and Stable Diffusion v2.0 (Rombach et al., 2021). For DALLE-3, we followed the default setting in OpenAI's API documentation, with the image size set to \u201c1024 \u00d7 1024\". We implemented Stable Diffusion using the StableDiffusionPipeline, using the EulerDiscreteScheduler, in the transformers library.\nImage Generation Given one data entry in DoFaiR, which provides (1) a historical event and (2) an involved participant class, we query both T2I models to generate an image of the group. Since we use an automated FairFace framework to identify and classify the demographics based on the faces of generated individuals, we instruct the model to generate clear faces using the prompt:\n\"Generate an image depicting faces of the representative people among the {participant class} in {event name} .\"\nDiversity Intervention We experimented with 2 diversity intervention prompts Bansal et al. (2022a) and Bianchi et al. (2023)'s works:\n\u2022 Bianchi et al. (2023) (adapted): \"from diverse gender / racial groups.\"\n\u2022 Bansal et al. (2022a): \u201cif all individuals can be a {participant class} irrespective of their genders / skin color or races.\u201d"}, {"title": "Results", "content": "Experiment results on the 4 proposed quantitative metrics are presented in Table 2.\nObservation 1: Both diversity intervention prompts boost demographic diversity at remarkable costs of factuality.\nComparing the reported scores in \u201cBaseline\u201d results and the two \u201cdiversity intervention\" results for both models, we observe a notable positive increase in the FDD metric, indicating a rise in demographic diversity in model-generated images that results in a greater divergence from the ground truth diversity level. At the same time, we capture that factuality-indicative scores\u2014DDA, IDA, and IDF-decrease remarkably after applying diversity intervention. This indicates a strong trade-off of demographic factuality for diversity.\nObservation 2: Models achieve lower demographic factuality for racial groups in historical events. On the DDA, IDA, and IDF metrics, both models perform worse in being factual to historical racial distributions than gender distributions. Additionally, both models have higher racial FDD scores than for gender, indicating a greater false divergence from factual racial diversity levels.\nObservation 3: Models are less capable of accurately depicting factual involved demographics. Comparing DDA and IDA, we discover that IDA scores for both gender and racial groups are lower than DDA scores for both models. It is more challenging for models to identify and reflect the factual involved demographic group in generations.\nHow does Diversity Interventions Influence Factuality Behavior? An In-Depth Analysis Figure 5 visualizes detailed behavioral changes in the involved demographic factuality accuracy of model generations on the same evaluation subject (i.e. the tuple with event, participant class, and ground truth demographic information) after applying diversity interventions. Results are averaged over the 2 types of intervention prompts experimented. On DoFaiR-Gender, 38.6% of all cases experienced an increase in diversity level after applying the intervention prompt, among which 68.13 % (therefore 26.3% overall) also witnessed a decrease in factuality. The influence of intervention prompts on generation di-versity is more obvious on DoFaiR-Race, where 54.7% of all cases witness a higher diversity level, among which 63.8% came with a decrease in factuality. Overall, we observed a high co-occurrence of increased diversity and decreased factuality when applying diversity interventions, indicating a remarkable trade-off between being diverse and being factual in image generations."}, {"title": "Fact-Augmented Interventions", "content": "The above experiment results demonstrate T2I models' lack of ability to understand and depict factual demographic distributions among historical figures in images, especially under diversity interventions. To resolve this issue, we first explore if the Chain-of-Thought (CoT) reasoning (Wei et al., 2022) from a LLM helps improve the demographic factuality in T2I generations: \u201cThink step by step.\u201d Experiment results in the \u201c+CoT\" rows in Table 2 reveal the limitation of the current CoT approach. The second column of images in Figure 6 shows a qualitative example, in which CoT fails to improve gender factuality. We highlight that the root of CoT's failure is due to the lack of factuality orientation in its reasoning direction: even if the reasoning steps specifically identified that only males were involved in the event, the CoT model could be easily affected by the diversity intervention and begin to plan out ways to falsely modify this historical fact."}, {"title": "Proposed Method", "content": "Based on the empirical insights, we introduce Fact-Augmented Intervention (FAI), a novel methodology to guide the intervention of T2I models with factual knowledge. Inspired by prior works on knowledge verbalization (Yu et al., 2023), in which high-quality knowledge could be elicited from LLMs, and retrieval-based extraction of web knowledges (Lewis et al., 2020), we experiment with 2 types of knowledge augmentation for FAI: Verbalized Factual Knowledge from a strong LLM, and Retrieved Factual Knowledge from reliable Wikipedia sources. We denote the FAI method using the two different factuality augmentation approaches FAI-VF and FAI-RF, respectively.\nFAI with Verbalized Knowledge FAI-VK utilizes a strong intermediate LLM to ellaborate on precise factual knowledge about the demographic distribution of the participants in historical events to be depicted. By augmenting the intervention prompt for T2I models with this verbalized factual knowledge, FAI-VK guides the T2I model toward factual demographic distribution as the example shown in Figure 6.\nFAI with Retrieved Knowledge FAI-RK directly leverages related historical documents retrieved from verified Wikipedia data sources, retrieved with the same query construction process as in Section 2.1.2, to provide precise and detailed factuality guidance for T2I models. In our experiments, we utilize an intermediate LLM to interpret all retrieved factual documents, which are related to demographic information about the participants in historical events to be depicted, and augment the image generation prompt with factual instructions. An example is shown in Figure 6."}, {"title": "Experimental Results", "content": "Setup We apply FAI-VK and FAI-RK on the T2I models evaluated in the previous section. Both methods are applied in conjunction with diversity interventions to evaluate their effectiveness in augmenting demographic factuality under the influence of diversity instruction prompts. In our preliminary experiments, SD failed to output meaningful images with the prolonged input potentially due to the weak long-context comprehension ability of the model since it was not trained on large language corpuses (example failure cases in Appendix D). Therefore, we only experimented with DALLE-3 for augmented intervention approaches.\nObservation 1: Both FAI-VK and FAI-RK are effective in mitigating the factuality tax of diversity interventions. Experiment results in Table 2 show that both proposed methods remarkably improve the factuality of the generated images by DALLE-3 at inference time, surpassing the performance of CoT. For instance, averaged on gender and race across 2 intervention prompts, FAI-RK achieves an average of 9.30% improvement in IDA over CoT. We also present qualitative examples in Figure 6: both FAI-VK and FAI-RK methods successfully augmented the intervened generation prompts with factual knowledge, guiding the T2I model to retain demographic factuality under the influence of diversity instructions.\nObservation 2: Demographic Factuality Under FAI Outperforms the Baseline Outcome. Furthermore, the level of quantitative factuality in images generated using FAI augmentation surpasses the factuality level of the baseline setting, where no"}, {"title": "Related Work", "content": "Bias in T21 Models A large body of works has explored different aspects of biases in T2I generation models. Naik and Nushi (2023); Zameshina et al. (2023); Zhang et al. (2023); Wan and Chang (2024) investigated gender biases in T2I generations, such as depicting a male \u201cCEO\u201d and a female \"assistant\" (Wan and Chang, 2024). Bansal et al. (2022a); Bianchi et al. (2023); Naik and Nushi (2023); Zhang et al. (2023); Luccioni et al. (2023); Bakr et al. (2023) discovered the reinforcement of racial stereotypes in T2I generations, such as depicting white \"attractive\u201d individuals and \"poor\" people of color. Wan et al. (2024) systematically surveyed and categorized additional related works in different bias dimensions.\nDiversity Intervention Approach for Bias Mitigation A number of previous studies have explored the use of \"ethical interventions\u201d, or diversity instructions, to mitigate gender and racial biases in T2I models (Bansal et al., 2022a; Fraser"}, {"title": "Conclusion", "content": "In this work, we developed the DoFaiR benchmark to reveal the factuality tax of applying diversity interventions to T2I models. Through evaluation experiments on DALLE-3, we highlight the challenges of aligning T2I models with human values of fairness using the one-for-all intervention method, demonstrating how such approaches that lack careful consideration can fail in preserving historical factuality. To resolve this trade-off, we propose the FAI method to augment image generation instructions with factual historical knowledge. Specifically, we devise 2 types of knowledge augmentation methods: FAI-VK, which uses knowledge elicited from a strong LLM, and FAI-RK, which adopts a retrieval-based knowledge extraction pipeline. Our proposed benchmark pioneers the holistic analysis of the factuality problem in diversity-driven T2I systems, and our method paves a path forward for developing techniques that preserve factual demographic distributions when specifically prompted to depict diversity."}, {"title": "Limitations", "content": "We hereby identify several limitations of our work. Firstly, this work only experimented with the English language. Second, due to the high cost of (1) querying GPT-40 for knowledge verbalization and retrieved knowledge summarization, and (2) DALLE-3's API for image generation, we were only able to conduct evaluation experiments on a proportion of the large-scale fully constructed data (with 3,809 race-related entries and 3,932 gender-related entries, as elaborated in Section 2.1). We acknowledge this limitation due to computational constraints. We also note that Google paused Gemini's image generation of people. Therefore, we cannot evaluate their T2I model. During our experiments, we did our best to ensure that the data sampled for experiments were balanced and were sizeable enough to produce meaningful experiment results. Third, since the generated images contain a large number of depicted faces with various demographic traits, we adopted an automated demographic classification approach using the FairFace classifier to identify gender and racial distributions in model generations. We hope to stress that the notation of \"race\" and \"gender\" in this study is not the self-identified social identities of individuals depicted, but rather the demographic traits demonstrated in synthesized images."}, {"title": "Ethics Statement", "content": "Experiments in this study employ Large Text-to-Image generation models, which have been shown by various previous works to contain considerable biases. We acknowledge that model generations can be biased and carry social stereotypes, and would like to highlight that the purpose of using such models is to unveil the underlying trade-off problem between diversity intervention and factuality. Future studies should consider exploring if social biases persist in model-generated images, and compare between bias extents in factual and non-factual outputs."}, {"title": "Dataset Details", "content": "In this section, we provide additional details of dataset construction."}, {"title": "Event and Involved Group Sampling", "content": "Descriptor-Based Seed Prompts We sample historical events and specific groups of people involved. To ensure the balance of data entries, we adopt template-based prompts that iterate through descriptors specifying different time periods, cultures, and dominant demographic groups involved. The prompt template used is shown in the first row of Table 4. Lists of seed descriptors are in Table 3."}, {"title": "Fact Retrieval", "content": "We adopt an automated pipeline to label demographic facts. wE decompose the demographic labeling process into (1) constructing effective retrieval queries tailored for desired information, (2) retrieving related documents from reliable Wikipedia sources, and (3) using retrieved documents to label the dominant demographic groups and involved demographic groups for different events.\nQuery Construction We adopt the gpt-4o-2024-05-13 model to automatically construct the queries for retrieving related documents. For a data entry with a historical event and a group of people specified, we hope to know (1) the dominant demographic group\u2014race or gender\u2014among the group of people, and (2) all involved demographic groups, i.e. which races/genders were part of the group in the event. Therefore, we construct queries to retrieve supporting documents to answer these two questions, respectively. To allow for easier parsing of output contents, we again control the model's output format to be json. Furthermore, we manually draft in-context examples of queries for a piece of seed data to better guide the model to output useful queries. Prompts and in-context examples used are shown in the \"Fact Retrieval\" rows in Table 4. Additionally, to search for related information about whether each racial/gender group was among the group in historical event, we include extra queries specifying each demographic group, in the format of: \u201cWere there any {race/gender} people among the {group} in the {event name}?\u201d"}, {"title": "Demographic Fact Labeling", "content": "Fact Labeling We utilize the retrieved documents to conduct fact-labeling on demographic information."}, {"title": "Human Verification Details", "content": "The following section outlines the human verification process conducted as part of our study, including detailed annotator instructions. The annotators are volunteering college students who are fluent in English and familiar with NLP research. Each annotator independently labeled 100 randomly sampled data entries from DoFaiR-Race and 30 entries from DoFaiR-Gender."}, {"title": "Citation and Reference Check", "content": "The LLM used for fact labeling, the GPT-40 version of ChatGPT, provides citations for its responses, indicating which part of which document supports its answer. Annotators are instructed to verify if LLM's citations correctly reference the supporting documents, check if the answers found in the documents match LLM's output and finally note discrepancies where LLM's citations do not support its answers or are incorrect."}, {"title": "Search for Data", "content": "If initial searches do not yield sufficient data to support or refute the races identified by LLM, annotators are instructed to search for related historical and geographical contexts and verify the absence of certain races in specific contexts, ensuring accuracy in annotations."}, {"title": "Verification Step", "content": "For each entry, refer to the dominant_factcheck_docs to locate documents that support or refute LLM's identified dominant race dominant_race_chatgpt. Annotate the dominant_race_chatgpt_correctness column with True if the LLM's response matches the information in the documents; otherwise, annotate with False. The same process applies to existence_race_chatgpt, dominant_gender_chatgpt, and existence_gender_chatgpt."}, {"title": "Details on FAI Approaches", "content": "To allow for easy reproduction of the proposed FAI approaches, we provide full prompts for both FAI methods in Table 6. Both FAI methods augment the diversity-intervened image generation prompt with factual knowledge. For FAI-VK, we use these prompts to query a strong intermediate LLM to verbalize its knowledge about the factual demographic distribution among the historical groups. For FAI-RK, we retrieve related factual documents from Wikipedia sources and use an intermediate LLM to summarize knowledge in the retrieved documents. Outputs of the intermediate LLMs are then concatenated to the diversity-intervened prompts to query T2I models for factuality-augmented image generation."}, {"title": "Impact of Long Context with Chain-of-Thought (CoT) on Stable Diffusion", "content": "We observed that the introduction of CoT and FAI methods to augment image generation prompts caused severe degradation in the quality of the images generated by the Stable Diffusion model. This degradation manifested as various artifacts that obstruct the identification of individuals depicted and were not present in the control images generated without these augmentations. For example, Figure 8 shows distorted features, unnatural colors, and incoherent elements in the generated images.\nDue to the degraded quality of the images, the FairFace classifier struggled to detect faces and assess demographic traits. Therefore, we did not proceed with Stable Diffusion for intervention-augmented experiments."}, {"title": "Qualitative Examples", "content": "Table 7 provides a number of qualitative examples of how proposed FAI methods improve demographic factuality. Compared to the diversity-intervened generation with no augmentation, FAI achieves both racial and gender factual correctness."}]}