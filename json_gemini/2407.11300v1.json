{"title": "Large Vision-Language Models as Emotion Recognizers in Context Awareness", "authors": ["Yuxuan Lei", "Dingkang Yang", "Zhaoyu Chen", "Jiawei Chen", "Peng Zhai", "Lihua Zhang"], "abstract": "Context-aware emotion recognition (CAER) is a complex and significant task that requires perceiving emotions from various contextual cues. Previous approaches primarily focus on designing sophisticated architectures to extract emotional cues from images. However, their knowledge is confined to specific training datasets and may reflect the subjective emotional biases of the annotators. Furthermore, acquiring large amounts of labeled data is often challenging in real-world applications. In this paper, we systematically explore the potential of leveraging Large Vision-Language Models (LVLMs) to empower the CAER task from three paradigms: 1) We fine-tune LVLMs on two CAER datasets, which is the most common way to transfer large models to downstream tasks. 2) We design zero-shot and few-shot patterns to evaluate the performance of LVLMs in scenarios with limited data or even completely unseen. In this case, a training-free framework is proposed to fully exploit the In-Context Learning (ICL) capabilities of LVLMs. Specifically, we develop an image similarity-based ranking algorithm to retrieve examples; subsequently, the instructions, retrieved examples, and the test example are combined to feed LVLMs to obtain the corresponding sentiment judgment. 3) To leverage the rich knowledge base of LVLMs, we incorporate Chain-of-Thought (CoT) into our framework to enhance the model's reasoning ability and provide interpretable results. Extensive experiments and analyses demonstrate that LVLMs achieve competitive performance in the CAER task across different paradigms. Notably, the superior performance in few-shot settings indicates the feasibility of LVLMS for accomplishing specific tasks without extensive training.", "sections": [{"title": "1. Introduction", "content": "As a crucial factor in mutual understanding, friendly communication, and maintaining long-term relationships, emotions play a vital role in our daily lives. In recent years, emotion recognition tasks have garnered increasing attention. Researchers are striving to identify and analyze the complex emotions expressed by humans from various modalities such as vision, language, and speech. So far, this technology has been applied in numerous fields (Pepa et al. (2021); Yang et al. (2023b)). In the past, research on visual emotion analysis has primarily concentrated on facial expressions Jiang et al. (2020), with most facial recognition datasets providing cropped facial images. However, this does not fully align with the way humans perceive emotions through visual cues. Typically, in natural scenes, we do not only observe a person's facial expressions but also their body language (such as gestures and postures), the environment, and their interactions with others. These elements collectively contribute to our assessment of emotional states. Kosti et al. (2019) recognized this aspect and introduced the task of Context-Aware Emotion Recognition (CAER). They combined contextual and environmental factors in static images to construct and release the EMOTIC dataset, making a significant contribution to the field of CAER. Due to the complexity and subjectivity of human emotions, fine-grained emotion recognition is challenging even for human experts. Different individuals may interpret the emotional context of the same image differently. Therefore, it can be said that there is no exact correct answer in emotion recognition. Traditional CAER models (Kosti et al. (2017); Lee et al. (2019); Zhang et al. (2019)) are typically trained and tested on fixed datasets. It raises issues of generalization, as the knowledge capacity of traditionally trained models is limited to the specific datasets they are trained on. Additionally, CAER models trained on particular datasets may learn the annotators' subjective emotional bias biases, which could cause confusion for others. Second, in real-world scenarios, it is not always feasible to obtain large amounts of labeled data. When a model is transferred to an unseen sentiment domain, collecting substantial data and retraining the model incur significant costs.\nLarge Language Models (LLMs) (Touvron et al. (2023); Achiam et al. (2023)) have learned human thought processes from billions of data points. Their massive parameter size translates to a vast knowledge capacity, enabling them to generalize well to different downstream tasks even in few-shot or zero-shot scenarios. Large Vision-Language Models (LVLMs) (Liu et al. (2024a); Lin et al. (2024)) combine the capabilities of LLMs with visual understanding, allowing the powerful abilities of these models to be transferred to the visual domain. By learning the correlation between vision and language from extensive image-text data, LVLMs have achieved remarkable results in tasks such as image classification (Radford et al. (2021)), object segmentation (Zhang et al. (2024)), and visual question answering (VQA)(Alayrac et al. (2022)). However, as far as we know, the exploration of LVLMs in the CAER task remains relatively underdeveloped.\nThis paper aims to explore the potential of leveraging various paradigms of LVLMs to enhance the CAER task. Specifically, we first perform supervised fine-tuning (SFT) on the LVLMs, which is the most common way for generalizing large models to downstream tasks. However, in real-world scenarios, it is often challenging to obtain large amounts of labeled data, and the knowledge distribution of fine-tuned models may be disrupted. Therefore, we extend our investigation to include zero-shot and few-shot paradigms. For the few-shot setting, we aim to leverage the In-Context Learning (ICL) (Brown et al. (2020)) capability of LVLMs to achieve emotion recognition with small samples. This paradigm allows LVLMs to improve their emotion recognition performance by learning from a few examples within the context, without updating the model parameters. During the few-shot inference, LVLMs do not rely on learning the distribution of the dataset to make predictions. Instead, it generates decisions by learning from relevant tasks and leveraging its own extensive knowledge, which avoids the impact of annotation biases inherent in the dataset. It has been provided that downstream performance is highly sensitive to the choice of in-context demonstrations by Zhang et al. (2024), and a good in-context demonstration should be semantically similar to the query. To achieve this, we propose a training-free framework to fully exploit the ICL capability of LVLMs. Within our framework, we design a demonstration retrieval module based on different contexts to retrieve the top-k examples most similar to the test sample. This allows us to fully consider various aspects of the images when selecting example samples. Then, we construct a prompt template that packages the top-k selected examples ranked by similarity into the template. Finally, we concatenate the instructions, demonstrations, and target test sample to serve as input to the LVLMs and obtain the corresponding sentiment judgment.\nFurthermore, to fully leverage the internal knowledge of large models, we extend the proposed framework to a few-shot Chain-of-Thought (CoT) paradigm. We utilize GPT-4 Vision's (Achiam et al. (2023)) powerful instruction-following capability to generate CoT rationales for each demonstration. These rationales are concatenated with the corresponding example images and provided as input to guide the model in using its rich knowledge for reasoning and simultaneously providing interpretable results.\nOur primary contributions are summarized below. (1) This paper synthetically explores the potential and promise of various paradigms of LVLMs applied to the field of traditional visual emotion recognition. (2) We design a training-free framework to effectively apply the ICL paradigm to the CAER task. The proposed demonstration retrieval module retrieves demonstrations with the highest semantic similarity to the current test image from both the person context and scene context. This approach fully harnesses the ICL capability of LVLMs. (3) We conduct extensive experiments and perform a comprehensive analysis of the results to elucidate the impact of different paradigms on the emotion analysis performance of LVLMs. Our findings show that large models, even without parameter updates, can achieve competitive results in the CAER task, even surpassing traditional methods."}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Context-aware Emotion Recognition", "content": "The expression of emotions is often multifaceted. In visual emotion recognition tasks, facial expressions are commonly regarded as the most expressive visual information for conveying emotions, and much of the existing work has focused on facial expression analysis (Jiang et al. (2020)). However, in uncontrolled natural scenes, human emotions typically need to be inferred from a combination of visual information, including facial expressions, body movements, interactions, and the surrounding context. Recently, there have been numerous attempts to address context-aware emotion recognition tasks. Kosti et al. (2019) introduced the EMOTIC dataset to encourage and support the Context-Aware Emotion Recognition (CAER) task, proposing a dual-stream convolutional architecture to separately extract in-formation from human bodies and the entire scene. Similarly, Lee et al. (2019) proposed a dual-stream architecture, but in contrast to EMOTIC, one branch is used to extract fa-cial features while the other branch processes the entire image with the facial information masked as context. Yang et al. (2022) released the HECO dataset for the CAER task and presented a novel multi-stream emotion recognition framework that incorporates four con-text information. Bose et al. (2023) utilizes pre-trained vision-language models (VLMs) to extract descriptions of foreground context from images and proposes a multimodal context fusion module that combines foreground cues with visual scenes and human-centered con-textual information for emotion prediction. Mittal et al. (2020) considered multiple signals such as facial expressions, postures, backgrounds, and depth to predict specific human emo-tions. Yang et al. (2023a, 2024) introduced the causal modeling patterns to address spurious correlations between context and emotions caused by harmful context biases.\nAlthough the above works have shown good performance on the CAER task, their capabilities are ultimately limited to the training datasets, resulting in poor generalization in real-world scenarios. Unlike these traditional training-based methods, we aim to explore the performance of various paradigms of LVLMs in the CAER task, seeking better approaches to overcome the limitations of traditional methods."}, {"title": "2.2. Large Models in Affective Analysis", "content": "As the scale of pre-trained language models continues to expand, the comprehension and generation capabilities of LLMs are rapidly advancing at an astonishing pace. Recently, several works have focused on exploring the emotion analysis capabilities of these large models. Lei et al. (2023) designed a retrieval template module and two emotional align-ment tasks, utilizing fine-tuning to reform emotion recognition in conversation (ERC). Sim-ilarly, Zhang et al. (2023) had contributed to the development of large models in the ERC task. They fine-tuned the models using conversational context and emotional knowledge and incorporated multimodal information by converting videos into text for the fine-tuning process. Liu et al. (2024b) proposed a series of annotation tools for comprehensive affective analysis based on fine-tuning various LLMs with instruction data.\nThe aforementioned works are based on LLMs and primarily focus on emotion analysis in the field of NLP. Zhao and Patras (2023) proposed DFER-CLIP, which is based on the CLIP (Radford et al. (2021)) model and specifically designed for dynamic facial expres-sion recognition (DFER) in the wild. Cheng et al. (2024) proposed Emotion-LLaMA, which integrates audio, visual, and text inputs for multimodal emotional recognition and reason-ing. Lian et al. (2023) comprehensively explored the performance of GPT-4 Vison (Achiam et al. (2023)) on generalized emotion recognition tasks. Xenos et al. (2024) utilized LLaVA (Liu et al. (2024a)) to generate textual descriptions of images as auxiliary information for training a multimodal architecture for the CAER task, but it did not fully explore the emo-tion recognition capabilities of LVLMs. Different with Xenos et al. (2024), we are not using LVLMs as auxiliary tools, but rather exploring their inherent emotion analysis capabilities in the CAER task."}, {"title": "3. Methodology", "content": ""}, {"title": "3.1. Proposed Framework for Few-Shot Setting", "content": "Figure 1 illustrates the overall architecture of our framework for the CAER task on the 2-shot setting. It is mainly composed of three parts: the Demonstration Retrieval Module, the Prompt Template Designing Module, and the Inference Module."}, {"title": "3.1.1. DEMONSTRATION RETRIEVAL MODULE", "content": "Previous works (Zhang et al. (2024)) have emphasized the importance of demonstration selection in ICL and shown that examples with semantic similarity to the test sample can effectively enhance the performance of LVLMs. Traditional visual demonstration retrieval methods typically rely on image similarity. However, for the CAER task, considering only the overall image similarity is insufficient. On one hand, a single image may contain multiple annotated individuals, and each of them may express different emotions. Consequently, the images retrieved for different individuals should be distinct. On the other hand, the similarity between person context and scene context should be considered separately to prevent either context from being overlooked. For instance, when the proportion of the person context in an image is small, scene context may dominate the consideration, leading to the loss of important information. Therefore, we propose a demonstration retrieval module that jointly considers the similarity of person context and visual scene context. This approach ensures that both contexts are equally valued, facilitating the selection of the most appropriate demonstrations for different individuals within the same image.\nPerson Context. Facial expressions, gestures, body postures, and gait are all crucial indicators for assessing human emotions. We collectively refer to these indicators as the person context. Based on the provided bounding box annotations [Xmin, Ymin, \u0425\u0442\u0430\u0445, \u0423\u0442\u0430\u0445], we crop the person from the image to obtain the person-specific context. Then, we use the image encoder (i.e., the pretrained ViT (Dosovitskiy et al. (2020))) to encode the cropped person and obtain the person-specific representations $f_{\\text{person}}$,\n$f_{\\text{person}} = \\text{ViT} (I_{\\text{person}}) $. (1)\nScene Context. In addition to the explicit context of the individual, the surrounding scene semantics are essential for understanding emotions. To account for the influence of scene context, we designate the remaining part of the image I after cropping out the labeled individual as the visual scene context $I_{\\text{scene}}$,\n$f_{\\text{scene}} = \\text{ViT} (I_{\\text{scene}}) $. (2)\nImage Similarity Rank. Given the test example $I_q$ and a set of candidate demon-stration images $D = \\{I_1, I_2, ..., I_N\\}$, where $I_i$ denotes the i-th candidate image. First, we crop each test image and candidate image into person and scene components, then encode these components to obtain their representations $F_q = (f_{\\text{person}\\_q}, f_{\\text{scene}\\_q})$ and $F_D = \\{(f_{\\text{person}\\_i}, f_{\\text{scene}\\_i})\\}_{i=1}^N$. Then we separately calculate the similarity of $F_q$ and $F_D$,\n$\\text{Sim}_{\text{person}i} = \\frac{f_{\\text{person}\\_q}f_{\\text{person}\\_i}}{|| f_{\\text{person}\\_q}|| || f_{\\text{person}\\_i} ||} ,$ (3)\n$\\text{Sim}_{\text{scene}i} = \\frac{f_{\\text{scene}\\_q}f_{\\text{scene}\\_i}}{|| f_{\\text{scene}\\_q}|| || f_{\\text{scene}\\_i} ||} ,$ (4)\n$\\text{Sim} (F_q, F_D) = \\{(\\text{Sim}_{\text{person}i} + \\text{Sim}_{\text{scene}i})/2\\}_{i=1}^N, $ (5)\nand then record the rank $R_q \\in \\mathbb{R}^N$ of each candidate image in D,\n$R_q = \\text{Rank} (\\text{Sim} (F_q, F_D)) . $ (6)\nCompared to ranking based on overall image similarity scores, the method of ranking based on the similarity scores of two distinct contexts better accounts for the influence of different contexts on the emotional semantics of images, thereby providing optimal demonstrations for the ICL learning of LVLMs. Finally, we take the top-k images with the highest similarity ranking as selected demonstrations."}, {"title": "3.1.2. PROMPT TEMPLATE DESIGNING MODULE", "content": "To better transfer the capabilities of LVLMs to the CAER task, we reframe the CAER task as a generative task. We construct a prompt template to bridge the gap when applying LVLMs to a subtask. As shown in Figure 1, for the CAER task, each input consists of three parts: Instruction, Demonstration, and Test Sample.\nInstruction. At the beginning of the input, we first provide LVLM with a clear task instruction and a label statement as follows: \u201cGiven the list of emotion labels: {class names}, please choose which emotion is more suitable for describing how the person in the red box feels\", where {class names} are substituted for the list of class names available in each dataset. Specifically, for multi-label classification tasks, we replace 'emotion is' with 'emotions are' to prevent LVLMs from producing a single-label output.\nDemonstration. Each of our demonstrations is an image-answering pair \\{I, A\\}. We select the top-k demonstrations obtained by the proposed demonstration retrieval module and concatenate them to form the overall demonstration.\n$\\sigma = \\text{Top-K} (R_q), $ (7)\n$\\text{demoni} = \\{F_{D^{\\sigma\\_i}}, Y_{\\sigma\\_i}\\}, $ (8)\nwhere $\\sigma$ are the indices of top-k similarity ranking, and $\\sigma = \\{\\sigma\\_1,...,\\sigma\\_K\\}$. Specifically, for the few-shot CoT paradigm, our demonstrations include not only the images and labels but also the rationales that explain why the image corresponds to the given emotion labels,\n$\\text{demoni} = \\{F_{D^{\\sigma\\_i}}, \\text{Rationale} (F_{D^{\\sigma\\_i}}, Y_{\\sigma\\_i}), Y_{\\sigma\\_i}\\}.$ (9)\nThe rationale is generated by GPT-4 Vision (Achiam et al. (2023)) due to its powerful instruction-following capabilities. Figure 2 shows an example of generated rationale. The final demonstration is expressed as follows:\n$\\text{Demonstration} = \\text{Concat} (\\text{demon}\\_1, ..., \\text{demon}\\_K) .$ (10)"}, {"title": "3.1.3. INFERENCE MODULE", "content": "Following the instruction and demonstration, we append the test sample at the end of the prompt as input to the LVLM and select the most likely generated sequence as the output,\n$\\text{Input}\\_q = \\{\\text{Instruction}, \\text{Demonstration}, I\\_q\\}, $ (11)\n$\\text{Output}\\_q = \\text{LVLM} (\\text{Input}\\_q, 0).$ (12)\nFinally, we post-process the output sequence to obtain the list of predicted labels."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Dataset and Evaluation Metrics", "content": "EMOTIC (Kosti et al. (2019)) is the most widely used context emotion dataset that col-lected samples in uncontrolled environments. It contains 23,571 images of 34,320 annotated agents, and each individual is assigned one or more of the 26 discrete emotion labels based on their apparent emotional states. The dataset is randomly split into training (70%), val-idation (10%), and testing (20%) sets. The training set is annotated by a single annotator, whereas the validation and testing sets are annotated by four and two additional annota-tors, respectively. Consequently, the average number of labels per sample is 1.96 for the training set, 6.15 for the validation set, and 4.42 for the testing set.\nHECO (Yang et al. (2022)) dataset consists of images from the HOI (Chao et al. (2018)) datasets, film clips, and images from the Internet. The dataset contains 9,385 images of 19,781 annotated agents. Each agent is labeled with one of the eight emotions: Surprise, Excitement, Happiness, Peace, Disgust, Anger, Fear, and Sadness. The dataset is randomly split into training (70%), validation (10%), and testing (20%) sets.\nSince the LVLM treats the CAER task as a text generation process, its output consists of textual emotion labels rather than probabilities. Consequently, we cannot use MAP (mean Average Precision) as an evaluation metric, as was done in previous works (Kosti et al. (2019); Mittal et al. (2020)). In this work, we report Precision, Recall, F1 Score, Hamming Loss, and multi-label Accuracy on the EMOTIC (both micro average and macro average). For the HECO, we use the Precision, Recall, F1 Score on macro average, and the standard classification accuracy for evaluation. Note that a smaller Hamming Loss and larger Accuracy, Precision, Recall, and F1 Score indicate better classification quality."}, {"title": "4.2. Experimental Setup", "content": "Extensive experiments are conducted on two LVLMs, including LLAVA-7B (Liu et al. (2024a)) and VILA-8B (Lin et al. (2024)). We find that LLAVA struggles with handling multiple image inputs, and its instruction-following capability significantly decreases as the number of input images increases. Therefore, for the few-shot setting, we only report the 2-shot results for LLAVA. For the EMOTIC and HECO datasets, We randomly select 200 examples from the training set as the candidate demonstrations set and chose k examples from the set for the k-shot ICL task. Specifically, due to the difference in the average number of labels between the EMOTIC training set and validation set, we additionally randomly sample 200 examples from the EMOTIC validation set as a candidate set to assess the impact of the number of demonstration labels on few-shot prediction results. To facilitate subsequent processing of the generated text, we specify \"reply with label(s) only\" for zero-shot prompts, while for few-shot tasks, the output format is demonstrated in the examples. All experiments are conducted on four Nvidia A800-80G GPUs. For the fine-tuning approach, we set the learning rate to le-5 and the batch size to 8."}, {"title": "4.3. Results and Analysis", "content": "The results of zero-shot setting, supervised fine-tuning (SFT) setting, few-shot setting, and few-shot CoT setting are shown in Table 1 and Table 2. Due to the differing task characteristics and complexities of the EMOTIC and HECO (i.\u0435., EMOTIC being a multi-label classification task and HECO a single-label classification task), we analyze the results of the two datasets separately.\nFor the EMOTIC, the fine-tuned LLAVA achieves the highest precision. VILA performs few-shot learning by retrieving demonstrations from the validation subset, obtaining the highest recall and F1 score. Compared to the trained EMOT-Net (Kosti et al. (2017)), the fine-tuned LVLMs exhibit significantly higher precision but considerably lower recall. This suggests that the fine-tuned LVLMs tend to output the most accurate emotions rather than the most comprehensive ones because the average number of labels in the EMOTIC training set is much smaller than that in the test set. Although EMOT-Net is also trained on the training set, it outputs probability values, unlike LVLMs which generate direct labels. This allows for dynamic threshold adjustment to balance precision and recall, preventing the imbalance seen in LVLMs.\nAnother notable observation is the significant gap between the micro average and macro average scores for both the trained EMOT-Net and the fine-tuned LVLMs. The lower macro average scores indicate poor performance on certain emotion categories, while the higher micro average scores suggest that the well-performing emotion categories are more prevalent in the dataset. This discrepancy highlights the issue of imbalanced label distribution in the dataset, consistent with previous findings (Yang et al. (2024)). Whether through training or fine-tuning, models are profoundly influenced by the distribution of the training set. For subjective annotations like emotions, this is not ideal. Focusing on fitting the training set's label distribution causes models to learn the annotators' emotional biases, and the imbalanced label distribution leads to insufficient learning of less prevalent emotions.\nFurthermore, we observe the performance of LVLMs in the few-shot CoT paradigm. We find that the addition of rationale examples increases precision but decreases recall. That's because without CoT, the model likely recognizes simple patterns, but with CoT examples, it needs to learn and focus on more detailed information. This makes the model more stringent and cautious in emotion classification, leading to the omission of some true labels. How to streamline the CoT process, allowing LVLMs to balance resources between analysis and decision-making during inference, will be the focus of our future research."}, {"title": "4.4. Ablation Studies", "content": "Impact of Demonstration Retrieval Method. We report the results of five retrieval ranking methods under the 6-shot setting in Table 3. In this case, \u201cOverall\" refers to rank-ing based on the similarity of the entire image, \u201cbody-based\" refers to calculating similarity using only the cropped body part, \"scene-based\" refers to calculating similarity using only the scene part, and \"random\" involves randomly selecting images as demonstrations. We observe that the proposed multi-context retrieval method performs the best, followed by overall image retrieval, while the random selection method performs the worst. This in-dicates that separately considering person context and scene context is beneficial for the CAER task. Furthermore, considering the similarity of the entire image is more effective than only considering the person context or scene context, demonstrating that each context contributes to retrieving the optimal demonstrations.\nImpact of Demonstration Number. To investigate the impact of the number of demonstrations on performance, we conduct experiments with varying numbers of examples, as shown in Table 4. We observe that, on both datasets, most metrics generally improve as the number of examples increases. Our framework performs best on average with 10-shot demonstrations. Beyond 10-shot, the improvements stabilize and some metrics even decline. This decline can be attributed to the fact that longer inputs make it difficult for the model to focus on key points, and more demonstrations may include semantically dissimilar examples, which can have a negative effect. Therefore, selecting an appropriate number of demonstrations is crucial for maximizing the potential of LVLMs."}, {"title": "4.5. Case Study", "content": "Figure 4 presents examples of the CoT results under different settings on the EMOTIC. We find that in the zero-shot setting, while the model's decisions are not poor, the generated rationale does not explain in detail why the person feels this emotion, and in some cases (e.g., Example 3), it is merely a redundant description of the decision. With few demon-strations, the model's CoT improves significantly by learning from the demonstrated CoT to generate rationale from different perspectives. And it often leads to more accurate de-cisions. CoT with images from the validation subset tends to predict more labels, whereas CoT with images from the training set yields more concise results. We can achieve our desired outcomes by constructing retrieval sets with different characteristics for the model. Additionally, we find that although sometimes the model predictions are not always entirely correct, its analysis is subjectively reasonable. For instance, in the first image, the model thinks the person should feel \"Excitement\" in a sports setting, which seems understandable even though this category is not among the ground truth labels of the image. In contrast, the \"Esteem\" label in the ground truth is not clearly perceivable, which we believe reflects the annotator's emotional bias. For the third image, the model provided the label \"Discomfort\", which is not one of the 26 emotion categories specified in the EMOTIC dataset. However, emotions are inherently complex and cannot be fully encapsulated by a limited set of categories. Models trained on specific datasets might ignore or consider \"Discomfort\" as an incorrect decision, but as humans, we recognize it as a right label. This underscores the necessity of our research: emotion analysis should be comprehensive rather than limited. Traditional methods cannot achieve this, whereas large models hold great potential in this regard."}, {"title": "5. Conclusion", "content": "This paper explores the potential and performance of various LVLM paradigms in the CAER task. Our findings indicate that LVLMs can outperform traditional models without addi-tional training through our proposed framework, demonstrating their significant potential in understanding contextual emotions. The powerful reasoning and generalization capabilities of large models provide a promising outlook: we envision that emotion analysis should not be constrained by the limitations of specific datasets but should advance toward a broader and more fine-grained emotional landscape.\nFuture Work. We plan to improve the performance of LVLMs in the Few-shot CoT paradigm by constructing higher-quality CoT demonstrations, guiding the model to gener-ate more logical and comprehensive rationales."}]}