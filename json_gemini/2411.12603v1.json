{"title": "STREAM: A Universal State-Space Model for Sparse Geometric Data", "authors": ["Mark Sch\u00f6ne", "Yash Bhisikar", "Karan Bania", "Khaleelulla Khan Nazeer", "Christian Mayr", "Anand Subramoney", "David Kappel"], "abstract": "Handling sparse and unstructured geometric data, such as point clouds or event-based vision, is a pressing challenge in the field of machine vision. Recently, sequence models such as Transformers and state-space models entered the domain of geometric data. These methods require specialized preprocessing to create a sequential view of a set of points. Furthermore, prior works involving sequence models iterate geometric data with either uniform or learned step sizes, implicitly relying on the model to infer the underlying geometric structure. In this work, we propose to encode geometric structure explicitly into the parameterization of a state-space model. State-space models are based on linear dynamics governed by a one-dimensional variable such as time or a spatial coordinate. We exploit this dynamic variable to inject relative differences of coordinates into the step size of the state-space model. The resulting geometric operation computes interactions between all pairs of N points in O (N) steps. Our model deploys the Mamba selective state-space model with a modified CUDA kernel to efficiently map sparse geometric data to modern hardware. The resulting sequence model, which we call STREAM, achieves competitive results on a range of benchmarks from point-cloud classification to event-based vision and audio classification. STREAM demonstrates a powerful inductive bias for sparse geometric data by improving the PointMamba baseline when trained from scratch on the ModelNet40 and ScanObjectNN point cloud analysis datasets. It further achieves, for the first time, 100% test accuracy on all 11 classes of the DVS128 Gestures dataset.", "sections": [{"title": "1. Introduction", "content": "Computer vision computes relationships between data points in spatial coordinates X, Y (in R2) or X, Y, Z (in R\u00b3), or spatio-temporal coordinates (R\u00b3 or R4), where one of the dimensions denotes time t. Convolutional neural networks based on structured, uniformly spaced and local linear operations successfully address this problem for classical camera recordings such as images or videos, which are themselves structured and discrete. Many modalities of recent interest, however, are neither structured nor uniformly spaced. Sensors such as Light Detection and Ranging (Li-DAR) or event-based cameras [21, 28] sample signals based on sparse processes, resulting in sparse geometric data with irregularly spaced coordinates. Point cloud analysis was the central research field for sparse geometric data over the past decade [13]. While early works followed voxel-based approaches [25, 50], point-based methods dominate the research landscape today [4, 5, 46, 48]. Meanwhile, event-based cameras [21, 28] raised considerable interest in the computer vision community. Although these cameras record sparse geometric data, most works collapse the sparse stream of events into frames [28, 56, 57], potentially losing unique properties of these cameras such as low latency and high dynamic range. The structural similarity between point clouds and event streams encourages methodological transfer, especially from the domain of point cloud analysis to event-based vision [35, 45]. Our work demonstrates the reverse: The inherently temporal state-space model formulation presented in sec. 3.2 improves event-based vision on the DVS128-Gesture dataset [1] to 100%, and at the same time provides a valuable inductive bias for geometric data such as point clouds as demonstrated on the ModelNet40 and ScanObjectNN datasets [43, 51].\nWe show that sparse geometric data with irregularly spaced coordinates as shown in fig. 1B can be naturally integrated in the state-space model formalism. In contrast to prior sequence models such as PointMamba [20] or Event-Mamba [36], we explicitly encode geometric structure into the state-space model parameterization by injecting relative differences of coordinates as step sizes.\nContributions. This paper makes the following main innovations over the state-of-the-art:\n\u2022 We propose a unified framework for modeling sparse geometric data with state-space models with irregularly spaced step sizes. The resulting model, STREAM, handles sparse geometric data such as point clouds and event-based vision.\n\u2022 Our state-space model formulation demonstrates a valuable inductive bias for geometric structure by improving the PointMamba [20] baseline by up to 2.84% when trained from scratch on the ScanObjectNN dataset [43].\n\u2022 We show compelling results for event-based vision, processed as a stream of events with our purely recurrent neural network without the usage of frames or spatial convolutions. For the first time, we demonstrate 100% classification accuracy on all 11 classes of the DVS128-Gestures dataset [1].\n\u2022 We provide an efficient CUDA implementation for integrating irregular step sizes based on the selective-scan implementation of [9]."}, {"title": "2. Related Work", "content": "2.1. Point Cloud Analysis\nEarly point-based methods such as PointNets [4, 31] directly pass the point cloud through a Multi-layer Perceptron (MLP) and introduce a permutation invariance of the set of points via pooling operations, implicitly integrating spatial information. In contrast, methods for explicitly integrating spatial information parameterize linear operators such as convolutions based on relative differences between point coordinates. The convolutional neural networks based on irregularly spaced operators presented in [17, 46, 48, 49] outperform PointNets on a range of point cloud analysis tasks. These works parameterize the convolution kernel with MLPs evaluated at the relative differences between point coordinates. To break the O (N2) complexity of computing pairwise interactions for all N points, locality constraints are added to scale to larger point sets (see fig. 1A). As shown in fig. 2A, our method also explicitly integrates spatial information in this sense, but with a kernel parameterized by a state-space model (SSM) instead of a MLP (see equation (3)). This parameterization enables the complete computation to be performed in O (N) steps.\nMore recently, sequence models demonstrated favorable results over convolutional methods [5, 20, 26, 52]. The point cloud is flattened into a sequence and then processed by transformer or state-space model based backbones. This formulation inherits many strong scaling properties of sequence models including masked pre-training [52] or generative pre-training [5] objectives. Transformers, however, suffer from O (N2) complexity in the number of points N, limiting their application to larger point clouds. State-space models, in contrast, have sequential O (N) complexity for inference and O (Nlog N) complexity for parallel training [10, 11]. At the same time, SSMs can learn long distant relationships between inputs including convolutional operators with rational transfer functions [10, 30]. The recently introduced Mamba state-space model [9] sparked widespread discourse, and was quickly adapted in many domains, including point cloud analysis. Works like Point-Mamba [20] or Point Cloud Mamba (PCM) [55] deploy Mamba in established point cloud frameworks such as the masked autoencoder developed in [26]. By relying on the default Mamba architecture, spatial information is integrated implicitly in these works similar to PointNets. In contrast, we show in sec. 3.2 that spatial information can be integrated explicitly via the SSM parameterization. This formulation allows us to remove specialized preprocessing methods to represent point clouds as sequences [20, 55], and simply order by the spatial coordinates X, Y, or Z."}, {"title": "2.2. Event-based Vision", "content": "Most state-of-the-art event-based vision methods construct frames from the asynchronous event-stream by binning events in regularly spaced time intervals [6, 28, 42, 56, 57]. Few methods operate on the sparse unstructured event streams directly. AEGNN [37] computes spatio-temporal features based on subsampled event streams with graph neural networks. Their method admits an event-based inference method where only nodes that correspond to incoming events are updated asynchronously. EventMamba [36] is tightly related to the point cloud analysis models discussed in sec. 2.1. They process subsampled raw event streams similar to [37, 39] with a combination of a point feature extractor and Mamba. Its high similarity with PointMamba [20] places EventMamba in the category of methods implicitly integrating spatio-temporal information. Conceptually closer to our model are filtering methods that process the stream of events recursively while explicitly integrating spatio-temporal information. Early methods like HOTS [18] create a spatial representation of the (X, Y)-plane of an event-based camera by integrating exact timestamps with exponentially moving averages. Similar to our method, EventNet [39] recursively iterates the event stream. In contrast to our state-space model, their recurrent network parameterization does not inherit stability guarantees from the theory of linear systems and lacks an efficient parallelization along the time dimension during training, limiting the scalability to longer streams. Event-SSM [38] proposed a discretization method for simplified state-space layers (S5) [40] to operate directly on asynchronous event streams. Their model excels at spiking audio classification, but falls behind the state-of-the-art in event-based vision. Concurrently to our work, S7 [41] explores improvements of the S5 architecture and Event-SSM through a more stable parameterization and input dependent state-space model parameters. In contrast, STREAM can improve sequence-based point cloud analysis (see sec. 4.1), and therefore drive the convergence between event-based vision and point cloud analysis forward.\nRelated from the perspective of discretizing continuous kernels is the TENNs framework [27]. They construct convolutions over finite time horizons with orthogonal polynomials that can be discretized on irregularly spaced timestamps. In practice, they choose regularly spaced frames, however, and show compelling results on event-based vision benchmarks. In comparison, STREAM parameterizes infinite time horizons via the state-space model."}, {"title": "3. STREAM", "content": "In this section, we present our method for efficiently modeling sparse sets of N coordinates {xi}_{i=0}^{N}, where xi \u2208 Rd. This general formulation includes data in 3D space, with x = (X, Y, Z), as well as the streams recorded by event-based cameras. In the latter case one of the dimensions (e.g. x(1)) is interpreted as event time while others (x(2), x(3)) denote spatial locations (see fig. 1C). Furthermore, higher dimensions (d > 3) can be used to denote data that includes other dimensions, such as color intensity, etc. STREAM is illustrated in fig. 2A-C.\nNotation. We denote tensor-valued variables and functions such as multi-dimensional coordinates x \u2208 Rd or matrices A \u2208 Rmxm in bold characters. Scalar variables such as the coordinates x(1), . . ., x(d) of a tensor x or the entries Aij of a matrix A are denoted in regular characters.\n3.1. Problem Statement\nConsider a set {xi}_{i=0}^{N} of points sparsely distributed in space. We assign a multi-dimensional representation vector ui to each coordinate xi as common practice in machine learning. Such input signals can be formulated mathematically as sums of Dirac delta pulses evaluated at the coordinates {xi}_{i=0}^{N}\nu(x) = \\sum_{i=0}^{N} \\delta (x \u2013 x_i) u_i, (1)\nwhere ui \u2208 R\" is the representation at xi. To reason about spatial relationships between these pulses, we define an interaction kernel \u03a6(x, x') that models pairwise interactions. A complete view of all possible interactions with the point xk is given by\ny (x_k) = \\int \\Phi (x_k, x') u (x') dx' (2)\ny (x_k) = \\sum_{i=0}^{N} \\Phi (x_k, x_i) u_i. (3)\nThis formulation contains the familiar convolution operator as a special case when \u03a6 (x, x') = \u03a6 (x \u2212 x'). Equation\""}, {"title": "3.2. Integrating Sparse Coordinates with State-space Models", "content": "State-space models (SSMs) have demonstrated outstanding performance on long-range dependency modeling tasks, while remaining efficient for training and inference [9, 11]. SSMs project the input through a linear recursive kernel to states h (t). The independent variable t is usually interpreted as, and aligned with, time in the input signal domain. The linear nature of SSMs allows the exact integration of sparse sets of points with irregularly spaced coordinates, as we will show below. A linear time-varying SSM acting on a scalar input function u (t) and producing a scalar output y (t) is defined through\nh (t) = A (t) h (t) + B (t) u (t) (4)\ny (t) = C (t) h (t), (5)\nwith u(t), y(t) \u2208 R, states h (t) \u2208 Rm and parameters A (t) \u2208 Rmxm, B (t) \u2208 Rm\u00d71, C (t) \u2208 R1\u00d7m. For the sake of simplicity, we assume that the parameters are represented by piecewise constant functions, i.e. Ai = A (ti), Bi = B (ti), Ci = C (ti) are constant on the intervals (ti-1,ti]. We show in appendix 6 that solving the linear system in equations (4) and (5) for the input in equation (1) yields the kernel function\n\\Phi (t_k, t_i) = C_k \\left(\\prod_{j=i+1}^{k} exp \\right)A \\Delta_j\\left) B_i, (6)\nwhere \u0394j = tj \u2013 tj\u22121. Importantly, the output y(t) of the state-space model at to,...,tN can be computed with the recursive formula\nh_k = h (t_k) = e^{A_k\\Delta_k}h_{k-1} + B_ku_k (7)\ny_k = y(t_k) = C_kh_k . (8)\nEquations (6) and (7) highlight how relative differences \u0394j = tj - tj-1 in the coordinate t explicitly parameterize our pairwise interaction kernel \u03a6. An example of the interaction defined by equation (6) can be found in Fig. 2C. For complex A \u2208 C, the exponentially oscillating kernel e^{A\\Delta_A} integrates the history of signals ui occuring in irregularly spaced intervals \u0394\u2081 with i <k. With this parameterization, our method differs from other Mamba based works, where"}, {"title": "3.3. Selective state-space models and STREAM", "content": "We develop our method based on the selective state-space model also known as Mamba [9], which was designed for regularly spaced modalities, and language modeling in particular. In accordance with this purpose, the integration time steps Ai and the matrices Bi, Ci are functions of the input signal ui, and do not carry explicit information about spatial relationships. Formally, Mamba is parameterized by\nA_i = A V_i \\Delta_i = \\Psi (\\text{Linear} (u_i)) (9)\nB_i = \\Delta_i\\text{Linear} (u_i) C_i = \\text{Linear} (u_i) (10)\n, where A is a learned diagonal matrix, and \u03a8 (u) = ln(1+\ne\") is the softplus function. This single-input ui single-output yi SSM is then repeated n times and wrapped by linear transformations to create a multi-input ui multi-output yi model as shown in fig. 2B. Note that the total state size is nm in the multi-input multi-output case.\nWe propose to explicitly represent the irregularly spaced pulse timings ti in the Mamba parameterization according to equation (7). Therefore, we explicitly set Ai to (ti - ti\u22121) \u03a8 (\u03b4), where \u03b4 is a trainable time scale parameter, in contrast to equation (9). We further decouple the coefficient of Bi in equation (10) from the time differences Ai to avoid zero coefficients in case of overlapping pulses ti-ti-1 = 0. With these adjustments, our model is defined by\nA_i = A V_i A_i = (t_i-t_{i-1}) \\Psi (\\delta) (11)\nB_i = \\Gamma_i\\text{Linear} (u_i) C_i = \\text{Linear} (u_i) (12)\n\\Gamma_i = \\Psi (\\text{Linear} (u_i)), (13)\nwhere \u03b4\u2208 Rn is a learnable parameter. We call our resulting state-space model parameterization STREAM for Spatio-Temporal Recursive Encoding of Asynchronous Modalities.\""}, {"title": "3.4. Modeling Point Clouds with STREAM", "content": "The explicit coordinate t in the STREAM module is one-dimensional and strictly ordered. For the case of event streams, this ordering corresponds to temporal order from past to future. Point clouds fit into this framework by ordering the set of points w.r.t. one of the spatial coordinates from left to right. As shown in fig. 2A,B, we select one of the coordinates to be integrated explicitly. Inserting the X coordinate for t in equation (11) without loss of generality and setting uk = \u0398 (Xk, Yk, Zk), where \u66f0 : R3 \u2192 Rn is a point wise encoder, yields our STREAM formulation for point cloud analysis. To achieve state-of-the-art results, we sort the point set by all three spatial coordinates respectively and feed the concatenation of the three resulting sequences to STREAM (see sec. 4.1.1, fig. 3)."}, {"title": "4. Experiments", "content": "We demonstrate empirically that the explicitly parameterization of the state-space model with coordinate differences Ai presented in sections 3.2 and 3.3 is a strong abstraction that exploits the sparse geometry of both point clouds and event-based vision. When trained from scratch, our method improves the PointMamba [20] baseline by up to 2.8% on the ScanObjectNN dataset [43]. At the same time, our method sets a new state-of-the-art for event-based gesture recognition on the DVS128-Gestures dataset [1].\n4.1. STREAM for Point Cloud Analysis\n4.1.1 Point Cloud Model\nOur goal is to evaluate the explicit SSM parameterization defined in sec. 3.3. Therefore, we align our architecture close to PointMamba [20], which uses the default Mamba module [9]. As visualized in fig. 3A, we sample N points and sort this point set by the X, Y, and Z coordinates respectively. The vector representations extracted from the point coordinates individually are scaled and shifted by learned parameters for the three sorted sequences separately to indicate the sorting dimension. The resulting sequence of 3N points is concatenated along the sequence dimension to form the input to our model. This preprocessing simplifies the ordering of the point set by avoiding the two space-filling Hilbert-curves applied by PointMamba at the cost of increasing the input sequence length from 2N to 3N.\nSimilar to [5, 20, 26], our model is composed of two stages. The input sequence is grouped into sets of 32 points that are independently processed by a STREAM module to represent local information. A backbone of 12 STREAM layers operates on the features extracted from these groups to agglomerate global information."}, {"title": "4.1.2 Point Cloud Results", "content": "We evaluate our method on two standardized point cloud classification benchmarks. The ModelNet40 dataset [51] contains 12311 clean point clouds sampled from 3D CAD objects. A more realistic dataset of physically scanned indoor scenes is given by ScanObjectNN [43]. ScanObjectNN contains about 15000 scans from 2902 unique objects with varying difficulty. In both cases, we train our method with the same hyperparameters as PointMamba [20].\nWe report the best overall classification accuracy, i.e. the average over all samples, obtained from 5 different random seeds. Tab. 1 shows that replacing the Mamba module in PointMamba with a STREAM module improves the performance of models trained from scratch across all instances on ScanObjectNN. In particular, the hardest instance PB-T50-RS benefits from the exact integration of spatial information and improves PointMamba from 82.48% to 85.32% for the best out of 5 random seeds. With a mean and standard deviation of (84.4 \u00b1 0.7)%, this poses a significant improvement.\nOn ModelNet40, STREAM improves the overall accuracy of PointMamba by 0.3% from 92.4% to 92.7% as reported in tab. 2. Despite the smaller gain compared to ScanObjectNN, the low variance on this dataset indicates a significant improvement on ModelNet40 as well. The mean and standard deviation of our model is (92.6 \u00b10.1)%. Our results show that the inductive bias of explicitly encoding spatial relationships in the SSM parameterization is particularly useful when training models from scratch."}, {"title": "4.2. STREAM for Event Streams", "content": "4.2.1 Event Stream Model\nIn contrast to most prior works, our method directly operates on the stream of events as visualized in fig. 4. The stream is encoded as a sequence of tokens (ti, ui), where each pixel of the event-based camera is uniquely encoded by a separate token, similar to how text is represented as tokens in modern language modeling [33]. This sequence is directly processed by a STREAM module. To accommodate large streams of up to 131 072 events per sample in GPU memory, the output of the first STREAM module is subsampled by a factor of 8 or 16 depending on the task. This way, every event is processed by the neural network at least once directly, in contrast to previous methods that subsampled the raw stream before presenting it to a neural network [37, 39]. To create a hierarchical architecture, the sequence is subsampled a second time after half the number of layers. The state dimension is increased by a factor of 2 upon the second subsampling. Our hierarchical subsampling architecture is visualized in fig. 4. We apply average pooling along the sequence dimension before computing the class labels. To improve generalization, we implement a set of geometric data augmentations and a variant of CutMix [53] that directly mixes event streams [38]."}, {"title": "4.2.2 Event Stream Results", "content": "We evaluate STREAM on two popular event-based datasets. The DVS128 Gestures dataset [1] consists of 1342 recordings of 10 distinct classes of hand gestures and an additional class of arbitrary other gestures. While this dataset features only a relatively small number of samples for 11 hand gesture categories recorded from 29 subjects, the total number of events in this dataset adds up to about 390 M events. Our model has an initial model dimension of n = 32 and a state-space dimension of m = 32, which is expanded by a factor of 2 upon subsampling. We deploy a total of 6 STREAM blocks, which by the notation of fig. 4 corresponds to L = 2. We train with a batch size of 32 on sequences of 65536 events such that the total input sequences created by CutMix are up to 131 072 events long, and subsample by a factor of 16. Evaluation is conducted on the full sequences of up to 1.5 M events. Remarkably, our fully event-based model sets a new state-of-the-art on the DVS128-Gestures dataset as reported in tab. 3, improving over both frame-based and event-based references. While [27] reported 100% accuracy on 10 out of the 11 classes, omitting the 'others' class, we for the first time present a model that can reach a maximum accuracy of 100% on all 11 classes of the dataset.\nIn addition to event-based vision, we evaluate STREAM on an event-based audio classification task. The Spiking Speech Commands dataset [7] contains more than 100 000 samples converted from the original Speech Commands dataset [47] with a median number of 8100 events per sample. The model dimension is again n = 32 with a smaller state-space dimension of m = 4 compared to the vision model. We deploy 8 STREAM blocks (L = 3 in fig. 4), and set the subsampling factor to 8. With this configuration, we report a classification accuracy of 86.3% in tab. 4, which settles between the small and large models reported in [38]."}, {"title": "4.3. Ablation Study", "content": "We compare our STREAM module against the Mamba module [9] in tab. 5. The central difference is the explicit integration of spatio-temporal information in the recurrent operator e^{A\\Delta_k} with \u0394k = tk - tk-1. We additionally experiment with all combinations of applying linear transformations activated by Softplus functions to renormalize the \u0394k and \u0393k parameters in equations (11) and (12), respectively. While the fairly small DVS128 Gestures dataset possesses high variance, STREAM consistently improves over Mamba. This effect is stronger expressed on the larger Spiking Speech Commands dataset, which is less affected by training noise due to the larger number of samples. Here, STREAM creates a significant margin to the Mamba baseline."}, {"title": "5. Discussion", "content": "We have introduced STREAM, a sequence model for point cloud and event stream data, which achieves competitive results on a range of benchmarks from point-cloud classification to event-based vision. Prior efforts of unifying these modalities transferred point cloud models to event streams, or ignore the spatio-temporal structure of either modality by applying the default Mamba model to a sequential view of the data. In contrast, our model exploits the dynamics of state-space models, a temporal process at first sight, to encode spatial geometric information into the models parameterization. This inductive bias proved valuable as STREAM improves our reference model, PointMamba, when trained from scratch on point cloud datasets such as ModelNet40 and ScanObjectNN. By design, STREAM applies to spatio-temporal modalities such as event-based vision. We operate on the stream of events without collapsing events into frames and without using 2D convolutions. This processing paradigm achieves 100% classification accuracy on all 11 classes of the event-based DVS128 Gestures dataset, a result that has so far only been achieved on the 10 predetermined classes by [27]. A practically relevant property of STREAM is that it allows asynchronous inference on streams of points or events recorded from LiDAR sensors or event-based cameras. We, therefore, expect STREAM to be well suited for sensor fusion of these modalities. Previous work has shown that SSMs can benefit significantly from self-supervised pre-training on larger datasets [20], which we will explore for our method in future research."}, {"title": "6. Derivations", "content": "This section provides complete derivations of equations (6) and (7). We restrict to the case of single-input single-output (SISO) state-space models with multi-dimensional state. Multi-input multi-output (MIMO) formulations like S4 [11] or Mamba [9] can be obtained by creating n parallel instances of the SISO model and mixing the input and output components with linear layers.\nOur motivation was to reason about spatial relationships between input pulses. Therefore, we defined an interaction kernel \u03a6 (x, x') that models pairwise interactions. A complete view of all possible interactions with the point xk is given by\ny (x_k) = \\int \\Phi (x_k, x') u u (x') dx' (14)\ny (x_k) = \\sum_{i=0}^{N} \\Phi (x_k, x_i) u_i. (15)\nWe will now derive a parameterization of the interaction kernel I with a state-space model.\nA linear time-varying state-space model acting on a scalar input function u (t) and producing a scalar output y (t) is defined through\nh (t) = A (t) h (t) + B (t) u (t) (16)\ny (t) = C (t) h (t), (17)\nwith u(t), y(t) \u2208 R, states h (t) \u2208 Rm and parameters A (t) \u2208 Rmxm, B (t) \u2208 Rm\u00d71, C (t) \u2208 R1\u00d7m.\nNote that we stick to our notation introduced in sec. 3, denoting vectors and matrices with bold face and scalar values with regular face.\n6.1. Solution of the Linear State-space Model\nLinear ordinary differential equations have a well known analytical solution. We refer the reader to standard calculus textbooks. As such, the linear dynamics of equation (16) for initial value ho = h (to) admit the analytical solution\nh (t) = h_0 + \\int_{t_0}^{t} \\exp \\left( \\int_{t'}^{t} A (t'') dt'' \\right) B (t') u (t') dt', (18)\nwhich can be checked by taking the derivative of h and comparing it to equation (16). Comparing equations (18), (17) and (14), we read off the kernel\n\\Phi (t, t') = C (t) \\exp \\left( \\int_{t'}^{t} A (t'') dt'' \\right) B (t') (19)\nNote that equation (18) can be evaluated on all coordinates t > to. The two canonical options for discretizing the variables h and y are equidistant steps resulting in a regular grid, or using the input coordinates to,..., tN. While most state-space model works discretize on equidistant steps, [40] shows on a toy task that the SSM formulation is capable of solving tasks with irregularly spaced steps as well.\nOur work differs from other recent Mamba based models such as PointMamba [20], Point Cloud Mamba [55], Event-Mamba [36], or SpikMamba [6] by integrating the true timings of the inputs in the following sense.\nWe will use ho = 0 for notational simplicity in the following. Discretizing equation (18) on the Dirac delta coded input (1) then yields\nh (t_k) = \\int_{t_0}^{t_k} \\exp \\left( \\int_{t'}^{t} A (t') dt' \\right) B (t) \\sum_{i=0}^{N} \\delta(t - t_i) u_i dt\nh (t_k) =  \\sum_{i=0}^{N}  \\int_{t_0}^{t_k} \\exp \\left( \\int_{t'}^{t} A (t') dt' \\right) B (t_i) u_i. (20)\nWe decompose the integral from ti to tk into the integrals from tj-1 to tj and sum over them\nh(t_k) = \\sum_{i=0}^{k} \\exp \\left(  \\sum_{j=i+1}^{k} \\int_{t_{j-1}}^{t_j} A dt \\right) B (t_i) u_i. (21)\nWe will further assume that A (t), B (t), C (t) are constant on the intervals (tj\u22121, tj] for j = i + 1, ...,k. Denoting A (tj) = Aj, B (tj) = Bj, C (tj) = Cj, h (tk) = hk, and \u0394j = tj - tj-1 we get\nh (t_k) = \\sum_{i=0}^{k} \\exp \\left(  \\sum_{j=i+1}^{k} A\\Delta_j  \\right) B_iU_i (22)\nh (t_k) = \\sum_{i=0}^{k} \\left(  \\prod_{j=i+1}^{k} exp(A\\Delta_j)  \\right) B_iU_i. (23)\nComparing to equation (15), we read off the discrete kernel function\n\\Phi(t_k, t_i) = C_k \\left(  \\prod_{j=i+1}^{k} exp(A\\Delta_j)  \\right) B_i. (24)\nProposition 1. The kernels parameterized by equation (24) contain convolution operations with rational kernels as a special case."}, {"title": "6.2. Recursive Computation of the Interaction Kernel", "content": "Let's expand equation (23) in a recursive form\nh_k = h(x_k) =  \\sum_{i=0}^{k} e \\prod_{j=i+1}^{k} A_j \\Delta_j  B_iU_i\nh_k =  B_kU_k +  \\sum_{i=0}^{k-1}  e \\prod_{j=i+1}^{k} A_j \\Delta_j B_iU_i\nh_k = B_kU_k +  e^{A_k \\Delta_k} \\sum_{i=0}^{k-1}  e \\prod_{j=i+1}^{k-1} A_j \\Delta_j  B_iU_i\nh_k = B_kU_k +  e^{A_k \\Delta_k} h_{k-1}.\nThis concludes the derivation of our recurrent operator\nh_k = B_kU_k +  e^{A_k \\Delta_k} h_{k-1}. (26)\nIn line with most recent SSM works, we choose A_i = A as a diagonal matrix for all i.\nRemark 1. Equation (26) allows asynchronous inference on streams of incoming coordinates in O (1) time per coordinate, or O (N) time for the full stream of coordinates."}, {"title": "7. Scan", "content": "Linear time-varying systems such as the one given by equation (26) resemble an associative operation, with well-known time complexity of O (log N) [3]. The goal of this section is not to provide a proof, but to give the reader a clear idea of how irrgularly spaced sequences can be parallelized in the same way that regular SSMs can be parallelized with the Scan primitive [9, 40]. The presentation follows [3].\nConsider the pair\nc_i = [a_i, b_i] (27)\nwith the binary operator \u2022 defined through\nc_i \u2022 c_j = [a_ia_j, a_i \u2022 b_j + b_i] . (28)\nAs [3] shows, the operator \u2022 is associative, i.e.\n(c_ic_j) \u2022 c_k = c_i \u2022 (c_jc_k) (29)\nAssociative operators can be parallelized to run in O(log N) time on sequence of length N given sufficiently many processors [3]. We use this primitive to parallelize our model. The pairs c are initialized as\nc_0 = [e^{Ad_0}, B_0u_0] c_N = [e^{Ad_N}, B_Nu_N] (30)"}]}