{"title": "MAKING SIGMOID-MSE GREAT AGAIN: OUTPUT RESET CHALLENGES SOFTMAX CROSS-ENTROPY IN NEURAL NETWORK CLASSIFICATION", "authors": ["Kanishka Tyagi", "Chinmay Rane", "Ketaki Vaidya", "Jeshwanth Challgundla", "Soumitro Swapan Auddy", "Michael Manry"], "abstract": "This study presents a comparative analysis of two objective functions, Mean Squared Error (MSE) and Softmax Cross-Entropy (SCE) for neural network classification tasks. While SCE combined with softmax activation is the conventional choice for transforming network outputs into class probabilities, we explore an alternative approach using MSE with sigmoid activation. We introduce the Output Reset algorithm, which reduces inconsistent errors and enhances classifier robustness. Through extensive experiments on benchmark datasets (MNIST, CIFAR-10, and Fashion-MNIST), we demonstrate that MSE with sigmoid activation achieves comparable accuracy and convergence rates to SCE, while exhibiting superior performance in scenarios with noisy data. Our findings indicate that MSE, despite its traditional association with regression tasks, serves as a viable alternative for classification problems, challenging conventional wisdom about neural network training strategies.", "sections": [{"title": "1 Introduction", "content": "Neural networks have emerged as a fundamental component of deep learning, enabling models to learn complex patterns from data through interconnected layers of nodes. Backed by the Universal Approximation Theorem [1], which establishes their ability to approximate any continuous function to arbitrary precision, these models have achieved significant success across diverse applications, including image recognition [2], language processing [3], and strategy games [4]. Central to the performance of these models are objective functions, which guide the training process by"}, {"title": "2 Background", "content": "In training neural networks, the choice of objective functions and optimization techniques plays a pivotal role in achieving accurate and efficient learning. For classification tasks, categorical cross-entropy has been the dominant loss function, minimizing the discrepancy between predicted and true class probabilities. While alternative loss functions like Focal Loss have emerged to address specific challenges such as class imbalance, Mean Squared Error (MSE) has primarily been associated with regression tasks, where it measures the average squared difference between predicted and actual values. Optimization techniques remain fundamental to effective neural network training. Methods such as gradient descent and its adaptive variants, particularly Adam [7], iteratively adjust model parameters to minimize the chosen objective function. These algorithms incorporate momentum, learning rate schedules, and adaptive adjustments to accelerate convergence and improve performance.\nNonlinear activation functions are crucial for capturing complex relationships within data. The Rectified Linear Unit (ReLU) [9, 10] and its variants have become standard choices, mitigating vanishing gradient issues and accelerating training. Recent research continues to explore adaptive combinations of activation functions to enhance network performance. Neural network optimization faces persistent challenges, including sensitivity to hyperparameter tuning, susceptibility to overfitting, and high computational demands. Novel approaches like Stochastic Gradient Tree (SGT) [8, 11] introduce tree-based updates for parameter adjustments, offering improved convergence and robustness. Complementary methods in regularization and explainable AI seek to address overfitting and interpretability concerns. While categorical cross-entropy remains prevalent in classification tasks, exploring alternative objective functions offers promising avenues for innovation. This study investigates MSE paired with sigmoid activation for classification tasks, proposing a unified framework that bridges classification and regression paradigms [12]. The sigmoid function traditionally provides probabilistic interpretation of outputs, but its combination with MSE introduces a novel learning dynamic that could impact classification performance.\nTo isolate the effects of this combination, we begin with a linear classifier analysis providing a controlled environment to study the interaction between MSE loss and sigmoid activation. This foundational approach enables clear understand-ing of core principles before extending to more complex architectures, setting the groundwork for future research in deeper models."}, {"title": "2.1 Structure and Notation", "content": "Consider Fig. 1, a linear classifier is represented by a weight matrix W, which transforms an input vector x into a discriminant vector y [13]. Each element w(m, n) in the weight matrix connects the nth input to the mth output.\nThe training dataset is denoted as (xp, tp), where xp is an N-dimensional input vector and tp is an M-dimensional target output vector. The index p ranges from 1 to N, where N\u2082 represents the total number of training patterns. To incorporate a threshold, the input vector x is augmented with an additional element set to 1, resulting in the augmented input vector xa = [1 : xT]T. Thus, xa contains Nu basis functions, where Nu = N + 1. For the pth training pattern, the output vector yp is computed as:\n$y_p = W \\cdot x_{ap}$\nwhere xap denotes the augmented input vector xa for the pth pattern."}, {"title": "2.2 Regression-Based Classifier", "content": "Training the linear classifier involves minimizing an error function E, which serves as a surrogate for a non-smooth classification error. Following the approach described in [14], we adopt a Bayesian perspective, where training can be viewed as maximizing the likelihood function or minimizing the Mean Squared Error (MSE) in a least squares sense. The MSE between the predicted outputs and target outputs is defined as:\n$E = \\frac{1}{N_v} \\sum_{p=1}^{N_v} \\sum_{i=1}^{M} [t_p(i) - y_p(i)]^2$\nHere, the target output for the correct class ic is denoted by tp(ic) = b, while for all incorrect classes id, tp(id) = 0, where b is a positive constant, typically set to 1. M represents the total number of classes, and a one-versus-all coding"}, {"title": "3 Mathematical Background", "content": "In neural networks, given an input vector x, the network computes a set of scores for each class c, represented as zc. These scores estimate the posterior probabili ties of classes, denoted by q(c|x), which are typically converted to probabilities using the softmax function:\n$q(c|x) = \\frac{e^{z_c}}{\\sum_i e^{z_i}}$\nFor simplicity, we use q(c|x) without explicitly including the model parameters 0. The cross-entropy loss measures the difference between the predicted class probabilities and the true class labels, defined as:\n$CE = - \\sum_{n=1}^{N} \\sum_{c} \\delta(c, c_n) \\log(q(c|x_n))$\nHere, N represents the number of training examples, c iterates over all possible classes, d(c, cn) is the Kronecker delta function (1 if c = cn, 0 otherwise), and xn and on denote the n-th training example and its true class label, respectively. Cross-entropy loss drives the network to produce accurate class probability estimates. From a Maximum Likelihood Estimation (MLE) perspective, minimizing the cross-entropy loss is equivalent to maximizing the likelihood of the observed data under the model. The likelihood P(D|0), where D represents the dataset and @ the model parameters, can be expressed as:\n$P(D\\|0) = \\prod_{n=1}^{N} q(c_n\\|x_n)$"}, {"title": "3.1 Problems With MSE Type Training of Neural Networks", "content": "Current regression-based classifiers, as well as some other models, face several challenges that hinder their performance. Our goal is to minimize the probability of testing error Petst in a multi-class application where M = Nc \u2265 2. Before we do this, let's consider problems that make it difficult to minimize the training MSE.\n(P1) Pattern bias errors: where the mean mt(p) of tp(i) differs from the mean my(p) of yp(i) increase E but not Pet.\n(P2) Consistent errors: The errors yp(i) - tp(i) may also be consistent, with their absolute values moving in the same direction as Pe. This arises when yp(ic) \u2264 tp(ic) or yp(ia) \u2265 tp(id) for any incorrect class id.\n(P3) Inconsistent errors: The errors yp(i) \u2013 tp(i) can be inconsistent, with their absolute values moving in a direction opposite to that of the probability of classification error Pe, while increasing the MSE. This typically occurs when Yp(ic) \u2265 tp(ic) for the correct class ic or yp(id) < tp(id) for any incorrect class ia.\n(P4) Outliers: Small consistent or inconsistent errors can escalate into outliers, which severely degrade performance [18].\n(P5) Redundant inputs: Excessive or irrelevant inputs lead to increased testing errors, a phenomenon often referred to as the Hughes phenomenon [18], commonly known as overfitting in machine learning. Redundant inputs can be categorized as:\n(C1) Inputs that are linearly dependent on other inputs.\n(C2) Inputs that are independent but provide no useful information for calculating the output discriminant vector y.\n(P6) Unbound Error Unlike for the Pe objective function, both consistent errors and inconsistent errors are unbounded, so E is unbounded as seen in the following lemma.\nLemma 1: Let p denote an arbitrary training pattern for a neural net classifier. Then lim|yp(i)|\u2192\u221e E \u2192 \u221e\nThe objective function Pe has certain properties as follows:\n1. Pe is bounded.\n2. Each classification error counts as \uad11, independent of:\n(a) The amount by which y(ia) exceeds y(ic),\n(b) The difference between my (p) and mt(p)."}, {"title": "4 The Potential for MSE-Based Optimal Classifiers", "content": "The minimum MSE solution for weight matrix W is obtained by solving equation 3. Let Wopt denote the weight matrix for a classifier (a linear classifier in our case) that is optimal in some sense, such as the minimum probability of error (MPE) or Bayes classifier, or a classifier solving problems P1 through P6.\nTheorem 1. W opt is the solution to a least squares problem.\nGiven Wopt, the optimal cross-correlation matrix Copt is found from equation (1) as:\n$C_{opt} = R \\cdot (W_{opt})^T$\nIn equation 12, the basis vectors Xp are given and unchangeable, so R is fixed. The only component of Copt under user control is the desired output vector t. Therefore, the optimal cross-correlation matrix is found as:\n$C_{opt} = E[X \\cdot (t_{opt})^T]$\nVarious classifiers with desirable properties can be designed through regression by properly choosing the desired output t. Equation 13 can be rewritten as:\n$C_{opt} = \\frac{1}{N_v} \\sum_{p=1}^{N_v} X_p (t_p)^T$\nAnd further as:\nc(i) = At(i)\nwhere c(i) is the ith column of Copt. The (n, p) element of the Nu \u00d7 N\u2082 matrix A is a(n,p) = Xp(n)/Nv. The pth element of the column vector t(i) is tp(i). Assuming that Nu \u2264 N and that A has rank Nu, equation 15 represents an underdetermined set of equations for t(i) and has uncountably many exact solutions. The challenge lies in finding one without prior knowledge of Wopt or Copt. Therefore, we need better ways for generating tp(i)."}, {"title": "4.1 Solution for Problem P1 and Nc = 2", "content": "Output reset (OR) [18] is used as an algorthm so solve the problem in P1. Use coded outputs so tp = tp(1) - tp(2) and\nYp = Yp(1) \u2013 Yp(2) = [tp(1) + ap] \u2013 [tp(2) + ap] = tp(1) \u2013 tp(2)\nIgnoring other types of noise. The N = 2 coded output case is immune to problem P1, unlike the uncoded case. Also, both coded and uncoded cases for Ne = 2 can have the same Pe. What happens for Nc > 2?"}, {"title": "4.2 Solution for Problem P3", "content": "Extending the idea from [18], this is similar to Ho-Kashyap [19] training. OR type 2 calculates tp(i) as follows:\n1. If yp(ic) > tp(ic) we set t'p(ic) = Yp(ic).\n2. If yp(id) < tp(id) for an incorrect class ia, we set t'p(id) = Yp(id).\nIn each OWO-BP iteration,\n1. BP Step: Calculate t(i) and use it in the delta function calculations.\n2. OWO Step: Re-calculate t(i) and use Xp \u00b7 (tp)T to update the C matrix.\nRefer to [18] for details on OWO-BP. OR type 2 clearly eliminates the inconsistent errors of (P3) and can greatly decrease training and validation errors [17-20]. Note that for a classifier having zero classification errors, E' will"}, {"title": "4.3 Solution for P1, P2 and P4", "content": "An improved OR is to change tp(i) so that E' ~ Pe\nCase 1: If xp is correctly classified, t'p(i) = yp(i) for all i, so no error results.\nCase 2: If xp is misclassified and yp(id) > Yp(ic) for one value of ia, then t'p(ic) = yp(ic) + b, tp(ia) = Yp(ia) \u2013 b.\nElse, t'p(i) = Yp(i)\nNote that this pattern contributes 2b2 to E', before the division by N\u03c5.\nCase 3: If xp is misclassified and yp(id) > Yp(ic) for K values of id, then t'p(ic) = Yp(ic) + K \u00b7 \u0454 and t'p(ia(k)) =\nYp(id(k)) e for 1 \u2264 k \u2264 K where\n$\\epsilon = b \\sqrt{\\frac{2}{K^2+K}}$\nElse, t'p(i) = yp(i)"}, {"title": "5 Enhancements in Linear Classifier Design", "content": "This section builds upon the linear classifier W described in subsection 2.2 and introduces a series of incremental improvements aimed at addressing the issues identified in (P1) through (P6). These enhancements involve modifications to the elements of the matrices R and C in equation (3)."}, {"title": "5.1 Target Adjustment using Output Reset", "content": "The objective of this subsection is to eliminate the inconsistent errors identified in (P1) and reduce variations in output vectors yp caused by (P5). This is achieved by developing new target outputs, denoted as t(i), while maintaining the constraint that the target margin satisfies tp(ic) - tp(ia) \u2265 1. Inconsistent errors can increase the error function E while either decreasing the classification error Pe or leaving it unchanged. Two sources of inconsistent errors are considered:\n1. Bias in the outputs, where the average of tp(i) differs from the average of yp(i). 2. Situations where yp(ic) > tp(ic) or yp(id) < tp(id), as described in (P1). To mitigate these inconsistent errors, we introduce a new error function E' [20] [21], where only the targets are adjusted, not the labels [22]. Following the approach outlined in [19], the new error function is defined as:\n$E = \\frac{1}{N_v} \\sum_{p=1}^{N_v} \\sum_{i=1}^{M} [t'_p(i) - y_p(i)]^2$\nHere, the adjusted target t\u201e (i) is expressed as:\n$t'_p(i) = t_p(i) + a_p + d_p(i)$"}, {"title": "6 Experimental Results", "content": "In this section, we present a comparative analysis of the 10-fold testing performance of three different linear classifiers: the softmax cross-entropy (SCE) classifier, the mean squared error with output reset (MSE-OR) classifier, and the sigmoidal mean squared error with output reset (SMSE-OR) classifier. The comparison focuses on the performance of the three classifiers, where each classifier is trained using the optimal learning factor optimization algorithm. The implementation details for these classifiers are described in the earlier sections. All models are trained for a single iteration, and 10-fold testing is employed to evaluate their performance. In this approach, the dataset is randomly partitioned into ten subsets of nearly equal size. In each fold, one subset is used for testing, seven subsets for training, and the remaining two subsets for validation. Before training, image data are normalized to have values between 0 and 1.\nThe models are trained for 5000 iterations, with accuracy evaluated based on the weights that yield the highest validation accuracy. The results for each classifier across various datasets are shown in Tables 1, 2, and 3."}, {"title": "7 Discussion", "content": "This study explored the use of MSE combined with a sigmoid activation function for classification tasks. Experiments conducted with a linear classifier highlighted the fundamental interaction between the objective function and activation mechanism, suggesting a unified framework that bridges classification and regression in neural networks.\n1. Initial results suggest that MSE with sigmoid may exhibit greater robustness to noise or mislabeled data compared to traditional cross-entropy-based methods. This potential advantage calls for further evaluation in real-world scenarios with imperfect datasets.\n2. The study reveals distinct differences in the learning dynamics when compared to categorical cross-entropy. Future work should involve a detailed analysis of convergence behavior and generalization, especially in the context of more complex deep learning architectures.\n3. The findings emphasize the need for a deeper theoretical understanding of how classification and regression tasks relate within neural networks, particularly in terms of learning trajectories and optimization dynamics."}, {"title": "8 Conclusion", "content": "This research presents an innovative approach to neural network design by proposing the use of MSE with sigmoid activation for classification tasks. While traditional models typically use categorical cross-entropy with softmax, our proposed method offers promising alternatives. By providing a framework that unifies classification and regression, this work broadens the theoretical foundations of deep learning. The initial results suggest that MSE with sigmoid may offer increased robustness to noise and data imperfections, making it a potential solution for developing more resilient classifiers in real-world applications with suboptimal data conditions. These findings challenge established norms and prompt a re-evaluation of the theoretical relationship between classification and regression in neural networks, potentially revealing new insights into learning dynamics. This study serves as a catalyst for rethinking conventional neural network paradigms, moving beyond cross-entropy towards more versatile and theoretically informed models. The exploration of MSE with sigmoid activation represents a meaningful step toward deeper understanding and broader applicability of neural network optimization principles."}, {"title": "9 Future Work and Open Questions", "content": "This study opens new avenues for further exploration. Evaluating the performance of MSE with sigmoid activation in advanced deep learning architectures (e.g., CNNs, RNNs, Transformers) across different domains is essential to determine its broader applicability. A comprehensive theoretical analysis of convergence and generalization properties for MSE with sigmoid-based classifiers is necessary to better understand their strengths and limitations. Additionally, designing specialized regularization techniques tailored for MSE and sigmoid could further enhance model robustness and generalization. Extensive benchmarking on diverse, real-world datasets will be crucial for a conclusive evaluation of this approach. Investigating the role of MSE with sigmoid in explainable AI may also offer insights into the decision-making processes of neural networks. Several open questions have emerged from this research:\n1. How does the convergence of MSE with sigmoid-based classifiers compare to that of traditional cross-entropy methods? Does it result in different optima, and what theoretical guarantees can be established?\n2. Does MSE with sigmoid foster distinct generalization properties compared to cross-entropy? Are there specific scenarios or dataset characteristics where it excels or shows weaknesses?\n3. Can a formal theoretical framework be developed to explain the observed ability of MSE with sigmoid to integrate classification and regression tasks? What are the fundamental mathematical relationships?\n4. Are existing optimization algorithms (e.g., Adam, SGD) effective for MSE with sigmoid, or is there a need for customized optimizers to fully exploit this approach?\n5. What new regularization methods, designed specifically for MSE with sigmoid, could enhance model robust-ness and generalization?"}]}