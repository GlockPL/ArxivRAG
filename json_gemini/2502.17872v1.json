{"title": "Contrastive Learning with Nasty Noise", "authors": ["Ziruo Zhao"], "abstract": "Contrastive learning has emerged as a powerful paradigm for self-supervised representation learning. This work analyzes the theoretical limits of contrastive learning under nasty noise, where an adversary modifies or replaces training samples. Using PAC learning and VC-dimension analysis, lower and upper bounds on sample complexity in adversarial settings are established. Additionally, data-dependent sample complexity bounds based on the l2-distance function are derived.", "sections": [{"title": "Preliminaries", "content": "In contrastive learning, for samples from a domain V, the learned representation is a distance \\(p:V\\times V \\rightarrow R\\). A popular distance function is lp-norm \\(p_p(x,y) = ||f(x) - f(y)||_p\\), for some representation function \\(f : V \\rightarrow R^d\\) in dimension d. Let \\(H\\) be a hypothesis class and \\(h_p \\in H\\) be an hypothesis with respect to an unknown distance function \\(p\\). A learning task is specified using a hypothesis class of boolean classifiers defined over an instance space, denoted V. A boolean classifier is a function \\(h: V^3 \\rightarrow {0,1}\\)."}, {"title": "The classical PAC model", "content": "In PAC model, the learning algorithm has access to labeled examples of the form \\((x,y^+,z^-)\\) from a distribution D. The examples are labeled by the target classifier h* in H. The goal of contrastive learning is to create a classifier from H which accurately labels subsequent unla-beled inputs. For a distance function p, the hypothesis with respect to this distance function is \\(h_p(x,y,z) = sign(\\rho(x, y) - \\rho(x, z))\\). When \\(h_p(x,y,z) = -1\\), the example will be labeled as \\((x,y^+,z^-)\\), meaning \\(\\rho(x, y) < \\rho(x, z)\\); otherwise \\(h_p(x,y,z) = 1\\) then label \\((x,y^-,z^+)\\).\nDefinition 1.1 (Contrastive learning, classical PAC case). A hypothesis class H is PAC learnable if there exist a learning algorithm that, for any \\(h^* \\in H\\), any input parameters \\(0 < \\epsilon < 1/2\\) and \\(0 < \\delta < 1\\) and any distribution D, when given access to samples in the form \\((x,y^+,z^-)\\), the minimum number of samples required is denoted as \\(n(\\epsilon, \\delta)\\), with probability at least \\(1 - \\delta\\) outputs a function \\(h_p \\in H\\) to achieve error rate \\(\\epsilon\\), that is\n\\(Pr_{(x,y,z)\\sim D}[h_p(x, y, z) \\neq h^*(x, y, z)] < \\epsilon\\)"}, {"title": "PAC learning with nasty noise", "content": "As in PAC model, a distribution D and a target classifier is given. For PAC model in the presence of nasty noise, the adversary draws a sample set S' of size n from the distribution D. Having full"}, {"title": "VC theory basics", "content": "Definition 1.3 (Shattering). Let X be an instance space. We say that a finite set \\(S \\subset X\\) is shattered by a hypothesis class H if, for each of the 2|S| possible labeling of the points in S, there exists some function in H consistent with that labeling.\nDefinition 1.4 (VC-dimension). The VC-dimension of a hypothesis class H, denoted VCdim(H), is the maximal size d of a set \\(S \\subset X\\) that can be shattered by H. If H can shatter sets of any integer d, we say H has infinite VC-dimension that VCdim(H) = x.\nLemma 1.5. For any two classes H and F over X,\n1. The class of negations \\({h|X\\backslash h \\in H}\\) has the same VC-dimension as the class H\n2. The class of unions \\({h\\cup f|h\\in H, f \\in F}\\) has the VC-dimension at most VCdim(H) + VCdim(F) +1.\n3. The class of intersections \\({h \\cap f|h\\in H, f \\in F}\\) has the VC-dimension at most VCdim(H) + VCdim(F) +1.\nDefinition 1.6. The dual \\(H^+ \\subseteq {0,1}^H\\) of a class \\(H \\subseteq {0,1}^X\\) is defined to be the set \\({x^+\\mid x \\in X\\) where x is defined by \\(x^+(h) = h(x)\\) for all \\(h\\in H\\).\nThe following claim gives a tight bound on the VC-dimension of the dual class:\nClaim 1.7. For every class H, \\(VCdim(H) > [log VCdim(H^+)]\\).\nIn the following discussion, we restrict our focus to finite-dimensional hypothesis classes. In this paper, the main use of the VC-dimension is in consisting \\(\\alpha\\)-samples.\nDefinition 1.8 (a-sample). A set of points \\(S \\subset X\\) is an \\(\\alpha\\)-sample for the hypothesis class \\(H\\subseteq {0,1}^X\\) under the distribution D over X, if it holds that for every h \u2208 H:\n\\(D(h) - \\frac{|S \\cap h|}{|S|} < \\alpha.\\)"}, {"title": "Theorem 1.9.", "content": "There is a constant c, such that for any class \\(H \\subseteq {0,1}^X\\) of VC-dimension VCdim(H), and distribution D over X, and any \\(\\alpha > 0, \\delta > 0\\), if\n\\(n \\geq \\frac{c}{\\alpha^2}(VCdim(H) +log \\frac{1}{\\delta})\\)\nexamples are drawn i.i.d. from X according to the distribution D, they constitute an \\(\\alpha\\)-sample for H with probability at least 1 \u2013 \u0431.\nDefinition 1.10 (Natarajan dimension). Let X be an instance space, Y be the set of labels, and let \\(H\\subseteq Y^X\\). We say that a set \\(S \\subset X\\) is N-shattered by H if there exist \\(f_1, f_2 : X \\rightarrow Y\\) such that \\(f_1(x) \\neq f_2(x)\\) for all \\(x \\in S\\) and for every \\(B \\subset S\\) there exists \\(g \\in H\\) such that:\n\\(g(x) = f_1(x) for x \\in B and g(x)) = f_2(x) for x \\notin B\\)\nThe Natarajan dimension Ndim(H) is the maximal size of a N-shattered set \\(S \\subset X\\).\nLemma 1.11. If |S| is finite, then for the sample complexity \\(n(\\epsilon, \\delta)\\) of the PAC case it holds that:\n\\(n (\\epsilon, \\delta) = O (\\frac{Ndim(H)log|S|}{\\epsilon} polylog (\\frac{1}{\\epsilon})\\) and \\(n (\\epsilon, \\delta) = \\Omega (\\frac{Ndim(H)}{\\epsilon} polylog(\\frac{1}{\\epsilon}))\\)\nVC-dimension is a special case of Natarajan dimension when |S| = 2."}, {"title": "Lower Bound", "content": ""}, {"title": "Lower bound in classical PAC case", "content": "Theorem 2.1 (Lower bound for arbitrary distances). For an arbitrary distance function \\(\\rho\\) and a dataset of size N, the sample complexity of contrastive learning is \\(n (\\epsilon, \\delta) = \\Omega (polylog(\\frac{1}{\\epsilon}))\\) in the classical PAC case.\nProof. Consider a graph that has a vertex representing each element in the dataset. Let the set of vertices be \\(V = {v_1, ..., v_n}\\). Let S be the set of all possible three-element combination consisting of vertices in V:\n\\(S = \\cup_{i\\in[N]}\\{(v_i, v_{i+1}, v_{i+2}), (v_i, v_{i+2}, v_{i+3}), ..., (v_i, v_{N-1}, v_{N})\\}\\)\nIf there exist a classifier can correctly label all pairs in S then changing the anchor within any pair in S should not affect its accuracy to correctly label the pair. So this classifier performs well on any arbitrary query from dataset V.\nThen we prove that the set of samples S is shattered. Let h be a true classifier for S. For every \\(i \\in [N]\\), we define a graph \\(G_i = (V_i, E_i)\\) where \\(V_i = {v_{i+1},..., v_N }\\) and \\(E_i\\) contains a directed edge \\((v_j, v_{j+1})\\) for each query \\((v_i, v_j, v_{j+1})\\) according to h. For example of labeling \\((v_i, v_j, v_{j+1})\\), the direction of the edge between \\((v_j, v_{j+1})\\) is from positive \\(v_j\\) to negative \\(v_{j+1}\\). The graph \\(G_i\\) is acyclic since it is an orientation of a path. Then we can topologically sort \\(G_i\\) to obtain some topological order \\(p^{(i)}_1, ..., p^{(i)}_{N}\\) for the vertices \\(v_{i+1},..., v_N\\). Consider a distance function defined as\n\\(\\rho(v_i, v_j) := n + p^{(i)}_j\\) for all \\(i < j \\leq n\\). Thus, all distances are in the range [N, 2N]. Therefore, this is a metric since triangle inequalities are satisfied."}, {"title": "Lower bound for lp-distances", "content": "For i, j, k such that \\(i < j, k\\) and |j \u2013 k| = 1, \\(v_i, v_j, v_k\\) will form a pair with anchor \\(v_i\\) in S. Ys will contain a labeling for this pair, if h give the label as \\((v_i,v_j^+, v_k^-)\\) then the edge between \\((v_j, v_k)\\) is from \\(v_j\\) to \\(v_k\\), accordingly \\(p^{(i)}_k > p^{(i)}_j\\) which indicates \\(\\rho(v_i,v_k) > \\rho(v_i, v_j)\\), vice versa. Therefore, we say that this distance function satisfies all the samples, the set of samples S is shattered. An \\(\\Omega(N^2)\\) lower bound on the VC-dimension follows. By applying Lemma 2.11, we have the lower bound of sample complexity \\(n(\\epsilon, \\delta) = \\Omega (polylog(\\frac{1}{\\epsilon}))\\)\nTheorem 2.2 (Lower bound for lp-distances). For any real constant \\(\\rho \\in (0,\\infty)\\), a dataset V of size N, and the lp distance \\(\\rho_p : V \\times V \\rightarrow R\\) in a d-dimensional space, the sample complexity of contrastive learning is \\(n(\\epsilon, \\delta) = \\Omega (polylog (\\frac{1}{\\epsilon}))\\) in the classical PAC case.\nProof. As shown in theorem 3.1, for any distance function, the lower bound of VC-dimension is \\(\\Omega(N^2)\\). Then we discuss the case that d < N, and will show that a sample set of size \\(\\Omega(N^d)\\) can be shattered. Let V be the dataset of size N. We construct a sample set S with N - d anchors and d other points with d > 2. We denote the dataset as V = \\({x_1,x_2, ..., x_{N-d}, y_1, y_2, \u2026, y_d}\\) where xi's represent anchors and yj's represent other points. The query set is defined as following:\n\\(S = \\cup_{i\\in[N-d]}\\{(x_i, y_1, y_2), (x_i, y_1, y_3), ..., (x_i, y_1, y_d)\\} \\)\nThere are (d-1)(n-d) samples in the set S. Recall that the distance function is lp-norm \\(\\rho_p(x, y) = ||f(x) - f(y)||_p\\), for some representation function \\(f : V \\rightarrow R^d\\) in dimension d. The corresponding hypothesis is \\(h_p(x, y, z) = sign (||f(x) - f(y) || - ||f(x) - f(z)||p)\\).\nNext, we define a representation function which can make the hypothesis hp satisfy labeling of queries. For points yj, let the j-th coordinate of f(yj) equal to 1, other coordinates equal to 0. For anchors xi, let the first coordinate of f(xi) be \\(\\frac{3}{p}\\). For j\u2208 \\({2,...,d}\\), \\(f(x_i)_j = 0\\) if \\((x_i, y_1,y_j)\\) is labeled as \\((x_i, y_1^+, y_j^-)\\), otherwise \\(f(x_i)_j = \\frac{1}{p}\\) if \\((x_i, y_1^+,y_j^+)\\). Using this representation function f, hp of will follow:\nHence, hp of can satisfy the labeling of all queries. The sample set S can be shattered. For the case that d = 1, any set of \\([\\]\\) disjoint queries can be satisfied, gives \\(\\Omega(N)\\) lower bound. Therefore, the lower bound of VC-dimension is \\(\\Omega(N^d)\\) when d < N."}, {"title": "Lower bound in nasty noisy case", "content": "Theorem 2.3. Let H be a non-trivial hypothesis class, \\(\\eta\\) be a noise rate. Given access to samples of the form \\((x,y^+,z^-)\\) from a distribution D and then corrupted by a nasty adversary, for any \\(\\epsilon < 2\\eta\\), \\(\\delta <1\\), there is no algorithm that learns a classifier from H with error \\(\\epsilon\\) with probability at least 1 - \\delta.\nProof. Consider there are two classifiers \\(h_1,h_2 \\in H\\) such that \\(Pr_D[h_1(x,y,z) \\neq h_2(x,y,z)] = 2\\eta\\). Assume that an adversary can force the labeled examples shown to the learning algorithm to be identically distributed whether h\u2081 or h\u2082 is the target classifier. Assume \\((x_1, y_1, z_1), (x_2,y_2, z_2)\\) be two samples from distribution D that satisfy \\(h_1(x_1,y_1,z_1) = h_2(x_1,y_1,z_1) = (x_1,y_1,z_1^+)\\) and \\(h_1 (x_2,y_2, z_2) = (x_2,y_2,z_2^-) \\neq h_2(x_2,y_2, z_2) = (x_2,y_2,z_2^+)\\). We define the distribution D to be"}, {"title": "Theorem 2.4.", "content": "For any non-trivial hypothesis class H, any noise rate \\(\\eta > 0\\), confidence parameter \\(0 < \\delta < \\frac{1}{2}\\) and \\(0 < \\Delta < \\epsilon\\), the sample size needed of contrastive learning with accuracy \\(\\epsilon = 2\\eta + \\Delta\\) in the presence of nasty noise with noise rate \\(\\eta\\) is \\(\\Omega(\\frac{1}{\\Delta^2})\\).\nProof. Consider a similar case shown in proof of Theorem 3.3. There are two classifier h1, h2 \u2208 H. Let \\((x_1,y_1, z_1), (x_2,y_2, z_2)\\) be two samples from distribution D that satisfy \\(h_1(x_1,y_1,z_1) = h_2(x_1,y_1,z_1) = (x_1,y_1,z_1^+)\\) and \\(h_1(x_2,y_2, z_2) = (x_2,y_2, z_2^-) \\neq h_2(x_2,y_2, z_2) = (x_2,y_2, z_2^+)\\). We define the distribution D to be \\(D(x_1,y_1,z_1) = 1 - \\epsilon\\), \\(D(x_2, y_2, z_2) = \\epsilon\\) and D(x,y,z) = 0 for all other samples. The target classifier h* can be either h\u2081 or h2.\nThe nasty adversary strategy is also similar. Let n be the size of samples required by the learning algorithm. First, adversary i.i.d draws n samples from the distribution D and labels them according to the target classifier. Then, for each occurrence of \\((x_1, y_1, z_1)\\), the adversary remain it unchanged, while for each occurrence of \\((x_2, y_2, z_2)\\), the adversary flips the label with probability \\(\\frac{\\epsilon}{2}\\). The modified samples indeed according to the binomial distribution Bin(n, \\eta\\), since distribution D is known to the adversary and Pr[Flip label of \\((x_2, Y_2, z_2))] = \\epsilon \u00b7 \\frac{1}{2} = \\eta\\. The modified sample set S is given to the learning algorithm. We denote \\(h^*((X_2,Y_2, z_2)) = (x_2, y_2, z_2^-)\\), labeled samples in the given set S are distributed according to the following distribution:\nWe will show that an algorithm that create a classifier from H with accuracy \\(\\epsilon\\) using samples drawn from distribution D and the size of samples is \\(m < \\frac{1}{\\frac{37}{42}\\eta\\Delta(1-\\eta)}\\). Let A be the algorithm. A outputs a classifier h using samples of size n. We denote the expected error of h as errA(n). Let B be the Bayes strategy: if majority label of \\((x_2, y_2, z_2) = (x_2,y_2, z_2^-)\\), outputs h\u2081; otherwise, outputs h2. We denote the expected error of the output classifier as errB(n). Since B minimizes the probability of choosing the wrong classifier, we have errB(n) \\leq errA(n) for all m."}, {"title": "Claim 2.5.", "content": "Let \\(S_{N,p}\\) be a random variable distributed by the binomial distribution with parameters N and p, and let q = 1 - p. For all \\(N > \\frac{37}{42}\\):\nBy Claim 3.5 with m ~ Bin(M, \\frac{\\eta}{\\eta+\\Delta}\\), if \\(M > \\frac{37(2\\eta+\\Delta)^2}{\\eta(\\eta+\\Delta)}\\), we have\nthen we consider the following inequality:\nwhich is implied by\nwhich is implied by the following two conditions:\nThese two conditions holds if we assume \\(\\frac{\\eta}{\\eta+\\Delta} \\geq \\frac{1}{2}\\), which"}, {"title": "Finally,", "content": "we have that Pr[errA(n) > \\epsilon] > Pr[errB(n) > \\epsilon] > Pr[BAD1] > \\frac{1}{342} under assumption\nthatPr< 37\u03b7/109, and, if we have that thenWe do so\n, we could prove that for a large enough a > 0We define as\n> log . Thus, the sample set of size log is a sample for the class of symmetric\nFinally, the problem is reduced to class\nbound\nis a\nsamples shown to it. However, is misclassifies all\nthe sample set We have proven for a sample set of size we have the with at\ndata points that have been and so the error of algorithm is at most a = O with a large constance\nfor any data points chosen randomly from and for all data point chosen at random from , is is at least of\ndimension Therefore, algorithm requires at least with a large constant to do PAC under this model."}, {"title": "For any non-trivial hypothesis class H with VCdim \u2265 3, any 0<\u03b5<1,0<\u03b4<1/2 and 0\u0394&lt;\u03b5, the sample size", "content": "Let set of samples in the form of V by hypothesis is the power set proof and is what\nmeans For each , is that the set is the same set"}, {"title": "Upper Bound", "content": ""}, {"title": "Upper bound in classical PAC case", "content": "Theorem 3.1 (Upper bound for arbitrary distances). For an arbitrary distance function \\(\\rho\\) and a dataset of size N, the sample complexity of contrastive learning is \\(n(\\epsilon, \\delta) = O (polylog(\\frac{1}{\\epsilon}))\\) in the classical PAC case.\nProof. Consider any set of samples \\(\\{(x_i, y_i, z_i)\\}}_{i=1,...,k}\\) of size k > N2. There exists a data point x such that there are at least N samples which have x as their anchor element. We denote them as \\((x, y_1, z_1), ..., (x,y_N,z_N)\\). Consider a graph that has a vertex corresponding to each element in the dataset. Create an undirected edge in this graph between each pairs of vertices \\((y_1, z_1), \u2026\u2026\u2026, (y_N, z_N)\\). Since the number of edges is equal to the number of vertices, there must exist a cycle C in this graph. We can index the vertices along this cycle as \\(v_1, ..., v_t\\). Now consider the labeling of the samples \\(\\rho(x, v_1) < \\rho(x,v_2) < \\rho(x,v_3) < ... < \\rho(x,v_t) < \\rho(x, v_1)\\). No distance function can satisfy this labeling and hence not all different labelings of this sample set are possible. Any sample set with size larger than N2 will not be shattered. Thus, the upper bound of VC-dimension is N2. By applying Lemma 2.11, we have the upper bound of sample complexity \\(n (\\epsilon, \\delta) = O (polylog(\\frac{1}{\\epsilon}))\\).\nTheorem 3.2 (Upper bound for lp-distances). For integer p, a dataset V of size N, and the lp dis-tance \\(\\rho_p : V \\times V \\rightarrow R\\) in a d-dimensional space, the sample complexity of contrastive learning in classi-cal PAC case is upper bounded as following: for even p, \\(n(\\epsilon, \\delta) = O (min(N^d,N^2) polylog(\\frac{1}{\\epsilon}))\\); for odd p, \\(n(\\epsilon, \\delta) = O (min(N^dlogN, N^2) polylog (\\frac{1}{\\epsilon}))\\); for constant d, \\(n(\\epsilon, \\delta) = O (polylog (\\frac{1}{\\epsilon}))\\).\nProof. In this proof, we will show the VC-dimension of contrastive learning for lp-distances using a dataset V of size N. We denote the dimension of representation space as d. For the first two cases, we assume d < n; in the third case, we consider constant d. The upper bound of VC-dimension is O(Nmin(d, N)) for even p \u2265 2 and O(NdlogN) for odd p > 1. To prove this, it is suffices to show that for every set of samples \\(S = (x_i, y_i, z_i)_{n=1}\\) of size n = \u2229(Nmin(d, N)) there exists labeling of S that cannot be satisfied by any embedding in a d-dimensional lp-space. By applying Lemma 2.11, we can have upper bounds for sample complexity \\(n(\\epsilon, \\delta)\\).\nOur proof uses the following result in algebraic geometry:\nClaim 3.3. Let m \u2265 l \u2265 2 be integers, and let P1,..., Pm be real polynomials on l variables, each of degree k. Let\n\\(U(P_1, ..., P_m) = {x \\in R^l|P_i(x) \\neq 0 for all i \\in [m]\\}\\)\nbe the set of points \u2208 R which are non-zero in all polynomials. Then the number of connected components in U(P1, ..., Pm) is at most \\((\\frac{4ekm}{l})^l\\).\nUpper Bound for Even p We denote the embedding of each data point v \u2208 V in d-dimensional space as f(v) = (f1(v), f2(v), ..., fd(v)) where fi is the i-th coordinate of the representation function f: V \u2192 Rd. Let V = \\((f_1(v_1), ..., f_d(v_1), ..., f_1 (v_N), ..., f_d(v_N))\\). For every sample (x, y, z) where x, y, z \u2208 V, we define the polynomial \\(P_{x,y,z} : R^{Nd} \\rightarrow R\\) as\n\\(P_{x,y,z}(V) = \\sum_{j=1}^d(f_j(x) - f_j(y))^p - \\sum_{j=1}^d(f_j(x) - f_j(z))^p.\\)"}, {"title": "Finally,", "content": "we have that We define that is is at for those for\nmust equal to and the distance must the\nThe set of the is for the number of be a there is no upper under assumption"}, {"title": "Data-dependent Sample Complexity", "content": "k negative\n1. \\(L_{con}(f) = E[l\\{(\\rho(f(x), f(x_i)}\\}_{i=1}^k)]\\)\n2. \\(\\widehat{L}_{con}(f) = \\frac{1}{n}\\sum_{j=1}^nl\\{(\\rho(f(x_j), f(x_{ji})\\}_{i=1}^{k+1})\\}\\\nTheorem 4.1. Assume \\(|| f(\\cdot)||_2 \\leq R\\) for any f \u2208 F. Let S be a sample set in the form of \\((x, y^+, z^-)\\). Let l : Rk \u2192 R+ is L-lipschitz w.r.t. the l2-norm. Then with probability at least 1 \u2013 5 over the training set S, for any f \u2208 F\nwhere\nLemma 4.2 (Vector contraction lemma). Let X be any set, \\((x_1,...,x_n) \\in X^m\\), let F be a class of functions f: X \u2192 l2 and let hi: l2 \u2192 R have Lipschitz norm L. Then\nLemma 4.3 (Generalization error bound). For a real function class G whose functions map from a set Z to [0,1] and for any \\(\\delta > 0\\), if S is a training set composed by n iid samples \\(z_j^{\\eta}=1\\), then with probability at least 1 \u2013 5, for all g \u2208G"}, {"title": "Proof of Theorem 5.1.", "content": "First, consider the Lipschitz continuity of distance function. Let f': V2 \u2192\nR2d be defined as\nand \\(\\rho: R^{2d} \\rightarrow R\\) be defined as\nThen\nClearly, the 12-norm distance function is 1-Lipschitz w.r.t. 12-norm. Thus, by lemma 5.2, we have\nNext, we proof theorem 5.1. According to L-Lipschitz of loss function l w.r.t. 12-norm, applying lemma 5.2, we have"}, {"title": "4.4.", "content": "If the generalization error , the data-dependent"}]}