{"title": "Performance-Driven QUBO for Recommender Systems on Quantum Annealers", "authors": ["Jiayang Niu", "Jie Li", "Ke Deng", "Mark Sanderson", "Yongli Ren"], "abstract": "We propose Counterfactual Analysis Quadratic Unconstrained Binary Optimization (CAQUBO) to solve QUBO problems for feature selection in recommender systems. CAQUBO leverages counterfactual analysis to measure the impact of individual features and feature combinations on model performance and employs the measurements to construct the coefficient matrix for a quantum annealer to select the optimal feature combinations for recommender systems, thereby improving their final recommendation performance. By establishing explicit connections between features and the recommendation performance, the proposed approach demonstrates superior performance compared to the state-of-the-art quantum annealing methods. Extensive experiments indicate that integrating quantum computing with counterfactual analysis holds great promise for addressing these challenges.", "sections": [{"title": "1 INTRODUCTION", "content": "The application of quantum computing [10, 45] in optimization challenges has grown [4, 6, 26, 36, 52]. A quantum annealer solves combinatorial optimization problems like Quadratic Unconstrained Binary Optimization (QUBO) problems [21]. Recent studies focus on formulating problems as QUBO problems, e.g. training problems in machine learning methods [13], instance selection problems"}, {"title": "2 RELATED WORK", "content": null}, {"title": "2.1 Quantum Annealing", "content": null}, {"title": "2.1.1 Recent applications", "content": "With the rapid advancement of quantum computing architectures [26, 36], researchers have started to explore the potential of leveraging quantum computers' computational advantages to address certain general problems, particularly in domains frequently confronted with NP-hard (non-deterministic polynomial-time hardness) optimization challenges [4, 6, 52] and the increasing computational demands of deep learning [2, 7, 11, 13]."}, {"title": "2.1.2 Coefficient matrix", "content": "The focus of existing research on quantum annealing based recommender systems is on formalizing problems into QUBO problems, with the key challenge being how to construct the coefficient matrix Q. In the context of the cold-start problem in recommender systems, CQFS [31] first trains a recommendation model based solely on user-item interactions, followed by another model trained using features. The coefficient matrix Q is constructed by comparing the consistency of item similarity between the two models. Although Nembrini et al. [16] mainly evaluates the applicability of quantum annealers in ranking and classification tasks, they also proposed MIQUBO, COQUBO and QUBO-boosting. Specifically, MIQUBO fills the coefficient matrix Q with mutual information and conditional mutual information between features and the classification labels, directing the optimization towards using the fewest features to maximize the dependence of the feature set on the classification task. On the other hand, COQUBO fills the coefficient matrix with the correlation between features and the classification labels, aiming to maximize the relationship between the selected feature set and the classification task. For QUBO-boosting, it constructs the coefficient matrix Q based on the impact of features on the final classification outcome. Similar to feature selection problems, instance selection using quantum annealers [32] involves embedding the relevance of training documents into the coefficient matrix, thereby identifying a smaller, more relevant training sample set for the task."}, {"title": "2.2 Counterfactual Analysis", "content": "Due to the opacity and lack of interpretability of deep learning models, which are often regarded as black-box models, it is challenging for researchers to understand and comprehend the underlying mechanisms of these models [44, 49].\nCounterfactual analysis is a causal inference tool that introduces perturbations to the internal structure or input of a model, allowing researchers to study its interpretability [19, 41] by observing how the model changes before and after the perturbation. This approach helps model designers understand which factors impact the model, enabling them to design more concise and efficient models, improve the model's final performance, and enhance downstream user experience [9, 18, 51]. For instance, CF2 [39] method combines the strengths of both factual reasoning and counterfactual analysis, accurately identifying the nodes and edges in graph structures that are crucial for the current classification task, thereby improving the classification accuracy of graph neural networks."}, {"title": "2.3 Gaps", "content": "The existing feature selection methods using quantum annealers focus on the coefficient matrix of the QUBO problem, attempting to align the optimization direction of the QUBO problem with the optimization needs of feature selection. However, existing methods do not align with the performance of the recommendation problem directly, thus hard to keep consistency with the feature selection requirements in recommender systems. For example, information-theory-based [17, 33, 42] feature selection methods like MIQUBO and CoQUBO [16] only incorporate dependencies between features and the classification labels into the coefficient matrix Q, making it less effective to optimize the model performance directly. Similarly, QUBO-Boosting [16] only uses the model outcomes. Although CQFS [31] incorporates collaborative information, it targets at cold start problem and relies on comparing item similarities from collaborative filtering models and content-based models. On the other hand, in recommender systems, existing work on counterfactual analysis mainly focuses on interpretability, with little attention paid to feature selection. To fill the research gap, we propose CAQUBO, which leverages counterfactual analysis to construct the coefficient matrix by connecting the relationship between features directly to the performance of recommender systems, thereby aligning the optimization direction of the QUBO problem to improve recommendation performance."}, {"title": "3 PRELIMINARIES", "content": null}, {"title": "3.1 Notation", "content": "Assume we have a dataset D = (U, V, F) that can be used to train the recommendation system, where U = {U1, U2,..., Um}, V = {v1, v2, ..., vn}, and F = {f1, f2, ..., f|F|} represent users, items, and the features of the items, respectively. Each item vi corresponds to a d-dimensional feature vector, denoted as f\u00a1 \u2208 Rd. We use a binary matrix B\u2208 {0, 1}m\u00d7n to represent the user-item interaction matrix, where Bij = 1 indicates that user ui has interacted with item vj, otherwise Bij = 0. The base model, i.e. the model for recommendation, is defined as Ge, where \u0398 represents the model parameters trained on the data D and the interaction matrix B. We can obtain the top-N recommendation list R(ui, N) for a specific user ui based on the model Ge, The top-N recommendations for all users can be written as R(U, N) = G(D, \u0392\u0399\u0398)."}, {"title": "3.2 QUBO", "content": "(Quadratic Unconstrained Binary Optimization) [21] is a common form of optimization problem widely used in quantum computing (such as Quantum Annealers) [21], combinatorial optimization (such as max-cut, graph coloring problems), and machine learning [13] (such as feature selection). It can be specifically formalized as an n-variable quadratic problem, with the objective of finding a set of solutions that minimize the value of the function, which can be expressed as follows:\nmin Y = xT Qx, (1)\nX\nwhere Y corresponds to the ground state energy in Quantum Annealing (QA) [29, 45], and x is a binary vector of length |F|, which denotes the size of the feature section F, with each element xi of the vector being either 0 or 1. Coefficient matrix Q is a symmetric matrix, where each element represents the relationship between the elements of x. |F| is the total number of features. Namely, in the context of feature selection, the elements in vector x are related to whether their corresponding features are selected; the elements in Q represents the relationship between features and the optimization problem to solve, determining the optimization direction of the QUBO problem.\nTo control the number of selected features, a penalty term is added to Equation 1 [16], which is then transformed to:\nmin Y = xT Qx + (\u03a3  Xi xi-k) (2)\nX i=1"}, {"title": "3.3 Coefficient Matrix Q", "content": "The values of the coefficient matrix Q represent the relationship between features and the optimization problem to solve. For the feature selection problem in recommender systems, let Indiv(fi) represent the contribution of feature fi to the performance of the recommendation model, while Comb(fi, fj) indicate the contribution to recommendation performance when both features fi and fj are selected together. The optimization goal is to maximize the contribution of the selected feature set to the model's performance. So, Q can be formulated as:\nQij =  [-Comb(fi, fj) if i \u2260 j\n{-Indiv(fi) if i = j.(3)"}, {"title": "4 CAQUBO", "content": "This section presents the performance-driven CAQUBO (Counterfactual Analysis Quadratic Unconstrained Binary Optimization) model"}, {"title": "4.1 Counterfactual Instances", "content": "Counterfactual Analysis adds perturbations to the base model's input variables and observes the changes before and after the perturbations [25, 40, 46, 50]. In this paper, we refer to these changes as counterfactual instances. Specifically, similar to [40, 50], in the context of feature selection for recommender systems, we measure the impact of item features by excluding the corresponding feature and analyzing the difference in recommendation performance between the recommendation lists generated by the base model with and without the corresponding feature. Following [8, 39, 47], we define these perturbations in the form of a mask:\nFmask = F \u2296 Mc,(4)\nwhere Mc sets certain features of all items to zero. Note, as defined in Equation 1, QUBO solves which features to be selected, so Fmask is defined at feature level accordingly. Namely, these excluded features of all items will be the same.\nNext, we employ the recommendation performance metric Mtc (e.g. nDCG [24], or Recall [23]) for Counterfactual Analysis, which is defined as:\n{ E\u2081 = G(FO)Mtc - G(Fi mask)Mtc\nEij = G(FO)Mtc - G(F) mask)Mtc,  (5)\nwhere Ei represents the counterfactual instance of removing fi, and Eij represents the counterfactual instance of removing both fi and fj. G(F)Mtc represents the Mtc value obtained by the Ge using all item features set F, while G(Fi Fmask|)Mtc represents the Mtc value obtained by the Ge using features set which is set F removing fi, The same applies to G(F) Mtc. G(F)Mtc. It is worth noting that when E > 0, it indicates that the performance of the base model decreases after removing the feature, while E < 0 indicates the performance of the base model improves. While E = 0, it means there is no influence on the performance of the base mode when removing corresponding feature(s)."}, {"title": "4.2 Construction of Q for QUBO", "content": "Since the QUBO problem is a minimization problem, we define Q as follows with counterfactual instances E:\nQij =  -E\u00a1 if i = j\n{(-Eij if i \u2260 j (6)"}, {"title": "4.3 Performance Analysis", "content": "In this section, we attempt to answer why and how the proposed CAQUBO drives the optimization direction of the QUBO problem toward the optimization of recommendation performance. First, the aim of QUBO is to minimize the energy value Y as defined in Equation 1. So, it is reasonable to assume: that because CAQUBO is performance-driven, there must be a clear correlation between the minimized Y values and the final recommendation performance when using the corresponding selected feature set. In order to investigate this, we conducted an analytic experiment: 50 features were randomly selected from the 150ICM dataset (detailed in Section 5), and CAQUBO and MIQUBO were deployed to select 45 features. It is observed that there is a clear and smooth negative correlation between Y and the final performance measured in nDCG@10 from CAQUBO, while there is an unclear trend in that of MIQUBO. This demonstrates that considering the relationship between features and the ground truth (e.g. Mutual Information) in MIQUBO cannot perform as well as considering the relationship between features and the recommendation performance in CAQUBO."}, {"title": "5 EXPERIMENT", "content": null}, {"title": "5.1 Experimental Design", "content": null}, {"title": "5.1.1 Datasets", "content": "We utilized two datasets, 150ICM and 500ICM, provided by CLEF 2024's QuantumCLEF Lab\u00b9, which focuses on benchmarking quantum annealing for information retrieval and recommender systems. The 150ICM dataset comprises 1,881 users, 5,000 items, and 64,890 interactions, with 150 sparse features per"}, {"title": "5.1.2 Baseline Models", "content": "To benchmark the performance of our proposed CAQUBO model, we selected a set of well-established baseline models, including CQFS [31], MIQUBO, COQUBO, and QUBO-Boosting [16]: i) CQFS: Following [31], CQFS fills the coefficient matrix based on the item similarity from collaborative filtering and content-based models, but the collaborative filtering required for the method is the based model used in this paper. \u2022 ii) MIQUBO and CoQUBO: Following [16], MIQUBO fills the coefficient matrix with mutual information and conditional mutual information between features and labels, while CoQUBO fills the matrix with the correlation between features and labels. The classification labels are replaced with interactions in the recommendation system, and interaction data undergoes 1:1 negative sampling. iii) QUBO-Boosting: Following [16], QUBO-Boosting fills the coefficient matrix with the predicted values from the Support Vector Classifier and the true labels. The SVC is replaced with the base model."}, {"title": "5.1.3 Metrics", "content": "We evaluated model performance using the Normalized Discounted Cumulative Gain (nDCG), a metric that measures ranking quality by considering both the relevance of items and their positions within the top-N recommendations. This is particularly effective for recommendation systems, as it takes into account not only the presence of relevant items but also their order in the ranked list. We set N to 10 in this study."}, {"title": "5.1.4 Base Models", "content": "To thoroughly evaluate the performance of the proposed CAQUBO model across various base recommendation models, we tested approaches ranging from classical methods to neural network-based models: i) Item-KNN [38], a classical model that uses the user-item interaction matrix to predict potential interactions based on similarities calculated using item features; \u2022 ii) MLP-DP/MLP-CON, we developed two neural network-based models: MLP-DP and MLP-CON, both utilizing a Multi-Layer Perceptron (MLP [14]), to evaluate CAQUBO's performance with different item feature processing strategies. These models allow us to investigate various methods for integrating item features. MLP-DP fuses the item's latent embedding with its features, passing this combined data through an MLP. The result is then used in a dot product with the user's embedding to predict interactions. MLP-CON, on the other hand, concatenates the item and user embeddings, along with the item's features, into a single vector, which is passed through an MLP to generate predictions. Both models employ Bayesian Personalized Ranking (BPR [37]) loss with a 1:1 negative-to-positive sample ratio for optimization. These experiments provide valuable insights into how different feature processing techniques impact model prediction performance; iii) NCF [22], Neural Collaborative Filtering (NCF) leverages both an MLP and Generalized Matrix Factorization (GMF) to model the non-linear and linear interactions"}, {"title": "5.1.5 Hyperparameter Tuning", "content": "For Item-KNN, we utilized a fixed number of neighbors (we set it as 100 by following CLEF 2024's QuantumCLEF Lab), to calculate predictions. For the neural network-based base models, a grid search [27] was performed to fine-tune key hyperparameters such as learning rate, batch size, and the number of layers. This optimization process was critical for achieving the best model performance and ensuring the accuracy of the counterfactual results."}, {"title": "5.1.6 The Configuration of k", "content": "The values of the number of features selected, k (in Equation 2) are set to [130, 135, 140, 145] for 150ICM, and [350, 400, 450, 470] for 500ICM. Additionally, we allow both CAQUBO and all baseline models to automatically determine the optimal number of features to maximize recommendation performance, indicated by *."}, {"title": "5.1.7 QUBO Optimization Methods", "content": "To assess the performance of traditional versus quantum optimization techniques, we selected traditional methods that can be used to optimize QUBO problems, including Simulated Annealing (SA) [3], Stochastic Gradient Descent (SGD) [54], and Tabu Search (TS) [20] as comparison methods. These are compared against Quantum Annealing (QA) [35] and Hybrid [28] approaches implemented on D-Wave [28], both of which are Quantum Processing Unit (QPU). The Hybrid method, which integrates classical and quantum optimization, is particularly useful for addressing the limitations of quantum annealing when dealing with large-scale data that exceeds the available qubits [15]. The comparison evaluates their optimization capabilities, computational efficiency, and scalability across different data sizes."}, {"title": "5.2 Experimental Results", "content": null}, {"title": "5.2.1 Baselines comparison", "content": "This section provides a comprehensive analysis of CAQUBO's performance compared to baseline models (CQFS, QUBO-Boosting, CoQUBO, and MIQUBO) across two datasets (150ICM and 500ICM), different optimization methods (SA, SGD, TS, QA, Hybrid), various number of features (k), and base models (Item-KNN, MLP-DP, NCF, MLP-CON). Special attention is given to the case when k = *, where the model is allowed to automatically select the optimal number of features. We did not optimize models using the QA optimization method in the 500ICM dataset due to quantum computational limitations, with '-' representing the cells where results are not available. All results of nDCG@10 are shown in Table 1.\nIn both 150ICM and 500ICM datasets, CAQUBO generally demonstrates strong performance across different optimization methods. However, there are scenarios where the baselines perform comparably or even better, depending on the optimization method, base model, and feature selection value. For the SA method, CAQUBO shows notable improvements in many cases. For example, in the Item-KNN model, when k = 130, the best-performing baseline is"}, {"title": "5.2.2 Stability of QA", "content": "We conducted experiments to examine the current stability of the used QA. As shown in Table 2, where letting the optimization methods (QA, Hybrid, and SA) to decide how many features to select by themselves, their final optimization results are comparable. However, when giving a constrain by setting k = 90% * |F| (namely selecting 90% of features from F, the corresponding Y values of QA increases significantly when the scale (the size of the feature set |F| increases, which is different from other optimization methods, SA and Hybrid. We refers this as QA's instability, and we study this further from the following three perspectives: i) the size of the problem (the size of feature set |F|): in D-wave, QA results are obtained by sampling 2,000 times and selecting the minimum Y. Figure 3a shows how the distribution of Y from single-sample QA varies with |F|, indicating that current QA methods remain stable only for small problem sizes but struggle to find optimal solutions as the problem size increases. ii) the size of the samples: In a single-sampel of QA, we vary the size of the samples ranging from 50, 500, 2000 to 10,000. Figure 3b shows that increasing the sample size can reduce the instability of single-sample QA. iii) the difficulty of the problem: following [1], we use data sparsity to measure the difficulty of the problem. Specifically, we set the size of the feature set to 50, then we randomly drop the feature values from all features with certain percentage to get various feature sets with various sparsities. Specifically, we randomly drop 60%, 40% and 20% of feature values, then measure their corresponding relevant improvements to their corresponding recommendation performance with all feature performance (as shown in Figure 3c). It is observed that QA tends to get larger but less stable improvement when the problem is harder (the feature set is sparser)."}, {"title": "5.2.3 Efficiency of QUBO Optimization Methods", "content": "To highlight the speed advantage of quantum annealers, we compared the solving times of QA, Hybrid, and classical methods such as SA, SGD, and TS across different problem scales in Figure 4. The results show that the solving time for the latter three methods increases exponentially with problem size, whereas the solving times for QA and Hybrid remain largely unaffected by problem scale."}, {"title": "5.2.4 Necessity of Quadratic Terms", "content": "Comparing the effects of removing two features simultaneously (\"comb\") versus removing a single feature (\"Indiv\") is essential, as it helps determine whether the current problem can be formulated as a QUBO problem and if a quantum annealer is necessary for solving it. The inter dependencies between features suggest that removing two features at once produces more accurate counterfactual instances for feature selection than removing only one. We selected two base models (Item-KNN and MLP-DP) to evaluate the performance of feature selection by comparing the use of only the diagonal elements of matrix Q (representing counterfactual instances generated by the"}, {"title": "5.2.5 Base Model Training Strategies", "content": "In Equation 5, the counterfactual instances E\u00a1 and Eij for the neural network-based model are generated during inference using the already trained base models. We retrained the NCF model using the features selected by CAQUBO and evaluated the model's performance on the test set, a process we refer to as CAQUBO Retrain. As shown in Table 4,"}, {"title": "6 CONCLUSION", "content": "This paper proposed CAQUBO, a performance-driven QUBO method for recommender sysrtems on quantum anneals, and it outperforms the state-of-the-art baselines models in terms of recommendation performance. Furthermore, based on the CAQUBO, we examined the stability of QA on the performance of recommendation problems and the necessity of the quadratic term in a QUBO-based recommender systems."}]}