{"title": "SparseFormer: Detecting Objects in HRW Shots via Sparse Vision Transformer", "authors": ["Wenxi Li", "Yuchen Guo", "Jilai Zheng", "Haozhe Lin", "Chao Ma", "Lu Fang", "Xiaokang Yang"], "abstract": "Recent years have seen an increase in the use of gigapixel-level image and video capture systems and benchmarks with high-resolution wide (HRW) shots. However, unlike close-up shots in the MS COCO dataset, the higher resolution and wider field of view raise unique challenges, such as extreme sparsity and huge scale changes, causing existing close-up detectors inaccuracy and inefficiency. In this paper, we present a novel model-agnostic sparse vision transformer, dubbed SparseFormer, to bridge the gap of object detection between close-up and HRW shots. The proposed SparseFormer selectively uses attentive tokens to scrutinize the sparsely distributed windows that may contain objects. In this way, it can jointly explore global and local attention by fusing coarse- and fine-grained features to handle huge scale changes. SparseFormer also benefits from a novel Cross-slice non-maximum suppression (C-NMS) algorithm to precisely localize objects from noisy windows and a simple yet effective multi-scale strategy to improve accuracy. Extensive experiments on two HRW benchmarks, PANDA and DOTA-v1.0, demonstrate that the proposed SparseFormer significantly improves detection accuracy (up to 5.8%) and speed (up to 3x) over the state-of-the-art approaches.", "sections": [{"title": "1 Introduction", "content": "Object detection has been a challenging yet fundamental task in computer vision for the last decade. Close-up settings such as MS COCO [28] have shown impressive performance with successful real-world applications. However, with the development of imaging systems and new application requirements like UAVs, detecting objects in high-resolution wide (HRW) shots with square-kilometer scenes and gigapixel-level resolutions have drawn increasing attention [5, 10, 15, 22, 25, 26, 31, 34, 35, 48, 58, 59].\nDetecting objects in HRW shots using close-up detectors is not effective due to several unique characteristics of HRW shots, as found in PANDA [49] and DOTA [51], compared to close-up shots like MS COCO. The most significant challenge is the sparse information in HRW shots, where objects often cover less than 5% of the image. This makes it difficult for detectors to extract key features from a sea of background noise, resulting in false positives within the background and false negatives within the object areas during training and testing. The second challenge is the varying scales of objects in HRW shots, with changes up to 100 times. Detectors relying on fixed settings of the receptive field and anchors cannot adapt to these extreme scales, as shown in Figure 1. For instance, YOLOv8 [20] underperforms in detecting small objects. While DINO [60] shows marginal improvement, it still falls short in adapting to such exaggerated scale changes, resulting in subpar detection of larger objects (Figure 2). Additionally, the typical two-stage downsampling schemes [5, 10, 21, 34] miss more small objects.\nThe slicing strategy [1] can result in incomplete boxes when using NMS to merge prediction boxes, as shown in Figure 5. Therefore, it is imperative to bridge the gap between object detection in close-up and HRW shots.\nMotivated by recent advanced techniques [33, 36, 42, 45, 46, 53] to enhance object detection accuracy, we present a novel detector for HRW shots that employs a sparse Vision Transformer, called SparseFormer. SparseFormer uses attentive tokens selectively to concentrate on regions of an image where objects are sparsely distributed, facilitating the extraction of fine-grained features. To achieve this, it learns a ScoreNet to assess the importance of regions. By examining the variance of importance scores of all regions, our SparseFormer prioritizes regions that capture rich fine-grained details. In this way, it can focus on complex image regions rather than less significant ones (e.g., smooth content from the background). Concurrently, it divides each HRW shot into non-overlapping windows to extract coarse-grained features. Sharing a similar spirit with the receptive field strategy of the original Vision Transformer [8], our proposed SparseFormer combines coarse and fine-grained features, achieving much higher efficiency than Swin Transformer [30]. This greatly helps to handle large scale variations and detect both large and small objects accurately.\nWe further present two innovative techniques to improve detection accuracy against huge scale changes. First, we observe that conventional NMS refers to confidence scores only to merge detection results, leading to incomplete bounding boxes on oversized objects. To address this, we propose a novel Cross-slice NMS scheme (C-NMS) that favors large bounding boxes with high confidence scores. The proposed C-NMS scheme greatly improves the detection accuracy of oversized objects. Second, we use a multi-scale strategy to extract coarse-grained and fine-grained features. The multi-scale strategy enlarges the receptive field, enhancing the detection accuracy on both large and small objects. In summary, the main contributions of this work are as follows:\n\u2022 We propose a novel sparse Vision Transformer based detector to handle huge scale changes in HRW images.\n\u2022 We further use cross-window NMS and multi-scale schemes to improve detection on large and small objects.\n\u2022 We extensively validate our method on two large-scale HRW-shot benchmarks, PANDA and DOTA-v1.0. Our method advances state-of-the-art performance by large margins."}, {"title": "2 Related Work", "content": "Close-up shot detection models. The majority of common object detection datasets, such as PASCAL VOC [9] and MS COCO [28], collect high-resolution images with close-up shots, which has greatly contributed to the development of object detection. Based on the detection head, the literature can be broadly categorized into two types: one-stage detectors and two-stage detectors. The primary objective of the two-stage object detection is accuracy, and it frames the detection as a \"coarse-to-fine\" process [3, 12, 13, 18, 39]. On the other hand, one-stage detectors have an edge in terms of speed, such as YOLO [37]. Subsequent works have attempted to make improvements such as more anchors, better architecture, and richer training techniques [11, 29, 38]. To sum up, the current detectors exhibit great speed and accuracy in close-up shots.\nHigh-resolution wide shot detection models. The introduction of imaging systems led to the development of a new benchmark for gigapixel-level detection with HRW shots called PANDA [49]. This benchmark has recently gained a lot of attention. Previous works on gigapixel-level detection focus on achieving lower latency through patch selection or arrangement [5, 10, 23, 24, 34]. However, they are unable to solve the unique challenges faced in HRW shots. Some works use sparse policies on patches [36], self-attention heads [33], and transformer blocks [33] for image classification. PnP-DETR [46] exploits a poll and pool sampler to extract image features from the backbone and feed the sparse tokens to the attention encoder. This approach shows to be effective for object detection, panoptic segmentation, and image recognition. However, the sparse sampling on the backbone has not been adequately studied yet. DGE [42] is a plugin for vision transformers, but it is not flexible enough to be extended to ConvNet-based models or use arbitrary-size images as input. Therefore, how to design a flexible and model-agnostic architecture for object detection in HRW shots remains underexplored.\nTransformer backbones. Transformers have been successful in natural language processing (NLP), and their potential for vision tasks has gained considerable attention. One such example is the Vision Transformer (ViT) [8], which uses a pure Transformer model"}, {"title": "3 Proposed Method", "content": "We address the unique challenges of HRW detection by proposing the Sparse Vision Transformer. This model efficiently extracts valuable features from sparse information, while enlarging the receptive field to handle huge scale changes. To tackle the problem of incomplete large objects on intersecting sliced areas, we modify vanilla NMS. Additionally, we introduce our HRW-based augmentation for both training and inference to enhance the detection accuracy for both large and small objects. The pipeline is shown in Figure 3."}, {"title": "3.1 Overview of SparseFormer", "content": "An ideal vision model should be able to extract meaningful information from sparse data using limited calculations, just like our human eyes tend to focus on valuable areas over unimportant background information. To achieve this, we design a novel Sparse Vision Transformer called SparseFormer. It dynamically selects key regions and enables dynamic receptive fields to cover objects with various scales. The overall framework of SparseFormer is illustrated in Figure 4. Inspired by Swin Transformer [30], we split the input image into non-overlapping patches to generate tokens. SparseFormer consists of four stages that work together to produce an adaptive representation. Each stage begins with a patch merging layer that concatenates the features of each group of 2 \u00d7 2 neighboring patches. The concatenated features are then projected to half of their dimension using a linear layer.\nEach stage of SparseFormer is centered around attention blocks that are designed to capture long-range and short-range interactions at different scales. To achieve this, we take both the advantages of the vanilla self-attention Transformer block and the Swin Transformer block. In this way, we develop two distinct types of sparse-style blocks. One is used to capture long-range interactions at a coarse grain, while the other focuses on short-range interactions at a finer scale. To facilitate this approach, we introduce the concept of Window which divides each feature map into equally spaced windows. Operations within each window are considered \"local\", while operations that encompass all windows are \"global.\"\nWe outline the global and local attention blocks in more detail. We construct the global block using the standard multi-head self-attention (MSA) [43] and MLP module with aggregated features, or only convolution layers, as detailed in Section 3.2. We construct the local block by adding a sparsification step and an inverse sparsification step before and after the Swin Transformer [30] block, as described in Section 3.3. Unlike previous work [46, 55], we do not build separate branches for global and local attention. Instead, the local attention is positioned after the global one to obtain more details, rather than different features. When a stage has multiple blocks, the ordering of global attention blocks (G) and local attention blocks (L) follows a pattern of 'GGLL'."}, {"title": "3.2 Global Attention on Aggregated Features", "content": "Feature aggregation. Global attention aims to capture coarse-grained features based on long-range interaction. As such, we generate low-resolution information by sparsifying the feature in each window. As shown in Figure 4, we begin each stage with the global attention block. The primary function of this block is to aggregate the features of each window. To achieve this, we take the input feature map z and divide it into windows of size M, ensuring they do not overlap. The left-top location of each window is given by (x, y), and each token within the window has a relative location (\u2206x, \u2206y). We then calculate the aggregated features using the following formula:\n$Z_{x',y'} = \\frac{\\sum_{\\Delta x,\\Delta y} \\alpha_{\\Delta x,\\Delta y}\u00b7 Z_{x+\\Delta x,y+\\Delta y}}{\\sum_{\\Delta x,\\Delta y} \\alpha_{\\Delta x, \\Delta y}}$  (1)\nHere, x' = x/M and y' = y/M. \u03b1\u2206x,\u2206y is the weight of each token. In this paper, we assign equal weights to all tokens by setting \u03b1\u2206x,\u2206y = 1. After aggregating the features using the above formula, we obtain the aggregated feature z which can be further used for attention.\nWindow-level global attention. Feature aggregation is a technique that reduces the number of tokens by a multiple of M\u00b2, which is equivalent to a Mx downsampling of resolution. This reduction in tokens allows us to use global attention interaction without expensive computation. With the aggregated features, consecutive global blocks are computed as follows:\n$z^{l} = Layer(z^{l-1}; \\Theta),$\t (2)\nwhere z\u00b9 refers to the output features of the 1-th global block.\nInverse aggregated features. The aggregated features contain abstract information that facilitates global content-dependent interactions among different image regions. However, their resolution"}, {"title": "3.3 Local Attention on Sparse Windows", "content": "Variance-based scoring. Note that coarse-grained features for each window can achieve high efficiency. However, we still need fine-grained features that can extract object details to accurately detect objects. As such, we drop certain windows based on their low information content to reduce computation. Our goal is to identify windows that require further local attention because their window-level feature cannot represent their inner token-level features.\nWe start with an initial feature map z of dimensions H \u00d7 W \u00d7C before applying global and local attention. We then get the aggregated feature \u017e from z using Equation (1) and apply the inverse sparsification function via Equation (3), which produces an intermediate feature map 2 with the same resolution as z. Next, we calculate the residual r between 2 and z and concatenate the features of each window to obtain the tokens of MX MXC dimensions that are\n$\\hat{z}_{x+\\Delta x,y+\\Delta y} = \\alpha_{\\Delta x,\\Delta y}Z_{x',y'}$  (3)\nHere, x = M x' and y = My', where (x', y') and (x, y) represent the location on the input and output feature maps, respectively. Additionally, (\u2206x, \u2206y) represents the relative location with respect to (x, y). We consider (x, y) as the top-left of each window on the output feature map, where the windows are partitioned in the same manner as in the feature aggregation process.\nThis step extracts the output feature map from the successive global block (Equation (2)). Then, we invert it using Equation (3) and denote the resulting feature map by zglobal. It is worth noting that the final global feature zglobal has the same resolution as the input feature map z. Even though aggregated features have a lower resolution, the global attention operation can provide more non-local information with little extra computation.\nH\nW\nX\nMin size. We construct a\nScoreNet using MLP to generate\nM\nthe scores based on each residual:\nScoreNet(z, 2) = SoftMax (MLP(z \u2013 2)), (4)\nwhere the MLP projects (M\u00d7 MX C)-dimensional features for each window to 1 dimension, and the SoftMax operation calculates the score for each window. A higher score indicates greater variance, meaning high-variance windows require fine-grained attention. In other words, we discard windows with lower scores during local attention. Once we have ranked the windows, we can selectively choose a part of them to generate finer-level features. Before doing so, we update the feature map z with global feature zglobal using:\nz\u2190 z + zglobal  (5)\nWindows sparsification. We start by analyzing the global attention and variance-based scoring to obtain the initial feature z and scores for each window. Next, we partition z into windows of size HW, in the same way as the ScoreNet. We represent these\nwindows as a matrix Z \u2208 RNXD, where N is the total number of\nwindows, i.e., N = M \u00d7 and D = M \u00d7\n<M X C. To determine\nwhich windows to keep, we define a hyperparameter k to repre-sent the keeping ratio. We maintain a binary decision mask vector A \u2208 {0, 1}N to indicate whether to drop or keep each window based on k and scores. The value of k would depend on the specific task at hand, and can be adjusted as required. The sparse matrix S \u2208 RK\u00d7N collects the one-hot encoding of vector A, where K is the number of keeping windows, i.e., K = k. N. Using this sparse matrix, we compute the features of the sparse windows as follows:\n2s = S x Z,  (6)\nThe output feature \u017bs \u2208 RK\u00d7D is then used as input for the local attention."}, {"title": "3.4 Cross-slice Non-Maximum Suppression", "content": "In HRW shot processing, the slicing strategy generates box candidates for each slice, which must then be merged into a mutually non-conflicting box set. However, using Non-Maximum Suppression (NMS) to select the highest-scoring boxes may lead to incomplete boxes when objects are on the edge regions of multiple slices (For a more detailed explanation and visual representation, refer to Figure 5). To address this, we propose a Cross-slice Non-Maximum Suppression (C-NMS) strategy, as shown in Algorithm 1, that prioritizes boxes with the maximum area across multiple slices, rather than just the highest scores. The C-NMS algorithm consists of two stages: a local suppression stage and a cross-slice suppression stage."}, {"title": "3.5 Multi-scale Training and Inference", "content": "Due to memory limitations, it is not possible to train and test super high-resolution datasets at their original size. Therefore, we use a slicing strategy in both the training and testing phases. To make better use of the multi-scale information, we use high-resolution images and divide them into slices of varying sizes using the slicing strategy. All slices are scaled to the same size, enabling effective training and inference for the object detector. We divide the image into grids of 16\u00d716, 8\u00d78, 4x4, and 2\u00d72 grids, respectively, and remove the slices with no objects. This approach allows us to analyze and understand the complex features of these images, ultimately improving the overall accuracy and effectiveness of the detector.\nDuring the inference phase, we use slicing windows of two sizes: the original one and one quarter of both height and width. Instead of simply combining the two windows, we set different receptive fields for the two types of windows with a threshold Ta. Based on the first window, we remove the prediction boxes larger than Ta. We only keep the boxes larger than Ta for the second window. This follows the idea of scale-specific design [40, 41], where we should arrange each window to cover the appropriate scale to improve performance. With this technique, we can quickly and accurately process high-resolution images."}, {"title": "4 Experiment", "content": "Datasets. Our evaluation is on two public benchmarks with HRW shots, PANDA [49] and DOTA-v1.0 [51]. PANDA is the first human-centric gigapixel-level dataset. It contains 18 scenes with over 15,974.6k bounding boxes annotated. Specifically, there are 13 scenes for training and 5 scenes for testing. DOTA is a large-scale dataset to"}, {"title": "4.1 Effectiveness Evaluation"}, {"title": "4.2 Ablation Study", "content": "Component effectiveness. We investigate the effectiveness of global block, C-NMS, multi-scale training (MS Train) and inference (MS Inference). Evaluation is conducted on the PANDA with k = 0.7. As shown in Table 2, all components can significantly improve performance with a little extra cost which also shows that our strategies are useful for object detection in HRW shots.\nKeeping ratio. Our strategy involves discarding the grids that are deemed unimportant. We study the impact of the grid-keeping ratio on the final performance. Table 4 presents our findings, where the keeping ratio k is represented as [k, k\u00b2, k\u00b3, k4] for each stage. As the features become sparser, we observe a significant reduction in FLOPs, but the decrease in accuracy is insignificant.\nEffect on ScoreNet. We study the effect of different post-processing of residual values which feed into ScoreNet (introduced in Section 3.3). z and 2 denote the original features and aggregated features, respectively, which are the same in Equation (4). As can be\n2AT \u00d7 25 + (1 \u2212 AT) \u00d7 Z. (8)\nHere, 2s is updated by local attention, while 2 is the output of each stage of SparseFormer. Finally, we revert 2 back into the original dimensional space of H \u00d7 W \u00d7 C to obtain the final feature map, denoted as z. The window-based attention, based on variance-based scoring, can extract more local information in a lightweight form, thus improving the detection performance of small objects while saving computation for the background.\nEnd-to-end Optimization. It is challenging to optimize the ScoreNet because we only use the output to sort the windows, and the gradient cannot be back-propagated. To overcome this issue, we implement the Gumbel-Softmax trick [32] to relax the sampling process, making it differentiable. This trick provides a bridge for gradient back-propagation between soft values and binarized values through re-parameterization. Hence, we re-write Equation (5) as:\nz \u2190 z + (1 \u2212 s) \u00d7 zglobal, (9)\nHere, s represents the output of the SoftMax function, which indicates the scores of windows."}, {"title": "4.3 Comparison on Edge Device", "content": "The HRW shots are usually captured by edge devices like UAVs. UAV detectors typically cannot run on large computing devices, but instead run on low-power edge devices. Because it is usually difficult to quantify FLOPs on edge devices, we use NVIDIA AGX Orin (top power 60W) to evaluate the average inference time of each detector on the gigapixel-level images from PANDA and the results are shown in Table 6. Notably, our method can largely reduce inference time compared to previous methods. Our method is 3x faster than PAN and has 5.8% increase of AP. We can see because of the complex head structure, the inference speed of dynamic-head [6] is not ideal. On the opposite, DINO [60] shows promising FPS than the previous work and the improvement of the speed is clearer. Compared to the competitive approach DEG [42], our method could achieve much better performance with faster speed."}, {"title": "4.4 Model-agnostic Study", "content": "It is noteworthy that our strategy is model-agnostic, enabling seamless integration with either ConvNet or Transformer architectures. This flexibility leads to the creation of SparseNet and SparseFormer. Building upon the previously mentioned SparseFormer, we have innovated by substituting every self-attention module with convolution layers. As illustrated in Table 1 and Table 3, SparseNet demonstrates performance that is not only comparable but also competitive with the renowned ResNet [19]. Especially noteworthy is that SparseNet reduces the GFLOPs up to 56% while increasing accuracy compared to CSL [56] and it achieves the lowest GFLOPs on the DOTA dataset [51], underscoring its efficiency and effectiveness in complex computational tasks."}, {"title": "4.5 Visualization of Sparse Windows", "content": "In order to better understand how window sparsification works, we visualize the selected windows from each stage in Figure 7. The red patches represent regions with higher scores, while the"}, {"title": "5 Conclusion", "content": "We introduced SparseFormer, a sparse Vision Transformer-based detector designed for HRW shots. It uses selective token utilization to extract fine-grained features and aggregate features across windows to extract coarse-grained features. The combination of fine and coarse granularity effectively leverages the sparsity of HRW shots, facilitating handling extreme scale variations. Our Cross-slice NMS scheme and multi-scale strategy help detect oversized and diminutive objects. Experiments on PANDA and DOTA-v1.0 benchmarks show significant improvement over existing methods, advancing state-of-the-art performance in HRW shot object detection."}]}