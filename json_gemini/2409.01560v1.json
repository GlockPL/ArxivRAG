{"title": "Blocks as Probes: Dissecting Categorization Ability of Large Multimodal Models", "authors": ["Bin Fu", "Qiyang Wan", "Jialin Li", "Ruiping Wang", "Xilin Chen"], "abstract": "Categorization, a core cognitive ability in humans that organizes objects based on common features, is essential to cognitive science as well as computer vision. To evaluate the categorization ability of visual AI models, various proxy tasks on recognition from datasets to open world scenarios have been proposed. Recent development of Large Multimodal Models (LMMs) has demonstrated impressive results in high-level visual tasks, such as visual question answering, video temporal reasoning, etc., utilizing the advanced architectures and large-scale multimodal instruction tuning. Previous researchers have developed holistic benchmarks to measure the high-level visual capability of LMMs, but there is still a lack of pure and in-depth quantitative evaluation of the most fundamental categorization ability. According to the research on human cognitive process, categorization can be seen as including two parts: category learning and category use. Inspired by this, we propose a novel, challenging, and efficient benchmark based on composite blocks, called ComBo, which provides a disentangled evaluation framework and covers the entire categorization process from learning to use. By analyzing the results of multiple evaluation tasks, we find that although LMMs exhibit acceptable generalization ability in learning new categories, there are still gaps compared to humans in many ways, such as fine-grained perception of spatial relationship and abstract category understanding. Through the study of categorization, we can provide inspiration for the further development of LMMs in terms of interpretability and generalization.", "sections": [{"title": "Introduction", "content": "Categorization is one of the most fundamental cognitive abilities of humans. As shown in Fig.1, visual categorization involves the process of organizing objects into categories based on shared features or attributes (category learning), and using the mental representation to complete cognitive tasks, such as classifying new objects (category use) [26]. The learning and use of categories is not only a significant research topic in cognitive science but is also considered a critical feature of artificial intelligence [11].\nWith the progressive enhancement of computer vision models, there should be an implicit improvement in the capability for categorization, evaluated by the development of diverse proxy tasks, such as object recognition. In recent years, multimodal models like CLIP [30] align visual and textual modalities, thereby liberating categorization from the constraint of datasets and advancing towards open-world scenarios. Moreover, Large Multimodal Models (LMMs) have integrated modalities such as vision into language models with a large number of parameters, displaying remarkable performance on numerous high-level visual tasks [34, 48] and holistic benchmarks [51, 52]. Excellent understanding shown in image captioning [6] and visual question answering [13, 16, 25] implies that LMMs seem to possess sufficient categorization ability [44]. However, there is a lack of direct, objective, and decoupled evaluations of LMMs' capabilities in the most fundamental tasks of visual perception.\nTo explore this question, a pure and in-depth benchmark is required to dissect categorization ability of LMMs. We argue that an effective benchmark should have the following characteristics: (1) Avoiding data leakage. Prevent not only data sample leakage [12, 24] but also leakage of evaluation categories. Similar to using abstract reasoning to test human intelligence in Wechsler Adult Intelligence Scale [41], some abstract and novel categories that are impossible to exist in the training set should be introduced. (2) Establishing quantitive and discriminative tasks. Select diverse and quantifiable evaluation tasks and questions to ensure objectivity and maximize dissection efficiency, allowing us to explore the boundaries of their capability through failure case analysis [12]. (3) Performing unit tests and integration tests. Design a diverse set of evaluation tasks that cover the entire cognitive process of category learning and use. These tasks should avoid unrestrained end-to-end questions to ensure a disentangled evaluation. To respond to these requirements, we will revisit the human cognitive process of categorization [26, 32, 39], and attempt to decouple the key evaluation points to probe the categorization process of LMMs, which will be detailed in Sec.2.\nTo meet the aforementioned requirements of categorization evaluation, we construct a synthetic dataset consisting of Composite Blocks (ComBo), which will be elaborated in Sec.B. The objects and categories in ComBo are entirely unseen to LMMs, meeting the need of preventing data leakage. Since the synthetic data is completely controllable, we can easily control the difficulty of the tasks and inexpensively generate a large number of questions with ground truth for quantitative evaluation. Inspired by the analysis of the categorization process, we design a series of tasks covering the entire cognitive process, aiming to comprehensively evaluate the categorization capability of LMMs. First, we evaluate LMMs' ability to perceive low-level patterns that is critical for accurate object recognition. Next, we explore their capability to align abstract category representations with human mental concepts by predefined semantic categories, which verifies the consistency of the learned concepts. Finally, we challenge the models with unseen abstract categories to examine their generalizability of categorization ability. These experiments are designed to illuminate the strengths and limitations of LMMs in replicating human-like category cognition, thereby pushing the boundaries of LMMs in understanding and interacting with the real-world objects.\nThe experimental results and analyses in Sec.4 reveal that while LMMs demonstrate enhanced categorization capability over traditional CV models, they continue to be stuck in spatial detail recognition, abstract conceptual reasoning, and learning unseen categories in some scenarios. The corresponding discussions and related work are presented in Sec.4.3 and Sec.5. Analyzing these failure cases allow us to explore LMMs' shortcomings from a more basic level and make effective promotions. We believe that studying the low-level visual capability such as categorization of LMMs will contribute to the further development of generalizability and interpretability in AI models."}, {"title": "Categorization Dissection", "content": "In this section, we will break down the design of categorization dissection, starting with an introduction to the cognitive processes of categorization of humans and LMMs in Sec.2.1, and based on this, we will introduce the design philosophy of our evaluation in Sec.2.2."}, {"title": "Overview of Categorization", "content": "As shown in Fig.2, the cognitive process of categorization involves information transmission between concrete space and abstract space [26]. The concrete space consists of perceivable visual entities in the real world, including various data forms of the object categories, such as a photorealistic ice cream, an ice cream sketch, and toy blocks like an ice cream. The abstract space is where both humans and LMMs store categorization rules about these categories respectively, such as the shape of the category \u201cice cream\" (typically consisting of a cone and ice cream balls) and some attributes (a cold dessert). Humans use mental representations to encode key aspects about category members [26], while LMMs store knowledge about entities in their internal implicit representation spaces, such as feature vectors.\nThus, the cognitive process of categorization can be represented as follows. (1) category learning: humans and LMMs perceive data in concrete space, gathering some items with common features together. They then abstract and summarize the commonalities of these items to form a concept representation of the category in abstract space. (2) category use: humans and LMMs utilize the concepts in abstract space to construct various cognitive functions. For example, classifying a newly encountered object as ice cream, inferring the cold taste and other attributes a new ice cream should have, and even implicitly applying it to tasks such as image captioning and visual question answering about a dessert shop."}, {"title": "Design Philosophy of Evaluation", "content": "Based on the cognitive process of object categorization described above, we design three evaluation tasks (green blocks in Fig.2) corresponding to different stages of the categorization process, in order to conduct a comprehensive evaluation of the categorization capability of LMMs.\nPattern Perception: pre-CL evaluation. When perceiving entities in the concrete space, patterns are the direct perceptual targets for humans and LMMs. The ability to accurately identify low-level patterns is a prerequisite for category learning (pre-CL). As shown in Fig.3 (a), we evaluate the ability of LMMs to recognize patterns in multiple dimensions, such as shape, material, color, etc., in a fully disentangled manner.\nAbstraction Alignment: post-CL evaluation. Alignment between the abstract spaces is one of the important topics in explainability AI (XAI) research [7]. We further explore whether LMMs' learned representations of category learning (post-CL) are aligned with human mental representations. As shown in Fig.3 (b), LMMs are asked to recognize abstract visual stimuli agreed upon by humans and align them with the correct semantic labels.\nCategory Building: full-chain evaluation. As shown in Fig.3 (c), to examine the categorization capability of LMMs from learning to use, we define several groups of abstract"}, {"title": "The ComBo Benchmark", "content": "We construct a large-scale repository of Composite Blocks for categorization (ComBo), where each object within the dataset is composed of two geometric primitives, named primary primitive and secondary primitive according to the size of the primitives. The primary and secondary primitives are contacted through a contact point on the primary primitive. The optional shapes of the primitives and the optional contact points on the primary primitive are all displayed in Fig.4. To enhance the visual diversity of ComBo, four different materials are assigned to the primitives. Additionally, the rubber and metal materials are further differentiated by five colors.\nBy enumerating all the values across the four disentangled dimensions of shape, material, color, and contact point, a total of 9,504 objects can be obtained, with each pair of objects differing in at least one dimension. We utilize a ray tracing based rendering engine [4] to render each composite object from 20 random viewpoints, culminating in the 190,080 images in ComBo, inspired by CLEVR [17]. More details are shown in supplementary materials.\nThe benchmark content and evaluation results are publicly available at: https://fubin29.github.io/Blocks-as-Probes/."}, {"title": "Tasks", "content": "As mentioned in Sec.2.2, to evaluate LMMs' categorization capability, we start with three tasks: Pattern Perception, Abstraction Alignment, and Category Building. Examples for three tasks and the corresponding answers by LMMs are illustrated in Fig.5, and some statistics about the benchmark are shown in Tab.1.\nPattern Perception. We randomly select 5,000 objects from ComBo as evaluation subjects, and sample one rendered image for each object. Participants are required to sequentially answer seven questions about the low-level patterns present in the object, as shown in Tab.2. All questions are multiple-choice, and a brief description of ComBo along with all the options are provided.\nAbstraction Alignment. In this task, we invite cognitive science experts to select appropriate natural categories that can be abstracted by our ComBo objects. Following a filtering and voting process, a consensus is reached on 24 categories, which include image samples and category labels, for evaluation purposes. Subsequently, we generate two types of multiple-choice questions, each comprising 120 questions, by incorporating distractors among the matched abstract objects and category labels. Img2Text requires the participants to choose the label that best matches the given image out of four category labels. Text2Img asks the participants to select the image that most resembles the given category from four image options. The detailed process of question generation is described in supplementary materials, and the validity of the questions is verified by user study discussed in Sec.4.2.\nCategory Building. In this task, we require participants to simulate the category formation in human consensus by constructing abstract categories of different granularities, and then classify the test samples. Abstract categories are groups of object clusters defined based on rules, where all composite objects in an abstract category have the same constraint (e.g., \"with a red cube as the primary primitive\"). We present multiple samples from two abstract categories to the participants, requiring them to observe and summarize the rules for building both abstract categories. Furthermore, we randomly show test samples belonging to the two categories to the participants multiple times. Participants should be able to classify all samples correctly when they understand the categories. We also design a similarity measurement method to calculate the evaluation task difficulty for classifying composite objects"}, {"title": "Experiments", "content": "In this study, we select the mainstream closed-source implementations of current LMMS (GPT-4V [28, 47], Gemini-1.5-Pro [31]) and open-source implementations (LLaVA-v1.5-13B [21], Qwen-VL-Chat [3]) as the subjects of our analyses. To evaluate GPT-4V and Gemini, we utilize their official APIs. For LLaVA and Qwen, we conduct local tests using a single NVIDIA A40 GPU. Additionally, we include other implementations as comparative references in different experiments, such as representation classification based on CLIP [30] pre-trained models, evaluations from human users, etc. More details and examples about image input and prompt are presented in supplementary materials."}, {"title": "Evaluation Results", "content": "Pattern Perception. Tab.2 demonstrates the pattern perception capability of various LMMs, without any fine-tuning or in-context prompting. Gemini and GPT-4V exhibit significantly stronger low-level pattern recognition and instruction-following capability, compared to open-source LMMs (even in simpler separate questions). Notably, Gemini and GPT-4V generally achieve the accuracy of larger than 90% in recognizing the primary primitive's shape and colors of both primitives. We also find that the recognition of patterns in smaller secondary primitives presents greater challenges, resulting in performance declines across all the LMMs. Overall, Gemini achieves the best results in all metrics, and especially excels in predicting the contact points, indicating its advanced spatial perception capability.\nConsidering the domain transfer challenges posed by the ComBo dataset, we conduct additional in-context learning experiments [1, 5, 38] on GPT-4V and Gemini, focusing on contact points and materials of both primitives. As shown in Fig.6, the results indicate that GPT-4V can significantly improve its performance through in-context learning. We speculate that Gemini's performance advantage over GPT-4V might stem from Gemini's exposure to similar block data during training and specialized training on spatial relationships.\nAbstraction Alignment. We invite 20 human participants to complete the user study to validate the reasonableness of our questions. The results indicate that the abstract objects in images bear a good resemblance to the mental representation of natural categories held by humans. Tab.3 presents the alignment between the category concepts learned by LMMs and"}, {"title": "Discussion", "content": "During the evaluation process, we find significant differences in the instruction following [29] capability among different LMMs. GPT-4V and Gemini can understand the most complete description of the questions, and can also receive multiple image inputs, allowing them to handle more complex problems that require reasoning between multiple images. In pattern perception evaluation, querying different patterns separately can improve the performance of LLaVA. Qwen's capability to follow complex instructions is slightly inferior to other LMMs, requiring more adjustments to the form of the questions."}, {"title": "Task Difficulty", "content": "In this section, we discuss the difficulty of our benchmark. Although ComBo, as a dataset rendered based on geometric primitives, defines problems that are formally similar to abstract reasoning problems [12] in other benchmarks, it is much less difficult than those benchmarks in terms of logical reasoning. To verify this, we use some smaller, commonly used computer vision models to complete the same tasks. By retrieval, models pre-trained on ImageNet-1k [33] can achieve similar performance to GPT-4V, and almost are able to completely solve the task after fine-tuning. See supplementary materials for more details. The fine-tuned small model is capable of completing the task, implying that it does not necessitate a complex reasoning process. However, current LMMs still show a significant gap compared to humans in such simple visual tasks, indicating that LMMs are far from being able to claim that the fundamental tasks of visual categorization have been completely solved."}, {"title": "Limitation and Future Work", "content": "We believe that evaluating both the lower and upper bounds of LMMs' capabilities is equally important. Compared to other comprehensive evaluation benchmarks, our evaluation benchmark leans more towards the in-depth evaluation of categorization capability, which is considered one of the most fundamental visual cognitive abilities. Notably, we utilize brand new synthetic data rather than real 2D images to completely prevent data leakage and facilitate decoupled, controllable evaluation. However, it is still necessary to use more complex, even real images to further evaluate the capabilities of LMMs in real-world application scenarios. In future work, we will involve more complex composite objects and controllable 3D models, and incorporate a wider array of cognitive tasks to further explore the current conclusion. Moreover, we are committed to developing new methodologies and datasets that enhance LMMs' performance in perceiving spatial details, reasoning about abstract concepts, and learning new categories. We believe these efforts will enhance the applicability and reliability of LMMs in various high-level tasks."}, {"title": "Related Work", "content": "Large Multimodal Models. LMMs [2, 9, 49] integrate visual [22] or other modalities [15, 43] into Large Language Models (LLMs), enabling them to handle a variety of multimodal tasks. High-performance and closed-source LMMs like PaLM-E [10], GPT-4V [47], and Gemini [35] represent a critical branch of development. These models benefit from substantial investments in proprietary datasets and computing resources, achieving superior performance across a range of complex tasks. Another branch consists of open-source models such as LLaMA-Adapter [53], LLaVA [22], MiniGPT-4 [54], Otter [18], and Qwen [3]. These LMMs are typically developed by modularly integrating other modalities into open-source LLMs [37]. Both branches have demonstrated strong capability in various applications, such as medical image understanding [20, 27] and embodied agents [40, 46]. Consequently, we select two models from each branch for evaluation in our study.\nLMM Benchmarks. Due to the more generalized multimodal perception and reasoning capability of LMMs, traditional vision-language benchmarks are inadequate for providing a comprehensive and sufficient evaluation. Consequently, recent developments in the evaluation of LMMs have primarily focused on several key aspects [49]: (1) addressing specific common issues such as visual shortcomings [36] and hallucinations [8, 14]; (2) comprehensive benchmarks that entail complex tasks and diverse capability [19, 23, 45, 50, 51]; (3) expert-level domain knowledge and advanced reasoning [52]. In contrast to these evaluation efforts, our study concentrates on assessing the fundamental categorization ability of LMMs."}, {"title": "Conclusion", "content": "In this work, we introduce the ComBo benchmark, focusing on evaluating the categorization capability of Large Multimodal Models (LMMs). Inspired by research on categorization in cognitive science, we design three evaluation tasks from different perspectives, comprehensively assessing the LMMs' ability in pattern perception, abstract concept alignment, and generalization of categorization. The evaluation results reveal that LMMs still exhibit deficiencies in spatial detail perception, abstract concept reasoning, and learning of new categories. Although in-context learning or Chain-of-Thought (CoT) techniques can further improve the performance of LMMs, there remains a gap compared to human categorization capability, providing recommendations for future improvements in LMMs."}]}