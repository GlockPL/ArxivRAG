{"title": "Interpreting Outliers in Time Series Data through\nDecoding Autoencoder", "authors": ["Patrick Knab", "Sascha Marton", "Christian Bartelt", "Robert Fuder"], "abstract": "Outlier detection is a crucial analytical tool in various fields. In critical systems like manufacturing,\nmalfunctioning outlier detection can be costly and safety-critical. Therefore, there is a significant need\nfor explainable artificial intelligence (XAI) when deploying opaque models in such environments. This\nstudy focuses on manufacturing time series data from a German automotive supply industry. We utilize\nautoencoders to compress the entire time series and then apply anomaly detection techniques to its latent\nfeatures. For outlier interpretation, we i) adopt widely used XAI techniques to the autoencoder's encoder.\nAdditionally, ii) we propose AEE, Aggregated Explanatory Ensemble, a novel approach that fuses\nexplanations of multiple XAI techniques into a single, more expressive interpretation. For evaluation of\nexplanations, iii) we propose a technique to measure the quality of encoder explanations quantitatively.\nFurthermore, we qualitatively assess the effectiveness of outlier explanations with domain expertise.", "sections": [{"title": "1. Introduction", "content": "Outliers represent exceptional instances that differ from a normal data distribution [1]. Artificial\nintelligence (AI) is pivotal in outlier (anomaly) detection applications, particularly in domains\nwith high-dimensional data, such as time series. By analyzing patterns, trends, and dependencies,\nalgorithms can effectively identify outliers and anomalous events in various domains, ranging\nfrom finance and healthcare to industrial processes [2, 1, 3]. In particular, manufacturing\nprocesses generate vast amounts of time series data, making timely and accurate outlier detection\ncritical for maintaining operational efficiency and safety. However, opaque neural networks\n(NN) often lack the interpretability necessary for high-stakes environments [4, 3]. Consequently,\nexplaining the model's decisions through explainable artificial intelligence (XAI) is essential to\nprovide transparency and foster trust in automated decision-making [5, 6, 7].\nThis work utilizes convolutional autoencoders (CAE) to compress univariate time series\ndata for anomaly detection in an automotive manufacturing plant. A complete time series is\nconsidered an outlier if the entire sequence deviates from the expected pattern. The purpose of\nutilizing CAE is to learn specific manufacturing process features and map a time series into a\nlow-dimensional space at its bottleneck. An unsupervised anomaly detection algorithm then\nuses these latent features to identify outliers. Therefore, we are interested in explaining how the"}, {"title": "2. Related XAI Approaches", "content": "The following section briefly mentions the XAI approaches used in this work. They were chosen\nfor their well-known status and ability to cover explainability from different aspects, e.g., local\nvs. global and model-agnostic vs. model-specific explanations. These techniques share the\ngoal of providing post hoc explanations, but each employs different approaches to achieve\nexplainability.\nCAM (Class Activation Mapping), proposed by Zhou et al. [17], is a local and model-specific\ntechnique for explaining convolutional neural networks (CNN). Selvaraju et al. [8] enhanced\nthis approach with Grad-CAM, incorporating gradients into the explanation process. This\nimprovement removes the requirement for a global average pooling layer, making the method\napplicable to a broader range of model architectures.\nLIME (Local Interpretable Model-agnostic Explanations), proposed by Ribeiro et al. [9], is\nanother well-known local but model-agnostic technique. It achieves interpretability by locally\napproximating the behavior of a complex neural network with an interpretable machine learning\nalgorithm.\nSHAP (SHapley Additive exPlanations) is a game theory-inspired model-agnostic technique"}, {"title": "3. Application of XAI to Autoencoder", "content": "Notation. An univariate time series instance t is fed into the convolutional autoencoder via the\nfunction $t = D(E(t))$, with encoder E, decoder D, and latent space L, where $L = E(t)$. The\noutput t is a reconstruction of t. We denote explanation $E$ as the output of an XAI technique\nfor a time series t."}, {"title": "3.1. Adapting XAI Techniques to Encoder", "content": "We employ a 1D convolutional autoencoder (1D CAE) to reduce feature dimensions and detect\nanomalies in time series data (see Section 4.1 for more details). We apply XAI techniques to the\nencoder since we use its output for anomaly detection. While the straightforward architecture\nfacilitates the application of the XAI methods introduced in Section 2, their utilization, although\nwidely applied in diverse machine learning scenarios, remains relatively limited within the realm\nof 1D CAE, mainly when applied to time series data. We adapt these methods to improve their\ncapability to provide 1D explanations for 1D convolutional networks in the form of heatmaps.\nThe application of the XAI methods above yields two distinct types of explanations:\n\u2022 Individual Feature Explanation: For each latent feature, $l_i \\in L$ (where i is the feature\nindex), we generate a dedicated heatmap. This allows us to inspect how individual features\nin t contribute to the reconstruction process (see Appendix A).\n\u2022 Combined Feature Explanation: In addition to the individual views, we also create\na unified heatmap that integrates all latent features into a single representation (see\nFigure 2a). This combined view provides a holistic understanding of how the interplay\nbetween features in t influences the reconstruction process. All experiments and figures\nin this paper use combined feature explanations."}, {"title": "3.2. AEE - Aggregated Explanatory Ensemble.", "content": "With the application of the covered XAI approaches (see Section 2), we generate a set of diverse\nexplanations. Each XAI technique provides distinct insights: Grad-CAM emphasizes spatial\nrelevance, LIME offers local interpretability, SHAP delivers global explanations, and LRP traces\nrelevance propagation (see Figure 2). By aggregating these methods, AEE leverages their\nstrengths for a holistic understanding of anomalies. We restrict AEE to a time series t that stores\nthe diverse explanations $E_x$ in an array E, where i indicates the index of t and x denotes the\nunderlying XAI technique. To ensure equal consideration for each explanation, we individually"}, {"title": "3.3. Quality Measurement of Encoder's Explanation", "content": "Given the interpretability constraints of the XAI results [5], we quantitatively analyze the\nexplanations generated by each method using a modified version of the quality measurement\nfunction proposed by Schlegel et al. [16]. In this work, the XAI techniques focus on the\nencoder's explainability, resulting in a multi-regression task. Using the reconstruction error as a\nquality measurement would involve the decoder, misleading the measurement of the encoder's\nexplanation. Instead, we aim to analyze the projections of the original time series t, a randomly\nperturbed version $t_o$, and a version perturbed based on explanation results $t_c$ in the latent space.\nThis approach operates independently of the decoder, focusing on explaining the techniques\napplied to the encoder. Adversarial perturbations [18], which manipulate predictions, suggest\nthat the distance between $t_o$ and t should be smaller than between t and $t_c$. Thus, we define\nthe quality measurement for the encoder as:\n$qme(t, t_o) \\leq qme(t, t_c) \\leq qme(t, t_o)$.\nHere, qme measures the Euclidean distance between the original and perturbed time series in\nthe latent space. The underlying theory is that perturbations based on explanation results have\na more significant impact on the model's predictions than random noise [14, 15]. The approach\napplies to individual and combined feature explanations, revealing the importance of features\nfor the outlierness property of the instance."}, {"title": "4. Experimentation", "content": "As introduced in Section 1, our demonstration employs univariate time series data\noriginating from a production plant. More specifically, it covers one process in a manufacturing"}, {"title": "4.1. Experimental Setup", "content": null}, {"title": "4.2. Qualitative Evaluation - Anomaly Interpretation", "content": "In the following, we discuss the utility of XAI techniques to interpret the encoder, focusing\non understanding why specific instances lead to anomalies by leveraging domain-specific\nknowledge of the underlying manufacturing process. We examine an exemplary time series\nclassified as NOK for all explanation techniques in Figure 2 and Figure 3. The illustrative\ncase diverges notably in its final third segment, as the pattern is expected to exhibit distinct\ncharacteristics compared to the preceding two thirds of the time series (see Appendix D). We\nexplicitly demonstrate the anomalies shown here using examples that are easy to visually\nunderstand as outsiders.\nWe initiate with Grad-CAM (Figure 2a), revealing a heatmap that distinctly accentuates\npositions later in t, precisely aligning with observable areas of technical failures in the manu-\nfacturing process. This targeted explanation effectively identifies the specific region preceding\nreal-world anomalies. Subsequently, LIME (Figure 2b) highlights the same area as CAM, but\nits interpretation is more straightforward because of its apparent intensity. Moreover, it also\nsubtly indicates regions in intermediate areas of the time series. SHAP (Figure 2c) pinpoints"}, {"title": "4.3. Quantitative Evaluation - XAI Quality Measurement.", "content": "Figure 4 depicts the QM (normalized Euclidean distances) distributions, where boxes represent\nthe interquartile range (IQR) from Q1 to Q3, with a median line (Q2). The fences extend \u00b11.5\ntimes the IQR. The OK category includes 100 randomly selected instances, and the NOK category\ncomprises 38. The noise/shuffle box (green) represents QM values $t_o$, and the XAI box (red)\nrepresents $t_c$.\nThe visualization indicates that each QM XAI score consistently outperforms its QM noise\ncounterpart. The scores for the NOK cluster are significantly higher, demonstrating the effective-\nness of using explanations for outlier interpretation. LRP and LIME overlap between Noise and\nXAI, while Grad-CAM and SHAP display a clearer separation in their explanations. The AEE\nproduces a significant result, indicating that aggregating multiple explanations sharpens the\ndistinction between relevant and irrelevant features within a time series, improving explanation\nquality."}, {"title": "4.4. Limitations and Future Work", "content": "Our study applied XAI techniques to CAE, leaving the potential for other architectures such\nas variational autoencoders (VAE) [21] and recurrent neural networks (RNN) [22, 23, 7] un-\nexplored. Additionally, the evaluation of these techniques was primarily based on qualitative\nassessments, as anomalies required examination by domain experts. Future research on datasets\nnot requiring expert knowledge should consider integrating additional quantitative methods to\ncomplement qualitative insights [24]. In addition, a clear distinction between explanation and\ninterpretation should be established [25], recognizing that not all explanations are inherently\nhuman-interpretable [26], as it was sometimes the case in this scenario."}, {"title": "5. Conclusion", "content": "This paper contributes to the application of XAI techniques to CAEs for analyzing outlier\nproperties within the latent space of time series data in the operational context of a manu-\nfacturing plant. We employed well-established XAI methods to demonstrate the practicality\nand effectiveness of these techniques in interpreting outliers. In addition, we introduced AEE,\nan ensemble of multiple XAI techniques. We quantitatively evaluated the different explana-\ntions using a QM approach specifically modified to fit the encoder of an AE. Moreover, the\napplication of XAI techniques provided explanations for these outliers, accurately highlighting\nthe abnormal segments within the time series. This alignment confirms the utility of XAI in\nproviding meaningful insights into anomalies and building confidence in the system through\nthe interpretation of XAI results."}, {"title": "A. Individual Feature Explanation", "content": "Figure 5 shows an instance that the pipeline classified as NOK, featuring the reconstructed\ntime series in red and the original time series in black. The underlying explanation is provided\nthrough individual feature explanations, where a distinct heatmap visually explains each latent\nfeature."}, {"title": "B. Autoencoder Architecture", "content": "In the following, we present the defined search space of hyperparameters for tuning an AE\nin this work. The search space has been explored by 100 runs and 500 epochs each. Figure 6\nrepresents the building blocks we tuned during this process.\n\u2022\nThe amount of CNN blocks consists of an optional dropout and max pooling layer. We\nrestrict this size to at least one and a maximum of three blocks. This number applies to\nboth the encoder and the decoder.\n\u2022 In contrast to the number of CNN blocks, this number varies for DNN blocks between\nthe encoder and decoder parts. Both can have up to two DNN blocks."}, {"title": "C. Latent Space Plot", "content": "The encoder's output projection is shown in Figure 7. This figure displays the latent variables\non a two-dimensional scale for easier interpretation. Each point on the plot corresponds to\na mapped instance, representing a complete time series from the test dataset. The colors\nindicate the categorization by DBSCAN in the latent space: red points are outliers, orange\npoints represent instances with manually detected deviations yet considered OK, and green\npoints indicate cases with no apparent deviations, also classified as OK."}, {"title": "D. Exemplary Non-Outlier Time Series", "content": "Figure 8 displays an exemplary time series classified as a non-outlier alongside its reconstruction\nby the autoencoder. The image demonstrates that the AE can meaningfully reconstruct the\ninput time series. Additionally, the pattern of this time series is typical for an instance without\napparent errors in this dataset."}]}