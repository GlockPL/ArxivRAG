{"title": "On the Statistical Complexity for Offline and Low-Adaptive Reinforcement Learning with Structures", "authors": ["Ming Yin", "Mengdi Wang", "Yu-Xiang Wang"], "abstract": "This article reviews the recent advances on the statistical foundation of reinforcement learning (RL) in the offline and low-adaptive settings. We will start by arguing why offline RL is the appropriate model for almost any real-life ML problems, even if they have nothing to do with the recent AI breakthroughs that use RL. Then we will zoom into two fundamental problems of offline RL: offline policy evaluation (OPE) and offline policy learning (OPL). It may be surprising to people that tight bounds for these problems were not known even for tabular and linear cases until recently. We delineate the differences between worst-case minimax bounds and instance-dependent bounds. We also cover key algorithmic ideas and proof techniques behind near-optimal instance-dependent methods in OPE and OPL. Finally, we discuss the limitations of offline RL and review a burgeoning problem of low-adaptive exploration which addresses these limitations by providing a sweet middle ground between offline and online RL.", "sections": [{"title": "1. INTRODUCTION", "content": "Reinforcement learning (RL) has gained remarkable popularity lately. Most people would attribute the surge to its usage in AI milestones such as AlphaGo [18, 54, 59, 78, 79] and in instruction-tuning large language models [9, 13, 68, 80]. We, however, argue that it is caused by a more fundamental paradigm shift that places RL in the front and center of nearly every Machine Learning (ML) application in practice. Why? Training an accurate classifier is most likely not the end goal of an ML task. Instead, the predictions of the trained ML model is often used as interventions hence changing the distribution of future data. Real-world applications are usually sequential decision-making problems, and trained ML models need to be combined with RL methods to perform high-quality decision-making. We provide three examples.\nNotably, most real-life RL problems are offline RL problems. Unlike Chess or Go with unlimited access to simulators, it is often unsafe, illegal, or costly to conduct experiments in the task environment. Instead, we need to work with an offline dataset collected from the environment, which poses fundamental problems on what can be learned offline and how (statistically) efficiently one can learn from the offline dataset. Three critical aspects of the offline RL problems are:\n\u2022 Long horizon problem. The long decision horizon in RL poses unique challenge for finding the optimal strategy. In particular, the undesired actions chosen at earlier phases will have long-lasting impact for the future, making the strategy suboptimal. Small deviations from the optimal policy early on can propagate and amplify over time, further complicating the learning process.\n\u2022 Distribution Shift and Coverage. Distribution shift is a fundamental challenge in reinforcement learning that occurs when the distribution of data the agent encounters during training differs from the distribution of optimal policies. When the overlap (measured by certain distribution distance metric) between the two distributions is small, it would be hard to find optimal actions due to the insufficient data coverage, especially when the offline dataset is collected using a suboptimal policy.\n\u2022 Function Approximation and Generalization. The state and action space of RL problems are often so large that a finite dataset cannot cover. In such cases, RL requires generalization across states through a certain feature representation of the states and a parametric approximation of the value functions. Learning such function approximations offline is more challenging.\nThis article aims to review recent advances of the statistical foundations for offline RL, covering both problems in offline policy evaluation and offline policy learning.\nSpecifically, we review what the fundamental learning hardness/statistical limits for offline RL under different MDP (Markov Decision Processes) or function approximation structures are. By examining the statistical results, we reveal how factors such as distribution shift and horizon length affect the learning hardness of the problems. We also introduce the related algorithms and highlight the theoretical techniques to achieve these results.\nPaper organization. We first introduce the mathematical notations and set up the problems of interest in Section 2. Then the remaining sections describe results in offline policy evaluation, offline policy learning and low-adaptive exploration under various assumptions (see Table 1).\nDisclaimer. The literature of offline RL is gigantic. It is not our intention to provide thorough coverage. Instead, the topics and results covered in this paper focus on a niche that the coauthors studied in the past few years. Since our goal is pedagogical, we do not make any claims about novelty and precedence of scientific discovery. Please refer to the bibliography and the references therein for a more detailed discussion."}, {"title": "2. NOTATIONS AND PROBLEM SETUP", "content": "We first provide the background for different problem settings that we consider in this article.\n2.1 Episodic time-inhomogenuous RL\nA finite-horizon Markov Decision Process (MDP) is denoted by a tuple \\(M = (S, A, P, r, H, d_1)\\) [81], where \\(S\\) is the state space and \\(A\\) is the action space. A time-inhomogenuous transition kernel \\(P_h : S \\times A \\times S \\leftrightarrow [0, 1]\\) maps each state action(\\(s_h, a_h\\)) to a probability distribution \\(P_h(\\cdot|s_h, a_h)\\) and \\(P_h\\) can be different across the time. Besides, \\(r : S \\times A \\rightarrow \\mathbb{R}\\) is the expected instantaneous reward function satisfying \\(0 \\leq r < R_{\\text{max}}\\). \\(d_1\\) is the initial state distribution. \\(H\\) is the horizon. A policy \\(\\pi = (\\pi_1,...,\\pi_H)\\) assigns each state \\(s_h \\in S\\) a probability distribution over actions according to the map \\(s_h \\rightarrow \\pi_h(\\cdot|s_h)\\) \\(\\forall h\\in [H]\\). An MDP together with a policy \\(\\pi\\) induce a random trajectory \\(s_1, a_1, r_1,..., s_H, a_H, r_H, s_{H+1}\\) with \\(s_1 \\sim d_1\\), \\(a_h \\sim \\pi(\\cdot|s_h)\\), \\(s_{h+1} \\sim P_h(\\cdot|s_h, a_h)\\), \\(\\forall h \\in [H]\\) and \\(r_h\\) is a random realization given the observed \\(s_h, a_h\\).\nBellman (optimality) equations. The value function \\(V_h^\\pi(\\cdot) \\in \\mathbb{R}^S\\) and Q-value function \\(Q_h^\\pi(\\cdot,\\cdot) \\in \\mathbb{R}^{S \\times A}\\) for"}, {"title": "2.2 Structured MDP models", "content": "In this article, we examine three fundamental yet representative MDP models (or related function approximation classes) that are well-structured. Despite their simplicity, as we will discuss in later sections, their statistical limits have not been well-understood until recently.\nTabular MDPs. Tabular MDP is arguably the most simple setting in RL. It is a Markov Decision Process with finite states \\(|S| < \\infty\\) and finite actions \\(|A| < \\infty\\). The most common tabular MDPs, such as Gridworlds, often have small state and action spaces. When the number of states and actions are large, they are generally not treated as discrete and are instead addressed using function approximators.\nLinear MDPs. An episodic MDP \\((S, A, P, r, H, d_1)\\) is called a linear MDP with a known (unsigned) feature map \\(\\phi : S \\times A \\rightarrow \\mathbb{R}^d\\) if there exist d unknown (unsigned) measures \\((\\nu_h)_{h \\in [H]}\\) over \\(S\\) and an unknown vector \\(\\theta_h \\in \\mathbb{R}^d\\) such that \\(\\forall s', s \\in S\\), \\(a \\in A\\), \\(h\\in [H]\\),\n\nwith \\(\\int_s ||\\nu_h(s')|| ds \\leq \\sqrt{d}\\) and \\(\\max(||\\phi(s,a)||^2, ||\\theta_h||^2) \\leq 1\\) for all \\(h \\in [H]\\) and \\(\\forall s, a \\in S \\times A\\).\nWhen specify \\(d = |S| \\times |A|\\) and \\(\\phi(x,a) = 1_{(x,a)}\\) be the canonical basis in \\(\\mathbb{R}^d\\), linear MDPs recover tabular MDPs. Thus, linear MDPs strictly generalize tabular MDPs and allow continuous state-actions spaces.\nLinear Functions approximation. By Bellman equation (1), Linear MDP aimplies that the value function \\(Q_h^\\pi\\) for any policy \\(\\pi\\) is a linear function in the feature vector \\(\\phi\\), i.e.,\n\\(Q_h^\\pi(\\cdot,\\cdot) \\in \\{\\langle \\phi(\\cdot, \\cdot), \\theta \\rangle | \\theta \\in \\mathbb{R}^d \\}\\)\nfor any \\(h\\in [H]\\) and \\(\\pi\\). It is sometimes sufficient to directly reason about these linear function approximations rather than relying on the stronger linear MDPs structures. There are various subtle differences in the various type of linear function approximation. For the purpose of this paper though, it suffices to just think about linear MDPs.\nFor more general MDPs, it is harder to impose tractable structures. Alternatively, we consider the following structured function class that is expressive enough to learn general MDPs.\nParametric Differentiable Functions. Let \\(S, A\\) be arbitrary state, action spaces and a feature map \\(\\phi(\\cdot,\\cdot) : S \\times A \\rightarrow \\Psi \\subset \\mathbb{R}^m\\). The parameter space \\(\\Theta \\in \\mathbb{R}^d\\). Both \\(\\Theta\\) and \\(\\Psi\\) are compact spaces. Then the parametric function class (for a model \\(f : \\mathbb{R}^d \\times \\mathbb{R}^m \\rightarrow \\mathbb{R}\\)) is defined as\n\\[\\mathcal{F} := \\{f(\\theta,\\phi(\\cdot,\\cdot)) : S \\times A \\rightarrow \\mathbb{R}, \\theta \\in \\Theta\\}\\]\nthat satisfies differentiability/smoothness condition: 1. for any \\(\\phi \\in \\mathbb{R}^m\\), \\(f(\\theta, \\phi)\\) is third-time differentiable with respect to \\(\\theta\\); 2. \\(f, \\frac{\\partial f}{\\partial \\theta}, \\frac{\\partial^2 f}{\\partial^2 \\theta}, \\frac{\\partial^3 f}{\\partial^3 \\theta}\\) are jointly continuous for \\((\\theta, \\phi)\\). Clearly, \\(\\mathcal{F}\\) generalizes linear function class (via choosing \\(f(\\theta, \\phi) = \\langle \\theta,\\phi \\rangle\\))."}, {"title": "2.3 Offline RL Tasks", "content": "The offline RL begins with a static offline data \\(\\mathcal{D} = \\{(s_h, a_h, r_h, s_{h+1})\\}_{h \\in [H]}\\) rolled out from some behavior policy \\(\\mu\\). In particular, the offline nature requires we cannot change \\(\\mu\\) and in particular we do not assume the functional knowledge of \\(\\mu\\). There are two major tasks considered in offline RL.\n\u2022 Offline Policy Evaluation (OPE). For a policy of interest \\(\\pi\\), the agent needs to evaluate its performance \\(v^\\pi\\) using \\(\\mathcal{D}\\). In general, there is a distribution mismatch between \\(\\pi\\) and \\(\\mu\\). The goal is to construct an estimator \\(\\tilde{v}^\\pi\\) such that \\(|v^\\pi - \\tilde{v}^\\pi| < \\epsilon\\) or mean square error \\(\\mathbb{E}_\\mu[(v^\\pi - \\tilde{v}^\\pi)^2] < \\epsilon\\).\n\u2022 Offline Policy Learning (OPL). This requires the agent to find a reward-maximizing policy \\(\\pi^* := \\text{argmax}_\\pi\\) given data \\(\\mathcal{D}\\). That is to say, given the batch data \\(\\mathcal{D}\\) and a targeted accuracy \\(\\epsilon > 0\\), the offline RL seeks to find a policy \\(\\pi_{\\text{alg}}\\) such that \\(v^* - v_{\\text{alg}} < \\epsilon\\).\nBoth OPE and OPL are essential to a real-world offline RL system since the decision maker should first run the offline learning algorithm to find a near optimal policy and then use OPE methods to check if the obtained policy is good enough. For instance, in finance, OPL can be applied for learning a strategy, but traders still need to run OPE for backtesting before deployment. On the other hand, they are also standalone research questions, e.g. doctors can be asked to evaluate a heuristic treatment plan that does not involve offline learning, which makes it a pure OPE problem.\nREMARK 1. The source of historical data \\(\\mathcal{D}\\) could be multilateral, and there are papers (e.g. [35, 77]) directly considers data distribution without specifying \\(\\mu\\). We incorporate a specific behavior policy \\(\\mu\\) to manifest the distribution mismatch between \\(\\mu\\) and \\(\\pi\\)."}, {"title": "2.4 Assumptions in offline RL", "content": "Due to the inherent distribution shift in offline RL, for both OPE and OPL, we revise different assumptions for different problem classes in Section 2.2. These assumptions are standard protocols for deriving provably efficient results.\nOffline Policy Evaluation. For tabular OPE, it requires marginal state ratios and policy ratios to be finite, as stated below.\nASSUMPTION 1 (Tabular OPE [93, 97]). Logging policy \\(\\mu\\) obeys that \\(d_m := \\text{min}_{t,s} d_t^\\mu(s) > 0\\). Also, \\(\\Gamma_s := \\text{max}_{t,s} \\frac{d_t^\\pi(s)}{d_t^\\mu(s)} < +\\infty\\) and \\(\\Gamma_a := \\text{max}_{t,s,a} \\frac{\\pi_t(a|s)}{\\mu_t(a|s)} < +\\infty\\).\nHaving bounded weights is necessary for discrete state and actions, as otherwise the unbounded importance ratio would cause the estimation error become intractable.\nASSUMPTION 2 (Linear OPE [15, 28]). Let the population feature covariance \\(\\Sigma_h := \\mathbb{E}_{\\mu,h} [\\phi(s,a)\\phi(s,a)^T]\\). Then we assume \\(\\text{min}_h \\lambda_{\\text{min}}(\\Sigma_h) > 0\\) with \\(\\lambda_{\\text{min}}\\) being the minimal eigenvalue.\nThis assumption ensures the behavior policy \\(\\mu\\) has good coverage over the state-action spaces. For instance, when \\(\\phi(x, a) = 1_{(x,a)}\\), the assumption above reduces to \\(\\text{min}_{s,a}d^\\mu(s, a) > 0\\).\nASSUMPTION 3 (Parametric OPE [104]). Policy completeness: assume reward \\(r \\in \\mathcal{F}\\) and for any \\(f \\in \\mathcal{F}\\), we have \\(P^\\pi f \\in \\mathcal{F}\\). Policy realizability: assume \\(Q^\\pi(\\cdot,\\cdot) = f(\\phi(\\cdot,\\cdot), \\theta^\\pi)\\) for some \\(\\theta^\\pi \\in \\mathbb{R}^d\\). Lastly, let the population feature covariance\n\\(\\Sigma_h := \\mathbb{E}_{\\mu,h} [\\nabla_\\theta f(\\phi(s,a), \\theta^\\pi) \\nabla_\\theta f(\\phi(s,a), \\theta^\\pi)^T]\\).\nThen we assume \\(\\text{min}_h \\lambda_{\\text{min}}(\\Sigma_h) > 0\\).\nPolicy completeness and policy realizability ensure the policy class is rich enough to capture \\(Q^\\pi\\). Besides, the assumption on the population feature covariance generalizes the Linear OPE case.\nOffline Policy Learning. Next, we summarize the common assumptions (from strong to weak) that can yield statistical sample efficiency for policy learning. After that, we introduce extra assumptions for offline learning in the function approximation settings.\nASSUMPTION 4 (Uniform data coverage [77, 99]). For behavior policy \\(\\mu\\), \\(d_m := \\text{min}_{h,s,a}d_h^\\mu(s,a) > 0\\). Here the infimum is over all the states satisfying there exists certain policy so that this state can be reached by the current MDP with this policy.\nThis is the strongest assumption in offline RL as it requires \\(\\mu\\) to explore each state-action pairs with positive probability at different time step \\(h\\). For tabular RL, it mostly holds \\(1/SA \\geq d_m\\) under Assumption 4. This reveals offline learning is generically harder than the generative model setting [2, 45] in the statistical sense. On the other hand, for task where it needs to evaluate different policies simultaneously (such as uniform OPE task in [99]), this is required as the task considered is in general a harder task than offline learning.\nASSUMPTION 5 (Uniform concentrability [12, 43, 82, 92]). For all policy \\(\\pi\\), \\(C_\\mu := \\text{sup}_{\\pi, h} ||d_h^\\pi(\\cdot,\\cdot)/d_h^\\mu(\\cdot,\\cdot)||_{\\infty}\\). The parameter \\(C_\\mu < +\\infty\\) is commonly known as \u201cconcentrability efficient\u201d.\nThis is a classical offline RL condition that is commonly assumed in the function approximation scheme (e.g. Fitted Q-Iteration in [43, 82], MSBO in [92]). Qualitatively, this is a uniform data-coverage assumption that is similar to Assumption 4, but quantitatively, the coefficient \\(C_\\mu\\) can be smaller than \\(1/d_m\\) due the \\(d_h^\\pi\\) term in the numerator. There are other variants of concentrability efficient [66, 94] that capture the data coverage of behavior policy slightly differently."}, {"title": "3. OFFLINE POLICY EVALUATION IN CONTEXTUAL BANDITS AND TABULAR RL", "content": "Let's start by the problem of offline policy evaluation (OPE) - the problem of evaluating a fixed target policy \\(\\pi\\) using data collected by executing a logging (or behavior) policy \\(\\mu\\).\nReaders may wonder why this is even a problem. Admittedly, in supervised learning, one can simply evaluate a classifier policy on a validation dataset. Similarly, in online RL, one can roll out policy \\(\\pi\\) to see how well it works.\nThe problem starts to arise in offline problems because we do not have data directly associated with policy \\(\\pi\\).\n3.1 OPE in contextual bandits\nLet us build intuition by considering the contextual bandit problem. Contextual bandit (CB) problem is a special case of RL with horizon \\(H = 1\\), where the initial state \\(s\\) is referred to as the \u201ccontext\u201d. For short horizon problems such as CB, the main challenge is to handle distribution shift. Motivated by a change of measure formula\n\\(v_{\\text{CB}}^\\pi = \\mathbb{E}_{s \\sim d_1, a \\sim \\pi(\\cdot|s)} [r(s,a)] = \\mathbb{E}_{s \\sim d_1, a \\sim \\mu(\\cdot|s)} [\\frac{\\pi(a|s)}{\\mu(a|s)} r(s,a)],\\)\nclassical methods employ importance sampling (IS) [30, 48, 71] to corrects the mismatch in the distributions under the behavior policy \\(\\mu\\) and target policy \\(\\pi\\). Specifically, let the importance ratio be \\(\\rho := \\pi(a|s)/\\mu(a|s)\\), then the IS estimator is computed as:\n\\[\\hat{v}_{\\text{IS-CB}} = \\frac{1}{n} \\sum_{i=1}^n \\rho(\\hat{s}^{(i)}, \\hat{a}^{(i)}) \\hat{r}(\\hat{s}^{(i)}, \\hat{a}^{(i)}).\\]\nIt is known to be effective for real-world applications such as news article recommendations [16, 47].\nThe mean square estimation error (MSE) of the IS estimator \\(\\hat{v}_{\\text{IS-CB}}\\) decomposes into two terms\n\\[\\frac{1}{n}(\\mathbb{E}_\\mu[\\rho(s,a)^2\\text{Var}[r|s, a]] + \\text{Var}_\\mu[\\rho(s,a)\\mathbb{E}[r|s, a]]).\\]\nThe first term comes from the noisy reward while the second term comes from the random \\((s, a)\\) pair. Interestingly, if we make no assumption about \\(\\mathbb{E}[r|s,a]\\) and the size of the state-space is large, then IS is minimax optimal [89, Theorem 1]. On the contrary, if \\(\\mathbb{E}[r|s, a]\\) can be estimated sufficiently accurately, then there are methods that asymptotically do not depend on the \\(\\text{Var} [\\mathbb{E}[\\cdot]]\\).\nPerhaps a bit surprising to some readers, the above conclusion implies that even for on-policy evaluation, i.e., \\(\\pi = \\mu\\) and \\(\\rho = 1\\), the naive value estimator of \\(\\Sigma_{i=1} r^{(i)}\\) (IS with \\(\\rho = 1\\)) can be substantially improved using a good reward model.\n3.2 \"Curse of Horizon\" in OPE for RL\nThe IS estimators are later adopted for long horizon sequential decision making (RL) problems. Concretely, denote the \\(t\\)-step importance ratio \\(\\rho_t := \\pi_t(a_t|s_t)/\\mu_t(a_t|s_t)\\) and the cumu-"}, {"title": "3.3 OPE in Tabular MDPs", "content": "The most basic model of interest is the tabular MDP, namely, MDP when the state and action spaces are finite and that the policy \\(\\mu\\) gets to visit all states and actions that \\(\\pi\\) visits. A statistical lower bound for OPE in the tabular MDP setting is established in [33].\nTHEOREM 3.1 (Cramer-Rao lower bound for tabular OPE [33]). For discrete DAG MDPs with horizon H, the variance of any unbiased estimator \\(\\hat{v}\\) with \\(n\\) trajectories from policy \\(\\mu\\) satisfies\n\\[n \\cdot \\text{Var}[\\hat{v}] \\geq \\sum_{t=1}^H \\frac{\\mathbb{E}_{\\mu_\\mathcal{D}}(s_t, a_t)^2}{\\mathbb{E}_{d^\\mu(s_t, a_t)^2}} \\text{Var} \\Big[V_{t-1}^\\pi(s_{t+1}) + r_t|s_t, a_t\\Big].\\]\nThe construction of the CR lower bound relies on computing the constrained version of Fisher Information Matrix. Under Assumption 1, this right hand side can be readily bounded by \\(O(\\Gamma_s\\Gamma_a H^3)\\) (after a change of measure into \\(\\mathbb{E}_\\pi[\\cdot]\\)), which\nmakes the IS estimators with a variance of \\(\\text{exp}(H)\\) exponentially suboptimal.\nMarginalized Importance Sampling. In [93, 97], we addressed the exponential gap by an idea that is now referred to as Marginalized Importance Sampling. If we re-examine the value objective with RL, by a change of measure formula,\n\\[v^\\pi := \\mathbb{E}_\\pi \\sum_{t=1}^H r_t = \\mathbb{E}_\\mu \\sum_{t=1}^H \\frac{d_t^\\pi(s_t)}{d_t^\\mu(s_t)} r_t,\\]\nwith \\(r_t(s) = \\mathbb{E}_{a \\sim \\pi(s)}[r_t(s,a)|s]\\). This reformulation reveals, rather than applying \\(\\rho_{1:t}\\), we could instead estimate the marginal state density ratio \\(d^\\pi/d^\\mu\\). Inspired by this observation, the Marginalized Importance Sampling (MIS) estimator is defined as\n\\[\\hat{v}_{\\text{MIS}} = \\frac{1}{n} \\sum_{i=1}^n \\sum_{t=1}^H \\frac{\\hat{d}_t^\\pi(\\hat{s}_t^{(i)})}{\\hat{d}_t^\\mu(\\hat{s}_t^{(i)})} \\hat{r}_t(\\hat{s}_t^{(i)}).\\]\nDifferent design choices for \\(\\hat{d}_t^\\pi, \\hat{d}_t^\\mu, \\hat{r}_t\\) in (3) yield different MIS estimators.\nState MIS (SMIS [93]). For SMIS, \\(\\hat{d}_t^\\mu(\\cdot)\\) is directly estimated using the empirical mean, i.e. \\(\\hat{d}_t^\\mu(s_t) := \\frac{1}{n} \\sum_i 1\\{\\hat{s}_t^{(i)} = s_t\\} := \\frac{n_{st}}{n}\\) whenever \\(n_{st} > 0\\) and \\(\\frac{\\hat{d}_t^\\pi(s_t)}{\\hat{d}_t^\\mu(s_t)} = 0\\) when \\(n_{st} = 0\\). Marginal state distributions are estimated via recursion \\(\\hat{d} = P^\\pi \\hat{d}_{t-1}\\), followed by the estimations \\(\\hat{P}_t(s'|s)\\) and state reward \\(\\hat{r}_t(s)\\) as:\n\\[\\hat{P}_t(s'|s) = \\frac{1}{n_s} \\sum_{i=1}^n \\frac{\\pi(a^{(i)}|s)}{\\mu(a^{(i)}|s)} \\mathbb{I}\\{(\\hat{s}_{t-1}^{(i)}, \\hat{s}_t^{(i)}) = (s, s')\\} = \\frac{\\hat{n}_t(s,s')}{n_s},\\]\n\\[\\hat{r}_t(s) = \\frac{1}{n_s} \\sum_{i=1}^n \\frac{\\pi(a^{(i)}|s)}{\\mu(a^{(i)}|s)} \\hat{r}_t^{(i)} \\mathbb{I}\\{\\hat{s}_{t-1}^{(i)} = s\\}.\\]\nSMIS (3) explicitly gets rid of the cumulative importance ratio \\(\\rho_{1:t}\\) and provides the polynomial sample complexity for horizon under Mean Square Error.\nTHEOREM 3.2. Under Assumption 1 and other mild regularity conditions, the MSE of state marginalized importance sampling satisfies\n\\[\\mathbb{E}[(\\hat{v}_{\\text{MIS}} - v^*)^2] = \\frac{1}{n} \\sum_{t=1}^H \\mathbb{E}_{d_t^\\mu(s)} \\Big[\\frac{d_t^\\pi(s)}{d_t^\\mu(s)}\\Big]^2 \\text{Var}_\\mu \\Big[\\frac{\\pi_t(a | s)}{\\mu_t(a | s)}(V_{t-1}^\\pi(s_{t+1}) + r_t) | s \\Big] \\cdot \\Big[1 + O(\\frac{\\log n}{n})\\Big] + O(\\frac{1}{n^2}).\\]\nThe big O notation hides universal constants.\nThe MSE of SMIS is \\(O(\\Gamma_s\\Gamma_a H^3/n)\\) and the result holds even when the action space is continuous. This exponentially improves over the standard IS.\nSMIS however, does not match the Cramer-Rao lower bound. In particular, the asymptotic MSE (modulo a \\(1 + O(n^{-1/2})\\) multiplicative factor and an \\(O(1/n^2)\\) additive factor) is\n\\[\\frac{1}{n} \\sum_{t=1}^H \\mathbb{E}_{d_t^\\mu(s)} \\Big[\\frac{d_t^\\pi(s)}{d_t^\\mu(s)}\\Big]^2 \\text{Var}_\\mu \\Big[\\frac{\\pi_t(a | s)}{\\mu_t(a | s)}(V_{t+1}^\\pi(s_{t+1}) + r_t) | s \\Big]\\]"}, {"title": "4. OFFLINE POLICY EVALUATION WITH FUNCTION APPROXIMATION", "content": "Next, we switch gears to consider OPE when the (state, action) pairs are described by a continuous feature vector \\(\\phi(s, a) \\in \\mathbb{R}^d\\). This covers most real-life RL problems (such as autonomous driving, robotic arm control, and health care).\nThe key challenge here is to generalize across unseen states while maintain the statistical optimality at the same time. In the discrete setting, empirical count (maximum likelihood estimate) is a natural algorithm that is optimal, but it cannot be generalized in the function approximation setting. However, to evaluate MDPs, the Bellman equations are universally true regardless of the setting. As a result, one can apply the approximate dynamic programming principles [70] for the given function class and data. This is realized by the following Fitted Q-Evaluation (FQE). For OPE with function approximation, we review the time-homogenuous RL (i.e. transition probabilities are identical across time \\(P_t = P\\)) and reformulate offline data\n\\[\\mathcal{D} = \\{(s_i, a_i, r_i)\\}_{i \\in [N]} \\text{ throughout the section } (N = nH).\\]\nFitted Q Evaluation. FQE is a variant of Fitted Q Iteration [4, 17] which is designed for policy optimization purpose. A brief history of FQI is discussed in Section 6. For a given function class \\(\\mathcal{F}\\) and data \\(\\mathcal{D}\\), FQE recursively estimates \\(Q_h, h \\in [H]\\) via \\((Q_{H+1} = 0)\\)\n\\[\\hat{Q}_h = \\text{argmin}_{f \\in \\mathcal{F}} \\frac{1}{N} \\sum_{i=1}^N (f(s_i, a_i) - y_i)^2 + \\lambda p(f),\\\\]\nwith \\(y_i = r_i + \\gamma \\mathbb{E}_a Q_{h+1} (s_{i+1}, a) \\pi( a | s_{i+1}) da\\). Here \\(p(f)\\) is a proper regularizer and is usually chosen as \\(L_2\\), i.e. \\(p(f) = ||f||^2\\). The OPE estimator is\n\\[\\hat{v} = \\mathbb{E}_{s \\sim d_1, a \\sim \\pi(\\cdot|s)} [\\hat{Q}_1 (s,a)].\\]\nThe squared loss function resembles the empirical approximation for Bellman question (1).\n4.1 Linear function approximation\nLinear function approximation considers the class \\(\\mathcal{F}_{\\text{lin}} = \\{f : f(\\cdot,\\cdot) = \\langle \\phi(\\cdot,\\cdot), \\theta \\rangle, \\theta \\in \\mathbb{R}^d\\}\\). Denote the shorthand \\(\\phi_h := \\phi(s_h,a_h)\\) and \\(\\phi^\\pi(s) := \\mathbb{E}_{a \\sim \\pi(\\cdot|s)}[\\phi(s,a)]\\), then FQE can be computed recursively via \\(Q_h(s,a) = \\langle \\phi(s,a), w_h \\rangle\\) with\n\\[w = \\Sigma^{-1} (\\frac{1}{N} \\sum_h^\\pi + R),\\\\]\nwhere \\(\\Sigma = \\frac{1}{N} \\sum_{i=1}^N \\phi_h \\phi_h^T + \\lambda I\\) and \\(R = \\frac{1}{N} \\sum_h r_h \\phi_h\\). FQE does not learn the model/transition dynamics, and it is generally regraded as a model-free approach. Interestingly, by using the components \\(\\frac{1}{N} \\sum_h^\\pi\\) to approximate the population counterparts \\(\\mathbb{E} \\phi\\pi\\), linear FQE is equivalent to the model-based plug-in estimator [15, 28]. This phenomenon is similar to TMIS, which can be interpreted as a model-based estimator.\nWhen the data coverage of the behavior policy \\(\\mu\\) spans the state-action space and the linear function class is expressive enough, FQE has the following efficiency guarantee.\nTHEOREM 4.1. Suppose assumption 2 (good data coverage) and policy completeness of assumption 3 (linear function class is rich enough) are satisfied, then FQE is a consistent OPE estimator with \\(\\sqrt{N}(v^\\pi - \\hat{v})\\) is asymptotically distributed to normal \\(\\mathcal{N} (0, \\sigma^2)\\). The asymptotic variance is given by\n\\[\\sigma^2 = \\sum_{h_1,h_2=1}^H \\left(\\mathbb{E}_{d_1^\\pi} [\\phi_{h_1}^\\pi]^T \\Sigma^{-1} \\Omega_{h_1,h_2} \\Sigma^{-1} \\mathbb{E}_{d_1^\\pi} [\\phi_{h_2}^\\pi]\\right)\\]"}, {"title": "5. OFFLINE POLICY LEARNING IN TABULAR RL: PESSIMISM AND INSTANCE-DEPENDENT BOUNDS", "content": "Policy learning differs from the policy evaluation in that it needs to optimize over a set of policies rather than just evaluating a given policy. Consider the case where there are finite number of policies \\(\\pi_1, \\pi_2,...,\\pi_K\\), and the estimates for the respective policies are \\(\\hat{v}_{\\pi_1}, ..., \\hat{v}_{\\pi_K}\\). If that is all the information provided, a natural algorithm for policy learning would be the ERM (Empirical Risk Minimizer)\n\\(\\hat{\\pi}^* = \\text{argmax}_{\\pi} \\hat{v}^\\pi\\).\nHowever, simply selecting policy via point estimators might not provide the best approach due to uncertainty, and the error in the estimators might cause incorrect prediction about the order of policies. In particular, the dataset could be biased toward certain states, contain many suboptimal actions, or even contain little information about the"}]}