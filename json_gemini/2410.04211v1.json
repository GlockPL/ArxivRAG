{"title": "Correlation-Aware Select and Merge Attention for Efficient Fine-Tuning and Context Length Extension", "authors": ["Ning Wang", "Zekun Li", "Tongxin Bai", "Guoqi Li"], "abstract": "Modeling long sequences is crucial for various large-scale models; however, extending existing architectures to handle longer sequences presents significant technical and resource challenges. In this paper, we propose an efficient and flexible attention architecture that enables the extension of context lengths in large language models with reduced computational resources and fine-tuning time compared to other excellent methods. Specifically, we introduce correlation-aware selection and merging mechanisms to facilitate efficient sparse attention. In addition, we also propose a novel data augmentation technique involving positional encodings to enhance generalization to unseen positions. The results are as follows: First, using a single A100 40GB GPU, we achieve fine-tuning on Llama2-7B with a sequence length of 32K, which is more efficient than other methods that rely on subsets for regression. Second, we present a comprehensive method for extending context lengths across the pre-training, fine-tuning, and inference phases. During pre-training, our attention mechanism partially breaks translation invariance during token selection, so we apply positional encodings only to the selected tokens. This approach achieves relatively high performance and significant extrapolation capabilities. For fine-tuning, we introduce Cyclic, Randomly Truncated, and Dynamically Growing NTK Positional Embedding (CRD NTK). This design allows fine-tuning with a sequence length of only 16K, enabling models such as Llama2-7B and Mistral-7B to perform inference with context lengths of up to 1M or even arbitrary lengths. Our method achieves 100% accuracy on the passkey task with a context length of 4M and maintains stable perplexity at a 1M context length. This represents at least a 64-fold reduction in resource requirements compared to traditional full-attention mechanisms, while still achieving competitive performance. Our code are available at GitHub Repository", "sections": [{"title": "1 Introduction", "content": "In various natural language processing (NLP) tasks, such as document-level sentiment analysis and code generation, the ability to model long-sequence dependencies effectively is crucial. This capability allows for capturing complex relationships over sequences spanning hundreds or thousands of tokens, which is essential for tasks where contextual information is dispersed. For example, accurately summarizing a lengthy document or generating coherent code relies on under- standing dependencies that go beyond adjacent tokens. Consequently, extending the context window enables LLMs to perform tasks that shorter context windows cannot handle and potentially enhances performance across a variety of NLP tasks.\nHowever, extending the context window presents numerous challenges. Longer sequences demand significantly more memory and computational resources, leading to slower training and inference times and higher resource consumption. Moreover, capturing long-range dependencies and using large numbers of tokens in autoregressive models can result in slower convergence, potentially due to underfitting caused by the large number of tokens involved in the autoregressive process.\nTo address these challenges, current research often adopts a strategy of pretraining on short sequences followed by efficient fine-tuning using positional interpolation or positional extrapolation for long sequence extension. Recent works have achieved promising results, such as LongLora, which combines sparse attention with improved LoRA (Low-Rank Adaptation) to extend Llama2-7B to 100K tokens using 8\u00d7A100 80GB GPUs. However, our analysis indicates that LongLora's sparse attention does not fully exploit sparsity, and its efficient fine-tuning can be further optimized to use fewer parameters, thereby requiring less memory and computational resources for similar extensions.\nIn this paper, we propose a novel approach that leverages correlation-based selection and merging mechanisms to achieve more efficient sparse attention, along with fine-tuning strategies that optimize parameter utilization. Our attention mechanism achieves a degree of controlled fitting by selecting variable token quantities, adjusting receptive fields, and configuring varying levels of compression. This flexibility allows our method to be effectively applied to fine-tuning across different datasets and domains, such as supervised fine-tuning (SFT). To exploit this capability further, we design a cyclic, randomly sampled, and dynamically expanding NTK (CRD NTK) positional encoding during fine-tuning to generalize positional information. This design facilitates significant extrapolation capabilities and theoretically supports arbitrary extensions of the context window. Moreover, in addition to length extension via fine-tuning, we present theoretical and empirical evidence that our method can disrupt positional invariance to a certain degree, enabling efficient automatic extrapolation during pre-training. Our method also integrates effectively with approaches like InfLLM, allowing for long-sequence inference without requiring additional fine-tuning. As such, we offer a comprehensive framework for extending context windows across the pre-training, fine-tuning, and inference stages.\nEfficiency: Our method utilizes a single A100 GPU in conjunction with DeepSpeed ZeRO-2 to fine-tune Llama2-7B for a context length of 32K, demonstrating that other methods, such as LongLora, encounter memory overflow issues. We provide algorithmic com- plexity analysis and memory consumption metrics during a model pre-training process. Furthermore, our approach operates orthogonally to methods like RingAttention.\nContext Length Extension of Positional Encoding: The paper points out that Full Attention, MS Attention, or other attention mechanisms can achieve high-ratio positional extrapolation through fine-tuning using either high-scale factor positional interpolation (PI) or NTK positional encoding. As a result, the length extension proposed in this paper is no longer limited by setting the PI or NTK scale factor parameters based on the ratio between pre- and post-extension lengths. Instead, we fine-tune using large scale factor parameters of our own choosing. Furthermore, based on theoretical analysis, we designed a CRD NTK positional encoding that better matches our attention mechanism (we believe a similar property applies to PI positional encoding as well). Thus, by fine-tuning with a context length of 16K, our method achieves context window extensions exceeding 1M for both Llama2-7B and Mistral-7B, yielding 100% accuracy on the passkey task at a context length of 4M and maintaining stable perplexity in text testing at 1M length. While further testing for longer lengths was limited due to resource constraints, theoretical and experimental analyses indicate that our method can support arbitrary context window extensions for both relative and partial absolute positional encodings. Our method intrinsically disrupts position invariance, allowing for substantial context window length extrapolation through the designed positional encodings.\nFlexible Fitting Degree: Our approach implements different quantities of selected and merged tokens, as well as varying degrees of compression and receptive field ranges, to achieve diverse convergence behaviors on the Llama2-7B model, resulting in different perplexity levels."}, {"title": "2 Related Work", "content": "Efficient Attention Mechanisms To fully exploit the inherent sparsity and positional relationships be- tween tokens, a significant body of research has focused on developing efficient attention mechanisms. These mechanisms reduce the computational complexity of attention operations by focusing on a subset of tokens at each step, thus processing long sequences more efficiently or reducing resource consumption. Existing methods first preserve local features and then use various strategies to attend to more distant tokens. For instance, BigBird and Performer use random patterns, Longformer and DETR use fixed patterns, while Biformer and Routing Transformer utilize relevance routing mechanisms. Our proposed relevance selection and merging mechanism adapts flexibly to various scenarios and is compatible with FlashAttention2, achieving more efficient and general sparse attention.\nPositional Encoding Another research direction aimed at extending sequence length in LLMs focuses on positional encoding techniques, such as positional interpolation and extrapolation. Most pretrained models are trained on fixed-length sequences with fixed positional encodings, leading to performance degradation when extended to unknown positions. Therefore, numerous studies have analyzed the impact of positional encodings and modified them through interpolation or extrapolation to extend to longer sequences. For example, Position Interpolation, NTK-aware, Yarn, and LongRope mitigate the effects of pretrained positional encodings by using interpolation with different scales based on frequency importance, effectively extending sequence modeling lengths."}, {"title": "3 Preliminary", "content": "3.1 Transformer\nThe Llama2 model used in this paper is based on the Transformer architecture, which consists of the core modules self-attention and MLP. The computation process of self-attention is as follows:\n$O = softmax(Q K^T)V$\nwhere Q, K, and V are obtained from X using embedding weights Wq, Wk, and $W_v$ respectively. The final output O is then passed through Wo to obtain the final output of the attention. Subsequently, the entire Transformer operation is completed through the MLP.\n3.2 LORA\nLow-Rank Adaptation (LoRA) is an efficient model fine-tuning method designed to reduce the computational resources and storage requirements when fine-tuning large pretrained models.\nThe core idea of LoRA is to decompose the weight matrix W of the pretrained model into two low-rank matrices, A and B. This decomposition is represented as $W = W_0 + \\Delta W$, where Wo is the original weight matrix and $\\Delta W = AB$ is the adjustment matrix obtained through low-rank decomposition. The matrices A and B have a rank of r, which is typically much smaller than the dimensions of W. During fine-tuning, only the parameters of matrices A and B need to be adjusted, while the original weight matrix Wo remains unchanged.\nThe LoRA experiments on various large language models (such as GPT-3 and BERT) have shown that the method can significantly reduce the computational resources and storage requirements while maintaining or even improving the model's performance.\n3.3 Positional Encoding\nPositional encoding can be divided into relative and absolute positional encodings. The most widely used method for relative positional encoding is the Rotary Positional Encoding (RoPE). The encoding formula is given as follows, where $\\theta_i = base^{-\\frac{2i}{d}}$:\n$PC_m = [cos(m \\theta_0) cos(m \\theta_0) cos(m \\theta_1) cos(m \\theta_1) \\ldots cos(m \\theta_{d/2-1}) cos(m \\theta_{d/2-1})]$\n$PS_m = [sin(m \\theta_0) -sin(m \\theta_0) sin(m \\theta_1) -sin(m \\theta_1) \\ldots sin(m \\theta_{d/2-1}) -sin(m \\theta_{d/2-1})]$\nThe output for the encoding of the m-th token is given by:\n$X_m = [x_0 x_1 x_2 x_3 \\ldots x_d x_{d-1}]PC_m+[x_1 x_0 x_3 x_2 \\ldots x_{d-1} x_d]PS_m$\nPositional Interpolation is performed by scaling the above position, where the index of the m-th position is transformed as follows:\n$P_fm = [f(m \\theta_0) f(m \\theta_0) f(m \\theta_1) f(m \\theta_1) \\ldots f(m \\theta_{d/2-1}) f(m \\theta_{d/2-1})]$\nNTK Positional Encoding: For NTK positional encoding, the base is rescaled as:\n$base' = base \\times scaled^{-{\\frac{2}{d}}} (scale \\gt 1), \\theta_i = base'^{-{\\frac{2i}{d}}}$\nOther Positional Encodings: Other positional encoding methods extend from the two encoding strategies described above. For example, YaRN posits that high-frequency in- formation is more important, so it retains the high-frequency portion of the positional encoding"}, {"title": "4 Methods", "content": "4.1 MS: Merge selection\nThe following is a pseudo-code and the theory in appendix A.2:\nWe propose a method for implementing more general and efficient sparse attention through correlation selection and merging mechanisms, as shown in Figure 2. This method consists of two main steps: selection of relevant regions and merging of these regions. The proposed method consists of two main steps: selection and merging.\nStep 1: Selection. First, the Q, K, and V tensors with shape (b, h, n, d) are segmented into regions, resulting in tensors $Q_s$ with shape $(b, h, n_{sq}, s_q, d)$, $K_s$, and $V_s$ with shape $(b, h, n_{sk}, s_k, d)$, where $s_q$ and $s_k$ denote the segment size. Each region is represented by a semantic token or an average compressed token, yielding $Q'$ and $K'$. A dot product or another similarity metric is then applied between $Q'$ and $K'$ to analyze the relevance between $Q_s$ and $K_s$ regions. To prevent information leakage, we apply a mask and control the number of selected tokens. This process results in the indices of the top-k most relevant $K_v$ regions for each $Q_s$ region, denoted as selectindx with shape (b, h, nsq, topk), where ns is the number of Qs regions and topk is the number of selected K and V regions.\nStep 2: Merging. In the merging step, the indices obtained from the first step and Qs are permuted and combined, effectively merging the selected regions. Qs is merged according to a specified number of segments merges, resulting in $Q_{ms}$ with shape $(b, h, n_{ms}, m \\cdot s, d)$. Similarly, selectindx is split, permuted, and merged into mselectindx with shape $(b, h, n_{ms}, topk, m) \\rightarrow (b, h, n_{ms}, topk \\cdot m)$. Due to previous topk operation, the permutation can ensure that each row is sorted by relevance, with the first m indices corresponding to the most relevant $K_v$ region for each of the merged $Q_s$ regions, the next m indices corresponding to the second most relevant $K_v$ region, and so on. After obtaining the merged indices mselectindx, performs unique operation while maintaining relevance order. The top-n indices, denoted as qmselectindx, are selected as the final indices corresponding to the merged $Q_s$ regions and their relevant $K_v$ regions.", "subsections": []}, {"title": "4.2 Reduced LongLora", "content": "We propose a method that selectively fine-tunes only the $W_k$ and $W_o$ weights, achieving results nearly identical to fine-tuning the entire attention mechanism. Specifically, since $Q K^T = X W_q W_k X^T$, updating only $W_k$ yields $W_q W_k$, effectively replicating the effect of simultaneously updating $W_k$ and $W_q$. This approach is particularly effective when $W_q$ is full rank. Similarly, fine-tuning $W_o$ follows the same rationale, as the final output is a linear mapping of $W_q W_k$. Since only the Q and K tokens undergo positional encoding, the weights to be fine-tuned are limited to $W_Q$ and $W_K$. Based on the above analysis, it suffices to only fine-tune $W_Q$ or $W_K$. In this work, for the fine-tuning of LLaMA, we adopt a low-rank fine-tuning of $W_Q$ and $W_K$, while for Mistral, we apply low-rank learning of $W_e$ for sequence length extension.\nAdditionally, since query tokens can be considered well-fitted through extensive training, learning the linear mapping for their corresponding key tokens is reasonable. Furthermore, as the attention mechanism becomes heterogeneous, updating the final classification head is a direct approach. However, considering the large number of parameters, altering the initial embedding to achieve a certain degree of equivalence is also a feasible consideration."}, {"title": "4.3 Cyclic, Randomly Truncated, and Dynamically Growing NTK Method", "content": "For positions not encountered during pretraining, it is necessary to scale these positions within the range of positions learned during pretraining. In this work, we employ the NTK method for scaling, adjusting the positional base proportionally. During fine-tuning, after a certain number of steps, the base is dynamically altered to accommodate different scales.\nTo address the issue of attention sink, we apply random shifts and cyclic modular operations. This approach not only alleviates the attention sink problem but also enables data augmentation of positional information, which improves generalization capabilities. By leveraging the model's more flexible fitting ability, we can better learn and generalize positional information. The positional encoding is as follows:\n$P_{fm,i} = f((\\frac{(m + random_p)\\%max}{2}) (base \\times (scale^{\\frac{-2i}{d}})))$"}, {"title": "4.4 The strategy for incorporating positional information during pre-training", "content": "Our algorithm leverages semantic token routing, where each semantic token encapsulates contextual information from its surrounding tokens, similar to a sliding window mechanism in convolution. Since different tokens are surrounded by distinct contexts, the resulting semantic tokens are unique, leading to different routed KV tokens and interpolated variables. As a result, we do not assign positional information during the selection process. Instead, after the selection process, ROPE positional encoding is applied to the K and Q tokens to enhance performance."}, {"title": "5 Experiment", "content": "5.1 Experimental Setup\nOur experiments were conducted on 2 \u00d7 A100 40GB GPUs, using the Llama2-7B model and Mistral-7B-v0.1 with the attention mechanism replaced by our MS Attention as described in Section 4.1. The training approach used the efficient fine-tuning method described in Section 4.2, with position interpolation applied. Finally, to extend to longer sequence lengths, we employed the recursive method described in Section A.6."}, {"title": "5.2 Main Results", "content": "5.2.1 Memory Usage and Fine-tuning Time\nTable 1 compares the memory usage and fine-tuning time of our method with LongLoRA. Our approach significantly reduces memory overhead and fine-tuning time. For example, during 32K length fine-tuning, our method could use stage2+offload optimizer for fine-tuning, while LongLora would OOM. When training with a sequence length of 16K, our memory usage is only 33.5GB, which is significantly less compared to the memory requirements of LongLoRA using 8 GPUs. Moreover, our method achieves comparable or even superior performance with significantly less training time.\n5.2.2 Length Extension: Pretraining\nTables 2 and 3 show the results of pretraining with MS Attention on PG19 and ImageNet, where our method outperforms various baselines. For PG19, test data is truncated when it is larger than the length of the test."}, {"title": "5.2.3 Length Extension: Fine-tuning", "content": "Theoretically, our method augments the positional data through data augmentation, leveraging the relatively strong fitting ability of our algorithm to adapt parameters to arbitrary ratios of NTK-based positional encoding. This allows for the extension to arbitrary positional encoding lengths.\nFirst, for the architecture, during fine-tuning, we replace the attention mechanisms in Llama and Mistral with our MS Attention. For parameter fine-tuning, since only the Q and K tokens incorporate positional information, we use LoRA to fine-tune the mapping weights of Wq and WK, while other parameters are only fine-tuned for embeddings and normalization layers. Regarding the choice of positional encoding, we apply a dynamically growing NTK method, starting with an initial scaling factor (set to scale = 4096 in this work), and increase this factor twofold after fine-tuning a small fixed data volume (32M tokens in our case).\nFor the passkey task, as shown in Table 5, using standard NTK positional encoding combined with our algorithm enables length extensions of up to 16x. Moreover, employing random cropping and circular modular sampling stabilizes this extension. Finally, by incorporating dynamic growth, we address the task of passkey generation for arbitrary lengths. For every 4x increase in base length, our approach can solve passkey tasks with a 2x length extension, potentially due to the square root relationship between positional encoding and the base length."}, {"title": "5.2.4 LongBench", "content": "Table 7 shows Few-shot Learning and Code Completion Evaluation on LongBench: These tasks do not require chat instruct fine-tuning. We have observed significant improvements in \"trec\", \"triviaqa\", \"lcc\", and \"repobench-p\" using our method on Llama2-7B-4k comparing with Llama2-7B-chat-4k, especially outperforming other models in the LCC task."}, {"title": "5.2.5 A controllable convergence for Llama2-7B", "content": "The proposed method allows for controllable convergence and fitting degrees by flexibly setting parameters such as the attention range, the number of selected tokens, and the types of multi-scale compressions, as illustrated in Tabel 8. This adaptability ensures that the model can be fine-tuned to achieve optimal performance across a variety of scenarios, providing robustness and efficiency in handling extended sequences."}, {"title": "5.2.6 ablation experiment", "content": "Firstly, in our pursuit of enhancing efficiency in micro-adjustments, we conducted fine-tuning on Wq, Wk, Wu, and Wo. The resulting Perplexity (PPL) is nearly identical to when fine-tuning is solely applied to Wk and Wo. In some instances, the latter even yields slightly higher results, as show in"}, {"title": "6 Conclusion", "content": "In this paper, we build upon the LongLoRA framework, utilizing a mechanism of selection and merging within the Attention mechanism (referred to as MS Attention). By employing our approach on 2 x A100 40GB GPUs, we achieve the same level of length extension for Llama2-7B as LongLoRA does on 8 x A100 80GB GPUs. Specifically, we extend the context length of Llama2-7B to 100K tokens, significantly reducing the resource requirements for handling long sequences. Moreover, through the introduction of recursion, our method maintains stable perplexity even with sequences up to 2M tokens in length. Finally, the flexibility of our MS Attention mechanism allows for adjustable selection size and selection quantity, which, combined with restricting the attention range of each token, enables adaptable fitting and convergence rates."}, {"title": "A Appendix / supplemental material", "content": "Analysis of Attention with Correlation-Aware Selection and Merging is presented below:\nA.1 Interpreting Attention from the Perspective of Interpolation\nIn this section, we reinterpret the theoretical formulation of the Attention mechanism by recasting it as a special interpolation formula. This perspective elucidates the role of different components within Attention and demonstrates its clustering effect. The detailed analysis is as follows:\nA.1.1 Bilinear and Cubic Spline Interpolation\nConsider the following interpolation formula:\n$f(x,y) = \\sum_{i=0}^{h} \\sum_{j=0}^{w} W_{ij} f(x + i, y + j)$\n$\\sigma(x, y) = \\sum_{i=0}^{h} \\sum_{j=0}^{w} g(d_{ij})v(x + i, y+ j)  or  \\sum_{i=0}^{h} \\sum_{j=0}^{w} g(d_{ij}).(x_i, y_j)$\nIn the above equations, h = 2 corresponds to bilinear interpolation, while h = w = 3 corre- sponds to cubic spline interpolation. Here, (x, y) represents the target interpolation location, and $d_{ij}$ denotes the distance metric between $(x_i, y_j)$ and (x, y). The distance metric $d_{ij}$ can be computed using various methods, such as the n-norm or inner product. The function $g(d_{ij})$ is a weighting function based on this distance metric. For bilinear interpolation, $g(d_{ij}) = \\frac{x_i-x}{x_i-x_j}$, while for cubic spline interpolation, $g(d_{ij}) = \\frac{x_i-x}{x_i-x_j}$.\nBy comparing these interpolation formulas with the Attention mechanism, we can approximate the global interpolation formula as follows: Let $g(d_{ij}) = softmax(q_i k)$ and $v(x_i, y_j) = v$. This demonstrates that the fundamental principles underlying both interpolation and Attention are similar. Both methods combine vectors with similar features, leading to interpolated results that emphasize these shared features, making them more useful for the task at hand. Although Attention does not strictly satisfy the conditions for interpolation\u2014since it does not require the value at position (xi, yj) to be v(xi, Yj)\u2014it can be considered a generalized form of interpolation or regression. This analogy may explain why training in language models is often referred to as autoregression.\nA.1.2 Interpolation Formula on Riemannian Manifolds\nWe can further extend this analogy by considering Attention as a special form of interpolation mapping or exponential mapping on a manifold. For instance, on a Riemannian manifold, there exists a matrix exponential mapping given by:\n$Exp_{(P, \\Lambda)}(T) = P^{1/2} exp (XP^{-1/2}TP^{-1/2})p^{1/2}$\nwhere T is a tangent vector matrix, typically a symmetric matrix that can be decomposed into $ww^\\dag$, and P is the metric matrix corresponding to a point on the manifold.\nBy generalizing the variables in the Attention formula, we can map $ww^\\dag$ to $w_qw_k$ and the metric matrices $P^{1/2}$ and $P^{-1/2}$ to the X matrices in Attention. Thus, the interpolation formula becomes:\n$Exp_{(P, \\Lambda)}(T) = exp (Xw_qw_k X)X^{-1}$\nThis transformation completes the interpolation and maps it to the V space."}, {"title": "A.2 Approximation and Convergence of Our MS Attention to Various Efficient Attention Methods", "content": "Our proposed Select-Merge Attention (MS Attention) can approximate and converge to the optimal solutions of many efficient Attention mechanisms that utilize a subset of key-value (KV) pairs. Below, we describe the computation process using the notations introduced in the algorithm description:\n- Selection:\n$A_s = QK^T, Idx = topkIndex(A_s)$\n- Merge:\n$Q_s = merge(Q_s^b), Idx = filter(merge_(Idx)), KV_s = Select(KV, Idx)$\n- Final Attention:\n$O = Attention(Q_s, K_s, V_s)$\nA.2.1 Landmark Attention\nLandmark Attention adjusts the attention scores by incorporating landmark tokens, with the output expressed as:\n$O = (softmax(Q K^T) \\cdot repeat(softmax(Q G^T), blocksize, dim = -1))V$\nWhen setting the split size of Q to 1 and the split size of K to blocksize, our MS Attention also calculates $softmax(Q K^T)$ and $softmax(Q G^T)$, which fully enables the adjustment of attention scores using semantic tokens. This operation can be approximated under other settings as well.\nAlternatively, if the score adjustment is performed without directly multiplying $softmax(Q K^T)$, treating semantic tokens as regular KV tokens during Attention computation, the output can be expressed as:\n$\\frac{sumexp(Q K^T) \\times softmax(Q K^T) V_s}{sumexp(Q K^T) + sumexp(Q K^T)} + \\frac{sumexp(Q K^T) \\times softmax(Q K^T) V}{sumexp(Q K^T) + sumexp(Q K^T)}$\nwhere $V'$ is a linear combination of V or $V_s$. This approach can also directly adjust the attention coefficient of $V_s$ in the output, allowing convergence to the optimal solution based on the task.\nA.2.2 BiFormer\nThe BiFormer computation process is as follows:\n$A_r = Q_r(K_r)^T, I_i = topkIndex(A_r), KV_i = Select(KV, I_i)$\n$O = Attention(Q, K_g, V_g)$\nWhen the Merge size is set to 1, our method fully degenerates to BiFormer. To achieve lower complexity, BiFormer requires setting the initial region $Q_r, K_r$ relatively large and then compressing it as a region representative. This approach reduces the accuracy of selection.\nWhen merge size is greater than 1, our method performs more fine-grained partitioning and selection. If BiFormer's selection is optimal, our algorithm can also converge to this optimal selection. Overall, compared to BiFormer, our method has a larger convergence space, and under the same fine partitioning and selection, our method has relatively lower computational complexity."}, {"title": "A.2.3 Routing Attention Methods", "content": "For the Routing Transformer, the following update rule is applied:\n$\\mu \\leftarrow \\lambda \\mu + \\frac{(1 - \\lambda)}{2} \\sum_{i: \\mu(Q_i) = \\mu} Q_i + \\frac{(1 - \\lambda)}{2} \\sum_{j: \\mu(K_j) = \\mu} K_j - \\mu$\nThis can be rewritten as:\n$\\mu \\leftarrow \\lambda \\mu + \\frac{(1 - \\lambda)}{2} - argmax_{q \\mu} ((\\mu Q^T)Q + \\frac{(1 - \\lambda)}{2} argmax_{q \\mu} ((\\mu K^T)K$\nwhere\n$argmax_{\\mu} (-) = \\begin{cases} 1 argmax(-, dim = -2) \\\\ 0 otherwise \\end{cases}$\n$Idx_Q = topkIndex(\\mu Q^T), Idx_K = topkIndex(\\mu K^T)$\nUsing the triangle inequality of the metric, the above approximation can select Q-related KV tokens with:\n$Idx = topkIndex(Q \\mu \\mu K^T)$\nOur semantic tokens in each Attention step also cluster similarly:\n$m_s = Softmax(\\sigma_i K_s)K_s$\nor\n$m_s = m_s + softmax(Q_s K^T)V_s$\nOur selection process can be written as:\n$Idx = topkIndex(m_s W_Q W_Q m_s^T)$\nwhere $m_s$ is the regional semantic token of Xs (e.g., average or $m_s = Softmax(\\sigma_i K_s)K_s$), equivalent to further clustering.\nTherefore, during selection, the cluster center can be used to represent the tokens within the relevant cluster, similar to the Routing Transformer. Our method, by only using the cluster center for selection, introduces minimal quantization error. However, because the selection quantity is sufficient, the loss is negligible. In contrast, the Routing Transformer loses some related Q and K tokens to ensure regular shape, introducing non-negligible error.\nA.2.4 Swin Transformer\nThe Swin Transformer utilizes local Attention and shifted local Attention. Our selection mechanism can completely converge to Swin Transformer when it is optimal.\n- Local Attention:\n$softmax(Q_{i:j} K_j)V_j$\nWhen this is the optimal solution, our selection mechanism will automatically select tokens within the local region.\n- Shifted Local Attention:\n$softmax(Q_{i:j} K_{+r:j+r})V_{i+r:j+r}$\nWhen this is optimal, our selection mechanism will automatically select tokens within the corresponding region $(K V_{i+r:j+r})$, where r is the cyclic shift offset.\nIn a similar manner, our method can approximate or cover many variants of the above Trans- formers.\nTo compare our method with these efficient Transformers, we conducted experiments on the PG19 datasets, as shown in the tables 13."}, {"title": "A.3 Detailed Parameter Settings", "content": "Our algorithm can encompass the majority of Q-attention to KV-subset methods by adjusting parameters such as the QKV segment size, the number of selected top-K high-relevance KV regions, and the number of merged Q regions. Below is a detailed discussion on the selection of these hyperparameters:\nA.3.1 QKV Segment Size Selection:\nThe QKV segment size is crucial and typically ranges from 8 to 128. This parameter must be chosen by balancing computational complexity and performance. In future work, we plan to incorporate Triton operators in the QK routing step to achieve linear spatial complexity, thereby mitigating the current limitations:\n- (1) During the selection and routing process, a semantic token represents each region, and the relevance between Q and K semantic tokens is measured. KV tokens related to the Q region are then selected based on this relevance. A larger QK region size results in a coarser semantic representation, leading to less precise KV token selection. To minimize this loss, more KV tokens must be selected in the second step, thereby reducing information loss due to coarse semantics. Thus, smaller segment sizes are preferred, though incorporating Triton operators in the QK routing step is anticipated to provide linear spatial complexity.\n- (2) Alternatively, a larger segment size can be set, and more KV tokens can be selected in the second step. This approach may increase algorithmic complexity as more KV tokens are likely to be used for interpolation within the same Q region. However, this issue can be alleviated through the third step of merging, where the selected KV tokens can be further merged and selected according to relevance, filtering out irrelevant information.\n- (3) Additionally, the segment size of Q regions is independent of the segment size of KV regions. A relationship between them should only be established when specific tasks and complexity constraints require it.\nA.3.2 Top-K High-Relevance KV Region Selection:\nThe number of selected top-K high-relevance KV regions is generally determined by factors such as the model's pre-training length and the amount of task-specific data. It may also need to be adjusted based on the granularity of the segmentation from the first step. Current large model training methods suggest that selecting 1K-8K KV tokens for interpolation is robust:\n- (1) The above represents a general approach, while scenario-specific selection yields higher performance and efficiency. For example, if the task's data volume is small, a relatively small number of high-relevance regions can be selected to maximize overfitting and memorize the critical data. Conversely, as the data volume increases, more KV tokens may be required for autoregressive prediction of the next token, necessitating the selection of more high-relevance regions."}, {"title": "A.3.3 Merging Q Regions:", "content": "The number of merged Q regions generally depends on the segmentation precision and space complexity. In scenarios where space complexity is not a major concern", "as": "n- a. Simple unique and sort operations for selection.\n- b. Unique operations followed by a secondary segmentation based on high-relevance scores, with allocation and selection according to region clustering.\nA.3.4 Algorithm Complexity Analysis\n(1) Let the segmentation sizes for the Q and KV regions be CQ and $C_K$, respectively. The time and space complexity of routing Q and K using dot products is O ($\\frac{N^2}{CaCk}$).\n(2) In the second step, the selection of high-relevance regions, denoted by $C_s$, results in space complexity of O ($\\frac{N^2}{Cs}$) due to the storage of necessary indices.\n(3) The merging step, with a merge size of $C_M$, involves combining the selected indices. Filtering algorithms can be introduced during merging; in this work, we employ unique and high-score selection. The space complexity for storing KV tokens after selection and merging is O($\\frac{N^2}{M C_S}$). (4) The final attention operation has a time complexity of $O(\\frac{N^2"}, {}]}