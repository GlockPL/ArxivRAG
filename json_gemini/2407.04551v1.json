{"title": "An AI Architecture with the Capability to Classify and Explain Hardware Trojans", "authors": ["Paul Whitten", "Francis Wolff", "Chris Papachristou"], "abstract": "Hardware trojan detection methods, based on machine learning (ML) techniques, mainly identify suspected circuits but lack the ability to explain how the decision was arrived at. An explainable methodology and architecture is introduced based on the existing hardware trojan detection features. Results are provided for explaining digital hardware trojans within a netlist using trust-hub trojan benchmarks.", "sections": [{"title": "I. INTRODUCTION", "content": "Hardware trojans are malware circuits that are injected within an integrated circuit (IC) during design stages, before the IC is manufactured. Once manufactured, the trojan cannot be removed nor can it be easily bypassed by software patches because it is baked into the IC chip. The trojan has become part of the DNA of the IC chip and unknowingly part of the intellectual property of the hardware design. Hence, attackers use hardware trojans to weaken the information security of the IC chip whose properties can be summarized as confidentiality, integrity and availability (CIA). For example, an availability hardware trojan will change the privileged mode of a processor from user to supervisor mode which allows for root access within the software operating system. An integrity hardware trojan can periodically leak or corrupt the sensitive information, such as encryption/decryption keys to the primary outputs (PO).\nFig. 2 shows a specific implementation of a confidentially hardware trojan using logic gates within an IC design file netlist. The dashed lines represent additional levels of logic gates. The trigger is conditional on input patterns within the IC which eventually originate from the primary inputs. The logical-and gate requires that all the inputs are logically true before triggering the payload. In order to be difficult to detect or accidentally trigger during IC test mode, the hardware trojan trigger should be a rare event condition. Given n inputs to a combinational IP circuit, the rarest condition would be one of $2^n$ inputs. Once the hardware trojan is triggered, the multiplexor circuit within the payload switches from normal operation to leaking sensitive information on the primary output pin of the circuit where it can be observed by the attacker.\nThis rare event hardware trojan model allows for detection through static circuit analysis by examining each net or group of logic gates within an IP circuit primary for input or fanin complexity. The greater the fanin then the higher the probability that a subcircuit is a potential suspect for a hardware trojan. Furthermore, payloads need to propagate their sensitive information to the primary output. The rare event fanin and primary output payload concepts forms the basis of trojan analysis of several methodologies [1]\u2013[4].\nFanin and output payload are features of a net or set of logic gates. Features are used widely in ML classification and pattern recognition. Individual features may contribute little to identification. The combination of features, however, is useful for effective classification. Combining features can also be useful for explaining the results of a system.\nThe ability to effectively explain decisions is necessary to establish trust in an automated system. Without an explanation, a user has little understanding and faith in a decision. This work approaches hardware trojan identification, in previous work, and attempts to compare two techniques of explaining decisions.\nThe first method leverages combinations of features, iden-tified as properties, to explain decisions of a system in rec-ognizing hardware trojans. An architecture is introduced that combines features, building an ML architecture that operates on feature combinations to identify and explain decisions in identifying hardware trojans.\nThe second method takes a case-based approach for explain-ing decisions. New samples are related to cases the system was trained on. A second explainable architecture is presented that utilizes a training index (TI) to rapidly provide relevant examples and references to cases in netlists for context."}, {"title": "II. RELATED WORK", "content": "Haswega et al. introduced a method of detecting hardware trojans from a gate-level netlist [2], [3]. Five features from each net are used to classify trojans. (a) Logic Gate Fanins (LGFi) which represents the number of inputs to the logic gates two levels upstream from a net. (b) Flip-Flop Input (FFi) which represents the number of upstream logic levels to a flip-flop. (c) Flip-Flop Output (FFo) which represents the number of logic levels downstream to a flip-flop. (d) Primary Input (PI) which represents the number of logic levels upstream to the closest primary input. (e) Primary Output (PO) which represents the number of logic levels downstream to the closest primary output. In the work, the authors also used three training strategies, due to the imbalance of the trojan to non-trojan training data ratio: no weighting, static weighting, and dynamic weighting. Apparently, dynamic weighting performed best. Other works address the imbalance in other means such as synthetic method over-sampling techniques [5]. These optimized methods still do not lead to a better understanding of the ML results.\nSeveral works discuss the combination of results of multiple trained neural networks (NN). Jacobs, et al. identified local experts of trained networks [6]. Other works treated multiple trained networks as committees and combined NN to form a collective decision [7], [8].\nAn explainable additive NN model is posed by Vaughan et al. where a system is composed layering distinct NNs that are trained on transforms of the inputs. A layer then combines the outputs of the distinct NNs to perform a prediction. Explainability comes from each distinct NN modeling features of the input which lends to interpretability of the architecture [9].\nAn explainable Artificial Intelligence architecture (XAI) using properties, transforms of input related to the proper-ties, Inference Engines (IE) for each property, probabilistic decision-making, and attributing decisions to relevant explain-able properties was posed [10]\u2013[12]. Like combined NN and additive NN systems, the explainable architecture examined decisions of multiple NNs.\nCase-based explanations for medical models, introduced by Caruana et al., suggested using a method based on k-nearest neighbor (KNN) distance in multidimensional feature space as effective in identifying like cases from training as explanations for new samples [13]. Like cases from training should produce similar results to new samples. The case-based method further suggested that leveraging training data is more difficult for more complex models such as NN as the training set is discarded. In the case of NNs, the activation of n hidden neurons are translated into points in an n-dimensional space and a KNN algorithm can be applied to find similar activation patterns. While this may suggest similarity to the NN's activation and behavior between like cases, the method does little to explain what is going on in the NN."}, {"title": "III. METHOD", "content": "This work utilized trust-hub.org trojan benchmark data [4], [14]\u2013[16]. The particular fifteen netlists used from trust-hub were: RS232-T1000, RS232-T1100, RS232-T1200, RS232-T1300, RS232-T1400, RS232-T1500, RS232-T1600, s15850-T100, s35932-T100, s35932-T200, s35932-T300, s38417-T100, s38417-T200, s38584-T100, and s38584-T300.\nFig. 3 depicts the flow of data processing and preparation. The verilog gate-level netlists containing trojans were processed using circuitgraph [17] which uses Lark, a Python parsing toolkit [18]. Directed graph representations of the netlists were built in NetworkX [19] and then queried to obtain the five features and class for each gate. Class can be defined as trojan (1) or non-trojan (0). Herein, when referencing class, trojan is often abbreviated as t and non trojan as n. Submodules in trust-hub netlists were not processed. Omitting\nof the properties was stored in the KB by reprocessing the training data and analyzing results to obtain performance metrics for each IE. Effectiveness as weights are used in tallying the votes to produce decisions, suspected trojan (t) or non-trojan (n), and confidence of the decisions. As was experienced with other highly imbalanced datasets, measuring effectiveness is a challenge. Effectiveness metrics were tried and $EPARS$ performed best [12].\nThe XAI function takes the decisions, votes, and confidence producing an explanation to justify the decisions of the system. The explanations relate to properties, hinting at relationships between features.\nIn building the rationale in XAI, a threshold of 0.05 was used for registering a vote worthy of mention. Without this threshold, each of the 31 properties were listed in the rationale, many with very little weight. The rationale composed by XAI is also organized with weights of properties sorted in a descending fashion to mention the highest contributing properties first.\nEach property $P_j$ has an explainability metric, $X_j$, signifying how explainable that property is. Explainability, $X_j$, is given in (2). The Explainability metric is based upon the number of features or cardinality of the $j^{th}$ property, $P$. Again, n, is the number of features in the input vector to the architecture.\nA property with one feature would have high explainability, $X_1$ = 1.0. Properties with few features are more explainable, while a property with more features is difficult to explain. Intuitively, this corresponds with reasoning that a ML model with the five features used by Haswega acted as an opaque box. If a single feature could easily indicate a trojan, that is likely explainable to a user. Explainability for property combinations in this work for the properties is indicated in the $X_j$ column of Table I.\n$X_j = 1.0 - \\frac{|P_j - 1|}{n - 1}$\nSVMs are based on fitting hyperplanes to bound, in mul-tidimensional space, classes that are learned by training data. KNN and the concept of distance translate well to the multi-dimensional space of features. Despite the notion suggested by Caruana et al., that KNN between a new sample and training data only explains the training data, this work suggests that using a KNN approach to training data is effective for providing an explanation, backed by training cases.\nFig. 5 depicts the flow of the case-based explainable method. It starts from the features from data preparation. The initial training phase, shown on the left vertical flow, includes deter-mining $\\gamma$ and C for the SVM. Training the SVM using the training data is completed next to produce an SVM model. Lastly, a training index is built. The right portion of the flow in Fig. 5 illustrates classification and explanation of new samples using the SVM Model and TI created in training. The decision from the SVM model and case-based justification from TI serve as input to compose results provided to the user.\n$b(class) = \\begin{cases} \\frac{|non - trojan|}{|trojan|} & \\text{class = trojan (t)} \\\\ 1.0, & \\text{class = non-trojan (n)} \\end{cases}$\nThe Decision Making Process (DMP) is responsible for considering the votes from the IEs and making a system-wide decision. As the DMP receives votes from the IEs, it obtains property effectiveness weights from the KB. Effectiveness\nAn important property of data preparation for explainability and trust in case-based explanations is preserving the origin of each sample for storage in the TI as the samples are combined for training. The origin in the context of training samples is comprised of the part number, version (indicating which of the 15 circuits the sample came from), line, name, and net. While originating fields are not useful to decision-making and classification, they provide crucial context for the explanation.\nThe TI was constructed of in-memory hashmaps to quickly retrieve the supporting data based on features. Hashmaps worked well for this application and relatively small dataset. In larger datasets, other key-value stores could be employed with comparable results. Because of the small size of the dataset and current compute, a simple linear search was sufficient for identifying nearest neighbors from training. As the dataset scales, a variety of more efficient algorithms exist for searching for nearest neighbors [21], [22].\nStatic and dynamic weighting far outperformed no weighting. Both static weighting and dynamic weighting help overcome the highly imbalanced dataset. When the static method utilizes a balance factor, b, given in 1, rather then an arbitrary number, it per-forms comparably to dynamic weighting. The balance factor, b, is given by the number of non-trojan nets over the number of trojan nets. In the 80% training set, b(trojan) = 263.7. When training the architecture, the static method, using balance, was employed due its performance and simplicity.\nKNN information presented to the user includes distance from the sample being considered, the classes (trojan and non-trojan) of training samples, and references to the samples in the context of the netlists they were extracted from. In addition to this information, the weight of neighboring trojans, w(t), and non-trojans, w(n), are calculated relative to the inverse square of distance, d, as shown in 3. Balance, from 1, is summed for each set of training samples at the distance to the sample considered. The weights are then summed to provide a correspondence metric, C, for KNN as shown in 4.\n$w(class) = \\sum_{i=1}^{class} \\frac{b(class)}{(d_i+1.0)^2}$\n$C(class) = \\frac{\\sum_{j=1}^{k} w(class)}{\\sum_{j=1}^{E_{classes}} w(j)}$"}, {"title": "IV. RESULTS", "content": "SVMs were trained using the three methods: no weighting, static weighting, and dynamic weighting. When static weight-\nTable II depicts the three examples from the test set used to demonstrate explainability. The first column in the table represents the example identifier (ID). The next five columns are the features. The last column indicates if the sample is a trojan (1) or non-trojan (0).\nStatic and dynamic weighting appeared to perform equivalent to dynamic weighting.\nA. Property Based Explainable Results\nTable II displays examples from the test set (20%) used to exhibit explainability. This section will present the property-based architecture results from examples using the static weighting technique.\nExample one, as shown in Table III, was appropriately predicted as a trojan (1) with 99.6% confidence. Fifteen properties, listed by ID from Table I in order of effectiveness, contributed sufficiently weighted votes to be registered (above the 5% threshold). Despite the high confidence, explainability was weighted at 41.1%, because fourteen of the properties above the threshold had three or more features, so they had limited explainability ($X_j \\leq 0.5$ in Table I). The alternate decision to identify example one as a trojan had confidence < 1% with none of the remaining properties above the threshold.\nThe property-based architecture improperly identified exam-ple two, suggesting the winning decision with medium, 74.3%, confidence was non-trojan. Table IV illustrates the properties, confidence, and explainability for example two. Ten properties voted for a non-trojan, while five suggested a trojan. Again, the explainability was low at only 35.0% for the winning decision due to the relatively large number of features in voting properties.\nExample three is labeled as non-trojan in the test set. The architecture improperly identified the sample as a trojan with medium, 58.4%, confidence. In Table V nine properties suggested the sample as a trojan while six suggested a normal net. Explainability was among the highest at 47.2%.\nB. Case-Based Explainable Results\nThe case based explainable architecture performed well, providing KNN for all of the test samples. Weights of KNN corresponded to the decision of the architecture in 97.4% of the cases.\nThe SVN in the case-based architecture for example one voted for a trojan. The KNN cases provided by the architecture are shown in Table VI where the distance column indicates the euclidean distance to the sample. The feature values indicate the feature vector of the training sample(s). The t: n column indicates the ratio of trojan (t) to non-trojan (n) nets for the samples in the training set. The w(class) columns indicate the weight for each class, trojan (t) and non-trojan (n) from 3. The case-based architecture further provides a quantitative measure for example one indicating the decision corresponds with neighbor weighting by suggesting a correspondence of 98.1% for the prediction of trojan based on neighbors from training data.\nReferences to the neighboring cases for example one are helpful in identifying those cases in context. Neighbors at (8, 1, 3, 2, 3)include all eleven matches e.g., part number RS232, version T1500, line 40, name NAND4X1, and net U294.QN as well as part RS232, version T1000, line 35, name NAND4X1, and net U299.QN. The output of the explainability results produces all references to KNN examples from the case-based training set.\nExample two was incorrectly identified as non-trojan by the case-based architecture. The KNN cases are depicted in Table VII. The architecture assigned a correspondence of 57.8% for the prediction of no trojan based on neighbors from training data. This was due to the single neighboring trojan's weight at distance one being overcome by other neighboring 196 samples. References to all 197 samples from training were provided for context.\nExample three, shown in Table VIII, was correctly identified as not a trojan by the SVM. The KNN gave a rare conflicting 99.6% correspondence metric for a trojan contradicting the prediction of the SVM. The two closest samples from training were all non-trojan with three samples. The three samples were overcome by the ten samples that were the next closest due to the balance."}, {"title": "V. CONCLUSION", "content": "The property-based explainable architecture produced rationale for decisions that had only marginal utility in relating the decisions to combinations of features of the input data. Explainability metrics for the property-based architecture were fairly low for the properties due to a generally high number of features in the properties above the threshold. The low explainability indicates that the rationale for the property based decisions was not compelling. As previously observed, with the property-based architecture, explainability was at the cost of accuracy compared to other leading unexplainable methods.\nExplanations and justification for the case-based explainable architecture outperformed the property based architecture on a subjective basis. The case-based architecture also benefitted from the slightly more accurate but unexplainable SVM. The cases from training were relevant and references to originating netlists of training samples added credibility and trust to the system. Quantitatively, the case-based architecture provided high correspondence, 97.4%, in weighting neighboring training cases to the decisions of the SVM.\nAttempting to apply the property-based explainable archi-tecture to a dataset with a low dimensional space (five features) did not work as well as a case-based architecture in explaining results in the case of hardware trojan detection. The case-based architecture clearly outperformed the property-based architecture in providing explanations and establishing trust in this application."}]}