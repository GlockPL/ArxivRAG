{"title": "Do Tutors Learn from Equity Training and Can Generative AI Assess It?", "authors": ["Danielle R. Thomas", "Conrad Borchers", "Sanjit Kakarla", "Jionghao Lin", "Shambhavi Bhushan", "Boyuan Guo", "Erin Gatz", "Kenneth R. Koedinger"], "abstract": "Equity is a core concern of learning analytics. However, applications that teach and assess equity skills, particularly at scale are lacking, often due to barriers in evaluating language. Advances in generative AI via large language models (LLMs) are being used in a wide range of applications, with this present work assessing its use in the equity domain. We evaluate tutor performance within an online lesson on enhancing tutors' skills when responding to students in potentially inequitable situations. We apply a mixed-method approach to analyze the performance of 81 undergraduate remote tutors. We find marginally significant learning gains with increases in tutors' self-reported confidence in their knowledge in responding to middle school students experiencing possible inequities from pretest to posttest. Both GPT-40 and GPT-4-turbo demonstrate proficiency in assessing tutors ability to predict and explain the best approach. Balancing performance, efficiency, and cost, we determine that few-shot learning using GPT-40 is the preferred model. This work makes available a dataset of lesson log data, tutor responses, rubrics for human annotation, and generative AI prompts. Future work involves leveling the difficulty among scenarios and enhancing LLM prompts for large-scale grading and assessment.", "sections": [{"title": "1 Introduction", "content": "Equity is a core concern of learning analytics (LA) [21], however, applications that teach and assess equity competencies are lacking. Intelligent tutoring systems, a core learning analytics application, have improved learning in STEM domains"}, {"title": "2 Related Work", "content": ""}, {"title": "2.1 The Role of Scenario-based Learning in Tutor Development", "content": "Scenario-based learning integrates educational activities within real-world contexts, promoting rapid development of skills through situational activities [2]. This instructional strategy has been successfully applied across multiple disciplines, such as medical education, fostering prospective thinking among high school students, and enhancing the growth of pre-service teachers (e.g., [1, 19, 28]). Furthermore, scenario-based models, including digital simulations, offer novice teachers and tutors valuable low-risk practice environments to gain situational experience [7, 8, 36]. Authentic scenarios, coming from real-life tutoring situations, are used by tutors to practice \"learning by doing,\" which is an instructional approach that emphasizes active participation and hands-on practice to acquire knowledge and skills through direct experience. The intention is that this learning transfers to real-life tutoring environments. Learning by doing requires the application of skills that model the needs of the real world [23], facilitating the transfer of new knowledge to similar experiences that trigger recall [31].\nPast studies have shown approximately 20% learning gain from pretest to posttest in scenario-based lessons covering tutor topics such as giving effective praise to students; reacting when a student makes an error; and determining what students know [34]. Both training and transfer scenarios (also known as pretest and posttest) follow a modified predict-observe-explain (POE) approach, theoretically connected to Gibbs' Reflective Cycle, a cyclical instructional model providing structure for learning by doing to individual learning experiences [12]. In line with Figure 1, tutors respond to a training scenario, or pretest, (that is, a student possibly experiencing an inequity) by asking them to predict how to best respond within 1) an open-ended question and 2) a multiple-choice question (MCQ). Then tutors explain their prediction via 3) an open-ended question and 4) a MCQ. Tutors then 5) observe the given research recommendation and receive feedback before they 6) explain the reasoning behind what they observed. Finally, the tutors complete the transfer scenario or posttest, following the same pattern of predicting the best responses and explaining their reasoning (7-10). The pretest and posttest each contain a maximum of four points (two MCQs and two open-ended questions), with tutor learning gains determined by subtracting tutor pretest score from the posttest score. This process provides a method for assessing the transfer of learning and, ultimately, the tutor's learning gain [7, 34]."}, {"title": "2.2 Addressing Educational Inequities through Scenario-based Learning", "content": "Ensuring equity remains one of the greatest challenges in education today [11, 13]. For example, a common situation that educators and tutors face is a student who cannot complete an assignment or access instructional materials outside of school because they do not have the Internet at home. Among school-age children worldwide, an estimated two-thirds do not have access to the Internet at home [37]. Many students do not recognize such inequities when they experience them. A student without the internet at home tasked with completing an assignment that requires broadband access will simply not do the assignment. They may not recognize the lack of access as inequitable and frankly unfair. Educators and tutors can play an important role in promoting equity. Rapid-cycle learning, in the form of short instructional activities for educators and tutors, is gaining traction to exercise applying strategies to advance equitable practices [11]. Rapid-cycle learning, exemplified by the situational judgment tests presented in this work, offers a method of providing instructional support to educators and tutors in attending to social justice and equity concerns [32]. Through brief, scenario-based learning activities, tutors participate in low-risk opportunities to pilot strategies to"}, {"title": "2.3 The Helping Students Manage Inequity Lesson", "content": "One approach for promoting equity-responsive practices among secondary students is instructing students on collaborating with adults and advocating for themselves [13]. The lesson draws on previous research to identify the key competencies of effective tutoring, with scenario-based learning activities developed to align with key competencies [34]. We strive to determine tutor learning gains similar to [34] while focusing specifically on how tutors respond to students experiencing needs, potentially indicative of inequities. The lesson objectives include: recognizing when a student may be experiencing inequity related to their learning; and applying strategies to help students manage inequities by assisting students to advocate for themselves.\nIn one of two scenarios (used interchangeably as a pretest or posttest), student Jeremiah could not complete his homework because he did not have access to the Internet at home. In this specific situation, the tutor does not know if the student did not have the internet for temporary reasons, such as a random system outage, or if the lack of internet access is associated with a systemic disparity affecting the student's ability to succeed, such as socioeconomic status. However, the research-recommended approach of the tutor is to help Jeremiah recognize his need and empower him to advocate for himself by exercising his voice [13]."}, {"title": "2.4 Benefits of Multiple-choice and Open-Ended Responses", "content": "Multiple-choice questions (MCQs) are a type of closed-response question often used in assessments due to their efficiency and objective grading [5]. However, they come with challenges such as encouraging reliance on test-taking strategies, issues with face validity, and difficulty in generating high-quality distractors or \"incorrect\" options [5]. In"}, {"title": "2.5 Using Generative Al to Assess Open-ended Tutor Responses", "content": "Early ASAG approaches for assessing open-ended responses relied on traditional machine learning techniques, such as feature extraction and bag-of-words models, which provided easily interpretable results [18, 27, 39]. However, these early models struggled with domain shifts, where subtle differences in assessment tasks significantly impaired their performance [17]. More recently, studies have turned to deep learning models for automated assessment of open-ended responses [9, 33, 40]. For example, [9] explored ASAG using Sentence-BERT (SBERT) to measure textual similarity within learner response grading. Although SBERT showed promise for generalization, it encountered difficulties with unseen questions, revealing the need for models capable of deeper contextual understanding and adaptability, particularly in more complex domains such as equity training.\nRecent advances in generative AI, particularly large language models (LLMs) such as GPT-4, have revolutionized ASAG by enabling models trained on extensive datasets to interpret and assess nuanced textual responses [6], with applications in tutor training [16, 20, 26]. LLMs offer flexibility, as they can be fine-tuned for specific educational tasks or used in their pre-trained form. Studies show that, with effective prompting, LLMs can capture the subtleties in learner responses, improving grading efficiency and scalability [17]. Nevertheless, challenges persist, as LLMs operate as \"black-box\" models, making their outputs difficult to interpret [6]. Furthermore, LLMs lack consistent knowledge of the pedagogy and content and are prone to hallucinations, generating confidently inaccurate or nonsensical information [39]. There remains an urgent need to explore how LLMs can be trained to assess equity-focused tutor responses, such as recognizing when tutors advocate for underserved students or support them in addressing inequities. These advancements could significantly improve tutor training to promote equitable education."}, {"title": "3 Method", "content": ""}, {"title": "3.1 Tutor Participants, Lesson Delivery, and Construct Validity", "content": "There were 81 college-student participants who completed the lesson, employed as paid tutors for a remote tutoring organization supporting middle-school students. While the demographics of the tutors were undisclosed, they exhibited cultural and racial diversity. Tutors' self-reported tutor experience levels were assessed using a 5-point Likert scale with 1 indicating little to no experience (novice) and 5 indicating an expert tutor. On average, the tutors reported an experience level of 3.2 (SD = 1.22). Furthermore, measures of tutor self-perceptions of their confidence in the topic were surveyed before pretest and after posttest using a 5-point Likert scale ranging from 1 (not at all confident) to 5 (extremely confident). After the posttest, tutors were surveyed regarding the application of the lesson content using a 5-point Likert scale (1- strongly disagree; 5- strongly agree): I am confident I can apply what I learned. The lesson was crafted by a university research team specializing in learning science, in collaboration with an equity-focused consulting firm,"}, {"title": "3.2 Human Open-ended Response Coding", "content": "Open-ended questions were coded, with \"correct\" tutor responses designated as \"1\" and \"incorrect\" as \"0.\" Two experienced researchers coded participant responses to assess interrater reliability. The responses were deemed correct if the tutors demonstrated understanding of the lesson objectives and the research-recommended approach. For predicting the best response, or correct response (score = 1), tutors should apply the strategy of assisting the middle or high school student in encouraging them to advocate for themselves by talking directly with their teacher or person in charge. In contrast, incorrect responses (score = 0) involved the tutor suggesting that the student find a way to solve the problem on their own or made suggestions that do not directly involve the student advocating for themselves in attending to their needs. Incorrect responses also include responses where the tutor merely indicates to a student to speak with a teacher without encouraging active advocacy. For explaining their chosen approach, tutor responses must demonstrate that the tutor recognizes that the student needs support in advocating for themselves and encouraging the student to act."}, {"title": "3.3 Inter-rater Reliability Among Human Graders", "content": "Human coders assessed the responses of all 81 tutors for the predict and explain open-ended responses. The results indicated a relatively high agreement in inter-rater reliability between the coders using the binary coding system (i.e., 0, 1). For responses requiring tutors to predict the best course of action, there was 89% agreement and a Cohen's k of 0.75. For responses asking tutors to explain their rationale, there was an 87% agreement rate and a Cohen's k of 0.73. Both reflect substantial agreement, supporting the reliability of the coding process."}, {"title": "3.4 Determining Tutor Learning Gains", "content": "We employed a mixed-effects ANOVA to examine the impact of lesson scenarios on tutor performance. The scenarios (i.e., Jeremiah or Alexis) served as the between-subjects factor, while time, specifically pretest and posttest, served as the within-subjects factor. Treating scenario as a fixed effect helps to determine if there is an imbalance in difficulty between the two scenarios, while considering the test time as a random effect accounts for variation within the subjects. The ANOVA was run on data of students who completed all training and test items, resulting in a reduced sample of 81 students. A split-half reliability analysis adjusting for a time factor between pre and post-test was used to determine the reliability of the employed test battery. Specifically, we correlated person parameters of Rasch models across all possible splits of items. This resulted in an average reliability of 0.489 across all assessment items, which is acceptable for a test including only eight assessment items, although lower than typical test batteries which feature more items [22]."}, {"title": "3.5 Prompt Engineering to Evaluate Tutor Responses", "content": "Drawing from recent advancements in prompt engineering, particularly within the domain of tutor training [25, 35], we developed prompts to leverage GPT-4-turbo and GPT-40 for evaluating the correctness of open-ended tutor responses. Model temperature was set to 0 to ensure the model operated deterministically, selecting the most likely next word (or token) based on the input, reducing variability. This approach leads to more predictable and generally more cautious responses. The output was limited to 300 tokens to avoid excessive verbosity. Using zero-shot and few-shot prompting approaches, we provided the models with examples to guide their assessment of the predict and explain responses. The creation of these prompts followed an iterative process, with several rounds of adjustments informed by feedback from initial model outputs. We employed several prompt engineering strategies to enhance model performance, including chain-of-thought prompting [38] to encourage step-by-step reasoning by prompting the model to provide the rationale. We used few-shot prompting (Brown et al., 2020), by supplying the model with relevant examples of correct and incorrect tutor responses to guide its output. Last, we used contextual priming to help the model understand the task within a specific context (i.e., \"You are a tutor evaluator...\"). Together, these techniques enabled the models to assess tutor responses with greater accuracy and consistency, aligning outputs more closely with human evaluations and enhancing overall model performance."}, {"title": "4 Results", "content": ""}, {"title": "4.1 RQ1: Is the scenario-based lesson effective in teaching tutors new skills for responding to students\npossibly experiencing inequities?", "content": "The analysis revealed a marginally significant main effect of time on tutor performance, $F(1,79) = 3.20$, $p = .078$, indicating an overall improvement in tutors' performance from pretest to posttest. This suggests a general learning effect or improved skills. This main effect was qualified by a significant interaction between scenario and time, $F(1, 79) = 4.31$, $p = .041$. This significant interaction implies that the degree of improvement from pretest to posttest varied between the two scenarios. There was no statistically significant main effect of the specific scenario on tutor performance, $F(1,79) = 0.54$, $p = .465$, indicating that the difficulty level between the Jeremiah and Alexis scenarios did not differ significantly in terms of overall tutor performance. Post-hoc contrasts were conducted to examine differences between pretest and posttest scores across the two scenario orders based on marginal means. Shining light on the significant interaction, learning gains in the Alexis:Jeremiah scenario order were not statistically significant, $M = -0.01$, $SE = 0.05$, $t(79) = -0.02$, $p = .846$. However, learning gains were significant in the Jeremiah:Alexis scenario order, $M = 0.12$, $SE = 0.04$, $t(79) = 3.07$, $p = .001$. The differences in assessment scores between the two scenario order conditions are visualized in Figure 4."}, {"title": "4.2 RQ2: How does tutors' self-reported confidence of their knowledge attending to students\nexperiencing potential inequities change from pretest to posttest, and do tutors feel they can apply\nwhat they learned?", "content": "We sought to determine if there was a change in tutors' self-reported confidence in their knowledge of the topic from pretest to posttest, as well as their perceived ability to apply what they learned. We compared tutors' Likert scale survey results collected prior to pretest and after posttest. Prior to beginning the lesson, tutors reported an average confidence level of 3.44 (SD = 1.09) on a scale ranging from 1 (not at all confident) to 5 (extremely confident). Following participation in the lesson, this confidence level increased to 4.51 (SD = 0.56). Among 81 tutors, 35 completed the post-lesson survey of confidence level. A paired sample t-test of those 35 tutors revealed a statistically significant difference between pretest and posttest confidence level (t(34) = 6.82, p < .001), indicating a reliable improvement in tutors' self-reported confidence in their knowledge attending to students experiencing potential inequities. Additionally, tutors were asked to rate their confidence in applying what they learned after completing the short lesson. The average self-reported score for this measure was 4.71 (SD = 0.46), indicating a high level of perceived ability to apply the acquired knowledge from the scenario to their own real-life tutoring."}, {"title": "4.3 RQ3: How effective are large language models in assessing tutor's actions in responding to students\nmanaging possible inequity?", "content": "GPT-4-turbo and GPT-40 showcased proficiency in evaluating tutor's actions in responding to students managing inequity, with better performance on the open responses tasking tutors to predict the best approach compared to the open-response questions tasking tutors to explain the rationale behind their provided approach. Accuracy measures the proportion of correct predictions out of all predictions, providing an overall sense of a model's performance. AUC (Area Under the ROC Curve), often used with imbalance datasets, assesses how well the model distinguishes between classes, while the F1 score balances precision and recall, offering a"}, {"title": "4.4 RQ4: How do the large language models GPT-40 and GPT-4-turbo compare in performance, efficiency,\nand cost?", "content": "Both GPT-40 and GPT-4-turbo performed well. For educational purposes at scale, it becomes necessary to determine the practical balance of performance, efficiency, and cost. For it doesn't matter how well a model performs if it is not feasible due to practicality and cost."}, {"title": "5 Discussion", "content": "This study investigated the efficacy of various instructional conditions on tutor learning, the impact of assessment methods on learning outcomes, and the role of generative Al in evaluating tutor performance. Several important insights emerged, which offer a comprehensive understanding of this present work."}, {"title": "5.1 Tutors demonstrate new learning on applying equity-focused skills, however, improved balance of\nscenario difficulty is needed.", "content": "Tutors displayed evidence of new learning when responding to students possibly experiencing inequities from pretest to posttest. However, the practice related to learning gains in both item order conditions included the same activities, hence differences in learning gains by scenario order condition can be attributed to differences in students or the assessments of both scenario order conditions. One hypothesis for why learning gains were significantly higher in one scenario-order condition is that one of the scenario batteries is systematically different from the other. Tutors who had the Jeremy scenario followed by the Alexis scenario demonstrated a 12% increase from the pretest to the posttest, respectively. This is not as high as previous work [34], however, equity training is a more nuanced and ill-defined domain. The Alexis test battery had a higher average student score at pretest than the Jeremiah test batter and was therefore easier (79. 9% compared to 72. 7%), although not significantly so based on a two-sided t-test, $t(12.87) = 0.88$, $p = .393$. Still, we tested the robustness of the main effect of time (i.e., learning gains) using an adjusted test score by applying z-score transformations within test-battery groups at pretest. That analysis consolidated a marginally significant learning gain between both scenario order conditions $F(1,79) = 2.82$, $p = .097$. The interaction between scenario order condition and time remained robust after adjustment, $F(1, 79) = 3.96$, $p = .050$. Similarly, a Rasch model adjusting for individual item-difficulty and a fitted effect of time to test learning gains resulted in a marginally significant time effect representing learning gains, $\\beta = 0.42$, $p = .066$."}, {"title": "5.2 Tutors reported improved confidence from pretest to posttest on applying equity-focused skills and\nare generally confident in applying what they have learned.", "content": "The results indicate a significant improvement in tutors' self-reported confidence from pretest to posttest, highlighting the effectiveness of the lesson in increasing their confidence of attending to students experiencing potential inequities. The statistically significant increase in confidence suggests that the lesson had a meaningful impact on tutors' perceptions of their own competence. Furthermore, the high average confidence in applying the acquired knowledge (4.71) suggests that tutors feel well prepared to transfer what they learned into practice. However, does the increase in self-perceived confidence in applying learned skills translate into actual learning gains from pretest to posttest? To explore the relationship between tutor self-reported confidence in applying the skills taught in the lesson and their individual"}, {"title": "5.3 Overall, Generative Al performed well, but not perfect, on assessing tutor performance.", "content": "The results show that GPT-4-turbo and GPT-40 are proficient at evaluating tutor responses, particularly in predicting the best approach when addressing student inequities. Few-shot prompting consistently outperformed zero-shot prompting across all metrics, demonstrating the value of providing examples to guide model responses [4]. Strong F1 scores in all models (ranging from 0.79 to 0.92) suggest generally good performance.\nHowever, some notable challenges persist, particularly when evaluating responses that require subjective interpretation. Despite iteratively modifying the prompts, the models occasionally scored tutor responses differently from human graders.\nDetermining the \"source of truth\" becomes especially challenging in such cases, as subjective responses often involve nuanced human judgment that LLMs, even with advanced prompting techniques, may struggle to fully capture [9]. Moreover, human error can introduce inconsistencies, leading to disagreements between evaluators or between human and AI assessments. Therefore, developing more objective and transparent methods for determining the \"source of truth\" is advantageous."}, {"title": "5.4 Balancing performance, cost, and speed, GPT-40 with few-shot learning is the optimal choice.", "content": "Currently, GPT-40 with few-shot learning emerges as the preferred model when considering performance, cost, and speed, especially compared to GPT-4-turbo. While GPT-4-turbo offers only marginal performance improvements, it is nearly five times slower (20 tokens/sec vs. 109 tokens/sec for GPT-40) and significantly more expensive [29]. Slow processing time would make it difficult to implement its use in providing automated assessment and immediate feedback to learners."}, {"title": "6 Limitations", "content": "This study has several limitations that merit consideration. First, the sample size of 81 tutor participants, while providing some insights, remains relatively small, potentially influencing the observed differences in learning gains across item counterbalancing conditions. Although we ruled out item difficulty as a primary factor, the small sample size raises the possibility that randomization differences between students could contribute to the gain differences. Finally, exploring more objective metrics, such as multiple-choice questions, as a potential\"source of truth\" could enhance the robustness of assessment methods."}, {"title": "7 Future Work and Conclusion", "content": "Future work should address several key areas. First, increasing the complexity of scenarios, particularly the Alexis scenario, could provide deeper insights into tutor performance. Furthermore, research could investigate how increased self-reported confidence translates into real-world tutoring effectiveness and long-term student outcomes. Another important direction is determining the predictive validity of lessons and tutor learning transfer by analyzing real-life tutor-student interactions and transcription data. Finally, exploring more objective metrics, such as multiple-choice questions, as a potential\"source of truth\" could enhance the robustness of assessment methods."}]}