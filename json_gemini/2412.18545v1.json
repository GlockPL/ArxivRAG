{"title": "Advancing Deformable Medical Image Registration with\nMulti-axis Cross-covariance Attention", "authors": ["Mingyuan Meng", "Michael Fulham", "Lei Bi", "Jinman Kim"], "abstract": "Deformable image registration, aiming to find a dense (pixel-wise) non-linear spatial correspondence between images,\nis a fundamental requirement for medical image analysis. Recently, transformers have been widely used in deep learning-based\nregistration methods for their ability to capture long-range dependency via self-attention (SA). However, the high computation and\nmemory loads of SA (growing quadratically with the spatial resolution) hinder transformers from processing subtle textural\ninformation in high-resolution image features, e.g., at the full and half image resolutions. This limits deformable registration as the\nhigh-resolution textural information is crucial for finding precise pixel-wise correspondence between subtle anatomical structures.\nCross-covariance Attention (XCA), as a \"transposed\" version of SA that operates across feature channels, has complexity growing\nlinearly with the spatial resolution, providing the feasibility of capturing long-range dependency among high-resolution image\nfeatures. However, existing XCA-based transformers merely capture coarse global long-range dependency, which are unsuitable\nfor deformable image registration relying primarily on fine-grained local correspondence. In this study, we propose to improve\nexisting deep learning-based registration methods by embedding a new XCA mechanism. To this end, we design an XCA-based\ntransformer block optimized for deformable medical image registration, named Multi-Axis XCA (MAXCA). Our MAXCA serves\nas a general network block that can be embedded into various registration network architectures. It can capture both global and\nlocal long-range dependency among high-resolution image features by applying regional and dilated XCA in parallel via a multi-\naxis design. Extensive experiments on two well-benchmarked inter-/intra-patient registration tasks with seven public medical\ndatasets demonstrate that our MAXCA block enables state-of-the-art registration performance.", "sections": [{"title": "1. Introduction", "content": "Medical image registration is a fundamental step for medical image analysis and has been an active research focus for decades\nIt spatially aligns medical images acquired from different patients, times,\nor scanners, which serves as a crucial step for various clinical tasks, including tumor growth monitoring and group analysis (Haskins,\nKruger, and Yan 2020). Deformable image registration aims to find a dense (pixel-wise) non-linear spatial transformation between\na pair of images, through which the two images can be spatially aligned after warping. Traditional methods typically formulate\ndeformable image registration as a time-consuming iterative optimization problem (Avants et al. 2008; Modat et al. 2010). Recently,\ndeep registration methods based on Convolutional Neural Networks (CNNs) and/or transformers have been widely recognized for\nfast end-to-end registration (Haskins, Kruger, and Yan 2020; Zou et al. 2022). These methods learn a mapping from image pairs to\nspatial transformations based on a set of training data, which have shown superior registration performance than traditional\nregistration methods (Zou et al. 2022).\nVisual transformer (Dosovitskiy et al. 2021) and its window-based variant, Swin transformer (Liu et al. 2021), have been widely\nused in vision tasks for the capability to capture long-range dependency via self-attention (SA). This capability enables transformers\nto surpass CNNs in deformable image registration as it enables larger receptive fields to capture large deformations between images\n(Chen et al. 2022; Chen, Zheng, and Gee 2023; Ma et al. 2023; Wang, Ni, and Wang 2023; Zhu and Lu 2022). However, the high"}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. Deformable Medical Image Registration", "content": "In the era of deep learning, deep registration methods commonly use convolutional layers or transformers as the basic unit to\nbuild registration networks. VoxelMorph (Balakrishnan et al. 2019), as one of the most commonly benchmarked deep registration\nmethods, used a hierarchical encoder-decoder CNN similar to Unet (\u00c7i\u00e7ek et al. 2016), motivating the wide use of CNNs and\nencoder-decoder architectures in subsequent studies (Dalca et al. 2019; Jia et al. 2022; Meng et al. 2022b; Mok and Chung 2020a).\nIn recent years, transformers have been widely used for deformable medical image registration. For example, Swin-VoxelMorph\n(Zhu and Lu 2022) used a pure transformer-based registration network similar to Swin-Unet (Cao et al. 2022). TransMorph (Chen et\nal. 2022) used a hybrid CNN-transformer registration network, where Swin transformers were employed after 4\u00d74\u00d74 patch\nembedding and convolutional layers were employed at the full and half image resolutions. These methods used transformers at\ndownsampled resolutions to reduce the computation/memory loads. An exception is ModeT (Wang, Ni, and Wang 2023), where\nmotion decomposition transformers were applied at the full image resolution by computing SA operations within the local\nneighborhood (kernel size=3) around each pixel. However, this approach inevitably compromised the transformer's capability to\ncapture long-range dependency.\nRecently, MLPs were introduced for deformable registration as superior alternatives to transformers. MLPMorph (Meng et al.\n2023b) used the same encoder-decoder architecture as VoxelMorph/TransMorph but employed MLPs in the encoder to capture fine-\ngrained long-range dependency beginning from the full resolution. This enabled MLP-Morph to gain better performance than its\nCNN-/transformer-based counterparts (VoxelMorph/TransMorph).\nNetwork architecture is also a crucial factor influencing registration performance. Many deep registration methods used the basic\nUnet-like direct registration architecture, e.g., VoxelMorph. Moreover, progressive registration architecture has also been widely\nused in recent coarse-to-fine deep registration methods to handle large deformations. For example, LapIRN (Mok and Chung 2020b)\ncascaded multiple laplacian pyramid networks to perform multiple registration steps. NICE-Net (Meng et al. 2022a) used a pyramid\nnetwork to perform coarse-to-fine registration in a single network iteration, and it has been extended to a transformer-based variant,\nNICE-Trans (Meng et al. 2023a). There also exist more complicated architecture designs. For instance, CorrMLP (Meng et al. 2024)\nused MLP blocks in a correlation-aware coarse-to-fine architecture, which incorporated correlation information into progressive\nregistration and attained state-of-the-art performance."}, {"title": "2.2. Cross-covariance Attention (XCA)", "content": "The concept of XCA was proposed in the cross-covariance image transformer (XCiT) (Ali et al. 2021), which achieved\ncompetitive performance with conventional SA-based transformers on natural image classification, detection, and segmentation.\nSubsequently, XCA was optimized for natural image restoration tasks (e.g., image denoising and deblurring) in an efficient\ntransformer model, named Restormer (Zamir et al. 2022). The Restormer used XCA-based transformer blocks to capture global long-\nrange dependency among high-resolution image features, which enabled it to achieve superior performance over SA-based\ntransformer models. Recently, XCA was also used for medical image classification, where a residual XCA-based transformer was\nproposed to extract spatial and global features from ultrasound images (Sarker et al. 2023). These XCA-based transformer models\ncalculated XCA on the entire image space, which merely captured coarse global long-range dependency and, unfortunately, are\nunsuitable for deformable medical image registration."}, {"title": "3. Method", "content": "Deformable image registration aims to find a spatial transformation $\\phi$ that warps a moving image $I_m$ to a fixed image $I_f$, so that\nthe warped image $I_m \\circ \\phi = I_m \\circ \\phi$ is spatially aligned with the $I_f$. In this study, we assume the $I_m$ and $I_f$ are two single-channel,"}, {"title": "3.1. Multi-Axis XCA (MAXCA) block", "content": "Our MAXCA block is inspired by the multi-axis blocked self-attention (Zhao et al. 2021) and multi-axis gated MLP (Tu et al.\n2022). These methods performed SA/MLP on two axes to realize two forms of sparse operations, namely regional and dilated\nSA/MLP, to capture local and global information. However, these SA/MLP-based methods were not designed for registration tasks,\nand they are inherently limited when applied to deformable medical image registration (as discussed in Section I). Further, the multi-\naxis design has also not been investigated in the context of XCA. Here, we optimize XCA with the concept of \"multi-axis\" for\ndeformable image registration by developing a Multi-Axis XCA (MAXCA) block.\nThe MAXCA block is illustrated in Figure 1. We assume that the input feature map $F_{in}$ has the size of $H \\times W \\times D \\times C$. The F\nis first projected to increase its channel number to 2C and then diverges into two feature heads, $F_l$ and $F_g$, in two parallel local and\nglobal branches. In the two branches, the feature maps are split into local regions according to a given region size R, resulting in\nregional feature maps in size of $(HWD/R^3) \\times R^3 \\times C$ with non-overlapping regions each with size of $R^3$. In the local branch, XCA\nis calculated on the 2nd axis (i.e., regional XCA within the regions); in the global branch, XCA is calculated on the 1st axis (i.e.,\ndilated XCA across the regions). Intuitively, applying XCA in the two parallel branches corresponds to local and global attention in\nthe feature map. Moreover, we propose to use convolutional QKV projection, in place of the common linear projection, to enhance the\nlocal context before computing feature covariance. To this end, we use 3\u00d73\u00d73 convolutional layers to project the $F_l$ and $F_g$ into $Q$,\n$K$, and $V$ before the region splitting. After the XCA calculation, the processed features are merged to restore the original shape.\nThe processed features derived from the local and global branches are concatenated and then are projected to reduce the channel\nnumber to C, with a residual connection from $F_{in}$. In addition, we followed the previous study (Meng et al. 2024) to apply a residual\nchannel attention module to highlight crucial feature channels, consisting of layer normalization, convolutional layers, LeakyReLU\nactivation, and squeeze-and-excitation (SE) channel-wise attention (Hu, Shen, and Sun 2018), with a residual connection."}, {"title": "3.2. Network Architecture", "content": "Our MAXCA block is exemplified in three network architectures: direct, progressive, and correlation-aware progressive\nregistration architectures, as illustrated in Figure 2. Detailed architecture settings, including feature dimensions, head numbers, and\nregion size, are provided in Appendix A.\nWe adopted the direct registration architecture widely used in existing deep registration methods including VoxelMorph\n(Balakrishnan et al. 2019), TransMorph (Chen et al. 2022), and MLPMorph (Meng et al. 2023b). It employs an Unet-style encoder-\ndecoder network to realize a direct mapping from the input images $I_f/I_m$to the displacement field $\\phi$. As shown in Figure 2(a), we\nemployed MACXA blocks at the encoder and denote this network as XCAMorph, following VoxelMorph, TransMorph, and\nMLPMorph. A 3\u00d73\u00d73 convolutional layer is used before the first MAXCA block to convert the input images into initial feature\nmaps. Patch merging modules are used at the encoder to downsample the feature maps between two MAXCA blocks. The decoder\nis composed of successive Conv blocks and upsampling operations. Each Conv block contains two 3\u00d73\u00d73 convolutional layers\nfollowed by LeakyReLU activation with a parameter of 0.2 and instance normalization.\nWe adopted the progressive registration architecture used in recent coarse-to-fine deep registration methods, NICE-Net (Meng\net al. 2022a) and NICE-Trans (Meng et al. 2023a). It consists of a CNN-based encoder that extracts two feature pyramids from $I_f/I_m$\nand a progressive registration decoder that performs multiple steps of coarse-to-fine registration (refer to NICE-Trans for detailed\ndescriptions). As shown in Figure 2(b), we employed MAXCA blocks at the progressive registration decoder and denote this network\nas NICE-XCA, following NICE-Net and NICE-Trans. For comparison, we also build an MLP-based progressive registration network\n(denoted by NICE-MLP) with Swin-MLP blocks (Liu et al. 2021) employed at the progressive registration decoder.\nWe also adopted the correlation-aware progressive architecture that was recently proposed in CorrMLP (Meng et al. 2024). It\nconsists of a CNN-based encoder and a correlation-aware progressive registration decoder, which incorporates image-level and step-"}, {"title": "3.3. Unsupervised Training", "content": "To remove the reliance on ground truth labels, recent deep registration methods tend to use image similarity metrics as the\ntraining loss to perform fully unsupervised training. We followed the common unsupervised training scheme for a fair comparison:\nThe learnable parameters @ are optimized using an unsupervised loss $\\mathcal{L}$ that does not require labels. The $\\mathcal{L}$ is defined as $\\mathcal{L} = \\mathcal{L}_{sim} +\\alpha \\mathcal{L}_{reg}$, where the $\\mathcal{L}_{sim}$ is an image similarity term that penalizes the differences between the warped image $I_m \\circ \\phi$ and the fixed image\n$I_f$, the $\\mathcal{L}_{reg}$ is a regularization term that encourages smooth and invertible transformations $\\phi$, and the $\\alpha$ is a regularization parameter\nthat is set as 1 by default.\nWe adopted negative local normalized cross-correlation (NCC) as the $\\mathcal{L}_{sim}$, which is a commonly used similarity metric in\ndeformable registration methods (Balakrishnan et al. 2019; Chen et al. 2022; Jia et al. 2022; Meng et al. 2024). For the $\\mathcal{L}_{reg}$, we\nimposed a diffusion regularizer on the $\\phi$ to encourage its smoothness. We adopted these common loss functions for a fair comparison\nwith existing deep registration methods, while other loss functions can also be easily embedded in our method to enable, e.g.,\ndiffeomorphic registration or multi-modal image registration."}, {"title": "4. Experimental Setup", "content": ""}, {"title": "4.1. Datasets and Preprocessing", "content": "Our method was evaluated on inter-patient brain image registration and intra-patient cardiac image registration, involving seven\npublic medical image datasets:\nFor brain image registration, we adopted six public 3D brain MRI datasets that have been widely used to evaluate medical image\nregistration (Meng et al. 2024). A total of 2,656 brain MRI images acquired from four public datasets, ADNI (Mueller et al. 2005),\nABIDE (Di Martino et al. 2014), ADHD (ADHD-200 consortium 2012), and IXI (IXI dataset 2022), were used for training; two\npublic brain MRI datasets with anatomical segmentation, Mindboggle (Klein and Tourville 2012) and Buckner (Fischl 2012), were\nadopted for validation and testing. The Mindboggle dataset contains 100 MRI images and was randomly split for validation/testing\nwith a ratio of 50%/50%. The Buckner dataset contains 40 MRI images and was used for independent testing. We performed inter-\npatient registration for evaluation, where 100 image pairs were randomly picked from each of the Mindboggle and Buckner testing\nsets, resulting in 200 testing image pairs in total. We performed standard brain MRI preprocessing procedures, including brain\nextraction, intensity normalization, and affine registration by FreeSurfer (Fischl 2012) and FLIRT (Jenkinson and Smith 2001). All\nimages were affine-transformed and resampled to align with the MNI-152 brain template (Fonov et al. 2011) with 1mm isotropic\nvoxels, and then were cropped into 144\u00d7192\u00d7160.\nFor cardiac image registration, we adopted the public ACDC dataset (Bernard et al. 2018) that contains 4D cardiac cine-MRI\nimages of 150 patients. Each 4D cine-MRI image contains tens of 3D frames acquired from different time points, including End-\nDiastole (ED) and End-Systole (ES) frames with segmentation labels of the left ventricular cavity, right ventricular cavity, and\nmyocardium. The ACDC dataset provides 100 cine-MRI images in the training set and 50 cine-MRI images in the testing set. We\nrandomly divided the training set into 90 and 10 cine-MRI images for training and validation and used the provided testing set for\ntesting. The intra-patient ED and ES frames were registered with each other (ED-to-ES and ES-to-ED), resulting in 100 testing image\npairs derived from the testing set. All cine-MRI frames were resampled with a voxel spacing of 1.5\u00d71.5\u00d73.15 mm and cropped to\n128\u00d7128\u00d732 around the center. The voxel intensity was normalized to the range from 0 to 1 through max-min normalization."}, {"title": "4.2. Implementation Details", "content": "Our method was implemented with PyTorch on an NVIDIA GeForce RTX 4090 GPU with 24 GB memory. We used an ADAM\noptimizer with a learning rate of 0.0001 and a batch size of 1. For brain image registration, our networks were trained for 100,000\niterations with inter-patient image pairs randomly picked from the training set. For cardiac image registration, our networks were\nfirst trained for 40,000 iterations with intra-patient image pairs that consist of two frames randomly picked from the same cine-MRI\nimage. Then, the networks were trained for another 10,000 iterations with intra-patient image pairs consisting of only ED and ES\nframes, which optimizes the networks to register ED and ES frames. We performed validation after every 1,000 training iterations\nand used the model weights achieving the highest validation result for final testing. Our implementation code is publicly available at\nhttps://github.com/MungoMeng/Registration-\u041c\u0410\u0425\u0421\u0410."}, {"title": "4.3. Experimental Designs", "content": "Our method was compared with fourteen existing deformable medical image registration methods, including traditional\noptimization-based registration methods and state-of-the-art deep registration methods. The included traditional methods are SyN\n(Avants et al. 2008) and NiftyReg (Modat et al. 2010), and we ran them using cross-correlation as the similarity measure. The\nincluded deep registration methods are VoxelMorph (Balakrishnan et al. 2019), TransMorph (Chen et al. 2022), Swin-VoxelMorph\n(Zhu and Lu 2022), TransMatch (Chen, Zheng, and Gee 2023), MLPMorph (Meng et al. 2023b), LapIRN (Mok and Chung 2020b),\nSDHNet (Zhou et al. 2023), ModeT (Wang, Ni, and Wang 2023), NICE-Net (Meng et al. 2022a), NICE-Trans (Meng et al.\n2023a), Dual-PRNet++ (Kang et al. 2022), and CorrMLP (Meng et al. 2024). All deep registration methods were trained using the\nsame loss functions as ours for a fair comparison.\nWe also conducted two ablation studies to further validate the effectiveness of our method. In the first ablation study, we replaced\nour MAXCA block with existing transformer or MLP blocks, including Swin transformer (Liu et al. 2021), Restormer (Zamir et al.\n2022), Swin-MLP (Liu et al. 2021), Multi-axis gated MLP (Tu et al. 2022), Hire-MLP (Guo et al. 2022), and sMLP (Tang et al.\n2022). In the second ablation study, we individually removed the global and local branches in our MAXCA block, and we also\nexplored the contribution of the convolutional projection by replacing it with the common linear projection. The two ablation studies\nwere performed with the direct registration architecture, and they also included a comparison baseline model that used Conv blocks\nat both the encoder and decoder."}, {"title": "5. Results and Discussion", "content": ""}, {"title": "5.1. Comparison with Existing Methods", "content": "Table 1 presents the quantitative comparison with existing registration methods. The SA-based TransMorph and NICE-Trans\nachieved higher DSCs than the CNN-based VoxelMorph and NICE-Net, which validates the benefits of capturing long-range\ndependency for registration. Applying MLPs in MLPMorph and NICE-MLP further improved DSCs over SA-based registration\nmethods. This is consistent with the previous findings (Meng et al. 2023b) showing that MLPs capture fine-grained long-range\ndependency at full resolution, enabling them to outperform existing SA-based registration methods. Nevertheless, our MAXCA block\nenabled higher DSCs than MLPs, producing consistent improvements in XCAMorph, NICE-XCA, and CorrXCA. By applying our\nMAXCA block in the state-of-the-art correlation-aware progressive registration architecture, our CorrXCA attained the highest DSCs\namong all the compared methods. We attribute the improvements to our optimized use of XCA, allowing for capturing both global\nand local long-range dependency among high-resolution features via attention mechanisms. Moreover, all deep registration methods"}, {"title": "5.2. Ablation Analysis on Network Blocks", "content": "Table 2 presents the DSC results of our ablation study on network blocks. The NJD results are omitted as all methods adopted\nthe same regularization settings and achieved similar NJDs. All transformer and MLP-based blocks outperformed the CNN baseline,\ndemonstrating the benefits of capturing long-range dependency for deformable image registration. Moreover, as Swin transformer\nblocks cannot be used at full resolution due to heavy loads of GPU memory (as reported in Appendix C), a Conv block was employed\nto process the full-resolution features, resulting in worse performance than other MLP/XCA-based blocks that can be used beginning\nfrom the full image resolution. This indicates the effectiveness of MLP/XCA in capturing long-range dependency at full resolution.\nThe XCA-based Restormer failed to outperform MLP-based blocks, which is attributed to the fact that the Restormer was not\ndesigned for deformable medical image registration and lacks the capability to capture fine-grained local correspondence. By\nconsidering both global and local long-range dependency via our multi-axis design, our MAXCA block outperformed MLP-based\nblocks and achieved the highest results among all the compared network blocks."}, {"title": "5.3. Ablation Analysis within MAXCA", "content": "Table 3 presents the DSC results of our ablation study within the MAXCA block. The NJD results are omitted as all methods\nadopted the same regularization settings and achieved similar NJDs. Removing either global or local branch from our MAXCA block\nresulted in degraded performance, which shows the individual contribution of each branch and also suggests that both global and\nlocal long-range contexts are beneficial for medical image registration. Nevertheless, we found that local contexts are more crucial,\nas removing the local branch led to larger performance degradation. Further, compared to the commonly used linear projection,\nemploying convolutional layers for the QKV projection resulted in better registration performance as it can enhance the local context\nbefore computing feature covariance."}, {"title": "6. Conclusion", "content": "In this study, we have introduced a novel Multi-axis XCA (MAXCA) block to demonstrate the optimized use of XCA for\ndeformable medical image registration. Our MAXCA block shows advantages in capturing both global and local long-range\ndependency among high-resolution image features via XCA, for finding dense pixel-wise correspondence between images. This\nblock has been validated on three registration architectures and produced consistent improvements over state-of-the-art deformable\nimage registration methods on the tasks of brain and cardiac image registration. We suggest that our MAXCA block can serve as a\ngeneral network block applying to various network architectures for image registration tasks. Further validation could be performed\nwith other registration network architectures, e.g., automatic fusion network (Meng et al. 2023c), or other registration tasks, e.g.,\nbrain tumor registration (Baheti et al. 2021; Meng et al. 2022c), in future studies."}, {"title": "Appendix A: Architecture Settings", "content": "We report the architectural hyper-parameters used in the three exemplified registration network architectures in Table A1 (direct\nregistration), Table A2 (progressive registration), and Table A3 (correlation-aware progressive registration). The architectural\nhyper-parameters include feature dimensions (channel numbers), head numbers of XCA, and region size R, at each pyramid scale.\nThe feature dimensions were chosen under the constraints of GPU memory (24 GB, NVIDIA GeForce RTX 4090 GPU), which\ncould be increased to pursue better registration performance if larger GPU memory is available.\nThe XCA head numbers were chosen so that each attention head processed the features with 12 channels (e.g., the features with\n24 channels were processed by two XCA heads). We also attempted to increase the XCA head numbers so that each attention head\nprocessed 8-channel features, but identified slightly degraded results.\nThe region size R was chosen based on empirical experiments. We tried to set R in a range from 4 to 12 and chose the R that\nresulted in the highest validation results."}, {"title": "Appendix B: Registration Runtime", "content": "Table A4 presents the runtimes of the compared registration methods, where the inference time required to register a pair of\nimages using GPU or CPU is reported. The GPU runtimes of traditional methods (SyN and NiftyReg) are unavailable for the lack\nof official GPU implementation.\nAs shown in Table A4, all deep registration methods are much faster than the traditional methods, allowing real-time registration\nwith GPU (<0.5s for one image pair). Among deep registration methods, the methods based on transformers/MLP tend to be slower\nthan the methods based on CNNs due to the additional computation required to model long-range dependency. The runtimes of our\nXCA-based methods (XCAMorph, NICE-XCA, and CorrMLP) are similar to their SA/MLP-based counterparts, suggesting that\nour method does not incur significant extra computational loads or registration runtime compared to the existing SA/MLP-based\ndeep registration methods."}, {"title": "Appendix C: Memory Consumption", "content": "Table A5 shows the GPU memory consumption of the compared network blocks, where the GPU memory consumed during\ntraining for brain image registration is reported. The Swin transformer blocks were applied beginning from the half image resolution\nto reduce to GPU memory consumption, while the full-resolution features were processed by a Conv block. Even so, the hybrid\nuse of Swin transformer and Conv blocks still used up the GPU memory (23.6GB out of 24GB), thus disabling the use of Swin\ntransformer blocks to capture fine-grained long-range dependency among full-resolution anatomical details. In contrast, the\nMLP/XCA-based network blocks can be applied beginning from the full image resolution under the same constraints of GPU\nmemory. Furthermore, compared to the existing MLP/XCA-based network blocks, our MAXCA block does not incur additional\nGPU memory consumption."}]}