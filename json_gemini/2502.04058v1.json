{"title": "STRATEGIC LEARNING WITH LOCAL EXPLANATIONS AS FEEDBACK", "authors": ["Kiet Q. H. Vo", "Siu Lun Chau", "Masahiro Kato", "Yixin Wang", "Krikamol Muandet"], "abstract": "We investigate algorithmic decision problems where agents can respond strategically to the decision maker's (DM) models. The demand for clear and actionable explanations from DMs to (potentially strategic) agents continues to rise. While prior work often treats explanations as full model disclosures, explanations in practice might convey only partial information, which can lead to misinterpretations and harmful responses. When full disclosure of the predictive model is neither feasible nor desirable, a key open question is how DMs can use explanations to maximise their utility without compromising agent welfare. In this work, we explore well-known local and global explanation methods, and establish a necessary condition to prevent explanations from misleading agents into self-harming actions. Moreover, with conditional homogeneity, we establish that action recommendation (AR)-based explanations are sufficient for non-harmful responses, akin to the revelation principle in information design. To operationalise AR-based explanations, we propose a simple algorithm to jointly optimise the predictive model and AR policy to balance DM outcomes with agent welfare. Our empirical results demonstrate the benefits of this approach as a more refined strategy for safe and effective partial model disclosure in algorithmic decision-making.", "sections": [{"title": "Introduction", "content": "Modern regulatory frameworks emphasise transparency in algorithmic decision making, mandating that decision makers (DMs) provide clear and understandable justifications for automated decisions [Selbst and Powles, 2017, Wachter et al., 2017a]. For example, the General Data Protection Regulation (GDPR) includes provisions commonly referred to as the right to explanation, which require DMs to inform agents (i.e., individuals affected by automated decisions) about the basis of these decisions in a comprehensible manner [Goodman and Flaxman, 2017]. These provisions aim to help agents understand and potentially contest the rationale behind algorithmic decisions. However, transparency can incentivise agents to manipulate their inputs to secure more favorable outcomes, triggering strategic adaptations by both agents and DMs [Hardt et al., 2016]. This dynamic has inspired extensive research into modeling strategic behavior and optimising decision making in such interactions [Miller et al., 2020]. Within this strategic learning domain, explainability is frequently interpreted as requiring full disclosure of the decision making model, including its structure and parameters [Shavit et al., 2020, Harris et al., 2022a, Vo et al., 2024]. This perspective assumes that full disclosure of the predictive model inherently satisfies the need for transparency, enabling agents to simulate and assess different scenarios using the disclosed information."}, {"title": "Preliminaries", "content": "To set the scene, we adopt the car insurance pricing scenario considered in Shavit et al. [2020] as our running example. In this scenario, a car insurance company (the DM) receives each customer (agent)'s profile and predicts their future accident cost to determine their insurance premium. Legal transparency requirements mandate the company to provide the customers with explanations for their decisions. In response, customers may strategically adjust their profiles\u2014such as purchasing a higher-tier car model or enrolling in a defensive driving course\u2014to influence both their actual accident risk and the premiums assigned by the insurer.\nWe start with the standard setup in which the DM fully discloses their predictive model. We then extend this framework to a more practical setting where agents are provided with only local explanations\u2014a scenario that better reflects current industry practices.\nNotations. We denote random variables and their realisations with uppercase and lowercase letters, respectively. {1,...,T} are denoted as [T]. We use X and Z to denote the respective spaces of agents' covariates that are observed and unobserved to the DM. Y denotes the outcome space of agents and G denotes the hypothesis space of the DM. More details about their interactions are given in the setup below.\nStrategic Learning. We consider the scenario in which a DM interacts with a set of T agents, indexed by t\u2208 [T]. Agents interact with the DM separately and independently. Each agent t interacts with the DM exactly once, referred to as round t. The agent t is characterised by the covariates $(x_t^{(b)}, Z_t) \\in Xx Z$ drawn independently from the same distribution $P_{X^{(b)},Z}$. Furthermore, the agent has an unrealised outcome $y_t^{(b)}:= h(x_t^{(b)}, z_t)$, where h: X \u00d7 Z \u2192 Y is a deterministic potential outcome function and $y_t^{(b)} \\in Y \\subseteq R$. In our running example, this setup corresponds to a customer with an observable feature vector $x_t^{(b)}$, an unobservable feature vector $z_t$, and a potential monetary cost $y_t^{(b)}$ that they would incur from future car accidents.\nAt each round t, the DM begins by publishing their predictive model g : X \u2192 Y that will be used to predict the outcome of this agent t. With this information, agent t strategically modifies their base covariate value from $x_t^{(b)}$ to $x_t$, and reports it to the DM. For instance, this corresponds to a customer's strategic adaptation of the profile to obtain an insurance contract with a lower price, e.g., $g(x_t) \\leq g(x_t^{(b)})$. Following prior work [Harris et al., 2022a, Vo et al., 2024], we assume the agent responds with $x_t := arg max_x u_t(g, x)$ where their utility function $u_t$ is defined as\n$u_t(g,x) := - (g(x) + c_t(x_t^{(b)}, x)),$ (1)\nand their goal is to minimise the predictive value while keeping the cost of their action, $c_t(x_t^{(b)}, x)$, as low as possible. Similarly, we assume the DM does not have access to agents' cost functions $c_t$ and allow these cost functions to be heterogeneous. We define the cost function as follows:\nDefinition 2.1 (Cost function). A function $c_t : X \u00d7 X \u2192 [0,\u221e)$ is a cost function for agent t if it satisfies $c_t(x_t^{(b)}, x_t^{(b)}) = 0$ and $c_t(x_t^{(b)}, x) > 0, x \\ne x_t^{(b)}$.\nAfter agent t reports $x_t$, they receive the final prediction $\\hat{y}_t := g(x_t)$ from the DM and realises the outcome $y_t := h(x_t, z_t)$. The round concludes.\nSince all agents have the same inital distribution $P_{X^{(b)},Z}$ and interact with the DM through the same mechanism, we use $P_{X,Z}$ to denote the joint distribution that governs the pairs $(x_t, z_t)$ of all agents and similarly, $P_{X,Y}$ for the case of $(x_t, y_t)$. Then, the DM's objective is to minimise their expected predictive error:\n$\\min_g E_{P_{X,Y}} [(g(X) - Y_t)^2] = \\min_g E_{P_{X,Z}} [(g(x_t) - h(x_t, Z_t))^2],$ \nwhere $P_{X,Z}$ and $P_{X,Y}$ depends on the DM's choice for g due to agents' strategic behaviour. In the context of insurance pricing, this objective reflects the company's effort to align insurance premiums with customers' expected accident costs. Premiums that are too low may lead to financial losses, while setting them too high could drive potential customers to competitors, resulting in lost business opportunities."}, {"title": "Strategic Learning with Explanations", "content": "We formalise the strategic learning scenario in which DM provides model explanations to agents instead of directly disclosing the predictive model. In this setting, to enable the DM to offer explanations tailored to individual agents\u2014commonly referred to as local explanations in the explainable machine learning community-we modify the interaction process so that each agent takes two sequential actions within a single round. Specifically, in each round t, the agent first submits their base covariate $x_t^{(b)}$, upon which the DM provides a preliminary decision $\\hat{y}_t^{(b)}$ computed with $g(x_t^{(b)})$. Instead of disclosing the full predictive model g to the agent, the DM now provides an explanation detailing how the preliminary decision $\\hat{y}_t^{(b)}$ was computed.\nDefinition 3.1 (Explanations). An explanation method is a tuple $(E,\u03c3)$ where $E$ is the space of feasible explanations and $\u03c3 : X \u00d7 G \u2192 E$ is an explanation policy that picks an explanation $e \u2208 E$ for the agent with base covariate $x \u2208 X$ w.r.t. the predictive model $g \u2208 G$. It is said to be a global explanation if $\u03c3$ is a constant function w.r.t. the value x. Otherwise, $\u03c3$ is said to generate local explanations.\nThe explanation space $E$ is general and depends on the choice of the explanation method. For example, the DM could employ a global surrogate model, such as a linear model or decision tree, to approximate the predictive model g [Molnar, 2020]. In this case, all agents receive the same explanation $e_t = f_*$ where $f_* : X \u2192 Y$ is a surrogate function belonging to the explanation space $E \\subset F = {f|f : X \u2192 Y}$. Alternatively, when the covariate $x_t^{(b)} \\in X \\subseteq R^d$ consists of d features, attribution-based methods such as SHAP [Lundberg, 2017] provide explanations $e_t = (e_{t1},..., e_{td}) \\in E \\subseteq R^d$ in the form of importance scores for each features in $x_t^{(b)}$. Several other explanation methods fit within this framework, and we provide a more comprehensive discussion of them in Appendix A.1.\nUpon receiving their tailored explanation $e_t$, agent t strategically modifies their covariate from $x_t^{(b)}$ to $x_t$. To formalise this procedure, since agents may respond differently to different types of explanations, we assume here a general form of agent's reaction model with $x_t := \u03c8(x_t^{(b)}, e_t, z_t)$ where $\u03c8$ is a deterministic and measurable function. Under this setting, the agents cannot query their true utility (Equation (1)) nor respond optimally without knowledge of g, unlike in the previous setting. Instead, their best response has to be based on the provided explanation $e_t$ and their reaction model $\u03c8$.\nAfter reporting $x_t$, the agent receives the final prediction $\\hat{y}_t$ and realises the outcome $y_t$, concluding the round. We consider the scenario in which the DM's objective is to choose a predictive model g and explanation method, characterised by $(E,\u03c3)$, that jointly minimise the predictive error, under the agents' strategic behaviour, as follows:\n$\\min_{g,(E,\u03c3)} E_{P_{X,Z}} [(g(X) - h(X_t,Z_t))^2],$ (2)\nwhere the DM's choice of both models affect the distribution of agents' strategic responses $X_t$."}, {"title": "Optimality of Local Explanations", "content": "We demonstrate that local explanations, by enabling the DM to incentivise individual agents, can achieve more favorable outcomes compared to global explanations, which provide the same, non-personalised information to all agents.\nProposition 3.2. Suppose that the DM's objective follows Equation (2) and denote $l(x_t^{(b)}, e_t, z_t) = (g(x_t) - y_t)^2$. For a given predictive model g and an explanation space E,\n$E_{X^{(b)}} [\\min_{X_t,z_t} l(x_t^{(b)}, e_t, z_t) | x_t^{(b)}] \\leq \\min E_{X^{(b)}, Z_t,e, Z} [l(x_t^{(b)}, e, z_t) | x_t^{(b)}].$\nThis proposition implies that, for a given choice of g and E, the minimal predictive loss had we release local explanations is at least as small as had we released global explanations. This is because local explanations"}, {"title": "Agents' Reactions under Local Explanations", "content": "Since the agent does not have access to the actual predictive model g, their best response $x_t$, which is influenced by the explanation $e_t$, might be suboptimal and can even lead to a reduction in their utility (in Equation (1)). We formally define such a property of an agent's responses below.\nDefinition 4.1 (Non-harmful responses). Let $v_t = {x \u2208 X : u_t(g, x) \u2265 u_t(g, x_t^{(b)})}$. If an agent's response $x_t$ belongs to this set, we call it a non-harmful response.\nTo better understand the extent to which explanation methods might induce harmful responses from agents and to develop preventive measures, we examine several common types of explanation methods in this section. This analysis uncovers patterns and provides a foundation for identifying and characterising classes of explanation methods that ensure only non-harmful responses from strategic agents."}, {"title": "Feature Attributions", "content": "Feature-attribution methods, such as LIME [Ribeiro et al., 2016] and SHAP [Lundberg, 2017], assign importance scores to input features in an attempt to explain each feature's contribution to a specific prediction. While SHAP has been applied in sequential decision-making settings [Rodemann et al., 2024, Adachi et al., 2024], these applications typically do not account for agents' strategic responses. However, as highlighted by Wachter et al. [2017b] and Molnar [2020], feature-attribution methods do not offer direct guidance on how features should be adjusted to achieve a desired prediction outcome. To illustrate this limitation further, we present a toy example in Appendix A.2."}, {"title": "Local Surrogate Models", "content": "When the agent only has access to the surrogate function $f_t$ as an approximation to the actual predictive model g, it is commonly assumed that the agent will optimise their action based on the corresponding surrogate utility function [Jagadeesan et al., 2021, Bechavod et al., 2022, Xie and Zhang, 2024]:\n$u_t(f_t,x) = -(f_t(x) + c_t(x_t^{(b)}, x)).$\nHence, the agent's best response becomes\n$x_t := argmin_x (f_t(x) + c_t(x_t^{(b)}, x)).$\nSince the surrogate utility function differs from the true utility function defined in Equation (1), the agent's best response $x_t$ might lead to a reduction in their true utility. To mitigate such risks, we establish the following necessary condition to safeguard against such situations.\nTheorem 4.2 (Necessary condition). Given an agent t with the base covariate $x_t^{(b)}$ who best responds against the surrogate utility function $u_t(f_t,\u00b7)$. If it holds, for every possible cost function $c_t$ (Definition 2.1), that the resulting best response $x_t$ belongs to the non-harmful set $v_t$ (Definition 4.1), i.e., $u_t(g, x_t) \u2265 u_t(g, x_t^{(b)})$, then the following also holds:\n$f_t(x_t^{(b)}) - f_t(x) \\leq g(x_t^{(b)}) - g(x) \\quad \\forall x \\in X^-,$ (3)\nwhere $X^- := {x : g(x) < g(x_t^{(b)})}$ is the set of potential responses with lower prediction scores for the agent.\nThis theorem implies that if the DM provides a surrogate function $f_t$ as an explanation to an agent t, but the condition in Equation (3) is violated, then there exists a cost function $c_t$ under which the agent is misled into"}, {"title": "Action Recommendation-based Explanations", "content": "Next, we consider the scenario in which the explanation provided to agent t takes the form $e_t = (x_t^{(r)}, \\hat{y}_t^{(r)})$, where $x_t^{(r)}$ is a recommended covariate update suggested by the DM, and $\\hat{y}_t^{(r)}$ is the corresponding predicted outcome if the agent follows this recommendation. We refer to this type of explanations as action recommendation (AR)-based explanations. There are various ways to design the explanation policy $\u03c3 : X \u00d7 G \u2192 X \u00d7 Y$ in practice, one of which is to have $\u03c3$ recommend a minimal change in an agent's features that allows this agent to obtain the desired prediction value. The explanations generated by such a $\u03c3$ is commonly referred to as counterfactual explanations in the explainable ML literature [Molnar, 2020]. From here onwards, we use the term AR-based explanation to refer to any explanation of the general form $e_t = (x_t^{(r)}, \\hat{y}_t^{(r)})$, and when it is clear from the context that $e_t$ is generated by a specific $\u03c3$ used in the counterfactual explanations literature (e.g., Wachter et al. 2017b), then we call it a counterfactual explanation. This allows us to provide an analysis for this class of explanations without restricting ourselves to any specific assumption on the behaviour of $\u03c3$. It also emphasises the fact that the explanation $e_t = (x_t^{(r)}, \\hat{y}_t^{(r)})$ provided to the strategic agent t is a guidance, rather than a counterfactual scenario.\nFollowing Tsirtsis and Gomez Rodriguez [2020], we assume the agent's best response $x_t = \u03c8(x_t^{(b)}, e_t, z_t)$ is as follows:\n$x_t :=\\begin{cases}\nx_t^{(r)} \\quad \\text{if } u_t(g,x_t^{(r)}) \u2265 u_t(g, x_t^{(b)}) \\\\\nx_t^{(b)} \\quad \\text{otherwise,}\n\\end{cases}$ (4)\nwhere the agent will update their covariate into the recommended value $x_t^{(r)}$ if the corresponding utility value is at least as good as that of the base value $x_t^{(b)}$. The reason an agent might break ties in favour of the recommended action is because in many application domains, obtaining a more favourable prediction $\\hat{y}$ is likely to result in better long-term well-being for them, such as an improvement in one's financial status or in our insurance-pricing example, lowering one's risk of getting into car accidents.\nSince the agent has access to both $x_t^{(b)}, \\hat{y}_t^{(r)}$, and their own cost function $c_t$, they can evaluate the two utility values, $u(g, x_t^{(r)})$ and $u(g, x_t^{(b)})$. Under this scenario, the agent's best response can never harm their true utility.\nRemark 4.4. For an agent t, any AR-based explanation policy $\u03c3 : (x_t^{(b)},g) \u2192 (x_t^{(r)},\\hat{y}_t^{(r)})$ will induce a best response $x_t$ that belongs to the set of this agent's non-harmful actions $v_t$. The response $x_t$ does not have to coincide with $x_t^{(r)}.$"}, {"title": "Algorithm with AR-based Explanations", "content": "As demonstrated in Section 4.3, AR-based explanations are desirable because they prevent agents from being misled into taking harmful actions. Furthermore, under a mild assumption of conditional homogeneity, these explanations are sufficient to induce any agents' non-harmful responses. In this section, we present an algorithm that enables the DM to operationalise AR-based explanations, allowing them to maximise utility by minimising prediction error."}, {"title": "Experiments", "content": "In this experiment, we apply Algorithm 1 to jointly train the predictive model g and the (local) AR-based explanation policy $\u03c3$ to minimise the DM's prediction error in Equation (5). To demonstrate the effectiveness of our proposed procedure, we compare its performance against two other procedures. In one of which we use RRM to jointly train the predictive model g and a global AR-based explanation policy while in the other case, we also run RRM to train the predictive model g but counterfactual explanations are used as the recommended actions.\nTo avoid confusion, we use $g^{loc}$ and $\u03c3^{loc}$ to denote the trained models resulted from our procedure, whereas $g^{glo}$ and $x^{(r), glo}$ are used to denote the trained predictive model and the optimal global AR-based explanation resulted from one of the two said baselines. Finally, for the other baseline, we use $g^{ce}$ to denote the trained predictive model and $\u03c3^{ce}$ to denote the policy that generates counterfactual explanations.\nExperimental setup. We construct a synthetic dataset with agents of 3-dimensional (observable) feature vectors $x^{(b)} \u2208 R^3$ and scalar (unobservable) features $z_t \u2208 R$. Each agent t has the cost function $c_t(x^{(b)}, x) =$"}, {"title": "Operationalising AR-based Explanations", "content": "Specifically, given the space of AR-based explanations E = X \u00d7 Y, the DM aims to find the optimal predictive model g and the explanation policy $\u03c3$ that jointly minimise the predictive loss in Equation (2), which is now rewritten as\n$\\min_{g,\u03c3} E_{P_{X,Z}} [(g(X_t) - h(X_t,Z_t))^2]$ (5)\nwhere $\u03c3'_t : x_t^{(b)}\u2192x_t^{(r)}$ denotes the mapping that generates the recommended action $x_t^{(r)}$. In particular, the AR-based explanation policy $\u03c3 : (x_t^{(b)}, g) \u2192 (x_t^{(r)}, g(x_t^{(r)}))$ is decomposed into two independent components: g and $\u03c3'$. Consequently, Equation (5) follows and it is possible to jointly optimise both g and $\u03c3'$.\nRepeated risk minimization (RRM). In practice, because the DM does not have access to the outcome function h and the unobserved variable $z_t$, an efficient way to solve Equation (5) is through the repeated risk minimisation (RRM) procedure [Perdomo et al., 2020]. While training g using RRM is straightforward, the same does not apply to $\u03c3'$, as this requires the DM to simulate how the loss function changes whenever $\u03c3'$ is updated. This simulation is only possible if the DM can anticipate how agents will respond to changes in $\u03c3'$. The idea of learning a response function of heterogeneous agents has been considered in prior work [Xie and Zhang, 2024], although for the case of local surrogate models as explanations. Here, we propose a similar approach tailored to AR-based explanations. Specifically, the DM can interact with agents through randomised recommended actions to learn a binary classifier $\u03be: (x^{(b)},x^{(r)}, \u0394g^{(r)}) \u2192 w_t$ that predicts their compliance behaviour $w_t$. Compliance is defined as $w_t = 1$ when the agent follows the recommendation, i.e., $x_t = x_t^{(r)}$, and $w_t = 0$ otherwise. The term $\u0394g_t^{(r)} := g(x_t^{(b)}) - g(x_t^{(r)})$ denotes the gain in prediction value for the agent t, serves as a useful feature for this classifier, as we utilise the assumed agents' behaviour defined in Equation (4) and Equation (1).\nTo generate necessary data to learn the classifier \u03be, the DM can employ a sampler \u03c0 to generate randomised recommended actions as follows: $X^{(r)} \\mid x^{(b)}, X^{(r)} \\ne x^{(b)} \u223c \u03c0(x_t^{(b)}, x_t^{(b)})$. Once \u03be is learned, the DM can simulate an agent's response as $\\hat{x}_t := w_t x_t^{(r)} + (1 \u2212 w_t)x_t^{(b)}$.\nPutting everything together, in the finite-sample case, the empirical objective for the DM in the i-th iteration of the RRM procedure is then\n$(g_i, \u03c3'_i) = arg\\min_{g, \u03c3'} \\min_{ \u03be} \\sum_{t\u2208[T_i]} (g(\\hat{x}_t) - y_t)^2,$\nwhere $\\hat{x}_t$ refers to the simulated agent's response based on the recommended action $x_t^{(r)} := (x_t^{(r)})$ and the inferred reaction model $\u03be$. In contrast, $y_t, y_t \u2208 [T_i]$ are the outcomes of $T_i$ agents collected from when the previous models $(g_{i\u22121}, \u03c3'_{i\u22121})$ are deployed, as usually done in the RRM procedure [Perdomo et al., 2020]. We provide more details in Algorithm 1."}, {"title": "On the No-Harm Guarantee of Explanations", "content": "Experimental setup. We use a quartic function as the predictive model of the DM where g(x) = x4 \u2212 x\u00b2 + 1 and use 2nd-order Taylor expansions as explanations. To illustrate that Taylor expansions [Xie and Zhang, 2024] as explanations do not guarantee no-harm, we generate a simple dataset of 100 agents with scalar features $x^{(b)} \u2208 R$ and"}, {"title": "Conclusion", "content": "To summarise, we address the challenge of providing actionable and safe explanations in strategic learning scenarios where DMs must balance transparency with utility optimisation. We formalise the class of action recommendation (AR)-based explanations, which ensure that agents act without incurring detrimental outcomes. By introducing the no-harm property and assuming a sub-homogeneity condition, we demonstrate that AR-based explanations enable DMs to achieve optimal outcomes while safeguarding agent welfare. Consequently, our work clarifies the distinctions of different explanation methods, through the lens of"}, {"title": "Related Work (Extended)", "content": "Strategic learning. Strategic classification was introduced by Br\u00fcckner et al. [2012] and further developed by Hardt et al. [2016], where they presented the first computationally efficient algorithms to learn near-optimal classifiers in strategic environments. Their key assumption was that agents have complete knowledge of the classifier due to information leakage, even when the system is designed to obscure the model. In contrast, our work weaken this assumption by considering scenarios where the learner (or DM) releases partial information about their model.\nSince then, numerous studies have further expanded the field of strategic classification by developing more efficient algorithms [Dong et al., 2018, Levanon and Rosenfeld, 2021, Ahmadi et al., 2021] or by incorporating new aspects such as social welfare [Hu et al., 2018, Milli et al., 2019], randomisation [Sundaram et al., 2023, Ahmadi et al., 2023, Shao et al., 2024], and repeated interactions [Harris et al., 2021, Cohen et al., 2023]. Another line of works concerns with providing incentive for strategic agents to improve [Kleinberg and Raghavan, 2020, Shavit et al., 2020, Harris et al., 2022a, Horowitz and Rosenfeld, 2023, Vo et al., 2024] with many of them focusing on regression settings and incorporating causal reasoning.\nIn particular, the works of Harris et al. [2022b] and Cohen et al. [2024] are closest to ours. As discussed throughout Section 4.3, Harris et al. [2022b] focuses on the obedience-inducing property (also known as the Bayesian incentive compatibility) of a subclass of action recommendations, whereas we focus on the no-harm property of action recommendations. As we also discuss in Section 4.3, identifying an AR-based explanation policy that can induce obedience for each individual agent is hard, especially when agents are heterogeneous (e.g., in Harris et al. [2022a], Shao et al. [2024]), and such individual-level identification might not be necessary if the DM only cares about optimising their expected utility, which is computed over the population of agents. Unlike action recommendations, Cohen et al. [2024] instead releases a subset of the hypothesis class to all agents, aligning with global explanations in our framework (Section 3). However, interpreting a set of models\u2014such as neural networks\u2014can be difficult for agents. In contrast, the AR-based explanations are not only more interpretable but also provide guidance that cannot mislead agents.\nCounterfactual explanations and algorithmic recourse. Tsirtsis and Gomez Rodriguez [2020], Karimi et al. [2022] explore counterfactual explanations and algorithmic recourse, for strategic agents. Although algorithmic recourse focuses on recommending actions to achieve better outcomes, it actual implementation often requires strong causal assumptions. These assumptions can render it impractical in more general settings where such causal knowledge is not justified. In contrast, our work adopts a weaker notion of desirability centred on agents' welfare\u2014ensuring non-harmful responses\u2014and examines a broader range of explanation types beyond counterfactuals. In particular, Tsirtsis and Gomez Rodriguez [2020] focuses on designing an efficient counterfactual explanation algorithm that balances the DM's utility while preventing model leakage. This approach can be interpreted as optimally reducing the explanation space within our framework. In contrast, we present a formal analysis across a range of explanation types and demonstrate the sufficiency of AR-based explanations.\nInformation design. The extensive literature on information design, as surveyed by Bergemann and Morris [2019], studies how to design information disclosure policies in a game of two parties. While our results are inspired by these works, e.g., Theorem 4.6, the goals differ significantly. As discussed in Section 4.3, information design aims at persuading agents with a general response model and does not necessarily ensure the no-harm property (Definition 4.1). In contrast, we study explanation methods that prioritise the no-harm property, ensuring agents' welfare is not compromised. By incorporating specific agent models in strategic settings, we establish the sufficiency of AR-based explanations without requiring the DM to account for agents' heterogeneous reaction models. Consequently, while the class of BCE suffices to rationalise any agents' behaviour, our setup of AR-based explanations suffice to rationalise any non-harmful agents' behaviour."}, {"title": "More Detailed Examples", "content": "This section contains more examples to illustrate our theory."}, {"title": "Examples of Explanation Methods", "content": "Below we provide some specific examples for global and local explanation methods that fit into our setting:\nGlobal surrogate models such as linear models or decision trees that approximate the true predictive model g [Molnar, 2020]. In this scenario, a constant explanation e \u2208 E, regardless of $x_t^{(b)}$, is some surrogate model f : X \u2192 Y for the true model g : X \u2192 Y.\nPartial descriptions of the model [Cohen et al., 2024]. In this scenario, a constant explanation e \u2208 E, regardless of $x_t^{(b)}$, is some subset $G_s \\subseteq G$ such that $g \u2208 G_s$.\nFeature attribution-based explanation methods that assign importance scores to features such as SHAP [Lundberg, 2017]. When the covariate $x_t^{(b)} \u2208 X \\subseteq R^d$ is a vector of d features, an explanation $e_t = (e_{t1},..., e_{td}) \u2208 E \u2208 R^d$ is a vector containing the importance scores of each features in $x_t^{(b)}$.\nLocal surrogate models such as Taylor's expansion [Xie and Zhang, 2024]. An explanation $e_t$ is some function $f_t : X \u2192 Y$ that approximates g in a local neighbourhood of $x_t^{(b)}$ and the space of explanations is the set of all such functions, e.g., $E \\subseteq F = {f | f : X \u2192 Y}$.\nCounterfactual explanations [Wachter et al., 2017b]. In this scenario, $e_t = (x_t^{(r)}, \\hat{y}_t^{(r)})$ and $E = {(x_t^{(r)}, \\hat{y}_t^{(r)}) : \\hat{y}_t^{(r)} < \\hat{y}_t^{(b)}}$, where $x_t^{(r)}$ denotes the recommended covariate for the agent to change to, in order to receive a more favourable prediction $\\hat{y}_t^{(r)} < \\hat{y}_t^{(b)}$ from the DM."}, {"title": "Shapley Value Example", "content": "Example A.1. Consider the predictive model g(x) = x1 \u2212 x2 for any x = [x1 x2] \u2208 R2, and an agent with $x_t^{(b)} = [16 4]^T$. For simplicity, suppose further that the 2 features X1, X2 are statistically independent and that X2 ~U([2,5]). The Shapley value of this agent's 2nd feature is\n$\\phi_2(g,x_t^{(b)})= \\frac{1}{2} (E [g(X_1, X_2 = 4)] - E [g(X_1, X_2)]+E [g(X_1 = 16, X_2)] - g(x_t^{(b)}) )\\newline=\\frac{1}{2} ( E [X_1, X_2 = 4] - E [X_1, X_2] + E [X_1 = 16, X_2] - [16 - 4])\\newline=\\frac{1}{2} (0 + E [16 - X_2]- E [X_1 - X_2])=\\frac{1}{2} (-32 + 2E [X_2])=-3 < 0.$\nHowever, if we consider X2 ~ U([2, 8]), then E[X2] = 28 and the Shapley value for this 2nd feature is\n$\\phi_2(g,x_t^{(b)})= \\frac{1}{2} (-32 + 2E [X_2])= 12 > 0.$\nOn the other hand, the partial derivative of the function g at the point x2 = 4 is\n$\\frac{dg}{dx_2} (x_2 = 4) = -8,$\nwhich implies that this agent can achieve a lower prediction score by increasing the value of their 2nd feature x2. However, the sign of the Shapley value for this feature, as we have shown, can vary depending on the distribution of the data, as a result, this Shapley value cannot say how this agent should change their feature to obtain better prediction score."}, {"title": "A Misled Agent", "content": "Example A.2. Suppose that an insurance company uses the $g(x) = x^2$ to predict the risk of a customer whose feature takes on the base value $x_t^{(b)} = 5$, which corresponds to the prediction"}]}