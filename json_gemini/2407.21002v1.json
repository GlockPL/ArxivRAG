{"title": "XHand: Real-time Expressive Hand Avatar", "authors": ["Qijun Gan", "Zijie Zhou", "Jianke Zhu"], "abstract": "Abstract-Hand avatars play a pivotal role in a wide array of digital interfaces, enhancing user immersion and facilitating natural interaction within virtual environments. While previous studies have focused on photo-realistic hand rendering, little attention has been paid to reconstruct the hand geometry with fine details, which is essential to rendering quality. In the realms of extended reality and gaming, on-the-fly rendering becomes imperative. To this end, we introduce an expressive hand avatar, named XHand, that is designed to comprehensively generate hand shape, appearance, and deformations in real-time. To obtain fine-grained hand meshes, we make use of three feature embedding modules to predict hand deformation displacements, albedo, and linear blending skinning weights, respectively. To achieve photo-realistic hand rendering on fine-grained meshes, our method employs a mesh-based neural renderer by leveraging mesh topological consistency and latent codes from embedding modules. During training, a part-aware Laplace smoothing strategy is proposed by incorporating the distinct levels of regularization to effectively maintain the necessary details and eliminate the undesired artifacts. The experimental evaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy of XHand, which is able to recover high-fidelity geometry and texture for hand animations across diverse poses in real-time. To reproduce our results, we will make the full implementation publicly available at https://github.com/agnJason/XHand.", "sections": [{"title": "I. INTRODUCTION", "content": "HAND avatars are crucial in various digital environ- ments, including virtual reality, digital entertainment, and human-computer interaction [1]\u2013[4]. Accurate represen- tation and lifelike motion of hand avatars are essential to deliver an authentic and engaging user experience. Due to the complexity of hand muscles and the personalized nature, it is challenging to obtain the fine-grained hand representation [5]- [8], which directly affect the user experience in virtual spaces.\nParametric model-based methods [5], [9], [10] have suc- ceeded in modeling digital human, which offer the structured frameworks to efficiently analyze and manipulate the shapes and poses of human bodies and hands. These models have played a crucial role in various applications, enabling com- puter animation and hand-object interaction [11]\u2013[15]. Since they predominantly rely on mesh-based representations, it restricts them to fixed topology and limited resolution of the 3D mesh. Consequently, it is difficult for these models to accurately represent the intricate details, such as muscle, gar- ments and hair. This hinders them from rendering high fidelity images [16]. Model-free methods offer effective solutions for representing hand meshes through various techniques."}, {"title": "II. RELATE WORK", "content": "A. Parametric Model-based Method\n3D animatable human models [5], [9], [10] enable shape deformation and animation by decoding the low-dimensional parameters into a high-dimensional space. Loper et al. [10] introduce a linear model to explicitly represent the human body through adjusting shape and pose parameters. MANO hand model [9] utilizes a rigged hand mesh with fixed- topology that can be easily deformed according to the pa- rameters. The low resolution of the template mesh hinders its application in scenarios requiring higher precision. To address this limitation, Li et al. [7] integrate muscle groups with shape registration, which results in an optimized mesh with finer appearance. Furthermore, parametric model-based methods [1], [2], [11], [43]\u2013[48] have shown the promising results in accurately recovering hand poses from input images, however, they have difficulty in effectively capturing textures and geometric details for the resulting meshes. In this paper, our proposed XHand approach is able to capture the fine details of both appearance and geometry by taking advantages of Lambertian reflectance model [49].\nB. Model-free Approach\nParametric models have proven to be valuable in incorpo- rating prior knowledge of pose and shape in hand geometry reconstruction [9], while their representation capability is restricted due to the low resolution of the template mesh. To address this issue, Choi et al. [17] introduce a network based on graph convolutional neural networks (GCN) that directly estimates the 3D coordinates of human mesh from 2D human pose. Chen et al. [18] present a UV-based representation of 3D hand mesh to estimate hand vertex positions. Mobrecon [50] predicts hand mesh in real-time through a 2D encoder and a 3D decoder. Despite the encouraging results, the above"}, {"title": "C. Neural Hand Representation", "content": "There are various alternatives available for neural hand representations, such as HandAvatar [27], HandNeRF [26], LISA [16] and LiveHand [8]. In order to achieve high fidelity rendering of human hands, Chen et al. [27] propose HandA- vatar to generate photo-realistic hand images with arbitrary poses, which take into account both occupancy and illumina- tion fields. LISA [16] is a neural implicit model with hand tex- tures, which focuses on signed distance functions (SDFs) and volumetric rendering. Mundra et al. [8] propose LiveHand that makes use of a low-resolution NeRF representation to describe dynamic hands and a CNN-based super-resolution module to facilitate high-quality rendering. Despite the efficiency in rendering hand images, it is hard for those approaches to capture the details of hand mesh geometry. Luan et al. [52] introduce a frequency decomposition loss to estimate the per- sonalized hand shape from a single image, which effectively address the challenge of data scarcity. Chen et al. introduce a spatially varying linear lighting model as a neural renderer to preserve personalized fidelity and sharp details under natural illumination. Zheng et al. facilitate the creation of detailed hand avatars from a single image by learning and utilizing data-driven hand priors. In this work, our presented XHand method focuses on synthesizing the hand avatars with fine- grained geometry in real-time.\nD. Generic Animatable Objects\nIn addition to the aforementioned methods on hand model- ing, there have been some studies reconstructing animatable whole or partial human bodies or animals [28]\u2013[30]. Face models primarily pay their attention to facial expressions, appearance, and texture, rather than handling large-scale defor- mations [32]\u2013[35]. Zheng at al. [32] bridge the gap between explicit mesh and implicit representations by a deformable point-based model that incorporates intrinsic albedo and nor- mal shading. To build human body model [5], [23], [25], [36]- [38], [53], numerous challenges arise from the intricate defor- mations, which make it arduous to precisely capture intricate details, such as textures and scan-like appearances, especially in smaller areas like the hands and face. Previous works [22], [39], [40] have explored to establish the correspondences"}, {"title": "III. METHOD", "content": "Given multi-view images \\({I_{t,i}}|i = 1,..., N, t = 1,...,T\\) for T frames captured from N viewpoints with pose \\({\\theta _t}|t = 1,..., T\\) and shape \\(\\beta\\) of their corresponding parametric hand models like MANO [9], our proposed approach aims to si- multaneously recover the expressive personalized hand meshes with fine details and render photo-realistic image in real-time. Fig. 2 shows an overview of our method. Given the hand pose parameters \\(\\theta\\), the fine-grained posed mesh is obtained from feature embedding modules (Sec. III-A), which are designed to obtain Linear Blending Skinning (LBS) weights, vertex displacements and albedo by combining the average features of the mesh with pose-driven feature mapping. With the refined mesh, the mesh-based neural renderer achieves real-time photo-realistic rendering with respect to the vertex albedo \\(\\rho\\), normals N, and latent code Q in feature embedding modules.\nA. Detailed Hand Representation\nIn this paper, the parametric hand model MANO [9] is employed to initialize the hand geometry, which effectively maps the pose parameter \\(\\theta \\in {R^{J \\times 3}}\\) with J per-bone parts and the shape parameter \\(\\beta\\in {R^{10}}\\) onto a template mesh M with vertices V. Such mapping \\(\\Omega\\) is based on linear blending skinning with the weights \\(W \\in {R^{|V|\\times J}}\\). Thus, the posed hand mesh M can be obtained by\n\\[M = \\Omega (M, W, \\theta, \\beta ).\\]\nGeometry Refinement. After increasing the MANO mesh res- olution for fine geometry using the subdivision method in [27], a personalized vertex displacement field D is introduced to allow the extra deformation for each vertex in the template mesh. The refined posed hand mesh \\(M_{fine}\\) can be computed as below\n\\[{M_{fine}} = \\Omega (M' + D,W', \\theta, \\beta ).\\]\nThe original MANO mesh [9], consisting of 778 vertices and 1538 faces, has limited capacity to accurately repre- sent fine-grained details [27]. To overcome this challenge by enhancing the mesh resolution to capture intricate features, we employ an uniform subdivision strategy on the MANO template mesh, as shown in Fig. 3. By adding new vertices at midpoint of each edge for three times, we obtain a refined mesh with 49,281 vertices and 98,432 faces. To associate skin- ning weights with these additional vertices, we compute the average weights assigned to the endpoints of the corresponding edges."}, {"title": "Feature Embedding Module", "content": "the feature embedding modules \\(\\Psi_D\\) and \\(\\Psi_{lbs}\\) to better capture the intricate details of hand mesh, LBS weights W' are derived from the LBS embedding \\(\\Psi_{lbs}\\). The displacement embedding \\(\\Psi_D\\) generates the vertex displacements D. Given the hand pose parameters \\({\\theta _t}|t = 1,...,T\\) for T frames, the mesh features are predicted as follows\n\\[{D_t} = {\\Psi _D}({\\theta _t}), {W_1} = {\\Psi _{lbs}}({\\theta _t}).\\]\nThus, the refined mesh \\(M_{fine}\\) at time t can be formulated as below\n\\[{M_{fine}} = \\Omega (M' + {D_t},{W_1},{\\theta _t}, \\beta ).\\]\nFeature Embedding Module. Generally, it is challenging to learn the distinctive hand features in different poses. To better separate between the deformation caused by changes in posture and the inherent characteristics of the hand, we present an efficient feature embedding module in this paper. It relies on the average features of hand mesh and computes offsets of features in different poses, as illustrated in Fig. 4.\nGiven a personalized hand mesh M and its pose \\(\\theta_t\\) at time t, our feature embedding module extracts mesh features \\(f_M\\) as follows\n\\[{f_M} = \\Psi ({\\theta |{f_M}}),\\]\nwhere \\(f_M\\) denotes the average vertex features of hand mesh.\nTo represent the mesh features of personalized hand gener- ated with hand pose \\(\\theta_t\\), we design the following embedding function\n\\[{\\Psi _D}({\\theta |{f_M}}) = {f_M} + \\Phi ({\\theta _t},Q) * K,\\]\nwhere \\(\\Phi\\) is vertex latent code to encode different vertices. \\(\\Phi\\) denotes a pose decoder that is combined with multi-layer perceptrons (MLPs). It projects the pose \\(\\theta_t\\) and latent code Q onto the implicit space. To align with the feature space, K"}, {"title": "B. Mesh Rendering", "content": "is the mapping matrix to convert the implicit space \\(R^m\\) into feature space \\(R^n\\), which subjects to\n\\[\\sum\\limits_{j = 1}^n {{K_{ij}}}  = 1,{\\kern 1pt} {\\kern 1pt} for{\\kern 1pt} {\\kern 1pt} i = 1,2,...,m.\\]\nThe personalized mesh features \\(f_M\\) can be derived by combining the average vertex features \\(f_M\\) and the pose- dependent offsets. Consequently, the LBS weights W can be derived with average LBS weights \\(f_{lbs}\\), pose decoder \\(\\Phi_{lbs}\\), latent code \\(Q_{lbs}\\) and mapping matrix \\(K_{lbs}\\) as follows\n\\[W = {\\Psi _{lbs}}({\\theta _t}|{f_{lbs}}) = {f_{lbs}} + {\\Phi _{lbs}}({\\theta _t},{Q_{lbs}}) * {K_{lbs}}.\\]\nSimilarly, the vertex displacements \\(D_t\\) can be obtained as follows\n\\[{D_t} = {\\Psi _D}({\\theta _t}|{f_D}) = {f_D} + {\\Phi _D}({\\theta _t},{Q_D}) * {K_D},\\]\nwhere \\(f_D\\) denotes average displacements. \\(\\Phi_D, Q_D\\) and \\(K_D\\) are pose decoder, latent code and mapping matrix for \\(\\Psi_D\\), respectively. The depths of \\(\\Phi_{lbs}\\) within the LBS embedding module \\(\\Psi_{lbs}\\) and \\(\\Psi_{lbs}\\), within the albedo embedding module \\(\\Phi_{lbs}\\) are set to 5, with each layer consisting of 128 neurons. Addi- tionally, the depth of \\(\\Phi_D\\) within the displacement embedding module \\(\\Psi_D\\) is 8, where the number of neurons is 512.\nRemark. The feature embedding modules allows for the interpretable acquisition of hand features \\(f_M\\) corresponding to the pose \\(\\theta_t\\). The average mesh features are stored in \\(f_M\\), while the features offsets are affected by the pose \\(\\theta_t\\). More importantly, the training objectives are greatly simplified by taking into account of the average features constraints, which leads to the faster convergence and improved accuracy.\nInverse Rendering. In order to achieve rapid and differen- tiable rendering of detailed mesh \\(M_{fine}\\), an inverse renderer is employed to synthesize hand images. Assuming that the skin color follows the Lambertian reflectance model [55], the rendered image B can be calculated from the Spherical"}, {"title": "C. Training Process", "content": "Harmonics coefficients G, the vertex normal N, and the vertex albedo \\(\\rho\\) using the following equation\n\\[{\\rm B}({\\pi ^2}) = \\rho  \\cdot SH(G,N),\\]\nwhere \\(\\pi^2\\) is camera parameter of the i-th viewpoint. SH(\u00b7) represents Spherical Harmonics (SH) function of the third order. N is the vertex normal computed from the vertices of mesh \\(M_{fine}\\). Similar to Eq. 4, the pose-dependent albedo \\(\\rho_t\\) can be obtained from feature embedding module \\(\\Psi_{\\rho}\\), with average vertex albedo \\(f_{\\rho}\\), pose decoder \\(\\Phi_{\\rho}\\), latent code \\(Q_{\\rho}\\) and mapping matrix \\(K_{\\rho}\\) as follows\n\\[{\\rho _t} = {\\Psi _\\rho }({\\theta _t}) = {f_\\rho } + {\\Phi _\\rho }({\\theta _t},{Q_\\rho }) * {K_\\rho }.\\]\nBy analyzing how the variations in brightness relate to the hand shape, inverse rendering with the Lambertian reflectance model can effectively disentangle geometry and appearance.\nMesh-based Neural Rendering. The NeRF-based methods usually employ volumetric rendering along its corresponding camera ray d to acquire pixel color [8], [26], which usually require a large amount of training time. Instead, we aim to minimize the sampling time and enhance the rendering quality by making use of a mesh-based neural rendering method that is able to take advantage of the consistent topology of our refined mesh.\nThe mesh is explicitly represented by triangular facets so that the intersection points between rays and meshes are located within the facets. The features that describe meshes, such as position, color, and normal, are associated with their respective vertices. Consequently, the attributes of intersection points can be calculated by interpolating the three vertices of triangular facet to its intersection point. The efficient dif- ferentiable rasterization [56] ensures the feasibility of inverse rendering and mesh-based neural rendering.\nGiven a camera view \\(\\pi^2\\), our mesh-based neural render \\(C(\\pi^2)\\) synthesizes the image with respect to the position x, normal N, feature vector h and ray direction d, where x, hand Nare obtained through interpolating with \\(M_{fine}\\). h in neural render \\(C(\\pi^2)\\) contains the latent codes \\(Q_D\\) and \\(Q_{\\rho}\\) detached from \\(\\Psi_D\\) and \\(\\Phi_{\\rho}\\), and feature vector \\(Q_{render}\\) [51]."}, {"title": "IV. EXPERIMENTS", "content": "modules \\(\\Psi_D\\), \\(\\Phi_{lbs}\\) and \\(\\Psi_{\\rho}\\). \\(L_{inv}\\) is introduced to minimize the errors of rendering images as follows\n\\[{L_{inv}} = {L_{rgb}} + {L_{reg}},\\]\nwhere \\(L_{rgb}\\) represents the rendering loss. \\(L_{reg}\\) is the regular- ization term. Inspired by [60], we use \\(L_1\\) error combined with an SSIM term to form the \\(L_{rgb}\\) as below\n\\[{L_{rgb}} = \\lambda \\sum {\\left| {{\\rm B}({{\\pi }^2}) - {I_i}} \\right|}  + (1 - \\lambda ){{\\pounds }_{SSIM}}(B({\\pi ^2}),{I_i}),\\]\nwhere \\(\\lambda\\) denotes the trade-off coefficient.\nTo enhance the efficiency in extracting geometric infor- mation from images, we introduce the part-aware Laplace smoothing term \\(L_{pLap}\\). The Laplace matrix A of mesh feature f is defined as \\(A = L \\times f\\). Hierarchical weights \\(\\Theta_{pLap}\\) are introduced to balance the weights of regularisation via different levels of smoothness. \\(G_i\\) in matrix \\(\\Theta_{pLap}\\) is defined as follows\n\\[{\\gamma _i} = \\left\\{ {\\begin{array}{*{20}{c}}{\\gamma _1}\\{\\gamma _2}\\end{array}{\\rm{ }}{\\begin{array}{*{20}{c}}{{{\\rm{p}}_1}}\\end{array}{\\rm{ }} < {A_i} < {p_2}} \\end{array}} \\right.,\\]\nwhere \\(\\{p_1, p_2, ...\\}\\) represent the threshold values for the hierarchical weighting and \\(\\{\\gamma _1,\\gamma _2, ...\\}\\) denote the balanced coefficients. The part-aware Laplace smoothing \\(L_{pLap}\\) is used to reduce excessive roughness in albedo and displacement without affecting the fine details, which is defined as follows\n\\[{L_{pLap}}(f) = \\sum {{p_{Lap}}A} .\\]\nBy employing varying degrees of hierarchical weights to trade- off Laplacian smoothing, \\(L_{pLap}\\) is able to better constrain feature optimization in different scenarios. In our cases, minor irregularities are considered to be acceptable, while excessive changes are undesirable. Therefore, the threshold p can be dynamically controlled through the quantiles of Laplace matrix A, where those greater than p will be assigned larger balance coefficients.\nThe following regularization terms are introduced to con- form the optimized mesh to the hand geometry\n\\[{L_{reg}} = {L_{pLap}}(\\rho ) + {L_{pLap}}(D) + {a_1}{L_{mask}} + {a_2}{L_e} + {a_3}{L_d}.\\]\nwhere \\(L_{pLap}(\\rho )\\) and \\(L_{pLap}(D)\\) are part-aware Laplacian smoothing terms to maintain albedo and displacement flatten- ing during training. \\(L_{mask}\\), \\(L_e\\) and \\(L_d\\) are utilized to ensure that the optimized hand mesh remains close to the MANO model, where each term is assigned with constant coefficients denoted by \\(a_1, a_2\\) and \\(a_3\\). Let \\({L_{mask}} = \\sum {\\left| {{\\rm M} - {M'}} \\right|} \\) represents the \\(L_1\\) loss between the mask M rendered during inverse rendering and the original MANO mask. \\(L_e\\) penalizes the edge length changes of \\(e_{ij}\\) with respect to MANO mesh as \\({E_{i,j}} = {\\left| {{{\\hat l}_{ij}} - {l_{ij}}} \\right|^2}\\), where \\({\\hat l_{ij}}\\) is the Euclidean distance ||\u00b7|| between adjacent vertices \\(V_i\\) and \\(V_j\\) on the mesh edges. \\(e_{ij}\\) denotes the edge distance of the subdivided MANO mesh M'. \\(L_d = \\sum {\\left| {{D_i}} \\right|} ^2\\) is employed to constrain the degree of displacement.\nLoss Functions of Neural Renderer. Once the latent codes \\(Q_D\\) and \\(Q_{\\rho}\\) of \\(\\Psi_D\\) and \\(\\Psi_{\\rho}\\) are detached, \\(L_{neu}\\) is used to"}, {"title": "V. CONCLUSION", "content": "A. Datasets\nInterHand2.6M. The InterHand2.6M dataset [41] is a large collection of images, each with a resolution of 512 \u00d7 334 pixels, accompanied by MANO annotations. It includes multi- view temporal sequences of both single and interacting hands. The experiments primarily utilize the 5 FPS version of this dataset.\nDeepHandMesh. The DeepHandMesh dataset [19] features images captured from five different viewpoints, matching the resolution of those in InterHand2.6M. It also provides corresponding 3D hand scans, facilitating the validation of mesh reconstruction quality against 3D ground truth data.\nB. Experimental Setup\nImplementation Details. In the experiments, our proposed XHand model is mainly trained and evaluated on the 5FPS version of Interhand2.6M dataset [41], which is made of large- scale multi-view sequences capturing a wide range of hand poses. Each sequence has dozens of images with the size of 512 x 334. As in [26], [27], XHand model is trained on the InterHand2.6M dataset with 20 views across 50 frames for each sequence. The remaining frames are used for evaluation. To assess the quality of mesh reconstruction, we conduct experiments on DeepHandMesh dataset [19], which consists of 3D hand scans along with images captured from five different views. The images are with the same size as those in InterHand2.6M dataset. We conducted all the experiments on a PC with NVIDIA RTX 3090 GPU having 24GB GPU memory.\nWe employ PyTorch and Adam Optimizer with a learning rate of 5e-4. To facilitate differentiable rasterization, we make use of the off-the-shelf renderer nvdiffrast [56]. As in [57], positional encoding is performed on d and x before feeding them into the rendering network. In our training process, the feature embedding modules are firstly trained for 500 epochs using inverse rendering. Then, feature embedding modules and neural render are jointly trained for 500 epochs, where the average features fm in feature embedding modules are updated every 50 epochs. We empirically found that the best performance is achieved in case of \\(\\lambda = w = 0.8\\), \\(a\u2081 = 10\\), \\(a2 = 1e5\\), and \\(a3 = 1e4\\). To avoid the excessive displacements and color variations, in \\(L_{pLap}(\\rho )\\), \\(p\u2081\\) is set to the first quartile of \\(\\Lambda_{\\rho}\\), \\(\\gamma_1\\) is set to 0.1, and \\(\\gamma_2\\) is 1. Similarly, in \\(L_{pLap}(D)\\), \\(p\u2081\\) is the median of \\(\\Lambda_{D}\\), and \\(\\gamma_1 = 0.1\\), \\(\\gamma_2 = 20\\). The lengths of latent codes \\(Q_{lbs}\\), \\(Q_D\\), \\(Q_{\\rho}\\) and \\(Q_{render}\\) are set to 10, 10, 10 and 20, respectively.\nEvaluation Metrics. In the experiments, we fit the hand mesh representations to multi-view images sequence for single scene. For fair comparison, we employ the same evaluation"}]}