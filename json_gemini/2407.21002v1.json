{"title": "XHand: Real-time Expressive Hand Avatar", "authors": ["Qijun Gan", "Zijie Zhou", "Jianke Zhu"], "abstract": "Hand avatars play a pivotal role in a wide array of digital interfaces, enhancing user immersion and facilitating natural interaction within virtual environments. While previous studies have focused on photo-realistic hand rendering, little attention has been paid to reconstruct the hand geometry with fine details, which is essential to rendering quality. In the realms of extended reality and gaming, on-the-fly rendering becomes imperative. To this end, we introduce an expressive hand avatar, named XHand, that is designed to comprehensively generate hand shape, appearance, and deformations in real-time. To obtain fine-grained hand meshes, we make use of three feature embedding modules to predict hand deformation displacements, albedo, and linear blending skinning weights, respectively. To achieve photo-realistic hand rendering on fine-grained meshes, our method employs a mesh-based neural renderer by leveraging mesh topological consistency and latent codes from embedding modules. During training, a part-aware Laplace smoothing strategy is proposed by incorporating the distinct levels of regularization to effectively maintain the necessary details and eliminate the undesired artifacts. The experimental evaluations on InterHand2.6M and DeepHandMesh datasets demonstrate the efficacy of XHand, which is able to recover high-fidelity geometry and texture for hand animations across diverse poses in real-time. To reproduce our results, we will make the full implementation publicly available at https://github.com/agnJason/XHand.", "sections": [{"title": "I. INTRODUCTION", "content": "HAND avatars are crucial in various digital environ- ments, including virtual reality, digital entertainment, and human-computer interaction [1]\u2013[4]. Accurate represen- tation and lifelike motion of hand avatars are essential to deliver an authentic and engaging user experience. Due to the complexity of hand muscles and the personalized nature, it is challenging to obtain the fine-grained hand representation [5]- [8], which directly affect the user experience in virtual spaces.\nParametric model-based methods [5], [9], [10] have suc- ceeded in modeling digital human, which offer the structured frameworks to efficiently analyze and manipulate the shapes and poses of human bodies and hands. These models have played a crucial role in various applications, enabling com- puter animation and hand-object interaction [11]\u2013[15]. Since they predominantly rely on mesh-based representations, it restricts them to fixed topology and limited resolution of the 3D mesh. Consequently, it is difficult for these models to accurately represent the intricate details, such as muscle, gar- ments and hair. This hinders them from rendering high fidelity images [16]. Model-free methods offer effective solutions for representing hand meshes through various techniques. Graph Convolutional Network (GCN)-based and UV-based represen- tations of 3D hand meshes [17], [18] enable the reconstruction of diverse hand poses with detailed deformations. Lightweight auto-encoders [12], [19] further enhance real-time hand mesh prediction. Despite these advancements in capturing accurate hand poses, these methods still fall short in preserving intricate geometric details.\nRecently, neural implicit representations [20], [21] have emerged as powerful tools in synthesizing novel views for static scenes. Some studies [16], [22]\u2013[26] have expanded these methods into the realm of articulated objects, notably the human body, to facilitate photo-realistic rendering. Live- Hand [8] achieves real-time rendering through a neural implicit representation along with a super-resolution renderer. Karun- ratanakul et al. [6] present a self-shadowing hand renderer. Corona et al. [16] introduce a neural model LISA that predicts the color and the signed distance with respect to each hand bone independently. Despite the promising results, it struggles to capture intricate high-frequency details and lacks capability of real-time rendering. Meanwhile, Chen et al. [27] make use of occupancy and illumination fields to obtain hand geometry, while the generated hand geometry lacks the intricate details and appears to be smoothing surface. These methods have difficulties in recovering the detailed geometry that usually plays a crucial role in photo-realistic rendering.\nIn addition to hand modeling methods, several studies have focused on reconstructing animatable human bodies or animals [28]\u2013[35]. Building accurate human body models presents significant challenges due to the complex deforma- tions involved, particularly in capturing fine details such as textures and scan-like appearances, especially in smaller areas like hands and faces [5], [23], [25], [36]\u2013[38]. To address these challenges, several approaches have been developed with detailed 3D scans. For instance, previous works [22], [39], [40] have focused on establishing correspondences between pose space and standard space through techniques such as linear blend skinning and inverse skinning weights. These advancements collectively contribute to more precise and realistic human body modeling, while their results for hand modeling remain smooth.\nTo address these challenges, we propose XHand, an ex- pressive hand avatar that achieves real-time performance (see Fig. 1). Our approach includes feature embedding modules that predict hand deformation displacements, vertex albedo, and linear blending skinning (LBS) weights using a subdivided MANO model [9]. These modules utilize average features of the hand mesh and compute feature offsets for different poses, addressing the difficulty in directly learning dynamic personalized hand color and texture due to significant pose- dependent variations. By distinguishing between average and pose-dependent features, our modules simplify the training"}, {"title": "II. RELATE WORK", "content": "A. Parametric Model-based Method\n3D animatable human models [5], [9], [10] enable shape deformation and animation by decoding the low-dimensional parameters into a high-dimensional space. Loper et al. [10] introduce a linear model to explicitly represent the human body through adjusting shape and pose parameters. MANO hand model [9] utilizes a rigged hand mesh with fixed- topology that can be easily deformed according to the pa- rameters. The low resolution of the template mesh hinders its application in scenarios requiring higher precision. To address this limitation, Li et al. [7] integrate muscle groups with shape registration, which results in an optimized mesh with finer appearance. Furthermore, parametric model-based methods [1], [2], [11], [43]\u2013[48] have shown the promising results in accurately recovering hand poses from input images, however, they have difficulty in effectively capturing textures and geometric details for the resulting meshes. In this paper, our proposed XHand approach is able to capture the fine details of both appearance and geometry by taking advantages of Lambertian reflectance model [49].\nB. Model-free Approach\nParametric models have proven to be valuable in incorpo- rating prior knowledge of pose and shape in hand geometry reconstruction [9], while their representation capability is restricted due to the low resolution of the template mesh. To address this issue, Choi et al. [17] introduce a network based on graph convolutional neural networks (GCN) that directly estimates the 3D coordinates of human mesh from 2D human pose. Chen et al. [18] present a UV-based representation of 3D hand mesh to estimate hand vertex positions. Mobrecon [50] predicts hand mesh in real-time through a 2D encoder and a 3D decoder. Despite the encouraging results, the above"}, {"title": "C. Neural Hand Representation", "content": "methods still cannot capture the geometric details of hand. Moon et al. [19] propose an encoder-decoder framework that employs a template mesh to learn corrective parameters for pose and appearance. Although having achieved the improved geometry and articulated deformation, it has difficulty in rendering photo-realistic hand images. Gan et al. [51] intro- duce an optimized pipeline that utilizes multi-view images to reconstruct a static hand mesh. Unfortunately, it overlooks the variations due to joint movements. Karunratanakul et al. [6] design a shadow-aware differentiable rendering scheme that optimizes the abledo and normal map to represent hand avatar. However, its geometry remains smoothing. In contrast to the above methods, our proposed XHand approach is able to simultaneously synthesize the detailed geometry and photo- realistic images for drivable hands.\nThere are various alternatives available for neural hand representations, such as HandAvatar [27], HandNeRF [26], LISA [16] and LiveHand [8]. In order to achieve high fidelity rendering of human hands, Chen et al. [27] propose HandA- vatar to generate photo-realistic hand images with arbitrary poses, which take into account both occupancy and illumina- tion fields. LISA [16] is a neural implicit model with hand tex- tures, which focuses on signed distance functions (SDFs) and volumetric rendering. Mundra et al. [8] propose LiveHand that makes use of a low-resolution NeRF representation to describe dynamic hands and a CNN-based super-resolution module to facilitate high-quality rendering. Despite the efficiency in rendering hand images, it is hard for those approaches to capture the details of hand mesh geometry. Luan et al. [52] introduce a frequency decomposition loss to estimate the per- sonalized hand shape from a single image, which effectively address the challenge of data scarcity. Chen et al. introduce a spatially varying linear lighting model as a neural renderer to preserve personalized fidelity and sharp details under natural illumination. Zheng et al. facilitate the creation of detailed hand avatars from a single image by learning and utilizing data-driven hand priors. In this work, our presented XHand method focuses on synthesizing the hand avatars with fine- grained geometry in real-time."}, {"title": "D. Generic Animatable Objects", "content": "In addition to the aforementioned methods on hand model- ing, there have been some studies reconstructing animatable whole or partial human bodies or animals [28]\u2013[30]. Face models primarily pay their attention to facial expressions, appearance, and texture, rather than handling large-scale defor- mations [32]\u2013[35]. Zheng at al. [32] bridge the gap between explicit mesh and implicit representations by a deformable point-based model that incorporates intrinsic albedo and nor- mal shading. To build human body model [5], [23], [25], [36]- [38], [53], numerous challenges arise from the intricate defor- mations, which make it arduous to precisely capture intricate details, such as textures and scan-like appearances, especially in smaller areas like the hands and face. Previous works [22], [39], [40] have explored to establish the correspondences between pose space and template space through linear blend skinning and inverse skinning weights. Alldieck et al. [13] em- ploy learning-based implicit representations to model human bodies via SDFs. Chen et al. [23] propose a forward skinning model that finds all canonical correspondences of deformed points. Shen et al. [54] introduce XAvatar to achieve high fidelity of rigged human bodies, which employ part-aware sampling and initialization strategies to learn neural shapes and deformation fields."}, {"title": "III. METHOD", "content": "Given multi-view images $\\{I_{t,i} | i = 1,..., N, t = 1,...,T\\}$\nfor $T$ frames captured from $N$ viewpoints with pose $\\{\\theta_t | t =\n1,..., T\\}$ and shape $\\beta$ of their corresponding parametric hand\nmodels like MANO [9], our proposed approach aims to si-\nmultaneously recover the expressive personalized hand meshes\nwith fine details and render photo-realistic image in real-time.\nFig. 2 shows an overview of our method. Given the hand\npose parameters $\\theta$, the fine-grained posed mesh is obtained\nfrom feature embedding modules (Sec. III-A), which are\ndesigned to obtain Linear Blending Skinning (LBS) weights,\nvertex displacements and albedo by combining the average\nfeatures of the mesh with pose-driven feature mapping. With\nthe refined mesh, the mesh-based neural renderer achieves\nreal-time photo-realistic rendering with respect to the vertex\nalbedo $\\rho$, normals $N$, and latent code $Q$ in feature embedding\nmodules.\nA. Detailed Hand Representation\nIn this paper, the parametric hand model MANO [9] is\nemployed to initialize the hand geometry, which effectively\nmaps the pose parameter $\\theta \\in \\mathbb{R}^{J\\times 3}$ with $J$ per-bone parts\nand the shape parameter $\\beta \\in \\mathbb{R}^{10}$ onto a template mesh $M$\nwith vertices $V$. Such mapping $\\Omega$ is based on linear blending\nskinning with the weights $W \\in \\mathbb{R}^{|V| \\times J}$. Thus, the posed hand\nmesh $M$ can be obtained by\n$M = \\Omega(M, W, \\theta, \\beta)$.\nGeometry Refinement. After increasing the MANO mesh res- olution for fine geometry using the subdivision method in [27], a personalized vertex displacement field $D$ is introduced to allow the extra deformation for each vertex in the template mesh. The refined posed hand mesh $M_{fine}$ can be computed as below\n$M_{fine} = \\Omega(M' + D, W', \\theta, \\beta)$.\nThe original MANO mesh [9], consisting of 778 vertices and 1538 faces, has limited capacity to accurately repre- sent fine-grained details [27]. To overcome this challenge by enhancing the mesh resolution to capture intricate features, we employ an uniform subdivision strategy on the MANO template mesh, as shown in Fig. 3. By adding new vertices at midpoint of each edge for three times, we obtain a refined mesh with 49,281 vertices and 98,432 faces. To associate skin- ning weights with these additional vertices, we compute the average weights assigned to the endpoints of the corresponding edges."}, {"title": "B. Mesh Rendering", "content": "the feature embedding modules $\\Psi_D$ and $\\Psi_{lbs}$ to better capture the intricate details of hand mesh, LBS weights $W'$ are derived from the LBS embedding $\\Psi_{lbs}$. The displacement embedding $\\Psi_D$ generates the vertex displacements $D$. Given the hand pose parameters $\\{\\theta_t | t = 1,...,T\\}$ for T_frames, the mesh features are predicted as follows\n$D_t = \\Psi_D(\\theta_t), W_1 = \\Psi_{lbs}(\\theta_t)$.\nThus, the refined mesh $M_{fine}$ at time t can be formulated as below\n$M_{fine} = \\Omega(M' + D_t, W_1, \\theta_t, \\beta)$.\nFeature Embedding Module. Generally, it is challenging to learn the distinctive hand features in different poses. To better separate between the deformation caused by changes in posture and the inherent characteristics of the hand, we present an efficient feature embedding module in this paper. It relies on the average features of hand mesh and computes offsets of features in different poses, as illustrated in Fig. 4.\nGiven a personalized hand mesh $M$ and its pose $\\theta_t$ at time t, our feature embedding module extracts mesh features $f_M$ as follows\n$f_M = \\Phi(\\theta | f_M)$,\nwhere $f_M$ denotes the average vertex features of hand mesh.\nTo represent the mesh features of personalized hand gener- ated with hand pose $\\theta_t$, we design the following embedding function\n$\\Phi(\\theta_t | f_M) = f_M + \\Phi(\\theta_t, Q) * K$,\nwhere $Q$ is vertex latent code to encode different vertices. $\\Phi$ denotes a pose decoder that is combined with multi-layer perceptrons (MLPs). It projects the pose $\\theta_t$ and latent code Q onto the implicit space. To align with the feature space, K"}, {"title": "Inverse Rendering", "content": "is the mapping matrix to convert the implicit space $\\mathbb{R}^m$ into feature space $\\mathbb{R}^n$, which subjects to\n$\\sum_{j=1}^{n} K_{ij} = 1, \\text{ for } i = 1, 2, ..., m$.\nThe personalized mesh features $f_M$ can be derived by combining the average vertex features $f_m$ and the pose- dependent offsets. Consequently, the LBS weights W can be derived with average LBS weights $f_{lbs}$, pose decoder $\\Phi_{lbs}$, latent code $Q_{lbs}$ and mapping matrix $K_{lbs}$ as follows\n$W = \\Psi_{lbs}(\\theta_t | f_{lbs}) = f_{lbs} + \\Phi_{lbs}(\\theta_t, Q_{lbs}) * K_{lbs}$.\nSimilarly, the vertex displacements $D_t$ can be obtained as follows\n$D_t = \\Psi_D(\\theta_t | f_D) = f_D + \\Phi_D(\\theta_t, Q_D) * K_D$,\nwhere $f_D$ denotes average displacements. $\\Phi_D$, $Q_D$ and $K_D$ are pose decoder, latent code and mapping matrix for $\\Psi_D$, respectively. The depths of $\\Phi_{lbs}$ within the LBS embedding module $\\Psi_{lbs}$ and $\\Phi_{\\rho}$ within the albedo embedding module $\\Psi_{\\rho}$ are set to 5, with each layer consisting of 128 neurons. Addi- tionally, the depth of $\\Phi_D$ within the displacement embedding module $\\Psi_D$ is 8, where the number of neurons is 512.\nRemark. The feature embedding modules allows for the interpretable acquisition of hand features $f_M$ corresponding to the pose $\\theta_t$. The average mesh features are stored in $f_m$, while the features offsets are affected by the pose $\\theta_t$. More importantly, the training objectives are greatly simplified by taking into account of the average features constraints, which leads to the faster convergence and improved accuracy."}, {"title": "B. Mesh Rendering", "content": "In order to achieve rapid and differen- tiable rendering of detailed mesh $M_{fine}$, an inverse renderer is employed to synthesize hand images. Assuming that the skin color follows the Lambertian reflectance model [55], the rendered image B can be calculated from the Spherical Harmonics coefficients G, the vertex normal N, and the vertex albedo $\\rho$ using the following equation\n$B(\\pi^2) = \\rho \\cdot SH(G, N)$,\nwhere $\\pi^2$ is camera parameter of the $i$-th viewpoint. $SH(\\cdot)$ represents Spherical Harmonics (SH) function of the third order. N is the vertex normal computed from the vertices of mesh $M_{fine}$. Similar to Eq. 4, the pose-dependent albedo $\\rho_t$ can be obtained from feature embedding module $\\Psi_{\\rho}$, with average vertex albedo $f_{\\rho}$, pose decoder $\\Phi_{\\rho}$, latent code $Q_{\\rho}$ and mapping matrix $K_{\\rho}$ as follows\n$\\rho_t = \\Psi_{\\rho}(\\theta_t) = f_{\\rho} + \\Phi_{\\rho}(\\theta_t, Q_{\\rho}) * K_{\\rho}$.\nBy analyzing how the variations in brightness relate to the hand shape, inverse rendering with the Lambertian reflectance model can effectively disentangle geometry and appearance.\nMesh-based Neural Rendering. The NeRF-based methods usually employ volumetric rendering along its corresponding camera ray d to acquire pixel color [8], [26], which usually require a large amount of training time. Instead, we aim to minimize the sampling time and enhance the rendering quality by making use of a mesh-based neural rendering method that is able to take advantage of the consistent topology of our refined mesh.\nThe mesh is explicitly represented by triangular facets so that the intersection points between rays and meshes are located within the facets. The features that describe meshes, such as position, color, and normal, are associated with their respective vertices. Consequently, the attributes of intersection points can be calculated by interpolating the three vertices of triangular facet to its intersection point. The efficient dif- ferentiable rasterization [56] ensures the feasibility of inverse rendering and mesh-based neural rendering.\nGiven a camera view $\\pi^2$, our mesh-based neural render $C(\\pi^2)$ synthesizes the image with respect to the position x, normal N, feature vector h and ray direction d, where x, hand Nare obtained through interpolating with $M_{fine}$. h in neural render $C(\\pi^2)$ contains the latent codes $Q_D$ and $Q_{\\rho}$ detached from $\\Psi_D$ and $\\Psi_{\\rho}$, and feature vector $Q_{render}$ [51]."}, {"title": "C. Training Process", "content": "modules $\\Psi_D$, $\\Psi_{lbs}$ and $\\Psi_{\\rho}$. $L_{inv}$ is introduced to minimize the errors of rendering images as follows\n$L_{inv} = L_{rgb} + L_{reg}$,\nwhere $L_{rgb}$ represents the rendering loss. $L_{reg}$ is the regular- ization term. Inspired by [60], we use $L_1$ error combined with an SSIM term to form the $L_{rgb}$ as below\n$L_{rgb} = \\lambda \\Sigma ||B(\\pi^2) - I_i||_1 + (1 - \\lambda)L_{SSIM}(B(\\pi^2), I_i)$,\nwhere $\\lambda$ denotes the trade-off coefficient.\nTo enhance the efficiency in extracting geometric infor- mation from images, we introduce the part-aware Laplace smoothing term $L_{pLap}$. The Laplace matrix $A$ of mesh feature f is defined as $A = L \\times f$. Hierarchical weights $\\Theta_{pLap}$ are introduced to balance the weights of regularisation via different levels of smoothness. $G_i$ in matrix $\\Theta_{pLap}$ is defined as follows\n$\\gamma_i = \\begin{cases}\n    \\gamma_1 & \\text{ if } 0 < A_i < p_1 \\\\\n    \\gamma_2 & \\text{ if } p_1 < A_i < p_2\n  \\end{cases}$,\nwhere $\\{p_1,p_2,...\\}$ represent the threshold values for the hierarchical weighting and $\\{\\gamma_1,\\gamma_2,...\\}$ denote the balanced coefficients. The part-aware Laplace smoothing $L_{pLap}$ is used to reduce excessive roughness in albedo and displacement without affecting the fine details, which is defined as follows\n$L_{pLap}(f) = \\sum \\Theta_{pLap} A$.\nBy employing varying degrees of hierarchical weights to trade- off Laplacian smoothing, $L_{pLap}$ is able to better constrain feature optimization in different scenarios. In our cases, minor irregularities are considered to be acceptable, while excessive changes are undesirable. Therefore, the threshold p can be dynamically controlled through the quantiles of Laplace matrix A, where those greater than p will be assigned larger balance coefficients.\nThe following regularization terms are introduced to con- form the optimized mesh to the hand geometry\n$L_{reg} = L_{pLap}(\\rho) + L_{pLap}(D) + a_1 L_{mask} + a_2 L_e + a_3 L_d$.\nwhere $L_{pLap}(\\rho)$ and $L_{pLap}(D)$ are part-aware Laplacian smoothing terms to maintain albedo and displacement flatten- ing during training. $L_{mask}$, $L_e$ and $L_d$ are utilized to ensure that the optimized hand mesh remains close to the MANO model, where each term is assigned with constant coefficients denoted by $a_1$,$a_2$ and $a_3$. Let $L_{mask} = \\sum_i ||M - \\hat{M}||_1$ represents the $L_1$ loss between the mask M rendered during inverse rendering and the original MANO mask. $L_e$ penalizes the edge length changes of $e_{ij}$ with respect to MANO mesh as $E_{i,j} ||\\hat{l_{ij}} - l_{ij}||^2$, where $\\hat{l_{ij}}$ is the Euclidean distance $||\\cdot||$ between adjacent vertices $V_i$ and $V_j$ on the mesh edges. $c_{ij}$ denotes the edge distance of the subdivided MANO mesh $M'$. $L_d = \\sum_i||D_i||^2$ is employed to constrain the degree of displacement.\nLoss Functions of Neural Renderer. Once the latent codes $Q_D$ and $Q_{\\rho}$ of $\\Psi_D$ and $\\Psi_{\\rho}$ are detached, $L_{neu}$ is used to"}, {"title": "B. Experimental Setup", "content": "To obtain a personalized hand representation, the parameters of the three feature embedding modules $\\Psi_D$, $\\Psi_{lbs}$, and $\\Psi_{\\rho}$, as well as the neural render C, require to be optimized based on multi-view image sequences. Our training process consists of three steps, including initialization, training feature embedding modules, and training the mesh-based neural render.\nInitialization of XHand. To train our proposed XHand model, the average features $f_M$ of mesh in feature embedding significantly affect training efficiency and results. Random initialization has great impact on training due to estimation errors in $\\Phi_{lbs}$ and $\\Phi_D$, which may lead to the failure of inverse rendering. Therefore, it is crucial to initialize the neural hand representation. To this end, the reconstruction result of the first frame (t = 1) is treated as the initial model.\nInspired by [51], [58], XHand model is initialized from multi-view images. The vertex displacement D and vertex albedo $\\rho$ of hand mesh are jointly optimized through inverse rendering. Mesh generation is obtained from Eq. 2, and the rendering equation is same as Eq. 11. The loss function during initialization is formulated as below\n$L_{init} = \\sum_i ||B(\\pi^2) - I_i||_1 + \\Sigma_i L \\times D + \\Sigma_i L \\times \\rho$,\nwhere L is the Laplacian matrix [59]. Laplacian terms L \u00d7 D and L \u00d7 $\\rho$ are employed to regularize the mesh optimization, as the mesh features are supposed to be smooth. Uniform weights of the Laplacian matrix are adopted in training. The outcomes D and $\\rho$ are used to initialize $\\Psi_D$ and $\\Psi_{\\rho}$. The initialization of $\\Psi_{lbs}$ is directly derived from MANO model [9].\nLoss Functions of Feature Embedding. Inverse rendering is utilized to learn the parameters of three feature embedding"}, {"title": "Loss Functions of Neural Renderer", "content": "To minimize the residuals between the rendered image and the ground truth like Eq. 15\n$L_{neu} = \\omega \\sum ||C(\\pi^2)-I_i||_1 + (1 - \\omega)L_{SSIM}(C(\\pi^2), I_i)$,\nwhere $\\omega$ denotes balanced coefficient.\nIV. EXPERIMENTS\nA. Datasets\nInterHand2.6M. The InterHand2.6M dataset [41] is a large collection of images, each with a resolution of 512 \u00d7 334 pixels, accompanied by MANO annotations. It includes multi- view temporal sequences of both single and interacting hands. The experiments primarily utilize the 5 FPS version of this dataset.\nDeepHandMesh. The DeepHandMesh dataset [19] features images captured from five different viewpoints, matching the resolution of those in InterHand2.6M. It also provides corresponding 3D hand scans, facilitating the validation of mesh reconstruction quality against 3D ground truth data."}, {"title": "C. Experimental Results", "content": "Implementation Details. In the experiments, our proposed XHand model is mainly trained and evaluated on the 5FPS version of Interhand2.6M dataset [41], which is made of large- scale multi-view sequences capturing a wide range of hand poses. Each sequence has dozens of images with the size of 512 x 334. As in [26], [27], XHand model is trained on the InterHand2.6M dataset with 20 views across 50 frames for each sequence. The remaining frames are used for evaluation.\nTo assess the quality of mesh reconstruction, we conduct experiments on DeepHandMesh dataset [19], which consists of 3D hand scans along with images captured from five different views. The images are with the same size as those in InterHand2.6M dataset. We conducted all the experiments on a PC with NVIDIA RTX 3090 GPU having 24GB GPU memory.\nWe employ PyTorch and Adam Optimizer with a learning rate of 5e-4. To facilitate differentiable rasterization, we make use of the off-the-shelf renderer nvdiffrast [56]. As in [57], positional encoding is performed on d and x before feeding them into the rendering network. In our training process, the feature embedding modules are firstly trained for 500 epochs using inverse rendering. Then, feature embedding modules and neural render are jointly trained for 500 epochs, where the average features $f_M$ in feature embedding modules are updated every 50 epochs. We empirically found that the best performance is achieved in case of $\\lambda = \\omega = 0.8$, $a_1 = 10$, $a_2 = 1e5$, and $a_3 = 1e4$. To avoid the excessive displacements and color variations, in $L_{pLap}(\\rho)$, $p_1$ is set to the first quartile of $A_\\rho$, $\\gamma_1$ is set to 0.1, and $\\gamma_2$ is 1. Similarly, in $L_{pLap}(D)$, $p_1$ is the median of $A_D$, and $\\gamma_1 = 0.1$, $\\gamma_2 = 20$. The lengths of latent codes $Q_{lbs}$, $Q_D$, $Q_{\\rho}$ and $Q_{render}$ are set to 10, 10, 10 and 20, respectively.\nEvaluation Metrics. In the experiments, we fit the hand mesh representations to multi-view images sequence for single scene. For fair comparison, we employ the same evaluation\nTo investigate the efficacy of our proposed XHand, we treat the subdivided MANO model [9] with vertex albedo as our baseline, which has the merits of the efficient explicit representation. Moreover, we compare our model against sev- eral rigged hand expression methods, including LISA [16], HandAvatar [27], HandNeRF [26], and LiveHand [8]. For fair comparison, LiveHand is re-trained with the same setting and LISA is reproduced by [8].\nWe firstly perform the quantitative evaluation on render- ing quality, as shown in Table I. The evaluation metrics of LISA [16] are adopted from LiveHand [8] and the results of HandNeRF [26] are obtained from their original paper. It can be seen that our proposed XHand approach achieves the best results with a PSNR of 34.3dB. Our baseline drives a textured MANO model through LBS weights. Due to lacking the ability to handle illumination changes across different scenes and poses, there exist some artifacts with a PSNR of 28.6dB. NeRF-based methods [8], [16], [26], [27] present the competitive PSNR results, which rely on MANO mesh without fine-grained geometry during rendering. By taking advantage of fine-grained meshes estimated by XHand, our method outperforms the previous approaches using volumetric repre- sentation in terms of the rendering quality. Benefiting from our design, XHand achieves 56 frames per second (FPS) on inference. Specifically, the feature embedding modules require 0.7 milliseconds, inverse rendering requires 15 milliseconds and the neural rendering module needs 0.1 milliseconds."}, {"title": "D. Ablation Study", "content": "Table II shows the results on DeepHandMesh dataset. Our method outperforms the annotated MANO mesh [9] and DHM [19] by 3.3 mm and 1.2 mm on P2S. This indicates that our proposed feature embedding module can accurately cap- ture the underlying hand mesh deformation comparing to the encoder-decoder scheme in DHM. More experimental results conduct on the DeepHandMesh [19] dataset are visualized in Fig. 7.\nFor better illustration, Fig. 6 shows the more detailed comparisons of rendering and geometry on InterHand2.6M test split. Due to the limited expressive capability, it is hard for the baseline MANO model [9] to capture muscle details varying across different poses. Although the hand meshes generated by HandAvatar [27] have more details than MANO, they are still smoothing compared to ours. In terms of geometry, our method exhibits more prominent skin wrinkles based on different poses. The NeRF-based method HandNeRF [26] and Live- Hand [8] yield the competitive render results, while they still rely on the MANO model and cannot obtain fine-grained hand geometry. On the contrary, our approach effectively presents an accurate hand representation by taking advantage of the feature embedding module and the topological consistent mesh model, resulting in enhanced rendering quality and geometry quality.\nWe perform extensive ablation experiments on the Inter- Hand2.6M dataset test set to validate the contributions of various modules and settings within our framework. First, we aim to demonstrate the performance improvements achieved by our proposed feature embedding module and part-aware Laplace smoothing strategy, consistent with our design inten- tions for the fusion modules. Second, we intend to showcase the robust performance of our XHand model across different numbers of views, highlighting its effectiveness even with limited viewpoints. Furthermore, we conduct a comparative analysis of various neural rendering networks. Based on this evaluation, we have chosen MLPs to enhance both the in- ference speed and the rendering quality, ensuring efficient and high-fidelity output. The following sections detail these ablation experiments and analyze the results comprehensively.\nAblation Study on Different Components. In the first row of Fig. 10, it can be seen that our method significantly highlights skeletal movements and skin changes. Moreover, our design resolves the issue of lighting variations. Our proposed part-aware Laplacian regularization effectively reduces the surface artifacts without sacrificing the details. The feature embedding modules are able to guide the learning of hand avatars by dis- tinguishing average features and pose features, which enhance the reconstruction accuracy.\nTable III shows that the level of mesh detail significantly affects image quality. The rendering results are substantially enhanced through feature embedding. The part-aware Lapla- cian regularization yields more realistic geometric results, indirectly improving the accuracy of the neural render. Fur- thermore, the Position Encoder in neural rendering leads to better image quality.\nAblation Study on Number of Views. Typically, the perfor- mance of each model is improved along with the increasing number of input images, particularly for the NeRF-based"}, {"title": "Choices of Neural Rendering", "content": "methods. Also, insufficient training data may lead to the re- construction failure. We conducted ablation experiments using different numbers of views as inputs. As shown in Table IV, we trained the model on sequences of 1, 5, 10, 20 and 30 views to demonstrate the impact of views. Despite being trained with a limited number of viewpoints, including as few as a single viewpoint, our method effectively captures the hand articulations. Furthermore, we achieve the competitive results in case of more than 10 input views.\nTraditional neural radiance fields [20", "62": "which leads to promising performance. To explore this, we conduct ablation experiments on both network architectures, as detailed in Table V. These experimental results demonstrate that a UNet with 4 layers achieves superior rendering quality, albeit at the expense of inference speed. In comparison to UNet, MLPs can enhance performance by 20% with only a marginal loss in accuracy. Therefore, we have chosen to employ MLPs as our neural renderer. Furthermore, our investigation into a well-designed image generation network, EG3D [61"}]}