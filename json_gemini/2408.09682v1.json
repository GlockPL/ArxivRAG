{"title": "Simulating Field Experiments with Large Language Models", "authors": ["Yaoyu Chen", "Yuheng Hu", "Yingda Lu"], "abstract": "Prevailing large language models (LLMs) are capable of human responses simulation through its unprecedented content generation and reasoning abilities. However, it is not clear whether and how to leverage LLMs to simulate field experiments. In this paper, we propose and evaluate two prompting strategies: the observer mode that allows a direct prediction on main conclusions and the participant mode that simulates distributions of participants' responses. Using this approach, we examine fifteen well-cited field experimental papers published in INFORMS and MISQ, finding encouraging alignments between simulated experimental results and the actual results in certain scenarios. We further identify topics of which LLMs underperform, including gender difference and social norms related research. Additionally, the automatic and standardized workflow proposed in this paper enables the possibility of a large-scale screening of more papers with field experiments.\nThis paper pioneers the utilization of large language models (LLMs) for simulating field experiments, presenting a significant extension to previous work which focused solely on lab environments. By introducing two novel prompting strategies\u2014observer and participant modes\u2014we demonstrate the ability of LLMs to both predict outcomes and replicate participant responses within complex field settings. Our findings indicate a promising alignment with actual experimental results in certain scenarios, achieving a stimulation accuracy of 66% in observer mode. This study expands the scope of potential applications for LLMs and illustrates their utility in assisting researchers prior to engaging in expensive field experiments. Moreover, it sheds light on the boundaries of LLMs when used in simulating field experiments, serving as a cautionary note for researchers considering the integration of LLMs into their experimental toolkit.", "sections": [{"title": "Introduction", "content": "Field experiments allow researchers to manipulate variables of interest in a real-world setting, facilitating the establishment of causal relationships between interventions and outcomes, a key advantage over more controlled but less realistic lab studies. Moreover, unlike lab studies that take place in controlled environments, field experiments are conducted in natural settings, which significantly enhances the relevance and applicability of the findings to actual economic phenomena, policies, and decision-making, making the results from the experiments more generalizable to real-world contexts. Given these advantages, field experiments have become increasingly popular over the years in both academia and industry (Dennis & Valacich, 2001).\nThe rapid advancements in Generative AI, particularly with the release of sophisticated Large Language Models (LLMs), have led scholars to explore the potential of these models for simulating human responses and behaviors, thus gaining novel insights in fields such as psychology (Aher et al., 2023), sociology (Manning et al., 2024), and economics (Horton, 2023). Specifically, these simulations involve LLMs as proxy participants in lab experiments, where the models are tasked with emulating human responses. Since field experiments are considered methodologically complementary to lab experiments, the feasibility of field experimental simulations is also worth exploring."}, {"title": "Method", "content": "Our process involves three steps, as illustrated in Figure 1. First, we select the top 40 most cited papers from the area of marketing and information systems in the INFORMS portal that contain \"field experiment\" in either the titles or abstracts, published between 2018 to 2024. We then manually examine these papers to ensure they are relevant to field experiments. As a result, 15 of these papers are qualified, as shown in Table 1.\nNext, we need to extract information from these papers that characterize the field experiments. We rely on Elicit.org, a fine-tuned LLM trained on academic papers, to extract dependent variables, theoretical framework, duration, intervention, intervention effects, population characteristics, and main findings using. We also manually examine the extracted information to ensure its validity."}, {"title": "Prompting for Observer Mode", "content": "In observer mode, given key experimental settings as inputs, the LLM acts as an observer to predict the main conclusions of field experiments. We design a prompting strategy (Figure 2), which is relies on the information automatically extracted from step 2 (Figure 1). Specifically, key components from A to F and the question section at the bottom are populated using the extracted information, which obviously varies across different experiments. For example, for \"C. Dependent Variables:\" this line is directly available from Elicit.org, whereas \"D. Participants Information:\" this line is also completed with \"Population characteristics\" collected previously. Following these settings, a question section asks the LLM to predict the main conclusions of existing papers in a specified manner.\nSometimes not all information required in this template is directly available from the feature extraction by Elicit.org. For example, \u201cA. The General Goal\" is unavailable from the extracted features. By including a series of key components such as treatments, dependent variables, and titles within the prompting template shown in Figure 3, the LLM can summarize the general goal of the corresponding paper according to the required format. Therefore, additional natural language processing is necessary to properly format the content to be added to the template, which is done by standalone GPT-4-turbo sessions.\nIn the question part of observer template, since the main findings from selected papers are extracted in step 2 of Figure 1, each question included in this part of the template corresponds to one of the main findings. As a result, the total number of questions in one observer prompting equals to the number of main findings of a paper. To test if the LLM can accurately predict the conclusions of field experiments, each finding is rephrased into two variants, reversed and non-related, by separate LLM sessions."}, {"title": "Prompting for Participant Mode", "content": "In this mode, the LLM acts as a participant in a field experiment, which receives instructions and makes choices exactly as a human participant would do in the actual field experiments. The method of instructing participants and the related experiment settings that participants should be aware of are extracted from the previous step, similar to those in observer mode. In the end, variants of instruction promptings from the second-person perspective are generated by the LLM using the generation prompting in Figure 5. Although each field experiment has a specific design, there are variants because different participants receive different treatments. Additionally, there is a need to profile participants since some studies focus on the impact of other factors, such as gender, resulting in more variants. For example, in simulating the first paper listed on Table 1, \"Do women avoid salary negotiations? Evidence from a large-scale natural field experiment.\" (Leibbrandt et al., 2015), at least two variants of synthetic participants (male and female) need to be generated since the paper focuses on the gender difference of the treatment effect. After instruction prompts for participant mode are generated, they are concatenated with questions that participants are expected to answer in the field experiment, for example, \"After the treatment, do you want to buy an additional unit of the product?\""}, {"title": "Results", "content": "After completing the workflow in Figure 1, we apply our framework to each and every paper. With observer mode, each paper has one prompting generated according to the template in Figure 2. With the participant mode, each paper has at least two participant promptings since field experiments naturally divide participants into at least two groups. In different groups, participants experience different scenarios and receive different treatment. If the paper considers additional factors, such as gender, there could be more than two variants. For instance, assigning female and male participants into two groups could result in four variants of promptings.\nWe use OpenAI's GPT-4-turbo API to obtain simulation results and record the responses. API parameters, including seed and temperature, are set to default. In observer mode, we prompt each paper 30 times, with each response containing predictions on all hypotheses of that paper. For participant mode, since there are at least two variants of prompting, we repeat each variant five times. Each response is a direct answer from participants, such as a decision on whether to buy a product or not, representing a data point collected from an experiment."}, {"title": "Data Memorization", "content": "Data memorization is a common concern in simulating experiments with LLMs. If the results given by the LLM are from its memory of training data instead of reasoning, the proposed idea has no instructional value as pilot testing for field experiments (Horton, 2023). As revealed in its documents, the training data cutoff of GPT-4-turbo-2024-04-09 is December 2023. Therefore, we test observer and participant modes on five papers published in INFORMS in 2024, listed in Table 4. It is important to note that we check all five papers to make sure they or their preprint versions were not publicly available before December 2023. Therefore, it is less likely they were included in OpenAI's training data. While the results of the former 15 papers are 66% (observer) and 47.9% (participant), the results of the new five papers are 60.2% (observer) and 66.7% (participant), according to Table 5 and Table 6. By comparison, we show that the proposed approach works in new papers that are not likely to appear in the training data, likely mitigating the potential threat of data memorization to our research."}, {"title": "Discussion and Future Work", "content": "This study makes several significant contributions to the field. First, we expand the existing literature on LLMs' emergent capabilities by demonstrating, for the first time, that LLMs have the potential to simulate field experiments. To the best of our knowledge, this is the first work in literature that considers simulating field experiments that require a more complex environment setting and workflow design compared to the simulation of lab studies. Second, we introduce two novel prompting strategies: observer model, which directly predicts the main hypotheses based on key experimental settings, and participant model, which engages the LLM as a virtual participant in the experiment. Our third contribution lies in the fact that we can identify the potential boundaries within which LLMs work well for field experimental simulations. For example, we show that LLMs can correctly predict most conclusions of field experiments that are not based on gender differences or social norms. Finally, our framework enables researchers to conduct innovative field experimental simulations by configuring the necessary settings, as demonstrated in the 'optional' section of Figure 1.\nDespite these contributions, our current approach has several limitations. One issue for observer mode is that, although we empirically conclude a few scenarios are infeasible for LLM-based simulation, the boundary between the feasible experimental scenarios and the infeasible is still unclear, impairing our approach's fidelity as pilot testing for field experiments. In participant mode, flaws in the automated prompt generation process may lead to incomplete or inaccurate simulated responses, hindering the verification of certain experimental conclusions. Additionally, the relatively low accuracy of participant mode diminishes the credibility of our proposed approach. This reduced accuracy may stem from the previously mentioned issues with prompt generation or inherent limitations in the LLM's ability to handle certain topics, such as those related to gender disparities."}]}