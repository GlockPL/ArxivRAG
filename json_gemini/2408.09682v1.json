{"title": "Simulating Field Experiments with Large Language Models", "authors": ["Yaoyu Chen", "Yuheng Hu", "Yingda Lu"], "abstract": "Prevailing large language models (LLMs) are capable of human responses simulation through its unprecedented content generation and reasoning abilities. However, it is not clear whether and how to leverage LLMs to simulate field experiments. In this paper, we propose and evaluate two prompting strategies: the observer mode that allows a direct prediction on main conclusions and the participant mode that simulates distributions of participants' responses. Using this approach, we examine fifteen well-cited field experimental papers published in INFORMS and MISQ, finding encouraging alignments between simulated experimental results and the actual results in certain scenarios. We further identify topics of which LLMs underperform, including gender difference and social norms related research. Additionally, the automatic and standardized workflow proposed in this paper enables the possibility of a large-scale screening of more papers with field experiments.\n\nThis paper pioneers the utilization of large language models (LLMs) for simulating field experiments, presenting a significant extension to previous work which focused solely on lab environments. By introducing two novel prompting strategies\u2014observer and participant modes\u2014we demonstrate the ability of LLMs to both predict outcomes and replicate participant responses within complex field settings. Our findings indicate a promising alignment with actual experimental results in certain scenarios, achieving a stimulation accuracy of 66% in observer mode. This study expands the scope of potential applications for LLMs and illustrates their utility in assisting researchers prior to engaging in expensive field experiments. Moreover, it sheds light on the boundaries of LLMs when used in simulating field experiments, serving as a cautionary note for researchers considering the integration of LLMs into their experimental toolkit.", "sections": [{"title": "Introduction", "content": "Field experiments allow researchers to manipulate variables of interest in a real-world setting, facilitating the establishment of causal relationships between interventions and outcomes, a key advantage over more controlled but less realistic lab studies. Moreover, unlike lab studies that take place in controlled environments, field experiments are conducted in natural settings, which significantly enhances the relevance and applicability of the findings to actual economic phenomena, policies, and decision-making, making the results from the experiments more generalizable to real-world contexts. Given these advantages, field experiments have become increasingly popular over the years in both academia and industry (Dennis & Valacich, 2001).\n\nThe rapid advancements in Generative AI, particularly with the release of sophisticated Large Language Models (LLMs), have led scholars to explore the potential of these models for simulating human responses and behaviors, thus gaining novel insights in fields such as psychology (Aher et al., 2023), sociology (Manning et al., 2024), and economics (Horton, 2023). Specifically, these simulations involve LLMs as proxy participants in lab experiments, where the models are tasked with emulating human responses. Since field experiments are considered methodologically complementary to lab experiments, the feasibility of field experimental simulations is also worth exploring.\n\nThe paper presents a framework powered by GPT-4 that conducts experimental simulation in two modes, namely, observer mode and participant mode. Specifically, the framework takes field experimental settings in two alternatives as an input: either user-provided inputs or extracted data from existing papers. Whereas the former approach allows users to input their desirable settings to execute novel field experimental simulations, the latter way extracts settings of a field experiment from existing paper by Elicit.org, a LLM specialized in comprehending academic texts. Once the key settings are obtained through either method, the framework proceeds to simulate the field experiment in both observer and participant modes. In observer mode, we prompt the LLM to observe a target field experiment, gathering information about its configurations and settings and predicting the main conclusions. In participant mode, we prompt the LLM with different profiles, and the LLM then role-plays a participant in the experiment, receiving instructions and treatments from organizers and answering questions as requested. In other words, observer mode directly predicts the main conclusions, while participant mode generates simulated data distributions that could reflect the main conclusions.\n\nWe employ these two strategies on GPT-4 to simulate 15 field experiments adopted from existing marketing and information system literature. Our results show that observer mode is quite promising, successfully replicating over 66% of field experiments. By contrast, participant mode of GPT-4 achieved only 47.9%. Moreover, we find that results are highly skewed because of topic sensitivity. In particular, the LLM performs poorly in field experiments concerning"}, {"title": "Prior Literature", "content": "Driven by their unprecedented reasoning, inference, and content-generation abilities, Large Language Models (LLMs) have found widespread application across both industry and academia. Among the various LLMs, the GPT (Generative Pre-trained Transformer) series by OpenAI stands out as one of the most prominent and widely used models.\n\nExisting research suggests that LLMs can simulate how individuals behave in various economics or behavioral economics studies, particularly using LLMs to simulate participants in lab experiments (Aher et al., 2023; Horton, 2023; Leng & Yuan, 2024; Manning et al., 2024). Aher et al. (2023) and Horton (2023) follow a two-step process, which includes profiling agents and prompting experimental backstories, Leng and Yuan (2024) advance the framework by an interaction mechanism between agents to allow a more complex experimental setting. Recently, Manning et al. (2024) propose an automated framework that includes the aforementioned features for lab experimental simulation of scenarios regarding between-human interactions.\n\nNotably, all existing LLM-powered experimental simulations are conducted for lab experiments. Lab experiments, however, are necessarily limited in relevance for predicting field behaviors (Harrison & List, 2004). By contrast, natural field experiments offer distinct advantages, such as improved econometric robustness and more effective control over participant recruitment. Promoting the adoption of behavioral field experiments has been a general shift, as evidenced by IS journals like Management Science (Gneezy, 2017). Moreover, extant studies merely propose prompting strategies and require the design of specific prompts for each laboratory experimental simulation (Aher et al., 2023; Horton, 2023; Leng & Yuan, 2024). By contrast, our novel framework, comprising observer and participant mode, introduces a unified approach to systematically examine the LLM's emergent abilities in simulating field experiments."}, {"title": "Method", "content": "Our process involves three steps, as illustrated in Figure 1. First, we select the top 40 most cited papers from the area of marketing and information systems in the INFORMS portal that contain \"field experiment\" in either the titles or abstracts, published between 2018 to 2024. We then manually examine these papers to ensure they are relevant to field experiments. As a result, 15 of these papers are qualified, as shown in Table 1.\n\nNext, we need to extract information from these papers that characterize the field experiments. We rely on Elicit.org, a fine-tuned LLM trained on academic papers, to extract dependent variables, theoretical framework, duration, intervention, intervention effects, population characteristics, and main findings using. We also manually examine the extracted information to ensure its validity."}, {"title": "Prompting for Observer Mode", "content": "In observer mode, given key experimental settings as inputs, the LLM acts as an observer to predict the main conclusions of field experiments. We design a prompting strategy (Figure 2), which is relies on the information automatically extracted from step 2 (Figure 1). Specifically, key components from A to F and the question section at the bottom are populated using the extracted information, which obviously varies across different experiments. For example, for \"C. Dependent Variables:\" this line is directly available from Elicit.org, whereas \"D. Participants Information:\" this line is also completed with \"Population characteristics\" collected previously.\n\nFollowing these settings, a question section asks the LLM to predict the main conclusions of existing papers in a specified manner.\n\nSometimes not all information required in this template is directly available from the feature extraction by Elicit.org. For example, \u201cA. The General Goal\" is unavailable from the extracted features. By including a series of key components such as treatments, dependent variables, and titles within the prompting template shown in Figure 3, the LLM can summarize the general goal of the corresponding paper according to the required format. Therefore, additional natural language processing is necessary to properly format the content to be added to the template, which is done by standalone GPT-4-turbo sessions.\n\nIn the question part of observer template, since the main findings from selected papers are extracted in step 2 of Figure 1, each question included in this part of the template corresponds to one of the main findings. As a result, the total number of questions in one observer prompting equals to the number of main findings of a paper. To test if the LLM can accurately predict the conclusions of field experiments, each finding is rephrased into two variants, reversed and non-related, by separate LLM sessions.\n\nSpecifically, as depicted in Figure 4, after a main finding is prompted to the LLM, it follows instructions to deconstruct the content and reorganize components into two variants of the original conclusion. The reversed variant means that the direction of the conclusion is inverted, while the non-related variant typically indicates that there is no causal relationship between the entities of interest. The original conclusion and the two variants are then included as a question in the"}, {"title": "Prompting for Participant Mode", "content": "In this mode, the LLM acts as a participant in a field experiment, which receives instructions and makes choices exactly as a human participant would do in the actual field experiments. The method of instructing participants and the related experiment settings that participants should be aware of are extracted from the previous step, similar to those in observer mode. In the end, variants of instruction promptings from the second-person perspective are generated by the LLM using the generation prompting in Figure 5. Although each field experiment has a specific design, there are variants because different participants receive different treatments. Additionally, there is a need to profile participants since some studies focus on the impact of other factors, such as gender,\n\nresulting in more variants. For example, in simulating the first paper listed on Table 1, \"Do women avoid salary negotiations? Evidence from a large-scale natural field experiment.\" (Leibbrandt et al., 2015), at least two variants of synthetic participants (male and female) need to be generated since the paper focuses on the gender difference of the treatment effect. After instruction prompts for participant mode are generated, they are concatenated with questions that participants are expected to answer in the field experiment, for example, \"After the treatment, do you want to buy an additional unit of the product?\""}, {"title": "Results", "content": "After completing the workflow in Figure 1, we apply our framework to each and every paper. With observer mode, each paper has one prompting generated according to the template in Figure 2. With the participant mode, each paper has at least two participant promptings since field experiments naturally divide participants into at least two groups. In different groups, participants experience different scenarios and receive different treatment. If the paper considers additional factors, such as gender, there could be more than two variants. For instance, assigning female and male participants into two groups could result in four variants of promptings.\n\nWe use OpenAI's GPT-4-turbo API to obtain simulation results and record the responses. API parameters, including seed and temperature, are set to default. In observer mode, we prompt each paper 30 times, with each response containing predictions on all hypotheses of that paper. For participant mode, since there are at least two variants of prompting, we repeat each variant five times. Each response is a direct answer from participants, such as a decision on whether to buy a product or not, representing a data point collected from an experiment."}, {"title": "Results of Observer Mode", "content": "The results of observer mode are shown in Table 2. Each paper contains between 2 to 6 main conclusions. We measure the accuracy of predictions by the LLM in the simulations using the following formula: $Accuracy (o) = N(predicted conclusion = actual conclusion) / 30$, where the numerator indicates the number of correct predictions, and the denominator 30 indicates the number of times observer mode template (Figure 2) was prompted to the LLM. As a result, \u201c1\u201d means that the LLM could always choose the correct conclusions in all 30 trials. Conceivably, $Avg (o)$ is the average of observer mode's accuracies of a paper: $Avg (o) = \\frac{21st conclusion}{ Number of conclusions} accuracy (o)$After checking the key words of the 15 papers and comparing them with the results in Table 2, we find that the result is skewed and sensitive to topics. Whereas the LLM achieves high simulation accuracy in topics such as willingness-to-pay (the 12th and 14th papers) and privacy (11th and 14th papers), it performs poorly on the following topics: gender difference (the 2nd paper), popularity information (the 4th paper), humanizing customer service chatbots (the 10th paper), and reciprocity (the 13th paper). For the gender-related topic, existing research has found there is a substantial gender bias in earlier versions of GPT-4 (Zhao et al., 2024). Popularity information, humanizing bots, and reciprocity are all related to social norms. Even the most advanced LLMs cannot achieve a human-level understanding of social norms (Yuan et al., 2024). We argue that a lack of awareness of social pressures could possibly result in deviations of simulated experimental results from actual results in the topics of popularity information and reciprocity. This issue has the potential to improve as researchers are enhancing the awareness of social norms in LLMs."}, {"title": "Results of Participant Mode", "content": "We prompt each variant of a field experiment five times to the LLM in separate sessions. For example, if the experiment has only one treatment and one control group, then each group will be prompted five times, as if there are five participants from the treatment group and five participants from the control group, totaling ten participants in the experiment. After we prompt them to LLM, we calculate the average treatment effect. If the average treatment effect's direction is the same as the direction of the actual conclusion, we mark it as 1, otherwise 0. It is worth mentioning that at the current stage, we can only simulate the direction of the treatment effect, not the size of the effect. For example, for a typical conclusion that training increases income by 15%, we are able to verify \"increase\" instead of \"15%\". Finally, we use \"x\" to represent that collected responses from the participant mode could not confirm or reject a conclusion. This may result from flaws in our participants' prompting generation or the design of the original paper. An example of the latter case is that a paper runs machine learning over field experimental results to get some conclusions. Simulated field experimental responses can neither confirm nor deny machine learning conclusions without running through the process.\n\nWe observe that participant mode results are notably less favorable compared to those of observer mode. We posit that the limitation does not stem from the LLM's inherent capability to simulate field experiments. In the automatic generation of participant mode promptings, converting the third-person perspective description of the workflow extracted from papers to the second-person perspective instructions for participants results in the loss of many details, leading to deviations from the real field experiment environment. Given these incomplete and altered experimental conditions, it is unsurprising that the LLM's responses diverge from those expected of actual human participants. Moreover, the process of generating questions for participant mode currently necessitates substantial human intervention, further complicating the simulation. Ongoing refinement of our methods is essential and will ultimately address these challenges, enhancing the accuracy and reliability of participant mode simulations."}, {"title": "Data Memorization", "content": "Data memorization is a common concern in simulating experiments with LLMs. If the results given by the LLM are from its memory of training data instead of reasoning, the proposed idea has no instructional value as pilot testing for field experiments (Horton, 2023). As revealed in its documents, the training data cutoff of GPT-4-turbo-2024-04-09 is December 2023. Therefore, we test observer and participant modes on five papers published in INFORMS in 2024, listed in Table 4. It is important to note that we check all five papers to make sure they or their preprint versions were not publicly available before December 2023. Therefore, it is less likely they were included in OpenAI's training data. While the results of the former 15 papers are 66% (observer) and 47.9% (participant), the results of the new five papers are 60.2% (observer) and 66.7% (participant), according to Table 5 and Table 6. By comparison, we show that the proposed approach works in new papers that are not likely to appear in the training data, likely mitigating the potential threat of data memorization to our research."}, {"title": "Discussion and Future Work", "content": "This study makes several significant contributions to the field. First, we expand the existing literature on LLMs' emergent capabilities by demonstrating, for the first time, that LLMs have the potential to simulate field experiments. To the best of our knowledge, this is the first work in literature that considers simulating field experiments that require a more complex environment setting and workflow design compared to the simulation of lab studies. Second, we introduce two novel prompting strategies: observer model, which directly predicts the main hypotheses based on key experimental settings, and participant model, which engages the LLM as a virtual participant in the experiment. Our third contribution lies in the fact that we can identify the potential boundaries within which LLMs work well for field experimental simulations. For example, we show that LLMs can correctly predict most conclusions of field experiments that are not based on gender differences or social norms. Finally, our framework enables researchers to conduct innovative field experimental simulations by configuring the necessary settings, as demonstrated in the 'optional' section of Figure 1."}, {"title": "Despite these contributions, our current approach has several limitations. One issue for observer mode is that, although we empirically conclude a few scenarios are infeasible for LLM-based simulation, the boundary between the feasible experimental scenarios and the infeasible is still unclear, impairing our approach's fidelity as pilot testing for field experiments. In participant mode, flaws in the automated prompt generation process may lead to incomplete or inaccurate simulated responses, hindering the verification of certain experimental conclusions. Additionally, the relatively low accuracy of participant mode diminishes the credibility of our proposed approach. This reduced accuracy may stem from the previously mentioned issues with prompt generation or inherent limitations in the LLM's ability to handle certain topics, such as those related to gender disparities.", "content": "Despite these contributions, our current approach has several limitations. One issue for observer mode is that, although we empirically conclude a few scenarios are infeasible for LLM-based simulation, the boundary between the feasible experimental scenarios and the infeasible is still unclear, impairing our approach's fidelity as pilot testing for field experiments. In participant mode, flaws in the automated prompt generation process may lead to incomplete or inaccurate simulated responses, hindering the verification of certain experimental conclusions. Additionally, the relatively low accuracy of participant mode diminishes the credibility of our proposed approach. This reduced accuracy may stem from the previously mentioned issues with prompt generation or inherent limitations in the LLM's ability to handle certain topics, such as those related to gender disparities."}]}