{"title": "How Mature is Requirements Engineering for AI-based Systems? A Systematic Mapping Study on Practices, Challenges, and Future Research Directions", "authors": ["Umm-e-Habiba", "Markus Haug", "Justus Bogner", "Stefan Wagner"], "abstract": "Artificial intelligence (AI) permeates all fields of life, which resulted in new challenges in requirements engineering for artificial intelligence (RE4AI), e.g., the difficulty in specifying and validating requirements for AI or considering new quality requirements due to emerging eth- ical implications. It is currently unclear if existing RE methods are sufficient or if new ones are needed to address these challenges. Therefore, our goal is to provide a comprehensive overview of RE4AI to researchers and practitioners. What has been achieved so far, i.e., what practices are available, and what research gaps and challenges still need to be addressed? To achieve this, we conducted a systematic mapping study combining query string search and extensive snowballing.", "sections": [{"title": "1 Introduction", "content": "In the past decade, the need for automation and intelligence has led to huge advancements in machine learning (ML) and artificial intelligence (AI) [1]. De- spite substantial growth in AI-based systems, we continue to see significant challenges and project failures [2]. Weiner [3] stated that according to recent data, 87% of AI projects do not make it into production, meaning that most projects are never deployed. One primary reason is that AI has disrupted tra- ditional software development practices, which are typically deductive, where requirements are explicitly defined and translated into code. In contrast, AI- based systems, i.e., systems that incorporate AI components [4], are developed inductively, as they learn and adapt from training data. This shift in approach makes it challenging to anticipate and understand the behavior of AI-based systems. Due to the critical role of AI-based systems, software engineering (SE) and the AI community must collaborate to develop new strategies to address these issues.\nRequirements engineering (RE), the systematic handling of software re- quirements [5], has been impacted by the complexities of AI-based systems [6]. Existing RE approaches are challenging to apply to AI-based systems because of their probabilistic nature and the need for constant adaptation. To ad- dress these challenges, RE needs to evolve to be compatible with AI-based systems [7]. The roles and responsibilities related to RE have changed, with data scientists now being responsible for specifying high-level requirements in ML systems, which can lead to systems that prioritize data quality over stake- holder requirements [8]. Hence, software engineers and data scientists must work together to address issues arising from the combination of AI and SE [9]. Kondermann [10] claimed that despite its long history, RE has yet to be used extensively for AI, especially computer vision, and called for more research to"}, {"title": "2 Related Work", "content": "With the recent growth of AI-based systems, the number of empirical studies on RE for AI is increasing, with more and more different aspects being investi- gated. There exist secondary studies that highlighted research in the field of SE for AI-based systems [4, 13-16]. However, these studies are not exclusively fo- cused on the RE process but consider it only as a part of SE. In relation to RE, we identified three secondary studies on RE for AI/ML [17\u201319] that are closely"}, {"title": "3 Research Design", "content": "We conducted this systematic mapping study by following the guidelines of Petersen et al. [21]. In the following, we outline our research questions (RQs) and their rationales, how we obtained articles, and how we addressed them systematically."}, {"title": "3.1 Research Questions and Rationales", "content": "To define the scope of our study, we formulated RQs that highlight the cate- gorization of the literature in a way that can be interesting for scholars and practitioners, while also providing insights into how RE approaches have been used for AI-based system development.\nRQ1: Where and when have RE practices for AI-based systems been published?\nWe aim to explore the recent trends in this research area in terms of the publication year of articles, and how active this area has been. For this purpose, we analyze the study distribution by year of publication and the most preferred venues for RE4AI.\nRQ2: How are the different RE topics distributed within the literature on AI-based systems?\nWe intend to use the SWEBOK [11] classification scheme to assign RE categories to our primary studies. The most recently published version of SWEBOK V3.0 has 8 knowledge areas. Using the RE knowledge area and its categories, we analyze frequently studied RE topics, as well as topics that may require further attention.\nRQ3: What is the maturity of the research in this area?\nTo evaluate the maturity of this area, we analyze the primary studies according to the criteria given below:\n\u2022 We use Wieringa et al.'s [12] classification scheme for RE publications."}, {"title": "3.2 Research Protocol", "content": "To execute an impartial, objective investigation, a research protocol is required. It manages the flow of research and maximizes the study's valuable findings. We created a research protocol that describes the parts of the study and is depicted in Fig. 1. The main steps of the research protocol are as follows.\n1. Search query formulation:\nWe formulated our search query using the first two elements of the PICO criteria [21]. The first element is the Population (P), which in- dicates RE publications. The second element is Intervention (I), which specifies AI, where ML and deep learning (DL) are a part of AI. We ex- cluded the Comparison(C) and Outcome (0) criteria from our search to broaden its scope, allowing us to capture a wider range of studies, which is especially useful for exploratory research or obtaining a comprehensive field overview as in an SMS. We constructed our query iteratively and restricted our search to article titles to achieve the best results. Initially, we evaluated four digital libraries ACM Digital Library, IEEE Xplore, ScienceDirect, and Springer for our search. Our findings indicated that IEEE Xplore and ACM Digital Library were most effective in handling our search query. Concerns may arise regarding the exclusivity of our selec- tion potentially overlooking relevant studies, particularly from Springer. To address this, we incorporated a snowballing technique, systematically reviewing references from our initial findings to ensure no significant work was overlooked. Alongside these selected digital libraries, we expanded our search to include meta-search engines, specifically Google Scholar and Web of Science, focusing on article titles to refine our results. This com- prehensive approach, combining direct searches with snowballing, was"}, {"title": "", "content": "designed to ensure thorough coverage of the literature. The finalized search string is given below:\n(\"requirement\" OR \"requirements\")\nAND\n(\"AI\" OR \"artificial intelligence\u201d OR \"ML\" OR \"machine learning\" OR \"DL\" OR \"deep learning\")\nThis query resulted in 123 articles from ACM Digital Library and 260 from IEEEXplore, while meta-search engines such as Web of Science re- turned 452 and Google Scholar 955 articles. In total, we obtained 1790 papers.\n2. Removal of duplicates:\nIn this step, we removed duplicated articles as we ran our query on two digital libraries and two meta-search engines. After the removal of duplicates, we are left with 795 papers.\n3. Inclusion/exclusion criteria:\nOur query yielded literature that included all keywords in their title. To rectify the literature according to the scope of our study, we developed inclusion (IC) and exclusion criteria (EC).\n(a) EC1: Not relevant to the scope of the study (i.e., studies that do not focus on RE for AI)\n(b) EC2: Published before 2010 and after July 2023\n(c) EC3: Not written in English\n(d) EC4: Secondary studies\n(e) EC5: Not peer-reviewed / not a scientific paper\n(f) EC6: Not accessible\n(g) IC1: The primary focus of the paper is requirements engineering\n(h) IC2: The paper targets AI-based systems\nDuring our study, we used Rayyan [24] to remove duplicates and applied inclusion/exclusion of articles. Subsequently, after eliminating duplication, we were left with 795 unique papers.\nThe inclusion/exclusion criteria mentioned above were used to refine articles that fit our study scope. EC3 and EC6 were designed to exclude studies that are not written in English and are accessible through any source. At the same time, EC2 is intended to strictly select the publication years considered during the mapping study. To exclude the secondary studies and the grey literature, we apply EC4 and EC5. IC1, IC2, and EC1 required an in-depth study of articles to analyze whether the article fits the scope of the study. The first three authors independently assessed 795 studies using these criteria. Discrepancies in their selections were resolved through discussion and consensus voting. This rigorous process resulted in the selection of 93 articles.\n4. Data extraction:\nWe extracted data from each primary study to answer our research questions described in section 3.1 above. We defined extraction sheets to record the necessary information related to each publication. Having a"}, {"title": "", "content": "specified extraction sheet will reduce the opportunity to include researcher bias. As a result, during data extraction, the researcher extracted data that should answer the research questions.\nBefore we started extracting data, a pilot extraction process was conducted to develop a shared understanding and avoid any confu- sion regarding the extraction process. This pilot ensured that each researcher clearly understood the research questions and respective ex- traction sheets. For this purpose, we selected three initial studies, and each researcher independently extracted data into their sheet. Afterward, we discussed the extracted data and further improved the extraction sheets.\nWe outlined the individual data cells according to each research question. Since each RQ has multiple fields, we maintained a separate spreadsheet for each RQ. RQ1 is primarily focused on studying metadata, including the year of publication, publishing venue, and the involved re- search community (Note: we classified papers based on author affiliations as industry, academic, or collaborative). To identify which RE topics have been covered frequently within the literature (RQ2), we classified the pri- mary studies using the SWEBOK [11] subcategories for RE. Moreover, to judge the maturity of this research area (RQ3), we classified literature according to the RE publication types proposed by Wieringa et al. [12] as well as empirical evaluation method [20].\nWe characterized RE practices in four dimensions: tool, techniques, model, and process. Therefore, RQ4 is designed to capture these de- tails and differentiate between practices that are new or already existing. Finally, we extracted challenges highlighted by different authors and synthesized literature to outline possible research directions (RQ5)."}, {"title": "", "content": "Eventually, we split up the extractions and assigned two researchers to each paper, and after every week, a synchronization meeting was held to discuss extraction as shown in Fig. 2. A separate consensus spread- sheet was maintained where all finalized entries were recorded. Further,"}, {"title": "", "content": "to analyse inter-rater reliability, the agreement level was measured using Cohen's kappa coefficient, which provides a robust statistical measure of inter-rater reliability.\nFor the inclusion/exclusion criteria, the researchers used the rayyan.ai tool and independently performed the inclusion/exclusion of papers. However, there was no conflict found during this process.\nFor the thematic analysis, the researchers evaluated 5 papers with 4 questions each, making a total of 20 evaluations.\n\u2022 Total items evaluated (N): 5 papers x 4 questions = 20\n\u2022 Agreement on all 4 questions for 3 papers: 3 x 4 = 12 agreements\n\u2022 Agreement on 3 questions for 1 paper: 3 agreements\n\u2022 Agreement on 2 questions for 1 paper: 2 agreements\n\u2022 Total agreements (A): 12 + 3 + 2 = 17\n\u2022 Total disagreements (D): 20 - 17 = 3\nCohen's kappa (\u043a) is calculated as follows:\n$\u041a=\\frac{P_o-P_e}{1-P_e}$\nWhere $P_o$ is the observed agreement and $P_e$ is the expected agreement by chance.\n(a) Observed Agreement $P_o$: Number of agreements / Total items evaluated =17/20 = 0.85\n(b) Expected Agreement $P_e$: Assuming equal probability for agreement and disagreement: $P_e$=(Probability of both agreeing)$^2$+(Probability of both disagreeing)\nAgain, assuming equal distribution (0.5 for agreement and 0.5 for disagreement):\n$P_e= (0.5 \u00d7 0.5) + (0.5 \u00d7 0.5) = 0.25 +0.25 = 0.50$\n(c) Cohen's K\u0430\u0440\u0440\u0430 (\u043a):\n$\u041a = \\frac{0.85-0.50}{1-0.50}= \\frac{0.35}{0.50} = 0.70$\nThese kappa values indicate a substantial level of agreement between the researchers, supporting the reliability of the conclusions derived from their analyses.\n5. Snowballing:\nFollowing the first iteration of extractions, we applied forward and backward snowballing according to the guidelines by Wohlin [25]. Snow- balling, which involves using the references of identified papers to find additional relevant studies, can be especially effective in fields where con- sistent terminology is lacking. This approach helped us identify important studies that might be missed due to inconsistent keyword use in database searches. By using snowballing, we ensured a more comprehensive review by capturing relevant research that might not be easy to find through tra- ditional database searches alone. To ensure overall coverage, snowballing iterations were performed until no further studies were included. The first round on the start set of 93 articles yielded 15 additional papers. After"}, {"title": "", "content": "extracting these 15 papers, the second round of snowballing was carried out, which resulted in 15 more articles. We then performed a third itera- tion of snowballing, which yielded 3 more articles. Lastly, snowballing on these 3 articles did not result in additional papers. After this process, we ended with a final set of 126 primary studies.\n6. Data synthesis:\nWe began data analysis and synthesis once extractions had been com- pleted. To categorize the retrieved data, we used both quantitative and qualitative analysis. Some extraction discrepancies and errors were detected throughout this process and were removed. The first author per- formed the synthesis and frequently presented the results to the rest, leading to iterative refinements.\nTo address RQ1, we conducted a frequency analysis to examine bibli- ographical data. For RQ2 and RQ3, we undertook quantitative analyses. Specifically, the analysis for RQ2 categorized literature based on SWE- BOK KAs, whereas RQ3 focused on classifying literature according to Wieringa's [12] framework. Additionally, we identified and analyzed the evaluation methods used for RE practices. To respond to RQ4, we em- ployed a combination of methods, including qualitative analysis through thematic analysis, as recommended by Cruzes et al. [26].\nAs RQ5 is divided into two sub-questions, we conducted a qualitative analysis for both questions. Further, we applied the thematic synthesis approach recommended by Cruzes et al. [26]. We then extracted free text from the papers and labeled the free text. Finally, we identified the most recurrent themes in the next step and assigned them to extracted text. In the following section, we present our data extraction results and their mapping to respective research questions."}, {"title": "4 Results", "content": "After data extraction, we move towards the data analysis phase. In this section, we summarise the results of our mapping study. Starting from RQ1, we systematically present results for each RQ, respectively."}, {"title": "4.1 Bibliometrics (RQ1)", "content": "This RQ covers the publication trends over the years. Based on the earliest published primary study, we see this field emerged in 2017. Although we started our search from the year 2010, we found the first relevant paper in 2017. As expected, there is a growing trend of studies in the field of RE for AI after that, as shown in Fig. 3.\nInitially, the exploration of this area was predominantly undertaken by the academic community. However, with the recent advancements in AI, there has been a noticeable increase in industrial engagement up to 2021. This trend is evidenced by the rise in the number of publications from the industry during that period. Although there was a slight decrease in industrial publications"}, {"title": "4.2 Distribution of RE Topics for AI-based Systems (RQ2)", "content": "Systematic Mapping Studies are typically employed to present a classification scheme for research topics within a specific field of interest. Analyzing the distribution of publications across these topics can provide insights into the breadth and depth of research, indicating the field's scope and its level of maturity.\nTo answer this research question, we used the well-established classifica- tion scheme of SWEBOK [11] for RE topics as shown in Fig. 5. It allows us to observe where RE4AI research has been focused and which topics may still require attention. One paper can be classified under more than one topic, de- pending upon which RE topics they addressed in their research. In addition, we analyzed topics and their sub-categories, such as which sub-topics have been addressed or remained unattended. It can help researchers identify further gaps in the current research landscape. Fig. 6 visualizes our general findings for this RQ."}, {"title": "", "content": "Within our analysis, we identified that 104 studies concentrated on require- ments analysis, marking it as the predominant category in our classification. This category is notable for the introduction of 33 RE practices, detailed in Section 4.4.2. The research work within this realm has primarily explored the integration of conceptual modeling [1], the classification of new (novel) require- ment types [2], and the assimilation of human-centric requirements into ML systems [3, 4].\nMoreover, our review reveals that 87 studies were dedicated to require- ments elicitation, representing the most substantial segment where established practices have been applied, as discussed in Section 4.4.1. This highlights the field's ongoing efforts to refine and utilize traditional RE methodologies within the context of evolving technological frameworks. Furthermore, interviews [5- 7], questionnaires [8], and scenarios have been used as practices in this area [9-11].\nFurther, we found that 77 papers discussed topics related to requirements specification, with 30 of these studies introducing new practices for specifying requirements. It is observed that the recent literature leans towards proposing practices specifically tailored to meet ML-related requirements, emphasizing stakeholder needs [12, 13]. Notably, only three studies adhered to existing practices for requirements specification.\nShifting the focus to the Requirements Validation Knowledge Area, 53 studies were identified that delve into requirements validation, out of which eight introduced novel practices for conducting requirements validation. Thus, aiming to enhance the validation processes in line with AI-based systems. Fig. 6 shows 22 studies focused on practical consideration, whereas 5 studies proposed new practices for practical consideration. These studies highlighted requirements attributes regarding explanatory capabilities, ethical guidelines, and quality characteristics. 16 studies covered RE processes and focused on tailored RE processes for ML-based systems. These processes aim to incor- porate ML-specific needs and additional types of requirements. Few studies highlighted the different perspectives in the business context during the RE process. We can observe that 13 studies proposed new practices for RE pro- cess, where 9 applied existing RE processes to AI-based systems, primarily focusing on Goal-Oriented Requirements Engineering (GORE) [14, 15]. The focus on requirements tools has been relatively limited, with only 4 studies identified in this domain proposing innovative tools to support the RE process for AI-based systems (see Section 4.4.2). These tools are particularly aimed at streamlining tasks related to the elicitation and specification of requirements. In the last category, i.e., software requirements fundamentals. Only 4 studies specifically addressed the topic of requirement definitions.\nThe trends we observed from these statistics suggest a field in transition, grappling with the unique challenges posed by AI and machine learning sys- tems. The use of existing practices in elicitation and modeling points to a reliance on traditional RE strengths. However, the need for new practices, es- pecially in analysis, specification, and the requirements process, suggests that"}, {"title": "4.3 RE4AI Research Maturity (RQ3)", "content": "To address this question, we classified the papers according to the taxonomy by Wieringa et al. [12].\nWe aim to highlight the research methods so far used by researchers in the RE4AI directions and how these practices have been evaluated.\nFig. 7 shows that 74 studies fall into the proposal of solution category, i.e., papers proposing a solution and establishing its relevance. Either the proposed solutions should be novel, or an existing solution should be adapted and ap- plied to a new domain. A new conceptual framework has been proposed in 21 studies, and we classify them as philosophical papers, as some of them do not provide a direct solution but all of them offer a new way of understanding and categorizing requirements.\nPapers that investigate proposed solutions' properties while the solution still requires implementation in RE are classified as validation research. 25 papers are in this category that validate a solution proposed in the same paper or elsewhere. Papers that apply RE techniques in practice or investigate the usage of RE practices are classified as evaluation research. The novelty of the practice is not essential in this case. Instead, the knowledge claim of the paper should be novel. 18 among our primary studies describe the authors' position, primarily to provoke discussions about RE4AI topics. These types of papers are categorized as opinion papers. Lastly, 11 studies reported personal experiences"}, {"title": "", "content": "and were labeled as personal experience. Papers could be classified with more than one of these categories.\nOur analysis (See Fig. 8) reveals a notable trend in research practices. Specifically, we observe that proposal of solution papers predominantly incor- porate validation research, with 20 studies validating solutions and 22 engaging in evaluation research. Additionally, 5 papers combine proposals of solutions with opinion and philosophical discourse, while 3 include personal experi- ences. In validation research, a common pattern emerges where 20 studies both propose and validate solutions, highlighting a preference for self-validation. Philosophical and opinion papers often intertwine, sharing a focus on con- ceptual framework. Evaluation methods vary, with case studies (43) being predominant, followed by surveys (15) and minimal use of mixed methods (1). This reflects a broader inclination towards practical validation in the proposal of a solution, while opinion and experience papers typically lack such research.\nFurther, to assess the maturity of the research, we investigate which type of research is conducted in each SWEBOK \u039a\u0391. Fig. 9 shows how RE SWE- BOK [11] topic are addressed using Wieringa's classification [12]. It should be noted that one paper can be in more than one publication type, so the to- tal adds up to more than 126. This analysis highlights that significant focus has been placed on requirement analysis, elicitation, specification, and valida- tion. However, foundational aspects of requirements, such as their fundamental principles, processes, and practical considerations, have been overlooked. Ad- ditionally, there is a notable shortage of tools to support the development of AI-based systems. The data reveals that while the initial activities of RE re- ceive considerable attention, there is still a deficiency in managing the overall RE process for AI-based systems effectively"}, {"title": "4.4 RE Practices for AI-based Systems (RQ4)", "content": "This question aims to highlight the use of existing RE practices and the di- rection in which new RE practices specific to AI have emerged. We extracted the practices according to the classification scheme provided in Section 3.1."}, {"title": "4.4.1 Usage of Conventional RE Practices for AI", "content": "We analyzed the suitability of current RE practices for AI by determining what RE practices have been used for such systems. The resulting model is shown in Table 2. We group the practices according to the RE topics in SWEBOK [11]. Since we found requirements modeling, which is not part of SWEBOK, to be an important topic among our papers, we included it as a distinct group. Further, we found requirements elicitation, requirements process, requirements validation, requirements analysis, and requirements specification KAs using existing practices of RE. Each paper may have multiple RE practices and could fall into multiple software requirements KAs."}, {"title": "4.4.2 New Practices Employed in RE4AI", "content": "For new practices, Table 3 provides a brief description of each practice and the type of practice, i.e., model, process, technique, or tool. We can observe that numerous studies have proposed new models, while a significant number also introduced new processes. Further, we will elaborate on how each KA has been addressed by researchers."}, {"title": "5 Open Challenges and Future Research Directions (RQ5)", "content": "This section highlights the prevailing challenges in the RE4AI literature and presents future research directions outlined among 126 primary studies. We used the thematic synthesis approach recommended by Cruzes et al. [26] to answer challenges and future directions in RQ5."}, {"title": "5.1 RE Challenges for AI-based Systems", "content": "In this section, we underline the challenges in RE4AI. We identified 27 chal- lenges classified into 9 categories as seen in Fig. 11. In the following subsections, we discuss them one by one."}, {"title": "5.1.1 Requirements Specification", "content": "In the requirements specifications, we encountered the most challenges, categorized into five types:\nHard to Specify Requirements Concretely The necessity for require- ments engineers to adopt new methods to deal with data biases and the"}, {"title": "", "content": "challenge of developing requirements when the data is not yet available high- light the difficulty in specifying requirements concretely for AI systems [121], [64]. The challenge of ensuring that legal regulations and ethical considera- tions are adequately considered requires a shift in perspective towards a data and analytics viewpoint [81]. The complexity of specifying non-functional re- quirements (NFR) on overall ML system performance and the difficulty of rigorously specifying requirements due to a lack of domain knowledge [81], [88]. The challenge of specifying explainability requirements and functionality that depends on input data underscores the difficulty of concretely specifying requirements in AI-based systems [62], [45]. The difficulty of specifying unam- biguous requirements, such as for a pedestrian detector component, further illustrates this challenge [90].\nIncomplete and Incorrect Knowledge: Challenges around less tan- gible characteristics are hard to express meaningfully, leading to overlooked and misconstrued requirements [116]. Incomplete, incorrect, and inconsistent knowledge encompassing missing or insufficient entities, mislabeled entities, and differing labels for the same entity or merged entities, highlighting issues of knowledge integrity [1].\nEmergent Functionality Hard to Specify in Advance: The entangle- ment of requirements where even minor changes can dramatically alter other requirements illustrates the challenge of specifying emergent functionality in advance [56].\nNew Type of Quality Requirements: The explicit specification of ex- plainability as a quality requirement presents a new challenge due to the lack of a systematic and overarching approach [62]. Standard requirements specifica- tion techniques become less applicable in AI-based systems where requirements are informed through training data, indicating a shift towards new types of quality requirements [111], [122].\nLack of Suitable Guidelines for AI Documentation: K\u00f6nigstorfer [17] and Treacy [84] underscore the issue of insufficient guidance on documenting AI, noting that many guidelines do not effectively connect principles with actionable requirements."}, {"title": "5.1.2 Explainability Challenges", "content": "Many studies have identified explainability as a noteworthy challenge. We arranged these challenges into three major categories:\nExplainability as a New Requirement: Ishikawa et al. [57] and Kuwajima et al. [65] highlighted explainability as an emerging requirement, aligning with the European Commission's ethical guidelines for trustworthy AI, which advocate for fairness and explainability. This category underscores the recognition of explainability as a crucial aspect of ethical AI development.\nNo Consistent Definition for Explainability: A significant challenge in the domain of explainability is the absence of a unified definition, making it difficult to pinpoint what 'explainability' precisely entails [98]. This ambiguity is emphasized by studies like those of K\u00f6hl et al. [62], and Suresh et al. [12],"}, {"title": "", "content": "who note that different stakeholders have varying interpretations of explain- ability. Furthermore, Jansen et al. [27] and Kim et al. [123] discuss the gap between stakeholders' expectations of AI explanations and their understand- ing of AI system actions, illustrating the complexity of achieving a common understanding of explainability across diverse groups.\nLack of Stakeholder-Centric Approaches for Explainability: The necessity for stakeholder-centric approaches in explainability is underscored by the challenges in ensuring AI-based systems are transparent enough to foster trust and accountability [115]. Suresh et al. [12] and Wang et al. [70] address the difficulties in creating AI-based systems that can effectively communicate their reasoning to users, particularly in critical situations. The literature sug- gests that existing model interpretability methods often fail to consider the end-user, typically being most comprehensible to those who develop them, such as ML researchers or developers [50]. This point is further elaborated by Dhanorkar [5], who argues for the need to extend beyond current explainabil- ity techniques to accommodate the diverse explanations required by different stakeholders in an AI system. Henin and Metayer [119] highlight the chal- lenge of developing explanation methods that cater to various explainees with distinct interests, advocating for personalized approaches to explainability.\nCollectively, these challenges indicate a growing awareness of the impor- tance of explainability in AI, the need for a clearer definition and understand- ing of what explainability means to different stakeholders, and the importance of developing approaches that prioritize the perspectives and needs of those stakeholders"}, {"title": "5.1.3 New Requirements Engineering Practices", "content": "The literature identifies critical areas where new Requirements Engineering (RE) practices are essential to address the unique challenges posed by AI-based systems. These areas are categorized into four key segments.\nIntegrating AI Components in System: The integration of AI com- ponents into systems presents novel challenges for RE, necessitating new validation techniques beyond traditional inspection and static reading, es- pecially where data quality is paramount [64]. [124] highlights the need for a revised RE process pipeline to effectively address and evaluate the re- quirements for these AI components, underscoring the importance of safety, reliability, and effectiveness in AI systems [82].\nData as a New Source of Requirements: Data quality and its role as a source of requirements for AI-based systems emerge as significant con- cerns. The traditional principles and techniques of RE are found inadequate in addressing the unique requirements of ML-based systems, prompting a reevaluation of existing RE practices [57].\nIntegrating New Concepts into Existing Practices: The challenge extends to integrating new concepts into established RE practices. Existing RE frameworks must evolve to accommodate the distinct needs of AI-based systems, requiring a comprehensive approach that includes strategic planning, technology selection, system validation, and maintenance processes [43].\nLack of Suitable RE Concepts and Methodologies for ML-based Systems: There is a conspicuous gap in RE concepts and methodologies tai- lored to ML-based systems. This deficiency points to a broader issue within the field, where RE practices fail to align with the legal and regulatory demands specific to ML systems. Ensuring compliance with relevant laws and regula- tions remains a primary concern for requirements engineers in this domain [81]."}, {"title": "5.1.4 Human-Centric Requirements Evaluation", "content": "Lack of Human-Centric Approaches: The challenges across papers [20, 21, 86, 95, 106] collectively highlight a significant shortfall in human-centric approaches within AI system development. Habiba et al. [86] outline issues such as the lack of a mediator role for effective communication among stake- holders, the absence of a unified explainability definition, and the shortfall in stakeholder-focused development methodologies, alongside a missing common language for all involved in ML projects. These issues underscore a widespread neglect of human-centered perspectives in AI's technical evolution.\nAhmad et al. [106] point out the increasing reliance on AI in software solutions that unfortunately often overlook essential human-centered consid- erations in favor of technical priorities, indicating a misalignment between technological progress and human values. Similarly, Yu and Yong [20] expose a specific lack of engagement with the needs and perspectives of Korean stake- holders in AI for Health, revealing both a geographic and cultural oversight in"}, {"title": "", "content": "stakeholder engagement. Gr\u00fcning et al. [21] discuss how companies frequently miss integrating user requirements in the innovation of business models and the creation of new AI products, especially in healthcare, leaving uncertain how AI might shape future business models in this vital sector. Lastly, Wang [95] criticizes the dominant focus on technical strategies like model extraction for interpretability, which neglects user expectations, highlighting a critical gap in aligning AI system development with actual user needs.\nHard to Evaluate Requirements: Habibullah et al. [18] underscore the importance of NFRs in maintaining ML system quality, noting differences in definitions and measurements of NFRs between traditional systems and ML systems, such as adaptability and maintainability. The difficulty in measur- ing NFRs like fairness and explainability due to their qualitative nature is compounded in safety-critical situations where both human and machine judg- ment are crucial. Additionally, challenges in NFR measurement are identified, including gaps in knowledge or practices, absence of measurement baselines, complex ecosystems, data quality issues, testing costs, bias in results, and domain dependencies. However, Bartlett [94] points out the complexity of defining sensor accuracy requirements to ensure reliable algorithm outputs, indicating a lack of straightforward or well-defined processes. Similarly, Dey et al. [107] observe that while there is an emphasis on specifying ML-specific performance requirements, there is insufficient guidance on systematically engineering data requirements that involve diverse stakeholders."}, {"title": "5.1.5 Gap between ML Engineers and End-Users", "content": "This section focuses on the challenges arising from the gap between ML engineers and end-users. We categorized this gap into three distinct groups.\nLack of a Collaborative Approach to Requirements and Design: Initiating from a lack of collaboration, Vogelsang and Borg [81] underlined that it is challenging for data scientists to explain performance measures and their relevance to the client in an effective and understandable way. Further- more, to ensure that customers understand the performance measures, data scientists should also have skills in communication and customer education. Likewise, Shergadwala and El-Nasr [3] underscored the need to understand the shared mental model of design teams during human-AI collaboration. Liao et al. [8] felt the need for explainability to make AI algorithms understandable to people. In contrast, Nalchigar and Yu [39] questioned the huge conceptual distance between business strategies, decision processes, and organizational performance. Lastly, Brennen [6] stressed it is essential to define a common terminology when discussing XAI to enable meaningful, productive conversa- tions that can move the field forward. It could include establishing a shared vocabulary and clearly defined concepts and providing guidance on how to classify and rank models based on their explainability.\nLack of Communication: Secondly, in lack of communication, a key challenge for software engineers developing ML systems is to determine how to capture customer requirements effectively and design user interfaces that"}, {"title": "", "content": "effectively convey data to the user [10"}]}