{"title": "The Impact of Feature Embedding Placement in the Ansatz of a Quantum Kernel in QSVMs", "authors": ["Ilmo Salmenper\u00e4", "Ilmars Kuhtarskis", "Arianne Meijer - van de Griend", "Jukka K. Nurminen"], "abstract": "Designing a useful feature map for a quantum kernel is a critical task when attempting to achieve an advantage over classical machine learning models. The choice of circuit architecture, i.e. how feature-dependent gates should be interwoven with other gates is a relatively unexplored problem and becomes very important when using a model of quantum kernels called Quantum Embedding Kernels (QEK). We study and categorize various architectural patterns in QEKs and show that existing architectural styles do not behave as the literature supposes. We also produce a novel alternative architecture based on the old ones and show that it performs equally well while containing fewer gates than its older counterparts.", "sections": [{"title": "I. INTRODUCTION", "content": "Kernel methods enable Support Vector Machines (SVMs) to classify data by projecting it into a higher-dimensional space using a feature map making the data linearly separable [1]. This linkage allows SVMs to handle complex and nonlinear relationships within the data. While classical kernel methods have been explored extensively, the field of quantum machine learning (QML) has produced a new way of creating them in the form of quantum kernels [2]-[4].\nQuantum computers enhance kernel methods by utilizing the exponential size of the Hilbert space to separate data points coming from different probability distributions [4]. This capability allows for the extraction of features from data and the identification of complex relationships with a depth and efficiency not achievable with classical computing [5]. While there exists theoretical and experimental evidence for the usefulness of these models, there are still some concerns over their applicability and whether they actually provide an advantage over classical models [6].\nAn interesting variant of quantum kernels is Quantum Embedding Kernel (QEK), which takes inspiration from varia-tional circuits, which in addition to the quantum feature map, employ parameterized layers in their architecture [7]. These parameterized layers are trained using a process called kernel target alignment, which aims to create a feature map that projects data points with different labels into separate regions"}, {"title": "II. BACKGROUND", "content": "In this study we concentrate on a specific implementa-tion of quantum kernels called QEKs, and how different layering strategies when designing the overall structure of their feature map affects their performance. One especially interesting aspect of the architectures is how the feature-dependent layers and parameterized layers are interwoven. There are two styles of architectures present in the literature: (1) data-first architecture, where the feature-dependent layers occur before the parameterized layers [7], [9], [10] and (2) data-last architecture, where this order is reversed [11], [12].\nThe use of data-last architecture in QEKs is motivated probably due to the gate erasure bug in the data-first archi-tectures, but to our knowledge, there have been no studies done on how having a parameter-layer before any feature-dependent operations affect the performance of a QEK model when compared against the data-first model. Our aim in this study is to investigate how the performance of these models compare against each other to see whether the use of data-last architecture actually makes sense beyond the theoretical aspects and also see whether better models could be created based on these results.\nIt is also important to note that QEKs are not the only kind of quantum kernel methods that can be impacted by these kinds of choices in architecture. For example [13] proposes a way to create kernels through an adaptive process of adding various entangling gates and parameterized gates layer by layer after the feature-dependent layer, where only one of the five types of different gates contains feature-dependant variables. This means that if the model generates a circuit with some non-parameterized parts at the end, the gate erasure bug will cause them to vanish when evaluating values for the kernel matrix.\nWe would also like to take notice of an article featuring a quantum kernel model that does not strictly refer to itself as QEK, but follows a structure quite similar to theirs [14]. In this article a parameterized layer is used to create a preliminary state to which the feature-dependent layer is applied. While this has been shown to increase the performance of the SVM in certain contexts, it is also noted that there exists certain preliminary states which can also harm the performance of the model instead. This result will be relevant during the analysis of the data-last architecture."}, {"title": "III. METHODOLOGY", "content": "Kernel methods are a powerful class of algorithms in ma-chine learning, particularly suited for tasks where linear clas-sification methods, such as Support Vector Machines (SVM), face challenges due to non-linearly separable data [15]. These methods operate by projecting the original data points x to a higher-dimensional feature space with a feature map \u03a6(x), where linear separation becomes feasible.\nInterestingly it is actually not necessary to compute where these projected data points are in the higher-dimensional space, which can be computationally very expensive, or even impossible [16]. This is made possible through the kernel trick, which can be used to train the SVM using a similarity measure based on the inner products between data points in the training set [17]. If our feature map has a well defined inner product, we can create a kernel function:\n$\\kappa(x, x') = \\langle \\Phi(x), \\Phi(x') \\rangle$ (1)\nThis kernel function can be used to create a kernel matrix K for a set of data points in our training set D, where each element of the matrix is the value of the kernel function between two data points. This allows us to find the weights for the SVM inside the projected space, which can then be used to classify or group new data points.\nInterestingly the concept of kernel function translates very naturally to quantum computing. If we create a quantum kernel function $|\\Phi(x)\\rangle = U(x)|0\\rangle$ for some data point, the inner product between two different projected data points can be computed easily using the Loschmidt echo test [18] or the swap test [19].\nIn the Loschmidt echo test, the inner product between two projected data points x and x' is computed by first applying the"}, {"title": "B. Quantum kernel architectures", "content": "Interesting question related to QEKs is how we should interweave the parameterized layers with the feature dependent layers. Like we showed previously in the background section of this article, there are two main approaches of constructing the kernel layering structure: (1) data-first architectures [7] and (2) data-last architectures [11].\nIn the data-first architectures the feature dependent layers are put before the parameterized layers, as shown in Figure 1. This way of constructing QEKs suffers from the gate erasure"}, {"title": "C. Experimental setup", "content": "To evaluate the performance and trainability of these three models, we focus on classification accuracy, kernel align-ment and time taken across a variety of datasets, including Hayes-Roth [23] (3 classes), Heart Disease [24] (2 classes), Seeds [25] (3 classes), and Wine [26] (3 classes), all features normalized between 0 and 1. The trained kernels were then given to a support vector classifier model (SVC), which was trained to be either a binary classifier in the case of having two classes, or a multi-class classification using a one-vs-many scheme [27].\nIn our experiment design, certain parameters remain con-stant across all trials for consistency: we use 5 qubits (wires), a batch size of 5, run 5000 optimization iterations, and conduct alignment tests every 250 iterations. 25 different models were trained for each variable parameters included the number of layers (ranging from 1 to 5) to assess the impact of these factors on performance. The test-training split was fixed to be consistent for each dataset, and the split was chosen manually from among 25 candidates to ensure that the models are easy to compare to each other. The results are consistent with the random test-training splits, but way less noisy, as the average dataset size was very small and had a large impact of the end performance. For the optimization method, we use gradient descent with the finite differentiation method. The ansatz structure is explained in Appendix A."}, {"title": "IV. RESULTS", "content": "The impact of the three architectural strategies on accuracy of the trained model can be seen in the Figure 3. From the results we can see that target alignment does produce models with higher levels of accuracy and the more layers you add to the models, the better the performance tends to be. With most of our datasets the performance seems to peak at 3 parameterized layers, after which adding more layers does not affect the accuracy of the model in any meaningful sense.\nThe impact of the gate erasure bug is very visible in these results. The data-first model with one parameterized layer has no variance in results as the single parameterized layer does nothing and almost never outperforms the other two architectures. With two parameterized layers or more the data-first models seems to be performing equally well to the data-last architecture, which is surprising, as the last parameter-ized layer should still resolve into an identity operator as predicted by the theory, resulting in one less parameterized layer compared to the data-last kernel. This seems to indicate that parameterized layers in between feature-layers tend to be more effective at increasing the accuracy of the trained model.\nThe performance of the data-last architecture is somewhat consistent with existing results - with one parameterized layer, the data-last architecture can achieve a higher accuracy than the data-first architecture, but sometimes they can also end up performing worse performance in the given task. This speaks to the volatility of the results and shows that there is no guarantee that the introduction of preliminary quantum states would help with separating the data points from one another.\nData-weaved model achieves higher accuracies in almost all test cases, with an equal amount parameterized gates compared to the other two models. The only exception for this is the Hayes-Roth dataset with one layer of parameters, and even there the average performance becomes the consistent with the other results further down the line.\nImportant thing to note about the performance of the three models is that the choice of architecture does not really affect the peak performance of the end model, as long as we increase the layer size enough. This means that the only meaningful difference between the three models is how many gates the models have, and more importantly, how many parameters are needed to optimize when training the model."}, {"title": "V. DISCUSSION", "content": "The differences between the three architectural choices seem to be consistent with the concepts from theoretical background. It seems that there are very few reasons to have parameterized circuits before or after any encoding layers, except if we are interested in using only one layer of parameters. While this does not make sense from the point of view of QEKs, there might be other styles of quantum kernel methods that might benefit from this strategy.\nThe simulated results show that while all of these models are capable of producing acceptable results, the data-weaved kernels are usually able to do more with similar access to resources. This means that according to these results, using data-weaved kernels seems to be the most sensible choice out of the three possibilities. Further research could be made into how much different choices of ansatze could impact the results, but at least in our experiences using different choice of feature-encoding or parameterized layers did not effect the results in meaningful ways.\nThe key reason for why data-weaved architectures should be used with quantum kernels comes mostly down to resource usage, at least in simulated environments. They are not more accurate than their alternative counterparts, as the user is free to choose how many layers they want to use in the training process of the model. The advantage gained from using data-weaved kernels is very dependent on the problem in question, but for example using our ansatze and feature-embedding layers, for a dataset with ten features, the decrease in the amount of gates would be around 11% with one qubit gates (from 90 to 80) and 30% with two qubit gates (from 30 to 20), which can be a very meaningful difference in the world of high performance computing. These savings would increase even further if more complicated ansatze are used.\nAn additional key factor influencing these results from classical point of view is the performance of optimizers in han-dling non-contributing parameters. In cases where parameters are nullified by the gate erasure bug, an optimizer not designed to disregard these parameters may experience inefficiencies. This slowdown arises as the optimizer attempts to adjust parameters that have no impact on the outcome. For example, optimizers using the parameter shift rule would probably spend some amount of time evaluating parameters that do not have an effect to the output system [28], while the use of SPSA-optimizers would cause the unnecessary parameters to be negligible during training, as they adjust all the parameters at the same time [29].\nFrom the quantum computing point of view, the choice of architecture can have a large impact on the performance of the circuit. If the data-weaved kernel requires one less parameterized layer of gates to operate at the optimal level of accuracy, this could have a sizable difference in performance compared to the data-last kernels, as at least in the near term future we have to be mindful of device noise caused by additional gate operations. Additional noise can be very harmful from the point of view of training a QML model, as shot noise is heavily linked with trainability of the model in question [30]. This problem is exacerbated further by the use of data-first kernels, which contain a layer of unnecessary gates in the end, that will be in the best case optimized out from the circuit, and in worst case, kept by the transpiler that refuses to remove the unnecessary gates, as they are part of the optimizable circuit.\nOne additional feature of the data-weaved kernel is the fact that the data-weaved models seem to have steeper curves with alignment during training, which is an indication of the trainability of the model. This could be interesting future work on the topic, to see whether these models are also capable of learning the optimal feature map quicker than the data-last counterparts.\nWhile QEKs are not the most common form of quantum kernels, it is also vital to see how these results affect models outside of QEKs. As the use of Loschmidt-echo test or swap test is widespread in the quantum kernel landscape, this begs the question whether similar problems with gate ordering could be influencing results across the field of research. These issues tend to be difficult to notice, as even something as dire as the gate erasure bug does not ruin one's result, but only increases the costs of computational resources."}, {"title": "VI. CONCLUSION", "content": "In this study, we explored different models of layer ar-chitectures for quantum embedded kernels, focusing on the intricacies of gate sequencing and their impact on performance and trainability. Our research indicates that the data-first or data-last approaches lead to inefficiencies, while the models where the parameterized parts are put in between feature encoding seem to fare better in most metrics. These inefficiencies culminated in a situation where the nullification of non-feature-dependent gates by the gate erasure bug causes data-first kernels to be computationally equivalent to the data-weaved kernels.\nThrough comprehensive testing across four datasets, we demonstrated that the data-weaved architecture, which posi-tions data embeddings at the ends of each layer, not only im-proved the accuracy in a majority of cases but also optimized the training process. The new architecture showed indications of a marked improvement in kernel alignment, suggesting a more efficient utilization of quantum resources.\nWhile adding additional layers can compensate for inher-ent architectural inefficiencies, we show how architectural choices can be used to gain advantages across the board. We highlight the importance of strategic gate placement and the active contribution of all parameters in the circuit. This study underscores the need for careful consideration in quantum circuit design, particularly in the NISQ-era, where every computational element counts."}, {"title": "APPENDIX", "content": "The quantum circuit design for each architecture involves starting every data embedding layer with Hadamard gates followed by $R_z(x_n)$ rotations where X is a data point from a dataset, the parameterized layer comprises of parameterized rotations $R_y(\\alpha_n)$ followed by parameterized CZs: $CR_z(\\beta_n)$, where $ \\alpha $ and $ \\beta $ are trainable parameters. The $CR_z(\\beta_n)$ gates are applied in a circular pattern."}]}