{"title": "E-VAE: DENOISING AS VISUAL DECODING", "authors": ["Long Zhao", "Sanghyun Woo", "Ziyu Wan", "Yandong Li", "Han Zhang", "Boqing Gong", "Hartwig Adam", "Xuhui Jiat", "Ting Liu"], "abstract": "In generative modeling, tokenization simplifies complex data into compact, structured representations, creating a more efficient, learnable space. For high-dimensional visual data, it reduces redundancy and emphasizes key features for high-quality generation. Current visual tokenization methods rely on a traditional autoencoder framework, where the encoder compresses data into latent representations, and the decoder reconstructs the original input. In this work, we offer a new perspective by proposing denoising as decoding, shifting from single-step reconstruction to iterative refinement. Specifically, we replace the decoder with a diffusion process that iteratively refines noise to recover the original image, guided by the latents provided by the encoder. We evaluate our approach by assessing both reconstruction (rFID) and generation quality (FID), comparing it to state-of-the-art autoencoding approach. We hope this work offers new insights into integrating iterative generation and autoencoding for improved compression and generation.", "sections": [{"title": "INTRODUCTION", "content": "Generative modeling aims to capture the underlying distribution of training data, enabling realistic sample generation during inference. A key preprocessing step is tokenization, which converts raw data into discrete tokens or continuous latent representations. These compact representations allow models to efficiently learn complex patterns, enhancing the quality of generated outputs.\nTwo dominant paradigms in modern generative modeling are autoregression (Radford et al., 2018) and diffusion (Ho et al., 2020). Autoregressive models generate samples incrementally, conditioning each new element on previously generated ones, while diffusion models progressively refine noisy inputs into clear representations, without relying on element-wise factorization. Tokenization is an essential in both: discrete tokens direct step-by-step conditional generation in autoregressive models, while continuous latents streamline the denoising process in diffusion models. Empirical results across language (Achiam et al., 2023; Anil et al., 2023; Dubey et al., 2024) and vision (Baldridge et al., 2024; Esser et al., 2024; Brooks et al., 2024) tasks show that tokenization\u2014whether discrete or continuous improves generative performance. We focus on tokenization for latent diffusion models, which excel at generating high-dimensional visual data.\nGiven its central role in both paradigms, understanding how tokenization works is essential. In language processing, tokenization is relatively straightforward, involving segmenting text into discrete units such as words, subwords, or characters (Sennrich et al., 2015; Kudo & Richardson, 2018; Kudo, 2018). However, tokenization in visual domains poses greater challenges due to the continuous, high-dimensional, and redundant nature. Instead of direct segmentation, compact representations are typically learned using an autoencoding (Hinton & Salakhutdinov, 2006). Despite rapid advancements in visual generation techniques, the design of tokenizers has received relatively little attention. This is evident in the minimal evolution of tokenizers used in state-of-the-art models, which have remained largely unchanged since their initial introduction (Van Den Oord et al., 2017).\nIn this paper, we address this gap by revisiting the widely adopted visual autoencoding formulation (Esser et al., 2021), aiming to achieve higher compression rates and improved reconstruction quality, thereby enhancing generation quality of downstream generative models. Our key idea is to rethink the traditional autoencoding pipeline, which typically involves an encoder that compresses"}, {"title": "BACKGROUND", "content": "We start by briefly reviewing the basic concepts required to understand the proposed method. A more detailed summary of related work is deferred to Appendix A.\nVisual tokenization. To achieve efficient and scalable high-resolution image synthesis, common generative models, including autoregressive models (Razavi et al., 2019; Esser et al., 2021; Chang et al., 2022) and diffusion models (Rombach et al., 2022), are typically trained in a low-resolution latent space by first downsampling the input image using a tokenizer. The tokenizer is generally implemented as a convolutional autoencoder consisting of an encoder, E, and a decoder, G. Specifically, the encoder, E, compresses an input image $x \\in \\mathbb{R}^{H \\times W \\times 3}$ into a set of latent codes (i.e., tokens), $E(x) = z \\in \\mathbb{R}^{H/f \\times W/f \\times n_z}$, where f is the downsampling factor and $n_z$ is the latent channel dimensions. The decoder, G, then reconstructs the input from z, such that $G(z) = x$.\nTraining an autoencoder primarily involves several losses: reconstruction loss $L_{rec}$, perceptual loss (LPIPS) $L_{LPIPS}$, and adversarial loss $L_{adv}$. The reconstruction loss minimizes pixel differences (i.e., typically measured by the $l_1$ or $l_2$ distance) between x and G(z). The LPIPS loss (Zhang et al., 2018) enforces high-level structural similarities between inputs and reconstructions by minimizing differences in their intermediate features extracted from a pre-trained VGG network (Simonyan & Zisserman, 2015). The adversarial loss (Esser et al., 2021) introduces a discriminator, D, which encourages more photorealistic outputs by distinguishing between real images, D(x), and reconstructions, D(G(z)). The final training objective is a weighted combination of these losses:\n$L_{VAE} = L_{rec} + \\lambda_{LPIPS} L_{LPIPS} + \\lambda_{adv} L_{adv},$\nwhere the \u03bb values are weighting coefficients. In this paper, we consider the autoencoder optimized by Eq. 1 as our main competing baseline (Esser et al., 2021), as it has become a standard tokenizer training scheme widely adopted in state-of-the-art image and video generative models (Chang et al., 2022; Rombach et al., 2022; Yu et al., 2022; 2023; Kondratyuk et al., 2024; Esser et al., 2024).\nDiffusion. Given a data distribution $p_d$ and a noise distribution $p_e$, a diffusion process progressively corrupts clean data $x_0 \\sim p_x$ by adding noise $\\epsilon \\sim p_e$ and then reverses this corruption to recover the original data (Song & Ermon, 2019; Ho et al., 2020), represented as:\n$x_t = \\sqrt{\\alpha_t} x_0 + \\sigma_t \\cdot \\epsilon,$\nwhere $t \\in [0, T]$ and $\\epsilon$ is drawn from a standard Gaussian distribution, $p_\\epsilon = \\mathcal{N}(0, I)$. The functions $\\alpha_t$ and $\\sigma_t$ govern the trajectory between clean data and noise, affecting both training and sampling. \nThe basic parameterization in Ho et al. (2020) defines $\\sigma_t = \\sqrt{1 - \\alpha_t}$ with $\\alpha_t = \\left(\\prod_{i=1}^t \\left(1 - \\beta_i\\right)\\right)$"}, {"title": "METHOD", "content": "We introduce e-VAE, with an overview provided in Figure 1. The core idea is to replace single-step, deterministic decoding with an iterative, stochastic denoising process. By reframing autoencoding as a conditional denoising problem, we anticipate two key improvements: (1) more effective generation of latent representations, allowing the downstream latent diffusion model to learn more efficiently, and (2) enhanced decoding quality due to the iterative and stochastic nature of the diffusion process.\nWe systematically explore the design space of model architecture, objectives, and diffusion training configurations, including noise and time scheduling. While this work primarily focuses on generating continuous latents for latent diffusion models, the concept of iterative decoding could also be extended to discrete tokens, which we leave for future exploration."}, {"title": "MODELING", "content": "\u20ac-VAE retains the encoder E while enhancing the decoder G by incorporating a diffusion model, transforming the standard decoding process into an iterative denoising task.\nConditional denoising. Specifically, the input $x \\sim p_x$ is encoded by the encoder as $z = E(x)$, and this encoding serves as a condition to guide the subsequent denoising process. This reformulates the reverse process in Eq. 3 into a conditional form (Nichol & Dhariwal, 2021; Saharia et al., 2022b):\n$p(x_{0:T}|z) = p(x_T) \\prod_{i=1}^T p(x_{(i-1) \\cdot \\Delta t}|x_{i \\cdot \\Delta t}, z),$\nwhere the denoising process from the noise $x_T = \\epsilon$ to the input $x_0 = x$, is additionally conditioned on z over time. Here, the decoder is no longer deterministic, as the process starts from random noise. For a more detailed discussion on this autoencoding formulation, we refer readers to Sec. 5.\nArchitecture and conditioning. We adopt the standard U-Net architecture from Dhariwal & Nichol (2021) for our diffusion decoder G, while also exploring Transformer-based models (Peebles & Xie, 2023). For conditional denoising, we concatenate the conditioning signal with the input channel-wise, following the approach of diffusion-based super-resolution models (Ho et al., 2022; Saharia et al., 2022b). Specifically, low-resolution latents are upsampled using nearest-neighbor interpolation to match the resolution of $x_t$, then concatenated along the channel dimension. In Appendix C, although we experimented with conditioning via AdaGN (Nichol & Dhariwal, 2021), it did not yield significant improvement and introduced additional overhead, so we adopt channel concatenation."}, {"title": "OBJECTIVES", "content": "We adopt the standard autoencoding objective from Eq. 1 to train \u20ac-VAE, with a key modification: replacing the reconstruction loss $L_{rec}$ used for the standard decoder with the score-matching loss $L_{score}$ for training the diffusion decoder. Additionally, we introduce a strategy to adjust the perceptual $L_{LPIPS}$ and adversarial $L_{adv}$ losses to better align with the diffusion decoder training.\nVelocity prediction. We adopt the rectified flow parameterization, utilizing a linear optimization trajectory between data and noise, combined with velocity-matching objective (Eq. 6):\n$\\mathbb{E}_{t \\sim \\pi(t), \\epsilon \\sim \\mathcal{N}(0,1)} [||G(x_t, t, z) - (\\epsilon - x)||^2] .$\nPerceptual matching. The LPIPS loss (Zhang et al., 2018) minimizes the perceptual distance be-"}, {"title": "NOISE AND TIME SCHEDULING", "content": "Noise scheduling. In diffusion models, noise scheduling involves progressively adding noise to the data over time by defining specific functions for $\\alpha_t$ and $\\sigma_t$ in Eq. 2. This process is crucial as it determines the signal-to-noise ratio, $\\frac{\\alpha_t}{\\sigma_t^2} = \\lambda_t$, which directly influences training dynamics. Noise scheduling can also be adjusted by scaling the intermediate states $x_t$ with a constant factor $\\gamma \\in (0, 1]$, which shifts the signal-to-noise ratio downward. This makes training more challenging over time while preserving the shape of the trajectory (Chen, 2023).\nIn this work, we define $\\alpha_t$ and $\\sigma_t$ according to rectified flow formulation, while also scaling $x_t$ by \u03b3, with the value chosen empirically. However, when \u03b3 \u2260 1, the variance of $x_t$ changes, which can degrade performance (Karras et al., 2022). To address this, we normalize the denoising input $x_t$ by its variance after scaling, ensuring it preserves unit variance over time (Chen, 2023).\nTime scheduling. Another important aspect in diffusion models is time scheduling for both training and sampling, controlled by \u03c0(t) during training and \u0394t during sampling, as outlined in Eq. 3 and Eq. 4. A common choice for \u03c0(t) is the uniform distribution U(0, T), which applies equal weight to each time step during training. Similarly, uniform time steps $\\Delta t = \\frac{1}{T}$ are typically used for sampling. However, to improve model performance on more challenging time steps and focus on noisy regimes during sampling, the time scheduling strategy should be adjusted accordingly.\nIn this work, we sample t from a logit-normal distribution (Atchison & Shen, 1980), which emphasizes intermediate timesteps (Esser et al., 2024). During sampling, we apply a reversed logarithm mapping function $P_{log}$, defined as:\n$P_{log} (t; m, n) = \\frac{log(m) - log (t \\cdot (m-n) + n)}{log(m) - log(n)},$\nwhere we set m = 1 and n = 100, resulting in denser sampling steps early in the inference process."}, {"title": "EXPERIMENTS", "content": "We evaluate the effectiveness of e-VAE on image reconstruction and generation tasks using the ImageNet (Deng et al., 2009). The VAE formulation by Esser et al. (2021) serves as a strong baseline due to its widespread use in modern image generative models (Rombach et al., 2022; Peebles & Xie, 2023; Esser et al., 2024). We perform controlled experiments to compare reconstruction and generation quality by varying model scale, latent dimension, downsampling rates, and input resolution.\nModel configurations. We use the encoder and discriminator architectures from VQGAN (Esser et al., 2021) and keep consistent across all models. The decoder design follows BigGAN (Brock et al., 2019) for VAE and from ADM (Dhariwal & Nichol, 2021) for e-VAE. Additionally, we experiment with the DiT architecture (Peebles & Xie, 2023) for E-VAE. To evaluate model scaling, we test five decoder variants: base (B), medium (M), large (L), extra-large (XL), and huge (H), by adjusting width and depth accordingly. Further model specifications are provided in Appendix B.\nTraining. The autoencoder loss follows Eq. 1, with weights set to $L_{LPIPS} = 0.5$ and $L_{adv} = 0.5$. We use the Adam optimizer (Kingma & Ba, 2015) with \u03b2\u2081 = 0 and \u03b2\u2082 = 0.999, applying a linear learning rate warmup over the first 5,000 steps, followed by a constant rate of 0.0001 for a total of one million steps. The batch size is 256, with data augmentations including random cropping and horizontal flipping. An exponential moving average of model weights is maintained with a decay rate of 0.999. Both VAE and e-VAE are trained to reconstruct 128 \u00d7 128 images. The default setup uses a downsampling factor of 16 and latent dimensions with 8 channels. We also test downsampling rates of 4, 8, and 32, as well as latent dimensions with 4, 16, and 32 channels. All models are implemented in JAX/Flax (Bradbury et al., 2018; Heek et al., 2024) and trained on TPU-v5lite pods.\nEvaluation. We evaluate the autoencoder on both reconstruction and generation quality using Fr\u00e9chet Inception Distance (FID) (Heusel et al., 2017) as the primary metric, computed on 10,000 validation images. For reconstruction quality (rFID), FID is computed at both training and higher resolutions to assess generalization across resolutions. For generation quality (FID), we generate latents from the trained autoencoders and use them to train the DiT-XL/2 latent generative model (Peebles & Xie, 2023). This latent model remains fixed across all generation experiments, meaning improved autoencoder latents directly enhance generation quality. We also report Inception Score (IS) (Salimans et al., 2016) and Precision/Recall (Kynk\u00e4\u00e4nniemi et al., 2019) as secondary metrics."}, {"title": "RECONSTRUCTION QUALITY", "content": "Decoder architecture. We explore two major architectural designs: the UNet-based architecture from ADM (Dhariwal & Nichol, 2021) and the Transformer-based DiT (Peebles & Xie, 2023). We compare various model sizes\u2013ADM:{B, M, L, XL, H} and DiT:{S, B, L, XL} with patch sizes of {4, 8}. ADM consistently outperforms DiT across the board. While we observe rFID improvements in DiT when increasing the number of tokens by reducing patch size, this comes with significant computational overhead. The overall result aligns with the original design intentions: ADM for pixel-level generation and DiT for latent-level generation. For the following experiments, we use the ADM architecture for our diffusion decoder.\nCompression rate. Compression can be achieved by adjusting either the channel dimensions of the latents or the downsampling factor of the encoder. The results show that e-VAE consistently outperforms VAE in terms of rFID, particularly as the compression ratio increases. Model scaling. We investigate the impact of model scaling by comparing VAE and e-VAE across five model variants. The results demonstrate that e-VAE consistently achieves significantly better rFID scores than VAE, with an average relative improvement of over 40%, and even the smallest e-VAE model outperforms VAE at largest scale. While the U-Net-based decoder of e-VAE has about twice as many parameters"}, {"title": "GENERATION QUALITY", "content": "Given the trained VAE and e-VAE models, we now evaluate their autoencoding performance. In practice, we first generate latents using the trained autoencoders, then train a new latent generative model based on these representations. The compact, learnable latent space produced by the encoder enhances the learning efficiency of latent generative model, while effective decoding of the sampled latents ensures high-quality outputs. Thus, both the encoding and decoding capabilities of autoencoder contribute to the overall generative performance. For this evaluation, we perform standard unconditional image generation tasks using the DiT-XL/2 model as our latent generative model (Peebles & Xie, 2023)."}, {"title": "ABLATION STUDIES", "content": "We conduct a component-wise analysis to validate our key design choices. We evaluate the reconstruction quality (rFID) and sampling efficiency (NFE). Our evaluation begins with a baseline model: an autoencoder with a diffusion decoder, trained solely using the score matching objective. This baseline follows the vanilla diffusion setup from Ho et al. (2020), including their UNet architecture, parameterization, and training configurations, while extending to a conditional form as described in Eq. 8. Impact of proposals. In (1), transitioning from standard diffusion to rectified flow (Liu et al., 2023) straightens the optimization path, resulting in significant gains in rFID scores and NFE."}, {"title": "DISCUSSION", "content": "Distribution-aware compression. Traditional image compression methods optimize the rate-distortion trade-off (Shannon et al., 1959), prioritizing compactness over input fidelity. Building on this, we also aim to capture the broader input distribution during compression, generating compact representations suitable for latent generative models. This approach introduces an additional dimension to the trade-off, perception or distribution fidelity (Blau & Michaeli, 2018), which aligns more closely with the rate-distortion-perception framework (Blau & Michaeli, 2019).\nIterative and stochastic decoding. A key question within the rate-distortion-perception trade-off is whether the iterative, stochastic nature of diffusion decoding offers advantages over traditional single-step, deterministic methods (Kingma, 2013). The strengths of diffusion (Ho et al., 2020) lie in its iterative process, which progressively refines the latent space for more accurate reconstructions, while stochasticity allows for capturing complex variations within the distribution. Scalability. As discussed in Section 4.1, our diffusion-based decoding method maintains the resolution generalizability typically found in standard autoencoders. This feature is highly practical: the autoencoder is trained on lower-resolution images, while the subsequent latent generative model is trained on latents derived from higher-resolution inputs."}, {"title": "CONCLUSION", "content": "We present \u20ac-VAE, an effective visual tokenization framework that introduces a diffusion decoder into standard autoencoders, turning single-step decoding into a multi-step probabilistic process. By exploring key design choices in modeling, objectives, and diffusion training, we demonstrate significant performance improvements. Our approach outperforms traditional visual autoencoders in both reconstruction and generation quality, particularly in high-compression scenarios. We hope our concept of iterative generation during decoding inspires further advancements in visual autoencoding."}, {"title": "EXPERIMENT SETUPS", "content": "In this section, we provide additional details on our experiment configurations for reproducibility.\nMODEL SPECIFICATIONS\nTable 4 summarizes the primary architecture details for each decoder variant. ADDITIONAL IMPLEMENTATION DETAILS\nDuring the training of discriminators, Esser et al. (2021) introduced an adaptive weighting strategy for \u03bb\u03bbadv. However, we notice that this adaptive weighting does not introduce any benefit which is consistent with the observation made by Sadat et al. (2024). Thus, we set \u03bb\u03bbadv = 0.5 in the experiments for more stable model training across different configurations.\nLATENT DIFFUSION MODEL\nWe follow the setting in Peebles & Xie (2023) to train the latent diffusion models for unconditional image generation on the ImageNet dataset. The DiT-XL/2 architecture is used for all experiments."}, {"title": "ADDITIONAL EXPERIMENTAL RESULTS", "content": "Conditioning. In addition to injecting conditioning via channel-wise concatenation, we explore providing conditioning to the diffusion model by adaptive group normalization (AdaGN) (Nichol & Dhariwal, 2021; Dhariwal & Nichol, 2021). To achieve this, we resize the conditioning (i.e., encoded latents) via bilinear sampling to the desired resolution of each stage in the U-Net model, and incorporates it into each residual block after a group normalization operation (Wu & He, 2018). Trajectory matching. The proposed denoising trajectory matching objective matches the start-to-end trajectory $x_t \\rightarrow x_0$ by default."}]}