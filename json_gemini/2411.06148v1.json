{"title": "Deep Reinforcement Learning for Digital Twin-Oriented Complex Networked Systems", "authors": ["JIAQI WEN", "BOGDAN GABRYS", "KATARZYNA MUSIAL"], "abstract": "Abstract-The Digital Twin Oriented Complex Networked System (DT-CNS) aims to build and extend a Complex Networked System (CNS) model with progressively increasing dynamics complexity towards an accurate reflection of reality a Digital Twin of reality. Our previous work proposed evolutionary DT-CNSs to model the long-term adaptive network changes in an epidemic outbreak. This study extends this framework by proposeing the temporal DT-CNS model, where reinforcement learning-driven nodes make decisions on temporal directed interactions in an epidemic outbreak. We consider cooperative nodes, as well as egocentric and ignorant \"free-riders\" in the cooperation. We describe this epidemic spreading process with the Susceptible-Infected-Recovered (SIR) model and investigate the impact of epidemic severity on the epidemic resilience for different types of nodes. Our experimental results show that (i) the full cooperation leads to a higher reward and lower infection number than a cooperation with egocentric or ignorant \"free-riders\"; (ii) an increasing number of \"free-riders\" in a cooperation leads to a smaller reward, while an increasing number of egocentric \"free-riders\" further escalate the infection numbers and (iii) higher infection rates and a slower recovery weakens networks' resilience to severe epidemic outbreaks.\n\nIndex Terms-Temporal Networks; Temporal Dynamic Process; Digital Twin; Reinforcement Learning.", "sections": [{"title": "I. INTRODUCTION", "content": "Accurate modelling of Complex Networked Systems (CNSs) that involves real-time interactions is crucial for addressing societal challenges such as gender inequality, crime, and epidemics. For example, organized crime and pandemic outbreak can cause significant economic losses. An advanced CNS model that mimics real-world social interactions, could help policymakers and authorities simulate various scenarios and develop better strategies to address these pressing issues. Thefore, our previous work proposed a new modelling paradigm: Digital Twin-Oriented Complex Networked Systems (DT-CNSs) [1], which aims to build and extend CNSs with increasing complexity of generations towards the ultimate goal - a Digital Twin (DT) of real networked systems (Fig. 1).\n\nAs shown in Fig. 1, the complexity of generations of DT-CNSs depends on the evolvability of the dynamics in DT-CNSs, interrelations between the dynamics in DT-CNSs, as well as the real-time interplay between the DT-CNSs and the reality. From generation 1 to 5, the temporal scale of network representation and modelling transforms to be more instantaneous, ranging from static (no change), evolving (slow/temporal changes captured in snapshots) to temporal (temporal changes captured in real time) [1]. The generations of DT-CNSs, as they approach a DT in generation 5, model the temporal changes in networks and dynamic processes, together with their interrelations, while allowing for the real-time interplay between reality and the DT-CNSs [1]. Under this conceptual framework, we proposed an extendable DT-CNSs modelling framework for complex social networks by introducing heterogeneous node features (a.k.a node attributes) and nodes' preferences to create relationships, while allowing these preferences to evolve under the impact of dynamic processes towards rewarding interactions [2], [3]. However, the proposed modelling framework builds DT-CNSs in generation 1, 2 and 3. Its extension towards generation 4 and 5 poses a challenge in data collection, processing and model updates considering the real-time information (Fig. 1). In this study, we progress this the DT-CNS modelling framework by modelling temporal networks and the temporal dynamic process on the networks, which brings the DT-CNS closer to the ultimate goal of a DT.\n\nReinforcement learning (RL) allows the agent to make decisions, observe the results, and then automatically adjust its strategy to achieve the optimal policy [4]. To address the scalability and efficiency issues of traditional RL, researchers proposed to use Deep Reinforcement Learning (DRL), leveraging Deep Neural Networks to enhance learning speed and performance [5], [4]. Current studies have employed DRL to implement multiple tasks in CNSs, such as influencing maximisation [6], [7], [8], [9], key-player identification [10], [11], network topology optimisation [12] and intelligent routing [13], [14], [15]. Some studies employed deep reinforcement learning to automatically make a decision in social networks, such as network resource allocation [16], rumour mitigation [17] and influence maximisation [18]. However, none of these studies employ deep reinforcement learning to optimally drive nodes' cooperation and free-riding behaviours in an epidemic outbreak.\n\nThis study proposes the temporal Digital Twin-Oriented Complex Networked System (DT-CNS) model driven by reinforcement learning algorithm. Our previous work [3] enables to model evolutionary DT-CNSs regarding the evolving networks, a dynamic process on the networks and their interrelated changes. This study extends this framework by introducing reinforcement learning-driven nodes to make decisions on temporal directed interactions in an epidemic outbreak. Under this framework, we consider nodes' heterogeneous features and changeable connection preferences, which combine the effects of preferential attachment and homophily. We consider three types of nodes: (i) cooperative nodes who maximise their total reward using a collective mind driven by reinforcement learning; (ii) egocentric nodes who maximise the individual"}, {"title": "II. MODEL", "content": "A. Components\n\nTemporal directed network at time t can be represented as Gt and is composed of two elements.\n\n$G_t(V_t, E_t)$  (1)\n\nVt represents the set of nodes. In this study, we assume a fixed number of nodes (N nodes) for each temporal network Gt. The set of nodes is defined as\n\n$V_t = {v_{0,t}, v_{1,t},..., v_{i,t},\u2026\u2026\u2026, v_{N,t}}$, (2)\n\nEt represents the set of edges, which refer to the directed connections between the nodes. The set of edges can be defined as\n\n$E_t = {e_{ij,t}|v_{i,t}, v_{j,t} \u2208 V_t, i \u2260 j}$, (3)\n\nwhere $e_{ij,t}$ represents a directed connection from node $v_{i,t}$ to node $v_{j,t}$ at time t. We assume no self-links in this study ($e_{ii,t}$ does not exist).\n\nB. Dynamics\n\n1) Agents: In this study, the nodes also represent the agents who act in response to the interaction reward. They can be divided into a set of agents (nodes) driven by different action styles, including the ignorant agents: $V^{Ign} = {v_{i,t}^{ign}}$, the set of egocentric agents: $V^{Ego} = {v_{i,t}^{ego}}$, the set of cooperative agents: $V^{Cop} = {v_{i,t}^{cop}}$. The nodes and the agents are interchangeable concepts in this study.\n\n$V = V^{Ign} \u222a V^{Ego} \u222a V^{Cop}$, (4)\n\nwhere cooperative nodes cooperate to maximise the average value of the overall interaction reward. The egocentric nodes and the ignorant nodes are \"free-riders\" of the cooperation, as they assume the other nodes are cooperative so they betray the cooperation by ignorant or egocentric actions. The ignorant nodes keep ignorant of the interaction reward and randomly"}, {"title": null, "content": "act. The egocentric nodes only consider individual interactions time t. The social capital embedded in nodes' interactions is\n\nand their respective individual rewards.\n\ndenoted by $\u015c^{ign}$ and constrained by the social capital limit\nIn the network evolution and the epidemic spreading pro- i,t\ncesses, each agent observes the network states, makes deci- $\u015c^{ign}$. The node $v^{ign}$ interacts under the social capital limit $\u015c_{i,t}$\nsions on temporal social contact and calculates the reward for objectives related to interaction reward $R^{ign}$. The social\naccording to their action styles. The temporal changes in inter- capital limit refers to people's maximum investment, due to\naction patterns and infection patterns influence the interaction personal capacity or social restrictions, in social interactions\nreward and reinforce the learning process of the agents. for expected returns [20], [21].\n\n2) States: We represent the states of the whole system at The node's preferences $SDNA_{i,t}^{ign}$ for the related features\n\ntime t with St, which includes the states of the network $S^{Net}_t$ $f^{ign}$ is defined as\n\nand the epidemic process $S^{Epi}_t$,\n\n$SDNA_{i,t}^{ign}(p_{i,t}^{ign}, w_{p_{i,t}}^{ign}, h_{i,t}^{ign}, w_{h_{i,t}}^{ign},c_{i,t}^{ign}, w_{c_{i,t}}^{ign})$ (10)\n\n$S_t = S^{Net}_t \u222a S^{Epi}_t$  (5)\n\nwhere $p_{i,t}^{ign}$ and $h_{i,t}^{ign}$ are preference vectors. These vec-\nIn this study, we model the epidemic spreading process with tors represent preferences for features and feature differences.\nthe \"susceptible-infected-recovered\u201d (SIR) model [19]. The The values of the preferences are denoted as \u22121, 0, and 1,\nstate $S^{Epi}_t$ of an epidemic outbreak can be defined as corresponding to negative, neutral, and positive preferences.\nThese two vectors are followed with the same length weighting\n$S^{Epi}_t (V^{seed}, \u03b6, T)$ (6) vectors $w_{p_{i,t}}^{ign}$ and $w_{h_{i,t}}^{ign}$, which include the weight of\n\nwhere $V^{seed}$ represents the seed set for the epidemic spread. preference with value within (0, 1]. Similarly, $c_{i,t}^{ign}$ determines\n\u03b6 denotes the infection rate of the epidemic spread at time the negative, zero and positive preference for connecting with\nt. The node $v_{i,t}$, gets infected at a probability of \u03b6, given a nodes with common friends by -1, 0 and 1, followed by the\nsingle interaction with an infected node and, once infected, weight of preference $w_{c_{i,t}}^{ign}$.\nrecovers after a time length of T before another interaction The epidemic risk $R_{i,t}^{ign}$ of a healthy node $v_{i,t}^{ign}$ at time t\noccurs.\nis determined as\nThe healthy node's infection probability can be calculated\nas\n\n$R_{i,t}^{ign} = \begin{cases}\n1 - \\prod\\{v_{j,t}\u2208V_t} (1 - \u03b6B_{j,t})I_{ij,t} & \\text{if } \u03b2^{ign}_{i,t}=0\\\\\n1 - \\prod\\{v_{j,t}\u2208V_t} (1 - \u03b6(1-B_{j,t}))I_{ij,t} & \\text{if } \u03b2^{ign}_{i,t}=1\n\\end{cases}$ (11)\n\n$pr(B_{i,t} = 1|B_{i,t-1} = 0) =  \\prod\\{v_{j,t}\u2208V_t} (1-I_{ij,t}B_{j,t})$  (7)\n\nwhere $\u03b2^{ign}_{i,t} = 0$ and $\u03b2^{ign}_{i,t} = 1$ each represents the healthy and\nwhere $pr(B_{i,t} = 1|B_{i,t\u22121} = 0)$ represents the probability of a the infected states of the node $v^{ign}_{i,t}$. $1-\\prod\\{v_{j,t}\u2208V_t} (1 - \u03b6B_{j,t})I_{ij,t}$\n\nhealthy node $v_{i,t}$ ($B_{i,t-1} = 0$) getting infected ($B_{i,t} = 1$). Its\n\nvalue depends on the epidemic transmissibility $pr = \u03b6$, the represents the probability of a healthy node $v^{ign}_{i,t}$ 's infection\n\ninteractions $I_{ij,t}$ with each neighbour $v_{j,t}$ and the neighbour's given its interactions $I_{ij,t}$ with its neighbour $v_{j,t}$ and the\ninfected condition $B_{j,t} = 1$. neighbour's infected condition 1\n\nThe network state $S^{Net}$ covers the states of each types of $B_{j,t} = 1. \\prod\\{v_{j,t}\u2208V_t} (1 \u2212\n\nnodes and the state of the social connections. \u03b6(1-B_{j,t})) represents the probability of an epidemic spread\n\nfrom the infected node $v^{ign}_{i,t}$ to its neighbours at time t.\n\n$S^{Net} =  \bigcup_{v_{i,t}^{ign} \u2208 V^{Ign}}S^{Ign}_{v_{i,t},t} \u222a \bigcup_{v_{i,t}^{Ego} \u2208 V^{Ego}}S^{Ego}_{v_{i,t},t} \u222a \bigcup_{v_{i,t}^{Cop} \u2208 V^{Cop}}S^{Cop}_{v_{i,t},t} \bigcup E_t$ (8) The state of the directed connection $e_{ij,t}$ from node $v_{i,t}$ to\n\nnode $v_{j,t}$ at time t is defined as:\n\nwhere $S^{Ign}_{v_{i,t},t}$, $S^{Ego}_{v_{i,t},t}$ and $S^{Cop}_{v_{i,t},t}$ each represents the\n\n$Se_{ij,t} (I_{ij,t}, w_{ij,t}, B_{ij,t}, w_{B_{ij,t}})$  (12)\n\nstate of the ignorant node $v_{i,t}^{ign}$, egocentric node $v_{i,t}^{ego}$ and\n\ncooperative node $v_{i,t}^{cop}$ at time t. $E_t$ represents where $I_{ij,t}$ represents the no interaction or interaction from\n\nthe state of social connections between the nodes node $v_{i,t}$ to node $v_{j,t}$ at time t with 0 and 1. $B_{ij,t}$ determines\nat time t. whether this node pair get closely bonded with 0 and 1, each\n\nindicating the unbonded and the bonded node pair. These two\nas\nThe state $S^{Ign}_{v_{i,t},t}$ for the ignorant node $v_{i,t}^{ign}$ can be defined factors are followed by the respective intensity factor $w_{ij,t}$ and\n$w_{B_{ij,t}}$ a.k.a relationship strength.\n\n$S^{Ign}_{v_{i,t},t} (f^{Ign}, SDNA_{i,t}^{Ign}, \u03b2_{i,t}^{Ign}, R_{i,t}^{Ign}, \u015d_{i,t}^{Ign}, B_{i,t}^{Ign}, R_{i,t}^{Ign}, B_{i,t}^{Ign})$ (9) 3) Observation: The nodes observe the related social con-\n\ntact and epidemic spread. The observation $O^{Ign}$ of an\nwhere $f^{ign}$ and $SDNA_{i,t}^{ign}$ each represent the features and ignorant node $v_{i,t}^{ign}$ at time t can be defined as\n\nthe related preferences of node $v_{i,t}^{ign}$ at time t. $B_{i,t}^{Ign}$ is\n\na binary value that represents the node's health condition at time $O_{v_{i,t},t}^{Ign} (S^{Epi}_t, S^{Ign}_{v_{i,t},t}, Se_{i,t}^{Ign}, Se_{i,t}^{Ign})$ (13)\nt. $B_{i,t}^{Ign} = 0$ indicates the healthy condition and $B_{i,t}^{Ign} = 1$\n\nindicates the node's infected condition. The node $v_{i,t}^{ign}$ suffers where $S^{Epi}_t$ represents the state of the epidemic spread at\nthe interaction risk $R_{i,t}^{Ign}$ in an epidemic outbreak and obtains time t. $S^{Ign}_{v_{i,t},t}$ represents the state of the ignorant node $v_{i,t}^{ign}$\nthe interaction reward $R_{i,t}^{Ign}$ from interacting with others at at time t. $Se_{i,t}^{Ign}$ and $Se_{i,t}^{Ign}$ each represents the states of the"}, {"title": null, "content": "directed interactions related to the ignorant node $v_{i,t}^{ign}$ until where \u03b7 > 1 denotes a penalty indicator. It increases the\n\nthe observation time at t. They can be defined as interaction score threshold when the ignorant node is infected\n$B^{ign}_{i,t}$ = 1.\n\n$S^{Ign}_{e_{i,t}} =  \bigcup_{v_{j,t}\u2208V,\u03c4Number of Free-Riders\n(m) \u03c4\u00b2 = 20 and $ = 0.05 (n) \u03c4\u00b2 = 20 and $ = 0.10 (0) \u03c4\u00b2 = 20 and $ = 0.15 (p) \u03c4\u00b2 = 20 and $ = 0.20\n\nFig. 7. The cumulative reward in \"free-rider\" scenarios given various setups of infection rate and recovery time.\ncompetitive influence maximization on evolving social networks,\" in \"Endogenous social distancing and its underappreciated impact on the\n\n2021 IEEE 8th International Conference on Data Science and Advanced epidemic curve,\u201d Scientific reports, vol. 11, no. 1, pp. 1-10, 2021.\n\nAnalytics (DSAA). IEEE, 2021, pp. 1-9. [23] S. Fujimoto, H. Hoof, and D. Meger, \"Addressing function approxi-\n\n[16] Y. He, C. Liang, F. R. Yu, and Z. Han, \"Trust-based social networks with mation error in actor-critic methods,\" in International conference on\n\ncomputing, caching and communications: A deep reinforcement learning. PMLR, 2018, pp. 1587-1596.\n\napproach,\" IEEE Transactions on Network Science and Engineering,\n\nvol. 7, no. 1, pp. 66-79, 2018.\n\n[17] H. Su, Y. Zheng, J. Ding, D. Jin, and Y. Li, \"Rumor mitigation in\n\nsocial media platforms with deep reinforcement learning,\" in Companion"}]}