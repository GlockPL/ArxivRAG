{"title": "Automating the Detection of Code Vulnerabilities by Analyzing GitHub Issues", "authors": ["Daniele Cipollone", "Changjie Wang", "Mariano Scazzariello", "Simone Ferlin", "Maliheh Izadi", "Dejan Kosti\u0107", "Marco Chiesa"], "abstract": "In today's digital landscape, the importance of timely and accurate vulnerability detection has significantly increased. This paper presents a novel approach that leverages transformer-based models and machine learning techniques to automate the identification of software vulnerabilities by analyzing GitHub issues. We introduce a new dataset specifically designed for classifying GitHub issues relevant to vulnerability detection. We then examine various classification techniques to determine their effectiveness. The results demonstrate the potential of this approach for real-world application in early vulnerability detection, which could substantially reduce the window of exploitation for software vulnerabilities. This research makes a key contribution to the field by providing a scalable and computationally efficient framework for automated detection, enabling the prevention of compromised software usage before official notifications. This work has the potential to enhance the security of open-source software ecosystems.", "sections": [{"title": "I. INTRODUCTION", "content": "With increasing pressure on faster software development and continuous updates, the risk of overseeing bugs or vulnerabilities increases. As such, software refinement and repair becomes increasingly critical. Developers work to address vulnerabilities as they are discovered using various methods, ranging from low-level tests on executed code to simulations of potential attack scenarios to prevent system disruption. Despite enormous efforts, it remains challenging to entirely eliminate vulnerabilities, especially in large code bases. Among these, zero-day vulnerabilities pose a particularly severe threat. They are flaws in applications (or even in operating systems) that still have not been discovered by the software manufacturer and, as a result, lack corresponding patches. Google's Threat Analysis Group reported a significant increase in the exploitation of zero-day vulnerability in March 2024, emphasizing the need for more proactive threat detection [1].\nThe Common Vulnerabilities and Exposures (CVE) system, managed by the MITRE Corporation [2], plays a crucial role in proactive and public vulnerability detection and classification. It offers a standardized framework for identifying, cataloging, and disclosing vulnerabilities. To ensure that organizations and tools consistently refer to the same vulnerability, the CVE system assigns a unique identifier to each reported vulnerability through a well-defined process [3]. Thus, each CVE entry includes a unique identifier, a concise description of the vulnerability, and associated metadata such as affected products, severity levels, and potential impacts. On the other hand, the software industry commonly adopts the term embargo for internal CVEs, to restrict or prohibit communication about vulnerabilities [4]. The embargo period describes the time that the flaw is known privately (prior to a deadline, e.g., go-live). Since their impact can be very unpredictable to several parties, embargoed flaws are very time-critical and require to be handled responsibly, thereby reducing the risk of disclosure in GitHub issues or PRs, public Bugzilla or Jira, public mailing-lists, or unnecessary broad communication. After the fix can be made public, the embargo is thus lifted.\nHints about vulnerabilities may be involuntarily disclosed before the official CVE disclosure. Current methodologies for vulnerability detection rely on a combination of community reporting and proactive research conducted by private, specialized security research services. The process significantly involves human expertise to identify vulnerabilities and relies on responsible disclosure, i.e., a vulnerability is made public only after the responsible parties have been given time to apply a patch. Nevertheless, information about vulnerabilities may still be shared through informal channels (e.g., mailing lists, GitHub issues, and blogs) [5], even before an official disclosure [6]. In these contexts, issues may initially be perceived as software bugs or malfunctions rather than recognized as vulnerabilities. For instance, vulnerability CVE-2016-5696 describes an issue in the Linux kernel that allows the hijacking of TCP, and was discussed on social media up to one month before its release in the CVE database [7].\nThe CVE creation process is lengthy. The process from vulnerability discovery to the official assignment of a CVE can be time-consuming, as it involves rigorous validation to confirm that the vulnerability meets the criteria for CVE classification. For example, in the case of CVE-2023-48305 [8], nearly six months passed between the initial discussions of the bug in the GitHub issues to the official creation of the CVE entry. This delay significantly affects end-users, as they may remain unaware of a critical vulnerability and continue using the compromised software without any official notification."}, {"title": "Can Transformer-based models detect early indicators of a potential CVE vulnerability disclosure?", "content": "Large Language Models (LLMs) are Transformer-based AI models trained on vast amounts of text, enabling them to understand and interpret human language, excelling in recognizing text patterns and extracting critical information [9], [10]. Unlike other ML models, LLMs can effectively process extensive human-written documents thanks to their large context windows. Also, LLMs have the capability to analyze and generate code across various programming languages, as demonstrated by their performance across multiple benchmarks [11]. This makes LLMs highly suitable for vulnerability detection, as they can automatically analyze content from several, different sources like social media, blogs, mailing lists, and GitHub issues to identify and report potential vulnerabilities to developers.\nScope of the paper. In this paper, we explore the potential of Transformer-based models to identify software vulnerabilities through the analysis of communication channels, specifically focusing on GitHub issues.\u00b9 This work aims to determine whether it is feasible to identify early indicators of software vulnerabilities with existing models. Our findings not only demonstrate the viability of these approaches, but also highlight their potential to greatly improve current cybersecurity practices by facilitating earlier vulnerability detection, thereby reducing the risk of exploitation.\nContributions. We make three contributions:\nWe are the first to build a dataset that classifies GitHub issues into those connected with CVEs and those that are not, associating the latter with their corresponding CVE records.\nTo the best of our knowledge, using our dataset, we are the first ones to investigate whether Transformer-based models (i.e., LLMs and embedding models) can effectively detect undisclosed vulnerabilities from GitHub issues.\nOur findings validate the effectiveness of Transformer-based models in this context, with accuracy ~2\u00d7 higher compared to statistical approaches. These results highlight their potential for early-stage vulnerability detection."}, {"title": "II. RELATED WORK", "content": "Early vulnerability detection. Detecting vulnerabilities before their public disclosure remains a largely unexplored research area. The study \u201cThe Tweet Advantage\" [7] examines ~709 K tweets that include CVE identifiers posted between May 23, 2016, and March 27, 2018, mapping them to a vulnerability lifecycle model. Their results indicate that nearly one-quarter of vulnerabilities were mentioned on Twitter prior to their official disclosure by vendors or authoritative sources. While the study underscores Twitter's potential as an early warning system for newly discovered vulnerabilities, it relies on simple regexes to detect CVE IDs within the text that indicate vulnerability descriptions. Previous work explored the use of deep learning techniques to assess vulnerability severity based\n1We plan to extend the work to mailing lists and blogs in the future. on textual descriptions. For example, Han et al. [12] integrate a word embedding layer with a Convolutional Neural Network (CNN) layer to automatically identify critical sentences in vulnerability descriptions and predict their severity. However, this approach relies on humans to first identify and extract relevant vulnerability descriptions, which presents additional challenges for developers who must manually filter input data before applying the model. Instead, we aim to build a system that autonomously selects and analyses the relevant input data.\nLLMs for vulnerability detection. Although the application of LLMs for vulnerability detection is not novel, most existing research focuses on identifying potential bug risks within code snippets [13]\u2013[16]. These methods are designed to detect vulnerabilities within well-defined contexts, such as through static code analysis. In contrast, our paper introduces a new approach by leveraging LLMs to detect vulnerabilities that have not yet been officially disclosed, using textual sources such as social media and GitHub issues."}, {"title": "III. A DATASET FOR VULNERABILITY DETECTION", "content": "Before applying different Transformer-based models, it is necessary to identify a suitable dataset for the task. Although previous research has explored the use of LLMs for vulnerability detection, these studies have primarily focused on static code analysis, hence, lacking any textual-based dataset derived from mailing lists or similar sources.\nTherefore, we are the first to create a dataset that directly links vulnerability-related textual discussions from GitHub issues in open-source projects with their corresponding entries in the CVE database.\nIn the following, we provide a detailed explanation of the methodology used for the dataset creation. We first outline the data source chosen for extraction, followed by a description of the data collection pipeline.\nData source. We aim to build a dataset that determines whether a specific textual description of an unexpected behavior is associated with a potential vulnerability. To achieve this, we require a vast amount of textual data paired with a ground truth indicating whether the text pertains to a disclosed vulnerability or not. We rely on the National Vulnerability Database (NVD), the U.S. government's repository of standards based vulnerability management data [17]. NVD offers a standardized platform for systematically identifying and tracking CVE-based vulnerabilities. It collects comprehensive information on vulnerabilities, including CVE identifiers, affected software versions, severity ratings, and potential impact assessments. Most importantly, each NVD entry provides links to related external resources, e.g., issues tracking (on GitHub or similar websites), release notes, and vendor advisories.\nData collection. We initially extracted all the vulnerabilities published in the NVD from January 1st, 2019, to June 2nd, 2024. The extracted data include key information such as CVE identifiers, vulnerability descriptions, and severity metrics. We excluded vulnerabilities that were under examination or had been rejected from the dataset, resulting in a total of 113,735"}, {"title": "IV. TRANSFORMER-BASED VULNERABILITY DETECTION", "content": "In this section, we explore the potential of using Transformer-based models for early vulnerability detection through textual data, i.e., GitHub issues. Leveraging the dataset built in Sec. III, we design three different approaches to detect vulnerabilities: (i) an embedding-based classifier employing XGBoost, (ii) an LLM-based detection utilizing GPT-3.5 (both base and fine-tuned versions), and (iii) a combined approach that integrates both LLMs and XGBoost."}, {"title": "A. Embedding-based Vulnerability Classifier", "content": "In the first approach, we do not directly rely on LLMs but instead use embedding models to train a task-specific classifier. Embedding models convert text into dense, high-dimensional vectors that capture semantic relationships and contextual nuances [18]. Modern embedding models are often derived from pre-trained LLMs [19], [20], as they can leverage the deep contextual comprehension and rich semantic representations acquired during their training. This leads to more precise and meaningful embeddings for downstream applications.\nIn our approach, depicted in Fig. 1, we begin by representing each GitHub issue (using the format shown in Fig. 2) as a vector in a continuous space, allowing us to identify patterns that may not be visible in the raw text. These embeddings are then fed as input features for a classifier, which we trained to produce a binary prediction on whether a given GitHub issue is associated with a vulnerability or not.\nModel selection. For embedding generation, we selected two models: the proprietary text-embedding-3-large from OpenAI [21], and the open-source NV-Embed-v2 from Nvidia [19], which, at the time of writing, is the leading model on the Massive Text Embedding Bench-mark (MTEB) leaderboard [22]. NV-Embed-v2 is derived from Mistral-7B-v0.1, while there is no public in-formation available on the architecture and training of text-embedding-3-large. For the classification task, we chose the XGBoost model [23], a powerful gradient-boosting algorithm well-known for its strong performance in classification tasks. We selected XGBoost due to its capability to handle large datasets, its resilience to overfitting, and its efficiency in producing accurate predictions 2.\nIntuition: vulnerabilities are captured in the embedding space. We run a simple analysis to explain why we believe embedding models are themselves a potentially powerful tool to predict vulnerabilities. Fig. 3 shows a t-SNE projection of the distribution of our dataset of GitHub issues, revealing how issues cluster within the embedding space and indicating the potential for effective classification."}, {"title": "B. LLM-based Vulnerability Detection", "content": "In this approach, we use LLMs to assess whether a GitHub issue is relevant for identifying a vulnerability. Compared to the binary classifier, an LLM provides several advantages, including the ability to conduct a more nuanced analysis of\n2We leave the comparison with other classification models as future work."}, {"title": "C. The Combined Approach", "content": "In this approach, we explore whether combining the LLM with the embedding-based classifier can improve vulnerability detection. Specifically, we aim to leverage the best of both strategies: (i) the ability of LLMs to extract key information and generate detailed textual vulnerability descriptions, and (ii) the superior performance of a task-specific classifier. LLMs excel at summarizing textual content that may include complex technical details or specialized jargon from particular software communities or programming languages. Conversely, while the task-specific classifier could surpass the LLM in classification accuracy, it only produces a binary output and does not provide a human-readable explanation of why a specific GitHub issue may indicate a potential vulnerability, which is much more valuable for developers and users.\nAs depicted in Fig. 4, we start by prompting the LLM to describe and explain the input GitHub issue, summarizing the key details and highlighting the main problem discussed in the issue. This output is then converted into an embedding, which is fed into the classifier as input. The outcome of this pipeline includes both a boolean value indicating whether the GitHub issue is a potential vulnerability (from the XGBoost classifier) and a detailed description (generated by the LLM).\nModel selection. We once again use GPT-3.5-Turbo as our LLM to interpret and explain the GitHub issues, for the reasons outlined in Sec. IV-B. The output is then embedded using NV-Embed-v2, followed by an XGBoost classifier on the embedded descriptions."}, {"title": "V. EVALUATION", "content": "In this section, we present a detailed analysis of the results from applying the different approaches outlined in Sec. IV to the dataset we created (see Sec. III).\nWe begin by examining the key performance metrics of the various approaches, i.e., precision, recall, and F1 score, and then discuss the practical implications of each strategy. Our focus extends beyond simply detecting vulnerabilities to also providing users with insightful descriptions and potential vulnerability alerts. Thus, we then quantitatively assess the quality of the descriptions generated by the LLMs.\nComparison baseline. We compare our proposed approaches with a baseline developed using techniques outlined in the paper \"The Tweet Advantage\" [7]. This baseline relies on statistical methods aimed at identifying keywords that signal vulnerability descriptions. The original method employed a regex to detect the CVE IDs within the text. For our implementation, we extended this by incorporating the following additional keywords to improve the identification of relevant issues: \"vulnerability\u201d, \u201cNVD\u201d, and \u201csecurity\".\nDataset split. As discussed in Sec. IV, we chose GPT-3.5-Turbo as our primary LLM, which has a training cutoff date of September 2021. However, our dataset includes vulnerabilities published before this date (i.e., from January 2019). To ensure that the evaluation data does not overlap with the model's training data, we excluded all issues and vulnerabilities created before the cutoff date, which accounted for 15% of the total dataset. An additional 3% of issues were excluded to prevent data contamination, as these issues were created before the cutoff date, but the vulnerabilities were detected afterward. We then divided the remaining data into two splits: post-cutoff training and post-cutoff test. The post-cutoff training split, comprising 49% of the total dataset, was used exclusively for training the XGBoost classifiers and fine-tuning the LLM. The post-cutoff test, making up 33% of the total dataset, is solely employed for model evaluation.\nNote that the dataset composition is imbalanced, with a larger proportion of issues not linked to any vulnerabilities (i.e., 3,611 non-vulnerability entries vs. 443 vulnerability entries). We intentionally did so to reflect a realistic scenario where vulnerability-related issues are less frequent."}, {"title": "A. Performance Evaluation", "content": "We begin by analyzing the key performance metrics of the various approaches. Table II reports precision, recall, and F1 score for the two main classes of interest, vulnerabilities (i.e., Vuln.) and non-vulnerabilities (i.e., No Vuln.), as evaluated on the post-cutoff test split of our dataset.\nTransformer-based approaches outperform statistical methods. We start by briefly examining the performance of all our Transformer-based approaches compared to the regex-based methodology of the baseline. Our proposed approaches retain a higher F1 score, with the combined pipeline (last row of the table) performing ~2\u00d7 better in detecting vulnerabilities compared to the baseline. The results confirm our hypothesis that Transformer-based models are more capable of identifying vulnerabilities, thanks to their deeper semantical understanding of the textual data, underscoring their potential for more accurate and reliable early vulnerability detection in real-world scenarios.\nThe LLM-based approach is not the best performing, but newer models could improve results with larger datasets. Despite LLMs have demonstrated exceptional performance across various domains, they do not perform best in the task of vulnerability detection (see second and third row of Table II). We speculate this is primarily due to two factors: (i) the chosen model version (GPT-3.5-Turbo) is somewhat outdated and lacks the advanced reasoning capabilities present in newer versions. However, using more recent models was not feasible due to their training cutoff date, as it would reduce the dataset to fewer than 100 items, which is insufficient for effectively validating our approach; and (ii) by handpicking some of the issues in the dataset, we observed that some refer to problems such as buffer overflows, heap overflows, or program crashes. In these cases, the LLM tends to flag such issues as potential vulnerabilities due to their severity. However, since these issues are not linked to any CVE entries, they are marked as incorrect responses during the evaluation process. To make a fairer evaluation, we fine-tuned GPT-3.5-Turbo using the same training set as the XGBoost classifier. As expected, the F1 score slightly improved, but the previously mentioned issues continued to persist. Indeed, this is not a definitive conclusion, as we believe that newer models could achieve better performance if evaluated with a sufficiently large post-cutoff dataset, which remains our future work. Moreover, also asking the LLM to be more or less conservative with the answers, may affect the results, which we leave for future studies.\nEmbedding models can detect vulnerabilities with high accuracy. Since the embedding models are also based on the Transformer architecture, they can focus on various parts of the input sequence, effectively capturing contextual relationships and semantic details. Combining these embeddings with XGBoost resulted in high accuracy for detecting vulnerabilities, as shown in Table II (fourth and fifth rows). Surprisingly, the open-source NV-Embed-v2 produced higher-quality embeddings than text-embedding-3-large from OpenAI. As mentioned in Sec. IV, the NV-Embed-v2 model is based on Mistral-7B-v0.1, which has a training cutoff around August 2021, aligning with the timeframe of the evaluation dataset and ensuring the results are valid. This finding has several important implications: (i) it demonstrates that open-source models can outperform proprietary ones, which is beneficial for scenarios where it is not possible to access external APIs; (ii) open-source models are typically smaller, allowing deployment and execution on commodity"}, {"title": "hardware, thereby reducing operational costs; and (iii) the overall pipeline remains highly efficient, as both the embedding model and XGBoost have significantly lower computational costs compared to LLMs.", "content": "This efficiency makes it an ideal and practical solution for this task, since it requires analyzing large volumes of textual data from diverse sources in real time. However, the main limitation of using binary classifiers is their inability to provide a textual description of the issue, which is instead valuable for developers and users.\nThe combined model achieves the best performance while providing insightful vulnerability descriptions. As already mentioned, the combined pipeline achieves an F1 score for vulnerability detection that is twice as high as the baseline (see the last row of Table II). It effectively combines superior classification accuracy (using NV-Embed-v2 and XGBoost) with the capability to produce informative descriptions of potential vulnerabilities. Although the improvement in classification performance over the embedding-only model is moderate, this integration allows for more detailed analysis of the issues through embeddings, boosting overall performance. We expect our system to scan millions of GitHub issues in real time, therefore efficiency and costs are important factors to consider. The combined approach stands out as the most efficient and cost-effective option, leveraging an economical LLM (GPT-3.5-Turbo) alongside a compact embedding model and XGBoost classifier that can be easily deployed on commodity hardware. Additionally, GPT-3.5-Turbo can be replaced with a similar open-weights LLM (e.g., Mistral-7B), further reducing operational costs. Indeed, we hypothesize that further improvements could be realized by using an LLM with advanced reasoning capabilities to generate the descriptions. In our evaluation, we used previously discovered vulnerabilities, so it was essential to consider the training cutoff to prevent biases. However, in a real-time deployment, it would be impossible for the LLM to include newly created GitHub issues or CVEs, allowing the use of more powerful models. We plan to explore this additional evaluation in future work.\nSensitivity analysis. We trained our XGBoost classifier on a dataset that consisted of 12.2% of positive elements (i.e., vulnerabilities), and tested it on a dataset with the same ratio. Now, we aim to assess how the F1 score varies with changes in this ratio without retraining the XGboost classifier, i.e., we do not modify the classifier but only adjust the proportion of positive elements over the total dataset. To calculate the F1 score for different ratios, we leverage the recall metric on both positive and negative samples since the recall metric does not depend on the ratio of positive to negative samples.\nFig. 5 shows the F1 score for vulnerability detection (y-axis) as the proportion of positive instances in the test dataset varies between 8% and 20%. We skew the analysis towards higher values as we hypothesize that most vulnerabilities after the cutoff date may still require to be reported through CVEs. This will inevitably increase the ratio once these vulnerabilities are reported. Our results demonstrate that the combined approach remains robust and outperforms other methods. Additionally, we notice that the performance of LLM-based approaches improves as the ratio increases, which aligns with the observation that LLMs tend to be \u201cconservative\u201d and flag anything potentially suspicious as a vulnerability.\nWe leave the dynamic retraining of the classifier based on the observed precision and recall metrics as an open question. This task is non-trivial as we observe a sharp difference, for instance, in the ratio of positive to negative samples in the pre- and post-cutoff datasets."}, {"title": "B. Quality of the LLM-Generated Descriptions", "content": "One significant advantage of utilizing LLMs is their ca\u0440\u0430-bility to generate insightful descriptions and identify potential vulnerability alerts. In the following analysis, we assess the quality of these generated descriptions to determine whether they provide sufficient information to indicate a vulnerability. We compare the descriptions produced by GPT-3.5-Turbo and its fine-tuned version with the official descriptions in the CVE records. To evaluate the similarity between the two descriptions, we (i) generate their embeddings using again the NV-Embed-v2 model, which converts the texts into dense vector representations to capture semantic meaning, and (ii) compute the cosine similarity between the vectors. The results are presented in Fig. 6, which displays a histogram of the distribution of similarity scores. Notably, the distribution of descriptions generated by the fine-tuned GPT-3.5 (Fig. 6(b)) shows a higher density at higher similarity levels compared to the base GPT-3.5 (Fig. 6(a)).\nOur results provide two important insights: (i) LLMs are capable of generating vulnerability descriptions that are semantically similar to the official descriptions. This confirms that LLMs excel in summarization and can offer valuable details for future, undisclosed vulnerabilities; and (ii) fine-tuning enhances the performance of the base LLM, allowing GPT-3.5 to generate descriptions that more closely match those found in CVE records."}, {"title": "VI. CONCLUSION", "content": "In this paper, we reveal the challenge of vulnerability detection prior to their official disclosure date and propose using Transformer-based models as a solution. By building a dataset that links GitHub issues with their corresponding CVE references, and evaluating three different Transformer-based approaches, we demonstrate that these models can simplify and effectively address this task. This work contributes by (i) introducing a novel research direction: detecting early indicators of potential CVE vulnerabilities, and (ii) proposing a framework for automated vulnerability detection that balances accuracy, explainability, and computational efficiency. By addressing these key challenges, our work provides valuable tools to help developers identify and mitigate vulnerabilities in their code bases, significantly enhancing the security of open-source software ecosystems."}]}