{"title": "Meta-DiffuB: A Contextualized Sequence-to-Sequence\nText Diffusion Model with Meta-Exploration", "authors": ["Yun-Yen Chuang", "Hung-Min Hsu", "Kevin Lin", "Chen-Sheng Gu", "Ling Zhen Li", "Ray-I Chang", "Hung-yi Lee"], "abstract": "The diffusion model, a new generative modeling paradigm, has achieved signif-\nicant success in generating images, audio, video, and text. It has been adapted\nfor sequence-to-sequence text generation (Seq2Seq) through DiffuSeq, termed\nS2S Diffusion. Existing S2S-Diffusion models predominantly rely on fixed or\nhand-crafted rules to schedule noise during the diffusion and denoising processes.\nHowever, these models are limited by non-contextualized noise, which fails to\nfully consider the characteristics of Seq2Seq tasks. In this paper, we propose the\nMeta-DiffuB framework\u2014a novel scheduler-exploiter S2S-Diffusion paradigm\ndesigned to overcome the limitations of existing S2S-Diffusion models. We employ\nMeta-Exploration to train an additional scheduler model dedicated to scheduling\ncontextualized noise for each sentence. Our exploiter model, an S2S-Diffusion\nmodel, leverages the noise scheduled by our scheduler model for updating and gen-\neration. Meta-DiffuB achieves state-of-the-art performance compared to previous\nS2S-Diffusion models and fine-tuned pre-trained language models (PLMs) across\nfour Seq2Seq benchmark datasets. We further investigate and visualize the impact\nof Meta-DiffuB's noise scheduling on the generation of sentences with varying\ndifficulties. Additionally, our scheduler model can function as a \"plug-and-play\"\nmodel to enhance DiffuSeq without the need for fine-tuning during the inference\nstage.", "sections": [{"title": "1 Introduction", "content": "The diffusion model, a novel generative approach, operates through a two-step process: it first\nintroduces noise to real data and then systematically removes this noise to facilitate data generation\n[12, 40, 30]. This model has demonstrated significant efficacy across several domains, including\nimage [13, 29, 38], audio [36, 18], video [18, 14], and text generation [1, 15, 21, 3, 24, 34]. The\ndiffusion model utilizes a technique known as noise scheduling to control the amount of noise imposed\nat each diffusion step [12]. DiffuSeq [8] has adapted this model to discrete generation tasks like\nsequence-to-sequence text generation (Seq2Seq), under a framework termed S2S-Diffusion. However,\nDiffuSeq employs fixed noise scheduling and does not accommodate the specific characteristics of\nSeq2Seq tasks [45, 44]."}, {"title": null, "content": "Seq2Seq is a foundational technique in natural language processing (NLP) that generates target\nsentences from specified conditional sentences. It supports a range of downstream tasks, including\nlanguage translation [41], image captioning [35], conversational modeling [39], and text summa-\nrization [28]. For Seq2Seq tasks, it is more reasonable to impose different levels of noise to each\nsentence in S2S-Diffusion models to address the varying semantic and contextual difficulties of\ngenerating sentences. This noise scheduling strategy can better adapt to the semantic characteristics\nand generation difficulties of each sentence, thereby improving the model's performance in various\ngeneration tasks. To meet the unique demands of S2S Diffusion, we introduce a contextualized\nnoise-scheduling strategy that accounts for the semantics of each conditional sentence and adapts\nto different training epochs. Existing S2S-Diffusion models, such as DiffuSeq, lack flexibility due\nto their reliance on fixed, non-contextualized noise-scheduling strategies. Furthermore, models like\nSeqDiffuSeq [45] and Dinoiser [44], which propose adaptive noise scheduling, are also limited by\ntheir non-contextualized approach.\nTo address the semantics of discrete conditional sentences for contextualized noise scheduling, we\nintroduce a novel scheduler-exploiter framework, Meta-DiffuB, which achieves trainable noise-\nscheduling inspired by Meta-Exploration [43]. Within this framework, our scheduler model dy-\nnamically schedules noise to train our exploiter model, which is updated based on the performance\nrewards it generates. Our exploiter model, an S2S-Diffusion model, leverages the noise scheduled\nby the scheduler model for updates and generation. By design, Meta-DiffuB naturally implements\ncontextualized noise scheduling. It achieves state-of-the-art performance on four Seq2Seq benchmark\ndatasets, outperforming existing S2S-Diffusion models [8, 45, 44] and fine-tuned pre-trained language\nmodels (PLMs) [10, 33].\nIn summary, we make three primary contributions with Meta-DiffuB:\n\u2022 We introduce and demonstrate the application of Meta-Exploration to diffusion models in\nSection 3, proposing Meta-DiffuB as a strategy to enhance S2S-Diffusion models. Our\nmain results, presented in Section 6.1, confirm that Meta-DiffuB achieves state-of-the-art\nperformance across four benchmark datasets.\n\u2022 We detail the operation of our scheduler model in Section 6.2, highlighting its capability to\nschedule noise. The noise scheduling approach of our scheduler model-applying less noise\nto the harder sentences and more to the easier ones-enhances the diversity and quality of\nthe generated text.\n\u2022 We reveal that our scheduler model can function as a \"plug-and-play\" model, easily integrated\ninto existing S2S-Diffusion models to enhance inference performance, as detailed in Section\n6.3."}, {"title": "2 Problem Statement, Preliminary", "content": null}, {"title": "2.1 Problem Statement", "content": "In this work, we focus on sequence-to-sequence text generation tasks. Given a conditioning sentence\nof length m, $w^{x} = \\{w_{1},..., w_{m}\\}$ , our objective is to train a diffusion model capable of generating a\ntarget sentence of length n, $w^{y} = \\{w_{1},..., w_{n}\\}$, based on the conditional sentence. Here, $w^{x}$ and\n$w^{y}$ represent the conditional and target sentences, respectively."}, {"title": "2.2 Preliminary", "content": "DiffuSeq [8] primarily follows the transformation method of Diffusion-LM [21] and incorporates\nthe diffusion and denoising processes from [12]. In the diffusion process, Diffusion-LM transforms\ndiscrete sentences into a continuous space. Given the real-world training sentence pair $w^{x \\oplus y}$\nconcatenated by $w^{x}$ and $w^{y}$, Diffusion-LM uses an embedding function emb to transform $w^{x\\oplus y}$\ninto continuous space, thereby obtaining the distribution $z_{0} \\sim q(z)$, where q represents the diffusion\nprocess. Then, $z_{0}$ is subjected to imposed noise, diffusing into a standard Gaussian distribution\n$z_{T} \\sim N(0,I)$. At each diffusion step t\u2208 [1,2,..., T], the noise is regulated by $q(z_{t}|z_{t-1}) =$\n$N(z_{t}; \\sqrt{1 - \\beta_{t}}z_{t-1}, \\beta_{t}I)$, where $\\beta_{t} \\in (0, 1)$ controls the amount of noise imposed at each diffusion\nstep. We denote 3 as containing a set of noise values $\\beta_{t}$, where a larger $\\beta_{t}$ indicates more Gaussian\nnoise imposed at that diffusion step. When t is large enough, $z_{0}$ gradually evolves into a standard"}, {"title": null, "content": "Gaussian noise distribution. The random distribution is gradually reduced in noise during the\ndenoising process to regenerate target sentences. The denoising process, which recovers $z_{0}$ by\nreducing the noise in $z_{t}$, can be defined as follows:\n$p_{\\theta}(z_{0:T}) = p(z_{T}) \\prod_{t=1}^{T} p_{\\theta}(z_{t-1}|z_{t}).$ (1)\nDiffusion-LM employs a trained, parameterized denoising distribution $z_{t-1} \\sim p_{\\theta}(z_{t-1}|z_{t})$ to grad-\nually recover $z_{t}$ from noise. This denoising distribution, parameterized by 0, is tailored to fit the\nposterior distribution $q(z_{t-1}|z_{t}, z_{0})$ of the forward process. The key difference between DiffuSeq [8]\nand Diffusion-LM [21] is that DiffuSeq imposes noise only on the target sentence part of $z_{t}$ to\nachieve classifier-free S2S Diffusion, termed Partial Noise [8]. Due to the implementation of Partial\nNoise in the diffusion process, conditional denoising is inherently classifier-free. To transform the\ncontinuous $z_{0}$ target sentences back into discrete sentences $w^{y}$, previous S2S-Diffusion models use a\nRounding Operation [21] to map the target sentence part of $z_{0}$ into $w^{y}$. The Rounding Operation is a\nmethod for choosing the most probable word for each position [21]. The denoising process primarily\nutilizes the variational lower bound ($L_{VLB}$) to optimize the negative log-likelihood [12]. Through\nthe simplification and derivation from DiffuSeq [8], the training objective function for S2S-Diffusion\nmodels can be defined as:\n$Min L_{VLB} = min(\\sum_{t=2}^{T} ||z_{z_{0}} - f_{\\theta}(z_{t}, t)||^{2} + || emb(w^{x \\oplus y}) - f_{\\theta}(z_{1}, 1)||^{2} + R(||z_{0}||^{2})], (2)$\nwhere learning process $p_{\\theta} (z_{t-1}|z_{t})$ is modeled as Transformer model $f_{\\theta}$. Previous diffusion models\ndeploy $\\beta$ by dividing the interval between the minimum value $\\beta_{1}$ and the maximum value $\\beta_{T}$ using a\nmathematical function to determine the fixed noise sequence $\\{\\beta_{1}, ..., \\beta_{T}\\} \\in \\beta$, as described in [12].\nThe mathematical function used by Diffusion-LM and DiffuSeq [21] is the sqrt function, which\nhas demonstrated superior performance in text generation compared to other fixed mathematical\nfunctions. However, DiffuSeq's noise scheduling is constrained by its non-contextual approach; it\ndoes not account for the semantics of each conditional sentence nor does it adapt to different training\nepochs."}, {"title": "3 Methodology", "content": "In this work, we propose a scheduler-exploiter framework named Meta-DiffuB for training S2S-\nDiffusion models with contextualized noise. Inspired by [43], our Meta-DiffuB includes a scheduler\nmodel, $B_{\\psi}$, parameterized by $\\psi$, and an exploiter model, $D_{\\theta}$, parameterized by $\\theta$. $B_{\\psi}$, a simple\nSeq2Seq model, considers the semantics of conditional sentences to schedule contextualized $\\beta$ for\nupdating $D_{\\theta}$ and is also updated based on the learning effectiveness of $D_{\\theta}$\u2014which refers to how well\nthe exploiter learns. Our exploiter, $D_{\\theta}$, an S2S-Diffusion model, leverages the noise scheduled by\n$B_{\\psi}$ for its updating and generation. The framework of our Meta-DiffuB, compared with DiffuSeq, is\nvisualized in Figure 1."}, {"title": "3.1 Noise Scheduling in the Scheduler Model", "content": "In this work, we propose a simple two-step approach for our scheduler $B_{\\psi}$, which is a Seq2Seq\nmodel, to schedule a set of noise values $\\beta_{t}$. Here, a larger $\\beta_{t}$ indicates more noise imposed\non the data. The input to $B_{\\psi}$ is consistently $w^{x}$ across both training and inference stages. Instead\nof directly scheduling the values of $\\beta$, $B_{\\psi}$ outputs a series of Meta-Instructions, simplifying the\ntraining into a time-series binary classification problem. In the first step, $B_{\\psi}$ samples a series of\nMeta-Instructions $l^{e} = \\{l_{1},..., l_{t}, ..., l_{T}\\}$ from $w^{x}$, where each $l_{t}$ is labeled either True or False.\nWe propose a 'skipping' method: a True label directs $B_{\\psi}$ to increase the noise by selecting $\\beta_{t+1}$\nfor the next diffusion step, whereas a False label maintains the same noise level $\\beta_{t}$. In the second\nstep, we transform $l^{e}$ using the fixed noise sqrt-function $\\beta^{sqrt} = \\{\\beta_{1},..., \\beta_{T}\\}$, as deployed by\n[21, 8], through the \u2018skipping\u2019 method to generate the new noise values $\\beta^{e} = \\{\\beta_{1}^{e}, ..., \\beta_{t}^{e}, ...\\}$. For\nexample, with the continuous Meta-Instructions $l = \\{T, F,T\\}$ and fixed noise values \\{1,2,3\\},\nour new scheduling of noise values will be \\{1,1, 2\\}. If consecutive scheduling noise values are the\nsame, no additional noise is introduced at that diffusion step [12]. Our two-step approach maintains\nthe same diffusion steps for parallel operations and contextualized $\\beta^{e}$ in the diffusion process. We\nutilize a Policy Gradient to update our scheduler model following Meta-Exploration, addressing the\nnon-differentiability of our two-step approach. The noise-scheduling mechanism of our scheduler\nmodel can be defined by the following equations:\n$l^{e} = B_{\\psi}(w^{x})$\n$\\beta^{e} = skipping(l^{e}, \\beta^{sqrt}).$ (3)"}, {"title": "3.2 Training the Exploiter", "content": "Unlike previous S2S-Diffusion models [8, 45, 44] that employ fixed or hand-crafted noise scheduling,\nwe utilize contextualized $\\beta^{e}$ to impose noise during the diffusion process. We also implement Partial\nNoise to achieve classifier-free S2S Diffusion [8]. In the denoising process, our exploiter model\n$D_{\\theta}$ restores the diffused data to generate the target sentences. During the diffusion process, we\nadopt the transformation method of Diffusion-LM to obtain $emb(w^{x+y})$, as described in Section 2.\nWe extend the original diffusion chain to a new Markov transition with our $\\beta^{e}$: $q_{\\theta}^{\\prime}(z_{0}|W^{x+y}) =$\n$N(emb(w^{x+y}), I)$ [21, 8]. Consequently, we can implement the objective function indicated in\nSection 2, derived from previous classifier-free S2S-Diffusion methods, to update our exploiter model\n$D_{\\theta}$ [8]. The training objective function for our exploiter model $D_{\\theta}$ in collaboration with $B_{\\psi}$ can be\ndefined as follows:\n$V_{\\theta}J(\\theta) = min[\\sum_{t=2}^{T} ||z_{0} - V_{\\theta}D_{\\theta}(z_{t}^{e}, t)||^{2} + || emb(w^{x \\oplus y}) - V_{\\theta}D_{\\theta}(z_{1}^{e}, 1)||^{2} + R(||z_{0}||^{2})].$ (4)\nSince $z_{0}$ is not diffused, there is no need to add the superscript of $B_{\\psi}$. $J(\\theta)$ is denoted as the gradient\nfor updating exploiter model $D_{\\theta}$. Then, we can update our exploiter model $D_{\\theta}$'s network weights:\n$\\theta^{\\prime} \\rightarrow \\theta + V_{\\theta}J(\\theta).$ (5)"}, {"title": "3.3 Contextualized Inference with Meta-DiffuB", "content": "In the inference stage, if our goal is to generate outputs based on $w^{x}$, $B_{\\psi}$ predicts contextualized\n$\\beta^{e}$ using $w^{x}$, as demonstrated in Section 3.1. We then concatenate $emb(w^{x})$\u2014transformed from"}, {"title": "3.4 Estimating the Meta-Reward of the Scheduler Model", "content": "In this section, we estimate the Meta-Reward of our scheduler model, which reflects the learning\neffectiveness of $D_{\\theta}$. We let $D_{\\theta}$ generate $Y_{D}$ and $D_{\\theta^{\\prime}}$ generate $Y_{D^{\\prime}}$, respectively, where Y\ndenotes the generated $w^{y}$ [21]. We assess the rewards for $Y_{D_{\\theta}}$ and $Y_{D_{\\theta^{\\prime}}}$, denoted as $R_{D}$ and $R_{D^{\\prime}}$,\nrespectively, which represent the rewards for $D_{\\theta}$ and $D_{\\theta^{\\prime}}$. In this study, we utilize the BLEU score\nto quantify these rewards. Consequently, the reward for the scheduler model (i.e., Meta-Reward) is\ndefined as $R_{B_{\\psi}} = R_{D_{\\theta^{\\prime}}} - R_{D_{\\theta}}.$"}, {"title": "3.5 Training the Scheduler Model with Meta-Reward", "content": "Since $B_{\\psi}$ generates Meta-Instructions to diffuse sentences using our two-step approach described in\nSection 3.1, we update the scheduler via policy gradients, incorporating both Meta-Instructions and\nthe calculated Meta-Rewards [43]. The training objective function for our scheduler model is defined\nas follows:\n$\\nabla_{\\psi}J(\\psi) = \\sum_{t=1}^{T} \\nabla_{\\psi}B_{\\psi}(l_{t}^{e} | w^{x}) \\cdot R_{B_{\\psi}}.$ (6)\nAfter we obtain $\\nabla_{\\psi}J(\\psi)$, we can update $B_{\\psi}$'s network weights:\n$\\psi^{\\prime} = \\psi + \\nabla_{\\psi}J(\\psi).$ (7)"}, {"title": "3.6 Exploration Epochs", "content": "Inspired by the exploration epochs of Meta-Exploration [43, 5, 19], we iteratively execute the\nprocedures from Section 3.1 to Section 3.4 to collect various indicators of learning effectiveness from\n$D_{\\theta}$ for updating $B_{\\psi}$. In practice, we keep the network weights of $D_{\\theta}$ fixed until the exploration epochs\nare completed. This approach ensures that the scheduler model schedules noise to $D_{\\theta}$ with consistent\nnetwork weights, promoting stable training [5]. Additionally, we can conduct the exploration epochs\nin parallel to save time by collecting learning effectiveness from $D_{\\theta}$ under consistent network weights.\nAfter accumulating the gradients for $B_{\\psi}$ from these exploration epochs, we update $B_{\\psi}$ to $B_{\\psi^{\\prime}}$, which\nin turn schedules new noise to update $D_{\\theta}$ to $D_{\\theta^{\\prime}}$. In summary, we present Algorithm (1) to detail the\nfull training process of the proposed Meta-DiffuB. The number of exploration epochs is denoted by\nE, with e \u2208 \\{1, ..., E\\} indexing the exploration epochs."}, {"title": "4 Experiments", "content": "In this section, we conduct experiments to verify the performance of our Meta-DiffuB on four\nbenchmark Seq2Seq datasets [48, 6, 17, 8]. We benchmark Meta-DiffuB against previous S2S-\nDiffusion models and fine-tuned pre-trained language models (PLMs), using the same datasets and\ntraining settings as employed by DiffuSeq [8]."}, {"title": "4.1 Datasets", "content": "In our experiment, we use four datasets: the Commonsense Conversation dataset (CC) [48], the\nQuasar-T dataset (QT) [6], the Wiki-Auto dataset (WA) [17], and the Quora Question Pairs dataset\n(QQP) [8]. These datasets consider a variety of tasks, including open-domain dialogue generation,\nquestion generation, text simplification, and paraphrase generation tasks, all within Seq2Seq contexts.\nFor a fair comparison, we employ the same datasets with identical settings for training all mentioned\nmodels, as outlined in [8, 45]. Detailed settings of these datasets are provided in Appendix A."}, {"title": "4.2 Baselines", "content": "We compare the proposed Meta-DiffuB with previous S2S-Diffusion models, including DiffuSeq [8],\nDinoiser [44], and SeqDiffuSeq [45]. DiffuSeq employs a fixed noise pattern in the training and\ninference stages using a sqrt function and has been successfully introduced to the Seq2Seq task as\nthe basic diffusion model. We also compare Meta-DiffuB with Dinoiser and SeqDiffuSeq, which are\nexisting S2S-Diffusion models that focus on noise scheduling. Dinoiser and SeqDiffuSeq utilize hand-\ncrafted rules that provide adaptive but not contextualized noise scheduling. Additionally, following\n[8, 45], we compare our Meta-DiffuB with three PLMs on Seq2Seq tasks. These PLMs include\nthe fine-tuned GPT-2-base (GPT2-base) [33], fine-tuned GPT-2-large (GPT2-large), and fine-tuned\nLevenshtein Transformer (LevT) [10]. We detail these baselines in Appendix B."}, {"title": "4.3 Training Setting", "content": "Our exploiter model employs the same network architecture and settings as DiffuSeq [8]. Our\nscheduler model uses the same network architecture as described in [4]. The exploiter model and\nscheduler architectures are detailed in Appendix C. For consistent comparison, all S2S-Diffusion\nmodels [8, 44, 45] follow the experimental settings of prior research [8] and are trained from scratch.\nThe diffusion step count is set at 2,000, and the maximum sequence length is 128. The Minimum\nBayes risk (MBR) [23] decoding size, denoted as |S|, is 10; this involves generating sentences from\n10 random seeds and selecting the best output sequence. Details on the implementation of MBR for\nall S2S-Diffusion models can be found in Appendix 6. The total batch size for both training and\ntesting phases is 2048. Experiments are conducted on NVIDIA A100 Tensor Core GPUs, utilizing 4\nGPUs for training and a single GPU for inference."}, {"title": "4.3.1 Discussion of Computational Intensity", "content": "To ensure a fair comparison during parallel exploration epochs, we avoid increasing the total batch\nsize. Instead, we reduce the batch size by dividing the total batch size by the number of exploration\nepochs deployed. In this work, we set the number of exploration epochs to 32 and the batch size to 64.\nTo update our scheduler, we run parallel exploration epochs every 100 training epochs with a total\nbatch size of 2048. The increased computational complexity of applying Meta-DiffuB to DiffuSeq is\npresented in Table 1."}, {"title": "4.4 Evaluation Metrics", "content": "To ensure a fair comparison, we follow the same evaluation metric settings as those used in previous\nS2S-Diffusion models [8, 45]. For quality assessment, we utilize standard text generation metrics such"}, {"title": "5 Model-Agnostic Characteristics of Meta-DiffuB", "content": "We conduct experiments on applying our Meta-DiffuB to other S2S-Diffusion models. Specifically,\nwe use Meta-DiffuB to modify the handcrafted noise-scheduling strategies of Dinoiser [44] and\nSeqDiffuSeq [45] on the WA and QQP datasets. The results, shown in Table 2, demonstrate that\nMeta-DiffuB can be considered a model-agnostic method for enhancing the performance of other\nS2S-Diffusion models. Additionally, we provide results for applying our Meta-DiffuB to RDM [47]\n(based on D3PM [47]) and other recent S2S-Diffusion models [42, 7, 22, 9], which are based on\nDiffuSeq [8] on machine translation datasets [31, 26] in Appendix E."}, {"title": "6 Experiments of Minimum Bayes Risk Decoding", "content": "Diffusion-LM proposes using Minimum Bayes Risk (MBR) to improve generation. Following the\nmethods described in [45, 8], we allow all S2S-Diffusion models to generate a set of candidate\nsentences from 10 random seeds and select the best output sequence that achieves the minimum\nexpected risk under a meaningful loss function. Specifically, in this work, we employ the BLEU\nscore as our loss function to evaluate performance, following the approach used in DiffuSeq [8].\nWe compare our Meta-DiffuB (De = DiffuSeq) with DiffuSeq [8] and GPT-2 [33], using MBR\ndecoding [21, 8, 45] on the WA and QQP datasets as described in DiffuSeq [8]. We specifically select\nGPT2-large and GPT2-base for comparison based on their superior performance on these datasets [8].\nIn this experiment, we apply MBR decoding to all three models while gradually increasing the\ncandidate sentence size |S|. The results of the MBR decoding are presented in Figure 2."}, {"title": "6.1 Experiment with Seq2Seq Benchmark Datasets", "content": "We demonstrate the performance of our Meta-DiffuB (De = DiffuSeq) in Table 3. This table\nshows that Meta-DiffuB (De = DiffuSeq) outperforms other PLMs [33, 10] and S2S-Diffusion\nmodels [8, 44, 45] in terms of generation quality and diversity, achieving the lowest M-R scores\nacross four datasets. Moreover, Meta-DiffuB (De = DiffuSeq) demonstrates significant improvements\nover previous S2S-Diffusion models [8, 45, 44] on all evaluation metrics considered."}, {"title": "6.2 Contextualized Noise Scheduling of Meta-DiffuB", "content": "For the experiment involving contextualized noise of Meta-DiffuB (D = DiffuSeq), we selected the\n200 hardest and 200 easiest generated sentences, labeled as (H) and (E), respectively. All models\nlisted in Table 3 assessed the generation difficulty of each sentence using BLEU scores, with lower\nBLEU indicating higher difficulty. The performance of generating (H) and (E) is evaluated in terms of\nBLEU and Self-BLEU, as shown in Table 4. We also detail the results of Table 4 evaluated by other\nmetrics in Appendix G. We tasked all S2S-Diffusion models with scheduling noise for sentences (H)\nand (E), as shown in Figure 3. Since our Meta-DiffuB (D = DiffuSeq) assigns specific noise to\neach sentence, we averaged the noise values for clearer visualization. Figure 3 displays the last 10\ndiffusion steps, as the noise differences in the initial steps are minimal. In Table 4, Meta-DiffuB (De\n= DiffuSeq) consistently outperforms other S2S-Diffusion models [44, 45, 8] in terms of generation\nquality and diversity for sentences (E) and (H). Notably, when generating the more challenging\nsentences (H), Meta-DiffuB Meta-DiffuB (De = DiffuSeq) maintains its performance, whereas other\nS2S-Diffusion models [44, 45, 8] experience a decline in both quality and diversity. Figure 3 shows\nthe benefits of Meta-DiffuB (De = DiffuSeq), which strategically imposes different noise levels for\nsentences (E) and (H). This noise-scheduling approach-applying less noise to the harder sentences\n(H) and more to the easier sentences (E)\u2014enhances the diversity and quality of the generated text.\nThe superior example of Meta-DiffuB in generating the hardest sentences (H) is further showcased\nin Appendix F, demonstrating its performance relative to other S2S-Diffusion models [44, 45, 8]. We\nalso provide a more detailed discussion about the noise strategy of our Meta-DiffuB in Appendix H."}, {"title": "6.3 Plug-and-Play Experiments with the Scheduler Model", "content": "Our pre-trained scheduler model, trained under Meta-DiffuB (De = DiffuSeq), which incorporates\nthe semantics of discrete sentences, demonstrates its effectiveness by scheduling noise for pre-trained\nDiffuSeq [8] models across various datasets. The results, presented in Table 5, show that our\npre-trained scheduler model, when applied across different datasets, enhances the performance of\npre-trained DiffuSeq models without any fine-tuning during the inference stage. This confirms that\nour scheduler model can function as a plug-and-play model across these datasets. Additionally, we\nprovide further experiments on different pre-trained schedulers under various S2S-Diffusion settings,\nas well as results on additional datasets in Appendix I."}, {"title": "7 Related Works", "content": null}, {"title": "7.1 Text Diffusion", "content": "[11, 1] define an absorbing state for generating discrete data. Diffusion-LM [21] and AnalogBits [3]\npropose imposing noise on continuous latent representations, using transformation functions to bridge\nthe discrete and continuous spaces of texts for both unconditional and controlled text generation."}, {"title": "7.2 Meta-Exploration", "content": "To transcend the limitations imposed by human-crafted rules in noise scheduling, we developed\nan additional model trained through Meta-Exploration, as inspired by [43]. Meta-Exploration is a\nReinforcement Learning (RL) training method that utilizes learning effectiveness to devise sampling\nstrategies that enhance model performance. Numerous studies [5, 37, 25, 19, 16] have employed\nMeta-Exploration to meta-learn scheduling strategies for applying additive Gaussian noise on actions\nand for sampling effective training data in RL tasks. We have adopted the Meta-Exploration concept\n[43] to train an additional model specifically for noise scheduling in S2S-Diffusion."}, {"title": "8 Broader Impact", "content": "In this work, our Meta-DiffuB demonstrates significant performance improvements over previous\nS2S-Diffusion models across four Seq2Seq tasks, as detailed in Section 4.1. Meta-DiffuB implements"}, {"title": "9 Conclusions", "content": "We propose integrating Meta-Exploration into S2S-Diffusion models through our newly developed\nMeta-DiffuB. By utilizing Meta-Exploration to schedule contextualized noise, our Meta-DiffuB\nmodel demonstrates significant performance improvements on four Seq2Seq benchmark datasets\ncompared to previous S2S-Diffusion models and PLMs. We have conducted a comprehensive\ninvestigation of the noise-scheduling capabilities of Meta-DiffuB and have visualized the results.\nImportantly, Meta-DiffuB has the potential to act as a plug-and-play model, providing a promising\napproach for enhancing other S2S-Diffusion models during the inference stage without the need for\nfine-tuning."}, {"title": "10 Acknowledgments", "content": "We would like to express our sincere gratitude to Professor Hung-yi Lee from NTU Speech Lab for\nhis invaluable guidance and insightful advice throughout this work. We are also deeply grateful to\nProfessor Ray-I Chang from NTU ICAN Lab for his mentorship and constructive feedback. Addi-\ntionally, we would like to thank the reviewers for their positive evaluation and valuable suggestions.\nFinally, we extend our appreciation to Maxora AI for providing the computational resources and\nenvironment that made this research possible, enabling us to make meaningful contributions to the\nfield."}, {"title": "F Showcase of Generated Sentences", "content": "Our Meta-DiffuB (De = DiffuSeq) achieves better generation diversity and quality on the hardest\ngenerated sentences (H), as evidenced by the examples provided in Table 8 and Table 9. These tables\nillustrate that Meta-DiffuB can generate more effective sentences (H) from both the WA and QQP\ndatasets than other S2S-Diffusion models. Table 8 shows the performance of our Meta-DiffuB (D\u04e9\n= DiffuSeq). It consistently generates the hardest sentences (H) with superior quality and diversity,\nproviding a reliable solution. In contrast, other S2S-Diffusion models [8, 44, 45] often produce\nrepetitive sentences, failing to ensure both quality and diversity. Our findings, as illustrated in Table 9,\nshow that Meta-DiffuB (De = DiffuSeq) also outperforms other models, generating sentences (H)\nwith superior diversity and quality on the QQP dataset."}, {"title": "G More Metrics on Contextualized Noise Scheduling of Meta-DiffuB", "content": "We present the results of Table 4 evaluated by other metrics in Table 10. Table 10 illustrates that our\nMeta-DiffuB (De = DiffuSeq) outperforms other S2S-Diffusion models in generating sentences (H)\nand (E) across all evaluation metrics in this study."}, {"title": "H Context-Aware Noise Generation in Meta-DiffuB: Analysis and Insights", "content": "In this section, we further discuss the noise generated by our Meta-DiffuB. As shown in Figure 4, the\ntotal noise per epoch produced by Meta-DiffuB exhibits less fluctuation compared to other rule-based\nmethods. While the noise variance is smaller than that of SeqDiffuSeq, it is larger than Dinoiser. From\nTable 3, we can see that"}]}