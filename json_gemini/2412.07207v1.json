{"title": "MAPLE: A Framework for Active Preference Learning Guided by Large Language Models", "authors": ["Saaduddin Mahmud", "Mason Nakamura", "Shlomo Zilberstein"], "abstract": "The advent of large language models (LLMs) has sparked significant interest in using natural language for preference learning. However, existing methods often suffer from high computational burdens, taxing human supervision, and lack of interpretability. To address these issues, we introduce MAPLE, a framework for large language model-guided Bayesian active preference learning. MAPLE leverages LLMs to model the distribution over preference functions, conditioning it on both natural language feedback and conventional preference learning feedback, such as pairwise trajectory rankings. MAPLE also employs active learning to systematically reduce uncertainty in this distribution and incorporates a language-conditioned active query selection mechanism to identify informative and easy-to-answer queries, thus reducing human burden. We evaluate MAPLE's sample efficiency and preference inference quality across two benchmarks, including a real-world vehicle route planning benchmark using OpenStreetMap data. Our results demonstrate that MAPLE accelerates the learning process and effectively improves humans' ability to answer queries.", "sections": [{"title": "Introduction", "content": "Following significant advancements in artificial intelligence, autonomous agents are increasingly being deployed in real-world applications to tackle complex tasks (Zilberstein 2015; Dietterich 2017). A prominent method for efficiently aligning these agents with human preferences is Active Learning from Demonstration (Active LfD) (Biyik 2022). Preference-based Active LfD, a variant of LfD, aims to infer a preference function from human-generated rankings over a set of observed behaviors using a Bayesian active learning approach.\nRecent advancements in natural language processing have inspired many researchers to leverage language-based abstraction for learning human preferences (Soni et al. 2022; Guan, Sreedharan, and Kambhampati 2022). This approach offers a more flexible and interpretable way to learn preferences compared to conventional methods (Sadigh et al. 2017; Brown, Goo, and Niekum 2019; Brown et al. 2019). More recent work (Yu et al. 2023; Ma et al. 2023) has focused on utilizing large language models (LLMs), such as ChatGPT (Achiam et al. 2023), with prompting-based approaches to learn preferences from natural language instructions. However, these methods often require significant computational resources and taxing human supervision, as they lack a systematic querying approach.\nTo tackle these challenges, we introduce a novel framework-MAPLE (Model-guided Active Preference Learning). MAPLE begins by interpreting natural language instructions from humans and utilizes large language models (LLMs) to estimate a distribution over preference functions. It then applies an active learning approach to systematically reduce uncertainty about the correct preference function. This is achieved through standard Bayesian posterior updates, conditioned on both conventional preference learning feedback, such as pairwise trajectory rankings, and linguistic feedback such as clarification or explanations of the cause behind the preference. To further ease human effort, MAPLE incorporates a language-conditioned active query selection mechanism that leverages feedback on the difficulty of previous queries to choose future queries that are both informative and easy to answer. MAPLE represents preference functions as a linear combination of abstract language concepts, providing a modular structure that enables the framework to acquire new concepts over time and enhance sample efficiency for future instructions. Moreover, this interpretable structure allows for human auditing of the learning process, facilitating human-guided validation before applying the preference function to optimize behavior.\nIn our experiments, we evaluate the efficacy of MAPLE in terms of sample efficiency during learning, as well as the quality of the final preference function. We use an environment based on the popular Minigrid (Chevalier-Boisvert et al. 2023) and introduce a new realistic vehicle routing benchmark based on OpenStreetMap (OpenStreetMap Contributors 2017) data, which includes text descriptions of the road network of different cities in the USA. Our evaluation shows the effectiveness of MAPLE in preference inference and improving human's ability to answer queries. Our contributions are threefold:\n\u2022 We propose a Bayesian preference learning framework that leverages LLMs and natural language explanations to reduce uncertainty over preference functions.\n\u2022 We provide a language-conditioned active query selection approach to reduce human burden.\n\u2022 We conduct extensive evaluations, including the design of a realistic new benchmark that can be used for future research in this area."}, {"title": "Related Work", "content": "Learning from demonstration Most Learning from Demonstration (LfD) algorithms learn a reward function using expert trajectories (Ng and Russell 2000; Abbeel and Ng 2004; Ziebart et al. 2008). Some of these approaches utilize a Bayesian framework to learn the reward or preference function (Ramachandran and Amir 2007; Brown et al. 2020; Mahmud, Saisubramanian, and Zilberstein 2023), and some pair it with active learning to reduce the number of human queries (Sadigh et al. 2017; Basu, Singhal, and Dragan 2018; Biyik 2022). However, these methods are unable to utilize natural language abstraction, whereas our method can use both. In addition, we employ language-conditioned active learning to reduce user burden, an approach not previously explored in this context.\nNatural language in intention communication With the advent of natural language processing, several works have focused on directly communicating abstract concepts to agents (Tevet et al. 2022; Guo et al. 2022; Wang et al. 2024; Sontakke et al. 2024; Lin et al. 2022; Tien et al. 2024; Lou et al. 2024). The key difference is that these works directly condition behavior on natural language, whereas we learn a language-abstracted preference function. This approach offers several advantages, including increased transparency, a more fine-grained trade-off between concepts, and enhanced transferability. The work most closely related to ours is (Lin et al. 2022), which infers rewards from language but restricts them to step-wise decision-making.\nOther lines of work (Yu et al. 2023; Ma et al. 2023) aim to learn reward functions directly by prompting LLMs. However, these methods are limited by the variables available in the coding space and often struggle with identifying temporally extended abstract behaviors. Further, these approaches can not utilize conventional preference feedback, whereas MAPLE can utilize both. Additionally, they either lack a systematic way of acquiring human feedback or rely on data-hungry evolutionary algorithms. In contrast, our approach employs more efficient Bayesian active learning.\nAbstraction in reward learning Several works leverage abstract concepts to learn reward functions (Lyu et al. 2019; Illanes et al. 2020; Icarte et al. 2022; Guan, Valmeekam, and Kambhampati 2022; Soni et al. 2022; Bobu et al. 2021; Guan et al. 2021; Guan, Sreedharan, and Kambhampati 2022; Silver et al. 2022; Zhang et al. 2022; Bucker et al. 2023; Cui et al. 2023). Two methods closely related to our work are PRESCA (Soni et al. 2022) and RBA (Guan, Valmeekam, and Kambhampati 2022). PRESCA learns state-based abstract concepts to be avoided, while RBA learns temporally extended concepts with two variants: global (eliciting preference weights directly from humans) and local (tuning weights using binary search). Our approach also leverages temporally extended concepts but learns preference functions from natural language feedback using active learning. Unlike RBA, which relies on direct preference weights from humans or binary search, our method uses LLM-guided active learning for more expressive and informative preference elicitation, thereby reducing human effort.\nSome works use offline behavior datasets or demonstrations to learn diverse skills (Lee and Popovi\u0107 2010; Wang et al. 2017; Zhou and Dragan 2018; Peng et al. 2018; Luo et al. 2020; Chebotar et al. 2021; Peng et al. 2021), which complement our approach. While MAPLE can also utilize such datasets in pre-training, the focus of MAPLE is to encode human preference in terms of these concepts using natural language.\nAlignment auditing Alignment auditing ensures that an agent's behavior aligns with human intentions by verifying that the agent has learned the correct preference function. While some works focus on alignment verification with minimal queries (Brown, Schneider, and Niekum 2021), they often rely on function weights, value weights, or trajectory rankings, which are difficult to interpret. In contrast, our approach leverages natural language to communicate with humans, facilitating validation and serving as a stopping criterion for the active learning process. Mahmud, Saisubramanian, and Zilberstein (2023) presents a notable alignment auditing approach related to our method, using explanations to detect misalignment and update distributions over preferences. While they employ a feature attribution method, we use natural language explanations. Additionally, they use human-selected or randomly sampled data points from an offline dataset for auditing, whereas we employ active learning to enhance efficiency.\nActive learning Previous works have explored different acquisition functions for active learning, typically focusing on selecting queries that maximize certain uncertainty quantization metrics. These metrics include predictive entropy (Gal and Ghahramani 2016), uncertainty volume reduction (Sadigh et al. 2017), mutual information maximization (Biyik et al. 2019), and maximizing variation ratios (Gal and Ghahramani 2016). Our approach complements these methods by integrating language-conditioned query selection to reduce user burden. While any of these methods can be paired with MAPLE, we opt for variation ratio due to its ease of calculation and high effectiveness."}, {"title": "Background", "content": "Markov decision process (MDP) A Markov Decision Process (MDP) M is represented by the tuple $M = (S, A, T, S_0, R, \\gamma)$, where S is the set of states, A is the set of actions, $T : S \\times A \\times S \\rightarrow [0, 1]$ is the transition function, $S_0$ is the initial state distribution, and $\\gamma \\in [0, 1)$ is the discount factor. A history $h_t$ is a sequence of states up to time t, $(s_0,..., s_t)$. The reward function $R : H \\times A \\rightarrow [-R_{max}, R_{max}]$ maps histories and actions to rewards. For some problems, a goal function $G : H \\rightarrow [0, 1]$ is provided that maps histories to goal achievements. In such problems, the reward function is typically $R : H \\times A \\rightarrow [-R_{max}, 0]$ and $\\forall a \\in A, T(s_g, a, s_g) = 1$ and $R(h_t \\cup s_g, a) = 0$ given the final state $s_g \\in h_t$. A policy $\\pi : H \\times A \\rightarrow [0, 1]$ is a mapping from histories to a distribution over actions. The policy $\\pi$ induces a value function $V^{\\pi} : S \\rightarrow \\mathbb{R}$, which represents the expected cumulative return $V^{\\pi}(s)$ that the agent can achieve from state s when following policy $\\pi$. An op-"}, {"title": "Problem Formulation", "content": "MAPLE We define a MAPLE problem instance as the tuple $(M-R, C, \\Omega, D_\\tau, H, L)$, where:\n\u2022 M-R is an MDP with an undefined reward function R.\n\u2022 H is the human interaction function that acts as the interface between the human and the MAPLE framework. Humans provide their feedback, preferences, and explanations in response to natural language queries posed by MAPLE.\n\u2022 IL is the LLM interaction function that generates natural language queries to the LLM and returns structured output in text files, such as JSON format.\n\u2022 C is an expanding set of natural language concepts ${c_1, c_2,..., c_n}$. We also use $C(\u00b7)$ to refer to a mapping model that takes a trajectory embedding $\\phi(\\tau)$ and a natural language concept embedding $\\psi(c_i)$ and maps them to a numeric value indicating the degree to which the trajectory \u03c4 satisfies the concept $c_i$. For non-Markovian concepts, $C(\u00b7)$ may be a sequence model such as a transformer. For Markovian concepts, we can define $C(\\phi(\u03c4), \u03c8(c_i)) = \\sum_{s\u2208\u03c4}C(\\phi(s), \u03c8(c_i))$, where $\\phi(s)$ is the state embedding.\n\u2022 \u03a9 is the space of all preference functions. In MAPLE, the preference functions $w$ over a trajectory \u03c4 are modeled as a linear combination of the concepts and their associated weights:\n$w(\\tau) = \\sum_{c_i \u2208 C} w_{c_i} \\cdot C(\\phi(\\tau), \\psi(c_i))$ (2)\n\u2022 $D_\\tau$ is a dataset of unlabeled trajectories ${\\tau_1, \\tau_2, ..., \u03c4_m}$.\nThe objective of MAPLE is to model the repeated interaction between a human and an agent, where the human communicates their task objective At in natural language, and the agent is responsible for completing the task in alignment with that objective. MAPLE accomplishes this by actively learning a symbolic preference function $w$ using large language models (LLMs), enabling the agent to optimize its behavior according to this function to ensure its actions align with human preferences.\nMotivating example Consider an intelligent route planning system that takes a source, a destination, and user preferences about the route in natural language, as illustrated in Figure 1. Datasets for several preference-defining concepts such as speed, safety, battery friendliness, smoothness, autopilot friendliness, and scenic view can be easily obtained and used to pre-train the concept mapping function C(\u00b7). The goal of MAPLE is to take natural language instructions from a human and map them to a preference function $w$ interactively so that a search algorithm can optimize it to find the preferred route. MAPLE incorporates preference feedback on top of natural language feedback to address issues like hallucination and calibration associated with directly using LLMs. Additionally, MAPLE allows the human to skip difficult queries and learns in-context which query to present, making the system more human-friendly. Furthermore, the preference function inference process in MAPLE is fully interpretable, enabling a human to audit the process thoroughly and provide the necessary feedback for improve-"}, {"title": "Detailed Description of the Proposed Method", "content": "A key innovation of MAPLE is the integration of conventional feedback from the preference learning literature with more expressive linguistic feedback, formally captured within a Bayesian framework introduced in RE-VEALE (Mahmud, Saisubramanian, and Zilberstein 2023):\n$P(w | F_h, F_\u03b9) \\propto P(F_h | w)P(F_\u03b9 | w)P(\u03c9)$ (3)\nAbove, $F_h$ represents the set of feedback observed in conventional preference learning algorithms, specifically in the context of this paper pairwise trajectory ranking.\u00b9 $F_\u03b9$ denotes the set of linguistic feedback. We can rewrite the equation as:\n$P(w | F_h, F_\u03b9) \\propto \\frac{P(F_h | w) P(w | F_\u03b9) P(F_\u03b9)}{P(F_h | F_\u03b9)} \\propto \\frac{Bradley-Terry Model}{P(F_h | w)} \\cdot \\frac{LLM Uniform}{P(\u03c9 | F_\u03b9)} \\cdot \\frac{P(F_\u03b9)}{P(F_h | F_\u03b9)}$ (5)\nHere, the likelihood of $F_h$ given $w$ is defined using the Bradley-Terry Model. The likelihood of $w$ given $F_\u03b9$ is estimated using an LLM. Beyond incorporating linguistic feedback via LLMS, MAPLE advances conventional active learning methods. Conventional active learning typically focuses on selecting queries that reduce the maximum uncertainty of the posterior but lacks a flexible mechanism to account for human capability in responding to certain types of queries. MAPLE's Oracle-guided active query selection enhances any conventional acquisition function by leveraging linguistic feedback to alleviate the human burden associated with difficult queries. In the rest of this section, we provide more details on MAPLE, particularly Algorithms 1 and 2.\nInitialization\nMAPLE starts by taking natural language instruction about task preference $A_{IT}$ and initializes the pairwise preference feedback set $F_h$, linguistic feedback $F_\u03b9$, and feedback about query difficulty $F_q$ (lines 1-2, Algorithm 1). After that, the initial set of weights is sampled using the LLM from the"}, {"title": "LLM-Guided Active Preference Learning", "content": "After initialization, MAPLE iteratively follows three steps: 1) query selection, 2) human feedback collection, and 3) preference posterior update, discussed below.\nOracle-guided active query selection (OAQS) At the beginning of each iteration, MAPLE selects a query q (a pair of trajectories) (lines 5-6, Algorithm 1) from $D_\u03c4$ that would reduce uncertainty the most while mitigating query difficulty based on human feedback. The query selection process is described in Algorithm 2, which starts by sorting all the queries based on an acquisition function $A_f$. In this paper, we use the variance ratio for its flexibility and high efficacy. In particular, for trajectory ranking queries, the score for $(\u03c4_i, \u03c4_j)$ is calculated as $\\mathbb{E}_{w\u223c\u03a9_\u03c4}[1 \u2212 max(P(\u03c4_i \\succ \u03c4_j | w), P(\u03c4_j \\succ \u03c4_i | w))]$. Note that other acquisition functions can also be used. Once sorted, OAQS iterates over the top K queries and selects the first query that the oracle (in our case an LLM) evaluates to be answerable by the human (lines 2-11). Finally, Algorithm 2 returns the least difficult query q among the top K query selected by $A_f$. We now analyze the performance of OAQS based on the characterization of the oracle.\u00b2"}, {"title": "Model", "content": "distribution $P(w | F_\u03b9)$ as $F_h$ is still empty (line 3, Algorithm 1). To sample $w$ from $P(w | F_\u03b9)$ we explore two sampling strategies described below.\nPreference weight sampling from LLM We directly prompt the LLM IL to provide linear weights $w$ over the abstract concepts. Specifically, we provide L with a prompt containing the task description T, a list of known concepts C, human preference $A_f$, and examples of instruction weight pairs, along with additional answer generation instructions G (see Appendix for details). The LLM processes this prompt and returns an answer :\n$A\u2190 L(prompt(T, C', A_4, D_1, G))$\nWe can take advantage of text generation temperature to collect a diverse set of samples. We define the set of all generated weights as A. Then $P(w_j | F_\u03b9)$ can be modeled for any arbitrary $w_j$ as follows:\n$P(w_j | F_\u03b9) = exp(\u2212\u03b2\\mathbb{E}_{w\u2208A} [Distance(w, w_j)])$ (6)\nIn this case, Euclidean or Cosine distance can be applied.\nDistribution weight sampling using LLM The second approach we explore is distribution modeling using an LLM. Here, we use similar prompts as in the previous approach; however, we instruct the LLM to generate parameters for $P(w | F_\u03b9)$. For example, for the weight of each concept $w_{c_i} \u2208 w$ we prompt L to generate a range $w_{c_i}^{range} = [w_{c_i}^{min}, w_{c_i}^{max}]$. Then we can define $P(w | F_\u03b9)$ as follows:\n$P(w | F_\u03b9) = \\begin{cases} 1, & \\text{if } w_{c_i} \u2208 w_{c_i}^{range}, \\forall w_{c_i} \u2208 w \\\\ 0, & \\text{otherwise.} \\end{cases}$ (7)\nWe can similarly model this for other forms of distributions, such as the Gaussian distribution. Once the initialization process is complete, MAPLE iteratively reduces its uncertainty using human feedback."}, {"title": "Definitions", "content": "Definition 1 Let Q denote the set of all possible queries, and $Q_A$ represent the subset of queries answerable by H. The Absolute Query Success Rate (AQSR) is defined as the probability that a randomly selected query q belongs to the intersection $Q \\cap Q_A$, i.e., $P(q \u2208 Q_A)$.\nDefinition 2 The Query Success Rate (QSR) of a query selection strategy is defined as the probability that a query q, selected by the strategy, belongs to $Q_A$, i.e., $P(q \u2208 Q_A | strategy)$.\nProposition 1 Assuming the independence of AQSR from acquisition function ranking, the QSR of a random query selection strategy: $P(q \u2208 Q_A | random) = AQSR$\nProposition 2 Under the same assumption of proposition 1, the QSR of a top-query selection strategy, which always selects the highest-rated query by $A_f$, $P(q \u2208 Q_A | top) = AQSR$.\nProposition 3 The QSR of the OAQS strategy is given by\n$AQSR \\frac{Y_1}{1 \u2212 [AQSR(1 \u2212 Y_0 \u2212 Y_1) + Y_0]^K}$\n$AQSRY_1 = \\frac{1 \u2212 [AQSR(1 \u2212 Y_0 \u2212 Y_1) + Y_0]^K}$\nwhere $Y_0 = P(\\mathcal{L}(F_q, q \\notin Q_A) = False)$ and\n$Y_1 = P(\\mathcal{L}(F_q, q \u2208 Q_A) = True)$. Here, we assume independence of AQSR, $Y_0$, and $Y_1$ from acquisition function ranking.\nCorollary 1 Based on Proposition 3, the OAQS will have a higher QSR than the random query selection strategy and top-query selection strategy iff, $Y_0 + Y_1 > 1$ as $K \u2192 \u221e$.\nDefinition 3 The Optimal Query Success Rate (OQSR) of a strategy is defined as the probability that the strategy returns the query $q^\u2217$ with the highest value according to an acquisition function $A_f$, among all answerable queries, i.e.,\n$\\begin{aligned} P(q^* = arg max_{q \\in Q} A_f(q)I(q \\in Q_A)), \\end{aligned}$\nwhere $q^\u2217$ is the query returned by the strategy.\nProposition 4 Under the similar assumption of proposition 1. assumption, the OQSR of a random query selection strategy is equal to $1/|Q|$.\nProposition 5 Under the similar assumption of proposition 1, the OQSR of a Top-Query Selection Strategy is equal to the AQSR.\nProposition 6 Under the same assumption of Proposition 3, the OQSR of the OAQS strategy is given by\n$OQSR = AQSR Y_1 \\frac{1 \u2212 [(1 \u2212 AQSR)Y_0]^K}{1-(1 \u2212 AQSR)Y_0}$\nCorollary 2 Based on Proposition 6, the OAQS strategy will have a higher OQSR than the top-query selection strategy if $(1-AQSR)Y_0+Y_1 > 1$ as $K \u2192 \u221e$, and then random query selection strategy if $AQSR Y_1 > \\frac{1-(1-AQSR) Y_0}{|Q|}$ as $K \u2192 \u221e$."}, {"title": "Handling unknown concepts", "content": "It should be noted that humans may provide instructions $A_{IT}$ that cannot be sufficiently captured by the available concepts in the concept maps. While this case is beyond the scope of this paper, several remedies exist in the literature to address this issue. LLMs can be prompted to add new concepts when generating weights. By leveraging the generalization capability of $C(\u00b7)$ we can attempt to apply these new concepts directly. If the new concept is significantly different from those in C, few-shot-learning techniques can be employed. In particular, during interactions, if a new concept is important, we can use non-parametric few-shot learning from human feedback, such as nearest neighbor search, to improve concept mapping (Tian et al. 2024). Finally, if a new concept arises repeatedly, it can be added to the concept map by retraining C with data collected from multiple interactions through few-shot learning, as considered in (Soni et al. 2022)."}, {"title": "OpenStreetMap Routing", "content": "We use OpenStreetMap to generate routing graphs for different U.S. states. The environment includes a concept mapping function capable of using ten different concepts: 1) Time, 2) Speed, 3) Safety, 4) Scenic, 5) Battery Friendly, 6) Gas Station Nearby, 7) Charging Station Nearby, 8) Human Driving Friendly, 9) Battery ReGen Friendly, and 10) Autopilot Friendly. The goal is to find a route between a given source and destination that aligns with user preferences. To generate $D_\u03c4$, we used 200 random source and destination pairs with randomly sampled weights from \u03a9. For modeling human interaction, we utilized two different datasets, each containing 50 human interaction templates. The first dataset, called \"Clear,\" provides clear, knowledgeable instructions. The second dataset, called \"Natural,\" obfuscates the \"Clear\" dataset with more natural-sounding language typical of everyday conversation and contextual information, for example:"}, {"title": "Experimental Results", "content": "We use three key metrics for evaluation: 1) the cosine distance between inferred preference weights (MAP of the distribution) and ground truth preference weights; 2) preference prediction accuracy, which evaluates the model's ability to generalize and accurately predict human preferences from an unseen set of trajectories; and 3) the policy cost difference, which compares the true cost of policies calculated using the ground truth preference function and the learned preference function."}, {"title": "Impact of linguistic feedback", "content": "Figure 4a-c presents the results of the OSM routing domain experiments. In this experiment, we did not apply OAQS; instead, we selected queries randomly from the dataset to isolate the impact of language. Several noteworthy insights emerge from the results. First, we observe that MAPLE outperforms B-REX on both the natural and clear datasets, demonstrating the effectiveness of integrating complex language feedback with conventional feedback. Additionally, as feedback increases, B-REX's accuracy begins to approach that of MAPLE. This suggests that MAPLE is particularly advantageous when feedback is limited, such as in online settings where the agent must quickly infer rewards.\nExamining the cosine distance offers further insight. Language alone appears almost sufficient to align the reward angle, as the cosine distance remains static despite the increasing number of queries. This suggests that preference feedback is more effective for calibrating the magnitude of the preference vector rather than its direction. In contrast, while B-REX achieves good accuracy with large amounts of feedback, it seems to exhibit significant misalignment, which could suggest overfitting and potential failure in out-of-distribution scenarios. Lastly, we evaluated existing publicly available models and found that both GPT-40 and GPT-40-mini outperformed other models. However, the small lo-"}, {"title": "Impact of OAQS", "content": "The results of the Oracle-Guided Active Query Selection (OAQS) using an LLM as an oracle are shown in Figure 5. In the routing environment, the Active Query Success Rate (AQSR) is approximately 0.64, while in the HomeGrid environment, it is 0.46. We first evaluated the capability of various models for in-context query selection (Figure 5c) using a dataset of 500 queries. The Mistral-7B model, used in the previous experiment, failed to meet the condition $Y_0 + Y_1 > 1$ in both environments. The Gemini-1.5-Pro model showed the best overall performance among"}, {"title": "Conclusions and Future Works", "content": "We introduced MAPLE, a framework for active preference learning guided by large language models (LLMs). Our experiments in the OpenStreetMap Routing and HomeGrid environments demonstrated that incorporating language descriptions and explanations significantly improves preference alignment, and that LLM-guided active query selection enhances sample efficiency while reducing the burden on users. Future work could extend MAPLE to more complex environments and tasks, explore different types of linguistic feedback, and conduct user studies to evaluate its usability and effectiveness in real-world applications."}]}