{"title": "Lean and Mean: Decoupled Value Policy Optimization with Global Value Guidance", "authors": ["Chenghua Huang", "Lu Wang", "Fangkai Yang", "Pu Zhao", "Zhixu Li", "Qingwei Lin", "Dongmei Zhang", "Saravan Rajmohan", "Qi Zhang"], "abstract": "Proximal Policy Optimization (PPO)-based Reinforcement Learning from Human Feedback (RLHF) is essential for aligning large language models (LLMs) with human preferences. It requires joint training of an actor and critic with a pretrained, fixed reward model for guidance. This approach increases computational complexity and instability due to actor-critic interdependence. Additionally, PPO lacks access to true environment rewards in LLM tasks, limiting its adaptability. Under such conditions, pretraining a value model or a reward model becomes equivalent, as both provide fixed supervisory signals without new ground-truth feedback. To address these issues, we propose Decoupled Value Policy Optimization (DVPO), a lean framework that replaces traditional reward modeling with a pretrained global value model (GVM). The GVM is conditioned on policy trajectories and predicts token-level return-to-go estimates. By decoupling value model from policy training (via frozen GVM-driven RL objectives), DVPO eliminates actor-critic interdependence, reducing GPU memory usage by 40% and training time by 35% compared to conventional RLHF. Experiments across benchmarks show DVPO outperforms efficient RLHF methods (e.g., DPO) while matching state-of-the-art PPO in performance.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have demonstrated state-of-the-art performance across a range of tasks (Achiam et al., 2023; Bubeck et al., 2023), including dialogue generation (Ouyang et al., 2022), summarization (Stiennon et al., 2020), and code synthesis (Gao et al., 2023a). 1 To further align these models with human preferences, Reinforcement Learning from Human Feedback (RLHF) has become an essential technique (Christiano et al., 2017; Bai et al., 2022b; Song et al., 2024). RLHF leverages human-labeled feedback to guide model behavior, producing more accurate and user-friendly responses over time (Bai et al., 2022a; Ganguli et al., 2022). Among RLHF algorithms, Proximal Policy Optimization (PPO) is widely adopted due to its balance between stability and performance (Schulman et al., 2017; von Werra et al., 2020; Huang et al., 2024c). Despite its effectiveness, PPO-based RLHF comes with significant challenges. It requires joint training of an actor (policy) and a critic (value function), while relying on a fixed, pretrained reward model (Ziegler et al., 2019; Ouyang et al., 2022; Wang et al., 2024). This joint optimization introduces dense computational complexity and training instability, as the actor and critic must co-adapt over time (Yao et al., 2023; Huang et al., 2024b). Unlike in traditional reinforcement learning (RL), where continuous environment interaction provides updated rewards, RLHF in LLMs lacks access to ground-truth environment rewards during training. As a result, the reward model cannot evolve to reflect changes in the policy's behavior, further complicating value estimation and policy optimization (Moskovitz et al., 2023). To mitigate these issues, several works have proposed simplifying the RLHF process by reducing the number of trainable components. Direct Preference Optimization (DPO) bypasses training both reward and value models, optimizing policies directly on preference data (Rafailov et al., 2024; Azar et al., 2024; Zhao et al., 2023; Park et al., 2024). While efficient, DPO lacks the iterative refinement of true reinforcement learning and struggles with distribution shifts (Xu et al., 2024). Other reward-only approaches, such as Re-Max (Li et al., 2023) and GRPO (Shao et al., 2024), use final rewards applied uniformly across all tokens but suffer"}, {"title": "2. Related Work", "content": "Reinforcement Learning in Language Model Optimization. Reinforcement learning has emerged as a prevalent method for fine-tuning large language models (LLMs), with Proximal Policy Optimization (Schulman et al., 2017) and its variations (Ramamurthy et al., 2022; Wu et al., 2023) being the most prevalent methods. These techniques largely adhere to the actor-critic paradigm (Sutton, 2018), This approach alternates between training a value estimator for the current policy and leveraging it to enhance policy performance. This bilevel process may result in a suboptimal policy, as demonstrated by empirical studies (Gao et al., 2023b). Moreover, the alternating optimization of the policy and critic models, along with the use of rewards provided by the reward model as environmental feedback, necessitates loading four models (including the reference model) simultaneously during training. This significantly increases training complexity and computational resource consumption (Yao et al., 2023; Hu et al., 2024). Training Efficiency. Many recent studies have sought to mitigate the computational complexity and resource consumption of the reinforcement learning (RL) step in RLHF. Methods such as DPO (Rafailov et al., 2024) and its variants (Meng et al., 2024; Ethayarajh et al., 2024; Hong et al., 2024) bypass reward modeling and the actor-critic learning framework by directly learning from preferences. However, existing research indicates that due to their offline nature, these approaches exhibit a performance gap compared to online RL (Xu et al., 2024). Some recent works have proposed a reward-only approach to reduce the training cost of the RL phase (Li et al., 2023; Gunter et al., 2024; Shao et al., 2024; Ahmadian et al., 2024). However, this method lacks value estimation and assigns the same reward score to each token, leading to high variance and instability during training (Hu, 2025). Unlike these approaches, our method pre-trains a global value model (GVM) and leverages it to guide RL training, providing token-level supervision signals. This not only reduces training resource consumption but also stabilizes the training process, achieving performance comparable to the original PPO. Value-base Inference. Some recent works have attempted to learn a value function and use it to guide the decoding phase of LLMs, thereby bypassing the RL optimization stage (Han et al., 2024; Kong et al., 2024; Mao et al., 2024). However, this approach significantly increases inference complexity and raises inference costs. In contrast, we leverage the learned value model to guide RL training, where the pre-trained value model helps the policy model converge more stably (Noukhovitch et al., 2024)."}, {"title": "3. Method", "content": "We propose DVPO for RLHF in which a GVM is trained once and then fixed to guide policy updates. This approach removes the need for joint policy-value training and mitigates the associated computational overhead and instability. As shown in Figure 1, our method comprises two primary stages: (1) Train GVM: Use offline trajectories (states, actions, returns, and policy-specific data) to learn a policy-conditioned action-value function Q. (2) Decoupled Value Policy Optimization: Freeze Q and optimize a policy using a standard RL objective (e.g., PPO), taking advantage estimates from the fixed GVM. In what follows, we first define our setting (\u00a73.1), then introduce the GVM and its training procedure (\u00a73.2), describe how we fix the GVM for stable policy optimization (\u00a73.3), and finally present a theoretical analysis showing that pre-"}, {"title": "3.1. Problem Setting", "content": "We model the sequence generation task in NLP as a Markov Decision Process (MDP). The response consists of T tokens, denoted by $y = y_{<T+1} := [y_1, y_2, . . . , y_T]$, where $y_t \\in Y$ and $y_{<1} = []$, indicating an empty prefix. Given a prompt x and the first t - 1 tokens $y_t$, the language model (LM) predicts the probability distribution for the next token as:\n\n$\\pi_{\\theta}(\\cdot | [x, y_{<t}])$.\n\nIn this MDP formulation, the state is defined as $s_t = [x, y_{<t}]$, representing the prompt (i.e., x) and the generated response up to step t. The action is the next generated token $a_t = y_t$. The ground-truth reward at the sentence level, denoted by r(x, y), is provided by human feedback."}, {"title": "3.2. Training a Global Value Model (GVM)", "content": "Our key insight is to learn a policy-conditioned action-value function Q(\u03c4, s, a) from offline data, where \u03c4 represents a sampled trajectory capturing how the policy behaves in unrelated contexts. This conditioning approximates how well a particular policy\u2014embodied by \u03c4\u2014would perform when taking action a in state s. Policy Conditioning via Trajectories. Traditional actor-critic methods require online adaptation of the value function to the actor's evolving behavior. In contrast, we aim for a single, global Q\u03c6 that generalizes across different policies, thus avoiding iterative re-learning. Instead of conditioning on explicit policy parameters, we leverage trajectories \u03c4 randomly sampled from the policy in question. Each is a sequence of question-answer pairs (in LLM tasks) or other interactions that reveal distinct policy traits (e.g., stylistic tendencies, correctness, domain expertise). Formally, we parametrize:\n\n$Q(\\tau, s, a) \\approx E_{s_0, a_0 \\sim \\pi(\\cdot|s) , \\tau}[\\{\\sum_{t=0}^{\\infty} \\gamma^t r(s_t, a_t) | s_0=s, a_0=a, \\tau\\}]$,\n\nwhere \u03c4 implicitly determines which policy \u03c0(\u00b7 | s) we are approximating. In practice, we train the global value model $Q_\\phi(T, s, a)$ using Temporal Difference (TD) learning. The target return $G_t$ is estimated based on future rewards and value predictions. Specifically, $G_t$ is computed as:\n\n$G_t = r(s_t, a_t) + \\gamma Q_\\phi(\\tau, s_{t+1}, a_{t+1}),$"}, {"title": "3.3. Decoupled-Value Policy Optimization", "content": "In traditional actor-critic methods, both the policy (actor) and value (critic) are trained simultaneously, which introduces instability due to their interdependence. This issue is exacerbated in offline RLHF settings, where no new environment rewards can be collected to correct misaligned updates. To address these challenges, we propose a decoupled-value policy optimization approach: the global value model Q\u03c6 is trained and fixed before policy optimization, decoupling the learning dynamics of the actor and critic. Policy Optimization Objective. Once the global value model Q\u03c6 converges, we fix its parameters and use it to guide policy updates. Let \u03c0\u03b8(\u03b1 | s) be the policy to be"}, {"title": "3.4. Theoretical Analysis: Equivalence of Pretrained Reward and Global Value Models", "content": "In an offline RLHF scenario where no new ground-truth rewards are available, pretraining either a reward model or a GVM provides essentially the same supervisory information for policy optimization. Below, we present a rigorous proof of this equivalence and discuss its implications.\n3.4.1. PRELIMINARIES\nAssume we have two possible pretrained models from the same offline dataset. Reward Model, R\u03c6(s, a), approximates an unknown ground-truth reward r(s, a). Global Value Model, $Q_{\\psi}(s, a)$, approximates the action-value function for a any given policy \u03c0. Here, $Q_{\\psi}(s, a)$ is a global value model conditioned on the trajectory \u03c4\u03c0, state s, and action a. However, to simplify the proof, we remove the explicit dependence on \u03c4\u03c0 by merging it into the definition of the state. Because no new rewards are collected during policy training, neither model can be updated to reflect policy changes. Hence, whichever model is used, the agent has a fixed signal to optimize against. We formalize this equivalence next.\nTheorem 3.1 (Equivalence of Pretrained Reward and GVM). Suppose:"}, {"title": "6. Conclusion", "content": "We propose Decoupled Value Policy Optimization (DVPO), a framework that eliminates joint actor-critic training in RLHF by leveraging a pretrained global value model (GVM). Unlike traditional PPO-based RLHF, which suffers from training instability and computational overhead, DVPO stabilizes policy optimization through static, token-level return-to-go estimates. Our theoretical analysis proves the functional equivalence of reward and value models under the constraint of no new reward feedback, justifying the use of a fixed GVM for efficient and scalable optimization. Empirical results demonstrate that DVPO achieves comparable performance to state-of-the-art RLHF methods on multiple benchmarks while reducing training time and GPU usage by over 35%. Future work will focus on refining the value model's training process to enhance prediction accuracy."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Training Details", "content": "SFT training. We use the following hyperparameters for instruction fine-tuning training. We employ a learning rate of 2e-5 with cosine decay, 2 warmup steps, and a batch size of 16. We calculate the loss only for the target tokens rather than the full input sequence, and we train for 3 epochs on the training data. we conduct the training on 8 NVIDIA A100 80G GPUs. Reward training. To enable the model to learn the relative ranking among different responses, we use a pair-wise loss. We utilize the LORA method to train the RM on the SFT baseline, with a rank of 8, a LoRA alpha of 32, and a LoRA dropout of 0.1. The task type is sequence classification. We use a learning rate of 2e-5 with linear decay and the AdamW optimizer for training over 2 epochs, with a batch size of 4. We conduct the training on 8 NVIDIA A100 80G GPUs . PPO training and GVPO training. For PPO training, we use a learning rate of 2e-6 and set the generate sample length to 1024. We employ a batch size of 8 and a mini-batch size of 2, with 4 PPO epochs and 2 gradient accumulation step. The target KL divergence is set to 0.1 and initial KL coefficient is set to 0.2. For a fair comparison, GVPO training was conducted using the same hyperparameter settings (e.g., batch size, sequence length, etc.). Global value model training. We initialize the value model from the SFT model. The training is conducted with a batch size of 32, a sequence length of 1024, and a learning rate of 2e-6. We employ the LoRA method to train the value model on the SFT baseline, using a rank of 8, a LoRA alpha of 32, and a LoRA dropout of 0.1."}, {"title": "B. GVM case study", "content": "Question: As an island, is Beijing located in Asia? Response1: _Be ij ing _is _not _an _island ... Value1: -0.1936 0.0333 -0.0821 -0.2597 0.2099 -0.4590 -0.1500 ... Response2: _Be ij ing _is _an _island _located ... Value2: -0.0883 -0.2493 0.1845 -0.6177 -0.4766 -0.1289 -0.1526 ... Figure 4. An example of the supervisory signal provided by a Global Value Model (GVM). The GVM is capable of providing token-level feedback. In this example, the GVM assigns a lower value to the incorrect response (response2: \\\"is an island\\\") and a higher value to the critical token \\\"not\\\" in the correct response (response1: \\\"not an island\\\"). For the same question, \\\"As an island, is Beijing located in Asia?\\\", the value model provides fine-grained supervisory signals for two different responses. The GVM assigns specific values to each token in the responses. These values represent the model's assessment of the importance or correctness of each token in the given context. As shown in Figure 4. For Response 1, the critical token \\\"not\\\" is given a higher value (0.2099), highlighting its significance in forming the correct response, \\\"not an island.\\\" For Response 2, the GVM assigns lower values to incorrect tokens, such as \\\"is\\\" (-0.6177) and \\\"an\\\" (-0.4766), indicating their contribution to the incorrect response, \u201cis an island.\u201d This token-level evaluation demonstrates the GVM's ability to guide learning by penalizing incorrect responses and reinforcing critical tokens in correct responses, thereby enhancing training accuracy and interpretability."}, {"title": "C. GVM performance", "content": "We evaluate the performance of the GVM from multiple perspectives. Specifically, we observe that the GVM assigns higher value scores to good actions and lower value scores to bad actions, as illustrated in Figure 4. We evaluate the global value model(GVM) on a held-out test set. For each question, we expect the value model to assign higher values to good responses compared to bad ones. We calculate the accuracy under various metrics (mean, P1, etc.) to assess the model's performance."}, {"title": "D. GPT4 evaluation Prompt", "content": "A crucial element of our experimental framework is the evaluation of win rates using GPT-4. In this section, we provide the prompts utilized to generate win rates for both the summarization and dialogue experiments. All experiments were conducted using the gpt-40-20240806 model. The sequence of responses was randomized for each evaluation to ensure unbiased results. GPT-4 as judge system prompt: Review the user's question and the corresponding response using the additive 5-pointscoring system described below. Points are accumulated based on the satisfaction of each criterion: - Add 1 point if the response is relevant and provides some information related to the user's inquiry, even if it is incomplete or contains some irrelevant content. - Add another point if the response addresses a substantial portion of the user's question, but does not completely resolve the query or provide a direct answer. - Award a third point if the response answers the basic elements of the user's question in a useful way, regardless of whether it seems to have been written by an Al Assistant or if it has elements typically found in blogs or search results. Grant a fourth point if the response is clearly written from an Al Assistant's perspective, addressing the user's question directly and comprehensively, and is well-organized and helpful, even if there is slight room for improvement in clarity, conciseness or focus. - Bestow a fifth point for a response that is impeccably tailored to the user's question by an AI Assistant, without extraneous information, refecting expert knowledge, and demonstrating a high-quality, engaging, and insightful answer. After examining the user's instruction and the response, please first give the total score. Then provide a brief explanation of your total score, up to 100 words. Output with the following format: Score: {total score} Evaluation evidence: {your brief explanation here} Remember to assess from the Al Assistant perspective, utilizing web search knowledge as necessary. To evaluate the response in alignment with this additive scoring model, we'll systematically attribute points based on the outlined criteria."}]}