{"title": "ROBOSPATIAL: Teaching Spatial Understanding to 2D and 3D Vision-Language Models for Robotics", "authors": ["Chan Hee Song", "Valts Blukis", "Jonathan Tremblay", "Stephen Tyree", "Yu Su", "Stan Birchfield"], "abstract": "Spatial understanding is a crucial capability for robots to make grounded decisions based on their environment. This foundational skill enables robots not only to perceive their surroundings but also to reason about and interact meaningfully within the world. In modern robotics, these capabilities are taken on by visual language models, and they face significant challenges when applied to spatial reasoning context due to their training data sources. These sources utilize general-purpose image datasets, and they often lack sophisticated spatial scene understanding capabilities. For example, the datasets do not address reference frame comprehension spatial relationships require clear contextual understanding, whether from an ego-centric, object-centric, or world-centric perspective, which allow for effective real-world interaction. To address this issue, we introduce ROBOSPATIAL, a large-scale spatial understanding dataset consisting of real indoor and tabletop scenes captured as 3D scans and egocentric images, annotated with rich spatial information relevant to robotics. The dataset includes 1M images, 5K 3D scans, and 3M annotated spatial relationships, with paired 2D egocentric images and 3D scans to make it both 2D and 3D ready. Our experiments show that models trained with ROBOSPATIAL outperform baselines on downstream tasks such as spatial affordance prediction, spatial relationship prediction, and robotics manipulation.", "sections": [{"title": "1. Introduction", "content": "The introduction of vision-and-language models (VLMs) in modern robotics has helped reconcile general computer vision with robotics control [59]. We have seen these models trained directly on robotics manipulation data [10] which allows robots to directly consume image and task description inputs and act in the real-world. Other works have used generic VLMs to help describe robotics scenes to accomplish specific tasks such as cleaning a table top [16]. We have also seen grasping augmented with text-based description [49]. Moreover, large language models (LLMs) are also important to modern robotics, they have been used extensively to write robot code from user prompts [29, 44] or on high-level planning [1, 21, 45].\nVLMs can recognize objects, classify scenes, and even provide general descriptions that capture high-level attributes. However, despite significant recent advancements, VLMs [30, 34, 40] still fall short in spatial understanding [26, 43, 56]. They struggle with tasks that require interpreting nuanced spatial relationships between objects, such as describing where one object is in relation to another or determining the best location to place an item within a specific condition. For example, while a model might accurately describe a \"bowl on the table,\" it lacks the ability to reason about where on the table the bowl is, where it should go to ensure accessibility or stability, or how it might fit among other objects. Furthermore, a critical limitation of existing VLM training datasets is their inability to capture reference frame understanding \u2014 the way we interpret spatial relationships changes drastically depending on whether"}, {"title": "2. Related Work", "content": "VLMs for Robotics. Vision-language models (VLMs) have emerged as pivotal tools in robotics, enabling systems to interpret and act upon complex visual and textual information. By integrating visual perception with language understanding, VLMs facilitate more intuitive human-robot interactions and enhance autonomous decision-making capabilities. Recent advancements have demonstrated the potential of VLMs in various robotic applications. For instance, vision-and-language action models (VLAs) [4, 27, 39] enable robots to interpret and execute complex instructions and output executable robot actions. Additionally, VLMs like GPT-4v [40] have been utilized for high-level task planning [51], allowing robots to generate detailed action sequences from natural language instructions. Furthermore, VLMs have been used for keypoint prediction [16, 22, 38], error analysis [14], grasp pose prediction [19]. Despite these advancements, integrating VLMs [5, 9, 57] into robotic systems presents challenges. One significant hurdle is the need for precise spatial reasoning to navigate and manipulate objects effectively. While VLMs excel in understanding and generating language, their ability to comprehend and reason about spatial relationships in dynamic environments remains limited [53, 55, 56]. Therefore, ROBOSPATIAL aims to tackle this gap by presenting a large scale pretraining and evaluation setup for teaching spatial understanding to VLM for robotics.\nSpatial Understanding with VLMs. Spatial understanding has been implicitly and explicitly part of various vision and question answering tasks [2, 3, 17, 23\u201325, 28, 46]. While many benchmarks and methods have been proposed, they often come with limitations: some focus exclusively on simulations [48] or generic images [8, 9, 17, 26, 33, 41, 43], others are difficult to evaluate [13, 48], rely on complete 3D scans [32, 36, 37, 58], do not consider reference frames [8, 9, 17, 32, 36, 37, 58], or use data generated by LLMs that may lack sufficient accuracy [32]. Furthermore, they often fail to address actionable, robotics-relevant spatial relationships such as spatial compatibility and context [13, 17, 26, 43, 54].\nInspired by prior works on spatial reasoning [26, 33]\u2014where the impact of reference frames and spatial configurations was explored in generic images [23, 31]\u2014we extend spatial understanding to a robotics-specific context with actionable spatial relationships such as spatial compatibility and spatial context. Our aim is to enable direct application to robotic workflows, such as task planning and verification.\nTo achieve this, we have developed and are planning to open-source a large-scale 2D/3D ready pretraining dataset, an automated data annotation pipeline, and trained models. We further show how our dataset can be used to teach spatial reasoning to a suite of vision-language models (VLMs) in in-domain and out-of-domain spatial reasoning datasets. We hope these resources lower the barrier to entry for exploring spatial understanding tailored to robotics."}, {"title": "3. Approach", "content": "In this section, we begin by explaining our selection of three spatial relationships: spatial configuration, spatial context, and spatial compatibility. Next, we provide an overview of the data generation pipeline used to construct ROBOSPATIAL. An overview of the RoboSPATIAL is provided in Figure 2.\n3.1. Spatial Relationships\nOur dataset is organized around three core spatial relationships that we believe to address the essential aspects of spatial reasoning for robotic tasks: object configuration, object context, and object compatibility. Configuration enables robots to understand and interpret the relative positioning of objects, which is crucial for directing navigation, manipulation, and interaction within complex environments. Context allows robots to assess the relationship between objects and their surrounding space, facilitating the identification of empty or occupied areas and aiding in efficient path planning and obstacle avoidance. Compatibility focuses on whether objects can coexist or interact without conflict in a given space, which is vital for object placement, assembly, and operational safety. Together, these spatial relationships provide a more nuanced and practical framework for robotic applications than metrics like distance\u2014which is hard to normalize across different scales, environments, and tasks\u2014thereby enabling robots to perform complex tasks with greater reliability.\n3.2. Dataset Generation\nThe goal of the data construction pipeline is to generate a high-accuracy spatial relationship dataset with minimal human intervention by use of carefully constructed heuristics. Our data generation process takes as input a source scene dataset Ds that contains RGB views, camera poses, and object annotations in form of text labels and oriented 3D bounding boxes. The output is a dataset D where each datum d\u2081 = (Ii, qi, ai, li) consists of an image I\u2081, a question qi, an answer ai, and a reference frame label li \u2208 {ego, world, object}. Each question belongs to one of the three core spatial reasoning categories or a fourth category for object grounding (Table 2).\nOur data generation process consists of two stages, (1) Spatial Relation Extraction, and (2) Question-Answer Generation.\n3.2.1. Spatial Relation Extraction\nIn the spatial relation extraction stage, we analyze the dataset to discover a set of spatial relations between objects or points in free space. Each spatial relation can be described as (Ii, ai, ti, ri, li), where I\u2081 is an image, ai is an anchor object, ti is a target object or a target free-space"}, {"title": "3.2.2. Question-Answer Generation", "content": "Based on the generated relationships, we construct three types of question-answer pairs corresponding to each spatial relationship type: spatial configuration, spatial context, and spatial compatibility. To ensure that the model reasons with vision rather than relying on language commonsense, we structure the questions and answers to be template-based and unambiguous. We use a template: {OBJECT/SPACE} {RELATIONSHIP} {ANCHOR OBJECT} {REFERENCE FRAME}, where the spatial relationship and reference frame is defined in Sec. 3.2.1. We maintain a balanced dataset with respect to spatial relationships to avoid introducing bias into the model. Furthermore, we use the 2D bounding box to create an auxiliary object-referring dataset to teach models what the object is, in order to avoid errors arising from a lack of object grounding. Using our pipeline we generated 3M spatial relationships, which is an order of magnitude larger than other spatial reasoning datasets shown in Table 1. Figure 2 shows an overview of the generated question types."}, {"title": "4. Experiments", "content": "4.1. Implementation\nWe apply our data generation process to three scene datasets and two tabletop datasets. The scene datasets include ScanNet [11], Matterport3D [7], and 3RScan [52]. The object tabletop datasets are HOPE [50] and GraspNet-1B [15]. We retrieve the 3D bounding box annotations and embodied images from EmbodiedScan [54]. This mix of datasets provides coverage for diverse indoor scenes featuring large objects for navigation, and smaller objects for learning priors for robot manipulation. In total, we generate a large scale dataset for spatial reasoning that contains around 3M spatial QA pairs with 5K 3D scans and 1M images. Statistics of these datasets are described in Table 3.\n4.1.1. Trained 2D/3D VLMS\n2D VLMs. For our dataset, we want to compare a suite of VLMs, including ones fine-tuned for spatial understanding and ones that are not. We chose two base VLMS: VILA-1.5-8B [30] and LLaVA-NeXT-8B [34]; three specialized VLMs: SpaceLLaVA-13B, a community implementation of SpatialVLM [8]; RoboPoint-13B [57], a model fine-tuned on synthetic datasets to predict points in empty areas referenced by an object; and Molmo-7B [12], a new family of VLMs specializing in pointing and counting We also include a closed-source model, GPT-40 [40]. We omit models such as SpatialRGPT [9] due to its reliance on external object mask generation as input, which omits the important aspect of object grounding.\n3D VLMs. Extending VLMs to the 3D domain introduces unique challenges due to the inherent complexity of 3D data. The scarcity of large-scale, high-quality 3D datasets hinders the development of end-to-end 3D VLMs. As such, there are few open source VLMs for 3D understanding that reason from 3D inputs. Among the available models, we selected: 3D-LLM [18], which lifts multi-view images into a 3D point cloud, and LEO [20], which directly takes as input 3D object point clouds by relying on a pre-processing object segmentation step. These models allow us to evaluate spatial understanding in 3D by processing data either through reconstructed 3D representations or directly from point clouds. By incorporating both approaches, we aim to assess the effectiveness of 3D VLMs in handling spatial information from different types of 3D data inputs.\nFine-tuning. Our evaluation includes both \"zero-shot\" evaluation of each VLM, plus evaluation after fine-tuning on ROBOSPATIAL for open-source VLMs. See the Appendix for further details.\n4.1.2. Spatial Understanding Evaluation\nWe evaluate our method in answering 3000 questions (1000 questions per spatial relationship) from our test set, and measure how often the model produces the correct answer. The evaluation includes binary yes/no questions, and questions requiring numeric responses. For yes/no questions, we report the success rate in producing the correct response. For numeric questions, we report the rate at which the produced numeric answer is within the convex hull of a set of points generated by our spatial relation extraction method. Note that this is potentially an overly strict metric, and therefore the reported performance on numeric questions reflects a lower-bound of the true performance. Table 4 shows the main results.\n4.1.3. Cross-Dataset Generalization Evaluation\nNext, we evaluate the generalization capability of our method by testing it across different scene types\u2014specifically, both indoor and tabletop scenes\u2014to control for any bias in the annotations of the underlying datasets that make up our benchmark. We train on data derived from subsets of the datasets corresponding to one scene type (either indoor or tabletop) and test on held-out"}, {"title": "4.1.4. Out-of-Domain Evaluation", "content": "To test the out-of-domain transferability of ROBOSPATIAL-trained models, we evaluate them on ROBOSPATIAL-Home, our manually annotated indoor scene dataset, and the validation spatial reasoning split of the Blink [17] benchmark.\nROBOSPATIAL-Home is a collection of 50 crowd-sourced indoor RGBD images captured using a handheld iPhone camera equipped with a built-in depth sensor. Each image is manually annotated with three spatial relationships, resulting in a total of 150 questions across 50 images. We designed ROBOSPATIAL-Home to test the ROBOSPATIAL-trained model's capability to generalize to unseen objects in real life cluttered scenes.\nBlink [17] is a visual reasoning benchmark designed to evaluate the reasoning capabilities of vision-language models (VLMs) using visual input alone. Blink was chosen to assess the generalization ability of ROBOSPATIAL-trained models on unseen spatial relationships such as \"next to,\" \"touching,\" and \"on top.\" Unlike benchmarks such as What's Up [26], which include spatial relationships similar to those in RobOSPATIAL, Blink [17] provides a more rigorous test of out-of-domain generalization. The validation spatial reasoning split of Blink consists of 143 binary yes/no questions about spatial configurations.\nThe results of the out-of-domain generalization evaluations are presented in Table 6 and visualized in Fig. 3."}, {"title": "4.2. Results", "content": "Our experiments on the held-out test split in Table 4, out-of-domain splits in Table 6, and real-world robot experiments in Table 7 clearly demonstrate the power of ROBOSPATIAL in teaching spatial reasoning to VLMs where training on RoboSPATIAL improved spatial understanding across all spatial reasoning benchmarks. We further answer the following detailed questions about ROBOSPATIAL-trained models:\nHow well does ROBOSPATIAL training generalize to unseen spatial relationships? While ROBOSPATIAL contains templated question-answer pairs and a fixed set of spatial relationship prepositions, Table 6 shows that ROBOSPATIAL training does generalize to other spatial relationship prepositions present in Blink [17], such as \"under,\" \"next to,\" and \"far away.\" We highlight that ROBOSPATIAL encompasses all six principal directions along the x, y, and z axes in 3D space, and that generalizing to novel relationships involves mapping each of these directions to one or more spatial relationship prepositions, a task at which LLMs excel. For example, \"on top of\" and \"on\" refer to the \"above\" direction in a world-centric frame, while \u201cunder\u201d corresponds to \"below.\" Furthermore, we observe that prepositions such as \"next to\" and \"beside\u201d imply that objects need to be in close proximity to one another. Since ROBOSPATIAL contains spatial context questions that teach the model to generate points close to a given object, this implicitly teaches what it means to be \"close\" to another object, enabling ROBOSPATIAL-trained models to understand concepts like \u201cnext to.\u201d However, ROBOSPATIAL training does not seem to generalize well to questions involving human-centric perspectives, since we focus solely on objects in our dataset for robotics use cases.\nDO ROBOSPATIAL-trained models understand nuanced perspectives? When humans refer to spatial relationships, we often imply a specific frame of reference. For example, \"in front of the car\" typically refers to the car's front hood. Therefore, questions in ROBOSPATIAL-Home omit explicitly specifying the reference frame to test the model's ability to align its understanding with the questioner's intended reference frame. Results in Table 6 show that models trained with ROBOSPATIAL can automatically infer the reference frame, with Figure 3 showing examples like \"Is the frame in front of the nightstand.\"\nAre 3D VLMs better at learning spatial relationships than 2D VLMs? While our results seem to imply that 3D VLMs outperform their 2D counterparts in learning spatial relationships, we point out that 3D-LLM [18] and LEO [20] are trained with indoor scan datasets, making the comparison between 2D and 3D models not entirely fair. Nevertheless, we hypothesize that 3D models have an advantage in spatial understanding due to their ability to incorporate depth information. Therefore, we developed ROBOSPATIAL to be both 2D and 3D ready to support further research in this area."}, {"title": "4.3. Real Robot Experiments", "content": "We designed a comprehensive suite of tasks that require spatial reasoning and object manipulation. We use a diverse set of objects, including various shapes (e.g., cubes, cylinders, boxes), colors (red, orange, blue, green), and everyday items (food items, toys) as shown in Figure 4. Our goal in selecting these objects is not to confound the methods with difficult-to-identify items but to make identification as simple as possible with no ambiguity. For each scene configuration, we asked two yes/no questions, followed by asking a pick-and-place question. We note that picking and placing were treated as independent tasks for evaluations. We conducted over 200 model queries during our experiments, where details of questions and scenes used in the experiments are in the Appendix. For robot experiments, we used a Kinova Jaco robot [6] paired with a ZED2 camera for RGB(D) perception and employ cuRobo [47] for collision-free motion planner.\nWe evaluate the following VLMs: LLaVA-NeXT [34] and RoboPoint [57], both with and without ROBOSPATIAL training; and Molmo [12] and GPT-40 [40] as baselines. Table 7 and Fig 4 presents the results of the real-world robot experiments. Our results show that for generic VLM such as LLaVa-Next [34], training on ROBOSPATIAL significantly improved its spatial understanding capabilities in robot manipulation tasks. Furthermore, ROBOSPATIAL-trained models were more likely to align predictions with the implied reference frame, as demonstrated in the \"place in front of the pony\" task, where only ROBOSPATIAL-trained models' predictions aligned correctly in front of the pony's head. Similarly, in the \"place in front of the orange juice box\u201d task, ROBOSPATIAL-trained models could identify an appropriate distance to place the object relative to its size. In contrast, for RoboPoint, the predicted point was too far to the object, making it difficult to place the object correctly. However, we note that GPT-40 performs comparably with ROBOSPATIAL-trained RoboPoint. We attribute this to GPT-40 being able to understand much more diverse language inputs, unlike ROBOSPATIAL, which was trained on templated language.\nHere, we show that our dataset enhances the performance of 2D VLMs in robotics tasks. In our robot setup, 2D VLMs project predicted points from pixel space into 3D space; however, even a 2-pixel error can translate into a displacement of 5 to 10 cm in 3D space, leading to task failure. Interesting future direction is exploring how perspective impacts 2D point selection in the context of VLMs. Another interesting future direction is to enable 3D VLMs to capably support partial 3D point cloud observations, unlike the models we currently train, which require complete colored 3D point clouds, so that they can be effectively utilized in real-world robotics scenarios."}, {"title": "5. Conclusion", "content": "We introduce ROBOSPATIAL and ROBOSPATIAL-Home, a large-scale 2D/3D spatial understanding training and evaluation dataset tailored for robotics. Experimental results show that models trained with ROBOSPATIAL are able to understand spatial relationships, generalize to unseen relationships, and infer nuanced reference frames, making them applicable in a wide range of tasks that require spatial understanding. We further demonstrate the real-world applicability of ROBOSPATIAL with robot experiments. Furthermore, our automatic data generation pipeline can be used to extend the dataset whenever needed. We show that ROBOSPATIAL has the potential to serve as a foundation for broader applications in robotic scenarios that require spatial understanding."}]}