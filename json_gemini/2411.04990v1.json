{"title": "Clustering in Causal Attention Masking", "authors": ["Nikita Karagodin", "Yury Polyanskiy", "Philippe Rigollet"], "abstract": "This work presents a modification of the self-attention dynamics proposed by Geshkovski et al. (2023b) to better reflect the practically relevant, causally masked attention used in transformer architectures for generative AI. This modification translates into an interacting particle system that cannot be interpreted as a mean-field gradient flow. Despite this loss of structure, we significantly strengthen the results of Geshkovski et al. (2023b) in this context: While previous rigorous results focused on cases where all three matrices (Key, Query, and Value) were scaled identities, we prove asymptotic convergence to a single cluster for arbitrary key-query matrices and a value matrix equal to the identity. Additionally, we establish a connection to the classical R\u00e9nyi parking problem from combinatorial geometry to make initial theoretical steps towards demonstrating the existence of meta-stable states.", "sections": [{"title": "Introduction", "content": "The introduction of the Transformer architecture Vaswani et al. (2017) has markedly impacted the landscape of natural language processing (NLP), signaling the advent of large language models. Central to the Transformer architecture is the self-attention mechanism, a special kind of layer that distinguishes it from preceding models such as ResNets. This innovation has yielded unprecedented performance not only in machine translation and text summarization but also in areas beyond NLP, including computer vision, speech recognition, and robotics. The flexibility and efficiency of Transformers underscore their integral role in the progression of artificial intelligence. Despite their widespread use, the theoretical foundations underlying their success remain underexplored.\nFollowing Sander et al. (2022), recent studies by Geshkovski et al. (2023a) and Geshkovski et al. (2023b) have proposed a mathematical framework to analyze Transformers as interacting particle systems, demonstrating that tokens, when modeled as particles, exhibit clustering under certain conditions on the Key, Query, and Value matrices. These works primarily focus on full (mean-field) attention mechanisms, where each token can interact with every other token. Building upon this foundation, our research extends the analysis to causal attention mechanisms, wherein each token is restricted to interact only with preceding tokens. This distinction is crucial, as causal attention is prevalent in Transformer models employed in generative AI and known as decoder architectures.\nCausal attention is crucial for sequence generation tasks, ensuring that each token only attends to previous tokens and not future ones, thereby preserving the correct temporal order. This mechanism, also known as autoregressive attention, masks future tokens during attention computation to prevent the model from accessing information it hasn't generated yet. At inference time, causal attention allows the model to generate text one token at a time, using previously generated tokens to inform the next, ensuring coherent and contextually accurate sequences. This step-by-step generation process is computationally efficient, as each token is produced in a forward pass without needing to revisit previous steps. In contrast to full attention, which considers all tokens simultaneously and is suitable for tasks like machine translation where the entire sequence is known, causal attention is essential for tasks requiring real-time, sequential output. This computational advantage explains the pervasiveness of causal attention not only in natural language processing but also in image generation with tools like DALL-E (Ramesh et al., 2021), VQGAN (Esser et al., 2021), or Parti (Yu et al., 2022) and multimodal foundation models, notably Chameleon (Team, 2024). More generally, the use of masked attention where tokens pay attention to a subset of other tokens has been driving recent scaling efforts and has led to"}, {"title": "Causal attention", "content": "Before describing our model of causal attention dynamics, we review the idea of Geshkovski et al. (2023b) for modeling the full attention dynamics. In that work, the evolution of representations of tokens through the layers is modeled as a system of n coupled Ordinary Differential Equations (ODEs) describing dynamics of a system of particles x1(t), . . ., xn (t). A brief part of their derivation of the dynamics from the transformers architecture is written in Section A.1. The particle position xk(t) corresponds to representation of the k-th token at layer t (where for convenience, t is allowed to take non-integer values) and due to RMSNorm the particles are forced to live on a unit sphere Sd-1. (RMSNorm layer usually also includes a multiplication by a trainable diagonal matrix D, but the effect of this step can be equivalently achieved by multiplying K, Q, V matrices by D.) These ODEs are parametrized by three matrices, known as the query Q, the key K and the value V, respectively, and that are assumed to be square d \u00d7 d matrices. More specifically, token k evolves according to\n$\\Pxk(t) = \\frac{1}{Zk(t)}(\\sum_{j=1}^{n}e^{\\beta (Qx_k(t),Kx_j(t))} Vx_j(t)),$ \nwhere $P_{xy} = y - \\frac{(x,y)}{x^2}x$ is the projection onto the tangent space of Sd\u22121 at x, and\n$Z_k(t) = \\sum_{j=1}^{n}e^{\\beta (Qx_k(t), Kx_j(t))}$ \nis a normalizing factor. Note that the dynamics of the k-th token depend on the positions of all tokens j \u2208 [n], which is a landmark characteristic of full attention leading to the so-called mean-field dynamics studied in Geshkovski et al. (2023b); see also Geshkovski et al. (2023a); Castin et al. (2024); Paul and Tr\u00e9lat (2024).\nIn this work we focus on causal attention, where the dynamics of token k depend only on the position of tokens j \u2264 k. As described in the introduction, this modification is by now the dominant type of transformer architecture in generative AI. To reflect causal masking, we modify the ODE governing the dynamics of token k as follows:\n$\\Pxk(t) = \\frac{1}{Z_k(t)}(\\sum_{j=1}^{k}e^{\\beta (Qx_k(t),Kx_j(t))} Vx_j(t)),$\nwhere the normalizing factor Zk(t) is naturally updated to\n$Z_k(t) = \\sum_{j=1}^{k}e^{\\beta (Qx_k(t), Kx_j(t))}$"}, {"title": "Single token dynamics", "content": "Note that in (CSA) dynamics, the first token is evolving fully autonomously without the influence of others. Thus, we start from the description of its evolution. It will also guide our understanding of the dynamics of subsequent tokens. The first token moves according to the equation\n$\\x(t) = P_{x(t)} (Vx(t))$"}, {"title": "Final Configuration", "content": "The system of n tokens that we are studying is far more complicated than for a single token. Even establishing convergence to some point as t\u2192 \u221e is challenging. In Geshkovski et al. (2023b), similar models were analyzed analytically by noticing that the dynamical system has the structure of the gradient flow of some potential function:\n$\\x(t) = \\nabla H(x).$\nFor such systems, groundbreaking results of \u0141ojasiewicz (1962, 1965, 1984) (see Haraux (2012) for a self-contained overview) guarantee convergence to a critical point of H assuming it is real-analytic.\nHowever, our system (CSA) does not have a gradient-flow structure and thus techniques of \u0141ojasiewicz are not applicable. On the other hand, we have a significant advantage in the hierarchical structure of our system, allowing us to study tokens sequentially.\nWe have already understood the evolution of the first token. In this section, we do two things. First, we describe, based on our analytical and numerical insights, conjectures about the asymptotic configuration x(t) for t \u2192 \u221e. The surprising result here is that only the spectral properties of V (and not K or Q) affect asymptotics. Second, we rigorously prove convergence to a single point for the special case of V = Id. We note that unlike the proof in Geshkovski et al. (2023b) (see also Markdahl et al. (2017); Criscitiello et al. (2024)), our result works for all K and matrices, while the proof in Geshkovski et al. (2023b) works only for QK = V and Markdahl et al. (2017); Criscitiello et al. (2024) is restricted to $Q^TK = Id$.\nOur main insight is that there are two major forces that drive each token: its internal force which is described by Lemma 3.1, and the external force induced by all the particles preceding it, which is either attractive or repulsive depending on the sign of the top eigenvalue(s) of V. The balance between the two forces is defined via attention.\nTo get a better grasp of how the external force works, we consider the case where the first (internal) force vanishes, that is, V = Id. In this case, the tokens collapse asymptotically to a single point.\nTheorem 4.1. Let V = Ia and Q, K be arbitrary matrices. Then, for almost any starting point (x1(0),..., xn (0)) with respect to the volume measure on (Sd\u22121)n, the causal transformer dynamics (CSA) converge to a single cluster:\n$\\forall k\\in [n], lim_{t\\to\\infty} x_k(t) = x_1(0).$\nWe prove this result in Section B.2. In the proof, weight functions are only required to be positive and continuously differentiable (C1). This ambiguity suggests that incorporating time-dependence of Q and K might not alter the theorem's validity, but it significantly adds complexity to the proof in dealing with non-autonomous systems.\nSteps similar to our proof of Theorem 4.1 can be followed to study the more general case of the matrix V \u2260 Id. Unfortunately, one runs into multiple technical issues with application of the stable-manifold theorem from dynamical"}, {"title": "Meta-stable clustering", "content": "As we discussed earlier, perhaps the most fascinating discovery of Geshkovski et al. (2023b) is the existence of meta-stable clusters in the full-attention dynamics. It turns out that the same phenomenon persists in the causal-attention dynamics that we study here.\nThe dynamical evolution of the system is illustrated in Fig. 2. At t = 150, the initially uniform distribution of 200 particles consolidates into seven distinct clusters. While Theorem 4.1 establishes the eventual collapse into a single cluster, these intermediate clusters exhibit remarkable metastability, persisting with negligible movement over extended time periods\u2014at least until t = 500 according to Fig. 2\u2014before sequential merging occurs. We define these meta-stable configurations as meta-stable clusters, with three-dimensional analogues shown in Fig. 1a and 1b.\nGiven that the time parameter in our dynamics corresponds to network depth in transformer architectures, the meta-stable configurations, rather than final states (achieved at t = exp(\u03a9(\u03b2))), hold greater practical significance. The emergence of meta-stable clustering and its associated dimensionality reduction may provide fundamental insights into transformers' capacity for generating efficient context-dependent representations.\nThe spread of the clouds depends on the relative importance of each token's own attention, that differs with various K, Q. There are choices of K and Q that result in complex interactions without structure. For example, when $Q^TK = [[0, -1, 0], [1,0,0], [0, 0, 1]], V = -I_3$"}, {"title": "Limitations", "content": "Our analysis presents both theoretical and practical limitations. From a theoretical perspective, we establish two key results: (1) strong R\u00e9nyi centers with separation distance c\u03b2-1/2 maintain quasi-stationarity for time scales of order exp(c2/2) per Lemma 5.1, and (2) exactly stationary centers attract all remaining particles (Theorem 5.2). In particular, by choosing c ~ B\u025b, we can get exponential in \u03b2 time of quasi-stationarity. However, this falls short of proving meta-stable clustering, as Theorem 5.2 provides no bounds on the convergence time. Consequently, we cannot guarantee that strong R\u00e9nyi centers remain sufficiently stationary during particle aggregation. A complete meta-stability theory would require demonstrating that each R\u00e9nyi center captures \u03a9(n) particles in O(1) time as n \u2192 \u221e. Currently, even the weaker claim of capturing w(1) particles remains unproven, presenting a crucial direction for future research.\nThe practical limitations stem from two model simplifications: the use of tied weights across layers (though supported by successful implementations, see Lan et al. (2020)), and the omission of the MLP layer central to Transformer architectures. Incorporating the MLP dynamics into our theoretical framework remains a significant open challenge."}, {"title": "Supplementary Material", "content": null}, {"title": "From Transformers to ODES", "content": "The derivation of the equation (SA) was thoroughly described in Geshkovski et al. (2023b), but for completeness, we briefly repeat it here to explain how the problem arises.\nIn general, a typical Transformer architecture consists of repeated layers of multi-head attention, multi-layer perceptrons (MLP), normalization, and residual connections Vaswani et al. (2017). In this work, we simplify this setting by focusing only on the geometric behavior of a single-head attention layer with normalization and residual connections, omitting the MLP for brevity.\nOne head of a standard attention layer is defined as follows. Given an input sequence represented by the token embeddings X \u2208 Rn\u00d7d, where n is the number of tokens and d is the dimension of the embedding space, and matrices WQ, WK, Wy to compute queries, keys, and values, the attention mechanism computes a weighted sum of values based on their relevance to a query in the following form\n$Attention(X) = softmax(\\frac{XW_QW_K^TX}{\\sqrt{d}}) XW_v$.\nBy adding an RMS normalization from Zhang and Sennrich (2019) and a residual connection, the transformation from layer t to layer t + 1 is given by:\n$X^{t+1} = X^t + RMSNorm(Attention(X)).\n(4)$\nHere, different tokens are represented as rows of the matrix X for computational reasons. For consistency with the convention that vectors are represented as columns, we transpose everything and denote a sequence of tokens encoded as particles in the d-dimensional embedding space Rd as (x1,...,xn), corresponding to the columns of XT. Additionally, to simplify the notation we denote V := WJ, Q := W, and K := W, and introduce an arbitrary temperature parameter \u1e9e instead of the fixed scaling factor 1/\u221ad. With these notational adjustments, one term of attention added to the k-th token can be written explicitly as:\n$attn(x_1,...,x_n)_k = \\frac{1}{Z_k}\\sum_{j=1}^{n}e^{\\beta (Qx_k,Kx_j)}Vx_j,$\nwhere\n$Z_k = \\sum_{j=1}^{n}e^{\\beta (Qx_k,Kx_j)}.$\nThe equation (4) can be interpreted as a discrete derivative, with Xt+1 \u2013 Xt representing the difference between layers. Therefore, the trajectory Xt can be viewed as a discretization of a continuous flow. RMS normalization ensures that tokens remain on the scaled unit sphere, but from properly rescaling Q, K, V we can assume that they stay on the standard unit sphere Sd-1. Combining all these observations, the dynamics of token propagation through layers can be expressed as:\n$X_k(t) = \\frac{1}{Z_k(t)}Px_k(t)(\\sum_{j=1}^{n}e^{\\beta (Q(t)x_k(t), K(t)x_j(t))} V (t)x_j(t)),$\nwith\n$Z_k(t) = \\sum_{j=1}^{n}e^{\\beta (Q(t)xkx(t),K(t)x(t))},$\nand the projector Pz(y) := y \u2212 (x, y)x/|x|2 ensuring that xk remains on the sphere. This leads to the equation (SA), and applying a causal constraint, where each token attends only to the previous ones, transforms it into the causal attention equation (CSA) studied in this work."}, {"title": "Interaction Potential", "content": "For completeness, here we describe the key properties of the interaction potential $h(x) = e^{\\beta(cos(x)-1)} sin(x)$ from (IntPot), which defines the interactions between particles, and its derivative g(x) = h'(x)."}, {"title": "Proof of Lemma 3.1", "content": "Let us show that trajectories x(t) of our system can be characterized as normalized solutions of a linear homogeneous ODE in Rd. Consider a solution y(t) of:\n$\\dot{y}(t) = Vy(t), y(0) = x(0).\n(5)$\nFor s(t) := y(t)/||y(t)||, we derive:\n$\\dot{s}(t) = \\frac{\\dot{y}(t)}{||y(t)||} - \\frac{y(t) \\dot{||y(t)||}}{||y(t)||^2}= \\frac{y(t)}{||y(t)||} \\frac{y(t) \\dot{y(t)}}{||y(t)||^2}= Vs(t) - (s(t), Vs(t))s(t) = P_{s(t)} (Vs(t))$"}, {"title": "Proof of Theorem 4.1", "content": "We begin with a simple geometric lemma.\nLemma B.1. Let x, y, z \u2208 Rd with ||x|| = ||y|| = 1. If |<y, z)| < (x, z), then:\n$(x, z) \u2265 (x, y)(y, z)$\nwith equality if and only if either: (i) (x, z) = 0, or (ii) |\u3008y, z)| = (x, z) and (x, y) = sign((y, z)).\nProof. By the Cauchy-Schwarz inequality and the hypothesis:\n$(x, y)(y, z) \u2264 (x, y)||(y, z)| < |(x, y)|(x, z) \u2264 (x, z),$\nwhere the last inequality follows since |(x, y)| \u2264 1 for unit vectors.\nThe equality conditions follow from examining when each inequality becomes equality in the chain above.\nWe continue with the proof of Theorem 4.1.\nProof. The system of particles is governed by equations\n$X_k = \\frac{1}{Z_k} \\sum_{j=1}^{k-1}e^{\\beta (Qx_k,Kx_j)} (X_j - (X_j, x_k)X_k),$\nwhere we omit t from the notation for simplicity.\nThis system is autonomous, so we first explore its critical points and their stability. For autonomous systems with established convergence, it is well-known that for any absolutely continuous initialization, the limiting point is strongly unstable with probability zero (see (Shub, 2013, Thm. III.7, Ex. III.3) and (Geshkovski et al., 2023b, Lemma B.1)). Note that the proof in Geshkovski et al. (2023b) is stated for gradient ascent dynamics but it readily extends to any smooth autonomous dynamics on a compact Riemannian manifold.\nDefine:\n$f_k(x) := \\frac{1}{\\sum_{j=1}^{k-1}e^{\\beta(Qx_k,Kx_j)}} \\sum_{j=1}^{k-1}e^{\\beta (Qx_k,Kx_j)} (X_j - (X_j, x_kX_k).$\nWe aim to (i) find stationary points x where all f(x) = 0 and (ii) analyze eigenvalues of the Jacobian $(\\frac{\\partial f}{\\partial x})$ at said stationary points.\nAny critical point must satisfy one of the following:"}, {"title": "Proof of Lemma 5.1", "content": "Proof. For d = 2, using polar coordinates xk = eik, the system becomes:\n$\\dot{\\phi}_{s_k} =  \\frac{1}{Z_{s_k}} \\sum_{j=1}^{k-1}h(\\phi_j - \\phi_{s_k}),\n(7)$\nwhere we recall that $h(x) = e^{\\beta(\\cos(x)-1)} \\sin x$ is the function studied in Lemma A.1 and\n$Z_k = \\sum_{j=1}^{k-1}e^{\\beta(\\cos(\\phi_j-\\phi_k)-1)}  \\geq 1$\ndue to the j = k term.\nLet or* denote the closest preceding particle to osk. Because of periodicity we may assume that |4j \u2212 4k| < \u03c0 for all particles. Without loss of generality, assume r* > Osk and set \u2206 = 4r* - sk.\nWe first show that A cannot decrease fast. To that end, observe that the evolution of follows:\n$\\frac{Sk-1Sk}{Sk\\zeta\\zeta} - \\frac{1}{Sk\\zeta\\zeta} = \\frac{1}{Sk\\zeta\\zeta} Skh(T)\n- \\frac{1}{Z} - \\frac{1}{Z} - \\frac{1}{Z} - \\frac{1}{Z}$\n$\\frac{s}{r} - \\frac{z}{r} - \\frac{t}{r} - \\frac{T}{r}$"}, {"title": "Proof of Lemma 5.3", "content": "Proof. We prove that the particles converge exponentially fast to some strongly stable critical point * via induction on their number.\nInduction base. In this proof the induction base for n 1 follows from the induction step proof applied to the first particle 41.\nInduction step. Consider 41(t),...,n\u22121(t). They do not depend on on and from induction hypothesis converge exponentially fast to some asymptotically stable point 61,\u06f0\u06f0\u06f0,\u22121. In particular, one has k \u2208 L1([0,\u221e)), k \u2208 [n-1].\nConsider full derivative"}]}