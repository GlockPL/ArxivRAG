{"title": "SELF-EXPLAINED KEYWORDS EMPOWER LARGE LANGUAGE MODELS FOR CODE GENERATION", "authors": ["Lishui Fan", "Mouxiang Chen", "Zhongxin Liu"], "abstract": "Large language models (LLMs) have achieved impressive performance in code generation. However, due to the long-tail distribution of LLMs' training data, low-frequency terms are typically underrepresented in the training process. Consequently, LLMs often misunderstand or overlook problem-specific, low-frequency keywords during code generation, compromising the accuracy of the generated code. To address this, we propose a novel technique named SEK (Self-Explained Keywords), which empowers an LLM for better code generation by extracting and explaining the key terms in the problem description with the LLM itself and ranking them based on frequency. Comprehensive experiments across three benchmarks, i.e., HumanEval(+), MBPP(+), and APPS, with five representative LLMs, show that SEK can significantly improve LLMs in code generation, yielding substantial and consistent gains. For instance, SEK improves the Pass@1 of DeepSeek-Coder-V2-Instruct from 85.4% to 93.3% on the Humaneval benchmark. Further analysis confirms that SEK enables the LLMs to shift their attention from low-frequency keywords to their corresponding high-frequency counterparts.", "sections": [{"title": "1 INTRODUCTION", "content": "Code generation aims to generate a code snippet that meets the intent described in natural language. This process can potentially reduce the costs of software development (Xu et al., 2022; Yin & Neubig, 2017; Vaithilingam et al., 2022). Recently, the notable success of LLMs such as ChatGPT (OpenAI, 2022) and Llama-3 (AI@Meta, 2024) has substantially enhanced the state-of-the-art in code generation. These LLMs demonstrate remarkable proficiency in comprehending natural language descriptions and translating them into code snippets.\nDespite the remarkable success, their training data suffer from the long-tail distribution problem (Chen et al., 2024d; Li et al., 2024a), which results in insufficient training of the correspondence between some low-frequency terms and their respective code implementations. Consequently, when performing real-world coding tasks, LLMs often struggle to translate certain low-frequency keywords into appropriate code implementations accurately, and may even overlook such keywords, potentially compromising the accuracy of the generated code.\nAs illustrated in Figure 1, the coding problem requires returning even digits within a given range in ascending order. The term even digits rarely appears in code datasets, causing LLMs to misinterpret it as even numbers. As a result, LLMs fail to recognize that it refers to the even numbers within the range of 0 to 9. However, if we explicitly explain even digits using common, high-frequency terms and prompt the LLM to focus on it, the LLM can successfully produce a correct implementation.\nHowever, converting low-frequency keywords into high-frequency natural language descriptions requires additional human effort for identification and explanation. Our key idea is that although the direct mapping from low-frequency terms to code is rare in code corpora, the semantics of these terms are typically understandable by LLMs after pre-training on large-scale general corpora. Moreover, previous works find that such problem-specific key items can be identified by LLMs (Fang et al., 2024; Fan et al., 2024). Based on this, this work proposes Self-Explained Keyword (SEK)."}, {"title": "2 METHODOLOGY", "content": "Code generation aims to generate a solution program based on a problem description. Typically, a problem description includes implementation requirements, and several test cases to help further understand the problem.\nFigure 3 illustrates the overview of SEK. SEK is designed to address the issue of LLMs overlooking low-frequency terms in the program description due to the long-tail distribution in their training data. To address it, one key is to leverage the LLM's capabilities to identify and explain potentially overlooked keywords within the problem description. We employ a carefully crafted prompt with a few-shot learning method to achieve this. After obtaining the keywords and their explanations, another challenge is how to effectively integrate them with the original problem description. For this purpose, we introduce a frequency-based ranking algorithm that prioritizes less frequent tokens, which are more likely to be overlooked by the LLM. These ranked keywords are then appended to the original problem description, serving to guide the LLM towards generating an accurate solution. The process comprises three main steps:\nKeyExtract & Explain (Section 2.1): Based on the problem description, SEK constructs a prompt to guide the LLM to identify and explain keywords within the problem description.\nKeyRank (Section 2.2): SEK employs a frequency-based ranking algorithm to prioritize the extracted keywords.\nPromptEnrich (Section 2.3), SEK concatenates the ranked keywords and their explanations with the original problem description to create an enriched problem description. This comprehensive formulation serves as the final input for the LLM to generate code solutions."}, {"title": "2.1 \u039a\u0395\u03a5EXTRACT & EXPLAIN", "content": "In this step, SEK extracts and explains keywords from the given problem description. Our key insight is that LLMs inherently possess strong understanding and reasoning abilities after training on large-scale general corpora, enabling them to explain crucial concepts within a problem description. The effectiveness of using LLMs for keyword extraction has also been demonstrated by recent studies (Maragheh et al., 2023; Lee et al., 2023). Inspired by this insight, SEK uses the LLM itself to perform the task with a prompt-based approach.\nSpecifically, SEK begins by designing a prompt to instruct an LLM for keyword extraction and explanation. The prompt is shown in Prompt for KeyExtract & Explain in Figure 3, which consists of three parts. First, it provides the overall instruction for the task, namely the generation of keywords and their corresponding explanations. Then, it specifies the format of input and output. Finally, it provides detailed guidelines. Intuitively, terms associated with input, output, and supplementary content (i.e., clarifications of keywords or specifications of value ranges) within the problem description are relatively important, as they contain the problem's core elements, objectives, and constraints (Guideline 1). For explanations, given the potential ambiguity in natural language expressions and the clarity of the public test cases, the generated explanations should be both precise and consistent with these test cases (Guideline 2,3). We also impose limitations on the keyword quantity to guarantee that the LLM identifies and outputs only the important keywords in the problem description (Guideline 4). Ultimately, to facilitate subsequent processing, we further emphasize the output format (Guideline 5). Additionally, we use several examples to leverage LLMs' in-context learning ability for understanding and solving this task."}, {"title": "2.2 KEYRANK", "content": "After extracting and explaining the keywords, the next goal is to enhance the original prompt. Previous research has demonstrated that LLMs are sensitive to the order of tokens in the prompt, known as position bias (Li et al., 2024b; Yu et al., 2024). It highlights the need to carefully arrange the extracted keywords. Notably, pragmatic human developers tend to place more important keywords at the beginning in practice (Hunt & Thomas., 2000). This preference may be reflected in the training dataset, leading LLMs to also focus more on the keywords written at the front. Therefore, we propose a set of heuristic rules to rank keywords by importance, namely KeyRank. The specific Algorithm is provided in the Appendix A.\nWe first examine the keywords extracted by two LLMs (Llama 3.1 and DeepSeekCoder-V2) for part of the coding problems in the APPS training set. These keywords can generally be categorized into three types: (1) Function keywords, which match the desired function names, such as count_nums in Figure 3. (2) General keywords, which appear in the problem description, like sum of digits in Figure 3. (3) Abstract keywords, which do not appear in any input; instead, they are abstract terms summarized from multiple concepts. For example, for two different concepts \"substring before the dot\" and \"substring after the dot\" in the problem description, LLM may combine them into a single keyword substring before/after the dot. The proportions of these three categories are 22.5%, 59.9%, and 17.7%.\nWe hypothesize that abstract keywords are the most important, as they encompass explanations across multiple concepts. General keywords refer to single concepts and are of secondary importance, while function Keywords, whose explanations have already appeared in the problem description, are the least important. Therefore, we propose ordering the keywords as abstract \u2192 general \u2192 function. Appendix E.1 demonstrates that this heuristic combination order yields the best results.\nMoreover, since general keywords represent the majority (59.9%) and LLMs could extract multiple general keywords for a single problem, we further perform an internal ranking of these general keywords. We argue that a keyword is more important if it appears more frequently in the problem description (i.e., higher term frequency). Conversely, if a keyword appears less frequently in a corpus (i.e., lower document frequency), the corresponding code conversion could be more challenging as we stated in the Introduction section, and thus its explanation is more significant. Therefore, we use the TF-IDF, a widely used metric that combines term frequency (TF) and inverse document frequency (IDF), to assess the importance of general keywords. TF-IDF is calculated as follows:\n$TF-IDF = \\frac{n_i}{\\sum_{k} n_k} \\times log \\frac{|D|}{1 + |\\{j : t_i \\in d_j\\}|}$"}, {"title": "2.3 PROMPTENRICH", "content": "After obtaining the ranked keywords and their explanations, SEK integrates them with the original problem. As shown in the enriched problem in Figure 3, SEK appends the ranking results to the end of the problem, providing additional explanations for key concepts in the problem. It's worth noting that, to maintain the coherence of the problem context, we insert the phrase \"Analyze the following key terms and their relationships within the problem context:\" after the problem. This acts as a semantic buffer, smoothly transitioning from the original problem description to the appended keywords. The enriched problem is then input into the LLM to generate the final solution."}, {"title": "3 EXPERIMENTAL SETUP", "content": "We conduct a series of experiments to evaluate the effectiveness of the proposed approach SEK. In this section, we describe our experimental setup, including the selected models, benchmarks, evaluation metrics, baselines, and implementation details."}, {"title": "3.1 STUDIED LLMS", "content": "We select five representative LLMs to evaluate SEK, balancing between open-source and proprietary models, as well as covering a range of model sizes and architectures. The open-source models include Llama-3.1-70B-instruct (Dubey & Abhinav Jauhri, 2024), which is a dense decoder-only model with 70-billion parameters, Mixtral-8\u00d722B-instruct-v0.1 (Jiang et al., 2024), which is a sparse Mixture-of-Experts (MOE) model having 141-billion total parameters with 39B active, and DeepSeek-Coder-V2-236B-Instruct-0724 (Zhu et al., 2024), which is a sparse MOE model having 236B parameters with 21B active. We access DeepSeek-Coder via DeepSeek-AI's API. For proprietary models, we include GPT-3.5-turbo-0125 (OpenAI, 2022) and GPT-4o-mini (OpenAI, 2024), accessed via OpenAI's API. Detailed specifications for each model are provided in the Appendix B."}, {"title": "3.2 BENCHMARKS AND EVALUATION METRIC", "content": "Following previous work (Chen et al., 2023b; Dong et al., 2023; Zhong et al., 2024b; Jiang et al., 2023b), We conduct experiments on three public code generation benchmarks HumanEval(+) (Chen et al., 2021; Liu et al., 2024), MBPP(+) (Austin et al., 2021; Liu et al., 2024), and APPS (Hendrycks et al., 2021). Considering the high cost of evaluating the entire APPS test problems and following prior work (Olausson et al., 2023; Huang et al., 2024b; Le et al., 2024; Yang et al., 2023), we randomly select 300 problems from the APPS test set for evaluation. To mitigate the uncertainty introduced by random sampling, we conduct multiple experiments with different sample seeds. More details are in Appendix E.3. For detailed descriptions of each benchmark, please refer to Appendix C. We evaluate model performance using the Pass@1 metric, which measures the ability to generate correct solutions in a single attempt. This also aligns with real-world scenarios where developers aim to produce accurate code on the first try."}, {"title": "3.3 BASELINES", "content": "\u2022 Default LLM: This approach is based on the EvalPlus framework (Liu et al., 2024), using problems from the benchmark as input to prompt LLMs for code generation.\n\u2022 CoT (Chain-of-Thought) (Wei et al., 2022): This approach generates a series of reasoning steps during the solution-generation process for each problem. To ensure comparative fairness, both the CoT baseline and SEK employ an equal number of demonstrations.\n\u2022 SelfEvolve (Jiang et al., 2023a): This approach first uses LLMs to generate problem-specific knowledge and produce initial code solutions based on such knowledge. Then, it iteratively refines code solutions with LLMs based on execution feedback. Notably, SelfEvolve uses different prompt templates for different benchmarks to extract knowledge. Since these prompt templates have been open-sourced, we consistently apply its two-stage prompts on HumanEval (see Appendix H) in our replication process. For a fair comparison, we remove the self-refinement module, and employ the same number of demonstrations as SEK.\n\u2022 Beam Search (Wiseman & Rush, 2016): This approach employs distinct search beams and optimizes selection during the decoding process. Given that SEK requires two LLM invocations per coding problem (one for generating keywords and explanations, and one for generating the code), we demonstrate the benefit of the process of SEK by comparing it with performing two searches within the LLM's searching space, i.e., beam search with a beam size of 2."}, {"title": "3.4 IMPLEMENTATION DETAILS", "content": "Prompt Design. It's worth noting that the implementations of SEK, CoT, and Beam Search are based on the EvalPlus framework. Specifically, the only difference between SEK and Default is the addition of keywords and explanations to the problem description. APPS contains problems in two formats: call-based format. Following previous work (Olausson et al., 2023; Inala et al., 2022; Chen et al., 2023b), we employ a two-shot prompt to guide the LLM to generate appropriate solutions for different formats.\nDemonstration Selection Strategy. Inspired by previous work (Wei et al., 2022; Mu et al., 2023; Wang et al., 2023), we adopt a differentiated strategy that varies based on benchmark complexity (See Appendix D). To reduce bias, we employ an LLM separate from our target LLMs (Claude-3.5-Sonnet) to generate keywords and explanations for each demonstration, which are then manually reviewed and refined (See Appendix D).\nConfiguration. In our experiments, we treat the LLMs as black-box generators and only need to set a few key interface parameters. We maintain consistent settings across all LLMs, employing greedy decoding for output generation. The maximum output length is uniformly set to 2048 tokens. Specifically, the LLMs accessed via APIs do not support Beam Search. Thus, we only implement Beam Search for Llama-3.1-70B-Instruct and Mixtral-8\u00d722B-Instruct-v0.1. Due to resource limitation, we compare SelfEvolve using GPT-3.5-turbo following the original paper (Jiang et al., 2023a) and additionally use two open-sourced LLMs (Llama-3.1 and Mixtral-8x22B)."}, {"title": "4 EXPERIMENTAL RESULTS", "content": "Overall, SEK substantially improves code generation performance, achieving notable gains across various LLMs and datasets. We observe that SEK achieves greater performance improvements on HumanEval(+) and APPS than MBPP(+). For instance, on HumanEval, SEK demonstrates an absolute average performance improvement of 4.4% over the Default, whereas, it achieves an improvement of 1.8% on MBPP. This may be because the problems in HumanEval(+) and APPS are more complex than those in MBPP, and simple problems are easy to understand and alleviate the need to extract and explain keywords. As shown in Table 3, the average number of tokens per problem is 26.1 for MBPP, while those numbers are 67.7 and more than 257.3 for HumanEval(+) and APPS. These results may indicate that SEK can better improve LLMs' problem-solving capabilities on relatively complex problems than on simple problems.\nWe first discuss the performance on HumanEval(+) and APPS. These benchmarks are relatively complex compared to MBPP, and better demonstrate the effectiveness of SEK. SEK consistently outperforms Default across most LLMs. For instance, SEK achieves average absolute improvements of 6.7%, 3.6%, and 3.7% on APPS-Introductory, APPS-Interview, and APPS-Competition, respectively. However, GPT-4o-mini is an exception, which experiences a slight performance decline on Humaneval(+). This may be because the built-in prudence of GPT-4o-mini (Huang et al., 2024a) makes it tend to select more generic keywords, and such generic keywords fail to help LLMs understand low-frequency terms in the problem description. This conjecture is further underpinned by an observation that CoT similarly fails to enhance GPT-4o-mini's performance. The consistent improvements of SEK across most LLMs highlight its effectiveness in enhancing the problem-solving capabilities of LLMs.\nCompared to Beam Search, which also invokes LLMs twice, SEK shows notable performance improvements. For instance, on Humaneval and Humaneval+, SEK achieves absolute average improvements of 4.0% and 3.7%, respectively, over Beam Search. These can be attributed to SEK's unique technique: appending the problem's critical parts to the end, enabling LLMs to focus on and comprehend these key concepts. In contrast, Beam Search merely expands the search space without gaining a deep understanding of the problem, leading to lower diversity in outputs (Li & Jurafsky, 2016). Consequently, it cannot enhance problem-solving capabilities in a targeted manner like SEK (See Appendix I for different cases).\nCompared to CoT and SelfEvolve, SEK demonstrates a notable and consistent performance advantage. For instance, on Humaneval and Humaneval+, SEK achieves absolute average performance improvements of 7.2% and 6.5% over CoT. In contrast, the performance of CoT and SelfEvolve are inconsistent, sometimes even lower than Default. For instance, with Mixtral-8\u00d722B-Instruct-v0.1, SelfEvolve's performance on APPS-Interview is 0.5% lower than Default. The unstable performance of CoT can be attributed to its inherent unsuitability for generation tasks (Sprague et al., 2024). Similar phenomena have been observed in prior work (Wang et al., 2024; Zhang et al., 2024; Luo et al., 2024; Jiang et al., 2023b). While both SelfEvolve and SEK utilize LLMs to extract relevant knowledge from problem descriptions, they differ in the types of extracted knowledge. SEK focuses on low-frequency keywords, which are more difficult to be mapped to code implementation. This enables SEK to effectively fill the knowledge gaps during code generation. In contrast, SelfEvolve tends to merely restate the complete problem description for problems in code generation"}, {"title": "4.1 MAIN RESULTS", "content": "Table 1 presents the performance of SEK and the selected baselines across five representative LLMs on Humaneval(+), MBPP(+) and APPS of different difficulty levels. To be noted, the Default results of Mixtral-8\u00d722B-Instruct-v0.1 and DeepSeekCoder-V2-Instruct on Humaneval(+) and MBPP(+) are from the official leaderboard of the EvalPlus (Liu et al., 2024). However, as the other three LLMs are not in this leaderboard, we adhere to the EvalPlus framework to obtain their results."}, {"title": "4.2 DISCUSSION", "content": "We conduct additional experiments to comprehensively evaluate SEK's performance and robustness.\nGuidelines in the prompt for KeyExtract & Explain provide essential guidance for LLMs, and KeyRank effectively prioritizes keywords. Our ablation studies confirm that both guidelines and KeyRank play crucial roles in enhancing performance. As shown in Figure 4(a)-4(b), We evaluate Llama-3.1 and Mixtral-8\u00d722B on Humaneval (+). Removing either the guidelines or the KeyRank module results in performance degradation. For instance, removing the KeyRank module results in performance decreases of 2.4% and 1.2% on HumanEval and HumanEval+, respectively, for Mixtral-8\u00d722B-Instruct-v0.1. Moreover, removing each guideline from the prompt individually also results in performance degradation in most cases (See Appendix E.2). It is worth mentioning that even without KeyRank, SEK remains superior to the Default baseline. For instance, without KeyRank module, Mixtral-8\u00d722B-Instruct-v0.1 shows a 2.5% improvement on HumanEval compared to the Default, underscoring the strength of SEK's core mechanisms.\nSEK demonstrates robustness to variations in demonstrations, and the corpus used in KeyRank. To show its performance is not tied to a fixed set of keyword explanations within the demonstrations used in KeyExtract & Explain, We conduct experiments using two additional sets of keyword explanations randomly generated from the same LLM (i.e., Claude-3.5-Sonnet). As shown in Figure 4(c), although there is performance variance among different keyword explanations, as would be expected when using exemplar-based prompting (Gao et al., 2021; Min et al., 2022; Reynolds & McDonell, 2021), the three sets of keyword explanations consistently outperform the Default. Additionally, to evaluate the robustness to the corpus used in KeyRank, we employ select different corpus, as shown in Table 2. We observe that using SEK with Llama-3.1-70B-Instruct still shows a 6.1% absolute improvement on Humaneval compared to Default. These results demonstrate the robustness of SEK."}, {"title": "4.3 CASE STUDY", "content": "To further evaluate the effectiveness of SEK, we conduct a qualitative analysis. As shown in Figures 5, we select one representative sample from HumanEval, use DeepSeek-Coder-V2-Instruct as the base model, and compare the outputs of SEK with Default and CoT. See Appendix J for more details.\nThe problem requires finding boredom from a string of words. A boredom refers to a sentence that begins with words starting with \u201cI\u201d. Default fails at the very beginning by incorrectly segmenting sentences, possibly due to an inadequate understanding of the concept sentence. Although CoT correctly segments sentences through chain-of-thought reasoning, it errs when searching for boredom, mistakenly seeking sentences starting with \"I\" instead of boredom, indicating an incomplete under-"}, {"title": "5 RELATED WORK", "content": "LLM-based code generation: Recent advancements in LLMs have significantly improved code generation capabilities. Models like CodeGen (Nijkamp et al., 2022), StarCoder (Li et al., 2023), and GPT series (Black et al., 2022; Chen et al., 2021) have demonstrated remarkable performance in translating natural language descriptions into code snippets. These models primarily use decoder-only architectures and next-token prediction for pre-training. A subset, including CodeT5 (Wang et al., 2021) and PLBART (Ahmad et al., 2021), employs encoder-decoder architectures. Our work builds upon these foundations, focusing on enhancing LLMs' problem-solving capabilities without additional training.\nPrompting techniques for code generation: Prompting techniques for code generation can be broadly categorized into three types: The first type utilizes external knowledge to enhance LLMs' understanding of coding problems or intermediate outputs (Mu et al., 2023; Nashid et al., 2023; Zhong et al., 2024a). For example, CEDAR (Nashid et al., 2023) retrieves relevant code examples from an external knowledge base to help LLMs understand task requirements. The second type relies solely on LLMs' inherent capabilities, using prompt design to guide LLMs in generating code snippets that meet specific requirements (Wei et al., 2022; Wang et al., 2023; Yao et al., 2024). For instance, Chain of Thought (Wei et al., 2022) employs a step-by-step, chain-of-thought style prompt to guide LLMs in producing correct results. The third type integrates the previous two types, leveraging both external knowledge and the LLM's inherent knowledge to solve coding problems (Chen et al., 2023c; Jiang et al., 2023a; Tian & Chen, 2023; Chen et al., 2024c;b). For example, Self-Debug (Chen et al., 2023c) uses the code execution results or the code explanations generated by the LLM itself to debug the incorrect code multiple times. SEK belongs to the second category. Different from other methods, it focuses on improving LLMs' comprehension of the problem by identifying and explaining the key concepts in the problem description with LLMs themselves.\nKeyword extraction: Keyword extraction methods have evolved from traditional statistical (Sparck Jones, 1972; El-Beltagy & Rafea, 2009; Florescu & Caragea, 2017; Rose et al., 2010) and graph-based approaches (Mihalcea & Tarau, 2004; Wan & Xiao, 2008; Gollapalli & Caragea, 2014; Grineva et al., 2009) to more advanced techniques leveraging language models (Mahata et al., 2018; Bennani-Smires et al., 2018; Sun et al., 2020; Arora et al., 2017). Recent works like Attention-Rank (Ding & Luo, 2021) and LLM-TAKE (Maragheh et al., 2023) use self-attention mechanisms and language models to identify significant keywords. Our work extends this concept to the domain of code generation, using LLMs to extract and explain problem-specific keywords to enhance code solution generation."}, {"title": "6 CONCLUSION AND LIMITATIONS", "content": "In this work, we propose SEK, a simple yet effective method to enhance the code generation capabilities of LLMs. SEK leverages the LLM to extract and explain keywords from the problem description, followed by ranking them based on their frequency. Through extensive experiments, we demonstrate that SEK facilitates LLMs in capturing and clarifying key concepts within problems, thereby generating more accurate code solutions.\nOne limitation of SEK is that the two-stage invocation process of SEK incurs additional computational overhead. Future work could explore compressing the process into one invocation. In addition, keywords are extracted and explained by LLMs, of which the quality cannot be guaranteed due to the hallucinations of LLMs (Ji et al., 2023). Mitigating this requires enhancing the factual accuracy"}, {"title": "A AGLORITHM OF KEYRANK", "content": "First, we initialize the General Keywords, Abstract Keywords, and output as $K_g$, $K_a$, $K_y$, respectively. $EXTRACTFUNCTIONNAME$ extracts the method name if provided in the problem description. Otherwise, it returns a null value. Then, keywords are classified and scored. They can be divided into three classes: Abstract Keywords, General Keywords, and Function Keyword. Abstract keywords do not appear in any input; they are abstract terms summarized from multiple concepts and stored in $K_a$. General keywords denote items in the problem description. We calculate their importance using TF-IDF based on a code-related corpus. General keywords and their scores are stored in $K_g$. Function keyword refers to the method name for solving the problem. Its explanation provides a coarse-grained description of the problem requirements. We assign a score of -1 to the function keyword, and also store them in $K_g$. Finally, $SORTDESCENDING$ sorts the keywords in $K_g$ based on their scores. The keywords are combined in the order of abstract, general, and function keywords, and are then returned as the Ranked Keywords."}, {"title": "B STUDIED LLMS", "content": "\u2022 Llama-3.1-70B (Dubey & Abhinav Jauhri, 2024) is an open-sourced, decoder-only language model, pre-trained on 15t tokens from public sources. In our experiments, we use the Llama-3.1-70B-Instruct version.\n\u2022 Mixtral-8\u00d722B (Jiang et al., 2024) is an open-source, sparse Mixture-of-Experts (MOE) model with 141B total parameters, utilizing 39B active parameters. We use the Mixtral-8\u00d722B-Instruct-v0.1 version.\n\u2022 DeepSeek-Coder-V2-Instruct-0724 (Zhu et al., 2024), developed by DeepSeek-AI, is an open-source MoE code language model pre-trained on 10.2T tokens. The instruction-tuned version is further trained on 11B tokens.\n\u2022 GPT-3.5-turbo-0125 (OpenAI, 2022) is a close-sourced LLM from OpenAI, building on GPT-3 with optimizations for more efficient text generation.\n\u2022 GPT-4o-mini (OpenAI, 2024) is a smaller, cost-effective\u00b2 variant of GPT-4 (OpenAI & Josh Achiam, 2024), offering strong performance across various tasks."}, {"title": "C BENCHMARK DETAILS", "content": "(1) HumanEval (Chen et al., 2021) consists of 164 hand-written programming problems, each including a method signature, docstring, body, and unit tests. We use both HumanEval and its extended version, HumanEval+(Liu et al., 2024), which enhances the original with 80\u00d7 additional test samples to address test case insufficiency (Liu et al., 2024).\n(2) MBPP (Austin et al., 2021) contains crowd-sourced Python programming problems. Our study uses the versions proposed by (Liu et al., 2024), including MBPP and MBPP+. Each of them contain 399 tasks, and the latter adds 35\u00d7 test samples.\n(3) APPS (Hendrycks et al., 2021) includes 10,000 coding problems from open-access websites, split equally into training and test sets. It includes two problem formats: call-based format (input via function parameters) and standard input format (using stdin/stdout). Problems are categorized into introductory, interview, and competition levels. There are three different difficulty levels of problems in APPS, i.e., introductory, interview and competition. Each of them has 1000, 3000, and 1000 tasks, respectively. Considering the cost of evaluating the entire APPS test set and following prior work (Olausson et al., 2023; Huang et al., 2024b; Le et al., 2024; Yang et al., 2023), we randomly select problems in accordance with the frequency distribution of these difficulty levels and sample 60, 180, 60 problems at the introductory, interview, and competition levels, respectively."}, {"title": "D IMPLEMENTATION DETAILS", "content": "Demonstration selection strategy. Specifically, for HumanEval, we select the first two problems as demonstrations. For MBPP, we choose the first problem. For APPS, considering the model's input length limitation and to avoid randomness, we select the two shortest problems from the first five problems in the training set. The reason for this differentiated strategy is that HumanEval and APPS problems are more complex, requiring more examples, while MBPP problems are relatively simple in form, and one example is enough.\nKeywords and explanations involved in demonstrations. The prompt for KeyExtract & Explain uses several demonstrations to guide LLMs to produce keywords and their explanations. To ensure the quality of each demonstration, we first employ Claude-3.5-Sonnet, an LLM separate from our target LLMs, to generate multiple sets of keywords and explanations for each demonstration. The generated contents are then manually reviewed, and the most accurate set for each demonstration is selected and used in the prompt. This can mitigate the potential bias in human-generated explanations. Additionally, for HumanEval(+) and MBPP(+) datasets, which provide function names, the first two authors discuss and write the explanation for the function name in each demonstration."}, {"title": "E ADDITIONAL EXPERIMENTS", "content": "In KeyRank, we combine different types of keywords based on the order of abstract \u2192 general \u2192 function. We investigate the influence of keyword combination orders by comparing the order used by SEK with three alternative ordering strategies using two LLMs, i.e., Llama-3.1-70B-Instruct"}, {"title": "E.1 INFLUENCE OF KEYWORD COMBINATION ORDERS", "content": "and Mixtral-8\u00d722B-Instruct-v0.1. Table 4 presents the experimental results, where the abbreviations Abs, Gen, and Func denote abstract keywords, general keywords, and function keywords, respectively. The results reveal performance variations across different keyword combination orders, indicating that the order of different keyword types impacts LLMs' comprehension of coding problems. The combination order used by SEK consistently yields optimal performance, suggesting its rationality."}, {"title": "E.2 INFLUENCE OF GUIDELINES", "content": "In Section 4.2, we investigate the effectiveness of the guidelines in the KeyExtract & Explain prompt as a whole. This section further investigates the impact of each guideline by removing it from the prompt and re-evaluate the performance of SEK with two LLMs, i.e., Llama-3.1-70B-Instruct and Mixtral-8\u00d722B-Instruct-v0.1 on HumanEval(+). Table 5 presents the experimental results, where the performance of the two LLMs decreases in almost all cases, indicating the contribution of each guideline to the effectiveness of SEK."}, {"title": "E.3 MORE EXPERIMENTS ON APPS", "content": "In the main experiment, we randomly sample problems from the APPS test set for evaluation due to limited resources. The performance of LLMs on APPS may be affected by the randomness of the selected samples. To mitigate this variability, we conduct additional experiments by randomly selecting three new subsets of problems at the introductory level from the APPS test set and using two LLMs for evaluation, i.e., Llama-3.1-70B-instruct and GPT-3.5-Turbo. The number of sampled tasks is fixed at 60, consistent with the main experiment. For reproducibility, the selected tasks are provided in Table 7. As shown in Table 6"}]}