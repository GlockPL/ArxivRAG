{"title": "Regress, Don't Guess \u2013 A Regression-like Loss on Number Tokens for Language Models", "authors": ["Jonas Zausinger", "Lars Pennig", "Kacper Chlodny", "Vincent Limbach", "Anna Ketteler", "Thorben Prein", "Vishwa Mohan Singh", "Michael Morris Danziger", "Jannis Born"], "abstract": "While language models have exceptional capabilities at text generation, they lack a natural inductive bias for emitting numbers and thus struggle in tasks involving reasoning over quantities, especially arithmetics. This has particular relevance in scientific datasets where combinations of text and numerical data are abundant. One fundamental limitation is the nature of the CE loss, which assumes a nominal (categorical) scale and thus cannot convey proximity between generated number tokens. As a remedy, we here present two versions of a number token loss. The first is based on an Lp loss between the ground truth token value and the weighted sum of the predicted class probabilities. The second loss minimizes the Wasserstein-1 distance between the distribution of the predicted output probabilities and the ground truth distribution. These regression-like losses can easily be added to any language model and extend the CE objective during training. We compare the proposed schemes on a mathematics dataset against existing tokenization, encoding, and decoding schemes for improving number representation in language models. Our results reveal a significant improvement in numerical accuracy when equipping a standard T5 model with the proposed loss schemes.", "sections": [{"title": "Introduction", "content": "As coined by Thawani et al. [14], numbers in natural texts are ubiquitous and important, yet system-atically neglected by language models (LMs). Even worse, while Transformers [15] were invented for NLP, they have permeated various scientific domains (chemistry, biology, etc [2, 8, 1]), where tabular/numerical data is more prevalent than in NLP and often even fundamental for constructing task definitions: Molecules are labeled with drug efficacy, chemical reactions with yield, and synthesis procedures are natural text interspersed with quantities and times. Still, LMs notoriously struggle even with simple arithmetic tasks like three-digit multiplication [5] for multiple reasons:\n1. Tokenization: Standard subword tokenization splits numbers into arbitrary tokens, disrupting their structure. Mitigation strategies include scientific notation [18] or digit-level tokenization [6], which may also preserve the decimal order of each digit [1]."}, {"title": "Methods", "content": ""}, {"title": "Number Token Loss", "content": "The idea of the Number Token Loss (NTL) is to add an additional loss term to the CE, which is only applied to number tokens and takes their numerical proximity into account. To achieve this, we propose two versions."}, {"title": "Number Token Loss with Mean Squared Error (NTL-MSE)", "content": "This loss compares the numerical value of the ground truth token with the weighted sum of the respective numerical token values, with the weights corresponding to the predicted class probabilities (cf. Figure 1 right). Given a model f(\u00b7), input tokens x<i (where i < N), the numerical value \u0177\u2081 of ground truth token y\u2081 and a vocab V consisting of tokens (with indices j, ..., k representing the number tokens), we compute NTL-MSE:\n$L_{NTL-MSE} = \\frac{1}{N} \\sum (y_i - f(x_{\\leq i})_{j:k}V_{jk})^2$ (1)\nInstead of a nominal-scale loss with regular CE, the NTL-MSE effectively conveys proximity between numbers. For example, if the label is [4] and the LM predicts a [5] instead of [9], the loss will be lower, matching our intuitive expectation, unlike the CE which gives constant loss no matter the proximity of the number (cf. Figure 2). This is sufficient for the vast majority of cases, however, since the NTL is not injective (like CE), it can return spuriously low loss for incorrect predictions. Consider e.g., a label [4] with 50% of the mass on [0] and 50% on [8] token, then NTL will be zero (Figure 3). While such cases are rare due to the softmax emphasizing logit differences, combining NTL with CE loss helps correct spurious cases, as CE continues refining predictions without reducing its value in these instances. However, to address this non-injectiveness, we propose a second version based on the Wasserstein-1 distance."}, {"title": "Number Token Loss with Wasserstein-1 distance (NTL-WAS)", "content": "This loss calculates the Wasserstein-1 distance between the predicted probability distribution of the (sorted) number to-kens and the ground truth probability distribution, which is 1 for the label token and 0 for all other tokens. Given the ground truth yi, a vocab V with number tokens ordered from indices j to k and the cumulative distribution function CDF(\u00b7), we compute NTL-WAS:\n$L_{NTL-WAS} = \\frac{1}{N} \\sum |CDF (y_i) \u2013 CDF (f(x_{\\leq i})_{j:k})|$ (2)\nAs one can see in Figure 2, this version of the NTL not only conveys proximity between numbers correctly but also eliminates the non-injectiveness problem, shown in Figure 3. Both versions of the NTL are scaled with \u5165 (0.3 unless mentioned otherwise) and added to the regular CE loss:\n$L = L_{CE} + \\lambda L_{NTL}$ (3)\nNote that both versions of the NTL shall be 0 for all non-numerical tokens. By changing the p-order in NTL-MSE, different Lp-norm losses can be obtained (e.g., NTL-MAE). Huber loss is also compatible. In Appendix A.2, we provide pseudo-code for both versions of the NTL."}, {"title": "Backbone T5 and model variants", "content": "We use a T5 backbone [11] (Appendix A.3) for our experiments and extend it with both versions of the NTL and the Regression-Transformer tokenization scheme[1], due to its flexible encoder-decoder architecture and its success in various natural language processing tasks."}, {"title": "Regression Transformer (RT)", "content": "The Regression Transformer [1] tokenizes numbers on digit level, considering both the position and value of each digit. Since standard learnable embeddings may not adequately preserve the inherent structure of numbers, it leverages an inductive bias to account for the relative proximity of the numbers through numerical encodings, further explained in Appendix A.5."}, {"title": "xVal encoding and decoding scheme", "content": "The xVal method [7] encodes real numbers using a single [NUM] token multiplied by its numerical value. For decoding (see Figure 1), a number head predicts the value while the token head outputs the sequence, replacing [NUM] during inference. However, this scheme is incompatible with T5 (see Appendix A.6). We thus use the xVal encoder and masked language modeling in our experiments."}, {"title": "Integration of the Number Token Loss", "content": "Both versions of our proposed NTL, depicted in the right panel of Figure 1, can be integrated into any model that treats numbers as clearly separated tokens of single digits by applying it as an additional loss term. Therefore, we adapt the tokenization scheme of the standard T5 model to tokenize all numbers on the digit level to make it compatible with the NTL. As RT already tokenizes numbers on digit level by default, we can integrate the NTL without any changes. Integrating NTL into xVal is not feasible, as xVal encodes every number with the same token. Moreover, xVal already uses both MSE and CE loss."}, {"title": "Experiments and results", "content": "To test the mathematical capabilities of the methods, we use a dataset with more than 25 million samples from the mathematical Q&A dataset from DeepMind [12]. The dataset comes with two sets of tests: interpolation tests, one for each type of question occurring in the training set, and extrapolation tests, which measure generalization along various axes of difficulty beyond that seen during training. We provide more information about the dataset in Appendix A.4. We evaluate all five models on the two test sets of this dataset and report the accuracy (how often the model predicts the number exactly), as well as the Mean Absolute Error (MAE) and the R2-score. Since the dataset is skewed with some very high values, we perform a log10 transformation on the predicted and ground truth numbers before calculating MAE and R2-score.\nAll experiments except the one with xVal are built upon the T5 implementation and language modeling trainer based on the Hugging Face transformers library [16]. We use the T5-base model as a pretrained base for our respective models. All models were trained for approximately one million steps with a batch size of 32 over a period of approximately 3 days. More details on the models' training hyperparameters can be found in Appendix A.7."}, {"title": "Conclusion", "content": "We introduced the Number Token Loss (NTL) for LMs to enhance their ability to handle numerical data by considering the numerical proximity between tokens. Our experiments unambiguously demonstrate the effectiveness of the NTL-WAS loss. This confirms our hypothesis that number representation in LMs can be effectively improved through a minor, architecture-agnostic modification of the loss function. By augmenting the standard CE loss with NTL, we provide a simple yet effective method that integrates seamlessly into existing architectures without requiring additional computational overhead. Experiments on the DeepMind Mathematics Dataset demonstrated that NTL significantly improves numerical reasoning, especially in models without specialized numerical embeddings. This approach offers a practical solution for enhancing language models in numerically rich domains, paving the way for more accurate and reliable applications in mathematics and science."}, {"title": "Appendix", "content": ""}, {"title": "Statement on code", "content": "The code for this paper is available at https://github.com/tum-ai/number-token-loss."}, {"title": "Algorithm for the Number Token Loss", "content": ""}, {"title": "T5 architecture", "content": "The T5 model is built upon the Transformer architecture [15], consisting of stacked self-attention and feed-forward layers in both the encoder and decoder. The encoder processes the input tokens to create contextualized representations, while the decoder generates the output tokens autoregressively, attending to both the encoder's outputs and the previously generated tokens. The model can be trained with both Masked Language Modelling (MLM) [9] and Causal/Auto-Regressive Language Modelling (CLM) [4], whereby we chose to use CLM."}, {"title": "Dataset", "content": "To test the mathematical capabilities of the methods, we use a subset of the mathematical question-answer dataset from DeepMind [12]. The dataset was generated synthetically and therefore contains limited linguistic variability, but is sufficient for our purposes to compare the mathematical capabilities of the different methods.\nThe dataset contains different modules and difficulty levels. For training and testing the models, we chose all difficulty levels but excluded modules where the answer contains complex fractions or variables. This allows us to focus on purely numeric answers to simplify the evaluation of the model and still leaves us with a large enough dataset of ~26 million samples.\nFor training, validation, and interpolation tests, we selected the following modules from the DeepMind mathematical question-answer dataset:\n\u2022 algebra__linear_1d.txt\n\u2022 algebra__linear_1d_composed.txt\n\u2022 algebra__linear_2d.txt\n\u2022 algebra__linear_2d_composed.txt\n\u2022 algebra__sequence_next_term.txt\n\u2022 arithmetic__add_or_sub.txt\n\u2022 arithmetic__add_sub_multiple.txt\n\u2022 arithmetic__mul.txt\n\u2022 numbers__div_remainder.txt\n\u2022 numbers__div_remainder_composed.txt\n\u2022 numbers__place_value.txt\n\u2022 numbers__round_number.txt\n\u2022 numbers__round_number_composed.txt\nFor extrapolation tests, we selected the following modules:\n\u2022 arithmetic__add_or_sub_big.txt\n\u2022 arithmetic__add_sub_multiple_longer.txt\n\u2022 arithmetic__mixed_longer.txt\n\u2022 arithmetic__mul_big.txt\n\u2022 arithmetic__mul_div_multiple_longer.txt\n\u2022 numbers__place_value_big.txt\n\u2022 numbers__round_number_big.txt\nThis resulted in a training dataset of 25,986,948 samples, a validation dataset of 13,026 samples, an interpolation test set of 130,000 samples, and an extrapolation test set of 70,000 samples."}, {"title": "Regression Transformer", "content": "The Regression Transformer [1] preserves the inherent structure of numbers by inducing information on relative proximity through numerical encodings that are set deterministically for all tokens. For every combination of a decimal place and digit value, a corresponding numerical token is added to the vocabulary. For instance, the number 11.4 is tokenized to [1_1, 1_0, 4_-1].\nNon-numeric tokens are set to zero vectors. The numerical encodings are designed so that their pairwise distances are symmetric and monotonically decreasing with the float value. The final encoding of the input tokens is obtained by summing over numerical and regular word encodings. The Regression Transformer numerical encodings NE at dimension j for numerical token tv,p with value v and decimal place p can be determined by\n$NEFloat (v, p, j) = (\u22121)^{v}  \\frac{v*30}{(j + 1)^{p}} $ (4)"}, {"title": "Challenges with Integrating xVal in Transformer Models like T5", "content": "In transformer models like T5, integrating numerical encoding schemes like xVal presents challenges. Relative positional encodings and pre-layer normalization disrupt the numerical scaling. This makes it difficult to preserve distinctions between values.\nIn T5, instead of using absolute positions for each token, relative positions between tokens are encoded. This helps the model understand relationships between tokens based on their distance, regardless of where they appear in the sequence. However, this relative encoding is applied uniformly across all tokens, including numerical tokens. Since relative position encoding doesn't account for the magnitude of numerical values, it essentially ignores the scaling factor introduced by the xVal method.\nPre-layer normalization is applied to the inputs before they enter each transformer layer. Normalization typically scales the inputs to a standard range, effectively reducing the impact of the differences in numerical embeddings introduced by the xVal method. As a result, even though the xVal method multiplies the [NUM] token embedding by its corresponding numerical value, this scaling gets neutralized by the normalization step, making the embeddings of different numbers more similar than they should be."}, {"title": "Training hyperparameters", "content": "We train each model for 1050000 iterations with a batch size of 32 using transformers [16] 4.42.4. We train with a learning rate of 1e-4 and weight decay of 0.01. All models were trained on single graphics processing units (GPUs) (NVIDIA RTX A6000). For the Number Token Loss, we trained with the hyperparameter A set to 0.3."}]}