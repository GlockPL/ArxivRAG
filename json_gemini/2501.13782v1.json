{"title": "Defending against Adversarial Malware Attacks on ML-based Android Malware Detection Systems", "authors": ["Ping He", "Lorenzo Cavallaro", "Shouling Ji"], "abstract": "Android malware presents a persistent threat to users' privacy and data integrity. To combat this, researchers have proposed machine learning-based (ML-based) Android malware detection (AMD) systems. However, adversarial Android malware attacks compromise the detection integrity of the ML-based AMD systems, raising significant concerns. Existing defenses against adversarial Android malware provide protections against feature space attacks which generate adversarial feature vectors only, leaving protection against realistic threats from problem space attacks which generate real adversarial malware an open problem. In this paper, we address this gap by proposing ADD, a practical adversarial Android malware defense framework designed as a plug-in to enhance the adversarial robustness of the ML-based AMD systems against problem space attacks. Our extensive evaluation across various ML-based AMD systems demonstrates that ADD is effective against state-of-the-art problem space adversarial Android malware attacks. Additionally, ADD shows the defense effectiveness in enhancing the adversarial robustness of real-world antivirus solutions.", "sections": [{"title": "1 Introduction", "content": "As the Android operating system powers a vast array of mobile devices worldwide, it has become a significant target for malware. Recent security reports have identified over 35 million Android malware samples since 2008, posing substantial threats to users' privacy and data integrity [10]. To counteract these threats, machine learning-based (ML-based) Android malware detection (AMD) systems have been developed and widely deployed in real-world mobile devices, offering powerful defense effectiveness against Android malware. Unfortunately, these ML-based AMD systems are vulnerable to problem space adversarial malware attacks [26,52,71], which manipulate the malicious application files, causing them to be misclassified as benign by the ML-based AMD systems.\nIn response to the growing threat of adversarial malware attacks, researchers have proposed various defense methods to enhance the adversarial robustness of ML-based AMD systems [25, 33, 34, 36, 55]. However, these methods primarily focus on defending against feature space attacks, leaving the more realistic and complex problem space attacks largely un-addressed. As defined by Pierazzi et al. [52], the key difference between feature space attacks and problem space attacks lies in the nature of their constraints. The adversarial example in feature space attacks is constrained to remain close to the original sample in terms of feature similarity. In contrast, problem space attacks occur in the input domain (e.g., the actual malware files) and must adhere to domain-specific constraints, such as program grammar.\nThe differing constraints between problem space attacks and feature space attacks make defending against problem space attacks significantly more challenging. The constraints of adversarial Android malware attacks in the problem space are in the Android application such as the program grammar [26], whereas in the feature space attacks, the constraints are typically the lp distance [17]. This discrepancy makes it challenging to mathematically identify all feasible adversarial regions due to the complexity of maintaining functional consistency, unlike in feature space attacks, where they remain nearby. Consequently, the generation of adversarial Android malware for problem space defense is typically incomplete. Additionally, adversarial malware generated in the problem space can exhibit much greater diversity and significant differences in feature vectors compared to the original samples. In contrast, feature space attacks produce adversarial examples that differ only minimally from the original samples. This diversity in problem space attacks further complicates the development of robust defenses, as the range of possible adversarial variations is far broader and less predictable.\nIn terms of the cost, generating the problem space adversarial Android malware is time-consuming [26, 42]. This process typically involves complex software manipulations, such as decompiling and recompiling APK files, which require significantly more time than feature space attacks, which only perturb numerical feature vectors. Furthermore, existing state-of-the-art (SOTA) works [33, 36] adopt more robust model architectures [36] or train more robust model parameters [33] to defend against the adversarial examples by improving the inherent robustness of the ML model. However, this approach may negatively impact the original malware detection performance of ML-based AMD systems. This is because strengthening robustness against adversarial perturbations may shift decision boundaries, leading to a less accurate fit for clean data.\nTo address the aforementioned limitations, we propose a practical adversarial Android malware defense to enhance the adversarial robustness of ML-based AMD systems, termed ADD. At a high level, ADD is designed as a plug-in that revisits samples classified as benign by the original ML-based AMD systems. This design minimizes the negative impact of ADD on the original ML-based AMD systems for two main reasons. First, the plug-in design employs another detection model to separate the detection ability of adversarial Android malware samples from regular malicious samples. Consequently, the original malware detection capability of the primary model remains unaffected by the adversarial Android malware samples. Second, adversarial Android malware attackers are primarily motivated to manipulate malicious samples to be misclassified as benign. Therefore, ADD only revisits samples classified as benign by the underlying model, further reducing the potential influence on the original detection performance of these methods.\nTo defend against practical adversarial Android malware attacks in the problem space, ADD shifts attention from the ML-based AMD systems to the adversarial Android malware attack methods by leveraging the constraints and trade-offs inherent in adversarial Android malware attack methods. The design of ADD is based on the following two key observations. First, adversarial perturbations in the problem space can only affect a limited set of features within the feature space due to problem space constraints [52]. We refer to the features that adversarial perturbations can perturb as the perturbable features and those that cannot be perturbed as the imperturbable features. Second, adversarial Android malware attack methods target the features that significantly reduce the confidence of the malware label to maximize attack effectiveness and efficiency. However, the targeted features are likely to introduce semantic incompatibility with the remaining features, which causes an incompatibility between the perturbable features and the imperturbable features in the adversarial malware sample.\nBased on these key observations, ADD first quantifies the perturbable feature space, which consists of perturbable features, and the imperturbable feature space, which consists of imperturbable features within the feature space of the ML-based AMD system. The space quantification algorithm is inspired by the Monte Carlo sampling methods [22, 54] by observing the changed features in the space quantification applications. However, the perturbable and imperturbable feature spaces may not align in dimensions, making it challenging to directly measure incompatibility. To address this, ADD employs encoder models to project the perturbable and imperturbable feature spaces into a common embedding space, allowing the incompatibility score to be computed by measuring the distance between the embedding vectors. To train the encoder models, ADD designs a customized contrastive learning loss utilizing the training dataset of the ML-based AMD system and pseudo adversarial malware samples introduced by us. Finally, any input sample with an incompatibility score exceeding a certain threshold is determined to be malicious.\nUpon building the encoder models, the emphasis shifts to determining the threshold for identifying the adversarial Android malware. To avoid the data snooping pitfall [7], ADD utilizes a calibration dataset comprising benign and malicious samples. These samples help measure ADD's potential influence on the original detection performance of the ML-based AMD system. As ADD only revisits the benign output of the target model, the potential influence only stems from the true negative samples and false negative samples. True negative samples may be falsely identified as malicious and false negative samples may be correctly re-identified as malicious by ADD. However, to the best of our knowledge, there are no existing metrics providing the fine-grained measures in two scenarios. Therefore, ADD designs two new metrics termed: True Negative Influence Rate (TNIR) and False Negative Influence Rate (FNIR), respectively. TNIR measures the proportion of true negative samples incorrectly classified as malicious, while FNIR measures the proportion of false negative samples correctly identified as malicious. Thus, the threshold calibration algorithm is designed to control TNIR at a specified level while maximizing FNIR. This approach ensures that the negative impact on the original detection performance is controlled while the positive impact is maximized.\nWe evaluate ADD using a comprehensive Android malware dataset containing 135,859 benign applications and 15,778 malicious applications (151,637 total) dated from 2016 to 2018 and a recent Android malware dataset containing 20,206 malicious applications dated from 2022. This evaluation demonstrates the defense performance of ADD against the SOTA problem space adversarial Android malware attack methods [14, 26, 59] across various ML-based AMD systems. Additionally, ADD significantly improves the false negative performance of ML-based AMD systems while maintaining an acceptable negative influence rate on the true negative samples. It outperforms the baseline methods (i.e., FD-VAE [36] and adversarial training [33]) in terms of the defense effectiveness against the SOTA adversarial Android malware attacks across various ML-based AMD systems. In particular, ADD achieves a normalized decreased attack success rate of over 95% against these SOTA adversarial Android malware attack methods in most settings. Additionally, ADD can improve the false negative rate by approximately 50% while only reducing the true negative rate of the original ML-based AMD system by 1%. Furthermore, ADD also demonstrates defense effectiveness under two adaptive attack scenarios. Moreover, ADD can be utilized to enhance the adversarial robustness of real-world antivirus solutions (AVs) in VirusTotal. Specifically, ADD achieves over 90% detection rate to detect the adversarial Android malware samples that reduce the number of detected engines in VirusTotal. ADD still maintains a detection rate of 70% to 80% for the adversarial Android malware generated from the more recent malware samples.\nThe key contributions of this paper are summarized as follows. We propose ADD, which is a practical adversarial Android malware defense framework for enhancing the adversarial robustness of ML-based AMD systems. In ADD, a novel contrastive learning loss is proposed to capture the hierarchical inequality relation of incompatibility scores between pseudo adversarial malware samples, regular malicious samples, and benign samples. Then, a threshold calibration algorithm that utilizes two new metrics is proposed to optimally select the threshold for incompatibility scores. To evaluate ADD, two new metrics termed TNIR and FNIR are proposed to measure its impact on the original detection performance."}, {"title": "2 Background", "content": "The machine learning method has become a pivotal component in detecting Android malware. The introduction of learning-based classifiers has significantly improved the effectiveness and generalization capabilities of Android malware detection compared to traditional signature-based methods. This advancement has led to the widespread deployment of ML-based AMD systems in real-world applications.\nTo detect the Android malware, ML-based AMD systems typically begin with the utilization of Android application analysis tools, such as Soot [63] and Apktool [5], to analyze the manifest file and the program code within the Android application. Based on the analysis results, these methods extract features that are then used by machine learning classifiers to classify. For instance, Drebin [8] extracts syntax features (e.g., permissions) from the manifest file and semantic features (e.g., function calls) from the program code and organizes categorical feature representations. MaMadroid [46] only extracts the function call graph features and organizes them as numerical feature representations. This organized feature vector is then used in the ML model for the classification. Different ML-based AMD methods may employ different ML classifiers, e.g., Drebin [8] employs the SVM and MaMadroid [46] utilizes the Random Forest (RF) classifier.\nWe utilize four SOTA and representative ML-based AMD methods in our evaluation. They are Drebin [8], Drebin-DL [25], MaMadroid [46], and APIGraph [70], according to the previous works [26, 40]. The rationale for choosing these specific models lies in their established performance performance and popularity in research. For instance, APIGraph achieves an AUROC score of approximately 0.95 in our evaluation (Table 7). Furthermore, these methods have been primary targets in many papers on adversarial malware attacks [26, 34, 35], making them relevant for our analysis. Moreover, the diversity of their feature spaces and classifiers also ensures a more comprehensive evaluation of our defense mechanisms."}, {"title": "2.1 ML-based AMD", "content": null}, {"title": "2.2 Adversarial Android malware attack", "content": "Adversarial Android malware attacks [19,25,26,35,52,71] aim at deliberately manipulating the Android malware samples detected by the ML-based AMD systems to be misclassified as benign samples while preserving the malicious functionality. Different from adversarial example attacks in image domain [17, 18, 62], the adversarial perturbations in Android malware domain must adhere the problem space constraints [52]. This necessitates that the adversarial perturbation be implemented in the APK file in accordance with program grammar and maintain functional consistency [26].\nTypically, adversarial Android malware attacks consist of two stages: adversarial perturbation preparation and adversarial perturbation injection, as illustrated in Figure 1. The adversarial Android malware attack methods first propose the APK manipulation methods that keep the problem space constraints as the potential adversarial perturbation. These manipulations can take the form of functionality-preserving code transformations [35,71] and the insertion of code segments from Android applications [26, 52]. In the second stage, these manipulation techniques are used during adversarial perturbation injection, where an algorithm is designed to select and inject adversarial perturbations optimally. The early adversarial Android malware attack methods [19, 25, 35, 52, 61,71] assume the attacker has the perfect knowledge or limited knowledge of the ML-based AMD systems, allowing them to leverage model parameters to guide the selection of adversarial perturbations. For instance, Pierazzi et al. [52] leverage the weights of the SVM classifier to identify the most effective perturbations for injection. More recent SOTA adversarial malware attack methods consider more practical scenarios where the adversary has zero knowledge of the internal workings of the ML-based AMD systems [14, 26]. These methods rely on additional insights from program semantics and query feedback to select optimal adversarial perturbations. For example, AdvDroidZero [26] employs the perturbation selection tree to choose the most effective adversarial perturbations, demonstrating effectiveness against real-world antivirus engines."}, {"title": "3 Methodology", "content": "We present the threat model of ADD by delineating three key components: defender goals, knowledge and capabilities, building upon previous works [6, 13, 16, 26].\nDefender goals. The defender aims to detect the adversarial Android malware samples that are misclassified by the ML-based AMD systems. Given that the adversarial Android malware attackers focus solely on the misclassification of malicious samples as benign samples [16, 26], the defender only needs to detect the adversarial Android malware samples among those classified as benign. Additionally, the defender seeks to preserve or enhance the original detection performance (e.g., precision, recall) of ML-based AMD systems. This goal aligns with the need for utility preservation in ML-based AMD systems, emphasizing their detection effectiveness and user experience.\nDefender knowledge. We assume that defenders could be the developers of the ML-based AMD systems who are motivated to protect them against the adversarial Android malware attack. Therefore, the defender has the knowledge of the internals of ML-based AMD systems, including feature spaces, model parameters and the training dataset. The defender also has the knowledge of the feasible perturbations for the Android malware sample. This is because the feasible perturbations are determined by the program grammars, which can be found in Android documentation [3] that is available for the defender. However, in practice, the defender remains uninformed about the specific adversarial Android malware attack methods employed by the adversary. Additionally, the defender cannot obtain any adversarial Android malware samples generated by the adversary. This is because the attackers typically do not disclose their adversarial Android malware attack method for selecting the adversarial perturbations.\nDefender capabilities. The defender could access the malicious and benign samples identified by the ML-based AMD systems in the APK file format and their corresponding detection results. This access is feasible because the defender is likely the developer of the ML-based AMD systems. The ML-based AMD systems process input samples in APK file format and generate corresponding detection results."}, {"title": "3.1 Threat model", "content": null}, {"title": "3.2 Key observations", "content": "The design of our method is inspired by two key observations derived from adversarial Android malware attack methods.\nObservation I. Adversarial perturbations in the problem space utilized in adversarial Android malware attack methods must comply with the Android program grammar and maintain functional consistency. Consequently, the adversarial perturbations could only perturb the feature vector of the malware sample within a restricted space [52]. For simplicity, we refer to the portion of feature space of ML-based AMD systems that can be perturbed by the plausible adversarial perturbations in the problem space as the perturbable feature space. Correspondingly, the features within this space are termed perturbable features. In contrast, we refer to the portion of feature space of ML-based AMD systems that cannot be perturbed by the plausible adversarial perturbations in the problem space as the imperturbable feature space. The features within this space are termed imperturbable features.\nObservation II. The logic underlying adversarial Android malware attacks is designed to perturb the most vulnerable feature within the feature space of ML-based AMD systems, which most significantly influences the confidence of the malware label. This is because perturbing the most vulnerable feature can help adversarial Android malware attack to maximize the attack effectiveness and attack efficiency. However, this approach overlooks the compatibility between the perturbed features and the original features within the malware sample. This means that the most vulnerable feature is unlikely to align with the most compatible feature. Consequently, perturbable features are more likely incompatible with imperturbable features within the malware sample.\nFor example, consider spyware designed for eavesdropping on meetings, disguised as a music application like YouTube Music. This malware may possess original audio-related features, such as microphone permissions, audio processing functions, and the application name. However, adversarial Android malware attack methods may inject non-audio-related features, such as GPS or camera permissions, to significantly reduce the confidence of the malware label and evade detection. These permission features, being perturbable, exhibit semantic incompatibility with the imperturbable features, such as audio processing functions and the application name."}, {"title": "3.3 Design overview", "content": "At a high level, ADD is an adversarial Android malware defense framework designed as a plug-in for ML-based AMD systems. It revisits the benign output of the ML-based AMD systems to enhance their robustness against adversarial Android malware attacks. Building on the key observations outlined in Section 3.2, ADD aims to detect adversarial Android malware samples by evaluating the incompatibility score between their perturbable features and imperturbable features. It operates through three stages: (1) the space quantification stage, (2) the feature projection stage, and (3) the threshold calibration stage, as shown in Figure 2. In the following, we briefly illustrate every stage, and the algorithmic descriptions for ADD are provided in Appendix A."}, {"title": "3.4 Space quantification stage", "content": "As demonstrated in Section 3.2, the adversarial perturbations could only affect the perturbable features in the feature space of the ML-based AMD system due to the problem space constraints. The goal of the space quantification stage is to distinguish between perturbable and imperturbable features. Although these features can theoretically be identified manually, as they are determined by the ML-based AMD system and Android grammar, the sheer size of the feature space (typically exceeding 10,000 features) makes manual identification impractical.\nTo address this challenge, ADD employs an experimental approach inspired by Monte Carlo sampling [22,54] to quantify the feature space. Specifically, ADD injects feasible problem space perturbations into Android applications and observes the resulting changes in the feature space. Features that change are classified as perturbable, while those that remain unaffected are classified as imperturbable.\nADD implements this process by applying each perturbation from a predefined malware perturbation set to two space quantification applications. Observed changes in the application's features are used to identify perturbable features, which are then aggregated to form the perturbable feature space. Unchanged features constitute the imperturbable feature space. The malware perturbation set is constructed strictly following the prior adversarial Android malware attack work [26], including manipulations of both the AndroidManifest.xml file and the DEX code.\nRegarding space quantification applications, ADD incorporates two specific applications: the No Activity application and the Empty Activity application, both generated using the default project templates in Android Studio [4] without any modifications. The rationale behind selecting these two applications is as follows. The problem space perturbations must preserve the functionality of the application. Thus, they will not influence the original feature of the space quantification application. This results in that the original features of the space quantification application will never be marked as the perturbable features. Therefore, an application with fewer features exerts minimal influence on space quantification. Thus, the No Activity application, devoid of any activity, is chosen for its minimal program semantics. However, since some problem space malware perturbations may require an activity in the original application to be present, they cannot be injected into the No Activity application, leading to incomplete quantification. To address this issue, ADD also includes the Empty Activity application, which contains only a basic main activity. The final perturbable feature space is obtained by combining the results from both the No Activity application and the Empty Activity application. The detailed algorithmic description of the space quantification algorithm can be found in Appendix B."}, {"title": "3.5 Feature projection stage", "content": "Upon acquiring the perturbable feature space and the imperturbable feature space, the emphasis shifts to determining their incompatibility score. However, these spaces are unlikely to align in dimensions, complicating the calculation of the incompatibility score. For instance, in our Drebin model [8], the perturbable feature space has 226 dimensions, whereas the imperturbable feature space has 9,774 dimensions. This stark disparity in feature dimensions renders direct computation of the incompatibility score unsuitable. To address these challenges, ADD employs encoder models designed to project the perturbable feature space and imperturbable feature space into a common lower dimensional space. The incompatibility score is then calculated based on the distance between the projected features.\nModel design. The encoder models are designed to project the perturbable and imperturbable features from high-dimensional spaces to a common low-dimensional space. This involves two separate encoder models for the perturbable feature space and the imperturbable feature space, respectively. The architecture of each encoder model is an MLP with the Rectified Linear Unit (ReLU) activation function [31] and dropout layer [60]. The input layer dimensions correspond to the respective dimensions of the perturbable and imperturbable feature spaces. The output layer dimension is set to a much smaller value, e.g., 32, than the input layer dimension [20, 32, 36, 68]. Considering the large gap between the input and output dimensions, ADD adopts an exponential dimension reduction policy to determine the dimensions of the hidden layers. This policy exponentially reduces the dimensions of the hidden layers, aiming to enhance model performance and optimize memory usage. More details of the exponential dimension reduction policy can be found in Appendix C.\nModel training. The objective of the encoder models is to learn the incompatibility score between the perturbable features and imperturbable features, reflecting it in the distance between the projected features. Samples with incompatibility scores exceeding the threshold are identified as malicious. The incompatibility score for a sample x is defined as follows:\n$IncalScore(x) = D(x) = l_2(EPS(PS(x)), EIPS(IPS(x))),$ (1)\nwhere PS and IPS represent the perturbable feature space and imperturbable feature space, respectively. EPS and EIPS denote the encoder models for projecting the perturbable feature space and imperturbable feature space, respectively. $l_2$ means the L2-norm loss according to the previous work [18].\nAs discussed in Section 3.3, ADD revisits only those samples classified as benign by the ML-based AMD system. There are three scenarios for a sample with a benign output:\nTrue benign samples: If the sample is indeed benign, ADD must retain this classification to avoid increasing false positives, which would degrade the original detection performance of the ML-based AMD system.\nMisclassified malicious samples: If the sample is actually malicious, ADD must identify it as malicious. This correction improves the original detection performance by reducing false negatives.\nAdversarial Android malware samples: If the sample is an adversarial Android malware, ADD must also identify it as malicious due to its inherent malicious functionality.\nTo achieve this goal, ADD employs a customized contrastive learning loss composed of three parts, each addressing one of the corresponding scenarios. The first part is designed to minimize any negative impact on the original detection performance of the ML-based AMD system. It ensures that true benign samples are correctly identified to control an increase in the false positive rate at an acceptable level. The second part aims to enhance the detection capabilities of the ML-based AMD system by ensuring that malicious samples, previously misclassified as benign, are correctly identified. This improvement reduces the false negative rate. The third part is focused on effectively detecting adversarial Android malware samples by recognizing their distinct incompatibility features. These three parts align with the goals of the defender, as discussed in Section 3.1.\nTo keep the detection results of the benign sample ($x_b$), the first part of the customized contrastive learning loss minimizes the incompatibility scores of these samples. This objective is formulated as follows:\n$D(x_b) = l_2(EPS(PS(x_b)), EIPS(IPS(x_b))),$ (2)\n$L_1 = \\frac{1}{|D_b|} \\sum_{x_b \\in D_b} D(x_b),$ (3)\nwhere PS and IPS, EPS and EIPS retain their meanings as defined in Equation 1. $D_b$ denotes the set of benign samples in the training set of the ML-based AMD system and $|D_b|$ represents the cardinality of the set. Thus, the first part of the customized contrastive learning loss focuses on minimizing the incompatibility scores of all benign samples in the training dataset of the ML-based AMD system.\nTo improve the original performance of the ML-based AMD system, the second part of the customized contrastive learning loss incorporates a malicious sample ($x_m$) from the training dataset of the ML-based AMD system. In this case, ADD aims to identify these samples them as malicious. Consequently, this part of the loss function ensures that the incompatibility score of a benign sample is smaller than that of a malicious sample. This objective can be formulated as:\n$D(x_m) = l_2(EPS(PS(x_m)), EIPS(IPS(x_m))),$ (4)\n$L_2 = \\frac{1}{|D_{bm}|} \\sum_{(x_b,x_m)\\in D_{bm}} max(D(x_b) - D(x_m)+m,0),$ (5)\nwhere PS and IPS, EPS and EIPS keep the same meaning with Equation 1. $D_{bm}$ is the set that contains the pairs of benign sample and malicious sample in the training set of ML-based AMD system, and $|D_{bm}|$ represents the cardinality of the set. m denotes a fixed margin (a hyperparameter).\nTo achieve effective detection of adversarial Android malware samples, one straightforward solution is to incorporate these samples into the detection model to learn their features. The techniques such as adversarial training [24, 44, 56, 69, 73] and certified training [30, 45, 47,57,58] are widely applied to enhance the adversarial robustness of ML models in image and text domains. However, in practice, defenders may lack knowledge about the specific adversarial Android malware attack methods employed by adversaries and cannot have access to adversarial Android malware samples generated by them (Section 3.1). Additionally, even if defenders know some known adversarial Android malware attack methods, generating adversarial malware in the problem space is computationally expensive [42]. For instance, AdvDroidZero [26] requires approximately 10 minutes to guarantee the success of generating an adversarial Android malware sample.\nTo address these challenges, ADD introduces the concept of pseudo adversarial malware to facilitate the training process of the encoder models. A pseudo adversarial malware sample is a perturbed feature vector of a malware sample that the ML model of the ML-based AMD system misclassifies as benign. These perturbations occur only in the perturbable feature space due to problem space constraints. To generate pseudo adversarial malware samples, ADD employs a customized random algorithm that randomly perturbs the features of a malicious sample in the perturbable feature space until it is classified as benign. This random perturbation strategy does not require knowledge of the specific adversarial Android malware attack methods. Additionally, the pseudo adversarial malware generation algorithm has low time complexity, addressing the computational cost challenge because enumerating all combinations of perturbable features to find all pseudo adversarial malware samples would have exponential complexity. Although the pseudo adversarial malware generation method may not cover all possible pseudo adversarial malware samples, it can still achieve good effectiveness. The use of randomness is a well-established approach in other applications and has shown to produce good results, such as random walks in graphs [27, 38, 48], Monte Carlo algorithms [22,28,54]. The detailed algorithmic description about the pseudo adversarial malware generation algorithm can be found in Appendix D.\nUpon acquiring the pseudo adversarial malware samples, ADD incorporates them into the training process of the encoder models to approximate adversarial malware samples. The goal is to ensure that the incompatibility score of a pseudo adversarial malware sample ($x_{pam}$) is higher than that of a regular malicious sample, as adversarial malware samples exhibit the most severe incompatibility in features. Consequently, the third part of the customized contrastive learning loss is formulated as follows:\n$D(x_{pam}) = l_2(EPS(PS(x_{pam})), EIPS(IPS(x_{pam}))),$ (6)\n$L_3 = \\frac{1}{|D_{pm}|} \\sum_{(x_{pam},x_m)\\in D_{pm}} max(D(x_{pam}) - D(x_m)+m,0),$ (7)\nwhere PS and IPS, EPS and EIPS keep the same meaning with Equation 1. m keeps the same meaning with Equation 5. $D_{pm}$ is the set that contains the pairs of pseudo adversarial malware samples and malicious samples, and $|D_{pm}|$ represents the cardinality of the set.\nIn summary, the customized contrastive learning loss is the sum of the three parts, and we train the encoder models end-to-end with this loss. Specifically,\n$L = \\lambda_1 L_1 + \\lambda_2 L_2 + \\lambda_3 L_3$ (8)\nwhere $\\lambda_1$, $\\lambda_2$, $\\lambda_3$ are hyperparameters, a common heuristic in machine learning [20, 67]."}, {"title": "3.6 Threshold calibration stage", "content": "After building the encoder models for feature projection, ADD needs to determine the threshold for the incompatibility score. If the incompatibility score of an input sample exceeds this threshold, ADD identifies it as malicious. To determine the threshold, ADD utilizes a calibration dataset to determine the threshold of the incompatibility score. As discussed in Section 3.1, the defender has no knowledge about the specific adversarial Android malware attack methods employed by adversaries and cannot have access to adversarial Android malware samples generated by them. Therefore, the calibration dataset consists of benign samples and regular malicious samples. To avoid the data snooping pitfall highlighted by Arp et al. [7], the calibration dataset needs to be separate from the test set used in the evaluation.\nBenign samples correctly classified as benign by the ML-based AMD system may be incorrectly identified as malicious by ADD, affecting the true negative rate of the original ML-based AMD system. Conversely, malicious samples misclassified as benign by the ML-based AMD system may be correctly identified as malicious by ADD, affecting the false negative rate. In response to these cases, ADD introduces two new calibration metrics: True Negative Influence Rate (TNIR) and False Negative Influence Rate (FNIR). TNIR represents the proportion of true negative samples incorrectly identified as malicious by ADD (denoted by $N_{itn}$) to the total number of the true negative samples (denoted by $N_{tn}$), i.e., TNIR = $N_{itn}/N_{tn}$. FNIR represents the proportion of false negative samples correctly identified as malicious by ADD (denoted by $N_{cfn}$) to the total number of the false negative samples (denoted by $N_{fn}$), i.e., FNIR = $N_{cfn}/N_{fn}$. Since a larger TNIR indicates a more negative impact on the original detection performance, ADD calibrates the thresholds by controlling TNIR at an acceptable level, e.g., 5%. Then ADD selects the best threshold that maximizes FNIR, as a larger FNIR signifies a more positive impact on the original detection performance.\nThe details of the threshold calibration algorithm can be found in Algorithm 1. Initially, the algorithm identifies the true negative samples and false negative samples from the calibration dataset using the ML-based AMD system (lines 2-3). For each training epoch, it calculates the incompatibility scores for these true negative and false negative samples (lines 6-7). The algorithm then calibrates the threshold based on the K percentile of the incompatibility scores of the true negative samples to control the TNIR (line 8). Using this threshold, the algorithm computes the FNIR and selects the epoch with the highest FNIR (lines 9-13). Ultimately, the corresponding threshold and encoder models for the chosen epoch are determined."}, {"title": "4 Evaluation", "content": "Implementation details. ADD is a defense framework with three stages designed as a plug-in for enhancing the robustness of ML-based AMD systems against adversarial Android malware attacks. Our implementation of the prototype of ADD is developed using Python, which manages the program logic of the defense process across the three stages. For building the encoder models for feature projection, we utilize PyTorch [49]. In our implemented encoder models, we set the dimension of the output layer as 32, a common setting in machine learning [11]. We train the encoder models for a maximum of 50 epochs using the Adam optimizer [29] with a learning rate of 0.001.\nRegarding the hyperparameters in the customized contrastive learning loss, we set the margin m in both L2 and L3 as 1.0 and set $\\lambda_1$ = $\\lambda_2$ = $\\lambda_3$ = 1.0. To construct the set containing pairs of benign and malicious samples, we uniformly sample a malicious sample from the training dataset of the ML-based AMD system for each benign sample. Similarly, to build the set containing pairs of pseudo adversarial malware samples and malicious samples, we uniformly sample a pseudo adversarial malware sample for each malicious sample in the training set of the ML-based AMD system.\nDatasets. We evaluate the defense effectiveness of ADD using two datasets: the primary dataset and the supplementary dataset. The primary dataset comprises 135,859 and 15,778 malicious samples, totaling 151,637 samples. This dataset is derived from the previous works [26, 52]. We follow the same procedure as in the previous work [26] for downloading the APKs from AndroZoo [1]. The samples within the primary dataset are dated between January 2016 and December 2018. The dataset has already been processed by Pierazzi et al. [52], adhering to the space constraints [50].\nWe employ both the time-aware split [7,50] and the random split for the primary dataset. The time-aware split aims to simulate the practical scenario in which the concept drift [12] problem naturally exists and should be considered. Specifically, we utilize samples dated between January 2016 and December 2017 as the training set, samples dated between January 2018 and July 2018 as the calibration set, and samples dated between July 2018 and December 2018 as the test set. In contrast, the random split aims to remove the effect of goodware/malware evolution, a setting commonly evaluated in previous works [52, 67]. Specifically, we randomly split the primary dataset into the training set, the calibration set, and the test set according to the ratio of about 6:3:1."}, {"title": "4.1 Experimental setup", "content": "To evaluate the temporal effectiveness of ADD, we also employ a supplementary dataset downloaded from VirusShare [64", "65": "using the supplementary dataset (Section 4.4).\nTarget models. We utilize four SOTA ML-based AMD systems, namely Drebin [8", "25": "MaMadroid [46", "70": "as our target ML-based AMD systems. To ensure the fidelity of our implementations, we strictly follow the descriptions and configurations provided in the previous works, as well as the corresponding open-source code [26, 52, 67"}]}