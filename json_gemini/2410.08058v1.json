{"title": "Closing the Loop: Learning to Generate Writing Feedback via Language Model Simulated Student Revisions", "authors": ["Inderjeet Nair", "Jiaye Tan", "Xiaotian Su", "Anne Gere", "Xu Wang", "Lu Wang"], "abstract": "Providing feedback is widely recognized as crucial for refining students' writing skills. Recent advances in language models (LMs) have made it possible to automatically generate feedback that is actionable and well-aligned with human-specified attributes. However, it remains unclear whether the feedback generated by these models is truly effective in enhancing the quality of student revisions. Moreover, prompting LMs with a precise set of instructions to generate feedback is nontrivial due to the lack of consensus regarding the specific attributes that can lead to improved revising performance. To address these challenges, we propose PROF that PROduces Feedback via learning from LM simulated student revisions. PROF aims to iteratively optimize the feedback generator by directly maximizing the effectiveness of students' overall revising performance as simulated by LMs. Focusing on an economic essay assignment, we empirically test the efficacy of PROF and observe that our approach not only surpasses a variety of baseline methods in effectiveness of improving students' writing but also demonstrates enhanced pedagogical values, even though it was not explicitly trained for this aspect.", "sections": [{"title": "1 Introduction", "content": "Writing high-quality essays often requires subject-specific and customized feedback from peers and experts, followed by multiple rounds of revisions (Fitzgerald and Markham, 1987; Hayes et al., 1987; MacArthur et al., 1991; Afrin and Litman, 2023). As students incorporate feedback into their writing, they not only improve the current piece but also advance the general writing skills, learn to critically self-assess their work (MacArthur, 2007), and gain a deeper understanding of the subject matter (Bangert-Drowns et al., 2004).\nRecent advances in language models (LMs) (Hoffmann et al., 2022; Chowdhery et al., 2023; Touvron et al., 2023; Jiang et al., 2024) make it possible to develop automatic feedback generation systems to provide concrete and actionable comments in a timely manner (Chamoun et al., 2024; D'Arcy et al., 2024), compared to the time-consuming process performed by humans. However, careful prompt engineering is necessary to incorporate precise instructions, ensuring that the generated feedback effectively guides students in improving the quality of their writing. More importantly, providing such detailed instructions is not a trivial task since there is still no general consensus about what attributes the feedback must entail to effectively contribute to students' learning outcomes (Nelson and Schunn, 2009). For example, Bitchener et al. (2005) show that including explanations in feedback can only improve the writing quality of specific revisions, and sometimes (e.g., if too lengthy) can negatively affect tenth-graders' overall writing performance (Tseng and Tsai, 2007).\nTo this end, our goal is to build an automatic feedback generation system that can be directly optimized to maximize students' writing revision performance, to avoid the complexity of explicitly specifying the criterion for effective feedback. However, involving actual students at every stage of the system-building process is impractical due to the time required and the potential negative impact on participants from an immature system (Latifi et al., 2021). To address this challenge, we first develop an LM-based student simulator that emulates the process of applying feedback to revise initial content, inspired by the recent efforts to simulate human processes (Park et al., 2023; Shanahan et al., 2023; Xu and Zhang, 2023; Lu and Wang, 2024). In our empirical evaluations of LM simulators, we discovered that by varying the temperature used in autoregressive decoding, we can effectively simulate a diverse array of behaviors to support a comprehensive testing of the feedback generator."}, {"title": "2 Data Description", "content": "We collected data from the essay assignments submitted by students enrolled in the Economics 101 course at the University of Michigan, Ann Arbor, United States. This assignment explores a scenario in which \u201can increase in the minimum wage in San Francisco could lead to burgeoning adoption of automation\u201d. To discourage this outcome, two policies are proposed: a) a tax on automation and b) a ban on automation. The students are instructed to craft a persuasive letter explaining the economic consequences of a minimum wage increase. Furthermore, they are tasked with presenting arguments against one of the aforementioned policies by using the tools and principles taught in the course. Refer Appendix A.1 to view the assignment prompt.\nThereafter, each student essay is reviewed by 3 peers to obtain a set of 3 feedback which identifies areas for improvement. This assignment uses scripted peer feedback (Latifi et al., 2021; Noroozi et al., 2016) wherein the peer reviewers are expected to provide feedback along a series of prompts. Refer Appendix A.2 to view the questions in the scripted peer feedback. Finally, the author revises the original essay based on the received peer reviews.\nIn total, we collected 363 datapoints, each comprising the initial writing, three peer feedback, and the revised essay. Among these, we utilized 291 essay-revision pairs along with the 873 (291 \u00d7 3) feedback for our train split, while the remaining 72 datapoints were used for testing. As demonstrated later, our approach yields enhanced performance despite the limited number of datapoints and the absence of expert annotated feedback. This assignment includes a detailed rubric for evaluating and grading student-written essays. Our approach utilizes this rubric to create an essay evaluation prompt for assessing revisions produced by LM student simulators. This prompt can be found in Appendix A.3."}, {"title": "3 PROF: Learning to Generate Feedback with Simulated Student Revisions", "content": "To directly optimize the feedback generation on student revising performance, we have two LMs that function as feedback generator and student simulator respectively, as illustrated in Figure 1. In \u00a73.1, we first describe the training of student simulators using the data from \u00a72, to emulate how students integrate feedback into revisions. In \u00a73.2 and \u00a73.3, we present how the feedback generator is initialized and iteratively optimized by the proposed PROF method based on desirable and undesirable feedback, as measured by simulated revisions' quality."}, {"title": "3.1 Student Simulation", "content": "We represent the dataset of the assignment submissions as D, whose jth element can be represented as a tuple (xj,{fi}3i=1,yj). Here, {fi}3i=1 represents the set of feedback applied by the student to revise the initial writing xi into yj.\nTo simulate the behavior of implementing one feedback in place of 3 feedback simultaneously, we instruct gpt-3.5 to combine the 3 feedback into a holistic feedback f\u02c6. Please refer Appendix A.4 to view the exact prompt.\nThereafter, we fine-tune two LMs, llama3-8b and gpt-3.5, of different scales to implement the feedback, i.e., generating y given xi and f\u02c6i. We represent the trained simulator as S and use it to sample revisions during feedback generator training for initial writing x and feedback f, i.e., y \u223c S(\u00b7|xi,f\u02c6i)."}, {"title": "3.2 Feedback Generator: Initialization", "content": "We initialize our feedback generator using 1lama3-8b and train it specifically for the task of generating peer feedback. To create paired data for fine-tuning, we use D where the jth assignment submission consists of three paired data points {xi,fi}3i=1 using the individual peer feedback. We represent the feedback generator by M from which the feedback f can be sampled for an essay x as f \u223c M(x). After this initialization process, we continue training using the PROF method to optimize student revision performance"}, {"title": "3.3 Feedback Generator: Optimization", "content": "Our approach assumes access to two functions for the iterative optimization of the feedback generator: (a) student simulator as described in \u00a73.1 and (b) automatic essay scoring system, where we use gpt-4 via few shot prompting owing to their strong capabilities in critically assessing the quality of natural language outputs (Zheng et al., 2023; Li et al., 2023; Zheng et al., 2024a). The rubrics employed to assess the quality of the essay are identical to the course rubrics. Refer to Appendix A.3 to view the complete prompt.\nLet the feedback generator at the start of tth iteration be represented by Mt. Our objective would be to use Mt in creating desirable feedback f+i and undesirable feedback f\u2212i for each initial essay xi in D. After constructing such kind of preference relationship for each datapoint, we use Direct Preference Optimization (DPO) loss (Rafailov et al., 2023) to train a new model Mt+1 for the next iteration. The following objective describes the relation between Mt+1 and Mt using the DPO loss:\n$M_{t+1} = \\underset{M}{\\text{arg min}} \\sum_{j=1}^{|D|}L_t(f_i^+, f_i^-, x_i)$                                   (1)"}, {"title": "4 Analysis of Student Simulators", "content": "In this section, we analyze the alignment between the properties of revision in student simulators and actual students, based on peer written feedback. Concretely, we examine the impact of the temperature parameter on the number of modifications (\u00a74.1), revision performance (\u00a74.2), and faithfulness to feedback (\u00a74.3), and compare it with the real students' revision process."}, {"title": "4.1 Temperature and Revisions", "content": "We first analyze the variation in the number of lexical modifications between two student simulators (initialized with llama3-8b and gpt-3.5 respectively) using different temperature settings and compare these results to the actual revision process of real students. The Ratcliff/Obershelp algorithm (Black, 2004) provides us with a list of edit operations (additions and deletions) required to transform one sequence into another. We categorize contiguous additions / deletions that involve less than a sentence as word-level modifications, with the number of words involved being counted. For modifications that span an entire sentence or more, we categorize them as sentence-level modifications, with the number of sentences involved being counted.\nFrom the plots shown in Figures 2 and 6, we see that the number of elements (words/sentences) added or deleted by the student simulators increases as the temperature is increased. This observation aligns with expectations, as higher temperature settings introduce more randomness during decoding, leading to increased number of alterations (Renze and Guven, 2024). Furthermore, the gpt-3.5-based student simulator tends to be more conservative than the 1lama3-8b-based counterpart, with a greater inclination towards preserving the original content."}, {"title": "4.2 Temperature and Revision Quality", "content": "In this analysis, we compute the quality of the revised essays based on peer feedback from both student simulators and actual students using gpt-4 as the judge. To validate the use of gpt-4 in evaluating essay quality, we compared its inferred scores with the scores assigned by teaching instructors of the Economics course to actual students' final revised essays. The mean squared error was 0.082 after normalizing the scores between 0 and 1, suggesting that the inferred scores are reliable and closely align with the expert-assigned scores.\nBased on the findings presented in Figure 3, it is evident that actual students exhibit a higher level of effectiveness in incorporating feedback compared to the student simulators. This is a desirable outcome as it creates a more challenging environment for our feedback generator during training and testing with student simulators. Among the student simulators, the model based on gpt-3.5 demonstrates superior implementation performance, with a slight improvement in the quality of revised essays as the temperature increases and reasonable alignment with the quality of revision from real students. This makes gpt-3.5 a suitable approach for automatically assessing feedback effectiveness. On the other hand, the llama3-8b based student simulator demonstrates a modest enhancement in quality at lower temperatures but experiences a decline beyond a temperature of 0.8."}, {"title": "4.3 Temperature and Revision Faithfulness", "content": "Next, we assess how faithful the student simulators are in implementing the feedback, in comparison to the revisions made by real students. For this purpose, we broke down the feedback into a list of distinct recommended changes, and then examined how many of these changes were incorporated in the revised writing, as compared to the initial version. We categorized the changes as either faithful (i.e., they adhered to the provided feedback) or unfaithful (i.e., they went beyond the scope of the feedback and made additional changes). For 10 samples, the number of these instances at 3 temperature values for the student simulators was manually annotated by a fluent English speaker. This resulted in the analysis of (3 + 3 + 1) \u00d7 10 = 70 revised essays, with 3 revisions from each of the student simulators and one actual revision.\nComparing the revisions generated by student simulators with those made by humans, we find that the simulators produce more unfaithful revisions and fewer faithful revisions, as shown in Table 1. While previous experiments indicate that higher temperatures result in more content-level modifications from the simulators, they do not always align with the provided feedback. To quantify the faithfulness of modifications, we define \u03b3 as the logarithm of the ratio between the number of faithful and unfaithful modifications. These results suggest that there is still room for improvement in the faithfulness of simulated revisions compared to actual human revisions."}, {"title": "5 Feedback Generator Setups", "content": "For our iterative optimization approach, we use 1lama3-8b based student simulator for training and emphasize on gpt-3.5 based one for testing. We do this for the following three reasons: (1) gpt-3.5 based student simulator better aligns with actual student revision, as demonstrated in \u00a74.2 and thus provides a more realistic testing environment to measure implementation performance. (2) Using gpt-3.5 based student simulator is prohibitively expensive when used in conjunction with an iterative optimization approach. Using llama3-8b as a student simulator makes our research more accessible due to lower training cost and easy access to the open-source models. (3) Training and testing on the same student simulator would not provide conclusive evidence of the effectiveness of our approach, as it might learn to exploit one type of student simulator while performing poorly on others. For completeness, however, we also include the effectiveness of the generated feedback using the llama3-8b based student simulator.\nRefer Appendix B for more details."}, {"title": "6 Results for Feedback Generation", "content": "We consider the following types of baselines: (1) gpt-3.5 / gpt-4: Using enterprise LLMs as a few-shot feedback generator by sampling in-context examples from peer-written feedback from the train-split. (2) sft-from-human: Fine-tuning 1lama3-8b on peer review feedback.\nOur method variants are named as \u201cPROF, \u03c4 = x\u201d where the llama3-8b based feedback generator is initialized with sft-from-human and trained using the iterative optimization framework described in \u00a73 along with the llama3-8b based student simulator executed at temperature x."}, {"title": "6.1 Intrinsic Evaluation", "content": "To intrinsically evaluate the quality of the feedback generated from different approaches, we employ LM as the judge (Chevalier et al., 2024; Ke et al., 2023) and evaluate along the following four major pedagogical dimensions (Jurenka et al., 2024):\n\u2022 Respects Guided Questions (RGQ): Given that the assignment uses scripted feedback, the generated responses are expected to follow a template with 6 prompts, each accompanied by a targeted feedback. Here, we assess how well each feedback component aligns with its respective prompt. To view the prompt template, refer to Appendix A.2.\n\u2022 Encourages Active Learning (EAL): Measures how well the feedback guides the students to make improvements on their own without explicitly revealing the concrete changes.\n\u2022 Deepens Metacognition (DM): Determine the effectiveness of the feedback in identifying and addressing student errors and misconceptions within the essay.\n\u2022 Motivates and Stimulates Student Curiosity (MSSC): Assess how well the feedback maintains a positive and encouraging tone that fosters curiosity and motivation.\nFor the score generated by gpt-4 in relation to a sample feedback, please see Appendix D.2. The idea of using a critic LLM to automatically assess the quality of feedback based on pedagogical aspects was inspired by Jurenka et al. (2024), who demonstrated a strong correlation between the generated scores and those provided by humans. In our study, we utilized gpt-4 to automatically assign pedagogical scores to the feedback.\nResults from gpt-4 based pedagogical evaluation. We used gpt-4 to assign scores ranging from 0 to 5 for each metric, representing lowest to highest quality. The average quality for each metric was then calculated, normalized between 0 and 100, and presented in Table 2. Our approach significantly improves performance for sft-from-human and achieves comparable results to enterprise LLMs, despite having significantly fewer parameters. This demonstrates the effectiveness of our approach without requiring high-quality feedback.\nAmong our models trained with student simulators at different temperatures, we observe the most significant improvements in the model trained at a temperature of 0.85. We believe that at lower temperatures, the student simulators only incorporate a limited number of feedback elements, as discussed in \u00a74.3. This limitation prevents the appropriate selection of desirable and undesirable feedback at each stage of preference learning. Conversely, at higher temperatures, the student simulator demonstrates a higher degree of unfaithfulness, leading to poor implementation performance, as demonstrated in \u00a74.3 and \u00a74.2, respectively. By using sft-from-human as the foundation for our feedback generator across various temperature settings, we achieve a notably improved model in terms of pedagogical alignment.\nValidation of gpt-4's pedagogical evaluation. To validate the pedagogical evaluation using gpt-4, we used 21 pairs of essays and peer reviews and had two proficient English annotators assess the peer reviews across various pedagogical dimensions by providing a score between 0 and 5 for each metric. We then calculated the Pearson correlation between the average normalized scores assigned by humans and those inferred by gpt-4."}, {"title": "6.2 Extrinsic Evaluation", "content": "Next, we use trained student simulators to gauge the efficacy of feedback generated by different systems. Although we discuss results by using gpt-3.5 based student simulator, we also include the results computed using 1lama3-8b based student simulator in Table 9 of Appendix D.1. gpt-3.5-based simulator aligns more closely with actual human performance and was not employed in training our models, making the effectiveness evaluation more reliable and trustworthy.\nFor each approach, we use greedy decoding to generate feedback. The student simulators are executed at 3 different temperatures: 0.7, 0.85, and 1.0, with 5 different seeds to mitigate the impact of randomness. Finally, we compute a score based on the course rubric prompt (refer to Appendix A.3), averaging the scores across different aspects and normalizing it between 0 and 100.\nTable 3 demonstrates that our approaches consistently outperform enterprise LLMs like gpt-3.5/gpt-4. Interestingly, the results also indicate a lack of correlation between extrinsic and intrinsic evaluations. While sft-from-human exhibits better extrinsic performance, its intrinsic performance is lower compared to the few-shot approaches. One possible explanation for this discrepancy is that sft-from-human provides explicit feedback, which may negatively impact the \"Encourages Active Learning\" metric (Table 2), but contribute to a more effective revision process. In most cases, one round of iterative optimization leads to the best extrinsic performance. However, it is important to note that these findings are specific to the experiments conducted on a domain-specific course and may not necessarily apply to broader datasets or different contexts."}, {"title": "6.3 Additional Analyses", "content": "RQ1: How does our training algorithm influence different feedback categories with the number of refinement iterations? In this analysis, we break down the feedback into distinct components and classify them into one of 3 categories: praise, solution, or problem. Our primary objective is to verify whether PROF effectively adjusts the frequency of these elements in a manner that is consistent with the learning sciences research on maximizing feedback effectiveness (Lizzio and Wilson, 2008; Nelson and Schunn, 2009; Cho and MacArthur, 2010).\nIf a segment solely describes an issue, it is labeled as a problem. If segment contains both problem and solution, it is still categorized as a solution. To view how our algorithm influences the number of praise elements, refer to Figure 4a. We notice that the average number of praise elements decreases with more optimization steps, which is corroborated by previous research indicating that praise has minimal impact on student performance (Kluger and DeNisi, 1996; Ferris, 1997). In general, the average number of solution and problem elements in the feedback (refer Figure 4b and 4c respectively) increases with the number of iterations which is known to impact implementation performance as supported by many prior works (Hayes et al., 1987; Matsumura et al., 2002; Bitchener et al., 2005; Sugita, 2006)."}, {"title": "RQ2: How does the faction of problem or solution feedback segments associated with a local Scope vary with the number of refinement iterations?", "content": "The feedback scope is broadly categorized into two classes: (a) local scope, which focuses on specific words, phrases, sentences, or paragraphs and is associated with narrow aspects such as surface features; (b) global scope, involves considering multiple parts or the entirety of the writing. Both local-scope and global-scope feedback have been observed to result in improved essay quality after implementation (Olson and Raffeld, 1987; Lin et al., 2001; Miller, 2003). To ensure effective feedback for enhancing essay quality, it is important to include an appropriate proportion of both locally and globally scoped problem and solution segments, rather than focusing solely on one type. Figure 5a illustrates that the fraction of locally-scoped feedback instances generally increases as the number of iterations grows, with the most significant increase observed for \u03c4 = 0.7. This is desirable as initially the generated feedback has very few instances of locally-scoped elements and PROF rectifies this by increasing it appropriately. When the student simulator operates at a low temperature and makes minimal edits, the optimization algorithm encourages the feedback generator to prioritize generating feedback that focuses on local scope. This is because feedback associated with local scope is more likely to be addressed through localized changes, which align with the minimal edits made by the student simulator at low temperatures."}, {"title": "RQ3: Is the consistency of Problem / Solution segments improved with the number of refinement iterations?", "content": "The consistency of a problem/solution is determined by two aspects: intrinsic correctness and consistency with respect to the content. Intrinsic correctness refers to the validity and absence of any logical fallacies in the feedback segment. Consistency with respect to the initial content refers to whether the identified problem is indeed an issue in the original content and whether the solution maintains the original stance of the essay without altering it.\nBased on Figure 5b, our training approach demonstrates an improvement in the consistency of the problem/solution segments as the number of refinement iterations increases. Notably, the consistency of PROF, \u03c4 = 1.0 shows the highest performance after 3 iterations. We attribute this observation to the complexity of the training environment, which influences the consistency of the generated feedback. In the case of the student simulator executed with a temperature of 1.0, which yields lower implementation performance and creates a more challenging environment for the feedback generator, the training algorithm guides the feedback generator to produce feedback with better consistency to achieve optimal implementation performance."}, {"title": "7 Related Works", "content": ""}, {"title": "7.1 Automatic Feedback Generation Systems", "content": "NLP systems have been developed to automatically provide formative feedback to improve students' writing (Liu et al., 2016; Zhang et al., 2019; Klebanov and Madnani, 2020). One significant challenge faced by these approaches is the creation of high-quality feedback datasets, which requires considerable time and effort. In contrast, our approach starts with peer-annotated reviews that may not initially be of high quality. However, through an iterative preference learning process, we steer our feedback generator towards producing better quality responses. Considering that many previous works have focused on collecting peer review datasets (Kang et al., 2018; Lin et al., 2023; Dycke et al., 2023), our approach can leverage these datasets for better initialization.\nWhile Language Models (LMs) can bypass the need for high-quality annotated feedback through few-shot prompting, they are computationally intensive and expensive (Han et al., 2023; Chamoun et al., 2024; D'Arcy et al., 2024). In contrast, our feedback generator offers a cost-effective solution using smaller LMs with fewer parameters and without relying on high-quality supervision."}, {"title": "7.2 Iterative Preference Optimization", "content": "In recent times, a novel paradigm has emerged, which entails the iterative application of offline reinforcement learning techniques (Rafailov et al., 2023). In this approach, model generates preference relations per iteration, which are used to construct potentially more informative relations than those observed so far. This updates the model parameters, resulting in a better aligned model. Examples of such approaches include Iterative DPO (Xu et al., 2023; Xiong et al., 2023; Gulcehre et al., 2023), Self-Rewarding LMs (Yuan et al., 2024), and SPIN (Chen et al., 2024). Previous approaches build preference relations by intrinsically evaluating the quality of the generation. However, exploring the utility of generation to establish preference relations has not been investigated before. To our knowledge, we introduce a method that constructs preference relations through extrinsic assessments, quantifying the utility of generated samples."}, {"title": "8 Conclusion", "content": "In this paper, we present an optimization technique called PROF that focuses on maximizing students writing revision performance through LM simulation. We conducted extensive analysis to showcase the alignment between our student simulators and actual revisions, while also demonstrating the flexibility of adjusting temperature to elicit diverse behaviors from the models. Through experiments, we show that our trained models not only exhibit better effectiveness but are more pedagogically aligned."}, {"title": "A Dataset Prompts", "content": "In this section, we attach all the prompts associated with the dataset described in \u00a72."}, {"title": "A.1 Assignment Prompt", "content": "The prompt in Table 4 was shown to the students to get the initial writing:"}, {"title": "A.2 Peer Review Prompt", "content": "The prompt in Table 5 was shown to the students while reviewing the initial writing:"}]}