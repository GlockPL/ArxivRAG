{"title": "COMMUNICATION-EFFICIENT FEDERATED LEARNING BASED ON\nEXPLANATION-GUIDED PRUNING FOR REMOTE SENSING IMAGE CLASSIFICATION", "authors": ["Jonas Klotz", "Bar\u0131\u015f B\u00fcy\u00fckta\u015f", "Beg\u00fcm Demir"], "abstract": "Federated learning (FL) is a decentralized machine learn-\ning paradigm, where multiple clients collaboratively train a\nglobal model by exchanging only model updates with the\ncentral server without sharing the local data of clients. Due\nto the large volume of model updates required to be transmit-\nted between clients and the central server, most FL systems\nare associated with high transfer costs (i.e., communication\noverhead). This issue is more critical for operational appli-\ncations in remote sensing (RS), especially when large-scale\nRS data is processed and analyzed through FL systems with\nrestricted communication bandwidth. To address this is-\nsue, we introduce an explanation-guided pruning strategy for\ncommunication-efficient FL in the context of RS image clas-\nsification. Our pruning strategy is defined based on the layer-\nwise relevance propagation (LRP) driven explanations to: 1)\nefficiently and effectively identify the most relevant and in-\nformative model parameters (to be exchanged between clients\nand the central server); and 2) eliminate the non-informative\nones to minimize the volume of model updates. The exper-\nimental results on the BigEarthNet-S2 dataset demonstrate\nthat our strategy effectively reduces the number of shared\nmodel updates, while increasing the generalization ability of\nthe global model. The code of this work will be publicly\navailable at https://git.tu-berlin.de/rsim/FL-LRP.", "sections": [{"title": "1. INTRODUCTION", "content": "Federated learning (FL) has emerged as a promising paradigm\nfor training machine learning models collaboratively, while\nensuring data privacy. In detail, it allows training a deep\nlearning (DL) model without having direct access to training\ndata distributed across decentralized archives (i.e., clients),\nparticularly in the case that data is unshared due to commer-\ncial concerns, privacy constraints, or legal regulations [1]. \u03a4\u03bf\nthis end, each client independently trains a local model using\nthe data on the clients, and then computes and sends only the\nmodel updates (e.g., weights or gradients) to a central server\n(i.e., a global model). The central server aggregates these up-\ndates to adjust the global model, which is then sent back to\nthe clients for further training. This approach ensures data\nprivacy, while allowing model training across clients. The de-\nvelopment of FL methods has gained significant attention in\nremote sensing (RS) [2-5]. We refer the reader to [1] for the\ndetails on FL and the analysis of the state-of-the-art FL meth-\nods in RS.\nDuring FL, the iterative exchange of model parameters\nbetween clients and the central server often involves trans-\nmitting large amounts of model updates. In particular, for op-\nerational RS applications, the communication overhead (i.e.,\ntransmission cost) becomes a critical constraint in the case of\nthe presence of: 1) a high number of training samples at each\nclient, 2) a high number of participating clients; 3) limited\nbandwidth, 4) high latency, and 5) energy-constrained clients\n[6]. All these issues make the deployments of FL systems\nimpractical [7], limiting their applicability in RS. The exist-\ning FL methods in RS do not address these issues, whereas\ncommunication-efficient FL has widely studied in machine\nlearning (ML) and computer vision (CV) communities. Ex-\nisting methods can be grouped into three categories: 1) model\ncompression [8, 9]; 2) knowledge distillation [10, 11] or 3)\npruning [12, 13]. The model compression-based methods aim\nto minimize communication overhead by compressing model\nparameters to be transmitted to the central server. However,\nthey introduce additional computational overhead on both\nthe clients and the central server due to the need for com-\npressing and decompressing model updates. The knowledge\ndistillation-based methods transfer knowledge from a global\nmodel to smaller models on clients. These methods impose\nsignificant computational demands as they require teacher-\nstudent training, involving the calculation of complex loss\nfunctions and the transfer of intermediate representations.\nThe pruning-based methods aim to reduce communication\noverhead by removing less significant parameters. Unlike\nmodel compression-based and knowledge distillation-based\nmethods, pruning-based methods introduce lower computa-\ntional overhead.\nWe focus our attention on pruning-based methods and in-\nvestigate the effectiveness of explanation-based pruning in the"}, {"title": "2. PROPOSED EXPLANATION-GUIDED PRUNING\nSTRATEGY FOR FL", "content": "Let us assume that a set of $K$ clients ${C_1, C_2, ..., C_K}$ is\navailable, where $C_i$ is the $i$-th client and $K$ is the total num-\nber of clients. $C_i$ has $M_i$ pairs $D_i = {{(x_i^z, y_i^z)}}_{z=1}^{M_i}$, where\n$x_i^z$ is the $z$-th training image and $y_i^z$ is the associated class\nannotation. For scene-level annotations, each training image\nis labeled with either a single label or multiple labels that cor-\nrespond to the overall content of the scene. We assume that\nthe data on clients is not shared, $M_i$ is very large, and the\ncommunication bandwidth available to the clients is limited.\nThus the communication overhead resulting from transmit-\nting large volumes of model updates to the central server be-\ncomes a critical challenge. To address this issue, we introduce\nan explanation-guided pruning strategy applied at the central\nserver in the context of FL.\nAs the definition of the proposed strategy strictly depends\non FL, the detailed explanation of the proposed strategy starts\nfrom the brief formulation of FL. Let us assume that the local\nDL model $d_i$ is trained using $D_i$ for $E$ epochs. Each client $C_i$\naims to find the optimal local model parameters $w_i$ by mini-\nmizing the local objective $O_i$ as follows:\n$O_i(B; w_i) = \\sum_{(x,y)\\in B} L(\\phi_i(x^i; w_i), Y)$, (1)\n$w_i^* = \\arg \\min_{w_i} O_i(D_i; w_i)$,\nwhere $L$ represents the task-specific loss function, such as cat-\negorical cross-entropy (CE) for scene-level single-label clas-\nsification, or binary CE for scene-level multi-label classifi-\ncation (MLC). FL methods collaboratively learn the param-\neters $w^*$ of the global model $\\phi^*$ over the entire training set\n$D_{train} = \\cup_{i\\in{1,2,...,K}} D_i$, by minimizing the following ob-\njective:\n$w^* = \\arg \\min_{w_i} \\sum_{i=1}^K \\frac{M_i}{|M|} O_i(D_i; w_i)$. (2)\nTo this end, the model parameters are aggregated at the central\nserver as:\n$w = \\sum_{i=1}^K a_i w_i$, (3)\nwhere $a_i$ is a hyperparameter controlling the importance of\nthe local updates $w_i$ of $C_i$. The parameter transmission $w_i$\nbetween the client and the central server causes the communi-\ncation overhead. To reduce the communication overhead, we\ninvestigate the effectiveness of explanation-guided pruning,\nwhich requires an explanation method capable of attributing\nrelevance not only to the input features but also to the model\nparameters. Backpropagation-based methods, such as LRP\n[15] or Integrated Gradients [16], are particularly suitable for\nthis purpose. Given its proven success in the literature, we fo-\ncus our attention on an LRP-based pruning method presented\nin [14, 17], and adapt it to an FL setup.\nIn LRP-based pruning, a DL model is viewed as a\ncollection of $p$ interlinked components, denoted as $\\phi = {\\psi_1,...,\\psi_p}$. These components can represent various\nstructural elements, such as entire layers, groups of neurons,\nconvolutional filters, or attention heads [17]. The server has\na global model $\\phi^*$ and a reference set $D_{ref} = {{(x^z,y^z)}}_{z=1}^{M}$,\nwhere $M$ specifies the total number of reference samples.\nTo prune the global model's least important components, we\ncompute the relevance $R_{\\psi_c}$ of each component $\\psi_c \\in \\phi^* $ over\nthe reference set using LRP. For an input $I_z \\in D_{ref}$, LRP\nattributes relevance scores $R_k$ to each neuron $k$ by tracing its"}, {"title": "3. EXPERIMENTAL RESULTS", "content": "In the experiments, we assessed our strategy in the context of\nmulti-label image classification in RS. We conducted the ex-\nperiments using the BigEarthNet-S2 v2.0 benchmark archive\n[18]. A subset of BigEarthNet-S2, comprising images col-\nlected during the summer season from Austria, Belgium, Fin-\nland, Ireland, Lithuania, Serbia, Portugal, and Switzerland,\nwas selected. Each image is annotated with multi-labels de-\nrived from the CORINE Land Cover Map database, following\nthe 19-class nomenclature defined in [19]. We used the train-\ntest split recommended in [18]. Furthermore, we allocated the\ntraining data such that each client exclusively held data from\na single country.\nWe used FedAvg [20] as the FL algorithm, though any FL\nalgorithm could be used. The FedAvg algorithm updates the\nglobal model by aggregating the weighted averages of locally\ntrained models from the distributed clients. The results of Fe-\ndAvg with our strategy are denoted as proposed and compared\nwith: 1) those obtained by using FedAvg without pruning (de-\nnoted as standard); 2) those obtained by using FedAvg with\nrandom pruning (denoted as random). We selected a ResNet-\n50 as the model architecture and trained it for 20 communica-\ntion rounds with 3 local training epochs per round. The num-\nber of clients was set to 8, all participating in every commu-\nnication round. We chose the Adam optimizer with a learning\nrate of 0.001 and a mini-batch size of 512. The warmup phase\nended after 9 communication rounds ($\\nu = 9$). The LRP hy-\nperparameters for the ResNet-50 were adapted directly from\n[17]. We assessed performance based on MLC accuracy, mea-"}, {"title": "4. CONCLUSION", "content": "In this paper, we have introduced an explanation-guided\npruning strategy for reducing communication overhead in\nFL in the context of RS image classification. By leveraging\nLRP-based pruning, our strategy identifies and removes less\ninformative model parameters, thereby reducing the com-\nmunication overhead between clients and the central server.\nSince most of the computations take place on the central\nserver, our strategy avoids significant computational demands\non the clients. We would like to emphasize that our strategy\nis independent of the number of clients, FL architectures,\nlearning tasks, and FL aggregation algorithms. Experimen-\ntal results on the BigEarthNet-S2 benchmark demonstrate\nthat the proposed strategy reduces communication overhead,\nwhile improving the global model's generalization capabil-\nity compared to standard FL training and random pruning\nstrategies. We would like to note that the proposed strategy\nassumes that the clients are associated with equal importance\nduring model aggregation. However, in real applications,\ndifferent clients may have different levels of importance. In\nfuture work, we plan to investigate the effectiveness of the ex-\nplanation methods for the estimation of the importance scores\nfor the clients, aiming to enable more effective aggregation\nof their contributions to the global model."}]}