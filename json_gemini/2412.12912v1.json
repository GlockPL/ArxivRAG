{"title": "Unsupervised Region-Based Image Editing of Denoising Diffusion Models", "authors": ["Zixiang Li", "Yue Song", "Renshuai Tao", "Xiaohong Jia", "Yao Zhao", "Wei Wang"], "abstract": "Although diffusion models have achieved remarkable success in the field of image generation, their latent space remains under-explored. Current methods for identifying semantics within latent space often rely on external supervision, such as textual information and segmentation masks. In this paper, we propose a method to identify semantic attributes in the latent space of pre-trained diffusion models without any further training. By projecting the Jacobian of the targeted semantic region into a low-dimensional subspace which is orthogonal to the non-masked regions, our approach facilitates precise semantic discovery and control over local masked areas, eliminating the need for annotations. We conducted extensive experiments across multiple datasets and various architectures of diffusion models, achieving state-of-the-art performance. In particular, for some specific face attributes, the performance of our proposed method even surpasses that of supervised approaches, demonstrating its superior ability in editing local image properties.", "sections": [{"title": "Introduction", "content": "Diffusion models have achieved remarkable success in the field of image generation, with notable examples including the Denoising Diffusion Probability Model (DDPM) (Ho, Jain, and Abbeel 2020), the Denoising Diffusion Implicit Model (DDIM) (Song, Meng, and Ermon 2020), and score-based generative models (Song et al. 2020). Diffusion models have set new benchmarks in various applications such as image generation, video generation, image editing, and inverse problems. When deploying diffusion models for downstream tasks like image editing, additional supervision such as textual descriptions or sketches is typically required, and this often involves the integration of supplementary modules or fine-tuning pre-trained models (e.g., Lora (Hu et al. 2021), DiffusionClip (Kim, Kwon, and Ye 2022)).\nThese techniques are meticulously engineered to minimize undesired alterations to the noise space, thereby avoiding irregular changes or artifacts. The necessity for such additional supervision or training underscores the current limitations in the inherent information available within diffusion models. Nevertheless, as pre-trained diffusion models have demonstrated unexpected versatility across various domains, it becomes imperative to explore and leverage their intrinsic capabilities to fully realize their potential. Therefore, it is more favored to leverage the pre-trained models for unsupervised image editing.\nDiffusion models have achieved impressive results in guided generation tasks. The most outstanding one is the use of Contrastive Language-Image Pretraining (CLIP) (Radford et al. 2021) to convert the user input text prompt into the condition of the diffusion model input, thereby generating high-quality images. GLIDE (Nichol et al. 2021), DALL-E 2 (Ramesh et al. 2022), and Stable Diffusion (Rombach et al. 2022) are some of the best models currently. These models use CLIP to align the text space with the latent space of the diffusion model, so as to achieve text-guided image generation. However, there is still no comprehensive exploration of the properties of the latent space of the diffusion model itself. Current research on the latent space of diffusion models remains a bit under-explored.\nIn the denoising process of the diffusion model, the image space is generally represented as $X{1:T}$, where {1 : T} represents the time step of denoising. In the denoising process of the diffusion model, the denoised image space is generally represented as x. All vector dimensions in $X{1:7}$ are the same as the initial input $x_0$ and the output $\\epsilon{1:7}$ after the denoising network. In this article, we abbreviate the pixel space of the diffusion model as x-space. The x-space within diffusion models is generally considered to be devoid of semantics."}, {"title": "Background", "content": "Image Editing in Diffusion Model. In the field of image editing, a large number of works based on diffusion models have emerged in recent years. These works highlight the potential and versatility of diffusion models in improving image editing performance. The image editing of diffusion models has multiple categories, such as text guidance, reference image guidance, semantic segmentation map guidance, and mask guidance, etc. These methods include both earlier traditional and current multimodal conditional methods (Huang et al. 2024).\nText-based methods are the most commonly used. GLIDE (Nichol et al. 2021) is the first work to use text to directly control image generation. Unlike image generation tasks, image editing focuses on changing the appearance, content or structure of existing images. DiffusionClip (Kim, Kwon, and Ye 2022) is an early method that uses CLIP for diffusion models. The success of CLIP has inspired many new explorations (Liu et al. 2024; Tan et al. 2024; Huang et al. 2023b; Jiao et al. 2023). For image editing based on diffusion models guided by other conditions, controlnet (Zhang, Rao, and Agrawala 2023) is a guidance that adds spatial condition control. Zhang et al. (2023b) further explores control methods in video generation. These works have also had a positive impact on tasks such as continuous learning (Zhu et al. 2023; Zhang et al. 2023a) and segmentation (Zhang et al. 2023c; Chen et al. 2024).\nAnother method is to operate on the latent space of the diffusion models. Their advantage is that the ability to edit images can be achieved without a large amount of paired training data, which depends on the pre-trained diffusion model. DragDiffusion (Mou et al. 2023) is one of the representatives of the methods, which can directly guide editing through image features. It observes that there is rich semantic information in the features of U-Net, and this information can be used to construct energy functions to guide editing.\nAlthough many methods of image editing have good results in changing content, color, texture, etc., when dealing with complex structures, such as fingers, eyes, corners of the mouth and other details, artifacts are often generated. It is still a challenge to improve the artifact generation during image editing of diffusion models.\nLatent Space Disentanglement. Extensive research has been conducted on the latent space disentanglement of generative models. \u03b2-VAE (Higgins et al. 2017) introduces a hyperparameter \u1e9e to the KL divergence term, enhancing the gap between prior and posterior distributions to achieve effective disentanglement. Additional unsupervised methods based on statistical features, such as B-TC-VAE (Kim and Mnih 2018), DIP-VAE (Kumar, Sattigeri, and Balakrishnan 2017), and Guided-VAE (Ding et al. 2020), have also demonstrated significant success. Generative Adversarial Networks (GANs) (Goodfellow et al. 2014) have a well-defined latent space to disentanglement.\nSome recent efforts have proposed different approaches to improve the disentanglement of GANs from various perspective (Karras, Laine, and Aila 2019; Karras et al. 2020; Ling et al. 2021; Wang et al. 2022). Furthermore, Re-SeFa (Huang et al. 2023a) addresses region-based semantics."}, {"title": "Methodology", "content": "In this section, we explain the reasons why previous classifier-guided methods alter local attributes while simultaneously affecting global attributes. Subsequently, we introduce our region-based semantic discovery method and a technique for controlling the generation of time-step edits.\nPreliminary: Denoising Diffusion Models\nDenoising Diffusion Probability Models (DDPM) is a generative model inspired by the diffusion phenomenon in thermodynamic systems. The original diffusion models do not have a latent space but recently Kwon(Kwon, Jeong, and Uh 2022) identified a potential semantic latent space at the bottleneck level of U-Net, referred to as h-space. They reformulated the inverse process of the diffusion model as:\n$x_{t-1} = \\sqrt{a_{t-1}}P_t(\\epsilon_f(x_t)) + D_t(\\epsilon_f(x_t)) + \\sigma_tz_t$. (1)\nwhere $z_t \\sim N(0, I)$, $\\epsilon_f$ is a neural network to predict noise from $x_t$. At the same time, $P_t(\\epsilon_f(x_t))$ and $D_t(\\epsilon_f(x_t))$ are expressed as:\n$P_t(\\epsilon_f(x_t)) = \\frac{x_t - \\sqrt{1-a_t}\\epsilon_f(x_t)}{\\sqrt{a_t}}$ (2)\nand\n$D_t(\\epsilon_f(x_t)) = \\sqrt{1-a_{t-1}} - \\sigma_t\\epsilon_f(x_t)$. (3)\n$\\sigma_t = \\eta \\sqrt{(1 - a_{t-1})/(1- a_t)}\\sqrt{1 - a_t/a_{t-1}}$. The range of \u03b7 is from 0 to 1, where 0 corresponds to DDIM and 1 corresponds to DDPM. Its value represents the degree of randomness. During the image editing process of Asyrp, the semantic vector $A_{ht}$ is only injected into $P_t$ without changing $D_t$. They proved that the shift caused by $A_{ht}$ is offset by the shift $D_t$ caused by $A_{ht}$. Therefore, applying $A_{ht}$ on $P_t$ and $D_t$ leads to the same output as the original output.\nRegion-Based Semantic Discovery\nWe argue that h-space has local Euclidean properties and consistency in different timesteps. This kind of space allows us to propose approaches analogous to the latent disentanglement methods in VAEs and GANs. Therefore, we associate semantic discovery with the matrix of the generative network, thereby achieving semantic discovery that does not require supervision.\nUnsupervised Editing through Jacobian. In generative models with explicit latent spaces, a semantic direction can usually maximize the output variations, which means a relation between image space and latent space. In diffusion models, if we assume that h-space is a latent space, then we also hope to find the connection between image space and h-space. Given a time step t and the corresponding x, our goal is to find a function that establishes a connection between the denoising network $P_t(\\epsilon_f(x_t))$ and h-space. To achieve this goal, we define the following function:\n$f(h_t) := \\epsilon_f(x_t, h_t)$. (4)\nwhere $x_t$ is determined image in specific timestep. The process of semantic editing can be described as:\n$x_{edit} = f(h_t + \\alpha \\Delta h_t)$. (5)\nInspired by previous work, we relate the direction of maximum semantic change to the Jacobian matrix of the function. The Jacobian matrix is a bridge that plays an important role in attribute editing. The Jacobian matrix is defined as $J_t = \\frac{\\partial \\epsilon_f}{\\partial h_t}$. Descending along the direction of the Jacobian matrix will result in the most significant change in the function, potentially identifying a semantic direction. To analyze this Jacobian matrix, we can perform Singular Value Decomposition (SVD):\n$J_t = U_t \\Sigma_t V_t^T$. (6)\nThe corresponding right singular vectors $V_t$ constitute the orthogonal vector set of the image perturbations in h-space. Finally, our editing method becomes:\n$f(h_t + \\alpha \\Delta h_t) \\approx f(h_t) + \\alpha V_t$. (7)\nHowever, directly computing the Jacobian matrix $J_t$ remains computationally expensive. Fortunately, solving the vector product $J_t^T J_t$ using power iteration is significantly more feasible. The eigenvectors of $J_t^T J_t$ can be accurately approximated to the right singular vectors of $J_t$. Next, we demonstrate why $J_t$ can represent the perturbation of h-space and is equivalent to the direction of the maximum change of the image. The known image prediction network $p_t$ is expressed as follows:\n$P_t(\\epsilon_f(x_t)) = \\frac{x_t - \\sqrt{1-a_t}\\epsilon_f(x_t)}{a_t}$ (8)"}, {"title": "Supressing Non-masked Semantic Variations", "content": "Under the premise that $x_t$ is fixed, if $P_t$ is partial derivative with respect to $h_t$, the result is:\n$\\frac{\\partial}{\\partial h_t} P_t(x_t, h_t) = \\frac{\\sqrt{1-a_t}}{\\sqrt{a_t}}\\frac{\\partial}{\\partial h_t} \\epsilon_f(x_t, h_t)$. (9)\nwhich further equals to:\n$\\frac{\\partial}{\\partial h_t} P_t(x_t, h_t) = \\frac{\\sqrt{1-a_t}}{\\sqrt{a_t}} J_t$. (10)\nThe above derivations show that $J_t$ represents the direction of the maximum value of image change.\nLocal Mask Definition. To specify the editing area and the non-editing area, we employed two approaches. The first method involves utilizing a pre-trained segmentation network to delineate multiple regions. Subsequently, we select the area of interest as the editing region and apply a mask to the non-editing area, using the Hadamard product to binarize the selected area. The second method consists of directly using a rectangular bounding box to define the area for editing, followed by the application of a mask to the non-editing region in a manner analogous to the previous approach. Compared to the first method, the second is more straightforward. Additionally, for the same region of interest, the area selected using this method is generally larger than that identified by the segmentation network. Overall, the second method tends to facilitate richer changes. Since we expect to find the semantics of the mask area, that is, to find the direction of change in the mask area, we add the mask to the noise prediction network $\\epsilon_t(x_t, h_t)$:\n$\\bar{\\epsilon}_t(x_t, h_t) = \\epsilon_f(x_t, h_t) \\odot mask$. (11)\n$J_{masked} = \\frac{\\partial}{\\partial h_t} \\bar{\\epsilon}_t(x_t, h_t)$. (12)\nSupressing Non-masked Semantic Variations. Recall that our goal is to maintain the global attributes while allowing for changes in local attributes. The aforementioned algorithm identifies the vector in h-space that corresponds to the maximum change in local attributes. This approach focuses solely on local modifications, but does not address the potential for global alterations. We have previously proved that the change in x-space is equivalent to the noise prediction network $\\epsilon_f(x_t, h_t)$. Therefore, our optimization goal can be expressed as:\n$arg_{h_t} max \\left( \\frac{\\partial \\epsilon_t(x_t, h_t)}{\\partial h_t} \\right) min \\left( \\frac{\\partial \\epsilon_f(x_t, h_t)}{\\partial h_t} \\right)$. (13)\nHere $\\epsilon_f(x_t, h_t)$ refers to the noise prediction network in the unmasked region.\nAs previously discussed, changes in h-space affect the extent of modifications in x. If a vector $h_1$ in h-space exhibits the largest change within the masked area but also significantly affects the non-masked area, we can find an alternative vector $h_2$ in h-space that induces the largest change specifically in the non-masked area. To address this, we need to consider the optimization objective in terms of Equation 13.\nThe Jacobian matrix J describes how a change in the h-space vector influences changes in x. Therefore, $J_m$ captures the sensitivity of the masked area, and $J_u$ captures the sensitivity of the non-masked area. By leveraging these Jacobian matrices, the orthogonal projection of $h_2$ onto the non-masked area direction helps refine $h_1$ such that it optimizes the changes in the masked area without causing unintended modifications elsewhere. Therefore, we perform the following orthogonal Jacobian projection to achieve the optimization goal:\n$J = J_m - \\frac{J_m}{J_u} J_u$ (14)\nWhere $J_m$ and $J_u$ stand for the Jacobians of the masked and unmasked areas, respectively.\nSummary\nWith the above analysis and discussion, we determine the space h-space where we are looking for semantics. We also define a new function f determined by the noise prediction network $\\epsilon_f$, the Jacobian matrix and its calculation method, and give a mask adding method. Solve the optimization target Equation 13 by orthogonal projection (Equation 14), so as to effectively obtain the semantic vectors of the target region."}, {"title": "Experiments", "content": "Since our method focuses on unsupervised semantic discovery and local image editing of diffusion models, there are not many baselines. We therefore mainly compare our method with supervised semantic discovery methods such Asyrp, Boundary Diffusion which requires attribute labels as priors, and Semdiff which mainly searches for semantic attributes globally.\nExperimental Setup\nWe conduct extensive experiments on multiple datasets and diffusion models with different structures to demonstrate the effectiveness of our approach. We conduct experiments on the datasets CelebA-HQ (Liu et al. 2015), LSUN-church (Yu et al. 2015), LSUN-bedroom (Yu et al. 2015), and the diffusion model architectures DDPM and iDDPM (Nichol and Dhariwal 2021). For the comparison methods Asyrp and Boundary Diffusion, we use the checkpoints provided by the official. For the method Semdiff, since they did not provide official checkpoints, we trained it exactly as they did and found comparable properties, and then performed a fair comparison with our method. We mainly use Fr\u00e9chet In-"}, {"title": "Main Results of Attribute Editing", "content": "In this subsection, we present the overall performance of our approach. Our specific experimental process is as follows: First, add a mask to a specific area on the image according to the Equation 11. Then we compute the Jacobian matrix of the mask and non-mask areas through Equation 12. After that, we use Equation 14 to get semantic vectors J. Finally, We apply the Jacobian matrix in timestpes of reverse process to obtain the results.\nWe mainly select the eyes region and the mouth region to find semantics. In Figure 3, we show the semantics found by our method in the eye and mouth regions. In the eye semantics, there are eyes open and eyes closed; in the mouth attributes, there are smiles, and slanting to the left and right. Interestingly, we also found some semantics for specific groups of people, such as editing the beard area for men, and changing the hair of people with long curly hair to straight hair. We show the above in Figure 4 and put more semantic editing results of other regions in the Appendix.\nWe further test the quantitative results shown in Table 1."}, {"title": "Comparison against Other Baselines", "content": "In this section, we compare our method with the state-of-the-art methods both qualitatively and quantitatively. We compare with Asyrp, Boundary diffusion, and Semdiff, which are current methods for semantic discovery and editing using diffusion models. Although our method finds local semantics, the smile attribute exists in the semantics of the mouth. And for fair comparison, their method has an open source smile attribute checkpoint, so we used the smile attribute that is unfavorable to ours for comparison. But we are surprised to find that even so, our unsupervised method is better than the supervised method, and has a great improvement effect than the unsupervised semdiff.\nIn Figure 5, we show the qualitative results. As can be seen from Figure 5, Asyrp will bring about a great change in the image structure, and for images with complex character"}, {"title": "Changes of Local and Non-local Regions", "content": "postures, it will bring more serious distortion results. Boundary Diffusion is better than Asyrp, but it still has a more serious impact on character identity, background and character details. The effect of Semdiff is intuitively good, but because this method only considers local semantic attributes, it does not consider the influence of other global parts. Therefore, the details of the characters, such as the noses in the first and third images, and the beards in the second male image, have more serious changes, which is not desirable. The last row is the result of our method. Our method not only ensures the invariance of the overall image (character identity, background, etc.), but also ensures the invariance of character details (clothes, nose, beard, etc.).\nTable 2 presents a quantitative evaluation of baselines and our method using metrics such as FID, ID, MSE, and LPIPS. Our method delivers best results across most metrics, particularly excelling in FID and ID, which assess image quality and person identity, respectively. MSE reflects image quality on the one hand and editing intensity on the other. Asyrp and Boundary Diffusion exhibit significantly higher MSE values due to larger structural changes. Although our method and Semdiff have slightly higher in MSE values, it does not necessarily indicate inferiority, as other metrics should also be considered for a comprehensive assessment.\nTo demonstrate the superiority of our method in local region editing, we compared the degree of change in local and non-local regions with Semdiff. we compared the MSE inside and outside the mask region when editing the eyes and mouth. Compared with Semdiff, when the overall MSE scale is close, our method is more inclined to edit the inner area of the mask, and the changes to the outer area of the mask are less than Semdiff. The qualitative results in Figure 7 also prove this point: when editing the masked region, Semdiff will cause changes in the rest of the image, while our method maintains the non-masked region well."}, {"title": "Discussion", "content": "We have demonstrated the state-of-the-art performance of our method in local attribute editing. More experimental results, including other attribute edits and different datasets, are available in the Appendix. First, since our method is aimed at local editing, when editing global attributes, the mask area needs to be expanded to a large enough area. Under this premise, our method will not be significantly better than previous research on global attribute editing. Moreover, when the mask area is very small (such as an eye, a tooth, etc.), our method cannot find significant semantic directions. Our future research will be directed towards extending our method to more fine-grained areas and global attributes."}, {"title": "Conclusion", "content": "To explore the latent potential within diffusion models, we propose a novel method utilizing the Jacobian projection technique for precise semantic editing of pre-trained models without additional training. Our approach allows for the identification and manipulation of semantic attributes in local regions by projecting the Jacobian of the masked area into a low-dimensional subspace orthogonal to the non-masked regions. This technique significantly enhances control over local image attributes while preserving overall image harmony. Furthermore, our method outperforms existing supervised approaches, especially in specific tasks such as facial attribute editing, demonstrating its substantial potential to advance unsupervised image editing techniques."}]}