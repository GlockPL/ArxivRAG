{"title": "Improving Clinical Note Generation from Complex Doctor-Patient Conversation", "authors": ["Yizhan Li", "Sifan Wu", "Christopher Smith", "Thomas Lo", "Bang Liu"], "abstract": "Writing clinical notes and documenting medical exams is a critical task for healthcare professionals, serving as a vital component of patient care documentation. However, manually writing these notes is time-consuming and can impact the amount of time clinicians can spend on direct patient interaction and other tasks. Consequently, the development of automated clinical note generation systems has emerged as a clinically meaningful area of research within AI for health. In this paper, we present three key contributions to the field of clinical note generation using large language models (LLMs). First, we introduce CliniKnote, a comprehensive dataset consisting of 1,200 complex doctor-patient conversations paired with their full clinical notes. This dataset, created and curated by medical experts with the help of modern neural networks, provides a valuable resource for training and evaluating models in clinical note generation tasks. Second, we propose the K-SOAP (Keyword, Subjective, Objective, Assessment, and Plan) note format, which enhances traditional SOAP (Podder, Lew, and Ghassemzadeh 2024) (Subjective, Objective, Assessment, and Plan) notes by adding a keyword section at the top, allowing for quick identification of essential information. Third, we develop an automatic pipeline to generate K-SOAP notes from doctor-patient conversations and benchmark various modern LLMs using various metrics. Our results demonstrate significant improvements in efficiency and performance compared to standard LLM finetuning methods.", "sections": [{"title": "Introduction", "content": "The generation of clinical notes is a critical task for healthcare professionals, serving as a vital component of patient care documentation. However, manually writing these notes is time-consuming, often taking clinicians between 10 to 30 minutes per note. This extensive time requirement may impact the amount of time clinicians can spend on direct patient interaction and other tasks. Consequently, the development of automated clinical note generation systems has emerged as an important area of research within AI for health. Recent advancements in large language models (LLMs) have significantly enhanced text summarization capabilities, offering the potential to streamline the creation of clinical notes. These models can assist clinicians in efficiently drafting their notes, allowing them to focus more on patient care and manage multiple tasks during consultations (Knoll et al. 2022).\nDespite this promise, the application in clinical note generation faces several challenges. Firstly, the quality of existing datasets is a major concern. While there are publicly available datasets like MTS-DIALOG (Ben Abacha et al. 2023) and ACI-BENCH (wai Yim et al. 2023), they often fall short in terms of the complexity and length required to accurately simulate real-life clinical interactions. Secondly, the current format of clinical notes, such as SOAP notes, although clear and structured, can be time-consuming for clinicians to review, particularly as patient conditions become more complex. This increased complexity results in longer notes with dispersed critical information, complicating the decision-making process (Davidoff and Miglus 2011). Lastly, there is a notable lack of comprehensive benchmarking for LLM-based clinical note generation, hindering the evaluation and comparison of different models and approaches in this domain. Addressing these challenges requires finetuning LLMs with domain-specific knowledge, developing high-quality datasets that reflect real-life clinical dialogues, and establishing thorough benchmarking protocols to advance the efficacy and reliability of automated clinical note generation systems (Zhou et al. 2024).\nOur work makes several significant contributions to the field of clinical note generation using large language models. First, we introduce CliniKnote, a comprehensive dataset of complex doctor-patient conversations paired with full clinical notes. Second, we propose the K-SOAP note format, enhancing traditional SOAP notes with a keyword section to improve information retrieval. Third, we perform thorough benchmarking of various large language models on this task and develop a novel method that significantly improves over standard LLM finetuning approaches.\nAll conversations in CliniKnote are simulated and recorded by medical experts to ensure they closely resemble real-life doctor-patient interactions, and the paired notes are also written by real medical experts. This dataset provides a valuable resource for training and evaluating models in clinical note generation tasks.\nUsing this dataset, we build the K-SOAP note format, which adds a keyword section at the top of traditional SOAP notes. This addition allows doctors to quickly recall domain knowledge at a glance. To achieve this, we combine clinical Named Entity Recognition (NER) and Relation Extraction (RE) to extract entities such as symptoms and diseases from the dialogue along with their relationships to the patient (e.g., present or absent). Previous clinical NER datasets like NCBI-disease (Dogan, Leaman, and Lu 2014) and BC5CDR (Li et al. 2016) only contain raw disease entities without relational information, highlighting the need for more comprehensive training resources like CliniKnote.\nFurthermore, we finetune various modern large language models and develop an automatic pipeline to generate K-SOAP notes from the conversations. The workflow of this pipeline is shown in figure 1. We benchmark CliniKnote using various automatic evaluation to assess the quality of the generated notes and their practical applicability in real-life clinical settings. We also investigate the effects of data augmentation, finetuned domain knowledge, and different adapters for various section segmentations. We found that domain knowledge finetuned models with 15 different adapters for each section achieve the best performance. Our results demonstrate that our proposed method significantly reduces time complexity and enhances the performance of automatically generating K-SOAP notes. The time required for reviewing and correcting notes generated by this pipeline is significantly shorter than the original reviewing and writing time."}, {"title": "Related Work", "content": "Various methods have been developed for clinical note generation. For instance, Singh et al. (2023) build large-scale sequence-to-sequence models for generating clinical notes from patient-doctor conversations. The use of pre-trained large language models (LLMs) for this task is increasingly popular (Zhou et al. 2024). Biswas and Talukdar (2024) demonstrates advanced prompting techniques to generate draft clinical notes using LLMs, while Grambow, Zhang, and Schaaf (2022) introduces in-domain pre-training to enhance transformer models' clinical summarization performance. Additionally, the MEDIQA-2023 Dialogue2Note shared tasks (Tang et al. 2023) include finetuning pre-trained dialogue summarization models and using few-shot in-context learning with GPT-4 to address medical dialogue summarization challenges.\nFor publicly available clinical conversation-note datasets, there are several notable examples. Dr.Summarize (Joshi et al. 2020), generated from a telemedicine platform, and the dataset presented by Chintagunta et al. (2021), created using GPT-3, both follow a snippet-summary format. For well-structured SOAP medical notes, ACI-BENCH (wai Yim et al. 2023) and PriMock57 (Korfiatis et al. 2022) feature role-played dialogues with complete SOAP notes but contain only 207 and 57 data points, respectively, which is insufficient for training modern large language models. MTS-DIALOG (Ben Abacha et al. 2023), previously the largest dataset with clinical notes, includes 1,700 doctor-patient conversations (16k turns and 18k sentences) and summarized clinical notes (6k sentences). However, MTS-DIALOG consists of segmented snippets and paired notes that include only some sections rather than full notes.\nNamed Entity Recognition (NER) and Relation Extraction (RE) are essential for extracting structured information from unstructured text in natural language processing. Recent advancements in deep learning, particularly with BILSTM-CRF and transformer-based models like BERT, have significantly improved accuracy in identifying entities across various domains, including the medical field. Notably, recent work combining NER and RE for extracting complex information from scientific texts (Dunn et al. 2022) and utilizing label-supervised Llama finetuning (Li et al. 2023) has achieved state-of-the-art performance. Building on these advancements, our work integrates cutting-edge NER and RE techniques with large language models to enhance clinical note generation from doctor-patient dialogues.\nAs large language models scale up, finetuning and storing all the parameters is prohibitively costly and eventually becomes practically infeasible. Recent advancements in natural language processing have led to the development of parameter-efficient pre-trained large language models. These models aim to reduce the computational and memory requirements while maintaining or even improving performance across various tasks. One notable approach is the use of adapter modules, which allow for the finetuning of a small number of additional parameters while keeping the majority of the pre-trained model fixed (Houlsby et al. 2019). This technique has been proven effective in domain adaptation and multilingual settings. Additionally, methods such as LoRA (Low-Rank Adaptation) (Hu et al. 2021) and Qlora(Quantized Low-Rank Adaptation) (Dettmers et al. 2023) have shown promise in optimizing model efficiency by introducing minimal changes to the original model architecture. These innovations are particularly relevant for applications in the medical domain, where models need to handle large volumes of complex data without incurring prohibitive computational costs. Our work builds upon these foundations by integrating parameter-efficient techniques"}, {"title": "CliniKnote Dataset Assessment", "content": "In this section, we discusse how we synthesize the conversations, SOAP notes, and keyword sections to create our CliniKnote dataset. We also highlight CliniKnote's advantages in terms of scale and structure by comparing with the previous largest public available note dataset MTS-DIALOGUE (Ben Abacha et al. 2023). The appendix shows a full example of CliniKnote."}, {"title": "Data Synthesis", "content": "CliniKnote's dialogue and SOAP note sections are curated in a similar process as described in (Fareez et al. 2022). A team of composed of senior medical students, medical residents, and medical doctors conducted simulated medical conversations. Following the completion of each simulated medical exam, the individual assuming the role of the physician would write a standard SOAP clinical note summarizing the findings, assessment, and medical plan of the simulated visit. Because each exam was simulated, there is no personal health information included in the dataset. The audio of the conversation was initially transcribed using automatic speech recognition (ASR) and then manually validated and corrected to minimize word error rates (WER). Through manual validation the WER of the transcripts were estimated to be approximately 1%.\nCliniKnote's keyword section contains all the symptoms and diseases mentioned in the conversation together with their relations to the patients labeled by GPT-4. Each keyword is prefixed to indicate its relation to the patient. Through the keyword section, we can directly find the useful symptom and disease information without looking at the full medical note. We have 8 types of entities in keyword sections in total, which are PRESENT SYMPTOM, PRESENT DISEASE, ABSENT SYMPTOM, ABSENT DISEASE, FAMILY DISEASE, PAST DISEASE, UNKNOWN DISEASE and UNKNOWN SYMPTOM respectively. The prefix PRESENT and ABSENT describe the relation between this entity and the patient. The prefix FAMILY and PAST mean that the patient has a family history of this entity while the prefix UNKNOWN means we are not sure this entity is present or absent currently and some further tests may need to be done."}, {"title": "Data Augmenting", "content": "During the development of medical exams, the simulations covered a limited number of allergies, thus the dataset lack diversity of the allergy field. Using vanilla CliniKnote for note generation might cause identical allergy generation ignoring the specific patient input. In order to enhance the data diversity of CliniKnote, we augment the allergy section of CliniKnote by randomly sampling more allergies. To be more specific, we first generate a list of 30 common allergies and pick a number from 0 to 4 randomly. Then we randomly pick that number of allergies from the list, feed those allergies and the original dialogue into GPT-4, and prompt GPT-4 to replace the original allergies with the allergies we feed. The prompt is shown in Appendix After replacing the allergies, we do a string comparison to make sure that is the only modified part in the dialogue. Through data augmentation for the allergy part, we have a more complementary version of CliniKnote, which could be generalized for more clinical-related tasks."}, {"title": "Data Statistics", "content": "The final CliniKnote we introduced has 1000 dialogues in training set and 200 dialogues in test set, both paired with the proposed K-SOAP medical notes including the sections of Chief Complaint, History Of Presenting Illness, Past Medical History, Past Surgical History, Family History, Allergies, Social History, Medication, Immunization History, Review of Systems, Vital Signs, Physical Exam, Diagnostic, Assessment, Plan, and keywords. In training set, We have 952,316 words and 489,594 words in medical notes. In testing set, we have 192,116 words in dialogues and 96,458 words in medical notes."}, {"title": "K-SOAP Generation Based On Medical Dialogue", "content": "The complete task aims to generate K-SOAP note based on the input dialogue. We divide this task into two parts. The first part is to generate the traditional SOAP note and the second is to do keyword extraction in NER format. The concatenation of these two parts made up to the final K-SOAP note."}, {"title": "SOAP Clinical Note Generation", "content": "The main task for our CliniKnote dataset is to generate the traditional SOAP clinical notes. To compare the performance of models with different sizes, domain knowledge, and released versions, we selected the following publicly available models to finetune for our benchmarking: qCammel-13b (Toma 2023), a finetuned version of the Llama-2 13B model, trained on a distilled dataset of 15,000 medical domain instructions using QLoRA; Llama2-13b (Touvron et al. 2023); Llama3-8b (AI@Meta 2024); and OpenBioLLM-Llama3-8B (Ankit Pal 2024), which is finetuned on the biomedical domain. Additionally, we include commercial models such as GPT-3.5-turbo and GPT-4 for our benchmarking. We use offical QLORA (Dettmers et al. 2023) to finetune all the large language models. For this supervised learning part, the input is the prompt filled with the original dialogue and the output is the clinical note in CliniKnote dataset reformatted in json mode as it will be more convenient to access. After finetuning, the Parameter-Efficient finetuning (PEFT) adapters (Mangrulkar et al. 2022) will be saved and loaded to do further evaluation."}, {"title": "Keyword Extraction", "content": "Besides generating SOAP content, we further generate the keyword section for each clinical dialogue to reduce the reviewing complexity. We treat this task as a NER task but with relation as the prefix. For keyword extraction, we use Label-supervised unmasked Llama finetuning (Li et al. 2023). Instead of autoregressive generation, we add a label predictor at the top of a Llama-7b model. The label predictor is a one-layer linear feed forward neural network. The dialogue sequences decoded by Llama will be fed into this predictor to predict the labels. We also keep the idea of removing the causal masks because their empirical studies show that using token representations learned with causal masks significantly underperforms in token classification tasks. In order to make this task more understandable to the model, we merge the entities into only 4 types, which are PRESENT, ABSENT, FAMILY and UNKNOWN. To be specific, we set the word to tag dictionary as 'O': 0, 'B-PRESENT': 1, 'B-ABSENT': 2, 'B-FAMILY': 3, 'B-UNKNOWN': 4, 'I-PRESENT': 5, 'I-ABSENT':6, 'I-FAMILY':7, 'I-UNKNOWN':8 while B means start and I means inside. The model will determine whether a word is a symptom or disease and what is the relation with the patient at the same time."}, {"title": "Evaluation of K-SOAP Note Generation", "content": "We present various evaluation metrics on the benchmarking of CliniKnote, to investigate the generated notes' quality and time complexity reduction. We aim to demonstrate the effectiveness of CliniKnote in producing high-quality clinical notes and its potential to enhance the workflow in medical practice by saving time and improving accuracy."}, {"title": "Evaluation Metrics", "content": "The evaluation of clinical note generation is particularly challenging, as it is difficult to accurately measure the relevance and quality of the generated notes due to the complexity and specificity of medical information.\nFor keyword evaluation, we only record and compare the accuracy, precision, recall and F1 score. For SOAP part note evaluation, we employ several metrics to ensure a comprehensive assessment. We start with the most traditional metric, ROUGE (Lin 2004), which evaluates similarity based on common subsequences between the generated and reference texts. In addition to ROUGE, we introduce more advanced metrics: BERTScore (Zhang* et al. 2020) and BLEURT (Sellam, Das, and Parikh 2020) (Pu et al. 2021). These metrics leverage contextual embeddings from BERT to evaluate the semantic similarity between generated and reference texts. BERTScore uses token embeddings to compute precision, recall, and F1 scores, providing a more nuanced and context-aware evaluation compared to traditional metrics. Similarly, BLEURT incorporates pre-trained transformer models to better capture the meaning and fluency of the text, offering insights into the semantic quality of the generated notes. We use Roberta-large model for BERTScore metric and the newest checkpoint BLEURT-20 for BLEURT metric.\nFinally, we include QuestEval (Scialom et al. 2021) in our evaluation framework. Unlike traditional metrics, QuestEval does not require any ground truth references. Compared with ROUGE, QuestEval substantially improves the correlation with human judgments over four evaluation dimensions: consistency, coherence, fluency, and relevance. By incorporating question-answering models, QuestEval evaluates the generated text's ability to provide accurate and relevant information, making it particularly suitable for complex domains like clinical note generation."}, {"title": "Experiments", "content": "In this section, we focus on the performance of various models after finetuning on CliniKnote. We also discuss different finetuning methods for keyword extraction."}, {"title": "Experimental Settings", "content": "We train all the models mentioned in this benchmarking with one A100-80GB GPU. For clinical note generation task, we train all the large language models for two epochs. We use the default setting for question-answer finetuning with QLoRA. We set the learning rate to 0.0002 and the warmup ratio to 0.03. For QLoRA hyperparameters, we set LoRA r to 64 and the loRA \u03b1 to 16 and all the models are quantized in 4 bits. For keyword task, we just use LoRA without quantizing the model. We set batch size = 8, learning rate = 1e-4, Max length = 256, LoRA r = 256, LoRA \u03b1=768, LoRA dropout=0.1. We train Llama2-7b model for 5 epochs."}, {"title": "Clinical Note Generation Results", "content": "In this section, all models are finetuned and tested in 3 modes, which are full note mode, section-4 mode and section-15 mode that contain different numbers of LORA adapter (Hu et al. 2021). Full note mode means we only train one adapter to generation the full clinical note. Section-4 mode means we train four different adapters to generate the full note. How we divide the four parts is a little different from how we separate Subjective, Objective, Assessment, and Plan parts. We only have Chief Complaint and History Of Presenting Illness in the first part. The second part contains Past Medical History, Past Surgical History, Family History, Allergies, Social History, Medication List, Immunization History and Review of Systems. The third part contains Vital Signs and Physical Exam while the final part contains Assessment, Diagnosis and Plans. Section-15 mode means we train 15 different adapters to generate all the fifteen sections contained in the note separately. As for commercial models, we try full note zeroshot generation and oneshot(one demo in prompt) in-context learning.\nFor baseline models, we choose Bart and Biobert as baseline models. Bidirectional autoregressive transformer Bart is the SOTA transformer model for note generation. For matching clinical tasks, we use the continued pre-training on PubMed abstract model of BART-large version. We further compare the performance of BioBart, which has the same model structure as BART but possesses distinct tokenizers and vocab- ulary size.\nshows the performance of all the models with the automatic metrics we pick. qCammel-13b-section-15 achieves overall best performance on ROUGE and BERTScores among all open-scouce models while GPT-40-oneshot is considered the best among all commercial models. To our surprise, section-15 mode does not show significant enhancement on the performance even though it takes 11 and 14 more adapters than other modes. All three modes of Llama3-8b show decent results. The section-4 mode of Llama3-8b shows second best peformance on ROUGE score and full mode has highest score on Bleurt, QuestEval and second highest score on Bertscore among all open-source models. However, OpenBioLLM-Llama3-8b has poor performance on almost all metrics. All three modes of OpenBioLLM-Llama3-8b shows lowest scores on ROUGE, BertScore, Bleurt and QuestEval, even lower than baseline models. The domain knowledge insertion triggers decreasing on information extraction ability. The benchmarking shows that appropriate in-domain instruction is beneficial while it could also jeopardize the model's original extraction ability."}, {"title": "Keyword Extraction Results", "content": "We use BioBart, SciBert, PubMed, BlueBert and Bert as baseline models for key words extraction task. For baseline models, we finetune the pretrained models on CliniKnote dataset. The learning rate is set to 1e-4, batch size is set to 128 and we train all baseline models for 10 epochs. Besides the label-supervised Llama finetuning, we also tried in-context learning and question-answer based finetuning on the same Llama2-7b model. shows the results among all the models and Llama2-7b with our label-supervised finetuning dominates all the metrics except for Precision and has the lowest EvalLoss. The precision of Llama2-7b-label-supervised is slightly lower than that of biobert. Neither in-context learning nor QA-based finetuning on Llama2-7b has decent performance on this task since it combines RE as prefix and makes it harder to predict for autoregressive LLM. Table 6 shows two demos of the Llama2-7b-label-supervised's prediction and the ground truth. Label-supervised finetuning enhances the entity recognition ability of large language models while maintaining their comprehension skills. The results also demonstrate that raw llms cannot handle this keyword extraction task with decent performance, which indicates the necessity of the training resources provided in CliniKnote keyword section."}, {"title": "Conclusion", "content": "In this study, we addressed the critical task of automatic clinical note generation to enhance the efficiency and accuracy of medical documentation. We introduced the K-SOAP format, which includes keywords at the top of traditional SOAP notes for quicker review and comprehension. Additionally, we presented CliniKnote, a large-scale dataset containing complex dialogues paired with K-SOAP notes. Training large language models on CliniKnote demonstrated substantial improvements in generating accurate clinical notes. Our comprehensive evaluation ensured a thorough assessment of the generated notes, making the assessment of the quality of generated notes more straightforward and robust. This work advances clinical note generation, driving further research and saving doctors time in documentation tasks."}, {"title": "Limitations", "content": "Although CliniKnote is created by experts to simulate real-life clinical inquiries, there are still robustness issues as 100% of the data in our dataset are clean. In real-life conversations, ASR systems may not always produce clean transcripts due to various factors such as accents, interjections, and expression variations. These factors pose significant challenges for the robustness of clinical note generation.\nAdditionally, the range of models used in this benchmarking is relatively narrow. Many open-source models with larger sizes or better configurations may be suitable for generating K-SOAP notes, indicating significant potential for improvement in this pipeline.\nFinally, the current evaluation metrics do not perfectly reflect the quality of clinical notes, as they struggle to capture key information and crucial errors in the medical domain. We hope that more advanced evaluation metrics for text summarization and note generation will be developed to address this issue. Moreover, human-expert evaluation still need to be completed. We are planning a more extensive evaluation for both CliniKnote and real-life clinical conversations with finetuned models to assess accuracy and the time savings for doctors in real clinical scenarios."}, {"title": "Ethics Statement", "content": "No protected health information were used in the creation of this dataset."}]}