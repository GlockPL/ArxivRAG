{"title": "DEBUGAGENT: EFFICIENT AND INTERPRETABLE ERROR SLICE DISCOVERY FOR COMPREHENSIVE MODEL DEBUGGING", "authors": ["Muxi Chen", "Chenchen Zhao", "Qiang Xu"], "abstract": "Despite the significant success of deep learning models in computer vision, they often exhibit systematic failures on specific data subsets, known as error slices. Identifying and mitigating these error slices is crucial to enhancing model robustness and reliability in real-world scenarios. In this paper, we introduce DebugAgent, an automated framework for error slice discovery and model repair. DebugAgent first generates task-specific visual attributes to highlight instances prone to errors through an interpretable and structured process. It then employs an efficient slice enumeration algorithm to systematically identify error slices, overcoming the combinatorial challenges that arise during slice exploration. Additionally, DebugAgent extends its capabilities by predicting error slices beyond the validation set, addressing a key limitation of prior approaches. Extensive experiments across multiple domains including image classification, pose estimation, and object detection show that DebugAgent not only improves the coherence and precision of identified error slices but also significantly enhances the model repair capabilities.", "sections": [{"title": "1 INTRODUCTION", "content": "Deep learning models have made substantial progress in computer vision tasks. However, they still exhibit systematic failures on critical subsets of data (Buolamwini & Gebru, 2018), known as \"error slices\". In high-stakes applications like healthcare (Giger, 2018) and autonomous driving (Fujiyoshi et al., 2019; Breitenstein et al., 2021), identifying error slices is crucial to improving model robustness and ensuring safety. At the same time, uncovering error slices in widely-used public models, such as CLIP (Radford et al., 2021) and BLIP (Li et al., 2022a), is also important, as these models are applied across various tasks by a large number of users.\nIdentifying coherent error slices subsets of failure samples that share common visual attributes \u2014 is challenging due to the lack of detailed visual attribute annotations in most evaluation datasets. Previous works (d'Eon et al., 2022; Yenamandra et al., 2023) typically attempt to identify error slices by clustering failure samples within an embedding space, and relying on human experts to manually annotate coherent slices. Some approaches (Eyuboglu et al., 2022; Jain et al., 2022) incorporate captioning models to assist with slice annotation. We refer to these approaches as \u201cslice-then-tag\" methods, where error slice identification is followed by descriptive tag generation. However, these methods often struggle to ensure the coherence of error slices (Gao et al., 2023; Johnson et al., 2023) due to the entangled embedding space (Chen et al., 2024). This makes it difficult for humans to interpret the slices or to conduct efficient model repair.\nTo address these challenges, particularly with the rise of multi-modal models, a new line of work (Gao et al., 2023; Chen et al., 2024; Liang et al., 2024; Metzen et al., 2023) has emerged that prioritizes visual attribute generation before slice discovery. For example, AdaVision (Gao et al., 2023) iteratively uses GPT (OpenAI, 2023) to establish potentially critical scenarios and retrieves"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 ERROR SLICE DISCOVERY", "content": "Error slice discovery refers to the process of identifying groups of failure cases in model predictions, akin to failure analysis in engineering disciplines. It helps engineers pinpoint a model's weaknesses and subsequently improve its performance. Due to its practical relevance, interpretability is a key requirement in error slice discovery. The discovered failure groups must be coherent, meaning they share similar and interpretable visual attributes.\nSlice-then-tag methods: Because of the absence of visual attribute annotations, early methods for error slice discovery (Eyuboglu et al., 2022; Jain et al., 2022; d'Eon et al., 2022; Yenamandra et al., 2023) typically cluster failure samples in an embedding space and then rely on human experts or"}, {"title": "2.2 VISUAL ATTRIBUTE AND TAG GENERATION", "content": "Visual attribute refers to a specific visual characteristic (e.g., \u201cobject pose\u201d), while tags are the possible values that describe the attribute (e.g., \u201cstanding\u201d, \u201clying down\"). Visual attribute and tag generation is fundamental to tag-then-slice approaches, providing the basis for identifying and analyzing error slices. Despite its importance, this area remains underexplored in error slice discovery. Existing methods often rely on human experts (Metzen et al., 2023; Jain et al., 2022; Eyuboglu et al., 2022) or directly query models like GPT with simplistic prompts (Chen et al., 2024), which are inadequate for the complexity of this task.\nIn other domains, attribute and tag generation has been extensively studied, such as in image tagging and interpretable image classification. However, these methods are not applicable to error slice discovery. Image tagging models (Zhang et al., 2024; Chen et al., 2023) are designed to identify image content and generate image-specific tags; however, the significant variation in these tags across dif-ferent images limits the ability to form coherent data slices. Similarly, approaches in interpretable image classification (Yang et al., 2023; Yan et al., 2023) generate attributes to differentiate between classes, whereas error slice discovery requires attributes that capture failure patterns and cause con-fusion between image classes, presenting a unique challenge."}, {"title": "3 METHOD: AUTOMATIC ATTRIBUTE AND TAG GENERATION", "content": "We present the workflow of DebugAgent in Figure 1. Attribute and tag generation serves as the foundation of DebugAgent, as it is closely linked to the coherence and coverage of error slices. Our attribute and tag generation consists of the following steps: attribute generation, tag determi-nation, and dataset-wide tag assignment. It addresses several key challenges identified in existing approaches."}, {"title": "3.1 KEY CHALLENGES IN ATTRIBUTE AND TAG GENERATION", "content": "Current methods (Chen et al., 2024; Liang et al., 2024; Eyuboglu et al., 2022) for generating at-tributes and tags for image datasets exhibit several critical shortcomings:\nNarrow Attribute Focus: Existing methods primarily concentrate on attributes related to the main objects of interest in the images, often overlooking crucial contextual factors such as background properties and global image characteristics. Furthermore, these attributes tend to be general rather than specifically tailored to error-related and task-specific needs.\nInconsistent and Biased Tagging: Tags are generated directly from the data without exter-nal references. Due to the biases of the data, the generated tags often have inconsistencies in granularity and semantics for the same attribute."}, {"title": "3.2 STRUCTURED AND COMPREHENSIVE GENERATION", "content": "To address these challenges, DebugAgent leverages the strengths of multi-modal models (in this paper, we use GPT (OpenAI, 2023)) for attribute and tag generation, combined with a structured process to ensure the accuracy, consistency, and coverage of the results."}, {"title": "3.2.1 ATTRIBUTE GENERATION", "content": "To effectively capture the properties related to error slices, the generated attribute set is required to cover a diverse range of image characteristics that influence model performance.\nThrough interviews with engineers and a detailed analysis of common errors in classification and object detection models, as illustrated in Figure 2, we identify two primary types of model errors: errors caused by data distribution issues (e.g., rare cases, distribution shifts, spurious correlations, etc), and errors resulting from inherent task difficulties (e.g., occlusions, small object sizes, low image resolutions, etc). In response to these error sources, we categorize attributes into three key types according to the subjects they refer to: main object, background, and global. This structured categorization ensures that we capture not only the features of the primary object but also essential contextual and global information of the images.\nMain object attributes capture properties that are directly related to the objects of interest, such as their shapes and colors. These attributes are crucial in tasks such as classification, where the model primarily focuses on distinguishing features of the main objects. Background attributes refer to elements surrounding the main objects, such as the environment and distracting objects in the scene, which are particularly important in tasks such as object detection. Global attributes describe overall"}, {"title": "3.2.2 TAG DETERMINATION AND ASSIGNMENT", "content": "Once the attributes are determined, the next step is to generate consistent and meaningful tags for each attribute. A common issue in previous works is the inconsistency in tag generation in terms of both granularity and alignment with the semantic definitions of the attributes. To mitigate this issue, our method employs a multi-stage refinement process that operates as follows.\nDebugAgent begins by generating an initial list of potential and unbiased tags for each attribute, establishing the semantic scope and granularity. To ensure that the generated tags are concrete and unambiguous, we employ clear conventions: for binary attributes, such as \u201cwhether an object is occluded\", we adopt a straightforward \u201cyes/no\u201d tag format, while for open-ended attributes, De-bugAgent generates a descriptive set of tags. Next, DebugAgent collects new tags by reviewing a subset of validation data, incorporating additional tags as needed to ensure comprehensive cover-age of variations. Once the attributes and tags are finalized, DebugAgent assigns these tags to all images within the dataset, ensuring that each image receives a tag corresponding to each attribute. This multi-stage process promotes consistency in tags throughout the dataset, thereby enhancing the robustness of subsequent analyses. We discuss DebugAgent's scalability and potential issues regrading attribute and tag generation in Appendix A.8.\""}, {"title": "4 \u041c\u0415\u0422\u041dOD: EXPLORING DATA SLICES", "content": ""}, {"title": "4.1 EFFICIENT SLICE DISCOVERY AND REPAIR", "content": "With the generated attribute and tag sets, we define \"data slices\" as data subsets that share a tag or a combination of tags from several attributes. Slice enumeration aims to comprehensively enumerate data slices, forming the foundation for error analysis and subsequent model repair."}, {"title": "4.1.1 BASIC NOTATIONS", "content": "Let $A = \\{a_1,a_2,...,a_n\\}$ be a set of attributes, where each attribute $a_i$ has a corresponding set of possible tags $T_i = \\{t_{i1}, t_{i2},..., t_{im_i}\\}$. Slice $S$ is defined as a combination of tags from these attributes, where each attribute contributes one tag to the slice:\n$S = \\{(\\alpha_1, t_{1j1}), (a_2, t_{2j2}),..., (a_k, t_{kjk})\\}$, with $t_{ij} \\in T_i, k \\leq n$"}, {"title": "4.1.2 KEY CHALLENGES IN SLICE ENUMERATION", "content": "Let the number of tags within a set $T_i$ be denoted as $|T_i|$. The primary challenge in slice enumeration arises from the combinatorial explosion of potential slices, which is upper-bounded by the term:"}, {"title": "4.1.3 PROPERTIES OF DATA SLICES", "content": "Data slices are characterized by two key properties: the average model performance and the data count. The average model performance (e.g., average accuracy in image classification, average object keypoint similarity (OKS) in pose estimation, and average intersection-over-union (IoU) in object detection) is essential for identifying error slices, as it reflects the model's performance on a specific subset of the data. The data count indicates the prevalence of the slice and is related to the reliability and generalization of the error pattern. In general, a model's performance on slices with larger data counts is more likely to generalize to unseen data belonging to those slices.\nMonotonicity: During enumeration, we define monotonicity by comparing a slice $S$ with its parent slice $S_p$, where $S_p \\subset S$. The average model performance is non-monotonic, as it may increase or decrease when moving from a parent slice to a child slice. In contrast, the data count is monotonic, as it always decreases or remains constant from parent to child slices."}, {"title": "4.1.4 ALGORITHM DESIGN", "content": "Breadth-First Tree-structured Enumeration. To address the above challenges, we propose a breadth-first tree-structured enumeration process. We establish a tree based on the generated at-tributes and tags, in which the slices stored in child nodes are formed by adding one attribute-tag pair to the slices stored in their parent nodes. The parent-child relationships of slices can then be represented by the parent-child relationships of nodes, and the numbers of attributes in the slices grow with the tree's depth. Note that a child node may correspond to multiple parent nodes in the tree. Crucially, since the data count monotonically decreases with deeper layers, data enumeration of a slice can be upper-bounded by its parent slices, significantly reducing the search space. We em-ploy breadth-first search (Bellman, 1958) (BFS) to ensure that parent slices are enumerated before child slices.\nPruning and Intersection. Uninformative slices, particularly those with low data counts, offer limited insight into model errors. Since the data count decreases monotonically with deeper layers, we can safely prune subtrees with the data counts of the root nodes (i.e., slices) fewer than $M$ (in our experiments, $M = 10$).\nSimilarly, due to the monotonicity of slice data counts, a necessary condition for any informative slice is that all of its parent slices must be retained. Therefore, rather than generating all possible slices for each new layer, we intersect the slices that survive pruning from the previous layer to form new candidates. We define two slices in the same layer as a matched pair if they share $k - 1$ attributes, where $k$ is the number of attributes in the slices of the current layer. By intersecting these matched slice pairs, we only maintain new slices that are likely to yield informative insights. Additionally, we use hash tables for fast-matched pair search and matrix multiplication to speed up data counting and accuracy calculations.\nPost-processing. Unnecessary tags that are not related to errors can be confusing when presented in error slices. Therefore, after enumeration, we conduct post-processing to remove slices that have higher average model performance than their parent slices. Notably, slice enumeration is dataset-specific and only needs to be executed once for all models on the same dataset, whereas the post-processing step must be performed individually for each model.\nIntegration with Model Repair. We incorporate image querying techniques for model repair. After post-processing, DebugAgent first ranks the slices based on their average model performance. Given a data pool, DebugAgent then assigns tags to the data and prioritizes those corresponding to error slices with the lowest average performance. This ensures that the most critical cases are addressed first, effectively targeting the model's weakest points for repair."}, {"title": "4.2 PREDICTING ERROR SLICES BEYOND THE VALIDATION SET", "content": "The validation set may not capture all potential error types and their corresponding data slices, leaving some high-risk slices unfixed in data repair. To mitigate this, we propose two strategies for predicting potential error slices. The predicted slices will serve as complements of discovered error slices when the validation set is limited in size.\nTag Substitution. We compute the text embeddings for all tags using CLIP (Radford et al., 2021). For each identified error slice, one of its tags is substituted with another tag from the same attribute that has the closest feature distance. This method is akin to data augmentation, allowing exploration of nearby regions in the feature space from the existing identified regions.\nInstruction-Based Method. We utilize few-shot learning with GPT to predict potential error slices based on the provided attributes and tags, following task-specific instructions. These instructions are similar to those used for generating error-related attributes. For instance, in image classifica-tion tasks, we instruct the model to generate slices that blur boundaries between closely related categories. In object detection and pose estimation tasks, we focus on generating slices prone to localization errors or occlusions. Unlike Tag Substitution which is built upon the model's identified error slices, the instructions-based method leverages GPT's extensive knowledge base without any prior information about the model's errors."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 COMPARISONS OF ATTRIBUTE AND TAG GENERATION", "content": "In this section, we present detailed comparisons of the attributes and tags generated by different error slice discovery techniques. All results are generated without human intervention. As illustrated in Figure 3, when applied to an image of a van in an object detection scenario, Domino produces overly generic descriptions such as \u201cvan\u201d or \u201cdelivery\u201d, offering little insight into the visual characteristics or potential error patterns. Furthermore, the unstructured nature of these tags limits their usefulness for systematic error analysis. HiBug improves upon this by generating more structured tags, such as \"color: white\" and \"make: van\", but it focuses primarily on surface-level characteristics, neglecting important background and global attributes.\nIn contrast, our method generates attributes and tags that are both task-specific and highly relevant to error slice discovery. We capture not only essential object properties such as \"object shape: van\" and \"object color: white\" but also more nuanced details, such as \"is object damaged: no\" and \"object visibility: fully visible\". Additionally, our method considers environmental factors and image quality indicators, such as \"background clutter: high\u201d and \u201cimage sharpness: high\", which are crucial for diagnosing model failures in real-world scenarios.\nMoreover, in the context of error slice discovery, precise dataset annotations are critical. We observe that attributes generated by HiBug are often ambiguous, leading to inconsistent tag semantics across the dataset. For instance, tags under \"wheel\" refer to both wheel shape and color, creating confu-sion. In contrast, our method generates clear, structured attributes and tags, ensuring consistency in both semantics and granularity. Overall, the attributes and tags generated by our method are more effective for model debugging and refinement compared to existing approaches. We provide a full list of the generated attributes and tags by our method and the baselines in Appendix A.2"}, {"title": "5.2 EFFECTIVENESS OF SLICE ENUMERATION", "content": "We conduct experiments to evaluate the performance of our slice enumeration algorithm in com-parison with both a naive enumeration approach and a baseline version of the breadth-first tree-structured algorithm. The naive approach is a brute-force method that lists all possible data slices and searches for matching data for each slice, as described in Section 4.1.3. The breadth-first tree-structured baseline refers to a simplified version of our slice enumeration algorithm, without pruning and intersection. We present the algorithms of the three methods in the Appendix A.7.\nAs shown in Figure 4, our method achieves approximately 115x and 510x speedups over naive enu-meration, and 12x and 7x speedups compared to the baseline tree-structured algorithm, for the enu-meration of slices with 3 and 4 attributes respectively. These significant improvements in efficiency allow for rapid slice enumeration across multiple attributes, substantially reducing the computational time required for model analysis in real-world scenarios. Additionally, we conduct an ablation study focusing on the enumeration of slices with 3 attributes to assess the effect of the data volume and total number of attributes on runtime. The results in Figure 5 demonstrate that runtime increases linearly with the data volume, while remaining feasible even in cases where tasks involve up to 72 attributes."}, {"title": "5.3 IDENTIFIED ERROR SLICES", "content": "We conduct experiments across three tasks - image classification, pose estimation, and object de-tection to evaluate our method's ability to identify error slices. For image classification, we select five bear species from ImageNet (Deng et al., 2009) and debug three models: ResNet18 (He et al., 2016), CLIP (Radford et al., 2021), and BLIP (Li et al., 2022a). For pose estimation, we use an industrial private dataset and debug the tiny, small, medium, and large variants of RTM-Pose (Jiang et al., 2023). For object detection, we use the \"Car\" and the \"Pedestrian\" instances of the KITTI (Geiger et al., 2012) dataset and debug four models: YOLOv8 (Varghese & Sambath, 2024), CO-DINO (Zong et al., 2023), ViTDet-L (Li et al., 2022b), and RTMDet-X (Lyu et al., 2022). We apply DebugAgent to automatically generate attributes and tags and perform slice enumeration, followed by error slice analysis based on the results. In the main paper, we consider slices with three attributes.\nError slices are defined as slices with average performance value (i.e. accuracy, OKS and IoU for the three tasks) lower than the overall model performance by a constant C (in our experiments, $C = 0.2$). For image classification, we identify 1086, 499 and 384 error slices for ResNet18, CLIP and BLIP respectively. Figure 6 showcases error slices of these models, revealing model-specific weaknesses. For example, the first slice for \u201cteddy bears\" suggests that ResNet18 struggles with distinguishing \u201cwhite\u201d and \u201cnot holding item\u201d teddy bears with \u201cpolar bears\u201d. Similarly, the third and fourth slices shows the potential dependency of CLIP and BLIP on colors in classifying bears. For pose estimation, we identify 11357, 5159, 2259, and 2053 error slices for the tiny, small, medium, and large RTMPose models respectively, with common errors arising when people are lying down, wearing black clothes, or crossing their legs. For object detection, we identify 4808,\""}, {"title": "5.4 PREDICTIONS OF UNSEEN SLICES", "content": "In this experiment, we use ResNet18, RTMPose-Tiny, and YOLOv8 respectively for the image clas-sification, pose estimation, and object detection tasks. We begin by obtaining the predicted error slices using both the tag substitution and instruction-based methods across all the three tasks. These methods generate 100, 20, and 40 slices respectively for image classification, pose estimation, and object detection (with 20 slices per class; pose estimation having only the person class). Subse-quently, we compute the model's performance on the predicted slices.\nThe results presented in Table 1 demonstrate that the model's average performance on these pre-dicted error slices is significantly lower than its overall performance. This highlights the effective-ness of our approach in predicting extra errors slices, which is particularly valuable when the valida-"}, {"title": "5.5 MODEL REPAIR", "content": "In this section, we evaluate the model repair capabilities across the three previously discussed tasks, focusing on querying new data based on the identified error slices for model improvement. For each task, we construct a query set from which additional data is selected, and the model is evaluated on a hold-out test set, both distinct from the validation set used for identifying error slices. We compare DebugAgent with HiBug (Chen et al., 2024), which is also an automated method, as well as random data selection. For a fair comparison, we implement the same data selection strategy for both DebugAgent and HiBug, prioritizing data that corresponds to slices with the lowest average model performance. The model we repair for the three tasks are respectively ResNet18, RTMPose-Tiny, and RTMDet-X.\nWe summarize the model's improvements in terms of accuracy, keypoint average precision (AP), and object mean average precision (mAP) for the three tasks in Table 2. DebugAgent consistently outperforms other methods; notably, it enhances model performance when random data selection yields only marginal improvements. This underscores its effectiveness in model repair."}, {"title": "6 DISCUSSION", "content": "Our experiments demonstrate that DebugAgent significantly advances error slice discovery. It im-proves both the coherence and coverage of identified data slices, which leads to a more interpretable and insightful error analysis process. Meanwhile, the efficient slice enumeration algorithm allows for the rapid discovery of slices across multiple attributes, enabling a more granular analysis of model errors.\nHowever, there are some limitations associated with DebugAgent. The attribute and tag generation process relies on GPT, which can occasionally produce errors. A primary concern might be the impact of incorrect tag assignments. Though such errors might slightly affect the coherence of error slices, they are unlikely to influence overall identification, as a few misclassified data points do not alter the average performance of a slice. We further discuss several potential issues and the scalability of DebugAgent in Appendix A.8.\nMoreover, we observe that existing slice discovery methods often follow diverse workflows, com-plicating direct and equitable comparisons. Future work can focus on developing a standardized and widely applicable benchmark to facilitate fair evaluations and drive progress in this area."}, {"title": "7 CONCLUSION", "content": "In this paper, we introduce DebugAgent, a comprehensive framework for efficient and interpretable error slice discovery aimed at enhancing model debugging and repair. By leveraging task-specific"}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 SCALABILITY AND ROBUSTNESS OF DEBUGAGENT", "content": "We address potential concerns regarding the scalability and robustness of DebugAgent, particularly focusing on the attribute and tag generation process. We hope that this discussion provides additional clarity and insight into the scalability and robustness of DebugAgent. We will also add alternative versions of DebugAgent with reliance on other multi-modal models, such as LLaVA (Liu et al., 2024) and QWen-VL (Bai et al., 2023), in the future."}, {"title": "A.1.1 ENSURING CORRECT ATTRIBUTE AND TAG GENERATION:", "content": "Although our method leverages GPT (OpenAI, 2023) for generating attributes and tags, we imple-ment several mechanisms to ensure correctness. First, we have designed an extensive set of rules to validate the generated outputs and handling exception scenarios. For example, during dataset-wide tagging, we verify that the names of the generated attributes and tags align with the predefined cat-egories and that each tag belongs to the appropriate tag set. Additionally, we introduce the tag \"not visible\" for attributes related to elements that may not be present in every image, such as background features, thereby enhancing flexibility and accuracy in handling various scenarios.\nSecond, we employ self-correction loops to refine the generation process. For instance, during attribute generation, we instruct GPT to validate the generated attributes, ensuring there is no con-ceptual overlap or inappropriate attributes for the objects of interest at hand.\nFinally, our prompting strategy incorporates a few-shot approach, featuring carefully curated ex-amples (distinct from the cases in our experiments). We have observed that this few-shot strategy significantly improves performance. During our experiments, we encountered minimal issues re-garding the correctness of the outputs."}, {"title": "A.1.2 INFLUENCES OF TAGGING BIASES:", "content": "The tagging process is analogous to a multi-label classification task. Although GPT-4 generally performs well, occasional tagging errors may occur. These errors might slightly affect the coherence of certain error slices, but they are unlikely to impact the overall identification process. Since slice identification is rooted in statistical analysis, the presence of a few misclassified data points typically does not alter the average performance of a slice."}, {"title": "A.1.3 HANDLING UNNECESSARY ATTRIBUTES:", "content": "It is difficult to assess the necessity of specific attributes prior to the error slice discovery phase. DebugAgent generates a substantially larger set of attributes than existing methods, some of which may not directly align with human recognition. However, only after analysis can the utility of these attributes be determined. For instance, in our classification analysis using BLIP, an attribute such as \"is man-made object presented\" may initially appear irrelevant. Yet, error slice discovery reveals that BLIP tends to misclassify black bears as sloth bears when man-made objects, such as the iron fence of the zoo, is present in the image. Furthermore, we have developed a post-processing algorithm that effectively removes unnecessary attributes during the error slice discovery stage, ensuring that only the relevant attributes are retained for slice analysis."}, {"title": "A.1.4 GENERALIZATION TO DIFFERENT TASKS AND DATASETS:", "content": "Extending DebugAgent to other tasks primarily involves adjusting the attribute and tag generation process, particularly the task-specific prompts used for generating relevant attributes. We provide prompt templates for easy task extension, allowing users to extend our method by simply modifying task descriptions. For different datasets within the same task, the primary variation lies in the change of the main object. Through additional experiments on the full ImageNet and COCO detection datasets (not included in the paper), we observe that DebugAgent generalizes well across a wide range of objects, demonstrating its scalability and flexibility for various tasks and datasets."}, {"title": "A.1.5 TIME COST OF DEBUGAGENT.", "content": "The most time-consuming part of DebugAgent is the process of tag assignment for all the images in the dataset, which typically accounts for over 95% of the total time, with larger datasets increasing this proportion. It operates with O(N), where N is the number of images in a dataset, because GPT-4V generate all attributes for one image in one step. In our experiments, a single query takes around 6 seconds. We implement parallel processing to accelerate these processes, since the tag assignments for different data are independent. For a dataset of 10,000 images, if we use 50 threads to accelerate the labeling process, it typically takes around 30 minutes to run DebugAgent. It is also worth noting that attribute and tag generation and slice enumeration only need to be performed once for a dataset. When iteratively improving the performance of a model, only the first round requires labeling and slice enumeration. Subsequent iterations only require the post-processing steps described in Section 4.1.4, which takes only a few seconds."}, {"title": "A.1.6 THE DIFFICULTIES IN EVALUATION.", "content": "Unlike other fields, we cannot find a standardized evaluation process for error slice discovery meth-ods. Studies often design custom-built datasets and conduct limited comparisons. We identify two key challenges:\nDifferences in workflow. Even when methods aim for similar goals, as mentioned in sec-tion2.1, some methods rely heavily on human analysis with LLM-assisted slice discovery, while others cluster data before human labeling. Approaches like ours and HiBug, however, label the data first and then discover slices based on attribute clustering. These workflow differences make it challenging to establish suitable baselines for evaluating each compo-nent of the method.\nError slices have no ground truths. Error slices require shared human-understandable at-tributes, but even for the same group of data, different individuals may define and label attributes in vastly different ways(Johnson et al., 2023)."}, {"title": "A.1.7 ARE ERROR SLICES DISTINCT BUGS?", "content": "In this paper, error slices are defined based on the combination of tags, and therefore, slices may overlap in some tags. While error slices are distinct in terms of their tag combinations, they do not necessarily correspond to distinct bugs. Fixing one error slice may improve the model's performance on other related error slices as well.\nTo test this, we designed a simple experiment to check the impact of fixing one error slice on others. For the ResNet model used in the classification task described in the main paper, we chose an error slice corresponding to class teddy bear, defined as (object color: white, object pose: sitting). We then queried the data matching this error slice (20 images) from a hold-out dataset. We fine-tuned the model on this data for 20 epochs and evaluated its accuracy on the validation set. We collected the model's accuracy on three types of data: (1) Data matching the selected error slice. (2) Data belonging to error slices that overlap with the selected slice (e.g., (object color: white, xxx), (xxx, object pose: sitting)). (3) Data belonging to error slices that does not overlap with the selected slice.\nThe results in Table 3 reveal a significant improvement in the model's performance on the fixed error slice. Interestingly, performance on overlapping error slices also improved, suggesting a potential relationship between these slices and shared model weaknesses. However, performance on non-overlapping data decreased slightly, likely due to overfitting to the specific distribution of the fine-tuning data. This experiment suggests that while error slices are defined by distinct tag combinations, they are not distinct bugs."}, {"title": "A.2 GENERATED ATTRIBUTES AND TAGS BY DEBUGAGENT AND THE BASELINE METHODS", "content": "We present a full list of attributes and tags generated by DebugAgent and HiBug for pose estima-tion in Table 4 and Table 5. The attributes generated by HiBug primarily focus on the main object, often neglecting task-specific requirements and potential errors. Furthermore, the tags frequently lack semantic consistency; for example, the tags associated with the attribute \"age\" include over-lapping terms such as \"young\u201d, \u201cteen\u201d, and \u201c12\u201d. Similarly, the tags for \"skin tone\" combine both race and color categories, resulting in semantic ambiguity. This inconsistency can adversely af-fect the data slicing process, leading to images with similar semantics being assigned to different slices (e.g., slice \u201cteen\u201d versus slice \"young\"). In contrast, the attributes and tags generated by De-bugAgent are specifically tailored to tasks and errors, ensuring semantic consistency. This makes DebugAgent more effective for model debugging and refinement.\""}, {"title": "A.3 DETAILED EXPERIMENTAL SETUP OF SLICE IDENTIFICATION", "content": "For image classification, we combine the original training and validation sets of ImageNet. Our error slice identification focuses on the last 850 images per class, while the first 500 images are used to train ResNet-18. CLIP (ViT-H-14) and BLIP are public models that have not been explicitly trained on ImageNet. For these models, we perform zero-shot classification by comparing image features with text features representing class names.\nFor pose estimation, our pose dataset contains 47,057 images (24,832 from COCO, with the remain-der from a private source), primarily used for rehabilitation training in hospitals to recognize patient movements and assess whether exercises meet required standards. In our experiments, models are pre-trained on the COCO portion. For error slice discovery, we use 7,057 images from the private portion as the validation set. For model repair, we select 1,241 images from the private portion.\nFor object detection, YOLOv8 is trained on the first 5000 images of the KITTI training set. The other models are pretrained on COCO and are set to focus only on car and pedestrian predictions. The remaining 2481 images from the dataset are used for error slice identification."}, {"title": "A.4 MORE CASES OF THE IDENTIFIED ERROR SLICES", "content": "We present additional visualizations of the identified error slices, along with some noteworthy ob-servations in Figure 8, Figure 10 and Figure 12. For instance, in Figure 8, we observe that BLIP (Li et al., 2022a) struggles to correctly classify most black bears whose fur color closely resembles brown. This suggests that the model's predictions may rely heavily on color rather than on biologi-cal features such as ear shape, body size, or the curvature of the bear's back. This poses significant challenges for researchers developing animal classification applications based on BLIP."}]}