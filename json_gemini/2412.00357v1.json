{"title": "Safety Alignment Backfires: Preventing the Re-emergence of Suppressed Concepts in Fine-tuned Text-to-Image Diffusion Models", "authors": ["Sanghyun Kim", "Moonseok Choi", "Jinwoo Shin", "Juho Lee"], "abstract": "Fine-tuning text-to-image diffusion models is widely used for personalization and adaptation for new domains. In this paper, we identify a critical vulnerability of fine-tuning: safety alignment methods designed to filter harmful content (e.g., nudity) can break down during fine-tuning, allowing previously suppressed content to resurface, even when using benign datasets. While this \"fine-tuning jailbreaking\" issue is known in large language models, it remains largely unexplored in text-to-image diffusion models. Our investigation reveals that standard fine-tuning can inadvertently undo safety measures, causing models to relearn harmful concepts that were previously removed and even exacerbate harmful behaviors. To address this issue, we present a novel but immediate solution called Modular LoRA, which involves training Safety Low-Rank Adaptation (LoRA) modules separately from Fine-Tuning LoRA components and merging them during inference. This method effectively prevents the re-learning of harmful content without compromising the model's performance on new tasks. Our experiments demonstrate that Modular LoRA outperforms traditional fine-tuning methods in maintaining safety alignment, offering a practical approach for enhancing the security of text-to-image diffusion models against potential attacks. Disclaimer: This paper contains harmful imagery and description. Reader discretion is advised.", "sections": [{"title": "1. Introduction", "content": "Fine-tuning text-to-image diffusion models has become a widespread practice for personalization and domain adaptation, enabling users to tailor models to specific styles or content [8, 57, 62, 63, 66, 68]. However, this prevalent practice exposes a critical vulnerability in the safety alignment of these models [3, 19, 83]. Safety alignment methods on text-to-image diffusion models are designed to suppress or remove harmful content-such as nudity or violent imagery-from the generated outputs [35, 65, 88]. While effective on pre-trained models, we found that such methods can fail after post-hoc fine-tuning, resulting in the unintended reappearance of suppressed harmful content, even when fine-tuning with benign datasets.\nThis phenomenon, often referred to as fine-tuning jailbreaking, has been observed and studied in large language models (LLMS) [59], but it remains underexplored in the context of diffusion models [18, 29, 40, 81, 84]. The re-emergence of harmful content poses significant ethical and legal concerns, undermining the safety measures put in place to prevent the generation of such content. Users who fine-tune models for benign purposes may unknowingly generate inappropriate or offensive content, which can lead to serious repercussions in real-world scenarios [36].\nTo illustrate the critical nature of safety vulnerabilities in fine-tuning, we consider two prominent use cases that highlight the potential for widespread impact:\nCompanies offering fine-tuning APIs: Many companies now provide fine-tuning APIs to meet demands for personalized AI models without disclosing model parameters to the public. These APIs include safeguards, such as data filters, to prevent harmful content during customization. However, our findings indicate that even with these controls, fine-tuning can inadvertently weaken a model's safety alignment. This jailbreak effect allows restricted content to resurface, exposing companies to significant ethical, legal, and reputational risks, as harmful outputs may be generated without the end-user's intent.\nBenign users unaware of fine-tuning risks: In contrast, many end-users fine-tune models solely for customized, non-harmful applications, often unaware that fine-tuning can erode existing safety filters. Relying on built-in safeguards, these users may unintentionally generate inappropriate content, creating ethical and social risks, especially if such material is inadvertently shared with vulnerable audiences. This highlights the need for user awareness of fine-tuning's potential safety impacts.\nOur investigation reveals that standard fine-tuning techniques can inadvertently undo safety alignments of text-to-image diffusion models. This happens because fine-tuning can adjust the model's parameters in a way that restores the capacity to generate harmful content that was previously suppressed. Additionally, this effect becomes stronger the more similar the dataset is to the previously erased concepts, and it can occur even with very few fine-tuning steps or training images. As fine-tuning is essential for adapting models to new applications and domains, it is crucial to develop methods that robustly preserve safety alignment throughout this process.\nTo mitigate this risk, we introduce a novel solution called modular low-rank adaptation (Modular LORA), which provides a structured approach to maintaining safety alignment during model updates. The main idea is to implement a safeguard as a low-rank adaptation (LORA) [25] rather than fully fine-tuning the target model. This modularizes the model's ability to block harmful content, allowing us to separate out the component dedicated to safeguarding. During fine-tuning, we can temporarily remove this safety LORA to prevent it from being inadvertently affected, and then reattach it afterward. In this paper, we show that the safety training component itself drives the relearning of harmful content, and our approach is simple yet effectively blocks the pathways that enable re-learning of harmful content."}, {"title": "2. Background", "content": null}, {"title": "2.1. Safety Alignment in Text-to-Image Models", "content": "Text-to-image diffusion models [1, 13, 66, 68], including Stable Diffusion [66], condition on text prompts to generate images that combine diverse concepts in imaginative and often unexpected ways, thereby enhancing their utility in various creative and practical applications [5, 53, 58, 67]. The deployment of stable diffusion models has sparked controversy, particularly regarding their training datasets, such as LAION-5B [71], which includes not-safe-for-work (NSFW), copyrighted, and potentially harmful content [77]. As a result, these models can inadvertently exhibit undesirable behaviors, as perfect filtering of training data and inference outputs remains challenging. Most recently, FLUX.1 [dev]\u00b9 is an open-sourced commercial-level text-to-image model which was trained with the flow matching objective [43] followed by additional classifier-free guidance (CFG)-guidance distillation [47]. However, its technical details including how much alignment effort has been put into model building have not been released to the public.\nTo address harmful content retention, several concept removal methods have been developed. These include inference-time approaches [4, 70], fine-tuning diffusion models [16, 32, 35], modifying cross-attention layer projec-"}, {"title": "2.2. Safety Alignment in Language Models", "content": "Reinforcement learning from human feedback (RLHF) [51] and its variants, such as direct preference optimization (DPO) [61], have become the standard approaches for aligning LLMs with human preferences to minimize the generation of provocative or problematic responses. However, despite these advancements, Qi et al. [59] were the first to show that fine-tuning can easily compromise the safety alignment of high-quality LLMS. Even for models like GPT-4, recent findings indicate that fine-tuning still weakens established safety constraints [87], underscoring that a complete solution has yet to be achieved. Subsequent studies have explored alignment mechanisms and limitations, revealing that current alignment approaches are both fragile and safety-critical components are sparsely located within model neurons [82]. Research has also shown that alignment typically modifies only activation levels, providing a shortcut for fine-tuning that may easily disrupt safety measures [37]. Overall, this suggests that issues such as catastrophic forgetting [34] - whereby previously learned knowledge can be easily lost - and an inherent tension between helpfulness and harmlessness contribute to the brittleness of alignment mechanisms [59]. Importantly, end-users are not easily constrained by specific algorithms, datasets, or protocols for fine-tuning, especially when they have access to the model's weights, which complicates ensuring safety in all scenarios. In this paper, we illustrate that while catastrophic forgetting is widely recognized, the issue extends beyond simply \"forgetting\" to models inadvertently relearning the exact previously constrained behaviors."}, {"title": "2.3. Parameter-Efficient Fine-Tuning", "content": "Fully fine-tuning large pre-trained models for specific tasks becomes prohibitively expensive as model and dataset sizes grow. To address this, parameter-efficient fine-tuning"}, {"title": "2.4. Arithmetic Model Merging", "content": "Recent works have leveraged model merging to achieve precise control over generative models without additional training or data [12, 26, 27]. In text-to-image diffusion models, rapid developments in model merging have addressed issues of conflict and interference [38, 72]. Zi-PLORA [72] enables the merging of independently trained LORA modules for user-guided style transfer via incorporating trainable mixing coefficients, while Direct Consistency Optimization (DCO) [38] improves merging by maintaining alignment with the original pre-trained model. Zhong et al. [91] proposed switching and compositing LORAs to avoid such issues, and Dravid et al. [14] explored the possibility of encoding semantics with LORAS and merging them in the parameter space. Inspired upon arithmetic merging methods, we propose learning a separate safety module, which can be merged later with downstream task-specific modules so as to prevent the re-emergence of unsafe content. For further related works, we refer readers to Appendix A."}, {"title": "3. Motivations and Observations", "content": null}, {"title": "3.1. Fine-Tuning Jailbreaking Attack", "content": "Recent advancements in text-to-image diffusion models have prioritized both image quality and safety alignment. One such model, FLUX.1, represents a commercially graded text-to-image diffusion model, pre-trained on large-scale internet-crawled datasets, and presumably, distilled on high-quality, safe image subsets. However, our findings reveal that, FLUX.1 remains vulnerable to fine-tuning jailbreaking attacks, where even minimal fine-tuning can reintroduce unsafe content and proprietary signatures, exposing a critical gap in its ability to maintain safety alignment.\nAs demonstrated in Fig. 1, fine-tuning FLUX.1 [dev] for as few as 2,000 steps on Pok\u00e9mon dataset using LORA [25] of rank 16 reveals previously hidden signatures in generated images. These signatures, which were prompted in text but almost absent in FLUX.1's outputs pre-fine-tuning (only 3% contained signatures), appear in 25% of images post-fine-tuning. The fact that these signatures did not exist in the fine-tuning dataset but emerged after fine-tuning suggests that fine-tuning does not merely disrupt alignment; it actively reactivates latent concepts within the model's weights, bypassing the effects of prior alignment. Similarly, as illustrated in Fig. 2, fine-tuning for just 1,500 steps on Pok\u00e9mon dataset causes FLUX.1 to generate more undressed, NSFW content, despite its initial alignment to avoid such outputs. This rapid reversion to unsafe behaviors after fine-tuning indicates that FLUX.1's current monolithic structure, lacking modularized safety mechanisms, is particularly vulnerable to potential fine-tuning attacks."}, {"title": "3.2. Factors Affecting Jailbreaking", "content": "While Sec. 3.1 focused on jailbreaking the state-of-the-art safety-aligned model FLUX.1, in this section, we showcase that unintended jailbreaking phenomenon with benign datasets is prevalent across various concept removal methods such as ESD [16], SDD [32, 33], and MACE [45], and we analyze the factors attributing to the jailbreaking phenomenon. Here, we show results mainly on ESD, and refer to Appendix B for further results."}, {"title": "3.3. Early Stages of Fine-tuning", "content": "In Fig. 5, we observe a rapid increase in the percentage of unsafe images during the early fine-tuning stages for both the full fine-tuned and LORA-based ESD models. This trend indicates that the models quickly become susceptible to generating unsafe content, even though the fine-tuning dataset is benign (Pok\u00e9mon). This early-stage \"jailbreaking\" effect suggests that the models are rapidly losing their"}, {"title": "3.4. What Safety and FT LoRAs Have Learned?", "content": "In this section, we aim to investigate what each safety and fine-tuning LORA module has learned using model arithmetic [12]. Built upon the premise that model weight arithmetic can add or remove knowledge as introduced in Sec. 2.4, we perform LORA weight negation \u2013 where negating LORA weight reverses its effect. Here, we fine-tune the LORA-based ESD model with the Pok\u00e9mon dataset. As shown in Fig. 8, negating the safety LORA weights $\\Delta W_{\\text{safe}}$"}, {"title": "4. Modularizing Safety Modules", "content": "To address the vulnerabilities introduced by standard fine-tuning, we propose a Modular LORA approach, which aims to modularize and temporarily detach safety components during fine-tuning. This method separates the safety and fine-tuning parts, allowing the model to retain its safety alignment even after the adaptations for target downstream task data. In the standard fine-tuning pipeline, the model parameters are updated as:\n$W' = W_0 + \\Delta W,$ \nwhere $W_0$ is the pre-trained model weight. After initial safety alignment, fine-tuning is typically applied as:\n$W' = W_0 + \\Delta W_{\\text{safe}} + \\Delta W_{\\text{ft}},$ \nwhere $\\Delta W_{\\text{safe}}$ represents the safety alignment LORA and $\\Delta W_{\\text{ft}}$ represents the fine-tuning LORA. However, as we have shown in Sec. 3, this approach can lead to unintended interactions, as fine-tuning can overwrite or bypass previously established safety constraints.\nIn the Modular LORA approach (illustrated in Fig. 10 and the provided concept figure), we introduce $\\Delta W$, a fine-tuning module that is trained independently, without the safety LORA $\\Delta W_{\\text{safe}}$ attached. This setup enables $\\Delta W$ to focus on style adaptation without interacting with or disrupting the safety alignment.\nThe final inference model is then defined as:\n$W^* = W_0 + \\Delta W_{\\text{safe}} + \\Delta W$ \nBy re-attaching the safety LORA $\\Delta W_{\\text{safe}}$ only at inference time, the model retains its safety constraints, while the fine-tuning component $\\Delta W$ integrates the new stylistic features without reactivating previously suppressed unsafe concepts. Through this modularized setup, $\\Delta W$ can learn new features without inheriting any unsafe or suppressed behaviors tied to the original pre-trained weights. Conversely, when fine-tuning occurs without separating the safety module, as in $\\Delta W_{\\text{ft}}$, the model is more likely to relearn previously restricted content. In the following section, we empirically demonstrate that safety and fine-tuning LORA modules can be effectively combined with minimal interference across various experimental setups."}, {"title": "5. Experiments", "content": null}, {"title": "5.1. Experimental Settings", "content": "We use Stable Diffusion v1.4 [66] as a test bed, which has also been widely studied for concept removal methods [16, 17, 32, 45, 70]. Our algorithm is built on Pytorch 2.1 [54], and all experiments are conducted on NVIDIA RTX 3090 and NVIDIA RTX A6000 machines. Refer to Appendix D for further experimental details.\nTraining datasets. We used a total of three datasets to evaluate whether NSFW concept defenses would break:"}, {"title": "5.2. Modular LoRA Prevents Jailbreaking", "content": "Tabs. 2 to 4 present a detailed evaluation of fine-tuning jailbreaking attacks on safety-aligned diffusion models. Specifically, we fine-tune safety-aligned pre-train model with three anime-style datasets. We can readily see that Modular LORA successfully mitigates jailbreaking compared to two fine-tuning baselines (full fine-tuning and LORA) in both tables. Refer to Appendix B for more quantitative and qualitative results."}, {"title": "6. Conclusion", "content": "This paper uncovered a significant safety vulnerability in fine-tuning text-to-image diffusion models: fine-tuning can unintentionally compromise safety alignments, allowing suppressed concepts to reappear even with benign datasets. This \"fine-tuning jailbreaking\" effect poses risks for both providers of fine-tuning APIs and end-users who may be unaware of the associated risks. To address this issue, we introduced Modular LORA, a method that preserves safety alignment by training Safety LORA modules separately from task-specific adaptations and merging them only during inference. This approach effectively prevents the relearning of harmful content while maintaining the model's adaptability, providing a practical safeguard to enhance the secure and responsible use of text-to-image diffusion models."}]}