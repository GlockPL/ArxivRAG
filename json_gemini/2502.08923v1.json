{"title": "CopySpec: Accelerating LLMs with Speculative Copy-and-Paste Without Compromising Quality", "authors": ["Razvan-Gabriel Dumitru", "Minglai Yang", "Vikas Yadav", "Mihai Surdeanu"], "abstract": "We introduce CopySpec, an innovative technique designed to tackle the inefficiencies LLMs face when generating responses that closely resemble previous outputs. CopySpec identifies repeated sequences in the model's chat history and speculates that the same tokens will follow, enabling seamless copying without compromising output quality or requiring additional GPU memory. To evaluate the effectiveness of our approach, we conducted experiments using five LLMs and five datasets: MT-Bench, CNN/DM, GSM-8K, HumanEval, and our newly created dataset, MT-Redundant. MT-Redundant, introduced in this paper, transforms the second turn of MT-Bench into a request for variations of the first turn's answer, simulating real-world scenarios where users request modifications to prior responses. Our results demonstrate significant speed-ups: up to 2.35\u00d7 on CNN/DM, 3.08x on the second turn of select MT-Redundant categories, and 2.66x on the third turn of GSM-8K's self-correction tasks. Moreover, we show that CopySpec integrates seamlessly with speculative decoding, yielding an average 49% additional speed-up over speculative decoding for the second turn of MT-Redundant across all eight categories. While LLMs, even with speculative decoding, suffer from slower inference as context sizes grow, CopySpec leverages the expanded context to accelerate inference, making it faster as the context size increases. Our code and dataset are publicly available at https://github.com/RazvanDu/CopySpec.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs) have revolutionized natural language processing (NLP), enabling great performance across a range of applications, including code generation, machine translation, and question answering. However, the computational demands of LLMs, particularly during inference, pose significant challenges for real-time applications and scalability in resource-constrained environments. Sequential token generation, a core bottleneck in standard decoding, limits throughput and increases latency. Speculative Decoding (Leviathan et al., 2023; Chen & Xu, 2023) has emerged as a promising approach to mitigate this issue by employing a smaller draft model to generate multiple token sequences, which are then verified by the larger target model. Despite its potential, existing speculative decoding methods often fail to fully exploit the inherent redundancies in LLM-generated outputs and require extra GPU memory or modifications to the original LLM, leaving considerable room for improvement.\nIn this work, we present CopySpec, a novel speculative decoding framework designed to address these limitations. CopySpec incorporates a learned copying mechanism into the draft process, enabling the model to detect and exploit predictable patterns in token sequences"}, {"title": "2. Related Work", "content": ""}, {"title": "2.1. Speculative Decoding", "content": "Speculative decoding is an effective approach for accelerating inference in LLMs by parallelizing token generation and verification. Leviathan et al. (2023) introduced the foundational framework, employing a small draft model to propose multiple tokens that a larger model verifies, significantly reducing inference latency. Medusa (Cai et al., 2024) expanded this idea by leveraging multi-head decoding to enable simultaneous token generation and verification, improving throughput.\nDynamic verification pipelines balance speed and accuracy by adjusting verification depth based on output quality (Liu et al., 2024). Token tree verification accelerates serving (Miao et al., 2023), while pipelined exact decoding handles compute-latency trade-offs (Yang et al., 2023). Knowledge distillation enhances draft-target model interaction (Zhou et al., 2023), and retrieval-based token validation improves efficiency (He et al., 2023). Speculative decoding has been further optimized in recent works. SpecHub (Sun et al., 2024) uses optimal transport to improve draft token acceptance rates, and SPEED (He & Wang, 2023) leverages early-layer hidden states for parallel token execution."}, {"title": "2.2. Copying Mechanisms in Language Models", "content": "Copying mechanisms are widely adopted in NLP to handle tasks that require replicating predictable patterns or segments. Gu et al. (2016) introduced CopyNet, a method that enables RNN sequence-to-sequence models to predict words based on a mixed probabilistic model of two modes, where one selects words from the source sequence. Similarly, in summarization tasks, Pointer Networks (Vinyals et al., 2015) and Pointer-Generator Networks (See et al., 2017) demonstrated the effectiveness of combining copying and generation to improve output fidelity and handle out-of-vocabulary tokens.\nMore recently, McCoy et al. (2023) analyzed the extent to which transformers copy from their training data, providing insights into copying behaviors in modern LLMs. Jelassi et al. (2024) showed that transformers outperform state space models in copying repetitive patterns.\nLastly, in a different domain, Andronov et al. (2024) introduced a copying mechanism into a transformer-based encoder-decoder that models chemical reactions by observing that portions of the input chemicals often remain unchanged in the output.\nWhile previous works have emphasized the importance of copying mechanisms in various applications, our work is the first to explore this concept in the specific context of LLM inference. CopySpec integrates a copying mechanism into speculative decoding, effectively reducing redundancy and enhancing efficiency across a wide range of tasks. By leveraging repeated patterns in the model's context, CopySpec introduces a novel approach to accelerate inference while maintaining high performance."}, {"title": "2.3. Fill-in-the-Middle (FIM) Techniques", "content": "Fill-in-the-Middle (FIM) enables language models to generate text segments within a given context, enhancing flexibility in tasks such as text and code infilling. Bavarian et al. (2022) introduced a data transformation approach for autoregressive models to learn infilling without sacrificing left-to-right generative performance, while Shen et al. (2023) proposed FiLM, enabling flexible generation by masking arbitrary positions.\nIn code generation, FIM techniques are crucial for editing and repair tasks. Models like Code Llama (Roziere et al., 2023) and InCoder (Fried et al., 2023), a utilize bidirectional context for structured prompts, achieving state-of-the-art results on benchmarks such as HumanEval. Frameworks such as Self-Infilling (Zheng et al., 2024) and benchmarks like SAFIM further enhance these methods with backward generation and syntax-aware metrics (Wang et al., 2023; Gong et al., 2024). Recent advancements, models like Codestral and CodeGemma, refine FIM techniques to improve alignment (Mistral AI, 2024; Team et al., 2024).\nHowever, it is important to emphasize the distinct advantages of our method compared to the FIM approach. Unlike FIM, which relies on labeled tokens such as <prefix> and <suffix> to guide the model in fixing a specific section of code bidirectionally. Our method operates label-free, enabling a more flexible and generalizable approach. Additionally, while FIM is constrained to modifying a single code segment (typically the middle), CopySpec extends this capability by allowing modifications in multiple regions, such as quite a few distinct places within the input. Furthermore, we maintain the architectural simplicity of a left-to-right LLM, ensuring that our method remains compatible with existing LLM frameworks while offering significant improvements in efficiency and versatility."}, {"title": "3. Method", "content": "Our method operates on the assumption that if the last y tokens generated by an LLM appear in the context, the tokens that followed them in the input context are likely to follow again in the output. Figures 1 and 2 illustrate this concept, By accurately identifying the start of such a segment, we can generate all tokens within the block in a single pass through the LLM, bypassing the need for a draft model to produce them incrementally. In the following subsections, we detail the implementation of this approach and its integration into a Speculative Decoding framework, demonstrating how it achieves substantial speed-ups."}, {"title": "3.1. Identifying the Tokens to Copy", "content": "To efficiently detect when the model begins generating a block that has already been produced earlier, we maintain a hash map containing all subsequences of y tokens from the context. During the generation process, we search this hash map for matches to the last y tokens generated. Adding a new tuple of tokens to the hash map and searching for a match after each generated token has a time complexity of \u039f(\u03b3). Since y is typically set to a small value (e.g., 3 or 5), the computational overhead for processing new tokens and finding matches is minimal and independent of the context size. This stands in contrast to alternative approaches that require searching the entire context for the last substring, which can become computationally expensive as the context grows.\nOur technique efficiently leverages larger contexts, allowing inference to become faster as the context size increases. By keeping y fixed, we ensure a balance between efficiency and precision. Additionally, we explored methods to utilize partial outputs without revealing the complete results and investigated how the semantic relationship between the preceding y tokens and the subsequent token can guide the optimal choice of \u03b3. Further details are provided in Appendix A."}, {"title": "3.2. Speculating on the Matched Tokens", "content": "After identifying a match of y tokens in the context, we extract the subsequent tokens from the context, as shown in Figure 2. These extracted tokens, which we call $S_{speculate}$, essentially simulate the behavior of a draft model where the probability for each token in $S_{speculate}$ is treated as 100%.\n$S_{speculate}$ is then verified directly by the LLM. Each verification yields 7 tokens that align with the LLM's ongoing generation, along with one additional guaranteed token. This approach mirrors vanilla speculative decoding (Leviathan et al., 2023), where speculative tokens are appended to the context, and the longest prefix matching the LLM's output is accepted. In Figure 2, $S_{speculate}$ is highlighted in blue. The output shows the accepted tokens in green, the extra guaranteed token in gold, and any rejected tokens in red. This process effectively treats the copied tokens as a \"perfect prediction,\" ensuring efficient token generation when patterns are detected.\nAfter each newly generated token or copying attempt, we re-evaluate the last y tokens in the context to identify a new match, allowing the model to utilize longer copyable blocks whenever possible. This eliminates the need for manual token generation between copying steps.\nIn cases where multiple matches exist for the last y tokens, we simplify the process by selecting the first match, though we acknowledge that alternative strategies could improve efficiency."}, {"title": "3.3. Merging with Vanilla Speculative Decoding", "content": "To further enhance our technique, we have integrated it within a vanilla Speculative Decoding framework. At each step of the generation process, we attempt to find matches in the context. If a match for the last y tokens is found, we use $S_{speculate}$ as draft tokens, effectively simulating a draft model with perfect confidence in those tokens. If no match is identified, we rely on a smaller draft model to generate 72 draft tokens. This dual approach allows us to dynamically choose between leveraging repetitive patterns through CopySpec and utilizing speculative decoding for efficient token generation in contexts with little or no redundancy.\nThis integration provides the best of both worlds: Speculative Decoding accelerates inference when the context size is small or lacks redundancy, while CopySpec builds on this speed-up in subsequent steps by taking advantage of repetitive patterns as the context size increases. As a result, the combined approach significantly enhances model efficiency across diverse scenarios.\nIt is also worth noting that when used as a stand-alone method, CopySpec does not require a draft model. This eliminates the need for additional GPU memory or modifications to the model, making it lightweight and easy to deploy. We explore the interplay between these techniques in Section 6, while Appendix B provides a detailed account of the full implementation, including key-value caching."}, {"title": "4. Experiments", "content": ""}, {"title": "4.1. Models and Hyperparameters", "content": "We evaluated our copying technique on five instruction-tuned LLMs: Qwen2.5-72B, Qwen2.5-32B, Qwen2.5-7B (Qwen et al., 2025), LLaMa3.1-70B, and LLaMa3.1-8B (Grattafiori et al., 2024), using 4 A100 GPUs with a batch size of 1. Unless stated otherwise, we set y to 3, |$S_{speculate}$| to 10, the max generation length to 1024, and the temperature to 0."}, {"title": "4.2. Evaluation Datasets", "content": "We evaluated our technique on five datasets, each targeting specific aspects of model performance: MT-Redundant, CNN/DM, GSM-8K, HumanEval, and MT-Redundant was designed to emphasize prompts requiring small variations to previous outputs, while CNN/DM focuses on extractive summarization. GSM-8K evaluates the model's self-correction capabilities, MT-Bench highlights scenarios with minimal copying potential to measure the technique's overhead, and HumanEval assesses coding capabilities. To accommodate the increased computational demands of GSM-8K and CNN/DM and our limited GPU resources, we restricted these datasets to 100 samples, ensuring they were of comparable size to the other datasets. For HumanEval, we employed the same instruction format as presented in EvalPlus (Liu et al., 2023)."}, {"title": "4.3. MT-Redundant", "content": "Most existing NLP datasets focus on tasks involving either single-turn interactions or scenarios where the model must entirely change its response in the second turn. These setups fail to capture realistic use cases where a user might request slight variations or refinements to a previous answer. To address this gap and highlight the capabilities of our technique, we introduce a new dataset, MT-Redundant.\nMT-Redundant is derived by modifying the second turn of MT-Bench (Zheng et al., 2023). In our dataset, the second turn replaces the original question with a prompt asking the model to review its previous answer and make specific adjustments or variations. This modification simulates real-world scenarios where incremental refinement or elaboration is required.\nOur dataset spans a diverse range of practical use cases, categorized into eight groups: Coding, Extraction, Humanities, Math, Reasoning, Roleplay, STEM, and Writing. These categories reflect realistic tasks encountered in various domains. Additionally, we adopted the same evaluation procedure from MT-Bench to ensure consistency and comparability of results.\nBy creating MT-Redundant, we aim to bridge the gap between artificial benchmarks and practical applications, providing a more representative evaluation for techniques like CopySpec in multi-turn interactions with reptitive information."}, {"title": "5. Discussion of Results", "content": "We analyze our main results in Table 1, which show the impact of our method on performance and the percentage of tokens copied across five LLMs and datasets. The results are aggregated for all turns in MT-Redundant and MT-Bench (two turns each) and the self-correction process in GSM-8K (three turns). Speedups range from 1.15x on MT-Bench, which has minimal redundancy, using Qwen2.5-72B-Instruct, to 2.35x on CNN/DM with the same model.\nWhile these results are notable, the key strength of our approach lies in its ability to enhance performance as context size grows. To illustrate this, next we break down scenarios by per-turn performance and analyze the effect of varying hyperparameters on the technique's effectiveness in a wide range of use-cases."}, {"title": "5.1. Speed-up by Turn and Category", "content": "We begin our analysis by examining the speedups achieved on MT-Redundant for both the first and second turns, as summarized in Table 2. The results indicate a substantial average speedup of 2.04\u00d7 for the second turn, compared to a more modest speedup of 1.08\u00d7 for the first turn. Notably, the performance in tokens per second (TPS) achieved by the model increases for the second turn, which features a larger context size. In contrast, the baseline model experiences a decline in TPS as the context size increases. Another notable aspect is that the observed speedup is highly dependent on the specific use case. For instance, we observe speedups as low as 1.2x in the Humanities category and as high as 3.08 \u00d7 for Roleplay. However, regardless of the use case, the speedup for the second turn remains consistently positive across all models for both MT-Redundant and MT-Bench.\nThe results for all five models on MT-Redundant and MT-Bench are detailed in Appendix C.2 and D.2 respectively. On average, the second round of MT-Redundant achieves a significant 91% speedup across all models, compared to 31% for MT-Bench. Notably, even on MT-Bench, which has less redundancy, the TPS achieved by CopySpec in the second turn is almost always higher than the baseline model's TPS in the first turn. These findings highlight how our approach effectively leverages increased context sizes to enhance performance, even in less favorable scenarios."}, {"title": "5.2. The Effect of Gamma (\u03b3)", "content": ""}, {"title": "5.3. Number of Tokens to Copy and Overhead", "content": "We evaluate the impact of the number of tokens copied on performance and estimate CopySpec's overhead by setting the number of copied tokens to zero, isolating the cost of token searching. Results in Table 4 show minimal overhead with differences from the base model nearly within the margin of error. Among the hyperparameters studied, setting |$S_{speculate}$ = 10| delivers the best performance, while larger values, such as 50 or 100, increase overhead and reduce tokens-per-second (TPS) efficiency."}, {"title": "6. Analyses", "content": ""}, {"title": "6.1. Orthogonality with Speculative Decoding", "content": "We followed the steps outlined in Section 3.3 to integrate our technique into a vanilla speculative decoding framework, as described in (Leviathan et al., 2023). Based on our observations from Section 5.2, we experimented with two different values of \u03b3 (3 and 5) to analyze their impact on performance when used alongside speculative decoding."}, {"title": "6.2. Effect on Reasoning", "content": "An important aspect of our analysis is evaluating the impact of our technique on the efficiency of self-correction. To this end, we implemented a self-refine framework, where the model generates Python code and iteratively refines it in two steps, following a process similar to (Madaan et al., 2023).\nOur technique becomes more effective in later turns as the model iterates over its prior reasoning, allowing progressively more tokens to be copied. This is reflected in a significant rise in the percentage of copied tokens, tokens per second (TPS), and T1, the average number of tokens accepted. Each copying attempt also becomes more precise as the model refines its reasoning and the context grows.\nWhen combined with SD using \u03b3 = 5, our approach achieves better results across all three turns, as shown in the table. The first turn benefits most from SD due to minimal copying, while later turns gain greater advantages from copying. This highlights the complementary nature of the two techniques and their combined effectiveness in improving efficiency and performance. Notably, while the TPS of the base model decreases by 0.85\u00d7 as context size grows, our technique reverses this trend, increasing the TPS in the last turn by 2.52\u00d7, showcasing its ability to leverage larger contexts for enhanced efficiency."}, {"title": "7. Conclusion", "content": "We introduced CopySpec, a method that identifies repeated token sequences in a growing context and copies them efficiently without additional GPU memory or significant cost. Using a rolling hash for y tokens, CopySpec speculates on larger token blocks to reduce redundant computation.\nResults across five LLMs and datasets, including MT-Redundant, show up to a 3.08\u00d7 speed-up in second-turn inference and a 49% boost when combined with speculative decoding, without altering output quality. Future work includes dynamically tuning y, refining match selection, and integrating CopySpec with parallel decoding frameworks."}, {"title": "Impact Statement", "content": "This work introduces a method to accelerate large language model (LLM) inference, thereby reducing the computational resources and costs associated with producing lengthy outputs. By improving efficiency, CopySpec can lower the barriers to using LLMs across various applications, ranging from education and research to industry-scale deployments.\nOn the positive side, faster inference decreases energy consumption per token, which can help mitigate the environmental impact of large-scale model serving. It also makes multi-turn interactions more accessible, potentially benefiting users with limited computational resources.\nHowever, increased efficiency may lead to the more frequent use of LLMs in contexts like spam generation or disinformation at scale. As with any generative method, careful deployment and robust content moderation remain necessary to reduce potential harm. CopySpec itself does not solve issues of model bias, misuse, or misinformation; rather, it highlights the need for responsible governance of rapidly evolving LLM capabilities."}, {"title": "A. Gamma (\u03b3) and Semantic Implications", "content": "In our framework, the generation speed of CopySpec is intricately tied to the choice of y, which governs the length of the left context used to identify repeated sequences. The selection of an optimal y is critical, as it directly impacts the model's ability to efficiently reuse tokens from the context, thereby accelerating generation. A carefully chosen y strikes a balance between providing sufficient contextual information for accurate copying and avoiding unnecessary computational overhead.\nIf y is too small (e.g., y = 1), the context provides insufficient information to reliably identify repetitions, resulting in missed reuse opportunities and slower generation. Conversely, when y is too large, the excessive context introduces redundancy and dilutes the immediate semantic relevance. While the acceptance rate may increase, the total number of tokens generated per second decreases because the model spends more time processing generate tokens itself and fewer tokens are copied in practice.\nThe challenge, therefore, lies in finding an optimal y that maximizes copying attempts while minimizing computational overhead. A well-chosen y ensures that the context is both semantically focused and computationally efficient, enabling the Copy mechanism to fully exploit repeated patterns in the generation process. This tradeoff underscores the importance of systematically tuning y to achieve the best performance across datasets.\nTo measure the semantic alignment between a token w and its left-y token context, we fine-tuned the token embeddings using a left-y skip-gram model, a modification of the traditional skip-gram approach. Unlike the standard skip-gram model, which maximizes the probability of a target word given a symmetric context window, our approach considers only the preceding y tokens as context.\nFormally, instead of maximizing the probability $\\Pi_{(w,C)\\in D} P(W|C)$, where C represents a symmetric context window around the word w, our left-y skip-gram model is trained to maximize $\\Pi_{(t,C_{left y})\\in D} P(t|C_{left y})$, where $C_{left y}$ consists only of the last y tokens in the sequence to predict the next token t. This ensures that the learned embeddings capture dependencies in a unidirectional manner, aligning with the way generative models process text.\nBy structuring the model in this way, we aim to quantify how much semantic meaning from the left-y tokens contributes to predicting the next token. Cosine Similarity is particularly well-suited for evaluating the semantic alignment between the left-y token context and the next token because it captures the directional similarity between their vector representations, regardless of magnitude. Since word embeddings encode semantic meaning in a high-dimensional space, CS provides a robust way to measure how well the left context conveys predictive information about the next token. Unlike Euclidean Distance, CS ensures that we focus solely on semantic coherence rather than raw frequency effects. This is crucial for CopySpec, as effective token reuse depends on the ability to recognize when a sequence of past tokens is not just lexically repeated but also semantically relevant to the next token. By analyzing trends in CS across different y-values, we can assess whether increasing the context length improves meaningful copying or merely introduces redundant information, thereby helping us fine-tune y for optimal efficiency.\nThe cosine similarity (CS) is computed as:\n$CS(C_{left}, t) = \\frac{C_{lefty} \\cdot t}{||C_{lefty}|| \\cdot ||t||}$\nHere, $C_{left y} = \\frac{1}{y} \\Sigma_{i=1}^{y} t_{i}$ represents the average embedding of the most recent y tokens, where {$t_{i}$}$_{i=1}^{y}$ are the embeddings of the last y tokens in the context.\nTo validate our intuitions, we conducted experiments to analyze the relationship between y (context length) and semantic alignment. Figure 5 illustrates the trends in Cosine Similarity and generation speed (TPS) as y varies.\nBy measuring Cosine Similarity and generation speed across varying y-token contexts, we provide empirical evidence that fine-tuning left-y skip-gram model for the best y is essential for maximizing efficiency. Future work can explore adaptive strategies that dynamically adjust y in the same hashmap based on context complexity, further optimizing the balance between copying effectiveness and computational cost."}, {"title": "B. Copying and Speculative Decoding with Truncated KV States", "content": "This appendix describes how our framework integrates a copying mechanism with speculative decoding, including details on partial acceptance, key-value (KV) cache truncation."}, {"title": "B.1. Notation and Variables", "content": "Sequence X1:t. Let X1:t be the currently accepted sequence of t tokens. Generating a new token moves us to position t+1.\nDictionary D. D records repeated y-length substrings and their earlier occurrences. If Xt\u2212y+1:t appears in D, we may copy subsequent tokens from that match.\nSubsequence length \u03b3. We use y tokens to detect repeats. That is, the last y tokens, s = Xt\u2212y+1:t, determine if a copy event is possible.\nMatch location p. If D indicates Xt\u2212y+1:t appears at position p, we attempt to copy tokens starting from p + \u03b3.\nChunk size m (copying). When a match is found, we form a copied chunk\n$X_{1:m} = (x_1,...,x_m) = X_{p+y:p+y+m-1}$.\nDraft limit 8 (speculative). If copying is not used, we let the draft model propose up to 8 tokens:\n$X_{1:\\delta} = (x_1,...,x_{\\delta})$.\nAcceptance and Draft Models. The target model $P_{target}(\\cdot | X_{1:n})$ decides whether each new token is accepted, while the draft model $p_{draft}(X_t | X_{1:n})$ only proposes tokens that must still pass $P_{target}$'s acceptance criterion.\nIndex i. In both copying and drafting, we iterate over newly proposed tokens with an index i \u2208 {1,...,m} or i \u2208 {1, ..., \u03b4}.\nAccepted count k. Out of the m (copied) or 8 (drafted) tokens, only k < m or k \u2264 d may be accepted under $P_{target}$. Rejected tokens are removed, and the key-value states are truncated to retain only X1:t+k."}, {"title": "B.2. Acceptance Criterion and KV Truncation", "content": "Any new token Xt+i must pass an acceptance criterion under $P_{target}$; for example, at temperature 0, we only accept it if it is the argmax of the target model's conditional distribution. If the token fails, we reject it (and all subsequent tokens in the same chunk) and roll back to X1:t+i-1.\nEach layer l of the target model stores key-value tensors (Ke, Ve) up to the final accepted token. If k < i \u2212 1 tokens in a chunk are accepted, we truncate (Ke, Ve) to t + k positions, ensuring the model remains consistent with the final accepted sequence."}, {"title": "B.3. Integrated Generation Procedure", "content": "Below is a single pseudocode listing that combines both copying and speculative decoding.\n1. Check for a Copy Opportunity:\n(a) Let s = Xt-y+1:t be the most recent y tokens of the accepted sequence X1:t.\n(b) Check if s is in D (the dictionary of repeats).\n\u2022 If no match exists, go to Step 3.\n(c) Otherwise, let p be the first occurrence in D(s) satisfying p + y-1<t-y+ 1 (ensuring no overlap)."}, {"title": "C. Extra Results on MT-Redundant", "content": "This appendix presents a detailed analysis of the performance improvements achieved by the CopySpec approach compared to baseline methods. The tables provide comprehensive results across various categories and model configurations, highlighting the computational efficiency and speed-ups observed on the MT-Redundant dataset."}, {"title": "C.1. Analysis of Gamma (y) on MT-Redundant", "content": "The analysis depicted in Figure 6 highlights the impact of the copying parameter y on both computational performance and the model's ability to reuse tokens effectively. As y increases, there is a notable rise in the percentage of copied tokens, demonstrating the model's improved ability to exploit repeated patterns within the context. However, this comes at the cost of reduced tokens per second (TPS) for higher y values, due to the increased computational overhead associated with processing larger context windows."}, {"title": "C.2. Speed-up by Category on MT-Redundant", "content": "Table 6 summarizes the tokens-per-second (TPS) performance for the Qwen-32B-Instruct model across two turns. The first turn reflects scenarios with minimal contextual information, while the second turn demonstrates significant gains in speed due to the larger context size and CopySpec's ability to leverage repeated token patterns effectively. Notably, categories such as Coding and Math exhibit speed-ups exceeding 2x in the second turn.\nIn Table 7, we observe a similar trend for the Qwen-7B-Instruct model, with CopySpec consistently improving TPS across both turns. The second turn results show substantial gains in categories like Reasoning and Math, where repetitive patterns in the context are more prominent."}, {"title": "C.3. Merging with Speculative Decoding on MT-Redundant", "content": "Finally, Table 10 explores the integration of CopySpec with speculative decoding for the Qwen2.5-32B-Instruct model and Qwen2.5-7B-Instruct as the draft model. The results highlight how combining these approaches can yield even greater computational efficiency. The analysis includes varying y values and draft token counts, showing that optimal parameter tuning further enhances performance, particularly in multi-turn scenarios."}, {"title": "D. Extra Results on MT-Bench", "content": "This appendix presents a comprehensive evaluation of the CopySpec approach on the MT-Bench dataset across various configurations and categories. The results highlight the consistent improvements in tokens-per-second (TPS) performance achieved by CopySpec compared to baseline models, demonstrating its efficiency and scalability."}, {"title": "D.1. Analysis of Gamma (\u03b3) on MT-Bench", "content": "Figure 7 presents a comprehensive visualization of how the copying parameter y affects the performance of the LLaMa3.1-8B-Instruct model on the MT-Redundant dataset. The figure captures the interplay between the percentage of tokens successfully copied, the number of copying attempts, and the resulting tokens per second (TPS)."}, {"title": "D.2. Speed-up by Category on MT-Bench", "content": "Table 11 provides the TPS performance of Qwen2.5-72B-Chat on two turns. The speed-ups are most notable in categories such as Extraction and Coding, where repetitive patterns allow CopySpec to outperform the baseline consistently. Average speed-ups for both turns reinforce the efficiency gains achieved.\nIn Table 12, the performance of Qwen2.5-32B-Chat is evaluated. CopySpec achieves significant speed-ups, particularly in the second turn, where contextual repetition becomes more prevalent. Categories like Math and Writing show marked improvements, underscoring CopySpec's ability to handle computationally intensive tasks effectively.\nTable 13 highlights the results for Qwen2.5-7B-Chat. While the base model already performs efficiently, CopySpec further enhances TPS, with average speed-ups exceeding 1.3\u00d7 in the second turn. These results confirm that CopySpec scales well across different model sizes.\nThe performance of LLaMa3.1-70B-Instruct is detailed in Table 14. CopySpec achieves consistent improvements across both turns, with substantial gains in computationally intensive categories such as Coding and Extraction. These results demonstrate the robustness of CopySpec when applied to larger models.\nTable 15 evaluates LLaMa3.1-8B-Instruct. While the model size is significantly smaller, CopySpec still yields notable improvements, particularly in the second turn, where repetitive token patterns amplify the efficiency of speculative copying."}, {"title": "D.3. Merging with Speculative Decoding on MT-Bench", "content": "Finally, Table 16 and Table 17 compares different speculative decoding configurations with and without CopySpec, using Qwen2.5-32B-Instruct as the target model and Qwen2.5-7B-Instruct as the draft model. This analysis explores the impact of varying y values and draft token counts, demonstrating that the integration of CopySpec with speculative decoding consistently leads to enhanced performance. The results emphasize the adaptability of CopySpec across diverse operational settings.\nThese tables collectively validate the effectiveness of CopySpec in accelerating large language model inference while maintaining high output quality. The findings in this appendix complement those in Appendix C, reinforcing the method's utility across datasets and configurations."}, {"title": "E. Extra Results on GSM-8K", "content": "This appendix provides an in-depth analysis of the CopySpec approach applied to self-correcting tasks and speculative decoding. The results demonstrate the effectiveness of CopySpec in improving token processing speed, leveraging context repetition, and enhancing self-correction efficiency without compromising model accuracy."}, {"title": "F. MT-Redundant Dataset Examples", "content": "This appendix provides one illustrative example from each of the eight categories in our new MT-Redundant dataset. MT-Redundant builds upon MT-Bench by modifying the second turn of each conversation into a request for variations or adjustments of the first turn's response, thus emulating real-world scenarios in which users seek revisions to previous outputs.\nSpecifically, we replace the original second-turn prompt in MT-Bench (Zheng et al., 2023) with one that instructs the model to revisit and refine its previous answer. All assistant responses in this appendix are generated using Qwen2.5-72B-Instruct."}, {"title": "G. Prompts Used", "content": ""}, {"title": "G.1. Example of Self-Correction on GSM-8K", "content": "This appendix presents an example of self-correction in code generation on the GSM-8K dataset. Using Qwen2.5-72B-Instruct, we generate an initial solution and apply multi-round prompting to iteratively refine and correct the generated code.\nTo ensure direct answer generation, we prompt the model to explicitly print the computed result, reducing intermediate ambiguities and improving overall accuracy."}, {"title": "G.2. Example of Extractive Summarization", "content": ""}]}