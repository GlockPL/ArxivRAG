{"title": "Estimating Commonsense Plausibility through Semantic Shifts", "authors": ["Wanqing Cui", "Keping Bi", "Jiafeng Guo", "Xueqi Cheng"], "abstract": "Commonsense plausibility estimation is critical for evaluating language models (LMs), yet existing generative approaches-reliant on likelihoods or verbalized judgments-struggle with fine-grained discrimination. In this paper, we propose ComPaSS, a novel discriminative framework that quantifies commonsense plausibility by measuring semantic shifts when augmenting sentences with commonsense-related information. Plausible augmentations induce minimal shifts in semantics, while implausible ones result in substantial deviations. Evaluations on two types of fine-grained commonsense plausibility estimation tasks across different backbones, including LLMs and vision-language models (VLMs), show that ComPaSS consistently outperforms baselines. It demonstrates the advantage of discriminative approaches over generative methods in fine-grained commonsense plausibility evaluation. Experiments also show that (1) VLMs yield superior performance to LMs, when integrated with ComPaSS, on vision-grounded commonsense tasks. (2) contrastive pre-training sharpens backbone models' ability to capture semantic nuances, thereby further enhancing ComPaSS.", "sections": [{"title": "1 Introduction", "content": "Commonsense knowledge\u2013the shared understanding of everyday phenomena and human experiences (Schank, 1983; Winograd, 1986; Hobbs, 1990)-is foundational to natural language understanding and generation. Despite the remarkable progress in large language models' (LLMs) text generation capabilities, ensuring commonsense plausibility in their outputs remains an unresolved challenge (Marcus, 2020; Elazar et al., 2021; Mahowald et al., 2024; Chen et al., 2023). This challenge arises not only from the inherent difficulty of acquiring and applying commonsense knowledge but also from the absence of reliable frameworks for evaluating textual plausibility. Effective evaluation of commonsense plausibility addresses this gap twofold: it identifies commonsense violations while offering quantifiable metrics to guide the development of techniques that augment LLM outputs.\nIn this work, we focus on developing generalizable methods for commonsense plausibility estimation (CSPE) that can be applied across diverse domains and tasks. This leads us to investigate zero-shot and few-shot approaches based on pre-trained LMs, which leverage their inherent knowledge without requiring additional training data or domain-specific fine-tuning.\nPrevious studies on zero or few-shot CSPE primarily adopt a generative perspective and can be categorized into two main approaches, likelihood estimation and verbalized judgments. The likelihood-based methods (Trinh and Le, 2018; Tamborrino et al., 2020; Holtzman et al., 2021) utilize token prediction probabilities from language models as an indicator, with the assumption that sentences consistent with commonsense knowledge tend to have a higher likelihood for their component tokens. The verbalization-based methods (Brown et al., 2020; Krause and Stolzenburg, 2024) ask pre-trained LMs to answer the plausibility of a sentence through natural language. The models can generate the answer based on knowledge stored in their parameters.\nHowever, approaches based on the generative perspective could be suboptimal for CSPE, since it is essentially a discriminative task. In this paper, we adopt a discriminative perspective for CSPE. In communication, commonsense knowledge is often assumed and left unstated, yet such omissions rarely hinder mutual understanding (Clark, 1996; Noveck and Sperber, 2004). Inspired by this, we propose ComPaSS, a method that measures Commonsense Plausibility through Semantic Shifts introduced when augmenting sentences with commonsense-related information. Plausible additions yield minimal semantic shifts, whereas implausible ones result in substantial deviations. For instance, adding 'black' to 'There is a penguin' results in a minor semantic shift, aligning with the penguins' natural coloration. By contrast, introducing 'green' creates a substantial shift, highlighting the implausibility of such an atypical attribute. To quantify semantic shifts, ComPaSS computes the similarity between embeddings of the original sentence (without explicit commonsense references) and its modified counterpart augmented with commonsense-related information.\nTwo aspects of semantic representations could influence the capability of ComPaSS in CSPE: the inclusion of commonsense knowledge and the discrimination of semantic nuances. These correspond to two key aspects of models used for obtaining sentence embeddings: 1) Modality. Language Models (LMs) often suffer from reporting bias (Gordon and Durme, 2013), which involves systematic distortions due to omitted commonsense details (e.g., 'penguins are black' is rarely stated) and statistical biases from fixed linguistic patterns (e.g., 'black sheep'). In contrast, vision-language models (VLMs) incorporate visual information, thus mitigating reporting bias, especially for visually-grounded commonsense knowledge (e.g., object colors or spatial relations) (Paik et al., 2021; Zhang et al., 2022). 2) Contrastive learning. By training a model to distinguish between semantically similar and dissimilar instances, it enhances the model's discriminative power. Representations from contrastively trained models exhibit sharper separability, which directly impacts the precision of semantic shift measurements. Given these considerations, we study how ComPaSS performs based on various backbones of both LMs and VLMs, with and without contrastive learning.\nWe evaluate ComPaSS against baselines on two fine-grained CSPE tasks that require ranking candidate answers by plausibility rather than binary classification. These tasks prioritize nuanced plausibility judgments, where answers may hold varying degrees of validity. The first task, attribute value ranking, involves ranking candidate attribute values (e.g., color, shape, material) for objects using structured triplets (e.g., determining that \"black\" is more plausible than \"green\" for penguin-color), evaluated on datasets like CoDa (Paik et al., 2021) and ViComTe (Zhang et al., 2022). The second task, commonsense frame completion (Cheng et al., 2024), challenges models to rank plausible completions for open-ended prompts (e.g., selecting 'farm' over 'truck' for 'Where are farmers with newly harvested crops?'), testing alignment with human preferences and broader commonsense reasoning. Together, these tasks assess ComPaSS across input formats (structured triplets vs. free-form text) and knowledge types (object-specific attributes vs. contextual, inferential commonsense).\nOur experiments reveal three critical insights. First, as a discriminative approach, ComPaSS consistently outperforms prior generative methods in fine-grained plausibility estimation, achieving superior results across diverse model backbones. This highlights the advantage of discriminative methods in capturing subtle plausibility distinctions. Second, utilizing ComPaSS, VLMs significantly outperform LMs for vision-grounded commonsense (e.g., object colors or shapes), demonstrating that visual information enhances representations and benefits CSPE. Third, models with contrastive pre-training yield significantly better results than those without, emphasizing the importance of representations that capture semantic nuances in plausibility measurement through ComPaSS."}, {"title": "2 Related Work", "content": "2.1 CSPE Based on Internal Knowledge\nThe sentence probability and perplexity computed by LMs can serve as indicators of commonsense plausibility, even in zero-shot settings (Trinh and Le, 2018; Davison et al., 2019; Liu et al., 2021a). For LLMs with instruction-following capability, they can be directly prompted to judge whether a given input is consistent with commonsense or not (Zhao et al., 2024). Beyond directly judging plausibility, some methods (Jung et al., 2022; Tafjord et al., 2022) evaluate the plausibility of hypotheses by scoring the validity of entailment paths generated by the LLMs, i.e., the reasoning chains justifying 'reasonable' or 'unreasonable' conclusions, and selecting the final prediction based on the highest-scoring path. VERA (Liu et al., 2023) adopts a discriminative approach, training a classification head to make predictions based on model representations, which fine-tunes LLMs on~7 million commonsense statements. In contrast, our approach also leverages internal knowledge from a discriminative perspective but does not require additional training."}, {"title": "2.2 CSPE Based on External Knowledge", "content": "Language models (LMs) may have insufficient or inaccurate knowledge, which led to some methods to incorporate external knowledge to better estimate commonsense plausibility. A typical approach is to augment the model's knowledge by retrieving relevant sentences from external sources (Zhang et al., 2021; Yu et al., 2022). Commonsense knowledge bases (KBs) (Speer et al., 2016; Sap et al., 2019; Hwang et al., 2020) store extensive commonsense knowledge, enabling the extraction of relevant subgraphs to evaluate sentence consistency with commonsense (Choi et al., 2022). To alleviate the coverage limitations of the KBs while leveraging the extensive knowledge encoded in LMs, COMET (Bosselut et al., 2019) introduced a dynamic KB by pre-training LM on existing commonsense KBs. Methods that utilize this dynamic KB (Ghazarian et al., 2023; Tian et al., 2023) demonstrate improved generalization across various commonsense reasoning tasks."}, {"title": "3 Task Definition", "content": "Formally, given an input instance $X_i = (c; a)$ consisting of a context $c$ and a candidate information $a \\in A$, where $A = \\{a_1, a_2, ..., a_K\\}$ denotes the context-dependent candidate set with size $K$, the task is to predict a plausibility score set $P_c = \\{p_1^c, p_2^c, ...,p_K^c\\}$ for all candidates, where each $p_i^c \\in \\mathbb{R}$ quantifies the plausibility of augmenting $c$ with $a_i$. The ground-truth scores are denoted as $G_c = \\{g_1, g_2, ..., g_K\\}$, where $g_i$ indicates the true score of $a_i$. Performance is measured by the correlation between $P_c$ and $G_c$.\nThe input can take two specific forms: for attribute value ranking task, the input is a structured triplet $x_i = (o, \\text{ has property } p; a)$. The context $c = (o, \\text{ has property } p)$, where o is a common object and p is a property. The candidate a represents the i-th attribute value for the specified property. For the commonsense frame completion task, the context c = q is a free-form question, the input is a question-answer pair $x_i = (q; a_i)$, where $a_i$ is the i-th plausible answer to this question."}, {"title": "4 ComPaSS", "content": "Our method, ComPaSS, is a zero-shot approach for estimating commonsense plausibility. We demonstrate in Figure 1 how this method works on different tasks. For each input, we first construct an anchor sentence (omitting the commonsense-related detail) and a candidate sentence (augmenting that detail). We then encode both sentences individually to obtain their semantic representations. Next, we calculate their semantic similarity, where the degree of semantic shift\u2014inversely proportional to similarity-quantifies plausibility."}, {"title": "4.1 Constructing Sentences", "content": "For each input context c and the candidate to be evaluated $a_i$, we construct two types of sentences: an anchor sentence $S_{anchor}$ that contains only the base context c while omitting target details, and a candidate sentence $S_{candi}$ that further incorporates commonsense-related information $a_i$. The construction process varies based on input type but follows a unified framework:\n$S_{anchor} = f_{anchor}(c, z_{anchor}),$ (1)\n$S_{candi} = f_{candi}(c, a_i, z_{candi}),$ (2)\nwhere $f(\u00b7) \\in \\{f_{anchor}(\u00b7), f_{candi}(\u00b7)\\}$ denotes the construction function, and $z \\in \\{z_{anchor}, z_{candi}\\}$ denotes task-specific templates or prompts.\nAs illustrated in Figure 1, the framework is instantiated differently based on the input format: For triplet inputs, we employ template-based construction, where z represents a pre-defined template (see Appendix A) and f(\u00b7) represents applying this template to generate a sentence. For question-answer pairs, we query GPT-4 (Achiam et al., 2023) for sentence transformation, where z denotes the prompt (see Appendix B) and f(\u00b7) represents query GPT-4 using the specified prompt. Since questions cannot be directly converted into coherent statements, we use a blank space as a placeholder when constructing anchor sentences."}, {"title": "4.2 Representing Sentences", "content": "Given anchor and candidate sentences, we encode them into dense semantic representations using a pre-trained model $\\theta$, which can be either a LM or a VLM. For each sentence $s \\in \\{S_{anchor}, S_{candi}\\}$, the model first processes the sentence along with special tokens (e.g., [CLS], [EOS], or others depending on the model architecture) and then outputs token hidden states:\n$H = \\theta(s) = \\{h_0, h_1, ..., h_l\\},$ (3)\nwhere $l$ denotes the sequence length, including the special tokens. The final sentence representation $r \\in \\{r_{anchor}, r_{candi}\\}$ is derived through architecture-specific strategies.\nFor encoder models, we use the hidden state of the designated semantic aggregation token as sentence representation. Some models (e.g., RoBERTa (Liu et al., 2021b)) use the initial '[CLS]' token for sentence representation (r = $h_0$), while others (e.g., CLIP (Radford et al., 2021)) utilize the final '[EOS]' token embedding (r = $h_l$).\nFor decoder models, we use the hidden state of the last token as sentence representation r = $h_l$, which naturally encapsulates the accumulated context. Alternatively, PromptReps (Zhuang et al., 2024) prompts the model to generate a new representative token at position $l+1$, using its hidden state as the sentence representation (r = $h_{l+1}$). We apply this strategy to models that are not enhanced by contrastive learning.\nThis architecture-aware representation strategy ensures ComPaSS's flexibility across different model backbones while maintaining optimal performance for each specific architecture."}, {"title": "4.3 Ranking with Semantic Shifts", "content": "We rank the candidate option $a_i$ by measuring how naturally it integrates into the context, quantified through semantic similarity between the anchor sentence representation $r_{anchor}$ and the candidate sentence representation $r_{candi}$. The underlying principle is that the more plausible the information, the smaller the semantic shifts it induces when added to the context, leading to higher semantic similarity. Formally, we define the commonsense plausibility score $p_i^c$ for each candidate $a_i$ as:\n$p_i^c \\approx \\text{sim}(r_{anchor}, r_{candi}),$ (4)\nwhere sim(\u00b7) denotes a similarity function (e.g., cosine similarity or dot product). Candidates are then ranked by their plausibility scores descendingly, with higher-ranked candidates representing more commonsense-consistent answers."}, {"title": "4.4 Discussion of Applicable LMs", "content": "This paragraph discusses the differences in applicable LMs between ComPaSS and generative methods based on likelihoods and verbalization. ComPaSS can utilize both encoder and decoder style models as long as they can yield reasonable sentence representations. Likelihood-based approaches can also leverage these two types of LMs. Candidate likelihoods can be estimated based on masked/next token prediction for encoders and decoders respectively. In contrast, verbalization-based approaches require LLMs-decoder-only LMs-to answer the plausibility estimation questions. This indicates the broader applicability of ComPaSS."}, {"title": "5 Experimental Setup", "content": "5.1 Datasets\nWe evaluate methods through two types of fine-grained commonsense plausibility estimation (CSPE) tasks, where candidates should be ranked based on commonsense plausibility. These tasks are carefully chosen to comprehensively evaluate methods across varying input formats (from structured triplets to free-form text) and commonsense knowledge levels (from specific attribute knowledge to general commonsense knowledge)."}, {"title": "5.1.1 Structured Attribute Knowledge", "content": "Color Dataset (CoDa) 1 (Paik et al., 2021) is a human-annotated dataset used for attribute value ranking, which provides color distributions for commonly recognized objects. It contains 521 objects, each with 11 candidate color attributes.\nVisual Commonsense Tests (ViComTe) 2 (Zhang et al., 2022) is another dataset used for attribute value ranking, which derived from the multimodal dataset, i.e., Visual Genome (Krishna et al., 2017). It offers attribute value distributions across a broader set of properties, including color, shape, and material. It contains 2,877 objects with 12 candidate color attributes, 706 objects with 12 candidate shape attributes, and 1,423 objects with 18 candidate material attributes."}, {"title": "5.1.2 Free-form General Knowledge", "content": "Commonsense Frame Completion (CFC) 3 (Cheng et al., 2024) is a dataset designed to evaluate implicit commonsense reasoning, which consists of questions accompanied by multiple plausible answers with human-annotated preference scores. It requires models to make probabilistic judgments about answer plausibility. The evaluation protocol employs a probabilistic framework that measures how well a model's predicted answer distribution aligns with human preferences. As the test set is not public, we use the validation set containing 55 questions for zero-shot evaluation."}, {"title": "5.2 Evaluation Metrics", "content": "Spearman's rank correlation coefficient \u03c1: We choose this as the primary metric following CoDa and ViComTe. It measures the correlation between the predicted ranks of candidates and their ground-truth ranks, focusing on the relative ordering rather than exact values. This emphasis on relative ordering aligns with the nature of commonsense plausibility assessment, where the exact probability values are less important than correctly identifying more plausible options over less plausible ones. A \u03c1 value of 1 indicates perfect correlation, 0 indicates no correlation, and -1 indicates perfect negative correlation.\nAccuracy: CoDa and ViComTe include binary comparison tasks where each object is paired with two attribute values, with one being more plausible than the other. The model need to rank the more plausible value higher. We use accuracy as the evaluation metric, which measures the proportion of correct rankings. This metric is particularly suitable for cross-attribute comparisons as it is unaffected by variations in the number of candidates, unlike the Spearman's rank correlation coefficient."}, {"title": "5.3 Methods for Comparison", "content": "5.3.1 ComPaSS with Various Backbones\nWe evaluate ComPaSS across diverse model architectures to assess its adaptability:\nFor LMs, we consider various open-source models, including RoBERTa-Large (Liu et al., 2021b) (RoBERTa), a widely used encoder-only LM, along with two decoder-only LLMs, Mistral-7B-Instruct (Jiang et al., 2023) (Mistral) and Qwen2-7B-instruct (qwe, 2024) (Qwen2), both demonstrating strong instruction-following capabilities. We also evaluate their contrastive learning-enhanced variants, i.e., sup-SimCSE-RoBERTa-Large (Gao et al., 2021) (ROBERTaw/CL), E5-Mistral-7B-Instruct (Wang et al., 2023, 2022) (Mistralw/CL) and gte-Qwen2-7B-instruct (Li et al., 2023) (Qwen2w/CL).\nFor VLMs, we test CLIP-ViT-L/14 (Radford et al., 2021) (CLIP), a multimodal representation model trained on image-text pairs using contrastive learning, which aligns semantically similar images and text into closely matching representations. We also consider its advanced variant EVA-CLIP-8B (Sun et al., 2023) (EVA-CLIP) with improved performance."}, {"title": "5.3.2 Baselines", "content": "We compare against two categories of baseline methods:\nCommonsense models (CSMs): These models are specifically designed for modeling commonsense knowledge: COMET-Atomic-2020-Bart (Bosselut et al., 2019) (COMET-Atomic) is a commonsense LM pre-trained on commonsense KBs. COMET is suitable for processing triple input, which can generate a probability score for each candidate. ACCENT (Ghazarian et al., 2023) assesses the commonsense plausibility of a sentence by first extracting structured tuples and then scoring them based on their compatibility with a commonsense KB. VERA-T5-XXL (Liu et al., 2023) (VERA-T5) is trained on ~7M commonsense statements and can directly estimate the commonsense plausibility of statements.\nLanguage models (LMs): We evaluate all open-source LMs used as the backbone of ComPaSS with both likelihood-based and verbalization-based approaches. For the likelihood-based method, the plausibility of a sentence is determined by the probability of predicting each token in the sentence sequentially, normalized by sentence length. A higher probability indicates greater plausibility. In the case of verbalization-based method, pre-trained language models are prompted in natural language (see Appendix C) to rank all candidate responses based on their plausibility. We also test closed-source LLMs including gpt-3.5-turbo-0125 (OpenAI, 2022) (GPT-3.5) and gpt-4-0125-preview (Achiam et al., 2023) (GPT-4), the latter introduces multi-modality and has superior capabilities."}, {"title": "6 Results and Analysis", "content": "6.1 Overall Results\nThe overall experimental results comparing baseline methods with our approach are presented in Table 1, which reveals several key findings:\nComPaSS achieves the best performance compared to baselines. Further comparison between RoBERTa, Mistral, and Qwen2, with and without ComPaSS, shows a consistent improvement when ComPaSS is applied. This validates our method's architecture-agnostic effectiveness. Notably, even VERA, which was specifically fine-tuned for CSPE, achieves only comparable performance to ComPaSS-enhanced models. Comparing the performance of different methods on LMs in the baseline, we find that verbalization-based methods fail to consistently outperform likelihood-based approaches, even when applied to generative models. This limitation highlights the challenges such methods face in making fine-grained distinctions required for precise plausibility estimation.\nVLMs demonstrate superior effectiveness in learning visual-related commonsense knowledge. Comparing the ComPaSS methods based on various backbones, we find VLMs exhibit particular strength in visual attribute ranking, with EVA-CLIP achieving the highest scores on CoDa (62.87), Color (51.73), and Shape (48.05), significantly outperforming even 7B parameter LLMs. This performance gap persists despite the LLMs' access to large-scale text corpora and additional parameters, underscoring the unique value of visual supervision. This performance gap highlights the limitations of text-only training, as even extensive textual data and additional parameters cannot fully compensate for the lack of visual grounding, which underscores the importance of multimodal learning for comprehensive commonsense understanding.\nDiscriminative approaches may offer a more parameter-efficient pathway compared to generative methods. Our experiments reveal that encoder-only models with millions of parameters like RoBERTa and CLIP-series models achieve comparable or even superior results to much larger decoder-only models (with billions of parameters) when combined with ComPaSS. This suggests that our discriminative method effectively leverages the semantic representation strengths of encoder models, which are generally more parameter-efficient than generative models. By focusing on representation-level semantics rather than token generation, ComPaSS aligns closely with the pre-training objectives of encoder models, maximizing their representation power.\nThe ability to discern semantic nuances in sentence representations is crucial for the performance of ComPaSS. As shown in Table 2, experiments with different RoBERTa variants reveal that applying ComPaSS to vanilla RoBERTa, which has weaker representation capabilities, leads to performance degradation. However, incorporating contrastive learning significantly improves performance, with even unsupervised contrastive training yielding substantial gains. Contrastive pre-training enables even subtle plausibility distinctions to manifest as measurable shifts in embedding space, making it essential to the performance of ComPaSS."}, {"title": "6.2 Further Analyses", "content": "6.2.1 Comparisons to Closed-source Models\nWe extend our evaluation to include state-of-the-art closed-source models, with results presented in Table 3. Notably, our method outperforms even GPT-4 across multiple tasks, demonstrating its effectiveness in fine-grained CSPE. This performance gap further highlights the limitations of verbalization-based approaches in capturing subtle distinctions required for precise plausibility estimation.\n6.2.2 Granular Analysis of Attribute Types\nWe analyze binary comparison results on CoDa and ViComTe across three attribute groups: single: includes objects with one dominant attribute value (e.g., snow's color), multi: includes objects with attributes mainly distributed among the top four values (e.g., a penguin's color), and any: includes objects with a broader attribute distribution (e.g., a T-shirt's color). As shown in Figure 3, VLMs demonstrate particular strength in the single group. This advantage primarily stems from how visual information overcomes reporting bias in textual data. For objects in the single category, their most common attribute is often not explicitly mentioned in text due to its widespread acceptance as common knowledge. However, these attributes are consistently and explicitly depicted in visual data. This inherent visual grounding allows VLMs to capture stereotypical attributes more effectively than text-based LLMs."}, {"title": "6.2.3 Effect of Template Format", "content": "We investigate the importance of sentence-level context in semantic shift measurement by comparing two approaches: word collocation comparison (e.g., 'penguin' and 'black penguin') and full sentence construction (e.g., \u2018There is a penguin' and 'There is a black penguin'). As shown in Figure 3(a), sentence-level inputs consistently outperform word-level comparisons for both LLMs and VLMs. This performance gap underscores the importance of complete sentence construction for ComPaSS, as sentence-level inputs better align with models' pre-training data formats."}, {"title": "6.2.4 Template Ensemble Methods", "content": "We investigate three template utilization strategies: The single-optimal ensemble approach uses the unified best-performing template, serving as an implicit ensemble. For explicit ensemble methods, score-level ensemble averages prediction scores across multiple templates, and representation-level ensemble fuses sentence representations from several templates before computing the final score. As shown in Figure 3 (b), both explicit ensemble strategies significantly improve LLM performance, with score-level ensemble showing more consistent gains. However, VLM shows limited improvement from ensemble methods, likely due to their simpler pre-training data structure. This contrast highlights LLMs' sensitivity to linguistic variations and their ability to benefit from diverse syntactic structures."}, {"title": "6.3 Case Study", "content": "We use the classic 'black sheep problem' to intuitively explain why ComPaSS is effective. Since 'black sheep' is an idiom, one is much more likely to mention a 'black sheep' than to specify the color of a sheep. Such reporting bias confuses the LMs that learn knowledge through probabilistic modeling. As shown in Figure 4, GPT-3.5 and GPT-4 both overestimate the probability of \u2018black\u2019 being the color of sheep even though sheep in black are rare. In contrast, our approach relies on semantic rather than probabilistic likelihood is able to distinguish between the linguistic meaning and the visual recognition of 'a black sheep', resulting in a more accurate estimation of the sheep's color. In addition, VLM calibrates the color distribution well by incorporating visual information."}, {"title": "7 Conclusion", "content": "We introduce ComPaSS, a discriminative framework for fine-grained commonsense plausibility estimation via semantic shift measurement. By leveraging the idea that plausible commonsense augmentations cause minimal semantic deviation, ComPaSS offers a generalizable approach for various tasks and model architectures. Our experiments show that discriminative methods outperform generative approaches in capturing nuanced plausibility distinctions, with ComPaSS consistently surpassing likelihood-based and verbalization-based baselines. Vision-language models also excel on visually-grounded commonsense tasks, addressing reporting bias through multimodal alignment. Finally, we emphasize the role of contrastive pre-training in improving semantic representation quality, directly enhancing plausibility estimation accuracy. Overall, ComPaSS highlights the value of utilizing semantic embeddings to extract commonsense knowledge from pre-trained models."}, {"title": "8 Limitations and Ethical Considerations", "content": "ComPaSS faces challenges in making absolute pointwise judgments. The method's reliance on semantic shift measurement inherently provides comparative assessments rather than definitive plausibility scores. This limitation stems from the difficulty in establishing absolute semantic distance thresholds for plausibility classification. Future work could explore calibration techniques to bridge this gap. In addition, for attribute value ranking task, our method relies on predefined templates to construct sentences for objects and candidate attributes. Automating template generation could be an important avenue for future improvement.\nAs our method relies on LLMs and VLMs, it inherits potential biases present in the training data. These biases, whether related to societal stereotypes or uneven distribution of information across certain attributes, could affect the model's judgment in ranking attribute plausibility. Consequently, our method may inadvertently perpetuate or amplify these biases, especially in scenarios where the model's understanding of an attribute is skewed by biased representations in the data. Addressing these biases is an important avenue for future work."}, {"title": "A Templates for Sentence Construction", "content": "The templates we used to construct anchor sentences and candidate sentences of different property are shown in Table A."}, {"title": "B Prompt for Sentence Transformation", "content": "The prompt we use for converting question-answer pair can be found in Figure 6."}, {"title": "C Prompt for Verbalization-based Method", "content": "The prompt we use for the verbalization-based method can be found in Figure ??"}, {"title": "D More Experimental Results", "content": "Since not all models are compatible with all methods, we exclude the results of incompatible model-method combinations from the main text. The complete results are provided in Table 5. Notably, the results of Mistralw/CL with the verbalization-based method is 0, as this model, trained via contrastive learning, has significantly lost its ability to follow instructions, preventing it from generating reasonable responses based on prompts."}]}