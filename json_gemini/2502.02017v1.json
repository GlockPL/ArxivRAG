{"title": "Multi-Domain Graph Foundation Models: Robust Knowledge Transfer via Topology Alignment", "authors": ["Shuo Wang", "Bokui Wang", "Zhixiang Shen", "Boyan Deng", "Zhao Kang"], "abstract": "Recent advances in CV and NLP have inspired researchers to develop general-purpose graph foundation models through pre-training across diverse domains. However, a fundamental challenge arises from the substantial differences in graph topologies across domains. Additionally, real-world graphs are often sparse and prone to noisy connections and adversarial attacks. To address these issues, we propose the Multi-Domain Graph Foundation Model (MDGFM), a unified framework that aligns and leverages cross-domain topological information to facilitate robust knowledge transfer. MDGFM bridges different domains by adaptively balancing features and topology while refining original graphs to eliminate noise and align topological structures. To further enhance knowledge transfer, we introduce an efficient prompt-tuning approach. By aligning topologies, MDGFM not only improves multi-domain pre-training but also enables robust knowledge transfer to unseen domains. Theoretical analyses provide guarantees of MDGFM's effectiveness and domain generalization capabilities. Extensive experiments on both homophilic and heterophilic graph datasets validate the robustness and efficacy of our method.", "sections": [{"title": "1. Introduction", "content": "Graphs, as a versatile data structure, are widely used across various domains, such as citation networks (Ebesu & Fang, 2017), social networks (Traud et al., 2012), and bioinformatics (Zhang et al., 2021). Inspired by recent advances in CV and NLP (Vidit et al., 2023; Cheng et al., 2024), researchers have sought to develop general-purpose graph foundation models. In particular, multi-domain graph pre-training has gained significant attention for its ability to integrate knowledge from various domains and enable effective transfer learning (Yu et al., 2025; Zhang et al., 2024b). This approach is viewed as a critical milestone toward the creation of truly general-purpose graph models.\nDespite significant progress, a substantial gap remains in fully understanding the richness and diversity of graph structural knowledge. Existing methods primarily rely on fixed graph topologies and apply uniform encoding mechanisms across all domains (Zhao et al., 2024; Yu et al., 2025), which severely limits their generalizability across diverse domains (Zhang et al., 2024a). In this paper, we revisit the multi-domain graph foundation model from a structural perspective and address a critical challenge: topology alignment across different domains. Achieving effective topology alignment is a non-trivial task, presenting two primary challenges that must be overcome to advance toward a truly general-purpose graph model.\nFirst, structural knowledge across domains often exhibits significant semantic differences. For instance, citation networks predominantly display highly homophilic patterns, whereas social networks and webpage link graphs frequently contain numerous heterophilic edges (Zheng et al., 2022). These differences necessitate domain-specific adaptations of encoding mechanisms. As a result, there is an urgent need for a unified framework that can adaptively capture critical information from both features and topology while effectively learning domain-invariant knowledge. Such a framework is essential to ensure robust generalization to downstream domains (Li et al., 2022).\nSecond, real-world graphs are inherently noisy (Jin et al., 2020b), often containing unreliable edges characterized by irrelevant, misleading, or missing connections. Additionally, graph learning algorithms are highly susceptible to adversarial attacks (Z\u00fcgner et al., 2020), further exacerbating the challenges of graph representation learning. These issues highlight the limitations of conventional training paradigms that rely on fixed graph topologies. Therefore, developing a robust multi-domain pre-training framework capable of effectively handling complex noise distributions is critical for constructing trustworthy and reliable graph foundation models."}, {"title": "2. Related work", "content": ""}, {"title": "2.1. Graph Pre-training", "content": "Graph pre-training aims to train Graph Neural Networks (GNNs) on large amounts of unlabeled data, enabling the transfer of the learned model to downstream tasks with limited supervision. This approach facilitates the acquisition of general knowledge about real-world graphs while reducing the dependence on labeled data (Hu et al., 2020; Jin et al., 2020a).\nPre-training methods can be broadly categorized based on their downstream task-tuning strategies. The first category follows a pretrain-and-fine-tune paradigm, emphasizing effective pre-training strategies in the upstream phase through self-supervised learning. For example, DGI (Velickovic et al., 2019) enhances training by maximizing the mutual information between global and local representations. Similarly, GraphCL (You et al., 2020) and SimGRACE (Xia et al., 2022) focus on minimizing the distance between representations of different augmentations, effectively capturing invariant and robust structural information.\nThe second type follows a pretrain-and-prompt-tuning paradigm, where pre-trained models are not fine-tuned for downstream tasks. Instead, these methods reformulate the input data to align with the pretext task (Gao et al., 2020). For instance, GPPT (Sun et al., 2022a) introduces a graph prompting function that transforms independent nodes into token pairs, reframing downstream node classification as an edge prediction task. Similarly, GPF (Fang et al., 2024) employs learnable perturbations in the feature space of downstream graphs, enabling implicit modifications to node features and graph structures. However, most existing methods are constrained to single-domain pre-training and tuning, significantly limiting their capacity to capture cross-domain knowledge and generalize to unseen domains."}, {"title": "2.2. Multi-domain Generalization", "content": "Domain generalization aims to achieve out-of-distribution (OOD) generalization by learning from multiple source domains (Zhou et al., 2022). In contrast to domain adaptation-which transfers prior knowledge from a single source domain to a specific target domain-domain generalization focuses on leveraging diverse information from multiple source domains to generalize effectively to unseen domains. This approach addresses two critical challenges: domain shift and the absence of target domain data (Blanchard et al., 2011).\nRecently, domain generalization on graphs has gained attention. These methods integrate and extract knowledge from multiple source domains during the upstream pre-training phase, enabling the transfer of this knowledge to tackle various graph-related tasks in previously unseen downstream domains. For example, GCOPE (Zhao et al., 2024) integrates multi-source graph topologies during the pre-training phase by introducing interconnected virtual nodes. Additionally, MDGPT (Yu et al., 2025) incorporates domain-specific tokens in the pre-training phase to align node features from different domains, and employs prompt-tuning during downstream tasks for efficient knowledge transfer. Despite these advancements, significant gaps remain in understanding the semantic differences and reliability of multi-domain topologies. Consequently, there is an urgent need to design a universal foundation model capable of achieving robust and generalized knowledge transfer across domains."}, {"title": "3. Problem Definition", "content": "A graph is formally represented as $G = (V, E, X_{ori}) = (A, X_{ori})$, where V is the set of nodes and E denotes the set of edges, $X_{ori} \\in \\mathbb{R}^{|V|\\times d'}$ represents the original feature matrix of nodes. A is the corresponding adjacency matrix. Each graph is associated with a domain $D_i \\in D$. Without loss of generality, we assume $D_i \\neq D_j$ for any pair of distinct graphs.\nGiven a set of graphs from different domains, say $G_i = (A_i, X_{ori}), i = 1,..., N$. We pretrain our graph foundation model on all visible graphs and test the performance on graph T from an unseen target domain $D_T \\notin D$. We summarize the notations in Appendix A."}, {"title": "4. Methodology", "content": "As illustrated in Figure 1, our proposed MDGFM consists of two modules: the Multi-domain pre-training module and the Downstream target-domain adaptation module.\nMulti-domain pre-training. For graphs originating from multiple source domains, we begin by unifying their feature matrices to a common dimensionality, ensuring compatibility across domains. Next, we apply token operators specifically designed to achieve semantic alignment between disparate graphs. To address the challenges posed by heterophily in downstream tasks, we leverage graph structure learning (GSL) to refine each source domain, integrating both feature and topology information during the refinement process. Additionally, GSL enables the model to learn domain-invariant knowledge for better transfer, where we prove the effectiveness in Section 5. Finally, the graph encoder is trained effectively using self-supervised signals, enabling robust representation learning.\nDownstream target-domain adaptation. Given that graphs often share common intrinsic patterns, pre-trained knowledge can be effectively transferred to unseen graphs (Zhao et al., 2024). To bridge the gap between source and target domains, we propose a dual-stream prompt framework. This framework combines meta prompts, which transfer broadly learned knowledge, with task-specific prompts designed to align with the unique characteristics of the downstream domain."}, {"title": "4.1. Feature Projection into Unified Semantic Space", "content": "Graphs from different domains often exhibit diverse features and structural patterns. To address these discrepancies and customize the information propagation process in GNNs, we employ both numerical and semantic alignment to bridge domain gaps. First, we transform the feature matrix of each graph into a consistent dimensionality shared across all domains, enabling uniform representation. This transformation is described by:\n$X_i = Proj(X_{ori}) \\in \\mathbb{R}^{|V_i|\\times d}$ (1)\nwhere d represents the aligned dimensionality, and $Proj(\\cdot)$ denotes a specific projection operation. In our approach, the Projection function is implemented using Principal Component Analysis (PCA) (Abdi & Williams, 2010). While this dimensional alignment ensures uniform feature dimensions across graphs, it primarily addresses numerical consistency and does not resolve the semantic disparities that remain between domains.\nTo achieve semantic alignment, we propose the concept of domain tokens denoted as $t_{D_i} \\in \\mathbb{R}^d$, which encode domain-specific characteristics by capturing unique contextual information from each domain. These tokens are applied via element-wise Hadamard multiplication with the feature matrices, acting as adaptive filters that modulate features to reflect their respective domain properties within a unified semantic space. This process not only preserves the distinctive traits of individual domains but also facilitates their alignment within a cohesive semantic framework.\nA significant challenge in building a graph foundation model lies in the substantial semantic variations across domains, which can hinder effective knowledge transfer (Hassani, 2022). To address this, we leverage shared, domain-agnostic information as a critical foundation for robust and efficient transfer learning. Specifically, we introduce the shared token $t_s \\in \\mathbb{R}^d$ which acts as a common semantic anchor to bridge discrepancies between domains. By focusing on shared patterns, $t_s$ captures transferable knowledge broadly applicable to new domains, reducing dependence on domain-specific features that may lack generalizability. This shared representation enables consistent semantic alignment, enhancing the model's adaptability and robustness to domain variations. The overall feature unification procedure can be formalized as:\n$X' = t_s \\odot (t_{D_i} \\odot X_i)$ (2)\nwhere $\\sigma(\\cdot)$ is non-linearity activation function to capture complex information."}, {"title": "4.2. Graph Topology-aware Alignment", "content": "Previous research on graph prompting (Sun et al., 2023; Dong et al., 2019) has predominantly emphasized feature alignment, often overlooking the critical role of structural discrepancies between graphs from different domains. However, rich semantic information is embedded within relational patterns and topology credibility, as characterized by metrics like homophily and heterophily edge ratios (Zhu et al., 2020). These structural characteristics underscore the importance of aligning graph structure patterns to enable meaningful knowledge transfer (Sun et al., 2022b; Zheng et al., 2022).\nRather than solely focusing on direct structural unification, our approach seeks to synchronize more reliable topology information through graph structure refinement on both source and unseen downstream graphs. This is achieved via the incorporation of GSL, which enhances the robustness and effectiveness of cross-domain graph alignment. By addressing structural discrepancies, our framework bridges the gap between diverse graph domains, facilitating more comprehensive knowledge transfer.\nUnlike prior GSL approaches (Zhu et al., 2021; Jin et al., 2020b), which primarily optimize graph structures based on feature similarity, our method incorporates both semantic and topological information for a more comprehensive refinement process. Specifically, we utilize $A X^r$ to capture the original structural information, where r represents the order of graph aggregation (default set to one). To effectively fuse X and $A X^r$ during graph refinement, we introduce a balance token $t_{B_i} \\in \\mathbb{R}^{2d}$, which operates in the following manner:\n$H_i = t_{B_i} \\odot [X, A X^r]$ (3)\nHere, $t_{B_i}$, dynamically balances the contributions of node features and aggregated structural information.\nFinally, consistent with existing GSL methods (Li et al., 2024), we apply post-processing techniques to reconstruct the refined adjacency matrix. Specifically, a similarity matrix is first computed using $H_i$ and then sparsified via k-nearest neighbors (kNN). In practice, we employ kNN sparsification with its locality-sensitive approximation to enhance efficiency (Fatemi et al., 2021). Subsequently, operations such as Symmetrization, Activation, and Normalization are performed sequentially to produce the final A. Building on this topology-aware refinement process, we propose a general pretraining framework capable of generating high-quality embeddings across multiple domains using contrastive learning. By optimizing a contrastive-based objective, the model learns generalized representations by leveraging rich sample-to-sample relationships from diverse perspectives (Yao et al., 2022). Mathematically, we maximize the mutual information between $G_{i1} = (A_i, X)$ and $G_{i2} = (A', X)$ as follows:\n$\\mathcal{L}= -I(G_{i1}; G_{i2} \\dagger I_e) \u2013 I(G_{i1}; G_{i2} \\dagger A)$ (4)\nwhere we use $\u2020$ to indicate positive samples when computing similarities, often utilizing the identity matrix $I_e$ for this purpose. The second term of $\\mathcal{L}$ incorporates the refined graph structure A to increase the number of positive samples. This loss formulation reduces the information gap between the original and refined graphs, ensuring that global structural properties are preserved during optimization. By maintaining semantic consistency in the graph structure, it enhances alignment and adaptability for downstream tasks."}, {"title": "4.3. Knowledge Transfer to Downstream Domain", "content": "Similar to in-context learning, downstream adaptation aims to enhance a model's ability to learn tasks using only a few examples provided as demonstrations (Dong et al., 2022), highlighting the need for upstream knowledge transfer. Inspired by the \"pre-training & prompting\" paradigm, our framework employs a dual-prompt strategy. Specifically, the meta prompt $p_m$ focuses on adjusting the distribution of learned knowledge by modeling the relationships between the target and source domains. This can be formalized as a function defined for feature X:\n$p_m(X) = t_s \\odot (\\sum_{i=1}^N \\alpha_i t_{D_i} \\odot X)$ (5)\nwhere $\\alpha_1, ..., \\alpha_N$ are trainable coefficients for the source domains. Meanwhile, the learnable specific prompt $p_s \\in \\mathbb{R}^d$ aims to directly align the target domain with the unified semantic space by learning from the limited available samples, ensuring precise adaptation to the downstream domain.\nAs discussed, for the downstream graph T($A_T$, $X_T$) where the feature dimension aligns with the given d, the dual prompts $p_m$ and $p_s$ operate on $X_T$ to achieve semantic unification. Since all tokens are optimized during the pretraining phase, only the coefficients and $p_s$ need to be trained in this process. The reduced number of learnable parameters enhances transfer performance by compensating for the scarcity of annotated samples. Consistent with upstream operations, we use $H_T = t \\odot [X_T, A X_T]$ to obtain $A'_T$, where t could be composed of $t_{D_i}$ (similar to $p_m$) or be directly trained. Finally, we obtain the node representations Z as follows, which can be applied to downstream tasks:\n$Z = GE(A'_T, \\beta p_m(X_T) + (1 - \\beta) p_s X_T; \\theta_{pre})$ (6)\nwhere GE denotes the graph encoder and $\\theta_{pre}$ the frozen parameters learned in pre-training phase. To mitigate the risk of harmful transfer from source graphs with significant discrepancies, as discussed by (Yan et al., 2024), the prompts are integrated using a learnable parameter, $\\beta$, instead of relying solely on concatenation. For graphs that significantly differ from the source domains, the meta prompt weights are attenuated to minimize the transfer of irrelevant information, thereby reducing its potential negative impact on downstream tasks.\nFor the downstream node classification task, the loss function $L_{dst}$ follows a universal task template grounded in subgraph similarity. Given a labeled training set $S = \\{(x_1, y_1),..., (x_i, y_i), . . . \\}$, where $x_i$ represents a node and $y_i \\in Y$ denotes the class label of $x_i$ with Y denoting the set of all possible class labels, the loss function is expressed as:\n$L_{dst}=\\sum_{(x_i,y_i) \\in S}  \\ln \\frac{exp(\\frac{sim(z_{x_i},z_{y_i})}{T})}{\\sum_{y\\in Y} exp(\\frac{sim(z_{x_i}, z_y)}{T})}$ (7)\nHere, $Z_{x_i}$ denotes the final embedding of node $x_i$, and $Z_y$ represents the class embedding for class y, computed as the mean embedding of all training instances belonging to y. We practically calculate $sim(\\cdot, \\cdot)$ by cosine similarity, while $T$ serves as a temperature parameter to control the shape of the output distribution."}, {"title": "5. Theoretical Analysis", "content": "In this section, we provide a theoretical analysis to demonstrate the domain generalization capabilities of MDGFM.\nDenote $P_t$ as the data distribution on feature space in the target domain. t could be changed into i for source domain, while X could be replaced by label space Y. With the covariate shift assumption, each domain is characterized by the distribution on X. Thus, we can approximate the target domain distribution $P_t$ within the convex hull of source domain distributions: $\\Lambda := {\\sum_{i=1}^M \\pi_i P_i | \\pi \\in \\Delta_M}$, where $\\Delta_M$ is the (M \u2013 1)-dimensional simplex so that each \u03c0 represents a normalized mixing weights. The generalization capability is quantified by the following:\nTheorem 5.1 (Domain generalization error bound). Let $\\gamma := \\min_{\\pi \\in \\Delta_M} \\partial_H(P_X, \\sum_{i=1}^M \\pi_i P_i^X)$ with minimizer $\\pi^*$ be the distance of $P_X$ from the convex hull $\\Lambda$, and $\\hat P_X := \\sum_{i=1}^M \\pi_i^* P_i^X$ be the best approximator within $\\Lambda$. Let $\\rho := \\sup_{P_X', P_X'' \\in \\Lambda} d_H(P_X', P_X'')$ be the diameter of $\\Lambda$. Then it holds that\n$\\epsilon_t(h) \\leq \\sum_{i=1}^M \\pi_i^* \\epsilon_i(h) + \\sqrt{\\frac{\\gamma^2}{2}} + \\frac{\\rho}{2} + A_H, (\\mathbb{P}, \\hat{\\mathbb{P}})$,\n$\\epsilon_i(h) \\leq \\epsilon_t + \\sqrt{\\frac{\\gamma^2}{2}} + \\frac{7\\rho}{2}$ (8)\nwhere $A_H, (\\mathbb{P}, \\hat{\\mathbb{P}})$ is the ideal joint risk across the target domain and the domain with the best approximator distribution $\\hat{\\mathbb{P}}$. $\\epsilon_1,\u2026,\\epsilon_M$ represent the source risks and $\\epsilon_t$ denotes the target risk (Albuquerque et al., 2019).\nTo minimize the upper error bound, we introduce the concept of invariant graph learning:\nAssumption 5.2. Given a graph G, there exists an optimal invariant graph learner $I^*(G)$ satisfying:\nInvariance Property: $\\forall e, \\mathbb{P}_e (Y|I^*(G)) = \\mathbb{P}_{e'} (Y|I^*(G))$.\nSufficient Property: $Y = w^*(g^*(\\Phi^*(G))) + \\epsilon, \\epsilon \\perp G$.\nwhere $g^*(\\cdot)$ is a representation learning function, $w^*$ is the classifier, e denotes the environments (i.e., domains), I indicates statistical independence and $\\epsilon$ is the random noise.\n$\\Phi^*(G)$ could generate invariant graphs across different domains, which is implemented as GSL procedure on all domains in our work. Moreover, by maximizing the mutual information between $G_{i1} (A_i, X_i)$ and $G_{i2}(A', X)$, our method retains sufficient task-relevant information, thereby satisfying the sufficient property. Motivated by Theorem 5.1, domain invariant representation learning minimizes the risks over all source domains corresponding to the first term of the bound, as well as the representation distribution differences among source and target domains in the hope of reducing \u03b3 and \u03c1. In general, based on the following theorem, our model achieves the minimal error bound due to its adherence to the invariant and sufficient properties.\nTheorem 5.3. Let $\\Phi^*$ be the optimal invariant graph learner and denote the complement as $G \\backslash \\Phi^*(G)$, i.e., the corresponding variant subgraph. Then we can obtain the optimal predictor under distribution shifts as follows:\n$arg \\min_{w,g} w \\circ g \\circ \\Phi^* (G) = arg \\min_f sup_e R(f|e)$ (9)\nif the following conditions hold: (1) $\\Phi^*(G)|G \\backslash \\Phi^*(G)$; (2) $\\forall \\epsilon'$ which is an invariant graph learner, $\\epsilon'$ such that $P_\\epsilon' (G,Y) = P_\\epsilon' (\\Phi(G), Y)P_\\epsilon' (G \\backslash \\Phi(G))$ and $P_\\epsilon' (\\Phi(G)) = P(\\Phi(G))$ (Li et al., 2022), where $R(f|e) = E_{G,Y}[l(f(G), Y)]$ is the risk of the predictor f on the domain e and $l(\\cdot, \\cdot) : Y \\times Y \\to R$ denotes a loss function."}, {"title": "6. Experiments", "content": "In this section, we evaluate our proposed MDGFM on few-shot node classification task. Specifically, we aim to answer the following four research questions:\nRQ1. How does MDGFM perform on multi-graph tasks in few-shot learning scenarios compared to current baselines? RQ2. How do the key components benefit our model? RQ3. Does our model rely on specific source domains and exhibit sensitivity to the removal of these source domains? RQ4. Does our model demonstrate robustness against attacks and deletion on the source domain."}, {"title": "6.1. Experimental Setups", "content": "Datasets. To ensure a comprehensive comparison, we conduct experiments on six primary datasets, including three homophilic graphs-Cora (Sen et al., 2008), Citeseer (Sen et al., 2008), and Pubmed (Namata et al., 2012)\u2014and three heterophilic graphs-Cornell, Chameleon, and Squirrel (Pei et al., 2020). Additionally, we include Penn94 (Traud et al., 2012), a large-scale graph dataset, as a downstream target domain. Detailed statistical information on these datasets is provided in Appendix B.\nBaselines. We compare our method against four categories of approaches:\nSupervised methods: These methods train a GNN on downstream tasks and directly infer results. We employ two well-known models: GCN (Kipf & Welling, 2016) and GAT (Velickovic et al., 2017).\nGraph Pre-training Methods: These approaches perform self-supervised pre-training across multiple isolated source domains, such as DGI (Velickovic et al., 2019) and GraphCL (You et al., 2020), before fine-tuning on a new downstream task. Notably, source domains are not trained simultaneously; instead, they are merged into a single batch object, forming an adjacency matrix composed of distinct blocks.\nGraph Prompting Methods: These methods freeze the parameters of a pre-trained model, unify downstream tasks and tune a single type of prompt accordingly. Representative methods include GPPT (Sun et al., 2022a) and GPF (Fang et al., 2024).\nMulti-domain Pre-training Methods: These methods integrate multiple source domains during upstream pre-training and transfer knowledge for few-shot learning on unseen target domains. For instance, GCOPE (Zhao et al., 2024) constructs a unified large-scale dataset with inter-dataset connections, while MDGPT (Yu et al., 2025) encodes domain-specific characteristics using unique tokens, followed by fine-tuning and prompt adaptation.\nFor a fair comparison, we use GCN as the backbone for all methods. To evaluate performance on unseen target domains, we focus on scenarios where the model generalizes to domains not encountered during pre-training. For the Cora, Citeseer, Pubmed, Chameleon, Squirrel, and Cornell datasets, we designate one dataset as the downstream target domain while using the remaining five as source domains during pre-training. Additionally, we extend this setup by using all six datasets as source domains and applying Penn94 as the target domain for few-shot learning."}, {"title": "6.2. Cross-domain Transfer Efficacy under Few-shot Learning Conditions (RQ1)", "content": "We compare our proposed MDGFM against all baselines on 1-shot and K-shot node classification tasks. Each experiment is repeated five times, and we report the average results in Tables 1 and 2.\nIn the one-shot setting, MDGFM consistently outperforms all baseline models, demonstrating superior performance on both homophilic and heterophilic graphs. This advantage stems from MDGFM's ability to effectively capture both domain-specific information from each source domain and shared patterns across domains. Notably, we observe that supervised methods occasionally surpass graph pre-training approaches, suggesting the presence of negative transfer in certain scenarios.\nAmong multi-domain pre-training methods, MDGPT outperforms GCOPE on homophilic graphs, as GCOPE's reliance on virtual nodes propagates homophilic graph information, inadvertently introducing noise. Conversely, on heterophilic graphs, GCOPE surpasses MDGPT, as MDGPT's simple integration of source domain tokens fails to fully capture cross-domain commonalities. For graph prompting methods, GPPT generally underperforms across most datasets, as it is not specifically designed for few-shot learning. While GPF achieves competitive performance, its high memory requirements pose a significant limitation.\nIn the few-shot setting, we observe trends similar to those in the one-shot scenario. Notably, on heterophilic graphs, certain methods exhibit a decline in performance compared to the one-shot case. This degradation is primarily due to the introduction of noise from the few-shot samples. In contrast, our model remains robust and does not experience performance deterioration as the number of training samples increases.\nQuantitatively, across the seven datasets, MDGFM outperforms the second-best model by up to 18.04% in the one-"}, {"title": "6.3. Ablation Study (RQ2)", "content": "In this section, we analyze the effectiveness of key components in our model by designing four variants and comparing their classification performance against MDGFM.\nw/o-refinedadj: Removes the GSL module.\nw/o-sumtoken: Excludes the shared token.\nw/o-topology: Constructs the graph without considering topological information in the GSL process, i.e., merely use X in Eq.(3) rather than $[X, A X^r]$ for the i-th graph.\nw/o-balance: Removes the balance token.\nEffectiveness of domain-invariant learning. MDGFM captures shared information across domains through a shared token. As shown in Figure 2, the w/o-sumtoken variant exhibits the weakest performance across most datasets, underscoring the importance of capturing common information for effective domain generalization.\nEffectiveness of topology structure alignment. MDGFM utilizes graph structure learning to extract information from graph topology structure. The w/o-refinedadj, w/o-topology, w/o-balance generally perform much worse than MDGFM, demonstrating the necessity of each component."}, {"title": "6.4. Domain Sensitivity (RQ3)", "content": "In this section, we examine the extent to which our model depends on specific source domains and its performance under limited source-domain conditions. To assess this, we systematically remove subsets of source domains and evaluate the model using Cornell as the target domain, comparing its performance against multi-domain pre-training baselines. Specifically, we exclude one to three source domains while keeping the target domain and all other parameters fixed.\nAccording to the one-shot result in Table 4, the performance of GCOPE and MDGPT deteriorates significantly, whereas MDGFM maintains strong performance and stability despite the removal of source domains. Our findings suggest that these methods extract varying degrees of knowledge from specific source domains. A counterintuitive phenomenon is that in some cases, removing some source domains leads to improved performance. This occurs because multi-domain graphs may result in conflicts or interferences rather than synergy (Yu et al., 2025). Our model aims to mitigate such negative influences by extracting the shared informative and beneficial aspects of the available knowledge."}, {"title": "6.5. Robustness Analysis (RQ4)", "content": "To evaluate the robustness of our proposed MDGFM, we assess the node classification performance under diverse attacks. Simple modification attacks are implemented by randomly adding or removing edges. Considering the varying influence of individual domains on the target domain, we apply these random attacks to all source domains and the target domain under one-shot learning.\nFurthermore, we employ Metattack (Z\u00fcgner & G\u00fcnnemann, 2019), an advanced adversarial attack that perturbs the training data to degrade the model performance after training. Under identical attack conditions, we compare MDGFM against multi-domain methods. As shown in Figure 3, the performance of GCOPE and MDGPT deteriorates as attack intensity increases. Notably, our model is quite stable and consistently outperforms all baselines across all attack scenarios, which can be attributed to our proposed topology alignment. Further analysis, including a sensitivity study of hyperparameters, is provided in Appendix C."}, {"title": "7. Conclusion", "content": "In this paper, we introduce MDGFM, a novel graph foundation model that unifies graphs from diverse domains into a universal semantic space. Our approach enhances domain generalization by extracting domain-invariant knowledge through both feature and topology alignment. We validate the effectiveness of MDGFM through theoretical analysis and empirical evaluation. To the best of our knowledge, this is the first work to explicitly address invariant information embedded in topology structures across homophilic and heterophilic domains. Our findings pave the way for advancements in graph-based domain generalization, with potential extensions to dynamic graphs and large-scale heterogeneous networks, further enhancing adaptability in real-world applications."}, {"title": "A. Notations", "content": ""}, {"title": "B. Datasets", "content": "We provide an in-depth explanation of datasets, where their statistics details are presented in Table 6.\nCitation network: The Cora and Citeseer (Sen et al., 2008) datasets represent a varied collection of scholarly articles in the field of computer science, where each node is associated with bag-of-words features and a categorical label denoting the corresponding topic of the paper. In contrast, the Pubmed (Namata et al., 2012) dataset consists of articles focused on diabetes sourced from the PubMed database. Each node in this dataset is characterized by an attribute vector, along with a label that identifies the specific type of diabetes addressed in the publication.\nWebKB: Cornell (Pei et al., 2020) is a subset derived from the WebKB dataset. In this dataset, each node corresponds to a web page, while the edges illustrate the hyperlinks that connect these pages. The features of nodes are expressed using bag-of-words features derived from the content of the web pages. Nodes are classified into five distinct labels: student, project, course, staff, and faculty.\nWikipedia network: The Chameleon and Squirrel (Pei et al., 2020) datasets comprise two page-to-page networks sourced from Wikipedia, each centered on specific themes. Nodes signify individual web pages, and edges represent their connections. Node attributes are characterized by collections of nouns gathered from the content of the pages. Additionally, each node is categorized according to the average monthly traffic that the corresponding web page receives."}, {"title": "C. Additional Experiments", "content": ""}, {"title": "C.1. Expansion of Robustness Experiments", "content": "We perform robustness experiments on the Cornell dataset before. To investigate whether the MDGFM model retains its robustness under the condition where the downstream target domain consists of homophilic graphs, we replicate the robustness experiments previously described on the Cora dataset, performance under metaattack is shown in the main text. Specifically, we apply random edge additions and deletions on all domains. We compare our results with those of MDGPT and GCOPE, as summarized in Figure 4. Our observations indicate that our model maintains commendable robustness when evaluated on homophilic graph datasets. Furthermore, we note that in the context of heterophilic graph datasets, GCOPE exhibits greater robustness compared to MDGPT, while the opposite trend is observed for homophilic graphs."}, {"title": "C.2. Sensitivity Analysis", "content": "We investigate the impact of the hyperparameter k in k-nearest neighbors (kNN) for downstream graph structure learning, which is crucial to our model. The performance of our model varies with changes in the hyperparameter k as illustrated in Figure 5. Overall, our model exhibits low sensitivity to changes in k, regardless of whether the downstream target domain is homophilic or heterophilic. When the target domain is homophilic graphs, larger values of k (such as 20 or 30) yield the best results. In contrast, for heterophilic graphs, relatively smaller values of k can produce satisfactory outcomes."}, {"title": "D. Experimental Details", "content": "For both one-shot and few-shot classification tasks, we pretrain the models on five datasets and subsequently perform predictions on the remaining dataset, ensuring that the downstream domain remains unseen during the training phase. The detailed experimental setup is summarized in Table 7, where a checkmark (\u221a) indicates visibility during pre-training, while the absence of the mark denotes invisibility. For the large-scale dataset Penn94, it is exclusively used as the target domain to evaluate the generalization capability of the model.\nAll experiments are conducted on a platform equipped with an Intel(R) Xeon(R) Gold 5220 CPU and an NVIDIA A800 80GB GPU, using PyTorch 1.10.1 and DGL 0.9.1. Each experiment is run five times and the average results are reported. We employ the Adam optimizer and set the batch size to 128. During the upstream pre-"}]}