{"title": "Transformer Neural Process - Kernel Regression", "authors": ["Daniel Jenson", "Jhonathan Navott", "Mengyan Zhang", "Elizaveta Semenova", "Seth Flaxman", "Makkunda Sharma"], "abstract": "Stochastic processes model various natural phenomena from disease transmission to stock prices, but simulating and quantifying their uncertainty can be computationally challenging. For example, modeling a Gaussian Process with standard statistical methods incurs an O(n\u00b3) penalty, and even using state-of-the-art Neural Processes (NPs) incurs an O(n\u00b2) penalty due to the attention mechanism. We introduce the Transformer Neural Process - Kernel Regression (TNP-KR), a new architecture that incorporates a novel transformer block we call a Kernel Regression Block (KRBlock), which reduces the computational complexity of attention in transformer-based Neural Processes (TNPs) from O((nc + nr)\u00b2) to O(n + ncnr) by eliminating masked computations, where nc is the number of context, and ny is the number of test points, respectively, and a fast attention variant that further reduces all attention calculations to O(nc) in space and time complexity. In benchmarks spanning such tasks as meta-regression, Bayesian optimization, and image completion, we demonstrate that the full variant matches the performance of state-of-the-art methods while training faster and scaling two orders of magnitude higher in number of test points, and the fast variant nearly matches that performance while scaling to millions of both test and context points on consumer hardware.", "sections": [{"title": "1 INTRODUCTION", "content": "The principle challenge of modern spatiotemporal Bayesian modeling is scale. As the number of observed locations increases from tens to thousands or hundreds of thousands, traditional techniques used to model spatiotemporal phenomena break down. Perhaps the most common method typically employed to model spatiotemporal processes is the Gaussian Process (GP). Gaussian Processes are a particularly well-behaved class of stochastic processes. Specifically, for a finite index set {t \u2208 T}, the collection = (\u04251,..., \u0425\u0442) follows a multivariate Gaussian distribution. This makes various analytic calculations tractable, facilitating regression, marginalization, and sampling with GPs.\nX\nWhile GPs provide a significant degree of flexibility in modeling, the analytic solutions they yield do not scale well in the number of observed locations. Using a GP to model spatial random effects within a Markov Chain Monte Carlo (MCMC) sampler incurs an O(n\u00b3) cost per sample, where n is the number of observed locations. This is because the covariance matrix must be inverted, or factorized in the case of Cholesky decomposition, at each iteration in order to generate a sample. Unfortunately, this means that for only n = 1,000 locations, nearly a billion operations must be performed to generate a single sample.\nIn order to accelerate Bayesian inference with spatiotemporal stochastic processes, there have been at least three prominent strains of research. The first is Variational Inference (VI), which aims to recast the inference problem as an optimization problem and maximize the Evidence Lower Bound (ELBO). The second aims to accelerate sampling by using a generative neural network-based approximation. This family tends to leverage Variational Autoencoders (VAEs). The third is a recent family of deep learning models called Neural Processes (NPs). These models use a meta-learning objective, meaning that once trained, the forward pass of the model takes as input \"context\" or observed points and returns a function. This function"}, {"title": "2 BACKGROUND", "content": "A number of general techniques have been developed to reduce the computational burden of modeling large spatiotemporal datasets. These include, but are not limited to variational inference (VI) (Blei, Kucukelbir, and McAuliffe, 2017), stochastic process emulation (Mishra et al., 2022; Semenova, Xu, et al., 2022; Semenova, Verma, et al., 2023), and neural processes (NPs) (Garnelo, Schwarz, et al., 2018; Garnelo, Rosenbaum, et al., 2018; Kim et al., 2019; Lee et al., 2020; Gordon et al., 2020; Nguyen and Grover, 2023). There is also a long literature on approximate methods to scale up Gaussian processes in particular which we do not cover in detail, see e.g. Hensman, Fusi, and Lawrence (2013), Rue, Martino, and Chopin (2009), Solin and S\u00e4rkk\u00e4 (2020), Wilson, Dann, and Nick-isch (2015), and Lindgren, Lindstr\u00f6m, and Rue (2010). While using distinct algorithmic approaches, all these methods provide approximations to the posterior distributions."}, {"title": "2.1 Variational Inference (VI)", "content": "VI (Blei, Kucukelbir, and McAuliffe, 2017; Murphy, 2023) approximates the posterior distribution by framing inference as an optimization problem, aiming to maximize the Evidence Lower Bound (ELBO) by minimizing the Kullback-Leibler (KL) divergence between a variational distribution qy(z) and the true posterior pe (z|x). Although VI is widely used, its effectiveness depends on selecting an appropriate variational family, and there are no guarantees on how close the ELBO is to the true log-likelihood, making uncertainty estimation challenging when the variational family poorly approximates the true distribution (Yao et al., 2018; Huggins et al., 2020)."}, {"title": "2.2 Stochastic Process Emulation", "content": "Another line of research aims to accelerate sampling by approximating samples from computationally intensive stochastic processes. This is the aim of models like PriorVAE, PriorCVAE, and \u03c0VAE (Semenova, Xu, et al., 2022; Semenova, Verma, et al., 2023; Mishra et al., 2022). Currently these models are all based on Variational Autoencoders (VAEs) (Kingma and Welling, 2022). VAEs consist of an encoder and decoder combined with a latent sampling process. They encode raw data into a vector of latent parameters, which are then used to sample a latent vector. This latent vector is then passed through the decoder, whose objective is to recreate the original data. The advantage of models like these is that if the latent distribution is simple, i.e. a multivariate normal with diagonal covariance, it can be very easy to sample. This means that a fully trained network can generate new samples from the original data distribution by sampling latents and passing them through the decoder. Furthermore, this can often be done in time linear in the number of layers in the network, which can be two orders of magnitude faster than sampling from a real GP (Semenova, Verma, et al., 2023). Because neural networks are inherently differentiable, they can also be transparently integrated into inference frameworks like NumPyro, where gradient-informed samplers like the No-U-Turn Sampler (NUTS) can easily pass gradients through the model. The principle challenge with this class of models is that the number of input and output locations is fixed and ordered, which means a new model must be retrained each time the number of observed locations changes or the location associated with each input changes. These models are also sensitive to the dimensionality of the latent vector, which induces an information bottleneck on autoencoded data and can cause oversmoothing in generated samples."}, {"title": "2.3 Neural Processes (NPs)", "content": "Neural Processes (NPs) are a family of models that use deep neural networks to represent stochastic processes. NPs are considered \"meta-learners\" because, instead of modelling a single function h:s \u2192 f, they take as input context points (sc, fc) and return a distribution over functions g : (sc,fc) \u2192 Ph\\sc,fc(h : ST \u2192 fr), allowing evaluation at test points sy without retraining. To be valid stochastic processes, NPs must ensure invariance to context/test point ordering and consistency under marginalization.\nThere are two main classes of Neural Processes, latent neural processes and conditional neural processes. Latent neural processes dedicate part of their architecture to generating latent parameters, which are sampled and passed through the decoder to generate coherent samples, similar to VAEs. The fundamental assumption of latent NPs is that the test points are independent conditional on the latent vector. For instance, if (sc, fc) represents a tuple of locations and function values at context (observed) points, (ST, fT) represents locations and function values at test (unobserved) points, and z represents a sampled latent vector, the likelihood of the function values at test locations can be formulated as follows:\np(fTST, sc, fc) = Jp(z | sc, fc)p(ft | ST, z)dz = Jp(z | sc, fc) \u03a0p(f)s, z) z)dz\ni=1\nConditional neural processes often have very similar architectures, except they avoid sampling a latent vector and condition on a fixed representation, r, of the context points. This implies the following factorization, assuming the encoder has already processed the context points, enc(sc, fc) = r:\nT\np(fr | sr, sc, fc) = p(f) | s,r)\ni=1\nIn practice, conditional neural processes tend to perform better. It is unclear whether this is because the latent vectors, z, are an insufficient representation of the latent parameters, or if the models are expending some of their finite capacity on producing good estimates of z at the cost of final predictive accuracy. Either way, the most performant NPs as measured by log-likelihood scores are conditional, specifically conditional transformer neural processes (TNPs)."}, {"title": "2.3.1 Attentive Neural Process (ANP)", "content": "The Attentive Neural Process (ANP) was introduced to address several limitations of the NP and CNP architectures (Kim et al., 2019). In particular, NPs and CNPs underfit on observed context points and generally tend to oversmooth the posterior predictive. One of the principle reasons for underfitting is that the decoder is unable to differentially attend to each of the context points when decoding at test points. For instance, if one context point is located at sc = -2 and the test point is at st = 2, i.e. opposite ends of the training region, sc equally influences the context vector used to decode at st despite having little to no influence on the behavior of function at st = 2. Accordingly, the authors propose an attentive version of the NP that allows both local and global context to be incorporated when decoding at test locations.\nThe ANP shares the NP architecture, but replaces MLPs with multihead dot product self-attention in both the latent and deterministic encoding paths. Then, in the deterministic path it adds cross-attention between test points and context points so that the associated context vector, rt, summarizes information relevant to the test location st, rather than being a global representation. This localized context vector, rt, is passed through the decoder with the global latent representation z, and the location, st, to produce (\u03bc\u03c4, \u03c3\u03c4). With the ANP and its conditional variant CANP, the NP family started to become a viable replacement to true GPs for inference."}, {"title": "2.3.2 Convolutional Conditional Neural Process (ConvCNP)", "content": "Convolutional Conditional Neural Processes (ConvCNP) were designed to incorporate translation equivariance into NPs (Gordon et al., 2020). When a model exhibits translation equivariance, it is able to identify a feature or function behavior regardless of how far its input has shifted in the domain. This improves the model's capacity to generalize beyond the training region.\nThe authors of ConvCNP define two architectures, one for on-the-grid data and one for off-the-grid data. Here we detail the off-the-grid version since it is more generic and can be used in both cases. First, the domain is partitioned into a fixed, uniform grid. Then, a positive-definite Reproducing Kernel Hilbert Space (RKHS) kernel is evaluated at each of the grid points using the context set. This value is then normalized using a density channel so the point values are invariant to the cardinality of the context set. This grid is then run through a CNN-based architecture, e.g. ResNet (He et al., 2016), to create an updated hidden state representation at grid locations. Finally, the decoder uses another RHKS kernel, typically the same one used in the encoder, to decode test points using the underlying hidden state grid values.\nConvCNPs perform very well on the standard NP benchmarks, often exceeding the performance of other models at a fraction of the number of learnable parameters. However, beyond simple examples, ConvCNP requires many more parameters, often on the same order as other models, to perform competitively. Furthermore, ConvCNP is very sensitive to the parameterization of the intermediate grid and the effective receptive field of the CNN layer. For instance, at lower lengthscales, the model performs better when there is a higher grid resolution, but this increase in grid resolution changes the receptive field of the CNN layer, so the CNN's kernels must be optimized in conjunction. Lastly, due to the fixed intermediate grid, ConvCNP suffers from the curse of dimensionality and is difficult to evaluate in non-contiguous regions of the domain."}, {"title": "2.3.3 Transformer Neural Processes (TNP)", "content": "Transformer Neural Processes (TNPs) (Nguyen and Grover, 2023) can be considered an extension of the Conditional Attentive Neural Process (CANP) that uses multiple transformer encoder blocks instead of stacked self-attention and cross-attention. A transformer encoder block consists of a self-attention layer followed by a feedfoward network with residual connections interspersed.\nBecause standard transformers are designed to work with sequences of data and use fixed positional embeddings, the TNP authors had to modify the architecture for NPs. Accordingly, TNPs dispense with the positional embeddings, merge the context and test sequences as input, and introduce a special attention mask. The context sequence consists of location and function value pairs, [(s1, f\u2081), ..., (Snc, fnc)], and the test sequence consists of location and zero-padded function value pairs, [(s1,0),..., (Snr, 0)]. Within each layer, a mask is applied that prevents context points from attending to test points and prevents test points from attending to other test points. This means that context points only attend to other context points and test points also only attend to context points. After the encoding stage, the embeddings for test points are passed through a prediction head that estimates the mean and covariance structures.\nThere are three original TNP variants: TNP-D, TNP-ND, and TNP-A. TNP-D (Diagonal) assumes test points can be factorized independently conditional on the context points, in line with most CNP variants. TNP-ND (Non-Diagonal) parameterizes a covariance matrix by estimating the elements in the lower triangular matrix of a Cholesky decomposition for test points. Lastly, TNP-A (Autoregressive) assumes that the test points can be factorized autoregressively. This means that each time a test point is predicted, its true value is then added to the context set and used when predicting the next test point.\nIn practice, we found TNP-D to consistently perform well. On the other hand, we found TNP-ND could become arbitrarily miscalibrated. When there are very few context points, TNP-ND maximizes the log-likelihood by collapsing the standard deviation for test points that lie near context points, which provides very precise estimates. However, this also causes the model to collapse uncertainty estimates for test points that lie far away from context points, leading to both high log-likelihoods and high miscalibration rates. On the other hand, TNP-A assumes that the true test points are observed autoregressively, which is equivalent to running any NP model forward one test point at a time. This method yields log-likelihoods that correspond to sequential observations, rather than joint distributions over all unobserved points. As we are interested in the general case, we do not include experiments on TNP-A."}, {"title": "3 TRANSFORMER NEURAL PROCESS - KERNEL REGRESSION (TNP-KR)", "content": "TNP-KR was inspired by TNP-D, which consistently performs well on NP benchmarks and does not make the same assumptions as TNP-A or suffer from miscalibration like TNP-ND. One of the principal limitations of all TNP variants, however, is the O((nc+\u043f\u0442)\u00b2) attention used by the encoder layers. Recall that ne is the number of context points and ny is the number of test points. This also is not strictly necessary because the special mask applied after attention discards many of the calculations, namely, those from context points to test points and those from test points to other test points. Accordingly, we introduce the KRBlock, a transformer block that avoids computing O(n + ncnT) attention values altogether. Unlike the original transformer encoder block, TNP-KR also uses pre-normalized residual connections (Xiong et al., 2020), which has been shown to stabilize training and improve performance.\nWe call this a KRBlock because the cross-attention from test to context points can be viewed as a form of Nadaraya-Watson kernel regression where the locations are the feature embeddings and the kernel is a dot product softmax kernel. Specifically, if a is the dot product softmax kernel, qi is a query embedding"}, {"title": "3.1 KRBlock", "content": "representing a test point, kj is a key embedding representing a context point, and vj is the value embedding associated with the same context point, cross-attention in the KRBlock can be formulated explicitly as Nadaraya-Watson kernel regression:\nf(qi) = \u03a3\u03a3a(qi, kj) Vj (1)\nj a(qi, kj)\nStacking KRBlocks then allows the model to perform iterative kernel regression on increasingly complex internal representations of test and context points. Cross-attention from test to context points costs O(ncnr) and the self-attention among contxt points costs O(n), making the total complexity O(n + ncnt).\nWhen there are a large number of context points, the O(n) term can still prove computationally prohibitive. For example, in satellite imagery, a comparatively small image of 300x500 results in 150,000 locations or pixels (Heaton et al., 2018). A common application is to inpaint pixels missing due to cloud cover. With 20% of pixels missing, there would still be 120000 context points. This means that even with a KRBlock, the space and time complexity would be on the order of 1200002, requiring nearly 60GB of memory and 14.4 billion multiplications per self-attention application. Thus, in order to further scale KRBlocks, we incorporate Performer attention, also known as fast attention (Choromanski et al., 2022)."}, {"title": "3.2 Fast Attention", "content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention matrix. It is nearly unbiased and offers uniform convergence and low variance (Choromanski et al., 2022). FAVOR+ constructs an attention matrix ALXL where A(i,j) = K(qi, kj) without ever fully materializing it. For a randomized mapping \u03c6 : Rd \u2192 R"}, {"content": "Fast attention is based on a complete algorithm called Fast Attention with Orthogonal Random features (FAVOR+). FAVOR+ allows attention to be calculated in linear space and time complexity without making any assumptions about the sparsity or rank of the attention"}]}