{"title": "Augmenting Channel Simulator and Semi-Supervised Learning for Efficient Indoor Positioning", "authors": ["Yupeng Li", "Xinyu Ning", "Shijian Gao", "Yitong Liu", "Zhi Sun", "Qixing Wang", "Jiangzhou Wang"], "abstract": "This work aims to tackle the labor-intensive and resource-consuming task of indoor positioning by proposing an efficient approach. The proposed approach involves the introduction of a semi-supervised learning (SSL) with a biased teacher (SSLB) algorithm, which effectively utilizes both labeled and unlabeled channel data. To reduce measurement expenses, unlabeled data is generated using an updated channel simulator (UCHS), and then weighted by adaptive confidence values to simplify the tuning of hyperparameters. Simulation results demonstrate that the proposed strategy achieves superior performance while minimizing measurement overhead and training expense compared to existing benchmarks, offering a valuable and practical solution for indoor positioning.", "sections": [{"title": "I. INTRODUCTION", "content": "The 3rd Generation Partnership Project (3GPP) has successfully conducted the fifth generation (5G) Advanced phase in its Release 18 [1]. As one of the most significant potential advancements to increase automation and efficiency of the industrial domain, the indoor positioning has attracted widespread attention. However, the indoor factory scenario presents challenges, including a high volume of obstructions leading to numerous non-line-of-sight (NLOS) scenarios. These conditions often lead to significant performance degradation for the traditional positioning algorithms [2], [3].\nTo address the issue of the limitations and difficulties of traditional positioning methods, a notable point is to integrate deep learning (DL) into indoor positioning systems, specifically fingerprint-based positioning. The basic idea of the fingerprint positioning is to create a mapping function between the channel measurements and the position coordinates, creating a main wireless radio environment among the transceivers [4]. Then, the user equipment (UE) can obtain its position by the measurements. The measurements of fingerprint positioning in the cellular systems are mainly channel impulse response (CIR), power delay profile (PDP), and so on [5]. However, DL-based indoor positioning faces the challenge of insufficient data available for training the model [6]. Especially, in the flexible and dynamic communication systems, obtaining sufficient measurements and labeling them is a labor-intensive and resource-consuming task.\nA powerful approach termed semi-supervised learning (SSL) was proposed, which involved using small number of labeled data combined with a large amount of unlabeled data to train the model [7]\u2013[9]. Typical SSL [10] may lead to serious performance degradation due to errors in the pseudo-labels of the unlabeled data. To address this issue, a smaller weighting coefficient was added to the unlabeled data during model training [11]. The weighting coefficient is a hyperparameter, which requires meticulous manual tuning. In [12], the concept of confidence for the pseudo-label of the unlabeled data was proposed, which helps to select reliable unlabeled data in classification tasks. However, there is a lack of a method to define a label confidence in regression tasks like positioning. Furthermore, in an indoor scenario, obtaining the unlabeled data is also a challenge that requires extra measurement overhead.\nTo address the aforementioned challenges, a DL-based positioning algorithm with reduced overhead is proposed. To lower the measurement overhead, the unlabeled data is generated by an updated channel simulator according to the stochastic parameters of the labeled data. This approach reduces the measurement expenses of the unlabeled data. Additionally, to capture the features of both the labeled and unlabeled channel data, a semi-supervised learning with a biased teacher is proposed. The main contributions of this paper are summarized as follows:\n\u2022 An updated channel simulator is utilized to generate unlabeled data, thereby reducing the measurement expenses associated with acquiring such data. Initially, labeled data collected within an indoor factory environment is utilized, and the primary channel parameters are analyzed to extract the channel stochastic parameters of the measurement environment. Subsequently, the typical TR 38.901 [13] channel simulator is updated with these stochastic parameters, resulting in the creation of an updated channel simulator (UCHS). The UCHS is then employed to simulate the unlabeled channel data, which closely resembles the labeled data of the measurement environment.\n\u2022 To efficiently capture the mapping function between the CIR and the position coordinate while minimizing training expenses, a SSL with a biased teacher (SSLB) is introduced. The SSLB approach involves weighting the unlabeled data based on the confidence, which is derived from the distance bias predicted by the teacher model. This methodology assigns higher confidence to more reliable unlabeled data. Notably, this work represents the first instance of introducing confidence into regression tasks, such as indoor positioning, resulting in a significant reduction in training expenses.\n\u2022 To obtain the confidence of the unlabeled data, a novel SSL structure is designed, termed as SSLB. This structure incorporates an additional reference signal and a position bias, added separately to the input and the output of the traditional SSL model. As a result, SSLB can extract additional bias information from the labeled dataset, which can then be transformed into the confidence of the unlabeled data."}, {"title": "II. PROBLEM DESCRIPTION", "content": "A. Adapting semi-supervised learning for indoor positioning\nThe dataset of traditional SSL consists of two parts: the labeled dataset and the unlabeled dataset. The input is the CIR and the output is the predicted position. The labeled dataset is represented as $\\mathcal{D}(\\mathbf{H}, \\mathbf{I})$, where $\\mathbf{H}$ is the labeled CIR and $\\mathbf{I}$ is its corresponding position coordinate. The unlabeled dataset is represented as $\\mathcal{E}(\\mathbf{H}^*)$, containing only the CIR.\nThe SSL model is divided into a teacher model $\\Theta^t$ and a student model $\\Theta^s$. Firstly, the teacher model is trained with labeled data, and then pseudo-labels of unlabeled data are generated using the trained teacher model. Finally, the student model is trained with both the labeled data and the unlabeled data. In this paper, the loss function is defined as the Euclidean distance between the predicted position and the real position as shown in (1). First, train the teacher model using this loss on the labeled dataset $\\mathcal{D}(\\mathbf{H}, \\mathbf{I})$.\n$\\mathcal{L} = \\frac{1}{N} \\sum_{i=1}^N ||\\mathbf{\\hat{l}}_i - \\mathbf{l}_i||^2$, (1)\nThen, the label of dataset $\\mathcal{E}(\\mathbf{H}^*)$ is generated with the trained teacher model\n$\\mathbf{l}^* = f(\\mathbf{H}^*, \\Theta^t)$, (2)\nwhere $\\mathbf{l}^*$ is the position coordinate of $\\mathbf{H}^*$. $f(.)$ represents the mapping function between the channel measurements and position coordinates. In this way, the dataset $\\mathcal{E}(\\mathbf{H}^*)$ is completed to $\\mathcal{E}(\\mathbf{H}^*, \\mathbf{l}^*)$. Finally, the student model is trained with the dataset of $\\mathcal{D}(\\mathbf{H}, \\mathbf{I})$ and $\\mathcal{E}(\\mathbf{H}^*, \\mathbf{l}^*)$, applying weights to pseudo-labeled data. The loss function is defined as follows:\n$\\mathcal{L}^w = \\frac{1}{N + N^*} [\\sum_{i=1}^N ||\\mathbf{\\hat{l}}_i - \\mathbf{l}_i||^2 + \\sum_{j=1}^{N^*} w^* ||\\mathbf{\\hat{l}}^*_j - \\mathbf{l}^*_j||^2]$, (3)\nwhere $\\mathcal{L}^w$ is the loss of weighted semi-supervised algorithm, $N$ and $N^*$ are the numbers of labeled data and unlabeled data, and $w$ and $w^*$ are the corresponding weights of labeled data and unlabeled data. Detailed descriptions can be referred to [10] [11]. it is worth mentioning that all models use the same DL model, illustrated in Fig. 1."}, {"title": "III. CHANNEL FEATURE AIDED DATA AUGMENTATION", "content": "A. Wireless channel feature extracting method\nIn this section, some specific methods for extracting channel parameters will be described.\n1) Angle spread: The angle spread (AS) is used to characterize the dispersion features of the multipaths in the spatial domain. Initially, a two-dimensional Discrete Fourier transform (DFT) is performed over the channel matrix $\\mathbf{H}(t)$ over the port domain, resulting in the angle domain channel matrix $\\mathbf{H}_{ang}$ [13] as follows:\n$\\mathbf{H}_{ang} = \\mathbf{U}^T \\mathbf{H}(t) \\mathbf{V}$, (4)\nwhere $\\mathbf{U} = [\\mathbf{u}_1; ...; \\mathbf{u}_{N_t}]^T$ is the transmit DFT matrix, and $\\mathbf{V} = [\\mathbf{v}_1, ..., \\mathbf{v}_{N_r}]$ is the receiver DFT matrix [14].\nThen, the receiving and transmitting AS are illustrated in the spatial domain. The angle spread value can be obtained as follows:\n$\\sigma_{AS} = \\sqrt{\\frac{\\sum_{m=1}^M \\Theta_m^2 P_m}{\\sum_{m=1}^M P_m} - (\\frac{\\sum_{m=1}^M \\Theta_m P_m}{\\sum_{m=1}^M P_m})^2}$, (5)\nwhere $P_m$ is the power of the $m$th multipath, $\\Theta_m$ is the angle of the $m$th multipath.\n2) Delay spread: The delay spread (DS) can reflect the richness of the channel multipaths. And the DS [15] can be obtained as follows:\n$\\sigma_{DS} = \\sqrt{\\frac{\\sum_{m=1}^M (\\tau_m - \\tau_{mean})^2 P_m}{\\sum_{m=1}^M P_m}}$, (6)\nwhere $\\tau_{mean} = \\frac{\\sum_{m=1}^M (\\tau_m P_m)}{\\sum_{m=1}^M (P_m)}$ is the mean delay.\nDue to the space limit, only the above two unique channel statistical parameters are delineated here. Other channel statistics parameters can refer to [14].\nB. Unlabeled data generation via UCHS\nThe TR 38.901 [13], considered as the state-of-the-art 3GPP channel model, is crucial for generating channel simulation data and evaluating physical layer techniques. It illustrates the stochastic distribution characteristics of channel multipaths, serving as a benchmark in a general scenario. However, in a specific location, the channel parameters in TR 38.901 may not be precise. Regrettably, in an AI-enabled indoor factory, the training data needs to reflect the actual propagation environment as accurately as possible.\nTo address the aforementioned issue, we propose a data augmentation method based on the channel simulator, which can simulate the unlabeled channel data corresponding to the specific area. This approach significantly reduces the collection overhead of unlabeled data. The generation process of unlabeled data is shown as follows:\n\u2022 Step 1: the labeled channel data is collected by the UEs distributed in a specific indoor factory. Since labeled data is essential for training the teacher model, there is no extra overhead of data collection at this stage.\n\u2022 Step 2: upload the labeled data from the UE to the BS.\n\u2022 Step 3: the primary statistical parameters of the channel data are extracted, as described in Section III-A, effectively depicting the wireless propagation environment.\n\u2022 Step 4: these statistical parameters are used to update the parameters of typical TR 38.901 channel model, resulting in the UCHS.\n\u2022 Step 5: the UCHS is used to simulate unlabeled channel data similar to the labeled data of the measurement environment."}, {"title": "IV. SEMI-SUPERVISED PSEUDO-LABELING WITH A BIASED TEACHER", "content": "Compared to traditional SSLs, our proposed method assigns different loss weights to the pseudo-labels of the unlabeled data based on the confidence scores. By doing so, pseudo-labels with low confidence are assigned small weights during the training stage of the student model, thereby mitigating the impact of pseudo-labels with large offset on the performance of the student model.\nThe proposed method pipeline, as illustrated in Fig. 2, consists of three primary stages: the bias teacher training stage, the confidence-based pseudo-labeling stage, and the weighted loss student training stage:\n\u2022 Stage 1: an additional reference signal and a position bias are separately added to the input and output of the traditional SSL model. This addition helps extract additional information from the labeled dataset. Specifically, the position bias represents the distance between the input signal and its nearest signal, referred to as the reference signal. This allows the teacher model to not only determine the mapping function between the input signal and the position coordinates but also to assess the similarity between input signals using the distance bias.\n\u2022 Stage 2: the trained teacher model is utilized to generate pseudo-labels and the position bias of the unlabeled data. The position bias can then be used to define the confidence scores of the unlabeled data through a kernel density estimation (KDE) [16] function.\n\u2022 Stage 3: a weighted loss strategy is employed, wherein different loss weights are assigned to different pseudo-labels in the loss function to train the student model. This approach aims to mitigate the impact of pseudo-labels with larger errors on the performance of the student model.\nA. The biased teacher training\nIn the context of object detection, models not only output the bounding boxes of the detected objects but also provide confidence scores for these detections. Inspired by this, a biased teacher model with two prediction heads is proposed. One prediction head, called the 'predict head\", is responsible for predicting the position coordinates $\\mathbf{l}$ corresponding to the input CIR. The other prediction head, named as the \"biased head\" is designed to predict the bias $\\Delta \\mathbf{l}$ of the reference signal relative to the input CIR position.\nThe first step involves using the k-nearest neighbors (KNN) [17] algorithm to search the reference signal $\\mathbf{H}_{ref}$ in labeled dataset $\\mathcal{D}(\\mathbf{H}, \\mathbf{I})$ that is closest to the input CIR $\\mathbf{H}_i$, thereby obtaining the reference signal data pair $(\\mathbf{H}_{ref}, \\mathbf{l}_{ref})$, i.e.,\n$\\mathbf{H}_{ref} = \\mathbf{H}_j, \\arg \\min_{\\mathbf{H}_j} ||\\mathbf{H}_i - \\mathbf{H}_j ||_F, i \\neq j,$ (7)\nwhere $|| \\cdot ||_F$ is Frobenius norm, and $i \\neq j$ excludes itself as the nearest neighbor.\nThe position bias is defined as:\n$\\Delta \\mathbf{l}_i = \\mathbf{l}_{ref} - \\mathbf{l}_i$. (8)\nThe total loss function of the teacher model is defined as:\n$\\mathcal{L} = \\mathcal{L}^P + \\mathcal{L}^b$, (9)\nwhere $\\mathcal{L}^P = \\frac{1}{N} \\sum_{i=1}^N ||\\mathbf{\\hat{l}}_i - \\mathbf{l}_i||$ and $\\mathcal{L}^b = \\frac{1}{N} \\sum_{i=1}^N ||\\Delta \\mathbf{\\hat{l}}_i - \\Delta \\mathbf{l}_i||$ are the position prediction loss and the bias prediction loss, respectively.\nB. Confidence-based pseudo-labeling\nThis process involves using the teacher model to generate estimated positions $\\mathbf{l}^*$ and position biases $\\Delta \\mathbf{l}^*$ for the unlabeled data, which can then be used for subsequent steps in the SSL framework.\nFor the unlabeled data, the confidence is determined by calculating the distribution of the position bias. This involves computing the position bias as $d = ||\\Delta \\mathbf{l}^*||$. Subsequently, the distribution can be fitted using KDE as follows:\n$f_h(d) = \\frac{1}{Nh} \\sum_{i=1}^N K(\\frac{d - d_i}{h})$, (10)\""}, {"title": "V. EXPERIMENTS AND RESULTS", "content": "A. Scenario deployment\nThe dense clutter high BS (DH) sub-scenario is a priority for evaluation in 3GPP for AI-enabled indoor factory (InF) [1]. In our considered scenario, there are 18 BSs uniformly distributed within the 60m*120m space with a spacing of 20m and height of 8m. The central frequency is 3.5 GHz and the bandwidth is 100 MHz in our experiment. The clutter density is 40% with the clutter height of 2m. The UEs are randomly generated within the space with a height of 1.5m.\nB. Experimental setup\nIn the experiment, a total of 17200 labeled channel samples are collected in the measurement scenario, where 10000 samples are randomly selected to construct the training dataset and the residual 7200 samples are allocated to the testing dataset. Besides, the unlabeled channel data are generated using UCHS. All the models are implemented using PyTorch and the Adam optimizer. The hyperparameters are learning rate 5e-2, an input batch size of 256, 150 epochs, and a cosine annealing rate scheduler. The criteria for InF positioning is the positioning accuracy at a cumulative distribution function (CDF) of 90% for users [1].\nC. Data structure\nIn the cellular positioning problem, UEs measure the time domain CIR $\\mathbf{H}(t)$ from multiple BSs. For each position, the measured $\\mathbf{H}(t)$ by each UE is in the form of $N_{BS} * N_{port} * N_{delay} * 2$ dimensions, where $N_{BS}$, $N_{port}$, and $N_{delay}$ represent the number of BS, the number of BS antenna respectively, and the number of the time delay, and 2 is from the complex-domain to real and imaginary parts. In most industrial scenario, the effective delay is mainly distributed in the first 64 delay taps. Under the current setup, this results in an extremely high-dimensional tensor for each position in the form of 18*4*64*2.\nD. Experimental results\n1) Main results: In this section, we carry out simulations to evaluate the performance of our proposed SSL with position bias, which we call as \"SSLB\" scheme hereafter. We compare the proposed alrotihm with the following three benchmark schemes:\n\u2022 a) SL: traditional supervised learning, whose reference signal $\\mathbf{H}_{ref}$ is represented by the input signal $\\mathbf{H}$, with all the predicted position biases being zero.\n\u2022 b) SLR: SL learning with reference signals. Its input signal contains the current CIR $\\mathbf{H}$ and the reference signal $\\mathbf{H}_{ref}$, and the output is $\\mathbf{l}$ and $\\Delta \\mathbf{\\hat{l}}$.\n\u2022 c) SSLR: SSL pseudo-label learning without a biased teacher.\nThe same DL model and training hyperparameters are used for the four schemes mentioned above. Our method aims to provide a convenient guideline for adaptively obtaining the weight of the unlabeled data using confidence. This is compatible with consistency-based pseudo-labeling SSL [12], [18]-[20], and combining them may lead to better results, but it is beyond the scope of this paper.\nFig. 4 illustrates the positioning accuracy for the four schemes with different labeled samples. It is evident that the SSLB achieves the highest positioning precision. For instance, with less than 6300 samples, SSLB can achieve the precision of 0.897 m, while the SL requires 9900 samples to achieve a precision of 0.969 m. Therefore, with the help of UCHS, our proposed SSLB can reduce the data overhead by almost 40% compared to SL. However, it is worth noting that the performance of the SSL pseudo-label method degrades at 2700 samples, due to the introduction of enormous inaccurate pseudo-labels by the teacher model, which deteriorates the performance of the student model. In contrast, SSLB achieves relatively stable performance, as it can automatically assign small weights to the unlabeled data with large offset.\n2) Ablation study:\na) The impact of weight on the test loss: In this section, we study the effects of weight magnitude $\\alpha$ on the SSLR and our proposed SSLB. In the student model training stage, for both the SSLR and the SSLB, the unlabeled weights are scaled by the formula $w = \\alpha w^*$. Fig. 5 compares the performance of SSLR and SSLB under different scaling coefficients. As the scaling coefficient decreases, the performance gap between the two schemes reduces, whereas the SSLB outperforms the SSLR in all cases. Table I demonstrates the position accuracy at CDF of 90% for users. With the decrease of scaling coefficients, the positioning precision of SSLR increases at first, and then decreases. However, the performance of KDE confidence based SSLB keeps relatively stable. Thus, our\nb) Impact of confidence calibration: To verify the effectiveness of the KDE confidence based method for transforming the position bias to confidence, we compare it with a linear transformation method. The linear transformation method directly considers position bias as confidence, assuming that small bias corresponds to high confidence levels. Firstly, the Euclidean distance of the position bias is computed. Then, this data is normalized within the range of 0 to 1, reversed by subtracting from 1, and finally divided by the mean. The comparison results between the linear method and KDE are presented in Table II."}, {"title": "VI. CONCLUSION", "content": "This paper has investigated the issue of indoor positioning with reduced expenses. On one hand, the the UCHS was used to generate unlabeled data and reduce measurement overheads. On the other hand, we proposed the SSLB, which adaptively weighted the unlabeled data based on confidence to differentiate the quality of the data. This approach effectively reduced the expenses associated with tuning the hyperparameters of the weights. Simulation results demonstrated that our proposed scheme exhibited superior performance, reducing the data measurement overhead by almost 40% compared to benchmark schemes. Furthermore, ablation study confirmed that our proposed scheme can automatically assign different weights to unlabeled data, significantly reducing training expenses."}]}