[{"title": "Beyond Expectations:\nLearning with Stochastic Dominance Made Practical", "authors": ["Shicong Cen", "Jincheng Mei", "Hanjun Dai", "Dale Schuurmans", "Yuejie Chi", "Bo Dai"], "abstract": "Stochastic dominance models risk-averse preferences for decision making with uncertain outcomes,\nwhich naturally captures the intrinsic structure of the underlying uncertainty, in contrast to simply\nresorting to the expectations. Despite theoretically appealing, the application of stochastic dominance in\nmachine learning has been scarce, due to the following challenges: i), the original concept of stochastic\ndominance only provides a partial order, therefore, is not amenable to serve as an optimality criterion;\nand ii), an efficient computational recipe remains lacking due to the continuum nature of evaluating\nstochastic dominance.\nIn this work, we make the first attempt towards establishing a general framework of learning with\nstochastic dominance. We first generalize the stochastic dominance concept to enable feasible compar-\nisons between any arbitrary pair of random variables. We next develop a simple and computationally\nefficient approach for finding the optimal solution in terms of stochastic dominance, which can be seam-\nlessly plugged into many learning tasks. Numerical experiments demonstrate that the proposed method\nachieves comparable performance as standard risk-neutral strategies and obtains better trade-offs against\nrisk across a variety of applications including supervised learning, reinforcement learning, and portfolio\noptimization.", "sections": [{"title": "Introduction", "content": "In machine learning and operations research, the prevalent paradigm of decision making in the presence of\nuncertain and stochastic outcomes is to maximize (resp. minimize) the expected utility (resp. loss) with\nrespect to the decision variables. However, the expectation of the decision-dependent utility function alone\noften depicts an overly simplified snapshot of its distribution, ignoring the intrinsic structure of the underlying\nuncertainty. As such, it fails to distinguish decisions with the same expected utilities but drastically different\noutcomes or model behaviors, especially when taking risk into consideration.\nThere are no shortage of risk-sensitive applications where taming the risk is at least as important as max-\nimizing the utility, examples including financial planning, medical examinations, robotics and autonomous\nsystems, to mention a few. In these high-stake applications, the principle of expectation may lead to inferior\ndecisions due to its uncertainty-agnostic nature. To motivate our discussions, we showcase three distinct\napplications where risk-averse solutions are of particular interest, which will run throughout this paper.\n\u2022 Risk-sensitive supervised learning. In standard supervised learning, one aims to find an optimally\nparameterized model such that the expected loss, which measures the difference between the model"}, {"title": "1.1 Learning with Stochastic Dominance", "content": "It is clear that one needs to go beyond expectations to handle risk, a topic that has been extensively re-\nsearched in many disciplines. Two approaches are called out for modeling risk-averse preferences: mean-risk\n[Markowitz and Todd, 2000] and stochastic dominance [Mann and Whitney, 1947]. Mean-risk models quan-\ntify the problem with two metrics: a mean that measures the expected outcome, and a risk that measures\nthe variability of outcomes. Popular choices of risk measures include variance [Markowitz and Todd, 2000],\nsemideviation [Ogryczak and Ruszczy\u0144ski, 1999], conditional value-at-risk [Rockafellar et al., 2000], entropic\nrisk [Rudloff et al., 2008], and so on. The mean-risk approach models risk-averse preferences by penalizing\nthe mean with the risk measure and allows simple trade-offs and efficient learning algorithms [Maurer and\nPontil, 2009, Duchi and Namkoong, 2019]. However, the design choices of the risk measure and corresponding\ntrade-offs are usually ad-hoc, lacking rigorous justifications.\nStochastic dominance. Stochastic dominance (SD) [Mann and Whitney, 1947, Lehmann, 2011], on the\nother hand, provides a more principled scheme of comparing real-valued random variables by considering\nthe full spectrum of their kth-order cumulative distribution functions, instead of condensing into a single\nscalar metric. In fact, efficient solutions found by the mean-risk approach can be stochastically dominated\nby other feasible solutions [Ogryczak and Ruszczy\u0144ski, 2002], suggesting SD offers stronger guidance and\nfiner granularity in modeling the risk-averse preference. In addition, the deployment of SD for comparing\nrandom variables does not need additional assumptions on the distribution (e.g., the mean-variance approach\nrequires normality [Levy, 2015]).\nAnother nice justification of SD comes from expected utility theory [Boutilier et al., 2006, Armbruster\nand Delage, 2015]. Specifically, if one solution stochastically dominants the other, it yields higher expected\nutility for any utility in a wide class of functions (e.g., non-decreasing functions for first-order dominance).\nSince SD implies higher expectation in the risk-neutral sense by setting the utility function as the identity\nfunction, SD is more selective in model selection as a risk-averse criterion, without the need to specify utility\nfunctions.\nChallenges. Despite the appealing theoretical properties of SD, applications of SD in machine learning\nremain scarce. In truth, practical algorithms for finding a desirable solution under the criterion of SD remain\nlacking due to the following critical challenges.\n\u2022 SD, in its existing form, only defines a partial order over all real-valued random variables, which\ndefies the standard optimization mindset of seeking optimality by optimizing a global scalar objective\nfunction;\n\u2022 Evaluating SD involves comparisons along a continuum of cumulative distribution functions and thus\nnecessitates computationally efficient algorithms."}, {"title": "1.2 Our contribution", "content": "In this work, we aim to establish a general framework of learning with stochastic dominance, by tackling\nthe two challenges mentioned above. We handle the first challenge by quantifying the degree of stochastic\ndominance as a functional and formulating SD optimality as a fixed point of the corresponding optimization\nprocess. This motivates the design of an iterative optimization procedure with non-stationary objective\nfunctions that can be solved efficiently. We summarize our contributions as follows.\n\u2022 We first generalize the original stochastic dominance concept to enable feasible comparisons between\nany arbitrary pair of random variables, paving the way to a general machine learning framework that\noptimizes stochastic dominance.\n\u2022 We propose Learning with Stochastic Dominance (LSD), a novel first-order method for finding ap-\nproximate optimal solutions in terms of stochastic dominance in the hypothesis space.\n\u2022 We establish convergence guarantees under mild technical assumptions despite the non-stationary\nnature of the optimization process. It is shown that LSD finds an e-approximate optimal solution\nwithin $O(\\epsilon^{-2})$ iterations, which introduces minimal computational overhead compared with standard\nmini-batch stochastic gradient method.\n\u2022 We draw connections between SD and distributionally robust optimization (DRO), allowing us to\ninterpret the proposed method as optimizing a surrogate distributionally robust loss.\nTo the best of our knowledge, this work presents the first attempt towards a computationally tractable\napproach for learning stochastic dominance optimal solutions, both practically and theoretically. Numerical\nexperiments are demonstrated to illustrate the effectiveness of our framework for finding risk-averse yet\nperformant solutions in a variety of learning tasks such as supervised learning, reinforcement learning, and\nportfolio optimization.\nThe rest of this paper is organized as follows. Section 2 develops a general learning framework using\nstochastic dominance. Section 3 presents a computationally efficient algorithm and its theoretical compu-\ntational complexity. Numerical results are presented in Section 4. Finally, we discuss connections to DRO\nin Section 5 and conclude the paper in Section 6."}, {"title": "1.3 Related works", "content": "Stochastic-dominance constrained optimization. In the literature, SD is often used to characterize\nthe feasible set of an optimization problem as a constraint w.r.t. a given competitor. In contrast, in our\nSD learning framework, we seek the optimal solution in the stochastic dominance sense within the whole"}, {"title": "2 Stochastic Dominance Learning", "content": "In this section, we first introduce the concept of stochastic dominance, and reveal the difficulty in defining\noptimality in terms of stochastic dominance. We then resolve this difficulty and establish the stochastic\ndominance learning framework.\nStochastic dominance. Let $X$ denote a real-valued random variable. The kth distribution function $F_k$\nis defined recursively as\n$F_1(X; \\eta) = P_X(X \\leq \\eta);$ (1)\n$F_k(X; \\eta) = \\int_{-\\infty}^{\\eta} F_{k-1}(X; a) da = \\frac{1}{(k-1)!} E_X [(\\eta - X)^{k-1}_+],$\n(2)\nwhere $F_1(X; \\eta)$ is the simply the standard cumulative distribution function (CDF). Then, $X$ dominates $Y$\nin the kth-order if [Mann and Whitney, 1947, Dentcheva and Ruszczy\u0144ski, 2003, Lehmann, 2011]\n$F_k(X; \\eta) \\leq F_k(Y; \\eta), \\forall \\eta \\in \\mathbb{R},$\n(3)\ndenoted as $X \\geq_k Y$. By definition, the kth-order dominance implies the $(k + 1)$th-order dominance. In\npractice, the popular choices is choosing $k = 1$ or $k = 2$. First-order stochastic dominance (FSD), by\ndefinition, pursues consistently a lower probability of the random variable falling below a threshold, which"}, {"title": "Generalized stochastic dominance for optimality.", "content": "One might be tempted to search for a model $\\theta^*$\nthat dominates all $\\theta \\in \\Theta$, i.e., the greatest element under stochastic dominance rule in the aforementioned\nlearning scenarios, which would imply (c.f. (3))\n$\\min_{\\eta} [F_k(X_{\\theta^*}; \\eta) - F_k(X_{\\theta}; \\eta)] \\geq 0, \\forall \\theta \\in \\Theta$.\n(4)\nHowever, such $\\theta^*$ is not guaranteed to exist due to the fact that SD only defines partial order among random\nvariables. In other words, there exist two random variables such that their order cannot be distinguished in\nthe sense of SD. Therefore, it is impossible to define \"optimality\u201d in the sense of (4). This gap hinders the\ndevelopment of a learning framework under SD from both theoretical justification and optimization-based\nalgorithm design, which motivates a more general definition of SD.\nWe propose the following Generalized Stochastic Dominance functional:\n$\\Omega(X, Y) = \\max_{\\eta \\in [a,b]} [F_k(X; \\eta) - F_k(Y; \\eta)],$\n(5)\nwhich quantifies the degree of stochastic dominance between $X$ and $Y$ over the interval $[a, b]$ in an unilateral\nway.\u00b9 Here, we restrict the choice of $\\eta$ to $[a, b]$ for numerical tractability as well as for circumventing the\nissue of $F_k(X_{\\theta}; \\eta) - F_k(X_{\\theta'}; \\eta) = 0 - 0 = 0$ when $\\eta$ falls below the support of $X$ for any two models $\\theta$ and\n$\\theta'$. The definition of $\\Omega$ stems from the property of maximal elements under the partial order of stochastic\ndominance:\n$\\max_{\\eta} [F_k(X_{\\theta^*}; \\eta) - F_k(X_{\\theta}; \\eta)] \\geq 0,$\n(6)\nwhere the equality is achieved only when $X_{\\theta^*} \\geq_k X_{\\theta}$. In other words, $\\theta^*$ is not dominated by other feasible\nsolutions. The proposition states the existence of such non-dominated solutions under mild conditions, with\nthe proof deferred in Appendix B.\nProposition 1. It is guaranteed that the non-dominated solution $\\theta^*$ exists as long as $\\Theta$ is compact and that\n$F_k(X_{\\theta}; \\eta)$ is continuous with regard to $\\theta$ for every $\\eta \\in \\mathbb{R}$.\nIn view of the generalized SD in (6), it is now natural to define a general learning problem through an\noptimization lens, by seeking an approximate optimal solution $\\theta^*$ such that for any $\\theta \\in \\Theta$, it holds that\n$\\Omega(X_{\\theta^*}, X_{\\theta}) \\geq -\\epsilon$.\n(7)\nIn other words, $\\theta^*$ is guaranteed to not be dominated by any other solution $\\theta$ by a margin of $\\epsilon$ over the\ninterval $[a, b]$."}, {"title": "3 LSD: First-order Optimization for Learning with SD", "content": "In this section, we design efficient first-order algorithm to solve (7), resolving the computational difficulty\ndiscussed in Section 1.\n3.1 Stochastic Gradient for SD Learning\nThe optimality condition (6) can be written as\n$\\theta^* = \\arg \\min_{\\theta} \\Omega(X_{\\theta}, X_{\\theta^*}),$ (8)\nwhich motivates us to interpret $\\theta^*$ as a fixed point of the following iterative process:\n$\\theta_{t+1} \\leftarrow \\arg \\min_{\\theta} \\Omega(X_{\\theta}, X_{\\theta_t}).$"}, {"title": "Subgradient calculation.", "content": "It remains unclear the optimization properties of (10) as well as how to estimate\ngradients. To proceed, we shall resort to the utility reformulation of $\\Omega$. Note that $\\Omega(X, Y)$ can be equivalently\nwritten in a variational form:\n$\\Omega(X, Y) = \\max_{\\mu \\in \\Delta([a,b])} \\int_a^b [F_k(X; \\eta) - F_k(Y; \\eta)] d\\mu(\\eta),$\n(11)\nwhere the maximum is taken over probability measures over $[a, b]$. For every choice of $\\mu$, changing the order\nof integral yields\n$\\int_a^b (F_k(X; \\eta) - F_k(Y; \\eta)) d\\mu(\\eta) = \\int_a^b [(\\eta - x)^{k-1}_+ f_X(x) dx - (\\eta - y)^{k-1}_+ f_Y(y) dy] d\\mu(\\eta)$\n$ = E_X [u(X)] - E_Y [u(Y)],$\n(12)\n$:=L(X,Y,u)$\nwhere the utility function $u$ is defined as\n$u(x) = \\frac{1}{(k-1)!} \\int_a^b (\\eta - x)^{k-1} d\\mu(\\eta).$\n(13)\nTherefore, we can write $\\Omega(X, Y)$ as\n$\\Omega(X, Y) = \\max_{u \\in U_k} {E_X [u(X)] - E_Y [u(Y)]}.$\nHere, $U_k = {u : u(x) = \\frac{1}{(k-1)!} \\int_a^b (\\eta - x)^{k-1} d\\mu(\\eta), \\mu \\in \\Delta([a, b])}$ collects all utility functions that can be\nexpressed in the form of (13).\nNote that when $k \\geq 2$, $u \\in U_k$ is non-increasing and convex, which guarantees $u(x_{\\theta})$ to be convex with\nregard to $\\theta$, as long as $x_{\\theta}$ is concave [Boyd and Vandenberghe, 2004]. When the sampling probability of $X_{\\theta}$\nis independent of $\\theta$, such as in supervised learning and portforlio optimization, $\\Omega(X, Y)$ takes maximum\nover a set of convex functions and is therefore convex as well. The subgradients of $\\Omega(X_{\\theta}, Y)$ are given by\n$\\partial_{\\theta} [\\Omega (X_{\\theta}, Y)] = conv{\\{\\partial_{\\theta} [E_{X_{\\theta}} [u(X_{\\theta})]] : u \\in U\\}}\\EU},\n$= conv{{Ex [du(Xe)] : u \u20ac EU},\nwhere $U^* = arg \\max_{u \\in U_k} L(X, Y, u)$, and $conv$ is the convex hull. The expectation formulation of the above\nequation allows estimation of the subgradient using sampling, i.e., the sample average, and subgradient chain\nrule, given by\n$\\frac{1}{N} \\sum_{i=1}^N \\partial_{\\theta}u(x_i) = \\frac{1}{N} \\sum_{i=1}^N \\partial_{x_i}u(x_i) \\partial_{\\theta} x_i,$\n(14)\nwhere ${x_i}_{i=1}^N$ are $N$ data points sampled from $X_{\\theta}$. This allows interpreting our proposed method as\nstochastic gradient methods with each sample $x_i$ dynamically weighted by $\\partial_{x_i} u(x_i)$. For learning tasks with\nmodel-dependent sampling probability , one can instead apply log-derivative trick for gradient estimation."}, {"title": "Final algorithm.", "content": "We summarize the algorithm procedure in Algorithm 1. Simply put, the algorithm\nfollows a nested-loop design, where the inner loop focuses on solving (10) by first obtaining $\\hat{u}$ that maximizes\nthe sample estimate of $L$ and then derive the stochastic subgradient with (14). We terminate the inner loop\nand update $\\theta_t$ when the progress condition (9) is approximately met. If (9) is not met within a certain\nnumber of iterations, we conclude that the current $\\theta_t$ is approximately optimal and return the solution."}, {"title": "3.2 Theoretical Analysis", "content": "Two questions arise naturally with regard to the theoretical guarantee of the proposed method: i), whether\nit is guaranteed to converge, and ii), whether it induces a significantly higher iteration complexity compared\nwith standard minibatch SGD methods. The concern stems from the fact that the dynamics of Algorithm\n1 cannot be interpreted as an optimization process targeting a fixed objective function, and that one round\nof inner loop alone can take $O(\\epsilon^{-2})$ iterations to end.\nThe following theorem addresses the concerns above by guarantying the convergence within $\\tilde{O}(\\epsilon^{-2})$ total\niteration complexity.\nTheorem 2. For second-order stochastic dominance $(k = 2)$, assume that $x_{\\theta}$ is concave with regard to $\\theta$, and\nbounded subgradient $|\\partial_{\\theta}x_{\\theta}||_2 < G^2$ and bounded k-th order CDF $F_k(X_{\\theta},\\eta) \\leq C, \\forall \\eta \\in [a,b]$. Let $\\eta_t = 1/\\sqrt{t}$,\nand sample size $N = \\tilde{O}(\\epsilon^{-2})$, $T_{max} = [4C/\\epsilon + 1]$, $T_i = O(\\epsilon^{-2})$. For any initialization $\\theta_0$, with probability\n$1 - \\delta$, Algorithm 1 finds $\\theta_t$ such that for any $\\theta$,\n$\\Omega (X_{\\theta}, X_{\\theta_t}) \\geq -\\epsilon$\nwithin $\\tilde{O}(\\epsilon^{-2})$ iterations."}, {"title": "3.3 Practical Implementation", "content": "When $k \\leq 3$, the computation of $\\hat{u}$ can be done in an efficient way that consumes $O(N)$ memory and\n$\\tilde{O}(N)$ time . Below we demonstrate the case with $k = 2$. Recall that each candidate utility\nfunction $u$ is associated with a probability measure $\\mu$ by\n$u(x) = E_{\\eta \\sim \\mu}[(\\eta - x)_+].$\n(15)\nFor $L(X, Y, u)$ induced by samples ${x_i}_{i=1}^N$ and ${y_i}_{i=1}^N$, we still have\n$L(X,Y, u) = \\int (F_2(x;\\eta) - F_2(Y;\\eta))d\\mu(\\eta),$"}, {"title": "4 Numerical Experiments", "content": "To demonstrate the versatility of our framework, we evaluate LSD on various tasks including supervised\nlearning, reinforcement learning, and portfolio optimization.\n4.1 Supervised Learning\nWe examined the performance of LSD on image classification tasks with MNIST and CIFAR-10 datasets.\nFor MNIST, we train a simple 6-layer convolutional neural network for 10 epoches. For CIFAR-10, we use\na 20-layer ResNet architecture and train for 200 epoches. In both experiments we set batch size to 128 and\nadopt stochastic gradient descent (SGD) method to optimize the models, with learning rate set to 0.1 and\nmomentum set to 0.9. We repeat the training procedure on 30 random seeds. The proposed method achieves\ncomparable classification accuracy with SGD method, and more stable cross-entropy loss under $l_\\infty$-bounded\ndistribution shift, characterized by the average absolute deviation from median .\n4.2 Reinforcement Learning\nWe adopt a modified version of the CliffWalking environment from OpenAI Gym, as illustrated in Figure 2.\nThe action space of the agent is given by {0,1,2,3}, representing moving by one step in four different"}, {"title": "4.3 Portfolio Optimization", "content": "We evaluate the performance of LSD on portfolio optimization with synthesized data and simulate the highly\nnoisy return variables by deploying mixtures of Gaussians with random generated mean and covariance. We\nset the number of stocks to 100 and the number of Gaussian mixtures to 20. To better reflect the heavy-tailed\nnature of the problem, we multiply each Gaussian sample's distance to its center by a random multiplier\ndrawn from $\\chi^2_3/3$. Table 1 compares the constructed portfolio with those resulting from the mean-variance\napproach MV [Markowitz and Todd, 2000] using different levels of variance penalty $\\lambda$, where Figure 4\nfurther illustrates the density of the portfolio returns.\nWhile none of the method simultaneously achieves the highest expected return and the lowest variance,\nwe can evaluate whether the portfolio finds a reasonable trade-off through the Sharpe ratio , a popular choice for measuring the risk-compensated performance, which distinguishes risky portfolios with"}, {"title": "5 Connections with DRO", "content": "Before finishing up the paper, we demonstrate a connection between SD and DRO, which might be of\nindependent interest. Given n samples ${x}_1^n$, the distributionally robust formulation seek to maximize the\nreturn under adversarial distribution shifts, i.e.,\n$\\inf_{P \\ll P_n, P \\in \\mathcal{B}(P_n)} E_P [X],$\nwhere $P_n$ denote the empirical measure of the samples and $\\mathcal{B}(P_n)$ is an uncertainty set centering around\n$P_n$. The following theorem demonstrate that when the uncertainty set is induced by $l_\\infty$ norm, the objective\nfunction can be written in a mean-risk form. The proof is postponed to Appendix E.\nTheorem 3. It holds that\n$\\inf_{P \\ll P_n, ||P-P_n||_{\\infty} \\leq \\rho/n} E_P[X] = E_{P_n} [X] - \\rho MAD[X],$\nwhere $MAD [X] = \\frac{1}{n} \\sum_{i=1}^n |x_i - \\bar{x}|$ denotes the mean absolute deviation from sample median $\\bar{x}$.\nIt is possible to extend the above result to more general choices of uncertainty set, where the relationship\nholds asymptotically (similar to Duchi et al. [2021]), from which we refrain for simplicity. On the other\nhand, we have the following result characterizing the consistency between stochastic dominance and mean-\nsemideviation models [Ogryczak and Ruszczy\u0144ski, 2001, Theorem 1]."}, {"title": "Theorem 4", "content": "Let $k \\geq 1$ and $X,Y \\in L_k$. If $X \\geq_(k+1) Y$, then $E[X] \\geq\nE[Y]$ and\n$E[X] - \\delta^{(k)}_X \\geq E[Y] - \\delta^{(k)}_Y$.\nHere, $\\delta^{(k)}_X$ denotes the kth central semideviation:\n$\\delta^{(k)}_X = E [(E[X] - X)^k_+ 1_{X<E[X]}], k = 1, 2, ...$.\nIn particular, the absolute semideviation at $k = 1$ can be written as\n$\\delta^{(1)}_X = \\int_{-\\infty}^{E[X]} (E[X] - x)f(x)dx = \\frac{1}{2} E[|X - E[X]||].$\nNote that it always holds that $E[|X - \\bar{X}|] < E[|X - \\mu_X|]$, where $\\bar{X}$ is the median, and $\\mu_X$ is the mean. It\nfollows that when there exists $\\theta^*$ such that $X_{\\theta} \\geq_2 X_{\\theta}, \\forall \\theta \\in \\Theta$, then $\\theta^*$ can be interpreted as an approximate\nsolution to the robust optimization problem\n$\\sup_{\\theta \\in \\Theta} \\inf_{P \\ll P_n} {E_P[X_{\\theta}]: ||P-P_n||_{\\infty} < \\frac{\\rho}{n}}$}\nfor all $\\rho \\in (0,1/2)$, in the sense that $\\theta^*$ maximizes a lower bound of the objective function. The approximation\nerror is bounded by $|\\mu_X - \\bar{X}|$"}, {"title": "6 Conclusion", "content": "This paper develops the first practical algorithm for finding an optimal solution in terms of (generalized)\nstochastic dominance for learning and decision making with uncertain outcomes. The method is computation-\nally efficient as it can be easily integrated with existing optimization methods with minimal computational\noverhead, and come with theoretical guarantees for finite-time convergence. Our work opens up opportunities\nto further explore the potential of stochastic dominance in risk-averse machine learning applications."}, {"title": "A LSD for Policy Optimization", "content": "We detail the procedure of LSD applying to policy optimization in Algorithm 3."}, {"title": "B Proof of Proposition 1", "content": "Let {$\\theta_i$}$_{i}$ be a chain under stochastic dominance rule, i.e., $X_{\\theta_i} \\geq_k X_{\\theta_j}$ when $i \\geq j$. The compactness of\n$\\Theta$ assures the existence of a limit point $\\theta \\in \\Theta$, to which a subsequence of {$\\theta_i$}$_{i}$ converges. According to the\ndefinition of stochastic dominance, for every $\\eta \\in \\mathbb{R}$ the sequence {$F_k(X_{\\theta_i};\\eta)$}$_{i=0}^{\\infty}$ is non-decreasing. Since\n$F_k(X_{\\theta};\\eta)$ is continuous with regard to $\\theta$, we have\n$F_k(X_{\\theta};\\eta) = \\lim_{t\\to\\infty} F_k(X_{\\theta_t};\\eta)$.\nBy definition, $X_{\\theta^*}$ stochastically dominates $X_{\\theta}$ for all $\\theta$ from the chain, establishing $\\theta$ as an upper bound of\nthe chain {$\\theta_i$}$_{i=0}^{\\infty}$. The existence of maximal element is then guaranteed by Zorn's lemma."}, {"title": "C Utility reformulation", "content": "By Fubini's theorem, we have\n$\\int_a^b (F_k(X;\\eta) - F_k(Y;\\eta))d\\mu(\\eta) $\n$ = \\int_a^b [(\\int_{\\infty}^{\\eta} (\\eta-x)^{k-1}f_X(x)dx - \\int_{\\infty}^{\\eta} (\\eta-y)^{k-1}f_Y(y)dy) d\\mu(\\eta)] $"}, {"title": "D Proof of Theorem 2", "content": "We start by introducing the following lemma which bounds the statistical error due to sampling when $k = 2$.\nLemma 5. Let $\\tilde{\\Omega"}, 2, "X, Y) = \\max_{u \\in \\mathcal{U}_2} {\\frac{1}{N} \\sum_{i=1}^N u(x_i) - \\frac{1}{N} \\sum_{i=1}^N u(y_i)}$, where {$x_i, y_i$}$_{i=1}^N$ are i.i.d. samples\nfrom X and Y. It holds with probability $1 - 2\\delta'$ that\n$|\\Omega_2(X, Y) - \\tilde{\\Omega}_2(X,Y)| \\leq \\frac{16(a + b)}{\\sqrt{N}} + 6 \\sqrt{\\frac{log(2/\\delta')}{N}}.$\nFor notational simplicity, we denote\n$\\tilde{\\Omega}_2(X_{\\theta_t}, X_{\\theta_t}) = \\max_{u \\in \\mathcal{U}_2} {\\frac{1}{N} \\sum_{i=1}^N u(x_{t,t,i}) - \\frac{1}{N} \\sum_{i=1}^N u(x_{t,i})}.$\nBy setting $\\delta' = \\frac{\\delta}{2 T_{max} (T_{max} + 1)}$ in Lemma 5 and invoking the union bound, we have\n$|\\tilde{\\Omega}_2(X_{\\theta_{t}}, X_{\\theta_t}) - \\Omega_2 (X_{\\theta_{t}}, X_{\\theta_t}) | \\leq \\frac{\\epsilon}{4}, \n(16a)\n$|\\tilde{\\Omega}_2(X_{\\theta}, X_{\\theta_t}) - \\Omega_2 (X_{\\theta}, X_{\\theta_t}) | \\leq \\frac{\\epsilon}{4}, \n(16b)\nfor all $0 \\leq t < T_{max}$, $0 \\leq t < \\mathcal{T}_{max}$ with probability $1 - \\delta$, on which we shall condition in the remaining part\nof the proof. We remark that with the remaining $\\delta$ probability the Algorithm may fail to find a $\\theta_t$ qualified\nfor the output condition, or return a sub-optimal solution that accidentally meet the condition. To proceed,\nwe show that $\\Omega$ satisfies the following triangular inequality:\n$\\Omega(X, Z) $\n$= \\max_{\\mu_{\\eta} \\in \\Delta'([a,b])} (F_k(X; \\eta) - F_k(Z; \\eta), \\mu(\\eta))$ \\\\"]}, {"title": "E Proof of Theorem 3", "content": "The relationship can be established immediately by the following lemma, with u being the distribution shift\n$P - P_n$.\nLemma 6. Let $x$ represents the median of {$x_i, 1 \\leq i \\leq n$}, and\n$U = {u \\in \\mathbb{R}^n | 1^T u = 0, ||u||_{\\infty} \\leq \\epsilon/n}$.\nWe have\n$\\sup_{u \\in \\mathcal{U}} u^T x = \\frac{\\epsilon}{n} \\sum_{i=1}^n |x_i - \\bar{x}|$.\nProof. Note that U is a convex polytope. Therefore $u_{\\mathcal{T}}$ achieves maximum at one of the vertices of U. Note\nthat the vertices of U can be written as\n$u_i = \\begin{cases} \\epsilon/n & i \\in \\Lambda_+\\n-\\epsilon/n & i \\in \\Lambda_-\\n0 & otherwise\\end{cases} : |\\Lambda_+| = |\\Lambda_-| = [\\frac{n}{2}].$\nWhen $\\Lambda_+$ collects the indices of [n/2] largest values in {$x_i$} and $\\Lambda_-$ collecting the smallest values, $u_{\\mathcal{T}}$\nachieves its maximum at\n$u^T x = \\frac{\\epsilon}{n} [\\sum_{i\\in\\Lambda_+} x_i - \\sum_{i\\in\\Lambda_-} x_i] = \\frac{\\epsilon}{n} \\sum_{i=1}^n |x_i - \\bar{x}|.$"}]