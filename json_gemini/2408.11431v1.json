{"title": "Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free Curricular Meaningful Learning", "authors": ["Kai Xiong", "Xiao Ding", "Li Du", "Jiahao Ying", "Ting Liu", "Bing Qin", "Yixin Cao"], "abstract": "Large Language Models (LLMs) are versatile and demonstrate impressive generalization ability by mining and learning information from extensive unlabeled text. However, they still exhibit reasoning mistakes, often stemming from knowledge deficiencies, which can affect their trustworthiness and reliability. Although users can provide diverse and comprehensive queries, obtaining sufficient and effective feedback is demanding. Furthermore, evaluating LLMs comprehensively with limited labeled samples is difficult. This makes it a challenge to diagnose and remedy the deficiencies of LLMs through rich label-free user queries. To tackle this challenge, we propose a label-free curricular meaningful learning framework (LaMer). LaMer first employs relative entropy to automatically diagnose and quantify the knowledge deficiencies of LLMs in a label-free setting. Next, to remedy the diagnosed knowledge deficiencies, we apply curricular meaningful learning: first, we adopt meaningful learning to adaptively synthesize augmentation data according to the severity of the deficiencies, and then design a curricular deficiency remedy strategy to remedy the knowledge deficiencies of LLMs progressively. Experiments show that LaMer efficiently and effectively diagnoses and remedies knowledge deficiencies in LLMs, improving various LLMs across seven out-of-distribution (OOD) reasoning and language understanding benchmarks, achieving comparable results to baselines with just 40% training data. LaMer even surpasses methods that rely on labeled datasets for deficiency diagnosis. In application, our label-free method can offer an effective knowledge deficiency diagnostic tool for efficient LLM development.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have made significant advancements in various fields recently [27, 28, 34]. By implicitly mining and learning information from vast amounts of unlabeled text via language modeling, LLMs have demonstrated remarkable generalization abilities. This enables them to answer a wide array of user queries across many applications such as healthcare [40, 62] and recommender systems [33, 69]. However, despite their potential, LLMs still have limitations. Due to their statistical nature, LLMs occasionally make reasoning mistakes [26, 66], which can undermine user trust and the reliability of their applications. A significant challenge is that the knowledge mining process is implicit, making it difficult to discern what LLMs are particularly good or bad at. This lack of transparency hinders targeted improvements and quality assurance of LLMs. Additionally, relying on users for sufficient and effective feedback is often difficult and impractical, as it requires extra effort and users typically seek answers to questions they do not fully understand. This situation poses a significant obstacle in continually improving LLMs based on massive label-free user queries.\nTo enhance the performance of LLMs, current researches predominantly follow two methodologies: unsupervised language modeling [16, 21] and supervised fine-tuning (SFT) [44, 73], which is"}, {"title": "2 Related Work", "content": "LLMs [25, 43] have demonstrated strong generalization capabilities, but they may not always perform satisfactorily on general or specific capabilities of interest to individuals. Recent works involved additional training to customize or enhance LLMs.\nFor enhancing the general capabilities of LLMs, Xie et al. [70] and Li and Lee [35] conducted continual pre-training on existing LLMs for alignment. Cui et al. [11] and Huozi-Team [24] conducted adaptions with massive Chinese data to enhance the Chinese capabilities of LLAMA [61]. Taori et al. [58] utilized 52K instruction data from ChatGPT [1] to finetune LLaMA and obtain a strong instruction-following LLM. Vicuna [8] adopted data from ShareGPT [53] to train LLaMA and achieved 90% quality of ChatGPT. Orca [45] and Orca-2 [44] adopt progressive learning and more data from GPT-4 [1] to further enhance the general abilities of LLMs. Different from the others, WizardLM [73] designed an evolve-instruct prompt to distill instruction data from ChatGPT with varying difficulty, encouraging LLMs to follow complex instructions. Zephyr [63] employed DPO [48] to align LLMs with human preference, while SPIN [7] devised a self-play method to achieve this.\nFor enhancing specific capabilities of LLMs, researchers used massive task-specific data to largely improve the desired abilities of LLMs such as math [41, 57], coding [20, 50], and reasoning [32, 75]. Furthermore, some works developed specificalized LLMs in various domains such as medical [65], biology [31], and chemistry [42].\nOur work aims to detect the knowledge deficiencies of LLMs and apply the appropriate remedies to repair these deficiencies, which can serve as a complement to or a patch for existing methods."}, {"title": "2.1 Continual Training on LLMs", "content": "LLMs [25, 43] have demonstrated strong generalization capabilities, but they may not always perform satisfactorily on general or specific capabilities of interest to individuals. Recent works involved additional training to customize or enhance LLMs.\nFor enhancing the general capabilities of LLMs, Xie et al. [70] and Li and Lee [35] conducted continual pre-training on existing LLMs for alignment. Cui et al. [11] and Huozi-Team [24] conducted adaptions with massive Chinese data to enhance the Chinese capabilities of LLAMA [61]. Taori et al. [58] utilized 52K instruction data from ChatGPT [1] to finetune LLaMA and obtain a strong instruction-following LLM. Vicuna [8] adopted data from ShareGPT [53] to train LLaMA and achieved 90% quality of ChatGPT. Orca [45] and Orca-2 [44] adopt progressive learning and more data from GPT-4 [1] to further enhance the general abilities of LLMs. Different from the others, WizardLM [73] designed an evolve-instruct prompt to distill instruction data from ChatGPT with varying difficulty, encouraging LLMs to follow complex instructions. Zephyr [63] employed DPO [48] to align LLMs with human preference, while SPIN [7] devised a self-play method to achieve this.\nFor enhancing specific capabilities of LLMs, researchers used massive task-specific data to largely improve the desired abilities of LLMs such as math [41, 57], coding [20, 50], and reasoning [32, 75]. Furthermore, some works developed specificalized LLMs in various domains such as medical [65], biology [31], and chemistry [42].\nOur work aims to detect the knowledge deficiencies of LLMs and apply the appropriate remedies to repair these deficiencies, which can serve as a complement to or a patch for existing methods."}, {"title": "2.2 Evaluation of LLMs", "content": "Since LLMs have strong and broad capabilities, evaluating LLMs becomes a tough and widely concerned issue.\nSome works constructed benchmarks or evaluation data to evaluate LLMs from general and specialized perspectives, such as natural language understanding [22, 36, 67], reasoning [15, 55, 80], math [37, 38], coding [47, 77], etc.\nDifferent from utilizing benchmarks with objective questions, some works started utilizing LLMs to evaluate LLMs subjectively. Liu et al. [39] applied GPT-4 as an evaluator to assess the quality of generated text in various pesperctives (such as coherence) and achieved better alignment with human evaluators. Bai et al. [2] evaluated the performance of existing LLMs with self-evaluation and peer-evaluation, achieving more precise judgments of existing LLMs. Furthermore, Zheng et al. [79] devised a chat framework to evaluate LLMs based on the discussions among LLMs."}, {"title": "3 Method", "content": "We design a label-free curricular meaningful learning framework named LaMer, which utilizes user queries to efficiently diagnose and remedy the knowledge deficiencies in LLMs without labels. As illustrated in Figure 2, LaMer consists of 3 steps: (1) Knowledge Retrieval retrieves relevant knowledge from an external knowledge base for each query to help diagnose knowledge deficiencies; (2) Label-free Knowledge Deficiency Diagnosis leverages relative entropy to automatically diagnose and quantify the knowledge deficiencies in LLMs, which does not rely on labels; (3) Curricular Meaningful Learning incorporates the idea of human conducting meaningful learning [60] to first adaptively synthesize examples in various scenarios for each knowledge deficiency, and then utilize a curricular deficiency remedy strategy to effectively and progressively repair the knowledge deficiencies from minor to severe."}, {"title": "3.1 Knowledge Retrieval", "content": "To introduce knowledge for the following deficiency diagnosis step, we employ an external knowledge base GenericsKB [5] to retrieve knowledge for the given query set D. GenericsKB is a large-scale resource containing 3.4M+ naturally occurring generic facts (such as \"Trees remove carbon dioxide from the atmosphere\").\nSpecifically, to ensure the quality of GenericsKB, we first filter out facts with a confidence score lower than 0.7 and remove duplicates. Next, we employ FlagEmbedding [76] to represent the facts in GenericsKB and each query in D as dense embeddings. Finally, for each query \\(d \\in D\\), we apply cosine similarity to retrieve \\(m\\) pieces of knowledge \\(K = \\{k_1,..., k_m\\}\\) from GenericsKB. The retrieved knowledge K and example d are used in step 2 to diagnose knowledge deficiencies of a specific LLM L."}, {"title": "3.2 Label-free Knowledge Deficiency Diagnosis", "content": "To diagnose the knowledge deficiencies of a specific LLM L in a label-free setting, we propose using relative entropy [30], also known as Kullback-Leibler divergence. This measure quantifies the additional information needed to transition from one distribution to another. Thus, by computing the relative entropy between predictive distributions of L before and after the introduction of knowledge, we can estimate the volume of information that this knowledge imparts to L. If L exhibits a high relative entropy on this knowledge, it suggests that the model either lacks this knowledge or is unable to effectively integrate it into its problem-solving processes, a knowledge deficiency in L is diagnosed.\nSpecifically, given a query \\(d \\in D\\), which possesses a question x and n options \\(O = \\{o_1, o_2,, o_n\\}\\). We first send d to LLM L to obtain the negative log-likelihood of each option \\(o_i\\) conditioned on x and O. Hereafter, we acquire a prior distribution of L over O:\n\\[p_i = L(o_i | x, O),\\]\n\\[P = Softmax([p_1,..., p_n]), \\tag{1}\\]\nwhere \\(p_i\\) denotes the negative log-likelihood of option \\(o_i\\) conditioned on x and O according to L. \\(P\\in \\mathbb{R}^n\\) denotes the prior distribution of L over O. Softmax is the normalization function.\nSecondly, for each retrieved knowledge \\(k \\in K\\) of d, we additionally introduce k to L to fetch a knowledge-based posterior distribution of L over O:\n\\[q_i = L(o_i | k, x, O),\\]\n\\[Q = Softmax([q_1,..., q_n]),\\tag{2}\\]\nwhere \\(q_i\\) denotes the negative log-likelihood of option of after introducing k to L. \\(Q\\in \\mathbb{R}^n\\) denotes the knowledge-based posterior distribution of L over O.\nSubsquently, we compute the relative entropy RE between P and Q to quantify the knowledge difficiency of LLM L on k:\n\\[RE = - \\sum_{i=0}^m P_i \\times (log(Q_i) \u2013 log(P_i)).\\tag{3}\\]\nFinally, after estimating RE on each knowledge of each query based on L, we filter the knowledge and corresponding queries that result in an RE larger than a threshold \\(\\tau\\). Each filtered knowledge and its associated query are then treated as a unit, representing a knowledge deficiency in LLM L. The RE is also a quantification of each knowledge deficiency in L.\nNote that there might be two situations: (1) Helpful: the retrieved knowledge has a positive impact on L, resulting in higher confidence in the correct option; (2) Misleading: the retrieved knowledge has a negative impact on L, resulting in higher confidence in the wrong option. We suppose that both situations can expose the knowledge deficiencies in L. The first situation suggests L might not grasp this knowledge or cannot properly apply this knowledge to problem-solving, while the second situation indicates that L does understand this knowledge and is easily misled by it."}, {"title": "3.3 Curricular Meaningful Learning", "content": "Humans adopt meaningful learning to induce and learn new knowledge through its application across diverse situations [60], leading to a deep understanding and integration of knowledge. Moreover, Humans exploit curriculum learning [3] to effectively learn new knowledge by progressing from easy to hard levels. In light of this, we combine them and design curricular meaningful learning to effectively remedy the diagnosed knowledge deficiencies of L.\nFirstly, we employ a meaningful learning strategy to generate varying examples in diverse scenarios according to the severity of the knowledge deficiencies in L. It is inspired by meaningful learning of humans [60] and the insights that LLMs typically require more tokens or examples to learn the knowledge if they have less prior understanding of it [18, 46]. This strategy can reduce the cost and make deficiency remedy more efficient. Specifically, for the diagnosed knowledge deficiencies of L, we divide them into 4 groups according to the severity (relative entropy) of them. For each group, we heuristically assign a number, which indicates the number of diverse examples we should synthesize for each knowledge deficiency in the group. The detailed groups and assigned numbers are illustrated in Table 1. Subsequently, we adopt a strong LLM ChatGPT [1] to synthesize the specified number of examples for the knowledge deficiencies in each group. The knowledge deficiency (knowledge and the corresponding query) is harnessed to guide the data synthesis process. Each synthesized example contains an input X as the query and an output Y as the response.\nSecondly, we devise a curricular deficiency remedy strategy to progressively remedy the knowledge deficiencies in LLM L from minor to severe. Specifically, we sort the generated examples in ascending order based on the severity of their corresponding knowledge deficiencies, and then feed them into training L sequentially. For each generated example < X, Y >, the training process is in an autoregressive manner to maximize a conditional probability:\n\\[L(X, Y, \\theta) = -\\sum_{t} logp_{\\theta} (Y_t | X, Y_{<t}), \\tag{4}\\]\nwhere \\(\\theta\\) denotes parameters of L. This approach results in an updated L with its deficiencies effectively remedied."}, {"title": "4 Experiments", "content": "We employ 4 different open-source LLMs for experiments to illustrate the general applicability of our proposed LaMer:\n\\begin{itemize}\n    \\item Mistral-7B-Instruct-V0.2 [25] (denoted as Mistral) is an efficient instruction-tunned LLM developed by Mistral AI.\n    \\item LLAMA-3-8B-Instruct [43] (denoted as LLaMA-3) is a dense LLM with massive pre-training on extremely large corpora, which is developed by Meta.\n    \\item Qwen2-7B-Instruct [74] (denoted as Qwen2) is a powerful multilingual LLM developed by Alibaba Cloud.\n    \\item Gemma-1.1-2B-IT [59] (denoted as Gemma-1.1) is an instruction-tunned LLM developed by DeepMind.\n\\end{itemize}"}, {"title": "4.1 Investigated LLMs", "content": "We employ 4 different open-source LLMs for experiments to illustrate the general applicability of our proposed LaMer:\n\\begin{itemize}\n    \\item Mistral-7B-Instruct-V0.2 [25] (denoted as Mistral) is an efficient instruction-tunned LLM developed by Mistral AI.\n    \\item LLAMA-3-8B-Instruct [43] (denoted as LLaMA-3) is a dense LLM with massive pre-training on extremely large corpora, which is developed by Meta.\n    \\item Qwen2-7B-Instruct [74] (denoted as Qwen2) is a powerful multilingual LLM developed by Alibaba Cloud.\n    \\item Gemma-1.1-2B-IT [59] (denoted as Gemma-1.1) is an instruction-tunned LLM developed by DeepMind.\n\\end{itemize}"}, {"title": "4.2 Baselines", "content": "We adopt a wide range of baselines for comprehensive comparisons:\n\\begin{itemize}\n    \\item Base employs the base LLMs in Section 4.1 to answer questions in each benchmark.\n    \\item AugGPT [12] uses ChatGPT [1] to generate questions and answers to augment LLMs with SFT. AugGPT does not provide chain-of-thought [68] in generated examples.\n    \\item Naive is an SFT method that randomly samples several pieces of knowledge from the knowledge base to synthesize new examples without considering whether the specific LLM possesses deficiencies on the sampled knowledge.\n    \\item Single follows a similar process to LaMer but synthesizes only 1 example per knowledge deficiency. Hence, the training data of Single is 40% of LaMer or the other methods.\n\\end{itemize}"}, {"title": "4.3 Evaluation Benchmarks", "content": "We choose 7 OOD benchmarks ranging from reasoning to language understanding, to evaluate the performance of LaMer and baselines:\n\\begin{itemize}\n    \\item Comm. [71] is a collection of 6 commonsense reasoning datasets: An abductive reasoning dataset aNLI [4], a commonsense reasoning dataset CommonsenseQA [56], a commonsense causal reasoning dataset COPA [49], a social interaction reasoning dataset SocialIQa [51], a physical interaction reasoning dataset PIQA [6], and an implicit strategy reasoning dataset StrategyQA [19].\n    \\item AGIEval [80] consists of diverse sets of standardized tests ranging from college admission tests (such as GRE and GMAT) to national civil service examinations.\n    \\item ARC [9] is the AI2 Reasoning Challenge, which is a benchmark of science exams spanning Grade 3 to Grade 9 with easy (ARC-e) and challenge (ARC-c) subsets.\n    \\item MMLU [22] aims to evaluate language comprehension, knowledge, and reasoning skills of LLMs with 57 tasks.\n    \\item BBH [55] is a subset of Big-Bench [54], which contains 23 hardest tasks focusing on challenging scenarios.\n    \\item CRASS [15] measures the counterfactual reasoning abilities of language models.\n\\end{itemize}"}, {"title": "4.4 Implementation Details", "content": "For the knowledge retrieval step, we choose e-CARE [13] and GSM8K [10], discarding the labels to obtain query set D. The size of GenericsKB after filtering is 200K. We utilize FlagEmbedding [76] with beg-large-en-v1.5 to encode facts and queries. We retrieve m = 4 facts as the knowledge for each query. Since GSM8K lacks options and is far from GenericsKB, we choose ChatGPT to generate distractors and m = 4 pieces of knowledge for GSM8K.\nFor the label-free knowledge deficiencies diagnosis step, the knowledge and corresponding query with an RE higher than \\(\\tau\\) = 0.1 is treated as a knowledge deficiency of LLM L. The size of selected knowledge deficiencies can refer to Table ?? in Appendix C.\nFor the meaningful learning strategy in curricular meaningful learning, we utilize a carefully designed prompt to instruct ChatGPT to synthesize examples. Finally, we synthesize 3,750 examples to enhance Mistral, Qwen2, and Gemma-1.1, while 1,250 examples are synthesized to enhance LLaMA-3 due to denser knowledge in it.\nFor the curricular deficiency remedy strategy, we adopt LoRA [23] for parameter-efficient fine-tuning. The rank r and a of LoRA are 128 and 8, respectively. We train LaMer for 3 epochs with a learning rate of 5e-5. The batch size is 32. The optimizer we used is Adam [29]. Two NVIDIA A100 80GB PCIe GPUs are used for training and the following evaluation.\nFor AugGPT and Naive, we randomly sample 3,750 facts from GenericsKB and the generated knowledge of GSM8K to generate the same number of training examples as LaMer for each LLM. While for Single, we randomly sample one example for each knowledge deficiency from the training data of LaMer. Therefore, Single enhances Mistral, Qwen2, and Gemma-1.1 with 1,500 examples, and it utilizes 600 examples to enhance LLaMA-3. The whole data synthesis process is the same as LaMer and the training setup is also the same as LaMer. All prompts for the process of GSM8K and data synthesis can refer to Appendix A. We utilize gpt-3.5-turbo-0125 for all relevant implementations based on ChatGPT."}, {"title": "4.5 Evaluation Details", "content": "For all benchmarks, we utilize free-form generation to evaluate all methods, the evaluation prompts for different LLMs can refer to the Appendix B. The performance of each method on each benchmark is averaged across all tasks in the corresponding benchmark."}, {"title": "4.6 Overall Results", "content": "The overall results of LaMer and baselines are shown in Table 2, from which we can have the following observations:\n\\begin{enumerate}\n    \\item On average, LaMer can outperform all baseline methods across different base LLMs, which is mainly due to the effectiveness of LaMer in diagnosing and remedying knowledge deficiencies without labels. This also reveals the general applicability of LaMer, making it a plug-and-play method to improve LLMs.\n    \\item All data augmentation methods can surpass the base LLM on most benchmarks, while Nive and Single obtain performance drops than base LLaMA-3 and base Gemma-1.1, this is mainly because\n\\end{enumerate}"}, {"title": "5 Case Study", "content": "We provide a case to demonstrate how LaMer diagnoses and remedies a knowledge deficiency of Mistral in Figure 3:\n\\begin{itemize}\n    \\item[(a)] For an initial query (without label) about \"silver\", we can retrieve the knowledge that \"silver is a mildly toxic element\".\n    \\item[(b)] The LLM offer a wrong response which produce a distribution P = [0.63, 0.37] over the options. After providing the LLM with the retrieved knowledge, the LLM offers a new response with a different distribution Q = [0.15, 0.85]. The relative entropy between P and Q is 0.5964, which means the knowledge brings a lot of information to the LLM. Hence, the combination of the knowledge and the query is a knowledge deficiency of the LLM.\n    \\item[(c)] According to Table 1, this knowledge deficiency is in normal group. Thus, we ask ChatGPT to synthesize two examples of this deficiency. These two examples are two applications in two different scenarios of the knowledge in this deficiency. Next, the synthesized examples will be used to train the LLM with other synthesized data.\n    \\item[(d)] After curricular meaningful learning, Mistral offers the correct answer to the initial query. The deficiency is remedied.\n\\end{itemize}"}, {"title": "6 Further Analysis", "content": "To further investigate the strengths and effectiveness of LaMer, we design several ablation studies and in-depth analyses: (1) comparisons between LaMer and label-reliant methods to demonstrate the efficiency and strengths of LaMer; (2) an effectiveness analysis of LaMer on remedying deficiencies by obtain the statistics on remedied examples of each method; (3) an ablation study to"}, {"title": "6.1 Comparisons to Label-reliant Methods", "content": "We conduct an initial analysis to reveal the efficiency of different methods in diagnosing deficiencies based on Mistral-7B-Instruct-v0.2 [25] and e-CARE [13]. Implementation details can refer to Appendix D.1. The results are shown in Table 3, we can have the following observations: our proposed relative entropy method significantly outperforms the perplexity method by recalling more deficiencies for the specific LLM. This proves the advantage and feasibility of our proposed relative entropy method.\nFurthermore, we adopt a label-reliant method LLM2LLM for comparison. LLM2LLM [32] utilizes labeled data to identify erroneous examples in existing datasets, and then synthesizes similar examples to improve a specific LLM. We use the labels of e-CARE and GSM8K to obtain 3,750 error examples, and then we generated similar examples based on the error examples. The number of training examples is the same as LaMer for each LLM. The results are shown in Figure 4 (full results can refer to Appendix D.2), we can find that: LaMer has varying advantages over LLM2LLM on different base LLMs. Although LLM2LLM can precisely detect the error examples of each LLM, the error examples are limited to the given initial dataset. LaMer diagnoses knowledge deficiencies, which can improve the coverage of diagnosed deficiencies in each LLM."}, {"title": "6.2 Effectiveness in Remedying Decificiencies", "content": "We conduct an analysis to examine if LaMer can effectively remedy more error examples. Specifically for each method, we first tally the number of examples in each benchmark that base LLM is wrong but correct after the enhancement by each method (denote as remedied examples). After that, we normalize the values across different methods. Finally, we use spider charts to visualize them. We choose Mistral and LLAMA-3 for investigation. The results are shown in Figure 5, we can infer:\n\\begin{enumerate}\n    \\item LaMer has advantages over the baselines on all benchmarks, this is attributed to that LaMer can diagnose more deficiencies via relative entropy, and remedy them more efficiently via curricular meaningful learning.\n    \\item In Table 2, although AugGPT can exceed LaMer on some benchmarks, it does not get the upper hand in Figure 5. This is mainly due to AugGPT cannot diagnosis the deficiencies of LLMs, and encouraging LLMs to give answers directly is not effective in remedying the deficiencies.\n\\end{enumerate}"}, {"title": "6.3 Effect of Curricular Deficiency Remedy", "content": "To investigate the effect of curricular deficiency remedy in LaMer, we randomly shuffle the data synthesized by LaMer to train the base LLMs. All the training and evaluation settings are the same as LaMer. We denote LaMer with shuffled data as LaMer*. The overall results are shown in Table 4, we can observe that:\n\\begin{enumerate}\n    \\item LaMer can outperform LaMer* on almost all benchmarks regarding each LLM. This is mainly because remedying less severe deficiencies helps remedy more severe ones. Curricular meaningful learning can make LLMs learn new knowledge more efficiently, just like humans conducting knowledge learning [11].\n    \\item When switched to randomly shuffled training data, LaMer* only suffers relatively small performance drops on each base LLM. This claims that the performance of LaMer improvement is stable and LaMer has robust applicability.\n    \\item LaMer* could achieve higher performance than the baselines in Table 2 on Mistral, LLaMA-3, and Qwen2. It clarifies that the advantages of LaMer are not solely due to the curricular deficiency remedy strategy, but primarily stem from the deficiencies diagnosis process based on relative entropy. This highlights the merits of our relative-entropy-based deficiencies diagnosis process.\n\\end{enumerate}"}, {"title": "6.4 Causes for the Advantages of LaMer", "content": "To explore the potential causes for the advantages of LaMer, we visualize the data synthesized by baselines and LaMer into 2D space. Specifically, we utilize FlagEmbedding [76] to represent each example in the synthesized data, and then reduce the dimensionality to 2 with the help of t-SNE [64]. Finally, we employ DBSCAN [14] to discover clusters and remove the noise data points. The eps and min samples of DBSCAN are 1.5 and 3, respectively. We also adopt Mistral and LLaMA-3 for experiments. The visualization is shown in Figure 6, from which we can infer that, data synthesized by LaMer has a significantly higher proportion at the outer edges of the whole distribution (LaMer and baselines). This might be a potential cause for the advancement of LaMer, since LaMer can utilize relative entropy to effectively discover more deficiencies."}, {"title": "6.5 Significance of Deficiencies Caused by Helpful and Misleading Knowledge", "content": "When diagnosing knowledge deficiencies, they can be caused by two kinds of knowledge: (1) helpful knowledge which has a very positive impact on the correct answer; (2) misleading knowledge which could lead LLMs to choose the wrong answer with much higher confidence than the right answer.\nTo find out the significance of the deficiencies caused by these two kinds of knowledge, we split the discovered knowledge deficiencies into two groups (Helpful and Misleading) with the help of golden labels. Then we respectively train the LLM with data synthesized based on Helpful and Misleading deficiencies. Finally, after the evaluation process, we respectively analyze the unique remedied examples brought by the data synthesized based on Helpful and Misleading deficiencies. The results are shown in Figure 7, from which we can have the following conclusions:\n\\begin{enumerate}\n    \\item Deficiencies caused by helpful and misleading knowledge have similar and undeniable significance, which means helpful and misleading knowledge can assist LLMs in remedying similar numbers but different examples. This is due to that both kinds of\n\\end{enumerate}"}, {"title": "7 Conclusion", "content": "In this paper, we design a label-free curricular meaningful learning framework (LaMer) based on relative entropy to first automatically discover the knowledge deficiencies from massive label-free user queries. Then we devise curricular meaningful learning which consists of a meaningful learning strategy and a curricular deficiency remedy strategy, to efficiently and effectively remedy the discovered knowledge deficiencies of corresponding LLM. The experiments show that our proposed LaMer can improve the coverage of diagnosed deficiencies, and surpass the baselines, making LLMs enhancement free of labeled data. The relative-entropy-based deficiency method provides a robust, efficient, and label-free deficiency diagnostic tool for existing LLMs to further unlock their potential."}, {"title": "A Prompts for Implementation", "content": ""}, {"title": "A.1 Prompt for Process GSM8K", "content": "Prompt for Processing GSM8K\nYou are an expert in math problems.\nFirst, please generate some background knowledge which can\nbe used to solve this question. The knowledge SHOULD be\ngeneral and applicable. Your generated knowledge CANNOT be\nquestion-specific. Like commonsense reasoning, I can give the\nknowledge \"Acid is corrosive.\" Just list the knowledge.\nSecond, please answer this question by giving a short ex-\nplanation and then give the answer. Third, please generate some\ndistractors for your answer and index them like \u201c(A)__ (B)__ ...\u201d.\nThe answer and distractors should be as concise as possi-\nble. Your response SHOULD follow the following format:\nBackground Knowledge: [The generated knowledge] Explana-\ntion: [Steps to achieve the answer] Answer: [A pure math part]\nDistractors: [Wrong answers]"}, {"title": "A.2 Prompt for Synthesizing Data", "content": "LaMer: e-CARE\nYou are an expert in creating reasoning and language under-\nstanding questions.\nHere are the requirements:\n1. You are given a fact and some examples for reference, the fact\ncan implicitly guide the solution for the reference examples. You\nshould understand the Internal Mechanism of this guidance to\nmake new examples.\n2. Each created example should contain a question, several\noptions (>=3), an answer, and a short explanation for the answer.\n3. The options should be a short sentence or phrase rather than\na single word whenever possible.\n4. Don't make any commonsense mistakes and ensure that your\nsolutions are accurate.\n5. You SHOULD propose totally new reasoning questions in\nvarious areas, including but not limited to history, law, medical,\nmath, science, computer science, psychology, AI, politics,\neconomics, etc.\n6. NOTE that the fact should be an implicit explanation for\nobtaining the true answer, which means the fact SHOULD NOT\nappear explicitly in the questions or the options.\n7. Only one option is the correct answer, the other options\nshould be much less plausible than the correct option or they are\njust wrong options.\n8. Therefore is no explanation in the reference examples, you\nSHOULD generate an explanation first and then give the answer\nfor your generated new questions.\n9. The question could be in any form, such as \"Why, What, How,\nWhich\" etc. You can also add a premise to form a question.\n10. The created examples cannot be different just in nouns.\nReference:\nKnowledge: { }\nExamples: { }\nYou MUST generate { } new examples. The examples\nMUST be totally different from each other and the reference\nexamples. Please return your response in the form:\nQuestion: [QUESTION]\nOptions: [CANDIDATE OPTIONS]\nAnswer: [The option index of the answer such as (B)]\nExplanation: [A concise explanation for the answer]\nQuestion: [QUESTION]\nOptions: [CANDIDATE OPTIONS]\nAnswer: [The option index of the answer such as (B)]\nExplanation: [A concise explanation for the answer]\nSince GSM8K [10] contains math problems and without options,\nwe adopt ChatGPT to generate distract options and knowledge for\neach query in GSM8K. The following is the prompt:"}, {"title": "B Prompt for Evaluation", "content": "Mistral\n[INST] Question: { }\nOptions: { }[/INST]\nLLaMA-3 and Qwen2\nuser\nQuestion: { }\nOptions: { }\nassistant"}, {"title": "C Size of Setected Knowledge Deficiencies", "content": "The size of selected knowledge deficiencies of each LLM in each\ngroup can refer to Table ??"}, {"title": "D Further Analysis", "content": ""}, {"title": "D.1 Deficiency Diagnosis", "content": "We formally describe the baselines as follows:\n\\begin{itemize}\n    \\item Golden Label adopt the labels to judge the response of\na specific LLM, this LLM would answer each question in\na chain-of-thought [68", "13": "as the dataset for"}]}