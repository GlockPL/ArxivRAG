{"title": "Diagnosing and Remedying Knowledge Deficiencies in LLMs via Label-free Curricular Meaningful Learning", "authors": ["Kai Xiong", "Xiao Ding", "Li Du", "Jiahao Ying", "Ting Liu", "Bing Qin", "Yixin Cao"], "abstract": "Large Language Models (LLMs) are versatile and demonstrate impressive generalization ability by mining and learning information from extensive unlabeled text. However, they still exhibit reasoning mistakes, often stemming from knowledge deficiencies, which can affect their trustworthiness and reliability. Although users can provide diverse and comprehensive queries, obtaining sufficient and effective feedback is demanding. Furthermore, evaluating LLMs comprehensively with limited labeled samples is difficult. This makes it a challenge to diagnose and remedy the deficiencies of LLMs through rich label-free user queries. To tackle this challenge, we propose a label-free curricular meaningful learning framework (LaMer). LaMer first employs relative entropy to automatically diagnose and quantify the knowledge deficiencies of LLMs in a label-free setting. Next, to remedy the diagnosed knowledge deficiencies, we apply curricular meaningful learning: first, we adopt meaningful learning to adaptively synthesize augmentation data according to the severity of the deficiencies, and then design a curricular deficiency remedy strategy to remedy the knowledge deficiencies of LLMs progressively. Experiments show that LaMer efficiently and effectively diagnoses and remedies knowledge deficiencies in LLMs, improving various LLMs across seven out-of-distribution (OOD) reasoning and language understanding benchmarks, achieving comparable results to baselines with just 40% training data. LaMer even surpasses methods that rely on labeled datasets for deficiency diagnosis. In application, our label-free method can offer an effective knowledge deficiency diagnostic tool for efficient LLM development.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have made significant advancements in various fields recently [27, 28, 34]. By implicitly mining and learning information from vast amounts of unlabeled text via language modeling, LLMs have demonstrated remarkable generalization abilities. This enables them to answer a wide array of user queries across many applications such as healthcare [40, 62] and recommender systems [33, 69]. However, despite their potential, LLMs still have limitations. Due to their statistical nature, LLMs occasionally make reasoning mistakes [26, 66], which can undermine user trust and the reliability of their applications. A significant challenge is that the knowledge mining process is implicit, making it difficult to discern what LLMs are particularly good or bad at. This lack of transparency hinders targeted improvements and quality assurance of LLMs. Additionally, relying on users for sufficient and effective feedback is often difficult and impractical, as it requires extra effort and users typically seek answers to questions they do not fully understand. This situation poses a significant obstacle in continually improving LLMs based on massive label-free user queries.\nTo enhance the performance of LLMs, current researches predominantly follow two methodologies: unsupervised language modeling [16, 21] and supervised fine-tuning (SFT) [44, 73], which is"}, {"title": "2 Related Work", "content": "LLMs [25, 43] have demonstrated strong generalization capabilities, but they may not always perform satisfactorily on general or specific capabilities of interest to individuals. Recent works involved additional training to customize or enhance LLMs.\nFor enhancing the general capabilities of LLMs, Xie et al. [70] and Li and Lee [35] conducted continual pre-training on existing LLMs for alignment. Cui et al. [11] and Huozi-Team [24] conducted adaptations with massive Chinese data to enhance the Chinese capabilities of LLAMA [61]. Taori et al. [58] utilized 52K instruction data from ChatGPT [1] to finetune LLaMA and obtain a strong instruction-following LLM. Vicuna [8] adopted data from ShareGPT [53] to train LLaMA and achieved 90% quality of ChatGPT. Orca [45] and Orca-2 [44] adopt progressive learning and more data from GPT-4 [1] to further enhance the general abilities of LLMs. Different from the others, WizardLM [73] designed an evolve-instruct prompt to distill instruction data from ChatGPT with varying difficulty, encouraging LLMs to follow complex instructions. Zephyr [63] employed DPO [48] to align LLMs with human preference, while SPIN [7] devised a self-play method to achieve this.\nFor enhancing specific capabilities of LLMs, researchers used massive task-specific data to largely improve the desired abilities of LLMs such as math [41, 57], coding [20, 50], and reasoning [32, 75]. Furthermore, some works developed specificalized LLMs in various domains such as medical [65], biology [31], and chemistry [42]. Our work aims to detect the knowledge deficiencies of LLMs and apply the appropriate remedies to repair these deficiencies, which can serve as a complement to or a patch for existing methods."}, {"title": "2.2 Evaluation of LLMs", "content": "Since LLMs have strong and broad capabilities, evaluating LLMs becomes a tough and widely concerned issue.\nSome works constructed benchmarks or evaluation data to evaluate LLMs from general and specialized perspectives, such as natural language understanding [22, 36, 67], reasoning [15, 55, 80], math [37, 38], coding [47, 77], etc.\nDifferent from utilizing benchmarks with objective questions, some works started utilizing LLMs to evaluate LLMs subjectively. Liu et al. [39] applied GPT-4 as an evaluator to assess the quality of generated text in various pesperctives (such as coherence) and achieved better alignment with human evaluators. Bai et al. [2] evaluated the performance of existing LLMs with self-evaluation and peer-evaluation, achieving more precise judgments of existing LLMs. Furthermore, Zheng et al. [79] devised a chat framework to evaluate LLMs based on the discussions among LLMs."}, {"title": "3 Method", "content": "We design a label-free curricular meaningful learning framework named LaMer, which utilizes user queries to efficiently diagnose and remedy the knowledge deficiencies in LLMs without labels. As illustrated in Figure 2, LaMer consists of 3 steps: (1) Knowledge Retrieval retrieves relevant knowledge from an external knowledge base for each query to help diagnose knowledge deficiencies; (2) Label-free Knowledge Deficiency Diagnosis leverages relative entropy to automatically diagnose and quantify the knowledge deficiencies in LLMs, which does not rely on labels; (3) Curricular Meaningful Learning incorporates the idea of human conducting meaningful learning [60] to first adaptively synthesize examples in various scenarios for each knowledge deficiency, and then utilize a curricular deficiency remedy strategy to effectively and progressively repair the knowledge deficiencies from minor to severe."}, {"title": "3.1 Knowledge Retrieval", "content": "To introduce knowledge for the following deficiency diagnosis step, we employ an external knowledge base GenericsKB [5] to retrieve knowledge for the given query set $\\mathcal{D}$. GenericsKB is a large-scale resource containing 3.4M+ naturally occurring generic facts (such as \"Trees remove carbon dioxide from the atmosphere\").\nSpecifically, to ensure the quality of GenericsKB, we first filter out facts with a confidence score lower than 0.7 and remove duplicates. Next, we employ FlagEmbedding [76] to represent the facts in GenericsKB and each query in $\\mathcal{D}$ as dense embeddings. Finally, for each query $d \\in \\mathcal{D}$, we apply cosine similarity to retrieve $m$ pieces of knowledge $\\mathcal{K} = \\{k_1,..., k_m\\}$ from GenericsKB. The retrieved knowledge $\\mathcal{K}$ and example $d$ are used in step 2 to diagnose knowledge deficiencies of a specific LLM $\\mathcal{L}$."}, {"title": "3.2 Label-free Knowledge Deficiency Diagnosis", "content": "To diagnose the knowledge deficiencies of a specific LLM $\\mathcal{L}$ in a label-free setting, we propose using relative entropy [30], also known as Kullback-Leibler divergence. This measure quantifies the additional information needed to transition from one distribution to another. Thus, by computing the relative entropy between predictive distributions of $\\mathcal{L}$ before and after the introduction of knowledge, we can estimate the volume of information that this knowledge imparts to $\\mathcal{L}$. If $\\mathcal{L}$ exhibits a high relative entropy on this knowledge, it suggests that the model either lacks this knowledge or is unable to effectively integrate it into its problem-solving processes, a knowledge deficiency in $\\mathcal{L}$ is diagnosed.\nSpecifically, given a query $d \\in \\mathcal{D}$, which possesses a question $x$ and $n$ options $\\mathcal{O} = \\{o_1, o_2,, o_n\\}$. We first send $d$ to LLM $\\mathcal{L}$ to obtain the negative log-likelihood of each option $o_i$ conditioned on $x$ and $\\mathcal{O}$. Hereafter, we acquire a prior distribution of $\\mathcal{L}$ over $\\mathcal{O}$:\n$\\begin{equation}\np_i = \\mathcal{L}(o_i|x, \\mathcal{O}),\\\\\n\\mathcal{P} = \\text{Softmax}([p_1,..., p_n]),\n\\end{equation}$"}, {"title": "3.3 Curricular Meaningful Learning", "content": "Humans adopt meaningful learning to induce and learn new knowledge through its application across diverse situations [60], leading to a deep understanding and integration of knowledge. Moreover, Humans exploit curriculum learning [3] to effectively learn new knowledge by progressing from easy to hard levels. In light of this, we combine them and design curricular meaningful learning to effectively remedy the diagnosed knowledge deficiencies of $\\mathcal{L}$.\nFirstly, we employ a meaningful learning strategy to generate varying examples in diverse scenarios according to the severity of the knowledge deficiencies in $\\mathcal{L}$. It is inspired by meaningful learning of humans [60] and the insights that LLMs typically require more tokens or examples to learn the knowledge if they have less"}, {"title": "4 Experiments", "content": "We employ 4 different open-source LLMs for experiments to illustrate the general applicability of our proposed LaMer:\n\\begin{itemize}\n    \\item Mistral-7B-Instruct-V0.2 [25] (denoted as Mistral) is an efficient instruction-tunned LLM developed by Mistral AI.\n    \\item LLAMA-3-8B-Instruct [43] (denoted as LLaMA-3) is a dense LLM with massive pre-training on extremely large corpora, which is developed by Meta.\n    \\item Qwen2-7B-Instruct [74] (denoted as Qwen2) is a powerful multilingual LLM developed by Alibaba Cloud.\n    \\item Gemma-1.1-2B-IT [59] (denoted as Gemma-1.1) is an instruction-tunned LLM developed by DeepMind.\n\\end{itemize}"}, {"title": "4.2 Baselines", "content": "We adopt a wide range of baselines for comprehensive comparisons:\n\\begin{itemize}\n    \\item Base employs the base LLMs in Section 4.1 to answer questions in each benchmark.\n    \\item AugGPT [12] uses ChatGPT [1] to generate questions and answers to augment LLMs with SFT. AugGPT does not provide chain-of-thought [68] in generated examples.\n    \\item Naive is an SFT method that randomly samples several pieces of knowledge from the knowledge base to synthesize new examples without considering whether the specific LLM possesses deficiencies on the sampled knowledge.\n    \\item Single follows a similar process to LaMer but synthesizes only 1 example per knowledge deficiency. Hence, the training data of Single is 40% of LaMer or the other methods.\n\\end{itemize}"}, {"title": "4.3 Evaluation Benchmarks", "content": "We choose 7 OOD benchmarks ranging from reasoning to language understanding, to evaluate the performance of LaMer and baselines:\n\\begin{itemize}\n    \\item Comm. [71] is a collection of 6 commonsense reasoning datasets: An abductive reasoning dataset aNLI [4], a commonsense reasoning dataset CommonsenseQA [56], a commonsense causal reasoning dataset COPA [49], a social interaction reasoning dataset SocialIQa [51], a physical interaction reasoning dataset PIQA [6], and an implicit strategy reasoning dataset StrategyQA [19].\n    \\item AGIEval [80] consists of diverse sets of standardized tests ranging from college admission tests (such as GRE and GMAT) to national civil service examinations.\n    \\item ARC [9] is the AI2 Reasoning Challenge, which is a benchmark of science exams spanning Grade 3 to Grade 9 with easy (ARC-e) and challenge (ARC-c) subsets.\n    \\item MMLU [22] aims to evaluate language comprehension, knowledge, and reasoning skills of LLMs with 57 tasks.\n    \\item BBH [55] is a subset of Big-Bench [54], which contains 23 hardest tasks focusing on challenging scenarios.\n    \\item CRASS [15] measures the counterfactual reasoning abilities of language models.\n\\end{itemize}"}, {"title": "4.4 Implementation Details", "content": "For the knowledge retrieval step, we choose e-CARE [13] and GSM8K [10], discarding the labels to obtain query set $\\mathcal{D}$. The size of GenericsKB after filtering is 200K. We utilize FlagEmbedding [76] with beg-large-en-v1.5 to encode facts and queries. We retrieve $m = 4$ facts as the knowledge for each query. Since GSM8K lacks options and is far from GenericsKB, we choose ChatGPT to generate distractors and $m = 4$ pieces of knowledge for GSM8K.\nFor the label-free knowledge deficiencies diagnosis step, the knowledge and corresponding query with an RE higher than $\\tau = 0.1$ is treated as a knowledge deficiency of LLM $\\mathcal{L}$. The size of selected knowledge deficiencies can refer to Table ?? in Appendix C.\nFor the meaningful learning strategy in curricular meaningful learning, we utilize a carefully designed prompt to instruct ChatGPT to synthesize examples. Finally, we synthesize 3,750 examples to enhance Mistral, Qwen2, and Gemma-1.1, while 1,250 examples are synthesized to enhance LLaMA-3 due to denser knowledge in it.\nFor the curricular deficiency remedy strategy, we adopt LoRA [23] for parameter-efficient fine-tuning. The rank r and a of LoRA are 128 and 8, respectively. We train LaMer for 3 epochs with a learning rate of 5e-5. The batch size is 32. The optimizer we used is Adam [29]. Two NVIDIA A100 80GB PCIe GPUs are used for training and the following evaluation.\nFor AugGPT and Naive, we randomly sample 3,750 facts from GenericsKB and the generated knowledge of GSM8K to generate the same number of training examples as LaMer for each LLM. While for Single, we randomly sample one example for each knowledge deficiency from the training data of LaMer. Therefore, Single enhances Mistral, Qwen2, and Gemma-1.1 with 1,500 examples, and it utilizes 600 examples to enhance LLaMA-3. The whole data synthesis process is the same as LaMer and the training setup is also the same as LaMer. All prompts for the process of GSM8K and data synthesis can refer to Appendix A. We utilize gpt-3.5-turbo-0125 for all relevant implementations based on ChatGPT."}, {"title": "4.5 Evaluation Details", "content": "For all benchmarks, we utilize free-form generation to evaluate all methods, the evaluation prompts for different LLMs can refer to the Appendix B. The performance of each method on each benchmark is averaged across all tasks in the corresponding benchmark."}, {"title": "4.6 Overall Results", "content": "The overall results of LaMer and baselines are shown in Table 2, from which we can have the following observations:\n(1) On average, LaMer can outperform all baseline methods across different base LLMs, which is mainly due to the effectiveness of LaMer in diagnosing and remedying knowledge deficiencies without labels. This also reveals the general applicability of LaMer, making it a plug-and-play method to improve LLMs.\n(2) All data augmentation methods can surpass the base LLM on most benchmarks, while Nive and Single obtain performance drops than base LLaMA-3 and base Gemma-1.1, this is mainly because"}, {"title": "5 Case Study", "content": "We provide a case to demonstrate how LaMer diagnoses and remedies a knowledge deficiency of Mistral in Figure 3:\n(a) For an initial query (without label) about \"silver\", we can retrieve the knowledge that \"silver is a mildly toxic element\".\n(b) The LLM offer a wrong response which produce a distribution $\\mathcal{P}$ = [0.63, 0.37] over the options. After providing the LLM with the retrieved knowledge, the LLM offers a new response with a different distribution $\\mathcal{Q}$ = [0.15, 0.85]. The relative entropy between $\\mathcal{P}$ and $\\mathcal{Q}$ is 0.5964, which means the knowledge brings a lot of information to the LLM. Hence, the combination of the knowledge and the query is a knowledge deficiency of the LLM.\n(c) According to Table 1, this knowledge deficiency is in normal group. Thus, we ask ChatGPT to synthesize two examples of this deficiency. These two examples are two applications in two different scenarios of the knowledge in this deficiency. Next, the synthesized examples will be used to train the LLM with other synthesized data.\n(d) After curricular meaningful learning, Mistral offers the correct answer to the initial query. The deficiency is remedied."}, {"title": "6 Further Analysis", "content": "To further investigate the strengths and effectiveness of LaMer, we design several ablation studies and in-depth analyses: (1) comparisons between LaMer and label-reliant methods to demonstrate the efficiency and strengths of LaMer; (2) an effectiveness analysis of LaMer on remedying deficiencies by obtain the statistics on remedied examples of each method; (3) an ablation study to"}, {"title": "6.1 Comparisons to Label-reliant Methods", "content": "We conduct an initial analysis to reveal the efficiency of different methods in diagnosing deficiencies based on Mistral-7B-Instruct-v0.2 [25] and e-CARE [13]. Implementation details can refer to Appendix D.1. The results are shown in Table 3, we can have the following observations: our proposed relative entropy method significantly outperforms the perplexity method by recalling more deficiencies for the specific LLM. This proves the advantage and feasibility of our proposed relative entropy method.\nFurthermore, we adopt a label-reliant method LLM2LLM for comparison. LLM2LLM [32] utilizes labeled data to identify erroneous examples in existing datasets, and then synthesizes similar examples to improve a specific LLM. We use the labels of e-CARE and GSM8K to obtain 3,750 error examples, and then we generated similar examples based on the error examples. The number of training examples is the same as LaMer for each LLM. The results are shown in Figure 4 (full results can refer to Appendix D.2), we can find that: LaMer has varying advantages over LLM2LLM on different base LLMs. Although LLM2LLM can precisely detect the error examples of each LLM, the error examples are limited to the given initial dataset. LaMer diagnoses knowledge deficiencies, which can improve the coverage of diagnosed deficiencies in each LLM."}, {"title": "6.2 Effectiveness in Remedying Decificiencies", "content": "We conduct an analysis to examine if LaMer can effectively remedy more error examples. Specifically for each method, we first tally the number of examples in each benchmark that base LLM is wrong but correct after the enhancement by each method (denote as remedied examples). After that, we normalize the values across different methods. Finally, we use spider charts to visualize them. We choose Mistral and LLAMA-3 for investigation. The results are shown in Figure 5, we can infer:\n(1) LaMer has advantages over the baselines on all benchmarks, this is attributed to that LaMer can diagnose more deficiencies via relative entropy, and remedy them more efficiently via curricular meaningful learning.\n(2) In Table 2, although AugGPT can exceed LaMer on some benchmarks, it does not get the upper hand in Figure 5. This is mainly due to AugGPT cannot diagnosis the deficiencies of LLMs, and encouraging LLMs to give answers directly is not effective in remedying the deficiencies."}, {"title": "6.3 Effect of Curricular Deficiency Remedy", "content": "To investigate the effect of curricular deficiency remedy in LaMer, we randomly shuffle the data synthesized by LaMer to train the base LLMs. All the training and evaluation settings are the same as LaMer. We denote LaMer with shuffled data as LaMer*. The overall results are shown in Table 4, we can observe that:\n(1) LaMer can outperform LaMer* on almost all benchmarks regarding each LLM. This is mainly because remedying less severe deficiencies helps remedy more severe ones. Curricular meaningful learning can make LLMs learn new knowledge more efficiently, just like humans conducting knowledge learning [11].\n(2) When switched to randomly shuffled training data, LaMer* only suffers relatively small performance drops on each base LLM. This claims that the performance of LaMer improvement is stable and LaMer has robust applicability.\n(3) LaMer* could achieve higher performance than the baselines in Table 2 on Mistral, LLaMA-3, and Qwen2. It clarifies that the advantages of LaMer are not solely due to the curricular deficiency remedy strategy, but primarily stem from the deficiencies diagnosis process based on relative entropy. This highlights the merits of our relative-entropy-based deficiencies diagnosis process."}, {"title": "6.4 Causes for the Advantages of LaMer", "content": "To explore the potential causes for the advantages of LaMer, we visualize the data synthesized by baselines and LaMer into 2D space. Specifically, we utilize FlagEmbedding [76] to represent each example in the synthesized data, and then reduce the dimensionality to 2 with the help of t-SNE [64]. Finally, we employ DBSCAN [14] to discover clusters and remove the noise data points. The eps and min samples of DBSCAN are 1.5 and 3, respectively. We also adopt Mistral and LLaMA-3 for experiments. The visualization is shown in Figure 6, from which we can infer that, data synthesized by LaMer has a significantly higher proportion at the outer edges of the whole distribution (LaMer and baselines). This might be a potential cause for the advancement of LaMer, since LaMer can utilize relative entropy to effectively discover more deficiencies."}, {"title": "6.5 Significance of Deficiencies Caused by Helpful and Misleading Knowledge", "content": "When diagnosing knowledge deficiencies, they can be caused by two kinds of knowledge: (1) helpful knowledge which has a very positive impact on the correct answer; (2) misleading knowledge which could lead LLMs to choose the wrong answer with much higher confidence than the right answer.\nTo find out the significance of the deficiencies caused by these two kinds of knowledge, we split the discovered knowledge deficiencies into two groups (Helpful and Misleading) with the help of golden labels. Then we respectively train the LLM with data"}, {"title": "7 Conclusion", "content": "In this paper, we design a label-free curricular meaningful learning framework (LaMer) based on relative entropy to first automatically discover the knowledge deficiencies from massive label-free user queries. Then we devise curricular meaningful learning which consists of a meaningful learning strategy and a curricular deficiency remedy strategy, to efficiently and effectively remedy the discovered knowledge deficiencies of corresponding LLM. The experiments show that our proposed LaMer can improve the coverage of diagnosed deficiencies, and surpass the baselines, making LLMs enhancement free of labeled data. The relative-entropy-based deficiency method provides a robust, efficient, and label-free deficiency diagnostic tool for existing LLMs to further unlock their potential."}, {"title": "A Prompts for Implementation", "content": ""}, {"title": "A.1 Prompt for Process GSM8K", "content": "Prompt for Processing GSM8K\nYou are an expert in math problems.\nFirst, please generate some background knowledge which can be used to solve this question. The knowledge SHOULD be general and applicable, Your generated knowledge CANNOT be question-specific. Like commonsense reasoning, I can give the knowledge \"Acid is corrosive.\" Just list the knowledge.\nSecond, please answer this question by giving a short explanation and then give the answer. Third, please generate some distractors for your answer and index them like \u201c(A)__ (B)__ ...\u201d.\nThe answer and distractors should be as concise as possible. Your response SHOULD follow the following format:\nBackground Knowledge: [The generated knowledge] Explanation: [Steps to achieve the answer] Answer: [A pure math part] Distractors: [Wrong answers]"}, {"title": "A.2 Prompt for Synthesizing Data", "content": "LaMer: e-CARE\nYou are an expert in creating reasoning and language understanding questions.\nHere are the requirements:\n1. You are given a fact and some examples for reference, the fact can implicitly guide the solution for the reference examples. You should understand the Internal Mechanism of this guidance to make new examples.\n2. Each created example should contain a question, several options (>=3), an answer, and a short explanation for the answer.\n3. The options should be a short sentence or phrase rather than a single word whenever possible.\n4. Don't make any commonsense mistakes and ensure that your solutions are accurate.\n5. You SHOULD propose totally new reasoning questions in various areas, including but not limited to history, law, medical, math, science, computer science, psychology, AI, politics, economics, etc.\n6. NOTE that the fact should be an implicit explanation for obtaining the true answer, which means the fact SHOULD NOT appear explicitly in the questions or the options.\n7. Only one option is the correct answer, the other options should be much less plausible than the correct option or they are just wrong options.\n8. Therefore is no explanation in the reference examples, you SHOULD generate an explanation first and then give the answer for your generated new questions.\n9. The question could be in any form, such as \"Why, What, How, Which\" etc. You can also add a premise to form a question.\n10. The created examples cannot be different just in nouns."}, {"title": "B Prompt for Evaluation", "content": ""}, {"title": "C Size of Setected Knowledge Deficiencies", "content": "The size of selected knowledge deficiencies of each LLM in each group can refer to Table ??"}, {"title": "D Further Analysis", "content": ""}, {"title": "D.1 Deficiency Diagnosis", "content": "We formally describe the baselines as follows:\n\\begin{itemize}\n    \\item Golden Label adopt the labels to judge the response of a specific LLM, this LLM would answer each question in a chain-of-thought [68] way. If the LLM gives the wrong answers according to the labels on some examples, then the examples are treated as the deficiencies of this LLM. We treat this method as the golden standard for diagnosing the knowledge deficiencies of LLMs.\n    \\item Perplexity computes the perplexity of each option based on a specific LLM, the option with the lowest perplexity is treated as the answer of the LLM, and then labels are introduced the judge the correctness of the LLM. Similar to Golden Label, wrongly answered examples are treated as the deficiencies of this LLM.\n    \\item Random method randomly samples the examples from a dataset.\n    \\item Relative Entropy is the method proposed in this paper.\n\\end{itemize}\nWe choose e-CARE [13] as the dataset for experiments, which is an explainable causal reasoning dataset with two options in each example. The whole set of e-CARE (train, dev, and test) is adopted for experiments. Statistics of e-CARE can refer to Table ??. The LLM we used is Mistral-7B-Instruct-v0.2 [25]."}, {"title": "D.2 Full results of LLM2LLM", "content": "We provide the full results of LaMer, all baselines, and LLM2LLM in Table 5, which demonstrates the advantages of our proposed LaMer."}]}