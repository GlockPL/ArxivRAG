{"title": "PromptLA: Towards Integrity Verification of Black-box Text-to-Image Diffusion Models", "authors": ["Zhuomeng Zhang", "Chong Di", "Fangqi Li", "Shilin Wang"], "abstract": "Current text-to-image (T2I) diffusion models can produce high-quality images, and malicious users who are authorized to use the model only for benign purposes might modify their models to generate images that result in harmful social impacts. Therefore, it is essential to verify the integrity of T2I diffusion models, especially when they are deployed as black-box services. To this end, considering the randomness within the outputs of generative models and the high costs in interacting with them, we capture modifications to the model through the differences in the distributions of the features of generated images. We propose a novel prompt selection algorithm based on learning automaton for efficient and accurate integrity verification of T2I diffusion models. Extensive experiments demonstrate the effectiveness, stability, accuracy and generalization of our algorithm against existing integrity violations compared with baselines. To the best of our knowledge, this paper is the first work addressing the integrity verification of T2I diffusion models, which paves the way to copyright discussions and protections for artificial intelligence applications in practice.", "sections": [{"title": "Introduction", "content": "Generative artificial intelligence has made significant progress in recent years. Notably, text-to-image (T2I) models [9, 22-24, 28], exemplified by Stable Diffusion (SD) [25], have found widespread applications. To generate unique and stylized images, methods including Textual Inversion [12], DreamBooth [26] and LoRA [27] have been proposed as convenient tuning schemes for T2I diffusion models. However, malicious users can also take advantage of these techniques to breach the integrity of models and produce misleading and illegal images, even when they are not authorized to do so. Therefore, in the commercial scenario where the model is only authorized for using as it is, it is essential to verify the integrity of T2I diffusion models to ensure that users do not modify the model through any technique. Upon the occurrence of misconducts, an integrity verification method should attribute malicious behaviors to the user rather than the model's original owner, as shown in Figure 1.\nHowever, research on the integrity of T2I diffusion models is relatively lacking, with most work focusing on ownership verification, such as Stable Signature [11] and Tree-ring [33]. In the field of integrity verification, the focus has remained on relatively basic classification tasks.\nCompared with the integrity verification of models for classification tasks, integrity verification of T2I diffusion models has the following characteristics: (i) randomness, (ii) complexity, and (iii) high access costs, as shown in Table 1. The integrity of classifiers can be examined by whether their predictions change for some trigger samples or not. However, due to the stochastic nature of the diffusion process, it is impossible to determine the integrity of models by simply comparing generated images for a fixed prompt. Moreover, generated images are inherently more complex compared to labels, and accessing the generated images of the model requires more time, which increases the difficulty of integrity verification. Reflecting modifications to models through generated images and selecting prompts for efficient integrity verification are key challenges in achieving integrity verification of T2I diffusion models.\nTo address the aforementioned challenges in verifying the integrity of T2I diffusion models, this paper makes the following contributions:\n\u2022 To the best of our knowledge, this paper is the first integrity verification scheme for T2I diffusion models. The difference"}, {"title": "Preliminary", "content": "During the integrity verification, a malicious user obtains the original model fo and modifies it using some strategy m, resulting in a modified model fm. In the white-box scenario, integrity violations are detected by comparing the hash values of weights [39] of fo and fm, after eliminating the structural symmetries [19]:\n$\\\\text{hash}(f_m) \\\\neq \\\\text{hash}(f_o) \\Rightarrow f_m \\\\neq f_o.$\nIn the black-box scenario, the internal weights cannot be accessed so the hash of a model is intractable. Instead, two models are judged to be different only if their performance can be differentiated. This paper focuses on the black-box scenario.\nTypical integrity violations of models for classification tasks include pruning [34], fine-tuning [4], feature extraction [37], etc. The integrity verification of classifiers uniformly relies on their outputs on a series of triggers $T = \\\\{t_n\\\\}_{n=1}^N$, which constitute their fragile fingerprints. If fm disagrees with fo on at least one trigger\nthen the integrity violation is detected:\n$\\exists t_n \\in T, f_m(t_n) \\neq f_o(t_n) \\Rightarrow f_m \\neq f_o.$\nThe triggers T should be samples sensitive to model changes [15, 35], e.g., samples close to the decision boundary [2, 32, 38].\nIntegrity verification of T2I diffusion models End-to-end finetuning becomes difficult for large T2I diffusion models. Instead, the toolkit for modifying T2I diffusion models includes DreamBooth [26], LoRA [27], direct parameter modifications, etc, which can also be used to breach the integrity of models. Integrity verification of T2I diffusion models is tantamount to find a prompt p such that fm (p) can be distinguished from fo (p). As shown in Figure 2, on the prompt which was used to conduct an integrity violation (\"A sks dog\" here), generated images from fm and fo can be easily distinguished. After eliminating the randomness by fixing the generator seed, a trivial comparison between images generated by different T2I diffusion models yields assertions on the integrity. However, the defender cannot always know the prompts used to conduct integrity violations, and the same model might produce different images given the same prompt due to the randomness in"}, {"title": "Related Work", "content": "Most existing studies focus on detection and tracing of a generated image using color-based [20], frequency-based [10], or learning-based [36] features. ManiFPT [30] applies these methods to the tracing of generated images from various models, including GANs [14], VAE, and LDM [25], etc. The intuition behind these copyright tracing methods is to extract the evidence embedded in the generated images during the generation process that characterizes the generative model. The same intuition also motivates our work: it is possible to extract fine-grained information from generated images that reflects the characteristics of the generative model, thereby telling whether the model has been tampered with or not.\nAs a fundamental component of nonassociative reinforcement learning, wherein the environment operates independently of the input actions, Learning Automaton (LA) seeks to evaluate the efficacy of actions in an unknown environment through iterative interactions, ultimately identifying the optimal action among available choices [21]. Owing to its adaptive capabilities, LA has been extensively employed in various applications, including mathematical optimization [7], pattern recognition [29], cybersecurity [3], and data mining [5]."}, {"title": "Method", "content": "To address the random nature of outputs from text-to-image (T2I) models, we follow the motivation that modifications to the model can be reflected in the feature distribution of generated images. This motivation is verified by an example shown in Figure 4, where the feature distribution of images generated by models fine-tuned using the DreamBooth differs from the feature distribution of images generated by the original model with the same prompt.\nTo numerically measure the distance between randomly generated images from T2I diffusion models under the same prompt, we leverage the KL divergence [16] and employ a variational approach by assuming that the underlying distributions of features is a multivariate normal distribution [13, 17, 18] and further reduce the bias with standard Bayesian estimation with non-informative prior [40]."}, {"title": "PromptLA", "content": "We remark that selecting the most discriminating prompt from a pool of candidates by interacting with black-box T2I diffusion models with the least number of queries is essentially a stochastic optimization task, which can be efficiently solved by reinforcement learning algorithms. We combine the learning automaton (LA) framework based on statistical hypothesis testing approach [8] and design a prompt selection algorithm (PromptLA). In a nutshell, an LA interacts with a stochastic environment (i.e., the feedback from the environment might be different even when the LA chooses the same action) by continually selecting actions, updating its strategy, and converging to the optimal action. In our setting, the environment consists of two T2I diffusion models {fo, fm}, while the set of actions is the set of candidate prompts. The notations that are used in defining the algorithm are summarized in Table 2.\nWe construct the pool of prompts $L = \\\\{p_i\\\\}_{i=1}^N$ with GPT-4 [1]. At the beginning, q prompts that have not been examined before are randomly chosen from L to form the action set A. The algorithm runs for multiple rounds and the action set at the r-th round is denoted by A(r). Naturally, |A(1)| = |A| = q and we set q to 5.\nAt the r-th round, all the remaining actions in the action set A(r) are chosen to interact with environment. To generate more feedback (thus reducing the bias in estimation the optimal prompt) with fewer queries, we conduct a cross-validation among all historical data. Concretely, for each action $a_i \\in A(r)$, the feedback sequence $F_i(r)$ is appended with r extra feedback $F_i(r) = F_i(r - 1) \\cup \\\\{\\beta_i^r(k)\\\\}_{k=1}^r$, where $\\beta_i^r(k)$ is the relative KL divergence computed by Eq. (5), in which P, P', Q are estimated from $n_k$ images produced by fo, fo, fm so far.\nThe action set updating strategy follows the statistical hypothesis testing proposed in Di et al. [8]. When the r-th round terminates, we compute the estimated reward probability $d_i(r)$\n$d_i(r) = \\frac{\\sum_{l=1}^r \\sum_{k=1}^l \\beta_i^r(k)}{|F_i(r)|}.$\nThe estimated optimal action am(r) after the r-th round is the one in A(r) with the highest estimated reward probability $d_i(r)$, i.e.:\n$a_{m(r)} = arg max d_i(r).$\nAfter Rs rounds, the statistical hypothesis testing start. Following Di et al. [8], the Student's t-test is adopted when |Fi(r)| \u2264 30, while the Z-test is adopted in later rounds. For each non-optimal action $a_i \\in A(r), i \\neq m(r)$, the statistical test is conducted given the feedback sequences of action ai and am(r) and the significance level \u03b1. If the null hypothesis $H_o: d_i = d_{m(r)}$ is rejected, action ai is eliminated from the current action set. All the remaining actions in the action set are explored in later rounds\nWe remark that one difference between integrity verification and ordinary optimization is that it is sufficient to find one prompt that can distinguish two models (rather than find the most discriminating one). There is no need to run the algorithm until convergence as in the traditional LA. The entire process terminates after min(R, Re) rounds and returns the most discriminating prompt at that time. The choice of Re reflects a trade-off between algorithm performance and time consumption.\nThe overall process of the proposed PromptLA is summarized in Algorithm 1."}, {"title": "Integrity Verification Framework of T2I Diffusion Models", "content": "As shown in Figure 1, T2I diffusion models publisher can conduct the integrity verification when he/she suspects that a malicious user who has been authorized to use fo only for benign purposes modified the model for harmful purposes.\nFor a suspicious model fm, the publisher first constructs a prompt library $L = \\\\{p_i\\\\}_{i=1}^N$ and sets a threshold \u03b8. Then PromptLA tests the prompt library gradually. Each time, the algorithm selects q prompts and returns the most discriminating prompt which is am from action set, and its corresponding relative KL divergence \u00e2m. If \u00e2m \u2265 0 then an integrity violation is reported. Otherwise, PromptLA continues to explore the remaining prompts in the library. If, after traversing the prompt library, no dm of prompts exceeds the set threshold \u03b8, the model's integrity is considered intact, and the highest value encountered during the process is recorded for the AUC calculation."}, {"title": "Experiments and Discussions", "content": "We used Stable Diffusion (SD) [25] v1.5 as original model whose integrity needs to be protected. It is a fully open-source and widely adopted T2I diffusion model. In the experimental environment of this paper, using default parameters, it takes 5 seconds to access the SD-v1.5 and generate an image.\nFrom the perspective of malicious users, the most commonly considered and the most easily applicable modifications are DreamBooth [26] and LoRA [27]. Other options include vanilla parameter modification or version rollback. They can hardly fulfill malicious purposes, but they serve as good examples of integrity violations for evaluation. In summary, the integrity violations studied in this paper are:\n\u2022 DreamBooth\u00b9: db1 to db4 denotes four kinds of DreamBooth, each with different training data.\n\u2022 Stylized fine tuning: dl denotes Dreamlike Diffusion 1.0, which is SD-v1.5 fine-tuned on high quality art.\n\u2022 Parameter modification: pa1 and pa2. Parameters of SDv1.5 related to the attention mechanism (having the keyword \"attentions\" in their names) are added with various levels of random noise. The noise is uniformly sampled from the [0, 1) interval and scaled by a coefficient that modulates its amplitude, with pa1 set at 0.001 and pa2 at 0.003.\n\u2022 Version rollback: v1.4 denotes using an older version model of stable diffusion, SD-v1.4.\nFor comparison, we transformed schemes designed for detection and tracing of generated images, such as Colorbased [20], Frequency-based [10], and Learning-based [36] into baselines for integrity verification. Different from the original multi-classification task, baselines for integrity verification use some features of generated images as training samples to train a binary classifier under known attacks, which is then tested on a test set. We used CNN as the classifier after feature extraction for training. To make better comparisons, we trained the model using various training set compositions, such as single-violation generated images and a combination of multi-violation generated images, as shown in Table 3.\nIntegrity verification schemes are evaluated in their AUC in the binary classification between intact T2I models from modified ones. The cost of a scheme is measured in the number of images generated by the T2I diffusion models until the verification process terminates.\nIn PromptLA, the number of images generated per prompt per model per round was set to n = 5, the starting round for filtering was Rs = 5, the total number of rounds was Re = 10. We considered two configurations to the significance level \u03b1, and the threshold \u03b8, as (0.01, 0.25) and (0.05, 0.3), which have good performance in comprehensive testing. These versions are denoted as PromptLA_v1 and PromptLA_v2. All methods were repeated for 20 times for each different integrity violations to compute the AUC and the average cost."}, {"title": "Accuracy Evaluation", "content": "Table 3 shows that our method outperformed the baselines across various integrity violations, especially in detecting fine-grained modifications that are hard to spot. Although baselines achieved good detection results for known attacks such as db1 or db4 (i.e., the defender has known the modification that the adversary has performed), they performed poorly on unknown attacks, indicating a lack of generalization. Even if all potential violations are used to train the model, baselines exhibit good performance only on a small subset of them. In contrast, PromptLA selects the appropriate prompt for any integrity violation, which demonstrated strong performance and generalization capability. As shown in Figure 6, the"}, {"title": "Stability Evaluation", "content": "In practice, image-level post-processing operations such as cropping and compression might change the characteristics of images, leading to false integrity alarms. Therefore, an integrity verification algorithm should overlook such image-level post-processing. As shown in Figure 5, when being confronted by random cropping and 85% JPEG quality compression, PromptLA_v1 and PromptLA_v2 both maintained the high AUCs, indicating robust detection performance for various violations. For pa1 in PromptLA_v2 against cropping and JPEG compression, the AUC even improved after image post-processing.\nThis demonstrates that the PromptLA algorithm's performance is stable under certain image-level post-processing such as random cropping and jpeg compression, thanks in part to the robustness of the image feature extraction model Inception-v3 [31]."}, {"title": "Ablation Study", "content": "Finally, we evaluated the necessity of the PromptLA algorithm for integrity verification of T2I diffusion models through ablation studies. Without PromptLA, we employed a vanilla integrity verification framework where the prompt selection module Randomly picks and tests prompts from the library, using n = 50 images generated from the model for each prompt. Random_v1 and PromptLA_v1 used the same threshold \u03b8 = 0.25, while Random_v2 and PromptLA_v2 used the same threshold \u03b8 = 0.3."}, {"title": "Discussion", "content": "The magnitude of integrity violations partially determines the difficulty of spotting them. However, different types of integrity violations can hardly be measured with respect to a unified metric regarding magnitude, either the distance in parameters or that in decision boundaries does not turn out to be conclusive. Due to a consistency concern, we studied this effect by confining the category of modifications to parameter-level modifications, where there is a well-defined order among all modification. As shown in Table 6, when the degree of T2I diffusion model parameter modification gradually increased, so did the AUC of integrity verification. Meanwhile, the Cost decreased for both PromptLA_v1 and PromptLA_v2, which result indicated that integrity violations of a larger magnitude are easier to detect. However, a theoretically unified and convincing metric on the magnitude of integrity violations to deep neural networks, including T2I models, remains a challenge to be addressed."}, {"title": "Conclusion", "content": "In this paper, we propose an integrity verification scheme of T2I diffusion models. Considering the randomness, complexity, and high querying cost associated with T2I diffusion models, we measure the modifications to the underlying model through the differences in the feature distributions of generated images, and propose a prompt selection algorithm based on learning automaton (PromptLA). Extensive experimental results demonstrate that our algorithm offers superior detection accuracy, efficiency, and generalization. PromptLA's performance remains stable under image-level post-processing. The discussion in the experimental section regarding the impact of the degree of integrity tampering on detection results is currently limited to parameter modifications, yet it paves the way to more comprehensive metrics to measure the level of integrity violations. In our future work, we will explore optimizing prompts in continuous spaces and aim to extend the integrity verification framework to various complex generative tasks beyond text-to-image."}]}