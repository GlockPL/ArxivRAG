{"title": "Plausibly Problematic Questions in Multiple-Choice Benchmarks for Commonsense Reasoning", "authors": ["Shramay Palta", "Nishant Balepur", "Peter Rankel", "Sarah Wiegreffe", "Marine Carpuat", "Rachel Rudinger"], "abstract": "Questions involving commonsense reasoning about everyday situations often admit many possible or plausible answers. In contrast, multiple-choice question (MCQ) benchmarks for commonsense reasoning require a hard selection of a single correct answer, which, in principle, should represent the most plausible answer choice. On 250 MCQ items sampled from two commonsense reasoning benchmarks, we collect 5, 000 independent plausibility judgments on answer choices. We find that for over 20% of the sampled MCQs, the answer choice rated most plausible does not match the benchmark gold answers; upon manual inspection, we confirm that this subset exhibits higher rates of problems like ambiguity or semantic mismatch between question and answer choices. Experiments with LLMS reveal low accuracy and high variation in performance on the subset, suggesting our plausibility criterion may be helpful in identifying more reliable benchmark items for commonsense evaluation.", "sections": [{"title": "1 Introduction", "content": "Commonsense reasoning about everyday situations involves soft judgments about the relative plausibility or likelihood of different possible outcomes. If a wine glass falls, a very likely outcome is that it breaks, but another technically possible outcome is that it bounces (e.g., because it lands on a trampoline). Datasets like the Choice of Plausible Alternatives (COPA; Roemmele et al., 2011) or Ordinal Common-sense Inference (Zhang et al., 2017) highlight this graded nature of commonsense reasoning. Many recently developed benchmark datasets for commonsense reasoning formulate problems as multiple choice questions (MCQs): PIQA (Bisk et al., 2020), Social IQa (Sap et al., 2019), CommonsenseQA (Talmor et al., 2019), among others.\nOur data is available at https://github.com/ shramay-palta/commonsense-mcq-plausibility\nThe advantages of MCQ evaluation are clear: with a single correct choice per question, system scores are easy to compute and understand. However, by their nature, commonsense reasoning questions typically do not have a single objectively correct answer; rather they admit many possible answers with varying degrees of plausibility. Under these conditions, what does it mean for a commonsense MCQ answer choice to be the \"correct\" answer?\nWe posit that the \u201ccorrect\u201d MCQ answer in this setting should be the one that human annotators agree is most plausible among options. In principle, the plausibility of an individual MCQ answer choice should depend only on the MCQ context (if applicable), question, and the answer choice itself, but need not depend on the other answer choices. Under this assumption, then, a valid procedure to determine the correct MCQ answer would be to rate the plausibility of each choice individually and select the highest-scoring option.\nIn this paper, we analyze two important commonsense MCQ benchmarks, Social IQa (SIQA; Sap et al., 2019) and CommonsenseQA (CSQA; Tal-2An obvious exception is if an answer choice directly refers to other options, e.g. \"None of the above.\""}, {"title": "2 Human Data Collection", "content": "We select CSQA (Talmor et al., 2019) and SIQA (Sap et al., 2019) for our study as they are popular MCQ benchmarks for general commonsense and social commonsense reasoning, respectively.\nSocial IQa: MCQ items consist of a short context describing a social situation, a question about a person in the situation, and three answer choices (see Fig. 1.) We randomly sample 125 questions from the validation split. These MCQ items were originally assigned a gold answer choice based on a majority vote of five annotators.\nCommonsenseQA: MCQ items consist of a question generated by humans using CONCEPT-NET (Speer et al., 2017) relations and five possible answer choices. We sample another 125 validation questions, which have gold labels based on approval by a second annotator after construction.\nFor each of the 250 sampled MCQ items from these two datasets, we collect two types of human judgments: individual plausibility ratings (\u00a7 2.1) and full question annotations (\u00a7 2.2). Annotators are recruited through Prolific and paid $15/hour; see Appendix A.4 for details, including annotation interfaces. Annotation counts for the two tasks are presented in Table 1."}, {"title": "2.1 Individual Plausibility Ratings", "content": "To obtain the plausibility ratings for each option for a given question, we break down each question q with choices $c_1, c_2, ...c_n$ into pairs (q, ci), where n = 3 for SIQA and 5 for CSQA.\nEach (q, ci) tuple is presented to annotators where they are instructed to \u201crate the plausibility of the answer choice for the given question on a 5-point Likert scale\". We use the plausibility Likert scale introduced by Zhang et al. (2017) for ordinal common-sense inference, defined as 1-Impossible, 2-Technically Possible, 3-Plausible, 4-Likely and 5-Very Likely.\nWe obtain 5 annotations for each (q, ci) tuple. To ensure independence, each annotator judges at most one (q, ci) tuple for a given question q. Krippendorff's \u03b1 on SIQA and CSQA is 0.46 and 0.64, respectively."}, {"title": "2.2 Full Question Annotation", "content": "In this setting, annotators are provided the full MCQ item with all the answer choices and asked to select the (single) best option, similar to the validation procedures used to obtain original gold labels. However, to measure human agreement, we re-collect these annotations ourselves in larger numbers. Each MCQ item first receives five annotations; if no answer choice receives a majority vote from the annotators by a margin of two or more, then five more annotations are collected for the item. Krippendorff's \u03b1 on SIQA and CSQA is 0.66 and 0.71, respectively. In over 87% of cases on both datasets, the majority vote from our annotators matches the original gold label in the datasets."}, {"title": "3 Plausibly Problematic MCQS", "content": "With these collected judgments, we consider three ways to define a \u201ccorrect\u201d answer choice for each MCQ item: (1) the original gold answer choices from SIQA or CSQA (Ydataset), (2) the majority-vote answer choice from full question annotation (Yfull), and (3) the answer choice with the maximum mean plausibility rating (Yplausibility). We hypothesize that Yplausibility should be predictive of Ydataset and yfull across MCQS, and that when they diverge it may be indicative of one or more problems with the underlying MCQ.\nTo corroborate this idea, first we show that a small difference in plausibility scores between the highest- and second-highest scoring answers in the individual plausibility setting is correlated with lower agreement on the full question annotations, for both datasets. This is consistent with the idea that disagreements on full MCQ annotations may arise when there is not a clear most-plausible answer.\nNext we compare Yplausibility to Ydataset. For both SIQA and CSQA, Yplausibility diverges from Ydataset in 22.4% of MCQs. We define these MCQs as \"plausibly problematic\" questions given that the answer choice selected as Yplausibility did not match Ydataset."}, {"title": "3.1 Qualitative Analysis", "content": "We conduct a manual inspection to identify the key issues with these \"plausibly problematic\" questions (identified using the plausibility judgements from \u00a7 2.1) and examine all such questions from SIQA and CSQA. We categorize the potential issues as:\n1) Semantic Mismatch or Constraints: A semantic discrepancy exists either between the question and at least one answer choice, or the question implies specific semantic limitations that at least one answer choice fails to meet; 2) Question is not coherent: The question is not properly structured, leading to confusion and lack of clarity or is a poor fit for the context; 3) Ambiguous: The question requires one or more implicit assumptions to pick an answer; 4) No good answer choices: There are no answer choices that are a good fit for the question; and 5) No Prominent Issue: There is no prominent issue with the question. Examples of questions with each of these labels are presented in Appendix A.5.\nAs seen in Figure 3a and Figure 3b, Ambiguous and Semantic Mismatch or Constraints are the most common issues with the \u201cplausibly problematic\" questions. The prevalence of these labels indicates that there are questions in both of these datasets which have multiple possible valid interpretations. We recommend future works to build upon our findings and urge dataset creators to ensure that the questions in their datasets do not have multiple different but valid interpretations, and that all answer choices should be geared towards one interpretation. We also encourage dataset creators to include of \"not applicable\u201d or \u201cquestion does not make sense\" option (Dowty, 1991), especially when creating datasets involving automatic assignments of questions.\nWe also observe very few cases where a question is tagged with the No Prominent Issue label, which could be attributed to noise from the human annotations (\u00a7 2.1). A similar analysis on an equal number of questions sampled randomly from the set of \u201cNon-Problematic\u201d Questions is presented in Figure 3a and Figure 3b. We find that a vast majority of the non-problematic questions would receive the No Prominent Issue label, suggesting that the questions were clear and had an answer choice which was clearly suited better than the others. This indicates that our approach is also able to identify non-problematic questions accurately.\""}, {"title": "4 Implications for LLM Evaluation", "content": "We prompt LLMS with the same task posed to humans in \u00a7 2.1 and \u00a7 2.2. We study multiple state-of-the-art LLMS: GPT-4 (gpt-4-0125-preview) (Achiam et al., 2023) with the OpenAI API, LLaMA-2 (7B, 13B and 70B) (Touvron et al., 2023), Mistral (7B and 7x8B) (Jiang et al., 2024) and Yi (6B, 9B and 34B) (AI et al., 2024). We prompt each LLM with the same 10 in-context examples for the Plausibility and Full settings."}, {"title": "6 Conclusion", "content": "In this work, we show that plausibility judgments are a useful tool for identifying potentially problematic commonsense MCQ items. With individual plausibility ratings, we are able to identify questions where the gold answer does not match the answer with the highest plausibility. Through manual analysis we identify several types of issues that are more prevalent among the identified subset. We show that LLMS and humans perform poorly on these questions, with a high degree of variance, suggesting they add noise to benchmark evaluations. Future work may investigate methods of incorporating plausibility judgments into the creation stage of benchmark development, as well as the application of these ideas to evaluating other types of benchmarks involving graded judgments beyond commonsense reasoning."}, {"title": "7 Limitations", "content": "Uncertainty can arise due to a variety of reasons such as multi-cultural and multi-ethnic aspects of commonsense reasoning. In this work while we introduce a new method to identify questions with multiple plausible answers, we are limited to a US-centric angle of uncertainty owing to the fact that our annotators are based in the US.\nAdditionally, our annotation framework is expensive and thus difficult to run on an entire dataset. However, since we are the first to explore plausibility of answer choices in commonsense reasoning situations, we hope that this work motivates other researchers to study plausibility more extensively. The identification and annotation of uncertainty can be subjective, leading to inconsistencies or disagreements among annotators. While we employed rigorous annotation protocols and made sure each question was annotated by at least 5 annotators, there may still be instances where ambiguity interpretation varies."}]}