{"title": "LLMs for Literature Review: Are we there yet?", "authors": ["Shubham Agarwal", "Gaurav Sahu", "Abhay Puri", "Issam H. Laradji", "Krishnamurthy DJ Dvijotham", "Jason Stanley", "Laurent Charlin", "Christopher Pal"], "abstract": "Literature reviews are an essential component of scientific research, but they remain time-\nintensive and challenging to write, especially due to the recent influx of research papers. This\npaper explores the zero-shot abilities of recent Large Language Models (LLMs) in assisting\nwith the writing of literature reviews based on an abstract. We decompose the task into\ntwo components: (1) Retrieving related works given a query abstract and (2) Writing a\nliterature review based on the retrieved results. We analyze how effective LLMs are for\nboth components. For retrieval, we introduce a novel two-step search strategy that first uses\nan LLM to extract meaningful keywords from the abstract of a paper and then retrieves\npotentially relevant papers by querying an external knowledge base. Additionally, we study\na prompting-based re-ranking mechanism with attribution and show that re-ranking doubles\nthe normalized recall compared to naive search methods, while providing insights into the\nLLM's decision-making process. In the generation phase, we propose a two-step approach\nthat first outlines a plan for the review and then executes steps in the plan to generate\nthe actual review. To evaluate different LLM-based literature review methods, we create\ntest sets from arXiv papers using a protocol designed for rolling use with newly released\nLLMs to avoid test set contamination in zero-shot evaluations. We release this evaluation\nprotocol to promote additional research and development in this regard. Our empirical\nresults suggest that LLMs show promising potential for writing literature reviews when the\ntask is decomposed into smaller components of retrieval and planning. Particularly, we find\nthat combining keyword-based and document-embedding-based search improves precision\nand recall during retrieval by 10% and 30% respectively, compared to using either of the\nmethods in isolation. Further, we demonstrate that our planning-based approach achieves", "sections": [{"title": "1 Introduction", "content": "Writing a literature review-finding, citing, and contextualizing relevant prior work is a fundamental\nrequirement of scientific writing. It is a complex task and can be broken down into two broad sub-tasks: 1)\nFinding relevant papers and 2) Generating a related work section that situates the proposed research within\nthe existing literature. This challenge is further amplified in the field of machine learning, with thousands\nof relevant papers appearing every month on ArXiv alone. We explore the utility and potential of large\nlanguage models (LLMs), in combination with retrieval mechanisms, to assist in generating comprehensive\nliterature reviews for scientific papers.\nSpecifically, we investigate the use of LLMs to generate the related work section of a paper based on its\nabstract. We use the term abstract loosely, not necessarily to refer to the actual abstract of the paper, but\nrather a textual passage that captures a concise summary of the paper's key contributions and scope. Using\nthe abstract as input allows our system to target the central ideas of the paper without requiring the complete\nmanuscript, which is often continuously evolving in the early stages of writing. While our experiments focus\non using the abstract, our framework is designed to be flexible and can make use of the entire manuscript as it\nevolves, albeit at a larger computational cost and the need to use models that support long context windows.\nThis approach provides valuable early-stage insights for authors seeking preliminary references to shape their\nwork, with the capacity to seamlessly incorporate additional information as the manuscript develops.\nThe architecture of our framework is illustrated in Figure 1. Correspondingly, in this work: 1) We introduce\nan LLM-based approach to retrieve relevant papers, where we first extract the keywords from an abstract or\nresearch idea paragraph using an LLM, and then feed these keywords to a keyword based search tool we\nexperiment with Google search and Semantic Scholar. We optionally also transform the abstract or idea into\nan embedding and use an embedding based search procedure as well. 2) We then employ a prompting-based\napproach to rank the set of retrieved candidate papers based on their relevance to the query abstract, also\nrequiring the LLM to attribute the relevance to specific excerpts in the candidate papers. We explore multiple\nre-ranking and aggregation strategies 3) To generate the literature review, we select top-k papers from the\nranked list and prompt the LLM to generate the related work section based on the query abstract and the\nabstracts of the selected papers. 4) Additionally, we examine the effectiveness of providing a writing plan to\nthe LLM that specifies which papers to cite at various points in the literature review. These plans can be\ngenerated entirely by the LLM, by the user, or a combination of the two. These plans serve as an intermediate\nrepresentation giving the user more control over the organizational structure of the literature review.\nOur most powerful model involves multiple innovations, where we use LLMs in multiple ways, namely for\ngenerating search queries, re-ranking search results, and attribution. We summarize the main contributions\nof our work as follows:\n\u2022\n\u2022\nTo answer the key question that our paper poses, we present a data collection protocol and multiple\ninstances of using it to collect arXiv papers. Critically, our protocol is based on using the most recent\npast month of arXiv papers in a rolling manner with the goal of avoiding test-set contamination\nwhen evaluating the most recent LLMs for literature review-related tasks. We then use this protocol\nto perform extensive retrieval and literature review generation experiments. We release both our\ndatasets and our code to the community.\nWe propose a novel LLM-based pipeline for the task of interactive literature review writing, which we\ndecompose into two distinct components: retrieval and generation. This also facilitates more controlled\nstudies investigating alternative LLM-based approaches for these sub-tasks. Our experiments focus\non evaluating fully automated variants of these sub-tasks, but our framing of the problem and our\nproposed solutions are easily integrated into scenarios where human users interact with systems that"}, {"title": "2 Related Work", "content": "The concept of literature review generation using large language models (LLMs) is built upon the foundation\nlaid by the Multi-XScience dataset proposed by Lu et al. (2020). This dataset paves the way for the challenging\ntask of multi-document summarization, specifically focusing on generating the related-work section of a\nscientific paper. As underlined by Lu et al. (2020), this approach favors abstractive models, which are well\nsuited for the task. However, unlike the approach suggested by Lu et al. (2020), our work introduces the\nuse of intermediate plans to improve the quality of generated literature reviews. The empirical evidence\npresented in our study shows that our novel strategy outperforms the vanilla zero-shot generation previously\nchampioned by the Multi-XScience dataset (Lu et al., 2020). (Note: This paragraph was entirely generated\nby GPT-4 following plan-based generation.)\nClosely related to our work, Gao et al. (2023) generates answers for questions based on the citations from\nWikipedia. Also related to our work, Pilault et al. (2020) examined LLM-based abstractive summarization of\nscientific papers in the Arxiv dataset of Cohan et al. (2018); however, their work was limited to creating\nthe abstract of a single document. Perhaps the most similar prior prompting-based approach to our work\nis known as 0-shot chain-of-thought prompting (Kojima et al., 2022; Zhou et al., 2022) where a model is\nprompted with 'Let's think step-by-step' (and similar prompts).\nRecent efforts have explored the application of proprietary and open-source LLMs for ranking (Sun et al.,\n2023; Ma et al., 2023; Pradeep et al., 2023a;b), where the LLM is passed a combined list of passages directly\nas input and prompted to rank them based on a criteria. Notably, only top-k candidates are passed as input\nto the LLM for re-ranking (Zhang et al., 2023). In our work, we instruct an LLM to output an ordering\nof the different candidate papers (e.g. [3] > [8] > [6]) in descending order based on the relevance to the\nuser-provided abstract.\nTraditional methods for Natural Language Generation have typically employed a rule-based modular pipeline\napproach comprising of multiple stages of generation with intermediary steps of content planning (selecting\ncontent from input while also determining the structure of the output), sentence planning (planning the\nstructure of sentences) and surface realization (surfacing the text in sentence) (Reiter & Dale, 1997; Stent\net al., 2004; Walker et al., 2007). Our proposed plan-based prompting technique draws a parallel between\nthe modern methods of end-to-end neural models for joint data-to-text generation with micro or content\nplanning (Gehrmann et al., 2018; Puduppully et al., 2019; Puduppully & Lapata, 2021) where we use plans\nto define the sentence structure of the generated output. While some recent works have explored planning in\nterms of devising actions (Yang et al., 2022; Song et al., 2023; Wang et al., 2023), prompting LLMs based on\nsentence plans have not been explored, to the best of our knowledge. We show two strategies of using plans\n1.) The model generates the sentence plan as an intermediary step and conditions on this generated plan to\noutput the final summary autoregressively. 2.) Humans can provide a ground-truth plan which results in an\niterative setting, inherently providing controllability to the generated text where LLMs are susceptible to\ngenerating additional content.\nAdditionally, Galactica has been developed to store, combine, and reason about scientific knowledge (Taylor\net al., 2022). It outperforms existing models on various scientific tasks and sets new state-of-the-art results on\ndownstream tasks. These findings highlight the potential of language models as a new interface for scientific\nresearch. However, the Galactica model was not developed to specifically address the problem of literature\nreview assistance and it was not instruction fine-tuned to follow writing plans, and as such it suffered from\nthe effect of hallucinating non-existent citations and results associated with imaginary prior work. Recent\nworks (Rodriguez et al., 2024a;b; Awadalla et al., 2024) have focused on building datasets multimodal of\ndocuments and scientific contents. However, our study focuses on exploring the zero-shot abilities of LLMs\nfor literature review generation and proposes a novel strategy that includes generating an intermediate plan\nbefore generating the actual text. Our empirical study shows that these intermediate plans improve the\nquality of generated literature reviews compared to vanilla zero-shot generation. Furthermore, we ensure the\nvalidity of our experiments by using a new test corpus consisting of recent arXiv papers to avoid test set\ncontamination. (Note: GPT-3.5 generated this paragraph with the 4th sentence added by the authors)."}, {"title": "3 Retrieval of Related Work", "content": "In this section, we discuss the creation of the corpus of arXiv papers to examine different retrieval strategies\nfor finding related works for a given paper abstract using different academic and generic web search engines,\nincluding Semantic Scholar and Google Search."}, {"title": "3.1 Dataset Construction.", "content": "We create two datasets that contain papers posted on ArXiv in August and December 2023 respectively,\nstarting with 1,000 papers from each month. We use ArXiv wrapper in Python to create RollingEval"}, {"title": "3.2 Retrieving Candidate Papers", "content": "To retrieve related work for a given paper abstract, first, for each query abstract in the dataset, we prompt\nan LLM to generate keywords that we use as queries for a search API (refer to Figure 14 in the Appendix\nfor the detailed prompt used for this task). Importantly, we add a timestamp filter to the search API to\nretrieve papers published stricly before the publication date of the query paper. In addition to evaluating\nmultiple search engines, we also experiment with generating multiple queries 6 from an LLM and various\nheuristics to combine the search results from each query (see Appendix D). We evaluate multiple general\nand academic search engines on the quality of the retrieved papers using coverage, which we define as the\npercentage of ground-truth papers retrieved by the search engine. Table 1 shows the coverage for different\nsearch engines and query heuristics. We note that using multiple queries achieves the highest coverage with\ncomparable results for Semantic Scholar search and SERP API. However, at best, we retrieve just under 7%\nof the ground truth papers. The low retrieval percentage of just under 7% can be attributed to several factors.\nFirst, the task of finding related work for a given paper is inherently challenging due to the diverse styles and\nmethods authors use in literature reviews. This stylistic variability means that a one-size-fits-all approach,\nsuch as generating search keywords and using a search engine, might not capture the nuanced criteria that a"}, {"title": "3.3 Re-ranking Candidate Papers", "content": "Next, given a list of retrieved papers, we explore re-ranking the list using an LLM. The retrieved abstracts\nand the original query abstract are used as input to an LLM Re-ranker, which provides a listwise ranking of the\ncandidate papers based on the relevance to the query abstract. We explore different strategies for reranking\nthe candidates, detailed as follows: a) Instructional permutation generation: we use the approach\nby Sun et al. (2023), which prompts the model to directly generate a permutation of the different candidate\npapers, thus producing an ordered list of preferences against providing intermediate scores; b) SPECTER2\nembeddings: We use SPECTER2 embeddings as an alternative to prompting-based strategies for reranking,\nwhere we rank the candidate papers based on their cosine distances to the SPECTER2 embedding of the\nquery abstract (see Appendix D for more details on SPECTER2 implementation); and c) Debate Ranking\nwith Attribution (Ours): our prompting-based approach that builds on the work of Rahaman et al.\n(2024), where we pass each candidate paper's abstract along with the query abstract and instruct the LLM to\n(1) generate arguments for and against including the candidate paper and (2) output a final probability of\nincluding the candidate based on the arguments. Crucially, we add an attribution step to this ranking module,\nwhere we instruct the LLM to extract verbatim sentences from the candidate abstract that support the\narguments, and we re-prompt the LLM if the extracted sentences are not present in the candidate abstract."}, {"title": "3.4 Retrieval and Re-ranking Experiments", "content": "We use an ensemble of search engines to retrieve candidates based on an abstract. We now describe the\nsearch engines used in our approach. Based on Table 1, we select the Semantic Scholar (S2) API as the search\nengine to retrieve search results using LLM-generated keywords. As discussed in Section 3.2, we also explore\nSPECTER2 embeddings for retrieval and compare it with the different prompting-based strategies.\nWe experiment with different combinations of search engine and the retriever method, and present our results\nin Figure 2. We present precision and normalized recall at different values of top-k recommendations, where\nwe calculate normalized recall as the proportion of the number of ground truth papers retrieved (instead of\nall ground truth papers). Formally, normalized recall and precision follow the definitions:\nNormalized Recall = \\frac{{\\text{Retrieved} \\cap \\text{Ground Truth}|}}{{|\\text{Ground Truth}|}} ; Precision@k = \\frac{{\\text{Retrieved} \\cap \\text{Ground Truth}}{k}(1)\nwhere k is the number of top-k recommendations, \"Retrieved\" denotes the set of top-k candidate papers, and\n\u201cGround Truth\" denotes the set of Ground Truth (GT) cited papers (papers cited by the query paper).\nFrom Figure 2, we note that debate ranking significantly outperforms permutation ranking, as denoted by\nthe higher precision and normalized recall at smaller values of top-k recommendations; however, SPECTER\nembeddings outperform both prompting-based strategies. Next, the higher precision and normalized recall\nvalues for SPECTER and S2+SPECTER settings suggest that SPECTER is also an excellent search engine."}, {"title": "3.5 Positive Effect of Attribution Verification", "content": "We conduct an ablation study on the first 100 papers of the larger set of 500, and focus on the top k = 40\npapers, which is representative of the typical number of papers cited in the Machine Learning community. In\nFigure 3 we show the result of removing the verification step in our debate re-ranking strategy, i.e. in this\nablation we do not check if the sentences extracted by the LLM are indeed present in the candidate paper\nabstract. We find that removing this kind of attribution verification leads to a drop in the precision and\nnormalized recalls, especially for lower k values. We perform a t-test to test the significance of the drop\nand find that for both precision and normalized recall, the drop is significant, with p-values of 4.7e - 4 and\n1.9e6 for precision and normalized recall curves, respectively. This indicates that proper attribution also\nallows the LLM to provide more accurate ranking of candidates."}, {"title": "4 Literature Review Generation", "content": "We now focus on generating the related work section of a scientific document from a user-supplied list of papers.\nIn a real-world scenario, this list might be obtained through traditional means, from the above-mentioned\nautomated methods, or some combination. We evaluate several dimensions of writing quality in the generated\ntext. Importantly, while modern LLMs can yield seemingly well-written text passages, \"hallucinations\"\nremain a problem and can be particularly egregious when LLMs are used in scientific writing (Athaluri et al.,\n2023). The hallucination of statements not entailed by the contents of cited papers and the hallucinations of\nimaginary papers that do not exist is a well-known issue of even the most powerful LLMs. We use ideas from\nretrieval augmented generation (RAG) techniques (Lewis et al., 2020) and instruction prompting to address\nthe key problem of hallucinations. Our work also aims to increase the number of papers from the desired set\nthat are indeed discussed (the coverage).\nWe present our general framework and problem setup in Figure 4. We use the abstract of a query paper\nthe one for which we generate a literature review, along with the abstracts of the set of papers to be cited\n(the retrieved abstracts of reference papers) to generate the related work section of the query paper. Our\napproach relies on prompting LLMs in different ways and measuring the similarity of the generated literature\nreview text to ground truth literature reviews found within a corpus of recent scientific papers i.e. ones not\nused in the training set of the underlying LLMs. We use both automated methods and human evaluations in\nour experiments below.\nWe propose to further decompose the writing task to increase passage quality, factual accuracy, and coverage.\nWe examine different strategies for generating a writing plan, a line-by-line description including citations of\nthe passage to write. These writing plans also give authors (users) control over the output passages. This is\nlikely essential in practice to meet author preferences and possible publication constraints. These plans are\ndefined and generated in such a way that the user can interact with and edit them if desired, or they can be\nused as an automatically generated intermediate (but human understandable) representation. We now we\ndescribe our proposed methods, elements of their use in practice, and their evaluation in more detail."}, {"title": "4.1 Plan Based Generation Approach & Model Variants", "content": "We now focus on generating the related work section of a scientific document from a user-supplied list of papers.\nIn a real-world scenario, this list might be obtained through traditional means, from the above-mentioned\nautomated methods, or some combination. We evaluate several dimensions of writing quality in the generated\ntext. Importantly, while modern LLMs can yield seemingly well-written text passages, \"hallucinations\"\nremain a problem and can be particularly egregious when LLMs are used in scientific writing (Athaluri et al.,\n2023). The hallucination of statements not entailed by the contents of cited papers and the hallucinations of\nimaginary papers that do not exist is a well-known issue of even the most powerful LLMs. We use ideas from\nretrieval augmented generation (RAG) techniques (Lewis et al., 2020) and instruction prompting to address\nthe key problem of hallucinations. Our work also aims to increase the number of papers from the desired set\nthat are indeed discussed (the coverage).\nWe present our general framework and problem setup in Figure 4. We use the abstract of a query paper\nthe one for which we generate a literature review, along with the abstracts of the set of papers to be cited\n(the retrieved abstracts of reference papers) to generate the related work section of the query paper. Our\napproach relies on prompting LLMs in different ways and measuring the similarity of the generated literature\nreview text to ground truth literature reviews found within a corpus of recent scientific papers i.e. ones not\nused in the training set of the underlying LLMs. We use both automated methods and human evaluations in\nour experiments below.\nWe propose to further decompose the writing task to increase passage quality, factual accuracy, and coverage.\nWe examine different strategies for generating a writing plan, a line-by-line description including citations of\nthe passage to write. These writing plans also give authors (users) control over the output passages. This is\nlikely essential in practice to meet author preferences and possible publication constraints. These plans are\ndefined and generated in such a way that the user can interact with and edit them if desired, or they can be\nused as an automatically generated intermediate (but human understandable) representation. We now we\ndescribe our proposed methods, elements of their use in practice, and their evaluation in more detail."}, {"title": "5 Generation Experiments", "content": "For the following studies on generating related work, we introduce an additional corpus. We extend the Multi-\nXScience corpus (Lu et al., 2020) to include the full text of research papers. We also reuse the RollingEval-Aug\nintroduced in Section 3. We use HuggingFace Transformers (Wolf et al., 2019) and PyTorch (Paszke et al.,\n2017) for our experiments' and calculate ROUGE scores (Lin, 2004) using the Huggingface evaluate library.\nDetails on the dataset and the implementation are in Appendix B and D, respectively. Similar to Lu et al.\n(2020), we extract the ground truth cited references as the relevant papers and evaluate only the generated\noutputs from different systems."}, {"title": "5.1 Generation Baselines", "content": "Extractive baselines As in Lu et al. (2020), we report the performance of LexRank (Erkan & Radev, 2004)\nand TextRank (Mihalcea & Tarau, 2004). We also create a simple one-line extractive baseline which extracts\nthe first line of the abstract and combines all the citations together to form the output.\nAbstractive finetuned baselines We use the model outputs of Hiersum (Liu & Lapata, 2019) and Pointer-\nGenerator (See et al., 2017) from Lu et al. (2020) for abstractive finetuned baselines. We also reproduce the\nfinetuned PRIMER (Xiao et al., 2021) model (considered to be SOTA).\nAbstractive zero-shot baselines We use the zero-shot single-document abstractive summarizers\nFlanT5 (Chung et al., 2022) and LongT5 (Guo et al., 2022) based on the T5 architecture (Raffel et al., 2020).\nSince Galactica (Taylor et al., 2022) is trained on documents from a similar domain, we include it along with\nFalcon-180B (Almazrouei et al., 2023).\nOpen and closed source models We use different chat versions (7B, 13B, 70B) of Llama 2-Chat (Touvron\net al., 2023) as zero-shot open-source LLM baselines. For closed-source models, we evaluate zero-shot both\nGPT-3.5-turbo (Brown et al., 2020) and GPT-4 (OpenAI, 2023). Since they perform best in our initial"}, {"title": "6 Results and Observations", "content": "From Table 3, we first note that unsupervised extractive models provide a strong baseline compared to\nabstractive 0-shot single document summarization baselines. Fine-tuning these abstractive models on Multi-\nXScience (released initially with the benchmark) improves performance at least to the level of extractive\nmodels. We reproduce the PRIMER model using their open-source code but find results lower than reported.\nAs such, we consider the Pointer-Generator method to be the current state-of-the-art (SOTA)."}, {"title": "7 Conclusions & Answering: Are We There Yet?", "content": "This work discusses, establishes and evaluates a pipeline to help people write literature reviews. To explore\nthis problem setting we have proposed and implemented a rolling evaluation procedure and collected several\nrecent datasets which we will make available to the community. We show that LLMs show significant potential\nfor writing literature reviews when the task is decomposed into these smaller and simpler sub-tasks that\nare within the reach of LLMs, namely through the use of LLM generated keword search and embedding\nbased search for relevant prior work. Notably, our experiments indicate that attribution based on citing\nextracted content from source material improves LLM re-ranking results. The most powerful LLMs evaluated\nin our studies exhibit both extremely promising paper re-ranking abilities as well as promising literature\nreview generation results. Importantly, LLM hallucinations can be substantially reduced using our proposed\nplan based prompting and retrieval augmented generation techniques. Our evaluation also reveals clear\nchallenges: 1) retrieving all relevant papers consistent with a given human generated literature review will\nrequires new querying strategies; 2) hallucinations can be significantly reduced using plan-based prompting,\nbut our approach does not completely eliminate hallucinations. Moreover, based on the proposed retrieval\nand generation pipeline, we build a full working demo (see Figure 15 of our Appendix for a screenshot), which"}, {"title": "A Ethics Statement", "content": "The potential for LLM and other NLP technology to help in scientific writing has led to the emergence of\nsystems such as Explainpaper which helps researchers understand the contents of the paper and Writefull12\nfor title and abstract generation. Scitet helps find appropriate resources and references while writing research\npapers. The usage of LLMs by researchers is so evident that some conferences (like ICLR) collect statistics\nfrom authors about their usage of these tools for retrieval (or discovery) or paraphrasing the related work\nsection and provide LLM-specific guidelines to authors.14 While writing assistant technology could have\ngreat promise as an aide to scientists, we think their use should be disclosed to the reader. As such assistants\nbecome more powerful they might be abused in certain contexts, for example where students are supposed to\ncreate a literature review as a part of their learning process. The use of such tools might also be problematic\nas authors of scientific work should read the articles that they cite and heavy reliance on such tools could\nlead to short term gains at the cost of a deeper understanding of a subject over the longer term. Any\ncommercially deployed or systems actually used by authors should also contain appropriate mechanisms to\ndetect if words have been copied exactly from the source material, and provide that content in a quoted style.\nAdditionally, the rolling evaluations which we present here does not involved training LLMs on arXiv papers.\nThis mitigates concerns regarding the copyright status of arXiv papers and their use for LLM training."}, {"title": "B New Datasets", "content": "While there are datasets available for different tasks in academic literature (see Table 7), we use the Multi-\nXScience dataset (Lu et al., 2020) for our experiments. Recent work (Chen et al., 2021b; Funkquist et al.,\n2022) also focuses on related work generation and provides a similar dataset. As part of this work, we release\ntwo corpora: 1. We extend the Multi-XScience corpus to include full-text of research papers and 2. We create\na new test corpus RollingEval-Aug consisting of recent (August 2023) arXiv papers (with full content)."}, {"title": "C Other Generation Experiments", "content": "Llama 2 fine-tuning In parallel, we also fine-tune Llama 2 models on the train set with the original\nshorter context, but they are very sensitive to hyperparameter configuration. When we instruct-finetune\nLlama 2 7B, it initially produces code. We find a slight improvement when fine-tuning the Llama 2 7B model\nfor 30k steps with an LR of 5e-6 over 0-shot model (see Table 8), but it quickly overfits as we increase the\nLR or the number of steps. We leave hyperparameter optimization, the fine-tuning of larger models with\nROPE scaling and plan-based generation for future work.\nLonger context While Llama 2 can ingest 4096 tokens, recent studies have found that it uses 19% more\ntokens (Kadous, 2023) than GPT-3.5 or GPT-4 (2048 and 4096 tokens respectively), implying that the\neffective number of words in Llama 2 is considerably lower than GPT-4 and only a bit higher than GPT-3.5.\nWe experiment with the popular RoPE scaling (Su et al., 2021) in 0-shot Llama models to increase the context\nlength (4k-6k). This permits using the full text of the papers instead of just their abstracts. Results in\nTable 9 show that directly using RoPE scaling on 0-shot models produces gibberish results. Instead, one needs\nto fine-tune the model with the longer context. In fact, a plan-based-longer-context CodeLlama (initialized\nfrom Llama 2 and trained with a 16k token context through RoPE scaling) improves on ROUGE1/L, but\nachieves comparable results as a shorter-context plan-based CodeLlama on ROUGE2. For reporting results"}, {"title": "D More implementation details", "content": "We use HuggingFace Transformers and PyTorch (Paszke et al., 2017) for our experiments.19 We calculate\nROUGE scores (Lin, 2004) using the Huggingface (Wolf et al., 2019) evaluate library20. To split sentences, we\nuse\u2018en_core_web_sm' model from SpaCy21. Additionally, we use Anyscale endpoints to generate 0-shot\nLlama 2 results and OpenAI API23 to generate results for GPT-3.5-turbo and GPT-4."}, {"title": "D.1 Generation Implementation", "content": "We use HuggingFace Transformers and PyTorch (Paszke et al., 2017) for our experiments. We calculate\nROUGE scores (Lin, 2004) using the Huggingface (Wolf et al., 2019) evaluate library To split sentences, we\nuse\u2018en_core_web_sm' model from SpaCy Additionally, we use Anyscale endpoints to generate 0-shot\nLlama 2 results and OpenAI API to generate results for GPT-3.5-turbo and GPT-4."}, {"title": "D.2 Demo implementation", "content": "We build our system using Gradio (Abid et al., 2019), which provides a nice interface to quickly and efficiently\nbuild system demos. Our user interface is also available at HuggingFace Space We query the Semantic Scholar API available to search for the relevant papers. Specifically, we use the\nAcademic Graph and Recommendations API endpoint. We use OpenAI API to generate results for LLM\nusing GPT-3.5-turbo and GPT-4 model. At the same time, our modular pipeline allows using any LLM\n(proprietary or open-sourced) for different components. We also allow the end-user to sort the retrieved\npapers by relevance (default S2 results), citation count, or year. More details about the demo system can be\nfound in our system paper."}, {"title": "D.3 SPECTER Implementation", "content": "We build an index of 150M SPECTER2 embeddings that we can use as an alternative to both a search engine\nand a prompting-based ranking module. Figure 6 shows our pipeline for creating the index. Specifically, the\nSPECTER2 database comes with 908 json.gz files containing compressed embeddings. For each json.gz file,\nwe construct a FAISS index that we can query for the nearest neighbors of a given query embedding. We\nperform index construction in a multi-threaded manner to speed up the process. Upon constructing a FAISS\nindex for all the json.gz files, we iterate over each query paper, search for top 100 relevant papers using the\nSPECTER embeddings in each FAISS index, and then finally merge the results to get the top 1000 papers\nfor each query paper."}]}