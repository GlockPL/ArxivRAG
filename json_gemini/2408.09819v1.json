{"title": "CMoralEval: A Moral Evaluation Benchmark for Chinese Large Language Models", "authors": ["Linhao Yu", "Yongqi Leng", "Yufei Huang", "Shang Wu", "Haixin Liu", "Xinmeng Ji", "Jiahui Zhao", "Jinwang Song", "Tingting Cui", "Xiaoqing Cheng", "Tao Liu", "Deyi Xiong"], "abstract": "What a large language model (LLM) would respond in ethically relevant context? In this paper, we curate a large benchmark CMoralEval for morality evaluation of Chinese LLMs. The data sources of CMoralEval are two-fold: 1) a Chinese TV program discussing Chinese moral norms with stories from the society and 2) a collection of Chinese moral anomies from various newspapers and academic papers on morality. With these sources, we aim to create a moral evaluation dataset characterized by diversity and authenticity. We develop a morality taxonomy and a set of fundamental moral principles that are not only rooted in traditional Chinese culture but also consistent with contemporary societal norms. To facilitate efficient construction and annotation of instances in CMoralEval, we establish a platform with AI-assisted instance generation to streamline the annotation process. These help us curate CMoralEval that encompasses both explicit moral scenarios (14,964 instances) and moral dilemma scenarios (15,424 instances), each with instances from different data sources. We conduct extensive experiments with CMoralEval to examine a variety of Chinese LLMs. Experiment results demonstrate that CMoralEval is a challenging benchmark for Chinese LLMs. The dataset is publicly available at https://github.com/tjunlp-lab/CMoralEval.", "sections": [{"title": "1 Introduction", "content": "Recent years have witnessed remarkable progress achieved by large language models in both natural language understanding and generation (Jobin et al., 2019). Despite such progress, a variety of risks have been found in the content yielded by LLMs, e.g., toxicity, unfaithfulness with hallucination (Guo et al., 2023). As LLMs become increasingly applicable to and integrated into real-world scenarios, the moral and ethical implications of their outputs should be regulated to ensure alignment with societal values and norms (Shen et al., 2023; Taddeo and Floridi, 2018).\nTo evaluate such alignment capabilities of LLMs, a wide variety of datasets have been proposed to examine dimensions like toxicity (Shaikh et al., 2023), bias (Parrish et al., 2022; Huang and Xiong, 2024) and fairness (Han et al., 2023). Among them, the assessment of morality can be traced back to the Moral Foundation Theory (MFT) (Graham et al., 2009). MFT categorizes moral precepts into five distinct domains, each comprising both positive and negative manifestations, e.g., Care/Harm or Fairness/Cheating. Over time, MFT has evolved into the foundational framework for subsequent specifications of datasets aimed at moral and ethical evaluation of LLMs (Guo et al., 2023).\nHowever, in striking contrast to the remarkable development of Chinese LLMs, moral benchmarks tailored to Chinese culture for evaluating the moral alignment capacity of Chinese LLMs remains underexplored. To bridge this gap, we propose CMoralEval, a multiple-choice QA dataset grounded in the moral norms of Chinese society. CMoralEval is meticulously curated through manual annotation on raw data collected froma Chinese legal and ethical TV program \u201cObservations on Morality\u201d\u00b9 and a set of \u201cChinese moral anomies\u201d\u00b2, followed by a rigorous quality review process. Specifically, from 833 episodes of the TV program over the past three years, we gener-"}, {"title": "2 Related Work", "content": "The proposition of MFT (Graham et al., 2009) has laid the foundation for numerous subsequent datasets related to morality and ethics. Some of these datasets are tailored to specific domains such as politics (Johnson and Goldwasser, 2018), social media (Hoover et al., 2020) and social sciences ( e.g., Social Chemistry 101 (Forbes et al., 2020)). Social Chemistry 101 (Forbes et al., 2020), based on MFT, annotates experiential norms from 12 dimensions, ultimately resulting in a dataset comprising 292K roles of thumbs (RoT). Construction of the fine-grained lexical resource MFD (Rezapour et al., 2019) involves meticulous refinement and expansion based on the foundation of MFT, carried out by thoughtful deliberation by a specialized team of experts. MFD is later extended to eMFD (Hopp et al., 2021) due to its limitations: MFD is formulated by a small group of experts, thus lacking the coverage of moral principles prevalent in the general population. Furthermore, it does not account for the variability of a single word that belongs to different categories defined by MFT in various contexts. ETHICS (Hendrycks et al., 2021), similar to Social Chemistry 101 (Hoover et al., 2020), establishes ethical benchmarks for specific scenarios based on several dimensions, including justice, deontology, virtue ethics, utilitarianism, and commonsense moral judgements.\nAdditionally, there are derivative moral benchmarks from previous datasets, such as Moral Stories (Emelin et al., 2021), which builds on RoTs from Social Chemistry 101 and serves as a crowdsourced collection of structured, branching narratives for the study of grounded, goal-oriented social reasoning. PROSOCIALDIALOG (Kim et al., 2022), derived from ETHICS (Hendrycks et al., 2021) and Social Chemistry 101 (Forbes et al., 2020), captures scenarios and employs artificial intelligence to generate responses encouraging prosocial behavior based on common-sense social rules (i.e., experiential rules, RoTs). Similarly, MIC (Ziems et al., 2022), another dialogue dataset, draws inspiration from RoTs but sources"}, {"title": "3 Dataset Curation", "content": "CMoralEval encompasses two distinct moral scenarios, each accompanied by questions derived from two data sources, and features a unique annotation process for each scenario. We have established a comprehensive annotation platform, enabling annotators to label various moral situations effectively. Simultaneously, stringent quality control measures are implemented to ensure the integrity and reliability of CMoralEval."}, {"title": "3.1 Data Sources", "content": "CMoralEval encompasses two types of scenarios\u00b3:\nExplicit moral scenarios In these scenarios, three options are provided, one being explicitly morally incorrect. For humans, selecting the correct answer is relatively straightforward, as it deviates noticeably from ethical standards.\nMoral dilemma scenarios These scenarios build on the explicit moral scenarios, creating new moral dilemmas. Among the three options presented, one is morally incorrect but also reasonable and tempting. For humans, a strong moral compass is required to correctly choose the answer, adding complexity to the decision-making process.\nAs indicated in Table 1, each type of scenarios encompasses two different data sources. One data source is derived from the Chinese legal and ethical TV program \"Observations on Morality\" over the past three years. The program features various daily content that covers virtually all the ethical situations prevalent in Chinese society. Program introductions serve as openly accessible resources, devoid of property rights concerns, thereby suitable for academic research.\nAnother data source is the collected Chinese anomies, which are primarily sourced through two channels. The first involves collecting academic papers from CNKI4, an information service platform in China focusing on academic resources. The second involves collecting relevant sections related to morality from mainstream newspaper media, such as \u201cXinhua Daily Telegraph\u201d, \u201cPeople's Daily\u201d and \u201cGuangming Daily\". We systematically review selected electronic editions of newspapers and academic papers over the past two years, culminating in the distill of 229 moral anomies in Chinese society."}, {"title": "3.2 Morality Taxonomy", "content": "In order to ensure the multidimensionality of CMoralEval, we have systematically taxonomized the moral dimensions within Chinese society. Drawing inspiration from the moral framework established in ancient Confucianism and national moral initiatives, we aim to construct an assessment dataset that is both representative and comprehensive."}, {"title": "3.3 Fundamental Moral Principles", "content": "Considering the complexity of morality and recognizing the diversity of narrators on the same matter among individuals (Huang et al., 2023), we have referenced various traditional Chinese cultures, including Confucianism, to define five fundamental moral principles in Chinese society: \u201cGoodness\u201d, \"Filial Piety\u201d, \u201cRitual\u201d, \u201cDiligence\u201d, and \u201cInnovation\".\nThese five fundamental moral principles, as core tenets within traditional Chinese cultural values, assert that any behavior contravening any one of them is deemed ethically inappropriate. Fundamental moral principles ensure that options generated are necessarily in violation of moral norms because actions contrary to RoT may not necessarily be ethically objectionable. For instance, consider a delivery person en route to deliver food who comes across someone drowning. If we set the RoT of delivery person as a narrator delivering food on time to ensure timely consumption by customers, jumping into the water to rescue someone would be contrary to this RoT. However, such an action does not violate our moral principles. Therefore, it is"}, {"title": "3.4 Templates Creation", "content": "As illustrated in Figure 1, we need to generate basic scenes based on the TV program introductions or collected moral anomies. During the annotation process, we use ChatGPT 3.56 to assist with the annotations. Initially, we generate three basic scenes based on each TV program introduction or collected moral anomies. Subsequently, we manually extract narrators and Roles of Thumb (RoT) from these basic scenes, with RoT involving behaviors and value judgments (Forbes et al., 2020). It should be noted that a basic scene may encompass different narrators, and conversely, a single narrator may encompass diverse RoTs, as illustrated in Appendix A.3.\nNext, we need to generate new scenes for the moral dilemma templates based on the basic scenes, along with its associated narrators and RoTs. We employ ChatGPT-3.5 to generate reasons that could contravene the RoTs. The basic scene and the contravening reasons are then concatenated and appropriately modified to ensure semantic coherence, resulting in a new scene. Since the new scene provides a reasonable justification for violating a certain RoT, it creates a moral dilemma.\nNext, we proceed to generate options based on the scenes, Narrator-RoT pairs, and fundamental moral principles.\nWe formulate three options for each scene (both the basic and new scene). The first two options come from the perpectives of given narrators, leading to the generation of options that align with and overtly contravene fundamental moral principles. To accurately discern whether the model's choice of morally aligned options is a result of genuine comprehension of the prompt and alignment with"}, {"title": "3.5 Data Instances Creation", "content": "We create data instances from the templates by adding variations. We find that in real life, people often adopt an evasive attitude when matters do not happen to themselves, especially in terms of morality. Therefore, the first variation modifies all templates to a third-person narrator. At the same time, we ensure that the options are from an observer's narrator. The second variation involves asking LLMs to choose \u201cthe most appropriate option\" and \"the most inappropriate option\u201d to test the consistency of LLMs in ethical judgment."}, {"title": "3.6 Quality Control", "content": "We have employed 15 annotators for the annotation task, complemented by three experts responsible for the review process. The selected annotators are senior-level students from higher-education institu-"}, {"title": "3.7 Dataset Statistics", "content": "Table 2 displays the fine-grained data statistics of annotated questions in the dataset, especially on the moral categories and average length of the dataset. It is evident that 13,164 out of 30,388 instances are Professional Ethics instances, accounting for 43.32%, followed by Personal Morality at 32.83%, and Social Morality at 24.02%. The average length of annotated questions in the dataset is 88.38 tokens, and each is presented as a multiple-choice question in Chinese with three options. Furthermore, we observe that within the category of Internet Ethics, 1,420 out of 2,896 instances, are associated with multiple moral categories. Additionally, other categories exhibit a similar trend, with over 30% of the data not exclusively belonging to a single category."}, {"title": "4 Experiments", "content": "We evaluated 26 open-source Chinese LLMs with the created benchmark CMoralEval, ranging in size from Chat Yuan-large-v2 (0.7B) (Xuanwei Zhang and Zhao, 2022) to Yi-34B-Chat (34B). All selected models have been fine-tuned, with some possibly undergoing value-aligned fine-tuning, such as Chinese-Alpaca-2-7B-RLHF (Cui et al., 2023) and Chinese-Alpaca-2-1.3B-RLHF (Cui et al., 2023), while others may have been fine-tuned for chat or other purposes.\nWe provide extra information for the chosen LLMs in Appendix A.5.1."}, {"title": "4.1 Evaluation Metrics and Settings", "content": "we chose the Im-evaluation-harness framework (Gao et al., 2023) as the evaluation method, for both zero-shot and 5-shot settings. For all selected models, we calculated the probability of the output sequence and chose the option with the highest probability as the model's preferred choice. Since LLMs chosen to evaluate are all SFT models, we adhered to the prompt templates used during the model's fine-tuning phase during evaluation. If a model does not provide one, we used LLAMA2's prompt template for our assessment.8\nComprehensive experiments, including both zero- and few-shot settings, were systematically conducted across all evaluated LLMs.\nFor the zero-shot setting, to evaluate the model's ability to select options that are ethically aligned, the used prompt is:\u201c\u6839\u636e\u7ed9\u5b9a\u7684\u573a\u666f,\u4f60\u8ba4 \u4e3a{persective}\u5e94\u8be5\u600e\u4e48\u505a,\u8bf7\u9009\u62e9\u6700\u6070\u5f53\u7684\u9009\u9879\u3002\u201d(\"Based on the following scene, what do you think {persective} should do? Please choose the most appropriate option.\") Conversely, when the objective is to prompt the model to select the ethically"}, {"title": "4.2 Results", "content": "Overall Performance Figure 2 displays five-shot results on the different subdivisions of CMoralEval (e.g., with different scenarios, different data sources, different narrators and different prompt settings). As zero-shot results are generally lower than few-shot results, we provide them in Appendix A.5.3. It can be observed that the Yi-34B-Chat model demonstrates the best overall performance. With other conditions held constant, LLMs provide better responses to questions of explicit moral scenarios. Moreover, when the model adopts an standby's narrator on scene, there is a noticeable decrease in accuracy, indicating that the model exhibits a certain degree of avoidance towards matters not directly concerning itself. Additionally, it has been found that LLMs' performance does not show a clear preference when faced with questions from different data sources, even though moral anomies are common societal issues closely related to residents' lives, it does not lead to improved model performance. When evaluating LLMs by having them choose morally appropriate options, they performs better; we will further analyze this phenomenon deeper.\nThe two RLHF models do not show strong performance across all scenarios, indicating that the alignment training does not result in consistently high moral alignment. In cases where the number of model parameters is small (e.g., Chinese-Alpaca-2-1.3B-RLHF (Cui et al., 2023)), the benefit of RLHF is not significant. This suggests that the RLHF process may not be as effective for small models as for large models.\nIt is noteworthy that despite exhibiting relatively higher performance in certain categories, the performance of the majority of LLMs still hovers around the vicinity of random guessing (0.33), indicating that, on the whole, the understanding of evaluated Chinese LLMs on the nuances in moral discernment remains significantly constrained.\nPerformance across Categories In our comparative analysis of LLMs across moral categories (shown in Figure 3), Yi-34B-Chat emerges as the most accurate model, particularly in Familial Morality and Personal Morality, suggesting nuanced capability in these moral contexts. Generally, LLMs with average performance show minimal differences across categories, while some smaller LLMs (e.g., robin-7b-v2-delta and robin-13b-v2-delta) tend to perform poorly in Familial Morality and Social Morality, suggesting difficulties in understanding collective moral contexts.\nAn important observation is the impact of model size on performance. Larger LLMs (e.g., Yi-34B-Chat) exhibit significant improvements, especially in Familial Morality and Social Morality, likely due to the more comprehensive training data capturing these collective moral concepts. However, we also observe that some LLMs with relatively small parameter sizes (e.g., Qwen-14B-Chat) exhibit promising performance. This could be attributed to the quality of the training data or the effectiveness of the training methodologies employed.\nSingle-Category vs Multi-Category Questions As previously mentioned, due to the complexity of morality, the five categories of morality are not strictly mutually exclusive. Consequently, we analyzed LLMs' average accuracy on single-category and multi-category questions. Results are shown in Figure 4.\nAcross both single- and multi-category questions, certain models, such as \u201cinternlm2-chat-7b\", \"internlm2-chat-20b\" and \"Yi-34B-Chat\u201d, demonstrate a stronger grasp of moral reasoning within the tested scope. It is found that LLMs demonstrates higher accuracy when responding to single-category questions than to multi-category questions.\nThe highest accuracies are seen in \u201cFamilial Morality-only\u201d, \u201cPersonal Morality-only\" and \"Professional Ethics-only\" categories, which might suggest that LLMs are more attuned to the moral nuances in these more personally relatable domains and small societal groups such as families and com-"}, {"title": "Consistency of LLMs", "content": "Figure 5 shows the controlled few-shot results on CMoralEval. When conducting experiments with controlled variables, the models demonstrate low accuracy rates across various scenarios, indicating a lack of consistency. This suggests that there may be inherent limitations in the models' capabilities to maintain uniform performance under varying conditions. LLMs generally perform worse when tasked with answering \"party_or_not\" questions. This suggests that models may have difficulty in processing questions that require understanding of reversed or negated concepts, which could be due to a lack of comprehension of the nuanced meaning within the question.\nYi-34B-Chat seems to be an outlier with comparatively better consistency. Some moral categories like Familial Morality and Social Morality have higher accuracies compared to others like Internet Ethics, suggesting that evaluated LLMs may be better at understanding and reasoning about certain moral domains over others."}, {"title": "5 Conclusion", "content": "In this paper, we have presented CMoralEval, a dataset comprising over 30,000 entries that span five moral categories, two types of scenarios and two data sources. The range of options for evaluating Chinese LLMs has been significantly expanded. This high-quality dataset, produced under stringent annotation standards, reveals that current Chinese LLMs exhibit considerable disparities and underperformance in moral reasoning, indicating substantial room for improvement."}, {"title": "Limitations", "content": "We have conducted extensive evaluations of various Chinese LLMs. Nevertheless, it would be advantageous to incorporate some English-dominated LLMs (e.g., ChatGPT, Mistral-7B) into the experiment. This would facilitate a comparative analysis between Chinese and English-dominated LLMs, offering insights into the disparities that may exist. Such addition would contribute to the richness of our study and add an intriguing dimension to our research endeavors."}, {"title": "Ethics Statement", "content": "Although the paper is a benchmark for evaluating the ethical and moral capabilities of Chinese LLMs, it is imperative to note that the research process adhere strictly to the ACL Ethics Policy. No violations of the ACL Ethics Policy occurred during the course of this study."}, {"title": "A Appendix", "content": "A.1 Conceptual Interpretation\nIn this section, we provide a detailed description of each moral category in Table 3 and the meaning of each fundamental moral principle in Table 4. Besides, in Table 5, we display the the examples of each scenarios. Furthermore, in Table 6, we present examples for each moral category, along with the fundamental moral principle they violate.\nA.2 Generating Different Scenes\nIn spite of the existence of two distinct data sources, many of the annotation procedures share similarities. We have employed different colors to represent distinctions within the same step. Red indicates annotations derived from TV program synopsis and blue signifies annotations derived from moral anomies. We provide a detailed account of each step in the annotation process, following the steps outlined for annotation. All prompts employed in this context are categorized into the following two segments.\n\u2022 Custom Instructions\nKnowledge Background: Designed to impart the necessary background knowledge to ChatGPT-3.5 for answering questions.\nOutput Requirements: Intended to standardize the output format of ChatGPT-3.5.\n\u2022 User Instructions: Additional prompts provided by the user for guidance.\nGenerating basic scenes In this step, we generate three different basic scenes based on the provided content. The full prompt is depicted below.\nKNOWLEDGE BACKGROUND\n# You are a data annotator currently engaged in a project.\n# The objective of this project is to construct a benchmark dataset for evaluating the morality of LLMs.\n# I require your assistance in completing this task. Your assignment is to generate realistic, concrete, and diverse ethical decision-making scenarios based on the provided {program synopsis} / {moral anomies}. The created scenarios should be imaginative, drawing appropriate associations from the given {program synopsis} / {moral anomies}, but with a creative content that is not closely tied to the {program synopsis}.\nPlease provide three such scenarios.\nOUTPUT REQUIREMENTS\n1. The generated scenarios should exclude real names.\n2. Initiate the scenarios in the third person, clearly stating the identity: \"You are a....\"\n3. Focus solely on scenario generation without supplementary explanations.\n4. Refrain from providing explicit guidance on character behavior.\n5. Present only objective facts, avoiding suggestions or recommendations for character actions.\nUSER INSTRUCTIONS\nprogram synopsis: {program synopsis}\nmoral anomies: {moral anomies}\nGenerating RoTs Upon obtaining the basic scenes, the first step involves the manual extraction of narrators, denoting the characters present in the ethical scenes. Once narrators are established, the subsequent task is to identify morally sound actions from the established narrator within the given scene, summarizing them concisely in a phrase. Following this, we generate the corresponding RoT based on these phrases and make the Narrator-RoT pair. We have observed that the accuracy of generation improves when examples are drawn from Social Chemistry 101 (Forbes et al., 2020). Therefore, we randomly selected five RoTs as reference examples.\nThe full prompt is depicted below.\nKNOWLEDGE BACKGROUND\n# You are a data annotator currently engaged in a project.\n# The objective of this project is to construct a benchmark dataset to evaluate the ethics and morality of LLMs.\n# I require your assistance in completing this task. Here are some examples: {Example RoT 1} :\n{Example RoT 5} \nOUTPUT REQUIREMENTS\n1. Provide responses in Chinese.\n2. Ensure that the responses do not exceed 20"}, {"title": "A.3 narrators & RoT", "content": "During the annotation process, we discovered that a single basic scene might encompass different narrators. Similarly, from one narrator, different Rules of Thumb (RoT) can be identified, and the options generated based on this narrator-RoT pair are entirely distinct, thereby enhancing the diversity of the dataset. This correspondence is detailed in Table 7."}, {"title": "A.4 Variations", "content": "A template can generate four final datasets by applying two variations (party/bystander and choose moral/not moral choice), thus enabling multidimensional evaluation of LLMs. A specific example is provided in Table 1.\nAll variations are made based on the original Chinese text. The first variation involves viewing the issue from the narrators of both the parties involved and the bystanders. Since all templates start with \u201c\u4f60\u662f...\u201d(\u201cYou are...\u201d), which means they originate from the narrator of the involved party, when we need to expand the template to a third-person narrative, it's necessary to remove the\u201c\u4f60\u662f...\u201d(\u201cYou are...", "\u4e00\u6761\u65b0\u95fb,\u5185\u5bb9\u662f": "(\u201cYou saw a news article in the newspaper, which read:\"). Furthermore, if the options do not use third-person pronouns but instead\u201c\u4f60\u201d(\u201cyou\u201d) or\u201c\u4f60\u7684\u201d(\"your\u201d), they should be replaced with\u201c\u4ed6/\u5979\u201d(\u201cHe/She\u201d) and\u201c\u4ed6\u7684/\u5979\u7684\u201d(\u201cHis/Her\u201d) respectively. The second variation modifies the prompt to let the model proceed from narrator, choosing the most appropriate or inappropriate option.\""}, {"title": "A.5 Experimental Details", "content": "A.5.1 Model Cards\nTable 9 shows the basic model info of the chosen models.\nA.5.2 Prompts\nWe provide the prompt examples of each variations in Table 8.\nA.5.3 Zero-shot Results\nWe provide zero-shot results on the different subdivisions of CMoralEval in Figure 6.\nWe provide zero-shot results for single- or multi-category questions in Figure 8.\nWe provide zero-shot results across categories of CMoralEval in Figure 8.\nWe provide zero-shot results on different categories of CMoralEval when applying variable controlling in Figure 9."}]}