{"title": "BRIDGING THE COMMUNICATION GAP: EVALUATING AI LABELING PRACTICES FOR TRUSTWORTHY AI DEVELOPMENT", "authors": ["Raphael Fischer", "Magdalena Wischnewski", "Alexander van der Staay", "Katharina Poitz", "Christian Janiesch", "Thomas Liebig"], "abstract": "As artificial intelligence (AI) becomes integral to economy and society, communication gaps between developers, users, and stakeholders hinder trust and informed decision-making. High-level AI labels, inspired by frameworks like EU energy labels, have been proposed to make the properties of AI models more transparent. Without requiring deep technical expertise, they can inform on the trade-off between predictive performance and resource efficiency. However, the practical benefits and limitations of AI labeling remain underexplored. This study evaluates AI labeling through qualitative interviews along four key research questions. Based on thematic analysis and inductive coding, we found a broad range of practitioners to be interested in AI labeling (RQ1). They see benefits for alleviating communication gaps and aiding non-expert decision-makers, however limitations, misunderstandings, and suggestions for improvement were also discussed (RQ2). Compared to other reporting formats, interviewees positively evaluated the reduced complexity of labels, increasing overall comprehensibility (RQ3). Trust was influenced most by usability and the credibility of the responsible labeling authority, with mixed preferences for self-certification versus third-party certification (RQ4). Our Insights highlight that AI labels pose a trade-off between simplicity and complexity, which could be resolved by developing customizable and interactive labeling frameworks to address diverse user needs. Transparent labeling of resource efficiency also nudged interviewee priorities towards paying more attention to sustainability aspects during AI development. This study validates AI labels as a valuable tool for enhancing trust and communication in AI, offering actionable guidelines for their refinement and standardization.", "sections": [{"title": "1 Introduction", "content": "As artificial intelligence (AI) advances, companies are increasingly integrating it into their daily operations. This involves different stakeholders, such as software developers, domain experts, and project leaders, who need to reach agreements despite their very different levels of expertise. To ensure the trustworthy [1] and sustainable [2, 3] use of AI, it is imperative to bridge the communication gaps between the diverse parties that develop, use, or are affected by AI solutions. Examples of these gaps include limited technical understanding (even on the developer side [4]) and unrealistic expectations [5], which can result in misuse and disuse of the technology [6]. Whether stakeholders aim to use AI services [7, 8] or develop custom machine learning (ML) models, comprehending AI behavior and its practical implications is crucial but nontrivial.\nTo make informed decisions, stakeholders thus require a comprehensible form of communication about practical AI properties and performance trade-offs-such as resource demands versus predictive quality [9]). Established forms of reporting such as papers and result databases mostly address experts and are biased towards focusing on predictive performance [10, 11]. To foster resource-awareness and be transparent towards audiences that are less proficient in AI,\nFischer et al. [12] proposed more comprehensible, high-level labels. In analogy to established systems such as the EU energy labels, these AI labels aim to inform about the intricate trade-offs occurring among different AI models without presupposing any more profound understanding of ML [13]. While the idea of AI labeling was positively acknowledged [14, 15, 16, 17] to possibly be an \u201cexcellent tool", "questions": "nRQ1: Who is interested in AI labeling and what are their problems with AI technology?\nRQ2: What are the practical benefits and limitations of AI model labeling?\nRQ3: How are AI labels perceived compared to other forms of reporting?\nRQ4: How do AI labels and the corresponding certifying authority affect the trustworthiness of AI systems?\nTo answer our research questions, we conducted semi-structured interviews with 16 participants from various application domains, covering diverse levels of AI expertise. Besides discussing their daily work with AI, we also confronted them with AI labels as displayed in Figure 1 to assess their advantages and limitations and drew comparisons with other forms of reporting \u2013 we highlight some first impressions in Table 1. Following thematic analysis, we developed an extensive code system around our research questions (over 1000 occurrences) and now contribute an in-depth discussion of the corresponding positions.\nOur findings demonstrate a strong interest in AI labeling and practical benefits for connecting ML experts with less- informed users despite occasional misunderstandings and concerns about technical complexity. Participants recognized clear practical benefits of AI labels, including their potential to enhance decision-making, facilitate communication, and promote knowledge transfer between ML experts and non-experts. Moreover, our study suggests that AI labels can act as \"nudges,\" fostering more informed and sustainable decision-making. Our interviewees highlighted the importance of usability, sustainability, and customizable labeling formats tailored to diverse audiences. Importantly, our analysis implies that labeling should not be understood as a 'one-fits-all' solution. Instead, future efforts should focus on developing interactive frameworks for generating AI reports that cater to specific user needs. For that, our research questions offer a foundation for refining and standardizing AI labeling procedures to achieve this goal. Our work validates the theoretical concept of AI labels for practical feasibility, showcasing its capability to bridge communication gaps and even benefit sustainable development. Moreover, we root our research in Open Science practices, making all supplementary results, such as the transcripts and coding system available at www.github.com/raphischer/labeling-evaluation. Based on our findings, we deem AI labeling a central communication format for bridging gaps in the field and making AI systems more trustworthy and sustainable."}, {"title": "2 Related Work", "content": "On the Current Challenges in AI Development Until recently, incorporating AI into business required skilled ML engineers and developers who analyzed the business use case and data at hand to train custom models. Small and medium-sized enterprises often struggled to keep pace in the race to make business and profit with AI, as it required substantial upfront investments in hardware and human expertise, often without guaranteed returns [23]. The last years, however, brought forth a paradigm shift that centers on the availability of AI-as-a-service (AIaaS) [7, 8]. It enables businesses to access AI capabilities via cloud services and easy-to-use interfaces, which differ in their levels of"}, {"title": "3 Methods", "content": "To evaluate the concept of AI labeling, we started out by formulating our central research questions (cf., Section 1) and obtaining ethical approval from the ethics committee of the University of Duisburg-Essen Computer Science faculty (ID: 2407SPWM1293). To actively foster reproducible research, we make all results, including the interview guide, transcripts, code system, and visualization scripts, publicly available at www.github.com/raphischer/labeling- evaluation.\nApproach and Recruitment For conducting our study, we opened a public call\u00b9 for participants to take part in semi-structured interviews. We particularly invited developers of AI systems but also indicated openness to anyone generally interested in the concept of AI labeling, which was abstractly teased with an exemplary figure. The campaign was spread via mailing lists and social media posts on LinkedIn, X, WhatsApp, and Instagram, resulting in a certain level of snowball sampling. While our social networks are naturally biased in consisting of fellow researchers and business partners, we were successful in recruiting a total of 16 practitioners from different fields, backgrounds, and levels of AI experience\u00b2 an overview is given in Table 2. Their level of AI skill was determined via self-assessment, based on some orientation help in our application form: The beginner (1 person) has a general idea of but no practical experience with AI, users (4) have practical experience with AlaaS, engineers (8) have performed basic ML on custom data, and experts (3) have extensively trained, deployed, and used ML models\u00b3. Two of our interviewees hold a doctorate as the highest professional qualification, eight have completed a full master's (or diploma) degree, three have graduated as bachelors (or are certified specialists), and the rest have successfully graduated from high school4.\nEvery registered application resulted in an interview, for which the participants were compensated with 15\u20ac. Written consent was obtained before the meeting, explaining how participants' identity will be protected by anonymization. The interviews were conducted via Zoom and only the audio data was saved and analyzed. All participants received the interview materials after the completed interview and are frequently updated on our study progress.\nInterview Structure Guided by our four research questions, we developed an internal interview guide that consisted of four parts. At the beginning of each interview (part one), we asked participants to introduce themselves, explain how their work relates to AI, and describe difficulties they face in their daily business. In a second step, the interviewees were initially presented with a prototype AI label, which was generated with the STREP software [10]. It features properties of MobileNetV3Small [63], a popular image classification model that is usually used in pretrained form, either locally or as-a-service. After discussing first impressions and explaining some concepts (if required), we subsequently showed participants a second label featuring an EfficientNet model [64]. Given both labels, as displayed in Figure 1, participants were asked to compare the information on both labels, explain which aspects they found helpful and confusing, and comment on what they would change about the label. In the third part, we investigated how interviewees reacted to different types of reporting (see Section 2). For that, we presented them different reports about MobileNetV3, namely the associated research paper [63], the model card on Hugging Face, a blog article 6, the documentation of Keras 7, the Papers With Code results [32], and an exemplary fact sheet of IBM\u00ba (which unfortunately are only available for IBM products). Interviewees were then asked to compare the approaches and describe how they would use them in their daily work an overview can be found in the Appendix (Figure 7). Lastly, we opened a conversation around trustworthiness in the context of AI labeling. The more specific questions in that part discussed possible providers or authorities for labeling models (i.e., who to trust with such a process) as well as the interviewees' position towards certification and regulation. For each interview, at least two interviewers from the author pool were present10.\nTranscription and Coding In a first step, the audio recordings were transcribed via the whisper-large-v3 speech recognition model [65], which we deployed locally with the help of the Shoutout tool\u00b9\u00b9. Afterward, each interview was manually revised to correct major errors in the transcription. For our analysis, we followed an inductive coding approach using MAXQDA 24.5. We discussed individually coded interviews in iterative cycles and mutually refined the coding system. It was organized in a hierarchical structure, with several overarching code families that individually address aspects of our central questions. The final system encompasses 136 codes assigned to a total of 1130 text"}, {"title": "4 Results", "content": "4.1 Who Is Interested in AI Labeling and What Are Their Problems With AI Technology? (RQ1)\nTo assess possible user groups of AI labels and their needs, we asked participants to describe their daily work in relation to the use of AI methods. We structured all answers, identifying (1) general problems and positions, (2) participants' types of daily work, (3) ML methods used, (4) AI applications and use cases, (5) specific tools and brands in use, as well as (6) requirements of AI - an overview is given in Figure 2.\nFirstly, the interviewees talked about several general problems related to the use of AI methods. Il described, for example, that it is difficult to \"get employees on board so that they can actually use the new tools\" (p. 26), and I11 mentioned issues with \u201ccustomer communication and expectation management\" (p. 50) \u2013 exemplifying communication gaps (seven mentions) as a general business problem (23 in total). While there seemed a general agreement that AI contributes to business growth (12 mentions), we also encountered various cases of insecurity about the use of AI and inconsistencies in answers, such as, for example, \u201cI have quite a few concerns, but on the other hand, I find AI very convenient\" (I15, p. 26). Moreover, interviewees also distinguished \"between AI tools that are used during work or AI tools that are incorporated into products\" (112, p. 28) \u2013 exemplifying the use scale of a possible labeling approach by also stressing the breadth that would need to be covered.\nOn similar lines, we encountered a broad spectrum of daily work (see Figure 2 upper middle) \u2013 software development, infrastructure and operationalization, consulting, as well as data exploration and analysis being most frequently mentioned. Relating to the use of AI within these different streams of work, we noticed that AlaaS was more frequently encountered than traditionally training models on custom data (upper right), possibly indicating a shift from a predominantly developer- to a customer-based perspective. We find further support for this speculation statements from our interviewees, such as I7 who stated that \u201cwhen people talk about AI today, they no longer mean deep learning, they mean solely and exclusively large language models\" (p. 4). The many mentions of in-house service applications is likely linked to the this phenomenon (lower left), however manufacturing and industry seem to offer even more use cases which were explicitly mentioned in four of our interviews. I7 also mentioned that \u201cmany companies are not yet ready to implement their own deep learning projects\u201d (p. 8), which explains why tools and brands (lower middle) like scikit-learn (for traditional ML, outside of deep learning) and OpenAI (for AIaaS) are most frequently mentioned.\n4.2 What Are the Practical Benefits and Limitations of AI Model Labeling? (RQ2)\nAs a possible solution to some of the discussed challenges, we next presented our interviewees with prototypical AI labels. A general overview of the interviewee's sentiment towards labels is displayed in Figure 3, indicating the number of comments on benefits, improvements, and limitations per interviewee. Note that the codes on improvements and limitations should not be considered as general positions against labeling \u2013 quite the opposite, they can be seen as evidence that the idea is interesting, yet further work and refinement is needed.\nA Trade-off Question \u2013 Simplicity Over Complexity We observed the general tendency that, on the one hand, interviewees deemed the simplicity of the labels helpful and necessary for decision-making and knowledge transfer, while, on the other hand, they also missed more detailed information. For example, interviewee I1 stated that labels would \"help in any case as there are more and more models, it is increasingly difficult to keep an overview, and the more compact the information is, the better\u201d (I1, p. 100). In this context, I14 perceived our labels as \u201cinformative at a glance", "comparing different models with each other and seeing how well they perform\" (I2, p. 70). Interviewees also mentioned advantages in the context of communication and knowledge transfer (20 mentions), with 17 seeing the \"greatest added value for customer presentations\" (p. 174) and 111 stating that customers \"really like\n4.3 How Are AI Labels Perceived in Comparison With Other Forms of Reporting? (RQ3)\nWe give an overview of actively used report forms in Figure 5 and generally found these findings to support our results from RQ2: many interviewees use high-level media and journalistic text the most, however the depth of academic papers is also important \u2013 this clearly reflects the previously mentioned simplicity versus complexity trade-off. Supporting low-key access to information, one interviewee mentioned that blogs like Medium are very popular \"because it's often a practical example that is well explained and easy to work with": "I8, p. 116). I13 and I14 highlighted that educational videos help a lot with getting started with ML and I12 explained that blogs and articles help with learning about \u201ctrends\u201d and seeing", "do": "p. 186). Conversely, participants also mentioned issues of trust and reliability for this report type:", "anything": "I6, p. 182).\nWhen comparing other reporting types with AI labels, we found that the access to fast information was especially pronounced. For example, I9 stated that reading the other formats \u201cis time-consuming. [...] That is the disadvantage of all other approaches", "significantly more text, significantly more data": "I3, p. 168), which must be consumed and understood: \u201cthere is quite a lot of complexity involved and I would first have to have a pretty good understanding of it\u201d (I8, p. 104). In contrast, the interviewees appreciated that once you are familiar with the label, \u201cyou don't even have to read [the sources] anymore you just know how good [the models] are.", "none of these [forms of reporting] are as easy to understand as the label": "p. 168).\nContrasting this need for information-at-a-glance, many interviewees also required access to more in-depth information. 13 stated, for example, that you \u201chave to look at [the paper], in any case", "trust in AI in general, and trust in a label in terms of a model's performance": "p. 152). Figure 6 gives an overview over the different reasons or origins for trust, however interviewees usually did not specify whether they relate to labels or general Al systems. Beyond the direct context of AI labels, interviewees also mentioned general AI skepticism (20 remarks) and regulation skepticism (8). As an example, Il reported very different views on AI in their company, \u201cfrom euphorically enthusiastic to rather skeptically rejecting", "it would help, yes. Because it's approved by professionals and trust is created": "p. 52). However, I11, for example, doubts that", "trust": "p. 164), and I7 questioned whether he can truly", "thing": "p. 82), and said he would rather \u201ctest [models] himself", "central": "I8, p. 124), \u201cofficial", "independent\" (115, p. 122) authority, however, struggled to give a clear answer as to who could take this position. Opposing this, nearly half of our interviewees raised concerns of subjectivity, as authorities could be \"bribed\" (I4, p- 160) or possibly trick the labeling system for a more positive outcome, as it has happened with organic labels (I8, p. 124). Upon the question who should then certify AI, I9 responded fittingly: \"quite democratically, the users\" of labeling systems (p. 227). This approach was greeted by mixed feelings, which can be seen from the remarks on self-certification versus third-party involvement \u2013 I3 believes that \u201cthere must be something centralized, such that not everyone is allowed to make up their own label\" (p. 184), yet others stated that having access to the certification framework \"creates transparency and you can check [...] if it works as I imagine it will\" (I5, p. 252). Placing performance at the core, I4 believed that \u201cmost people probably don't care [about properly understanding the system]. The main thing is that the end result is correct": "p. 180) - mirroring previous empirical results [67]. I8 puts it similarly:", "it": "p. 116).\nExploring whether AI labels can be a means to create trust in AI necessitates to distinguish between different target audiences. We found that our participants anticipated different trust requirements, depending on the trustees' levels of AI proficiency. I11 saw the perspective of end-users' to be especially important, because \u201cas an user of an AI, then of course I have the least trust", "twofold": "On the one hand, concerns were raised that AI end-users might be overwhelmed or disinterested by the technicality of the metrics in the display (115, p. 106, 11, p. 96). On the other hand, it was positively remarked that metrics like power consumption made the AI model performance more understandable and tangible for users (I6, p. 210). Developers were remarked to inherently trust the systems they build:"}, {"title": "5 Discussion, Limitations, and Future Work", "content": "The findings from this study reveal several key themes in the discussion of AI labeling practices, which relate back to our research questions. In the following, we discuss these central points: the inherent trade-offs involved in designing labels (RQ2 & RQ3), the potential of labels as nudges (RQ2), the ongoing challenge of trust in labels and their certifying authorities (RQ4), and lastly, the diverse needs and expectations of practitioners (RQ1 & RQ3). Each aspect presents both challenges and opportunities for improving AI transparency, communication, and trust.\nA critical theme that emerged from our study revolves around the trade-off between simplicity and depth, as discussed in Section 4.2. Generally, participants agreed on the need for simplicity, especially to facilitate quick decision-making and communication. However, interviewees also expressed concerns of oversimplification and acknowledged the limitations of high-level labeling, especially when it comes to capturing the nuances of model performance and application suitability. This reflects a broader tension in the field of AI communication: on the one hand, labels are meant to distill complex, often highly technical information into digestible, easily accessible formats; on the other hand, this simplification risks omitting essential details that could impact users' understanding and trust in the model's capabilities. This tension highlights a central challenge for how to design AI labeling systems: labels must strike a balance between providing an overview that is both accessible and meaningful without sacrificing important detail. The desire for interactivity provides a potential solution to this dilemma. The ability to adjust the importance of specific criteria based on user preferences could allow for a more dynamic, user-driven label experience. This would enable users to engage with the label in a way that reflects their specific needs and for example prioritize predictive accuracy, resource efficiency, or interpretability. In addition, it is important to acknowledge the connections between labeling and other forms of reporting. By linking multiple representations, interested users can dive deeper into the intricacies which might reinforce their trust in labels. By shaping reports and labels towards users' priorities, it may be possible to navigate the trade-off between simplicity and depth more effectively.\nWhile not the focus of this study, it also became clear that another important role of AI labels is their potential as nudges, influencing user decisions by emphasizing certain aspects of model performance. As our results in Section 4.2 indicate, labels can function as a tool for guiding decision-making by drawing attention to key trade-offs between model attributes such as accuracy and energy consumption. In this sense, labels do more than simply present information \u2013 they actively shape the decision-making process by highlighting the factors deemed most important.\nTrustworthiness was a recurrent topic in the interviews, with three central concerns as discussed in Section 4.4: (1) trust in the label, (2) trust in the entity responsible for labels, and (3) label suitability for increasing trust. Regarding the label's trustworthiness, participants highlighted the importance of clear, reliable metrics, but also expressed skepticism about the adequacy of labels to fully represent the complexity of AI models. For experts, labels serve as a starting point for decision-making, but they are not a substitute for hands-on testing or exploring technical details. The question of authoritative responsibility for labeling was contentiously discussed. Participants suggested that a neutral, centralized authority (e.g., independent regulatory bodies or even academia) would lend legitimacy to AI labels, however concerns were raised regarding subjectivity and potential for bias. Others advocated a more democratized approach, suggesting that developers themselves could play a key role in evaluating and certifying AI models. This underscores the difficulty of establishing trust in labeling systems, particularly as stakeholders may have competing interests in how AI systems are presented and evaluated. Following the growing trend of open source AI development and corresponding user-centric, community driven transparency could help in making labels trustworthy, however makes consistency and reliability all the more important. For truly establishing them as a means to increase trust in AI, labels need to be seen as part of a larger trust-building process that involves transparency, verifiability, and user experience.\nLastly, the striking diversity in participants' backgrounds, roles, and expertise as discussed in Section 4.1 underscores the necessity for AI labels to be adaptable to different user groups and contexts. Some develop their own AI models, however many others only interact with available AI services. While labels are often seen as a promising tool for simplifying the communication of AI-related information, the broad spectrum of users, from technical experts to non-technical stakeholders, indicates that a \"one-size-fits-all\" approach would likely fall short. Our results suggest that with any unified approach, labels must allow for customization, ensuring that different audiences can extract the information they need. This need for adaptability aligns with previous research suggesting that AI reporting must consider varying audience expertise levels and roles [10]. In practice, this means that AI labels should incorporate flexibility, allowing users to choose the level of detail they wish to see. In short, to become a useful tool, our study evidences that AI labels should balance accessibility and detail, shape decision-making by emphasizing key factors, support trust through transparency and verifiability, and enable customization for diverse audience needs.\nWhile our study provides valuable insights, we also acknowledge potential limitations such as a sampling bias from recruiting participants via social media, which likely attracts those already interested in AI labeling while excluding skeptics. Additionally, creating and presenting the labels ourselves may have introduced response bias due to social"}, {"title": "6 Conclusion", "content": "Our study highlights the multifaceted role of AI labeling in fostering trust and informed decision-making across different user groups. We found evidence that AI labels are valuable due to their accessibility and potential to transfer knowledge, however must overcome challenges related to diverse audiences, technical comprehension, and metric transparency. To maximize their impact, AI labeling systems should incorporate interactive features that allow for customization based on stakeholder priorities and knowledge. Moreover, independent certification processes are essential to bolster trustworthiness. By integrating these improvements, AI labels can serve as a cornerstone in the development of fair, accountable, and transparent AI systems, ultimately aligning technical advancements with societal expectations."}]}