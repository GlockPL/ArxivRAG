{"title": "EH-MAM: Easy-to-Hard Masked Acoustic Modeling for Self-Supervised Speech Representation Learning", "authors": ["Ashish Seth", "Ramaneswaran Selvakumar", "S Sakshi", "Sonal Kumar", "Sreyan Ghosh", "Dinesh Manocha"], "abstract": "In this paper, we present EH-MAM (Easy-to-Hard adaptive Masked Acoustic Modeling), a novel self-supervised learning approach for speech representation learning. In contrast to the prior methods that use random masking schemes for Masked Acoustic Modeling (MAM), we introduce a novel selective and adaptive masking strategy. Specifically, during SSL training, we progressively introduce harder regions to the model for reconstruction. Our approach automatically selects hard regions and is built on the observation that the reconstruction loss of individual frames in MAM can provide natural signals to judge the difficulty of solving the MAM pre-text task for that frame. To identify these hard regions, we employ a teacher model that first predicts the frame-wise losses and then decides which frames to mask. By learning to create challenging problems, such as identifying harder frames and solving them simultaneously, the model is able to learn more effective representations and thereby acquire a more comprehensive understanding of the speech. Quantitatively, EH-MAM outperforms several state-of-the-art baselines across various low-resource speech recognition and SUPERB benchmarks by 5%-10%. Additionally, we conduct a thorough analysis to show that the regions masked by EH-MAM effectively capture useful context across speech frames.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) has emerged as one of the most effective paradigms of speech representation learning when labeled data is scarce. The task is to learn general-purpose speech representations from unlabeled data that can then be transferred to Spoken Language Processing (SLP) tasks like Automatic Speech Recognition (ASR), Speech Emotion Recognition (SER), etc. Progress in SSL for speech has led to significant performance improvements in a range of low-resource SLP tasks including Phoneme Recognition (PR), Keyword Spotting (KS), etc. Masked Acoustic Modeling (MAM) has been one the most prevalent pretext tasks for SSL-based speech representation learning wherein the model tries to reconstruct frames that are masked at the input, utilizing the context of the surrounding frames. Although a considerable amount of research in MAM has been performed, most has focused on improving model architectures and pre-text tasks, with very limited progress in improving the masking algorithm. Most MAM algorithms still perform random masking of input frames. On the other hand, selective masking strategies for other domains, like computer vision (CV) that focuses on masking useful context, have shown significant improvements over random masking. This can be attributed to multiple factors, including: (1) Variable Information Content: Variable information content in data translates to variable learning signals for the reconstruction task. For instance, in Masked Language Modeling (MLM), the reconstruction of high-frequency stop words such as \"the\" or \"is\" offers minimal discriminative power due to the ubiquity and low semantic load of these words. In speech, for example, this can be translated to reconstructing frames corresponding to random noise or partial phonemes, where much of the frames is already available as context. (2) Progressive Learning: Random masking fails to imitate the progressive human learning process. Humans do not receive knowledge uniformly; instead, they are exposed to progressively more complex information as they advance in the learning process. Mimicking this progression in the masking algorithm by initially exposing the model to simpler, more predictable speech patterns and gradually introducing more complex, less predictable ones can significantly enhance the learning trajectory. This approach aligns better with how humans learn, moving from simpler to more complex information, and helps the model develop a deeper understanding of language over time.\nMain Contributions. To overcome the aforementioned problems, in this paper, we propose EH-MAM (Easy-To-Hard adaptive Masked Acoustic Modelling), a novel selective and adaptive masking scheme for MAM. We build EH-MAM on the core hypothesis that hard regions, characterized by collections of speech frames that are more difficult to reconstruct, serve as stronger signals for the learning process. Fig. 2 shows the results of a simple experiment we performed to validate our hypothesis. By selectively masking hard regions, we notice a greater and consistent drop in WER performance for ASR. This suggests that masking hard regions captures useful context in the speech input. Our main contributions are as follows:\n\u2022 We propose EH-MAM, a novel self-supervised speech representation learning algorithm. In contrast to solving a predefined MAM pre-text task, such as reconstructing randomly masked frames, EH-MAM aims to generate and align itself towards a more formidable MAM pre-text task. For generating a challenging MAM pre-text task, we first identify a collection of difficult frames to reconstruct, also called hard regions, followed by selectively masking them. We propose a lightweight loss predictor that predicts the frame-level reconstruction loss values and determines hard regions based on the output. To train the loss predictor jointly with MAM, we design a novel auxiliary loss that forces the predictor to learn the relative correlations between speech frames. Finally, to align the model towards reconstructing hard regions, we propose an easy-to-hard masking strategy that guides the EH-MAM learning.\n\u2022 We show the effectiveness of the speech representation learned by EH-MAM through extensive evaluations on low-resource speech recognition benchmarks and downstream evaluation on SUPERB. EH-MAM beats prior arts with a relative improvement of 5%-10%\n\u2022 We perform a comprehensive analysis to demonstrate that regions masked by the EH-MAM effectively capture useful context across speech input."}, {"title": "2 Related Work", "content": "Self-Supervised Learning. SSL has emerged as a prevalent speech representation learning paradigm, demonstrating impressive downstream performance under low-resource settings. At its core, SSL relies on the quality of pretext tasks for capturing varied learning signals from unlabeled data sources. Based on the nature of the pretext tasks, the SSL frameworks are further categorized into the following sub-categories: 1) Contrastive Approaches: The pretext task is designed to maximize latent space similarity between the anchor and positive samples while minimizing the similarity between the anchor and negative samples. 2) Generative Approaches: These methods primarily focus on first building a target by randomly masking multiple speech frames and then reconstructing them by optimizing a similarity measure (MSE or Cross Entropy) between the predicted frames and the targets. The pretext task includes predicting future input from past inputs, masked from unmasked or the original from some other corrupted view. Masked Acoustic Modeling has undoubtedly seen the most success for speech representation learning.\nMasked Acoustic Modeling (MAM) Conventional MAM architectures first perform frame-level masking, where randomly selected speech frames are masked using various existing masking strategies, including block or random masking. Next, they either employ a single encoder network like BERT to predict masked regions in a speech input or utilize self-distillation methods, where the student learns to reconstruct masked information under the guidance of an identical teacher network. Although a considerable amount of research in MAM has undergone towards improving model architecture and introducing novel pretext tasks, developing better masking strategies is still under-explored."}, {"title": "3 Methodology", "content": "In this Section, we explain the EH-MAM methodology. We first provide an overview of the EH-MAM learning paradigm (in Section 3.1), followed by details on the reconstruction and auxiliary loss formulations (in Sections 3.2.1, 3.2.2). Finally, we introduce the easy-to-hard masking algorithm (in Section 3.2.3).\n3.1 Overview of EH-MAM\nWe illustrate EH-MAM in Fig. 3. At its core, EH-MAM incorporates the self-distillation based SSL training paradigm for solving MAM pretext task, similar to. Specifically, EH-MAM consists of two identical networks, a teacher \\({f_{\\theta^t}, d_{\\delta^t} }\\) and a student \\({f_{\\theta^s}, d_{\\delta^s} }\\). A separate decoder \\(d_f\\) is employed for reconstructing masked frames from the student representations. The context encoders \\(f_\\theta\\) are built using K-layered transformers, whereas the decoder \\(d_f\\) and the loss predictor \\(d_\\delta\\) are constructed with light-weight D-layered 1D-convolution layers. During pre-training, the teacher parameters \\(\\theta^t, \\delta^t\\) are updated by performing exponential moving average (EMA) of the student parameters \\(\\theta^s, \\delta^s\\). Formally, we define the update as follows:\n\\[\\omega^t = \\lambda \\omega^t + (1 - \\lambda)\\omega^s\\]\nwhere \\(\\omega^t = {\\theta^t, \\delta^t }\\), \\(\\omega^s = {\\theta^s, \\delta^s }\\), and \\(\\lambda\\) is the decay rate. The student and decoder parameters are updated using gradient descent.\nAt each training iteration, we first extract low-frequency feature representations \\(Z \\in \\mathbb{R}^{N\\times d}\\) from raw speech signals \\(x \\in \\mathbb{X}\\) and feed it to the teacher network to get frame-level predicted reconstruction loss values \\(L_{rec}^t = d_{\\delta^t}(f_{\\theta^t}(Z))\\). With the help of \\(L_{rec}^t\\), we generate binary mask indexes \\(M_A = {0,1}\\) using the easy-to-hard masking strategy (introduced in Section 3.2.3), followed by creating a masked version of the original speech input \\(\\tilde{Z} \\leftarrow Z \\cdot M_A\\). Finally, the student is trained with gradient descent to minimize a weighted combination of reconstruction loss \\(L^{rec}\\) (introduced in Section 3.2.1) and an auxiliary loss \\(L^{aux}\\) (introduced in Section 3.2.2). Formally, we define the objective function below:\n\\[L^{joint} = L^{rec} + \\alpha L^{aux}\\]\nwhere \\(\\alpha\\) is a balancing parameter and is set to 0.05 throughout the experiments (further ablation on this can be found in Appendix C.2)."}, {"title": "3.2 Selective Masking with EH-MAM", "content": "Motivation: EH-MAM distinguishes itself from the conventional self-distillation-based SSL training methods that are fixated on solving a predefined MAM task generated using random masking, by enforcing the teacher to generate more challenging pretext tasks. To achieve this, EH-MAM first uses the teacher to identify hard regions, a collection of speech frames that are difficult to reconstruct, and then selectively mask these hard regions to create challenging MAM pretext tasks for the student to solve. Being constantly challenged by the teacher further directs the student to develop a much more nuanced understanding of speech. Additionally, we take inspiration from the recent studies in NLP and CV that have highlighted the significance of generating formidable pretext tasks for MLM (Masked Language Modeling) and MIM (Masked Image Modeling) using selective masking.\nTo reweigh the model attention towards reconstructing such hard regions, we introduce the loss predictors \\(d_{\\delta^s}, d_{\\delta^t}\\) for the student and teacher networks, respectively. Further to train the loss predictor, we also propose an auxiliary objective function \\(L^{aux}\\), that the model optimizes alongside the reconstruction loss \\(L^{rec}\\)."}, {"title": "3.2.1 Reconstruction Loss", "content": "As shown in Fig. 3, we first reconstruct the masked frames by feeding student representations \\(f_{\\theta^s}(\\tilde{Z})\\) to a decoder \\(d_f\\). Similar to , the goal of \\(d_f\\) is to reconstruct the teacher representation for time steps that are masked in the student input. To achieve this, we compute a reconstruction loss \\(L^{rec}\\) between the student and the teacher representations. Formally, we define reconstruction loss \\(L^{rec}\\) as follows:\n\\[L^{rec} = ||M_A \\cdot f_{\\theta^t}(Z) - d_f(f_{\\theta^s}(\\tilde{Z})) ||\\]\nwhere \\(M_A \\cdot f_{\\theta^t}(Z)\\) represents teacher representations associated with the masked speech input."}, {"title": "3.2.2 Loss Predictor and Auxiliary Loss", "content": "Motivation: Given the sequence of frame-level reconstruction loss values \\(L^{rec} \\in \\mathbb{R}^N\\), our goal is to create a challenging MAM pretext task for the student by selectively masking frames with high reconstruction values. As original reconstruction loss values \\(L^{rec}\\) are computed only for the masked regions (see Section 3.2.1), it provides limited information for deciding which frames to mask. To mitigate this problem, we introduce lightweight loss predictors \\(d_{\\delta^s}, d_{\\delta^t}\\), which can be easily integrated with the student-teacher network, and add reconstruction loss predicting capabilities across both networks. To train these loss predictors, we propose a novel auxiliary loss \\(L^{aux}\\) that guides it towards capturing relative correlations between individual frames rather than forcing the predictor to generate exact frame-level reconstruction values.\nSpecifically, for each masked frame \\((i, j)\\) where \\(i \\neq j\\) and \\((i, j) \\in \\{1,2, ..., N\\}\\), if \\(L_{i}^{rec} > L_{j}^{rec}\\) than the predicted counterpart \\(L_{i} = d_{\\delta^s}(f_{\\theta^s}(Z))\\) must also have \\(L_{i} > L_{j}\\). To formulate this constraint as a differentiable objective function, we first define a target distribution as an indicator variable \\(I\\) that captures the relative correlations between original reconstruction loss values, such as \\(L_{i}^{rec} > L_{j}^{rec}\\). Formally we define this as follows:\n\\[I_{i,j} = \\begin{cases} 1, & L_{i}^{rec} > L_{j}^{rec} \\text{ and } \\{i, j\\} \\in M_A \\\\ 0, & \\text{otherwise} \\end{cases}\\]\nNext, similar to \\(I\\), we introduce a predicted distribution \\(S\\) for representing the relative differences in the predicted reconstruction values \\(L\\). \\(S\\) is formally defined with a sigmoid function as:\n\\[S_{i,j} = \\frac{e^{(L_{i}-L_{j})}}{1+e^{(L_{i}-L_{j})}}\\]\nwhere \\(S_{i,j} > 0.5\\) if \\(L_{i} > L_{j}\\). Finally, we formulate our auxiliary objective function \\(L^{aux}\\) by first computing a vanilla cross entropy \\(H(\\cdot)\\) between the target distribution \\(I\\) and the predicted distribution \\(S\\): \\(L^{aux} \\leftarrow H(I, S)\\) and then minimizing it jointly with the reconstruction loss. We define the formulation of \\(L^{aux}\\) below:\n\\[L^{aux} = \\sum_{i=1}^{N} \\sum_{j=1}^{N} I_{i,j} \\log S_{i,j} + \\tilde{I_{i,j}} \\log (1 - S_{i,j})\\]\nwhere \\(\\tilde{I_{i,j}} = 1 - I_{i,j}\\). \\(\\{i, j\\} \\in M_A\\) means that the \\(i\\) and \\(j\\) frames are masked during pre-training."}, {"title": "3.2.3 Selecting Hard Regions for Reconstruction", "content": "Motivation: Fig. 4 shows that during the initial stage of a EH-MAM pre-training, the reconstruction loss values are significantly high and exhibit low discriminative power (\\(L_{i}^{rec} \\approx L_{j}^{rec}\\)). This leads to increased stochasticity in the overall selective masking process. Thus, inspired by the general human learning approach, where humans do not perceive knowledge uniformly but are subjected to a learning environment where they progressively comprehend more complex information, we propose an easy-to-hard masking strategy that guides the model to progressively mask harder regions for reconstruction. Specifically, we linearly increase the proportion of mask indices associated with hard regions at each training epoch. We define hard regions as a collection of speech frames that the model finds difficult to reconstruct.\nWe illustrate the masking strategy in Fig. 3. At each training iteration \\(t\\) and with a masking percentage \\(P\\), we first compute \\(P_S\\) and \\(P_R\\), the individual masking percentages for selective and random masking respectively. Precisely, we update \\(P_S\\) and \\(P_R\\) linearly as \\(P_S = P \\times \\frac{t}{T}\\) and \\(P_R = 1 - P_S\\), where \\(T\\) is the total number of training iterations. In selective masking, for each sampled batch \\(z \\in Z\\), we build mask \\(M_S\\) by selecting frame indices associated with the top \\(k\\) predicted reconstruction values \\(L\\). We use \\(k = [P_S F(z)]\\), where \\(F(z)\\) denotes the number of speech frames for an input batch \\(z\\). To build a random mask \\(M_R\\), we randomly sample \\([P_R F(z)]\\) frame indices. Finally, we compute the adaptive mask \\(M_A\\) by taking a union of \\(M_S\\), \\(M_R\\). We summarize the complete process of easy-to-hard masking in Algorithm 1"}, {"title": "4 Experimental Setup", "content": "Pre-training Following, we pre-trained our model with 960 hours of unlabelled speech from LibriSpeech corpus. Due to resource constraints, we use a base variant of the context encoder, with the number of transformer layers K = 12 and masking percentage = 50%. For the loss predictor and the reconstruction decoder, we utilize 1D-convolution layers, with the number of convolution layers D = 4. Moreover, a balancing parameter \\(\\alpha\\) is introduced and set to 0.05 during the joint optimization of reconstruction and auxiliary loss. All the pre-training experiments are performed on 4 \u00d7 A100 40GB GPUs, for 400k updates and using a batch size of 63 minutes of speech (Additional details on the hyper-parameters used in EH-MAM can be found in Table 5).\nFine-tuning Similar to to show the effectiveness of the learned speech representation, we fine-tune only the student counterpart with an additional CTC layer. We conduct a comprehensive evaluation under a low-resource labeled data setting using a 10 mins / 1 hour / 10 hours split from LibriLight benchmark and 100 hours split from Librispeech. For all the splits, we follow a similar fine-tuning setup as wav2vec2 (We provide additional fine-tuning details for all the splits in the Appendix B.1). We also perform a SUPERB (Speech"}, {"title": "5 Results and Analysis", "content": "In this section, we present the quantitative and qualitative results. For quantitative evaluation, we first fine-tune EH-MAM on LibriLight and evaluate across all the test splits. Next, to show the scalability of the speech representations learned by EH-MAM, we conduct a downstream evaluation on SUPERB benchmark. Additionally, we also perform a qualitative analysis on the masked regions predicted by the EH-MAM. All the results reported for EH-MAM are averaged across five runs.\n5.1 Evaluation on Low-Resource ASR\nFor low-resource ASR evaluation, we follow a similar procedure as wherein we fine-tune only the student counterpart of EH-MAM with an additional CTC layer on top. We perform fine-tuning using low-resource labeled datasets under four different setups, 10min/1hour / 10hour from LibriLight"}, {"title": "5.3 Qualitative Analysis", "content": "EH-MAM mask useful context: To show EH-MAM does mask useful context, we conduct a simple experiment wherein during ASR inference, we selectively mask the frames with high predicted reconstruction value using the loss predictor and compare the increase in relative WER with random masking. As shown in Fig. 5, under SUPERB evaluation setting for ASR (refer Section 4), we find selectively masking frames with EH-MAM constantly shows a higher relative WER when compared to random masking across various masking percentages. Higher relative WER indicates that a selective masking scheme with the EH-MAM masks useful context in a speech input."}, {"title": "6 Conclusion", "content": "In this paper, we propose EH-MAM, a novel SSL framework for learning robust speech representations. In contrast to prior work that relies on random masking schemes for creating MAM pretext tasks, EH-MAM first identifies hard regions to reconstruct using a teacher network and then challenges the student to reconstruct them by progressively introducing hard regions throughout the learning process. Next, we introduce an easy-to-hard masking scheme that guides the EH-MAM to mask harder regions to reconstruct step-by-step. EH-MAM outperforms all the other models on popular low-resource ASR benchmarks and downstream evaluation on SUPERB."}, {"title": "7 Limitations and Future Work", "content": "EH-MAM and our experimental setup have a few limitations, as mentioned below:\n\u2022 We do not employ a LARGE size encoder in EH-MAM, for example, a 24-layer variant used by due to compute constraints.\n\u2022 The loss-predictors used in EH-MAM increase the trainable parameter count compared to other baselines such as data2vec 2.0 during pre-training. However, we acknowledge that this accounts only for a slight increase in the total parameter count (roughly 5%).\n\u2022 Due to recourse constraints, we conduct the downstream evaluation on SUPERB for context and semantic-related tasks. We plan to extend the evaluation across speaker and paralinguistic tasks in the future."}]}