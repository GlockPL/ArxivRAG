{"title": "CONNECTIONS between SCHEDULE-FREE OPTIMIZERS, ADE-MAMIX, AND ACCELERATED SGD VARIANTS", "authors": ["Depen Morwani", "Nikhil Vyas", "Hanlin Zhang", "Sham Kakade"], "abstract": "Recent advancements in deep learning optimization have introduced new algorithms, such as Schedule-Free optimizers, AdEMAMix, MARS and Lion which modify traditional momentum mechanisms. In a separate line of work, theoretical acceleration of stochastic gradient descent (SGD) in noise-dominated regime has been achieved by decoupling the momentum coefficient from the current gradient's weight. In this paper, we establish explicit connections between these two lines of work. We substantiate our theoretical findings with preliminary experiments on a 150m language modeling task. We find that AdEMAMix, which most closely resembles accelerated versions of stochastic gradient descent, exhibits superior performance. Building on these insights, we introduce a modification to AdEMAMix, termed Simplified-AdEMAMix, which maintains the same performance as AdEMAMix across both large and small batch-size settings while eliminating the need for two different momentum terms. The code for Simplified-AdEMAMix is available on the repository: https://github.com/DepenM/Simplified-AdEMAMix/.", "sections": [{"title": "1 INTRODUCTION", "content": "Recently, numerous optimization algorithms have been introduced for deep learning such as Lion (Chen et al., 2023), ScheduleFreeSGD/AdamW (Defazio et al., 2024), and AdEMAMix (Pagliardini et al., 2024). While these optimizers have been proposed with distinct motivations, they share a common characteristic: each modifies the momentum scheme employed in optimization.\n\nA separate body of theoretical research has focused on accelerating gradient descent in noisy environments. Although classical momentum methods, such as heavy-ball or Nesterov momentum, are sufficient to accelerate deterministic gradient descent (particularly for quadratic functions), they do not accelerate SGD (Jain et al., 2018; Liu & Belkin, 2020). This limitation has led to the development of alternative momentum schemes aimed at achieving acceleration in the presence of noise(Jain et al., 2018; Vaswani et al., 2019; Liu & Belkin, 2020; Gupta et al., 2023). Notably, all proposed accelerated SGD methods can be interpreted as decoupling the momentum coefficient from the weight assigned to the current gradient in the optimizer update.\n\nOur primary contribution is to establish a direct connection between the ideas developed in these two research directions. Specifically, we demonstrate that Schedule-Free SGD is mathematically equivalent to performing accelerated SGD followed by weight averaging. Furthermore, optimizers such as Lion, Schedule-Free AdamW, and AdEMAMix can be understood as combining preconditioning techniques with accelerated SGD approaches. While certain aspects of these connections have been noted in prior literature (Defazio, 2021), to the best of our knowledge, the relationship between these recently proposed optimizers and accelerated SGD has not been formally established before.\n\nTo validate our theoretical findings, we conduct experiments using a 150m decoder-only transformer model, trained on 15b tokens with a small batch size of 32k tokens, ensuring that the training process operates in a noise-dominated"}, {"title": "2 RELATED WORK", "content": "We review the existing literature on accelerated SGD variants and optimization algorithms that are directly relevant to our work.\n\nJain et al. (2018) introduced an accelerated SGD variant that demonstrated improved convergence rates for the least-squares problem. Kidambi et al. (2018) further simplified the update rule for this variant and formally established that momentum does not provide acceleration in this specific case. Subsequent works (Liu & Belkin, 2020; Vaswani et al., 2019; Gupta et al., 2023) extended these results to general convex and strongly convex functions under various theoretical assumptions.\n\nOver the years, several optimizers have been proposed that exhibit similarities to the accelerated SGD variants described above. Lucas et al. (2019) introduced a method that incorporates a weighted sum of multiple momentum terms, each with distinct coefficients, to compute the final update. Ma & Yarats (2019) developed an optimizer explicitly inspired by the theoretical framework established in Jain et al. (2018). More recently, Chen et al. (2023) proposed an optimizer discovered via a genetic search algorithm, which, similar to previous accelerated SGD variants, assigns different weights to the gradient and the momentum coefficient in the update step. Additionally, Pagliardini et al. (2024) introduced a method that blends two distinct momentum scales in the final update."}, {"title": "3 BACKGROUND", "content": ""}, {"title": "3.1 MOMENTUM", "content": "Momentum is a well-established technique for accelerating the convergence of gradient descent in deterministic settings. The momentum update for weights $w_t$, with a momentum coefficient $\\beta$, is given by:\n\n$m_t = \\beta m_{t-1}+ \\nabla f(w_t);\\ \\ \\ w_t = w_{t-1} - \\eta m_t$"}, {"title": "3.2 WEIGHT AVERAGING", "content": "Weight averaging is a widely used technique in stochastic optimization to reduce noise in the iterates. Instead of returning the final iterate $w_T$, a weighted average $\\tilde{w}_T$ of the iterates is computed, where the weights are denoted by $\\gamma_t$:"}, {"title": "3.3 ACCELERATED SGD", "content": "In this section, we provide a generalized framework encompassing many accelerated SGD methods:\n\n$m_t = \\beta_{a,t}m_{t-1}+ g_t, \\ \\ \\ w_{t+1} = w_t - \\eta_{a,t}m_t - \\delta_{a,t}g_t$  (1)\n\nwhere $\\beta_{a,t}, \\eta_{a,t}, \\delta_{a,t}$ are (possibly time-dependent) scalar coefficients, and $g_t$ represents the stochastic gradient evaluated at $w_t$. We use the subscript 'a' to indicate coefficients that adhere to this specific accelerated SGD formulation.\n\nWe first note that setting $\\delta_{a,t} = 0$ recovers standard SGD with momentum. Additionally, as observed in prior work, many accelerated SGD algorithms proposed in the literature\u2014such as those introduced by Jain et al. (2018); Vaswani et al. (2019); Liu & Belkin (2020); Gupta et al. (2023)\u2014fall directly within this framework. A precise demonstration of this equivalence is provided in Appendix B."}, {"title": "4 CONNECTIONS BETWEEN EXISTING OPTIMIZERS AND ACCELERATED SGD", "content": "In this section, we theoretically establish precise connections between existing optimizers, such as Schedule-Free optimizers and AdEMAMix, and accelerated SGD. Based on these insights, we propose a simplified variant of AdE-MAMix that utilizes a single momentum term while maintaining performance comparable to AdEMAMix across both small and large batch size regimes."}, {"title": "4.1 SCHEDULE-FREE SGD", "content": "Schedule-Free SGD (Defazio et al., 2024) is a recently introduced constant learning rate optimizer designed to eliminate the need for scheduling. Following the notation used in Defazio et al. (2024), the update equations are given by:\n\n$y_t = (1 - \\beta)z_t + \\beta x_t$\n\n$z_{t+1} = z_t - \\gamma g(y_t)$\n\n$x_{t+1} = (1 - c_{t+1})x_t + c_{t+1}z_{t+1}$\n\nHere, $y_t$ represents the current model weights (where the gradient is evaluated), while $x_t$ denotes the weights used for evaluation.\n\nWe first express the update in terms of $y_t$ and $m_t$, where we define:\n\n$m_{t+1} = \\frac{x_t - z_{t+1}}{\\gamma}$"}, {"title": "4.2 LION", "content": "The update rule for Lion (Chen et al., 2023) is given by:\n\n$m_t = \\beta_1m_{t-1}+ (1 - \\beta_1)g_t$\n\n$\\theta_t = \\theta_{t-1} - \\eta sign(m_t)$\n\n$m_t = \\beta_2m_{t-1}+ (1 - \\beta_2)g_t$.\n\nLion (Chen et al., 2023) can be directly interpreted as an accelerated SGD method followed by a coordinate-wise sign operation."}, {"title": "4.3 MARS", "content": "In this section, we demonstrate that the practical version of the recently proposed optimizer MARS (Yuan et al., 2024), referred to as MARS-Approx, follows the accelerated SGD framework, supplemented by a preconditioning step. The update equations (ignoring bias correction and clipping) are given by:\n\n$c_t = g_t + \\frac{\\beta_1}{1 - \\beta_1}[g_t - g_{t-1}]$\n\n$m_t = \\beta_1m_{t-1} + (1 - \\beta_1)c_t$\n\n$v_t = \\beta_2v_{t-1} + (1 - \\beta_2)c_t$\n\n$x_{t+1} = x_t - \\eta \\frac{m_t}{\\sqrt{v_t} + \\epsilon}$\n\nwhere $m_t$ and $v_t$ represent the first- and second-order momentum terms, respectively, and $x_t$ denotes the model parameters. Rewriting the update using $m_t = m_t - \\gamma g_t$, we obtain:"}, {"title": "4.4 ADEMAMIX", "content": "The recently proposed optimizer AdEMAMix (Pagliardini et al., 2024) shares structural similarities with accelerated SGD-based AdamW. However, instead of using a linear combination of the current gradient and the momentum term as in accelerated SGD, AdEMAMix maintains two distinct momentum terms with different coefficients and computes their linear combination. The algorithm is formally stated in Algorithm 1.\n\nTo simplify our analysis, we consider a variant of AdEMAMix with $\\beta_1 = 0$. As demonstrated in Pagliardini et al. (2024), this simplified version achieves performance nearly equivalent to the full version for small batch sizes. Our experiments in Section 5 corroborate this finding. With $\\beta_1 = 0$, AdEMAMix aligns with the general accelerated SGD framework (Equation (1)). Furthermore, we show that the prescribed schedules for $\\beta_3$ (momentum coefficient) and $a$ (which controls the relative weight assigned to the current gradient) in AdEMAMix closely match theoretical schedules proposed for accelerated SGD (Gupta et al., 2023).\n\nIn smooth convex optimization, achieving acceleration in stochastic settings requires a momentum scheme of the form:\n\n$\\beta_{a,t} = 1 - \\frac{k}{t}$\n\nfor some constant $k > 0$, as established by Gupta et al. (2023). The AdEMAMix optimizer approximately follows this scheme by scaling up $\\beta_3$ accordingly.\n\nAdditionally, note that in accelerated SGD schemes, momentum is maintained in the standard form:\n\n$m_t = \\beta_{a,t}m_{t-1} + g_t$\n\nwhereas in Algorithm 1, the momentum update follows:"}, {"title": "4.5 SIMPLIFIED ADEMAMIX", "content": "Building on the insights discussed above, we propose a simplified optimizer that eliminates the need for maintaining two separate momentum terms and removes the requirement for scheduling $a$. The optimizer is formally presented in Algorithm 2, where we employ theory-style momentum (instead of the exponential moving average (EMA) style). In the final update, we assign a fixed weight $a$ to the gradient. We note that setting $a = 0$ recovers the standard Adam optimizer (subject to appropriate transformations of $\\eta$ and $\\beta_1$). In Section 5, we demonstrate that this simplified variant matches the performance of AdEMAMix across both small and large batch sizes."}, {"title": "5 EXPERIMENTS", "content": "In this section, we present experiments conducted on a 150-million-parameter decoder-only transformer model for a language modeling task using the C4 dataset. The model is trained with a sequence length of 1024 and a batch size of 32, ensuring that the training operates in a noise-dominated regime."}, {"title": "5.1 LARGE BATCH SIZE EXPERIMENTS", "content": "While the previous experiments focused on the small batch size regime (i.e., training with noisy gradients), we now conduct experiments in the large batch size regime to assess whether these algorithms generalize effectively. In this setup, we train models with a batch size of 1 million tokens over 3 billion tokens (\u2248 Chinchilla scale).\n\nSchedule-Free AdamW: As shown in Figure 2, Schedule-Free AdamW performs significantly worse compared to AdamW. We attribute this performance gap to the coupling between weight averaging and momentum coefficients. At higher batch sizes, the optimal momentum value is significantly lower than $1 - \\frac{1}{t}$. Although one could use a scaling factor $\\approx 1 - \\frac{r}{t}$ for some $r \\geq 1$, a higher $r$ reduces the effective weight averaging window.\n\nAnother key distinction between AdamW and Schedule-Free AdamW is the order in which momentum and preconditioning are applied. AdamW applies momentum before preconditioning, whereas Schedule-Free AdamW applies preconditioning before momentum, making it algorithmically similar to LAProp (Ziyin et al., 2021). However, as shown in Figure 2, the performance of AdamW is comparable to that of LAProp, suggesting that this difference is not the primary cause of the performance gap.\n\nAdEMAMix: For large batch sizes, as previously observed in Pagliardini et al. (2024), Figure 3 shows that setting $\\beta_1 = 0.0$ in AdEMAMix results in a significant performance drop compared to using two separate momentum terms. This degradation occurs because AdEMAMix assigns a fixed weight of 1 to the current gradient, whereas theoretical accelerated SGD variants (Gupta et al., 2023) require a diminishing weight on the current gradient as batch size increases."}, {"title": "6 CONCLUSION", "content": "In this work, we establish explicit connections between accelerated SGD variants and several recently proposed optimizers, including Schedule-Free optimizers, AdEMAMix, MARS, and Lion. We also present empirical evidence demonstrating that AdEMAMix, which aligns most closely with theoretical accelerated SGD variants, achieves superior performance in small batch size training.\n\nBuilding on this connection, we introduce Simplified-AdEMAMix, which removes the need for maintaining two separate momentum buffers. We empirically show that Simplified-AdEMAMix matches the performance of AdE-MAMix across both small and large batch sizes while eliminating the additional memory overhead associated with AdEMAMix."}]}