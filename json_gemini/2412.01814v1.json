{"title": "COSMOS: Cross-Modality Self-Distillation for Vision Language Pre-training", "authors": ["Sanghwan Kim", "Rui Xiao", "Mariana-Iuliana Georgescu", "Stephan Alaniz", "Zeynep Akata"], "abstract": "Vision-Language Models (VLMs) trained with contrastive loss have achieved significant advancements in various vision and language tasks. However, the global nature of contrastive loss makes VLMs focus predominantly on foreground objects, neglecting other crucial information in the image, which limits their effectiveness in downstream tasks. To address these challenges, we propose COSMOS: CrOSs-MOdality Self-distillation for vision-language pre-training that integrates a novel text-cropping strategy and cross-attention module into a self-supervised learning framework. We create global and local views of images and texts (i.e., multi-modal augmentations), which are essential for self-distillation in VLMs. We further introduce a cross-attention module, enabling COSMOS to learn comprehensive cross-modal representations optimized via a cross-modality self-distillation loss. COSMOS consistently outperforms previous strong baselines on various zero-shot downstream tasks, including retrieval, classification, and semantic segmentation. Additionally, it surpasses CLIP-based models trained on larger datasets in visual perception and contextual understanding tasks. Code and models will be open-sourced at ExplainableML/cosmos.", "sections": [{"title": "1. Introduction", "content": "CLIP [59] uses an instance-level contrastive loss [56] to learn text and image representations, where the text gets embedded close to its corresponding image. The contrastive loss function employed to train CLIP is inherently global, as it matches the entire image to the text. This leads to the dominant objects in the image suppressing the recognition of other smaller objects [12, 42, 60, 82], which results in poor performance for dense prediction tasks, such as semantic segmentation [53], and failure to distinguish similar-looking images with different visual patterns [69]. CLIP also demonstrates a lack of contextual understanding, as its text encoder perceives a caption as a bag-of-words, ignoring the sequence of words [33, 68, 79].\nPrevious works [21, 43, 52, 53, 61] have tackled these limitations by masking the text or the image [21, 43, 61], or explicitly optimizing local-to-global image representation. For instance, SLIP [52] and SILC [53] improved the visual representations by integrating self-supervision into the CLIP image encoder. However, we argue that enhancing only the image encoder leads to sub-optimal results, since this does not fully exploit the multi-modality of the data. Therefore, we propose COSMOS, to learn CrOSs-MOdality embeddings through Self-distillation. Unlike previous self-supervised methods [21, 52, 53], COSMOS incorporates a unique text-cropping strategy and a cross-attention module to learn multi-modal representations for various downstream tasks.\nOur text-cropping strategy (Fig. 1, top) is inspired by the multi-crop augmentation for images [7] which optimizes local-to-global image correspondence during training. We randomly sample from the long synthetic caption datasets [65, 83] to construct sentences of varying length. By applying multi-crop augmentation to both images and text, we enhance the visual and text encoders simultaneously within a self-distillation framework [7], aligning the representation of the student VLM with that of the teacher.\nOur cross-attention module integrated in the student allows the model to attend to the other modality's representation, ensuring cross-modality information is captured during training. Inside the module, image patch tokens are conditioned on texts or word tokens are conditioned on images to learn visual and textual grounding. As illustrated in Fig. 1 (bottom), our cross-attention module focuses on different objects from the entire image, which are highlighted in both images and captions based on the attention weights. This demonstrates that COSMOS is able to effectively localize relevant information in images and captions.\nWe demonstrate the adaptability of COSMOS by pre-training it on four datasets varying from 3M to 30M pairs. For evaluating our method, we perform zero-shot experiments on 2 retrieval, 11 classification, and 6 segmentation benchmarks, as well as, the visual perception [69] and contextual understanding tasks [32, 33]. The extensive experiments on these downstream tasks reveal that our method consistently outperforms the previous CLIP-based methods, under the same pre-training data budget. COSMOS pre-trained on only 12 million examples outperforms the Open-CLIP [13] models trained on billions of image-text pairs, in both image-to-text and text-to-image retrieval tasks.\nOur contributions can be summarized as follows: (1) We design a text-cropping strategy which promotes local-to-global text representation learning, fully leveraging multi-modality in the self-distillation of VLMs. (2) We demonstrate that cross-modality embeddings are effective to simultaneously self-distill both image and text encoders. (3) COSMOS reveals fine-grained perception of images and compositional understanding of language, outperforming baselines trained on larger datasets."}, {"title": "2. Related Work", "content": "Self-Supervised Learning (SSL) learns discriminative features from unlabeled data [30]. Initially, models like BERT [20] and GPT [58] were designed to predict masked words or the next word in a sequence, eliminating the need for additional labels. Similarly, vision models benefit from self-supervised techniques, either with contrastive learning or masked image modeling. Based on contrastive learning, BYOL [29] and DINO [7] trained a student network to produce image embeddings that closely resemble those of a teacher network, with the teacher being an exponential moving average of the student. On the other hand, masked image modeling divided the input image into visual tokens and predicted a randomly masked subset of them. For example, MAE [31] directly predicted the original pixel from the masked patches, and BEiT [3] employed a pre-trained tokenizer as its target.\nContrastive learning in vision-language pre-training has achieved great success in various downstream tasks, demonstrating strong zero-shot transfer ability [26\u201328, 34, 40, 59, 77]. As these models are trained to learn visual representations aligned with the language representation, most MLLMs [1, 41, 46, 47, 86] adopt these vision encoders to process the image input. However, instance-level contrastive learning suffers from feature suppression, where the model learns only the dominant features in the data while neglecting other valuable features [12, 42, 44, 60, 66, 69, 75, 82]. In other words, the model creates so-called simple shortcut features and decision rules that do not consider all available distinguishing features. Some works [10, 55, 65, 70, 72, 81, 83] have employed datasets equipped with long synthetic captions generated by MLLMs. These datasets help mitigate feature suppression by learning different features described in the captions and avoiding overfitting on a single caption. Referring to Zheng et al. [83], we employ image datasets with long synthetic captions. Instead of simply training our model with these additional captions, we create text augmentation with the synthetic captions and introduce the cross-attention module for self-distillation.\nFollowing the success of SSL, recent works [21, 36, 43, 52, 53, 61] have applied SSL objectives to contrastive vision-language pre-training. These approaches also tackled the feature suppression limitation, where the new objective reduced the inductive bias to prevent the creation of shortcut features. For example, SLIP [52] combined the SimCLR [11] loss with the contrastive loss, resulting in better transferability compared to the original CLIP. SILC [53] explicitly matched local crops to global crops during training inspired by DINO [7], while MaskCLIP [21] learned more fine-grained features by applying masked modeling to both images and text. Similar to SLIP [52] and SILC [53], we also employ self-supervised learning for vision-language pre-training. While SLIP [52] and SILC [53] only improved the image representation without improving the text representation, we propose a text-cropping technique that enables us to improve the text and image representations simultaneously with our novel cross-modality self-distillation loss."}, {"title": "3. COSMOS Model", "content": "We describe our augmentation technique, which is the key component of self-distillation in Sec. 3.1, COSMOS's architecture in Sec. 3.2, and the training objective in Sec. 3.3."}, {"title": "3.1. Text and Image Augmentations", "content": "Text Cropping Strategy. For performing self-supervised learning based on text, we propose a text-cropping strategy utilizing long caption datasets [65, 83]. These datasets consist of images paired with detailed synthetic captions generated by MLLMs. Inspired by the multi-crop strategy for images [7], we define global and local concepts for text captions. The global crop of a caption comprises one to five sentences randomly sampled from the long synthetic caption, while the local crop of a caption consists of a single sentence, also randomly sampled from the long caption. The number of sentences in the global captions is randomly determined at each training iteration. This adaptation ensures that the global caption generally describes larger areas of the original image, e.g. the fox with vibrant red fur is the main subject of the image, in Fig. 1 top, while the local caption focuses on relatively smaller regions, e.g. the snow covers the ground and the trees.\nImage Cropping Strategy. Following [7], we adopt the standard multi-crop strategy [6] for images as illustrated in Fig. 1 top, utilizing two global views and several local views. Both local and global views are processed through the student encoder, while only the global views are passed through the teacher encoder, thereby promoting local-to-global correspondences. It is noteworthy that the image cropping and text cropping strategies are executed independently, meaning their global and local crops might not contain the same information. For instance, in Fig. 1 top, the local image crop displays the fox's tail, while the local text crop describes the image's background. This configuration is the standard setting of COSMOS, unless otherwise stated."}, {"title": "3.2. Model Architecture", "content": "The overall architecture of COSMOS is depicted in Fig. 2. Both the student model Ge and the teacher model Go are Vision-Language Models (VLMs), each comprising an image encoder Io and a text encoder Te, such that Ge =\n{10, To}, where * \u2208 {s,t}. The student and teacher models share the same architecture and are initialized with identical weights. Additionally, the student model includes a cross-attention module Co = {C, C}, which extracts cross-modal embeddings.\nAs shown on the left side of Fig. 2, two random multi-modal transformations are applied to an image-text pair p, resulting in ps and pt. The student model Ge processes ps, followed by the cross-attention module and normalization operation to produce the output fs. Similarly, the teacher model Ge processes pt to generate the output ft, which serves as the target. The student model, along with the cross-attention module, is optimized by matching its output fs with the teacher's output ft (Lcosmos), while standard contrastive loss is calculated within the student (LCLIP). A stop gradient operation is applied to the teacher model to ensure that gradients propagate only through the student.\nThe teacher parameters Ot are updated at each iteration using the Exponential Moving Average (EMA) of the student parameters \u03b8 (i.e., \u03b8t = \u03bb\u03b8t + (1 \u2212 \u03bb)0s with the parameter \u03bb defining the update schedule). This learning mechanism, combined with multi-modal augmentations, enables COSMOS to learn the correspondence between local features from the student and global patterns from the teacher.\nThe VLMs and the cross-attention module are detailed on the right side of Fig. 2. Student image and text embeddings are extracted from the image encoder Io and text encoder Te, respectively. The image embedding consists of image tokens and a class token [cls], while the text embedding consists of text tokens and an end-of-text token [eot]. The final cross-modal embeddings of image h\u2081 and text hr are calculated as follows:\nh\u2081 = C (q = [cls], kv = txt-tok) + [cls]  (1)\nhT = C(q = [eot], kv = img-tok) + [eot]  (2)"}, {"title": "3.3. Training Objective", "content": "Similar to CLIP [59], we employ a standard contrastive loss to align the image and text embeddings produced by the student model Ge. For each batch of size B, the image and text encoders, Io. and To, extract sets of [cls] tokens (CLS) and [eot] tokens (EOT). We then apply a symmetric InfoNCE loss formulation [56], where paired image-text embeddings form positive pairs, while unpaired embeddings are treated as negative samples. The contrastive InfoNCE loss is defined as:\nLnce(x, y) = \nB\n1\nlog\nexp ((xi, Yi)/T)\n\u03a3=1 exp ((xj, Yi)/T)\n (3)\nwhere (,) denotes the cosine similarity function, x and y are embeddings, and 7 is a learnable temperature parameter. The symmetric InfoNCE loss is composed of a y-to-x loss and an x-to-y loss, and is defined as:\nLsym-nce (x, y) = 1/2 (Lnce (x, y) + Lnce (Y, X)).  (4)\nThe total contrastive loss LCLIP is calculated between the image and text embeddings of the student as follows:\nLCLIP = Lsym-nce (CLSs, EOTs). (5)\nWe proceed by computing the cross-modality self-distillation (COSMOS) loss between the cross-modal tokens produced by the student and the [cls] and [eot] tokens produced by the teacher. Given the image cross-modal tokens h\u2081 and the text cross-modal tokens hy computed using Eq. (1) and Eq. (2), the COSMOS loss is formulated as:\nLCOSMOS = (Lsym-nce (h1, CLSt) + Lsym-nce (h1, EOTt)\n1\n4\n+ Lsym-nce(hT, CLSt) + Lsym-nce(hT, EOTt)) (6)\nwhere CLSt and EOT\u0165 are the set of [cls] and [eot] tokens extracted by the teacher Ge. The final training loss Ltotal is the sum of the CLIP loss and the COSMOS loss:\nLtotal = LCLIP + LCOSMOS. (7)\nAs mentioned in Sec. 3.1, all crops are passed through the student while only global crops are given to the teacher. Thus, LCLIP is calculated between all crops within the student, optimizing the model to construct the overall structure of image and text representations by aligning the embeddings through contrastive learning. On the other hand, Lcosmos is computed between global crops given to the teacher and all crops given to the student, encouraging the model to learn fine-grained representations by predicting the rich global context of the teacher from the local feature of the student (the detailed pseudocode of the training procedure is in supplementary Sec. C.3).\nSimple Loss Formulation. Previous works [21, 43, 53, 61] often rescale the CLIP loss when combining it with distillation objectives, to ensure that the new loss does not disrupt the original learning mechanism of CLIP. Simply adding self-distillation loss without scaling the CLIP loss often underperforms the original model. To overcome this limitation, one must grid-search for the optimal loss scale factor. In our method, both CLIP and the COSMOS loss are updated at the same scale, eliminating the need for additional hyperparameter tuning to adjust the ratio of the two losses."}, {"title": "4. Experiments", "content": "Pre-training Datasets. We pre-train COSMOS on Conceptual Caption 3M (CC3M) and 12M (CC12M) [64], as well as, YFCC15M [16] a subset of YFCC100M [67]. The combination of these three datasets (CC3M, CC12M, and YFCC15M) results in a merged dataset of 30M pairs, referred to as Merged-30M, following DreamLIP [83]. As mentioned in Sec. 3.1, we utilize synthetic long captions to obtain multi-modal augmentation. For CC3M, CC12M and YFCC15M, we use the synthetic captions generated by Zheng et al. [83]. The new captions were obtained using InstructBLIP [17], LLaVA-1.5 [46], and ShareGPT4V [10].\nImplementation Details. COSMOS is built on the Open-CLIP [13] code repository, where we primarily adopt the default settings of their implementation. We use ViT-B/16 as the vision encoder designed for 224 \u00d7 224 image sizes, while our CLIP text encoder has the sequence length of 77. Each encoder consists of 12 transformer layers, and the embedding size is fixed at 512. We train COSMOS on A100 GPUs for 32 epochs, using a learning rate of 5 \u00d7 10-4 and a batch size of 1,024 for CC3M, or 4,096 for CC12M, YFCC15M and Merged-30M. Following DINO [7], we crop two global image views at scales between 0.4 and 1.0, resizing them to 224 x 224 pixels, and six local image views"}, {"title": "4.1. Zero-Shot Image-Text Retrieval", "content": "As shown in Tab. 1, COSMOS achieves 53.1% and 40.1% R@1 score for image-to-text (I2T) and text-to-image (T2I) retrieval on MSCOCO [45]. These results are 12.3% and 10.3% higher than DreamLIP [83] (40.8% and 29.8%, respectively). When we switch to a larger pre-training set (CC12M), COSMOS further improves its performance, reaching 64.2% and 48.9% R@1 scores, outperforming CLIP [59] by 8.0% and 9.1%, respectively. When we employ YFCC15M [16] as the pre-training dataset, COSMOS attains a performance of 67.5% and 50.9% in terms of I2T and T2I R@1 scores. The highest performance of 68.0% and 52.5% on the MSCOCO [45] dataset is obtained by COSMOS, when Merged-30M is used for pre-training. COSMOS also achieves the best performance for image-text retrieval on the Flickr30k [78] dataset, achieving an R@1 score of 92.9% and 80.3% in terms of I2T and T2I retrieval when Merged-30M is used for pre-training, while SigLIP [80] attains only 89.9% and 73.6%, respectively. Overall, COSMOS outperforms all baselines on both datasets (Flickr30k [78] and MSCOCO [45]) in the image-text retrieval task.\nWe include other baselines for state-of-the-art comparison, including CLIP-based models (e.g., OpenCLIP [13], MetaCLIP [74], and Llip [39]) trained on larger datasets and models based on text augmentation (e.g., LaCLIP [23] and MLLM-A [48]) and self-supervised learning (e.g., SLIP [52], MaskCLIP [21], and SF-CLIP [61]). It is noteworthy that COSMOS trained on Merged-30M even outperforms Llip [23] trained on 2.5B image-text pairs (68.0% vs"}, {"title": "4.2. Zero-Shot Image Classification", "content": "We observe in Tab. 2 that COSMOS outperforms the baselines CLIP [59], SigLIP [80], DreamLIP [83], MaskCLIP [21], and SF-CLIP [61] on most of the evaluation datasets, when the pre-training dataset contains the same number of samples. For instance, COSMOS reaches an accuracy of 37.1% on ImageNet when the pre-training dataset is CC3M. As expected, the classification task greatly benefits from the pre-training dataset size, i.e. COSMOS pre-trained on CC12M achieves a score of 51.4% on ImageNet, reaching 57.6% when pre-trained on Merged-30M.\nAdditionally, COSMOS surpasses all baselines regardless of the pre-training set, in terms of average accuracy computed over the 11 datasets. For instance, when pre-trained on YFCC15M, COSMOS achieves an average accuracy of 50.0%, while SigLIP [80] and DreamLIP [83] reach 44.2% and 46.3%, respectively. It is interesting to note that on two fine-grained datasets, i.e. Cars and SUN397, the accuracy improvements especially for Merged-30M trained COSMOS is significant (67.8% vs 64.7% on SUN397, 31.2% vs 23.4% on Cars, compared to DreamLIP [83]).\nFor reference, models trained on billions of samples, e.g. OpenCLIP [13], MetaCLIP [74], and Llip [39] achieve higher accuracy, implying that the larger the number of images the higher the accuracy for the classification task, since the model is required to learn the global information."}, {"title": "4.3. Zero-Shot Semantic Segmentation", "content": "As shown in Tab. 3 top, COSMOS trained on only 30M samples outperforms the best OpenCLIP model [13] trained on 1B samples in all benchmarks. In the datasets with a background category, COSMOS (29.6%, 12.5%, and 15.2%, respectively) surpasses the performance of OpenCLIP (1B) model (22.5%, 11.9%, and 11.4%, respectively) by a large margin. Remarkably, COSMOS almost doubles the performance on the fine-grained Cityscapes [15] dataset and achieves 20.0% average mIoU over all segmentation benchmarks compared to 16.5% obtained by the OpenCLIP model (1B). While CLIP-based models often suffer from feature suppression, COSMOS alleviates it through cross-modality embeddings and local-to-global matching.\nIn addition, we apply SCLIP [71] to our method where the weight of the last transformer block is adapted to construct the Correlative Self-Attention (CSA) block. Wang et al. [71] claimed that the CSA block maximizes the localization of patch embeddings without extra training. Although the CSA mechanism of SCLIP is tailored for the original CLIP model, it also improves the segmentation results of COSMOS. As a result, our COSMOS, trained on 30M samples, achieves an average mIoU of 37.8%, which is comparable to the 38.2% achieved by SCLIP [71], which was trained on 400M samples. Notably, COSMOS outperforms SILC [53] in several benchmarks despite a significantly smaller training budget (30M vs 10B samples). For instance, our model (vs SILC) achieves 34.7% and 23.2% (vs 26.9% and 20.8%) in terms of mIoU on the Cityscapes and COCO-stuff datasets. These results suggest that COSMOS can effectively generate fine-grained multi-modal representations with a much smaller dataset size."}, {"title": "4.4. Visual Perception & Contextual Understanding", "content": "We evaluate COSMOS on the visual perception and contextual understanding tasks [32, 33, 69] to demonstrate the robustness of our representations. COSMOS and previous methods [59, 80, 83] trained on the Merged-30M dataset are compared in Tab. 4, alongside the OpenCLIP [13] models trained on larger datasets. These tasks are specifically designed to assess the perception capabilities of vision-language models, as both visual and text input must be understood in detail to solve them.\nIncreasing the data size generally improves the performance of the OpenCLIP models [13], as shown by the SugarCrepe results. Interestingly, COSMOS not only surpasses the previous baselines, but also outperforms the OpenCLIP models, demonstrating that self-distillation and cross-modality learning play crucial roles in the compositional understanding of multi-modal data.\nIt is also noteworthy that COSMOS (trained on 30M samples) achieves the same performance (25.9%) as OpenCLIP [13] (trained on 1B samples) on the challenging benchmark MMVP-VLM [69]. This result suggests that COSMOS utilizes the pre-training data more efficiently, being able to learn distinguishing features with less data. Additionally, on the SugarCrepe dataset [33], where the task is to choose the correct caption among two captions (e.g., in the wrong caption some words are replaced, swapped inside the sentence or added), COSMOS achieves an average accuracy of 86.6%. This result signifies that COSMOS is able"}, {"title": "4.5. Ablation Study", "content": "We validate the effectiveness of each component in Tab. 5, starting with a naive CLIP model trained on image and text pairs without augmentation (row 1). This simple setup achieves an accuracy of 15.0% and 10.7% in terms of zero-shot performance on MSCOCO image-to-text and text-to-image tasks. Applying image augmentation (row 2) improves all metrics, while EMA (row 3) shows marginal effects on the results. Applying self-distillation on the image encoder brings additional improvements, as it enforces local-to-global correspondence (row 4). Row 5 reveals that employing long synthetic captions as text augmentation plays an important role, boosting the accuracy on MSCOCO to 50.4% and 37.5%. Simply adapting self-distillation for the text encoder (row 6) slightly improves the performance (51.0% and 38.0% on MSCOCO). The self-distillation mechanism is further enhanced by applying a cross-attention module (row 7) reaching the best accuracy of 53.1% and 40.1% on MSCOCO retrieval task.\nAdditionally, we reproduce previous self-supervised methods based on CLIP, such as SILC [53] and SLIP [52], using the same synthetic dataset on which COSMOS is trained. We then directly compare these methods with COSMOS in the supplementary Sec. E.1."}, {"title": "4.6. Qualitative Results", "content": "We present additional qualitative results of COSMOS in Fig. 3. We visualize the normalized attention weights of the cross-attention modules for both images and text. For clarity, we use only three colors for the captions, discretized by their original weight values (dark blue for low value, light green for intermediate value and yellow for high value). Image attention results are based on the input text, while text attention results are conditioned on the input image.\nThe qualitative results illustrate that various objects are highlighted in both the image and text, regardless of their location and size. In the top-left example, the sheep from the foreground and turbines from the background are accurately captured in both image and caption, despite occupying only a small part of the image. Additionally, our module can attend to specific attributes of objects, such as the wooden fence or the silver sports car in the bottom-right example. More qualitative results are in the supplementary Sec. A."}, {"title": "5. Conclusion and Limitations", "content": "Our COSMOS, a model for VL pre-training, uses cross-modality representation for self-distillation. Specifically, we design a text-cropping strategy based on long caption datasets, which enable us to learn local-to-global text correspondences. Our cross-attention module for self-supervised learning further encourages our model to learn both visual and textual grounding, overcoming previous shortcomings of the contrastive vision-language pre-training. Experimental results on various downstream tasks demonstrate that COSMOS outperforms strong baselines trained on much larger datasets on image-text retrieval, semantic segmentation and compositional understanding.\nCOSMOS is mainly restricted by the memory budget as it requires double inference during training. This may hinder the training of our method on larger batch sizes or models. Adjusting the number of global and local crops can mitigate this issue. Additionally, our text-cropping strategy is based on long captions generated by MLLMs, which may introduce hallucinations in the synthetic captions."}]}