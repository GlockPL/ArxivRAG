{"title": "Large Language Models as Realistic Microservice Trace Generators", "authors": ["Donghyun Kim", "Sriram Ravula", "Taemin Ha", "Alexandros G. Dimakis", "Daehyeok Kim", "Aditya Akella"], "abstract": "Workload traces are essential to understand\ncomplex computer systems' behavior and man-\nage processing and memory resources. Since\nreal-world traces are hard to obtain, synthetic\ntrace generation is a promising alternative.\nThis paper proposes a first-of-a-kind approach\nthat relies on training a large language model\n(LLM) to generate synthetic workload traces,\nspecifically microservice call graphs. To cap-\nture complex and arbitrary hierarchical struc-\ntures and implicit constraints in such traces, we\nshow how to fine-tune LLMs to generate recur-\nsively, making call graph generation a sequence\nof easier steps. To further enforce learning\nconstraints in traces and generate uncommon\nsituations, we argue for applying additional in-\nstruction tuning steps to align our model with\nthe desired trace features. Our evaluation re-\nsults show that we can generate diverse realis-\ntic traces under various conditions and outper-\nform existing methods in accuracy and validity.\nWe demonstrate that our synthetically gener-\nated traces can effectively replace real data to\noptimize important microservice management\ntasks. Additionally, our model adapts to down-\nstream trace-related tasks, such as predicting\nkey trace features and infilling missing data.", "sections": [{"title": "1 Introduction", "content": "Computer system workload traces document hard-\nware or software events that occur as applications\nexecute on computing machines, receive requests,\nprocess them, and serve responses. Such traces\nare vital for analyzing complex computer systems\nand optimizing their CPU, memory, networking\nresource allocation, and management. However,\nobtaining access to real-world traces is often chal-\nlenging due to limited public data availability and\nthe difficulty of collecting them at large scale from\ndiverse environments, especially in complex cloud\ncomputing settings. As an alternative, synthetic\ntraces provide limitless size and variety, offering\nsignificant advantages for testing and analysis, in-\ncluding the ability to simulate challenging condi-\ntions like stress-testing environments. While recent\nadvances in generative machine learning, including\nLSTMs (Sherstinsky, 2020), GANs (Goodfellow\net al., 2014), and diffusion models (Ho et al., 2020),\nhave improved synthetic trace generation, these\nmethods typically only generate specific fields,\nsuch as the number of requests or resource types\n(Bergsma et al., 2021), or are confined to fixed-\nstructure traces, like network packets (Jiang et al.,\n2023; Yin et al., 2022).\nIn this paper, we show how to use large lan-\nguage models (LLMs) (Brown et al., 2020; Tou-\nvron et al., 2023) to generate synthetic workload\ntraces. LLMs have been successfully adapted be-\nyond natural language to domains like protein se-\nquences (Shen et al., 2024), code (Roziere et al.,\n2023), and tabular data (Borisov et al., 2023). In\naddition, LLMs can produce outputs well-aligned\nwith user inputs through fine-tuning (Ouyang et al.,\n2022; Wei et al., 2021) and generalize to new\nprompts at inference (Chung et al., 2022; Sanh\net al., 2021). Thus, we posit that LLMs have the\npotential to generate synthetic traces that accurately\ncapture real-world systems trace structures while\nadhering to user specifications.\nDespite their potential, using LLMs for synthetic\nsystems trace generation presents significant chal-\nlenges. Traces are often logged in tabular format\nand structured as graphs, which can vary in depth\nand width. Representing these traces as text se-\nquences, optimal for modern autoregressive LLMs,\nis non-trivial. Moreover, trace data often contain\ncomplex implicit constraints that rely on relation-\nships between multiple trace features. For example,\nan application process's start time must precede\nthat of all its child processes, while the parent pro-\ncess's end time must be later than the end time of\nits children; such constraints need to hold across all\nnodes in the application's graphical representation."}, {"title": "2 Background", "content": "Microservice Call Graphs. In modern software\narchitecture, an application is typically constructed\nas a constellation of multiple microservices (Gan\net al., 2019; Luo et al., 2022; Huye et al., 2023),\neach with specific functionalities and dependen-\ncies on one another. When a user interacts with an\napplication, such as sending an HTTP request, a\ncomplex sequence of communications among these\nmicroservices is triggered. Thus, a user request in-\nduces a microservice call graph, which maps the\ncontrol/data flow and dependencies among the mi-\ncroservices involved in fulfilling the user's request.\nFigure 1 illustrates a social network application\nwith eight microservices. Red arrows indicate com-\nmunications between microservices involved in\nprocessing the user's request. The request first\nreaches a microservice (e.g., \u201cFront end\u201d in Fig-\nure 1) and waits for the communication to termi-\nnate. If the microservice requires additional com-\nmunication to handle the request, then it triggers\nanother microservice call (e.g., from \"Front end\" to\n\"Authentication\" in Figure 1). The communications\ntriggered by a user's request form a microservice\ncall graph with four microservices. The vertices\nof the graph correspond to microservices (or the\nclient), while the edges correspond to API calls\ninvoking the microservices. Note that not all edges\nappear in the graph, as some services may not be\ninvoked for a given request.\nA call graph can be represented as a tabular trace\ncapturing API call features (i.e., edges) such as re-\nquest source/destination, request type (e.g., HTTP\nand RPC), and start/finish times. Given their hi-\nerarchical structure, the tabular trace should pre-\nserve the parent-child relationships by ensuring\nthat the child's source matches the parent's destina-\ntion. Also, the start/end times of each call should\nbe consistent with each other: (1) a microservice's\nstart time must precede its finish time, and (2) the\nparent-child relationships must be honored, i.e., the\nparent's start (finish) time must precede (follow)\nthe child's. Finally, the IDs within a call graph\n(dot-decimal numbers provided for each call) must\nalso be hierarchically connected to form a DAG.\nSynthetic Trace Generation using Machine\nLearning. Microservice traces play a pivotal role\nin designing and evaluating techniques for improv-\ning the performance and reliability of microservice\napplications and optimizing the use of underly-\ning resources. Representative use cases include\ntechniques for critical path analysis (Zhang et al.,\n2022), anomaly detection (Xie et al., 2023), root\ncause analysis (Ikram et al., 2022), cluster man-\nagement (Qiu et al., 2020), and cluster schedul-\ning (Singhvi et al., 2021). Unfortunately, obtaining\ndiverse real-world traces to study such techniques\nthoroughly remains challenging as publicly avail-\nable traces are typically limited in size and only\ncover specific narrow settings far from the diversity\nexpected when operating in the cloud.\nGiven the importance and scarcity of public"}, {"title": "3 Training LLMs for Microservice Traces", "content": "Our goal is to train a generative model for mi-\ncroservice call graph traces, enabling end-users\nto simulate diverse scenarios like rare microser-\nvice invocations that exhibit high response times.\nTo achieve this, we condition the model's output\non user-specified attributes, including the invoked\napplication, the number of microservice commu-\nnications (i.e., graph edges), and overall applica-\ntion latency. Given the limitations of existing trace\ngeneration methods, we leverage LLMs. We ini-\ntialize our model from LLMs pre-trained on large\ntext datasets, as these models have proven effective\nwhen adapted to specialized domains such as pro-\nteins (Shen et al., 2024) and code (Roziere et al.,\n2023). Moreover, LLMs support flexible condi-\ntioning mechanisms, including natural language\nprompts (Ouyang et al., 2022) and structured input\nsequences (Borisov et al., 2023).\nThis section describes our approach to training\nan LLM for microservice call graph generation in\ntwo stages. First, we pre-train the model to capture\nthe interactions in real-world call graph data and\nintroduce a recursive subgraph generation to en-\nhance its ability to generate large, structured graphs.\nSecond, we fine-tune the model with instruction\ntuning, enabling it to generate call graphs with\nuser-specified attributes and ensuring constraint ad-\nherence through natural language reasoning."}, {"title": "3.1 Pre-training", "content": "We pre-train our model on call graphs using an\nautoregressive language modeling objective. This\nstage adapts the general-purpose LLM, which was\npreviously trained to model natural language text\nsequences, to the more specialized domain of mi-\ncroservice call graphs."}, {"title": "3.1.1 Encoding Call Graphs as Text", "content": "LLMs expect sequences of text as input, so we\nmust encode our dataset of call graphs into text-\nbased representations before training our model.\nAs detailed in \u00a72 and shown in Figure 1, microser-\nvice call graphs are initially stored as tables. Rows\nrepresent edges (i.e., communications between mi-\ncroservices), while columns describe features for\neach edge. We follow the method proposed by\nGReaT (Borisov et al., 2023) and encode features\nin a natural language format. Our encoding proce-\ndure preserves all necessary information to recover\nthe unique graph that produced the tabular data.\nBesides edge features, we also encode global at-\ntributes of the call graph to serve as conditioning\ninformation for the model.\nA tabular call graph $X$ has $m$ feature columns"}, {"title": "3.1.2 Recursive Generation", "content": "We propose to break down the task of generating a\ncall graph into a series of recursive layer generation\ntasks to handle complex structures. Starting from\nthe initial attributes, or prompt $c$, the task for the\nmodel at each layer is to generate the edges origi-\nnating from the Start Node specified in the prompt.\nThe model also generates a new prompt for the\nnext layer based on the previous layer prompt and\nthe edges generated in the current layer. This new\nprompt is then re-used to condition the model's\noutput for the next layer. The recursive process\ncontinues until the requested attributes $c$ are met.\nFormally, for an encoded call graph $t =$\n$[t_1, t_2,..., t_n]$, we partition the edges $t_i$ into a\nsequence of layers $[t^1, t^2, ..., t^l]$, where $l \\leq n$.\nEach layer consists of a sequence of edges that\nshare the same parent (i.e., source) node, ensur-\ning that no edge appears in multiple layers. For\ncall graph conditions $c$ that describe $t$, we intro-\nduce layer conditions $c^j, j \\in {1,2,...,l + 1}$.\nLayer condition $c^j$ encodes the attributes of the\nremaining portion of the call graph after the se-\nquence of layers $[t^1, t^2, ..., t^{j-1}]$ has been gen-\nerated, and we define $c^1 := c$ and $c^{l+1} := \\emptyset$.\nWe decompose the conditional call graph distri-\nbution as a chain of conditional layer distributions:\n$p(t|c) = \\prod_{k=1}^l P(c^{k+1}, t^k|c^k)$. In other words,\nthe model predicts call graphs from user prompts\niteratively layer-by-layer. For layer $k$ the model\ntakes $c^k$ as input and produces the sequence of\nedges $t^k$ followed by the conditions $c^{k+1}$ of the\nnext layer. The process continues recursively, us-\ning $c^{k+1}$ to predict the next layer, $k + 1$. Figure 2\nillustrates an example of this recursive generation."}, {"title": "3.2 Instruction Tuning", "content": "After pre-training, we perform supervised fine-\ntuning to enhance the model's ability to gener-\nate call graphs based on user instructions. Unlike\npre-training, we exclude the initial call graph at-\ntributes $c$ (equivalent to the first-layer conditions\n$c^1$) from the loss computation, treating them as\na fixed prompt. Users can provide additional in-\nstructions, and \u00a74.4 presents results for two instruc-\ntion types. To further aid reasoning, we supple-\nment instructions with programmatically generated\nprompts that convert numerical and non-linguistic"}, {"title": "3.2.1 Intermediate Instructions", "content": "The model often struggles to generate consistent\nnext-layer conditions $c^{k+1}$ based on the current\nlayer's edges $t^k$ and conditions $c^k$, sometimes vio-\nlating physical constraints (e.g., assigning a layer\nhigher latency than the overall call graph). Inspired\nby work showing that LLMs improve with explicit\nstep-by-step reasoning (Wei et al., 2022; Nye et al.,\n2021), we introduce natural language reasoning\nsteps to reinforce constraint adherence. For ex-\nample, we (1) compute remaining edges from the\nNum Edges attribute in $c^k$ and edges in $t^k$, and (2)\nderive the Remaining Depth in $c^{k+1}$ as Child's\nremaining depth = current remaining depth\n- 1 = . These intermediate instructions are\ninserted before $c^{k+1}$ during instruction fine-tuning.\nWe give an example of these steps in \u00a7B.3."}, {"title": "4 Evaluation", "content": "We thoroughly demonstrate the effectiveness of our\nmethod in two major aspects: (1) synthetic trace\nquality in terms of structural validity (\u00a74.1), dis-\ntribution similarity (\u00a74.2), and usefulness to train\nand evaluate machine learning-driven microservice\nmanagement tasks (\u00a74.3), and (2) benefits from our\nuse of LLMs in terms of instruction-following ca-\npabilities (\u00a74.4) and trace-related downstream task\nperformance (\u00a74.5).\nWe initialize our model from Llama-2 7B (Tou-\nvron et al., 2023) and train with LoRA (Hu et al.,\n2022) on 1.36 million microservice call graph sam-\nples from the Alibaba v2022 dataset (Luo et al.,\n2022), corresponding to 1.1B tokens. Further de-\ntails on data preprocessing and training hyperpa-\nrameters are provided in Appendix B. We compare\nsynthetic trace quality with various structured data\ngeneration methods such as GReaT (Borisov et al.,\n2023) and TVAE (Xu et al., 2019), and downstream\ntask performance with one of the state-of-the-art\nLLMs, Llama-3.1 405B (Dubey et al., 2024)."}, {"title": "4.1 Structured Reasoning Accuracy", "content": "This experiment demonstrates how recursive gen-\neration and instruction tuning with intermediate\ninstructions enhance LLMs' ability to accurately\nconstruct microservice call graphs. We evalu-\nate our model by generating traces with specified\nnum_edges and depth. A trace is deemed accu-\nrate if it correctly matches the specified num_edges\nand depth and adheres to all structural constraints,\nsuch as valid DAG formations and appropriate\nstart/finish times for communications, detailed in\nAppendix C. We generate 50 samples for each\n(num_edges, depth) pair across ranges of $1 <$\nnum_edges $\\leq 30$ and $1 <$ depth $\\leq 6$.\nBaselines. We compare our model (recursive\n+ instruction) to Llama-2 7B models trained on\ntext-encoded call graphs (1) without recursive gen-\neration and tuning with intermediate instructions\n(baseline) and (2) with recursive generation but\nno instruction tuning (recursive). Both baseline\nmodels are given num_edges and depth as inputs\nduring training (see Figure 9 for a training data ex-\nample of the baseline model). Baselines are trained\nusing the same hyperparameters and number of to-\nkens as our model. The baseline model follows\nGReaT (Borisov et al., 2023), representing call\ngraph traces as the tabular data format.\nResults. Figure 3a and Figure 3b present the accu-\nracy of generated call graphs across varying num-\nbers of edges and depths. Generally, as complex-\nity increases (i.e., more edges or greater depth),\nthe baseline model's accuracy decreases signifi-\ncantly-dropping below 25% for edges > 15 and\nnearing zero for depths > 4. In contrast, the recur-\nsive generation model maintains higher accuracies,\nby approximately 30% and 35%, respectively. This\nimproved performance is attributed to the model\nbreaking down complex generation tasks into sim-\npler, more manageable sub-tasks.\nFigure 3c illustrates how decoding temperature\naffects accuracy. Both models show decreased per-\nformance as the temperature increases, but the re-\ncursive model consistently outperforms the base-\nline, maintaining about 10% higher accuracy even\nat a temperature of 1. Further, instruction tuning\nenhances model accuracy by 23% to 36% by di-\nrecting the model to adhere to specific generation\ninstructions, such as the number of edges per layer,\nwhich are outlined in \u00a7B.3.\nMore results on accuracy with varying model\nsizes and memorization are in \u00a7D.2 and \u00a7D.3."}, {"title": "4.2 Similarity of Real and Synthetic Traces", "content": "To assess the quality of synthetic traces, we com-\npare them to real traces from the validation dataset.\nWe generate 50K call graphs using prompts derived\nfrom the validation set and measure their similarity\nto the original traces.\nBaselines. We compare the following synthetic\ntrace generation methods:"}, {"title": "4.3 Synthetic Data as Training data for ML-Driven Microservice Management", "content": "Synthetic datasets can be used as a substitute for\nscarce real data in the training process for ML-\ndriven microservice management tasks. Thus, we\nassess how well microservice management tasks\nusing ML models for critical component extraction\nin FIRM (Qiu et al., 2020) and anomaly detection\nin TraceVAE (Xie et al., 2023) perform when the\nmodels are trained on the synthetic datasets. The\nML models are evaluated using real test data, and\ntheir results are compared to their original perfor-\nmance when trained on the real training dataset.\nWhen choosing training data, we select a subset\nof traces from real data and label them with corre-\nsponding conditions (e.g., critical microservices).\nThen, we extract instructions from real data and\nuse them to generate synthetic traces. We exclude\ninvalid call graphs using the same accuracy metrics\nin \u00a74.1 before training. We train the models on 5K\nsynthetic call graphs and test on 2K real call graphs,\nusing the same test dataset across all experiments.\nFor baselines, we use synthetic traces generated by\nGReaT and the Alibaba probabilistic model. Each"}, {"title": "4.4 Instruction-following Capability", "content": "Enabling users to specify desired characteristics\nof synthetic data is crucial for trace generation.\nSuch \"custom\" traces are useful to study corner\ncases and debug microservice management tech-\nniques. We assess our instruction-tuned model's\ncapacity to accurately generate call graphs with\nspecified attributes, such as high latency and rare\ncommunication types. We also explore the model's\nperformance when prompted with combinations\nof these attributes that were not included in the\ntraining data.\nWhen constructing the instruction tuning train-\ning datasets, we embed specific instructions to\nguide the generation of call graphs:\n\u2022 High Latency: Instructions specify that call\ngraphs should exhibit latencies above the 90th\npercentile (p90) of the training dataset's distri-\nbution, varying by service. For example: Build\na call graph with high latency.\n\u2022 Uncommon Communications: Instructions in-\ndicate that the call graph layer should include\na communication occurring in less than 10% of\nthe training data. For example: Include an\nedge from (SRC) to (DEST) with (TYPE)\ncommunication type.\nWe avoid combining these specific instructions\nin training samples to test the model's response to\nnovel instruction combinations during inference.\nResults. Figure 6 presents the instruction-\nfollowing accuracy for high latency and uncommon\ncommunication. We assessed this by filtering 1K\nvalidation instructions to see how many generated\ncall graphs met the defined criteria (e.g., exceed-\ning p90 latency). We also compared these results\nagainst outputs generated without specific instruc-\ntions to assess the impact of tailored prompts.\nAdditionally, we evaluate the model's perfor-\nmance when both instructions were combined, a\nscenario not covered during training. The model's\nability to satisfy both conditions simultaneously,\ndespite not being explicitly trained to do so, is de-\ntailed in the right of Figure 6. Higher accuracy in\nthe absence of instructions may arise from inherent"}, {"title": "4.5 Adapting Models for Trace-Related Downstream Tasks", "content": "We extend our evaluation beyond generating syn-\nthetic traces, demonstrating the utility of our pre-\ntrained model in performing downstream tasks re-\nlated to microservice traces. The trace pre-trained\nmodel is adapted to each downstream task through\nadditional fine-tuning. We focus on scenarios\nwhere partial information from distributed envi-\nronment traces is available, emphasizing the chal-\nlenges posed by incomplete data. This section\ncompares our fine-tuned model with the standard\nLlama-2 7B, which lacks specific training on call\ngraph data, and with Llama-3.1 405B by providing\ntask descriptions and up to 16 examples in prompts\n(i.e., in-context learning (Brown et al., 2020)), to\nhighlight the need for domain-specific training.\nPredicting Uncommon Communications. The\ntask is to predict uncommon communication pat-\nterns (as in \u00a74.4) based on the first 10 lines of a\ntrace. For fine-tuning, we adapt both the original\nLlama-2 7B and our trace pretrained model to this\nbinary classification task using 15K samples. Each\nsample's prompt comprised the first 10 edges of a\nreal trace, with binary labels indicating the pres-\nence of uncommon communication patterns in the\nsubsequent trace sections.\nAs shown on the left side of Figure 7, the origi-\nnal Llama-2 model achieves only 60.6% accuracy,\nindicating insufficient training for recognizing un-\ncommon patterns. Additionally, in-context learn-\ning with Llama-3.1 405B shows lower accuracy\n(45.6%), suggesting that larger models trained on\ngeneral internet data struggle with domain-specific\ntasks. In contrast, our model achieves 76.8% ac-\ncuracy, demonstrating its enhanced capability to\ninterpret and predict based on partial trace data.\nInfilling Missing Data. Missing data is common\nin large-scale trace logging, such as in Alibaba's\nmicroservice call graphs, where 67% of traces con-\ntain missing values (Huye et al., 2024). This task\nfocuses on fine-tuning our model to accurately infill\nmissing data in microservice call graphs, consider-\ning partial information. Specifically, we conduct\ntwo experiments on infilling (1) a missing attribute\nand (2) a missing call connecting two layers.\nIn the first experiment, we construct a training\ndataset with 1.2K questions, each containing a\nsequence of edges with one attribute marked as\n[MISSING]. The missing value is the unknown\nground truth for prediction, so these are multi-class\nclassification problems. Attributes targeted include\ncommunication type (e.g., HTTP, RPC) or desti-\nnation microservice. We evaluate the model on a\n6K-sample test dataset, where our model demon-\nstrated over 70% accuracy in predicting the correct\nattributes, significantly outperforming the accuracy\nof baselines by about 30% to 40% as reported in\nthe middle of Figure 7.\nThe second experiment's dataset comprises 1K\nsamples, each representing a pair of parent and\nchild layers with a missing connecting edge tagged\nas [MISSING]. After training, we evaluate both\nmodels on 5K test cases to generate the correct\nedge, ensuring the finish time matched or exceeded\nthe start time. The right part of Figure 7 shows that\nwhile the original Llama-2 model scored only 24%\naccuracy and Llama-3.1 405B reached 34%, our\nmodel maintained a high accuracy of 66%, under-\nscoring its robustness in more complex tasks.\nThese experiments demonstrate the capabilities\nof our trace pre-trained model to effectively adapt\nto handle infilling tasks that even large foundation\nmodels like Llama-3.1 405B cannot achieve."}, {"title": "5 Conclusion", "content": "This paper introduces a training method for adapt-\ning pre-trained LLMs to generate microservice\ntrace graphs using recursive call graph genera-\ntion and instruction tuning. Our approach out-\nperforms baselines in producing accurate, valid\ncall graphs with improved distributional similar-\nity to real traces. We demonstrate that synthetic\ntraces can effectively replace real data for training\nmicroservice management tasks, such as critical\ncomponent extraction and anomaly detection. Ad-\nditionally, instruction tuning enhances graph gen-\neration based on user-specified features, enabling\napplications in prediction and data infilling. While"}, {"title": "6 Limitations", "content": "This section discusses a few limitations of our work\nand potential approaches to overcome them.\nThe recursive method improves the accuracy of\ncall graph generation compared to generating the\nentire trace at once, but a key drawback is that\npreviously generated edges are discarded, as only\nthe conditioning information from the prior layer\nis passed to the next layer generation steps. Al-\nthough dropping previously generated edges has lit-\ntle impact on the output in microservice call graph\ngeneration, where direct neighbors exert the most\ninfluence (Zhang et al., 2024), incorporating past\ninformation, such as prior layers or a time series\nof call graph traces, could enhance the capture\nof longer-range dependencies and temporal pat-\nterns. However, efficiently compressing historical\ntrace information while preserving critical details\nremains an open challenge. In future work, we\nwill consider this direction to compress long-range\ntraces and generate synthetic traces conditioned on\nthe compressed traces.\nFurthermore, our method uses manually con-\nstructed instruction templates, which may lead to\nsuboptimal generation quality, as we are not using\nthe full potential of language models pre-trained\nwith trillions of tokens (Touvron et al., 2023). Fol-\nlowing the methods of prior work (Liu et al., 2024;\nGunasekar et al., 2023; Li et al., 2024), we believe\nthat diversifying instructions using LLM-generated\noutput is a potential method to improve the abil-\nity of LLMs to follow user intentions. However,\nnaively guiding LLMs to generate instructions for\ntrace generation may result in instructions that lack\nuseful characteristics for downstream tasks. In fu-\nture work, we plan to integrate domain-specific\nknowledge of traces to improve the usefulness and\ndiversity of instructions generated by LLMs.\nLastly, we focused on generating microservice\ncall graphs in this paper, but other system traces,\nsuch as operating system (OS) call graphs, share a\nsimilar hierarchical structure. The primary differ-\nences in OS call graphs lie in their greater depth\nand the increased diversity of node and edge types.\nEvaluating whether our approach can be effectively\napplied to such traces remains an open question."}, {"title": "Ethics Statement", "content": "There are no ethical concerns\nraised by our work as the data used in this study is\npublic with sensitive information redacted."}, {"title": "A Other Related Work", "content": "Adapting LLMs for Specific Domains. Pre-\ntrained LLMs are increasingly adapted for special-\nized domains due to their vast, diverse training\ndatasets, which enable broad generalization capa-\nbilities. Examples include fine-tuning LLMs for\nprogramming (Roziere et al., 2023), quantitative\nreasoning (Lewkowycz et al., 2022), and semicon-\nductor manufacturing (Liu et al., 2023). Our work\nis the first to apply this approach to computer sys-\ntem traces involving data with specific structures\nand constraints. Our focus is on generating syn-\nthetic trace data by fine-tuning these models to\nhandle the specific requirements of this domain.\nMaking Language Models Follow Instruc-\ntions. Recent advancements have focused on\nenhancing LLMs' ability to follow instructions\nthrough prompting (Li and Liang, 2021; Shin et al.,\n2020; Wei et al., 2022) and instruction tuning\n(Ouyang et al., 2022; Wei et al., 2021; Chung et al.,\n2022). These two sets of methods are relevant to\nour setting since they augment powerful pre-trained\nLLMs to improve their performance on new tasks.\nOur approach seeks to refine output expressiveness\nwithin set prompts, aiming for greater fidelity in\nsynthetic data production.\nMulti-step Reasoning with LLMs. Iterating\nwith LLMs over multiple steps is an effective strat-\negy to solve complex problems. For instance,\nTree-of-thoughts (Yao et al., 2024) solves prob-\nlems by decomposing into smaller thoughts and\nexploring diverse reasoning paths over different\nthoughts. Multi-step reasoning is also useful to\nhandle long-context scenarios by summarizing it-\neratively (Wang et al., 2023) and diving into sub-\nproblems (Lee and Kim, 2023). In contrast to the\nabove approaches, our approach learns to generate\ntraces with specific structures and instructions for\nsubsequent layers."}, {"title": "B Training Details", "content": "We train all models with 4x A100 80GB GPUS\nin our cluster with the hyperparameters described\nin Table 1. We apply LORA ((Hu et al., 2022))\nadapters to query and key projection matrices of\nattention layers with rank = 8, alpha = 16, and\ndropout = 0.1. For the downstream task train-\ning in \u00a74.5, we freeze the backbone model and\nonly train the last classification layer for the pre-\ndiction task. For the infilling downstream task, we"}, {"title": "D Additional Evaluation Results", "content": "This section provides a more detailed analysis of\nthe results from \u00a74.1, accuracy to generate call\ngraphs adhering to all structural constraints while\nmatching the specified attributes in prompts (i.e.,\nnum_edges and depth). Figure 12 offers a closer\nlook at Figure 3a and Figure 3b, where each grid\npoint (X, Y) represents accuracy for prompts with\nX edges and a maximum depth of Y. Figure 12a,\nFigure 12b, and Figure 12c correspond to the same\nsettings as (baseline), (recursive), and (recursive +\ninstruction) from \u00a74.1, respectively. The results in\nFigure 12 show that the recursive generation and\ninstruction tuning improves accuracy across most\ncombinations of (# Edges, Depth). However,\nsome configurations in Figure 12b and Figure 12c\nexhibit lower accuracy, likely due to the distribution\nof training data in terms of edge count and depth.\nIn addition, we conduct an ablation study, where\nwe remove intermediate instructions during instruc-\ntion tuning to see the impact of intermediate in-\nstructions in generating correct call graphs. For"}, {"title": "D.2 Structured Reasoning Results varying Model Sizes", "content": "To evaluate the impact of model size on trace gen-\neration performance, we report the generation accu-\nracy of models with varying numbers of parameters.\nSpecifically, we compare four models: Llama-3.2\n1B, Llama-3.2 3B, Llama-2 7B, and Llama-2 13B.\nEach model undergoes pre-training (\u00a73.1) using the\nsame training dataset (same as the Recursive setup\ndescribed in \u00a74.1).\nFigure 14 presents the microservice call graph\ngeneration accuracy across different model sizes.\nOverall, models with a larger number of parameters\ndemonstrate higher accuracy, with this trend being\nparticularly evident in Figure 14c. Notably, models\nwith more parameters perform better as the depth\nof prompts increases. For instance, the 13B model\nachieves a 20 percentage point improvement over\nthe 7B model for inputs with a depth greater than 4\nas shown in Figure 14b."}, {"title": "D.3 Memorization", "content": "We assess whether synthetic traces are generated\nby memorizing training data by measuring the\npercentage of traces that exactly match the struc-\ntures and call graph attributes found in the training\ndata. Specifically, for the synthetic traces gener-\nated in \u00a74."}]}