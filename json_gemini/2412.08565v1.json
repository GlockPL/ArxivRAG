{"title": "GenPlan: Generative sequence models as adaptive planners", "authors": ["Akash Karthikeyan", "Yash Vardhan Pant"], "abstract": "Offline reinforcement learning has shown tremendous success in behavioral planning by learning from previously collected demonstrations. However, decision-making in multi-task missions still presents significant challenges. For instance, a mission might require an agent to explore an unknown environment, discover goals, and navigate to them, even if it involves interacting with obstacles along the way. Such behavioral planning problems are difficult to solve due to: a) agents failing to adapt beyond the single task learned through their reward function, and b) the inability to generalize to new environments not covered in the training demonstrations, e.g., environments where all doors were unlocked in the demonstrations. Consequently, state-of-the-art decision-making methods are limited to missions where the required tasks are well-represented in the training demonstrations and can be solved within a short (temporal) planning horizon. To address this, we propose GenPlan: a stochastic and adaptive planner that leverages discrete-flow models for generative sequence modeling, enabling sample-efficient exploration and exploitation. This framework relies on an iterative denoising procedure to generate a sequence of goals and actions. This approach captures multi-modal action distributions and facilitates goal and task discovery, thereby enhancing generalization to out-of-distribution tasks and environments, i.e., missions not part of the training data. We demonstrate the effectiveness of our method through multiple simulation environments. Notably, GenPlan outperforms the state-of-the-art methods by over 10% on adaptive planning tasks, where the agent adapts to multi-task missions while leveraging demonstrations on single-goal-reaching tasks.", "sections": [{"title": "1 Introduction", "content": "An intelligent autonomous agent must be adaptable to new tasks at runtime, beyond those encountered during training. This is crucial for operating in complex environments that may introduce distractors (i.e., objects the agent has not seen before) and have multiple novel goals.\nExample 1. Consider the mission illustrated in figure 1, where the agent navigates a complex environment filled with distractors and must adapt to various goals in run-time. Note that the goal position is not available to the agent, so it must rely on image observations and instruction prompts to plan and execute tasks, including exploration, unblocking paths, and navigating multiple rooms."}, {"title": "2 Related Works", "content": "Offline RL. Offline RL focuses on learning policies from collected demonstrations, as illustrated in figure 2A, without further interaction with the environment (Levine et al. 2020). A key challenge in this approach is the distribution shift between the training demonstrations and the runtime distribution. To address this, several regularization strategies have been proposed, such as reducing the discrepancy between the learned and behavioral policies (Fujimoto, Meger, and Precup 2019; Kumar et al. 2019) and incorporating pessimism into value function estimation (Kumar et al. 2020). Alternatively, a BC objective allows policies to be learned in a task-agnostic manner by maximizing likelihood or mutual information (Eysenbach et al. 2019; Campos et al. 2020). However, these approaches often fail to generalize effectively. Our work, in contrast, leverages a flow-based model to improve generalization and training stability.\nSequence Models in RL. Sequence models in deep learning have been extensively studied in the domain of language modeling, from early sequence-to-sequence models (Hochreiter and Schmidhuber 1997) to BERT (Devlin et al. 2018). In RL, these sequence models have been applied to learn value functions and compute policy gradients, leading to improved performance through model architecture, such as convolutions and self-attention mechanisms, which enable temporally and spatially consistent predictions. More recent works (Chen et al. 2021; Janner, Li, and Levine 2021; Furuta, Matsuo, and Gu 2022) have adopted an autoregressive modeling objective. This approach utilizes the self-attention mechanism of sequence models, where conditioning on desired returns or goal states guides the generation of future actions that lead to those states or returns, assuming they were observed during training. These strategies aim to answer the question of what action is typically taken next, based on experience, assuming that the desired outcome will occur (Chen et al. 2021; Paster, McIlraith, and Ba 2022). Such a behavior cloning approach is designed to map observations to actions, with guidance signals indicating the closeness of the agent's actions to those observed in demonstrations. While effective for behavior cloning tasks, these methods fail in scenarios as seen in Example 1.\nPlanning with Sequence Models. Planning Transformer (Sun et al. 2022) introduces procedural planning with transformers by framing planning as a model-based RL problem. It employs a joint model of action and state representation to mitigate compounding errors inherent in transformer-based architectures. LEAP (Chen et al. 2023) addresses planning as an iterative energy minimization problem, utilizing a Masked Language Model (MLM) to learn trajectory-level energy functions. This approach, utilizes Gibbs sampling with MLM, to helps prevent error accumulation and shows generalization to novel test scenarios. However, LEAP's reliance on an oracle for goal positions limits its effectiveness, particularly in larger mazes, as shown in our simulation studies (see Table 1). Without the oracle, performance declines significantly, especially with unseen goals. This limitation can cause the agent to enter loops, leading to stalling actions, where it fails to progress. To overcome these issues, GenPlan learns a goal generation module, enabling adaptation to unseen goals during runtime. Additionally, it adopts a stochastic policy to facilitate exploration and adaptive skill learning."}, {"title": "3 Preliminaries and Problem Statement", "content": "Notations. We model discrete data as a sequence $(x_1,...,x_H)$, where $H$ is the horizon. This sequence is represented by $x$, with each $x_k$ denoting a step in the sequence. The superscript $x^t$ indicates the time step in the CTMC, with $t\\in [0, 1]$. Each $x \\in X$ takes a discrete value from the set $X = \\{1,...,x_{|x|}\\}$, with $|X|$ denoting the cardinality of this set. This notation applies to any discrete variable, such as a state $s$ or an action $a$. We denote the dataset by $D$, representing a collection of demonstrations. We use $C$ and $U$ to denote categorical and uniform distributions, respectively. Additionally, we utilize $\\delta_{\\{i, j\\}}$ to represent the Kronecker delta, which equals 1 when $i = j$ and 0 otherwise.\nReinforcement Learning. We extend the standard Markov Decision Processes (MDP) framework by incorporating a sequence of goals $G = \\{g_1,...,g_n\\}$ within the state space $S$. Formally, we consider learning in a MDP $M = (S, A, P, R)$. The MDP tuple comprises $S$ and $A$ to denote the state and action spaces, respectively, with discrete states $s_k \\in S$ and actions $a_k \\in A$. The transition function $P(.s_k, a_k)$ returns a probability distribution over the next"}, {"title": "4 GenPlan: Method", "content": "Overview. The denoising planner integrates the DFM with sequence models and is built on two key components: (a) the denoising model $p_{\\theta|t}$, trained as described in Section 4.3, and (b) the rate matrix_$R_t(x_t,x_t|x^1)$ (Appendix D). Together, these components enable planning using the algorithm outlined in Section 4.4. In \u00a7 4.2-4.2, we discuss several design choices that enhances the planner's performance."}, {"title": "4.1 Energy-Guided Denoising Model", "content": "Given demonstrations (see Section 3), our aim is to learn an implicit energy function for a sequence, denoted as $E(a^t)$. This energy function is designed to assign lower energy to optimal action sequences. We define the energy as the sum of negative pseudo-likelihood over the horizon, formulated as $(a^1) = E_{(a^1)\\sim D} \\sum_H[-logp_{\\theta|1,t}(a^1|a^0, o)]$ adapted from (Goyal, Dyer, and Berg-Kirkpatrick 2021).\nOur approach leverages the DFM objective (Campbell et al. 2024, 2022) to learn a locally normalized energy score. This score allows us to evaluate generated rollouts and frame planning as an iterative denoising process. Energy guidance ensures that the optimization process stays at an energy minimum at each step (Goyal, Dyer, and Berg-Kirkpatrick 2021;"}, {"title": "4.2 Joint Denoising Model", "content": "Goal Generation. GenPlan addresses complex planning tasks by using instruction prompts or the environment's observations to define objectives. We achieve this by learning a goal distribution conditioned on observations o, which guides the action denoising process. Unlike existing methods (Chen et al. 2021, 2023) that rely on simulators for runtime goal positions and task proposals, GenPlan jointly learns the goal distribution, extracting a goal sequence g jointly with actions a during planning. This dynamic goal proposal promotes exploration and improves performance in long-horizon tasks by reducing error accumulation.\nState Sequence. In addition to goal generation, we also learn a state denoising model. We found that goal conditioning becomes ambiguous when the proposed goal is far from the agent's position. To prevent the agent from stalling or getting stuck in local regions, we learn the state sequence, which further aids in learning the action distribution.\nThese auxiliary modules are trained similarly to the action sequence (denoising based DFM) and can generalize to out-of-distribution tasks, provided the energy function is generalizable-successful trajectories receive low energy. Next, we briefly describe the processes involved in training and sampling from DFM (Campbell et al. 2024, 2022).\nForward Diffusion. We begin by corrupting samples drawn from the dataset D using the noise schedules (eq. 3a,3b). Specifically, we apply $E_{p_{data}(7^1),t\\sim U[0,1]}p_{mask} (7^t|7^1)$, where t is the CTMC timestep that controls the amount of corruption. The corruption process is applied to s, a, and g, with the jump rate controlled by t, though different sequences can evolve at varied rates if needed.\nBackward Diffusion. The denoising objective is applied at the trajectory level, making the transition from noise to data distribution challenging, particularly for longer horizons. This requires an iterative process. Using the corrupted tokens as input, we train the joint denoising model ($p_{\\theta|t}$) to approximate the true data distribution. We employ a negative-log-likelihood-based loss (eq. 5) to predict $s^1$, $g^1$, and $a^1$. As noted in (Campbell et al. 2024), the denoiser $p_{\\theta|1,t}(7^1 | 7^t)$ is independent of the choice of rate matrix used for sampling, $R_t(x_t, x'_t | x^1)$. This offers flexibility during sampling and simplifies the training. Following algorithm 2, we can sample the reverse CTMC to generate rollouts.\nObservation Conditioning. We employ a transformer-style architecture (Chen et al. 2021), using bi-directional masks as in (Devlin et al. 2018) to allow future actions to influence preceding ones. Observations o are encoded using FiLM (Feature-wise Linear Modulation) (Perez et al. 2018) to process images I, instruction prompts \u00a7, and agent positions. The context length is typically set to 1 but can be extended to increase the agent's memory (see figure 2B)."}, {"title": "4.3 Training Objective", "content": "The training objectives described in \u00a7 4.1-4.2 are used to learn the model parameters (\u03b8, \u03bb). Here, \u03b8 represents the parameters of the denoiser $P_{\\theta|t} po$ which is parameterized as a transformer, as discussed in \u00a7 4.2. The parameter $\u03bb \u2208 [0, 0]$ is the Lagrangian multiplier associated with the constraints in eq. 4b. We alternate gradient descent steps between updating $p_{\\theta|t}$ and $\u03bb$. In practice, $\u03bb \\to 0$, ensuring that the lower bound $\u03b2$ on entropy is satisfied (Zheng, Zhang, and Grover"}, {"title": "4.4 Planning", "content": "Once the denoising model has been trained, we can generate trajectories that approximate the data distribution by reversing the CTMC and interpolating noise back into the data through iterative denoising ($I_{max}$ iterations). The following algorithm outlines this process, based on the approach in (Campbell et al. 2024). This procedure is applied to all components of the trajectories."}, {"title": "5 Simulation Studies", "content": "We evaluate the performance of GenPlan in BabyAI (Chevalier-Boisvert et al. 2019) and MetaWorld v2 (Yu et al. 2020a; Wolczyk et al. 2021), focusing on the agent's adaptive and generalization capabilities. Additionally, we assess the significance of individual components within GenPlan. We implemented GenPlan using Python 3.8, and trained it on a 12-core CPU alongside an RTX A6000 GPU."}, {"title": "5.1 BabyAl", "content": "BabyAI offers a diverse set of tasks and environments (discrete) focused on planning and task completion.\nSimulation Setup. We conduct simulations in a modified BabyAI suite (Appendix B) following three paradigms.\n1. Trajectory Planning (TP). The agent navigates to one or more goals in a maze world, with map layouts, agent initialization, and goal positions varied in each run, evaluating GenPlan's generalization.\n2. Instruction Completion (IC). The agent operates in a multi-objective environment requiring complex decision-making, including exploration, object manipulation, key collection, and sequential goal completion. Task order and navigation are critical.\n3. Adaptive Planning (AP). The model, trained on single-goal plane-world tasks, is evaluated for zero-shot adaptation across different environments without additional fine-tuning. This includes testing the agent in increasingly complex environments, such as mazes with multiple goals and closed doors, where the agent must demonstrate door-opening and navigation skills. Additionally, we assess the model's ability to adapt to environments where it must unblock obstacles to succeed, as there is no alternative solution (see GoToSeqS5R2Un Table 2). This evaluation tests the robustness of the denoising procedure when conditioned on out-of-distribution (OOD) observations and its ability to recover successful trajectories under novel environments (see figure 1).\nThe agent can choose from six actions: left, right, forward, open, drop, or pick up, to navigate and interact with the environment. Success rates in reaching goals and completing tasks are reported across 250 novel environments unseen during training. In each run, the map layout, goals, obstacles, and agent positions are randomized. As described in Section 4.2, the planner uses an image observation and instruction to denoise the action sequences. Further environment and implementations details are available in Appendix B.3.\nBaselines. We evaluate both versions of GenPlan, named GenPlan-M and GenPlan-U, corresponding to the masked and uniform noise variants, respectively. These are compared against several baselines:\n1. LEAP (Chen et al. 2023): A MLM based multi-step planner, which has access to simulator-based goal-conditioning for sub-goals.\n2. LEAP GC: A variant of LEAP where simulator access is removed to evaluate its unconditional rollouts.\n3. LEAPH: A variant of LEAP that incorporates joint sequence prediction similar to GenPlan, while being constrained by a lower bound on entropy."}, {"title": "5.2 MetaWorld-v2", "content": "Simulation Setup. Our simulation setup closely follows that of (Schmied et al. 2023). We benchmark GenPlan on MetaWorld tasks, which encompass various robotic manipulation challenges. Both MDDT (Schmied et al. 2023) and GenPlan are trained on five tasks (In-Dist.) and evaluated on nine tasks, including four new tasks (Out-Dist.) specifically designed to test adaptation, consistent with the BabyAI experiments, we directly evaluate the zero-shot performance.\nBaselines and Results. We follow the pre-processing steps from MDDT (Schmied et al. 2023), with GenPlan utilizing a flow-based generative loss instead of the auto-regressive loss, as described in Section 4. We use the masking variant of GenPlan. During sampling, we follow return conditioned sampling (Chen et al. 2021). Although the results are comparable to MDDT, GenPlan-M achieves slightly better scores in adaptive tasks, due to its more robust"}, {"title": "6 Conclusion", "content": "We study the problem of learning to plan from demonstrations, particularly in scenarios where an autonomous agent must perform previously unseen tasks, including in unseen environments. We propose GenPlan, a generative behavioral planner that frames planning as iterative denoising. Through simulation studies, we demonstrate how joint denoising improves performance in complex and long-horizon tasks.\nLimitations. (1) The entropy lower bound $\u03b2$ in eq. 4b is currently a hyper-parameter that must be manually specified. (2) We assume access to near-optimal demonstration trajectories for training (Section 5.1); however, this assumption may not hold in all settings. Initial results show that GenPlan performs well even when trained on datasets with a mixture of sub-optimal demonstrations (Section 5.2). However, further studies are needed to assess its robustness to sub-optimality in demonstrations.\nFuture Work. To address the above limitations, we plan to extend GenPlan for online fine-tuning via hindsight experience replay (Zheng, Zhang, and Grover 2022; Furuta, Matsuo, and Gu 2022). Additionally, GenPlan offers a flexible and scalable framework that can be extended to multi-agent learning settings (Meng et al. 2022)."}, {"title": "A Organization of Appendix", "content": "The Appendix is organized as follows. Appendix B presents implementation details and additional results for the BabyAI environment. Appendix C covers the implementation and environment details for MetaWorld tasks. Finally, Appendix D briefly discusses sampling and the choice of rate matrix in algorithm 2."}, {"title": "B BabyAI Implementation Details.", "content": "B.1 Input and Networks.\nThe various inputs and their corresponding dimensions are presented in Table 4 for the BabyAI environments. We construct GenPlan using the transformer structure described in (Chi et al. 2023), where the inputs are: token embeddings for observations, states, and actions are generated through the following mappings (also see figures 2 and 7):\n$FiLM(I_{ctx}, S_{ctx}, \u00a7,k) \\to tok_o  fw_s(s_k) \\to tok_s  fw_a(a_k) \\to tok_a$\nThe observation token $tok_o$ is derived from the image I, state $s_{ctx}$, instruction \u00a7, and timestep k in the CTMC. The FiLM (Feature-wise Linear Modulation) function (Perez et al. 2018) transforms these inputs into an observation embedding. The context length, ctx, acts as a memory for the agent and provides past trajectories of length ctx. This embedding, along with the corrupted state $s_k$ and action $a_k$ tokens generated by the embedding networks $fw_s$ and $fw_a$, is fed into the transformer decoder stack. The transformer's cross-attention mechanism is employed in this joint denoising process. The decoder then predicts the true labels $s^1$, $g^1$, and $a^1$, with the observation tokens guiding the denoising process (figure 2). Consequently, we have H + 1 tokens in the cross-attention module, where the +1 corresponds to the CTMC timestep-based embedding, and 2H tokens serve as input to the transformer's decoder block. Further details on the input dimensions and the architecture are provided in Table 5."}, {"title": "B.2 Environments.", "content": "Table 6 provides detailed settings of the BabyAI environments. We evaluate three task paradigms, with tasks in IC and AP being particularly challenging. For example, in the BlockUnlock task, the agent must remove an obstacle by picking it up and repositioning it, then pick up a key to unlock the door, and finally proceed to the goal, which is always located in the locked room. Due to the complexity of these tasks, we observe that only GenPlan succeeds, while other baselines fail. Similarly, in AP tasks, the agent must generalize to more complex environments than those encountered during training. For additional details, see Figures 4, 5, and 6."}, {"title": "B.3 Baseline Models.", "content": "We use the official implementations for both LEAP (Chen et al. 2023) and DT (Chen et al. 2021). The BabyAI simulator is used to obtain the final goal and sub-goals for goal-conditioned planning in the baseline models. all the models are trained in an end-to-end fashion."}, {"title": "B.4 Additional Results and Discussion.", "content": "Coverage. The goal generation module (Section 4.2) effectively proposes sub-goals and goal regions, providing clear guidance to the agent. In contrast, the baseline models offer ambiguous guidance (see figure 3), which often results in agent stalling. However, GenPlan is able to efficiently explore the environment and demonstrate generalizability (see figures 8 and 9). This difference is particularly noticeable in adaptive tasks, as illustrated in figure 8.\nEffect of Iterative Denoising. We observe that increasing the number of iterations initially leads to higher success rates, but beyond a certain point, the success rates begin to decline (figure 10A). The maximum number of iterations, $I_{max}$, determines the number of CTMC steps required to invert the flow. To address this, we use H/2 iterations for denoising the trajectories, allowing roughly two dimensions to transition simultaneously in a single step (see Algorithm 2).\nEnergy Landscape. We investigate the energy assignment to trajectories with varying noise levels throughout training and observe that (a) GenPlan learns to reduce the energy for clean sequences and (b) increases the energy for corrupted trajectories (figure 10B). Since this denoising is done jointly, the energy measure provides an implicit sense of the goodness of an action at a particular state (also see figure 3). This helps verify that the objective in eq. 4a indeed facilitates denoising.\nRate of Convergence DFM. In the masking interpolant, the model easily identifies which indices are corrupted, simplifying the training process. This is evident through faster convergence rates. We report the correct action rate (i.e., the portion of the corrupted trajectory that the model can recover) and observe that masked denoising converges much faster (figure 11A). Consequently, the masked tokens receive the lowest attention scores. In contrast, it is relatively harder for the model to determine which tokens are corrupted in the uniform interpolant, leading to slower convergence.\nEntropy. Continuing our discussion from Section 5.1, we report success rates by varying the entropy lower bound. For TP and IC tasks, we observe that entropy levels above 0.3 offer little benefit. However, for AP tasks, we set the entropy to 0.7, as this facilitates task discovery (i.e., performing novel actions that are absent from the dataset) (see figure 11B).\nStochasticity in Environments. We extend GenPlan to handle stochastic environments. While (Paster, McIlraith, and Ba 2022) highlights that planning in stochastic settings often leads to suboptimal results, GenPlan can recover successful trajectories by denoising action sequences. As long as the objective function\u2014minimizing energy for successful trajectories-is maintained, GenPlan is able to generalize across various environments, even when accounting for stochastic factors.\nIn this experiment, the agent operates in an environment where it has a 20% chance that its chosen action (e.g., left or right) will be randomly mapped to another action with uniform probability. To address these stochastic settings, we replan after each step using single-step rollouts, unlike the multi-step rollouts used in the settings presented in Tables 1 and 2. We apply the same replanning strategy to the baseline models for a fair comparison.\nThe results in Table 8 show that GenPlan performs on par with baseline methods in stochastic environments, where both use feedback at each step. However, in adaptive environments, even with feedback, the baseline model (DT) fails, while GenPlan and LEAP succeed."}, {"title": "C MetaWorld Implementation Details.", "content": "C.1 Setup.\nEach episode lasts for 200 timesteps per task. In all our experiments on Meta-World, we report success rates and mean rewards.\nInput. To handle multi-domain learning and adapt to various tasks, we model the action space as a categorical distribution. We apply min-max tokenization for action discretization into 64 bins, as described in (Schmied et al. 2023). In MetaWorld, the state and action spaces are consistent across all environments (|S| = 39, |A| = 4).We use the dataset from (Schmied et al. 2023), collected through task-specific Soft Actor Critic (SAC) agents, resulting in demonstrations that range from random to"}, {"title": "D Rate Matrices and Sampling", "content": "Once the denoising model $p_{\\theta|t}$ is trained, we select a rate matrix to simulate the CTMC for generating samples (see Algorithm 2). Campbell et al. proposes the following rate definition as an initial choice (for simplicity, let us consider the case where H = 1):\n$R_t^+(x_k, j_k|x) := \\frac{ReLU (OtP_{t/1}(j_k|x) - dtP_{t/1}(x_k|x))}{Z_tP_{t/1}(x_k|x)}$,\nhere, ReLU(a) = max(a, 0) and $Z_t$ is the number of states with non-zero mass at t, defined as $Z_t = |\\{x : P_{t|1}(x|x) > 0\\}|$. Note that $R_t^+(x_k, j_k|x) = 0$ when $P_{t|1}(x|x) = 0$ or $P_{t|1}(j_k|x) = 0$. Additionally, when $x_k = j_k$, we have $R_t^+(x_k,x_k|x) = - \\sum_{j_k\\neq x}R_t^+(x_k, j_k|x).$"}]}