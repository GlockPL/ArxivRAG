{"title": "RECLAIMING RESIDUAL KNOWLEDGE: A NOVEL PARADIGM TO LOW-BIT QUANTIZATION", "authors": ["R\u00f3is\u00edn Luo (Jiaolin Luo)", "Alexandru Drimbarean", "James McDermott", "Colm O'Riordan"], "abstract": "This paper explores a novel paradigm in low-bit (i.e. 4-bits or lower) quantization, differing from existing state-of-the-art methods, by framing optimal quantization as an architecture search problem within convolutional neural networks (ConvNets). Our framework, dubbed CoRa (Optimal Quantization Residual Convolutional Operator Low-Rank Adaptation), is motivated by two key aspects. Firstly, quantization residual knowledge, i.e. the lost information between floating-point weights and quantized weights, has long been neglected by the research community. Reclaiming the critical residual knowledge, with an infinitesimal extra parameter cost, can reverse performance degradation without training. Secondly, state-of-the-art quantization frameworks search for optimal quantized weights to address the performance degradation. Yet, the vast search spaces in weight optimization pose a challenge for the efficient optimization in large models. For example, state-of-the-art BRECQ necessitates 2 \u00d7 104 iterations to quantize models. Fundamentally differing from existing methods, CoRa searches for the optimal architectures of low-rank adapters, reclaiming critical quantization residual knowledge, within the search spaces smaller compared to the weight spaces, by many orders of magnitude. The low-rank adapters approximate the quantization residual weights, discarded in previous methods. We evaluate our approach over multiple pre-trained ConvNets on ImageNet. CoRa achieves comparable performance against both state-of-the-art quantization-aware training and post-training quantization baselines, in 4-bit and 3-bit quantization, by using less than 250 iterations on a small calibration set with 1600 images. Thus, CoRa establishes a new state-of-the-art in terms of the optimization efficiency in low-bit quantization.", "sections": [{"title": "1 Introduction", "content": "ConvNets are favored as vision foundation models, offering distinct advantages such as the inductive bias in modeling visual patterns, efficient training, and hardware-friendliness. Network quantization is indispensable in enabling efficient inference when deploying large models on devices with limited resources. Representing floating-point tensors as integers significantly reduces the computational requirements and memory footprint.\nYet, low-bit quantization often leads to severe performance degradation. For example, the standard accuracy of a resnet18, pretrained on ImageNet , plummets to a mere 1.91% from 67.32%, with 4-bit weight-only quantization (WOQ), using min-max clipping. To tackle this issue, two research lines are undertaken: quantization-aware training (QAT) and post-training quantization (PTQ)."}, {"title": "2 Preliminaries", "content": "Dataset and classifier. Let (X, Y) be an image dataset where X denotes images and y denotes labels. We use Q(y|x; \u03b8) to represent a classifier, where \u03b8 denotes parameters. Q predicts the probability of a discrete class y given image x.\nQuantization. We use [W]n to denote the n-bit quantization of tensor W. The clipping range refers to the value range in quantization . We use two clipping schemes: (1) min-max clipping chooses the minimum and maximum values. (2) normal clipping chooses [\u00b5 \u2212 k \u00b7 \u03c3, \u03bc + k \u00b7 \u03c3], where \u03bc denotes the mean of the tensor, \u03c3 denotes the standard deviation of the tensor and k determines the range. Details are in Appendix A.\nKolda mode-n matricization and tensorization. Let Z \u2208 R^{I_1\u00d7\u2026\u00d7I_N} be a N-order tensor. The Kolda mode-n matricization of Z, denoted as Z(n), refers to unfolding Z so that: the n-th dimension becomes the first dimension, and the remaining dimensions are squeezed as the second dimension by \u03a0_{i\u2260n}I_i.\nLet Y \u2208 R^{I_n\u00d7J} (J = \u03a0_{i\u2260n}I_i) be a matrix. The mode-n tensorization of Y, denoted as Y[n,I\u2081\u00d7\u2026\u00d7In\u22121\u00d7In+1\u00d7\u2026\u00d7IN], refers to folding Y into the shape I\u2081 \u00d7 \u00b7\u00b7\u00b7 \u00d7 IN. Readers can further refer to the literature. Details are also provided in Appendix B.\nResidual convolutional operator. Let W \u2208 R^{m\u00d7n\u00d7k_1\u00d7k_2} be the weights of a convolution operator, where m denotes output channels, n denotes input channels and k\u2081 \u00d7 k2 denotes filter kernel size. We refer to W as convolutional operator for brevity. We use W \u2297 x to denote the convolution operation. Convolutional operators are linear operators. We refer to \u2206[W]n := W \u2013 [W]n as quantization residual operator, or residual operator if without ambiguity.\nTheorem 1 (Residual Convolutional Representation). Suppose a singular value decomposition given by: (\u2206[W]n)(1) = US_rVT (r = rank(S_r)). Then the factorization holds true:\nW \u2297 x = [W]_n \u2297 x + \u2211_{i=1}^{r}B_iA_i \u2297 x\nwhere A = (SV^T)_{[1, r\u00d71\u00d71]} and B = (US)_{[1, n\u00d7k_1k_2]}. The B \u2297 A is referred as r-rank residual operator. The proof is provided in Appendix C."}, {"title": "3 Method", "content": "We frame the optimal quantization as an architecture search problem. Suppose a L-layer floating-point ConvNet Q:\nQ(y|x; \u03b8) := Q(y|x; W(1),..., W(L))\nin which W(l) denotes the parameters of the l-th layer and \u03b8 := {W(1),..., W(L)}. The quantized Q with bit-width n is:\nQ(y|x; \u0189) := Q(y|x; [W(1)]_n,\u2026\u2026\u2026, [W(L)]_n)\nwhere \u0189 := {[W(1)]_n,\u2026\u2026, [W(L)]_n}.\nApproximating residual knowledge. According to Theorem 1, in the l-th layer, the residual operator \u2206[W(1)]_n is approximated by a r_l-rank residual operator:\nW(l) \u2297 x - [W(l)]_n \u2297 x = \u2206[W1)]_n \u2297 x \u2248 B^{(l)} A^{(l)} \u2297 x"}, {"title": "3.1 Differentiable relaxation", "content": "Equation (6) is not differentiable with respect to r. Solving the discrete combinatorial optimization problem in Equation (6) often entails iterative algorithms, e.g. evolutionary algorithms (EA) and integer programming (IP) . Nevertheless, the huge discrete search spaces remain a significant hurdle. For instance, the number of possible combinations of low-rank adapter sizes in a resnet50 is above 10^{18}.\nTo enable efficient optimization, firstly, we relax the r in Equation (6) from discrete integers to continuous values. Secondly, we differentiably parameterize the operations of choosing r, by using a high-order normalized Butterworth kernel. With these endeavors, Equation (6) is differentiable with respect to r. We are able to use standard gradient descent algorithms to efficiently optimize (e.g. SGD and Adam)."}, {"title": "3.2 Parameterized differentiable thresholding", "content": "Hard thresholding. Suppose S^(l) is the singular value matrix of [\u2206W^(l)]_n. Suppose S^(l) chooses the r_l largest values of S^(l):\nS^(l) = diag(\u03c3^(l)_1,...,\u03c3^(l)_R_l)\nS^(l) = diag(\u03c3^(l)_1,...,\u03c3^(l)_r_l,0,\u2026\u2026,0)  0 \u2264 r_l \u2264 R_l\nFormally, choosing the r_l can be formulated as the Hadamard product (i.e. element-wise product) of a thresholding mask matrix \u03a6^*(r_l) and S^(l) in that:\nS^(l) = \u03a6^*(r_l) \u2299 S^(l)\n\u03a6^*(r_l) = diag(1,..., 1, 0, ..., 0 ).\nr_l ones  R_l-r_l zeros\nWe refer to Equation (9) as hard-thresholding, which is not differentiable with respect to r.\nSoft thresholding. We differentiably approximate the hard-thresholding with a high-order normalized Butterworth kernel (NBK) . An k-order NBK with a cut-off rank r_l is a vector map \u03a6(r_l) : r_l \u2192 [0,1]^{R_l} defined by:\n\u03a6_i(r_l) := ( 1 / {1 + (i / r_l)^{2k}} )\nr_l < R_l\nr=1\nwhere n is the order. \nConverting residual operator to low-rank operator. In the l-th layer, we differentiably convert a high-rank residual operator \u2206[W(l)]_n into a low-rank operator B \u2297 A with rank r_l, by using Equation (10):\n\u2206[W]n \u2248 B \u2297 A\n\u2248 (U^{(l)} [\u03a6(r_l) \u2299 S^{(l)}]^{frac{1}{2}})[1,n\u00d7k_1k_2] \u2297 ([\u03a6(r_l) \u2299 S^{(l)}]^{(1)})^T [1,r_l\u00d71\u00d71]\nwhich is differentiable with respect to r_l."}, {"title": "3.3 Neural combinatorial optimization", "content": "By combining Equation (6) and Equation (7), the optimization loss is:\nL(r) := E_{(x,y)~(x,y)} {-log Q(y|x; \u00d5, \u03c6, r) + \u03bb \u00b7 exp{ReLU (w^T r \u2013 b)}}.\nThe optimal r = {r_1,\u2026\u2026,r_L} are found using gradient descent optimizers, e.g.SGD and Adam."}, {"title": "3.4 Tricks for stable optimization", "content": "Stable optimization for the proposed neural combinatorial optimization in Section 3.3 is challenging. We adopt several tricks to numerically stabilize the optimization process.\n\u2022 Gradient clipping. To stabilize the optimization, the solver clips the gradients into the range of [-0.2, 0.2].\n\u2022 Adaptive gradient. Equation (7) gives non-linear gradients with respect to ranks. Smaller ranks have smaller gradients towards zero, while larger ranks have larger gradients. We believe this design favors the stabilization of optimization.\n\u2022 Solution clamping. The solver clamps the ranges of the solutions after every gradient update, guaranteeing that the rank is not less than 1 and not greater than the limit R_l.\n\u2022 Anomaly reassignment. The solver detects numerical anomalies. If a NaN rank value is detected, it is replaced with rank 1."}, {"title": "4 Experiments", "content": "We conduct experiments from five aspects: (1) ablation study (Section 4.1), (2) comparing with state-of-the-art QAT and PTQ baselines (Section 4.2), (3) extensive evaluation (Section 4.3), (4) performance scalability with respect to model sizes (Section 4.4) and (5) hyper-parameter sensitivity (Section 4.5).\nReproducibility. We sample 1600 images from the ImageNet validation set as our calibration set while using the remainder as our validation set. We use normal clipping with k = 4.0 to quantize the main network and min-max clipping to quantize adapters. The order n of NBK is set to 4.0. The penalty coefficient \u03bb is set to 1. The batch size is 32. The target budget b is set to 5%, which results in a 1.25% increase in memory footprint with 8-bit quantization for low-rank adapters. The optimizer is Adam without weight decay. The learning rate is set to 0.01. We use a maximum 250 iterations for all experiments.\nTestbed. All experimental results, including the measured results of floating-point reference accuracy, are conducted on the M2 chip of a MacBook Air, equipped with a GPU of size 24 GiB. Due to the choice of the validation set, in tandem with the random seed and the hardware acceleration implementation in the testbed, the results of reference accuracy are slightly lower compared to the results from pytorch. However, this does not affect the results, we obtain using pytorch, for a fair comparisons with baselines. We report the results that we measured on our own testbed rather than using the results from the literature.\nEquivalent quantization bit-width. Let n and m be the quantization bit-widths of the main network and adapters. The equivalent quantization bit-width is given as: n + m \u00b7 b. For example, suppose n = 4, m = 8 and b = 5%, the equivalent quantization bit-width is 4.4-bits."}, {"title": "4.1 Ablation study", "content": "We conduct ablation experiments to show the design considerations in: (1) normal clipping is better than min-max clipping, (2) the results with optimal ranks outperform the results with heuristic choices, and (3) quantizing residual adapters with 8-bits does not affect performance. \nIntriguingly, we can quantize adapters while the performance remains almost unchanged. This can significantly reduce the amount of extra parameters which are used to retain residual knowledge."}, {"title": "4.2 Comparing with baselines", "content": "We compare our method against both state-of-the-art QAT and PTQ baselines. We choose four QAT baselines: PACT , LSQ , DSQ , and DoReFa-Net . We choose three PTQ baselines: AdaRound , AdaQuant , and BRECQ . We quantize the resnet18 and resnet50 (pre-trained on ImageNet) with 4-bits and 3-bits quantization.\nTop-1 accuracy. Our results achieve comparable performance against the baselines. The results are shown in Table 1. Optimization efficiency. Our method is more efficient by many orders of magnitude than state-of-the-art baselines. The results are shown in Figure 5 and Table 1. Notably, our method uses only 250 iterations with very minimum extra parameter cost. We have established a new state-of-the-art in terms of optimization efficiency."}, {"title": "4.3 Extensive evaluation", "content": "We show the results of extensive performance evaluation over multiple image classifiers pre-trained on ImageNet in Figure 9. Our results achieve comparable performance against the floating-point reference models with the differences within 2.5%."}, {"title": "4.4 Performance scalability", "content": "Figure 6 shows the performance scalability of CoRa with respect to model sizes, which are assessed by the numbers of filters. The result shows that the top-1 accuracy difference to floating-point models decreases with respect to the number of filters in models."}, {"title": "4.5 Hyper-parameter sensitivity", "content": "There are two hyper-parameters: the order n of the NBK in Equation (10) and the \u03bb in the loss function. We empirically investigate how their choices affect performance. The experiments are on a resnet18 pre-trained on ImageNet. Figure 7 shows that the NBK order k = 4 achieves best performance. Figure 8 shows that \u03bb achieves best performance between 0.5 and 1. It is notable that k and \u03bb are model-dependent."}, {"title": "5 Related work", "content": "Low-rank convolutional operator approximation. Low-rank approximation of convolutional operators is promising in accelerating the computations . However, convolution operations are not matrix multiplications. Conventional low-rank approximation, e.g. LoRa  and QLoRa  , fails to approximate convolutional operators. Relatively few works in the literature have explored this problem. Denton et al. decompose filters into the outer product of three rank-1 filters by optimization . Rigamonti et al. use rank-1 filters to approximate convolutional filters by learning . Jaderberg et al. reconstruct low-rank filters with optimization, by exploiting the significant redundancy across multiple channels and filters . A recent work, Conv-LoRA, approximates filters with the composed convolutions of two filters for low-rank fine-tuning on ConvNets . However, Conv-LoRA does not solve the problem of converting existing operators to low-rank operators without training. Previous works need to reconstruct low-rank filters by learning, thus they do not satisfy our needs. CoRa uses Theorem 1 to convert existing residual operators into low-rank operators without training."}, {"title": "6 Future work", "content": "CoRa introduces a novel paradigm in low-bit quantization and demonstrates significant optimization efficiency, with a new state-of-the-art result, as shown by our experiments compared to baselines. This paper exclusively investigates this paradigm on ConvNets. Future research will aim to explore this paradigm further from three aspects: (1) enhancing the performance of existing quantization methods (e.g. QAT and PQT) by reclaiming the residual knowledge using CoRa; (2) extending this paradigm to architectures beyond ConvNets, such as transformers; and (3) broadening the scope to more diverse tasks, including large vision models (LVMs) and large language models (LLMs)."}, {"title": "7 Conclusions", "content": "We explore a novel paradigm, in optimal low-bit quantization, differing from existing state-of-the-art methods, by re-framing the problem as an architecture search problem, of optimally reclaiming quantization residual knowledge. Thanks to significantly smaller search spaces of adapters, our method is more efficient yet achieves comparable"}, {"title": "Appendix", "content": null}, {"title": "A Uniform quantization", "content": "We formally introduce uniform quantization, which refers to the integer representations of floating-point tensors by taking the quantization intervals uniformly .\nSuppose:\n[.]: R^{I_1\u00d7I_2\u00d7\u2026\u00d7I_N} \u2192 Z^{I_1\u00d7I_2\u00d7\u2026\u00d7I_N}\nis an element-wise rounding operator in tensor space such as round(\u00b7), floor(\u00b7) or ceil(\u00b7) in pytorch .\nQuantization operator. Suppose:\n[\u00b7]_n : R^{I_1\u00d7I_2\u00d7\u2026\u00d7I_N} \u2192 Z^{I_1\u00d7I_2\u00d7\u2026\u00d7I_N}\nis a n-bit quantization operator which sends tensors from floating-point representations to n-bit integer representations.\nDe-quantization operator. We define the de-quantization operator as:\n[\u2022]1: Z^{I_1 \u00d7I_2\u00d7\u2026\u00d7 I_N} \u2192 R^{I_1 \u00d7I_2\u00d7\u2026\u00d7I_N}\nLet x \u2208 R^{I_1\u00d7I_2\u00d7\u2026\u00d7I_N} be some floating-point tensor. Let [\u03b1, \u03b2] \u2282 R be the quantization representation range (i.e. quantization clipping range) where \u03b1, \u03b2\u2208 R. Choosing the clipping range constant [\u03b1, \u03b2] is often referred as 'calibration' . Let s \u2208 R be some scale constant determined by quantization bit-width and representation range [\u03b1, \u03b2]:\n\u03c2(\u03b7; \u03b1; \u03b2) = {\u03b1-\u03b2}/{2^n-1}\nSuppose z \u2208 Z denotes the integer representation zero point (i.e. quantization bias), the quantization operator [\u00b7]n can be formulated as:\nx^* = [x]_n  def  \u230a{x}/{-\u03c2(\u03b7; \u03b1; \u03b2)}-z\u230b\nwhich represents \u00e6 into the range:\n\u230a{\u03b1}/{\u03c2(\u03b7; \u03b1; \u03b2)}-z\u230b, \u230a{\u03b2}/{\u03c2(\u03b7; \u03b1; \u03b2)}-z\u230b \u2282 Z.\nThe choices of z determine two schemes for uniform quantization: (1) Symmetric quantization scheme and (2) asymmetric quantization scheme (i.e. affine quantization) . For example, the scheme z := {\u03b1}/{\u03c2(\u03b7; \u03b1,\u03b2)} is dubbed as 'asymmetric quantization scheme' or 'affine quantization scheme' where the floating-point representation zero-point is mapped to the bias z. The scheme z := 0 is dubbed as \u2018symmetric quantization scheme' where the floating-point representation zero-point is mapped to zero.\nAccordingly, the de-quantization operator [\u00b7]\u00b9 can be formulated as:\nx = [x^*]^1 def  s(n; \u03b1, \u03b2) \u00b7 (x^* + z)."}, {"title": "B Mode-n tensor product", "content": "Let Z \u2208 R^{I_1 \u00d7 I_2 \u00d7\u2026\u00d7I_N} be a N-order tensor where I\u2081 denotes the i-th dimension size. A \u2018fiber' refers to the vector created by fixing N 1 dimensions. A 'slice' refers to the matrix created by fixing N - 2 dimensions.\nThe mode-n matricization of tensor Z is also known as tensor \u2018unfolding'. For example, a tensor with shape 8\u00d716\u00d73\u00d73 can be unfolded into a matrix with shape 8 \u00d7 144. The mode-n matricization of tensor Z is denoted as Z(n) by arranging the mode-n fibres of Z as columns. The mode-n product is known as tensor-matrix product. The mode-n product of tensor Z and matrix Y \u2208 R^{J\u00d7I_n} is defined as Z \u00d7_n Y  def  YZ(n).\nAccordingly, we also define the inverse mode-n tensorization (i.e. 'folding') as the inverse operation and denote it as Z[n, J\u2081\u00d7J2\u00d7\u2026\u00d7JK] where J1 \u00d7 J2 \u00d7\u00b7\u00b7\u00b7 \u00d7 JK denotes the folding dimensions of the unfolded dimensions in Z(n)."}, {"title": "CProof: Residual convolutional representation", "content": "We aim to prove:\nW \u2297 x = [W]_n \u2297 x + \u2211_{i=1}^{r}B_iA_i \u2297 x\nas stated in Theorem 1.\nHowever, the convolutional operations are not matrix multiplications. We can not directly use the results such as singular value decomposition (SVD). Strictly proving the Theorem 1 demands some efforts. We use the knowledge from tensor algebra to show that Theorem 1 holds.\nDefinition 1 (Unfolding operator). Let:\nT(k_1 \u00d7 k_2,s_1 \u00d7 s_2) : R^{n\u00d7w\u00d7h} \u2192 R^{n\u00b7k_1\u00b7k_2\u00d7w'\u00d7h'}\nbe the 'unfolding' operation which arranges some input with shape 'n \u00d7 w \u00d7 h' to shape 'n \u00b7 k\u2081 \u00b7 k2 \u00d7 w' \u00d7 h' for convolution operation with respect to kernel size k1 \u00d7 k2 and stride size s1 \u00d7 s2. In particular, if the stride size is 1 \u00d7 1, we simplify the notation as:\nT(k_1 \u00d7 k_2).\nSuppose x \u2208 R^{n\u00d7w\u00d7h}. For example, the operator with stride s1 \u00d7 s2 and no padding is defined by:\nT(k_1 \u00d7 k_2, s_1 \u00d7 s_2)(x)(c,u,v) := x(c mod (n\u00b7k_1 k_2),\u230a{u}/{s_1}\u230b, \u230a{v}/{s_2}\u230b)\nwhere c, u and v are indices. Clearly:\nT(1 \u00d7 1)(x) = x\nholds true as a particular case. Readers can refer to the implementation of the operation unfold in pytorch.\nLemma 2 (Tensor mode-n product factorization). Let Z \u2208 R^{I_1\u00d7I_2\u00d7\u2026\u00d7I_N} be some tensor. Let A \u2208 R^{R\u00d7I_n} and B\u2208 R^{J\u00d7R} be matrices. Below identity holds:\nZ\u00d7_n A\u00d7_n B= Z \u00d7_n (BA).\nLemma 3 (Convolution mode-n representation). Let W \u2208 R^{m\u00d7n\u00d7k_1\u00d7k_1} be the weights of some \u2018k1 \u00d7 k1' 2D convolutional operator where m denotes output channels and n denotes input channels. Let x \u2208 R^{n\u00d7w\u00d7h} be some input with size w \u00d7 h and channels n. According to the definition of 2D convolution, the convolution W \u2297 x can be represented as:\nW \u2297 x = T(k_1 \u00d7 k_2)(x) \u00d7_1W(1).\nTheorem 4 (Convolution factorization). Let Z \u2208 R^{m\u00d7n\u00d7k_1\u00d7k_2} be the weights of some 2D convolutional operator. Suppose:\nZ(1) = B(1)A(1) \u2208 R^{m\u00d7n\u00b7k_1\u00b7k_2}\nwhere A \u2208 R^{d\u00d7n\u00d7k_1\u00d7k_2} and B \u2208 R^{m\u00d7d\u00d71\u00d71} are the weights of two convolutional operators with kernel sizes \u2018k1 \u00d7 k1' and \u20181 \u00d7 1' respectively. Let x \u2208 R^{n\u00d7w\u00d7h} be some input. Using Lemma 3 and Lemma 2:\nZ\u2297 x = T(k_1 \u00d7 k_2)(x) \u00d7_1 Z(1)\n= T(k_1 \u00d7 k_2)(x) \u00d7_1 (B(1)A(1))\n= T(k_1 \u00d7 k_2)(x) \u00d7_1 A(1) \u00d7_1 B(1)\n= (A \u2297 x) \u00d7_1 B(1)\n= T(1 \u00d7 1)(A \u2297 x)B(1)\n= B \u2297 A \u2297 x."}, {"title": "Corollary 5 (Convolutional singular value decomposition).", "content": "Suppose:\nZ(1) = USVT = US (SV)T\nwhere:\nSS^\u2020 = S.\nSet:\nB(1) := US^\u2020\nand:\nA(1) := (SV)T.\nUsing Theorem 4:\nZ\u2297 x = (US)[1,d\u00d71\u00d71] \u2297 (SV)[1,n\u00d7k1xk2] \u2297 X.\nProof. We now show that the Theorem 1 strictly holds by using Theorem 4 and Corollary 5. Suppose W \u2208 R^{m\u00d7n\u00d7k_1\u00d7k_2} be the weights of some convolutional operator. Set:\nZ := \u2206[W]n\u2208R^{m\u00d7n\u00d7k_1\u00d7k_2}.\nConvolutional operators are linear operators. The convolution quantization residual representation is:\nW \u2297 x = [W]_n \u2297 x + \u2206[W]_n \u2297 X\n= [W]_n \u2297 x + (US)[1,d\u00d71\u00d71] \u2297 (SV) [1,n\u00d7k1xk2] \u2297 x.\nThere is nothing to do. Theorem 1 holds as demonstrated."}, {"title": "D Rank normalization coefficients", "content": "Suppose a model with L convolutional filters. Suppose the l-th layer has parameter size \u0398\u03b9. Suppose the adaptation of the i-th layer has parameters \u039e\u2081. The maximum parameter size of the l-th adapter is \u0398\u03b9.\nThe overall size of the adapters is given by:\n\u03a3_{i=1}^{L}{\u039e_i}/{\u0398_\u03b9}.\nNormalizing with respect to the overall model size:\n\u03a3_{i=1}^{L}\u0398_i.\nThus, the running budget is:\n{r_i}/{R_l} \u00b7 \u0398\u03b9 \u00b7 {1}/{\u03a3_{i=1}^{L}\u0398_i} < b.\nSet:\n\u03c9\u03b9 = {1}/{R_l} \u00b7 {\u0398\u03b9}/{\u03a3_{i=1}^{L}\u0398_i}\nwhich is referred as l-th layer rank normalization coefficient."}, {"title": "E Equivalent quantization bit-width", "content": "Suppose the l-th layer parameter size \u0398\u03b9. The the l-th layer adapter size is:\n{r_i}/{R_l} \u00b7 \u0398\u03b9.\nThe equivalent quantization bit-width \u00a7 is:\n\u03be = {\u2211_{i=1}^{L}n\u22c5\u0398\u03b9 +\u2211_{i=1}^{L} m\u00b7{\u039e_i}}/{\u2211_{i=1}^{L}\u0398\u03b9}\nSimplifying the Equation (48):\n\u00a7 = n + m. b."}]}