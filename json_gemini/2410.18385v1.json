{"title": "Link, Synthesize, Retrieve: Universal Document Linking for Zero-Shot Information Retrieval", "authors": ["Dae Yon Hwang", "Bilal Taha", "Harshit Pande", "Yaroslav Nechaev"], "abstract": "Despite the recent advancements in information retrieval (IR), zero-shot IR remains a significant challenge, especially when dealing with new domains, languages, and newly-released use cases that lack historical query traffic from existing users. For such cases, it is common to use query augmentations followed by fine-tuning pre-trained models on the document data paired with synthetic queries. In this work, we propose a novel Universal Document Linking (UDL) algorithm, which links similar documents to enhance synthetic query generation across multiple datasets with different characteristics. UDL leverages entropy for the choice of similarity models and named entity recognition (NER) for the link decision of documents using similarity scores. Our empirical studies demonstrate the effectiveness and universality of the UDL across diverse datasets and IR models, surpassing state-of-the-art methods in zero-shot cases. The developed code for reproducibility is included in the supplementary material.", "sections": [{"title": "1 Introduction", "content": "In information retrieval (IR), zero-shot learning is an essential problem that emerges when dealing with a new language or domain with little to no availability of the associated queries. Traditional IR methods primarily utilized sparse retrieval, while recent methods revolve around dense retrieval (DR), demonstrating the promising result (Neelakantan et al., 2022). Yet, using pre-trained DR directly on zero-shot cases results in substantial performance degradation, requiring dedicated fine-tuning (Izacard et al., 2021; Zhang et al., 2021).\nOne strategy for fine-tuning without relying on query traffic involves expanding the queries based on existing queries or documents with rule-based methods or language models (LMs) to obtain additional context in unseen domains (Wang et al., 2023, Jagerman et al., 2023; Weller et al., 2024). RM3 (Abdul-Jaleel et al., 2004) and AxiomaticQE (Yang and Lin, 2019) are classical ways to expand the queries with additional relevant terms while the recent studies indicate that large LMs (LLMS) can produce sophisticated synthetic data (Schick and Sch\u00fctze, 2021), often resulting in better transfer learning than human-curated datasets (Liu et al., 2022). While LLMs like Gemini (Team et al., 2023) generate superb synthetic queries for fine-tuning, devising a cost-effective way for IR remains challenging without additional recipes like dimensionality reduction (Hwang et al., 2023b).\nTo address the limitations of document-to-query generation, we propose a novel algorithm called Universal Document Linking (UDL), which offers an intuitive yet effective solution for zero-shot."}, {"title": "2 Motivation", "content": "Figure 1 illustrates the overall flow of fine-tuning a retrieval model in zero-shot scenario, where actual queries do not exist during fine-tuning. Instead, we use documents to generate synthetic queries, which aids the IR model in learning the distribution of the unseen domain (Thakur et al., 2021).\nAccording to Hwang et al. (2023a) and our initial findings (Table 11), merely increasing the size of synthetic data doesn't consistently improve results. This is because query augmentation associates a synthetic query with a single document, whereas queries in datasets can be linked to multiple documents. Our insight from this led us to develop a method to link similar documents for the generation of synthetic queries that cover multiple documents."}, {"title": "3 Universal Document Linking", "content": "Algorithm 1 outlines the procedural steps in the UDL. In the first step, denoted as A, the appropriate similarity model is selected for each dataset. We explore term frequency-inverse document frequency (TF-IDF) and pre-trained LM to derive document embeddings. Notably, TF-IDF considers lexical similarity, which is valuable for identifying unique features (e.g., disease like COVID), while pre-trained LM provides semantic similarity, aiding in contextual understanding. To determine the suitable similarity model, we initially compute TF-IDF scores for all documents, followed by calculating $D_M$ based on the Shannon entropy of terms using TF-IDF. Entropy values greater than 1 (i.e., numerator in $D_M$) describe high uncertainty since random variables have approximately uniform distribution in multiple classes. This concept is extended to the term entropy (Equation (2)) where we calculate the entropy for each term across documents.\nTo accommodate the $D_M$ for the massive documents, we introduce the y value where articles and relatively common terms are mostly distributed in entropy greater 1 as expected (see Table 12). Documents with an overwhelming presence of these terms are not desirable for TF-IDF since it can obscure the unique characteristics of documents, affecting link decisions. In such cases, considering semantically similar documents using pre-trained LM proves to be a more viable alternative.\nAfter defining the similarity model, we proceed to determine the criteria in step B for deciding whether candidate documents should be linked. Each dataset contains varying levels of domain-specific terminology, which must be taken into account during document linking. To address this, we initially translated non-English documents into English using Google Translator 2 to handle multilingual cases. After removing special characters, we compute $D_T$ based on the number of keywords extracted from NER models that are pre-trained on general ($N_g$) and specialized documents ($N_s$) while considering the vocabulary size of each NER for unbiased comparison. Note that a large size of vocabulary can have a higher chance of capturing broad keywords. The entity coverage is detailed in Table 9, where $N_g$ effectively identifies keywords in documents related to the natural conversation and question-answering (QA), while $N_s$ adequately finds keywords from professional jargon like medical and scientific claims.\nBased on this analysis, a higher value of $D_T$ indicates that a dataset is more similar to a group of general documents, enabling the linking of diverse documents without concerns of domain-specific jargon, resulting in a lower score (i.e., \u03b4). Conversely, a lower $D_T$ value suggests that a dataset consists of specialized documents, which benefits from linking similar documents that share domain-specific jargon, resulting in higher scores (i.e., 1 \u2013 \u03b4). Thus, general and specialized documents are considered opposites. In Section 4, we tested the UDL across multiple datasets from different domains (e.g., QA, scientific documents) to show its applicability without requiring a specific NER for each domain. This was confirmed with the selected NERs but our UDL could be readily extendable to any other NER.\nFinally, in step C, we calculate the cosine similarity between documents based on the model from step A and establish links when the similarity surpasses a score from step B."}, {"title": "4 Results and Discussions", "content": "Experimental Setup The details of the experimental setup are covered in Appendix A, where we empirically set two hyperparameters in UDL as y=0.7 and \u03b4=0.4, and reported the averaged NDCG@k (N@k) and Recall@k (R@k), along with the standard deviation (SD). For reproducibility, the training framework is covered in Appendix B, and the code is included in the supplementary material. Steps of fine-tuning are as follows: (1) Classifying linked and unlinked documents based on UDL, taking into account the order of the linked ones. (2) Feeding them as the inputs to the models and generating the synthetic queries with the same process as the original approach (e.g., model or prompt-based generations). (3) Fine-tuning the IR models based on generated queries.\nResearch Questions We aim to address four research questions (RQs): RQ1. What is the most suitable query augmentation method in zero-shot IR? RQ2. How does UDL enhance zero-shot IR? RQ3. How well does UDL generalize? RQ4. Is UDL competitive with state-of-the-art (SOTA)?\nMain Results Table 2 shows averaged results based on different query augmentations where we generated the same number of queries for each method. The overall trend of LM-based approaches outperforming simpler methods persists when UDL is added. However, a relatively parameter-efficient combination of UDL+QGen (218M) showed the best performance overall (RQ1), outperforming UDL+OpenLLaMA (3.1B). This promises significant savings of computational resources at scale. From our initial investigation, we found that OpenLLaMA tends to become more verbose after incorporating UDL, which may increase the risk of hallucination. In contrast, QGen generates more concise queries that are likely more accurate and relevant to the document. Additionally, we did not modify the LLM prompts based on UDL in this work, which presents a valuable future direction to optimize the prompts to better cover linked ones.\nFurthermore, we ablated the document merging mechanism of UDL by generating the synthetic queries from each document individually and mapping them to documents found by the linking procedure (Mapping+QGen in Table 2). While this still outperformed the corresponding baseline (QGen), it performed worse than complete UDL. This suggests that generating queries from the merged documents improves model generalization by introducing harder queries with increased ambiguity compared to the original. Indeed, Table 1 anecdotally shows that resulting queries fit both linked documents and are generally less specific. Besides, the linking mechanism itself provides a more exhaustive way of identifying positive query-document pairs, improving the performance (RQ2). Figure 2 illustrates this behavior: Distributions with UDL are more compact, have fewer outliers, and allocate higher ranks for relevant documents.\nLastly, we investigated the influence of decisions in UDL separately. We compared the results between fixed similarity models (i.e., TF-IDF or LM+QGen) and flexible ones (i.e., UDL+QGen) where the latter excels. Also, we tested the results by fixing the similarity scores (i.e., Fixed score (0.4) or Fixed score (0.6)+QGen) and LM where flexible scores from UDL enhances the performance. Therefore, our evolved approach with flexible choices of the similarity models and scores promises the results."}, {"title": "5 Conclusions", "content": "We propose a novel UDL to mitigate the limitations of conventional fine-tuning of IR models in zero-shot. UDL uses entropy and NER to tailor a linking method for each dataset with diverse tasks. Our comprehensive experiments show the effectiveness of UDL across various datasets and models."}, {"title": "6 Limitations", "content": "The proposed UDL offers significant advantages as an application. However, there are three possible limitations to consider. Firstly, while we consistently surpassed naive fine-tuning, there is an inherent limit to the enhancements. The performance of the retrieval model is influenced by the quality of synthetic queries. In general, the advanced pseudo-query generation methods manage multiple documents more effectively, indicating a valuable future direction to combine UDL with competitive pseudo-query generation approaches for further improvement. It also highlights the importance of selecting appropriate query augmentation strategies early in the project. Secondly, there is potential to introduce dynamic criteria, such as \u03b3 and \u03b4 in UDL, which were empirically defined in this study. Adjustments could be made for each candidate document, tailored to the similarities between documents and their types. Lastly, our comprehensive evaluation of UDL spanned ten datasets with diverse domains and languages (see Tables 3 - 5). There is a scope to extend this to larger documents and other languages, which was challenging due to computational resource constraints. These identified limitations present valuable research directions for those considering the proposed UDL in their applications."}, {"title": "A Setup", "content": "Databases We tested ten datasets where the summary of the database is shown in Table 7: NF-Corpus (Boteva et al., 2016) has automatically extracted relevance judgments for medical documents. SciFact (Wadden et al., 2020) consists of expert-annotated scientific claims with abstracts and rationales. ArguAna (Wachsmuth et al., 2018) contains the pairs of argument and counterargument from the online debate. SCIDOCS (Cohan et al., 2020) has seven document-level tasks from citation prediction, document classification, and recommendation. Climate-FEVER (Diggelmann et al., 2020) consists of real-world claims regarding climate-change with manually annotated evidence sentences from Wikipedia. TREC-COVID (Voorhees et al., 2021) contains the COVID-related topics with a collection of literature articles where biomedical experts measure the relevancy between articles and topics. Quora (Csernai, 2017) is built for identifying the duplicate question which is necessary for a scalable online knowledge-sharing platform. GermanQuAD (M\u00f6ller et al., 2021) is high-quality and human-labeled German dataset which includes the self-sufficient questions with all relevant information. ViHealthQA (Nguyen et al., 2022) consists of health-interested QA in Vietnamese. Multi-Aspect Amazon ESCI Dataset (MA-Amazon) (Reddy et al., 2022) has user queries for product search and long lists of product information like title, description, brand, color with four relevance labels.\nModels In this work, we considered the diverse sets of models where the summary of them is covered in Table 8: For query augmentation, we tested five pre-trained models: PEGASUS (Summarization) (Zhang et al., 2020), T5-Base (QGen) (Raffel et al., 2020) for English datasets, mT5-Base (QGen) (Xue et al., 2020) for Vietnamese and German databases, Flan T5-Base (Flan) (Chung et al., 2024), OpenLLaMA (Geng and Liu, 2023; Computer, 2023; Touvron et al., 2023).\nFor retrieval task, eight pre-trained retrieval models are experimented: M-Distilled USE (Yang et al., 2019), All-MPNet (Song et al., 2020), Distilled-BERT (Sanh et al., 2019), SGPT (Muennighoff, 2022), V-SBERT (Nguyen and Nguyen, 2020), V-SimeCSE (Gao et al., 2021), G-Electra (Clark et al., 2020), G-XLM-R (Conneau et al., 2020).\nFor pre-trained LM in similarity model, we employed three pre-trained models: All-MPNet"}, {"title": "Quality of Synthetic Queries", "content": "Algorithm 2 reveals the overall logic of quality checking based on the offered train set in NFCorpus and SciFact. We first found train data which covers same documents considered as linking in UDL. Then, we measured the cosine-similarity between the train query and relevant documents, and compared this with the cosine-similarity between the generated synthetic query and those same documents. If generated query has higher scores, this argues that our generated data has enough quality to link the single/multiple documents.\nFrom our analysis, 93% of generated queries properly maps both documents where it increases up to 99% for single document. Thus, most of queries generated from linked documents in UDL have the sufficient quality to map the relevant documents without additional quality control."}, {"title": "B Notes on Reproducibility", "content": "Total Computational Budget and Infrastructure used For UDL and fine-tuning the retrieval models, we employed the Intel(R) Xeon(R) CPU @ 2.20GHz and NVIDIA A100. All of them used RAM 80GB and we trained three times with different seeds to get the averaged results. For decision of similarity model, TF-IDF required about 34 seconds and LM needed about 174 seconds for 10K documents. For decision of similarity score, it took about 787 seconds for 10K documents. The query augmentation for 10K documents took about 6699 seconds for summarization, 2970 seconds for Flan, 12542 seconds for OpenLLaMA and 721 seconds for QGen. Other augmentations like random cropping and RM3 are fast enough to be negligible. Fine-tuning is affected heavily by the size of the model and synthetic queries. For example, it took about 20 seconds when training a 135M parameters model with 11K queries and 4K documents. Note that, these computational costs do not affect the inference time during retrieval. In all experiments, we mainly utilized the BEIR environment (Thakur et al., 2021; Kamalloo et al., 2023) to evaluate the retrieval performances."}, {"title": "C Term Entropy in UDL", "content": "Equation (2) explains the term entropy measurement used in UDL.\n$E(X) = - \\sum_{i=1}^{N} P(X_i) \\log_2 P(X_i)$ (2)\nwhere E is the entropy, X is the term, P(Xi) is the distribution of terms across documents, N is the number of documents."}, {"title": "Algorithm 1 Universal Document Linking", "content": "$DT = \\begin{cases} \\delta, & \\text{if } K_{N_s} \\times V_{N_s} > K_{N_g} \\times V_{N_g}  \\\\ 1 - \\delta, & \\text{otherwise} \\end{cases}$ (1)"}]}