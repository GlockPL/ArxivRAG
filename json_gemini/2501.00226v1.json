{"title": "Generative Emergent Communication: Large Language Model is a Collective World Model", "authors": ["Tadahiro Taniguchi", "Ryo Ueda", "Tomoaki Nakamura", "Masahiro Suzuki", "Akira Taniguchi"], "abstract": "This study proposes a unifying theoretical framework called generative emergent communication (generative EmCom) that bridges emergent communication, world models, and large language models (LLMs) through the lens of collective predictive coding (CPC). The proposed framework formalizes the emergence of language and symbol systems through decentralized Bayesian inference across multiple agents, extending beyond conventional discriminative model-based approaches to emergent communication. This study makes the following two key contributions: First, we propose generative EmCom as a novel framework for understanding emergent communication, demonstrating how communication emergence in multi-agent reinforcement learning (MARL) can be derived from control as inference while clarifying its relationship to conventional discriminative approaches. Second, we propose a mathematical formulation showing the interpretation of LLMs as collective world models that integrate multiple agents' experiences through CPC. The framework provides a unified theoretical foundation for understanding how shared symbol systems emerge through collective predictive coding processes, bridging individual cognitive development and societal language evolution. Through mathematical formulations and discussion on prior works, we demonstrate how this framework explains fundamental aspects of language emergence and offers practical insights for understanding LLMs and developing sophisticated AI systems for improving human-AI interaction and multi-agent systems.", "sections": [{"title": "I. INTRODUCTION", "content": "LANGAUGE evolves and changes over time as a result of decentralized human communications [1]-[4]. Sentences are generated to describe a wide range of phenomena, including external events, emotions, and intentions. In particular, the system of language is not static but dynamic [5]-[8]. As Peirce, the founder of semiotics, suggested, symbols, including language, can be characterized by a triadic relationship of sign, object, and interpretant [9]\u2013[11]. Here, sign corresponds to words, sentences and other signals. In particular, the correspondence between sign and object, which is signified by a sign (i.e., signifier), is determined by an interpretant. In other words, the meaning of a sign, that is, language, depends on culture and context, and so on.\nThe series of studies on emergent communication (EmCom), which is also referred to as emergent language (EmLang), and symbol emergence has attempted to explain the emergence of language and sharing of meanings of language among agents [2], [12]\u2013[16]. However, computational models and general theories that provide a comprehensive and integrative understanding of symbol and language emergence [8], [17] and address the interdependency between the two aspects are lacking: first, world modeling by agents, which depends on their embodiment and environmental adaptation [18], [19]; and second, language emergence, which involves the evolution of language structure reflecting structural knowledge of the world that becomes embedded in language through distributional semantics [20], [21]. Although we are entering the era of large language models (LLMs) and generative artificial intelligence (AI), a need for a theoretical framework that can explain the dynamic and semantic aspects of language emergence in embodied cognitive developmental systems still remains [8], [16], [22], [23].\nRecently, numerous studies have discussed how LLMs possess a model of the world. LLMs learn the distribution of word or linguistic token sequences and become intelligent to the extent that they can solve a wide range of tasks, including question answering, machine translation, and conversations [24], [25]. These capabilities are widely believed to be based on distributional semantics [20], [22]. However, the emergence of a latent structure that enables distributional semantics in our human language is not yet uncovered, though the mechanism through which neural networks find the hidden structure of language is gradually being revealed. Even before the invention of LLMs, word2vec, skip-gram, or continuous bag of words language models could internally form relative knowledge of concepts, for instance, \"London\" - \"UK\" + \"France\" ~ \"Paris\" [26], [27]. Two language models independently modeling different languages can perform unsupervised machine translation through structural alignment to a certain extent [28]. Recently, it has become clearer that LLMs have much knowledge of the world, including color similarity, physical properties of materials, and spatial knowledge [29]-[33]. This implies that the capability of LLMs is based on the mysterious nature of language. We believe that uncovering this nature of human language is the crucial mission of studies on symbol/language emergence and EmCom/EmLang.\nIn related fields, for instance, evolutionary robotics, cognitive and developmental robotics, artificial life, computational linguistics, and machine learning (ML), a wide range of studies on language evolution, EmCom, and symbol emergence have been conducted [2], [14], [17], [23], [34]. Most of these studies relied on specific types of language games, for instance, referential, signaling, and naming games. Since the late 2010s, empowered by the representation learning and language modeling capability of deep neural networks (DNNs), studies on EmCom have been boosted again [35]-[37]. However, most studies only treated the formation of communication protocols in specific game settings.\nThese approaches somehow failed to construct a general framework capturing symbol emergence from the viewpoint perspective of general principles of environmental adaptation, such as the free-energy principle (FEP), predictive coding (PC), and world modeling. Recently, world models have garnered significant attention as representation-learning models that incorporate action outputs and temporal dynamics of agent-environment interactions [18], [19], [38]. This aligns with broader theoretical frameworks such as PC and the FEP. PC posits that the brain constantly predicts sensory information and updates its internal models to enhance predictability [39], whereas FEP provides a more generalized framework explaining the self-organization of biological systems through minimization of free energy [40], which is associated with the idea of the Bayesian brain proposed by [41]. Notably, FEP extends beyond individual cognition to explain the self-organization of cognitive and biological systems in detail [42]\u2013[44], making it a promising foundation for understanding symbol emergence at both individual and collective levels.\nTo address these challenges, Taniguchi proposed the collective predictive coding (CPC) hypothesis [16]. The idea is based on a generative view of cognitive systems. The CPC assumes that not only individual agents but also groups of agents committing to symbolic communications can be modeled as generative models. Regarding individual agents, the FEP and active inference provide a generative view of a cognitive system. In other words, the CPC hypothesis extends the idea of the FEP and PC to a societal level [45]. The CPC hypothesis argues that symbol/language emergence can be modeled as decentralized Bayesian inference of a shared latent representation in a hierarchical manner.\nThis study aims to provide a theoretical view that bridges the idea of world models, that is, internal models for the sensorimotor observations and dynamics, and the emergence of language. To this end, we extend the idea of the CPC hypothesis and formalize the framework of generative emergent communication (generative EmCom) by introducing a generative view to the existing framework of EmCom. This allows us to develop a more integrative general theory of multi-agent systems performing EmCom and symbol emergence systems.\nThe main contributions of this paper are as follows:\n1) Propose a new framework of EmCom known as generative EmCom, demonstrating that the emergence of communication in multi-agent reinforcement learning (MARL) can be derived from the perspective of control as inference (CaI) within the framework of generative EmCom, and clarify the relationship between conventional discriminative EmCom and the new generative EmCom.\n2) Present the mathematical framework showing how LLMs become a collective world model in a specific sense through collective PC.\nThe remainder of this paper is organized as follows: Section 2 introduces the framework of generative EmCom, formalizing the collective PC hypothesis and its mathematical foundations. Section 3 demonstrates the formulation of language games as decentralized Bayesian inference, with a focus on the Metropolis-Hastings naming game (MHNG). Section 4 explores the application of generative EmCom to MARL, showing how it facilitates cooperative behavior. Section 5 discusses the relationship between the proposed generative framework and conventional approaches to EmCom. Section 6 examines LLMs through the lens of collective world models, providing new insights into their capabilities and limitations. Finally, Section 7 concludes the paper and discusses future directions"}, {"title": "II. GENERATIVE EM\u0421\u043e\u043c", "content": "A. Collective predictive coding\nThe CPC hypothesis was proposed to explain the emergence of symbol systems, particularly language [16], [46]. The CPC extends the idea of PC and FEP from the individual cognition level to a societal level. Taniguchi et al. were the first to formulate the CPC from the perspective of FEP and active inference [45]. This suggests that symbol emergence follows a process of minimizing free energy across the entire multi-agent system.\nAlthough PC theory suggests that individual brains constantly predict sensory information and update their internal representations, including world models. CPC suggests that a group of agents, for instance, a human society, predict sensory information of all of the agents and update its external representations, that is, symbol systems.\nA question is raised. How can we update the external representations, e.g., language, while our brains are disconnected physically? The CPC hypothesis suggests that a type of language game performs a decentralized Bayesian inference among the group (e.g., [47], [48]). In this framework, language games (such as naming games) can be interpreted as implementing decentralized Bayesian inference of shared representations. A representative example is MHNG explained in Section II-C. The CPC hypothesis argues that symbol systems emerge as a result of decentralized Bayesian inference performed collaboratively by multiple agents.\nAlthough the encoding of sensory information through internal representations is ensured by the plasticity of neural systems, the plasticity of external representations is guaranteed by the flexibility of our symbol systems. The arbitrariness of symbol systems is a widely recognized characteristic of symbols in semiotics [11]. Peirce referred to the process by which subjects assign meaning to symbols according to culture and context as the semiosis. Although our brains are physically and electrically separated, they are informationally connected through communication using a flexible symbol system. Therefore, with appropriate communication and symbol system update algorithms, we can encode information into the symbol system as an external representation. In fact, the CPC hypothesis can consider that humans collectively perform this action in language emergence.\nThis implies that language collectively encodes information about the world as observed by numerous agents through their sensory-motor systems. The CPC hypothesis study [16] did not provide a clear and detailed explanation regarding this point while proposing a new perspective on why LLMs seem to possess knowledge about the real world. This is one of the main topics of this paper.\nEssentially, CPC hypothesizes that human language is formed through a process of collective PC, where the symbol system emerges to maximize the predictability of multi-modal sensory-motor information obtained by members of a society, that is, minimize the collective free energy of a group of agents. This approach provides a unified framework for understanding symbol emergence, language evolution, and the nature of linguistic knowledge from the perspective of environmental adaptation and brain science."}, {"title": "B. Generative EmCom", "content": "Numerous studies on EmCom/EmLang have been conducted. Major approaches are language games, MARL, and iterated learning models (ILMs) [6], [12], [49]\u2013[51]. Recently, EmCom studies based on language games, particularly referential games, have become a dominant approach in the research community of EmCom. These studies utilize the representation learning capability of DNNs, give agents language encoders and decoders, and make them learn to generate and interpret a complex sequence of tokens to identify its meaning. In a typical setting of referential games, a speaker agent encodes a given object to a sequence of tokens using a neural network, for instance, long short-term memory, and the listener agent decodes it and identifies the object among some other items. This approach considers EmCom as an optimization of communication protocol. Generally, this is in line with Shannon's communication model [52]. This approach focuses less on the synergy between the perceptual and communicative aspects of language. This approach can be regarded as a discriminative model-based approach to EmCom/EmLang.\nUnlike traditional EmCom models, generative EmCom models consider language emergence as a generative model. In the simplest case, a probabilistic graphical model (PGM) of generative EmCom based on CPC hypothesis can be described mathematically as follows:\nGenerative model:\n$p({x_k}, {z_k}, m) = p(m) \\prod_k p (x_k | z_k) p (z_k | m)$ (1)\nInference model:\n$q (m, {z^*}_k | {x^*}_k) = q (m | {z}k) \\prod_k [q (z_k|x_k)$ (2)"}, {"title": "C. Language Game as Decentralized Bayesian Inference", "content": "The hypothetical argument that language game can perform the decentralized Bayesian inference has a computational basis though whether actual language communication can realize such decentralized Bayesian inference in our human society is an open question. The MHNG is an instance of this idea. The MHNG comprises the following steps:\n1) Perception: Speaker and listener agents (Sp and Li) observe the d-th object, obtain $x_d^{Sp}$, and $x_d^{Li}$ infers their internal representations $z_d^{Sp}$ and $z_d^{Li}$, respectively.\n2) MH communication: Speaker mentions the name $m_d^{Sp}$ of the d-th object by sampling it from $P(m_d^{Sp}|z_d^{Sp}, O_{SP})$. The listener determines whether it accepts the naming with probability $\\gamma \\triangleq \\min \\Big(1, \\frac{P(z_d^{Li}|m_d^{SP})}{P(z_d^{Li}|m_d^i)}\\Big)$.\n3) Learning: After MH communication was performed for every object, the listener updates its global parameters $\\Theta^{Li}$ and $\\Psi^{Li}$.\n4) Turn-taking: The speaker and listener alternate their roles and go back to (1).\nIt has been demonstrated that the MHNG is equivalent to the Metropolis-Hastings algorithm for inferring latent variables in a probabilistic generative model. This model conditions the internal representations $z_k$ of multiple agents, acting as representation learning machines, on a common external representation m. Although the original study assumed two agents and a categorical message m, the core PGM of generative EmCom underlying this theory does not make these assumptions. Consequently, this fundamental idea can be extended in various ways.\nAs described, unlike referential games, MHNG assumes a joint attention performed by two agents. This assumption may seem strange from the game-theoretic approach to EmCom, such as referential games. However, developmental studies suggest that joint attention is fundamental to human infant language acquisition. Human infants develop joint attention capabilities before vocabulary explosion occurs, and joint attention serves as a crucial foundation for language acquisition [7]. Gergely et al. highlighted that when children incorporate parental instructions into their learning process, they presuppose that parents have the intention to teach them [53]."}, {"title": "III. EMCOM FOR MULTI-AGENT COOPERATION", "content": "A. Generative EmCom for Multi-agent Reinforcement Learning\nCommunication and language are often considered to emerge to facilitate multi-agent cooperation. In recent years, studies on MARL with communication channels have been progressing. Initial methods in multi-agent deep RL include DIAL [63] and CommNet [64]. These methods connect the networks of agents through messages, enabling the learning of necessary messages for cooperative behavior through backpropagation. Additionally, multi-agent deep deterministic policy gradient (MADDPG), an"}, {"title": "B. Generative EmCom on World models", "content": "The generative EmCom for MARL can be extended to involve representation learning in the same way as we discussed in Section 2.3. This extension implies the integration of the idea of world models into generative EmCom.\nThe concept of a world model represents an internal model within an agent that captures the dynamics of environmental states, their responses to the actions of the agent, and their relationships with sensory inputs [18], [19], [79]. The concept of world model has its origins in the early days of AI and robotics studies [80]. Initial studies on ML investigated techniques for agents to autonomously construct and adapt their world models [81], [82]. Currently, the term generally refers to"}, {"title": "IV. RELATIONSHIP WITH CONVENTIONAL EMCOM", "content": "Recent years have witnessed an increasing presence of research papers focused on EmCom at ML conferences such as NeurIPS and ICLR. This trend underscores the renewed interest in EmCom. In the field of EmCom, Lewis' signaling game [87] framework is often used as a model of communication while other various formulations have also been proposed [35]\u2013[37]."}, {"title": "V. LARGE LANGUAGE MODEL AS COLLECTIVE WORLD MODELS", "content": "A. Our argument\nBased on the discussion in Sections 2 and 3, we explain the main argument of this paper, stating that LLMs are collective world models.\nTo clarify the link between this argument and existing studies relating language and perceptual information, Figure 5 shows three levels of tasks relating language to visual information (an example of multimodal sensory information) and action streams, along with PGMs for each. These represent (A) image captioning and generation tasks, corresponding language to a still image [113]\u2013[115]; (B) video captioning and generation tasks, corresponding language to a video, that is, a sequence of visual stimuli [116], [117]; and (C) action-dependent video captioning and generation, corresponding language to dynamic perceptual and action information, respectively2. In particular, from the perspective of representation learning, (A) and (B) correspond to representation learning of images and videos, whereas (C) corresponds to world models explained in Section III-B. Language-conditioned world models and robotics foundation models, known as vision-language-action models, studied in robotics and autonomous vehicles correspond to (C) [118]-[122].\nCorresponding to the three levels of captioning and generation tasks, we can consider generative EmCom. Figure 5 describes PGMs for generative EmCom corresponding to the three levels, respectively. Notably, they are only an instance of the generative model shown in the CPC hypothesis (Figure 1) and generalization of studies based on the MHNG described in Sections 2 and 3.\nHere, $\\Theta_k$ represents the global parameters of the k-th agent, for instance, the parameters of neural networks for representation learning. In (C), the global parameter $\\overline{\\Theta}_k$ includes not only the parameters of the prediction model of the world (i.e., Equations 9 and 10 ) but also those of policies $p(a_t|z_t)$. Let us consider (C) as a generative model corresponding to a single super-agent, virtually. The super-agent has multimodal sensory information combining the observations ${x_{k:t}}$ and actions ${a_{k:t}}$ of every agent. In this case, (C) can be considered as a type of hierarchical world model combining K agents and integrating their experiences. We can refer to the world model owned by the super-agent as a collective world model, where {$\\Theta, {\\Theta_k}_k$} are its parameters.\nFrom this perspective, the corpus data that comprise sen- tences uttered by agents are considered as a sample from ap- proximate posterior distributions $m^{[i]} \\sim q(m|{x_{k,1:t}, a_{k,1:t}}_k)$, where $m^{[i]}$ denotes the i-th sentence describing the j-th observation ${x_{k,1:t}, a_{k,1:t}}$.\nLanguage modeling implies approximating the distribution of word sequences $p(m|\\phi_k)$ in a corpus, where $\\phi_k$ denotes a parameter of the language model of the k-th agent. This implies that the language model approximates the posterior distribution $qm|{x_{k,1:t}, a_{k,1:t}}_k) \\approx p(m|\\theta^{LM})$.\nThe rationale behind distributional semantics as modeled by LLMs can be comprehended through the CPC framework. When language emerges through CPC, it functions as a latent space that develops through representation learning based on the collective experiences of all agents in this case, humans. These experiences encompass not only external sensory infor- mation but also interception and behavioral decision-making. Within this framework, sentences can be mathematically considered as samples drawn from the posterior distribution over messages m, formally expressed as $q(m|{x_k,a_k}_k)$, where $x_k$ and $a_k$ represent the observations and actions of agent k, respectively. This formulation naturally leads to the structural reflection of the distribution of ${x_k, a_k}_k$ in that of m, analogous to the working mechanism of representation learning in individual agents.\nA key insight is that language serves as a shared external representation that effectively combines the world models of multiple human agents. When examining the graphical model depicted in Figure 6 (C) as a cognitive model of a single \"super-agent,\" we can interpret it as a factorized hierarchical world model incorporating K distinct modalities. Through this lens, an LLM that models the distribution $q(m|{x_k, a_k}_k)$ can be considered as a collective world model one that integrates"}, {"title": "B. Existing argument", "content": "LLMs learn to predict text tokens in an autoregressive manner, and unlike human cognition or prediction, they are not designed to interact with environments or achieve specific goals beyond word prediction. Nevertheless, some studies have"}, {"title": "VI. CONCLUSION", "content": "This study proposed a theoretical framework that unified EmCom, world models, and LLMs through the lens of CPC. We introduced the concept of generative EmCom as an alternative formulation of the conventional EmCom, which is based on a discriminative model-based language game such as referential games, and described their relationships. The concept, generative EmCom, is based on the CPC hypothesis,"}]}