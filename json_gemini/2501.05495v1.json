{"title": "LSEBMCL: A Latent Space Energy-Based Model for Continual Learning", "authors": ["Xiaodi Li", "Dingcheng Li", "Rujun Gao", "Mahmoud Zamani", "Latifur Khan"], "abstract": "Continual learning has become essential in many practical applications such as online news summaries and product classification. The primary challenge is known as catastrophic forgetting, a phenomenon where a model inadvertently discards previously learned knowledge when it is trained on new tasks. Existing solutions involve storing exemplars from previous classes, regularizing parameters during the fine-tuning process, or assigning different model parameters to each task. The proposed solution LSEBMCL (Latent Space Energy-Based Model for Continual Learning) in this work is to use energy-based models (EBMs) to prevent catastrophic forgetting by sampling data points from previous tasks when training on new ones. The EBM is a machine learning model that associates an energy value with each input data point. The proposed method uses an EBM layer as an outer-generator in the continual learning framework for NLP tasks. The study demonstrates the efficacy of EBM in NLP tasks, achieving state-of-the-art results in all experiments.", "sections": [{"title": "I. INTRODUCTION", "content": "Label prediction for continuously occurring instances is crucial in practical applications like online news summaries, product classification, and dialogue learning systems. To address these scenarios, models must acquire, fine-tune, and transfer knowledge over time, a concept referred to as continual learning [1]. Continual Learning (CL) aims to create systems that can rapidly acquire new skills and integrate them with prior knowledge, mimicking human learning. A key challenge in CL is catastrophic forgetting, where models forget previously learned knowledge when training on new tasks.\nApproaches to mitigate catastrophic forgetting can be categorized into: (1) storing exemplars from previous tasks; (2) parameter regularization during fine-tuning; and (3) task-specific parameter allocation. These methods aim to retain prior knowledge while learning new tasks. Our method prevents forgetting by sampling data from previous tasks using an Energy-based Model (EBM) during training. The EBM is first trained on each task, enabling the retention of prior knowledge and improving performance on subsequent tasks.\nAn EBM associates scalar energy values with input data points, assigning lower energy to more likely inputs and higher energy to less likely ones. EBMs are applicable to various tasks, including classification and regression. For instance, Pang et al. [2] used EBMs in the latent space to enhance the expressivity of generative models, demonstrating its utility in improving latent space structure for both generation and classification.\nDespite advancements in CL, EBM applications in this domain remain limited. For example, [3] applied EBMs for classification in complex CL scenarios like boundary-agnostic and class-incremental learning. Unlike their approach, which uses EBMs as the core model, we employ EBMs as an external sampling mechanism, simplifying updates and preserving flexibility. Additionally, while their work focuses on computer vision datasets, our study applies EBMs to natural language processing datasets.\nOur proposed method, LSEBMCL, makes three key contributions: (1) integrating an EBM layer into a continual learning framework for NLP tasks for the first time; (2) addressing catastrophic forgetting in NLP tasks using EBM; and (3) achieving state-of-the-art performance across experiments, demonstrating the effectiveness of our approach."}, {"title": "II. RELATED WORKS", "content": "A. Continual Learning\nContinual learning involves sequential tasks and is particularly relevant in scenarios where data arrives in a non-i.i.d. manner and new tasks emerge. However, deep neural networks face the challenge of catastrophic forgetting, which hinders their ability to retain prior knowledge. Current continual learning methods fall into three categories: (1) Replay methods [4], [5]; (2) Regularization-based methods [6], [7]; and (3) Parameter isolation methods [8], [9]. Replay methods either store raw samples or generate pseudo-samples using generative models. For instance, iCaRL [10] stores class exemplars, and GEM [11] uses gradients from previous tasks to constrain updates. LAMOL [12] reduces forgetting by generating artificial examples of previous tasks. Regularization-based methods avoid storing raw inputs, introducing regularization terms to consolidate knowledge. LwF [13] uses model outputs as soft labels, while MAS [14] estimates parameter importance for adaptation. IDBR [15] applies disentanglement-based regularization for text classification, and LPC [16] combines parameter calibration with logit preservation. Parameter isolation methods allocate distinct parameters per task to prevent forgetting, as seen in PackNet [17] and HAT [18]. Our method employs replay-based EBMs to generate artificial samples for previous tasks.\nB. Energy-based Models\nEnergy-based models (EBMs) represent probability density functions via an energy function E(x), mapping realistic points to low energy values and unrealistic points to high values [19]. EBMs offer simplicity, stability, and parameter efficiency. Recent advancements enable EBMs to model high-dimensional data [20], [21], and latent space EBMs [2] improve model expressivity for tasks such as text generation and trajectory modeling. EBMs have been shown to prevent catastrophic forgetting in continual learning [3]. Unlike [3], which uses EBMs as the primary model for classification, our method employs EBMs as an outer-generator for tasks such as classification and text generation. Other applications of EBMS include joint training with pretrained text encoders to enhance calibration [22] and leveraging low-dimensional structures for anomaly detection [23]."}, {"title": "III. METHODOLOGY", "content": "We initiate the process with a pre-trained base model, specifically the latest Large Language Model (LLM) Mistral 7B. [24]. We have developed the LSEBMCL model, which consists of four main components. The initial component is the Inference Network, followed by Operator 1, Operator 2, and finally, the Energy Function. In the subsequent sections, we will provide a detailed introduction to each of these components individually.\nA. Inference Network\nFollowing decaNLP [25], we make the datasets pre-processed as a QA (Question Answering) problem. First, we convert continual learning tasks to a unified QA format as:\n$< x, y > \\in D$ (1)\nwhere x is the Question, y is the Answer, and D is a training set of QA. We target handling diverse tasks, covering Question Answering (QA), Natural Language Inference (NLI), Sentiment Analysis (SA), Semantic Role Labeling (SRL), etc. We introduce an inference network, denoted as $A_{\\Psi}(x)$, which is also referred to as the \"energy-based inference network\" (as depicted at the bottom of Figure 1). This network is parameterized by \u03a8 and trained with the objective of achieving the following goal:\n$A_{\\Psi}(x) \\approx \\underset{y \\in Y_R(x)}{\\operatorname{argmin}} E_{\\Theta}(x, y)$ (2)\nwhere $Y_R(x)$ are the true labels. Specifically, we train the inference network parameters \u03a8 as follows (assuming \u0398 is the parameters of EBM):\n$\\Psi^* = \\underset{\\Psi}{\\operatorname{argmin}} \\sum_{<x,y> \\in D} E_{\\Theta}(x, A_{\\Psi}(x))$ (3)\nB. Operator 1\nWe employ two operators $o^1$ and $o^2$ that are used to map zt logits into distributions for use in the energy. As shown at the middle of Figure 1, we seek an operator $o^1$ to modulate the way that logits zt output by the inference network is fed to the decoder input slots in the energy function. $o^1$ is the operation for feeding inference network outputs into the decoder input slots in the energy.\nC. Operator 2\nAn operator $o^2$ to determine how the distribution $p_{\\Theta}(\\cdot | ...)$ is used to compute the log probability of y. $o^2$ is the operation for computing the energy on the output. Explicitly, then, we write each local energy term as\n$e_m(x, y) = -o^2(z_m) \\log p_{\\Theta}(\\cdot | o^1(z_m), x_m)$ (4)\nOur objective is to minimize the aforementioned energy function concerning the variable $z_t$ in our inference networks. The softmax operation is selected for $o^1$ and $o^2$.\nD. Energy Function\nFigure 1 shows the proposed latent space EBM continual learning model. We design an EBM layer to play the role of outer-generator. After training each task, the EBM model generates samples based on data from previous tasks before training on the new task. During training on the new task, the model not only trains a model on the new data but also trains on the extra data generated from previous tasks by the EBM model as shown at the top of Figure 1. The equation is shown below:\n$E_e(x, y) = \\sum_{m=1}^{M} e_m(x, y)$ (5)\nwhere \u0398 is the parameters of the EBM. m means the index of the task and M means the total number of tasks. $e_m(x, y)$ is calculated using the following equation:\n$e_m(x, y) = - \\log p(y_m | x_m)$ (6)\nIn Figure 1, for each task, we have:\n$p_{\\Theta}(x, z) = p_{\\alpha}(z) p_{\\beta}(x | z)$ (7)"}, {"title": "IV. EXPERIMENTS", "content": "In this section, we assess the performance of our model on various tasks. In our experiment, we focus on task-incremental learning. We conduct a comparison of our method with eleven different techniques. All the methods use Mistral 7B as the backbone pretrained large language model. In the continual learning scenario, we train the model sequentially on a series of distinct tasks, following a predetermined order. After each training phase, we assess the model's performance on all previously encountered tasks.\nA. Experimental Setup\n1) Tasks, Datasets, and Metrics: We collect datasets for five different tasks related to natural language processing mentioned in decaNLP [25], including question answering, semantic parsing, sentiment analysis, semantic role labeling, and goal-oriented dialogue. To compare our method with [26], we also conduct experiments on four text classification tasks: news classification, sentiment analysis, Wikipedia article classification, and question-and-answer categorization with five datasets, following the same procedure for producing equal-sized datasets. Due to limited computational resources, we did not train on all datasets. We use a corresponding evaluation metric for each task. The scores for the metrics range between 0 and 100%.\n2) Methods for Comparison: The paper discusses various approaches to tackling the problem of catastrophic forgetting in continual learning, where a model trained on a sequence of tasks tends to forget the previous tasks when trained on subsequent tasks. The approaches considered in the paper include fine-tuning, multi-task learning, replay methods and architecture-based methods LAMOL, RVAE-LAMOL [27], HMI-LAMOL [28], PMR [29], regularization-based methods such as Online EWC [30] and MAS [14], Gradient Episodic Memory (GEM) [11], Improved Memory-Based Parameter Adaptation (MBPA++) [26], IDBR [15], and other methods like ProgPrompt [31]:\n(1) LSEBMCL: Uses top-k sampling with k = 1. LSEBMCLGEN denotes a sampling ratio \u03b3, applying the same GEN token across tasks. (2) LAMOL: Employs k = 20 for top-k sampling and \u03bb = 0.25 for LM loss weighting. (3) RVAE-LAMOL: Enhances LAMOL with a residual variational autoencoder. (4) HMI-LAMOL: Adds hippocampal memory indexing to improve generative replay via compressed features. (5) PMR: Stores minimal samples for efficient continual learning. (6) Fine-tuning: Sequentially trains tasks without task interaction. (7) Multitask learning: Trains all tasks simultaneously, serving as a continual learning upper bound. (8) Regularization methods: Includes Online EWC and MAS for mitigating forgetting. (9) GEM: Randomly samples 5% of prior task data for gradient calculation. (10) MBPA++: Combines sparse experience replay with local adaptation. (11) IDBR: Uses disentanglement-based regularization for text classification. (12) ProgPrompt: Prevents catastrophic forgetting without data replay or extensive task-specific parameters.\nB. Experimental Results\n1) SST, QA-SRL, and WOZ Tasks: To gain a preliminary understanding of the effectiveness of the different methods and the impact of the task order, we conducted an experiment on three small datasets: SST, QA-SRL, and WOZ. We trained all methods except for the multitasked method on six different orders of tasks. We evaluated the model's final score after training on each order, and the results are presented in Table II. Based on the results, we made several observations. We observed several things as follows: (1) Fine-tuned, EWC, MAS, GEM, LAMOL, and RVAE-LAMOL had worse performance than LSEBMCL even with \u03b3 = 0 and significantly worse than LSEBMCL with \u03b3 > 0. (2) LSEBMCLGEN achieves the best performance, even approximating the multitasked upper bound with 2.6%, implying little forgetting during continual learning. (3) Task order does influence performance with LSEBMCL. (4) When using LSEBMCL, the performance of old tasks remained almost the same throughout the training process. Increasing the sampling ratio \u03b3 improved the performance, particularly when increased from 0 to 0.05. (5) A better continual method had a smaller standard deviation, indicating it was less affected by the task order. LSEBMCL even achieves the lowest standard deviation among all the baselines, indicating its robustness to task order variations.\n2) Five DecaNLP Tasks: In this sequential training experiment, five tasks were tackled in order of decreasing size, commencing with the largest task (SQuAD 2.0) and concluding with the smallest (WOZ). This task sequence was determined by the constraints of limited computing resources. Notably, LSEBMCL exhibited superior performance across all tasks, outperforming other methods by a significant margin and even approximates the multitasked upper bound with 0.9%, as detailed in Table III. Moreover, the effectiveness of LSEBMCL demonstrated further enhancement with an increase in the sampling ratio \u03b3. The experiment's results underscore LSEBMCL's remarkable efficacy and suitability for diverse tasks, confirming its robust performance.\n3) Text Classification Tasks: We compare our proposed method, LSEBMCL, against the state-of-the-art MBPA++,"}, {"title": "V. CONCLUSION", "content": "In this study, we introduce an innovative approach known as LSEBMCL that integrates an EBM layer into the continual learning framework for NLP tasks. In addition to its promising applications in NLP tasks, it holds potential implications for computer vision tasks. Leveraging the expressive power of the EBM prior in text modeling, we construct a latent space conducive to interpretable generation and text classification. To achieve this, we devise a novel prior distribution that integrates continuous latent variables for generation and discrete latent variables for inducing structural elements. Furthermore, we utilize the EBM to generate samples from previous tasks when training the model on new tasks. The experiments demonstrate the superior performance of our proposed approach."}]}