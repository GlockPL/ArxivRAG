{"title": "PhD Knowledge Not Required: A Reasoning Challenge for Large Language Models", "authors": ["Carolyn Jane Anderson", "Joydeep Biswas", "Aleksander Boruch-Gruszecki", "Federico Cassano", "Molly Q Feldman", "Arjun Guha", "Francesca Lucchetti", "Zixuan Wu"], "abstract": "Existing benchmarks for frontier models often test specialized, \u201cPhD-level\u201d knowledge that is difficult for non-experts to grasp. In contrast, we present a benchmark based on the NPR Sunday Puzzle Challenge that requires only general knowledge. Our benchmark is challenging for both humans and models, however correct solutions are easy to verify, and models' mistakes are easy to spot.\nOur work reveals capability gaps that are not evident in existing benchmarks: OpenAI o1 significantly outperforms other reasoning models that are on par on benchmarks that test specialized knowledge. Furthermore, our analysis of reasoning outputs uncovers new kinds of failures. DeepSeek R1, for instance, often concedes with \u201cI give up\" before providing an answer that it knows is wrong. R1 can also be remarkably \"uncertain\" in its output and in rare cases, it does not \"finish thinking,\u201d which suggests the need for an inference-time technique to \"wrap up\" before the context window limit is reached. We also quantify the effectiveness of reasoning longer with R1 and Gemini Thinking to identify the point beyond which more reasoning is unlikely to improve accuracy on our benchmark.", "sections": [{"title": "1 Introduction", "content": "There has been significant recent interest in large language models (LLMs) that are trained to employ reasoning at inference time. These reasoning models, which include OpenAI o-family models (OpenAI, 2024), Gemini 2.0 Flash Thinking (Google, 2024), and DeepSeek R1 (DeepSeek-AI et al., 2025), achieve state-of-the-art results on several challenging benchmarks, and far surpass the capabilities of the last generation of LLMs that do not employ test-time compute. For many researchers, the goal is to develop tasks that are extremely difficult for any human, to help develop models that exceed human ability. Thus the latest benchmarks evaluate models on very challenging tasks, such as college-level math competition problems, very difficult programming problems, and problems that require deep domain expertise in academic disciplines. Some of these benchmarks are carefully designed by people who have or are pursuing PhDs and equivalent degrees (Rein et al., 2024; Phan et al., 2025). A consequence of designing problems this way is that they are not only challenging for humans to solve\u2014as intended\u2014they are also very challenging for humans to understand and verify. Thus most people cannot understand why these problems are hard, check that answers are indeed correct, or verify that models are reasoning correctly and efficiently about a problem. This problem will become more important with the proliferation of reasoning models."}, {"title": "2 Related Work", "content": "Benchmarks that Require PhD Knowledge As models keep getting better, the benchmarks that we use to quantify their capabilities get saturated. Several recent benchmarks have been explicitly designed with the belief that models are approaching superhuman capabilities in particular domains, and these benchmarks are thus designed to have extremely challenging domain-specific questions. GPQA (\u201cGoogle-proof Q&A\") (Rein et al., 2024) is a recent example, where benchmark problems were created and vetted by teams experts who had or were pursuing PhDs in physics, chemistry, and biology. Remarkably, the latest generation of reasoning models appear to be saturating GPQA in just a few months. HLE (\"Humanity's Last Exam\") (Phan et al., 2025) is a newer, larger, and harder benchmark that is similarly designed. HLE covers many more areas of knowledge, with questions written by people who hold advanced degrees, and even the latest models still perform very poorly. At the time of writing, the best performing model, OpenAI 01, achieves a 9.1% accuracy. These benchmarks are very valuable, but, by design, each problem is only comprehensible by people who have narrow subject-matter expertise. An individual cannot hope to answer or even understand most of the questions on these benchmarks.\nMath Benchmarks A number of notable benchmarks, such as GSM8K (Cobbe et al., 2021) and MATH (Hendrycks et al., 2021), evaluate the mathematical capabilities of the models. Arguably, frontier models have nearly saturated many of these benchmarks (Lei et al., 2024; Zhong et al., 2024). However, Mirzadeh et al. (2024) show that the models perform significantly worse on slight variations of GSM8K problems, e.g., ones involving different numbers or names of people. Adding a distractor sentence to the problem has a particularly significant result, even decreasing the performance of o1-preview by 17.5%. Gulati et al. (2024) show a benchmark of problems sourced from the William Lowell Putnam Mathematical Competition, also featuring variable name and constant variants. Although o1-preview achieves a precision of 41.95% on their benchmarks, its accuracy is around 30% lower on the altered problems. Such issues may be caused by data contamination and the tendency of all models so far to exhibit token bias (Jiang et al., 2024). While math benchmarks are invaluable in discovering the capabilities and limitations of logical reasoning in state-of-the-art models, the problems in such benchmarks inevitably require a strong mathematical background for a reader to appreciate, follow along, or catch errors in the models' reasoning.\nBenchmarks That Do Not Require World Knowledge ARC-AGI (Chollet, 2019) is perhaps the best known benchmark of reasoning and abstraction benchmark for AI. ARC-AGI is carefully constructed to require a small number of priors, whereas our benchmark tests both a model's reasoning ability and its ability to recall extensive general knowledge. The ARC-AGI tests can be challenging for people, but \u201cEach task included in ARC has been successfully solved by at least one member of a group of three high-IQ humans.\u201d In contrast, a few hundred people submit correct solutions to the Puzzle Challenges every week.\nBenchmarks That Exercise General Knowledge There is a long tradition of using puzzles to evaluate language models. Most closely related to our work are benchmarks constructed from cryptic crosswords in The Guardian (Rozner et al., 2021), synthesized brain teasers (Jiang et al., 2023), and the \u201con air\u201d questions from the NPR Sunday Puzzle (Zhao & Anderson, 2023). While we also source our benchmark from the same show, our benchmarks are disjoint: Jiang et al. (2023) create a benchmark from the on-air questions that are carefully designed for the contestant to solve live. In contrast, we create our benchmark from the off-air \"weekly challenges\" that are designed to be significantly harder. Some puzzles explicitly tell listeners to use a dictionary or atlas to help them work through the puzzle. As we shall see, the weekly challenges can stump current-generation models too.\""}, {"title": "3 The Sunday Puzzle Challenge Dataset", "content": "We present how we curate the Sunday Puzzle Challenge Dataset from the NPR Sunday puzzles, including filtering the questions for ease of testing (\u00a73.1) and prompting method (\u00a73.2). We then discuss our findings of the results with state-of-the-art models on the benchmark, including overall results (\u00a74.1) and insights gleaned (\u00a74.2)."}, {"title": "3.1 Building and Validating The Dataset", "content": "We scrape thirteen years of transcripts of the Sunday Puzzle Challenges from the web, and use them to build our dataset. We systematically reviewed and edited the scraped data as follows.\nAdding Context A handful of challenges require context that is not evident from the challenge text. The most common missing context is the current date, which we correct by making dates explicit when necessary, as in the following example:"}, {"title": "3.2 Prompting Format", "content": "We prompt every model to answer each challenge zero-shot, without any formatting instructions or any additional instructions other than the challenge text itself. Thus the model freely generates its answer and explanation. To check correctness, we ignore capitalization and punctuation and test that every phrase in the gold answer appears in the model-generated answer.\nAlthough we use a zero-shot prompt, we observe that some of the challenges have examples, as illustrated by the following."}, {"title": "4 Results", "content": "Model Selection and Configuration We focus on evaluating the latest generation of models that use test-time compute to reason before producing a final answer: 1) OpenAI 01, 2) OpenAI 03-mini, 3) OpenAI o1-mini, 4) Google Gemini 2.0 Flash Thinking Experimental 01-21, and 5) DeepSeek R1 We also include GPT-40 and Claude Sonnet 3.5 as non-reasoning baselines.\nWe configure models for generation as follows. For o1 and 01-mini, the API requires using temperature 1. For Gemini 2.0 Flash Thinking, we use the API's default temperature of 0.7. For DeepSeek R1, we use temperature 0.6, top-p 0.95, and 32,768 limit on output tokens: this is the configuration the R1 paper uses for its experiments. For GPT-40 and Sonnet, we use temperature 0.2 and top-p 0.95, which are commonly used for reasoning tasks."}, {"title": "4.1 Main Results", "content": "We report mean accuracy on our benchmark in Figure 1. The figure shows that OpenAI 01 significantly outperforms the other models (59% accuracy). The next best performing model is 03-mini with high reasoning effort (47%), followed by 03-mini with default settings (36%) and R1 (35%). The non-reasoning models, Sonnet 3.5 and GPT-40, do much more poorly than the best reasoning models, which indicates that the benchmark does exercise reasoning ability.\nIt is interesting to compare models' relative performance on our benchmark to their performance on benchmarks that require deep technical knowledge. For example, on the GPQA Rein et al. (2024) of PhD-level science questions, the R1, 01, and o3-mini models achieve comparable performance. However, our benchmark indicates that o1 has substantially better general knowledge."}, {"title": "4.2 How Models Give Up", "content": "We observe several well-known failure modes of LLMs, such as hallucinations and faulty calculations. However, a novelty of reasoning models is that they can \u201cgive up\" on the problem. On 142 / 595 challenges, R1 explicitly outputs \u201cI give up\" while reasoning. Broadly speaking, there are two kinds of \u201cgive ups\u201d that we observe.\nOut-of-thin-air final answer The following challenge elicits an \u201cI give up\" with an \u201cout-of-thin-air\" answer that does not appear anywhere in the reasoning output."}, {"title": "4.3 Getting Stuck Reasoning with R1", "content": "We ran our experiments with a 32,768 output token limit, following the example of the R1 paper. However, this limit is not high enough for the dataset. On 50 challenges, we encounter cases where R1 gets stuck during reasoning: it does not output the token before we reach the 32,768 output token limit. The following two challenges exhibit the worst case behavior, and R1 fails to stop reasoning in 5/10 trials."}, {"title": "4.4 A Newfound Lack of Confidence", "content": "We observe that R1 and Gemini Thinking can be remarkably uncertain in their output. Sometimes the model will \u201cgive up\" and output a wrong answer, only to immediately take it back, try to provide a good one, and fail again. Interestingly, the models seem to repeat themselves in such a state more often than when thinking (Figure 4). We estimate this occurred in 29 R1 responses, 18 Gemini Thinking responses and 3 o1-mini responses."}, {"title": "4.5 How Much Reasoning Is Necessary?", "content": "How much reasoning is necessary? Since we can access the reasoning output from R1 and Flash Thinking, we can empirically determine a \u201creasoning budget\" that is finer-grained than the three \u201creasoning effort\" settings presented offered by the OpenAI API. Figure 6a depicts the distribution of reasoning-output lengths on the Sunday Puzzle Challenge, showing that most attempts generate fewer than 20,000 tokens. Figure 6b then shows accuracy as a function of reasoning length for both models. Gemini Thinking achieves its accuracy plateau at roughly 10,000 tokens, whereas R1's accuracy continues to improve and surpasses Gemini Thinking at around 3,000 tokens.\nThus we conclude that 10,000 tokens of reasoning is unlikely to significantly improve accuracy with either model. Consequently, when the model's reasoning output surpasses 10,000 tokens, we are likely better off interrupting generation rather than waiting longer (and spending more). A similar approach that quantifies how accuracy improves without reasoning length is likely helpful for other tasks as well."}, {"title": "5 Conclusion", "content": "We present a benchmark for reasoning models with questions that do not require PhD-level knowledge, but instead exercise U.S.-centric general knowledge. The benchmark problems are challenging for both humans and models, but humans can easily verify the correctness of an answer, and models' mistakes are also obvious.\nWe uncover new failure modes in reasoning models. We observe that models can \u201cgive up\" on a difficult problem and deliberately return an incorrect answer. In rare cases, R1 can get stuck \"thinking forever\u201d. Finally, by examining the reasoning outputs of R1 and Gemini Thinking, we quantify the effectiveness of reasoning longer, allowing us to set a token budget beyond which accuracy reaches a plateau on these tasks. Our work identifies nuances of model behavior on accessible yet challenging benchmarks, and also point to areas for improvement reasoning models."}]}