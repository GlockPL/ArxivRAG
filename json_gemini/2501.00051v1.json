{"title": "DDD-GenDT: Dynamic Data-driven Generative Digital Twin Framework", "authors": ["Yu-Zheng Lin", "Qinxuan Shi", "Zhanglong Yang", "Banafsheh Saber Latibari", "Sicong Shao", "Soheil Salehi", "Pratik Satam"], "abstract": "Digital twin (DT) technology has emerged as a transformative approach to simulate, predict, and optimize the behavior of physical systems, with applications that span manufacturing, healthcare, climate science, and more. However, the development of DT models often faces challenges such as high data requirements, integration complexity, and limited adaptability to dynamic changes in physical systems. This paper presents a new method inspired by dynamic data-driven applications systems (DDDAS), called the dynamic data-driven generative of digital twins framework (DDD-GenDT), which combines the physical system with LLM, allowing LLM to act as DT to interact with the physical system operating status and generate the corresponding physical behaviors. We apply DDD-GenDT to the computer numerical control (CNC) machining process, and we use the spindle current measurement data in the NASA milling wear data set as an example to enable LLMs to forecast the physical behavior from historical data and interact with current observations. Experimental results show that in the zero-shot prediction setting, the LLM-based DT can adapt to the change in the system, and the average RMSE of the GPT-4 prediction is 0.479A, which is 4.79% of the maximum spindle motor current measurement of 10A, with little training data and instructions required. Furthermore, we analyze the performance of DDD-GenDT in this specific application and their potential to construct digital twins. We also discuss the limitations and challenges that may arise in practical implementations.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, rapid advances in computing, communication, and AI have enabled the creation of software systems that accurately model physical processes, called the digital twin model [1-3]. These virtual representations of real-world entities are designed to simulate, predict, and optimize interaction with their physical counterparts, bridging the gap between the physical and digital domains. Such DTs are finding applications across various fields. In climate science, DT improves the accuracy of weather prediction and supports disaster preparedness by modeling complex atmospheric systems [4]. In medicine, digital twins are advancing precision medicine by enabling personalized diagnosis and treatment through patient-specific simulations [5]. In manufacturing, digital twins serve as virtual replicas of production systems, enabling real-time monitoring, maintenance, optimization, and safety [6]. These diverse applications demonstrate the transformative potential of DTs across industries.\nHowever, building accurate DTs of the physical world requires large amounts of high-quality data that is difficult to collect and acquire due to challenges in instrumentation, privacy, and data collection costs [7]. In addition, these challenges increase with the complexity of the physical system, requiring a set of behavior models to build an accurate DT [8], highlighting the need to explore more flexible and efficient digital twin construction methods. Furthermore, the concept of digital twins goes beyond simulating physical shapes or behaviors and emphasizes continuous updating with real-time data and history data [9]. Given this, we consider the physical system and its digital twin as interconnected dynamic systems that progress over time. This system considers two-dimensional changes, including mechanical and process changes, illustrated in Figure 1. The vertical axis represents changes in mechanical"}, {"title": "II. BACKGROUND", "content": "A digital twin is a software system that models the physical system. The virtual model interacts with the physical system through continuous data interaction. The model reflects the system behavior based on the physical state and provides feedback to the physical system based on the digital twin output. This concept first originated in the field of aerospace engineering. Shafto et al. defined digital twins as simulation-based systems engineering in the 2010 NASA Technology & Processing Roadmap draft [13]. This report suggests that the digital twin concept can obtain high-fidelity models with multiphysics and multiscale simulations, sensor updates, and history data. Using digital twins' continuous prediction outputs can study the impact of vehicle parameters, abnormal conditions, and environmental factors on the system and improve mission success rates. While digital twins are powerful and offer many advantages, computing resources or methods will limit the construction of complex behavioral physics models, making the simulation model difficult for physical interaction or not reflecting the physical behavior [14].\nWith the maturity of machine learning technology and benefiting from the development of computer computing power, digital twin modeling based on machine learning has brought new opportunities for developing digital twins [2, 3]. It enables the modeling of complex systems that originally required huge computing resources and were difficult to formulate with physics and statistics. The machine learning-based DT developments demonstrate the great potential of machine learning applications in building digital twins. More insights into the physical process can be obtained through the continuous interaction between the machine learning-based digital twin model and the physical system."}, {"title": "B. Dynamic Data Driven Applications Systems", "content": "The concept of digital twins has various advantages and has become a potentially transformative technology with applications in many fields, but they are not without limitations. One of the main challenges is that relying on predefined models limits their ability to simulate systems that change dynamically over time [15]. For example, in the operation of industrial equipment, in addition to the physical behavior caused by different manufacturing processes, the resulting vibration, temperature, mechanical wear and tear, etc. These will result in physical behavior changes over time, which"}, {"title": "C. LLM with Physical Systems", "content": "Many researchers have recently proposed LLM as the bridge between reality and virtuality, and its impact spans from laboratories to industry. Coscientist, an artificial intelligence system for automated chemical experiments proposed by Boiko et al. [18], which can semi-automatically plan the overall experiment and operate heater vibrator modules and liquid handling instruments. The system completed complex chemical synthesis tasks, demonstrating the potential of LLM for operating instruments to complete complex tasks. In industrial manufacturing applications, Gkournelos et al. used the GPT model with another customized GPT model for human-robot Collaboration to control human-robot collaborative robots using natural language [19]. In addition, to cope with diverse product manufacturing requirements, Zhao et al. reported that using multi-agent technology based on LLM to schedule manufacturing equipment on the physical shopfloor can make it more flexible in responding to changes in task requirements [20]. This experiment shows that multiple LLM-based agents can effectively schedule and improve efficiency on the physical shop floor. By developing peripheral interfaces and fine-tuning models for specific tasks, LLM can operate physical systems and understand tasks well. However, using LLM interactive with physical systems is still in its early stages. Our DDD-GenDT framework hopes to take advantage of LLM by using a small amount of data to construct a digital twin to map physical behavior. This framework enables the LLM-based digital twin to interact with the physical system continuously.\nAttracted by LLM's multitask characteristics, researchers began to use LLMs for time series-related tasks. This includes using carefully designed prompts to test the performance of general LLMs for time series prediction or using a large number of time series to train LLMs, which makes it good at"}, {"title": "III. THE DIGITAL TWIN REFERENCE ARCHITECTURE", "content": "To address the growing complexity and diversity of digital twin (DT) systems, we define a digital twin reference architecture as a foundational guide for the building and implementing DT systems [23]. This architecture illustrates the connection between physical and virtual spaces through digital twinning, as shown in Figure 2.\nIn the physical space, the basis is the physical system, and the sensors attached to the physical system capture data to measure the physical quantities of interest [24] and transmit"}, {"title": "IV. DDD-GENDT FRAMEWORK", "content": "This article proposes a framework called DDD-GenDT. The concept of dynamic decision-making and abstraction forms for digital twins are inspired by Kapteyn et al. [29], enabling the framework to adapt to different physical states and interact with physical systems. The concept of DDD-GenDT is illustrated in Figure 3. We apply it to CNC machine tools as a concrete example, but please note that its abstraction forms and dynamic LLM-based digital twin allow this framework to be applied in various fields. In the following subsections, we will discuss the details of the LLM-based dynamic DT and how it interacts with the physical systems. Table I lists the main notations."}, {"title": "A. LLM-Based Dynamic Digital Twin", "content": "The goal of LLM-based dynamic digital twin is to obtain a mapping of the current physical behavior state through adaptive input. To use LLM as a dynamic digital twin, we use the DT Trigger component to monitor the current execution state $P_c$ and obtain history data from different measurements corresponding to the current state. The history data are time series data, denote as $x_{k,1:L_w} = [x_{k,1}, x_{k,2},...,x_{k,L_w}]$ and $x \\in \\mathbb{R}$, where $L_w$ is the window length, and k represents the process state index. The definition of history data for input is defined as Equation 1.\n$X_{hist} = [x_{r-m:r-1}]_{k=P_c}$ (1)\nHere, $x_{r-n:r-1}$ denotes the ensemble time series data from the n measurements before the current measurement $C_r$ up to $C_{r-1}$. The index $k = P_c$ indicates that we focus on the measurements corresponding to the current execution state $P_c$. The expansion of this history data is written as $[x_{r-m} ... x_{r-2} x_{r-1}]$. To control the randomness of model output, we used Top-P ($P_{top}$) and Temperature (T). The temperature adjusts the randomness of the predictions of the model. Lower values make the output more deterministic and focused, while higher values increase diversity and creativity by favoring less probable tokens [30]. Top-P controls the range of possible results by limiting the model to a subset of the most probable symbols whose cumulative probability is below a threshold [31]. Next, we input these historical data ($X_{hist}$), Top-P ($P_{top}$), and Temperature (T) as parameters into the LLM"}, {"title": "B. Interactive with Physical System", "content": "The way in which LLM-based DT interacts with the physical system is illustrated in Figure 4. The LLM, as a dynamic DT, continuously maps the corresponding physical behavior state that changes over time through the input of dynamic history data. In addition, based on the output of the digital twin and the current physical behavior observations, the LLM-based dynamic DT can make decisions on the physical system. After obtaining the output $ \\hat{Y} $ of the LLM-based dynamic DT's mapping of the current physical state, we combine it with the observation $O_c$ of the current physical state to calculate the quantities of interest $Q_c$. The $O_c$ is a current machining state measurement of the online workpiece, where $O_{c,1:L_w} = [0_{c:1}, 0_{c:2},..., O_{c:L_w}]$ and $0 \\in \\mathbb{R}$. In this case, the quantities of interest $Q_c$ is the root mean square error (RMSE) between the current observation $O_c$ and the LLM-based dynamic DT output $ \\hat{Y_c}$, and $Q_c$ defined as Equation 7.\n$Q_c = e_{RMSE} = RMSE(O_c, \\hat{Y_c})$ (7)\nNext, if the error between the current measurement and the digital twin output is too large, there is a possibility of anomaly. If there are abnormalities during the processing, it may lead to material waste or equipment damage. Therefore, early warning or stopping the machine is crucial to avoid further impact. The $U(.)$ is the control input function, which controls the physical system, CNC machine, based on $Q_c$. We can consider continuing manufacturing, reporting a warning, or stopping the machine through the threshold $T_{low}$ and $T_{high}$ setting. The reason for providing the warning is to avoid the variability of the errors generated by large language models, which may frequently bring the machine to a halt and cause damage to the machine. The control input function $U(.)$ is defined in Equation 8.\n$U(Q_c) = \\begin{cases} Continue & \\text{if } Q_c < T_{low} \\\\ Warning & \\text{if } T_{low} \\leq Q_c \\leq T_{high} \\\\ Stop & \\text{if } Q_c > T_{high} \\end{cases}$ (8)"}, {"title": "V. EXPERIMENT AND RESULT ANALYSIS", "content": "This study will use CNC machining with spindle current measurement as an example of an application of DDD-GenDT. When CNC machining workpieces are being manufactured, different machining behaviors will have different patterns in power consumption curves, and we can analyze and gain insight into the machining process by observing these patterns. In 2019, Lin et al. reported using side channels to measure CNC spindle power consumption as a monitoring indicator of the cutting process, which can be used to detect product quality during the cutting process to avoid cyber-physical attacks [33]. The framework can continuously monitor the output of the interaction between the digital twin and physical measurements during manufacturing, avoiding material waste caused by anomalies in the early stages. In addition, tool wear will affect the spindle motor current, which can be used to assess tool condition [34, 35]. Therefore, the spindle motor's electrical behavior is a vital monitor target of the CNC machining process's quality and security.\nWe used GPT-4 and GPT 3.5 Turbo as LLM-based dynamic DTs in the DDD-GenDT framework in experiments. For the GPT 3.5 Turbo and GPT-4 models, we use the LLMTime method proposed by Gruver et al. [12] to generate time series data corresponding to the physical state. Our scenario is to apply a digital twin to map the CNC cutting process, and We conducted experiments using the spindle current in the NASA milling data set [36]. Figure 5 is the spindle current measurement from case 1 after the 8Hz Butterworth low-pass filter in the NASA milling dataset, and Table II shows the details of the measurement values of the flank wear in different runs. Here, \"runs\" refer to repeated executions of the same CNC cutting program to the machining process. Each \"run\" represents a complete manufacturing cycle of a workpiece using the same machining process. We can observe that from the first run to the final run, the current level of the spindle motor increases, different fluctuation behavior, and different stable and finished time lengths due to tool wear. Our goal is to use the LLM-Based DT method to adaptively reflect the physical behavior of the motor current behavior under different tool wear conditions. This section discusses physical state mapping and LLM-based dynamic DT output error analysis using the DDD-GenDT framework."}, {"title": "A. Digital Twin Mapping of Physical States", "content": "The example results of mapping the spindle motor current in different processing stages and the visualization results are shown in Figure 6. The light-colored areas above and below the solid line are plus and minus one standard deviation. In these results, GPT-4 is closer to ground truth than GPT 3.5 Turbo in the initial cutting stage. Although GPT 3.5 Turbo captures some physical behavior, its behavior deviates significantly"}, {"title": "B. LLM-based Dynamic DT Output Analysis", "content": "To analyze the performance of different models, we used ten runs in case 1 from the NASA milling data set to evaluate the performance. In this dataset, the CNC cutting program is fixed for all runs within the same case, ensuring that observed variations in the signal collection are caused by physical system changes not caused by the program. Therefore, we divide the signal using the sliding window, treat each divided window as a state [38], and use the same state of the previous n runs as LLM input. n here is set to 4 to maximize the use of the LLM input length. The error is quantified by comparing the model outputs with the corresponding ground truth states, as defined in Equation 11. This error metric is represented as a vector, where each element corresponds to the Root Mean Square Error (RMSE) computed between the predicted output $\\hat{Y}$ and the ground truth $Y_i$ for a specific state i. The formulation is expressed as:\n$Err = \\begin{bmatrix} RMSE(Y_1, \\hat{Y_1}) \\\\ RMSE(Y_2, \\hat{Y_2}) \\\\ ... \\\\ RMSE(Y_n, \\hat{Y_n}) \\end{bmatrix}$ (11)\nThis structured representation ensures that the error is systematically evaluated across all states, comprehensively assessing the model's performance. After obtaining the error information Err, we use a box plot to visualize the error distribution of the physical state mapping of ten runs. In Figure 7, the box plots illustrate the RMSE distribution of CNC spindle current across ten runs for GPT-3.5 Turbo and GPT-4. The median RMSE values of GPT-4 are consistently lower than those of GPT-3.5 Turbo, highlighting its superior performance in mapping the physical state of the spindle motor current. Moreover, the interquartile range (IQR) variability is significantly narrower for GPT-4 than GPT-3.5 Turbo, indicating more stable and reliable predictions across different runs. The overlaid red line shows the progression of tool wear as the runs progress, indicating a gradual increase in wear. Interestingly, GPT-4 maintains a relatively low RMSE and reduced error and variability despite increasing tool wear, demonstrating its robustness in dynamically adapting to the system's physical changes."}, {"title": "VI. DISCUSSION", "content": "This section summarizes the experimental results from Section IV, highlighting the performance and robustness of GPT-4 in physical state mapping. The average of GPT-4 $Err_{avg}$ in ten runs is 0.479A, which is 4.79% of the maximum spindle motor current measurement of 10A. These values can be referenced when determining the threshold parameters $T_{low}$ and $T_{high}$ for the control function. Although this level of fidelity may not effectively detect subtle anomalies, GPT-4 remains a competitive solution within the DDD-GenDT framework, especially given its ability to perform well with limited data through zero-shot behavior forecasting.\nFurthermore, the results demonstrate that GPT-4 achieves performance levels comparable to traditional deep learning methods, such as 1D CNN AE, with significantly reduced training data. In Table II, the 1D CNN AE models, trained with 2434 and 1217 state windows (Runs 1-4 and Runs 3-4, respectively), achieve slightly better RMSE performance. However, the training cost for these models is substantially higher because they rely on larger datasets and fixed-data set training. This highlights the key advantage of GPT-4: its ability to achieve comparable performance with a minimal dataset, leveraging dynamic zero-shot learning. Figure 8 shows that starting from Run 7, the RMSE of GPT-4 is very close to that of 1D CNN AE (Trained with Run 3-4), and its performance is stable and gradually optimized as the physical behavior of the spindle motor current due to the growth of tool wear stabilizes. Furthermore, while fine-tuning the LLM for specific tasks can reduce inference errors, it requires substantial data quality, computational resources, and significant investments in expertise and hardware. Therefore, dynamically adjusting the prompt input to map the physical state used in this article is a low-cost and effective method [39]."}, {"title": "VII. CONCLUSION", "content": "In this study, we proposed the DDD-GenDT framework using LLM as a dynamic digital twin to capture and map the physical state behavior of complex systems and proposed the CNC machining power consumption curve as an application case to make LLM Dynamic DT closely related to the physical system interact and take control. Our experimental results show that GPT 3.5 Turbo and GPT-4 exhibit acceptable fidelity and low variability, making them competitive options for integration into the dynamic decision-making process of the DDD-GenDT framework. Despite inherent computational limitations and input length constraints, the LLMs show great promise in generating time series for physical state mapping with a small amount of data.\nThe main limitation of this study is the computational resource requirements of LLMs. The limitation of their input length makes LLMs cannot input a large amount of history data, high-frequency data, or text description of physical behavior to provide more information to facilitate the mapping fidelity of physical states [40]. Also, because computing resources and model architecture limit the LLM response time and are not fast, this framework is unsuitable for a system with high-speed physical state change behaviors. However, without fine-tuning and training on LLMs, using dynamic historical corresponding data input to generate physical state mapping with good robustness is still a cost-effective solution with acceptable fidelity. Future work will focus on LLM-based dynamic DT for edge computing and improvement of inference speed and accuracy to improve the usability of the framework proposed in this article.\nIn conclusion, this framework enables LLM to be coupled to physical systems as a dynamic digital twin by leveraging abstraction forms. Our experimental result demonstrates the"}]}