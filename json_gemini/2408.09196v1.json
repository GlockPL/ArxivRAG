{"title": "Maintainability Challenges in ML: A Systematic Literature Review", "authors": ["Karthik Shivashankar", "Antonio Martini"], "abstract": "Background: As Machine Learning (ML) advances rapidly in many fields, it is being adopted by academics and businesses alike. However, ML has a number of different challenges in terms of maintenance not found in traditional software projects. Identifying what causes these maintainability challenges can help mitigate them early and continue delivering value in the long run without degrading ML performance. Aim: This study aims to identify and synthesise the maintainability challenges in different stages of the ML workflow and understand how these stages are interdependent and impact each other's maintainability. Method: Using a systematic literature review, we screened more than 13000 papers, then selected and qualitatively analysed 56 of them. Results: (i) a catalogue of maintainability challenges in different stages of Data Engineering, Model Engineering workflows and the current challenges when building ML systems are discussed; (ii) a map of 13 maintainability challenges to different interdependent stages of ML that impact the overall workflow; (iii) Provided insights to developers of ML tools and researchers. Conclusions: In this study, practitioners and organisations will learn about maintainability challenges and their impact at different stages of ML workflow. This will enable them to avoid pitfalls and help to build a maintainable ML system. The implications and challenges will also serve as a basis for future research to strengthen our understanding of the ML system's maintainability.", "sections": [{"title": "I. INTRODUCTION", "content": "Modern Software applications rely heavily on Machine learning (ML) systems and are used in various tasks to provide meaningful insights learned from growing and evolving data. Many companies have adapted ML in their service offering and also delivered value internally. The increasing adoption of ML has introduced new challenges associated with data management and processing, model training and deployments, data and model quality assurance, and other development practices [2].\nML systems are data-driven and data-dependent, thereby creating entanglement between data features and model performance, making them susceptible to model staleness and training-serving skew, which may degrade the performance of the ML system without proper mitigation strategies in place [10]. Furthermore, this data dependency behaviour costs more than code dependency, making them particularly vulnerable to Technical debt (TD) and incurring massive ongoing maintenance costs compared to traditional software projects [17].\nHence, it is imperative that organisations and practitioners understand how to develop maintainable ML systems and how they can continue to deliver value in the long run. The first step towards overcoming this problem is understanding maintainability challenges when developing ML systems and how different stages in the workflow affect their maintenance. Unfortunately, no systematic literature review has investigated maintainability challenges in ML systems despite their importance. To fill the gap in the current literature, we conducted a Systematic Literature Review (SLR) on Maintainability challenges in ML systems. We have the following Research Questions (RQ), which guided our SLR study. (RQ1) What are the Data Engineering Maintainability challenges? (RQ2) What are the Model Engineering Maintainability challenges? (RQ3) What are the current maintainability challenges when Building an ML systems? We have selected 56 primary studies for inclusion in this SLR. The main contributions of our SLR study are as follows. Contribution 1: ML maintainability challenges which are identified and discussed using our RQ's will help the community to make prudent choices when developing or maintaining ML system or application. Contribution 2: Mapping of the 13 synthesised interdependent stages and its impact on maintainability will guide the practitioner to be wary of the dependencies and cost involved during maintenance when dealing with each stages. Contribution 3: We have also distilled implications for Developers of ML tools and synthesised opportunity for further research."}, {"title": "II. BACKGROUND", "content": "A. Maintainability of Software Systems\nSoftware maintainability means \"the ease with which a software system or component can be modified to correct faults, improve performance or other attributes and adapt to a changing environment\" [5]. Software systems are frequently changed to meet changing customer requirements that may arise from various factors, including changes in technology or enhancing existing features [16]. Compared to development, software maintenance consumes more resources, effort, and time. It is estimated that Software developers spend about 70% of their time on maintenance [12]. There is also a high degree of complexity in today's software, and the size of the software has grown considerably, making maintenance increasingly difficult [13]. An organisation's and a product's success relies directly on its software maintainability [14]. Therefore, producing \"software that is easy to maintain\" could save a lot of time and money and deliver long term value.\nB. Data and Model Engineering\nIn ML, data is the first-class citizen and it is well known that the majority of the time spent on ML development is spent on processing data [L12]. ML algorithms cannot perform well without handling dirty data since data quality profoundly impacts model accuracy. ML workflows usually begin with acquiring and preparing the data for training. Creating high-quality training data is typically a tedious, repetitious process [L14]. Data engineering pipelines typically involve a sequence of operations on a set of data from various sources. These operations aim to create training and testing datasets for the ML algorithms. Generally, data engineering is divided into many stages: Data acquisition and exploration, Data processing, Data validation and management [15].\nModel training is the process of feeding an algorithm with data to learn patterns instead of having to manually discover and encode those patterns [1]. A model engineering pipeline consists of several operations that result in a final model usually used by ML engineers and data science teams. These operations include Model Training, Hyper-Parameter Optimization (HPO), Model Governance, Model Monitoring, Model Testing, Model drift, and Model Deployment [15].\nC. Related Works\nNumerous studies have identified the different types of TDs, mainly an extension of Sculley et. al. [17], and anti-patterns that emerge in the development of ML systems, and how they affect Model performance [10]. Furthermore, we found studies discussing challenges related to applying existing SE techniques to ML development, using a case study approach, and an empirical study discussing the use of mature engineering techniques to increase reliance on ML components [3], [7]. We also found a paper that examines the challenges associated with ML deployment and provides a framework for accelerating ML development [6] and architectural challenges for ML systems [2]. Another study identifies and categorises data management challenges faced by practitioners at different stages of ML workflow [9]. Nevertheless, all these papers do not address the different maintainability challenges that arise at various stages of ML workflow, for example, how the data-dependent and stochastic nature of ML affects the maintainability of model testing, data validation, and detecting model drifts. We seek to fill this gap in research by synthesising the maintainability challenges at each stage in the ML workflow and how they are interdependent and impact each other using an SLR approach."}, {"title": "III. METHODOLOGY", "content": "The following section details our rigorous strategy, closely adhering to the guidelines suggested for conducting SLRs by Kitchenham et al. [8]. We also provide a replication package, available at this https://doi.org/10.5281/zenodo.6400559 Zenodo link.\nA. Research method\nFig. 1. shows an overview of how this SLR study was conducted. The selected databases were screened using well-defined search terms and queries to obtain the desired papers. The resultant papers are then evaluated iteratively based on inclusion and exclusion criteria. The categorization of unclear and conflicting papers was reviewed by the second author.\nB. Step 1: Search strategy:\nDatabases included in this Systematic Literature Review (SLR) are IEEE Xplore, ACM Digital Library, Web of Science, Google Scholar and Scopus. Keywords contained in the title, abstract and index terms of the literature are identified by the following search query for most of the databases which facilitated these features see the replication package Zenodo link for the exact search query used for different databases.\n\u2022 TITLE-ABS-KEY ( machine AND learning AND software AND (ml OR ai OR dl OR neural OR intelligence OR learning ) AND ( adapt* OR maintain* OR scal* OR exten* OR evol* OR flex* ) AND ( system OR architect* OR design OR build OR application OR engineering OR test) AND (data OR algorithm OR debt OR pattern OR code))\nThe query contains all primary keywords from research questions with alternative spellings, synonyms, and keywords obtained from pilot searches and related papers.\nThe search was conducted for articles published between January 1, 2014, and January 15, 2022. We chose to start from 2014 because the adoption and development of many ML projects and libraries started around that year [4]. We limited our search to the first 1000 papers from Google Scholar and the first 2000 papers from Scopus in order by relevance, due to a limitation in their respective systems.\nC. Step 2: Selection criteria:\nWe performed three iterations using the inclusion and exclusion criteria as shown in Fig. 1. In the first iteration, we analysed the paper's title and abstract, which resulted in 697 papers for the next iterations; at the end, the second iteration a total of 190 papers were selected and for the final third iteration a total of 56 papers were selected for inclusion in this SLR study.\nInclusion criteria\n\u2022 Paper that answers at least one Research questions (RQ).\n\u2022 Paper focuses on the maintainability aspect of the ML system.\nExclusion criteria\n\u2022 Research papers that are not written in English language.\n\u2022 Publication for which the full text is not available.\n\u2022 Grey literature.\n\u2022 Duplicate papers and shorter version of already included publications.\nD. Step 3: Data Extraction and Analysis\nOur next step was to analyse the selected literature and extract data related to our research question. A paper selected from the literature was studied in-depth and assigned to one or more of the three RQ. Using the open coding technique, recurrent concepts were systematically identified [11]. Additional axial coding was required to reduce the growing complexity of some emerging concepts (e.g. different stages in data preprocessing like cleaning, splitting and other transformation steps) [11]. The authors frequently discussed emerging results to maintain code consistency and high abstraction levels. The emerging subtopics associated with each RQ are discussed in detail in Section IV.\nE. Step 4: Data Synthesis\nOur data analysis reports the maintainability challenges associated with different stages of the ML development process in Section IV. In our synthesis step, we identified how these different stages in ML development are interdependent and influence the maintainability of each other (Section V, Table 1). We then model such relationships using a mapping diagram, as shown in Fig. 2. Finally, in Section VI, we distilled new insights and a roadmap for ML tools developers and researchers as a final synthesis step. In order to ensure transparency and reproducibility, we also made all study artefacts publicly available at https://doi.org/10.5281/zenodo.6400559 .\nF. Threats to Validity\nThe internal validity is affected by potentially hidden bias, which can affect the consistency and accuracy of the results. SLR synthesis may be biased since it relies on subjective interpretation, especially in mapping Section V, where the author adds a layer of interpretation over the identified maintainability challenges. Although we followed our SLR protocol closely and resolved any conflict between the two authors, other researchers may have obtained slightly different results. External validity concerns the generalisability of the research study. Although we reviewed all the existing research studies, our findings are limited to the results from the final 56 primary studies. Consequently, the maintainability challenges from this SLR may not apply to all scenarios."}, {"title": "IV. RESULTS", "content": "Our analysis summarises the maintainability challenges associated with different stages of ML workflow. In subsection A, 18 papers were reviewed for the RQ1; in subsection B, 32 papers were reviewed for the RQ2, and in subsection C, 21 papers were reviewed for the RQ3.\nA. Data Engineering Maintainability Challenges\nDataset creation is a manual, slow and error-prone process with inherent bias associated with the data or its collection strategy, which affects the overall performance and quality of the model [L1], [L8], [L14]. In addition to that, lack of ownership, documentation and transparency in the creation process also undermines its quality [L15]. Usually, datasets are susceptible to missing data, outliers, adversarial and poisoned data [L16] and need to be handled using appropriate data processing strategies with ongoing maintenance as the model is continuously being updated with new data to avoid degradation in ML performance.\nData preprocessing pipeline handles data errors like missing data, outliers, lack of metadata, adversarial data and other quality attributes like bias and unfairness associated with the dataset to prepare the data for training [L1], [L2]. Furthermore, the model's performance and data features are entangled, so even minor changes in the data feature, like handling missing data or the choice of data splitting strategy, will affect the model's accuracy [L5]\u2013[L7], [L9], [L10]. ML workflow being an iterative process, poor model performance or accuracy may often necessitate reevaluating the choice of the data preprocessing steps and handling changes at different stages in the data pipeline, usually in a trial and error manner, which is a waste of resource and time.\nData management process includes data acquisition and integration from multiple sources, managing and facilitating manipulation of different modalities of data, modifying annotation or labelling, object serialization and also storing multiple formats of the data. The large scale nature of the data, particularly in Deep Learning (DL), makes this process quite complex and challenging when dealing with an actively evolving dataset [L3].In addition to that, the highly experimental nature of the ML project also demands provenance tracking, indexing, tracking data transformation steps, and storing intermediary results to ensure reproducibility and reuse of the processed data in the ML workflow. Most of these capabilities require significant maintenance effort and complex engineering and DevOps solutions [L4], [L13].\nData validation challenges are profound when data may change as it evolves and error due to possible bugs in the data source [L2] consequently making it complex to monitor and validate what is happening in the data. Most ML models are complex black boxes, so it becomes unclear whether the learned model still effectively solves the intended use case [L11]. The data validation pipeline will continuously check and monitor for data errors. However, it is pretty challenging to set up and demand substantial engineering resources for its development and maintenance; most engineering teams choose to ignore it in their workflow if it is not a requirement [L12].\nB. Model Engineering Maintainability Challenges\nHPO: Finding an optimal Hyper-parameter is a prolonged process; without expert knowledge, it is often done on a trial and error basis. Because models performance, efficiency and rate of convergence of models are all dependent on HPO [L22], [L23]. Wrong choices in these parameters often directly influence the learnability and rate of coverage in the Model training stage, which may often lead to retraining the model with different parameters. Many techniques are available for automated HPO like Bayesian optimisation, Meta-learning and Neural Architecture Search (NAS). Automating HPO requires setting up and maintaining an orchestration pipeline to run the optimisation and to keep track of these parameters and results for workflow reproducibility [L20], [L21].\nModel training: Maintainability challenges associated with Model training are setting up the infrastructure to automate the training pipeline and monitor the model performance for every iteration. The training utilises extensive computational resources and is very time consuming and costly, especially for Deep Neural Networks [L32]. It is often required to retrain with new data constantly to keep the model updated. Even the choice of model training techniques like incremental training and federated learning will add to the complexity of managing, integrating and deploying the training pipeline to other systems and applications [L18], [L19].\nModel testing challenges are mainly due to the stochastic nature of ML, rapidly changing input and expected output parts of test cases, oracle issues and emergent functional behaviour, which creates a moving target. Therefore, they are fundamentally different from traditional software projects. As a result, posing new challenges for authoring and maintaining unit tests and regression tests [L33]-[L37]. Finally, fault testing is also difficult to manage in ML when the learning is based on training data which makes it hard to interpret results from a complex model [L55], [L56]. There are still many challenges and open problems related to ML testing and its maintainability.\nModel deployment challenges arise when transitioning from the test or prototype stage to the production stage, where the model may be deployed and integrated with other models or applications in a different environment set up. These challenges include maintaining glue code, set up monitoring, logging and handling feedback loops [L9], [L10], [L27]. In addition to that, based on the requirement, it may be challenging to deploy models when the memory and power are a constraint on different platforms (Mobile or edge device). This may often result in many maintainability challenges, and deployment issues like model conversion, platform support, vendor-specific optimisation libraries and packages and other interoperability issues [L26], [L28], [L29].\nModel drift are caused by many factors such as data seasonality or changing drift types, evolving data source, and fluctuation in data collection [L5], [L17]. All these factors may lead to model staleness and degradation in performances. Most of the methods for detecting drift are expensive to implement because they require knowledge of drift detection algorithms, engineering the solution into existing pipelines, and ongoing maintenance to detect new drifts because it is not possible for the algorithm to detect all drift [L17], [L19].\nModel monitoring maintainability challenges are caused by evolving input data, fine-grained nature of the quality metrics, prediction bias, and understanding what are the critical metrics of data and model to monitor and how to alert on them [L19], [L24]. Furthermore, ML applications in production can also influence their behaviour over time and may lead to undesired feedback loops. Engineers have to build and maintain custom solutions in order to monitor the ML application effectively, with an orchestration pipeline, centralised dashboards for performance monitoring and governance, detecting feedback loops and continuously monitoring the retrained model [L26], [L27]. Systems logs are another means to monitor in an ML system, where log entries are typically created in an ad hoc, unstructured and uncoordinated fashion, thereby limiting their usefulness [L25].\nModel governance: It is common for high-risk ML applications to involve cross-disciplinary efforts to define quality metrics and requirements for monitoring the production environment, as well to access its quality in a real-life setting [L19]. However, the stochastic nature of ML systems make the process painstaking hard to document and manage the risk of the model and to ensure compliance with all regulations and minimum standards [L26]. There is also little guidance for sharing and version controlling ML models and their artefacts such as weights, hyper-parameters, and training and testing sets. Researchers often share ML models through customised websites or GitHub because there are no standard methods. Without publishing these artefacts, it is almost impossible to verify or build upon published results [L30], [L31] which impacts the reproducibility and verification of models.\nC. Current Maintainability Challenges in Building a ML system\nArchitecture of ML system: Current ML solutions do not meet the needs of practitioners [L24], [L41], [L45], more framework agnostic, easy to use tooling is needed to ensure the model can be maintained and updated from prototype to production stage [L38]. In addition, there is a disconnect between theory and practice when it comes to data processing, model building, quality assurance and how to maintain of ML systems [L39], [L40]. Most platforms tend to support only one model framework, leading to a tight coupling between the modelling and infrastructure layers. Consequently, practitioners are limited in their ability to develop models and prevented from exploring and using cutting-edge algorithms [L41]. In general, cloud providers do not think about providing a standard programming model that makes ML practitioners' lives easier; they typically use either a black box or a complex runtime environment to approach ML, which offers simplicity at the cost of flexibility [L43].\nQuality of ML system: ML has unique quality attributes concerns during development, such as data-dependent behaviour, detecting and responding to drift over time, handling bias, and timely capture of ground truth for retraining of a model to deliver a quality ML system [L42], [L50], [L52]. Additional, quality concerns include a lack of specifications, defined standards, and documentation in ML workflow and an inability to communicate about model quality due to a lack of a common language [L51], [L52]. Maintaining the quality is challenged when ML systems are built to adapt to new situations and contexts, which raises uncertainties regarding the runtime product quality and dependability in an evolving ML system [L35], [L53].\nAutoML: Non-experts have no idea which of the many ML algorithms to use in order to achieve good performance. AutoML alleviates this challenge by automating model selection and hyper-tuning [L44], [L46]. In practice, however, most existing AutoML systems ignore the important stage of data processing [L49]. Therefore, it is hard to minimise expert intervention easily with current computing technologies because developers need to understand how to perform feature engineering, data processing and evaluate bias, interpretation of the model performance and ongoing maintenance cost when dealing with TD like hidden-feedback loops [L47].\nMLOps: In practice, engineers spend significant effort developing ad hoc programs for new problems by writing glue code to connect components from different software libraries, processing different forms of raw input, and interfacing with external systems. All these steps are tedious and error-prone and lead to the emergence of brittle pipeline jungles [L27], [L45], [L48] which are hard to maintain in an MLOps setup. Additionally, using MLOps in a multi-organisation context creates the usual integration problems that emerge in APIs, data formats, privacy, and security, especially from the perspective of governance, auditing, and regulations [L54] which need to be maintained with custom solutions on an ongoing basis."}, {"title": "V. INTERDEPENDENCE OF MAINTAINABILITY", "content": "The synthesis from our data analysis shows how different stages in the ML workflow affect the maintainability of others (arrows visible in Fig. 2 and Table 1). A single arrow starting from one stage and pointing to another shows how the former stage impacts the maintainability of the latter. Two-way arrows indicate that the two ML stages impact each other. The number associated with each arrow in Fig 2. is referenced by row no in Table 1, where we explain the details and the sources from the SLR. This model will guide practitioners when evaluating the dependencies and maintenance costs for each stage of an ML system life-cycle.\nOur analysis also reveals an anti-pattern, which we call Repetitive Maintenance (blue oval with arrows in Fig. 2): Poor model accuracy or performance and other quality issues observed in Model testing [L33]-[L37], governance [L19], [L26] and monitoring [L19], [L24] stages may necessitate multiple costly modification and reevaluations of the steps in dataset creation [L8], [L14], data preprocessing [L5]\u2013[L7], [L9], [L10], data management [L3], data validation [L2], [L11], HPO [L22], [L23], and model training [L18], [L19]."}, {"title": "VI. IMPLICATIONS", "content": "A. Implication for Developer of the ML tools\nFrom our SLR, it emerged that many ML workflows demand provenance tracking, publishing of ML models and their artefacts, tracking data transformations, querying and storing intermediate steps [L4], [L30], [L31].There is a lack of standard tools and methods, which allows ML tools developers to build solutions based on these technology gaps.\nMany ML projects fail at the prototyping stage because setting up infrastructure for deployment and maintenance requires integration and management of glue code, ad-hoc pipelines, and data monitoring. So there is a need for additional tooling and frameworks to facilitate the transition from prototype to production environments where the model can easily be maintained and updated [L9], [27], [38].\nThere is also a lack of ease to use language-independent tools and solutions that can be integrated with any existing frameworks [L17], [L24], [L41], [45]. As a result, we see a chance for developers to develop tools and solutions, in particular improving data preprocessing [L7], data management [3], data validation [12], Model drift detection [17] and Model monitoring [19], [24].\nIn collaborative or multi-organisational projects, monitoring processes are complex because different teams have different metrics and requirements, especially in terms of governance and regulations and also a lack of standards to communicate about ML issues and their quality [4], [51]. Therefore, we need patterns of integration to make this work for collaborative ML projects [54].\nB. Implication for Researchers\nWhen developing robust and reliable ML systems, developers and researchers face an increasingly difficult challenge. Due to entanglement and data-dependent behaviour, different data processing steps and approaches affect the model's performance differently. It is unclear even for experienced developers how to select between several data processing steps and how they will affect the model's performance [1], [5].\nAs a community, ML testing and monitoring face many challenges and open problems [19], [24], [36]. The concern is that ML systems constantly adapt to new data, creating a moving target and posing a different set of challenges to maintain unit and regression testing than traditional software projects [33]\u2013[35], [37]. Therefore, it is essential to know what critical metrics of data and model to test and monitor and how to alarm them when monitoring ML applications [19], [24]. More research in ML testing and monitoring will benefit the entire ML community."}, {"title": "VII. CONCLUSION", "content": "In this SLR, we have screened more than 13000 papers and analysed 56 in-depth, compiling a comprehensive catalogue of maintainability challenges and their interdependence affecting ML workflow. Our findings will assist practitioners in understanding maintainability challenges and their impact at different stages of the ML workflow. This will help early identification, avoid costly pitfalls, and develop mitigation strategies. Moreover, we provide directions for tool development and further research to improve the maintainability of ML systems."}]}