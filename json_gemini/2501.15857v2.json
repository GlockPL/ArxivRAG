{"title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?", "authors": ["Yutong Yin", "Zhaoran Wang"], "abstract": "Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources. For example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce (C=g(B)=g(f(A)) ) even without encountering (ABC) together, showcasing the generalization ability of human intelligence. In this paper, we introduce a synthetic learning task, \"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers in replicating this skill and interpret its inner mechanism\u00b9. In the training phase, data consist of separated knowledge fragments from an overall causal graph. During testing, Transformers must infer complete causal graph traces by integrating these fragments. Our findings demonstrate that few-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on FTCT by revealing correct combinations of fragments, even if such combinations were absent in the training data. Furthermore, the emergence of compositional reasoning ability is strongly correlated with the model complexity and training-testing data similarity. We propose, both theoretically and empirically, that Transformers learn an underlying generalizable program from training, enabling effective compositional reasoning during testing.", "sections": [{"title": "Introduction", "content": "Humans exhibit a generalized reasoning ability that integrates knowledge from diverse sources. For example, if one learns (B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce ( C = g(B)=g(f(A)) ) without direct exposure to ( ABC ). We formally define this capability as compositional reasoning\u2014the skill to integrate discrete pieces of knowledge from multiple sources to form a coherent reasoning, even in the absence of explicit examples connecting these pieces during learning. This ability is a manifestation of a broader concept known as systematic compositionality\u2014 understanding and generating an infinite number of expressions by combining a finite set of known components and rules (Fodor and Pylyshyn, 1988; Chomsky, 2002). Transformer-based large language models demonstrate signs of compositional reasoning by producing comprehensive content that includes elements not likely to co-occur within the training data, suggesting the emergence of general intelligence (Press et al., 2022; Zhou et al., 2022a; Bubeck et al., 2023). However, the complexity and ambiguity of their natural language training and testing data make it hard to scientifically validate the compositional reasoning ability and explore the underlying mechanisms."}, {"title": "Related Works", "content": "Step-by-step reasoning. Chain-of-Thought (CoT) prompting (Nye et al., 2021; Wei et al., 2022c,b; Kojima et al., 2022) enables language models to conduct step-by-step reasoning, significantly boosting their performance in complex tasks like mathematical deduction and code generation (Cobbe et al., 2021; Suzgun et al., 2022; Zhou et al., 2022b; Lee et al., 2023; Achiam et al., 2023; Romera-Paredes et al., 2024). Our research emphasizes few-shot CoT prompting (Wei et al., 2022c), which initiates reasoning by integrating CoT examples into prompts. Interpretability studies suggest CoT's efficacy arises from models' enhanced expressivity via intermediate reasoning steps (Feng et al., 2024; Li et al., 2024b,a). Besides the expressivity perspective, we additionally examine CoT generalization in out-of-distribution (OOD) settings, showing few-shot CoT prompts can elicit correct reasoning even with previously unseen prompts. Another study (Prystawski et al., 2024) evaluates data's role in CoT's capacity for generalized reasoning. Our FTCT structure draws inspiration from their Bayesian networks, additionally inserting contextual noise and complicating value relationships. While they focus on locality structure's impact on CoT efficacy, we investigate how various training factors influence compositional reasoning emergence and conduct an in-depth analysis of the mechanisms within Transformer structures that elicit such capability.\nIn-context learning. In-context learning (ICL) (Brown, 2020; Garg et al., 2022; Min et al., 2022; Wei et al., 2023) enables language models to perform various tasks by interpreting examples within the provided prompt, without needing explicit tuning. This capability allows Transformers trained on our FTCT to emulate the order of vertices in few-shot examples. Several theoretical studies (Xie et al., 2021; Li et al., 2023; Wang et al., 2024; Hahn and Goyal, 2023; Wies et al., 2024; Zhang et al., 2023) treat ICL as implicit Bayesian inference. Another set of works (Dai et al., 2022; Von Oswald et al., 2023; Aky\u00fcrek et al., 2022; Ahn et al., 2024) argue that ICL functions similarly to gradient descent from a function approximation perspective. Notably, a mechanism within Transformers, known as \"induction heads\" (Elhage et al., 2021; Olsson et al., 2022; Bietti et al., 2024), is identified"}, {"title": "Fragmented at Training, Chained at Testing", "content": "We introduce the structure of FTCT dataset and corresponding training and testing loss, illustrating the reason why it measures the model's compositional reasoning ability."}, {"title": "Causal Strcture", "content": "We represent knowledge relationships with a directed graph G = (V,E), where knowledge points are simulated by vertices V, a subset of the alphabet Vall := {A, B, . . ., Z, a, b, . . ., z}. Each vertex v in V has a value q(v) from its associated set VALS(v) \u2282 Z. Relationships between knowledge points are represented by the edges set E. Each edge e := (v1, v2) \u2208 E defines the relationship between the parent vertex v\u2081 and the child vertex v2 by the operation op(e), satisfying q(v2) = op(e) o q(v1). We assume that op(e) only represents addition or subtraction operation like +a or -b. Multi-step reasoning paths are represented by the chains T(G) := {[V1, V2, . . ., Vn] | n \u2208 N, (Vi, Vi+1) \u2208 E}. The depth of G, denoted as N, is the length of the longest chain in T(G). The \u201cCausal Structure\""}, {"title": "Data Generation", "content": "To simulate disconnected knowledge, training data excludes the longest chain in T(G) and instead includes shorter child chains with length M < N, interspersed with M' noise vertices from Vall - V. Child chains vertices are merged with noise vertices, preserving their order. Child chain vertices receive values based on their edge operations, while noise vertices get random values. The final vertex-value sequence is formatted as seq := [V1, Q1, ..., Um, qm], where m = M + M'. As shown in \"Fragmented at Training\""}, {"title": "Chained at testing", "content": "To test models' compositional reasoning ability, the testing data consists of longest chains (length N) from T(G) without noise, formulating the sequence seq := [v1, q1,..., VN, qN], where (v1,...,\u03c5\u03bd) \u2208T(G)."}, {"title": "Few-shot learning", "content": "For both training and testing datasets, multiple sequences with the same vertices order are concatenated into few-shot document, which is formatted as\ndock := [seq(1), \\n, ..., \\n, seq(k)], where seq(i) := [v1, q\u00ba), ..., UL, q)]\nwhere L is the sequence length which can be either m or N, and k is the shots number ranging from 0 to K. The k-shot input and label are formatted as\ninpk := dock + [v1, qk+1)], labk [02, 92\n:=,..., UL, q(k+1)].\nThe model should generate lab autoregressively from inpk. Especially, the zero-shot input inpo requires reasoning without any preceding examples."}, {"title": "Downstream processing", "content": "This process adapts sequences into natural language-like sentences by adding punctuation, contextual details, and stating the reasoning goal upfront. As illustrated in \u201cDownstream Processing\", a few-shot document like [A, 110, Z, 1, B, 111, H, 5, \\n, A, 102, Z, 4, B, 103, H, 9] transforms into the sentence\n\"H=?: ... A=110, ... Z=1, ... B=111,\nH=5 \\n H=?: A=102,Z=4, ... B=103, ... H=9\"\""}, {"title": "Training and Testing Loss", "content": "For any input and label sampled from Dtrain, we train the language model to autoregressively generate label given input. With the length of label lab defined as dk, the training loss is formatted as\nLtrain := -Elab,inp~Dtrain \u2211\u2211log (Pmodel (lab+1 | inp + lab\u0131:)).\nFor any input and label sampled from Dtest, given the input, we test how well a language model can generate sentence having the same vertex-value pairs as the label. Specifically, We define a decoding function dec(lab) := lab = [v2, q2, ...,UN,qN] to decode the vertex-value information from the processed label. Given the input, the sentence generated by the model is defined as model(inp). We measure model's testing loss with k-shot prompt by\nLk test:= -Ek, labk~Dtest 1{dec(model(inp*))=dec(lab*)}(3.1)"}, {"title": "Empirical Findings", "content": "Our empirical findings highlight the essential role of few-shot CoT prompts in enabling compositional reasoning in Transformers during testing. We evaluate model's compositional reasoning ability using the following criterion:\nWhole chain accuracy: Measures if the model's generation contains all vertices and values along the reasoning chain in a correct order. For (inp, lab) sampled from Dtest, it measures whether dec(model(inp*)) contains all elements from dec(lab) in a correct order.\nFurther, we decompose the compositional reasoning ability into two sublevel abilities the ability of generating correct vertices order and the ability of deducing correct values given preceding paths, which are evaluated respectively by these criteria:\nTesting vertices accuracy: Measures if the model correctly outputs all vertices in dec(lab).\nTesting values accuracy: Measures if the model outputs correct values of intermediate vertices, given correct preceding reasoning paths sampled from Dtest. For the causal structure in Figure 1, this is tested by prompting models with sentences like \u201c... A=100, B="}, {"title": "Few-shot CoT Prompting Enables Compositional Reasoning", "content": "Our empirical findings highlight the essential role of few-shot CoT prompts in enabling compositional reasoning in Transformers during testing. We evaluate model's compositional reasoning ability using the following criterion:\nWhole chain accuracy: Measures if the model's generation contains all vertices and values along the reasoning chain in a correct order. For (inpk, labk) sampled from Dtest, it measures whether dec(model(inpk)) contains all elements from dec(labk) in a correct order.\nFurther, we decompose the compositional reasoning ability into two sublevel abilities\u2014the ability of generating correct vertices order and the ability of deducing correct values given preceding paths, which are evaluated respectively by these criteria:\nTesting vertices accuracy: Measures if the model correctly outputs all vertices in dec(labk).\nTesting values accuracy: Measures if the model outputs correct values of intermediate vertices, given correct preceding reasoning paths sampled from Dtest. For the causal structure in Figure 1, this is tested by prompting models with sentences like \u201c... A=100, B="}, {"title": "The Similarity between Training and Testing Data Determines the Emger-gence of Compositional Reasoning", "content": "For each FTCT task, we measure the similarity between training and testing data by the relative knowledge ratio \\( \\lambda := M/N \\), where M is the child chain lengths and N is the causal graph depth. We find that compositional reasoning emerges as \\( \\lambda \\) increases.  illustrates the relationship between \\( \\lambda \\) and model's compositional reasoning ability. For each \\( \\lambda \\), the compositional reasoning ability is measured by the optimal few-shot testing performance of Transformers trained on tasks whose relative knowledge ratio is \\( \\lambda \\). A phase transition occurs: compositional reasoning remains weak when \\( \\lambda < 0.3 \\) and distinctly emerges when \\( \\lambda \\geq 0.3 \\). In essence, a larger \\( \\lambda \\) makes few-shot CoT prompts more similar to training data, thereby enhancing testing performance. However, the fact that testing accuracy approaches one with \\( \\lambda = 0.3 \\)\u2014a ratio significantly smaller than 1-underscores the non-triviality of our results."}, {"title": "Multi-Layer Attention Mechanism Enables Compositional Reasoning", "content": "We show that compared with other simpler structures, multi-layer Transformers excel at imitating vertices order from few-shot examples and deducing correct values with preceding reasoning paths, leading to outstanding compositional reasoning ability. In addition to compositional reasoning metrics (Section 4.1), we introduce the following criteria to evaluate in-distribution performance:\nValidation loss: Tracks the validation loss during training.\nTraining vertices accuracy: Assesses how well the model imitates the vertices order from the few-shot examples sampled from Dtrain.\nTraining values accuracy: Measures if the model outputs correct values of vertices from child chains, given preceding reasoning paths sampled from Dtrain.\nWe assess the performance of various models on the FTCT task with a causal structure depth of 5 and a child chain length of 3. Models include Transformers (TF) of various layers and heads, and multi-layer perceptrons (MLPs) of different depths . Depth of Transformer enables the imitation of vertices. Table 1 indicates that Transformers with at least 2 layers and 2 heads achieve optimal in-distribution and compositional reasoning performance. As complexity decreases, performance deteriorates, notably with a significant drop in the vertices accuracy, both training and testing, while values accuracy remains optimal. Such phenomenon indicates that depth in Transformers is crucial for imitating vertices order in few-shot examples, hence enhancing compositional reasoning. This is because induction heads for in-context learning are less likely in single-layer Transformers .\nAttention mechanism enables the deduction of sparse values information. For MLPs with appropriate window sizes, both the training and testing values accuracy remain low. Conversely, even the simplest Transformer (1 layer, 1 head) achieves nearly optimal values accuracy, suggesting that MLPs struggle to capture sparse value information in noisy contexts as effectively as Transformers. Interestingly, MLPs perform well in generating vertices order during testing but not during training, possibly due to the extra noise vertices in the training data, suggesting a different knowledge memorization approach that warrants further study."}, {"title": "Transformer Does Compositional Reasoning via the Underlying Program", "content": "As discussed in Section 4.3, the multi-layer attention mechanism of Transformers is crucial for compositional reasoning. However, how Transformers achieve this ability through training remains"}, {"title": "Underlying Program", "content": "We construct a text-generating program which provably achieves optimal performance on both training and testing data. Key components are summarized here, with an algorithm (Algorithm 1) in the Appendix. Given any input sentence 21:7 that contains at least 1-shot example, the program executes the following two parts iteratively:\nIn-context learning: If the last token zy is a comma \",\", zT-3 must be a vertex vi. The program identifies all vi in the previous few-shot examples and attends to their next vertex Vi+1, returning vi+1's contextual tokens.\nParent retrieving: If the last token zy is an equation token \u201c=\u201d, zT\u22121 must be a vertex vj. The program retrieves the parent of vj from the preceding context. If vj belongs to the child chain from T(G) and has a parent vj\u2081 with value qj\u2081 in the preceding context, the program returns qj = op(Vj1, Vj) \u00a9 qj\u2081 with probability one. Otherwise, it returns value qj randomly sampled from vi's value set VALS(vi)."}, {"title": "Transformer is Expressive Enough to Simulate the Underlying Program", "content": "We prove that Transformer is expressive enough to simulate the underlying program by explicitly constructing a 2 layers Transformer. Representing the model parameters by 0, we state\nLemma 5.3. There exists a 2 layer Transformer with parameters 0* that approximates Algorithm 1 with arbitrarily small error.\nThe ability of a 2-layer Transformer to simulate the underlying program aligns with the empirically observed performance . By expressing the training and testing loss as functions of the model parameters 0, we summarize Lemmas 5.1, 5.2, and 5.3 into the following theorem.\nTheorem 5.4. There exists a Transformer model parameterized by 0* that satisfies\nLtrain (0*) - ming Ltrain (0) | < 6\nLkest (0*) = 0\nwhere e is an arbitrarily small value,\nwhere k = 2,..., \u039a."}, {"title": "Empirical Evidence of the Underlying Program", "content": "We present empirical evidence showing that Transformers are simulating the underlying program through two mechanisms-induction head and attention assignment, which respectively facilitate the in-context learning and parent retrieving."}, {"title": "Induction Heads", "content": "By plotting Transformer's attention heat map, we provide empirical evidence showing the existence of induction heads that enables in-context learning. As described in previous works (Elhage et al., 2021; Olsson et al., 2022), induction heads are two heads of the Transformer in different layers that collaborate to copy patterns. For example, with input sentences like \u201c. . . [A][B]. . . [A]\u201d, the first head in a shallow layer copies information from the first [A] to [B], while the second head in a deeper layer recognizes [B] and retrieves its context from [A], guiding the model to output [B] as the next token. In our task, we discovered similar induction heads operating in a slightly different manner. Given an input sentence formatted as (for clarity, we highlight comma tokens at different positions with boxes and different colors):\n\"...Vi = qi,... Vi+1 =qi+1... \\n ... v\u2081 = d\u2081Li\u201c\nThe head in the shallower layer copies the information of vi and vi+1 to \",\". The head in the deeper layer attends \",\" along with the information of vi+1 to \",\", making the model to output the contextual information of vi+1.\nTo empirically demonstrate this pattern, we trained a 3-layer, 3-head Transformer on the FTCT task with a causal structure depth of 13 and a child chain length of 6, generating attention heatmaps for each layer.  For each comma in the black frame, the distribution of its attention weights to preceding tokens is shown using colored boxes the brighter the color, the more attention paid. In Layer 1, each comma attends to its previous two vertices, recording their information in its hidden state. In Layer 2, each comma uses this information to identify the preceding comma whose vertex is next to be output."}, {"title": "Attention Assignment", "content": "By linear probing (Hewitt and Manning, 2019; Clark, 2019; Allen-Zhu and Li, 2023), we empirically show that the parent retrieving is facilitated by proper attention assignment-focusing on the value of parent vertex while ignoring others.\nFor each sentence sampled from either the training or testing data, we identify the equation token \"=\", where its corresponding vertex has a parent in the preceding context. Specifically, we examine input formatted as:\n\u201c ... Uj\u2081 = qj1, . . . Vj = q...\u201c\nwhere vj\u2081 is the parent of vj. We construct the probing dataset by each time picking a position i < j (including j1), replacing qi with randomly sampled qf, and recording the Transformer's hidden state for this modified sentence. We train a linear function (details in Appendix G.3) to predict q from the hidden states. If the Transformer attends to qi, the linear function should predict q with high accuracy. If not, the accuracy should be low."}, {"title": "Conclusion", "content": "Our research validates the potential of Transformers in doing compositional reasoning on synthetic data and investigates the inner mechanism eliciting such ability. We demonstrate that few-shot CoT prompting enables Transformers to perform compositional reasoning by providing the information of correct order of knowledge points. We also find that compositional reasoning ability emerges when the training-testing data similarity and the model complexity are above certain thresholds. We further show that Transformers develop compositional reasoning by learning an underlying program during training, which minimizes both training and testing loss. This program leverages in-context learning and parent retrieving mechanisms, facilitated by induction heads and attention assignment.\nThrough experiments on synthetic data, we demonstrate the potential of Transformers to develop generalized reasoning skills, indicating that the impressive performance of contemporary large language models extends beyond mere memorization of vast data. While our conclusions may not directly apply to real-world models trained on extensive natural language datasets, we believe that our analysis offers valuable insights into the training processes and understanding of today's large language models."}]}