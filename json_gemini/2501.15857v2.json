[{"title": "Are Transformers Able to Reason by Connecting Separated Knowledge in Training Data?", "authors": ["Yutong Yin", "Zhaoran Wang"], "abstract": "Humans exhibit remarkable compositional reasoning by integrating knowledge from various sources.\nFor example, if someone learns ( B = f(A) ) from one source and ( C = g(B) ) from another,\nthey can deduce (C=g(B)=g(f(A)) ) even without encountering (ABC) together, showcasing the\ngeneralization ability of human intelligence. In this paper, we introduce a synthetic learning task,\n\"FTCT\" (Fragmented at Training, Chained at Testing), to validate the potential of Transformers\nin replicating this skill and interpret its inner mechanism\u00b9. In the training phase, data consist of\nseparated knowledge fragments from an overall causal graph. During testing, Transformers must\ninfer complete causal graph traces by integrating these fragments. Our findings demonstrate that\nfew-shot Chain-of-Thought prompting enables Transformers to perform compositional reasoning on\nFTCT by revealing correct combinations of fragments, even if such combinations were absent in the\ntraining data. Furthermore, the emergence of compositional reasoning ability is strongly correlated\nwith the model complexity and training-testing data similarity. We propose, both theoretically and\nempirically, that Transformers learn an underlying generalizable program from training, enabling\neffective compositional reasoning during testing.", "sections": [{"title": "Introduction", "content": "Humans exhibit a generalized reasoning ability that integrates knowledge from diverse sources. For\nexample, if one learns (B = f(A) ) from one source and ( C = g(B) ) from another, they can deduce\n( C = g(B)=g(f(A)) ) without direct exposure to ( ABC ). We formally define this capability as\ncompositional reasoning\u2014the skill to integrate discrete pieces of knowledge from multiple sources to\nform a coherent reasoning, even in the absence of explicit examples connecting these pieces during\nlearning. This ability is a manifestation of a broader concept known as systematic compositionality\u2014\nunderstanding and generating an infinite number of expressions by combining a finite set of known\ncomponents and rules (Fodor and Pylyshyn, 1988; Chomsky, 2002). Transformer-based large language\nmodels demonstrate signs of compositional reasoning by producing comprehensive content that\nincludes elements not likely to co-occur within the training data, suggesting the emergence of general\nintelligence (Press et al., 2022; Zhou et al., 2022a; Bubeck et al., 2023). However, the complexity and\nambiguity of their natural language training and testing data make it hard to scientifically validate\nthe compositional reasoning ability and explore the underlying mechanisms."}, {"title": "Related Works", "content": "Step-by-step reasoning. Chain-of-Thought (CoT) prompting (Nye et al., 2021; Wei et al.,\n2022c,b; Kojima et al., 2022) enables language models to conduct step-by-step reasoning, significantly\nboosting their performance in complex tasks like mathematical deduction and code generation\n(Cobbe et al., 2021; Suzgun et al., 2022; Zhou et al., 2022b; Lee et al., 2023; Achiam et al., 2023;\nRomera-Paredes et al., 2024). Our research emphasizes few-shot CoT prompting (Wei et al., 2022c),\nwhich initiates reasoning by integrating CoT examples into prompts. Interpretability studies suggest\nCoT's efficacy arises from models' enhanced expressivity via intermediate reasoning steps (Feng\net al., 2024; Li et al., 2024b,a). Besides the expressivity perspective, we additionally examine\nCoT generalization in out-of-distribution (OOD) settings, showing few-shot CoT prompts can elicit\ncorrect reasoning even with previously unseen prompts. Another study (Prystawski et al., 2024)\nevaluates data's role in CoT's capacity for generalized reasoning. Our FTCT structure draws\ninspiration from their Bayesian networks, additionally inserting contextual noise and complicating\nvalue relationships. While they focus on locality structure's impact on CoT efficacy, we investigate\nhow various training factors influence compositional reasoning emergence and conduct an in-depth\nanalysis of the mechanisms within Transformer structures that elicit such capability.\nIn-context learning. In-context learning (ICL) (Brown, 2020; Garg et al., 2022; Min et al., 2022;\nWei et al., 2023) enables language models to perform various tasks by interpreting examples within\nthe provided prompt, without needing explicit tuning. This capability allows Transformers trained\non our FTCT to emulate the order of vertices in few-shot examples. Several theoretical studies (Xie\net al., 2021; Li et al., 2023; Wang et al., 2024; Hahn and Goyal, 2023; Wies et al., 2024; Zhang et al.,\n2023) treat ICL as implicit Bayesian inference. Another set of works (Dai et al., 2022; Von Oswald\net al., 2023; Aky\u00fcrek et al., 2022; Ahn et al., 2024) argue that ICL functions similarly to gradient\ndescent from a function approximation perspective. Notably, a mechanism within Transformers,\nknown as \"induction heads\" (Elhage et al., 2021; Olsson et al., 2022; Bietti et al., 2024), is identified"}, {"title": "Fragmented at Training, Chained at Testing", "content": "We introduce the structure of FTCT dataset and corresponding training and testing loss, illustrating\nthe reason why it measures the model's compositional reasoning ability."}, {"title": "Causal Strcture", "content": "We represent knowledge relationships with a directed graph G = (V,E), where knowledge points are\nsimulated by vertices V, a subset of the alphabet Vall := {A, B, . . ., Z, a, b, . . ., z}. Each vertex v in\nV has a value q(v) from its associated set VALS(v) \u2282 Z. Relationships between knowledge points\nare represented by the edges set E. Each edge e := (v1, v2) \u2208 E defines the relationship between\nthe parent vertex v\u2081 and the child vertex v2 by the operation op(e), satisfying q(v2) = op(e) o q(v1).\nWe assume that op(e) only represents addition or subtraction operation like +a or -b. Multi-step\nreasoning paths are represented by the chains T(G) := {[v1, v2, . . ., Vn] | n \u2208 N, (Vi, Vi+1) \u2208 E}. The\ndepth of G, denoted as N, is the length of the longest chain in T(G). The \u201cCausal Structure\" in\nFigure 1 illustrates a causal structure with depth N = 3."}, {"title": "Data Generation", "content": "Step 1.1. Fragmented at Training:\nTo simulate disconnected knowledge, training data\nexcludes the longest chain in T(G) and instead includes shorter child chains with length M < N,\ninterspersed with M' noise vertices from Vall - V. Child chains vertices are merged with noise vertices,\npreserving their order. Child chain vertices receive values based on their edge operations, while noise\nvertices get random values. The final vertex-value sequence is formatted as seq := [V1, Q1, ..., Um, qm],\nwhere m = M + M'. As shown in \"Fragmented at Training\" in Figure 1, the training data includes\nsequences like [A, 100, Z, 3, B, 101, H, 1], where [A, B] is a child chain from T(G) with values\nfollowing the \"+1\u201d operation, and [Z, H] are sampled from Vall \u2013 V with randomly assigned values."}, {"title": "Chained at testing:", "content": "To test models' compositional reasoning ability, the testing\ndata consists of longest chains (length N) from T(G) without noise, formulating the sequence\nseq := [v1, q1,..., VN, qN], where (v1,...,\u03c5\u03bd) \u2208T(G). Refer to \u201cChained at Testing\" in Figure 1."}, {"title": "Few-shot learning:", "content": "For both training and testing datasets, multiple sequences with\nthe same vertices order are concatenated into few-shot document, which is formatted as\ndock := [seq(1), \\n, ..., \\n, seq(k)], where seq(i) := [v1, q\u00ba), ..., UL, q)]\nwhere L is the sequence length which can be either m or N, and k is the shots number ranging from\n0 to K. The k-shot input and label are formatted as\ninpk := dock + [v1, qk+1)], labk := [02, 92 (k+1)\n,..., UL, q(k+1)].\nThe model should generate lab autoregressively from inpk. Especially, the zero-shot input inpo\nrequires reasoning without any preceding examples. See \"Few-Shot Examples\" in Figure 1."}, {"title": "Downstream processing:", "content": "This process adapts sequences into natural language-like\nsentences by adding punctuation, contextual details, and stating the reasoning goal upfront. As\nillustrated in \u201cDownstream Processing\" in Figure 1, a few-shot document like [A, 110, Z, 1, B, 111,\nH, 5, \\n, A, 102, Z, 4, B, 103, H, 9] transforms into the sentence\n\"H=?: ... A=110, ... Z=1, ... B=111, ... H=5 \\n H=?: ... A=102, ... Z=4, ... B=103, ... H=9\"\nwith \"...\" indicating tokens' context. The processed input and label are denoted as inp and lab\nFor brevity, we define Dtrain as the distribution of inputs and labels generated by Step 1.1, 2, 3,\nand Dtest as the distribution of inputs and labels generated by Step 1.2, 2, 3. The detailed data\ngeneration with specific sampling methods is in Appendix B."}, {"title": "Training and Testing Loss", "content": "Training loss: For any input and label sampled from Dtrain, we train the language model to\nautoregressively generate label given input. With the length of label lab defined as dk, the training\nloss is formatted as\nL_{train} := -\\mathbb{E}_{lab, inp \\sim D_{train}} \\sum_{k=0}^{K-1} \\sum_{t=1}^{d_k -1} log (P_{model}(lab_{t+1}^k | inp^k + lab_{1:t}^k)).\nTesting loss: For any input and label sampled from Dtest, given the input, we test how well a\nlanguage model can generate sentence having the same vertex-value pairs as the label. Specifically,\nWe define a decoding function dec(lab) := lab = [v2, q2, ...,UN,qN] to decode the vertex-value\ninformation from the processed label. Given the input, the sentence generated by the model is\ndefined as model(inp). We measure model's testing loss with k-shot prompt by\nL_{k test}:= -\\mathbb{E}_{inpk, labk \\sim D_{test}} 1\\{dec(model(inp^k))=dec(lab^k)\\}."}, {"title": "Empirical Findings", "content": "Our empirical findings highlight the essential role of few-shot CoT prompts in enabling compositional\nreasoning in Transformers during testing. We evaluate model's compositional reasoning ability using\nthe following criterion:\nWhole chain accuracy: Measures if the model's generation contains all vertices and values\nalong the reasoning chain in a correct order. For (inpk, labk) sampled from Dtest, it measures\nwhether dec(model(inpk)) contains all elements from dec(labk) in a correct order.\nFurther, we decompose the compositional reasoning ability into two sublevel abilities the ability of\ngenerating correct vertices order and the ability of deducing correct values given preceding paths,\nwhich are evaluated respectively by these criteria:\nTesting vertices accuracy: Measures if the model correctly outputs all vertices in dec(labk).\nTesting values accuracy: Measures if the model outputs correct values of intermediate\nvertices, given correct preceding reasoning paths sampled from Dtest. For the causal structure\nin Figure 1, this is tested by prompting models with sentences like \u201c... A=100, B=\". The\nmodel is considered to output accurate values if and only if it outputs \"101\" as the next token.\nWe trained 3-layer 3-head GPT-2-like Transformers (details in Appendix G) on FTCT training set\nwith varying graph depths and child chain lengths. Figure 2 (left) shows the testing performance for\nTransformers trained on graph depths N = 5, 10, 15, with k-shot CoT prompting (k from 0 to 4).\nDifferent curve colors represent different child chain lengths M = 2,3,4,6. Our conclusions are:\nFew-shot CoT enables compositional reasoning by revealing correct vertices order.\nWhole chain accuracy is low with zero-shot prompts but increases sharply with more shots. At\nzero-shot, values accuracy is optimal while vertices accuracy is near zero. This indicates that\nfew-shot CoT prompts enhance models' compositional reasoning by revealing the correct vertices\norder. Notably, such order has not appeared in the training data. The ability to understand and\nimitate the OOD vertices order stems from Transformers' in-context learning via induction heads\n(Section 5, 6).\nTransformer outputs correct values with OOD reasoning paths. High testing values\naccuracy shows Transformers' robust performance in deducing correct values with the OOD com-\npositional reasoning paths. Such an ability ensures models to iteratively output correct values of"}, {"title": "Few-shot CoT Prompting Enables Compositional Reasoning", "content": "Our empirical findings highlight the essential role of few-shot CoT prompts in enabling compositional\nreasoning in Transformers during testing. We evaluate model's compositional reasoning ability using\nthe following criterion:\nWhole chain accuracy: Measures if the model's generation contains all vertices and values\nalong the reasoning chain in a correct order. For (inpk, labk) sampled from Dtest, it measures\nwhether dec(model(inpk)) contains all elements from dec(labk) in a correct order.\nFurther, we decompose the compositional reasoning ability into two sublevel abilities the ability of\ngenerating correct vertices order and the ability of deducing correct values given preceding paths,\nwhich are evaluated respectively by these criteria:\nTesting vertices accuracy: Measures if the model correctly outputs all vertices in dec(lab).\nTesting values accuracy: Measures if the model outputs correct values of intermediate\nvertices, given correct preceding reasoning paths sampled from Dtest. For the causal structure\nin Figure 1, this is tested by prompting models with sentences like \u201c... A=100, B=\". The\nmodel is considered to output accurate values if and only if it outputs \"101\" as the next token."}, {"title": "The Similarity between Training and Testing Data Determines the Emger-gence of Compositional Reasoning", "content": "For each FTCT task, we measure the similarity between training and testing data by the relative\nknowledge ratio \\lambda := M/N, where M is the child chain lengths and N is the causal graph depth. We\nfind that compositional reasoning emerges as \\lambda increases. Figure 2 (right) illustrates the relationship\nbetween \\lambda and model's compositional reasoning ability. For each \\lambda, the compositional reasoning\nability is measured by the optimal few-shot testing performance of Transformers trained on tasks\nwhose relative knowledge ratio is \\lambda. A phase transition occurs: compositional reasoning remains\nweak when  \\lambda < 0.3 and distinctly emerges when \\lambda \u2265 0.3. In essence, a larger \\lambda makes few-shot CoT\nprompts more similar to training data, thereby enhancing testing performance. However, the fact\nthat testing accuracy approaches one with  \\lambda = 0.3\u2014a ratio significantly smaller than 1-underscores\nthe non-triviality of our results."}, {"title": "Multi-Layer Attention Mechanism Enables Compositional Reasoning", "content": "We show that compared with other simpler structures, multi-layer Transformers excel at imitating\nvertices order from few-shot examples and deducing correct values with preceding reasoning paths,\nleading to outstanding compositional reasoning ability. In addition to compositional reasoning\nmetrics (Section 4.1), we introduce the following criteria to evaluate in-distribution performance:\nValidation loss: Tracks the validation loss during training.\nTraining vertices accuracy: Assesses how well the model imitates the vertices order from\nthe few-shot examples sampled from Dtrain.\nTraining values accuracy: Measures if the model outputs correct values of vertices from\nchild chains, given preceding reasoning paths sampled from Dtrain.\nWe assess the performance of various models on the FTCT task with a causal structure depth of 5\nand a child chain length of 3. Models include Transformers (TF) of various layers and heads, and\nmulti-layer perceptrons (MLPs) of different depths (details in Appendix G). Table 1 summarizes the\nresults. For brevity, we only show the performance of models prompted by CoT with the optimal\nshots number.\nDepth of Transformer enables the imitation of vertices. Table 1 indicates that Transform-\ners with at least 2 layers and 2 heads achieve optimal in-distribution and compositional reasoning\nperformance. As complexity decreases, performance deteriorates, notably with a significant drop\nin the vertices accuracy, both training and testing, while values accuracy remains optimal. Such\nphenomenon indicates that depth in Transformers is crucial for imitating vertices order in few-shot\nexamples, hence enhancing compositional reasoning. This is because induction heads for in-context\nlearning are less likely in single-layer Transformers (Section 6).\nAttention mechanism enables the deduction of sparse values information. For MLPs\nwith appropriate window sizes (details in Appendix G.2), both the training and testing values\naccuracy remain low. Conversely, even the simplest Transformer (1 layer, 1 head) achieves nearly\noptimal values accuracy, suggesting that MLPs struggle to capture sparse value information in noisy\ncontexts as effectively as Transformers. Interestingly, MLPs perform well in generating vertices order\nduring testing but not during training, possibly due to the extra noise vertices in the training data,\nsuggesting a different knowledge memorization approach that warrants further study."}, {"title": "Transformer Does Compositional Reasoning via the Underlying Program", "content": "As discussed in Section 4.3, the multi-layer attention mechanism of Transformers is crucial for\ncompositional reasoning. However, how Transformers achieve this ability through training remains"}, {"title": "Underlying Program", "content": "We construct a text-generating program which provably achieves optimal performance on both\ntraining and testing data. Key components are summarized here, with an algorithm (Algorithm 1)\nin the Appendix. Given any input sentence 21:7 that contains at least 1-shot example, the program\nexecutes the following two parts iteratively:\nIn-context learning: If the last token zy is a comma \",\", ZT-3 must be a vertex vi. The\nprogram identifies all vi in the previous few-shot examples and attends to their next vertex\nVi+1, returning vi+1's contextual tokens.\nParent retrieving: If the last token zy is an equation token \u201c=\", zT\u22121 must be a vertex vj.\nThe program retrieves the parent of vj from the preceding context. If vj belongs to the child\nchain from T(G) and has a parent vj\u2081 with value qj\u2081 in the preceding context, the program\nreturns qj = op(Vj1, Vj) \u00a9 qj\u2081 with probability one. Otherwise, it returns value qj randomly\nsampled from vi's value set VALS(vi)."}, {"title": "Transformer is Expressive Enough to Simulate the Underlying Program", "content": "We prove that Transformer is expressive enough to simulate the underlying program by explicitly\nconstructing a 2 layers Transformer. Representing the model parameters by \u03b8, we state\nLemma 5.3. There exists a 2 layer Transformer with parameters \u03b8* that approximates Algorithm 1\nwith arbitrarily small error.\nThe ability of a 2-layer Transformer to simulate the underlying program aligns with the empirically\nobserved performance in Table 1. By expressing the training and testing loss as functions of the\nmodel parameters \u03b8, we summarize Lemmas 5.1, 5.2, and 5.3 into the following theorem.\nTheorem 5.4. There exists a Transformer model parameterized by \u03b8* that satisfies\n{\nL_{train} (\u03b8^*) - min_{\u03b8} L_{train} (\u03b8) | < \u03b5, where \u03b5 is an arbitrarily small value,\nL_{kest} (\u03b8^*) = 0, where k = 2,..., K.\n}"}, {"title": "Empirical Evidence of the Underlying Program", "content": "We present empirical evidence showing that Transformers are simulating the underlying program\nthrough two mechanisms-induction head and attention assignment, which respectively facilitate\nthe in-context learning and parent retrieving."}, {"title": "Induction Heads", "content": "By plotting Transformer's attention heat map, we provide empirical evidence showing the existence\nof induction heads that enables in-context learning. As described in previous works (Elhage et al.,\n2021; Olsson et al., 2022), induction heads are two heads of the Transformer in different layers that\ncollaborate to copy patterns. For example, with input sentences like \u201c. . . [A][B]. . . [A]\u201d, the first head\nin a shallow layer copies information from the first [A] to [B], while the second head in a deeper\nlayer recognizes [B] and retrieves its context from [A], guiding the model to output [B] as the next\ntoken. In our task, we discovered similar induction heads operating in a slightly different manner.\nGiven an input sentence formatted as (for clarity, we highlight comma tokens at different positions\nwith boxes and different colors):\n\"...Vi = qi,... Vi+1 = qi+1 ... \\n ... v\u2081 = d\u2081...\"\nThe head in the shallower layer copies the information of vi and vi+1 to \u201c,\". The head in the deeper\nlayer attends \",\" along with the information of vi+1 to \",\", making the model to output the contextual\ninformation of vi+1.\nTo empirically demonstrate this pattern, we trained a 3-layer, 3-head Transformer on the FTCT\ntask with a causal structure depth of 13 and a child chain length of 6, generating attention heatmaps\nfor each layer. Complete heatmap plots are available in Appendix L, and an abbreviated version is\nshown in Table 2 which displays the average attention weights of heads in different layers for their\nrespective tokens. For each comma in the black frame, the distribution of its attention weights to\npreceding tokens is shown using colored boxes the brighter the color, the more attention paid. In\nLayer 1, each comma attends to its previous two vertices, recording their information in its hidden\nstate. In Layer 2, each comma uses this information to identify the preceding comma whose vertex\nis next to be output."}, {"title": "Attention Assignment", "content": "By linear probing (Hewitt and Manning, 2019; Clark, 2019; Allen-Zhu and Li, 2023), we empirically\nshow that the parent retrieving is facilitated by proper attention assignment-focusing on the value\nof parent vertex while ignoring others.\nFor each sentence sampled from either the training or testing data, we identify the equation\ntoken \u201c=\", where its corresponding vertex has a parent in the preceding context. Specifically, we\nexamine input formatted as:\n\u201c ... Uj\u2081 = qj1, . . . V j = qj ...\"\nwhere vj\u2081 is the parent of vj. We construct the probing dataset by each time picking a position i < j\n(including j1), replacing qi with randomly sampled qf, and recording the Transformer's hidden state\nfor this modified sentence. We train a linear function (details in Appendix G.3) to predict q from\nthe hidden states. If the Transformer attends to qi, the linear function should predict q with high\naccuracy. If not, the accuracy should be low.\nTable 3 shows the results for 3-layer, 3-head Transformers trained on multiple FTCT tasks. For\nsentences sampled from training or testing data, linked to prompts with shot numbers 0-4, the"}, {"title": "Conclusion", "content": "Our research validates the potential of Transformers in doing compositional reasoning on synthetic\ndata and investigates the inner mechanism eliciting such ability. We demonstrate that few-shot CoT\nprompting enables Transformers to perform compositional reasoning by providing the information\nof correct order of knowledge points. We also find that compositional reasoning ability emerges\nwhen the training-testing data similarity and the model complexity are above certain thresholds. We\nfurther show that Transformers develop compositional reasoning by learning an underlying program\nduring training, which minimizes both training and testing loss. This program leverages in-context\nlearning and parent retrieving mechanisms, facilitated by induction heads and attention assignment.\nThrough experiments on synthetic data, we demonstrate the potential of Transformers to develop\ngeneralized reasoning skills, indicating that the impressive performance of contemporary large\nlanguage models extends beyond mere memorization of vast data. While our conclusions may not\ndirectly apply to real-world models trained on extensive natural language datasets, we believe that\nour analysis offers valuable insights into the training processes and understanding of today's large\nlanguage models."}, {"title": "Proof of Lemma 5.1", "content": "For any sentence 21:T, its last token zy must be one of the tokens {om, c(Uj )k, Uj, o\u00aeq, qj}. Then we\ndiscuss the distribution of the next token in Dtrain when zy is any one of them.\n\u2022 When z = om, that means ZT-3 must be a vertex vi and the next token will be one of\nthe contextual tokens of vi+1. To determine which specific vertex vi+1 is, we can refer to\nprevious examples to find what is the next vertex after every vi. After determining vi+1,\nin Dtrain, the next token follows the distribution of c(vi+1)1. Because c(Vi+1)1,\u2026\u2026, C(Vi+1)li+1\nare sampled from CONT(Vi+1) without replacement, so c(vi+1)1 ~ Uniform(CONT(Vi+1)), deducing\nPtrain( | 21:T) = Uniform(CONT(vi+1)).\n\u2022 When zy = c(vj)h, the previous h \u2212 1 tokens must be c(vj)1,\u2026\u2026\u2026C(Vj)h\u22121. Because lj is sampled\nfrom Uniform(1,|CONT(vj)|), then we have P(lj = \u00b7 | lj \u2265 h) ~ Uniform(h, |CONT(vj)|). So with\nprobability 1/(|CONT(vj)|-h+1) the next token is vj, with probability (|CONT(vj)|-h)/(|CONT(vj)|-\nh + 1) the next token is sampled from Uniform(CONT(vj) \u2013 {c(j)1,...,C(vj)h}). Combining them\ntogether, we have that Ptrain( | 21:T) = Uniform(CONT(vj) \u222a{vj} - {c(vj)1,\u2026\u2026\u2026, C(Vj)h})."}, {"title": "Proof of Lemma 5.2", "content": "First write down inpk and labk:\nseq = [UL, 0q, oqu, c(v1)1, . . ., C(V1)l\u2081, V1, 0eq, q1,0cm, . . ., om, c(UL)1,..., C(UL)IL, UL, Oeq, qL]\ndock = [seq(1), \\n,...,\\n, seq seq(k)]\ninpk = dock + [\\n] + [UL, 0q, oqu, c(v1)(k+1), ..., c(v1)(k+1), 01, 0eq, q1k+1),0cm]\nlabk = [c(v2)(k+1) ., C(V2)(k+1), 02, 0q, q2 2k+1), ocm,..., ocm, c(UL)(k+1), ..., C(UL) (k+1), UL, Oeq, qLk+1)]\nAfter decoding, we have dec(labk) = [V2, 92 (k+1),..., UN, q(k+1)]. According to the program, we have\nPprog( | inpk) = Uniform(CONT(22)).\nIn this situation, the greedy decoding will randomly sample a c(v2) uniformly and concatenate it to\nthe end of inpk, then we have that\nPprog( [inp | [inp, c(v2),..., c(v2)]) = Uniform(CONT(v2) + {12} - {c(v2)1,\u2026\u2026\u2026, c(v2)}).\nIt will keep sampling elements from CONT(v2) until it samples the vertex v2. Suppose that it samples\nr\u00bd times until it samples v2. Then we have that\narg max Pprog( | [inp, c(v2)(k+1), ..., c(v2)(k+1), v2]) = 0eq\narg max Pprog( | [inp, c(v2)(k+1),..., c(v2)(k+1), 02, 0eq, q2 (k+1)]) = q2k+1)\narg max Pprog( | [inp, c(v2)(k+1), ..., C(U2) (+1),..., c(v2)(+1), 02, 02, 0eq, q2k+1)]) = 0cm\nFollowing such routine, the greedy decoding on algorithm 1 we finally sample\nprog(inp) = [c(v2)(k+1), ..., c(v2) (k+1), 02, 0eq, q2k+1), ..., c(vv) (k+1), ..., c(UN)(+1), N, ,C(UN)(k+1), UN, 0eq, qk+1)].\nSo we have\ndec(prog(inpk)) = [v2, q2 (k+1),..., VN, qk+1)] = dec(labk)."}, {"title": "Attention Mechanism", "content": "When zy = v;, the next token is o\u00ba\u00ba, deducing Ptrain (oq | 21:T) = 1.\nWhen z = oq, zT\u22121 must be a vertex vj and the next token must be a value qj. If vj \u2208 Vall \u2013 V,\nthen q; is sampled from Uniform(VALS(vj)), deducing Ptrain( | 21:T) = Uniform(VALS(vj)). If\nvj \u2208 V and vj is not the first vertex from V in this sentence, there must exist vj\u2081 \u2208 V which is\nthe parent of v; previously. In this situation, qj = op(vj\u2081, vj) 0qj\u2081 with probability one, deducing\nPtrain (op(Uj1, vj) \u00a9 qj1 | 21:T) = 1. Otherwise, if vj is the first vertex from V in this sentence, then\nqj is sampled from Uniform(VALS(vj)), deducing Ptrain(\u00b7 | 21:T) = Uniform(VALS(vj)).\nWhen z = qj, the next token must be om, deducing Ptrain (om | 21:T) = 1.\nIn any situation, Pprog(\u00b7 | 21:T) always equals to Ptrain( | 21:T)."}, {"title": "First Layer", "content": "We directly give the construction. We set Wh = Ia for all 1", "memory": "nWr = \\sum_{t=4}^{Tmax}(P_Q -3, 0_{4d_1}) (W_{comp} (o_{cm}, t), 0_{4d_1}) + \\sum_{t=4}^{Tmax}(P_Q -3, 0_{4d_1}) (W_{comp} (o_{dlm}, t), 0_{4d_1})\nzT=ocm/odlm\n+ \\sum_{v \\in V_{all}} (W_{tok} (o_{\\varepsilon}), 0_{4d_1}) (W_{tok} (v), 0_{4d_1})+ \\sum_{\u03b1 \\in A-V_{all}-\\{o_{cm},o_{dlm}\\"}]}, {}]