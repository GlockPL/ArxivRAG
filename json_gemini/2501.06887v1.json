{"title": "MedGrad E-CLIP: Enhancing Trust and Transparency in AI-Driven Skin Lesion Diagnosis", "authors": ["Sadia Kamal", "Tim Oates"], "abstract": "As deep learning models gain attraction in medical data, ensuring transparent and trustworthy decision-making is essential. In skin cancer diagnosis, while advancements in lesion detection and classification have improved accuracy, the black-box nature of these methods poses challenges in understanding their decision processes, leading to trust issues among physicians. This study leverages the CLIP (Contrastive Language-Image Pretraining) model, trained on different skin lesion datasets, to capture meaningful relationships between visual features and diagnostic criteria terms. To further enhance transparency, we propose a method called MedGrad E-CLIP, which builds on gradient-based E-CLIP by incorporating a weighted entropy mechanism designed for complex medical imaging like skin lesions. This approach highlights critical image regions linked to specific diagnostic descriptions. The developed integrated pipeline not only classifies skin lesions by matching corresponding descriptions but also adds an essential layer of explainability developed especially for medical data. By visually explaining how different features in an image relates to diagnostic criteria, this approach demonstrates the potential of advanced vision-language models in medical image analysis, ultimately improving transparency, robustness, and trust in AI-driven diagnostic systems.", "sections": [{"title": "1. Introduction", "content": "Cancer is characterized by the uncontrolled growth of body cells and is a major global health concern. Among its various forms, skin cancer is the most common, primarily affecting areas of the body frequently exposed to the sun. The primary cause of skin cancer is excessive exposure to ultraviolet (UV) radiation, which can lead to life-threatening conditions in as little as six weeks. Early identification of skin diseases is critical as it can significantly improve outcomes and reduce healthcare costs.\nVarious methods have been developed to detect and differentiate skin lesion types [8] [1] [10] [2] [25]. Melanomas, the most serious form of skin cancer, exhibit a range of characteristics, such as the presence or absence of pigmentation and diagnostic features like the whitish veil. Clinicians have established different guidelines such as the ABCDE rule-Asymmetry, Border irregularity, Color variation, Diameter, and Evolution to track changes in lesions [21]. However, image resolution variations can complicate diameter assessment, and these features alone may not ensure accurate diagnosis of different types of melanomas. Consequently, the Menzies method was developed as a simplified dermoscopy technique for melanoma diagnosis,by focusing on key \"negative\" and \"positive\u201d dermoscopic features [29]. Despite its improved accuracy over the ABCDE rule, the Menzies method has high sensitivity [18] [4], which can lead to false-positives, especially when used by less experienced clinicians. To overcome the limitations of the Menzies method, the 7-point checklist was introduced [31]. However, this method also presents challenges for non-experts, as accurate diagnosis without specialized tools is difficult.\nThe complexity of diagnosing skin lesions highlights the need for manual evaluation by clinicians. Nonetheless, automated techniques using deep neural networks offer promising solutions by improving the precision and reliability of skin lesion detection and classification [16] [34]. Despite their potential, these methods are often perceived as 'black boxes,' which makes it challenging for clinicians to trust their outputs. While some studies have focused on enhancing the explainability of medical data to build transparency and trust [9] [22], they have not addressed the importance of highlighting specific regions in relation to their corresponding textual descriptions, which would further enhance explainability and interpretability. Furthermore, no existing classification method fully integrates all diagnostic techniques.\nTo address this research gap, we developed a pipeline aimed at assisting clinicians by training CLIP [23] on dermoscopic skin lesions images and their descriptions, incorporating features from all diagnostic techniques. Additionally, we introduced MedGrad E-CLIP, a novel explainability approach for skin lesion analysis that enhances the gradient-based CLIP [36] method by emphasizing fine-grained features crucial for accurate diagnosis. This method highlights subtle changes in lesion images by computing entropy, identifying specific regions relevant to each diagnosis and aligning them with corresponding textual descriptions, thus bridging visual data with diagnostic terms. This enhanced transparency promotes trust among clinicians, enabling them to understand and verify the Al's decision-making process."}, {"title": "2. Related Works", "content": ""}, {"title": "2.1. CLIP in Medical Data", "content": "CLIP has gained attraction in medical domains with models like Med-CLIP [32] for semantic matching, eCLIP [13] using radiologist eye-gaze data, Mammo-CLIP [6] for multi-view mammograms, and ConVIRT [35] for unsupervised pretraining with image-text pairs. MITER (Medical Image-TExt joint adaptive pRetraining) [28] combines multi-level contrastive learning, pathCLIP [11] enhances gene identification via image-text embeddings, CLIPath [14] fine-tunes CLIP with residual connections, and PubMedCLIP [7] excels in medical VQA tasks. Despite these advancements, several research gaps remain in using CLIP models for medical tasks, particularly concerning generalizability across diverse medical domains and explainability."}, {"title": "2.2. Explainability in Medical Data", "content": "The growing use of deep learning in medical image detection, classification, and segmentation has raised concerns about the black-box nature of these models, making trust and transparency crucial for clinician acceptance.\nNumerous studies have focused on enhancing the explainability of models applied to medical data [20]. In the realm of image-based explanations, the primary objective is to identify the specific parts of an image (pixels or segments) that most significantly influence a model's prediction. Prominent techniques include gradient-based methods for convolutional neural networks (CNNs), such as Guided Backpropagation, CAM, Grad-CAM [26], Grad-CAM++ [5], Guided GradCAM [27], SmoothGrad [30] and DeepLIFT [15]. These were developed to further enhance the interpretability of model predictions by offering more refined visual explanations that highlight the regions of the input most responsible for the model's decisions in medical data.\nIn addition to gradient-based methods, other approaches like SHAP (SHapley Additive exPlanations) [17], LIME (Local Interpretable Model-agnostic Explanations) [24] and Layer-wise Relevance Propagation (LRP) [3] have been developed to provide more generalizable explanations across different types of data. These techniques offer insights into the contribution of individual features to the model's predictions. However, some medical data often rely on multiple data sources such as images, EHR and clinical notes. Grad E-Clip [36], an emerging gradient based method which provides comprehensive explanations across different modalities by highlighting important areas in the images. It works well for general images but struggles with complex medical images, where small, important details often go unnoticed. This highlights the need for more precise explainability methods for reliable medical image analysis."}, {"title": "3. Method Overview", "content": ""}, {"title": "3.1. Dataset", "content": "Based on our work requirement which uses images with specific dermoscopic structure criteria, we used the PH\u00b2 and Derm7pt datasets. The PH\u00b2 [19] image database contains a total of around 200 dermoscopic images of melanocytic lesions, including common nevi, atypical nevi, and melanomas. These includes clinical and histological diagnoses and the identification of several dermoscopic structure criteria.\nSimilarly, Derm7pt [12] is a dermoscopic image dataset that contains over 2000 clinical and dermoscopy images along with corresponding structured metadata tailored for training and evaluating computer aided diagnosis (CAD). This dataset includes the 7-point checklist for assessing the malignancy of skin lesions, making it a valuable resource for our study."}, {"title": "3.2. Data Prepration", "content": "The dataset is organized with each row as a unique image-text pair, removing duplicates to prevent overfitting. For text preprocessing, special characters and unnecessary punctuation are removed. While, images are resized to 224x224 pixels to meet the input requirements of the image encoder. To increase the number of image-text pairs, augmentations are applied: images are augmented through flipping and rotating, while text descriptions are reordered to create variations for the same image. These augmented text descriptions are then tokenized to create a format compatible with the text encoder, splitting the text into tokens (words). This careful pairing and preprocessing of images and text is crucial, as CLIP relies on learning the relationships between image-text pairs to function effectively"}, {"title": "3.3. Contrastive Learning Image Pretained - CLIP", "content": "Contrastive Language-Image Pre-training (CLIP) consists of two key components: an image encoder and a text encoder, both of which are jointly trained to extract feature embeddings from images and text into a shared representation space. In this study, a pre-trained model with a vision transformer (ViT) is used as an image encoder, while a transformer-based encoder is used for text. Given an image-text pair (I, T), the matching score between their extracted image features $f_{I} \\in R^{D}$ and text features $f_{T} \\in R^{D}$ is:\n$S(f_{I}, f_{T}) = cos(f_{I}, f_{T}) = \\frac{f_{I}f_{T}}{\\|f_{I}\\|\\|f_{T}\\|}$                                                                                                                                                             (1)\nAs shown in Figure 1, CLIP was trained on colored dermoscopic images paired with their structure criteria, which served as descriptive annotations. These trained weights were then employed for the classification of new image-text pairs."}, {"title": "3.4. MedGrad E-CLIP", "content": "This study presents an explainability method, specifically designed for analyzing medical imagery, such as skin lesions. While Grad E-CLIP highlights regions with high gradient values, it mainly relies on the loosened similarity function which reduces the sparsity issue of softmax attention by allowing more distributed and significant spatial importance. However, it still tends to bias attention towards dominant regions, often overlooking subtle but diagnostically significant details. This limitation can be particularly problematic when using the CLIP model, trained on skin lesion dataset, as it may miss important diagnostic information, thus reducing interpretability.\nTo address this, we introduce a weighted entropy mechanism, replacing Grad E-CLIP's spatial importance. Our approach moves beyond traditional spatial weighting by using entropy to directly measure the variability or complexity within regions, allowing the model to distribute focus across areas with subtle and rich information. In this method, entropy is calculated locally to measure the variability and uncertainty of pixel intensity values within a defined neighborhood. It quantifies the information content by evaluating the distribution of gray-level values, capturing the complexity and richness of local image patterns. The entropy weight ($w_e$) is derived from the entropy values of each region, where higher entropy values correspond to regions with greater pixel-level complexity and information density. These weights allow the model to focus more on subtle patterns that are often diagnostically significant but overlooked by gradient-based attention mechanisms. As shown in Eq. (3):\n$H_i = ReLU(\\sum_{c}(W_c V_i w_e))$       (3)\nwhere $w_c$ is the channel importance, $v_i$ represents the pixel values at spatial location i and $w_e$ represents the computed entropy weights.\nThis results in a more balanced representation, enhancing the model's robustness and interpretability, especially in medical imaging where significant details and changes are crucial for accurate diagnosis. By introducing entropy weighting, the model shifts its focus towards regions with higher complexity and variability rather than merely the most activated areas, enabling a more comprehensive analysis of subtle diagnostic features and ultimately improving the explainability and effectiveness of medical image analysis."}, {"title": "4. Our Approach", "content": "In this study, we have developed a fully connected pipeline for classifying and differentiating various skin lesions, leveraging a dual-modality approach that integrates image and textual data. As shown in Figure 2, data were collected from two different databases, including images and their corresponding text descriptions. The collected images and text were then pre-processed, with images resizing, text organization, and data augmentation applied to both modalities. The dataset comprised of 17 distinct skin lesion types as classes. After pre-processing, 75% of the dataset was allocated for training and the remaining 25% for testing. The data was trained for 30 epochs with a batch size of 64, using the Adam optimizer with a learning rate of 1e-5. These settings achieved the optimal results during hyperparameter tuning. The loss use was the mean of image and text cross-entropy.\nAfter training, the updated model weights were used to evaluate the model's performance on the test dataset. To enhance the interpretability of the newly trained CLIP model, explainability techniques were applied, offering both visual and textual insights into the decision-making processes of the model. These explainability approaches were subsequently compared to evaluate their effectiveness.\nOur framework extends beyond just applying existing explainability techniques like Grad CAM, and Grad E-CLIP, to a pre-trained CLIP model by developing a comprehensive classification framework that effectively integrates image and text pairs. A critical aspect of our methodology is the development of MedGrad E-CLIP to incorporate computed entropy weights, replacing traditional spatial importance metrics. Our modified approach computes a local entropy within the region covered by the disk around each pixel that emphasizes minute changes within the lesion images. This adjustment allows for a more detailed understanding of the model's decision-making process. By enhancing both the accuracy and interpretability of classifications, our methodology emphasizes the critical role of transparency in AI-driven diagnostics, making a significant contribution especially to AI-driven medical diagnostics."}, {"title": "5. Experiments", "content": "Our experiments were performed out on Google Colab, utilizing a TESLA T4 GPU. We conducted these experiments using the ViT-B/16 architecture, which is based on a transformer model with a 16x16 patch size. The experiments were divided into two main parts i.e. performance of CLIP and performance of Explainability on CLIP."}, {"title": "5.1. Performance Evaluation of the CLIP Model", "content": "In this part we evaluate the performance of the CLIP model trained on custom skin lesion dataset. The training and testing performances are evaluated allowing for a direct comparison of their performance. \nAccuracy, being the most commonly used metric, evaluates the overall performance of deep learning models. In addition to accuracy, other evaluation metrics such as Sensitivity, Specificity, Precision, and F1-score are also assessed for the CLIP model. These provide a more comprehensive evaluation of the model's performance by offering insights into its ability to correctly identify true positives, avoid false negatives, and maintain a balance between precision and recall. Furthermore, the CLIP Score (SCLIP) is calculated as the cosine similarity between the image and text embeddings.\n$S_{CLIP} = \\frac{f_{img}(I) \\cdot f_{text}(T)}{\\| f_{img} (I) \\| \\| f_{text} (T) \\|}$   (4)\nThese metrics indicate the effectiveness of the learning algorithm, as the training curves reach a point of stability.\nIt is clear that the performance of the CLIP model improves significantly on trained model, leading to enhanced classification performance."}, {"title": "5.2. Performance Evaluation of the MedGrad E-CLIP", "content": "The second part of the experiments focused on evaluating the explainability of the CLIP models both before and after training. Gradient-based explainability methods, such as Grad-CAM, Grad E-CLIP and our proposed MedGrad E-CLIP explainability method, were employed to analyze the image-text pair understanding of model's decision-making process on the skin lesion dataset. The results indicate that trained CLIP model on our custom data not only improved the model's accuracy on the skin lesion dataset but also influenced the explainability of the model's outputs. In this study weighted entropy ensures that the model's attention is distributed across not only the most prominent features but also those regions that provide critical, subtle information, regardless of texture and detail variations within the images. \nThese heatmaps shows that our MedGrad E-CLIP method provides superior explainability by capturing subtle changes along with their relation to each input text compared to Grad-CAM and original Grad E-Clip, where some information were getting lost.\nAs discussed in [36], the Grad E-CLIP model excels at identifying common perceptual attributes such as color, but it struggles with physical attributes like shape and material, and is less effective at grounding objects with comparative attributes, like size and positional relationships. Our proposed method addresses these limitations of the Grad E-CLIP by capturing detailed features such as full asymmetry and symmetry in 1 axis. Furthermore, our method effectively highlights subtle variations. Similarly, our method, MedGrad E-CLIP, excels in explaining \"melanoma\" and associated features like \"fully symmetric\u201d and \"light brown color,\" even when features are absent or missing, thereby providing no explainability for those absent features."}, {"title": "6. Conclusion", "content": "This paper proposed MedGrad E-CLIP, an enhanced version of Grad E-CLIP that works well for medical data especially skin lesion dataset and which is able to capture subtle changes. The trained CLIP model on custom skin lesion dataset not only achieves improved accuracy but also generates more precise and relevant visualizations. We compared our proposed MedGrad E-CLIP with existing gradient-based methods, such as Grad-CAM and Grad E-CLIP, and showed that MedGrad E-CLIP provides superior visual explanations ensuring that it is able to capture fine-grained details by highlighting regions in the images more accurately and features that align well with their textual descriptions, thereby making the model's predictions more interpretable and reliable.\nThis work has certain limitations, particularly in the explainability of image-text pair relevance for cases where alignment between images and textual descriptions may lack clarity. Moreover, there are some cases where our proposed method excessively highlights features outside the intended regions, potentially leading to over-explanation.\nIn future work, we will focus on expanding the dataset and incorporating more detailed textual annotations for each skin lesion to enhance explainability and strengthen correlations between image and text pairs, thereby refining model alignment. Additionally, clinical trials will be conducted in collaboration with dermatologists to assess and validate the model's applicability in real-world clinical settings, offering critical insights into its performance and reliability in practical diagnostic scenarios. To further validate our approach's robustness and reliability, we will perform quantitative assessments, such as insertion and deletion analysis, and statistical significance tests, to evaluate the effectiveness of the entropy-weighted attention mechanism and its ability to highlight diagnostically significant regions. Finally, efforts will also be directed toward addressing the susceptibility of gradient-based explanations to adversarial attacks, aiming to improve their stability and robustness."}]}