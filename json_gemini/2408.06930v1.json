{"title": "Diagnosis extraction from unstructured Dutch echocardiogram reports using span- and document-level characteristic classification", "authors": ["Bauke Arends", "Melle Vessies", "Dirk van Osch", "Arco Teske", "Pim van der Harst", "Ren\u00e9 van Es", "Bram van Es"], "abstract": "Background Clinical machine learning research and artificial intelligence driven clinical decision support models rely on clinically accurate labels. Manually extracting these labels with the help of clinical specialists is often time-consuming and expensive. This study tests the feasibility of automatic span- and document-level diagnosis extraction from unstructured Dutch echocardiogram reports.\nMethods We included 115,692 unstructured echocardiogram reports from the University Medical Center Utrecht, a large university hospital in the Netherlands. A randomly selected subset was manually annotated for the occurrence and severity of eleven commonly described cardiac characteristics. We developed and tested several automatic labelling techniques at both span and document levels, using weighted and macro F1-score, precision, and recall for performance evaluation. We compared the performance of span labelling against document labelling methods, which included both direct document classifiers and indirect document classifiers that rely on span classification results.\nResults The SpanCategorizer and MedRoBERTa.nl models outperformed all other span and document classifiers, respectively. The weighted F1-score varied between characteristics, ranging from 0.60 to 0.93 in Span Categorizer and 0.96 to 0.98 in MedRoBERTa.nl. Direct document classification was superior to indirect", "sections": [{"title": "1 Background", "content": "Unstructured electronic health record (EHR) data contains valuable information for a broad spectrum of clinical machine learning applications, including the creation of clinical decision support systems, semi-automated report writing, and cohort identification. The extraction of accurate clinical labels is essential to realize these applications. Relying solely on structured data for this purpose often yields disappointing outcomes, primarily due to two key reasons. Firstly, collecting structured data has only recently gained momentum in clinical practice, leaving a large volume of historical data underutilized. Secondly, the structured data that is collected, may suffer from a lack of precision and reliability [1]. International Classification of Disease (ICD) coding specifically was identified as unreliable for phenotyping EHRS [2-4].\nData annotation is identified as one of the main obstacles in developing clinical natural language processing (NLP) applications [5]. Therefore, utilizing labels extracted from unstructured data has the potential to enhance both data volume and data quality.\nEchocardiography, the most commonly performed cardiac imaging diagnostic [6], provides a detailed anatomical and functional description of a wide range of cardiac structures. Data from echocardiography reports are consequently used in many"}, {"title": "", "content": "aspects of patient care, as well as many clinical trials. However, the heterogeneous format of the reports, as well as medical text characteristics such as abundant short- hand, domain-specific vocabularies, implicitly assumed knowledge, and spelling and grammar mistakes, make extracting accurate labels challenging. For label extraction, we often resort to automated techniques, because manual extraction by domain experts is both costly and time-consuming.\nPrevious work on data extraction from echocardiography reports has primarily focused on extracting quantitative measurement values from structured, semi- structured and unstructured parts of the report using rule-based methods [7-10]. Rule-based text-mining systems such as MedTagger [11], Komenti [12], and CTAKES [13] are examples of low-code tools that allow clinicians to develop and apply rules for rule-based text mining. These rule-based methods offer several advantages, as they are transparent, easily modifiable, and do not require large amounts of labelled training data. Furthermore, they can be quite effective despite their simplicity. While their performance can vary based on the developer's expertise and attention to detail, a more specific downside of rule-based methods is their inherent inability to generalize beyond the set of predefined rules.\nNLP methods based on machine learning may overcome some of these disadvantages, as they are able to learn rules implicitly from labelled data. In the biomedical field, several open-source systems, such as GATE [14] and CTAKES [13] are available to employ these methods. Additionally, an abundance of model architectures is available for label extraction, including token classification models [15], conditional random fields (CRF) [16], recurrent neural network (RNN) such as long short-term memory (LSTM) [17] and transformers such as BERT [18], support vector machine (SVM)"}, {"title": "", "content": "[19] and AutoML methods [20]. However, in the broader field of named entity recog- nition (NER) in medical imaging reports, there does not seem to be one overall best-performing method [17, 21, 22]. For span identification performance in partic- ular, multiple factors may influence performance, including model architecture and span characteristics such as span frequency, distinctive span boundaries and span length [23].\nNER in the medical imaging report domain has mostly been described in English texts [7, 10, 24]. There are limited studies in other languages, such as Dutch [25, 26], German [27], and Spanish [28]. Few publicly available pre-trained Dutch language models exist, and include BERTje [29] and RobBERT [30, 31]. Verkijk and Vossen recently created MedRoBERTa.nl, a version of RoBERTa [32] finetuned on Dutch EHR data [33]. Furthermore, Remy, Demuynck and Demeester developed a multi- lingual large language model BioLORD-2023M using contrastive learning, which is able to identify biomedical concepts and sentences [34]. To the best of our knowledge, none of these models have been finetuned with the goal of information extraction from Dutch echocardiogram reports.\nIn this work, we focus on span and document label extraction from unstructured Dutch echocardiogram reports for a wide range of clinical concepts. To capture the most meaningful clinical concepts, we constructed a custom ontology which incorporates most major cardiac abnormalities. We explicitly focused on extracting qualitative labels from unstructured text, as several algorithms exist to extract mea- surement values from structured and semi-structured data. We evaluated three NLP methods for span-level label extraction, and six NLP methods for document-level label extraction. The best-performing span and document classification models are"}, {"title": "", "content": "available on the Huggingface model repository\u00b9. Additionally, the developed code is publicly available on GitHub\u00b2.\n2 Methods\nThis section provides a detailed description of our data and the data annotation pro- cess, followed by an overview of our experiments. We employ several NLP methods for extracting span and document-level labels from Dutch echocardiogram reports. Additional information on model parameters is detailed in Additional File 1.\n2.1 Data overview\nOur dataset consisted of 115, 692 unstructured echocardiogram reports collected dur- ing routine clinical care from 2003 to 2023, stored in the EHR at University Medical Center Utrecht (UMCU), a large university hospital in the Netherlands. Over this period, there has not been a universal standard for report writing. Reports con- taining fewer than fifteen characters were excluded, as were reports with fewer than thirty characters that lacked any description of a medical concept. These reports often contained only the phrase \"For the report, see the patient's chart\u201d.\n2.2 Data annotation\nIn a randomly selected subset of the unstructured text portions of these reports, we manually annotated eleven common cardiac characteristics, which included left and right ventricular systolic function and dilatation, valvular disease, pericardial effusion, wall motion abnormalities, and diastolic dysfunction."}, {"title": "", "content": "Annotations were checked sample wise by doctors. In cases of uncertainty, cases were jointly reviewed to achieve consensus. Several rounds of training iterations were completed before commencing the annotation task. To streamline the annotation pro- cess, each echocardiogram report was annotated for one characteristic at a time, resulting in eleven separate annotation files. For an overview of labelling instructions, see Additional file 2. Prodigy [35] was employed for the annotation task.\nTo ensure an adequate number of labels, we established the following requirements: for each characteristic, a minimum of 5000 documents were annotated, with the same documents used for each characteristic. In addition, to ensure sufficient training data, a minimum of 50 span labels per class, per characteristic were required, resulting in more than 5000 annotated documents for several characteristics (Table 2). Document- level labels were constructed using the span-level labels. Given a multitude of span labels in one document, we aggregated the labels by selecting the most severe label per characteristic. For comparison we also employed a simplified label scheme with only three possible labels: not mentioned, normal, or present."}, {"title": "2.3 Data splits", "content": "We split the dataset in a training and testing set, allocating 80% and 20%, respectively. We used one train/test split for a simple practical reason: we developed regular expres- sions for identifying candidate spans and for direct labelling only on the train split. Performing this extraction for N folds would be infeasible and realistically also not truly independent unless we use different annotators for each fold. Since the amount"}, {"title": "", "content": "of labelled cases may differ for each characteristic due to label prevalence and our requirements, a preemptive split was made using all 115, 692 reports. Consequently, the training and testing splits may not add up to exactly the prespecified percentages. In Table 3, we report the distribution of span-level labels for each data split.\n2.4 Span classification\nWe present three approaches for span classification. First, we employed a rule-based approach using regular expressions as a baseline method. Second, we used a NER+L extractor, where clinical concept spans are identified and subsequently classified. Finally, we implemented a greedy span classification approach, where all possible spans are classified, and only those exceeding a threshold model certainty are presented.\n2.4.1 Approximate list lookup\nGiven a dictionary of lists containing phrases, where each list represents a target label, we can build a very simple pseudo-model. This model indexes the phrases using phrase embeddings, then uses these embeddings to convert spans from unseen texts into fuzzy search keys. We used a dictionary with token-based regular expressions to extract matching phrases, denoting this method as approximate list lookup (ALL). The advantage of this approach lies in its transparency and the ease with which the pseudo-model can be improved by simply adding or removing phrases. The rule-based algorithm was constructed as follows:"}, {"title": "2.4.2 MedCAT", "content": "MedCAT is a semi-supervised NER+L extractor that supports bilateral LSTM (biL- STM) and transformer-based span-classifiers [36] (Figure 2). The benefit is that not all token spans are scanned. However, this requires training the MedCAT model to create a context-database that contains context vectors that are indicative for medi- cal concepts. We performed unsupervised training on the training split and added the spans that were defined during the manual labelling process to MedCAT's vocabulary and context-database. The initial span-detector introduces a selection bias compared to a greedy span-classifier. Consequently, we expected a higher precision but lower recall, as some spans may be missed. We trained a different span classifier for each characteristic where all classifiers were integrated into one MedCAT modelpack. To reduce the occurrence of false negatives, we explicitly added a negative label for each class, set to 1 whenever a class was otherwise unlabelled.\n2.4.3 spaCy SpanCategorizer\nSimilar to MedCAT, spaCy's SpanCategorizer [37] operated in two stages: tokeniza- tion and span suggestion, followed by span classification. It employed a rule-based"}, {"title": "2.5 Document classification", "content": "For document classification, we used six methods. We implemented two baseline methods: one utilizing a bag-of-words (BOW) approach with medical word embed- dings, and the one using indirect document classification via a span-to-document label heuristic, where the best performing span classification method was used to aggre- gate span-based classifications into document classifications. We also employed SetFit in combination with the multilingual BioLORD-2023 embeddings. Another method involved using RoBERTa, specifically the MedRoBERTa.nl model for this work. Addi- tionally, we applied a RNN model, specifically a bidirectional GRU, and a bidirectional convolutional neural network (CNN).\n2.5.1 Bag-of-words\nOur baseline BOW approach involved several feature extraction steps, detailed in Figure 4. First, the text was tokenized. Next, we applied term frequency-inverse doc- ument frequency (TF-IDF) weighting to each token within a document. We then enriched the features with topic modelling weighting, as described by Bagheri et al."}, {"title": "2.5.2 Span classifier heuristic", "content": "We selected the best performing span classifier based on its end-to-end performance. Then, we aggregated the span labels into a document label for each characteristic. The process is similar to how we constructed document labels: given a multitude of span labels within one document, we aggregated them by selecting the most severe label per characteristic. This heuristic allowed for more granular analysis by indicating which spans lead to the document classification. However, we expected performance loss due to the increased complexity of span classification.\n2.5.3 Set Fit\nReimers et al. [39] employed Siamese networks with contrastive learning on similar and dissimilar sentences to produce transformer-based encoders that capture seman- tic information along different axes of similarity, such as polarity and temporality. Tunstall et al. [40] utilized these so-called \"sentence encoders\u201d to develop SetFit, a"}, {"title": "2.5.4 MedRoBERTa.nl", "content": "The BERT model, developed by Devlin et al. [43], is a transformer-based machine learning technique known as Bidirectional Encoder Representations from Trans- formers. It is designed to pre-train deep bidirectional representations by jointly"}, {"title": "2.5.5 Recurrent networks", "content": "The LSTM, gated recurrent unit (GRU), and quasi-recurrent neural network (QRNN) are types of RNNs suitable for both span and document classification tasks, the lat- ter simply being a special case of span classification. We used the bidirectional GRU (biGRU), which processes sequences both from left-to-right and right-to-left. This bidirectional approach helps maintain context over longer token sequences. Known downsides of RNNs are the lack of parallelisation and the sensitivity to hyperparam- eter tuning, both of which result in a significant amount of computational resources required for training.\n2.5.6 Convolutional Neural Networks\nCNNs are another powerful type of neural network, often used for span and docu- ment classification tasks. In our study, we utilised a bidirectional variant of CNN,"}, {"title": "2.6 Performance evaluation", "content": "Span classification involves two distinct tasks, identifying spans and subsequently clas- sifying them. Therefore, our performance evaluation included two aspects. We assessed span identification performance using a token-based coverage expressed using the Jaccard index. For span classification, we evaluated assuming the correct spans are identified. Additionally, we measured end-to-end performance, which combines both span identification and span classification. For both span and document classification, we reported weighted and macro precision, recall, and F1-score.\nFinally, in clinical practice it is important to consider the number of false labels, i.e. the number of spans that are falsely labelled with any class value (other than \"no label\" or \"normal\"). We present the rate of false labelling relative to the total number of identified spans for our span classification task."}, {"title": "3 Results", "content": "This section provides the performance scores on the span and document-level label extraction tasks.\n3.1 Span classification\nTable 5 shows that for most characteristics, SpanCategorizer achieved the highest weighted and macro F1-scores for the span classification task. However, ALLrule performed particularly well in classifying valvular disorders. This high performance may be attributed to these disorders being often described with very short, distinct phrases (Table 4). Conversely, the remaining characteristics are typically described using longer, less distinctive spans, where SpanCategorizer demonstrated a better per- formance. MedCAT demonstrated a lower precision and recall in the span classification task. These results may be due to an imperfect span suggestion. This hypothesis is supported by Table 6 and 7, which illustrate a high performance in span classification when the exact spans containing a label are suggested, but a low Jaccard-index when comparing MedCAT's end-to-end predicted spans containing a label with the ground truth. In addition, Table 8 details that MedCAT has a high percentage of false pos- itive span labels, leading to a reduced precision. This indicates that using MedCAT combined with a greedy span suggester could improve results even further.\n3.2 Document classification\nResults for the document classification task are presented in Table 9 and 10. From these tables, MedROBERTa.nl outperforms all other models on weighted and macro F1-score, precision, and recall. Indirect document classification using span classifiers resulted in a suboptimal performance, highlighting the added value of direct docu- ment classification models. BOW, our second baseline approach, performed quite well"}, {"title": "", "content": "considering that we did not perform feature processing except TF-IDF and lemma- tisation. An explanation might be that, because we are dealing with short staccato notes, containing little elaborations, and primarily containing statements of facts.\nAnother reason may be that the number of negations is limited in echocardiogram"}, {"title": "4 Discussion", "content": "This study aimed to explore and compare various NLP methods for extracting clinical labels from unstructured Dutch echocardiogram reports. We developed and evaluated several approaches for both span- and document-level label extraction on an internal test set, demonstrating high performance in identifying eleven commonly described cardiac characteristics, including left and right ventricular systolic dysfunction, left and right ventricular dilatation, diastolic dysfunction, aortic stenosis, aortic regur- gitation, mitral regurgitation, tricuspid regurgitation, pericardial effusion, and wall motion abnormalities. The main findings indicate that SpanCategorizer consistently outperformed other models in span-level classification tasks, achieving weighted F1- scores ranging from 0.60 to 0.93 across these characteristics, while MedROBERTa.nl excelled in document-level classification with a weighted Fl-score exceeding 0.96 for all characteristics.\nIn this study, we observed a variation in results of different span classification approaches. The baseline approach, using regular expressions, achieved a high per- formance for some characteristics but performed poorly for others. These outcomes are likely linked to span length, frequency, and distinctiveness [23]. Our most poorly performing characteristics - left ventricular systolic dysfunction, pericardial effusion, and wall motion abnormalities - have larger span lengths, and lower span frequencies. Macro performance is particularly impacted by the 'severe' classes, which have a low span frequency and high span length, which have both been previously linked to worse performance [23].\nThe MedCAT approach has a very high overall precision but lacked recall due to imperfect span suggestions. Therefore, for medical applications, it may be more effective to use a greedy span-classifier as the primary span suggestion method, with"}, {"title": "", "content": "a NER+L extraction serving as an augmentation tool to extract additional features. Alternatively, to make the MedCAT model more robust, we should consider using fuzzy matching with varying proximities, using tools like clinlp [46], instead of adding possible spans directly from the training phase of the labelling process in Prodigy. Another approach, given the results from the document classification task, could involve training a RoBERTa-based or CNN/biGRU span classifier, using either a MedCAT or greedy span suggester. Additionally, a joint entity/relation extraction model could be constructed [47]. However, these approaches are outside the scope of the current paper and require significantly higher computational cost.\nFor document classification, the MedROBERTa.nl model demonstrated the best over- all performance. This aligns with previous findings, which highlight the additional value of BERT-based models in cases involving infrequently occurring spans [23]. We did not attempt to train a BERT-based model from scratch due to the lim- ited number of available documents. Previous studies have shown that pre-training on a small corpus yields suboptimal results, whereas models with general domain pre-training, such as MedRoBERTa.nl, achieve highly competitive results without requiring domain-specific feature engineering [48-53]. The biGRU and CNN models demonstrated a competitive performance, especially considering their significantly lower computational cost. Alternatives like TextCNN or hierarchical architectures such as Hierarchical Attention Networks might perform better with longer contexts, such as discharge summaries [54, 55].\nThe BOW approach, while effective considering its simplicity, could have been extended with more sophisticated weighting mechanisms, such as incorporating nega- tion estimation, part-of-speech tagging, and dependency parsing. These additions could have improved the contextual understanding of the text, potentially leading to"}, {"title": "", "content": "better document classification. However, such extensions would require significantly more complex feature engineering and computational resources, which were beyond the scope of this study.\nRegarding the SetFit method, three remarks can be made. First, training a new sentence transformer from scratch based on the MedRoBERTa.nl model might yield better results than using the arithmetic mean. Second, the BioLORD model is con- trastively trained to discriminate between medical span-level concepts, rather than explicitly between semantic differences. Third, we achieved performance close to the best-performing method using only 10% of the data. Therefore, this approach may be most suitable given the resources required for manual data labelling.\nThe class distribution in our dataset reflects real-world practice, with over 75% of documents lacking a label for at least one characteristic, and a small percentage containing moderate or severe labels. While this distribution poses challenges for model performance, particularly in terms of macro scores, it also highlights the need for models to perform well under realistic clinical conditions. Expanding the dataset was not feasible due to the extensive manual labeling process, which already took several months. An alternative approach to enhance model performance could involve utilizing English BERT-based models on translated texts, as suggested by Muizelaar et al. [51].\nWe employed a single train/test split for our experiments, which, while practical, could introduce certain limitations. One potential concern is the risk of overfitting to the specific data in the training set, particularly when using handcrafted features like regular expressions. This might result in models that perform well on the test set but may not generalize as effectively to new, unseen data. Ideally, a cross-validation"}, {"title": "", "content": "approach would provide a more comprehensive evaluation by averaging performance across multiple splits, thereby reducing the variance and offering a more robust assessment of model performance. However, given the infeasibility of developing reg- ular expressions for each fold, our approach represents a pragmatic balance between practical constraints and methodological rigor. The use of a single split also means that our performance estimates may be somewhat optimistic, as they are tied to the specific characteristics of the selected test set. This is particularly relevant for our span classification tasks, where the performance varied significantly across different span types. In future work, incorporating cross-validation or a more extensive test set could help mitigate these limitations, providing a clearer picture of how well these models might perform in broader clinical applications.\nOur findings suggest distinct use cases for span and document classification within clinical practice. Span classification, while adding a layer of explainability by highlighting specific spans that contribute to a particular label, exhibit too much variability in performance to be reliably used in clinical settings. This inconsistency, especially across different characteristics, limits its utility for direct clinical appli- cation at this stage. In contrast, document classification demonstrated significantly better and more consistent performance, making it a more viable option for integra- tion into clinical workflows. This approach could be effectively used for tasks such as constructing patient cohorts for research or automating parts of the diagnostic pipeline. Additionally, we observed that reducing the number of labels significantly improved the performance of document classification models. This reduced label model might be employed to flag cases that require more detailed review, either by activating a clinician's attention or by supporting active labeling in research settings, such as using Prodigy. This approach not only enhances model accuracy but also"}, {"title": "", "content": "provides a practical pathway for implementing NLP tools in clinical environments where efficiency and precision are essential.\nConclusions\nIn this study, we developed several NLP methods for span- and document-level label extraction from Dutch unstructured echocardiogram reports and evaluated the per- formance of these methods on an internal test set. We demonstrate high performance in identifying eleven cardiac characteristics. Performance for span classification ranges between a weighted F1-score of 0.60 and 0.93 for all characteristics using the Span- Categorizer model, while for document classification we achieve a weighted F1-score of >0.96 for all characteristics using the MedROBERTa.nl model.\nFor future research, we suggest to use the SpanCategorizer and MedROBERTa.nl model for span- and document-level diagnosis extraction from Dutch unstructured echocardiogram reports, respectively. In case of a limited amount of data, SetFit may be a suitable alternative for document classification. SpanCategorizer and MedROBERTa.nl have been made publicly available through HuggingFace. Future work may include validation in external institutions or the extension to other cardiac characteristics.\nList of abbreviations\nALL approximate list lookup\nbiGRU bidirectional GRU\nbiLSTM bilateral LSTM\nBOW bag-of-words\nCNN convolutional neural network\nCRF conditional random fields"}, {"title": "", "content": "EHR electronic health record\nGRU gated recurrent unit\nICD International Classification of Disease\nLSTM long short-term memory\nNER named entity recognition\nNLP natural language processing\nQRNN quasi-recurrent neural network\nRNN recurrent neural network\nSVM support vector machine\nTF-IDF term frequency-inverse document frequency\nUMCU University Medical Center Utrecht\nDeclarations\nEthics approval and consent to participate\nThe UMCU quality assurance research officer confirmed under project number 22U- 0292 that this study does not fall under the scope of the Dutch Medical Research Involving Human Subjects Act (WMO) and therefore does not require approval from an accredited medical ethics committee. The study was performed compliant with local legislation and regulations. The need for patient consent was waived, as sub- jects are not actively involved in this study. All patient data used was used after pseudonomisation.\nConsent for publication\nNot applicable."}, {"title": "Availability of data and materials", "content": "The datasets generated and/or analysed during the current study are not publicly available due to potential privacy-sensitive information, but are available from the cor- responding author upon reasonable request and local institutional approval. Research code is publicly available on GitHub, via https://github.com/umcu/EchoLabeler.\nCompeting interests\nThe authors declare that they have no competing interests.\nFunding\nThe work received funding from the European Union's Horizon Europe research and innovation programme under Grant Agreement No. 101057849 (DataTools4Heart project). The collaboration project is co-funded by PPP Allowance awarded by Health~Holland, Top Sector Life Sciences & Health, to stimulate public-private part- nerships. This publication is part of the project MyDigiTwin with project number 628.011.213 of the research programme COMMIT2DATA Big Data & Health which is partly financed by the Dutch Research Council (NWO).\nAuthors' contributions\n\u0392.\u0391., M.V., \u0392.E., and R.E. were responsible for conceptualisation. Data curation and manual labelling was performed by B.A. and M.V.. Methodology was developed by B.E., B.A. and M.V.. P.H. and A.T. were responsible for project supervision and administration. B.E., B.A., and M.V. performed experiments and validated the results. A.T. and D.O. gave clinical input on endpoint definitions. The original draft of this manuscript was written by B.A. and B.E.. All authors read and approved the final manuscript."}]}