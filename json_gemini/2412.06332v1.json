{"title": "Not All Errors Are Equal: Investigation of Speech Recognition Errors in Alzheimer's Disease Detection", "authors": ["Jiawen Kang", "Junan Li", "Jinchao Li", "Xixin Wu", "Helen Meng"], "abstract": "Automatic Speech Recognition (ASR) plays an important role in speech-based automatic detection of Alzheimer's disease (AD). However, recognition errors could propagate downstream, potentially impacting the detection decisions. Recent studies have revealed a non-linear relationship between word error rates (WER) and AD detection performance, where ASR transcriptions with notable errors could still yield AD detection accuracy equivalent to that based on manual transcriptions. This work presents a series of analyses to explore the effect of ASR transcription errors in BERT-based AD detection systems. Our investigation reveals that not all ASR errors contribute equally to detection performance. Certain words, such as stopwords, despite constituting a large proportion of errors, are shown to play a limited role in distinguishing AD. In contrast, the keywords related to diagnosis tasks exhibit significantly greater importance relative to other words. These findings provide insights into the interplay between ASR errors and the downstream detection model.\nIndex Terms: Alzheimer's disease, speech recognition, pretrained language model, spoken language processing", "sections": [{"title": "1. Introduction", "content": "Alzheimer's disease (AD) is a neurodegenerative disorder characterized by progressive cognitive impairment, including deterioration in memory, attention, and executive function. Due to the irreversible progression of AD pathology [1], early detection and diagnosis play a pivotal role in facilitating timely intervention and management, conventionally relying on in-person clinical assessments [2, 3]. With recent progress in spoken language technology, speech-based automatic AD detection has emerged as a promising area due to its potential for more cost-effective and scalable AD screening [4, 5].\nAutomatic detection of Alzheimer's Disease (AD) from speech has attracted increasing attention in recent years. Early research attempted to identify AD by utilizing handcrafted acoustic and linguistic features [6, 7, 8, 9]. For example, Winer and Frankenberg et al. [8, 9] demonstrated that parts-of-speech (POS), word categories, and pause features are highly related to AD. More recently, deep embedding features from pre-trained models have been widely explored for AD detection, leveraging either speech-based models [10, 11, 12] or text-based models [13, 14, 15]. Among these models, pre-trained language models (PLMs) such as BERT [16] and RoBERTa [17], have shown remarkable performance by leveraging their strong ability to capture rich linguistic and semantic patterns. These PLMs can be used either as feature extractors [13, 18, 19] or directly fine-tuned on the AD detection task [14]. Moreover, researchers have made further enhancements to PLMs [20, 21, 22], pushing the state-of-the-art detection accuracy to new heights.\nWhile PLMs have achieved impressive performance in AD detection, the practical applications of these approaches largely rely on Automatic Speech Recognition (ASR) front-ends to transcribe speech into text, as transcription errors could lead to significant bias in PLM embeddings and alter the decisions of downstream AD detection models. Many works have investigated tailored ASR systems for AD or other disordered speech. Early works explored incorporating language models [23] and Maximum A Posteriori (MAP) adaptation [24] for ASR in AD detection. Recent efforts have investigated novel approaches such as data augmentation [25, 26, 27], domain adaptation [28] and neural architectural search [29]. Nevertheless, due to the difficulty in collecting in-domain data, there is still a substantial accuracy gap between ASR for disordered speech and normal speech.\nOn the other hand, many studies have shown a non-linear relationship between ASR Word Error Rate (WER) and AD detection accuracies. For instance, ASR transcription with notable errors (WER ~30%) could yield equivalent AD detection accuracy as manual transcription does [19, 30], while ASR models with lower WER might not necessarily lead to better AD detection results [31, 29]. Few studies have investigated the in-depth relationship between ASR errors and AD detection. [23] might be the first work revealing that the performances of ASR and AD detection systems are weakly correlated. Balagopalan et al. [32] demonstrated that deletion error is effective in the AD detection model using a handcrafted feature set. Recently, Li et al. [31] showed that ASR errors could serve as helpful cues for fine-tuned PLMs to identify speech with AD. These findings suggest that certain ASR error words might be trivial and harmless to the downstream AD detection task, while others could have a strong effect.\nThis work aims to explore the impact of ASR errors on Alzheimer's Disease (AD) detection using a BERT-based model. We first implement a subject ASR and AD detection systems which achieves the same accuracy when evaluated with ASR and manual transcriptions. We then conducted a thorough analysis based on the ASR errors. It reveals that not all ASR errors are equally detrimental to downstream AD detection. Certain words, such as stopwords, constitute a large proportion of errors but are less significant concerning the picture description tasks performed in the ADRESS data used in this study. In contrast, task-related keywords occupy only 9% of all ASR errors but have proven to play a pivotal role compared to other words. These findings provide insights into the interplay between ASR errors and the downstream detection model, benefiting the development of ASR-robust AD detection systems."}, {"title": "2. Approach", "content": "2.1. Data\nThe data used in this work comes from the Alzheimer's Dementia Recognition Through Spontaneous Speech (ADRESS) Challenge 2020 corpus [33]. This challenge selects a sub-task of Pitt Corpus in the DementiaBank database [34], which requires all the participants to describe the Cookie Theft picture [35]. The ADRESS corpus consists of 156 different English speakers' audio samples with corresponding transcripts. Among them, 78 of the speakers are healthy controls (35 male, 43 female) while the rest are with AD (35 male, 43 female). The corpus is divided into a standard train (108 speakers, about 2 hours) and test (48 speakers, about 1 hour) sets with balanced distributions of age, gender, and disease conditions.\n2.2. Automatic Speech Recognition Models\nWe employed a tailor-made ASR system based on the Time Delay Neural Network (TDNN). This model was trained with 1000-hour Librispeech corpus and adapted on 59-hour Pitt corpus [34]. Additionally, data augmentation, speaker adaptation, and a Transformer language model were adopted to enhance the performance on the in-domain speech. As a result, the employed system has shown state-of-the-art level performance on the ADRESS corpus. More details can be found in [25].\n2.3. AD Detection Models\nWe formulate AD detection as a binary classification problem, i.e. classifying participants as healthy controls or individuals with AD. The development of the detection system follows work [19], which has shown superior detection performance on the ADRESS corpus. This system utilizes the BERT model [16] to generate embeddings from the automatically transcribed text. Specifically, we take the embedding corresponding to the [CLS] token for each speaker throughout the entire transcript. This process yields a feature vector with a dimensionality of 768, which serves as the input representation for the following analysis. To avoid the over-fitting problem, we use the Principal Component Analysis (PCA) method to reduce the dimension of the embeddings to 108, which is the size of the training set. The compressed embeddings are then fed into a Support Vector Machine (SVM) with linear kernel and regularization parameter C1 for classification. Both PCA and SVM models are fitted with manual transcription. As for evaluation, we use accuracy scores as the main criterion, where precision, recall, and F1 scores are also used for analysis."}, {"title": "3. AD Detection with ASR Transcriptions", "content": "The non-linear dependency between ASR WER and AD detection accuracy has been observed in multiple studies [19, 29, 30, 31]. To look into this observation, we first set up a subject system using the above-mentioned settings.\nTable 1 shows the classification results of the subject detection system. Note that the detection model was fitted with manual transcription, and ASR transcriptions were only used in the test stage. We can see that even though the ASR system has a WER of 33.9%, it still achieves an equivalent detection accuracy of 88% compared to manual transcriptions."}, {"title": "4. Analysis of speech recognition errors", "content": "Based on the detection system in Section 3, this section characterizes the composition of ASR errors and investigates why a high word error rate did not significantly degrade AD detection performance. We will start with error distribution and then examine certain types of errors using the test set transcriptions.\n4.1. Basic analysis\nThe classification results show that there are 40 out of 48 cases (ASR-robust cases) where ASR and manual transcriptions yielded identical predictions. To look into details, we first draw the distribution of the top 20 ASR errors. Given that these errors did not lead to prediction changes, we assume that frequent errors play a relatively marginal role in AD detection. As shown in Figure 1, we can first observe these errors follow a long-tail distribution, with a small proportion of words constituting a large proportion of errors. Moreover, we noticed that most of these top word errors are stopwords, which have been considered to carry less semantic weight and are not directly related to the picture description task used to diagnose AD patients.\nIn addition to overall errors, we conducted a thorough case study to observe the individual patterns of ASR errors. Particularly, we generated alignment maps for each participant to visualize the compositions of ASR transcriptions and their discrepancy with manual transcription. As showcased in Figure 2, the alignment map compares manual and ASR transcriptions word-by-word, where correct words are marked as squares and errors are marked as crosses. Following the findings in the previous section, we highlight the low-semantic stopwords\u00b9 with blue colors. In comparison, we additionally highlight the relatively high-semantic task-related keywords with red colors. As the diagnosis of AD in the ADRESS corpus is based on a picture description task, we hypothesize that content words related to the elements in the target picture may play a role in distinguishing\nFollowing NLTK [36] (v3.8.1) Stopwords List\n4.2. Anayisis of stopwords\nA series of experiments was conducted to examine the role of stopwords in AD detection. First, Table 4 presents a detailed breakdown of stopwords and non-stopwords in ASR errors from the ASR-robust cases. It shows that a small proportion of stopword types (24%) constitute the majority of errors (60%), while the remaining 76% types of words constitute only 40% of errors. This aligns with the observation in Figure 1 and 2. Given these errors did not affect AD classification, we hypothesize that when using BERT features, the misrecognized stopwords error may not significantly propagate to the downstream detection decision. Meanwhile, we note that the non-stopword errors also show an insignificant impact on classification results, suggesting that words other than stopwords could also play a marginal role in distinguishing AD. We will discuss this further in the later part of this paper.\nWords ablation experiments To verify the impact of stopwords on downstream classification, we conduct a word ablation experiment to probe the variation of BERT embeddings when stopwords are misrecognized. Starting from manual transcriptions, we randomly removed or substituted 10% to 100% of stopwords in 10% increments and observed the corresponding BERT embeddings. For substitution, we replaced random stopwords with an equal number of non-stopwords randomly sampled from all manual transcriptions. We then projected the high-dimensional BERT embeddings of the edited transcriptions onto a 2-dimensional subspace orthogonal to the original SVM decision hyperplane. Figure 3 shows the resulting visualizations from an example participant. When gradually removing or substituting stopwords (Figure 3 (a) & (c)), BERT embeddings show a shift towards the decision boundary. This suggests that stopwords may have an effect on AD detection. However, the edited transcription still falls on the negative (recognized as healthy) side until all stopwords are removed/substituted. In contrast, when gradually removing/substituting the same number of non-stopwords (Figure 3 (b) & (d)), the embeddings shift across the decision boundary with a more direct trend. Moreover, in this case, substitutions result in a larger embedding variance compared to removal. We attribute this to the randomness in the substitution operation, as random words were sampled for replacement.\nFurthermore, to reduce randomness and quantify this experiment across participants, we calculate the hyperplane offset for each participant and each removal/substitution. This offset represents a signed distance from BERT embedding to the SVM hyperplane.\n$d = (w^x + b)/||w||$ \n\nThe above equation presents calculation details where w and b are the norm vector and interception of SVM, x is the input vector, and || || stands for L2 norm. The resulting d reflects\n4.3. Analysis of task-related keywords\nIn comparison to stopwords, this part investigates keywords related to elements in the Cookie Thief picture used for AD diagnosis. We highlight that the WER for keywords is only 14.3%, which is much lower than the overall WER of 33.9%. The high accuracy in keywords is understandable, as task-related keywords are frequently mentioned words in the training set, with only 602 distinct keywords in total. Compared to stopwords, most keywords have more syllables, making them easier to recognize. Furthermore, keyword errors account for just 9% of all ASR errors. This aligns with the observations using the alignment maps in Section 4.1.\nWords ablation experiments We conducted word ablation experiments using the same settings as in the stopword investigation, replacing stopwords with keywords. The visualizations of BERT embeddings are exampled in Figure 5. It is not surprising that the results of editing keywords and stopwords are opposite. Removing or substituting keywords in transcriptions causes BERT embeddings to shift across the SVM decision boundary, whereas similar manipulations of non-keywords do not produce this effect. Figure 4 (b) depicted the hyperplane offset experiment. There is also a clear divergence when editing keywords and non-keywords. Opposite to stopwords, removing or substituting keywords shows more significant changes in embedding offset. This divergence indicates keywords may play a distinctly meaningful role compared to other words. Some of the 88% non-keywords may also be non-stopwords, which could explain the insignificant non-stopwords in Table 4. Notably, editing keywords didn't result in embedding shifts as effectively as expected, i.e., the averaged offset started to be positive only when 70% of the keywords were removed. We interpret this as an indication that there exists information above the word level that can be helpful to AD classification. E.g., sentence length and the interplay between words within the transcriptions."}, {"title": "5. Conclusions", "content": "We performed a series of analyses to investigate the effect of ASR transcription errors in BERT-based Alzheimer's Disease (AD) detection. Specifically, we explored why transcriptions with notable word error rates could yield detection accuracy equivalent to that of manual transcriptions. We have shown that not all ASR errors are equally detrimental to downstream AD detection. Certain word types, particularly stopwords, constitute a large proportion of errors while carrying limited information for AD discrimination than others. The task-related keywords were demonstrated to be significant to AD classification while occupying only 9% of all word errors. These findings provide insights into understanding the influence of ASR errors on downstream AD detection and facilitate the development of robust AD detection systems. While this work focused on ASR at the word level, future work could explore their influence on other linguistic structures such as syntactic dependencies and discourse coherence."}]}