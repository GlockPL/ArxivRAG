{"title": "SWAT: Sliding Window Adversarial Training for Gradual Domain Adaptation", "authors": ["Zixi Wang", "Yubo Huang", "Wenwei Luo", "Tonglan Xie", "Mengmeng Jing", "Lin Zuo"], "abstract": "Domain shifts are critical issues that harm the performance of machine learning. Unsupervised Domain Adaptation (UDA) mitigates this issue but suffers when the domain shifts are steep and drastic. Gradual Domain Adaptation (GDA) alleviates this problem in a mild way by gradually adapting from the source to the target domain using multiple intermediate domains. In this paper, we propose Sliding Window Adversarial Training (SWAT) for Gradual Domain Adaptation. SWAT uses the construction of adversarial streams to connect the feature spaces of the source and target domains. In order to gradually narrow the small gap between adjacent intermediate domains, a sliding window paradigm is designed that moves along the adversarial stream. When the window moves to the end of the stream, i.e., the target domain, the domain shift is drastically reduced. Extensive experiments are conducted on public GDA benchmarks, and the results demonstrate that the proposed SWAT significantly outperforms the state-of-the-art approaches.", "sections": [{"title": "1. Introduction", "content": "Conventional machine learning methods or approaches frequently presuppose that training and testing data adhere to a uniform distribution, thereby ensuring the model's generalizability to novel data. However, this assumption is not always valid in practical applications, where domain shifts\u2014defined as discrepancies between the source and target domains\u2014can significantly impair model performance (Farahani et al., 2021). To illustrate, an image classifier developed using images from a controlled laboratory may have reduced accuracy when deployed on real-world images captured under varying lighting or weather conditions (Ganin & Lempitsky, 2015b; Tzeng et al., 2017).\nTo address this issue, Unsupervised Domain Adaptation (UDA) proposed to mitigate domain shifts by aligning feature distributions between a labeled source domain and an unlabeled target domain (Pan & Yang, 2009; Hoffman et al., 2018). Nevertheless, Kang et al. (2019); Tang & Jia (2020); Yang et al. (2020) have revealed that when the domain gaps are large, direct alignment of the source and target domains can negatively impact the model and cause the negative transfer. Direct feature-level alignment between source and target domains cause geometric distortion from enforcing rigid distribution matching (e.g., MMD), where forcing alignment of non-overlapping support sets amplifies classifier boundary errors (Zhao et al., 2019). Additionally, static alignment blindness ignores the latent domain evolution trajectory, leading to suboptimal adaptation paths; and discriminability erosion where excessive invariance learning causes loss of category-aware structures (Liang et al., 2020).\nGradual Domain Adaptation (GDA)(Kumar et al., 2020) is proposed to alleviate this problem in a mild way by gradually adapting from the source to the target domain using multiple intermediate domains, as shown in Fig. 1. Recently, GDA advances include theoretical analyses of error propagation in self-training (Wang et al., 2022), optimal transport-guided intermediate domain generation via Wasserstein geodesics (GOAT) (He et al., 2023), and normalizing flow-based feature alignment (Sagawa & Hino, 2022). Nevertheless, current GDA methods face two fundamental bottlenecks: (1) Existing GDA frameworks rely on discretized intermediate domains (e.g., fixed-step interpolations), leading to quadratic error accumulation (Kumar et al., 2020; He et al., 2023) due to noise propagation in pseudo-labeling across stages. Rigid phase divisions further disrupt feature manifold continuity, causing semantic distortion and degraded transfer fidelity. (2) Manually designed intermediate domains (He et al., 2023) often fail to align with real-world domain shift dynamics. Uniform interpolation assumptions (e.g., linear paths) introduce spatial redundancy (ineffective transition regions) and density imbalance (over-dispersion or mode collapse), destabilizing adaptation trajectories (Xiao et al., 2024).\nWe present a novel framework for gradual domain adaptation, dubbed Sliding Window Adversarial Training (SWAT). This framework progressively transfers knowledge from the source domain to the target domain through adversarially transported intermediate representations. SWAT constructs a feature transport flow by iteratively aligning the model to a sequence of dynamically adjusted intermediate domains. This flow is optimized by curriculum-guided window sliding, enabling smooth transitions. Specifically, our contributions are as follows:\n1. Sliding Window Adversarial Training: We incorporate adversarial learning with a curriculum sliding window mechanism. Instead of fixed interpolations, SWAT employs a sliding window that adaptively focuses on regions along the source-to-target path.\n2. Adversarial Flow Matching: We propose a bidirectional adversarial framework unifying temporal flow alignment and feature matching. The flow generator enforces domain continuity through sliced Wasserstein optimization across evolving domains, while the discriminator progressively filters out source-specific features through adversarial attenuation. This co-evolutionary optimization achieves simultaneous domain invariance and target discriminability.\n3. Experimental Superiority: Experiments on Rotated MNIST (96.7% vs. 86.4% SOTA) and Portraits (87.4% vs. 86.16% SOTA) demonstrate superiority, with ablation studies showing 36.0% error reduction in extreme shifts (Fig. 8(c))."}, {"title": "2. Related Work", "content": "Unsupervised Domain Adaptation (UDA) aims to align feature distributions between labeled source domains and unlabeled target domains to mitigate domain shifts. Traditional approaches leverage statistical measures such as Maximum Mean Discrepancy (MMD) (Chen et al., 2020) to enforce domain invariance, while facing fundamental limitations under severe distribution divergence, despite their effectiveness for moderate domain gaps. Rigid MMD-based alignment may forcibly align non-overlapping supports, distorting classifier boundaries (Zhao et al., 2019), while direct source-target alignment risks erasing category-discriminative structures\u2014a phenomenon termed negative transfer (Tang & Jia, 2020; Yang et al., 2020). Such limitations arise from static alignment strategies that ignore the geometric continuity of latent domain trajectories. Unlike static UDA frameworks, ours SWAT dynamically adjusts alignment granularity dynamically via a Wasserstein-based curriculum.\nAdversarial Domain Adaptation frameworks, including DANN (Ganin & Lempitsky, 2015a) and CDAN (Long et al., 2018), have revolutionized alignment through adversarial training. These methods employ gradient reversal layers or conditional adversarial networks to learn domain-invariant representations. However, these methods enforce fixed pairwise alignment between source and target domains, leading to mode collapse when domain supports are disjoint (Zhao et al., 2019) or under large distribution gaps due to gradient competition (Pezeshki et al., 2021). Recent advances, such as spectral regularization (Pezeshki et al., 2021), partially alleviate these issues but retain the rigidity of discrete alignment steps. In contrast, our SWAT redefines domain adaptation as a continuous manifold transport process. By constructing intermediate domains along a feature transport flow, SWAT avoids abrupt transitions and assimilates novel target modes progressively\u2014a critical failure point for conventional UDA and adversarial methods alike.\nGradual Domain Adaptation (GDA) addresses scenarios where data shifts gradually, decomposing the overall shift into smaller steps via intermediate domains (Farshchian et al., 2018; Kumar et al., 2020). While existing methods leverage self-training (Xie et al., 2020), gradient flow-based geodesic paths (Zhuang et al., 2024), style-transfer interpolation (Marsden et al., 2024), or optimal transport (He et al., 2023), they often suffer from catastrophic forgetting of source knowledge during incremental adaptation. Our SWAT framework uniquely preserves source-acquired information through adversarial flow calibration, where Wasserstein-guided intermediate domains progressively integrate target features.\nDomain Flow Generation Model (DLOW) proposes to synthesize intermediate domains between source and target domains through adversarial learning (Gong et al., 2021). The core idea involves training bidirectional generators that progressively transform data distributions from the source to target domain (and vice versa) under a parametric control variable. By incorporating adversarial discriminators and cycle-consistency constraints inspired by CycleGAN (Zhu et al., 2017). This framework demonstrates the feasibility of continuous domain interpolation. Our method uses an adversarial encoder to bridge the distance between domains,"}, {"title": "3. Problem Setup", "content": "Domain Space Let $X \\subset \\mathbb{R}^d$ denote the input space and $y = \\{1, ..., k\\}$ the label space. We model each domain as a joint probability distribution $P_t(X,Y) = P_t(X)P_t(Y|X)$ over $Z = X \\times Y$, where $t \\in \\{0, ..., n\\}$ indexes domains along the adaptation path.\nGradually Shifting Domain In the gradually domain setting(Kumar et al., 2020), given a sequence of domains $\\{P_t\\}_{t=0}^n$ with gradually shifting distributions, where $P_0$ is the labeled source domain and $P_n$ the unlabeled target domain, GDA aims to learn a hypothesis $h : X \\rightarrow Y$ that minimizes target risk $\\epsilon_n(h)$, under two core assumptions (Kumar et al., 2020; Long et al., 2015): Bounded Successive Divergence and Conditional Invariance.\n$W_1(P_t, P_{t+1}) \\leq \\Delta, \\forall t \\in \\{0, ..., n \u2013 1\\}$\n$P_t(Y|X) = P_{t+1}(Y|X), \\forall t \\in \\{0, ..., n \u2212 1\\}$\nwhere $W_1$ is the Wasserstein-1 distance and $\\Delta$ quantifies maximum inter-domain drift. Conditional probability consistency ensures that label semantics remain stable during adaptation.\nModel Pretraining in the source domain The goal of pretraining in the source domain is to learn a model $C: X \\rightarrow Y$ that maps input features x from the training data set $D = \\{(x, y)\\}$ to their corresponding labels y. Considering the loss function l, the classifier benefit on $D_t$ is denoted by $C$, defined as:\n$C = \\underset{C}{arg \\min} E_{(x,y)\\sim D_t} [l(C(x), y)].$ (1)\nGradual Domain Adaptation The goal of gradual domain adaptation is to learn a classifier C that generalizes well to the target domain $D_n$ by progressively transferring knowledge from the labeled source domain $D_0$ and a series of unlabeled intermediate domains $D_1, D_2, ..., D_{n-1}$. The adaptation process involves multi-step pseudo-labeling and self-training, where the model $C_0$ is trained on the source domain and then adapted to the intermediate domains by the following self-training procedure ST($C_t, D_t$):\n$ST(C_t, D_t) = \\underset{C'}{arg \\min} E_{x\\sim D_t} [l(C'(x), \\hat{y}_t(x))].$ (2)\nIn particular, $\\hat{y}_t (x) = sign(C_t(x))$ is the pseudo-label generated by the model $C_t$ for unlabeled data of $D_t$, where $D_t$ denotes the unlabeled intermediate domain. Meanwhile, $C'$ is the next learned model, also denoted by $C_{t+1}$."}, {"title": "4. Methodology", "content": "4.1. Method Overview\nThe proposed Sliding Window Adversarial Training (SWAT) models gradual domain adaptation as a continuous feature flow matching process guided by adversarial learning. SWAT integrates three core mechanisms. First, As depicted in Fig. 3, the framework establishes a time-conditional probability path $\\{p_t(h)\\}_{t\\in[0,n]}$ over latent space $H \\subseteq \\mathbb{R}^z$, where the encoder $f_t$ and classifier $g_t$ evolve smoothly from source ($f_0, g_0$) to target ($f_n, g_n$). Second, a bidirectional adversarial architecture (eq. 4.3) employs a forward generator $G_m$ to map source features $H_0$ to intermediate domains via Wasserstein-optimal transport, and a reverse generator $G_s$ to project features back to $H_0$ for cycle consistency (Eq. 6), which ensure distributional alignment. Third, discriminator D and encoder f form an adversarial architecture to encourages domain-invariant features, enabling SWAT to balance stability and flexibility during domain shifts.\n4.2. Continuous Feature Flow Matching\nAs shown in Fig. 3, $H \\subseteq \\mathbb{R}^z$ is a z-dimensional space feature, $g \\circ f$ denotes the model, where $f \\in X \\rightarrow H$ is the encoder which maps the input data into the latent feature space H, while $g \\in H \\rightarrow Y$ serves as the classifier, which predicts the labels based on the encoded features. Specifically, when traversing the source, intermediate, and target domains, the encoders are represented as $f_0, f_1,..., f_n$. Similarly, the classifiers can be represented as $g_0, g_1,..., g_n$, where $f_t$ and $g_t$ denote the model that continuously evolves across domains.\nThe feature domains produced after encoding by f are represented as $H_0, H_1, ..., H_n$. f, g, and H encapsulate a series of continuous transformations and domains that evolve pro-"}, {"title": "4.3. Sliding Window Adversarial Training (SWAT)", "content": "The proposed sliding window adversarial training is visualized in Fig. 2. Unlike conventional discrete domain transfers, SWAT enables continuous feature transfering along the domain stream $H^z$ ($z \\in [0, n]$) through sliding window. The core idea lies in the parametric sliding window defined by $p\\in [0, 1]$, which controls the relative position between $H^z$ and its neighboring critical domains $H_l, H_r$ ($r = l+1$). This creates adaptive transmission paths $H_0 \\leftrightarrow H^{(l+p)}$ where the target domain smoothly shifts with p.\nHere, $l\\in \\{0,1, . . ., n \u2212 1\\}$ denotes the index of the left neighboring domain, and the parameter p controls the distance from the source domain to the left critical domain. In particular, when p = 0, the transfer occurs to the left domain $H_l$; when p = 1, the transfer occurs to the right domain $H_r$. The expression $H^{(l+p)}$ refers to a domain located between $H_l$ and $H_r$, with r = l + 1 representing the index of the neighboring right domain.\nIn our cross-domain translation model, we define $G_m$ as the transformation function that maps a sample h from the source domain $H_s$ to a target domain within the domain stream $H^{\\tilde{z}}$, where $z \\in [0, n]$ indicates the position of the target domain within the stream. Conversely, $G_s$ denotes the reverse transformation, mapping features from any domain in the stream $H^z$ back to the source domain $H_s$. Thus, our SWAT model can be expressed as the bidirectional transformations:$G_m: H_s \\rightarrow H^{\\tilde{z}}$ and $G_s : H^z \\rightarrow H_s$.\nAs in the DLOW model described in Section 2, we employ a Generative Adversarial Network (GAN) (Goodfellow et al., 2014) for our cross-domain transport model. Instead of using a standard GAN, we opt for the Wasserstein GAN"}, {"title": "4.4. The Overall Objective for Gradual Domain Adaptation", "content": "Similar to previous GDA mehtods, we also optimize the self-training loss as follows:\n$L_{List} = E_{h\\sim}[l(g(h), \\hat{y}(h))],$ (7)\nwhere l is the cross-entropy loss. The difference with previous GDA methods is that when h comes from the unlabeled domain, $\\hat{y}_t(x)$ is the pseudo-label generated by the model g. But when h is a feature generated by $G_m (h_o)$, it represents the ground-truth label of the original sample $h_o$ from the source domain.\nThe overall objective of the sliding window adversarial training is formulated as follows:\n$L = (1 \u2212 p)L_l + pL_r,$ (8)\nwhere $L_i$ is the adversarial training loss which defines as: $\\mathcal{L}_i = L_{adv}^i + L_{cycle}^i + L_{list}^i.$ By optimizing Eq. 8, we achieve continuous flow matching in the feature space. To best understand our method, we summarize the pseudo-code of SWAT in Algorithm 1 and provide an illustration of its architecture in Fig. 5."}, {"title": "5. Experiments", "content": "5.1. Datasets\nRotated MNIST Constructed from MNIST (Deng, 2012), this dataset (He et al., 2023) contains 50,000 source domain images (original digits) and 50,000 target domain images rotated by 45\u00b0. Intermediate domains interpolate rotation angles between 0\u00b0 and 45\u00b0.\nColor-Shift MNIST MNIST images are normalized to [0,1] for the source domain and shifted to [1,2] for the target domain (He et al., 2023), with intermediate domains generated by linearly interpolating color intensity."}, {"title": "5.4. Inter-Domain Distance Reduction Analysis", "content": "Our experimental results quantitatively and visually validate that the proposed Sliding Window Adversarial Training (SWAT) effectively reduces domain gaps while preserving semantic consistency. Fig. 7 illustrates this through two complementary perspectives.\nQuantitative Analysis via A-Distance: As shown in the left panel of Fig. 7, the A-distance (Ben-David et al., 2010) between the source domain $H_0$ (red curve) and target domain $H_n$ (blue curve) exhibits drastic fluctuations (peak at 1.498) when using a fixed encoder. This indicates unstable feature alignment under large domain shifts. In contrast, our SWAT framework (orange curve) maintains near-zero distances (< 0.11) to critical intermediate domains $H_l, H_r$ across all positions, achieving a 63.7% reduction in the average A-distance between $H_0$ and $H_n$ (0.104 vs. 1.284). The symmetrical decay of bidirectional distances confirms balanced adaptation between forward and backward domain transitions. Orange curve achieves near-zero distances to adjacent critical domains $H_l, H_r$ throughout the adaptation path, demonstrating smooth domain transitions.\nFeature Visualization Analysis The T-SNE visualizations (Fig. 7) quantitatively reveal the geometric impact of adaptation strategies:\n1. Unconstrained Alignment Failure: Direct mapping to $H_n$ sans flow matching (Fig. 7(b)) causes catastrophic cluster"}, {"title": "5.5. Ablation Study", "content": "Ablation Study on Adversarial Flow Matching We validate the necessity of hierarchical feature learning through controlled experiments on Rotated MNIST. By progressively enabling multi-scale feature aggregation in our sliding window framework, we observe systematic performance im-"}, {"title": "6. Conclustion", "content": "This study advances domain adaptation through the novel Sliding Window Adversarial Training (SWAT) framework, which addresses critical limitations in handling gradual domain shifts. By integrating adversarial learning with flow matching, SWAT eliminates dependency on predefined intermediate domains and enables continuous feature flow alignment via bidirectional adversarial optimization. Extensive validation across diverse benchmarks demonstrates SWAT's superiority over existing methods, with significant improvements in domain-invariant feature preservation and adaptation stability. This work establishes a foundation for exploring more sophisticated domain adaptation strategies in complex real-world scenarios, paving the way for enhanced generalization capabilities in evolving environments."}, {"title": "Impact Statement", "content": "This paper presents work whose goal is to advance the field of Machine Learning. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Experimental Details", "content": "A.1. Implementation\nFor the Rotated MNIST, Color-Shift MNIST, and Portraits datasets, we implemented a CNN with three convolutional layers with 32 channels. After the encoder, we added a fully connected classifier with two hidden layers of 256 units each. For the Cover Type dataset, we adopted a similar approach using three fully connected layers with ReLU activations, where the hidden dimensions increase from 128 to 256 to 512 units, ending with an output layer matching the number of classes.\nOur transport architecture includes generators composed of a single residual block containing three linear layers. The discriminator is built with three linear layers, each having 128 hidden units and paired with ReLU activation functions. We used the Adam optimizer for optimization (Kingma & Ba, 2014), Dropout for regularization (Srivastava et al., 2014), and Batch Normalization to stabilize training (Ioffe & Szegedy, 2015). The number of intermediate domains generated between source and target domains is treated as a hyperparameter, with the model's performance evaluated for 0, 1, 2, 3, or 4 intermediate domains. All the code was ran on NVIDIA RTX 4090 GPUs.\nIn addition, we followed (Kumar et al., 2020) to filter out the 10% of data points where the model's predictions exhibit the least confidence. However, instead of relying on the typical uncertainty measure, we define the confidence level as the difference between the largest and the second-largest values in the model's output. We have found that this produces better results and we use this setting in all comparative tests.\nWe pretrain the encoder and classifiers f, g on four datasets, and the results of the pretrain are shown in Fig. 9, where the accuracy varies across multiple domains. All of our experiments, including ablations on the GOAT, GST method in section 5.5, are performed using the same pretrained model. With a total of six domains in the setup, the precision of the four datasets for the classifications trained on the source domain directly using the classification results in the subsequent domains are shown in Fig. 9. The accuracies fall roughly stepwise in line with our expectations for the problem setup."}, {"title": "A.2. Results of Our Method", "content": "We present a comparison of our proposed SWAT method with multiple datasets, including Rotated MNIST, Color-Shift MNIST, Portraits and Cover Type, as detailed in Tables 4 through 7. Each experiment was repeated multiple times, with the results shown as mean values along with variance intervals. The leftmost column of each table represents the performance obtained using only adversarial training, which corresponds to the method without flow matching.\nIn Tables 4 to 7, the column \"# Given Domains\" indicates the number of domains included in the experiment, comprising both the source and the target domains. The \"Inter-domain counts in SWAT\" columns indicate the number of inter-domain"}]}