{"title": "Parallel Key-Value Cache Fusion for Position Invariant RAG", "authors": ["Philhoon Oh", "Jinwoo Shin", "James Thorne"], "abstract": "Recent advancements in Large Language Models (LLMs) underscore the necessity of Retrieval Augmented Generation (RAG) to leverage external information. However, LLMs are sensitive to the position of relevant information within contexts and tend to generate incorrect responses when such information is placed in the middle, known as 'Lost in the Middle' phenomenon. In this paper, we introduce a framework that generates consistent outputs for decoder-only models, irrespective of the input context order. Experimental results for three open domain question answering tasks demonstrate position invariance, where the model is not sensitive to input context order, and superior robustness to irrelevent passages compared to prevailing approaches for RAG pipelines.", "sections": [{"title": "Introduction", "content": "In Retrieval Augmented Generation (RAG) (Guu et al., 2020; Lewis et al., 2021; Izacard et al., 2022), models first extract relevant information from a knowledge base and then incorporate this extracted information with its parameteric knowledge to generate the response. This two-step approach is the de-facto approach for knowledge-intensive tasks (Lewis et al., 2021; Petroni et al., 2021).\nHowever, decoder-only models exhibit an intrinsic positional bias, assigning more attention to tokens at the beginning or end of the input sequence while often overlooking relevant context located in the middle, a problem known as the 'Lost in the Middle' (Liu et al., 2023). Previous works to address this issue involves training with specific prompt (He et al., 2024) or data-intensive training (An et al., 2024). Other works aimed at modifying positional embeddings (Hsieh et al., 2024b) or reducing positional attention bias in LLMs (Yu et al., 2024a). Yet, none of these methods fully guarantee a solution to this intrinsic bias in LLMs for RAG."}, {"title": "Method", "content": "Notation Our KV-Fusion architecture is illustrated in Figure 2. For clarity, we refer to this prefill decoder as Dp, which is characterized by the number of key and value heads |H|, each with a dimension of dh. We denote the trainable decoder as Dt, and represent the set of input passages as C = {C1, C2, ..., CN } with fixed token length n for each ci. This set of passages represents smaller chunks of a long document or retrieved contexts. Lastly, let L represent the total number of layers in Dp and Dt, and let l denote the lth layer.\nPrefill Decoder (Dp) extracts the KV cache from multiple input passages in parallel, resulting in the injection of identical local positional embeddings {P1, P2, ..., pm}. The layer-wise cache representation for each c\u2081 is as follows:\n{k, v}{=1 = Dp(ci), ki, v\u2208 R|H|xnxdn\nNext, we reshape layer-wise KV-caches by concatenating along the token axis over N contexts, forming a single cache for each layer l:\nK = RES({k}1) V\u00b2 = RES({}1)\nHere, K\u00b9, V\u00b9 \u2208 R|H|\u00d7(N\u00d7n)\u00d7dh are reshaped KV-cache for the corresonding layer over input passages. These caches prefill and serve as grounding knowledge for training Dt.\nTrainable Decoder (Dt) takes two inputs: (1) reshaped KV-caches ({K\u00b9, V\u00b9}}=1) and (2) target tokens, which contain instruction queries, and answers with a length of m tokens. To ensure sequential alignment of positional information with the KV-caches, position information starting from Pn+1 to Pn+m are assigned. We then train D\u2081 using next-token prediction, conditioning on the reshaped KV-caches rather than previous tokens:\nD\u2081(y|q, C') D\u2081(y|q, {K\u00b9, V\u00b9}}=1)\nHere, q denotes the instruction with query tokens and y is answer tokens. C' represents the set of input passages tokens, and {K\u00b9, V\u00b9}{_1 is the reshaped KV-cache corresponding to C'. We illustrate the details of KV-Fusion in Appendix A.1"}, {"title": "Experiment Setup", "content": "We consider three open domain question answering datasets: Natural Questions (Kwiatkowski et al., 2019), TriviaQA (Joshi et al., 2017), and POPQA (Mallen et al., 2023) . For the base retrieval corpus, we utilize a December 2018 Wikipedia snapshot consisting of 21 million passages, following (Yen et al., 2024; Yu et al., 2024b). Lastly, we use the DPR (Karpukhin et al., 2020) as our baseline retriever to extract the top-40 passages for each dataset.\nDataset Construction To enhance the robustness in RAG, we train models with irrelevant contexts (Fang et al., 2024; Yoran et al., 2024a). To this end, we draw the best gold context and extract key phrases among candidate passages by prompting gpt-40 API with a fine-grained template. If all responses are negative, the instance is discarded. Otherwise, we retain the extracted key phrases as evidence, which is later used for training. Negative contexts are sampled from DPR-retrieved passages that do not contain any answer. Each training instance consists of one gold context and 19 negative contexts. The prompt for this process and statistics of all datasets are desribed in Appendix A.2.\nMetric and Evaluations Exact Match (EM) Accuracy is used for evaluation. (Asai et al., 2023; Mallen et al., 2023). However, we observe that as more documents are added to the input, baseline models tend to generate instrinsic knowledge or hallucinated responses (Hsieh et al., 2024a). To address this, we incorporate answerability into the prompt, requiring responses to be concise, and limited to a single sentence. Lastly, we set a 48-token"}, {"title": "Training", "content": "Input Formating Each input passage is formatted with 'Title:{title}' and 'Context:{text}', followed by a document boundary, \u2018===='. For target tokens, we preprend a signal token, <|question_answering|>, to guide the model's behavior during inference (Asai et al., 2023). Next, we append instruction and \u2018Question:{question}'. Finally, we add answer tokens, which contain both answer string and a key phrase as evidence, as described in Section 3.1. We hypothesize that appending key phrases enhances the the model's robustness (Thoppilan et al., 2022; Menick et al., 2022). Format examples are provided in Appendix A.4.\nTechnical Details We initialize both Dp and Dt with the Llama3-8B model (Dubey et al., 2024). We fine-tune on each dataset with a maximum learning rate of 2 \u00d7 10-5 using the AdamW. Across all dataset, we use a batch size of 64 on four A100(80G) GPUs. For the NQ and TQA datasets, models are trained for 2 epochs. For the POPQA dataset, we fine-tune it on top of TQA fine-tuned model due to its small training size. The same procedure is applied to the Llama3.1-8B. Detail hyperparameters are reported in Appendix A.5."}, {"title": "Results", "content": "Position Invariant RAG To demonstrate the position-agnostic property, we test models with the gold context placed at varying positions. For each dev dataset, we construct 10 versions by inserting gold context at every alternate location (1st, 3rd, etc.), along with an additional dev set where all 20 contexts are randomly shuffled. To manage the increased inference time, we evaluate the first 500 instances. As shown in Figure 3, KV-Llama3 maintains consistent accuracy across all datasets, regardless of the position of the gold context, while conventional Llama3 shows varying accuracy. A similar pattern is observed with KV-Llama3.1 and Llama3.1 as shown in Appendix A.6. Figure 4 emphasizes this difference: the accuracy of the baseline model drops considerably with shuffled contexts, while the KV models maintain stable performance. In the shuffled scenario, KV-Llama3 achieves higher accuracy than baselines on the NQ, TQA, and POPQA datasets, with similar trends observed for KV-Llama3.1. These findings sug-"}, {"title": "Ablations", "content": "Token-Level Consistency Previous research (He et al., 2024; Hsieh et al., 2024b; Yu et al., 2024a) on positional bias primarily focuses on metrics like accuracy to assess position-agnostic behavior. It is often concluded that the model is position-agnostic, if the model maintains a consistent metric (such as accuracy). In this section, we further investigate targets token-level consistency. We define Token-Level Match (TLM) as the accuracy of Exact Match (EM) between responses in two scenarios: (1) POS1, where the gold context is at top, and (2) Shuffled, with randomly ordered contexts. Here, p and s denote the responses in POS1 and Shuffled settings, and N is the total number of instances. We experiment with top-20 dev set in Section 4.\nTLM(p, s, N) = \\frac{\\sum_{i=1}^{N} EM(p_i = s_i)}{N}\nThe results are shown in Table 2. With greedy-decoding, both Llama3 and Llama3.1 produce tokens that vary between the two settings, resulting in low TLM scores across datasets. Specifically, their TLM scores are significantly below 20, highlighting their susceptibility to positional bias.\nIn constrast, KV-Fusion demonstrates nearly perfect TLM scores across all datasets. This indicates that KV-Fusion can generate identical tokens regardless of context order perturbations, highlightening its robust position-agnostic behavior and maintaining token-level consistency even in scenarios where context order is altered.\nComparison with Rerankers To enhance the end-to-end performance of the RAG pipeline, rerankers (Nogueira et al., 2020; Zhuang et al., 2023; Ma et al., 2024) are often used to prioritize relevants context while filtering out irrelevant ones (Glass et al., 2022; Yu et al., 2024b). However, we hypothesize that a robust model capable of handling irrelevant passages could eliminate the need for a reranking module. To test this, we rerank the top-40 retrieved passages from the test sets in Section 4 and retain only the top-20 passages, discarding the lower-ranked ones. We utilize point-"}, {"title": "Related Works", "content": "Retrieval Augmented Generation (RAG) With recent advancements in LLMs(Team et al., 2024; OpenAI, 2024), Retrieval Augmented Generation (RAG) have proven to be effective in complementing LLMs across various tasks: managing long-tail information (Mallen et al., 2023), reducing hallucinations (Huang et al., 2023b; Shi et al., 2024a), and improving interpretability (Borgeaud et al., 2022; Rudin et al., 2021). The idea of utilizing external knowledge has become prevalent, particularly in knowledge-intensive (Thorne et al., 2018; Lewis et al., 2021; Petroni et al., 2021), where retrievers like DPR and Contriever (Karpukhin et al., 2020; Izacard et al., 2021) first retrieve relevant information, and readers like FiD, ATLAS (Izacard and Grave, 2020; Izacard et al., 2022) incorporate the retrieved information to make predictions.\nRobustness and Bias in RAG Pipeline Despite the promising capabilities of the RAG system, one major challenge is the notable drop in performance when irrelevant contexts exist during inference. (Shi et al., 2023; Oh and Thorne, 2023), along with incorrect responses even when the gold context appears in the middle (Liu et al., 2023). To address these issues, Xu et al. 2023 trained an auxiliary LLM to summarize and extract relevant contexts, while Yoran et al. 2024b proposed a simple Natural Language Inference (NLI) model to eliminate unnecessary passages. Also, He et al. 2024 suggests decomposing inference into multi-step resasoning, enabling the model to generate accurate response regardless of the context order. Other methods focus on internal features, such as adjusting position hidden states or calibrating attention biases (Hsieh et al., 2024b; Yu et al., 2024a). However, none of these approaches fully resolve a complete solution for 'Lost in the Middle' problem.\nKey Value Cache in RAG Recent studies employing Key-Value Caches (KV caches) in the RAG pipeline have gained attention. For example, RAG-Cache (Jin et al., 2024) leverages precomputed KV caches for retrieval and faster inference. Similarly, Cache Augmented Generation (CAG) (Chan et al., 2024) augments the knowledge via KV caches, demonstrating its efficiency for long-context understanding. TurboRAG (Lu et al., 2024) utilizes KV caches for training RAG pipeline, whereas our work focuses on using KV caches to address the 'Lost in the Middle' problem, enhancing robustness and enabling a order-invariant pipeline."}, {"title": "Conclusion", "content": "This paper presents KV-Fusion, a lightweight training scheme aimed at addressing positional bias and improving robustness of decoder-only models in RAG pipeline. KV-Fusion trains language models to be context-order invariant by extracting and prefilling KV caches with identical positional information, then training decoder-only models using these caches. The results not only highlight the robustness of KV-Fusion in handling a large number of input passages but also its position-invariant property. Our empirical evaluations on three open-domain datasets indicate that KV-Fusion can improve performance and reliability of the RAG system."}, {"title": "Limitations", "content": "One limitation of this work is its focus on question answering. Although the most common dataset for evaluating LLMs' understanding with large context would be the needle-in-a-haystack (NIAH) dataset (Kamradt, 2023), our experiments are centered around question-answering, which is more challenging than NIAH (Hsieh et al., 2024a).\nSecond limitation is that our experiments are limited to single-hop question answering, where multi-step reasoning is not required. For example, datasets like HotpotQA (Yang et al., 2018) and MuSiQue (Trivedi et al., 2022) require multiple passages to derive answers. This work, however, focuses on single-hop question-answering datasets, making it difficult to assess the impact of KV-fusion in multi-hop datasets.\nThird limitation is that this work does not fully explore the use of KV-cache for training LLMs. Recently, training LLMs by conditioning key-value caches has gained attention (Sun et al., 2024), though our approach remains underexplored in terms of language modeling. However, we present strong empirical results to solve 'Lost in the middle' problem. We hope our work can facilitate future studies on utilizing key-value cache for training LLMs."}, {"title": "Details for KV-Fusion Implementation", "content": "This section describes the pseudocode for KV-Fusion and Python implementation of the RES function."}, {"title": "Algorithm for KV-Fusion", "content": "This section further elaborates on the KV-Fusion algorithm, which can be implemented using standard language modeling. For clarification, we provide the pseudocode with a single-instance example. As explained in Section 2, KV-Fusion is built upon two decoders. First, Dp processes a set of input passages retrieved by retrievers, C = {C1, C2, . . .,cN}, and generates Key-Value (KV) caches in parallel.\nThese KV caches are reshaped by the RES function into the form {K\u00b9, V\u00b9}, to prefill the cache in Dt. Next, Dt processes target tokens, t = {t1, t2,..., tm}, along with their positional information, p = {Pn+1,Pn+2,...,Pn+m}. Specifically, the target tokens consist of two parts: the query part, which includes instructions, q = {t1, t2, ..., tk}, and the answer part, y = {tk+1, tk+2,...,tm}. Along with the prefilled KV cache, we train D\u2081 by prompting it with q and using the standard language model loss to generate y. For implementation, we use huggingface transformers(Wolf et al., 2020) and PyTorch(Paszke et al., 2017) libraries."}, {"title": "RES Implementation", "content": "To train D\u2081 seaminglessly with huggingface transformers(Wolf et al., 2020) and PyTorch(Paszke et al., 2017), extracted KV-cache need to be reshaped to prefill the caches in Dt. To this end, we implement RES function down below, which can also process batch of instances."}, {"title": "Prompt Template", "content": "Your task is to find Evidence from a given Document based on a Question and its corresponding Answer. Specifically, the Document contains the Answer for the given Question. Your job is to extract the Evidence from the document."}, {"title": "Dataset Statistics", "content": "NQ and TriviaQA are filtered versions provided by Karpukhin et al. 2020 under the CC BY-NC 4.0."}, {"title": "Baseline Model Template and Example", "content": "Strictly based on listed documents (titles and contexts) above, answer the given question clearly and concisely in a single sentence. If none of the documents provide a valid answer, respond with Unanswerable. Question: who got the first nobel prize in physics? ANSWER:"}, {"title": "KV-Model Input Format Template and Example", "content": "The following examples outline the input format template along with a concrete example for Dp."}, {"title": "Hyperparameters for training", "content": ""}]}