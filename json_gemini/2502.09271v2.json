{"title": "LiSA: Leveraging Link Recommender to Attack Graph Neural Networks via Subgraph Injection", "authors": ["Wenlun Zhang", "Enyan Dai", "Kentaro Yoshioka"], "abstract": "Graph Neural Networks (GNNs) have demonstrated remarkable proficiency in modeling data with graph structures, yet recent research reveals their susceptibility to adversarial attacks. Traditional attack methodologies, which rely on manipulating the original graph or adding links to artificially created nodes, often prove impractical in real-world settings. This paper introduces a novel adversarial scenario involving the injection of an isolated subgraph to deceive both the link recommender and the node classifier within a GNN system. Specifically, the link recommender is mislead to propose links between targeted victim nodes and the subgraph, encouraging users to unintentionally establish connections and that would degrade the node classification accuracy, thereby facilitating a successful attack. To address this, we present the LiSA framework, which employs a dual surrogate model and bi-level optimization to simultaneously meet two adversarial objectives. Extensive experiments on real-world datasets demonstrate the effectiveness of our method. Code is available at https://github.com/Wenlun-Zhang/LiSA.", "sections": [{"title": "1 Introduction", "content": "In recent years, Graph Neural Networks (GNNs) have shown remarkable proficiency in modeling graph-structured data. GNNs primarily operate on a message-passing mechanism. This process of iterative information aggregation enables GNNs to effectively update node representations while incorporating the structural properties. However, the powerful message-passing mechanism of GNNs also brings its share of challenges and limitations. Malicious attackers can intentionally perturb the graph structure and features. Such deliberately tampered information can propagate among nodes, leading to a degraded performance of GNNs [6,20,21,2,14,1]. Recognizing the importance of designing trustworthy GNNs, investigating various attack methods has become an essential initial undertaking [5]. Consequently, there is a growing focus on adversarial attacks targeting GNNs within academic and industrial communities. Among these, manipulation attacks [20] have been demonstrated to be particularly effective. These attacks involve adding or deleting edges, or modifying node features in the original graph, all within a constrained budget.\nNonetheless, adversaries frequently encounter practical hurdles when attempting to manipulate these data, as it usually requires direct access to the backend data, a task that is not easily achievable. On the other hand, node injection attacks (NIAs) [11,12,19] present a more feasible approach. Such attacks degrade classification performance by introducing malicious nodes linked to target nodes, spreading harmful attributes. Within citation networks, adversaries may craft counterfeit papers and insert references to target papers, ultimately causing them to be misclassified. This kind of emerging attack method have demonstrated high success rates and are increasingly seen as a more practical method in real-world scenarios. Injecting fake nodes is often more feasible than modifying data on existing nodes, making these techniques particularly insidious.\nThere is no doubt about the practicality and effectiveness of NIAs. Nevertheless, current research often focuses on ideal conditions, which implies that adversaries can freely establish connections between target nodes and injected fake nodes. However, this assumption does not universally hold across various applications. Especially in applications where security is critical, like social networks, fake users who connect with the target user can subtly alter the ground truth label of the target user, leading to targeted advertising. However, it is not certain that target users will approve connection request with fake accounts. Thus, evaluating these attack models under the assumption of cost-free linking can be misleading, potentially leading to an overestimation of the attack outcomes. These concerns highlighted above have motivated us to focus on enhancing the likelihood of adversaries successfully linking fake nodes to the target nodes. Given the prevalence of link recommendation algorithms within GNN-based applications, we explore the potential to exploit these link recommenders to facilitate malicious link formation. The proposed subgraph injection attack (SIA) scenario is shown in Fig. 1. Our method does not manually add edges between target victim nodes and the aggressor fake nodes. Instead, it deceives the link recommender of GNN into actively initiating links formation. Once these links are established, the classification performance on the target victim nodes suffers a considerable decline, ultimately enabling a successful attack. This novel approach to adversarial attacks is crucial for real-world applications yet remains largely unexplored. Moreover, the uncertain feasibility of attacking multiple GNN models simultaneously poses a significant challenge in solving this problem.\nIn this work, we investigate a unique SIA scenario, which injects an isolated subgraph to deceive both the link recommender and the node classifier in a GNN simultaneously. Specifically, it aims to mislead the link recommender into creating adversarial links from targeted victim nodes to the subgraph, thereby undermining the accuracy of node classification. Our contributions in this study are outlined as follows: (1) We explore an innovative attack scenario utilizing the link recommender to compromise the node classification accuracy within a GNN. (2) We introduce Link Recommender-Subgraph Injection Attack (LiSA), a pioneering framework crafted to effectively generate subgraphs by employing a dual"}, {"title": "2 Related Works", "content": "Adversarial attacks on GNNs aim to exploit model vulnerabilities, resulting in errors or inaccurate predictions. These attacks are generally categorized into evasion, poisoning, and backdoor attacks, each characterized by distinct methodologies and consequences. Evasion attacks occur during the test phase, where adversaries manipulate the input graph data to lead the GNN to make incorrect predictions [20,19,17]. These manipulations could involve strategic alterations in the graph structure or node features, significantly affecting the performance of the GNN. The stealth of evasion attacks, which do not require changes to the trained model, makes them particularly challenging to detect. Following evasion attacks, poisoning attacks are conducted during the training phase of the GNN. In this scenario, attackers inject the training set with crafted data [6,11], either by adding, deleting edges, or modifying nodes. These alterations subtly distort the training process, leading the model to unwittingly integrate these adversarial modifications. The performance of a model consequently deteriorates, introducing specific errors or biases, undermining its reliability and accuracy. Lastly, backdoor attacks are implemented by inserting hidden triggers in the GNN during its training phase [16,18,4]. When activated in new data, these triggers cause the model to generate incorrect results, while maintaining normal functionality under standard conditions. This makes backdoor attacks particularly deceptive.\nUnfortunately, none of the existing attack methods can fully demonstrate their capabilities in practical scenarios. Attackers do not have the capability to modify the original graph in evasion and poisoning attacks. Even with the utilization of relatively accessible malicious injection techniques, the failure to generate"}, {"title": "3 Preliminaries", "content": "In this section, we begin by introducing the notations used in this paper. Building upon this foundation, we then elaborate on the threat model and problem formulation related to the specific attack scenario of the SIA. For preliminary analysis, we investigate a heuristic technique known as GraphCopy to assess the viability of the proposed SIA scenario and establish a foundational benchmark."}, {"title": "3.1 Notations", "content": "We define the original graph as $G_o = (A_o, F_o)$, with adjacency matrix $A_o \\in \\mathbb{R}^{N_o \\times N_o}$ and feature matrix $F_o \\in \\mathbb{R}^{N_o \\times D}$. The target victim nodes, $v_t \\in V$, are selected for the attack. An isolated subgraph, denoted as $G_s = (A_s, F_s)$, is injected, forming the poisoned graph $G_p = (A_p, F_p)$:\n$A_p = \\begin{bmatrix} A_o & 0 \\\\ 0 & A_s \\end{bmatrix}, \\ F_p = \\begin{bmatrix} F_o \\\\ F_s \\end{bmatrix}$     (1)\nwhere $A_p \\in \\mathbb{R}^{(N_o+N_s)\\times (N_o+N_s)}$ and $F_p \\in \\mathbb{R}^{(N_o+N_s)\\times D}$ is the adjacency matrix and features of the poisoned graph."}, {"title": "3.2 Threat Model and Problem Formulation", "content": "Attacker's Goal. The objective of the adversary is to deceive both the link recommender, conceptualized as a form of link prediction model, and the node classifier simultaneously in a GNN system. This is achieved through the strategic injection of a deliberately crafted subraph, which will mislead the link prediction model to form connections with the target nodes, thereby influencing classification accuracy thereafter.\nAttacker's Knowledge and Capability. Attackers lack specific knowledge regarding the GNN models employed, including the particular type of link prediction and node classification model utilized within the system. Nevertheless, attackers possess access to the original attributed graph as well as a restricted set of training and validation labels. Attackers are capable of introducing an isolated subgraph to the original graph. However, any alteration to the original graph or establishment of connections between the original graph and the subgraph is strictly prohibited. Additionally, the features and node degrees of the subgraph must closely resemble those of the original graph to maintain inconspicuousness."}, {"title": "3.3 Preliminary Analysis", "content": "At the outset of validating the proposed SIA scenario, we initiate its characterization employing a heuristic approach called GraphCopy. The core concept of GraphCopy focuses on creating a subgraph through a process that duplicates target nodes and their environmental surroundings, under the assumption that the message-passing mechanisms within GNNs would produce highly similar representations for these replicated structures:\n$G_s = Copy(\\mathcal{N}_n(v_t)), \\ \\mathcal{N}_n(v_t) = \\{u \\in V | dist(u, v_t) \\leq n\\}$      (5)\nHere, $Copy()$ is the replicating operation, $\\mathcal{N}_n(v_t)$ represent the n-hop neighborhoods set on $v_t$, and $dist(u,v_t)$ gives the minimum distance between nodes u and $v_t$. Such similarity seemingly facilitates the formation of links between the original graph and subgraph. However, simply generating links does not achieve our primary goal of reducing the accuracy of node classification. To address this, we introduce a perturbation in the form of Gaussian noise to the node features of the duplicated subgraph. By fine-tuning the number of hops and the intensity of perturbation, we can achieve a balance that facilitates link generation while reducing node classification accuracy, thereby achieving the adversarial objectives. Although we verified that GraphCopy can partially achieve the adversarial goal of SIA, it faces two significant limitations that reduce its effectiveness and practicality in real-world scenarios. First, simply duplicating the graph and altering node features does not ensure a high success rate for adversarial attacks. Second, replicating the n-hop neighborhood of a target node leads to a subgraph"}, {"title": "4 Methodology", "content": "In this section, we investigate an optimization-based approach for generating subgraphs to realize the SIA scenario. While extensive research has focused on targeting either node classification or link prediction models independently [13], the exploitation of the interaction between these two distinct model types to achieve an adversarial objective remains largely unexplored. Moreover, achieving dual adversarial goals within the SIA scenario presents significant technical challenges. Intuitively, generating links is more likely when there is a close resemblance in the node representations between the target victim nodes and the nodes of the aggressor subgraph, whereas a greater disparity in these representations increases the chances of the victim nodes being misclassified. As a result, a more deliberate subgraph optimization technique is essential to balance two conflicting objectives and identify the optimal subgraph for SIA. To this end, we introduce the LiSA framework, as depicted in Fig. 2."}, {"title": "4.1 LiSA Framework", "content": "The LiSA framework is designed to challenge the performance of two GNN models through a strategic deployment of dual-surrogate models for bi-level optimization in a black-box attack context. We select the GAE [8] and GCN [9] as the"}, {"title": "4.2 Training Algorithm", "content": "The training procedure for the subgraph involves a two-tiered process with inner and outer iterations. In the inner iterations, we simultaneously train surrogate"}, {"title": "5 Experiments", "content": ""}, {"title": "5.1 Experiment Settings", "content": "Datasets To evaluate the effectiveness of our attack methods, we perform experiments on five distinct datasets. These datasets include Cora and PubMed, both citation networks, Amazon-Photo and Computers, co-purchase networks from a popular e-commerce platform, and FacebookPagePage, representing a social network graph.\nExperimental Verification We conduct an experimental verification of our LiSA framework using GAE and VGAE [8] for link recommender, along with GCN [9], SGC [15], and GraphSAGE [7] as node classifiers. Our attack methodology is applied across all datasets with the presumption that the target user would establish links with nodes of top three linking scores in the entire poisoned graph. To assess the feasibility of our methods, we randomly select 1000 nodes from each original graph as target victim nodes. We then generate subgraphs and execute attacks on these nodes individually. This approach allows us to compute both the link success rate (LSR) and the overall attack success rate (ASR). The LSR is defined as the proportion where at least one adversarial edge is generated between the target node and subgraph, while the ASR refers to the misclassfication rate on these victim nodes. For the link recommender, we use 85% of the edges for training, with half serving as positive edge labels with negative sampling for both subgraph optimization and attack verification. For node classification, 5% of the nodes are randomly chosen for training labels. These labels are utilized in subgraph optimization and the pretraining of a node classifier to identify the second highest output class as the attack label. To maintain a consistent basis for comparison across different datasets, we employ a subgraph comprising five nodes for each target victim node attack. However, we tailor hyperparameters such as the number of subgraph edges $n_e$, and the balance parameters $\\alpha$ and $\\beta$ for each dataset, ensuring optimal performance and adaptability to the specific characteristics of each dataset. We also compare the LiSA framework with baseline of GraphCopy in the SIA scenario, and an additional baseline of conventional NIA scenario. This particular version of NIA incorporates the probability of successful link creation, aiming to demonstrate the influence of link generation on the success of NIAs."}, {"title": "5.2 Attack Results", "content": "The attack results using the LiSA framework are presented in Table 1, while the misclassification rates for the original clean graphs are also provided, serving as a reference for evaluating the impact of our attacks. The outcomes of the attack indicate that the subgraphs produced by the LiSA framework successfully establish links between target nodes and subgraphs, leading to a notable decrease in the accuracy of node classification performance. This demonstrates the effectiveness of the LiSA framework within the SIA scenario. As illustrated in Fig. 3, the LiSA framework surpasses GraphCopy by achieving a higher ASR, thereby demonstrating its superiority over GraphCopy as an optimization-based method. Notably, the average attack budget for GraphCopy is proportional to the square of the average node degree in the dataset. In denser graphs, such as Amazon-Photo and Computers, attacking a single node requires a subgraph size exceeding 10% of the original graph. Conversely, LiSA achieves higher ASR with a much smaller budget, requiring only five nodes for each target node attack, highlighting the efficiency and practicality in real-world scenarios.\nMoreover, it is critical to compare the SIA strategy with the traditional NIA approach. While NIA is highly effective under the idealized assumption of a 100% LSR, the ASR significantly decreases with the introduction of a variable LSR. When comparing the LiSA framework against the NIA, it becomes evident that the ASR of LiSA is comparable to that of NIA at a 50% LSR, and it largely surpasses the performance of NIA at a 10% LSR. This indicates that LiSA demonstrates superior attack efficacy in scenarios where successful link generation is a limiting factor of the attack. Consequently, in such instances, transitioning from the conventional NIA to the SIA strategy via the LiSA framework could be a wiser decision."}, {"title": "5.3 Impacts of Subgraph Size and User Added Links", "content": "We conduct a series of experiments on the Amazon-Photo dataset to investigate how subgraph size impact the attack. Newly generated links are chosen based on top linking scores in the experiments. Our initial observation reveal a decline in LSR as the subgraph size increased, a trend depicted in Fig. 4(a). This decline likely stems from the challenges associated with optimizing larger subgraphs, especially when adjusting both structure and features. However, we noted a stabilization in LSR when the subgraph size exceeds 5. Another interesting finding related to the response of ASR to the number of links added by users, as illustrated in Fig. 4(b). With the addition of a single link, we observe that the"}, {"title": "5.4 Subgraph Analysis", "content": "To generate links between the target node and the subgraph, we expect the node embedding of the subgraph to be similar to that of the target victim node. To test this hypothesis, we train a GAE model on the original clean graph, use its GCN encoder to extract the node embedding of the target node and subgraph nodes, and calculate the cosine similarity between them. Experiments are conducted on both the Cora and PubMed datasets, and the similarity distributions are compared with those of NIA and GraphCopy. As shown in Fig. 5, nodes trained using NIA tend to have a negative similarity with the target nodes, with the distribution peaking at -0.4, indicating a low similarity and resulting in a very low LSR. For SIA, we observe that the node embedding tend to show high similarity, with the distribution peaks shifting to positive. Comparing LiSA with GraphCopy, we find that LiSA carefully refines the subgraph, further shifting the peak by approximately 0.4 compared to GraphCopy. As a result, the subgraph nodes with extremely high similarity are proposed to the target nodes by the link recommender, leading to a successful attack."}, {"title": "5.5 Ablation Studies", "content": "We perform ablation studies of LiSA on the Cora and PubMed dataset as depicted in Fig. 6. This analysis involves experiments with variations of the LiSA framework, including the omission of the surrogate models for node classification and link prediction (denoted as w/o cls and w/o link, respectively). Additionally, we explore the impact of focusing solely on node features optimization (labeled as w/o str) and solely on adjacency matrix optimization with randomly initialized node features (labeled as w/o feat) to discern the specific contributions of these optimization strategies.\nThese findings reveal that adversarial links can be formed effectively through the use of the link prediction surrogate model, with the subgraph thus generated showing a marginal ASR improvement against the clean graph. Conversely, relying only on the node classification surrogate model fails to generate any links between target node and the subgraph. This highlights the significance of utilizing dual surrogate models, which can greatly enhance the attack to accomplish both adversarial objectives. Regarding optimization strategies, we observe that focusing exclusively on the structural aspects without adjusting the node features has no improvement on the attack. However, optimizing only the node features partially achieve the adversarial objectives, indicating that feature optimization plays a primary role in the SIA scenario. Nevertheless, structural optimization emerges as a critical supporting factor, enhancing the overall efficacy of the attack. Thus, the experimental results suggest that each component of LiSA is crucial to improve the attack performance."}, {"title": "6 Conclusions", "content": "This study presents a new attack scenario, along with a novel attack framework that utilizes link recommender to compromise the node classification accuracy of GNNs through subgraph injection. Unlike conventional attacks that require direct manipulation of the original graph or the addition of links to artificially created nodes, LiSA executes an attack indirectly by injecting an isolated subgraph, thereby enhancing practicality in real-world scenarios. Our experiments across various real-world datasets demonstrate the effectiveness of LiSA, illustrating significant degradation in GNN performance."}, {"title": "A Training Algorithm", "content": "The training algorithm of LiSA is shown in Algorithm 1."}, {"title": "B GraphCopy Settings and Attack Results", "content": "GraphCopy introduces a fundamental approach for conducting adversarial attacks, achieving the adversarial objectives to a certain degree, as demonstrated in Table 2. However, it faces two significant limitations that reduce its effectiveness and practicality in real-world scenarios. First, simply duplicating the graph and altering node features does not ensure a high success rate for adversarial attacks. Second, replicating the n-hop neighborhood of a target node leads to a subgraph size that increases quadratically with the node degree. For nodes with high degrees, the attack budget increase substantially, making GraphCopy impractical for use in dense or large-scale networks. In the experimental configuration of GraphCopy, we create the subgraph by replicating the 2-hop neighborhoods surrounding the target nodes and introducing a 10% Gaussian perturbation to the features of the duplicated nodes."}, {"title": "C Dataset Statistics", "content": "The datasets employed in this study are enumerated in Table 3."}, {"title": "D NIA Settings, Results and Analysis", "content": "To evaluate our method in the context of the traditional NIA scenario, we introduce a modified baseline version of NIA that takes into account the LSR. In this variation, each targeted victim node is paired with a single aggressor node, with each connection being subject to a regulated probability of successful formation. The influence of link generation on the effectiveness of the NIA is assessed by modulating LSR of 100%, 50%, and 10%. Results of these assessments are detailed in Table 4.\nWith a 100% LSR, aggressor nodes are guaranteed to link to their target victims, resulting in a significantly high ASR, even when only a single aggressor node is employed. Under these conditions, link generation is not a critical factor in the attack scenario, and NIA demonstrates superior performance over SIA. However, at a 50% LSR, the ASR notably declines as expected, showcasing the advantages of SIA wherein its ASR becomes comparable to that of NIA. When the LSR is reduced to just 10%, the effectiveness of NIA is greatly diminished, making SIA a more viable strategy for achieving adversarial objectives."}, {"title": "E Parameter Sensitivity Analysis", "content": "In this subsection, we delve deeper into how the hyperparameters $\\alpha$ and $\\beta$ influence the LSR and ASR of LiSA. As previously mentioned, $\\alpha$ regulates feature"}, {"title": "F Subgraph Training Time Analysis", "content": "Table 5 presents the subgraph training time of LiSA for each attack. The subgraphs of Cora/PubMed/FacebookPagePage, and Amazon-Photo/Computers were trained with 200 and 300 epochs, respectively, using an A6000 GPU with 48GB of memory."}]}