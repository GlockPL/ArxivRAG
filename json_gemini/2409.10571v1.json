{"title": "ASFT: Aligned Supervised Fine-Tuning through Absolute Likelihood", "authors": ["Ruoyu Wang", "Jiachen Sun", "Shaowei Hua", "Quan Fang"], "abstract": "Direct Preference Optimization (DPO) is a method for enhancing model performance by directly optimizing for the preferences or rankings of outcomes, instead of traditional loss functions. This approach has proven effective in aligning Large Language Models (LLMs) with human preferences. Despite its widespread use across various tasks, DPO has been criticized for its sensitivity to the effectiveness of Supervised Fine-Tuning (SFT) and its limitations in enabling models to learn human-preferred responses, leading to less satisfactory performance. To address these limitations, we propose Aligned Supervised Fine-Tuning (ASFT), an effective approach that better aligns LLMs with pair-wise datasets by optimizing absolute likelihood for each response, rather than using the Bradley-Terry model, and eliminates the need for a reference model. Through theoretical gradient analysis, we demonstrate that ASFT mitigates the issue where the DPO loss function decreases the probability of generating human-dispreferred data at a faster rate than it increases the probability of producing preferred data. Additionally, we compare ASFT to DPO and its latest variants, such as the single-step approach ORPO, using the latest instruction-tuned model Llama3, which has been fine-tuned on UltraFeedback and HH-RLHF. We evaluated performance on instruction-following benchmarks like MT-Bench and traditional text generation metrics such as BLEU-4 and ROUGE-L. Extensive experiments demonstrate that ASFT is an effective alignment approach, consistently outperforming existing methods.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) trained on large datasets have shown exceptional abilities in generating text that resembles human writing (Brown et al. 2020; Jiang et al. 2023; Zhang et al. 2022). However, their outputs can frequently deviate from human values and intentions, especially in sensitive or nuanced situations. To address these challenges, researchers have created techniques that utilize human feedback to inform the training process, thereby enhancing the alignment of LLMs with human expectations. Reinforcement learning from human feedback (RLHF) (Christiano et al. 2023; Stiennon et al. 2022; Ouyang et al. 2022) is a widely used approach for fine-tuning language models, aiming to enhance their alignment with human values and preferences. While classical RLHF approach (Schulman et al. 2017) has demonstrated notable success in aligning LLMs with human values, it also poses significant challenges. These challenges include the high computational costs involved in training both the reward and policy models, the risk of reward hacking, and the instability of the training process stemming from the complex interactions between the two models.\nTo overcome the limitations of RLHF, recent research has introduced Direct Preference Optimization (DPO) (Rafailov et al. 2024), a method that directly optimizes human preferences. DPO simplifies the RLHF pipeline by eliminating the need for a separate reward model. Instead, it directly uses human preferences to update the language model's parameters. Despite its considerable success across various tasks, DPO still presents certain limitations that can result in less satisfactory performance, as highlighted by previous research (Ethayarajh et al. 2024; Xu et al. 2024; Feng et al. 2024). Specifically, DPO has been criticized for its sensitivity to the effectiveness of the Supervised Fine-Tuning (SFT) phase and its requirement for a reference model, which makes it challenging to implement (Xu et al. 2024). In other words, LLMs that lack proper and effective SFT often demonstrate suboptimal performance when undergoing DPO. Empirical evidence suggests that SFT and instruction tuning are essential for LLMs to comprehend and follow human directives, which is a prerequisite for effectively aligning with curated human feedback (Bai et al. 2022). Moreover, the DPO loss function tends to decrease the probability of generating human-dispreferred data more rapidly than it increases the likelihood of producing preferred data, thereby hindering the model's capacity to learn and generate human-preferred responses (Feng et al. 2024). In fact, alignment methods based on the Bradley-Terry model (Bradley and Terry 1952) all encounter this issue.\nIn this work, we theoretically analyze the challenges in gradient optimization of preference alignment methods based on the Bradley-Terry model. To solve the problem, we propose Aligned Supervised Fine-Tuning (ASFT), an effective approach that better aligns LLMs with pairwise datasets by optimizing the absolute likelihood for each response, rather than relying on the Bradley-Terry model. To simplify the training process, policy model can learn human preferences solely from SFT without requiring an additional reference model. Our algorithm core is to maximize the maximum likelihood estimation of chosen answers and minimize the maximum likelihood estimation of rejected answers. This approach significantly bolsters the robustness of large models during training in Section 4. In summary, ASFT has the following advantages:\n\u2022 Lower computational overhead and more efficient training process: ASFT can align LLMs with human preferences solely during the SFT stage, eliminating the need for a reference model.\n\u2022 More reasonable gradient descent strategy: ASFT can mitigate the issues caused by gradient optimization strategies that impede the model's ability to learn human-preferred responses.\n\u2022 Significant performance on benchmark: ASFT demonstrated superior performance on MT-Bench and particularly on Arena-Hard, where it outperformed SFT by 48% and significantly exceeded the results of DPO and its variants."}, {"title": "2 Related Works", "content": "2.1 Direct Preference Optimization (DPO)\nDPO (Rafailov et al. 2024) is a method that focuses on aligning LLMs with pair-wise preference data (x, yw, Y\u0131) by directly optimizing the relationship between model outputs and human preferences. DPO simplifies the RLHF pipeline by eliminating the need for a separate reward model, instead, it employs a closed-form expression with an optimal policy to reparameterize the reward function r:\nr(x, y) = a log \\frac{\\pi_{\\rho}(y | x)}{\\pi_{ref}(y|x)} + a log Z(x) \\qquad(1)\nBy integrating this reward construction into the Bradley-Terry (BT) ranking objective (Bradley and Terry 1952), \np(Yw > Y\u0131 | x) = \u03c3 (r (x, yw) \u2212 r (x,y\u0131)) \\qquad(2)\nDPO can represent the probability of preference data using the policy model instead of the reward model, resulting in the following objective:\nL_{DPO} (\\pi_{\\theta}; \\pi_{ref}) = -E_{(x,yw,y1)~D} [log \u03c3 (alog \\frac{\\pi_{\\theta} (yw | x)}{\\pi_{ref} (Yw | x)} - alog \\frac{\\pi_{\\theta} (y_l | x)}{\\pi_{ref} (Y_l x)} )] \\qquad (3)\nThis objective allows DPO to optimize the policy directly with respect to the preferences, without the need for an intermediate reward model.\n2.2 Alignment without Reference Model\nEmpirical findings suggest that, non-reinforcement learning (non-RL) alignment methods (Rafailov et al. 2024; Azar et al. 2023) are sensitive to the effectiveness of reference model (Feng et al. 2024; Tunstall et al. 2023). In contrast, there have been approaches that aim to build human-aligned language models by conducting SFT using filtered datasets alone (Zhou et al. 2023; Haggerty and Chandra 2024) indicated SFT with a limited amount of filtered data can effectively enhance the model's capabilities. Furthermore, the proposal and significant performance of certain single-step fine-tuning methods (Hong, Lee, and Thorne 2024; Meng, Xia, and Chen 2024) clearly demonstrate the feasibility of aligning large models without the need for reference model. ORPO (Hong, Lee, and Thorne 2024) explores the role of SFT, providing a theoretical foundation for incorporating preference alignment into SFT. SimPo (Meng, Xia, and Chen 2024) demonstrates the potential of single-step fine-tuning for aligning large models through extensive experiments. However, the feasibility for aligning large models without reference model is still insufficiently explored from the perspective of gradient strategies.\n2.3 Limitations of Bradley-Terry model in aligning LLMS\nThe Bradley-Terry (BT) model, often used in the context of learning from human preferences, has certain limitations when applied to large model alignment, particularly within the framework of reinforcement learning and non-reinforcement learning (non-RL) alignment method. Azar et al. conducted extensive research on the assumptions underlying the application of the BT model in aligning LLMs. Feng et al. concluded that DPO impairs the ability of large models to learn human preference responses, based on their analysis of the optimization process of DPO. In fact, not only DPO but also other alignment methods based on the BT model face this issue. We conducted a detailed discussion on the optimization process of alignment methods based on the BT model and compared them with the optimization strategy of ASFT in Section 4."}, {"title": "3 Method", "content": "In this section, we first review the derivation of the BT model. We then explain why \u2013 log \u03c3(x) is commonly used as a loss function in deep learning models. Finally, we derive the ASFT loss function by introducing new scoring rules and calculating the scores for the selected response and the rejected answer separately.\n3.1 Bradley-Terry model derivation\nThe Bradley-Terry model is a classic human preference model. It is a probabilistic framework used to predict the outcomes of two competitors (such as individuals or teams). Commonly applied to pairwise comparison data, it estimates and compares the relative abilities of individuals or projects. The BT model is a probabilistic model, where the probability of the preference data i > j is given by:\nP(i > j) = \\frac{P_i}{P_i + P_j} \\qquad(4)\nWhere pi is the positive real fraction of i, we can reparameter it in the following form:\nP(i > j) = \\frac{e^{\\beta_i}}{e^{\\beta_i} + e^{\\beta_j}} = \\frac{1}{1+e^{-(\\beta_i-\\beta_j)}} \\qquad(5)\nThe score function corresponding to the participants is given by: Pi = e\u00dfi, which is consistent with the sigmoid function."}, {"title": null, "content": "\u03c3(x) = \\frac{1}{1+ e^{-x}} \\qquad (6)\nFinally, we can use Maximum Likelihood Estimation (MLE) to calculate the score for each player:\narg \\min \\sum_{i,j} \u2013 log \u03c3 (\u03b2i \u2013 \u03b2j) \\qquad (7)\n\u03b2\n3.2 -Log sigmoid function\nThe -logsigmoid function can be expressed as:\n\u2013 log(\u03c3(x)) = - log (\\frac{1}{1+ e^{-x}} ) = \u2013 log \u03c3(x) \\qquad (8)\nThrough the formula, we can observe that the -logsigmoid function has two fundamental properties:\n\u2022 As x \u2192 -\u221e, f(x) \u2248 x\n\u2022 As x \u2192 +\u221e, f(x) \u2192 0\nThese properties make the -logsigmoid function an effective choice for a loss function in machine learning models, where it provides both numerical stability and penalizes incorrect predictions effectively.\nFor the BT model, the larger the difference between score the ro (x, yw) and r\u00f8 (x, y\u0131), the smaller the value of the loss function -logsigmoid:\nL_R (r_{\\phi}, D) = -E_{(x,yw,y1)~D} [log \u03c3 (\u03b2r_{\\phi} (x, yw) \u2013 \u03b2r_{\\phi} (x,y1))] \\qquad (9)\n3.3 Aligned Supervised Fine-Tuning\nUnlike the BT model, which constructs the loss function by comparing two different answers, our approach uses the probability of the response output by the LLMs as its score. Specifically, we design the loss function to maximize the score of the chosen response while minimizing the score of the discarded response.\nThe score for the chosen response is as follows:\nS (x, y) = \u03c0\u03b8 (\u03b6\u03c9 | x)\n= \u03c3(\\frac{\\pi_{\\theta} (Ywx)}{1 \u2013 \u03c0_{\\theta} (\u03b6\u03c9 | x) + \u03c0_{\\\u03b8} (\u03c8\u03c9 | x)} ) = \u03c3 (log \\frac{\\pi_{\\theta} (Ywx)}{1 \u2013 \u03c0_{\\\u03b8} (\u03b6\u03c9 | x)} ) \\qquad (10)\nLet log \\frac{\\pi_{\\theta} (\u03b6\u03c9 | x)}{1 \u2013 \u03c0_{\\\u03b8} (\u03b6\u03c9 | x)} denote fo (x,y), then we define\nASFT optimisation objective as:\nmax E(x,yw,y1)~Do (fo (x,yw))\n\u03c0\u03b8 \\qquad(11)\nmin E(x,yw,y1)~Do (fo (x, y1))\n\u03c0\u03b8\nThe above equation is equivalent to\nmax E(x,yw,y1)~Do (fo (x, yw))\n\u03c0\u03b8 \\qquad (12)\nmax E(x,yw,y1)~D 1 \u2013 0 (fo (x, y))\n\u03c0\u03b8\nThe loss function form of Lalign obtained from above is as follows:\nL_{align} = - log \u03c3 (fo (x, yw)) \u2013 log \u03c3(- fo (x, y)) \\qquad (13)\nThe final objective function ASFT in Equation 14 consists of two components: Supervised Fine-Tuning (SFT) loss LSFT and loss for alignment Lalign.\nL_{ASFT} = E_{(x,yw,Y1)} [L_{SFT} + \u03b2L_{align}] \\qquad (14)\nLSFT in Equation 14 follows the standard approach of minimizing the negative log-likelihood (NLL) loss, which is derived from causal language modeling. This objective is aimed at maximizing the probability of generating the target tokens given the input context.\nIn contrast to alignment methods based on the BT model, which utilize relative likelihoods for aligning large models, Lalign in Equation 13 employs absolute likelihoods to further align large models based on paired data. This approach enhances the model's ability to adjust its outputs more precisely according to specific performance criteria, thereby improving its effectiveness in tasks that require fine-grained distinctions between similar inputs."}, {"title": "4 Improvements in The Optimization Process", "content": "Previous studies have shown that DPO impedes the ability of LLMs to learn human-preferred responses due to limitations in its gradient optimization process. (Feng et al. 2024) Indeed, we find that alignment methods based on BT models suffer from this issue. In this section, we conducted a detailed analysis of the gradients in alignment methods based on the BT model and provided insights from a gradient perspective on SFT plays a crucial role in enhancing the performance of reference-dependent approaches. Subsequently, we discuss the advantages of the gradient strategy employed by ASFT and demonstrate its exceptional performance, even without the need for a reference model derived through SFT.\n4.1 Optimization Process of BT model\nExisting alignment methods often frame the task as a binary classification problem, employing the Bradley-Terry (BT) model to derive a negative log-likelihood loss in Equation 9. To simplify the analysis, we denote r\u00f8 (x, yw) = log x1 and ro (x, yw) = log x2 1 . Thus, the objective function of BT model can be written in the following form:\nL_R (r, D) = -log \u03c3 (\u03b2r\u03c6 (x, yw) \u2013 \u03b2r\u00a2 (x,y1)) = -log (\\frac{x_1^{\\beta}}{x_1^{\\beta} + x_2^{\\beta}}) \\qquad (15)\nTo ensure that x1 reflects the probability of generating a human-preferred response, we adopt a form similar to r(x, y) x log \u03c0\u03b8 (y | x) in DPO, setting r\u00f8 (X, Yw) = log x1 for consistency. The definition of x2 follows the same principle."}, {"title": null, "content": "We denote the transformed form as LR (X1;12). And Feng et al. have calculated the partial derivatives of Equation 15 with respect to x1 and 22:\n{\\frac{\\partial L_R(X_1;X_2)}{\\partial x_1} = \\frac{\\beta x_1^{\\beta -1}}{x_1(x_1^{\\beta} + x_2^{\\beta})}\\{\\frac{\\partial L_R(X_1;X_2)}{\\partial x_2} = \\frac{-\\beta x_2^{\\beta -1}}{x_2(x_1^{\\beta} + x_2^{\\beta})} \\qquad(16)\nTo evaluate the impact on generation probabilities for human-preferred and dispreferred responses, and to enable a systematic comparison with ASFT, we present a visualization of the optimization plane (loss landscape) and the gradient field in Figure 1(a). Building upon the findings of (Feng et al. 2024), we distill several critical features of the gradient field, which we summarize into the following key aspects:\n\u2022 Case 1 In the initial optimization stages, when x1 is very small and x2 is very large, LLMs tend to generate dispreferred responses. The BT gradient flow mainly increases X1, with minor changes to x2, as shown in the top left corner of Figure 1(a).\n\u2022 Case 2 When 21 and 22 are both large, LLMs may generate both preferred and dispreferred responses. In this scenario, The BT gradient flow slightly increases x1 and decreases x2, leading to minimal overall changes and potentially trapping the optimization in saddle points. as illustrated in the top right corner of Figure 1(a).\n\u2022 Case 3 When x2 is very small, indicating limited LLM response variability, the BT gradient flow quickly decreases x2 with minor adjustments to x1, as depicted in the lower part of Figure 1(a).\nBy analyzing the gradient vector field of BT model, we identified two limitations of BT model.\nOn one hand, LR (X1; 12) has a more significant impact on 22 than on x1, because the gradient with respect to x2 is larger than that for x1. As a result, LR (X1; 12) decreases the probability of generating human-dispreferred data more rapidly than it increases the probability of generating preferred responses.\nOn the other hand, the initial state in the gradient vector field significantly influences the final optimization outcome. Specifically, the initial position of LLMs with SFT might be located in the lower-left corner of Figure 1(a), indicating low probabilities of generating both human-preferred and dispreferred responses, with the gradient direction not fully prioritizing the enhancement of preferred responses. Alternatively, if the initial position is in the upper-right corner of Figure 1(a), the very small gradients present there can lead to slow convergence and difficulties in escaping local minima, potentially resulting in insufficient learning from human-preferred data.\n4.2 Improvements in ASFT\nTo analyze the optimization process of ASFT and compare it with the BT model, We apply the same transformation to the alignment loss Lalign in ASFT, as shown in Equation 17. Denote the transformed form as Lalign (X1; X2). We then analyze its partial derivatives with respect to x1 and 22, respectively.\nL_{align} = log \u03c3 (fo (x, yw)) \u2013 log \u03c3(- fe (x,y\u0131)) = log \u03c3 (log \\frac{X_1}{1 - X_1}) - log \u03c3 (log \\frac{1}{1 - X_2}) \\qquad (17)\nThe partial derivatives of Equation 17 with respect to x1 and x2 are given by:\n{\\frac{\\partial L_{align} (X_1;X_2)}{\\partial x_1} = \\frac{1}{x_1} \\\\{\\frac{\\partial L_{align} (X_1;X_2)}{\\partial x_2} = \\frac{1}{1 -x_2}} \\qquad (18)\nWe compute the partial derivatives of the alignment loss function Lalign with respect to x1 and 22, and construct the corresponding gradient fields in Figure 1(b) to visually reveal the dynamic characteristics of ASFT. Our analysis shows that in all three scenarios previously discussed, the gradients in ASFT more effectively adjust 21 and 22, enabling LLMs to better learn to generate responses aligned with human preferences while avoiding those that are not.\n\u2022 For Case 1, the ASFT gradient flow simultaneously increases x1 and decreases x2, allowing the loss function to converge more quickly to the optimal solution.\n\u2022 For Case 2, the ASFT gradient flow rapidly reduces X2 with minimal change in x1, helping LLMs quickly decrease the probability of generating responses that are misaligned with human preferences.\n\u2022 For Case 3, the ASFT gradient flow rapidly increases x1 with minimal change in 12, aiding LLMs in quickly increasing the likelihood of generating responses that align with human preferences.\nThe alignment loss in ASFT has a balanced impact on both x1 and 12. As a result, ASFT is able to learn how to generate human-preferred responses while simultaneously learning to avoid generating responses that humans do not prefer.\nFor each pairwise preference data (x, yw, y\u0131) \u2208 D, the update rate of x1 in Lalign (X1; 12) with respect to x2 is 1-22. As x1 tends to increase and 12 tends to decrease during optimization, we have 1-2 \u2192 1. This indicates that Lalign updates 21 and 22 in a balanced manner. As a result, ASFT effectively identifies and addresses the key factors in different scenarios during the optimization process. It prioritizes addressing the primary issues without showing a fixed bias toward either X1 or 12. This characteristic allows ASFT to avoid unnecessary actions, thereby enhancing optimization efficiency. In other words, ASFT guides LLMs to focus simultaneously on both improving responses that align with human preferences and mitigating those that do not.\nBased on the characteristics of the gradient field of Lalign, we find that ASFT is not sensitive to the initial model.\n\u2022 When the initial position of the LLMs is in the upper-right corner of Figure 1(b), ASFT shifts its focus to reducing the probability of generating human-dispreferred responses. In this scenario, ASFT demonstrates its ability to quickly decrease this probability and prevent the LLMs from generating responses that are not aligned with human preferences.\n\u2022 When the initial position of the LLMs is on the left side of 1(b), ASFT shifts its emphasis to increasing the probability of generating human-preferred responses. In this case, ASFT is able to rapidly enhance this probability. Specifically, when the initial position of the LLMs is in the upper-left corner of Figure 1(b), ASFT can simultaneously increase the probability of generating human-preferred responses while decreasing the probability of generating dispreferred responses.\nIn summary, by analyzing the gradient field of the alignment loss in ASFT, we conclude that ASFT does not suffer from the limitations of BT model, such as an excessive focus on dispreferred responses. This means that ASFT can increase the probability of generating human-preferred responses while decreasing the probability of generating responses that humans do not favor. Additionally, ASFT is not sensitive to the initial model, making it robust across various starting points.\n4.3 Analysis of Feasibility Without a Reference Model\nBased on the previous analysis, we know that the loss function in BT models is sensitive to the reference model. The reference model, used as the initial model, carries inherent uncertainty, which can negatively impact optimization results, leading to suboptimal performance in fine-tuning large models for alignment. Specifically, after the SFT stage, the reference model is more likely to fall into Case 2, where the BT model's optimization process may struggle with getting stuck at a saddle point. In general, removing the reference model makes the initial position of LLMs unpredictable. When the initial model is located on the left side of Figure 1(a), the focus of optimization should be on increasing the probability of generating human-preferred responses. Specifically, in Case 3, the BT model tends to focus more on reducing the probability of generating dispreferred responses, which can hinder optimization by providing insufficient gradients to maximize the probability of generating preferred responses. Therefore, despite the improved memory and computational efficiency from removing the reference model, the objective function under the BT model has inherent limitations, leading to instability in the results."}, {"title": "5 Experiments", "content": "5.1 Experiment Setups\nModel and training settings. We applied our proposed algorithm to fine-tune the Llama3-8B-Instruct (AI@Meta. 2024) to validate its effectiveness. We observed that tuning hyperparameters is crucial for achieving optimal performance across all offline preference optimization algorithms 2. Specifically, for ASFT, setting the parameter \u03b2 within the range of 0.1 to 0.5 generally yields satisfactory results. In this experiment, \u03b2 is set to 0.1. The learning rate for the training process is set to 1e-05.\nReward dataset. We employed the UltraFeedback (Cui et al. 2024) dataset, a GPT-4 annotated instruction tracking dataset containing approximately 64,000 instructions. This dataset is strategically designed to overcome the challenges associated with acquiring human feedback by automating the collection of high-quality AI feedback. Additionally, we utilized the HH-RLHF (Bai et al. 2022) dataset to evaluate the performance of our model on traditional text generation metrics. This approach enables a comprehensive assessment of the model's capabilities in generating coherent and contextually relevant responses under various conversational scenarios.\nBaselines. We compared ASFT with other offline preference optimization methods, including IPO (Azar et al. 2023), DPO (Rafailov et al. 2024), and ORPO (Hong, Lee, and Thorne 2024). IPO is a theoretically grounded method that circumvents the assumptions made by DPO, specifically the notion that pairwise preferences can be substituted with point-based rewards. ORPO introduces an odd-ratio comparison term involving a reference-free model, enabling direct contrasts between winning and losing responses with the policy model, and is jointly trained with the SFT objective.\nEvaluation metrics. We primarily employed two of the most popular open-ended instruction tracking benchmarks to evaluate our model: MT-Bench (Zheng et al. 2023) and Arena-Hard v0.1 (Li et al. 2024). These benchmarks assess the model's multifaceted conversational capabilities across\n2For ORPO, \u03bb is typically set to 0.1. For DPO, \u03b2 is commonly within the range of [0.1, 0.5]."}, {"title": "5.2 Experimental Results", "content": "Difference in benchmark discrimination\nAlthough both MT-Bench and Arena-Hard are widely utilized, we observed that MT-Bench exhibits poor separability among different methods. The minor differences between methods in MT-Bench could be attributed to randomness, potentially due to its limited evaluation dataset size. In contrast, Arena-Hard demonstrates superior discriminative ability, providing clearer distinctions between the performances of various approaches.\nEfficiency of ASFT\nASFT operates without the need for a reference model, making it lighter and simpler to implement than DPO and similar reference-dependent methods. This results in reduced computational expenses and enhances the efficiency of the training process. During model fine-tuning, ASFT reduces time by 13% and decreases peak GPU memory usage by 16.7% compared to reference-dependent methods (Figure 2(a))."}, {"title": "6 Limitations", "content": "Hyperparameter in experimental setting.\nDue to the incorporation of the \u03b2 in the loss function, manual tuning is necessary. Currently, neither experimental data nor theoretical analysis has provided a definitive method for optimally setting the beta parameter across various scenarios. Future research could focus on developing methodologies to automate the determination of beta value. This advancement would significantly improve the adaptability and efficacy of the approach.\nHarmlessness and honesty.\nAlignment methods are designed to adjust the output distributions of LLMs to improve their helpful, safety, and honesty. However, our current research primarily focuses on enhancing the utility of large models using the UltraFeedback dataset (Cui et al. 2024), with less attention given to safety and honesty, which are essential for deploying LLMs across diverse domains. Future research should prioritize developing strategies to bolster both the safety and integrity of large models. From a robustness standpoint, even well-aligned models may become vulnerable when exposed to even minimal amounts of unsafe data. Additionally, fine-tuning aligned LLMs on benign datasets could inadvertently compromise their built-in safety protocols. Addressing these challenges is crucial for ensuring that enhancements in utility do not come at the expense of reduced safety or compromised ethical standards.\nPoor performance in mathematics Although the ASFT algorithm outperforms other methods in many aspects, we found that it exhibits a higher error rate when dealing with mathematical reasoning problems. This may be due, in part, to the Llama model's limited ability to solve mathematical problems in a zero-shot setting, and also because the preference optimization objective might not effectively increase the likelihood of preference sequences (Pal et al. 2024), potentially hindering learning from mathematical preference pairs. Future work could explore strategies to address the suboptimal performance of large models on complex reasoning tasks."}, {"title": "7 Conclusion", "content": "In this work, we address the alignment of large models within a gradient optimization framework. By analyzing the gradients of alignment algorithms based on the BT model, we introduce a novel and practical approach: ASFT. This method aligns models using absolute likelihoods, overcoming the issue where algorithms loss function based on the BT model tends to decrease the probability of generating human-dispreferred data more rapidly than it increases the likelihood of producing preferred data. Additionally, we thoroughly discuss the theoretical justification for ASFT's operation without a reference model. The effectiveness of our approach is robustly validated through its application to the Llama3-8B model, demonstrating ASFT's significant potential in more effectively integrating language models with human feedback."}, {"title": "A Proof of Theorems", "content": "Theorem A.1. The partial derivatives of LR (x1,x2) = log (\\frac{x_1^{\\beta}}{x_1^{\\beta} + x_2^{\\beta}}) with respect to x1 and x2 are given by:\n{\\frac{\\partial L_R(x_1;x_2)}{\\partial x_1} = \\frac{\\beta x_1^{\\beta -1}}{x_1(x_1^{\\beta} + x_2^{\\beta})}\\{\\frac{\\partial L_R(x_1;x_2)}{\\partial x_2} = \\frac{-\\beta x_2^{\\beta -1}}{x_2(x_1^{\\beta} + x_2^{\\beta})} \\qquad(19)\nProof. For  \\frac{\\partial L_R(x_1;x_2)}{\\partial x_1}\n\\frac{\\partial L_R (X_1; X_2)}{\\partial x_1} =  x_1^{\\beta} + x_2^{\\beta}  (\\frac{\\beta x_1^{\\beta -1}x_2^{\\beta} - \\beta x_2^{\\beta -1}}{(x_1^{\\beta} + x_2^{\\beta})^2}) X\n=   (\\frac{\\beta x_1^{\\beta -1}(x_1^{\\beta} + x_2^{\\beta}) - \\beta x_2^{\\beta-1}}{x (x_1^{\\beta} + x_2^{\\beta})})\n=\\frac{\\beta x_1^{\\beta -x}}{x(x_1^{\\beta} + x_2^{\\beta})}\n=\\frac{\\beta x_1^{\\beta}}{x_1 (x_1^{\\beta} + x_2^{\\beta})} \\qquad(20)\nFor \\frac{\\partial L_R(x_1;x_2)}{\\partial x_2}\n\\frac{\\partial L_R (X_1; X_2)}{\\partial x_2} = \\frac{1}{x_1^{\\beta} + x_2^{\\beta}} \\beta  x_2^{\\beta-1}\n=\\frac{\\beta x_2^{\\beta-1}}{x_1^{\\beta} + x_2^{\\beta}} \\qquad(21)\nTheorem A.2. The partial derivatives of Lalign (x1,x2) = - log \u03c3 (log \\frac{x_1}{1 - X_1}) - log \u03c3 (log \\frac{1}{1 - X_2}) with respect to x1 and X2 are given by:\n{\\frac{\\partial L_{align}(x_1,x_2)}{\\partial x_1} = \\frac{1}{x_1} \\\\{\\frac{\\partial L_{align}(X_1;X_2)}{\\partial x_2} = \\frac{1}{1 -x_2}} \\qquad (22)\nProof. We perform the following transformation on Lalign (X1,X2):\nL_{align} (x_1, x_2) = -log\u03c3 (log \\frac{x_1}{1 - X_1}) -log\u03c3 (log \\frac{1}{1 - X_2})\n= -log\u03c3 (log \\frac{x_1}{1 - X_1 + X_1}) -log\u03c3 (log \\frac{X_2}{1 - X_2 + X_2})\n= -log x1 - log (1 - x2) \\qquad (23)\nThe partial derivatives of Lalign (x1, x2) with respect to x1 and 12 are given by:\n\\frac{\\partial L_{align} (X_1;X_2)}{\\partial x_1} = \\frac{1}{x_1} \\qquad (24)\nand\n\\frac{\\partial L_{align} (X_1; X_2)}{\\partial x_2} = \\frac{1}{1 -x_2} \\qquad (25)"}, {"title": "B Pseudocode", "content": "PyTorch code for the ASFT Loss is provided below:\nimport torch.nn.functional as F\ndef preference_loss(self, chosen_logps, rejected_logps, beta):\n\"\"\"\nchosen_logps: Log-probabilities of the chosen options.\nrejected_logps: Log-probabilities of the rejected options.\nASFT_loss: The total loss which combines the supervised fine-tuning loss\n(sft_loss) and the scaled alignment loss (beta * log_f_theta).\nlog_f_theta: Computes a modified log odds ratio between the chosen and rejected\noutputs.\nsft_loss: The negative log probability of the chosen outputs, which represents\nthe supervised fine-tuning (SFT) loss.\nbeta: A scaling factor applied to the log_f_theta, balancing the influence of the\nalignment-focused loss against the supervised fine-tuning loss.\n\"\"\"\nlog_f_theta = F.logsigmoid(chosen_logps + torch.log1p(-torch.exp(chosen_logps))) \\\nF.logsigmoid(rejected_logps + torch.log1p(-torch.exp(rejected_logps)))\nsft_loss = -chosen_logps\nASFT_loss = sft_loss + beta * log_f_theta\nreturn ASFT_loss"}, {"title": "C Experimental Details", "content": "All models are fine-tuned from the publicly accessible meta-llama/llama3. Flash-Attention 2 ((Dao 2023)) was applied for all the models during fine-tuning for computational efficiency. For optimizer, we use AdamW optimizer ((Loshchilov and Hutter 2019)) with a learning rating of 1e-05. And the linear warmup with cosine decay was applied for the learning rate. For input length, every instance was truncated and padded to 2"}]}