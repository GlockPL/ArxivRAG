{"title": "AttenGluco: Multimodal Transformer-Based Blood Glucose Forecasting on AI-READI Dataset", "authors": ["Ebrahim Farahmand", "Reza Rahimi Azghan", "Nooshin Taheri Chatrudi", "Eric Kim", "Gautham Krishna Gudur", "Edison Thomaz", "Giulia Pedrielli", "Pavan Turaga", "Hassan Ghasemzadeh"], "abstract": "Diabetes is a chronic metabolic disorder characterized by persistently high blood glucose levels (BGLs), leading to severe complications such as cardiovascular disease, neuropathy, and retinopathy. Predicting BGLs enables patients to maintain glucose levels within a safe range and allows caregivers to take proactive measures through lifestyle modifications. Continuous Glucose Monitoring (CGM) systems provide real-time tracking, offering a valuable tool for monitoring BGLs. However, accurately forecasting BGLs remains challenging due to fluctuations due to physical activity, diet, and other factors. Recent deep learning models show promise in improving BGL prediction. Nonetheless, forecasting BGLs accurately from multimodal, irregularly sampled data over long prediction horizons remains a challenging research problem. In this paper, we propose AttenGluco\u00b9, a multimodal Transformer-based framework for long-term blood glucose prediction. AttenGluco employs cross-attention to effectively integrate CGM and activity data, addressing challenges in fusing data with different sampling rates. Moreover, it employs multi-scale attention to capture long-term dependencies in temporal data, enhancing forecasting accuracy. To evaluate the performance of AttenGluco, we conduct forecasting experiments on the recently released AIREADI dataset, analyzing its predictive accuracy across different subject cohorts including healthy individuals, people with prediabetes, and those with type 2 diabetes. Furthermore, we investigate its performance improvements and forgetting behavior as new cohorts are introduced. Our evaluations show that AttenGluco improves all error metrics, such as root mean square error (RMSE), mean absolute error (MAE), and correlation, compared to the multimodal LSTM model, which is widely used in state-of-the-art blood glucose prediction. AttenGluco outperforms this baseline model by about 10% and 15% in terms of RMSE and MAE, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "According to the World Health Organization [1], the prevalence of type 2 diabetes has increased significantly over the last decades. In 2022, 14% of adults aged 18 years and older were living with diabetes, double the 7% reported in 1990 [2]. This increase is attributed to various factors such as sedentary lifestyles, stress, poor diet, and an aging population [3]. As a result, type 2 diabetes poses a significant public health challenge that requires urgent attention and intervention. Poor management of type 2 diabetes can lead to the progression of chronic health complications and an increased risk of both hyperglycemic and hypoglycemic events. Effectively managing blood glucose levels through consistent monitoring and accurate forecasting is crucial as early intervention measures to prevent hyperglycemic and hypoglycemic events. Accurate glucose prediction is essential for optimizing insulin dosages, meal planning, and exercise habits to maintain blood glucose levels within a safe range. CGM devices have been developed as an advanced technology to support diabetes management. CGM devices provide valuable insights into blood glucose fluctuations by collecting continuous glucose signals. The CGM data allows patients to monitor fluctuations and trends in their blood glucose levels more effectively by providing real-time blood glucose level measurements. Thus, CGM devices have grown significantly in recent years, making them a widely adopted tool for diabetes prevention. Furthermore, physiological and behavioral variables, such as physical activity levels (e.g., walking or running) and stress levels, affect blood glucose fluctuation [4]. Therefore, the accurate forecasting of blood glucose levels can be evaluated by combining BGL signals with other physiological and behavioral variables. This data integration enables a more comprehensive and personalized approach to managing diabetes, especially for individuals with type 2 diabetes.\nRecently, artificial intelligence (AI) and machine learning algorithms have played a critical role in the control and prediction of blood glucose levels. These advanced technologies leverage data from CGM devices and integrate with physiological signals, such as stress levels, heart rate, and physical activity signals. By analyzing these complex datasets, the algorithms can identify trends and patterns in blood glucose fluctuations with high accuracy.\nSequential machine learning models, notably Long Short-Term Memory (LSTM) networks and Gated Recurrent Units (GRUs) [5], are extensively employed in forecasting time-series signals due to their ability to capture temporal dependencies. GRU-based models outperform traditional methods in univariate time-series classification tasks[6]. Moreover, these models are highly effective for time-series forecasting when optimized with suitable algorithms [7]. These models have also been extensively applied in predicting Type 1 diabetes outcomes [8]. However, they often struggle to capture long-term dependencies inherent in time-series data, which in turn limits their effectiveness in long-term forecasting [9]. Research indicates that while LSTMs are designed to manage longer sequential correlations compared to traditional RNNs, they still encounter challenges in memorizing extended sequences [10].\nRecently, transformers have emerged as a powerful model"}, {"title": "II. RELATED WORK", "content": "Recent years have seen a surge in blood glucose management technologies. CGM systems, wearable health monitoring devices, and automated insulin delivery systems (AIDS) collectively provide real-time data and partial automation for diabetes care [2], [16]. CGMs offer continuous monitoring of glucose levels, synchronizing with mobile applications for timely alerts on hyperglycemia and hypoglycemia [1]. Wearable sensors further extend coverage to physiological and behavioral metrics, such as heart rate variability and physical activity levels [3], [17]. By combining CGM outputs with additional signals, AIDS can regulate insulin dosage more precisely [18]. However, limitations persist in terms of sensor calibration, missing data, and user non-adherence [16].\nEarly prediction efforts relied on statistical and time-series models, notably Autoregressive Integrated Moving Average (ARIMA) [19]. Although ARIMA and similar approaches are straightforward, they often fail to capture the complex, nonlinear patterns of glycemic fluctuation. Machine learning (ML) techniques, such as support vector regression and random forests, typically reduce forecasting error by 5-15% compared to ARIMA [19], [20]. However, they still struggle with deeper temporal dependencies over longer prediction windows [20].\nDeep learning techniques, widely applied across various domains such as healthcare [21], [22], [23], classification tasks [24], offer improved forecasting accuracy by effectively modeling intricate temporal dependencies and nonlinear patterns in time series data. Furthermore, integrating causal knowledge into learning frameworks [25], [26] can enhance adaptability and facilitate knowledge transfer across different environments. LSTM architectures are proposed to mitigate vanishing and exploding gradients in recurrent neural networks [4]. By gating internal states, LSTMs retain long-term context for extended horizons, outperforming classical ML methods in certain datasets [4]. Despite these improvements, LSTM-based models often demand significant computational resources and meticulous tuning, making them less flexible for large-scale or highly variable glucose data [10]. GRUs streamline the gating structure of LSTMs, converging 15-25% faster for some time-series tasks [5], [6]. Nevertheless, GRUs still encounter challenges related to sensor inaccuracies, incomplete user logs, and irregular sampling rates [10]. Hybrid methods that integrate convolutional layers with recurrent modules reduce some errors by 2-4% [6], yet extensive clinical validation for blood glucose forecasting remains limited.\nTransformers adopt self-attention instead of recurrent loops which facilitates parallel learning over extensive sequences [11]. This approach outperforms RNNs by 5-10% in mean squared error (MSE) for long-horizon predictions [9], [27]. However, many existing implementations assume large, consistent datasets with minimal missing points. Glucose monitoring, conversely, often faces sensor dropouts and user non-adherence, limiting straightforward application [28]. Gluformer [29] developed a transformer-driven blood glucose forecasting model by providing uncertainty intervals rather than single-point estimates. Although a 1-2 mg/dL improvement in short-horizon RMSE has been observed, the absence of multi-scale/cross-attention hinders the model's ability to integrate additional clinical or activity data [29].\nIn summary, current blood glucose prediction models have notable limitations. ARIMA struggles with nonlinearities [19], [20], while ML models such as support vector regression improve RMSE but fail in long-range forecasting [19], [20]. LSTMs and GRUs improve but suffer from irregular sample rates of various sensors [10], [4]. Recurrent models still encounter inefficiencies for long-horizons forecasting [10]. Transformers, including Gluformer, introduce multi-head attention but lack effective cross-attention for integrating multimodal data [30], [28]. A more robust approach combining multi-scale and cross-attention is needed for accurate, real-world glucose forecasting."}, {"title": "III. PROPOSED METHOD", "content": "In this section, we introduce our proposed framework for blood glucose prediction. An overview of the AttenGluco framework is shown in Fig. 1. The framework comprises three main components: (a) a sensing module that gathers physiological and behavioral signals from wearable sensors, (b) a preprocessing module for time-series data preparation, and (c) a machine learning forecasting model utilizing the Transformer architecture for blood glucose prediction.\nOur transformer-based model predicts blood glucose levels (BGL) in individuals with type 2 diabetes by incorporating CGM data alongside activity information. The attention mechanism within the Transformer facilitates the effective integration of multi-time series signals recorded at different sampling rates. Additionally, it is well-suited for predicting highly fluctuating signals such as BGLs. To validate the effectiveness of our proposed model, we conduct experiments using the publicly available AI-READI (Flagship) dataset. The following sections provide a detailed explanation of the forecasting problem and key components of AttenGluco.\n\\textbf{A. Forecasting Problem}\nThe problem of blood glucose forecasting with multimodal input data can be formulated as a time series prediction task. Let $X = [X_1, X_2, ..., x_k]$ represent a set of $k$ sensor-derived measurements in the sensing data component. The observation from the $i$th sensor is denoted as $x_i = [x_{i,1},..., X_{i,t}]$, where $t$ is the sampling duration. Our proposed framework, AttenGluco, leverages CGM data ($x_g$) and activity data such as walking steps ($x_{ws}$) and walking time intervals ($x_{wi}$), which represent the duration between consecutive walking events. The multi-step forecasting output is expressed as $\\hat{X}_g = [x_{g,t+1},...,x_{g,t+m}]^T$, where $m$ represents the number of predicted time steps, commonly referred to as the prediction horizon (PH). Mathematically, the forecasting task can be formulated as $\\hat{g} = f(X; \\Theta)$, where $f$ represents the forecasting model, parameterized by $\\Theta$, which is learned during the training process.\n\\textbf{B. AttenGluco}\nAttenGluco is composed of two primary stages. The first stage, data preparation, focuses on collecting and processing physiological and behavioral data to serve as input for the forecasting model. This stage also includes data interpolation to handle missing values and normalization for consistency. During this phase, BGLs are recorded using a CGM device, while additional behavioral metrics, such as physical activity, are gathered from wearable sensors such as smartwatches, as depicted in Fig. 1. The second stage is the multimodal forecasting model, which utilizes these preprocessed inputs for blood glucose prediction.\nThe forecasting model is developed based on the Transformer architecture. This architecture leverages an attention mechanism to extract time-dependent patterns from fused irregular time-series data while also capturing long-term dependencies. This approach enables the model to effectively process complex temporal relationships. The structure of our proposed Transformer-based forecasting model is shown in Fig. 2.\nThe standard transformer architecture is typically made up of an encoder-decoder for data reconstruction. However, we modified this design for our forecasting model and framed it as a supervised learning task. Specifically, we eliminated the decoder and only utilized the encoder for data representation learning. Our customized transformer architecture incorporates two attention mechanisms: cross-attention and multi-scale attention. The cross-attention mechanism integrates various time series data with variant sample rates, while the multi-scale attention captures temporal dependencies within the signals to reduce the effect of random noise [31]. By incorporating these attention mechanisms, our Transformer-based approach enhances the accuracy of BGL forecasting.\nOur Transformer architecture consists of embedding and positional encoding layers, followed by cross-attention, feed-forward, Add & Norm layers, and a multi-scale attention block. The input variables $x_g$, $X_{ws}$, and $X_{wi}$ are initially processed through an embedding layer $f_{embed}(\\cdot)$, then passed through a positional encoding function $f_{pos}(\\cdot)$, producing the transformed representations $X_G$, $X_{ws}$, and $X_{wI}$, respectively. Each resulting matrix resides in $\\mathbb{R}^{t \\times d_{model}}$, where $t$ represents the sampling duration and $d_{model}$ is a hyperparameter. The multi-head attention mechanism in Transformer architectures [11] functions by scaling values $(V \\in \\mathbb{R}^{t \\times d_{model}})$ based on the relationships between keys $(K \\in \\mathbb{R}^{t \\times d_{model}})$ and queries $(Q \\in \\mathbb{R}^{t \\times d_{model}})$. The mathematical formulation of the attention mechanism is presented in Eq. 1.\n$\\text{Attention}(Q, K, V) = \\text{Softmax}(\\frac{Q K^T}{\\sqrt{d_{model}}})V$ (1)\nWe designed a two-branch cross-attention layer, where both branches receive $X_G$ as the query. In one branch, the keys and values correspond to $X_{ws}$, while in the other, they correspond to $X_{wI}$. The cross-attention (CA) of the first branch is computed using Eqs. 2 and 3.\n$CA(X_G, X_{ws}, X_{ws}) = [H_1,\u2026\u2026, H_{mH}]W^{CA}$ (2)\n$H_h = \\text{Attention}(X_G W_Q^A, X_{ws}W_K^A, X_{ws}W_V^A)$ (3)\nWhere $W_Q^A$, $W_K^A$, and $W_V^A$ are weight matrices specific to the attention head and belong to $\\mathbb{R}^{d_{model} \\times d_{model}}$. Moreover, $W^{CA} \\in \\mathbb{R}^{(mH \\cdot d_{model}) \\times d_{model}}$ is the final weight matrix that projects the concatenated attention head outputs into the original model dimension. The attention mechanism for the second branch follows the same computation, with the $X_{WI}$ as both the key and the query.\nThen, the attention outputs from both branches are combined to incorporate cross-attention information. The resulting data is passed through a linear feedforward network followed by an Add & Norm module. The processed output, $X_{CA}$, is then fed into a multi-scale attention mechanism comprising three multi-head attention branches, each designed for different downsampling (DS) rates. These branches apply downsampling factors of 1, 2, and 4, where a factor of 1 indicates no downsampling, as illustrated in Fig. 2.\nFor the first branch, the multi-scale attention mechanism MA on $X_{CA}$ is computed by using Eqs. 4 and 5.\n$MA(X_{CA}, X_{CA}, X_{CA}) = [H_1,..., H_{mH}]W^{MA}$ (4)\n$H_h = \\text{Attention}(X_{CA} W_Q^{MA}, X_{CA} W_K^{MA}, X_{CA} W_V^{MA})$ (5)\nEach attention branch utilizes query, key, and value weight matrices, $W_Q^{MA}$, $W_K^{MA}$, and $W_V^{MA}$, all belonging to $\\mathbb{R}^{d_{model} \\times d_{model}}$. The outputs from all attention heads are concatenated and projected back into the original model dimension using the final weight matrix $W^{MA} \\in \\mathbb{R}^{(mH \\cdot d_{model}) \\times d_{model}}$. The remaining two branches follow the same computational process but operate on downsampled input data. This approach improves the model's capability to capture both fine-grained details and long-term temporal dependencies within the input signals.\nThe outputs from the three multi-scale attention branches are summed and passed through a feed forward network, an Add & Norm block, and a fully connected layer. This final configuration generates $m$ predicted CGM values. Each prediction corresponds to a measurement taken every 5 minutes, meaning that $m$ samples collectively provide forecasts for $m \\times 5$ minutes into the future. In summary, Algorithm 1 describes the data processing pipeline in AttenGluco."}, {"title": "IV. RESULTS & DISCUSSION", "content": "In this section, we first introduce the AI-READI dataset used to train AttenGluco. We then compare its performance against a baseline model consisting if a 1D-CNN and LSTM for blood glucose forecasting to highlight the significance of our model for providing accurate forecasting. The baseline model, a multimodal LSTM, is commonly employed in state-of-the-art blood glucose prediction. The comparison is conducted using error metrics, including Root Mean Square Error (RMSE) and Mean Absolute Error (MAE), as well as correlation analysis. We investigate various training and testing scenarios to comprehensively evaluate the performance of AttenGluco.\n\\textbf{A. Dataset Description}\nThe dataset used in this study is the publicly available AI-READI Flagship Dataset. This dataset is designed to advance AI and machine learning research on Type 2 Diabetes Mellitus (T2DM). Collected from 1,067 participants across three U.S. sites. It includes individuals with and without T2DM, balanced across sex, race, and diabetes severity. The dataset consists of four categories: healthy individuals, individuals with prediabetes, individuals with T2DM on oral medication, and individuals with T2DM on insulin.\nA key feature of the dataset is its multi-modal structure, where participants were monitored over ten days using a Dexcom G6 CGM for real-time blood glucose, a Garmin Vivosmart 5 for physical activity and heart rate variability, and a LeeLab Anura sensor for environmental factors such as air quality and temperature. The dataset also includes survey data, clinical assessments, and retinal imaging. Daily step counts are recorded via an accelerometer, with occasional gaps due to device recharging. The heart rate sensor also computed a stress index (0-100) based on heart rate variability.\nFor this study, CGM data and walking activity (steps and intervals) are extracted as key features. After filtering out subjects with missing data, 896 participants are included in the final analysis, distributed as follows: 323 healthy individuals, 207 pre T2DM, 258 with T2DM on oral medication, and 108 with T2DM on insulin.\n\\textbf{B. Experimental Setup}\nThe baseline model follows a 1D-CNN architecture coupled with an LSTM. The 1D-CNN consists of two convolutional layers with 64 and 128 filters, each using a kernel size of 3. This is followed by a two-layer LSTM with 128 and 64 output features. The LSTM output is then passed through an MLP composed of three fully connected layers.\nBoth AttenGluco and the baseline model receive a sliding window of historical data covering 6.66 hours (400 minutes) as input. Training is conducted for 300 epochs with a learning rate of 0.001, optimizing the Mean Squared Error (MSE) using the Adam optimizer. Forecasting performance is assessed across three prediction horizons (PHs): 5 minutes, 30 minutes, and 60 minutes. To ensure consistency, each model undergoes five independent training runs. Model performance is assessed across all subjects, with comparisons based on RMSE [32], MAE [32], and Correlation [33].\nAs mentioned in section IV-A, the AI-READI dataset categorizes subjects into four cohorts (healthy, pre-T2DM, oral, and insulin) based on diabetes severity. To evaluate AttenGluco's performance across these cohorts, we conducted three distinct experiments under different scenarios (subject training, cohort-wise fine-tuning, and forgetting analysis) and compared the results with the baseline model. The details of each scenario will be discussed in the following sections.\n1) Isolated Subject Training: In this scenario, the CGM and activity data of AI-READI participants are first grouped according to their respective cohorts. The proposed model is then applied to each subject individually, with 85% of their data used for training and the remaining 15% reserved for testing. After evaluating one subject, the model is reinitialized before being trained and tested on the next. \n2) Cohort-Wise Fine-Tuning: In the cohort-wise fine-tuning scenario, the model is trained progressively within each participant category, unlike the isolated subject scenario where it is reset for each subject. Here, the model is first trained on one subject and then fine-tuned sequentially across the other subjects in the same category, with each subject serving as both training and testing data. This process continues until all subjects in a category have been used. Once a category is completed, the model is reinitialized before moving on to the next cohort. The average performance metrics for each category are presented in Table II. This approach enables the model to gradually adapt to variations within each cohort; therefore, it achieves better performance than the previous scenario.\n3) Continual Learning and Forgetting Analysis: Even though transferring the model to and fine-tuning it on new subjects enhances the model's performance on new data, it simultaneously leads to the loss of previously learned knowledge. This phenomenon, known as catastrophic forgetting [34], is a well-known issue that happens with model retraining. The problem becomes more pronounced when there is a significant distribution shift between the old and new data, which causes the model to prioritize recent patterns while disregarding past ones.\nWe hypothesize that a distribution shift exists among the four cohorts, which potentially causes the model to forget"}, {"title": "V. CONCLUSION", "content": "In this study, we proposed AttenGluco, a multimodal Transformer-based framework for long-term blood glucose forecasting using CGM and activity data. By integrating cross-attention and multi-scale attention, our model effectively fuses heterogeneous time-series data and captures long-term dependencies. Our evaluation on the AI-READI dataset demonstrated that AttenGluco outperforms baseline models under different test and train scenarios across various subject cohorts. AttenGluco improved RMSE by about 10% in the isolated subject training scenario. In the cohort-wise fine-tuning scenario, RMSE improvements are even more pronounced, with reductions of about 12%. Additionally, AttenGluco achieved higher correlation scores across all groups, further validating its enhanced predictive capability. Our analysis of forecasting accuracy at different prediction horizons (5, 30, and 60 minutes) shows that AttenGluco consistently outperformed the baseline model, with the most notable gains observed at longer horizons, where it reduced RMSE by up to 3.18 compared to the baseline. Furthermore, our forgetting analysis revealed that AttenGluco maintains lower error rates when fine-tuned on new cohorts. By improving long-term blood glucose forecasting, AttenGluco has the potential to advance precision medicine for diabetes care, enabling more proactive and individualized interventions to maintain optimal glucose levels."}, {"title": "Algorithm 1 AttenGluco model", "content": "Input: Preprocessed and normalized data, including CGM signal (xg), walking steps (xws), and walking time intervals (xwi),\nCross-attention block (CA), Multi-scale atention block (MA),\nEmbedding function (fembed), Positional encoding function (fpos),\nTwo Add & Norm block ($f_{add}$),$f_{norm}$), Two Feedforward model\n($f_{FF}$), Linear model ($f_{lin}$)\nOutput: Predicted BGL $\\hat{g}$\nBegin\n$\t[X_{G}, X_{WS}, X_{WI}] \\leftarrow f_{pos}(f_{embed}([X_{g}, X_{ws}, X_{wi}]))$\n$\tX_{CA1} \\leftarrow CA(X_{G}, X_{WS}, X_{WS})$\n$\tX_{CA2} \\leftarrow CA(X_{G}, X_{WI}, X_{WI})$\n$\tX_{CA} \\leftarrow f^{(1)}_{add}(f_{norm}(X_{CA1} + X_{CA2}))$\n$\t[X^{2}_{CA}, X^{4}_{CA}] \\leftarrow Downsample(X_{CA}, 2), Downsample(X_{CA}, 4)$\n$\tX_{MA1} \\leftarrow MA(X_{CA}, X_{CA}, X_{CA})$\n$\tX_{MA2} \\leftarrow Upsample(MA(X^{2}_{CA}, X^{2}_{CA}, X^{2}_{CA}), 2)$\n$\tX_{MA3} \\leftarrow Upsample(MA(X^{4}_{CA}, X^{4}_{CA}, X^{4}_{CA}), 4)$\n$\tX_{MA} \\leftarrow f^{(2)}_{add}(f_{norm}(X_{MA1} + X_{MA2} + X_{MA3}))$\n$\t\\hat{g} \\leftarrow f_{lin}(X_{MA})$\n$\treturn \\hat{g}$\nEnd"}]}