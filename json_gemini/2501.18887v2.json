{"title": "BUILDING BRIDGES, NOT WALLS: ADVANCING INTERPRETABILITY BY UNIFYING FEATURE, DATA, AND MODEL COMPONENT ATTRIBUTION", "authors": ["Shichang Zhang", "Tessa Han", "Usha Bhalla", "Himabindu Lakkaraju"], "abstract": "The increasing complexity of AI systems has made understanding their behavior a critical challenge. Numerous methods have been developed to attribute model behavior to three key aspects: input features, training data, and internal model components. However, these attribution methods are studied and applied rather independently, resulting in a fragmented landscape of approaches and terminology. This position paper argues that feature, data, and component attribution methods share fundamental similarities, and bridging them can benefit interpretability research. We conduct a detailed analysis of successful methods of these three attribution aspects and present a unified view to demonstrate that these seemingly distinct methods employ similar approaches, such as perturbations, gradients, and linear approximations, differing primarily in their perspectives rather than core techniques. Our unified perspective enhances understanding of existing attribution methods, identifies shared concepts and challenges, makes this field more accessible to newcomers, and highlights new directions not only for attribution and interpretability but also for broader AI research, including model editing, steering, and regulation.", "sections": [{"title": "1 Introduction", "content": "As AI systems grow increasingly complex, understanding their behavior remains a critical challenge [Arrieta et al., 2020, Longo et al., 2024]. Researchers have developed methods to explain AI systems by attributing their behavior to three distinct aspects: input features, training data, and internal model components. Feature attribution methods identify influence of input features at test time, revealing which aspects of the input drive the model's output [Zeiler and Fergus, 2014, Ribeiro et al., 2016, Horel and Giesecke, 2020, 2022, Lundberg and Lee, 2017, Smilkov et al., 2017]. Data attribution analyzes how training data shape model behavior during the training phase [Koh and Liang, 2017, Ghorbani and Zou, 2019, Ilyas et al., 2022]. Component attribution examines the internal workings of the model by analyzing how specific components, such as neurons or layers in a neural network (NN), affect model behavior [Vig et al., 2020, Meng et al., 2022, Nanda, 2023, Shah et al., 2024]. While numerous attribution methods have been developed for each of these three aspects, and some survey papers have been published [Guidotti et al., 2018, Covert et al., 2021, Wang et al., 2024, Hammoudeh and Lowd, 2024, Bereska and Gavves, 2024], they have been studied and used rather independently by different communities, creating a fragmented landscape of methods and terminology for similar ideas [Saphra and Wiegreffe, 2024].\nOur position is that feature, data, and component attribution methods can be bridged to advance not only interpretability research, by stimulating cross-aspect knowledge transfer, but also broader AI research, including model editing, steering, and regulation. We show that these three types of attribution employ common methods and they differ primarily in perspective rather than core techniques. In the following sections, we first formalize a unified attribution problem that encompasses all three aspects to show these seemingly distinct approaches fall under the same"}, {"title": "2 The Attribution Problem", "content": "Researchers have developed various attribution methods to analyze model behavior from different perspectives. In this section, we formally introduce three types of attribution problems and show how they fall under a unified framework.\nConsider a learning problem with d-dimensional input features x = [x1,x2,...,xd]. During training, a dataset of n data points: Dtrain = {x(1), x(2),...,x(n)} is used to train a model fo with parameters @ and components c = {c1, c2,..., cm } by optimizing the loss function L(0). At test (inference) time, the model generates an output fo (xtest) for a new input xtest. For notational simplicity, we omit @ and \u201ctest\u201d and use f and x when the context is unambiguous. A notation summary is provided in Appendix A. The core objective of all three problems is to attribute the model's output f(x) to different elements and quantify their influence with attribution scores.\nFeature attribution quantifies how input features influence model outputs. These features may represent pixels in images, tokens in text, or other domain-specific units. We denote the attribution score of feature xi as \u03c6i(x).\nData attribution analyzes how training data shape model behavior. We quantify the influence of each training point x(i) \u2208 Dtrain through its attribution score \u03c8i(x).\nComponent attribution studies the role of model components in generating outputs. The components can have various definitions, such as neurons or layers in a NN. We denote the attribution score of component ck as \u03b3k(x).\nAs illustrated in Figure 1, these three attribution problems share a fundamental connection: they all seek an attribution function g that assigns scores to specific elements (features xi, training points x(i), or components ck) for a given test output f(x), differing only in the choice of elements."}, {"title": "3 Understanding Feature Attributions", "content": "Feature attribution quantifies how individual features xi of an input x influence a model's output f(x) through at-tribution scores \u03c6i(x). Applied to model inference at test time, it explains model behavior without altering model parameters. The attribution results can be used to perform feature selection, identify spurious correlations, and jus-tify model predictions to gain user trust. Feature attribution methods can be broadly classified into three categories: perturbation-based methods, gradient-based methods, and linear approximation methods. We discuss some prominent methods in each category below and provide more details in Appendix C."}, {"title": "3.1 Perturbation-Based Feature Attribution", "content": "Perturbation-based methods attribute feature importance by measuring how model outputs change when input features are modified and especially removed. They are also referred to as removal-based methods [Covert et al., 2021].\nDirect Perturbation represents a straightforward application of perturbation analysis. The pioneering Occlusion method [Zeiler and Fergus, 2014] in computer vision replaces image pixels with grey squares and measures changes in the model's prediction. The method assumes that occluding crucial pixels will significantly impact the output. For images, pixel attribution scores create a saliency map highlighting the most influential regions. RISE [Petsiuk, 2018] advanced this approach by perturbing multiple image regions and combining their attribution results. The final attribution score weighs each attribution result by the model's predicted probability for that perturbed image.\nGame-Theoretic Perturbation While intuitive, direct perturbation fails to capture synergistic interactions between multiple features. Cooperative game theory addresses this limitation by modeling features as players collaborating toward the model's output. The Shapley value [Shapley, 1953] provides a foundational solution within this framework and has inspired numerous feature attribution methods [Sundararajan and Najmi, 2020]. Computing Shapley value attributions involves measuring a specific type of perturbation: how adding a feature xi to different feature subsets changes the model's output compared to the subset alone, known as the marginal contribution of xi to the subset. The final attribution score captures feature interactions by aggregating these marginal contributions across all possible feature subsets. Although theoretically sound, Shapley value methods face computational challenges as their complexity grows exponentially with feature dimensionality. To overcome this challenge, various approximation methods have been proposed, and Kernel SHAP (or simply SHAP) introduced by Lundberg and Lee [2017] has gained widespread adoption because of its efficient kernel-based approximation.\nPerturbation Mask Learning is based on the idea that perturbation of including or excluding features can be viewed as applying a binary mask for each feature. Mask learning methods advance this idea by using learnable masks representing feature inclusion probabilities, which offer more nuanced control compared to binary masks. Dabkowski and Gal [2017] pioneered this approach for image classification by introducing a masking model that generates pixel masks, aiming to identify a minimal set of features that sufficiently maintain the prediction of the original input. The masking model acts as the attribution function g, where mask values represent feature attribution scores. While initial training is required, the masking model generates masks through a single forward pass at test time, which significantly"}, {"title": "3.2 Gradient-Based Feature Attribution", "content": "Gradients have emerged as a powerful tool for feature attribution. Gradients of model outputs f(x) with respect to input features x, \u2207x f(x), quantify output sensitivity to small input changes [Erhan et al., 2009, Baehrens et al., 2010], measuring feature influence without requiring perturbations. Gradient-based attribution has superior computational efficiency compared to perturbation-based methods. While the latter requires O(d) model evaluations for d features, gradient-based approaches need only a single or a few forward and backward pass(es) to compute \u2207x f(x).\nGradient-based feature attribution emerged from computer vision, where it gained widespread adoption for gener-ating attribution scores as image saliency maps [Simonyan et al., 2013], also known as sensitivity maps [Smilkov et al., 2017]. The \"vanilla gradients\u201d method uses the gradients of the output class (log)probability with respect to input pixels as attribution scores [Simonyan et al., 2013]. Since then, researchers have proposed numerous enhanced gradient-based methods. For example, Gradients \u00d7 Input [Shrikumar et al., 2017] multiplies gradients with input val-ues, Integrated Gradients [Sundararajan et al., 2017] accumulates gradients along a path from a baseline to the actual input, and Integrated Hessians [Janizek et al., 2021] further extends the analysis to feature interactions by computing the Hessian matrix. These methods leverage different gradient formulations to provide more accurate and stable at-tribution scores. A notable advancement is SmoothGrad [Smilkov et al., 2017], which generates multiple copies of the input with added Gaussian noise and computes sensitivity maps for each noisy sample. By averaging these maps, SmoothGrad reduces noise while preserving salient features that consistently influence model outputs."}, {"title": "3.3 Linear Approximation for Feature Attribution", "content": "Linear approximation methods offer an alternative approach to feature attribution by fitting a simple linear surrogate model around the input of interest. These methods approximate the complex behavior of f near a specific input x using a linear model, normally in the form of g(x) = wTx + b with coefficients w and bias b. Then the coefficient wi directly provides a feature attribution score of feature xi.\nLIME [Ribeiro et al., 2016] exemplifies this approach. It samples instances around the input of interest, obtains model predictions for these samples, and fits a sparse linear model to capture the local model behavior. An innovation of LIME is its use of binary indicators (0 or 1) rather than actual feature values as inputs to the linear model, only representing feature inclusion or exclusion. The resulting linear model coefficients directly explain how each feature's presence influences the approximated model's output. Later, a variant of LIME called C-LIME improves attribution robustness through its unique neighbor sampling approach for continuous features [Agarwal et al., 2021].\nNotably, LIME can also be viewed through a perturbation lens, as it fundamentally perturbs input features to approxi-mate the model's output. This connection points to a unification: many feature attribution methods can be understood within a common mathematical framework of local function approximation, which we explore next."}, {"title": "3.4 Unifying Feature Attributions via Local Function Approximation", "content": "While the original algorithms of feature attribution methods discussed above can be viewed in their respective three categories, many of them can be unified under a common local function approximation framework [Han et al., 2022]. Within this framework, a model f is approximated around a point of interest x in a local neighborhood distribution Z by an interpretable model g using a loss function l. Han et al. [2022] show that eight prominent feature attribution methods (Occlusion, KernelSHAP, Vanilla Gradients, Gradients \u00d7 Input, Integrated Gradients, SmoothGrad, LIME, and C-LIME) can be viewed as specific instances of this framework, distinguished only by their unique choices of local neighborhoods Z and loss functions l (Appendix Table 3).\nThe local function approximation framework [Han et al., 2022] enhances our understanding of feature attribution meth-ods in several important ways. First, it provides conceptual coherence to the field. While different methods appear to have distinct motivations, this framework reveals their shared fundamental goal of local function approximation. Second, placing diverse methods under a single framework enables direct comparisons among them. This comparative lens allows us to better understand their similarities, differences, and behavior, such as why different methods some-times generate disagreeing or even contradictory explanations for the same model prediction [Krishna* et al., 2024]. Third, this unification enables theoretical simplicity. Instead of studying methods seperately, theoretical analyses can be performed using the framework and applied to each method, as shown by the no free lunch theorem and guiding principle in Han et al. [2022]. Fourth, the conceptual understanding brought about by unification leads to principled, practical recommendations [Han et al., 2022]. Additional details on this unification are provided in Appendix C.6."}, {"title": "4 Understanding Data Attributions", "content": "Data attribution studies how the training dataset Dtrain shapes model behavior. These methods are also known as data valuation, as they help assess the value of data from vendors and content creators. For each training example x(i), an attribution score \u03c8i(x) traces back to the training phase to quantify its influence on the model's output f(x) for a test point x. These scores characterize training data properties, help identify mislabeled data, and justify training data values. Like feature attribution, data attribution methods can be organized into three categories: perturbation-based methods, gradient-based methods, and linear approximation methods. We examine prominent methods from each category below, with additional details in Appendix D."}, {"title": "4.1 Perturbation-Based Data Attribution", "content": "Perturbation-based data attribution observes the model behavior changes after removing or re-weighting the training data points and subsequently retraining the model, so these methods are also referred to as retraining-based meth-ods [Hammoudeh and Lowd, 2024].\nLeave-One-Out (LOO) Attribution is a prominent example of this approach, analogous to direct perturbation in feature attribution. The method trains a model on the complete dataset and then separately removes each individual data point and retrains the model. The attribution score for each removed point is determined by the difference in performance between the original and retrained models. The LOO approach has a long history in statistics [Cook and Weisberg, 1982] and has proven valuable for modern AI model data attribution [Jia et al., 2021]. It provides valuable counterfactual insights with its main limitation being computational cost, as it requires retraining the model for each data point. Many newer attribution methods can be viewed as efficient approximations of LOO. A natural extension of LOO is to leave a set of data points out to evaluate their collective impact through retraining [Ilyas et al., 2022].\nGame-Theoretic Data Attribution represents the successful application of game theory to quantify training sample influence similar to feature attribution. As a direct perturbation method, LOO attribution overlooks interactions be-tween data points, potentially missing subtle influence behaviors [Lin et al., 2022, Jia et al., 2021]. Game-theoretic data attribution methods address this by treating training data points as players in a cooperative game, aiming to fairly distribute the model's performance among training samples. Data Shapley [Ghorbani and Zou, 2019] first applied Shapley values to data attribution by computing each training point's aggregated marginal contribution across all pos-sible training data subsets. Although theoretically sound, game-theoretic methods face prohibitive computational costs for large datasets, as each marginal contribution requires model retraining and there are 2n possible subsets. Various approximation methods have been proposed to address this challenge, which we discuss in Appendix D.2."}, {"title": "4.2 Gradient-Based Data Attribution", "content": "Gradient-based data attribution methods leverage the gradients of the loss with respect to training data \u2207\u03b8L(f\u03b8(x(i))) and test data \u2207\u03b8L(f\u03b8(x)) to assess the impact of training points x(i) on model output f(x). As in Charpiat et al. [2019], simple dot product (GradDot) and cosine similarity (GradCos) between these two gradients are used as simi-larity measures and consequently attribution scores \u03c8j(x). Like feature attribution, gradient-based methods often offer greater computational efficiency than perturbation-based methods since they typically require no retraining.\nInfluence Function (IF), a classic statistical technique originally developed for analyzing influential points in linear regression [Cook and Weisberg, 1980], has been adapted for modern AI models [Koh and Liang, 2017]. IF approx-imates LOO model parameter changes by Taylor expansion, avoiding explicit retraining. This approximation builds on computing both the gradient and the (inverse) Hessian of the loss with respect to model parameters. IF offers an effective and computationally feasible alternative to LOO, but it also faces several challenges. Its convexity as-sumptions often do not hold for modern AI models, and its Hessian computation remains expensive for large models. Many methods have been proposed to address these limitations; we discuss IF and these enhancements in detail in Appendix D.3.\nTracing (Training) Path While many gradient-based methods follow IF to compute gradients at the final model parameters, TracIn [Pruthi et al., 2020] introduces a novel approach that traces the influence of training instances throughout the entire training process. The method attributes influence by computing dot products between training and test data gradients at each training step from the initial model parameters to the final model parameters at the end of training, accumulating these to capture a training point's total influence across the training path. This path tracing approach provides valuable insights into training dynamics while avoiding limitations of LOO and IF, such as assigning identical attribution scores to duplicate training data points. TracIn also offers greater flexibility than IF by eliminating the convexity assumption and Hessian matrix computations. On the other hand, its tracing requires storing intermediate model checkpoints during training, increasing both memory usage and computational costs."}, {"title": "4.3 Linear Approximation for Data Attribution", "content": "Datamodel [Ilyas et al., 2022] applies linear approximation to data attribution, similar to LIME in feature attribution. It constructs a linear model g with n coefficients and {0, 1}n vectors as inputs, where each input represents a subset of training data. g is learned to map any counterfactual subset of training data to output f(x), where f is trained on this subset with the given model architecture and training algorithm. The coefficients of g thus represent the attribution scores of the training data points. The method's counterfactual nature enables evaluation of other attribution methods via the Linear Datamodeling Score (LDS), which compares their attribution score rankings to Datamodel's ranking. While Datamodel can effectively capture model behavior, constructing this large linear model requires extensive coun-terfactual data obtained by training model f on various subsets, making it computationally intensive. TRAK [Park et al., 2023] addresses these computational challenges by estimating Datamodels in a transformed space where the learning problem becomes convex and can be approximated efficiently. It further improves efficiency through random projection of model parameters and ensemble attribution results of multiple trained models. Though the ensemble approach still requires some model retraining on different subsets, it achieves high estimation accuracy with signifi-cantly fewer retraining iterations than Datamodel. Furthermore, both approaches can be viewed as perturbation-based methods, similar to LIME, as they systematically vary training data to construct linear models."}, {"title": "5 Understanding Component Attributions", "content": "Component attribution, an emerging approach within mechanistic interpretability, seeks to understand AI models by reverse engineering their internal mechanisms into interpretable algorithms. Operating primarily at test time for model inference, it quantifies how each model component ck contributes to a model output f(x) through an attribution score \u03b3k(x). Components can be defined flexibly across different scales - from individual neurons and attention heads to entire layers and circuits (subnetworks). By identifying components responsible for specific behaviors, this approach enables deeper model understanding and targeted model editing. Like feature and data attribution, component attri-bution methods fall into three categories: perturbation-based, gradient-based, and linear approximation approaches. Below we examine key methods from each category, with additional details provided in Appendix E."}, {"title": "5.1 Perturbation-Based Methods", "content": "In component attribution, perturbation-based methods are fundamentally quite similar to perturbation-based methods in feature and data attribution. Components of the model, whether neurons, circuits, or layers, are similarly perturbed to measure their effect on model behavior. Generally, the perturbations are chosen carefully to attempt to localize behaviors related to specific tasks or concepts.\nCausal Mediation Analysis [Pearl, 2022, Vig et al., 2020] is based on the abstraction of models to causal graphs. These graphs consist of nodes, which can be components such as neurons, circuits, attention heads, or layers, and directed edges that represent the causal relationships between nodes. Causal mediation analysis is defined by an input cause x and an output effect f(x) that is mediated by intermediate causal nodes between x and f(x). By perturbing these intermediate components, ck, changes in f(x) can be measured to get attribution scores \u03b3k(x). These indirect effects are often measured counterfactually in order to calculate each component's contribution towards a particular behavior, such as a correct factual prediction. To do so, the activations of all intermediate components ck are measured during three separate runs: a clean run with no perturbations, a corrupted run where intermediate activations are perturbed, and a corrupted-with-restoration run that measures whether a single component can restore the prediction. The corrupted run can be repeated multiple times with different random noise added to obtain a more robust attribution score. This analysis is frequently referred to as causal tracing [Meng et al., 2022] or activation patching, and also path patching [Wang et al., 2022] when patching is applied to paths connecting components. By comparing the outputs of the clean and corrupted run, or by looking at the corrupted-with-restoration run, one is able to find the specific mediator components that are either sufficient or necessary for the desired behavior. By changing the dataset, metric, and causal mediators, we can model the relationship between each component and various model behaviors.\nGame-Theoretic Component Attribution [Ghorbani and Zou, 2020] follows a similar approach to game-theoretic methods in feature and data attribution to quantify the contributions of each neuron to the model's performance. These methods take into account the interactions between neurons by modeling neurons as players in a cooperative game to fairly distribute contributions. In particular, Neuron Shapley [Ghorbani and Zou, 2020] extends prior works on Shapley values to component attribution, ensuring computational feasibility through sampling-based approximations and a multi-armed bandit algorithm that efficiently identifies neurons with large attribution scores.\nMask Learning and Subnetwork Probing [Csord\u00e1s et al., 2020, Cao et al., 2021] adopts a similar concept to feature attribution, attempting to approximate either the model's or a probe's performance on a given task by searching for"}, {"title": "5.2 Gradient-Based Component Attribution", "content": "To further decrease the computational complexity of component attribution methods, researchers have developed al-terations of causal tracing that leverage gradient-based approximations requiring only two forward passes and a single backward pass to generate attributions. Attribution patching [Nanda, 2023] is the simplest gradient-based approxima-tion of causal tracing. Intuitively, attribution patching leverages a linear approximation of the model for the corrupted prompts and measures the local change when patching a single activation from the corrupted to clean input. This is achieved by computing the backward pass for the corrupted output with respect to the patching metric and storing the gradients with respect to the activations. Note that for feature and data attributions, gradients are taken with respect to the input features or training data, not the model activations. Finally, the method takes the difference between the clean and corrupted activations and multiplies it by the cached gradients to obtain attribution scores."}, {"title": "5.3 Linear Approximation for Component Attribution", "content": "Given the rapid increase in model size and the combinatorial nature of searching for effective components, component attribution also employs linear approximations like LIME and Datamodels. COAR [Shah et al., 2024] attempts to decompose model behavior in terms of various model components by predicting the counterfactual impact of ablating each component, similar to many forms of causal mediation analysis. Given the computational complexity of this problem, they employ linear approximations by assigning scores to each component of a model and estimating the counterfactual effect of removing sets of components by simply summing their corresponding scores. Thus, the complexity of relationships between components is abstracted away through the linear approximation."}, {"title": "6 Position and Contributions", "content": "Feature, data, and component attribution methods have largely been studied as separate problems, resulting in the parallel development of similar methods from different communities with distinct terminologies. We argue that these methods can be unified into a holistic view. Having demonstrated their methodological similarities across three types of attribution, we now summarize their common concepts and challenges and identify promising research directions through cross-aspect knowledge transfer. We believe that our unified view will bridge the current fragmented land-scape, make the field more accessible to newcomers, and help advance research in interpretability and beyond."}, {"title": "6.1 Common Concepts of Attribution Methods", "content": "As we discussed in the previous sections, attribution methods across features, data, and components can be catego-rized into: perturbation-based, gradient-based, and linear approximation. We provide a detailed discussion of these categories in Appendix B with Table 1 summarizing all the methods we discussed. Beyond algorithmic similarities, conceptual ideas also transfer across aspects of attribution. One example is the deliberate introduction of randomness and smoothing to enhance attribution robustness. This idea has proven effective across three attribution types through random noisy samples used by SmoothGrad for feature attribution, attribution results ensembled over multiple retrain-ings by TRAK for data attribution, and aggregated results from multiple corrupted runs in causal mediation analysis for component attribution. Another example is tracking and aggregating results along paths, as Integrated Gradients along paths from base input to target input in feature attribution, TracIn tracing training paths to reveal dynamic data influences in data attribution, and path patching tracking component effects along residual stream paths in component attribution. These shared concepts highlight the fundamental connections of attribution methods."}, {"title": "6.2 Common Challenges of Attribution Methods", "content": "Attribution methods also face common challenges that impact their reliability and practical utility, which we briefly discuss below and extend in Appendix F.\nComputational Challenges present substantial barriers preventing attribution methods from being applied to large models. These challenges appear in all three types of attributions, rooted in their shared technical methods. For perturbation-based methods, the curse of dimensionality makes it intractable to comprehensively analyze high-dimensional inputs, large training datasets, and models with numerous components. Gradient-based methods offer"}, {"title": "6.3 Cross-Aspect Attribution Innovation", "content": "The connections among feature, data, and component attributions discussed in the sections above suggest multiple promising directions for future research. One research direction is to leverage insights from one type of attribution to develop methods for another. This can be directly identified as filling in the empty cells in Table 1. For example, while the Shapley value has been successfully applied across all three types of attribution, many other game-theoretic notions have only been used in feature attribution and not for data and component attribution. In addition, some advanced gradient techniques are common for feature and data attribution methods, but not for component attribution. The Hessian matrix, for example, has been used to obtain second-order information in Integrated Hessians for feature attribution and extensively in all IF-related methods for data attribution, and can be explored for component attribution.\nMoreover, seeing the theoretical connections among feature, data, and component attributions enables us to draw inspiration from one area to advance our understanding of another as a whole. For example, we demonstrated that diverse feature attribution methods all perform local function approximation (\u00a7 3.4). This framework can potentially also apply to data and component attributions. We know that feature attributions perform function approximation of the blackbox model's predictions over the space of input features. One may hypothesize that data attributions perform function approximation of the model's weights over the space of training data points and that component attributions perform function approximation of the model's predictions over the space of model components. If so, function approximation may unify data and component attribution methods as well. Such theoretical unification may provide plentiful benefits to data and component attribution, including conceptual coherence, elucidation of method properties, theoretical simplicity, and clearer practical recommendations.\nAnother research direction is to move towards more holistic analyses of model behavior. These attribution methods provide insight into model behavior through different lenses: input features, training data, and model components. Each type of attribution provides different and complementary information about model behavior. For example, for a given model prediction, feature attributions may not suggest that the model is relying on sensitive features to make predictions, but model component attributions may uncover a set of neurons that encode biased patterns. In this sense, focusing only on one type of attribution, i.e., studying only one part of the model, is insufficient to understand model behavior. Thus, future research may develop approaches to enable more comprehensive model understanding, such as understanding how to use different types of attribution methods together, the settings under which different attribution types may support or contradict one another, and the interactions between the three model parts (e.g., how patterns in the training data are encoded in model neurons)."}, {"title": "6.4 Connections to Other Areas of AI", "content": "Attribution methods also hold immense potential to benefit other AI areas. Especially with a unified view integrating feature, data, and component attribution, researchers can not only gain deeper insights of model behavior but also edit and steer models towards desired goals and improve model compliance with regulatory standards.\nModel Editing [De Cao et al., 2021, Mitchell et al., 2021, Meng et al., 2022, inter alia] focuses on precisely modifying models without retraining. It enables researchers to correct model mistakes, analogous to fixing bugs in software. This approach is particularly valuable for large language models (LLMs), which encode vast information in their parame-ters and are prohibitively expensive to retrain. It can be viewed as a downstream task of attribution methods. Once attribution methods locate an issue, editing methods can be applied to the problematic parts. While editing aligns most closely with component attribution, other attribution types serve essential complementary functions. Feature attribu-tion identifies spurious correlations requiring correction, and data attribution reveals problematic training samples that influence model behavior. The unified attribution framework provides a holistic perspective that enables more efficient and accurate editing, especially when component attribution alone proves insufficient [Hase et al., 2024].\nModel Steering [Zou et al., 2023, inter alia] differs from model editing by integrating a steering vector into the model's inference process rather than modifying model parameters. While editing focuses on specific knowledge modifications, steering guides model behavior at a higher level, such as enhancing truthfulness and harmlessness in LLMs. Similar to model editing, a unified attribution framework can significantly enhance steering by better localizing target components to steer and generating more effective steering vectors through relevant features and training data.\nModel Regulation [Oesterling et al., 2024, inter alia] is an emerging field examining the relationship between AI systems, policy, and societal outcomes. Regulation and policy frequently stress the need for transparency of AI sys-tems as well as users' right to an explanation. Attribution methods provide an avenue for practitioners to ensure that AI systems meet these legal and ethical requirements, by providing information about the overall AI system as well as specific input-output behavior. Feature attribution reveals input processing patterns, data attribution exposes training data influences, and component attribution illuminates architectural roles. This multi-faceted understanding enables more targeted and effective regulation. For example, when addressing biased behavior, feature attribution can be used to identify discriminatory input patterns, data attribution to trace problematic training samples or copyright infringements, and component attribution to locate architectural elements needing adjustment. These complemen-tary perspectives provide the comprehensive understanding needed to guide model regulation toward desired societal outcomes."}, {"title": "7 Alternative Views", "content": "While we propose a unified view of attribution methods, alternative views exist for characterizing them.\nOne alternative view suggests that feature, data, and component attribution represent fundamentally distinct research directions, each with its own terminology and research community. Despite using similar methods, these fields differ in their motivations and methodological approaches. The term \"attribution\" carries unique meanings in each case. Feature attribution has historically been synonymous with \u201cexplainable AI,\u201d data attribution was part of \"data-centric AI\" focusing on concrete applications like data selection, and component attribution has emerged recently as one branch of \"mechanistic interpretability\u201d to reverse-engineer algorithms in foundation models. This rather independent evolution has produced distinct best practices within each domain [Guidotti et al., 2018, Hammoudeh and Lowd, 2024, Bereska and Gavves, 2024] and separate communities that may seem to resist unification. Nevertheless, we argue that meaningful unification across these three aspects is possible and will help advance interpretability research.\nResearchers have also proposed various taxonomies within each type of attribution. Feature attribution methods, for example, were classified as model-agnostic or model-specific approaches [Arrieta et al., 2020]. Model-agnostic methods treat models as black boxes and generate attributions based solely on input-output relationships, while model-specific methods rely on known model architectures. Although these categorizations aid method selection, they have limited generalizability to other aspects of attribution. Component attribution methods, for instance, are inherently model-specific since they analyze defined model components. Our unification approach extends beyond attribution-specific frameworks to take a broader, cross-aspect perspective. This comprehensive view uncovers connections that remain obscured when examining methods in isolation. Consider gradient-based approaches: while methods from all three types of attribution employ gradients, they compute them for different purposes with different objectives. Thus, though aspect-specific categorizations offer valuable insights, our cross-aspect framework provides a holistic understanding of the attribution landscape."}, {"title": "8 Conclusion", "content": "In this paper, we have presented a unified view on three traditionally separate attribution methods for interpretability: feature attribution, data attribution, and component attribution. While these methods have evolved independently to explain different aspects of AI systems we demonstrate that they share fundamental technical building blocks such as perturbations, gradients, and linear approximations. This unified view not only bridges the fragmented landscape of attribution methods but also identifies common challenges and opportunities for cross-attribution knowledge transfer, ultimately working toward more comprehensive approaches to advancing AI interpretability research."}, {"title": "A Summary of Notations", "content": "In Table 2, we summarize the notation used in this paper."}, {"title": "B Summary of Methods", "content": "As we discussed in the main body", "approaches": "perturbation-based methods", "gradients": "computing gradients of model outputs with respect to input features to quantify feature importance, calculating gradients of loss functions with respect to spe-cific training data points to analyze data influence, or using gradients to approximate the effects of modifying model components. Linear approximation methods fit linear models to approximate complex model behaviors. The inputs to these linear models can be input features, training data points, or model components. In some cases, binary indicators replace the actual elements as inputs to simplify the approximation.\nIn total, thousands of attribution methods of all three types have been proposed making a comprehensive literature summary infeasible. In Table 1, we summarize"}]}