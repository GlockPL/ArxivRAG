{"title": "DELE: Deductive EL++ Embeddings for Knowledge Base Completion", "authors": ["Olga Mashkova", "Fernando Zhapa-Camacho", "Robert Hoehndorf"], "abstract": "Ontology embeddings map classes, relations, and individuals in ontologies into R\", and within R\" similarity between entities\ncan be computed or new axioms inferred. For ontologies in the Description Logic EL++, several embedding methods have\nbeen developed that explicitly generate models of an ontology. However, these methods suffer from some limitations; they do\nnot distinguish between statements that are unprovable and provably false, and therefore they may use entailed statements as\nnegatives. Furthermore, they do not utilize the deductive closure of an ontology to identify statements that are inferred but not\nasserted. We evaluated a set of embedding methods for EL++ ontologies, incorporating several modifications that aim to make\nuse of the ontology deductive closure. In particular, we designed novel negative losses that account both for the deductive closure\nand different types of negatives and formulated evaluation methods for knowledge base completion. We demonstrate that our\nembedding methods improve over the baseline ontology embedding in the task of knowledge base or ontology completion.", "sections": [{"title": "1. Introduction", "content": "Several methods have been developed to embed Description Logic theories or ontologies in vector spaces [10,\n11, 21, 29, 40, 42, 44, 55]. These embedding methods preserve some aspects of the semantics in the vector space,\nand may enable the computation of semantic similarity, inferring axioms that are entailed, and predicting axioms\nthat are not entailed but may be added to the theory. For the lightweight Description Logic EL++, several geometric\nembedding methods have been developed [21, 29, 40, 42, 55]. They can be proven to \u201cfaithfully\" approximate\na model in the sense that, if a certain optimization objective is reached (usually, a loss function reduced to 0),\nthe embedding method has constructed a model of the EL++theory. Geometric model construction enables the\nexecution of various tasks. These tasks include knowledge base completion and subsumption prediction via either"}, {"title": "2. Preliminaries", "content": null}, {"title": "2.1. Description Logic EL++", "content": "Let \u2211 = (C, R, I) be a signature with set C of concept names, R of role names, and I of individual names. Given\nA, B \u2208 C, r \u2208 R, and a, b \u2208 I, EL++concept descriptions are constructed with the grammar \u22a5 | \u315c | \u0410\u041f\u0412 | \u2203r.A |\n{a}. ABox axioms are of the form A(a) and r(a,b), TBox axioms are of the form A \u2286 B, and RBox axioms are\nof the form r\u2081 0 r20\u2026\u20260rn \u2286 r. EL++generalized concept inclusions (GCIs) and role inclusions (RIs) can be\nnormalized to follow one of these forms [3]: C D (GCI0), \u0421\u041f\u0414 \u0415 (GCI1), C \u2286 \u2203R.D (GCI2), \u2203R.C\u2286 D\n(GCI3), C\u22a5(GCI0-BOT), CID \u2286 \u22a5 (GCI1-BOT), \u2203R.C\u3157 (GCI3-BOT) and rs (RIO), r\u2081\u2082s (RI1),\nrespectively.\nTo define the semantics of an EL++ theory, we use [3] an interpretation domain \u2206\u00b9 and an interpretation function\n.1. For every concept A \u2208 C, A\u00b9 \u2286 \u22061; individual a \u2208 I, a\u00b9 \u2208 \u2206\u00b9; role r \u2208 R, r\u00b9 \u2208 \u0394\u0399 \u00d7 \u0394\u0399. Furthermore, the\nsemantics for other EL++ constructs are the following (omitting concrete domains and role inclusions):\n1 = 0\n\u03a4\u0399 = \u0394\u0399,\n(\u0391\u03a0 \u0392) = A \u2229 BI,\n(\u2203r.A)\u00b2 = {a \u2208 \u2206\u00b9 | \u2203b : ((a, b) \u2208 r\u207a \u028cb\u2208AZ)},\n(a) = {a}\nAn interpretation I is a model for an axiom C \u2286 D if and only if C\u00b9 \u2286 D1, for an axiom B(a) if and only if\na\u00b9 \u2208 B1; and for an axiom r(a, b) if and only if (a, b) \u2208 r\u012b [4]."}, {"title": "2.2. Knowledge Base Completion", "content": "The task of knowledge base completion is the addition (or prediction) of axioms to a knowledge base that are\nnot explicitly represented. We call the task \u201contology completion\" when exclusively TBox axioms are predicted.\nThe task of knowledge base completion may encompass both deductive [24, 48] and inductive [7, 15] inference\nprocesses and give rise to two subtly different tasks: adding only \"novel\" axioms to a knowledge base that are not\nin the deductive closure of the knowledge base, and adding axioms that are in the deductive closure as well as some\n\"novel\" axioms that are not deductively inferred; both tasks are related but differ in how they are evaluated.\nInductive inference, analogously to knowledge graph completion [12], predicts axioms based on patterns and\nregularities within the knowledge base. Knowledge base completion, or ontology completion, can be further distin-\nguished based on the information that is used to predict \u201cnovel\u201d axioms. We distinguish between two approaches to\nknowledge base completion: (1) knowledge base completion which relies solely on (formalized) information within\nthe knowledge base to predict new axioms, and (2) knowledge base completion which incorporates side information,\nsuch as text, to enhance the prediction of new axioms. Here, we mainly consider the first case."}, {"title": "3. Related Work", "content": null}, {"title": "3.1. Graph-Based Ontology Embeddings", "content": "Graph-based ontology embeddings rely on a construction (projection) of graphs from ontology axioms mapping\nontology classes, individuals and roles to nodes and labeled edges [57]. Embeddings for nodes and edge labels are\noptimized following two strategies: by generating random walks and using a sequence learning method such as\nWord2Vec [39]or by using Knowledge Graph Embedding (KGE) methods [54]. These type of methods have been\nshown effective on knowledge base and ontology completion [10] and have been applied to domain-specific tasks\nsuch as protein-protein interaction prediction [10] or gene-disease association prediction [1, 11]. Graph-based\nmethods rely on adjacency information of the ontology structure but cannot easily handle logical operators and\ndo not approximate ontology models. Therefore, graph-based methods are not \u201cfaithful\u201d, i.e., do not approximate\nmodels, do not allow determining whether statements are \u201ctrue\u201d in these models, and therefore cannot be used to\nperform semantic entailment."}, {"title": "3.2. Geometric-Based Ontology Embeddings", "content": "Multiple methods have been developed for the geometric construction of models for the EL++ language. ELEm-\nbeddings [29] constructs an interpretation of concept names as sets of points lying within an open n-dimensional ball\nand generates an interpretation of role names as the set of pairs of points that are separated by a vector in R\", i.e., by\nthe embedding of the role name. EmEL++ [40] extends ELEmbeddings with more expressive constructs such as role\nchains and role inclusions. ELBE [44] and BoxEL [55] use n-dimensional axis-aligned boxes to represent concepts,\nwhich has an advantage over balls because the intersection of two axis-aligned boxes is a box whereas the intersec-\ntion of two n-balls is not an n-ball. BoxEL additionally preserves ABox facilitating a more accurate representation\nof knowledge base's logical structure by ensuring, e.g., that an entity has the minimal volume. Box2EL [21] rep-\nresents ontology roles more expressively with two boxes encoding the semantics of the domain and codomain of\nroles. Box2EL enables the expression of one-to-many relations as opposed to other methods. Axis-aligned cone-\nshaped geometric model introduced in [42] deals with ALC ontologies and allows for full negation of concepts and\nexistential quantification by construction of convex sets in R\". This work has not yet been implemented or evaluated\nin an application.\""}, {"title": "3.3. Knowledge Base Completion Task", "content": "Several recent advancements in the knowledge base completion rely on side information as included in Large\nLanguage Models (LLMs). [23] explores how pretrained language models can be utilized for incorporating one on-\ntology into another, with the main focus on inconsistency handling and ontology coherence. HalTon [9] addresses"}, {"title": "3.4. Approximate Semantic Entailment", "content": "We follow [19] to state that when a model M of a theory T is also a model of an axiom C D defined over T, we\ncall it entailment and denote it as T = C \u2286 D. In this sense, semantic entailment can be understood as entailment\nover all the models of T, which is expressed as Mod(T) \u2286 Mod(C \u2286 D). Geometric-based ontology embedding\nmethods construct geometric models for EL++theories. However, since the collection Mod(T) is a class, it is not\npossible to construct all the possible geometric models. Therefore, we refer as approximate semantic entailment to\nthe construction of a finite set of geometric models for a EL++theory.\nIn the context of bioinformatics, methods such as DeepGOZero [28] formulate the prediction of protein functions\nas an entailment problem, relying on ELEmbeddings to generate a model for the Gene Ontology. Subsequently,\nthe extension to approximate semantic entailment is implemented in [30], where it is effectively showed that the\ngeneration of multiple models improves predictive performance of protein functions."}, {"title": "4. Methods", "content": null}, {"title": "4.1. Datasets", "content": null}, {"title": "4.1.1. Gene Ontology & STRING Data", "content": "Following previous works [21, 29, 44] we use common benchmarks for knowledge-base completion, in particular\na task that predicts protein-protein interactions (PPIs) based on the functions of proteins. We also use the same data\nfor the task of protein function prediction. For these tasks we use two datasets, each of them consists of the Gene\nOntology (GO) [59] with all its axioms, protein-protein interactions (PPIs) and protein function axioms extracted\nfrom the STRING database [37]; each dataset focuses on only yeast proteins. GO is formalized using OWL 2\nEL [17].\nFor the PPI yeast network we use the built-in dataset PPIYeastDataset available in the mOWL [58] Python\nlibrary (release 0.2.1) where axioms of interest are split randomly into train, validation and test datasets in ratio\n90:5:5 keeping pairs of symmetric PPI axioms within the same dataset, and other axioms are placed into the training\npart; validation and test sets are made up of TBox axioms of type {P1} \u2286 \u2203interacts_with.{P2} where P1, P2\nare protein names. The GO version released on 2021-10-20 and the STRING database version 11.5 were used.\nAlongside with the yeast interacts_with dataset we collected the yeast has_function dataset organized in the same\nmanner with validation and test parts containing TBox axioms of type {P} \u2203has_function.{GO}. Based on\nthe information in the STRING database, in PPI yeast, the interacts_with relation is symmetric and the dataset\nis closed against symmetric interactions. We normalize each train ontology using the updated implementation of\nthe jcel [36] reasoner \u00b9 where we take into consideration newly generated concept and role names. Although role\ninclusion axioms may be utilized within the Box\u00b2EL framework we ignore them since neither ELEmbeddings nor\nELBE incorporate these types of axioms."}, {"title": "4.1.2. Food ontology", "content": "Food Ontology [14] contains structured information about foods formalized in SRIQ DL expressivity [10] in-\nvolving terms from UBERON [41], NCBITaxon [16], Plant Ontology [22] etc. The data for subsumption prediction\nwas extracted from the case studies used to evaluate OWL2Vec* [10]\u00b2; the train part of the ontology was restricted\nto EL fragment and normalized using the jcel [36] reasoner. Since the normalization procedure splits each complex\naxiom into a set of shorter axioms including subsumptions between atomic concepts from the signature, it may\nresult in adding axioms represented in the validation or test part of the ontology to the train part; to avoid this,\nwe filtered out such axioms from the original validation and test datasets after the train ontology for subsumption\nprediction was normalized. Additionally, as described in Section 4.1.1, we remove entailed axioms from the test\ndataset."}, {"title": "4.2. Evaluation Scores and Metrics", "content": "For GO & STRING data, we predict GCI2 axioms of type {P\u2081} interacts_with.{P2} or {P} \ub4dc\nhas_function. {GO} depending on the dataset. On Food Ontology, we predict GCIO axioms of type C\u2286 D, C\nand D are arbitrary classes from the signature. For each axiom type, we use the corresponding loss expressions\nto score axioms. This is justified by the fact that objective functions are measures of truth for each axiom within\nconstructed models.\nThe predictive performance is measured by the Hits@n metrics for n = 1, 10, 100, macro and micro mean rank,\nand the area under the ROC curve (AUC ROC). For rank-based metrics, we calculate the score of CR.D or\nCD for every class C from the test set and for every D from the set C of all classes (or subclasses of a certain\ntype, such as proteins or functions for domain-specific cases) and determine the rank of a test axiom C\u2286 \u2203R.D.\nFor macro mean rank and AUC ROC, we consider all axioms from the test set; for micro metrics, we compute\ncorresponding class-specific metrics averaging them over all classes in the signature:\nmicro_MRC_\u2203R.D = Mean(MRc({C \u2286 \u2203R.D, D \u2208 C})) \nmicro_MRC_D = Mean(MRc({C \u2286 D, D \u2208 C})) \nmicro_AUC_ROCCER.D = Mean(AUC_ROCc({C \u2286 \u2203R.D, D \u2208 C})) \nmicro_AUC_ROCC\u2286D = Mean(AUC_ROCc({C \u2286 D, D \u2208 C})) \nAdditionally, we remove axioms represented in the train set or deductive closures (see Section 5.2.1) to obtain\ncorresponding filtered metrics (FHits@n, FMR, FAUC)."}, {"title": "4.3. Training Procedure", "content": "All models are optimized with respect to the sum of individual GCI losses (here we define the loss in most general\ncase using all positive and all negative losses):\nL = ICD + ICODE + ICCER.D + 1\u2203R.CD + lc + ICND1 + R.CC+\n+lCZD + ICNDZE + lcg\u2203R.D + 13R.CZD + lcg+ + IcndZ1 + lar.C"}, {"title": "5. Results", "content": null}, {"title": "5.1. Negative sampling and objective functions", "content": "Ontology embedding methods select negatives by replacing one of the classes with a randomly chosen one; e.g.,\nfor axioms of type C D represented within the ontology C\u2286 D' for some arbitrary or semantically valid concept\nD'. ELEmbeddings, ELBE and Box\u00b2EL use a single loss for \u201cnegatives\", i.e., axioms that are not included in the\nknowledge base; the loss is used only for axioms of the form C\u2286 \u2203R.D (GCI2) which are randomly sampled; nega-\ntives are not sampled for other normal forms. Correspondingly, the embedding methods were primarily evaluated on\npredicting GCI2 axioms (Box\u00b2EL was also evaluated on subsumption prediction); this evaluation procedure might\nhave introduced biases towards axioms of type GCI2, and influenced the ability of geometric models to predict\naxioms of other types.\nConsequently, we also sample negatives for other normal forms and add \"negative\u201d losses (i.e., losses for the\nsampled \"negatives\") for all other normal forms. We test the effect of the expanded negative sampling and negative\nloses first on a small ontology that can be embedded and visualized in 2D space, and then on a larger application.\""}, {"title": "5.1.1. ELEmbeddings Negative Losses", "content": "For ELEmbeddings, we construct the following \u201cnegative\u201d losses:\nlosscZD(c, d) = max(0, rn(c) + rn(d) \u2013 ||fn(c) \u2013 fn(d))|| + y) + |||f,(c)|| \u2013 1| + |||f\u2081(d)|| \u2013 1| \nlosscndZe(c, d, e) = max(0, -rn(c) \u2013 rn(d) + ||fn(c) \u2013 f\u2081(d))|| - y)+\n+max(0, rn (c) \u2013 ||f\u2081(c) \u2013 fn(e))|| + y) + max(0, rn(d) \u2013 ||fn(d) \u2013 fn(e))|| + y)+\n+|||f,(c)|| - 1| + |||f\u2081(d)|| \u2013 1| + |||fy(e)|| \u2013 1| \nloss\u2203r.CZD(r,c,d) = max(0, rn(c) + rn(d) \u2013 ||fn(c) \u2013 f\u2081(r) \u2013 f\u2081(d))|| + y)+\n+|||fy(c)|| - 1| + |||f\u2081(d)|| \u2013 1|"}, {"title": "5.1.2. ELBE Negative Losses", "content": "ELBE is a model that relies on boxes instead of balls. The negative losses for ELBE have the following form:\nlosscZD(c, d) = || max(zeros, -ec(c) \u2013 ec(d)| + e,(c) + e\uff61(d) + margin)||\nlosscndZe(c, d, e) = || max(zeros, -|ec(new) \u2013 ec(e)| + e(new) + e,(e) + margin) ||\nloss\u2203R.CZD(r, c, d) = ||max(zeros, -ec(c) \u2013 ec(r) \u2013 ec(d))| + e,(c) + e\uff61(d) + margin)||\nlosscg1(c) = max(0, \u0454 \u2013 ||e,(c)||)\nlosscmpg_(c, d) = max(0, \u025b \u2013 ||e.(new) ||)"}, {"title": "5.1.3. Box2EL Negative Losses", "content": "Box2 EL is also based on boxes but uses a different relation model compared to ELBE. The corresponding negative\nloses are designed as follows:\nlosscZp(c, d) = || max(0, \u2212(d(Box(C), Box(D)) + y))||\nlosscndZe(c, d, e) = || max(0, \u2013(d(Box(C) \u2229 Box(D), Box(E)) + y))||\nloss=r.CZD(r,c,d) = (\u03b4 \u2013 \u03bc(Head(r) \u2013 Bump(C), Box(D)))2\nlosscg1(c) = max(0, \u0454 \u2013 ||0(C)||)\nlossc_dg_(c,d) = max(0, \u025b \u2013 ||o(Box(C) \u2229 Box(D))||)"}, {"title": "5.1.4. Experiments", "content": "We evaluate whether adding negative losses for all normal forms will allow for the construction of a better model\nand improve the performance in the task of knowledge base completion. We formulate and add negative losses for\nall normal forms given by equations 6-23.\nFirst, we investigate a simple example corresponding to the task of protein function prediction using the EL-\nEmbeddings model. Let us consider an ontology consisting of two axioms stating that there are two disjoint\nfunctions {GO1} and {GO2}, and proteins having these functions are also disjoint: {GO1} \u220f {GO2} \u2286 1,\n\u2203has_function.{GO\u2081} \u220f has_function.{GO2} \ub4dc \u3157. After normalization, the last axiom is substituted by the\nfollowing three axioms: \u0391\u03a0\u0392\u2286 \u22a5, \u2203has_function.{GO\u2081} \u2286 B, \u2203has_function.{GO2} \u2286 A where A, B are new\nconcept names. To visualize the results, we embed these axioms in 2D space.\nSince we are interested in predicting not only axioms of type C \u2203R.D for which negative sampling is used in\nthe original ELEmbeddings, ELBE and Box\u00b2EL, we also examine the effect of all negative losses utilization during\ntraining on Food Ontology for subsumption prediction (see Table 3). We find that the ELEmbeddings model does\nnot improve on the Food Ontology subsumption prediction task, but ELBE with additional losses improves over the\noriginal model; Box\u00b2EL with additional losses surpasses its version with just GCI2 negative loss in Hits@n metrics.\nAdditionally, we evaluate the performance on a standard benchmark set for protein-protein interaction (PPI)\nprediction (see Table 2). For this task, the test axioms are of the type GCI2. We observe that ELEmbeddings and\nELBE with negative losses for all normal forms integrated demonstrate superior performance compared to their\ninitial configurations in terms of Hits@n metrics; it also allows Box\u00b2EL to lower ranks of test axioms. Generally,\nfor the task of PPI prediction, additional negative sampling improves performance."}, {"title": "5.2. Negative sampling", "content": "In the case of knowledge base completion where the deductive closure contains potentially many non-trivial\nentailed axioms, the random sampling approach for negatives may lead to suboptimal learning since some of the\naxioms treated as negatives may be entailed (and should therefore be true in any model, in particular the one con-\nstructed by the geometric embedding method). As an example, let us consider the simple ontology consisting of two\naxioms: \u0391\u03a0\u0392 C and D B. For the ABC axiom, random negative sampling will sample AB C' where\nC' is one of A, B, C, D. Since the knowledge base makes the axioms AB \u0410, \u0410\u041f\u0412 B, and ABC true, in\n75% of cases we will sample a negative for this axiom that is actually true in each model."}, {"title": "ALGORITHM 1", "content": "An algorithm for computation of axioms in the deductive closure using inference rules; axioms in bold correspond to\nsubclass/superclass axioms derived using ELK reasoner (here we use the transitive closure of the ELK inferences);\nplain axioms come from the knowledge base.\nfor all CDE in the knowledge base do\nCODE C'C D' D E E'\nC'D'E'\nend for\nfor all CR.D in the knowledge base do\nCR.D C C D D' RER'\nC'R' D'\nCR.D DR.E ROR'S\nCS.E\nend for\nfor all R.CD in the knowledge base do\n\u2203R.CD C\u2286 C D \u2286 D' R'\u2286R\nR'.C'D'\nend for\nfor all CD in the knowledge base do\nCOD C'\u2286 C D'\u2286 D\nC'D'\nend for\nfor all R.C\u3157 in the knowledge base do\nR.CCC R'ER\nR'.C'\nend for"}, {"title": "5.2.1. Deductive Closure", "content": "The deductive closure of a theory T refers to the smallest set containing all statements which can be inferred\nby deductive reasoning over T; for a given deductive relation, we call T = {|T - } the deductive closure\nof T. In knowledge bases, the deductive closure is usually not identical to the asserted axioms in the knowledge\nbase; it is also usually infinite. Representing the deductive closure is challenging since it is infinite, but, in EL++,\nany knowledge base can be normalized to one of the seven normal forms; therefore, we can compute the deductive\nclosure with respect to these normal forms, and this set will be finite (as long as the concept and role names are\nfinite). However, EL++reasoners such as ELK [26] compute subsumption hierarchies, i.e., all axioms of the form"}, {"title": "ALGORITHM 2", "content": "Additional entailed axioms\nfor all concepts C, D, E, E' in the signature do\n\u0421\u041f\u0406\u0421\u0415\nDCI\nCODE\n\u0395C\u0395'\n\u0421\u041f\u0415\u0421\u0415\nCCE DE C'C D'ED EC \u0395'\nC'D'E'\nend for\nfor all relations R and all concepts D \u2260 1 in the signature do\nR.D\nCC\nCR.D\nend for\nfor all relations R and all concepts C \u2260 1 in the signature do\n3R.CT\nend for"}, {"title": "5.2.2. Experiments", "content": "Using the example introduced in Section 5.1.4 and the ELEmbeddings embedding model, we demonstrate that\nnegatives filtration may be beneficial for constructing a model of a theory. Apart from axioms mentioned earlier, i.e.,\n{GO\u2081} {GO2} \u2286 \u22a5, \u0410 \u041f \u0412 \u2286 \u22a5, \u2203has_function.{GO\u2081} \u2286 B and \u2203has_function.{GO2} \u2286 A, we add 10 more\naxioms about 5 proteins {P1},..., {P5} having function {GO\u2081} (i.e., {P;} \u2286 \u2203has_function.{GO\u2081}, i = 1, ..., 5),\nand 5 proteins {Q1},..., {Q5} having function {GO2} (i.e., {Q;} \u2286 \u2203has_function.{GO2}, i = 1, . . ., 5).\nshows the constructed models with and without negatives filtering. We observe that the model with filtered negatives\nprovides faithful representation of GCI3 axiom \u2203has_function.{GO2} = A and axioms introducing proteins having\nfunction {GO2} as opposed to its counterpart with random negatives."}, {"title": "5.3. Evaluation Strategies", "content": "In the task of knowledge base completion with many non-trivial entailed axioms, the deductive closure can also\nbe used to modify the evaluation metrics, or define novel evaluation metrics that distinguish between entailed and"}, {"title": "6. Discussion", "content": "We evaluated properties of ELEmbeddings, ELBE and Box\u00b2EL, ontology embedding methods that aims to gen-\nerate a model of an EL++theory; the properties we evaluate hold similarly for other ontology embedding methods\nthat construct models of EL++theories. While we demonstrate several improvements over the original model, we\ncan also draw some general conclusions about ontology embedding methods and their evaluation. Knowledge base\ncompletion is the task of predicting axioms that should be added to a knowledge base; this task is adapted from\nknowledge graph completion where triples are added to a knowledge graph. The way both tasks are evaluated is\nby removing some statements (axioms or triples) from the knowledge base, and evaluating whether these axioms\nor triples can be recovered by the embedding method. This evaluation approach is adequate for knowledge graphs\nwhich do not give rise to many entailments. However, knowledge bases give rise to potentially many non-trivial\nentailments that need to be considered in the evaluation. In particular, embedding methods that aim to generate a\nmodel of a knowledge base will first generate entailed axioms (because entailed axioms are true in all models);\nthese methods perform knowledge base completion as a generalization of generating the model where either other\nstatements may be true, or they may be approximately true in the generated structure. This has two consequences:\nthe evaluation procedure needs to account for this; and the model needs to be sufficiently rich to allow useful pre-\ndictions.\nWe have introduced a method to compute the deductive closure of EL++knowledge bases; this method relies\non an automated reasoner and is sound. We use all the axioms in the deductive closure as positive axioms to be\npredicted when evaluating knowledge base completion, to account for methods that treat knowledge base completion\nas a generalization of constructing a model and testing for truth in this model. We find that some models (e.g.,"}, {"title": "Appendix D. Deductive Closure Computation Example", "content": "Let us add two more axioms to the simple ontology example from Section 5.1.4 about proteins {P} and {Q}\nhaving functions {GO1} and {GO2}, respectively. ELK will infer the following class hierarchy:\nConcepts D where C D\nC\n\u22a5, {P}, {Q}, A, B, {GO1}, {GO2}, T\n{P}\n{P}, B, T\n{Q}\n{Q}, A, T\nA\nA, T\nB\nB, T\n{GO1}\n{GO1}, \u03a4\n{GO2}\n{GO2}, T\nT\nT\nIn this small protein function prediction example there are two disjointness axioms: AB and {GO\u2081} \u03a0\n{GO2}. Taking into consideration the concept hierarchy and inference rules from part 2 the algorithm will\ninfer the following GCI1 and GCI1_BOT axioms:"}]}