{"title": "Bel Esprit: Multi-Agent Framework for Building AI Model Pipelines", "authors": ["Yunsu Kim", "AhmedElmogtaba Abdelaziz", "Thiago Castro Ferreira", "Mohamed Al-Badrashiny", "Hassan Sawaf"], "abstract": "As the demand for artificial intelligence (AI) grows to address complex real-world tasks, single models are often insufficient, requiring the integration of multiple models into pipelines. This paper introduces Bel Esprit, a conversational agent designed to construct AI model pipelines based on user-defined requirements. Bel Esprit employs a multi-agent framework where subagents collaborate to clarify requirements, build, validate, and populate pipelines with appropriate models. We demonstrate the effectiveness of this framework in generating pipelines from ambiguous user queries, using both human-curated and synthetic data. A detailed error analysis highlights ongoing challenges in pipeline construction. Bel Esprit is available for a free trial at https://belesprit.aixplain.com\u00b9.", "sections": [{"title": "1 Introduction", "content": "AI has traditionally been applied to atomic tasks, each addressed by a single model. Over time, task coverage has expanded through multi-task learning and, more recently, with generally pre-trained models exhibiting emergent capabilities. Even models capable of handling multiple tasks are often insufficient for complex tasks, especially those involving multiple inputs or outputs, e.g., multimodal content moderation or multilingual video dubbing (Figure 1). Such tasks can be better addressed by integrating different models, where each model handles a specific aspect of the problem. By constructing a pipeline of interconnected models, we can automate intermediate steps and facilitate seamless task transitions. This approach, known as cascading models into a pipeline, has been widely used in applications like speech translation and voice conversion."}, {"title": "2 Task Definition", "content": "Pipeline generation is defined as a structured prediction task, where the input is a user query describing a computational task, and the output is a pipeline of AI functions designed to solve it. Each AI function may have one or more parameters, such as the language in speech recognition. The final output"}, {"title": "3 Framework", "content": "In this work, we use a large language model (LLM) to process user queries and generate pipeline structures through guided prompts. Instead of producing the pipeline in a single step, the framework follows a sequential yet partially cyclic flow involving multiple subagents (Figure 2). The process begins with Mentalist, followed by Builder, which creates an initial pipeline. This pipeline is then reviewed by Inspector. If the review fails, it loops back to Builder for revisions until an error-free pipeline is generated or the maximum iteration limit is reached. Once the pipeline passes inspection, it proceeds to Matchmaker, completing the final pipeline. Details on the Matchmaker are provided in Appendix C, while the remaining components are discussed in this section."}, {"title": "3.1 Mentalist", "content": "Mentalist is the agent responsible for interacting with the user and analyzing their requirements."}, {"title": "3.1.1 Query Clarifier", "content": "User queries are often too ambiguous to build a correct solution. For example, they may lack a detailed problem description, such as how \u201crisk\u201dis defined in a risk management system, or omit properties of the data, like the language of the input text. Query Clarifier, a chat-based interface, addresses this by converting potentially ambiguous user queries into fully developed solution specifications. It identifies missing information and prompts the user in a friendly manner to fill in the gaps. Once all necessary details are gathered, the system enters a confirmation stage, summarizing the conversation into a refined query that clearly outlines the solution's inputs, outputs, and their modalities, along with their relationships. An example conversation is shown in Figure 3."}, {"title": "3.1.2 Specification Extractor", "content": "After the user confirms the clarified query, Specification Extractor extracts technical details from it in a structured format. The specification includes a short name, modality, and required parameters (e.g., language) for each input and output (Table 1). Relying solely on long natural language queries often results in errors when building a solution. In contrast, structured information offers clear guidance on which input and output nodes must be included, providing a strong foundation for constructing the intermediate flows. If the user attaches files to clarify the task, Mentalist analyzes them and links them to the corresponding inputs in the specification (Appendix B)."}, {"title": "3.2 Builder", "content": "After Mentalist finalizes the user requirements, Builder constructs the pipeline graph based on the initial user query, the refined query (Section 3.1.1), and the extracted specification (Section 3.1.2). Builder is implemented using an LLM prompted with information on data types, function identifiers, node types, and graph constraints (Appendix A). Given the complexity of this task (Section 2), a few example pipelines are included in the prompt to guide the generation process. Builder's output can be in any structured format, such as DOT or JSON, as long as it maintains a bijective mapping to the intended graph structure."}, {"title": "3.2.1 Chain-of-Branches", "content": "Building a large graph in a single step is highly challenging. Generating token sequences in structured formats often leads to issues like hallucination and loss of consistency within the structure. Inspired by the chain-of-thought approach , we decompose the solution graph into subgraphs, focusing on distinct branches. Each branch represents a path from one or more input nodes to an output node within the graph, meaning a pipeline graph with N output nodes will have N branches. These branches can act as standalone solutions to subproblems derived from the user query. After constructing one branch, additional branches can often reuse nodes (inputs or functions) from existing branches, thereby reducing the number of nodes and edges required for each branch. To leverage this approach, we prompt the LLM to generate one branch at a time, completing all nodes and edges for that branch before moving to the next (Figure 4). At the start of each branch, we instruct the model to generate a brief comment marked with a special symbol to clarify the sub-problem it addresses, ensuring the boundaries between branches remain distinct and coherent."}, {"title": "3.3 Inspector", "content": "LLMs are particularly vulnerable to errors in scientific reasoning when dealing with lengthy contexts. Even with a clarified and detailed query, inaccuracies may still arise due to the complexity of the solution or misinterpretation of user requirements. To address these errors in LLM outputs, many previous studies have employed a critic model to assess potential inaccuracies. Similarly, we developed Inspector, which analyzes the builder's output to identify errors in both the graph structure and semantic alignment with user requirements."}, {"title": "3.3.1 Syntax", "content": "The first inspection phase assesses the integrity of the generated graph. Independent of the graph's intended function, we can detect structural errors that violate model pipeline constraints, often related to improper node connections (see Appendix A). Some violations can be mechanically corrected immediately upon detection. Figure 5a illustrates such a case during the generation of Branch 1 in Figure 4. The output from a function node should connect to a single output node, but here, multiple output nodes are linked to the same function output. This issue often arises when the user specifies multiple outputs in the solution. In such cases, we can resolve the error by retaining only one of the output nodes and removing the duplicates. Figure 5b illustrates an example where no simple correction is feasible. The machine translation (MT) node requires text input, yet audio extracted from a video input is routed directly to it. Resolving this modality mismatch involves either locating an existing node in the graph that produces the necessary text output or creating a new node to perform the required transformation. Such complex corrections require assistance from the LLM to reconstruct the graph (Section 3.2)."}, {"title": "3.3.2 Semantics", "content": "The second phase of inspection verifies whether the generated graph effectively fulfills the user's specified requirements, even if it contains no syntactic errors. We employ an LLM to evaluate the semantic alignment of the graph flow with the user's specified requirements. For each branch, the LLM is provided with a natural language summary that lists the nodes sequentially, outlining the path and context within the pipeline. The LLM then identifies the corresponding requirements in the specification (Section 3.1.2) and flags any unmatched or missing steps in the branch path. Figure 6 shows an example where the branch passes structural checks but fails in semantic alignment. In this case, the English transcription is routed directly to a French text-to-speech (TTS) node, assuming the same text modality suffices"}, {"title": "4 Experimental Setup", "content": "To evaluate pipeline generation, we prepared query-pipeline pairs with evaluation metrics."}, {"title": "4.1 Data", "content": "Manual creation Given the high-level scientific nature of the task, we recruited five AI solution engineers at aiXplain, Inc. to create realistic tasks and their corresponding pipelines. To avoid redundancy, each expert's work was shared with others, ensuring that no duplicate pipelines were produced. Each pipeline was then reviewed and, if necessary, revised by at least one other expert.\nStructured synthesis with human correction To scale data collection, we automate the initial pipeline creation using rule-based expansion. In this approach, nodes in a pipeline graph are expanded by adding others that match the input-output specifications. Starting with one or more input nodes, we construct a tree-like structure that can branch into multiple output nodes. To manage complexity, we parameterize the number of AI function nodes and restrict each node to have a maximum of two children. An LLM generates specifications and clear queries that enumerate the inputs and outputs. To simulate realistic user interactions, we then synthesize an initial user query by intentionally introducing ambiguity into the LLM prompt. Details for this dataset are provided in Appendix D."}, {"title": "4.2 Metrics", "content": "Exact Match (EM) First, we compute the proportion of cases where the generated pipeline exactly matches the reference pipeline. Two nodes are considered a match if their types are identical and, if applicable, their functions and parameters are the same. For LLM nodes, we match prompts based on cosine similarity of their sentence embeddings, with a threshold of 0.5. For script nodes, we consider two code snippets a match if an LLM determines they perform the same task. Edges are matched if they connect the same source and target nodes with identical parameters. Since node and edge numbering may differ between the generated and reference pipelines, determining an exact match (EM) requires solving the graph isomorphism problem. We adapt the VF2 algorithm to account for our problem in this process.\nGraph Edit Distance (GED) In our initial study, we found that many non-matching pipelines differ only slightly, typically by a single node or edge. Assigning a full penalty to such cases is too severe, as EM fails to capture incremental improvements. Therefore, we adopt a more fine-grained metric-graph edit distance (GED)\u2014to quantify the deviation between the generated and reference graphs by counting the number of edit operations required to make them identical. These operations include insertion, deletion, or substitution of nodes or edges, with equal weight of 1.0 assigned to each for simplicity. We apply the same matching conditions for nodes and edges as used in EM. We use NetworkX's implementation of the depth-first GED algorithm. Given the potential computational cost for large pipelines, we limit the running time for each pipeline pair to 60 seconds."}, {"title": "4.3 Models", "content": "Mentalist's query clarifier (Section 3.1.1) and Builder (Section 3.2) utilize GPT-40, while the rest of the framework, including data synthesis and evaluation, relies on the Llama 3.1 70B model when LLM as-"}, {"title": "5 Results", "content": "Table 2 shows the pipeline generation performance across various framework configurations. Starting with a baseline pipeline builder, we incrementally incorporate components from Mentalist, Builder, and Inspector, achieving +9.5% EM and -28.1% GED overall. For the Builder, the high-end proprietary model (GPT-40) outperforms open-source alternatives, with performance declining as model size decreases. Smaller models like Llama 3.1 8B and Solar 10.7B yielded unacceptable performances, with EM rates below 3%. Each component's contribution is evident in GED improvements for GPT-40 but less consistent for weaker models. Appendix E shows an example of the incremental contribution of the components in pipeline generation. EM fails to capture these nuanced improvements. As a side note, semantic inspection occasionally confuses weaker Builders, leading to unnecessary graph repetitions and sporadic performance drops.\nAmbiguity of query As shown above, ambiguity in user queries is inevitable in user-facing applications and is a primary factor contributing to poor pipeline generation performance. To quantify its impact, we used the GPT-40 model to rate the ambiguity of queries in our dataset in three levels: unambiguous, ambiguous, and very ambiguous. Pipelines in our dataset were then categorized by these levels, and performance metrics were computed for each group, as shown in Figure 7. The results demonstrate that pipeline generation becomes increasingly challenging with higher ambiguity. The Mentalist subagent significantly improves performance in such cases by clarifying"}, {"title": "6 Edit Analysis", "content": "This section analyzes errors in generated pipelines using GED, highlighting discrepancies from reference pipelines. Figure 9 shows error distributions in our framework with GPT-40 and the Builder (Table 2, last row). Most errors stem from node substitutions, often due to parameter mismatches or incorrect node types (Figure 10).\nNode insertions occur when the builder fails to address all query requirements, often in large pipelines. Node deletions typically result from redundant function repetitions in separate paths (Figure 14b). Both edits are also required when a"}, {"title": "7 Conclusion", "content": "This paper introduces a novel task of generating AI solution pipelines from user queries and proposes Bel Esprit, a multi-agent framework, consisting of Mentalist, Builder, and Inspector, which incrementally improve pipeline quality through query clarification, stepwise construction, and validation. Future work includes employing retrieval-augmented generation (RAG) with a pool of valid pipelines, incorporating user feedback for pipeline editing, and extending the framework to generate autonomous agents beyond static pipelines."}, {"title": "Limitations", "content": "Although the Mentalist (Section 3.1) enhances performance in ambiguous scenarios, the system still struggles with highly ambiguous queries, especially when critical input or output requirements are missing.\nPipeline building (Section 3.2) and matchmaking (Appendix C) are restricted to a predefined pool of AI functions. Expanding this pool and incorporating their parameter details increases the prompt length, leading to higher computational costs. Generic nodes (Appendix C.1) address this partially but are currently limited to text-to-text functions.\nThe Inspector (Section 3.3) does not verify the generated code for script nodes (Appendix C.2), requiring custom test cases tailored to each script, which is not yet automated."}, {"title": "A Graph Constraints", "content": "Below you can find the list of constraints to be a valid graph for a model pipeline.\nInput nodes\n\u2022 An input node should have no previous nodes\n\u2022 An input node should have only one output parameter\nOutput nodes\n\u2022 An output node should have no next nodes\n\u2022 There should be no multiple output nodes with the same incoming link\nRouter nodes\n\u2022 A router node should have a single input node as its predecessor\n\u2022 A router node should have two or more output parameters, each of which has a different modality\n\u2022 A router node should not be connected with another router node\nFunction nodes\n\u2022 A function name should exist in the predefined list of functions\n\u2022 Parameters of a function node should exist in the predefined list of parameters\n\u2022 A function node should have all its required input parameters\nGeneral Node I/O\n\u2022 An input parameter should have only one incoming link\n\u2022 An output parameter should have at least one outgoing link if it is not an output node\nConnectivity\n\u2022 Every node should be reachable from an input node\nEdges\n\u2022 An edge should connect existing parameters\n\u2022 The connected parameters should have the same modality"}, {"title": "B Attachment Matcher", "content": "In our pilot study, we found that many users begin their interaction with Bel Esprit by attaching a file they want to work with, for example, stating, \u201cI want to work with this text file to extract all named entities and identify any grammatical errors.\u201d Once a solution is generated, users often want to know which input node in the pipeline graph corresponds to the attached file. While matching is straightforward when there is only a single input node, it becomes more challenging when there are multiple input nodes, especially when some share the same modality.\nIn such cases, semantic analysis of the conversation is necessary to determine the specific characteristics of each input. Files may also be attached mid-conversation, with contextual clues before and after the attachment providing critical information for accurate matching. Attachment Matcher detects these associations and assigns each attached file to the appropriate input node. Note that file names themselves are not passed to the builder, as they may not be directly relevant to the solution."}, {"title": "C Matchmaker", "content": "The pipeline graph generated by the builder specifies only the data flow logic of the solution, without assigning specific models to each function node. Model selection is treated as a detail independent of the core solution structure. For instance, a dubbing solution could employ an ASR model from"}, {"title": "C.1 Generic Nodes", "content": "Recent LLMs can perform generic tasks beyond their specific training when given a clear prompt. When certain AI functions are unavailable, we insert a custom LLM node for text-to-text tasks, with the prompt derived from the relevant part of the user query. This approach is useful for tasks like paraphrasing, domain mixing, or creative writing, where specialized models are scarce."}, {"title": "C.2 Script Generator", "content": "Some nodes are designated not for AI tasks but for simpler functions, such as counting words or extracting text from a PDF. General LLMs are not suited for these tasks, as a basic script with the right libraries is sufficient."}, {"title": "D Query-Pipeline Dataset", "content": "We define five pipeline types to balance diversity and simplicity during synthesis by excluding overly complex connections:\n\u2022 Sequential: A single input linearly flows through a chain of nodes, where each node uses the output of the previous one.\n\u2022 Single-branching: A single input branches into multiple independent paths, each ending in an output node.\n\u2022 Sequence-branching: A single input is processed by a linear sequence of nodes first and then branches into multiple paths.\n\u2022 Merging: Distinct branches from separate inputs merge and continue processing.\n\u2022 Router: Distributes a single input via a router node to specialized paths based on modality.\nReview process Pipeline synthesis involves randomness, occasionally producing nonsensical results. To address this, an LLM first assigns binary labels to filter such pipelines based on their descriptions. Retained pipelines then undergo a second review by one of the authors of this paper to ensure their integrity and applicability to real-world business contexts. Notably, the search-based generation ensures logical correctness, eliminating the need for human reviewers to validate graph structures.\nStatistics We synthesized around 100 pipelines for each of the five types. After review, we curated a dataset of 441 pipelines, comprising 82 real and 359 synthetic examples."}, {"title": "E Qualitative Example", "content": "Figure 14 illustrates an example of incremental improvements in pipeline generation. The initial user query is ambiguous, as it does not specify the input language. The plain Builder assumes English as the input language and generates a pipeline accordingly. Mentalist refines the query to explicitly indicate that the input language is unknown, resulting in a pipeline that first performs language"}]}