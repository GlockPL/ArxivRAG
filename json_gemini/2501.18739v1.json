{"title": "Neural Graph Pattern Machine", "authors": ["Zehong Wang", "Zheyuan Zhang", "Tianyi Ma", "Nitesh V Chawla", "Chuxu Zhang", "Yanfang Ye"], "abstract": "Graph learning tasks require models to comprehend essential substructure patterns relevant to downstream tasks, such as triadic closures in social networks and benzene rings in molecular graphs. Due to the non-Euclidean nature of graphs, existing graph neural networks (GNNs) rely on message passing to iteratively aggregate information from local neighborhoods. Despite their empirical success, message passing struggles to identify fundamental substructures, such as triangles, limiting its expressiveness. To overcome this limitation, we propose the Neural Graph Pattern Machine (GPM), a framework designed to learn directly from graph patterns. GPM efficiently extracts and encodes substructures while identifying the most relevant ones for downstream tasks. We also demonstrate that GPM offers superior expressivity and improved long-range information modeling compared to message passing. Empirical evaluations on node classification, link prediction, graph classification, and regression show the superiority of GPM over state-of-the-art baselines. Further analysis reveals its desirable out-of-distribution robustness, scalability, and interpretability. We consider GPM to be a step toward going beyond message passing.", "sections": [{"title": "1. Introduction", "content": "Graphs are essential modeling tools for a wide range of applications, such as social network analysis, drug discovery, and recommendation systems. Many real-world tasks can be transformed into graph classification or regression problems. For instance, determining the chemical properties of molecules can be framed as classifying molecule graphs, where atoms are nodes and bonds are edges (Hu et al., 2020). In these graph-related tasks, the graph topology itself often provides critical insights for downstream tasks (Xu et al., 2019; Zhao et al., 2022). For example, triadic closure\u00b9 (Granovetter, 1973), where three nodes are interconnected, illustrates that a friend of a friend is also a friend in social networks, highlighting stable relationships among nodes. It is a fundamental pattern for predicting the occupations or interests of individuals (node classification) (Jin et al., 2020) and for recommending new friends (link prediction) (Huang et al., 2015). Another example is the benzene ring, a cyclic hydrocarbon with six carbon atoms, in molecules (Rong et al., 2020). The structure of the benzene ring indicates molecular stability due to delocalized electrons, making it resistant to certain chemical reactions like addition reactions. In this paper, we refer to these substructures (triangles, rings, etc.) as substructure patterns or graph patterns.\nTo model graph-structured data, graph neural networks (GNNs) (Kipf & Welling, 2017; Hamilton et al., 2017; Veli\u010dkovi\u0107 et al., 2018) have been introduced and have achieved remarkable success in tasks such as node classification, link prediction, and graph classification. Despite these empirical successes, basic GNNs rely on message passing (Gilmer et al., 2017) to iteratively aggregate neighborhood information and update node representations, rather than directly learning from graph patterns. Therefore, numerous studies (Xu et al., 2019; Verma & Zhang, 2019; Garg et al., 2020; Chen et al., 2020; Tang & Liu, 2023; Zhang et al., 2024a) have shown that message passing struggles to identify even the most fundamental patterns, such as triangles, stars, and k-cliques, due to its limitation to the 1-dimensional Weisfeiler-Leman (1-WL) isomorphism test. To enhance the pattern modeling capabilities of message passing, advanced methods such as positional embeddings (Murphy et al., 2019; Loukas, 2020), graph transformers (Kreuzer et al., 2021; Rampasek et al., 2022), and expressive GNNs (Morris et al., 2019; Zhao et al., 2022) have been proposed, breaking the 1-WL test bound. However, these methods essentially complement message passing and cannot yet directly learn from graph patterns. Additionally, they encounter other limitations, such as lack of interpretability, quadratic complexity, and biased inductive biases.\nTo go beyond the existing message-passing frameworks, re-"}, {"title": "2. Related Works", "content": "We primarily discuss methods that utilize graph patterns as tokens in this section. A detailed discussion of the limitations of message passing and existing improvements is provided in Appendix A. Early methods for leveraging graph patterns often relied on primitive training paradigms. For instance, DGK (Yanardag & Vishwanathan, 2015) uses graph kernels to measure relationships between (substructure) patterns, while AWE (Ivanov & Burnaev, 2018) employs anonymous random walks to encode pattern distributions in graphs. More recent approaches tokenize graphs into sequences of substructures. GraphViT (He et al., 2023), for example, splits a graph into multiple subgraphs using graph partitioning algorithms (Karypis, 1997), encodes each subgraph individually via message passing, and aggregates the resulting embeddings to represent the entire graph. Similarly, GMT (Baek et al., 2021) decomposes nodes into a multiset, with each set representing a specific graph pattern. However, these methods are limited to graph-level tasks. On the other hand, NAGphormer (Chen et al., 2023) and GC-Former (Chen et al., 2024) utilize Hop2token and neighborhood sampling, respectively, to tokenize patterns representing individual nodes; Yet, they are tailored for node-level tasks. To address this, SAT (Chen et al., 2022), GNN-AK"}, {"title": "3. Neural Graph Pattern Machine", "content": "Let G = (V, E) denote a graph, where V is the set of nodes with |V| = N, and E \u2286 V \u00d7 V is the set of edges with |E| = E. Each node $v \\in V$ is associated with a feature vector $x \\in R^{d_n}$, and each edge $e \\in E$ (if applicable) is associated with a feature vector $e \\in R^{d_e}$. We define graph patterns as subgraphs that represent small, recurring substructures within graphs, such as triangles, stars, cycles, etc."}, {"title": "3.1. Pattern Tokenizer", "content": "Tokenization is an essential step for converting a given instance into a sequence of patterns. Existing methods typically rely on pre-defined strategies. For example, large language models (LLMs) employ pre-defined vocabularies (Brown et al., 2020) to decompose a sentence into a sequence of tokens, where each token represents a concept or entity. Similarly, vision transformers use patch tokenizers to partition an image into a sequence of grids (Dosovitskiy et al., 2021). However, a universal tokenizer for graphs has yet to be established. A straightforward approach is to construct a unified substructure vocabulary via efficient subgraph matching (Sun et al., 2012), and then use this vocabulary to describe patterns for each instance (e.g., node, edge, or graph). However, the vocabulary construction and pattern"}, {"title": "Definition 3.1 (Anonymous Walk (Ivanov & Burnaev, 2018)).", "content": "Given a random walk $w = (v_0, v_1,..., v_L)$, its corresponding anonymous walk is defined as a sequence of integers $\\phi = (\\gamma_0, \\gamma_1,..., \\gamma_L)$, where $\\gamma_i = min_{pos}(w, v_i)$. The mapping from the random walk to its anonymous path is represented by $w \\rightarrow \\phi$."}, {"title": "Proposition 3.2.", "content": "Given a node $v \\in V$, assume the task requires information from the k-hop ego-graph $B(v, k) = (V', E')$. A sufficiently large set of patterns, sampled via l-length anonymous walks with $l = O(|E'|)$, can provide distinguishable topological representations."}, {"title": "Proposition 3.3.", "content": "Jointly encoding semantic paths and anonymous paths via any bijective mappings provides a comprehensive representation of the graph inductive biases encapsulated in the corresponding graph pattern."}, {"title": "Applicability to Graph-based Tasks.", "content": "The method is capable of tokenizing nodes, edges, and graphs. For node-level tasks, the tokenizer samples k patterns for a node v. For edge-level tasks, the tokenizer samples k patterns starting from the endpoints u and v of an edge e = (u, v). For graph-"}, {"title": "3.2. Pattern Encoder", "content": "As discussed above, any graph pattern can be represented as the combination of a semantic path and an anonymous path. Given a graph pattern, let the semantic path be $w = (v_0,..., v_n)$ and the anonymous path be $\\phi = (\\gamma_0,..., \\gamma_n)$. These paths are encoded separately using two distinct encoders, which are combined to form the pattern embedding:\n$p = \\rho_s(w) + \\lambda\\cdot \\rho_a(\\phi),$\nwhere p denotes the pattern embedding, $\\rho_s$ and $\\rho_a$ are the encoders for the semantic path and the anonymous path, respectively, and $\\lambda$ is a weighting coefficient."}, {"title": "Semantic Path Encoder.", "content": "To encode the semantic path, we construct a sequence of node features according to the semantic path as $[x_0,...,x_n]$, where nodes may appear multiple times. The encoder $\\rho_s$ can be any model capable of processing sequential data. By default, we use the transformer encoder due to its superior expressiveness in capturing long-range dependencies. The alternatives can be mean aggregator or GRU (Chung et al., 2014). The encoding process is defined as:\n$\\rho_s(w) = \\rho_s([h_0,..., h_n]), h_i = Wx_i + b,$\nwhere $x = [x || e]$ represents the concatenation of node features x and optional edge features e (if applicable). Note that in a path, the number of edges is one less than the number of nodes. To address this mismatch, edge features are padded with a zero vector at the beginning of the sequence."}, {"title": "Node Positional Embedding.", "content": "To incorporate advanced topological information, it is optional to concatenate node positional embeddings (PEs) with the node features. This approach has been shown to be effective to enhance model expressiveness (Kreuzer et al., 2021; Rampasek et al., 2022; He et al., 2023; Chen et al., 2023). The augmented node features are represented as:\n$x_i = [x_i || e_{i-1,i} || a_i],$\nwhere $x_i$ denotes the node features, $e_{i-1,i}$ represents optional edge features, and $a_i$ refers to the optional positional embedding. In this work, we utilize widely adopted PEs, including random-walk structural embeddings (RWSE) (Rampasek et al., 2022) and Laplacian eigenvector embeddings (Lap) (Kreuzer et al., 2021). Empirically, GPM achieves competitive performance without the use of PEs."}, {"title": "Anonymous Path Encoder.", "content": "For an anonymous path $\\phi = (\\gamma_0,..., \\gamma_n)$, we adopt a similar approach to encode the path as used for semantic paths, with a key distinction: anonymous nodes lack explicit features. Instead of employing"}, {"title": "3.3. Important Pattern Identifier", "content": "For each graph instance, we use a set of encoded patterns $P = [p_0,..., p_m]$ to describe its characteristics. Since the patterns are randomly sampled from the graph, it is essential to identify the most relevant patterns for downstream tasks. To achieve this, we employ a transformer encoder to highlight the dominant patterns by learning their relative importance. The encoding process is defined as follows:\n$Q = PW_Q, K = PW_K, V = PW_V,$\n$Attn(P) = softmax(\\frac{QK^T}{\\sqrt{d}})V \\in R^{n \\times d_{out}},$\n$P' = FFN(P + Attn(P)),$\nwhere $W_Q$, $W_K$, $W_V$ are trainable parameter matrices, $d_{out}$ is the dimension of the query matrix Q, and FFN represents a two-layer MLP. We utilize multi-head attention, which has proven effective in practice by concatenating multiple attention mechanisms.\nLet $P' = [p'_0, ..., p'_m]$ denote the output of the final transformer layer. These outputs are aggregated for downstream predictions using an additional prediction head:\n$\\hat{y} = Head(\\frac{1}{m} \\sum_{i=1}^m p'_i)$,\nwhere Head is a linear prediction layer. A mean readout function is applied over all pattern embeddings to compute the instance embedding, which is then used for prediction."}, {"title": "3.4. Training Strategies", "content": "Test-Time Augmentation. For a single instance, we select k patterns to represent its characteristics, where empirical evidence suggests that a larger k generally improves performance. However, increasing k imposes additional computational costs, as the self-attention has a quadratic time complexity with respect to the input length, $O(k^2)$. Inspired by test-time scaling in LLMs (Snell et al., 2024), we address this trade-off by training the model with m patterns and using k patterns during inference, where $k \\gg m$. By default, we set m = 16 and k = 128. To further reduce the computational overhead of pattern sampling, we pre-sample k patterns during preprocessing and randomly select m patterns during training. The time consumption is minimal, taking less than 10 seconds on most datasets. Even for the largest dataset used (~2,500,000 nodes), the sampling time remains under 2 minutes on an Nvidia A40 GPU.\nMulti-Scale Learning. Recent studies have demonstrated the effectiveness of multi-scale learning across various domains (Liao et al., 2019; Chen et al., 2021). Inspired by the success of multi-scale learning in visual transformers (Chen et al., 2021), which process small and large patch tokens within a single model, we propose sampling patterns of varying sizes. Specifically, during tokenization, we sample patterns with lengths $[l_1, l_2, ...]$ instead of using a fixed length l. By default, the multi-scale lengths are set to [2, 4, 6, 8]. Note the number of patterns remains unchanged."}, {"title": "3.5. How Does GPM Surpass Message Passing?", "content": "Advancing Expressivity. Message passing frameworks can distinguish non-isomorphic graphs that are distinguishable by the 1-WL isomorphism test (Xu et al., 2019). We demonstrate that GPM surpasses message passing in expressiveness under the following mild assumptions: (1) the graphs are connected, unweighted, and undirected, and (2) the number of sampled patterns is sufficiently large."}, {"title": "Theorem 3.4.", "content": "GPM can distinguish non-isomorphic graphs given a sufficient number of graph patterns."}, {"title": "Theorem 3.5.", "content": "For k \u2265 1, there exist graphs that can be distinguished by GPM using walk length k but not by the k-WL isomorphism test."}, {"title": "4. Experiments", "content": null}, {"title": "4.1. Applicability to Graph-based Tasks", "content": "Node Classification. We conduct experiments on benchmark datasets of varying scales, with their statistics and homophily ratios summarized in Table 1. The datasets include Products, Computer, Arxiv, WikiCS, CoraFull, Deezer, Blog, Flickr, and Flickr-S (Small). We adopt the dataset splits from Chen et al. (2023) and Chen et al. (2024): public splits for WikiCS, Flickr, Arxiv, and Products; 60/20/20 train/val/test split for CoraFull and Computer; 50/25/25 split for the remaining datasets. Accuracy is used as the evaluation metric. The baselines include message passing GNNs (GCN, GAT, APPNP, GPR-GNN, OrderedGNN), random walk-based GNNs (RAW-GNN, RUM), and graph transformers (GraphGPS, SAN, NodeFormer, GOAT, NAGphormer, GraphMamba, VCR-Graphormer, and GCFormer). As presented in Table 1, our GPM consistently outperforms"}, {"title": "Link Prediction.", "content": "We evaluate the link prediction performance on three datasets: Cora, Pubmed, and ogbl-Collab. Following Guo et al. (2023), we split the edges into 80/5/15 train/val/test sets and use Hits@20 for Cora and Pubmed, and Hits @50 for ogbl-Collab for evaluation. The baselines include GCN, LLP, RUM, NodeFormer, and NAGphormer. The results, along with dataset statistics, are summarized in Table 2. Notably, graph transformer methods such as NodeFormer and NAGphormer do not achieve competitive performance and, in some cases, perform worse than message passing GNNs. This discrepancy may arise from the inductive bias provided by message passing, which iteratively updates node representations based on their neighborhoods and is inherently well-suited for modeling connectivity between nodes. Our GPM achieves the best performance across all evaluated datasets. This superior performance is likely due to the effectiveness of the captured patterns in accurately reflecting the connectivity between nodes."}, {"title": "Graph Classification and Regression.", "content": "We evaluate the model on six graph datasets: social networks (IMDB-B, COLLAB, Reddit-M5K, Reddit-M12K) for classification and molecule graphs (ZINC and ZINC-Full) for regression. We use 80/10/10 train/val/test splits for social networks, and the public splits for molecule graphs. The baselines include message passing GNNs (GIN, PNA, GNN-AK), graph kernels (DGK), random walk-based methods (AWE, CRaWl, AgentNet, RUM), and graph transformers (GMT, SAN, Graphormer, GPS, SAT, DeepGraph, GraphViT, GEANet). The results, along with dataset statistics, are presented in Table 3. We observe that graph transformers generally outperform message passing GNNs, likely due to their superior capability in modeling long-range dependencies. Notably, our GPM consistently outperforms all other methods across datasets of varying scales, particularly outperforming methods that also utilize graph patterns as tokens (e.g., GMT, SAT, GraphViT). This might because these methods still rely on message passing as the encoder for individual patterns,"}, {"title": "4.2. Out-of-Distribution Generalization", "content": "We evaluate the model robustness under distribution shifts between training and testing sets. Under the setting, the model is trained on a source graph and evaluated on a target graph, with a 20/80 val/test split. We conduct experiments on citation networks, ACM and DBLP (using accuracy as the metric), as well as social networks Twitch (using AUC as the metric). The Twitch dataset consists of six graphs (DE, EN, ES, FR, PT, RU), where the model is trained on DE and evaluated on the remaining graphs. For baselines, in addition to the methods used in previous experiments, we include domain adaptation and OOD generalization baselines such as DANN, SR-GNN, StruRW, and SSReg. The experimental results are summarized in Table 4. We observe"}, {"title": "4.3. Scalability", "content": "Large Graphs. Each graph instance (e.g., node, edge, or graph) is represented by k patterns, with an encoding complexity of $O(k^2)$ and an overall complexity of $O(nk^2)$ (see Appendix D), where $n \\gg k^2$ denotes the number of instances. This design enables GPM to efficiently scale to large graphs using mini-batch training. We evaluate GPM on large-scale graph datasets, e.g., Products, ogbl-Collab, and ZINC-Full (Tables 1, 2, and 3). GPM achieves competitive performance across these large-scale benchmarks.\nLarge Models. Leveraging the transformer architecture, GPM naturally scales to larger model sizes by stacking additional transformer layers, as illustrated in Figure 4 (Top). Empirically, increasing model parameters enhances performance on large-scale graphs. In contrast, message passing GNNS (GAT in this case) struggle to scale due to the over-smoothing effect. The architectural details of large-scale GPM models are presented in Table 6 in Appendix.\nDistributed Training. Transformers have demonstrated remarkable efficiency in distributed training due to the parallelization of self-attention mechanism (Shoeybi et al., 2019). In contrast, message passing GNNs are less efficient for distributed training, as their iterative message passing introduces sequential dependencies and incurs significant communication overhead when computational nodes are distributed across machines. By leveraging a transformer-based architecture, GPM achieves superior efficiency in distributed training compared to message passing GNNs on PRODUCTS, as shown in Figure 4 (Bottom). Further details and discussions are provided in Appendix G.1."}, {"title": "4.4. Ablation Study", "content": "Impact of Model Components. Table 5 presents the model component ablation study results. Both positional embeddings (PE) and anonymous paths (AP) contribute to capturing topological information, with PE encoding relative node positions and AP characterizing pattern structures. Empirically, AP has a greater impact than PE, suggesting that the model prioritizes understanding pattern structures over"}, {"title": "4.5. Model Interpretation", "content": "GPM leverages self-attention to identify the most relevant patterns for downstream tasks by utilizing a class token to aggregate pattern information. As an illustrative example, Figure 5 presents the 24-th molecule graph from ZINC (colors indicate different atom types) along with its top-9 key patterns, demonstrating that GPM effectively captures topologically significant structures such as stars and rings in molecules. Similarly, Figure 8 in Appendix visualizes the results on COMPUTERS, highlighting that triangle structures are predominant in e-commerce networks."}, {"title": "5. Conclusion", "content": "We propose GPM, a novel graph representation learning framework that directly learns from graph substructure patterns, eliminating the need for message passing. The architecture comprises three key components: a pattern sampler, a pattern encoder, and an important pattern identifier. Extensive experiments across node-, link-, and graph-level tasks demonstrate the effectiveness of GPM, showcasing its superior robustness, scalability, and interpretability. Moreover, GPM offers enhanced model expressiveness and a greater capacity for capturing long-range dependencies."}, {"title": "Boarder Impact", "content": "We introduce a novel graph representation learning framework that goes beyond the traditional message passing. By design, our GPM addresses inherent challenges in message passing, such as restricted expressiveness and over-squashing. This capability enables our framework to better scale to complex tasks requiring large models or involving large graphs. Also, GPM provides an interface to enable model interpretation, facilitating the applications requiring clear insights on graph topology, such as social network analysis and drug discovery. Potential ethical considerations include the misuse of the technology in sensitive domains, such as surveillance or profiling, and the reinforcement of biases present in the training data."}, {"title": "A. Limitations of Message Passing and Recent Advances", "content": "The message passing paradigm (Kipf & Welling, 2017; Hamilton et al., 2017; Veli\u010dkovi\u0107 et al., 2018; Wang et al., 2023; 2024b;a; Zhang et al., 2024b;c; Wang et al., 2024c) in GNNs has well-documented limitations, including restricted expressiveness, over-smoothing, over-squashing, and an inability to effectively model long-range dependencies. Given our focus on learning graph patterns, this discussion emphasizes the issues of expressiveness and notable advancements. Pioneering work by Xu et al. (2019) revealed that the expressive power of GNNs is fundamentally bounded by the 1-WL isomorphism test. Building on this, Corso et al. (2020) demonstrated that no GNN employing a single aggregation function can achieve the expressiveness of the 1-WL test when the neighborhood multiset has uncountable support. Consequently, this constraint renders GNNs incapable of identifying critical graph structures, such as stars, conjoint cycles, and k-cliques (Chen et al., 2020; Garg et al., 2020; Zhang et al., 2024a).\nTo address the expressiveness limitations of GNNs, three primary strategies have emerged. The first focuses on developing expressive GNNs that enhance the message passing framework to surpass the constraints of the 1-WL test. For instance, Maron et al. (2019b); Chen et al. (2019); Maron et al. (2019a) introduced k-order WL GNNs to emulate the k-WL test within GNN architectures. Similarly, Alsentzer et al. (2020); Bouritsas et al. (2022); Bodnar et al. (2021); Zhao et al. (2022) proposed advanced message passing mechanisms capable of detecting substructures in graphs, while Murphy et al. (2019); Loukas (2020) incorporated node positional embeddings to boost the representational power of message passing GNNs. Despite their increased expressiveness, these approaches often suffer from computational inefficiency (Azizian & marc lelarge, 2021).\nAn alternative approach leverages random walk kernels to guide the message passing process, constraining interactions to a limited range of nodes (Jin et al., 2022; Martinkus et al., 2023; T\u00f6nshoff et al., 2023; Wang & Cho, 2024). Notably, T\u00f6nshoff et al. (2023) demonstrated that random walk-based methods can capture both small substructures and long-range dependencies, while Wang & Cho (2024) showed that sufficiently long random walks can distinguish non-isomorphic graphs. Despite their strengths, these methods tend to emphasize long-range dependencies at the expense of localized information (T\u00f6nshoff et al., 2023) and lack interpretability regarding the specific graph knowledge being learned.\nLastly, graph transformers (GTs) (Kreuzer et al., 2021; Ying et al., 2021; Dwivedi & Bresson, 2020; Rampasek et al., 2022; He et al., 2023; Chen et al., 2022) have emerged as a compelling alternative to traditional message passing GNNs. Leveraging a global attention mechanism, GTs can capture correlations between any pair of nodes, enabling effective modeling of long-range dependencies. Both theoretical and empirical studies (Kreuzer et al., 2021; Ying et al., 2021) demonstrate that, under mild assumptions, graph transformers surpass the expressive power of WL isomorphism tests. This represents a fundamental advantage over message passing GNNs in terms of expressiveness. However, the quadratic complexity of all-pair node attention poses significant computational challenges, limiting the applicability of GTs to smaller graphs, such as molecular graphs (Wu et al., 2023).\nCrucially, the aforementioned methods primarily focus on developing advanced message passing frameworks rather than directly encoding graph patterns. As a result, they may still inherit the limitations associated with message passing."}, {"title": "B. Implementation Details", "content": null}, {"title": "B.1. Environments", "content": "Most experiments are conducted on Linux servers equipped with four Nvidia A40 GPUs. The models are implemented using PyTorch 2.4.0, PyTorch Geometric 2.6.1, and PyTorch Cluster 1.6.3, with CUDA 12.1 and Python 3.9."}, {"title": "B.2. Training Details", "content": "The training of transformers is highly sensitive to regularization techniques. In our setup, we use the AdamW optimizer with weight decay and apply early stopping after 100 epochs. Label smoothing is set to 0.05, and gradient clipping is fixed at 1.0 to stabilize training. The learning rate follows a warm-up schedule with 100 warm-up steps by default.\nAll experiments are conducted five times with different random seeds. The batch size is set to 256 by default."}, {"title": "B.3. Model Configurations", "content": "We perform hyperparameter search over the following ranges: learning rate {1e-2, 5e-3, 1e-3}, positional embedding dimension {4, 8, 20}, dropout {0.1, 0.3, 0.5}, weight decay {1e-2, 0}, and weighting coefficient $\\lambda \\in {0.1, 0.5, 1.0}$. For pattern sampling, we set p = 1, q = 1 by default (see Appendix H for details). The model configuration includes a hidden dimension of 256, 4 attention heads, and 1 transformer layer."}, {"title": "B.4. Architectures in Model Scaling Analysis", "content": "In our scalability analysis (Section 4.3), we evaluate the performance of GAT and GPM across different model scales. The detailed model architectures and corresponding parameter counts are provided in Table 6."}, {"title": "C. Proof", "content": null}, {"title": "C.1. Proof of Proposition 3.2", "content": "We prove the proposition by introducing the following theorem first."}, {"title": "Theorem C.1", "content": "Given a graph G = (V, E), one can reconstruct B(v, k) = (V', E'), where n = |V'|, m = |E'|, the ego-graph induced by node v \u2208 V with k radius, via an anonymous walk distribution $D_l$, where l = O(m) starting at node v."}, {"title": "C.2. Proof of Proposition 3.3", "content": "As discussed, any graph pattern can be represented as a combination of a semantic path, which captures semantic information (i.e., the specific nodes forming the pattern), and an anonymous path, which encodes topological structure (i.e., the overall pattern structure). To effectively extract both types of information, these two paths can be encoded separately, preserving semantic meaning and structural insight independently.\nTo ensure lossless compression, we employ bijective mappings to project these paths, guaranteeing that distinct paths maintain unique representations. Given a semantic path w and its corresponding anonymous path $\\phi$, we introduce two"}, {"title": "C.3. Proof of Theorem 3.4", "content": "We outline the proof by first (1) establishing the expressiveness of a simplified variant of GPM and (2) extending this result to the general case.\nThe learning process of GPM consists of three key steps: (1) Extracting n patterns using l-length random walks, where each pattern is uniquely defined by its semantic path w and anonymous path $\\phi$. (2) Encoding the semantic and anonymous paths separately using neural networks and combining their representations. (3) Passing the encoded graph patterns through a transformer for final predictions.\nTo analyze expressiveness, we consider a simplified variant of GPM with the following modifications: (1) Setting n = 1. (2) Replacing neural networks with universal and injective mappings $\\rho_s$ and $\\rho_a$. (3) Using a mean aggregator over encoded patterns instead of a transformer.\nUnder this setting, the model is essentially trained on a single l-length path w. Additionally, we impose the following mild assumptions:"}, {"title": "Assumption C.2.", "content": "The graphs are connected, unweighted, and undirected."}, {"title": "Assumption C.3.", "content": "The walk length l is sufficiently large."}, {"title": "Theorem C.4", "content": "Up to the Reconstruction Conjecture, encoding the l-length random walks (combining semantic path and anonymous path) produces distinct embeddings for non-isomorphic graphs."}, {"title": "C.4. Proof of Theorem 3.5", "content": "The proof follows the same structure as Theorem 3.4: (1) Defining a simplified variant of GPM, (2) Proving the theorem on this simplified model, and (3) Extending the results to the general case.\nTo ensure the proof is self-contained, we reintroduce the design of the simplified variant and its extension to the full model.\nThe simplified variant of GPM includes the following modifications: (1) Replacing neural networks with universal and injective mappings $\\rho_s$ and $\\rho_a$. (2) Using a mean aggregator over encoded patterns instead of a transformer.\nUnder the same assumptions as in Theorem 3.4, namely: (1) The graphs are connected, unweighted, and undirected, and (2) The number of sampled patterns is sufficiently large, we apply the following corollary to directly prove Theorem 3.5 for the simplified case."}, {"title": "Theorem C.5", "content": "Up to the Reconstruction Conjecture, two graphs $G_1$, $G_2$ labeled as non-isomorphic by the k-WL test is the necessary, but not sufficient condition that encoding the k-length random walks sampled from these two graphs produces the same embedding."}, {"title": "D. Complexity", "content": "We analyze the time complexity of three key components: pattern sampling", "encoders": "the mean encoder with complexity 2 \u00d7 O(k), the GRU encoder with complexity 2 \u00d7 O(kl), and the transformer encoder with complexity 2 \u00d7 $O(k \\cdot l^2)$ (note both semantic and anonymous paths should be encoded). The tranformer encoder over encoded graph patterns introduces an additional complexity of $O(k^2)$. Thus, the maximum total complexity per instance is\n$O(kl) + 2 \\times O(k \\cdot l^2) + O(k^2) \\approx O(k^2).$\nWe compare this complexity to that of existing graph transformers for both node-level and graph-level tasks. For node classification, GPM's time complexity for encoding all nodes in a graph is $O(n \\cdot k^2)$, where n is the number of nodes, and $k^2 <"}]}