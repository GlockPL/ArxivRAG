{"title": "Enhancing Bidirectional Sign Language Communication: Integrating YOLOv8 and NLP for Real-Time Gesture Recognition & Translation", "authors": ["Hasnat Jamil Bhuiyan", "Mubtasim Fuad Mozumder", "Md. Rabiul Islam Khan", "Md. Sabbir Ahmed", "Nabuat Zaman Nahim"], "abstract": "The primary concern of this research is to take American Sign Language (ASL) data through real time camera footage and be able to convert the data and information into text. Adding to that, we are also putting focus on creating a framework that can also convert text into sign language in real time which can help us break the language barrier for the people who are in need. In this work, for recognising American Sign Language (ASL), we have used the You Only Look Once(YOLO) model and Convolutional Neural Network (CNN) model. YOLO model is run in real time and automatically extracts discriminative spatial- temporal characteristics from the raw video stream without the need for any prior knowledge, eliminating design flaws. The CNN model here is also run in real time for sign language detection. We have introduced a novel method for converting text based input to sign language by making a framework that will take a sentence as input, identify keywords from that sentence and then show a video where sign language is performed with respect to the sentence given as input in real time. To the best of our knowledge, this is a rare study to demonstrate bidirectional sign language communication in real time in the American Sign Language (ASL).", "sections": [{"title": "I. INTRODUCTION", "content": "Sign language is a visual language expressed through physical movements instead of spoken words. Hands, eyes, facial emotions, and movement are all used as visual clues in this language. Like any other language, sign language has its own grammatical rules and linguistic structures. However, use of sign language still isn't a common practice across the globe. According to the World Health Organization (WHO), over 1.5 billion people in the world live with hearing loss [21]. Despite such a huge chunk of the global population suffering from the issue, only a minority of people know how to effectively communicate with them due to lack of awareness about sign language.There are more than 300 sign languages present in different regions of the world [21]. Since English is the language mostly used throughout the world, our work will be based on American Sign Language (ASL).\nThe objectives of this work are breifly discussed below:\n\u2022 Creating a framework based on natural language pro- cessing (NLP) that can represent accurate sign language gestures and movements in real time when text input is provided, allowing hearing people to communicate with sign language users.\n\u2022 Developing real time YOLO and CNN models that will recognize and translate sign language gestures into text.\n\u2022 Assessing the performance and accuracy of the suggested models by using real-world sign language data in rigorous testing and validation and taking into account aspects such as recognition accuracy, precision, translation quality, speed, and robustness.\n\u2022 Evaluating the effectiveness of the suggested framework and models."}, {"title": "II. RELATED WORKS", "content": "Here are some of the selected research works that inspired us to start working on our thesis topic in depth. First we will discuss the works related to text to sign language translation. In [1],a unique approach to translating English sentences to Indian Sign Language (ISL) is seen. Their proposed system takes a text input and converts it to ISL with the help of Lexical Functional Grammar (LFG). In [6], an approach to transform Malayalam text to Indian Sign Language using animation for displaying is seen. Their system uses the Hamburg Notation System shortly known as HamNoSys for representing signs. Moreover, The authors in [7] used an approach for converting Greek text to Greek sign language. Translation is done using Vsigns, a web tool used for the synthesis of virtual signs. A system is proposed in [8] where text in English language is taken as input and then translated to HamNoSys representation. This is afterward converted into SiGML. A mapping system is used to link the text to the HamNoSys notation. This work may not be a direct example of text- to-sign language conversion which we expect. However, this provides us with insights into converting text to a signed notation system. Similar research works were done in [9] and [16]. Furthermore,in [17], the authors proposed a machine translation model that takes both example based and rule- based Interlingua approaches to convert Arabic Text to Arabic Sign Language. Another work of Arabic Sign language for the deaf is presented in [19]. Adding to that,in [13], a text-to-sign language conversion system for Indian Sign Language (ISL) is made which takes into account the language's distinctive alphabet and syntax. The system accepts input in alphabets or numerals only.\nNow, we will discuss the works related to sign language recognition. In [3], the authors attempted to recognize the English alphabet and gestures in sign language and produced the accurate text version of the sign language using CNN and computer vision.In [2], the researchers worked on reviewing multiple works on the recognition of Indian Sign Language (ISL). Their review of works on Histogram of Orientation Gradient(HOG), Histogram of Edge Frequency(HOEF) and Support Vector Machine (SVM) gave us meaningful insights. A similar work is seen in in [20]. Furthermore,in [4], the authors worked on Indonesian sign language recognition was done using a YOLOv3 pre-trained model. They used both image and video data. The system's performance was incredibly high during using image data and it was comparatively low while using video data. A similar work was done in [5] using YOLOv3 model.From [10], we learnt how the researchers worked on making an Italian sign language recognition system that identifies letters of the Italian alphabet in real-time using CNN and VGG-19. The work of the authors in [11] and in [14], was insightful about how deep learning works on sign language detection. Moreover in [18], the authors developed an Android app that can convert real-time ASL input to speech or text where SVM was used to train the proposed model. Additionally in [12], we were introduced to the idea of using surface electromyography (sEMG), accelerometer (ACC), and gyroscope (GYRO) sensors for subword recognition in Chinese Sign Language. Lastly in [15], the authors worked on a sign language-to-voice turning system that uses image processing and machine learning."}, {"title": "III. METHODOLOGY", "content": "Our work is mainly divided into two parts:\n1) Text to sign language conversion\n2) Sign language to text conversion"}, {"title": "A. Text to Sign Language", "content": "We have implemented spoken language sentences to sign language translation using Natural Language Processing (NLP). The framework utilizes the Natural Language Toolkit (nltk) for part-of-speech tagging to identify the tense of the input sentence. A dictionary is used to hold verb counts for various tenses, and a list of predefined stop words is utilized for filtering. After the words have been lemmatized, particular tense-indicating words like \"Before,\" \"Will,\" or \"Now\" are added in accordance with the determined tense.\nWe have created our own dataset containing videos of sign language performed corresponding to words,alphabets and digits from 0 to 9. The most frequently used words in the English language are included in this dataset. In total, there are 150 videos. The words will be added to a list after each word in the input sentence and its matching part-of-speech tag have been processed and lemmatized according to their POS tag. For every word in the list, its corresponding video file will be looked for in the dataset. The word will be split up into individual alphabets if there isn't a video available in the dataset. Finally, the processed words and the original sentence will be passed to a html template for rendering video demonstrating sign language performed. The primary benefit of this framework is that it does not require system modifications when a new word is added to the dataset.\nWe have developed a webpage to take input and translate the words into sign language for this part. For this website, the Django framework was used, along with Javascript, HTML, and CSS for the front end. In this website, we have three main pages: HomePage, Sign up and Log-in. The homepage features a navigation bar with links to the Sign Up and Login pages. It also includes a video of sign language representation of the world \"Hello\". Below the video, there is a call-to-action button labeled \"Click to Start.\" This button directs the user to the Log-in page. The new users can create accounts in the sign up page using necessary credentials like username and password. Users are redirected to their personal dashboard, which includes a text input field, after logging in. It is possible to provide textual or audio input. The sentence is divided into key words when input is received. Users can submit audio input by speaking sentences aloud by using the microphone icon button located beneath the text input box. The website translates spoken words into text by using the JavaScript Web Speech API for speech recognition. Users can click the \"Submit\" button after they have typed their sentence or provided audio input. This action triggers the processing of the input, extracting keywords from the sentence, and generating the corresponding sign language representation. A list of keywords extracted from the submitted sentence is shown beneath the text input area. On the right side of the dashboard, there is a play/pause button. Users can click on that button to watch the videos demonstrating the sign language representation of each keyword."}, {"title": "B. Sign Language to Text", "content": "For our sign language detection, we have developed YOLOv5 and v8 models, YOLOv8 is a state-of-the-art model in the ongoing time. This YOLO model is an upgraded version of its predecessors. It uses image and video data to train and can detect the gestures and other detection scenarios with exceptional accuracy. YOLOv8 uses a custom data loader mosaic which helps the data to be loaded into the model while training and testing the raw data. However, the data collection is more likely the same with v5 and v8 models as both have a similar architectural pattern. Both of these models are compatible with multiple data collection styles. We used bounding box prediction data style as it was more suitable for us as our hands move constantly and we will have to deal with more than one shape and aspect ratios. Therefore, we added our hand gestures with bounding boxes as it can be easily manipulated and the end result was satisfactory for both of the models."}, {"title": "IV. EVALUATION OF RESULTS (YOLO MODELS AND CNN MODEL)", "content": "From the table in fig. 15 it is visible that although the CNN model shows a high accuracy, sometimes it makes false detection. The model is unable to always perfectly identify the hand signs where both hands have been used to perform the sign despite providing enough data. So, it is difficult for this model to identify or recognize comparatively complex hand sign gestures. After seeing its performance, we have come to the conclusion that this model is not suitable for large scale work.\nThe YOLO models on the other hand have performed much better than the CNN model. Though the YOLOv5 model shows a little bit less accuracy than the CNN model, it does not do any type of false detection. The YOLOv8 model also does not have any case of making any wrong detection. The YOLOv8 model functions better than the YOLOv5 model. YOLOv5 shows an accuracy of 92.8 percent after training the model for 100 epochs but the YOLOv8 model shows an accuracy of 95.6 percent after training the model for just 50 epochs. Therefore it is evident that the YOLOv8 model is the best one here for real time sign language detection out of all the models used."}, {"title": "V. CONCLUSION", "content": "In the text to sign language conversion framework, there are certain sentences that contain stop words (For example \u2013 apostrophe) that we utilized for filtering are not compatible with the framework. One further drawback is that the video outputs are not smooth enough. We want to work on these problems in the future and also incorporate a 3D model with smoother transitions.Adding to that, we aim to work with video data on the YOLOv8 model. If we get enough resources to increase the computational power, we will work on a large dataset where there will be hundreds of classes and add also facial expression recognition in our work. In addition to that, we plan to make an app version of this model and framework too. To top it off, we state that our work here on ASL detection can also be applied to other sign languages as well.According to the World Health Organization (WHO), with 1.5 billion people in the world already suffering from hearing loss and the number can increase to over 2.5 billion by 2050. [21] The deaf community is deprived of basic human rights like health care, education and even minimum wage jobs simply because of their inability to communicate with the hearing people using spoken language. Our YOLO based model and the NLP based framework aim to bridge this communication gap that is prevalent in the community for a long time by providing the fastest real time solution. This will ensure an equal spot for the deaf people in the society by overcoming the language barrier. In conclusion, our system will be helpful for both hearing and hearing impaired people to communicate effectively with one another by shortening the existing communication gap."}]}