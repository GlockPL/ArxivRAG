{"title": "GQWformer: A Quantum-based Transformer for Graph Representation Learning", "authors": ["Lei Yu", "Hongyang Chen*", "Jingsong Lv", "Linyao Yang"], "abstract": "Graph Transformers (GTs) have demonstrated significant ad-vantages in graph representation learning through their globalattention mechanisms. However, the self-attention mecha-nism in GTs tends to neglect the inductive biases inherentin graph structures, making it chanllenging to effectively cap-ture essential structural information. To address this issue, wepropose a novel approach that integrate graph inductive biasinto self-attention mechanisms by leveraging quantum tech-nology for structural encoding. In this paper, we introduce theGraph Quantum Walk Transformer (GQWformer), a ground-breaking GNN framework that utilizes quantum walks on at-tributed graphs to generate node quantum states. These quan-tum states encapsulate rich structural attributes and serve asinductive biases for the transformer, thereby enabling the gen-eration of more meaningful attention scores. By subsequentlyincorporating a recurrent neural network, our design ampli-fies the model's ability to focus on both local and global in-formation. We conducted comprehensive experiments acrossfive publicly available datasets to evaluate the effectivenessof our model. These results clearly indicate that GQWformeroutperforms existing state-of-the-art graph classification al-gorithms. These findings highlight the significant potentialof integrating quantum computing methodologies with tra-ditional GNNs to advance the field of graph representationlearning, providing a promising direction for future researchand applications.", "sections": [{"title": "Introduction", "content": "Inspired by the celebrated success of Transformers in mod-eling structured data across various domains, researchershave begun to explore the application of Transformers tograph data (Rong et al. 2020). Unlike traditional attentionnetworks, which aggregate node features solely from adja-cent nodes, Graph Transformers (GTs) consider pairwiserelationships between all nodes within a graph. This approachallows each node to directly attend to every other node,thereby capturing long-range dependencies and interactionsfrom distant nodes, significantly enhancing their expressivepower.\nHowever, the self-attention mechanism in GTs often ne-glects the inherent inductive biases of graphs, especiallythose related to graph topology. The absence of structural in-formation consideration leads to indiscriminate attention toall nodes, thereby failing to discern the instrinsic differenceswithin the graph. To mitigate this, numerous studies havecentered on encoding positional or structural information tomodel these inductive biases, thereby bolstering the Trans-former's capacity to handle graph data (Ying et al. 2021;Chen, O'Bray, and Borgwardt 2022; Yeh, Chen, and Chen2023). While significant strides have been made, the full po-tential of integrating graph structure information with nodefeatures remains untapped. Notably, as illustrated in Figure1, for structurally identical graphs with different features,their attribute-aware structural encodings should vary, butexisting methods fail to model this differentiation. In addi-tion, the over-globalizing problem in GTs (Xing et al. 2024)indicates that attention patterns, when indiscriminately aug-mented with globalizing properties, can lead to the accumu-lation of redundant information from distant nodes. There-fore, it is imperative to focus more on the local nuanceswithin the graph.\nIn order to resolve the aforementioned issues in GTs, wepropose Graph Quantum Walk Transformer (GQWformer),an innovative framework that integrates GTs with intrin-sic inductive bias by leveraging Quantum Walks (QWs) toencode the structural information of graphs. Specifically,GQWformer comprises two major components: the GraphQuantum Walk Self-attention Module (GQW-Attn) and theGraph Quantum Walk Recurrent Module (GQW-Recu). Ourapproach begins with the execution of attribute-aware QWson a node-attributed graph. This encoding approach is sen-titive to both the topological and contextual informationof the graph, capturing a comprehensive representation ofthe graph's structural attributes. The learned QW encodingsare then utilized by GQW-Attn to measure the correlationsbetween node pairs. By directly incorporating QW encod-ings into the attention biases, GQW-Attn ensures precisecontrol over each node's attention weights relative to othernodes. This integration of QW encodings effectively embedsthe graph's structural attributes into the GT model. Sub-sequently, GQW-Recu processes the complete sequencesdrived from the QWs, preserving and effectively utilizingthe temporal and sequential dependencies to learn robustembedding for target nodes, which enhances the local pro-cessing power of the model. Additionally, the length of theQWs dictates the extent of each node's interactions, allow-ing for a flexible adjustment of the model's receptive field."}, {"title": "Related Work", "content": "Graph Transformers. The transformer architecture havebeen applied to graph modeling, leading to the proposal ofvarious encoding strategies. These strategies include lapla-cian eigenfunctions (Kreuzer et al. 2021), node degree cen-trality (Ying et al. 2021), kernel distance (Mialon et al.2021), shortest paths (Ying et al. 2021), random walks (Yeh,Chen, and Chen 2023) and structure-aware methods (Chen,O'Bray, and Borgwardt 2022). Additionally, The issue ofover-globalizing in GTs suggests the necessity of integratinglocal modules. Consequently, multiple studies (Zhang et al.2022; Wu et al. 2022; Liu et al. 2023; Yeh, Chen, and Chen2023; Kong et al. 2023; Wu et al. 2024) have been conductedto capture local information either implicitly or explicitly.\nQuantum Graph Learning. Recently, quantum comput-ing, an emerging research domain, has demonstrated sub-stantial potential in machine learning, including graph learn-ing (Liu, Arunachalam, and Temme 2021; Tang, Yan,and Edwin 2022). With unique principles like superposi-tion and entanglement, quantum computing redefines tra-ditional paradigms of information understanding and pro-cessing. When applied to graph data, these principles facili-tate effective graph feature characterization within a high-dimensional Hilbert space (Cong, Choi, and Lukin 2019;Schuld and Killoran 2019), enabling the extraction of un-detectable and atypical graph patterns (Huang et al. 2021).Indeed, quantum computing offers a novel paradigm for han-dling graph-structured data, thereby revolutionizing graphlearning (Yu et al. 2023). Quantum walks (QWs), the quan-tum equivalent of their classical counterparts, serve as a uni-versal model for quantum computing. These walks providea powerful method for encoding graph data using qubits andrepresenting them with quantum states, proving effective ingraph learning (Zhang et al. 2019; Bai et al. 2021; Yan, Tang,and Yan 2022)."}, {"title": "Preliminary", "content": "In this paper, we focus on node-attributed graphs. Let G =(V, E) denote a graph with an adjacency matrix $A \\in${0,1}$^{n\\times n}$ and a node feature matrix $X \\in R^{n\\times d}$. In the fol-lowing, Scalars or elements of a set are denoted by italiclowercase letters, vectors are denoted by boldface lowercaseletters and matrices are represented by boldface captical let-ters."}, {"title": "Transformer", "content": "Recent advancements have leveraged the Transformer archi-tecture (Vaswani et al. 2017) to aggregate node features us-ing the self-attention mechanism. The self-attention mecha-nism is mathematically defined as follows:\n$a_{ij}^{(l)} = \\frac{\\exp(\\frac{q_i^{(l-1)}(k_j^{(l-1)})^T}{\\sqrt{d}})}{\\Sigma_{v \\in V} \\exp(\\frac{q_i^{(l-1)}(k_v^{(l-1)})^T}{\\sqrt{d}})}$\n$h_i^{(l)} = \\Sigma_{j \\in V} a_{ij}^{(l)} v_j^{(l-1)}$\n$\\widehat{h}_i^{(l)} = \\gamma a_i^{(l)} + h_i^{(l-1)} $          (1)\nHere, the node embedding of $v_i$ at the l-th layer, denotedas $h_i^{(l)}$, is aggregated from the embeddings of all nodes inthe graph. The term $h_i^{(l-1)}$ represents the node embeddingfrom the previous layer. To compute the self-attention, eachnode embedding $h_i^{(l-1)}$ is projected into three distinct vec-tors: the query vector $q_i^{(l-1)}$, the key vector $k_i^{(l-1)}$, and thevalue vector $v_i^{(l-1)}$, using the projection matrices $W^Q$, $W^K$and $W^V$, respectively. The resultant attention vector $a_i^{(l)}$ isthen combined with the previous layer's embedding $h_i^{(l-1)}$,scaled by a factor \u03b3, to form the new node embedding $h_i^{(l)}.$"}, {"title": "Quantum Computing", "content": "In quantum computing, the basic unit of information is aqubit. Two possible states for a qubit are the computationalbasis states |0) = (1,0) and |1) = (0, 1). Notation like '|)'is called bra-ket notation. A qubit is also possible to formlinear combinations of states, often called superpositions:\n|\u03c8} = \u03b1|0} + \u03b2|1},                                  (2)\nwhere the number a and \u03b2 are complex numbers that satisfy||a||2 + ||3||2 = 1. Formally, a quantum system on n qubitslives in the n-fold tensor product Hilbert space $H = (C)^\\otimes d$with resulting dimension 2d. A quantum state is representedas a unit vector |\u03c8) \u2208 H. A quantum gate is a unitary opera-tion U on H, and the action of a unitary operator is denotedas 2) = UV1). The state space of a composite system isthe tensor product of the state space of its components."}, {"title": "Graph Quantum Walk", "content": "Quantum walks (QWs), introduced by (Aharonov, Davi-dovich, and Zagury 1993), represent a quantum analog ofclassical random walks. Unlike the stochastic evolution seenin classical random walks, QWs evolve through a unitaryprocess. This unitary evolution gives rise to fundamentallydifferent behaviors compared to classical walks, primarilydue to the phenomenon of interference between different tra-jectories of the walker. Such interference can lead to fasterspreading and different probability distributions, which arekey features exploited in various quantum algorithms. Twokinds of QWs have been introduced in the literature, namely,continuous time QWs (Farhi and Gutmann 1998) and dis-crete time QWs (Ambainis 2003). In this paper, we adoptthe discrete time QW framework for general graphs as out-lined in (Kendon 2006).\nGiven a graph G(V, E), where V and E are vertex setand edge set respectively. The formulation of QWs involvesdefining two crucial Hilbert spaces. One is the positionHilbert space $H_p$ which encapsulates the superposition overvarious positions, i.e., nodes, in the graph. Formally, $H_p$ isdefined as the span of the position basis vector {|v), v \u2208 V}.The position vector of a QW can then be expressed as a lin-ear combination of position state basis vectors:\n|\u03c8p} = \u03a3\u03b1vv},                                (3)\nv\u2208V\nwhere {$\u03b1_v,v \\in V$} are the complex amplitudes satisfy-ing the unit $L^2$-norm condition $\\Sigma_{v \\in V} ||\u03b1_v||^2 = 1$, $||\u03b1_v||^2$ is theprobability of finding the walker at vertex v. This formula-tion allows for the QW to be in a superposition of multiplevertices simultaneously, a fundamental aspect of quantummechanics that distinguishes QWs from their classical coun-terparts.\nAnother one is the coin Hilbert space $H_c$ capturing thesuperposition of possible directions in which the walker canmove from each node. Formally, $H_c$ is defined as the spanof the coin basis vectors {|i), i \u2208 [1, ..., d]}, where i indexesthe edges incident to a vertex v and d is the maximum degreeof the graph. The coin state of a QW at vertex v can then beexpressed as a linear combination of coin state basis vectors:\n|\u03c8c} = \u03a3\u03b2vi\u00e9s,                         (4)\ni\u2208 [1,...,d]\nwhere the coefficients {$\u03b2_{v,i}, i \\in [1, ..., d]$} satisfy the unit$L^2$-norm condition $\\Sigma_{i} ||\u03b2_{v,i}||^2 = 1$. If a measurement isperformed on the coin state of the walker at vertex v, thequantity $|| B_{v,i} ||^2$ represents the probability of observing thewalker at node v and selecting the i-th neighbor of v for itssubsequent move. Overall, the comprehensive state space ofthe QW is described as the tensor product of the position andcoin Hilbert spaces H = $H_p \\bigotimes H_c$.\nThe time-evolution of QW over a graph is governed bytwo key unitary operators: the coin operator and the shiftoperator. Let $`\\psi_t> = `\u03c8_p> `\u03c8_c>` denote the state of thewalker at time t. A single step in the QW process involvesthe sequential application of these two operations. Firstly,the coin operator C is applied, which operates exclusivelyon the coin Hilbert space, effecting a unitary transformationof the coin state at each vertex. This transformation can beexpressed as:\n`\u03c8t+1> = (I \u2297 C)(`\u03c8p> \u2297 `\u03c8c>),                          (5)\nwhere I denotes the identity operator. Subsequently, a uni-tary shift operator S is applied to swap the states of verticesconnected by edges, thus facilitating the walker's movementacross the graph. The shift operator acts on both the coin andposition Hilbert spaces. Formally, this step can be describedas:\n{$t+1> = \\4+1) & \\vt+1) = S(|V) & \\vt+1)).                   (6)\np\nIn shorthand notation, the unitary evolution of the walk isgoverned by the composite operator U = S(I\u2297C). Ifthe initial state of the QW on the graph is denoted by 6\u00ba),then after t time steps, the state of the walks is described by|$t> = Ut|6\u00ba).\nIt's essential to note that QWs diverge significantly fromclassical random walks, primarily due to the pronounced in-fluence of the initial superposition and the coin operator.These elements introduce additional degrees of freedom, en-abling deep learning techniques to more effectively fit datathrough a controlled diffusion process."}, {"title": "Method", "content": "This subsection presents the methodology of GQWformer,designed to capture global structural information while em-phasizing local details within graphs. The central concept ofthis approach is to harness the power of QWs on attributedgraphs to model and interpret the intricate interactions be-tween nodes, thereby producing highly informative encod-ings. Figure 2 provides an illustrative overview of the GQW-former architecture, We will now delve into the specifics ofeach component."}, {"title": "Attribute-aware Graph Quantum Walk", "content": "We employ the framework of multiple non-interacting QWson arbitrary graphs, as introduced in (Rohde et al. 2011), tofacilitate the learning of structural encodings in graph data.\nConsider a graph with n vertices, the QW process in-volves n separate, non interacting walks running in paral-lel, with each walk originating from a unique node in thegraph. Suppose the maximum degree of the graph is d,the initial state is encapsulated in the superposition tensor|60) \u2208 Cnxnxd, which represents the collective state of then walkers. The evolution of the QW can be decomposed intoa sequence of discrete steps. At each step t, the current su-perposition tensor |\u00f8t) is updated using a set of coin opera-tors, following by swapping the states along the edges of thegraph.\nAs previously mentioned, coin operators are pivotal inmodifying the spin state of the QW, thereby governing theevolution of this non-classical walk process over the graph.These coin operators can exhibit spatial variability acrossdifferent nodes in the graph or temporal variability alongthe steps of the walk. However, the basic QW frameworkdoes not inherently account for node feature information. Toaddress this, we can learn a function that generates distinctcoin operations at each node, based on the features of neigh-boring nodes. This ensures that even structurally identicalgraphs can produce different walks if their node features dif-fer. Consequently, learning QWs on a graph is achieved bylearning the appropriate coin operators. In general, the func-tion that generates the coin operators could be an arbitraryfunction that yields a valid coin operator C \u2208 Cd\u00d7d.\nIn this paper, we focus on the use of elementary uni-tary matrices for the coin operators. These matrices take theform:\n$U = I - 2ee/(ee^T)$,               (7)\nwhere I represents the identity matrix and e is an arbitraryvector. This specific form of unitary matrix can be computedefficiently during the forward pass of the neural network andits gradients can similarily be computed efficiently duringbackpropagation, making it well-suited for integration intolearning algorithms. For a given node $v_i$, the coin operatorcan be generated as follows:\n$C_i = I - 2g(v_i)g(v_i)^T/(g(v_i)+g(v_i)^T)$.                           (8)\nHere, g($v_i$) is a function related to the features of node $v_i$.This approach ensures that the coin operators are adaptivelylearned based on the node features, thereby enabling the QWprocess to capture both structural and feature-based informa-tion within the graph.\nInspired by graph attention networks and diverging fromthe approach presented in (Dernbach et al. 2019), we propose a novel function g($v_i$) designed to compute attentionscores between the node $v_i$ and each of its neighbors. Thefunction is designed as:\n$g(v_i) = a(WX_{N(v_i)}, WX_a)$,         (9)\nwhere $X_{N(v_i)}$ denotes the features matrix of the neighborsof $v_i$, $X_a$ is a matrix with d rows, and each row is the featurevector of $v_i$ itself, $W \\in C^{F\\times F'}$ is a learnable weight matrixthat linearly transforms the feature vectors of nodes, and a isan attention function that computes the attention scores. Byincorporating these elements, our proposed function g($v_i$)can dynamically evaluate the influence of each neighbor and"}, {"title": "Graph Quantum Walk Transformer", "content": "While Transformer excel at modeling the global informa-tion, they often struggle to capture essential graph structuralinformation. Here we extract structural infromation usingquantum method. Building upon this foundation, we introduce GQWformer, which extends the original GT architec-ture by incorporating these QW encodings, thereby intro-ducing a potent inductive bias into the self-attention mech-anism. Recognizing that the GQW-Attn module may over-look certain local structure information, we further introducethe GQW-Recu module to supplement the focus on localstructural details."}, {"title": "Graph Quantum Walk Self-attention Module", "content": "Compared to conventional GNNs, Transformer offers a moregeneralized framework by meticulously analyzing the pair-wise correlations between every node. One of the significantadvantages of Transformers is their global receptive field,which allows each node to attend to the information fromany other node in the graph, thus enabling a comprehensive processing of node representations. However, the self-attention mechanism in Transformers primarily focuses oncalculating the semantic similarity between nodes, often ne-glecting the crucial topological information inherent in thegraph. Therefore, to fully harness the power of Transformersin graph-based tasks, it is imperative to effectively integratethe structural information of graphs into the model.\nWith the sequence {M\u00ba, M\u00b9, ..., MT} obtained in theprevious section, we interpret M\u00b9 as a structural encodingmatrix, where the (i, j)-th element of M\u00b9, denoted as $p_{ij}$,encodes the distance between node $v_i$ and node $v_j$. Asprevious described, this distance is a trainable parameter thatis sensitive to both the structural and attribute informationof the graph. The proposed QW encoding serves as a biasterm in the attention module, thereby incorporating an at-tention bias into the attention score calculation. Specifically,Graph Quantum Walk Self-attention Module (GQW-Attn) isformulated as:\n$a_{ij}^{(l)} = \\frac{\\exp(\\frac{q_i^{(l-1)}(k_j^{(l-1)})^T}{\\sqrt{d}} + p_{ij})}{\\Sigma_{v \\in V} \\exp(\\frac{q_i^{(l-1)}(k_v^{(l-1)})^T}{\\sqrt{d}} + p_{iv})}$         (10)\nBy incorporating the QW encoding as an attention bias, theGQW-Attn ensures that the attention mechanism not onlycaptures the semantic similarity but also respects the topo-logical structure of the graph, leading to more expressive androbust node representations.\nAs the parameter T increases, the QW extends its explo-ration to larger regions of the graph. Consequently, $M^T$ encodes more valuable information between node pairs com-pared to the initial state. This extended exploration allowsthe model to capture long-range dependencies and intricatestructural relationships within the graph. The ability todynamically adjust the extent of this exploration via the parame-ter T provides the model with flexibility and adaptability tovarious graph structures and scales."}, {"title": "Graph Quantum Walk Recurrent Module", "content": "The se-quence {M\u00ba, M\u00b9, ..., MT} encapsulates the interaction ofnodes with their neighbors, dictated by the graph's topologi-cal structure and node attributes. Recurrent neural networks(Schuster and Paliwal 1997) form the foundation of manysequence-to-sequence methods, the work of (Huang et al.2019) extends this paradigm to graph domains by introduc-ing graph recurrent networks (GRN), where GCN can be in-terpreted as a special case of GRN. In this vein, we exploreto utilize GRN to learn the quantum embedding for the ini-tial nodes of QWs.\nSpecifically, we take the quantum encoding sequence{M\u00ae, M\u00b9, ..., M} as the primary input. As depictedin Figure 3, the first input should be M, which are the initialstate of the first node, specifically the target node. To precessthis sequence, we employ two sequences of GRU cells, theytake the same input, but run in oppsite directions, one run-ning forward and the other running backward. In this bidi-rectional GRU setup, 6; denotes the learned representationfor the j-th backward cell, while 8; represents the corre-sponding forward cell. We first concatenate ; and ; toform the output of the layer, denoted as oj. Notably, oo is theoutput of the target node, it already combines the informa-tion in the sequence. Subsequently, we apply a pooling oper-ation to merge the outputs {01, ..., or} into a single vector\u014d. This pooling step serves to further distill the informationcontained within the entire sequence. Then we obtain \u00f4 byconcatenating 00 with \u014d. Assuming the output of GQW-Attnis \u00f6, the final embedding of the target nodes is defined as0 = 0 + \u00f6.\nThe pooling operation within GQW-Recu serves to induc-tively learn from the neighboring nodes. Moreover, GQW-Recu capitalizes on the inherent order information of the se-quence, ensuring that temporal and directional dependenciesare thoroughly integrated into the node representations. As aresult, GQW-Recu yield a comprehensive and more infor-mative representation of the target nodes.\nThe integration of GQW-Recu ensures that our model isnot only aware of global structures but also deeply attuned tolocal intricacies, which are vital for accurate graph analysis."}, {"title": "Readout", "content": "Following the methodology outlined in (Yeh,Chen, and Chen 2023), we add a special node to thegraph, and make it connect to all other nodes within thegraph. This virtual node facilitates efficient information ex-change among nodes, thereby enhancing overall perfor-mance (Gilmer et al. 2017). Furthermore, since the virtualnode aggregates information from all nodes in the graph, weadopt its hidden feature as the whole graph embedding andtrain an additional classifier for the downstream tasks."}, {"title": "Experiments", "content": "In this section, we conduct a comprehensive empirical anal-ysis of the proposed GQWformer framework, focusing onits performance in graph classification tasks. We evalu-ate GQWformer using five real-world network datasets,benchmarking its effectiveness against several state-of-the-art GNNs."}, {"title": "Experimental Settings", "content": "Datasets. For graph classification, we test GQWformeron five TUDataset benchmarks (Morris et al. 2020) fromvarious domains, including one biology (i.e., PROTEINS),two chemistry (i.e., MUTAG and PTC), and two social (i.e.,IMDB-B and IMDB-M) datsets.\nBaselines. To demonstrate the effectiveness of our pro-posed method, we compare GQWformer with the fol-lowing 11 baselines: DGCNN (Zhang et al. 2018), IGN(Maron et al. 2018), GIN (Xu et al. 2018), PPGNS (Maronet al. 2019), Natural GN (de Haan, Cohen, and Welling2020), RWNN (Nikolentzos and Vazirgiannis 2020), CRaWl(Toenshoff et al. 2021), CIN (Bodnar et al. 2021), GSN(Bouritsas et al. 2022), GNN-AK (Zhao et al. 2022) andRWC (Yeh, Chen, and Chen 2023).\nImplementation Details. In the task of graph classifica-tion, given the embedding representations of all graphs, anda set of training graphs with labels, the goal is to predictthe labels of the remaining graphs. The classification perfor-mance is measured by the accuracy score.\nWe adhere to the conventional settings outlined in (Yeh,Chen, and Chen 2023) to validate the performance of GQW-former. Specifically, for all experiments, we employ a linearlearning rate scheduler with the end learning rate le-9. Theoptimizer of GQWformer is AdamW with a weight decay of0.01. To mitigate the risk of exploding gradient, we set a gra-dient clipping value to 1. Additionally, the dropout for FFN,GQW-Attn, and GQW-Recu modules is maintained at 0.1.The QW is configured to a length of 4, and the number ofGQWformer blocks is also set to 4.\nIn line with (Yeh, Chen, and Chen 2023), ten-fold cross-validation is employed to select the training and test sets. Foreach dataset, given the entire set of graphs G, we randomlyselect 90% of G as the training set, while the remaining isused as the test set."}, {"title": "Effectiveness Evaluation", "content": "From Table 1, it is evident that our method demonstratesperformance on par with, and often superior to, variousbaseline methods. Specifically, for the chemistry datasets,GQWformer outperforms all baselines by at least 0.5% onMUTAG, and 2.3% on PTC in terms of accuracy. Simi-larly, GQWformer improves the accuracy by 1.3% on thebiological dataset PROTEINS, and for the social networks,it achieves an accuracy increase of 0.5% on IMDB-B and0.8% on IMDB-M. Especially, RWC (Yeh, Chen, and Chen2023) is a recently proposed GNN model, which employsthe RW-SAN to measure the global pairwise correlationsand RW-Conv to carefully analyze the local substructures ofgraphs. Our GQWformer method consistently and signifi-cantly outperforms RWC on all five datasets by a large mar-gin, underscoring its effectiveness and robustness in graphclassification tasks. In summary, the empirical results pre-sented in Table 1 clearly showcase the superiority of GQW-former over existing baseline methods, including the recentRWC model."}, {"title": "Ablation Studies", "content": "We conduct ablation studies to evaluate the importance ofdifferent modules in GQWformer on the PTC dataset, aspresented in Table 2. These studies provide a comprehensiveanalysis of each component's contribution to the overall per-formance of our model. Firstly, we validate the significanceof the GQW-Attn module. The experimental results clearlydemonstrate that incorporating GQW-Attn significantly enhances the performance of GQWformer. The adoption ofGQW-Attn allows the model to effectively capture and uti-lize the structural information inherent in the graph data. Additionally, assigning attention based on QW further ampli-fies the model's performance, highlighting the crucial roleof this mechanism in improving graph representation andensuring a more accurate classification. Next, we investigatethe effectiveness of the GQW-Recu module. GQW-Reculeverages the inherent temporal and sequential dependenciesof the QW sequence to focus on local details, thereby bol-stering the performance of GQWformer. This module's abil-ity to exploit these dependencies facilitates a more nuancedand comprehensive understanding of the graph's local struc-ture, which translates into better classification accuracy. Furthermore, we compare different encoding strategies, includ-ing the vanilla QW, which operates independently of fea-ture information, the invariant QW, which is also attribute-aware, as introduced in (Dernbach et al. 2019), and our pro-posed QW encoding, designed to be more sensitive to thegraph's attribute information. Our encoding strategy provesto be more powerful than both the vanilla and invariant QWencodings on the PTC dataset. This superiority underscores the effectiveness of our approach in capturing the essentialfeatures and relationships within the graph data. In conclu-sion, the ablation studies highlight the indispensable roles ofGQW-Attn and GQW-Recu in enhancing the performanceof GQWformer. Additionally, our proposed QW encodingstrategy demonstrates its effectiveness in graph classification tasks."}, {"title": "Sensitivity Analysis", "content": "In our study, a critical aspect of GQWformer is the evalua-tion of the impact of the QW length on model performance.To thoroughly investigate this, we conducted a series of experiments with varying walk lengths using the PTC dataset,as detailed in Table 3. The GQWformer mechanism bene-fits from longer walk sequences by exploring a greater num-ber of nodes within the graph, thereby potentially capturingmore intricate global structural patterns. However, this in-creased exploration comes with inherent tradeoffs. Specifi-cally, there is a balance to be struck between the enhancedperformance derived from a more comprehensive explo-ration and the computational efficiency that may be com-promised with longer walks. Additionally, there is a delicateequilibrium between the ability to capture global structuralinformation versus local structural details. Our experimentalresults underscore the significance of walk length, demon-strating that it indeed has a substantial impact on model per-formance. Longer walks allow the model to capture moreextensive global structures, but they also risk diluting thefocus on local details and increasing computational costs.These findings highlight the necessity of carefully select-ing the walk length to optimize the balance between per-formance and efficiency, as well as between global and localstructural information."}, {"title": "Conclusion", "content": "We have proposed a novel GNN, termed Graph Quan-tum Walk Transformer (GQWformer), this innovative modelleverages the unique properties of QWs to capture rich topo-logical and contextual information from graph structures. Byintegrating a GQW-Attn module, GQWformer effectivelyextracts global pairwise relationships, while the GQW-Recucomponent adeptly focuses on local details. Experimentalresults demonstrate that GQWformer consistently achievesstate-of-the-art performance across a diverse range of do-mains, including biological networks, chemical compounds,and social interaction graphs, particularly excelling in graphclassification tasks."}]}