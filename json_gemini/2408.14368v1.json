{"title": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal Conditioned Policy", "authors": ["Peiyan Li", "Hongtao Wu", "Yan Huang", "Chilam Cheang", "Liang Wang", "Tao Kong"], "abstract": "The robotics community has consistently aimed to achieve generalizable robot manipulation with flexible natural language instructions. One of the primary challenges is that obtaining robot data fully annotated with both actions and texts is time-consuming and labor-intensive. However, partially annotated data, such as human activity videos without action labels and robot play data without language labels, is much easier to collect. Can we leverage these data to enhance the generalization capability of robots? In this paper, we propose GR-MG, a novel method which supports conditioning on both a language instruction and a goal image. During training, GR-MG samples goal images from trajectories and conditions on both the text and the goal image or solely on the image when text is unavailable. During inference, where only the text is provided, GR-MG generates the goal image via a diffusion-based image-editing model and condition on both the text and the generated image. This approach enables GR-MG to leverage large amounts of partially annotated data while still using language to flexibly specify tasks. To generate accurate goal images, we propose a novel progress-guided goal image generation model which injects task progress information into the generation process, significantly improving the fidelity and the performance. In simulation experiments, GR-MG improves the average number of tasks completed in a row of 5 from 3.35 to 4.04. In real-robot experiments, GR-MG is able to perform 47 different tasks and improves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and generalization settings, respectively. Code and checkpoints will be available at the project page: https://gr-mg.github.io/.", "sections": [{"title": "I. INTRODUCTION", "content": "The robotics research community is striving to achieve generalizable robot manipulation using language-based instructions. Among various methods, imitation learning from human demonstrations is one of the most promising endeavors [1]\u2013[5]. However, human demonstration data are scarce. And the process of collecting human demonstrations with action and language labels is time-consuming and labor-intensive. On the other hand, robot play data without language labeling is a scalable source of data. It requires no video clipping or language labeling and can be collected without human constant supervision [6]. These data can be automatically labeled with hindsight goal relabeling [7]: any frame in a video can be used as a goal image to condition the policy for predicting actions to evolve towards this frame from previous frames. Furthermore, there is a substantial collection of text-annotated human activity videos without action labels on the internet. These data contain valuable information about how the agent should move to change the environment according to the language description. Can we develop a policy to effectively leverage all the above partially annotated data?\nPrevious methods have ventured into this domain, but most were limited to using data that lacked either language labels or action labels [4], [7]\u2013[10]. Recent initiatives introduce diffusion models for generating goal images [11] or future videos [12], [13]. The generated image or video is then used as inputs for goal-conditioned policies or inverse dynamics models to predict actions, enabling the use of all the above-mentioned partially annotated data.\nHowever, these approaches often overlook crucial information, such as task progress, during the goal generation phase. This omission can lead to inaccurate generated goals which significantly affects the subsequent action predictions. Furthermore, these policies rely solely on images or videos, making them brittle in the case where the generated goal is inaccurate.\nTo tackle these issues, we introduce GR-MG, a model designed to support multi-modal goals. It comprises two modules: a progress-guided goal image generation model and a multi-modal goal-conditioned policy. Datasets without action labels (e.g., text-annotated internet videos) can be used to train the goal image generation model. Datasets without language labels (e.g., robot play data) can be used to train the policy, with a null string as the language condition. Given that robot manipulation is a sequential decision-making process, where the task progress information can enhance prediction accuracy, we incorporate a novel task progress condition into our goal image generation model. This significantly improves the performance of both goal image generation and thus action prediction. During the training of the policy, we sample goal images from trajectories and condition the policy on both the goal image and the text instruction or solely on the goal image if text is unavailable. During inference, the policy utilizes both the language instruction and the goal image generated from the goal image generation model. Since GR-MG is conditioned on both the text and the goal image, the policy can still rely on the language condition to guide action prediction even if the generated goal image is inaccurate, substantially improving the robustness of the model.\nWe perform extensive experiments in the challenging CALVIN simulation benchmark [14] and a real-robot plat-"}, {"title": "II. RELATED WORK", "content": "Language is probably the most flexible and intuitive way for a human to specify tasks for a robot [1], [2], [4], [5], [15]. However, language and visual signals exist in different domains. And the information contained in a language instruction may be too abstract for a visual policy to comprehend. A line of research explores goal-image conditioned policies [16]. But obtaining goal images during rollouts can be challenging. Recently, some studies have proposed leveraging both language and goal images as conditions during training. They utilize data with language labels alongside robot play data that without language labels [3], [7], [17], [18]. A common approach is to project both the goal image and the language into a shared latent space to condition the policy. However, only the language is used as the condition during inference. GR-MG differs from these methods in that it uses both languages and goal images during inference and training. The goal image is generated via a diffusion model from the language and current observation. The most related work to GR-MG is Susie [11]. It also uses a diffusion model to generate the goal image. The goal image is then fed into a diffusion policy [19] for action prediction. GR-MG follows a similar paradigm but differs in two key aspects. First, it introduces a novel task progress condition in goal image generation, significantly enhancing the accuracy of the generated goal image. Second, GR-MG conditions the policy using both the language and the goal image, whereas Susie relies solely on the generated goal images. This design enables GR-MG to address situations where the generated goal image is inaccurate, thereby improving performance in challenging out-of-distribution environments.\nGiven that collecting fully annotated robot data with action and language labels is time-consuming, many studies turn to alternative sources of data with partial annotations. One approach involves learning useful representations from text-annotated internet videos, which are subsequently used for downstream policy learning [20], [21]. Recently, RT-2 [2] introduced a vision-language-action model that incorporates both robot data and large-scale vision-language data during training, demonstrating powerful generalization capabilities. Another popular method is to learn a video model or world model from internet video data to predict future videos [8], [12], [13], [22]. An inverse dynamics model or goal-conditioned policy is then employed to predict actions based"}, {"title": "III. METHOD", "content": "We aim to learn a policy to address the problem of language-conditioned visual robot manipulation. Specifically, the policy takes as inputs the following. 1) A language instruction $l$ that describes the task to be accomplished. 2) Observation images $o_{t-h:t}$ which include RGB images from the current and previous $h$ timesteps. A static global view camera and a hand-mounted camera are used to capture these images. 3) Robot states $s_{t-h:t}$ which include the 6-Dof pose of the end-effector and the binary gripper state of the current and previous $h$ steps. The policy outputs an action trajectory $a_t$ in an end-to-end manner:\n$a_t = \\pi(l, o_{t-h:t}, s_{t-h:t})$ (1)\nThe training of a language-conditioned policy requires fully annotated robot trajectories which contain both action/state labels and language labels:\n$T = \\{l, (o_1, s_1, a_1), (o_2, s_2, a_2), \\dots,(o_T, s_T, a_T)\\}$ (2)\nHowever, collecting fully annotated data is time-consuming. The motivation behind GR-MG is to utilize partially annotated data. One such data includes videos without action labels, which can be collected at scale from the internet and various public datasets [23]\u2013[25]. Another type of partially annotated data consists of robot play data without language labels. These can be gathered on a large scale by allowing robots to execute a policy without constant human supervision [6]."}, {"title": "B. Network Architecture", "content": "The network architecture of GR-MG is illustrated in Fig. 2. It consists of two modules: a progress-guided goal image generation model and a multi-modal goal conditioned policy.\n1) Progress-guided Goal Image Generation Model: This module generates the goal image based on the current observation image, a language description of the task, and the task progress. Specifically, we use InstructPix2Pix [26], a diffusion-based image-editing model, as the network. Following the approach in Susie [11], we generate a sub-goal representing an intermediate state rather than the final state, updating this sub-goal at fixed time intervals during the rollout. However, unlike Susie which generates the goal image based solely on the current image and the language instruction, we incorporate task progress information in the image generation process. The insight is that robot manipulation is a sequential decision process instead of static image editing. Additionally, a current image can be ambiguous in the case where there are identical observation throughout the rollout of a task, e.g., moving an object back-and-forth. The progress information can provide valuable global information for the goal image generation model. During training, the progress of a frame can be easily obtained from its timestep in the"}, {"title": "D. Inference", "content": "During inference, the task progress is initially set as zero. And the goal image generation model uses the current observation image, language instruction, and the task progress to generate goal images. The goal image is passed to the multi-modal goal-conditioned policy together with the language instruction, sequence of observation images, and sequence of robot states to predict action trajectories and task progress.\nAs the goal image is set as N steps away from the current image in training, we run the goal image generation model only every $n < N$ steps in a closed-loop manner for efficiency."}, {"title": "IV. EXPERIMENTS", "content": "We perform extensive experiments in a simulation benchmark and the real world to evaluate the performance of GR-MG. Throughout the experiments, we aim to answer three questions. 1) Can GR-MG perform multi-task learning and deal with challenging settings including unseen distractors, unseen backgrounds, and unseen objects? 2) Does incorporating robot play data enhance the performance of GR-MG, especially when fully annotated data are scarce? 3) How does the inclusion of videos without action labels improves the performance of GR-MG?"}, {"title": "A. CALVIN Benchmark Experiments", "content": "We first perform experiments in the challenging CALVIN [14] benchmark environment. CALVIN focuses on language-conditioned visual robot manipulation. It contains 34 tasks and four different environments, i.e., Env A, B, C, and D (Fig. 3). Each environment contains a Franka Emika Panda robot with a parallel-jaw gripper and a desk for performing different manipulation tasks. Real-robot rollout videos can be found in the supplementary material and the project page."}, {"title": "3) Data Scarcity:", "content": "Obtaining large-scale fully annotated trajectories is challenging, especially in real-world settings. In this experiment, we simulate the data scarcity by training GR-MG with only 10% of the fully annotated data in the ABC\u2192D split (about 1.8k trajectories for 34 tasks). However, we make use of all the robot play data in Env A, B, and C to train the policy first before finetuning on the 10% fully annotated trajectories. The robot play data contains 1M frames, while the 10% fully annotated data encompasses 0.1M frames. Results are shown in Tab. I. GR-MG significantly outperforms GR-1 [4], improving the success rate from 67.2% to 90.3% and the average length from 1.41 to 3.11. We also compare with a variant of our method, GR-MG w/o play data, which does not utilize robot play data during training. GR-MG significantly outperforms this variant, highlighting the effectiveness of incorporating robot play data. Based on our observations, GR-MG w/o"}, {"title": "B. Real-Robot Experiments", "content": "To evaluate the performance of GR-MG in the real world, we perform experiments on a real robot platform. The platform consists of a Kinova Gen-3 robot arm equipped with a Robotiq 2F-85 parallel-jaw gripper and two cameras, i.e., one static camera for capturing the workspace and another camera mounted on the end-effector. We collected about 15k human demonstrations across 29 tasks, which include pick-and-place manipulation, articulated object manipulation, and pouring. See Fig. 3 for some examples. We use these data to train GR-MG.\nIn this experiment, we aim to verify the effectiveness of leveraging partially annotated data without action labels in the training of goal image generation model. Some robot datasets have action labels. But their embodiments and coordinate settings are different from the target platform, making them hard for direct usage in model training. However, if the language labels are provided, they can be used as partially annotated data without action labels. In this experiment, we train the goal image generation model with our real robot data, Something-Something-V2 [23], and the videos extracted from RT-1 dataset [1], [37]. We note that, in general, any video with text annotations can be included in the training.\nWe design four different settings to evaluate the model performance (see Fig. 5): Simple, Unseen Distractor, Unseen Background, and Unseen Object. In Simple, the scene is set to be similar to those in the training data. In Unseen Distractor, unseen distractors are added to the scene. This setting is challenging because the model may be distracted by these new elements, especially if they have similar colors. In Unseen Background, the background is modified by introducing two new tablecloths that were not present in the training data. In Unseen Object, the robot is instructed to manipulate objects that were not included in the training dataset. And the language instructions are adjusted accordingly, i.e., the language instructions are also unseen. In total, we evaluate 47 different language instructions: 29 of which were seen during training, while the remaining tasks were unseen. We compare the performance of GR-MG with GR-1 [4] and a variant of GR-MG, GR-MG w/o part. ann. data, which uses only the fully annotated data for training the goal image generation model. To ensure a fair comparison, given that GR-1 can also utilize text-annotated videos via generative video pre-training, we incorporate Something-Something-V2 and RT-1 into its pre-training."}, {"title": "V. CONCLUSIONS", "content": "In this paper, we introduce GR-MG, a novel method that utilizes multi-modal goals to predict actions. GR-MG uses both a language and a goal image to condition the action prediction. It requires only the language as input and generate the goal image based on the language via a goal image generation model. This design enables GR-MG to effectively leverage large amounts of partially annotated data that are missing either language or action labels. To enhance the robustness of the generated goal images, we incorporate task progress information in the goal image generation model. GR-MG demonstrates exceptional performance in both simulation and real-world experiments, showing strong generalization across various out-of-distribution settings. Given the capability to utilize partially annotated data, we plan to scale up the training of both the goal image generation model and the policy in the future. Additionally, we plan to investigate integrating depth information to further improve the accuracy of action prediction."}, {"title": "A. Data", "content": "To train the goal generation model, we resize all images to 256\u00d7256 before encoding. Additionally, we sample a ground truth goal image from future frames within a range defined by $k_{min}$ and $k_{max}$. The values for different datasets are listed in Tab. II. We sample tuples of $(l, o_t, o_{t+k}, p)$ for training, where $l$ represents the language instruction, $o_t$ is the current observation, $o_{t+k}$ is the target goal image, and $p$ indicates the progress information. $p$ is computed from the timestep of the input image in the video.\nTo train the policy, all images are resized to 224\u00d7224 to match the input requirements of the MAE [30] encoder. The policy takes h steps of observation history as input and outputs an action trajectory of $k_{act}$ steps. Groundtruth goal images are sampled from future frames within the trajectory. Specifically, we sample an image from the next step up to the next $k_{pmax}$-th step. Similar to GR-1 [4], we train the policy to predict both actions and future images. We set the target future image as the next 3rd frame. See Table II for more details."}, {"title": "B. Progress-guided Goal Image Generation Model", "content": "The progress-guided goal image generation model is based on InstructPix2Pix [26]. The goal image generation is performed via a diffusion process in the latent space defined by a VAE image encoder [33]. Firstly, the input image is encoded into a latent representation by the VAE encoder. Secondly, the U-Net denoises the latent features conditioned on the input image and the text. The text condition is injected via cross-attention. Finally, the denoised latent features are decoded by the VAE decoder to produce the goal image. We employ classifier-free guidance [38] during the diffusion process. We set the number of denoising steps as 50 during inference."}, {"title": "C. Multi-modal Goal Conditioned Policy", "content": "For the multi-modal goal-conditioned policy, each image is first encoded into 196 patch tokens and one global token corresponding to the output of the input [CLS] token. The 196 patch tokens are then reduced to 9 tokens with a Perceiver Resampler [39]. We use CLIP [29] to encode the language instruction. Robot states are encoded via linear layers. Before inputting the tokens to the GPT model, we"}, {"title": "D. Training", "content": "Our goal image generation model is trained on 16 NVIDIA A100 GPUs (80GB) for 50 epochs. Training takes approximately 18 hours on the CALVIN Benchmark and about 30 hours in the real-robot experiments. During training, we apply data augmentation using CenterCrop and ColorJitter. Additionally, we find that using an Exponential Moving Average (EMA) is crucial for stabilizing the performance of GR-MG.\nWe train the policy on 32 NVIDIA A800 GPUs (40GB) for 50 epochs, which takes about 17 hours for the CALVIN Benchmark and 7 hours for the real-robot experiments. The training loss consists of five components: arm action prediction loss $l_{arm}$, gripper action prediction loss $l_{gripper}$, future image prediction loss $l_{img}$, KL divergence loss from VAE $l_{kl}$ [31], and progress loss $l_{prog}$:\n$\\mathcal{L} = l_{arm} + 0.01l_{gripper} + 0.1l_{img} + l_{kl} + l_{prog}$ (3)\nWe list the training hyperparameters in Table III."}]}