{"title": "GR-MG: Leveraging Partially Annotated Data via Multi-Modal Goal Conditioned Policy", "authors": ["Peiyan Li", "Hongtao Wu", "Yan Huang", "Chilam Cheang", "Liang Wang", "Tao Kong"], "abstract": "The robotics community has consistently aimed to achieve generalizable robot manipulation with flexible natural language instructions. One of the primary challenges is that obtaining robot data fully annotated with both actions and texts is time-consuming and labor-intensive. However, partially annotated data, such as human activity videos without action labels and robot play data without language labels, is much easier to collect. Can we leverage these data to enhance the generalization capability of robots? In this paper, we propose GR-MG, a novel method which supports conditioning on both a language instruction and a goal image. During training, GR-MG samples goal images from trajectories and conditions on both the text and the goal image or solely on the image when text is unavailable. During inference, where only the text is provided, GR-MG generates the goal image via a diffusion-based image-editing model and condition on both the text and the generated image. This approach enables GR-MG to leverage large amounts of partially annotated data while still using language to flexibly specify tasks. To generate accurate goal images, we propose a novel progress-guided goal image generation model which injects task progress information into the generation process, significantly improving the fidelity and the performance. In simulation experiments, GR-MG improves the average number of tasks completed in a row of 5 from 3.35 to 4.04. In real-robot experiments, GR-MG is able to perform 47 different tasks and improves the success rate from 62.5% to 75.0% and 42.4% to 57.6% in simple and generalization settings, respectively. Code and checkpoints will be available at the project page: https://gr-mg.github.io/.", "sections": [{"title": "I. INTRODUCTION", "content": "The robotics research community is striving to achieve generalizable robot manipulation using language-based instructions. Among various methods, imitation learning from human demonstrations is one of the most promising endeavors [1]\u2013[5]. However, human demonstration data are scarce. And the process of collecting human demonstrations with action and language labels is time-consuming and labor-intensive. On the other hand, robot play data without language labeling is a scalable source of data. It requires no video clipping or language labeling and can be collected without human constant supervision [6]. These data can be automatically labeled with hindsight goal relabeling [7]: any frame in a video can be used as a goal image to condition the policy for predicting actions to evolve towards this frame from previous frames. Furthermore, there is a substantial collection of text-annotated human activity videos without action labels on the internet. These data contain valuable information about how the agent should move to change the environment according to the language description. Can we develop a policy to effectively leverage all the above partially annotated data?\nPrevious methods have ventured into this domain, but most were limited to using data that lacked either language labels or action labels [4], [7]\u2013[10]. Recent initiatives introduce diffusion models for generating goal images [11] or future videos [12], [13]. The generated image or video is then used as inputs for goal-conditioned policies or inverse dynamics models to predict actions, enabling the use of all the above-mentioned partially annotated data.\nHowever, these approaches often overlook crucial information, such as task progress, during the goal generation phase. This omission can lead to inaccurate generated goals which significantly affects the subsequent action predictions. Furthermore, these policies rely solely on images or videos, making them brittle in the case where the generated goal is inaccurate.\nTo tackle these issues, we introduce GR-MG, a model designed to support multi-modal goals. It comprises two modules: a progress-guided goal image generation model and a multi-modal goal-conditioned policy. Datasets without action labels (e.g., text-annotated internet videos) can be used to train the goal image generation model. Datasets without language labels (e.g., robot play data) can be used to train the policy, with a null string as the language condition. Given that robot manipulation is a sequential decision-making process, where the task progress information can enhance prediction accuracy, we incorporate a novel task progress condition into our goal image generation model. This significantly improves the performance of both goal image generation and thus action prediction. During the training of the policy, we sample goal images from trajectories and condition the policy on both the goal image and the text instruction or solely on the goal image if text is unavailable. During inference, the policy utilizes both the language instruction and the goal image generated from the goal image generation model. Since GR-MG is conditioned on both the text and the goal image, the policy can still rely on the language condition to guide action prediction even if the generated goal image is inaccurate, substantially improving the robustness of the model.\nWe perform extensive experiments in the challenging CALVIN simulation benchmark [14] and a real-robot plat-"}, {"title": "II. RELATED WORK", "content": "A. Leveraging Multi-Modal Goals in Policy Learning\nLanguage is probably the most flexible and intuitive way for a human to specify tasks for a robot [1], [2], [4], [5], [15]. However, language and visual signals exist in differ-ent domains. And the information contained in a language instruction may be too abstract for a visual policy to com-prehend. A line of research explores goal-image conditioned policies [16]. But obtaining goal images during rollouts can be challenging. Recently, some studies have proposed lever-aging both language and goal images as conditions during training. They utilize data with language labels alongside robot play data that without language labels [3], [7], [17], [18]. A common approach is to project both the goal image and the language into a shared latent space to condition the policy. However, only the language is used as the condition during inference. GR-MG differs from these methods in that it uses both languages and goal images during inference and training. The goal image is generated via a diffusion model from the language and current observation. The most related work to GR-MG is Susie [11]. It also uses a diffusion model to generate the goal image. The goal image is then fed into a diffusion policy [19] for action prediction. GR-MG follows a similar paradigm but differs in two key aspects. First, it introduces a novel task progress condition in goal image generation, significantly enhancing the accuracy of the generated goal image. Second, GR-MG conditions the policy using both the language and the goal image, whereas Susie relies solely on the generated goal images. This design enables GR-MG to address situations where the generated goal image is inaccurate, thereby improving performance in challenging out-of-distribution environments.\nB. Leveraging Partially Annotated Data in Policy Learning\nGiven that collecting fully annotated robot data with action and language labels is time-consuming, many studies turn to alternative sources of data with partial annotations. One approach involves learning useful representations from text-annotated internet videos, which are subsequently used for downstream policy learning [20], [21]. Recently, RT-2 [2] introduced a vision-language-action model that incorporates both robot data and large-scale vision-language data during training, demonstrating powerful generalization capabilities. Another popular method is to learn a video model or world model from internet video data to predict future videos [8], [12], [13], [22]. An inverse dynamics model or goal-conditioned policy is then employed to predict actions based"}, {"title": "III. METHOD", "content": "We aim to learn a policy to address the problem of language-conditioned visual robot manipulation. Specifically, the policy takes as inputs the following. 1) A language instruction l that describes the task to be accomplished. 2) Observation images $o_{t-h:t}$ which include RGB images from the current and previous h timesteps. A static global view camera and a hand-mounted camera are used to capture these images. 3) Robot states $s_{t-h:t}$ which include the 6-Dof pose of the end-effector and the binary gripper state of the current and previous h steps. The policy outputs an action trajectory at in an end-to-end manner:\n$a_{t} = \\pi(l, o_{t-h:t}, s_{t-h:t})$ (1)\nA. Data\nThe training of a language-conditioned policy requires fully annotated robot trajectories which contain both ac-tion/state labels and language labels:\n$T = \\{l, (o_1, s_1, a_1), (o_2, s_2, a_2), \\ldots,(o_T,s_T,a_t)\\}$ (2)\nHowever, collecting fully annotated data is time-consuming. The motivation behind GR-MG is to utilize partially anno-tated data. One such data includes videos without action labels, which can be collected at scale from the internet and various public datasets [23]\u2013[25]. Another type of partially annotated data consists of robot play data without language labels. These can be gathered on a large scale by allowing robots to execute a policy without constant human supervi-sion [6].\nB. Network Architecture\nThe network architecture of GR-MG is illustrated in Fig. 2. It consists of two modules: a progress-guided goal image generation model and a multi-modal goal conditioned policy.\n1) Progress-guided Goal Image Generation Model: This module generates the goal image based on the current observation image, a language description of the task, and the task progress. Specifically, we use InstructPix2Pix [26], a diffusion-based image-editing model, as the network. Fol-lowing the approach in Susie [11], we generate a sub-goal representing an intermediate state rather than the final state, updating this sub-goal at fixed time intervals during the roll-out. However, unlike Susie which generates the goal image based solely on the current image and the language instruc-tion, we incorporate task progress information in the image generation process. The insight is that robot manipulation is a sequential decision process instead of static image editing. Additionally, a current image can be ambiguous in the case where there are identical observation throughout the rollout of a task, e.g., moving an object back-and-forth. The progress information can provide valuable global information for the goal image generation model. During training, the progress of a frame can be easily obtained from its timestep in the"}, {"title": "IV. EXPERIMENTS", "content": "trajectory (or video). During inference, such information is not accessible. To address this, we train the policy to predict task progress alongside the action trajectory, as we will outline below. To incorporate progress information into the generation model, we append a sentence to the task description, e.g., \"pick up the red block. And 60% of the instruction has been completed.\". We use T5-Base [27] to encode the text.\n2) Multi-modal Goal Conditioned Policy: Following GR-1 [4], we employ a similar GPT-style transformer net-work [28]. For completeness, we briefly review GR-1 here. GR-1 takes as input a language instruction, a sequence of observation images, and a sequence of robot states. The model then outputs an action and a prediction of the future image. The language is encoded using a text encoder [29]; images are encoded via MAE [30]; robot states are encoded through linear layers. To predict actions, GR-1 includes a [ACT] query token, and for future image prediction, it learns multiple [OBS] query tokens. The embeddings of different signals and the query tokens are sequenced by timestep and input into the transformer. For more details, we refer readers to [4].\nThe difference between GR-MG and GR-1 is threefold. Firstly, GR-MG is a multi-modal goal-conditioned policy which takes both a goal image and a language instruction as the condition. GR-1 only takes the language. To inject the goal image condition, we use MAE [30] to tokenize the goal image generated by the goal image generation model and append the tokens at the front of all the input tokens (Fig. 2). The goal image tokens can be attended by all the subsequent tokens for conditioning. Secondly, in order to predict task progress, a [PROG] query token is included as shown in Fig. 2. The output embedding of the token is passed through linear layers to regress the progress value. Finally, we follow recent work [31] to predict action trajectories instead of a single action as in GR-1 via a conditional variational autoencoder (CVAE) [32], [33]. Specifically, we use a VAE encoder to encode the action trajectory into a style vector embedding. We concatenate the style vector embedding, the output embedding of the [ACT] token and k learnable token embeddings together and input them to a transformer to predict an action trajectory of k steps.\nC. Training\nFor the training of goal image generation model, we follow the training setting in InstructPix2Pix [26] and train a noise prediction model as in DDPM [34]. We sample the frame which is N steps away from the current frame in the trajectory/video as the goal image. As a rough task progress can already be very informative and can bring about more stable training, we discretize the task progress predicted from the policy from 0% to 100% into 10 bins before passing it to the goal image generation model.\nFor the training of multi-modal goal-conditioned policy, we initialize its weight with the pre-trained model weight de-rived from the generative video pre-training on Ego4d [25] in alignment with GR-1 [4]. Subsequently, leveraging our multi-modal goals, the policy can be first trained on data without language annotations, e.g., robot play data. Specifically, we input a null string for the text input in this stage. After that, we can further finetune the policy with fully annotated robot trajectories.\nThe input task progress in the training of the goal image generation model is computed from the timestep of the image in the video. The goal image in the training of the policy is sampled from the ground truth goal image from the trajectory. Therefore, the training of the two modules are independent, although they are interconnected during inference. We leave the investigation of co-training the two modules as future work.\nD. Inference\nDuring inference, the task progress is initially set as zero. And the goal image generation model uses the current obser-vation image, language instruction, and the task progress to generate goal images. The goal image is passed to the multi-modal goal-conditioned policy together with the language instruction, sequence of observation images, and sequence of robot states to predict action trajectories and task progress. As the goal image is set as N steps away from the current image in training, we run the goal image generation model only every n < N steps in a closed-loop manner for efficiency.\nWe perform extensive experiments in a simulation bench-mark and the real world to evaluate the performance of GR-MG. Throughout the experiments, we aim to answer three questions. 1) Can GR-MG perform multi-task learning and deal with challenging settings including unseen distractors, unseen backgrounds, and unseen objects? 2) Does incorpo-rating robot play data enhance the performance of GR-MG, especially when fully annotated data are scarce? 3) How does the inclusion of videos without action labels improves the performance of GR-MG?\nA. CALVIN Benchmark Experiments\n1) Experiment Settings: We first perform experiments in the challenging CALVIN [14] benchmark environment. CALVIN focuses on language-conditioned visual robot ma-nipulation. It contains 34 tasks and four different environ-ments, i.e., Env A, B, C, and D (Fig. 3). Each environment contains a Franka Emika Panda robot with a parallel-jaw gripper and a desk for performing different manipulation tasks. Real-robot rollout videos can be found in the sup-plementary material and the project page.\n2) Generalization to Unseen Environments: We perform experiments on the ABC\u2192D split: we train GR-MG with data collected from Env A, B, and C and evaluate in Env D. This experiment helps us evaluate the performance of GR-MG on handling unseen environments. In total, the number of fully annotated trajectories from Env A, B, and C is about 18k. We use these data for training GR-MG. Following the evaluation protocol from [14], we command the robot to"}, {"title": "V. CONCLUSIONS", "content": "ulation and real-world experiments, showing strong general-ization across various out-of-distribution settings. Given the capability to utilize partially annotated data, we plan to scale up the training of both the goal image generation model and the policy in the future. Additionally, we plan to investigate integrating depth information to further improve the accuracy of action prediction."}, {"title": "APPENDIX", "content": "A. Data\nTo train the goal generation model, we resize all images to 256\u00d7256 before encoding. Additionally, we sample a ground truth goal image from future frames within a range defined by $k_{min}$ and $k_{max}$. The values for different datasets are listed in Tab. II. We sample tuples of (l, $o_t$, $o_{t+k}$, p) for training, where l represents the language instruction, $o_t$ is the current observation, $o_{t+k}$ is the target goal image, and p indicates the progress information. p is computed from the timestep of the input image in the video.\nTo train the policy, all images are resized to 224\u00d7224 to match the input requirements of the MAE [30] encoder. The policy takes h steps of observation history as input and outputs an action trajectory of $k_{act}$ steps. Groundtruth goal images are sampled from future frames within the trajectory. Specifically, we sample an image from the next step up to the next $k_{pmax}$-th step. Similar to GR-1 [4], we train the policy to predict both actions and future images. We set the target future image as the next 3rd frame. See Table II for more details.\nB. Progress-guided Goal Image Generation Model\nThe progress-guided goal image generation model is based on InstructPix2Pix [26]. The goal image generation is per-formed via a diffusion process in the latent space defined by a VAE image encoder [33]. Firstly, the input image is encoded into a latent representation by the VAE encoder. Secondly, the U-Net denoises the latent features conditioned on the input image and the text. The text condition is injected via cross-attention. Finally, the denoised latent features are decoded by the VAE decoder to produce the goal image. We employ classifier-free guidance [38] during the diffusion process. We set the number of denoising steps as 50 during inference.\nC. Multi-modal Goal Conditioned Policy\nFor the multi-modal goal-conditioned policy, each image is first encoded into 196 patch tokens and one global token corresponding to the output of the input [CLS] token. The 196 patch tokens are then reduced to 9 tokens with a Perceiver Resampler [39]. We use CLIP [29] to encode the language instruction. Robot states are encoded via linear layers. Before inputting the tokens to the GPT model, we use linear layers to align the dimension of all tokens to the hidden size of the GPT model. The hidden size, number of heads, and number of layers for the GPT model is 384, 12, and 12, respectively. We follow [31] to predict the action trajectory via a cVAE [32], [33].\nD. Training\nOur goal image generation model is trained on 16 NVIDIA A100 GPUs (80GB) for 50 epochs. Training takes approx-imately 18 hours on the CALVIN Benchmark and about 30 hours in the real-robot experiments. During training, we apply data augmentation using CenterCrop and ColorJitter. Additionally, we find that using an Exponential Moving Average (EMA) is crucial for stabilizing the performance of GR-MG.\nWe train the policy on 32 NVIDIA A800 GPUs (40GB) for 50 epochs, which takes about 17 hours for the CALVIN Benchmark and 7 hours for the real-robot experiments. The training loss consists of five components: arm action prediction loss $l_{arm}$, gripper action prediction loss $l_{gripper}$, future image prediction loss $l_{img}$, KL divergence loss from VAE $l_{kl}$ [31], and progress loss $l_{prog}$:\n$L = l_{arm} +0.01l_{gripper} +0.1l_{img} + l_{kl} + l_{prog}$ (3)\nWe list the training hyperparameters in Table III."}]}