{"title": "Exploring fully convolutional networks for the segmentation of hyperspectral imaging applied to advanced driver assistance systems", "authors": ["Jon Guti\u00e9rrez-Zaballa", "Koldo Basterretxea", "Javier Echanobe", "M. Victoria Mart\u00ednez", "In\u00e9s del Campo"], "abstract": "Advanced Driver Assistance Systems (ADAS) are designed\nwith the main purpose of increasing the safety and comfort of vehicle oc-\ncupants. Most of current computer vision-based ADAS perform detection\nand tracking tasks quite successfully under regular conditions, but are\nnot completely reliable, particularly under adverse weather and changing\nlighting conditions, neither in complex situations with many overlapping\nobjects. In this work we explore the use of hyperspectral imaging (HSI) in\nADAS on the assumption that the distinct near infrared (NIR) spectral\nreflectances of different materials can help to better separate the objects\nin a driving scene. In particular, this paper describes some experimen-\ntal results of the application of fully convolutional networks (FCN) to\nthe image segmentation of HSI for ADAS applications. More specifically,\nour aim is to investigate to what extent the spatial features codified by\nconvolutional filters can be helpful to improve the performance of HSI\nsegmentation systems. With that aim, we use the HSI-Drive v1.1 dataset,\nwhich provides a set of labelled images recorded in real driving condi-\ntions with a small-size snapshot NIR-HSI camera. Finally, we analyze the\nimplementability of such a HSI segmentation system by prototyping the\ndeveloped FCN model together with the necessary hyperspectral cube\npreprocessing stage and characterizing its performance on an MPSoC.", "sections": [{"title": "1 Introduction", "content": "Today, thanks to the availability of small-size, portable, snapshot hyperspectral\ncameras, it is possible to set-up HSI processing systems on moving platforms.\nThe use of drones for precision agriculture and ecosystem monitoring is probably\none of the most active and mature application domains [5]. The research into\nhow hyperspectral information can be used to develop more capable and robust\nADAS is, on the contrary, in its infancy [6,11,12]. HSI provides rich information\nabout how materials reflect light of different wavelengths (spectral reflection),\nand this can be used to identify and classify surfaces and objects in an scene.\nThus, with the application of appropriate information processing techniques, HSI\ncan help to enhance the accuracy and robustness of current ADAS for object\nidentification and tracking and, eventually, can be used for scene understanding,\nwhich is a step forward in the achievement of more capable and intelligent ADS\n(Autonomous Driving Systems).\nHSI segmentation of real driving scenes is, however, challenging for a variety\nof reasons. First, the spectral reflectance signatures of the different objects, e.g.\nmetallic white vehicles bodies and road marks, may be weakly separable. Second,\nextracting spatial features that could help segmenting items with similar spectral\nreflectances is difficult as a consequence of the enormous diversity of shapes,\nview angles and scales. Finally, it should be always kept in mind that developed\nsegmentation algorithms need to be computed with very demanding latency\nrequirements on resource constrained onboard processing platforms.\nIn this article we describe some results of a research that investigates how\nFCN can be applied to enhance the segmentation accuracy of images acquired\nin real driving scenarios with a small-size mosaic snapshot hyperspectral cam-\nera. We present a simple application example of scene understanding for the\nseparation of the drivable (tarmac) and non-drivable areas (identifying sky and\nvegetation) in the acquired image sequences as well as for the recognition of road\nmarks, which could be used to enhance automatic lane keeping and trajectory\nplanning systems for ADS. Finally, we describe the rapid prototyping workflow\nused to develop a functional HSI segmentation processing system on a Xilinx\nZynq UltraScale MPSoC, from algorithm exploration and model optimization\nto the final implementation."}, {"title": "2 Experimental Setup", "content": "When dealing with a semantic segmentation problem, it is of utmost importance\nto adapt the structure of the neural network to the unique characteristics of the\ndataset. Thus, once the suitability of the dataset has been verified, a hyperpa-\nrameter tuning and optimization process should be carried out on the neural\nnetwork."}, {"title": "2.1 The Dataset", "content": "As it is reported in [1], there are very few datasets of hyperspectral imagery\nfor ADAS and ADS applications, one of which is precisely presented in [1], HSI\nDrive. HSI Drive v1.1 contains 276 images of urban, road and highway scenarios\nin diverse weather (sunny, cloudy, rainy and foggy) and lightning (dawn, midday,\nsunset) conditions taken during Spring (121 images) and Summer (155 images).\nThe driving scenes have been recorded with a Photonfocus camera that in-\ncludes an Imec 25-band VIS-NIR (535nm-975nm) sensor based on a CMOSIS\nCMV200 image wafer sensor. The global resolution is 1088 x 2048 pixels with\n5\u03bcm x 5\u00b5m size. However, as the spectral bands are extracted from a mosaic\nformed by 5x5 pixel window Fabri-Perot filters, the final resolution of the HSI\ncubes is 216 x 409 x 25 [8]. This implies including a preprocessing stage in the\nprocessing pipeline that is addressed in Subsection 4.1.\nThe original labelling separates the scenes into 10 classes taking into account\nthe surface reflectances of the materials. Those classes are: Road, Road Marks,\nVegetation, Painted Metal, Sky, Concrete/Stone/Brick, Pedestrian/Cyclist, Wa-\nter, Unpainted Metal and Glass/Transparent Plastic. Furthermore, it has to be\nnoted that the labelling of the dataset has followed a weak approach in order to\nprovide the network with the most precise data. This means, for example, that\npixels that are in the junction of two or more surfaces have been left out of the\nlabelling process. However, these pixels do take part in the training process of\na convolutional network. In fact, the training of convolutional neural networks\nwith weakly labelled datasets is a line of research itself [10].\nSpectral separability analysis measures the differences in the surface re-\nflectance patterns of the materials that belong to different classes, which is an\nindex of how well a semantic classifier could perform. One of the most common\ncriteria in remote-sensing applications is JeffreysMatusita distance [4]. It ranges\nfrom 0 to 2 but does not have a linear interpretation as 0-1 values mean very\npoor separability, 1.0-1.9 values account for moderate separability and 1.9-2.0\nvalues indicate good separability [1]."}, {"title": "2.2 FCNs for HSI Image Segmentation", "content": "The neural network selected to perform semantic segmentation is a typical FCN\nknown as U-Net [9] which was originally intended for biological image segmen-\ntation but has been widely used for other segmentation tasks, such as, precision\nagriculture [10] and aerial city recognition [3]. The idea of using a FCN is to\ncombine the intrinsic spectral characteristics of the different classes with the\nspatial relationships that should be extracted by the convolution operations.\nWe have adapted the original architecture of the U-Net [9] to the unique\ncharacteristics of the dataset to achieve the best trade-off between segmentation\nperformance and computational complexity. With this aim we have performed a\ngrid search of the optimum combination of model hyperparameters by evaluating\nthe segmentation accuracy on a subset of 45 images selected from all possible\nenvironment/weather conditions."}, {"title": "3 Segmentation Results", "content": "The first experiment focuses on segmenting 3 classes: Road, Road Marks and\nNo Drivable (the remaining classes). The proposed low-complexity segmenta-\ntion system would be aimed at a possible final system for the discrimination of\ndrivable and non-drivable zones, together with a lane-keeping aid.\nIn a second experiment we have added two additional classes to the model\ntraining; Vegetation and Sky. These two categories have been selected due to\ntheir satisfactory spectral separability indexes (see Table 1). The exploration of\nmore complex segmentation models including all classes in the dataset has also\nbeen performed but obtained results are irregular and not concluding, and will\nrequire further investigation.\nIn order to perform a neural network training over this dataset, the 276\nimages have to be divided into training, validation and test subsets. This division\nhas been performed as follows: 162 images for training, 57 for validation and 57\nfor testing, preserving class proportionality in all the three subsets.\nThe chosen metrics to evaluate the segmentation ability of the neural network\nare accuracy, precision and intersection over union (IoU). As Equations 1, 2 and 3\nshow, accuracy accounts for the false negatives (FN), precision takes into account\nthe false positives (FP) and IoU combines both aspects:\n$A_{i} = \\frac{TP}{TP + FNi}$ (1)\n$P = \\frac{TP}{TP + FP}$ (2)\n$IoUi = \\frac{TP}{TP + FN + FPi}$ (3)\nwhere i is the class index such that, for example, FNi accounts for the pixels\nthat have been predicted as not belonging to class i, but are actually part of\nclass i.\nAs a consequence of the dataset being heavily imbalanced (the number of\npixels in the test dataset is: Road 2,067,379; Road M. 99,426; Veget. 820,804;\nSky 163,127 and Other 363,345) it is useful not to only represent the global\nmetrics but also the mean values and, more specifically, the weighted scores. In\norder to do that, some weighting factors, which are related to the inverse of the\nfrequency of the classes in the dataset, have been previously computed."}, {"title": "3.1 U-Net", "content": "Table 2 collects the performance, in accordance with the above mentioned met-\nrics, of the modified U-Net and also the segmentation metrics after the over-\nlapped patches have been joined to reconstruct the images to their original reso-\nlution. The comparison depicts that the use of overlapping patches improves the\nsegmentation, specially the precision, compared to the case in which the patches\ndo not overlap. This is because neural networks tend to fail to predict the pixels\nof the patch contours because they lack surrounding information."}, {"title": "3.2 A Comparison with Baseline Spectral Classifiers", "content": "From the above described results it can be concluded that the contribution of\nthe spatial information provided by the convolution filters is, indeed, relevant\nto overcome the limitations inherent to the spectral separability of the different\nobjects that can be present in real driving scenes. In order to get a more precise\npicture of this contribution we have compared the obtained results to those\nachieved with a baseline purely spectral classifier based on a three-hidden-layer\nfeedforward ANN. The exploration and optimization process carried out has"}, {"title": "4 Workflow for Rapid Prototyping", "content": "As the implementation of this kind of neural networks in SoCs is a challenging\nand time consuming process, we have decided to explore the use of high-level\nautomatic code generation tools to achieve a rapid prototyping of the system."}, {"title": "4.1 Image Preprocessing", "content": "Raw images acquired from mosaic snapshot cameras need to undergo a prepro-\ncessing pipeline in order to be converted into hyperspectral cubes. This process,\nwhich starts with raw image cropping and finishes with band normalization,\nneeds to be taken into account when characterizing the throughput of the whole\nsegmentation system. The rest of the steps are reflectance correction, partial\ndemosaicing (original resolution is not restored), band alignment and spatial\nfiltering. This processing has been codified in C language and compiled to be\nexecuted as an embedded Linux application in the microprocessor of the MPSOC\nas part of the HW/SW codesign for the implementation of the system."}, {"title": "4.2 Neural Network Deployment", "content": "The design, training, validation and test of the FCN has been performed using\nMATLAB's Deep Network Designer. For the segmentation system prototyping\nprocess on the MPSoC we have used Vitis AI, a development platform for AI\ninference on Xilinx hardware platforms. Since there is not a direct procedure for\nthe implementation of MATLAB-generated deep models we had to first export\nthe neural network to open neural network exchange (ONNX) representation\nand then import it to Keras, our chosen framework, via onnx2keras, an ONNX\nto Keras neural network converter [7]. The next steps are the freezing and quan-\ntizating processes of the neural network that are necessary due to the fact that\nVitis AI favours integer computing.\nThe workflow continues by creating a Tensorflow inference graph from the\nKeras model and by removing the unnecessary information from training and\nsaving only the required elements to compute the requested outputs. As it is\nknown, inference is computationally expensive and, in order to reach the high-\nthroughput and low-latency requirements of ADAS applications, a high memory\nbandwidth is required. Vitis AI Quantizer exploits quantization and VAI Opti-\nmizer applies channel pruning techniques to meet those issues.\nAccording to [13], by converting the 32-bit floating point weights and acti-\nvations to 8-bit integer format, Vitis AI quantizer can reduce computing com-\nplexity without losing prediction accuracy and, as the fixed-point network model\nrequires less memory bandwidth, a faster speed is provided.\nFinally, the product of the quantization is loaded at runtime in the system\ncomposed by the ARM CPU and the DPU accelerator in the MPSoC by a Python\nVART API. Although the quantization output of the 3-class experiment presents\nno appreciable IoU degradation, it has to be stated that the 5-class quantized\nmodel experiences a noticeable loss of performance on some images, an issue to\nbe addressed in the future by Quantization Aware Training or Finetuning [13]."}, {"title": "5 Conclusions", "content": "The incorporation of richer spectral information through HSI improves the seg-\nmentation results of purely spectral models. Besides, it is confirmed that the\nuse of spatial information via convolution operations outperforms purely spec-\ntral models, even when dealing with images as intricate and heterogeneous as\nthose that must be processed in real driving scenarios. However, the contribution\nof the spectral information in spectro-spatial convolutional models needs to be\nfurther investigated since our experiments reveal that the spectral information\nis being overshadowed by the spatial information in the training process of the\nFCN segmentation. We expect that the more effective incorporation of the spec-\ntral information to the AI models should improve segmentation performance in\ntricky situations such as when there are areas with shadows, there is degradation\nin the materials to be segmented, there are surfaces with very high reflectance\nin conditions of extreme lighting or there are multiple overlapping objects.\nThe improvement of the segmentation performance involves investigating fur-\nther modifications to the proposed U-Net such as using 3D convolutions or ap-\nplying multiscale convolution techniques to extract spatial features at different\nscales. The use of different image preprocessing techniques (modifying the par-\ntial demosaicing step, for example) and the addition of a postprocessing stage\n(not to label pixels with uncertain prediction, for instance) will also be assessed,\nbut their applicability will always be subject to the demanding throughput re-\nquirements of ADAS/ADS.\nIn turn, a workflow which combines the use of MATLAB's (Deep Learning\nand Image Processing toolboxes) and Xilinx's tools (AI development and de-\nployment environments) for the rapid prototyping of AI applications has been"}]}