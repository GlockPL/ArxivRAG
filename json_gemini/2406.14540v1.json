{"title": "IRASim: Learning Interactive Real-Robot Action Simulators", "authors": ["Fangqi Zhu", "Hongtao Wu", "Song Guo", "Yuxiao Liu", "Chilam Cheang", "Tao Kong"], "abstract": "Scalable robot learning in the real world is limited by the cost and safety issues of real robots. In addition, rolling out robot trajectories in the real world can be time-consuming and labor-intensive. In this paper, we propose to learn an interactive real-robot action simulator as an alternative. We introduce a novel method, IRASim, which leverages the power of generative models to generate extremely realistic videos of a robot arm that executes a given action trajectory, starting from an initial given frame. To validate the effectiveness of our method, we create a new benchmark, IRASim Benchmark, based on three real-robot datasets and perform extensive experiments on the benchmark. Results show that IRASim outperforms all the baseline methods and is more preferable in human evaluations. We hope that IRASim can serve as an effective and scalable approach to enhance robot learning in the real world. To promote research for generative real-robot action simulators, we open-source code, benchmark, and checkpoints at https://gen-IRASim.github.io.", "sections": [{"title": "1 Introduction", "content": "The field of embodied AI has witnessed remarkable progress in recent years. Real robots are now able to complete a wide variety of tasks [1, 2, 3, 4, 5]. However, real robots are costly, unsafe, and require regular maintenance which may restrict scalable learning in the real world. And rolling out robot trajectories in the real world can be time-consuming and labor-intensive, although it is necessary for model evaluation and reinforcement learning. While efforts have been made to create powerful physical simulators [6, 7, 8, 9], they are still not visually realistic enough. Also, they are not scalable for the reason that it takes effort to build new environments in simulation. What if we can create an interactive real-robot action simulator that can simulate robot trajectories in a way that is both accurate and visually indistinguishable from the real world? With such a simulator, agents can interactively control virtual robots to interact with diverse objects in various scenes in the simulator. It enables robots to improve policies by learning from simulated experiences without safety concerns and maintenance efforts. And the improved policy can consequently produce a large amount of simulated but realistic \"real-robot\" trajectories for training. Furthermore, the simulator can be leveraged as a dynamics model for imagining outcomes of different proposed candidate actions for model-based reinforcement learning.\nRecent advances in generative models showcase extraordinary performance in generating realistic texts [10], images [11], and videos [12]. Inspired by these successes, we take the first step to leverage generative models in building a real-robot action simulator. To this end, we propose IRASim, a novel method that generates extremely realistic videos of a robot executing an action trajectory,"}, {"title": "2 Related Work", "content": "World Models. Learning a world model (or dynamics model) [16, 17] which predicts future observations based on the current observation and actions has been increasingly popular recently [18, 19, 20, 21, 22, 23, 15, 24, 25, 26, 27]. VLP [25] exploits text-to-video models as dynamics models to generate video plans for robots. DreamerV3 [20] and DayDreamer [21] leverage recurrent state space models (RSSMs) [28] to learn a latent representation of states through modeling a world model for reinforcement learning. GAIA-1 [22] uses a generative model that leverages videos, texts, and actions to generate photorealistic driving scenes. Recently, Genie [24] learns an interactive environment from unlabelled internet videos which allows users to act in the environments via latent actions. Probably, the most relevant work is UniSim [15] which builds a universal simulator for modeling real-world interaction. It generates videos from texts and demonstrates controlling a robot arm to move in 2D space with language instructions. Our method can be considered as a world model for robot manipulation. It takes as inputs real-robot physical actions instead of text [15] or latent actions [24]. Besides 2D actions in previous work [18, 19, 15], IRASim also deals with complex 7-Dof robot actions in 3D space which involves translation, rotation, and gripper actions.\nVideo Models. Video models generate video frames either unconditionally or with conditions including classes, initial frames, texts, strokes, and/or actions [18, 29, 30, 31, 32, 33, 34, 35, 36, 37, 38, 39, 22, 40, 15, 12, 41, 24, 42]. VideoGPT [30] leverages a simple GPT-like architecture to autoregressively generate VQ-VAE [43] encoded tokens for video generation. Recently, diffusion models [44, 45] are becoming more and more popular in video generation [31, 32, 33, 39, 38, 15, 12]. A popular choice of architecture is U-Net [31, 39] which has also been widely used in image diffusion models [11]. Sora [12] showcases extraordinary video generation capability with Diffusion Transformers [13]. Our method also leverages Diffusion Transformers as the backbone [41, 13]. A relevant line of work is to control video synthesis with motions. These methods use either user-specified strokes [46, 47], bounding boxes [40], or human poses [48, 36] as conditions. Our method differs from previous work in that we seek to model complex 3D real-world actions in the video to learn a real-robot action simulator.\nScaling Real-World Robot Learning. Rolling out policies in the real world is essential in scaling up robot learning. Firstly, it is necessary for model evaluation [1, 2, 3, 4, 5]. Secondly, as real-robot data are scarce for the reason that data collection often requires costly human demonstrations, an alternative is to roll out a policy to collect data (e.g. dataset augmentation (DAgger) [49, 5, 50]). Finally, real-robot reinforcement learning requires rolling out robots in the real world to collect trajectories [51, 52, 53]. However, policy rollout in the real world is time-consuming. And human supervision is often needed to ensure safety which can be labor-intensive. Moreover, scaling up real-world evaluation would necessitate building and maintaining a large number of robots. To tackle this challenge, recent work [54] shows a correlation between evaluation in a physical simulator and on real robots. Our method aims to build a real-robot action simulator as an efficient and scalable alternative for real-world policy rollout."}, {"title": "3 Methods", "content": "3.1 Problem Statement\nWe define the trajectory-to-video generation task as predicting the video of a robot that executes a trajectory given the initial frame $I^1$ and the action trajectory $a_{1:N-1}$:\n$I_{2:N} = f(I^1, a_{1:N-1})$\nwhere N denotes the number of frames in the video; $a^i$ denotes the action at the i-th timestep. In this paper, we focus on predicting videos for robot arms. A typical action space for robot arms contains 7"}, {"title": "3.2 Preliminaries", "content": "Diffusion Models. Before delving into our method, we briefly review preliminaries of diffusion models [44, 45]. Diffusion models [44] typically consist of a forward process and a reverse process. The forward process gradually adds Gaussian noises to data $x_0$ over T timesteps. It can be formulated as $q(x_t|x_0) = \\mathcal{N}(x_t; \\sqrt{\\bar{\\alpha}_t}x_0, 1 - \\bar{\\alpha}_tI)$, where $x_t$ is the diffused data at the t-th diffusion timestep and $\\bar{\\alpha}_t$ is a constant defined by a variance schedule. The reverse process starts from $x_T \\sim \\mathcal{N}(0, I)$ and gradually remove noises to recover $x_0$. It can be mathematically expressed as $p_\\theta(x_{t-1}|x_t) = \\mathcal{N}(x_{t-1}; \\mu_\\theta(x_t, t), \\Sigma_\\theta(x_t, t))$, where $\\mu_\\theta(\\cdot)$ and $\\Sigma_\\theta(\\cdot)$ denote the mean and covariance functions, respectively, and can be parameterized via a neural network.\nIn the training phase, we sample a timestep $t \\in [1, T]$ and obtain $x_t = \\sqrt{\\bar{\\alpha}_t}x_0 + \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_t$ via the reparameterization trick [45] where $\\epsilon_t \\in \\mathcal{N}(0, I)$. We leverage the simplified training objective to train a noise prediction model $\\epsilon_\\theta$ as in DDPM [45]:\n$\\mathcal{L}_{simple}(\\theta) = ||\\epsilon_\\theta(x_t, t) - \\epsilon_t||^2$\nIn the inference phase, we generate $x_0$ by first sampling $x_T$ from $\\mathcal{N}(0, I)$ and iteratively compute\n$x_{t-1} = \\frac{x_t - \\sqrt{1 - \\bar{\\alpha}_t}\\epsilon_\\theta(x_t, t)}{\\sqrt{\\alpha_t}}$\nuntil t = 0. For conditional diffusion processes, the noise prediction model $\\epsilon_\\theta$ can be parameterized as $\\epsilon_\\theta(x_t, t, c)$ where c is the condition that controls the generation process. Throughout the paper, we use superscript and subscript to indicate the timestep of a frame in the input video and the diffusion timestep, respectively.\nLatent Diffusion Models. Directly diffusing the entire video in the pixel space is time-consuming and requires substantial computation to generate long videos with high resolutions [31]. Inspired by [11, 41], we perform the diffusion process in a low-dimension latent space z instead of the pixel space for computation efficiency. Following [39], we leverage the pre-trained variational autoencoder (VAE) in SDXL [55] to compress each frame I in the video to a latent representation with the VAE encoder $z^i = Enc(I^i)$ where i \\in {1, 2, ..., N}. The latent representation can be decoded back to the pixel space with the VAE decoder $I^i = Dec(z^i)$."}, {"title": "3.3 IRASim", "content": "IRASim is a conditional diffusion model operating in the latent space of the VAE introduced in Sec. 3.2. The condition c includes the latent representation of the initial frame of a video $z^1 = Enc(I^1)$ and an action trajectory $a_{1:N-1}$. The diffusion target is the latent representations of the subsequent N - 1 frames of the video in which the robot executes the action trajectory, i.e. $x = z_{2:N}$. Inspired by Sora's remarkable capability of understanding the physical world [12], we similarly adopt Diffusion Transformers (DiT) [13] as the backbone of IRASim. In the design of IRASim, we aim to address three key aspects: 1) consistency with the initial frame 2) adherence to the given action trajectory and 3) computation efficiency. In the following, we describe details of IRASim and discuss pivotal design choices to achieve the aforementioned objectives.\nTokenization. Each latent representation $z^i = Enc(I^i)$ contains P tokens of D dimensions, where P denotes the number of patches per frame. By sequencing the latent representations of all frames by timestep order, the video is tokenized to $N \\times P$ tokens. Spatial and temporal positional embeddings are added to the tokens to allow awareness of patch positions within frames and timesteps in the video, respectively. The VAE is frozen throughout the training process.\nSpatial-Temporal Attention Blocks. Standard transformer blocks apply Multi-Head Self-Attention (MHA) to all tokens in the input token sequence, resulting in quadratic computation cost. We thus leverage the memory-efficient spatial-temporal attention mechanism [56, 24, 41] in the transformer block of IRASim to reduce the computation cost (Fig. 2). Specifically, each block consists of a spatial attention block and a temporal attention block. In the spatial attention block, MHA is confined to"}, {"title": "4 Experiments", "content": "In this section, we perform extensive experiments on IRASim Benchmark which includes three challenging real-robot datasets to verify the performance of IRASim. We aim to answer three questions: 1) Is IRASim effective on solving the trajectory-to-video task on various datasets with different action spaces? 2) How do different components contribute to the final performance of IRASim? 3) Can we leverage IRASim as a robot action simulator to allow humans to control robots in an image? More details and results can be found in the Appendix."}, {"title": "4.1 IRASim Benchmark", "content": "IRASim Benchmark is designed for the trajectory-to-video generation task. We source data from three publicly available robot manipulation datasets: RT-1 [1], Bridge [14], and Language-Table [4]. We split each dataset into training, validation, and test sets. The dataset statistics are shown in Table 3 in Appendix B. In RT-1 and Bridge, a robot arm with a gripper moves in the 3D space to perform manipulation which interacts with objects in the scene. The action spaces of RT-1 and Bridge consist of 1) 6-Dof arm action in the 3D space $T \\in SE(3)$ and 2) a continuous gripper action $g \\in [0, 1]$. In Language-Table, a robot arm moves in a 2D plane to move blocks with a cylindrical end-effector. The action space is 2-Dof translation in the 2D space $p \\in \\mathbb{R}^2$. See Fig. 1 and 3 for example images of these datasets. We convert the arm action of all datasets to relative delta actions."}, {"title": "4.2 Experiment Setup", "content": "We perform experiments on IRASim Benchmark. During training, we sample video clips containing 16 continuous frames from episodes using a sliding window. We resize videos, and the resolutions after resizing for RT-1, Bridge, and Language-Table are $256\\times320$, $256\\times320$, and $288\\times512$, respectively. We perform experiments on video generation on short trajectories and long trajectories. Short trajectories, which are segments of complete episodes, consist of 16 frames and 15 actions. The video can be generated in one diffusion generation process. For long trajectories, we utilize complete episodes from the dataset. Long videos can be rolled out in an autoregressive manner. The initial frame of the first diffusion process is the given ground-truth frame, while the initial frame of each subsequent diffusion process is the last output frame from the previous process.\nIRASim Variants. We follow standard transformers which scale the hidden size, number of heads, and number of layers together. In particular, we perform experiments on four configurations: IRASim-S, IRASim-B, IRASim-L, and IRASim-XL. Details of these models are shown in Tab. 7 in Appendix E. If not specified otherwise, throughout the paper, we report the results of IRASim-XL which contains 679M trainable parameters in total. We denote IRASim with frame-level and video-level adaptation as IRASim-Frame-Ada and IRASim-Video-Ada, respectively.\nBaselines. To evaluate the effectiveness of IRASim, we compare with two state-of-the-art baseline methods, i.e. VDM [31] and LVDM [39]. Both methods are diffusion models based on a U-Net architecture. LVDM diffuses videos in a latent space while VDM operates in the pixel space. These methods demonstrate strong capabilities on the text-to-video task. To impose trajectory conditions on video generation, we encode the trajectory into an embedding to condition the diffusion process in both methods. This is similar to the text embedding used for text-to-video generation in the original papers [31, 39]. LVDM is configured such that its number of parameters is similar to IRASim. As"}, {"title": "5 Limitations and Conclusion", "content": "In this paper, we present IRASim, a novel method that generates videos of a robot that executes an action trajectory given the initial frame. Results show that our method is able to generate long-horizon and high-resolution videos that are almost visually indistinguishable from ground-truth videos.\nSimilar to other generative methods, one limitation of our method is hallucination. Also, the inference speed of IRASim is not real-time, despite having a high throughput \u2013 it costs only 8GB memory during inference. Finally, IRASim currently does not support flexible input resolutions and action spaces, restricting its capability to fully utilize robot data of various resolutions and action spaces. In the future, we will investigate incorporating different robot data in training and accelerate the"}, {"title": "A Additional Qualitative Results", "content": "In this section, we present additional qualitative results on comparing IRASim with baseline methods.\nA.1 Video Generation of Short Trajectories\nResults are illustrated in Fig. 7. These results demonstrate that IRASim-Frame-Ada surpasses other methods in aligning frames with actions and modeling the interaction between robots and objects.\nA.2 Video Generation of Long Trajectories\nResults are illustrated in Fig. 8. IRASim-Frame-Ada generates consistent and long-horizon videos, accurately simulating the entire trajectory. Additionally, IRASim-Frame-Ada maintains its superior performance in frame-action alignment and robot-object interaction as observed in the short-trajectory setting.\nA.3 Scaling\nResults are shown in Fig. 9. IRASim-Frame-Ada consistently improves the quality of the generated video in terms of reality and accuracy with the increase of model size."}]}