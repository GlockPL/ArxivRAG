{"title": "Is It Really Long Context if All You Need Is Retrieval? Towards Genuinely Difficult Long Context NLP", "authors": ["Omer Goldman", "Alon Jacovi", "Aviv Slobodkin", "Aviya Maimon", "Ido Dagan", "Reut Tsarfaty"], "abstract": "Improvements in language models' capabilities have pushed their applications towards longer contexts, making long-context evaluation and development an active research area. However, many disparate use-cases are grouped together under the umbrella term of \"long-context\", defined simply by the total length of the model's input, including \u2013 for example \u2013 Needle-in-a-Haystack tasks, book summarization, and information aggregation. Given their varied difficulty, in this position paper we argue that conflating different tasks by their context length is unproductive. As a community, we require a more precise vocabulary to understand what makes long-context tasks similar or different. We propose to unpack the taxonomy of long-context based on the properties that make them more difficult with longer contexts. We propose two orthogonal axes of difficulty: (I) Diffusion: How hard is it to find the necessary information in the context? (II) Scope: How much necessary information is there to find? We survey the literature on long-context, provide justification for this taxonomy as an informative descriptor, and situate the literature with respect to it. We conclude that the most difficult and interesting settings, whose necessary information is very long and highly diffused within the input, is severely under-explored. By using a descriptive vocabulary and discussing the relevant properties of difficulty in long-context, we can implement more informed research in this area. We call for a careful design of tasks and benchmarks with distinctly long context, taking into account the characteristics that make it qualitatively different from shorter context.", "sections": [{"title": "1 Introduction", "content": "The ability to deal with ever-longer contexts has been one of the most notable trends among the emerging capabilities of large language models (LLMs). Starting with a few hundred tokens as the"}, {"title": "2 Task Design in Long Context", "content": "Evaluating the performance of NLP models over very long contexts is a fast-changing area of re- search (Bishop et al., 2024; Wu et al., 2024). Mea- surements are regularly updated to account for new capabilities which improve with extrapolation ar- chitectures (Vaswani et al., 2017; Su et al., 2024) and training data (He et al., 2023; Chen et al., 2023). Evaluators were tasked with designing measure- ments of long-context capabilities cheaply, effi- ciently, and quickly, while matching realistic use- cases as much as possible. The most common way of differentiating long-context tasks, besides the context's length, is whether they are naturally- constructed or synthetically-constructed (Tay et al., 2020; Bai et al., 2023; Hsieh et al., 2024).\nNatural construction. A simple yet effective way of \"moving the goalpost\" for context length is by modeling long-context tasks based on short- context tasks. This was done, for example, with QA (Ko\u010disk\u00fd et al., 2018, cf. Dunn et al., 2017), summarization (Huang et al., 2021, cf. Narayan et al., 2018), and NLI (Koreeda and Manning, 2021, cf. Williams et al., 2018). Specialized domains like legal (Bruno and Roth, 2022; Nguyen et al., 2024) and literature (Wang et al., 2022; Kryscinski et al., 2022) often involve longer texts, turning typically short-context tasks such as QA and NLI into long- context scenarios. Another more native methodol- ogy is to create new tasks which inherently require a long context, such as multi-document summariza- tion (Fabbri et al., 2019; Angelidis et al., 2021), survey generation (Gao et al., 2024), and structured data aggregation (Caciularu et al., 2024). Both methodologies share the constraint that, due to their natural construction (i.e., using natural text), once created, they are difficult to modify for longer con- texts as models' long-context capabilities improve.\nSynthetic construction. A more flexible ap- proach, sacrificing natural construction for length control, is to use distractors to synthetically in- crease the context length. This method allows for cheap and efficient (in terms of task construction cost) evaluation of models' full context length ca- pabilities, with difficulty adjusted by controlling the distractors. Tasks like Needle-in-a-Haystack (NIAH; Ivgi et al., 2023; Kamradt, 2023) and PassKey retrieval (Mohtashami and Jaggi, 2023) were created to evaluate a model's ability to pin- point specific information amid lengthy distrac-"}, {"title": "3 What Makes Long Context More than Retrieval?", "content": "We require a taxonomy to capture task difficulty variations beyond mere \u201cnumber of tokens\". We fo- cus on the information that is canonically required to solve the task as the conditioning variable. Our classification can be summarized via the following two questions, when asked about a given task:\n(I) How difficult is it to find and extract the required information?\n(II) How much information is needed to be found?\nFor instance, consider the task of summarizing a book, in comparison to a NIAH task of iden- tifying a numerical detail in a long financial re- port (e.g., \"how much did the company earn in 2015?\"). Although both tasks involve long texts, information requirement and accessibility vary sig- nificantly. The NIAH task focuses on localized, identifiable information, while summarization re- quires extracting key details dispersed throughout the text, tangled together with irrelevant content. Therefore, we can say that the book summarization task is more difficult on both axes (I) and (II).\nBelow we give more formal descriptions of the two axes characterized by the questions above.\n(I) Diffusion. Although the question above intu- itively defines \u201cdifficulty of information finding\", we offer a more concrete description. Between two similar tasks, we consider the information harder to find in one task compared to another if: (1) It is more obscured (e.g., linguistically, semantically, contextually, etc); (2) it is more sparse, such that it is interspersed with non-required information; (3) its indicators are less redundant, such that there are less places where the same information is available.\n(II) Scope. The property of scope is simpler, and refers to the minimal quantity of information needed to solve the task. Importantly, we are not concerned with precise metric for \"quantity of in- formation\" at this stage \u2013 it can refer to quantity of tokens, sentences, relations, cells in a table, etc. Any metric that reliably captures difficulty for an established solver is sufficient for our purposes.\nIllustrative example. To illustrate, consider the Wikipedia entry for New York City and a simple question: \"What is the estimated population of the city?\" Since the answer needs a small snippet of information, we say that the task has small scope. And since it is easily-accessible, we say that it has low diffusion. Consider, instead, the question \"how many syllables are in this document?\u201d \u2013 since this question requires the entire document to answer, we say that it has large scope, but if we consider counting syllables as straightforward, then we say its diffusion is still low. Finally, with the question \"Was the city's mayor elected before or after the city was affected by Hurricane Sandy?\u201d \u2013 since it requires snippets from two different areas of the text, we can say that when compared to the question about the city's population, the diffusion is higher, but not as high as for the question \u201cWhat makes the city a prominent place on the world stage?\u201d which poses a challenge on both axes."}, {"title": "4 Challenging Long Context Is Under-Explored", "content": "Revisiting the works surveyed in \u00a72, they clearly differ with respect to both scope and diffusion.\nWith respect to diffusion. The information needed for tasks ranges from easily accessible to highly diffused and difficult to detect. On low diffu- sion we have NIAH (Kamradt, 2023; Mohtashami and Jaggi, 2023) and a myriad of factual single-"}, {"title": "5 Discussion: Towards Genuinely Difficult Long-Context Task Design", "content": "Challenges. Designing meaningful long-context tasks amidst rapid model progress is profoundly challenging, making the deficiency in tasks repre- senting difficulty on both the diffusion and scope axes unsurprising. One source of this challenge is the lack of diverse, coherent long texts, as models'"}, {"title": "6 Conclusions", "content": "We present a taxonomy of factors that make long- context tasks more challenging compared to short ones. This is in contrast with the existing litera- ture that refers only to the length of the input as the hallmark of long context, and as a result ends up conflating tasks of different character when as- sessing the ability of models to understand longer text. We reviewed works on evaluation in a long- context setting and found that the most challenging setting, in which the information needed is of large scope and is highly diffused within the input, is under-explored. Finally, we suggested some leads for future work to tackle this imbalance towards a more informative long context evaluation."}, {"title": "7 Limitations", "content": "Formality. In the context of this work, we have deliberately adhered to a taxonomy based on an intuitive description, in the interest of utility to a wide diversity of research and flexibility for future work. Difficulty in searching for and extracting information, and quantity of information, are both vague terms that can only be grounded in the con- text of a specific family of tasks and use-cases. We intend for this work to serve as a call to action and a tool for a shared vocabulary in the interest of more informed long-context task design in the future, rather than to anchor the taxonomy to a specific and fragile point in time.\nRetrieval is still interesting. Although we ar- gue that small scope and low diffusion tasks are the least indicative of the model's ability to long- context capabilities, tasks that are well-served by implicit retrieval or by traditional retrieval-based pipelines are certainly relevant and useful in a va- riety of common use-cases (Stylianou et al., 2021; Bruno and Roth, 2022; Gao et al., 2023).\nOther uses for a long-context window. This pa- per deals only with long inputs that serve as inputs to a task. The long context of course can have other purposes as well, like containing many in-context examples (Bertsch et al., 2024) or containing other modalities and structures (Jiang et al., 2023)."}]}