{"title": "Experience of Training a 1.7B-Parameter LLaMa Model From Scratch", "authors": ["Miles Q. Li", "Benjamin C. M. Fung", "Shih-Chia Huang"], "abstract": "Pretraining large language models is a complex endeavor influenced by multiple factors, including model architecture, data quality, training continuity, and hardware constraints. In this paper, we share insights gained from the experience of training DMaS-LLaMa-Lite, a fully open source, 1.7-billion-parameter, LLaMa-based model, on approximately 20 billion tokens of carefully curated data. We chronicle the full training trajectory, documenting how evolving validation loss levels and downstream benchmarks reflect transitions from incoherent text to fluent, contextually grounded output. Beyond standard quantitative metrics, we highlight practical considerations such as the importance of restoring optimizer states when resuming from checkpoints, and the impact of hardware changes on training stability and throughput. While qualitative evaluation provides an intuitive understanding of model improvements, our analysis extends to various performance benchmarks, demonstrating how high-quality data and thoughtful scaling enable competitive results with significantly fewer training tokens. By detailing these experiences and offering training logs, checkpoints, and sample outputs, we aim to guide future researchers and practitioners in refining their pretraining strategies. The training script is available on Github at https://github.com/McGill-DMaS/ DMaS-LLaMa-Lite-Training-Code. The model checkpoints are available on Huggingface at https://huggingface.co/collections/ McGill-DMaS/dmas-llama-lite-6761d97ba903f82341954ceb.", "sections": [{"title": "I. INTRODUCTION", "content": "Large language models (LLMs) have achieved unprecedented performance across a variety of natural language understanding and generation tasks. These advancements have been driven by innovations in model architectures, pretraining methodologies, and the availability of large-scale, high-quality data [1]\u2013[3]. However, despite significant progress, many aspects of LLM training remain underexplored. Practical challenges-ranging from data curation and training stability to qualitative and quantitative evaluation-play a critical role in determining model performance but are often overlooked in favor of final benchmark results.\nIn this paper, we present our experience pretraining DMaS-LLaMa-Lite, a 1.7-billion-parameter LLaMa-based model, on 20 billion tokens of carefully curated data. Unlike many existing studies that focus solely on the final outputs of large-scale training, we emphasize the training process itself and the insights gleaned from it. Specifically, we document the following key aspects of our work:\n1) Training Dynamics: We analyze how validation loss and downstream benchmarks (e.g., Hellaswag, ARC) evolve over 40,000+ training steps and correlate these metrics with improvements in text fluency, coherence, and factual accuracy.\n2) Practical Lessons Learned: We demonstrate the critical importance of restoring optimizer states when resuming training from checkpoints, as failure to do so results in abrupt loss spikes and degraded model performance. We also explore the effects of hardware transitions (e.g., switching between single-GPU and multi-GPU setups) on training stability and efficiency.\n3) The Impact of High-Quality Data: By leveraging a carefully curated subset of the FineWeb-Edu dataset, we show that high-quality training data enables strong model performance, even when trained on significantly fewer tokens compared to other models such as TinyLLaMa.\n4) Qualitative Observations: Using diverse evaluation prompts, we track how the model transitions from repetitive, semantically incoherent outputs to fluent, contextually appropriate completions. These observations highlight how different stages of training contribute to linguistic and factual improvements.\nOur findings collectively offer insights into the practical challenges and trade-offs encountered during the pretraining of LLMs. By sharing detailed training logs, checkpoint artifacts, and sample completions, we aim to support reproducibility and provide guidance for both researchers and practitioners seeking to optimize their own training strategies. Importantly, this work serves as a foundation for future studies that may uncover additional best practices, methodological refinements, or novel evaluation techniques in the pretraining of large language models.\nThe remainder of this paper is organized as follows: Section II describes the model architecture, training setup, and data preprocessing. Section III presents our findings from the pretraining experience including quantitative and qualitative evaluations and a discussion of broader implications, while Section IV concludes with key takeaways and directions for future work."}, {"title": "II. MODEL CONFIGURATION AND TRAINING SETUP", "content": "Our model, DMaS-LLaMa-Lite, is a 1.7B-parameter (1716M) variant of LLaMa [3] adapted to use a GPT-2 tokenizer. The configuration details are presented in Table I.\nThe model features 36 transformer layers, each with a hidden size of 2048 and an intermediate size of 5120 in the feed-forward layers. We employ 32 attention heads, grouped into 8 key-value heads, and use a rotary positional embedding scheme. Norm layers use RMS normalization [4], and the model's non-linear activation is SiLU [5]. The model is trained on tokens produced by the GPT-2 tokenizer.\nThe training was conducted using a subset of the HuggingFaceFW/fineweb-edu 100BT [6] dataset, derived from the broader FineWeb dataset. FineWeb-Edu comprises approximately 1.3 trillion tokens of high-quality educational content, curated through a meticulous multi-stage filtering process. FineWeb itself is a 15-trillion-token dataset sourced from 96 Common Crawl snapshots, designed to"}, {"title": "III. EVALUATION AND RESULTS", "content": "Figure 1 provides a comprehensive visualization of the training process, capturing critical metrics such as training and validation loss, Hella accuracy, learning rate, gradient norm, and tokens processed per second. This visualization supplements the validation loss curve with additional context about the dynamics of training.\nAs shown in Figure 1, validation loss decreases steadily over the course of 40,000+ steps. The upward trajectory of Hella accuracy mirrors the decline in validation loss, providing an additional indicator of the model's qualitative improvements. Hella accuracy reflects the alignment of generated outputs with task-specific correctness criteria, reinforcing that lower validation loss corresponds to better adherence to instructions and prompts.\nA notable finding is the impact of resuming training without restoring optimizer states. This practice led to temporary spikes of validation loss, Hellaswag accuracy, gradient norm and degraded performance. For example, Figure 1 (A) shows the difference in validation loss trajectories before and after training restart at step 8750 and step 10250. From Figure 1 (D) we can tell that the reason was that we switched between training on one and two GPUs, resuming training without optimizer states resulted in a increase in validation loss, requiring several thousand steps to recover.\nKey Takeaways: The combined view of validation loss, Hella accuracy, and other metrics reveals a strong correlation between decreasing loss and qualitative improvements in model outputs. Furthermore, the importance of maintaining training continuity is reinforced by the observed loss spikes and reduced performance when optimizer states are not restored during checkpoint-based restarts.\nWe evaluated the model outputs at various checkpoints using a set of prompts designed to test several criteria:\nCoherence (C): Does the completion logically follow from the given prompt? Are the ideas well-connected and form a consistent narrative or argument?\nFluency and Grammar (F): Is the text grammatically correct and stylistically natural? Are there spelling or punctuation errors?\nRelevance to Prompt (R): Does the completion address the topic or instruction provided in the prompt? Does it stay on-point rather than drifting into unrelated content?\nFactual Accuracy (FA): Are factual claims correct? Does the model misstate well-known facts or historical information?\nDepth and Completeness (D): Does the completion provide a thorough and informative response given the complexity of the prompt?\nCreativity (Cr): For narrative or imaginative prompts, does the model produce original, interesting, or vivid content?"}, {"title": "IV. CONCLUSION", "content": "In this paper, we documented the pretraining experience of DMaS-LLaMa-Lite, a fully open-source, 1.7-billion-parameter LLaMa-based language model trained on approximately 20 billion tokens of a carefully curated dataset. By chronicling the model's training trajectory through validation loss, downstream benchmarks, qualitative evaluations, and practical challenges, we demonstrated how the training process influences model outputs and performance.\nOur analysis showed that model quality and coherence improve steadily as validation loss decreases. Early checkpoints produce semantically incoherent or factually dubious content, but with more training steps, completions become increasingly fluent, contextually relevant, and factually accurate. Correlation analyses further indicate that improvements in validation loss and downstream scores (e.g., Hellaswag) closely mirror enhancements in subjective quality measures.\nThe training journey underscored key best practices and pitfalls. We found that maintaining continuity in training is paramount; resuming training without restoring optimizer states led to abrupt increases in validation loss and required"}]}