{"title": "Capturing and Anticipating User Intents in Data Analytics via Knowledge Graphs", "authors": ["Gerard Pons", "Besim Bilalli", "Anna Queralt"], "abstract": "In today's data-driven world, the ability to extract meaningful information from data is becoming essential for businesses, organizations and researchers alike. For that purpose, a wide range of tools and systems exist addressing data-related tasks, from data integration, preprocessing and modeling, to the interpretation and evaluation of the results. As data continues to grow in volume, variety, and complexity, there is an increasing need for advanced but user-friendly tools, such as intelligent discovery assistants (IDAs) or automated machine learning (AutoML) systems, that facilitate the user's interaction with the data. This enables non-expert users, such as citizen data scientists, to leverage powerful data analytics techniques effectively.\nThe assistance offered by IDAs or AutoML tools should not be guided only by the analytical problem's data but should also be tailored to each individual user. To this end, this work explores the usage of Knowledge Graphs as a basic framework for capturing in a human-centered manner complex analytics workflows, by storing information not only about the workflow's components, datasets and algorithms but also about the users, their intents and their feedback, among others. The data stored in the generated Knowledge Graph can then be exploited to provide assistance (e.g., recommendations) to the users interacting with these systems. To accomplish this objective, two methods are explored in this work. Initially, the usage of query templates to extract relevant information from the Knowledge Graph is studied. However, upon identifying its main limitations, the usage of link prediction with knowledge graph embeddings is explored, which enhances flexibility and allows leveraging the entire structure and components of the graph. The experiments show that the proposed method is able to capture the graph's structure and to produce sensible suggestions. To demonstrate the feasibility of the approach, a prototype is presented.", "sections": [{"title": "1. Introduction", "content": "Machine Learning (ML) is a branch of Artificial Intelligence which focuses on building algorithms from data that can be used to perform different complex tasks (e.g., identify hidden patterns, make predictions, provide suggestions, etc.). This conversion of data into knowledge has numerous applications across multiple fields and industries, and its relevance is growing with the increasing availability and abundance of data. However, since transforming raw data into valuable information can be time-consuming and challenging, various efforts have been done to simplify some of the laborious steps of the ML workflow creation for experts, while enabling inexperienced users to construct valuable models without requiring significant programming or ML expertise. For instance, Intelligent Discovery Assistants (IDAs) are used to interactively assist users while creating Data Analytics (DA) workflows. To this end, they leverage expert derived rules, previously successful workflows, metadata about the input dataset or about the operators to propose workflows [1]. Another approach is AutoML [2], which is the automation of various ML stages by looking for the best workflow (or workflow component) by optimizing the analytical problem over a search space of algorithms and parameters.\nUsing these tools, however, is not completely straightforward for non-technical users as various decisions still have to be made throughout the whole process (e.g., selecting the analytical purpose, identifying a suitable metric, etc.). These decisions can have a huge impact on the quality of the results, hence guiding users while generating ML workflows can improve the effectiveness of the analysis and make the whole process more efficient [3]. However, generating these recommendations is not a trivial task, as they should depend on the user's characteristics (e.g., area of expertise, role in a company), user's past experiences, datasets used, etc.\nTo this end, this work proposes a method to provide recommendations"}, {"title": "2. Related Work", "content": "In the past decade, different works have developed KGs or ontologies\u00b9 to capture ML workflows and experiments. Additionally, some works have also studied the usage of such ontologies to enhance automatic decision making in the different steps of the DA process. In the following sections, the most relevant works are introduced, and the differences with our approach are stated.\n2.1. Capturing Data Analytics via Ontologies\nVarious ontologies have been created to capture different parts of the DA process. In [9], the DMOP ontology is designed to assist in making well-informed decisions at different stages of the DA process. It contains descriptions (e.g., assumptions, properties, optimization strategies, etc.) of DA tasks, workflows, algorithms, hypotheses and data. In [10], the OntoDM ontology is presented, defining a general DA space represented in a three-layered structure: the specification, implementation and application layer. In [11] the BIGOWL ontology is presented to support knowledge management in Big Data analytics, by representing how different components are"}, {"title": "3. Knowledge Graph Design", "content": "The goal of this work is to design a KG capable of capturing ML workflows together with users' information, which then will be leveraged with the objective of facilitating the creation of analytical workflows. First, the new concepts that have been included in the KG are introduced, with special attention to the user intents. Then, we explain the domain and scope of the graph, the reused resources for its creation and finally the design of its classes and properties. The resulting KG will define the expressivity considered in the rest of the paper.\n3.1. Preliminaries\nLet us start with a brief explanation of the new user's input classes that are being considered in this work, which are not present in the ontologies presented in Section 2 (see Table 1), and that will be modeled in the KG.\n3.1.1. User Intents\nUser intents in the context of data analysis state, at different levels of abstraction, the main goal of the analysis in which the user is interested without specifying the details of how it should be accomplished. Correctly identifying user intents is crucial for the generation of fit-for-purpose workflows. In a high level view, intents can be classified in five different groups [13]:\n\u2022 Describe: the aim is to provide a description of the instances or variables of the dataset. For example, clustering techniques or outlier detection methods could be used for this purpose.\n\u2022 Assess: the focus is on providing assessment of the performance of different processes or on comparing them to a baseline. Therefore, techniques such as benchmarking or KPI assessment are usually applied for this purpose."}, {"title": "3.2. Design Methodology", "content": "A KG has been developed to establish a standardized terminology for annotating ML workflows. The methodology used for its creation is based on the one presented in [15], whose steps (although modified for this work's characteristics) are explained next.\n3.2.1. Domain and scope\nThe KG has been designed to cover all the processes of the creation of ML analytical flows. For that, it should represent the different steps of the workflows, their characteristics and how they are connected. Moreover, it should take the user into account, by annotating their interaction, both the input (e.g., datasets with its characteristics, preferences, etc.) and the output (i.e., feedback), and their context (e.g., user's education, user's organization, etc.). The KG should be able to answer general queries about the DA process (e.g., Which preprocessing algorithm is more frequently used before a Random Forest classifier?) but also user specific questions (e.g., Which algorithm constraints has user-11 previously used with multiclass classification problems?).\n3.2.2. Reusing existing resources\nWhen designing KGs, data engineers are encouraged to utilize existing ones if possible as, besides reducing the effort required to model a domain, reusing existing ontologies enhances interoperability across existing and future applications."}, {"title": "4. Instantiating the Knowledge Graph", "content": "The KG presented in Section 3 has been designed with the objective to annotate the end-to-end interaction of a user and the workflows generated within the context of a DA Assistant. This is, once a user requests a ML"}, {"title": "5. Anticipating User Intents", "content": "The KG designed so far solves the problem of capturing the whole analytics workflow, including information about the users. Next, we explain how the information stored in the KG can be leveraged to assist the users when they are creating analytical workflows. To this end, we provide recommendations for their input (see Figure 1 arrow 2), which can be crucial for generating relevant workflows. For instance, if the users select a dataset, suggestions of potential analyses based on the dataset characteristics should be provided to them. It is important to note that these user inputs, enumerated\n5.1. Basic Approach\nThe first proposed method to exploit the KG involves using predefined SPARQL queries to retrieve the information needed to assist users. These queries can be defined to take advantage of past interactions with the system, both from the user requesting the experiment and from others. Therefore, it is a highly interpretable method, designed with domain knowledge heuristics, which can overcome the lack of user's interaction data by giving relevance to the expert creating the queries. Moreover, contrarily to most recommendation algorithms, it can automatically exploit new information to provide the recommendations, as there is no model to be retrained.\nTo illustrate the approach, let's think of a user who requests a workflow for the Iris dataset and his intent is classification. Then, the system could suggest the metric for which the workflow generation should be optimized. To answer it, the system starts by running a very specific query, and it keeps increasing the level of generalization until an answer is returned. For instance, the system could run Query 1: Which is the most frequent metric the user has specified in the past with the Iris dataset for a Classification task? However, the user may not have used the Iris dataset before, hence another query could be used to allow more generalization: Query 2: Which is the most frequent metric the user has specified in the past for Classification tasks? Yet, it may be the first time the user interacts with the system, hence different queries must be run in this case. Query 3: Which is the most frequent metric used with the Iris dataset for a Classification task by all the users? in case the Iris dataset has been used before, or Query 4: Which is the most frequent metric used in Classification tasks by all users? when the"}, {"title": "5.2. Exploiting the Graph's Structure", "content": "Besides offering a very powerful way of storing and representing data, the role of graphs is not only restricted to their use as knowledge bases. For\n5.2.1. Knowledge Graph Embeddings\nKnowledge graph embeddings (KGEs) are transformations of the different elements (i.e., entities and relations) of a KG into low-dimensional feature vectors, which focus on preserving as much original information from the graph as possible. There are multiple techniques to obtain these KGEs representations. Translational models (see Figure 8) represent the relationships between entities by translating the embedding of the head entity to the tail entity using the relation entity, with methods such as TransE [21], TransH [22], TransR [23] or RotatE [24]. Bilinear models optimize the bilinear product between the entities of a relationship, with algorithms such as DistMult [25] or ComplEx [26]. More recently, methods exploiting Neural Network based approaches have also been considered, such as ConvKB [27].\nIndependently of the technique, these generated KGEs can be used for the anticipation task by creating ML algorithms able to recommend or suggest the inputs to the users. These algorithms are known as Recommender Systems (RSS), which are tools that aim to provide personalized item recommendations to users based on their preferences, past interactions and/or item characteristics. KGs are an ideal data structure for RSs [28, 29], as they naturally capture the user-item interactions as well as the user and item properties. Therefore, KGE could enrich the elements' representations by capturing complex characteristics, such as relationships or context. Moreover, the embeddings allow to partially mitigate the cold-start problem for the recommendation of new artifacts, as they benefit from being linked to already embedded entities and relations. For instance, a new algorithm will have an associated Intent and will be linked to some algorithm classes and characteristics from DMOP, which will be shared with other algorithms already present in the KG. Therefore, these algorithms will be close in the"}, {"title": "5.2.2. Link Prediction", "content": "To overcome this difficulty, the Link Prediction (LP) method has been explored to act as a RS. LP is a technique that can be applied over KGs [30, 31] in order to find missing or future relations between entities. Therefore,"}, {"title": "6. Experimental Evaluation", "content": "In this section, different experiments of the application of KGEs and LP to the designed KG have been performed, having two main goals. On the one hand, they aim to evaluate the KGEs' ability to capture the whole KG, by comparing different embedding models and parameter configurations. On the other hand, they assess to which extent the LP task can be used to provide sensible recommendations in the studied domain.\n6.1. Methodology\nThe experiments have been created following the general methodology presented in [32], which focuses on good practices in the experiment design"}, {"title": "6.1.1. Goals and Systems", "content": "The experiments have been conducted to evaluate the performance of different KGEs in order to determine their ability to capture the KG and to potentially be effective for the general LP task. While doing so, the influence of the different parameters on the performance has been studied."}, {"title": "6.1.2. Services and Outcomes", "content": "To produce the experiments, the PyKeen [33] Python library has been used. This library enables the creation of reproducible KGEs experiments, with multiple functionalities, tuning parameters and models. With that, from a set of triples, KGEs have been learned in a training phase and assessed over a validation set in the evaluation stage."}, {"title": "6.1.3. Metrics", "content": "As in the vast majority of ML tasks, in KGE a loss function is optimized in the training phase to obtain the most suitable embeddings. When KGEs are learned for LP, one of the most usual loss functions is the Margin Ranking Loss (MRL):\n$MRL = \\sum_{(h_i,r_i,t_i)} max(0,m+f(h_i^*, r_i, t_i) - f(h_i, r_i, t_i))$\nThe objective of this loss function is to give a better score to positive triples (i.e., those that exist in the dataset) than to the negative ones (i.e., those which are synthetically generated by corrupting some of the entities of the positive triple [34]), emulating the LP task. When the difference between the score of the two entities is larger than the specified margin (m), no loss is accounted for. The score function f depends on the given embedding model. For instance, for TransE, which is a simple embedding technique that sees the translation as the addition of the head and relation embeddings, the scoring function is the q-norm of the mismatch of the translation in the embedding space:\n$f(h,r,t) = - ||e_h + e_r \u2013 e_t||_q$\nDuring the evaluation phase, the head (?, r, t) and the tail (h, r, ?) are removed from the triple of the test set, and LP is performed on both incomplete triples."}, {"title": "6.1.4. Parameters", "content": "The main parameters that can affect the generation of the embeddings are:\n\u2022 Models: Different proposed models capture the head and tail relationship based on different principles and assumptions. Hence, the performance on the LP task can be widely influenced by the model choice.\n\u2022 Embedding Dimension: This is the dimension of the vector space in which the entities are represented. In general, the larger the dimension, the more hidden relationships that can be captured by the model. However, increasing the dimension can lead to overfitting on the training data (and thus poorer performance over unseen triples) and computational complexity problems. Therefore, a wide range of values will be explored to find a trade-off between the complexity of the hidden relationships captured and generalization.\n\u2022 Learning Rate: The learning rate is an important hyperparameter in the training of various ML algorithms, as it determines the step size at which the model updates its parameters during the training process. It can have a significant impact on the performance of the model, as a high learning rate could cause the model to overshoot the optimal solution, leading to unstable behavior, or, on the other hand, a low learning rate could result in slow convergence or the model getting stuck in a local minimum.\n\u2022 Number of negative samples per positive sample (NPP): Previous studies [35] indicate that increasing its value may improve the evaluation metric, albeit at the cost of greater computational complexity."}, {"title": "6.1.5. Factors to Study", "content": "In the experiments, the variables that have been studied are:\n\u2022 Models: Popular translational (i.e., TransH, TransE, TransR, RotatE) and bilinear models (i.e., DistMult, ComplEx) have been studied.\n\u2022 Embedding Dimension: The values tested range from 2 to 256, in the powers of two, as it is a usual rule-of-thumb in structure sizes when GPUs are used. In all our experiments the performance of the model stabilizes with embedding sizes within the proposed range.\n\u2022 Learning Rate: The range of values explored starts at 0.1, and then exponentially lower values (e.g., 0.01, 0.001) have been assessed.\n\u2022 NPP: The explored range has been between 1 (the usual default value) and 100.\nThe remaining two variables (i.e., Negative Samples Generator and Number of Epochs) have not been studied as they have been fixed in advance. First of all, for the Negative Samples Generator, the Bernoulli generator has been used instead of the Uniform one, as it generates higher quality corrupted triples for the training phase by selecting the best entity (i.e., tail or head) to change. Regarding the Number of Epochs, although in the experimental set up Early Stopping has been enabled, the maximum value has been fixed to 300, which has not been reached by any experiment."}, {"title": "6.1.6. Evaluation Techniques", "content": "As previously mentioned, the PyKEEN library has been used to generate and evaluate the experiments. The different combinations of models and hyperparameters have first been tested in a grid search approach. To perform the experiments, the data has been split in three sets (i.e., train, test and validation) in a stratified manner. This is, they have been split taking into account that, in order to perform the test and validation phases, the entities and relationships appearing in their respective sets must have been first embedded in the training phase. To be more specific, the different sets which have been used for each task are:\n\u2022 Training Set: This is the dataset partition that is used for learning the embeddings. The different models with the specified hyperparameter configuration are trained with this set by minimizing the MRL function. It is important to note that this loss function can neither be compared between models, as they use different techniques to define distances or errors, nor between parameters (e.g., larger embedding sizes can cause the errors to be higher due to vector dimensionality, or a higher number of negative samples per positive sample causes the summation on the loss function to contain more terms). Hence, the results of the loss of the training phase have only been used to assess the performance in scenarios where all the other variables that influence it are fixed.\n\u2022 Validation Set: In order to use the correct number of epochs and to decrease the computational burden of the experiments, the embeddings have been trained with Early Stopping using the validation set. In this experimental setup, the maximum number of epochs has been set to 300, the number of epochs between validation steps to 15 and the patience to 2. With this configuration, if for an amount of time equivalent to 10% of the total time allocated for training the model has not improved, the learning is stopped.\n\u2022 Test Set: After the training has finished, the final embeddings are used to evaluate the test partition. With that, the different metrics (e.g., Hits@k, average rank, etc.) are obtained. These results have been used to compare the different models and configurations.\nOne thing to note is that with the standard LP evaluation, all the entities present in the KG are ranked and used in the Hits@k metrics. Hence, each"}, {"title": "6.1.7. Dataset", "content": "The dataset used for the generation of the embeddings is made of the triples defining the KG's schema, the triples defining the experiments (both classification and regression), the triples describing the datasets and the auxiliary triples created to instantiate the knowledge base with the scikit-learn concepts used by the workflow generator. This corresponds to more than 20.000 triples, summarized in Table 3, which have been partitioned in the three previously defined sets."}, {"title": "6.1.8. Experiment", "content": "The experiments have been distributed in two phases in order to maximize the information without excessive computational resources. On the first one, all the mentioned models have been evaluated with a not extensive number of configurations, in order to identify the general effects of the parameters on the LP performance. When needed, more concrete experiments have"}, {"title": "6.2. Results", "content": "First of all, the general metric used to evaluate the experiments has been Hits@3, due to its resemblance to the recommendation task. Additionally, as explained in Section 6.1.6, the embeddings are expected to perform significantly better in the tail predictions than in the head ones. This behavior has been clearly observed, as the tail to head average performance value is 8.2 for ComplEx, 2.7 for DistMult, 4.3 for RotatE, 8.6 for TransE, 7.7 for TransH and 9.1 for TransR. Hence, unless a different evaluation is stated, the following experiments, which explore the influence of the parameters under study on the evaluation score, have been reported with the tail prediction of the realistic (i.e., giving the average ranking position between the entities that have the same loss) Hits@3 metric.\nFor the embedding dimension parameter, the expected and previously described behaviour can be seen in Figure 10 (left). The average evaluation performance grows with the embedding dimension size, but it stabilizes (even decreases) for the high values of the dimension (i.e., 128 and 256). This behavior is represented for a fixed configuration in Figure 10 (right), where except for the RotatE model, the other models reach their best performance when the embedding dimension presents intermediate values (i.e., 32 or 64). It can also be seen that depending on the model, the embeddings are not capable of capturing the information contained in the KG when a small embedding size (i.e., 2) is used, giving an evaluation score close to 0.\nRegarding NPP, a similar behavior is observed (Figure 11, top-left), where there is a stabilization of evaluation score gain as the NPP increases. Hence, it is clear that for the addressed task, using the standard configuration of 1"}, {"title": "6.2.1. Domain Expert Validation", "content": "The LP results over the whole KG have been used to compare different models, to observe how their performance changes with different hyperparameters and to have a general idea of how well the KG has been captured. Additionally, although there is no data available for an offline evaluation"}, {"title": "7. Prototype", "content": "Finally, we developed a prototype\u00b9 to show the feasibility of our approach, which has been created as a user interface for an AutoML tool [8]. Our system assists users in specifying the ML workflows they would like to execute. The proposed interface for this tool is shown in Figure 14. As can be seen, users are assisted while stating their intent, the evaluation requirements and their constraints, by getting ordered suggestions resulting from the two approaches presented in this work.\nThe interaction is as follows: First of all, the users select the dataset they want to use (see upper-left part of Figure 14). If the dataset has not been previously used by the system, its relevant characteristics are extracted and annotated. Next, the users have to choose their Intent. Depending on the"}, {"title": "8. Conclusions and Future Work", "content": "The first objective of this work was to create a KG capable of capturing the entirety of data analytics processes, from the different workflow steps and their characteristics to the user's inputs and feedback. To this end, new concepts such as Constraints, Preferences, and Intents have been introduced to extend and connect existing ontologies (DMOP and Person).\nThe resulting KG has been populated in accordance with its schema from a variety of sources, and has been used to explore the second goal, which is the anticipation of user intentions. For that, the KG has been first leveraged for knowledge extraction through queries by creating query templates capable of retrieving the necessary information to provide recommendations. To solve the limitations posed by the querying method, the usage of knowledge graph embeddings for recommendation tasks with link prediction has been implemented, assessing its performance by the ability to capture the whole KG. Different experiments have shown that this approach can successfully capture the graph structure and give sensible recommendations through LP.\nRegarding future work, the exploitation of the KG by different modules"}]}