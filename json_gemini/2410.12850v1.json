{"title": "RECURFORMER: NOT ALL TRANSFORMER HEADS\nNEED SELF-ATTENTION", "authors": ["Ruiqing Yan", "Linghan Zheng", "Xingbo Du", "Han Zou", "Yufeng Guo", "Jianfei Yang"], "abstract": "Transformer-based large language models (LLMs) excel in modeling complex\nlanguage patterns but face significant computational costs during inference, es-\npecially with long inputs due to the attention mechanism's memory overhead.\nWe observe that certain attention heads exhibit a distribution where the attention\nweights concentrate on tokens near the query token, termed as recency aware,\nwhich focuses on local and short-range dependencies. Leveraging this insight,\nwe propose RecurFormer, a novel architecture that replaces these attention heads\nwith linear recurrent neural networks (RNNs), specifically the Mamba architec-\nture. This replacement reduces the cache size without evicting tokens, thus main-\ntaining generation quality. RecurFormer retains the ability to model long-range\ndependencies through the remaining attention heads and allows for reusing pre-\ntrained Transformer-based LLMs weights with continual training. Experiments\ndemonstrate that RecurFormer matches the original model's performance while\nsignificantly enhancing inference efficiency. Our approach provides a practical\nsolution to the computational challenges of Transformer-based LLMs inference,\nmaking it highly attractive for tasks involving long inputs.", "sections": [{"title": "INTRODUCTION", "content": "Transformer-based LLMs (OpenAI, 2023; Touvron et al., 2023) excel at modeling complex language\npatterns but come with significant computational costs. During inference, the prefill phase processes\nthe input in parallel, generating the first token and initializing the key-value cache (KV-Cache),\nwhile the generation phase recursively produces each token, adding new keys and values to the KV-\nCache (Radford et al., 2019; Brown et al., 2020). In long input tasks like document-based dialogue\ngeneration, the attention mechanism's overhead in the prefill phase leads to memory issues, making\noptimization crucial (Yang et al., 2024).\nCurrently, one of the optimization methods applicable to the prefill phase is PyramidInfer (Yang\net al., 2024), which reduces the KV-Cache size by progressively removing tokens with lower at-\ntention weights. However, the intrinsic drawback is that token removal can decrease generation\nquality as the sequence length increases. Moreover, in modern Transformer-based inference frame-\nworks, such as vLLM (Kwon et al., 2023), the prefix caching strategy is employed, which caches\nthe KV-Cache from long documents or dialogue history to reduce computational complexity during\nthe prefill phase in long document retrieval and multi-turn dialogue scenarios. Since it is difficult to\npredict which tokens may be needed in future token generation, such scenarios are not suitable for\ncompression through token eviction.\nInspired by the dependency length minimization (DLM) phenomenon (Futrell et al., 2015) observed\nin quantitative linguistics and the principles of attention mechanisms, we identified a local, short-\nrange token attention distribution pattern, referred to as the recency aware, where attention is con-\ncentrated on tokens close to the query token. Additionally, we discovered an attention distribution\npattern where the attention weights are independent of relative position, termed contextual retrieval,\ncharacterized by attention being distributed across tokens at arbitrary positions in the sequence,\nreflecting a global focus."}, {"title": "RELATED WORK", "content": "Optimization Strategies for Efficient Inference. Improving the computational and memory effi-\nciency of Transformer-based LLMs has become a key research focus, driven by the need for scalable\ninference in complex natural language processing applications. Optimization methods can be cate-\ngorized into token eviction-based and non-eviction-based approaches. Token eviction-based strate-\ngies reduce storage and computation by dynamically removing less important tokens, functioning\nas a sparse attention mechanism. BigBird (Zaheer et al., 2020) pioneered this approach by retaining\nspecific positional and random tokens, while H2O (Zhang et al., 2023) introduced attention-weight-\nbased eviction to prioritize key information. Further optimizations, such as Scissorhands (Liu et al.,\n2024) and PyramidInfer (Yang et al., 2024), evict different tokens at each layer, with PyramidIn-\nfer optimizing the prefill phase by expelling tokens with low attention at each layer progressively.\nHowever, these methods become less efficient as the decoded sequence grows, especially in tasks\nrequiring the retention of dialogue history or external knowledge.\nNon-eviction-based methods, such as DMC (Nawrot et al., 2024), maintain higher generation qual-\nity by merging the key and value of the current token with the last one in the KV-Cache. While\neffective at controlling KV-Cache growth, non-eviction methods face limitations in the prefill phase\ndue to their sequential nature, which limits parallelization. This study aims to develop a non-eviction\ntoken optimization method applicable to both the prefill and generation phases, with the goal of re-\nducing cache size in Transformer-based LLMs while maintaining generation quality comparable to\nthe original model.\nLinear RNNs. Non-linear RNNs, as the typical form of RNNs, are fundamentally constrained by\nnon-linear dependencies in state transitions, which limit their ability to perform parallel computa-\ntions across the sequence dimension. Inspired by physical systems, T-LSTM (Balduzzi & Ghifary,\n2016) introduced a pioneering approach to linearize hidden state transitions by decoupling input\nand hidden states. This linearity enables cell states to be updated through associative operations.\nBuilding on this property, GILR-LSTM (Martin & Cundy, 2018) employs a parallel scan strategy to\naccelerate computation across sequence elements.\nRecently, Linear RNNs based on state-space models (SSM) have garnered considerable attention.\nBy discretizing SSM, these models become applicable to discrete sequences, and when combined\nwith the Hippo (Gu et al., 2020) memory update mechanism, they yield the structured state-\nspace (S4) model (Gu et al., 2022). Expanding upon S4, the selective structured state-space (S6)\nmodel (Gu & Dao, 2023) further enhances sequence modeling capacity by increasing the dimen-\nsionality. By integrating S6 with gating architecture, the Mamba (Gu & Dao, 2023) was introduced,\nshowing strong potential in sequence modeling tasks. However, due to limited memory capacity,\nMamba still falls short in tasks such as sequence copying and long-range dependency modeling\ncompared to attention-based architectures like Transformer (Jelassi et al., 2024). In RecurFormer,\nwe identify scenarios that circumvent Mamba's limitations in modeling long-range dependencies,\nby focusing on cases where short-term dependencies are predominant."}, {"title": "METHODOLOGY", "content": "Overview. The process begins by identifying attention heads in the model that exhibit a strong\nshort-range focus, characterized by recency aware. Once identified, we replace the attention mech-\nanism in these heads with more efficient linear RNNs, which are better suited for modeling local\ndependencies. This replacement strategy allows us to maintain the model's generation ability while\nreducing cache size in both the prefill and generation phase. Our approach involves two steps:\n\u2022 Select replaced head(Sec. 3.1): We first calculate the recency ratio (RR) for each attention head\nin the pre-trained LLMs and assign a recency aware index (RA-I). Attention mechanisms in heads\nwith higher RA-I are prioritized for replacement by a linear RNNs.\n\u2022 Replace and continual training(Sec. 3.2): We replace the attention mechanism in selected heads\nwith a linear RNNs, enabling RecurFormer to capture local dependencies more efficiently. contin-\nual training is then conducted to restore performance, ensuring it matches the original Transformer\nwhile reducing computational costs during prefill and generation phases."}, {"title": "SELECTING HEADS FOR REPLACEMENT", "content": "The key insight behind RecurFormer is the observation that not all attention heads in a Transformer-\nbased LLM exhibit the same focus distribution. We define the recency aware as the phenomenon\nwhere the attention weights concentrate more on tokens that are temporally closer to the query token,\nwhile others exhibit a more uniform or distant focus across the sequence, which we call contextual\nretrieval. To quantify the degree of the recency aware in each head, we introduce the concept of RR,\nwhich is computed as\n$RRh = \\frac{\\sum_{|i-j|\u2264k} Ah,i,j}{\\sum_{i,j} Ah,i,j}$\nwhere $Ah,i,j$ represents the attention weight from token i to token j in head h, and k is a threshold\nthat defines the local region of token interactions, i.e., the local range around the diagonal of the\nattention matrix. A high RR indicates that the head primarily focuses on recent tokens, suggesting\nthat this head is mainly capturing short-range dependencies.\nWe begin by calculating the RR for each attention head to quantify its focus on recent tokens, aiming\nto identify attention distributions that align with the recency aware. For each sample, we compute\nthe RR for every attention head and set a threshold a. Attention heads with an RR exceeding a are\nrecorded, and we tally the number of times each head is recorded across the dataset, defining this\ncount as the head's RA-I.\nHowever, during this process, we encountered a recurring issue where certain attention heads con-\nsistently assign disproportionately high attention weights to the first token. This attention sink\nphenomenon (Zaheer et al., 2020), as illustrated in Figure 2b, distorts the RR measurement. The\nexcessive attention to the first token makes it difficult to accurately identify attention heads that are\ngenuinely focused on short-range dependencies, which the recency aware is meant to capture.\nTo address this issue, we analyzed the characteristics of the first token. In causal attention mecha-\nnisms, the first token remains unaffected by subsequent tokens during attention-based information\nmixing, leading to a relatively fixed value distribution. When this special distribution is passed\nthrough the model's fully connected layers, it results in lower L1 and L2 norms compared to other\ntokens (Yan et al., 2024), as shown in Figure 2a. A token's contribution to the final output depends\non both its attention weight and its value vector, which can be expressed as:\n$v^{update}_{t} = \\sum_{i=0}^{t} Ah,t,ivi,$\nwhere $v^{update}$ represents the updated value for token t, $Ah,t,i$ is the attention weight between token t\nand token i in head h, and vi is the value vector for token i. Despite often receiving higher attention\nweights, the first token's value vector exhibits lower L1 and L2 norms relative to other tokens, which\nlimits its overall contribution to the final output (Guo et al., 2024; Devoto et al., 2024), as Figure 2c.\nTo mitigate the influence of the first token on RR calculations, we modify the RR calculation by\nexcluding the first token. This adjustment provides a clearer measure of the head's focus on short-\nrange dependencies, offering better insight into the recency aware.\nFurthermore, when replacing the attention mechanism with a linear RNNs, the input gate in the\nlinear RNNs provides an effective solution to this issue. It projects the value vector vt for each token\nin the sequence, adjusting the value distribution across all tokens so that the value vector of the first\ntoken can become similar to those of the other tokens. Specifically, this transformation is expressed\nas $v'i = Bivi$, where B\u2081 is the weight vector of the input gate applied to vi, the original value vector\nfor token i. This projection allows the first token to appropriately influence other tokens, as it would\nin the attention mechanism, without requiring additional sequence-level modeling operations."}, {"title": "REPLACING SELECTED HEADS AND CONTINUAL TRAINING", "content": "Employing attention mechanisms in recency aware dominant heads results in inefficiencies. To\naddress this, we propose substituting these heads with a linear RNNs structure, termed Mamba,\nwithin Transformer models. Mamba (Gu & Dao, 2023), based on the S6 architecture, is designed\nto model sequence information more efficiently. Although its performance in modeling long-term"}, {"title": "EXPERIMENTS", "content": "We evaluated RecurFormer on Qwen2 (Team, 2023) and Llama2 (Touvron et al., 2023) series to\nassess cache reduction and generation quality. Ablation studies explored different \u1e9e values, and\ncontinued training confirmed RecurFormer's ability to reuse pre-trained weights. We visualized\nthe RA-I values of different heads across various models, alongside the attention distributions, and\nanalyzed the contributions of the first token."}, {"title": "GENERATION QUALITY AND CACHE SIZE REDUCTION", "content": "Backbones. We utilize the Qwen2-0.5B, Llama2-7B, and Qwen2-7B models as the base models,\nsourced from the official versions. Llama2-7B uses Multi-Head Attention (MHA), while Qwen2\nseries use Grouped Query Attention (GQA) (Ainslie et al., 2023).\nTask for Evaluating Generation Quality. Evaluating natural language generation quality is often\nsubjective. To provide an objective measure, we use the linked list reasoning task, HashHop (Magic,\n2024). In this task, the model reconstructs a linked list from its first element. We assess performance\nusing the haq metric, which is the ratio of the longest correct sequence starting from the first element\nto the total target list length, as shown in the hgg column of Table 2. This provides a clear measure\nof the model's ability to maintain quality as generation sequence length increases. Further details\nand examples are in Appendix A.1.\nTask for Statistic Cache Size. We evaluated Qwen2-0.5B, Llama2-7B, and Llama2-13B using\nrandomly generated token sequences as inputs. The results, shown in the cs10k and cs60k columns\nof Table 2, demonstrate that RecurFormer not only increases the allowable input length but also\nsignificantly reduces cache size growth as the generation length extends. When \u1e9e is 0, corresponding\nto the original model, we define the cache size under this condition to be 1.0000.\nExperiment Implementation. RecurFormer was constructed based on the original models with B\nset to 0.9, except for the model with 0.5B parameters, where \u1e9e was set to 0.5. All experiments were\nperformed on a single A100 GPU with 80G of memory, with a batch size of 1. Further details on\nthe computation of the RA-I and sample selection are provided in Appendix B.\nMain Results. The primary objective of our experiments was to demonstrate that RecurFormer\ncan significantly reduce cache size while maintaining generation quality at a level comparable to\nunoptimized models. Specifically, we aimed to validate the effectiveness of RecurFormer across\ndifferent attention mechanisms, such as MHA and GQA, as well as across models with varying\nparameter sizes. As shown in Table 2, RecurFormer achieved a cache size reduction of 89.7%\nat 10,240 tokens and 90.0% at 61,440 tokens for Llama2-7B, as indicated by the CS10k and CS60k\ncolumns, with only a minimal decrease in generation quality, where the hgq score is 0.839 compared\nto 0.894 for the unoptimized model. Similarly, for Qwen2-7B, RecurFormer reduced cache size by"}, {"title": "CONTINUAL TRAINING", "content": "To validate the effectiveness of RecurFormer in inheriting the weights from Transformer-based\nLLMs, we performed continued training on RecurFormer with a \u1e9e value of 0.5, constructed from\nQwen2-0.5B, Llama2-7B, and Qwen2-7B. The training was conducted using a masked predic-\ntion task on the Wikipedia English training set. The results, presented in Figure 3, demonstrate\nRecurFormer's ability to converge with the original models and achieve comparable performance.\nAfter convergence, we evaluated both RecurFormer and the corresponding original models on the\nvalidation set, using the masked prediction task and calculating the perplexity (PPL), as Table 3."}, {"title": "ABLATION STUDIES", "content": "To verify the necessity of retaining attention heads in RecurFormer, we extended Table 2 with dif-\nferent \u1e9e values. When \u1e9e is set to 1.0, meaning no attention heads are retained in RecurFormer, we\nobserve a significant degradation in generation quality in Table 4, with hpgq dropping to 0. This\nphenomenon occurs consistently across models with different parameter sizes, as well as those uti-\nlizing either MHA or GQA as the base models. Intrigued by this finding, we conducted additional\nexperiments using the MQAR task on randomly initialized Transformer and RecurFormer models.\nThe MQAR task requires the model to return the value corresponding to a given key in a sequence of\nkey-value pairs, with a detailed explanation of the task provided in Appendix A.2. The training set\nconsists of samples with 4 to 64 pairs, and pair lengths ranging from 64 to 256. The test set contains\nsamples with 4 to 256 pairs, and lengths ranging from 64 to 1,024. We used a randomly initialized\nTransformer model with rotary position encoding (ROPE) (Su et al., 2024) and GQA, specifically\nwith 2 layers, 512 embedding dimensions, 8 attention heads, and 4 key-value heads. After selecting\nthe heads to be replaced in each layer based on \u00df, we trained the model on the training set.\nWe recorded the proportion of correct value predictions, i.e., accuracy, on the validation set for\ndifferent \u1e9e values, as shown in Figure 4. Specific examples and descriptions of the MQAR task\nare provided in Appendix A.2. Despite the MQAR task being simpler than the HashHop task, we\nobserved that RecurFormer, with all attention heads replaced by Mamba blocks, still exhibited poor\naccuracy, indicating that retaining the attention mechanism in RecurFormer is necessary."}, {"title": "ANALYTICAL AND STATISTICAL EXPERIMENTS", "content": "RA-I Visualization. We selected attention heads with different RA-I values from the Qwen2-0.5B,\nLlama2-7B, and Qwen2-7B models and visualized their behavior, as shown in Figure 5. The figure\npresents the average attention distribution heatmaps for these heads over 512 English samples, each\ncontaining more than 1030 tokens. For the analysis, we used the first 256 tokens of each sample\nas input. Attention heads with a high RA-I tend to focus on tokens close to the query token, re-\nsulting in a bright main diagonal that reflects the model's emphasis on local context dependencies.\nIn contrast, heads with a low RA-I exhibit a more uniform attention distribution across the entire\nsequence, capturing longer-range dependencies. The attention distribution for low RA-I heads dis-\nplays a mosaic-like pattern, indicating a more global focus. This distinction suggests that high RA-I\nheads are better suited for handling short-range dependencies, while low RA-I heads play a crit-"}, {"title": "CONCLUSION", "content": "We explored the necessity of self-attention in all heads of Transformer-based LLMs and demon-\nstrated the potential of replacing some self-attention mechanisms with a Linear RNNs. By iden-\ntifying the recency aware phenomenon, where certain attention heads focus on recent tokens, we\nintroduced RecurFormer. In RecurFormer, we replace self-attention with a linear recurrent neural\nnetwork in these specific heads. This substitution reduces computational costs and cache size dur-\ning both the prefill and generation phases without compromising performance. Our experiments\nshowed that RecurFormer effectively reuses pretrained Transformer weights with continual training,\nmaintaining generation quality while enhancing efficiency. However, RecurFormer faces a key lim-\nitation in efficiently parallelizing the computation of Mamba blocks and self-attention heads within\nthe same layer. This challenge is particularly evident under small batch sizes, affecting hardware\nresource utilization, which we aim to improve in future work. By integrating linear recurrence\ninto Transformer architectures, RecurFormer provides a promising foundation for future large-scale\nmodel architectures, potentially leading to more efficient and scalable neural networks."}, {"title": "TASKS DESCRIPTIONS", "content": "HashHop provides an objective evaluation of how the quality of generated content changes with\nsequence length. In this task, the model must sequentially reconstruct a linked list, starting from the\nfirst element. Each element in the linked list is represented as a random string of length he, with the\ntarget list containing hp connections, and the total input consisting of hi characters.\nWe use the hgq metric to evaluate the model's performance, defined as the ratio of the longest correct\nsequence starting from the first element to the total length of the target list. This metric objectively\nreflects the model's ability to preserve generation quality as the sequence grows.\nFor example:\n10-17,\n9985,\n0405, 0102,\n2134, 03-04,\n6223, 02 \u2192 03,\n4273, 05-06.\nIn this case, the valid linked list is:\n01 02 03 04 05 \u2192 06.\nHere, the elements in light gray represent random pairs that serve as distractors, and the symbol \u21d2\nindicates the starting point of the linked list. The task challenges the model to connect pairs where\nthe second element of one pair matches the first of another, forming a valid linked list."}, {"title": "MQAR", "content": "For example:\nA\u21924, F1, B3, C\u21926\nFor the queries:\nA?, C?, F?  4, 6, 1.\nHere, the ? indicates the position where the model needs to predict the corresponding value for each\nkey based on the provided key-value pairs."}, {"title": "EXPERIMENT IMPLEMENTATION DETAIL", "content": "From the Wikipedia English dataset, we randomly selected 1024 samples where\nthe number of tokens exceeded 1030. For each sample, we extracted the first 1024 tokens to compute\nthe RA-I for each attention head in the Transformer-based LLMs, setting a to 102.\nConfiguration of Mamba block. The Mamba block parameters were configured as follows: dconv\nwas set to 4, dstate was set to 16, dtrank was set to 256, and kepd was set to 2.\nConfiguration of HashHop task. The HashHop dataset was generated with he set to 8, hp set to\n16, and hi set to 6144. We continued training RecurFormer on the HashHop dataset, experimenting\nwith various values of B. After completing the continued training on the HashHop dataset, the\nmodel's performance was evaluated using the haq metric.\nConfiguration of PyramidInfer. We configured PyramidInfer by adjusting the inter-layer token\neviction ratio Preduce and the minimum token retention count Pmin to match the cache size as closely\nas possible with our approach. For Llama2-7B it was set to 0.7, and Pmin was set to 32."}]}