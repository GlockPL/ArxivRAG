{"title": "A Call for Critically Rethinking and Reforming Data Analysis in Empirical Software Engineering", "authors": ["Matteo Esposito", "Mikel Robredo", "Murali Sridharan", "Guilherme Horta Travassos", "Rafael Pe\u00f1aloza", "Valentina Lenarduzzi"], "abstract": "Context. Empirical Software Engineering (ESE) is the core of innovation in the broader SE fields via its qualitative and quantitative empirical studies. Albeit empirical methodologies have driven the advances in SE practices and research, concerns over their correct application have sparked as early as 2006 in the Dagstuhl seminar on SE. So far, advancements would be expected since some previous studies have investigated it in specific software engineering areas, highlighting the misconceptions about using inadequate statistics strategies to analyze data. However, it is unclear how it has evolved since then and how it could influence confidence in decision-making based on the results of primary studies.\nObjective. To analyze three decades of SE research and look for eventual mistakes in choosing adequate statistics strategies to support the data analysis, i.e., flawed statistical techniques for analyzing data reported in the technical literature. Besides, it will evaluate current experts' accuracy in detecting and addressing these issues.\nMethods. To perform a literature survey (pilot study) in the technical literature to collect a large sample of empirical studies, i.e., circa 27k works, reporting data analysis, and filtering them using LLM to identify adequate (\"good\") and inadequate (\"bad\") statistical methodological patterns. To randomly select 30 primary studies (15 good and 15 bad) and conduct a focus group-based workshop with 33 Empirical Software Engineering experts to observe their capabilities in identifying and remediating those methodological statistics issues.\nResults. Our findings reveal widespread statistics issues within the published primary studies regarding empirical software engineering studies. Besides, the performance of the experts in identifying eventual mistakes in the use of statistical methods, models, and tests in these primary sources raises serious concerns about the general ability of empirical software engineering researchers to spot them and suggest proper adjustments.\nConclusions. Despite our study's eventual limitations, its results shed light on recurring issues from promoting information copy-and-paste from past authors' works and the continuous publication of inadequate approaches that promote dubious results and jeopardize the spread of the correct statistical strategies among researchers. Besides, it justifies further investigation into empirical rigor in software engineering to expose these recurring issues and establish a framework for reassessing our field's foundation of statistical methodology application. Therefore, this work calls for critically rethinking and reforming data analysis in empirical software engineering, paving the way for our work soon.", "sections": [{"title": "1 Introduction", "content": "The term \"Software Engineering\" first emerged in the mid-20th century, with early usages including a 1965 letter from ACM president Anthony Oettinger (Meyer, 2013; Tedre, 2014) and lectures by Douglas T. Ross at MIT in the 1950s (Mahoney, 1990). These pioneers recognized the need for a disciplined approach to software development. Margaret H. Hamilton, when working on the Apollo Guidance Computer, coined the term \"software engineering\" (SE) to denote the rigor and importance of the practices involved (Rayl, 2008; Hamilton, 2018). SE was popularized during the 1968 NATO Software Engineering Conference, which addressed the \"software crisis\", i.e., the challenges of developing complex software systems that were over budget, behind schedule, and low-quality (Hack Reactor, 2020). In response to these challenges, researchers recognized the need for a more scientific approach to software development. This led to adopting empirical methods, such as controlled experiments, case studies, and surveys, to systematically evaluate software processes and tools. Victor Basili, an empirical software engineer pioneer, advocates for using empirical studies to build a science of computer science (Basili et al., 1994).\nOver the decades, Empirical Software Engineering (ESE) has evolved to encompass various research methods, including quantitative and qualitative approaches Felderer and Travassos (2020). This evolution reflects a growing recognition of the importance of empirical evidence in understanding and improving software engineering practices. However, early warnings regarding the uncritical adoption of"}, {"title": "2 Background", "content": "This section presents the background concept that lies as the foundation of our study."}, {"title": "2.1 Empirical Studies", "content": "In SE, W\u00f6hlin et al. (2012) identified various types of qualitative and quantitative empirical studies that researchers use to explore and enhance practices in the field. These methods offer distinct ways to investigate complex phenomena and tackle research questions, each with strengths and trade-offs.\nCase studies focus on in-depth explorations of specific, real-world contexts, such as a particular software development project or team. They excel at uncovering the \"how\" and \"why\" behind outcomes, providing rich, detailed insights (Lenarduzzi et al., 2017). For instance, a case study might examine how a team adopts agile practices in a fast-paced environment. While these studies reveal valuable nuances, their findings often struggle to extend to broader contexts.\nAuthors of controlled experiments manipulate independent variables and observe their impact on dependent variables in a controlled setting. This approach is ideal for testing hypotheses and identifying causal relationships (Basili et al., 1994). For instance, a study might evaluate many static analysis security testing tools to see which helps developers detect vulnerabilities more reliably (Esposito et al., 2024a). The controlled environment ensures precision but often sacrifices real-world relevance.\nResearchers frequently turn to surveys for a broader view. These studies collect data from large groups of participants using questionnaires or interviews. Surveys are ideal for spotting trends, gauging opinions, or understanding behaviors across diverse populations (Torchiano et al., 2017). However, their breadth usually comes at the cost of the depth offered by case studies or the experimental rigor of controlled studies.\nAction research takes an interactive and iterative path, with researchers actively engaging in problem-solving within the study environment. This approach blends practical problem-solving with knowledge generation, making it invaluable for connecting theory to real-world practice (Staron, 2020). For example, researchers might collaborate with a software team to improve their testing process while simultaneously studying the impact of those changes.\nEthnographies dive deeply into cultural and social dynamics by immersing researchers in the daily lives of software engineers or teams (da Silva et al., 2013)."}, {"title": "2.2 Data Analysis Taxonomy", "content": "Data analysis in SE is a vast topic, and it would be beyond the scope of the present paper to comprehensively map all the strategies adopted by past and current authors. Nonetheless, a taxonomy is needed to enable our investigation. Therefore, we compile our data analysis taxonomy leveraging the work of Livingstone (2009) and Meloun and Militky (2011), which are the widest adopted statistical manuals, based on suggestions of colleagues and experts in the statistic domain. In other words, Livingstone (2009); Meloun and Militky (2011)'s guidelines are for statisticians what Kitchenham et al. (2004); Runeson and H\u00f6st (2009); W\u00f6hlin et al. (2012)'s guidelines are for our community. Moreover, for each category and item of the categories, we double-checked the original author paper in which the technique and test were defined. Therefore, we avoided personal interpretation of any collectors, surveyors, or users of such techniques referencing only the original author's interpretation of their work, e.g., Spearman's interpretation of its p. Therefore, our taxonomy includes the following categories and items:\nVariable type (Kaliyadan and Kulkarni, 2019). It refers to the categorization of variables (e.g., nominal, ordinal, interval, or ratio) and their roles (e.g., dependent, independent) in analysis;\nDistributional check (Meloun and Militky, 2011). It involves assessing whether data follows a specific distribution, such as normality, using tests (e.g., Shapiro-Wilk) or visual methods (e.g., Q-Q plots).\nPreprocessing (Livingstone, 2009). It covers data preparation steps like cleaning, normalization, transformation, and handling missing values to make data suitable for analysis.\nModels (Agresti, 2015). They encompass the mathematical, statistical, or AI-based frameworks used for analysis, such as regression, classification, and clustering algorithms or machine learning and neural networks.\nPerformance evaluation (Botchkarev, 2018). It focuses on metrics and techniques to assess the effectiveness of models, such as accuracy, precision, recall, or error rates.\nHypothesis testing (Soetewey, 2021). It involves statistical tests to determine whether there is enough evidence to reject a null hypothesis, such as t-tests, ANOVA, or chi-squared tests;\nPost-hoc correction (Armstrong and Hilton, 2010). It refers to adjustments made to account for multiple comparisons in hypothesis testing, reducing the risk of Type I errors, using methods like Bonferroni or Holm corrections."}, {"title": "2.3 Methodological Statistical Pattern", "content": "We define a methodological pattern as a structured pathway or sequence of steps when applying specific methods to analyze data, test hypotheses, or evaluate results. These patterns guide the researcher in choosing appropriate tools, techniques, and tests to ensure valid and reliable outcomes. A pattern can be seen as a roadmap where each decision or step is drawn from our taxonomy, ultimately influencing the quality of the analysis. We defined as:\nA good (or adequate) methodological pattern is a pathway that adheres to methodological guidelines, validates assumptions at every stage, and uses appropriate techniques based on data characteristics.\nA bad (or inadequate) methodological pattern ignores key assumptions, skips critical validation steps, or misuses statistical techniques.\nFor instance, Figure 1 presents three possible patterns: one good (case a) and two bad (cases b, c). More specifically, for case b, we note that a distributional check was carried out; it resulted in data that was not normally distributed, but a parametric test was performed. In the same vein, case c did not perform a distributional check but performed a parametric test."}, {"title": "3 Related Works", "content": "Our investigation stemmed from three main related works upon which our point of view was set. In late 2006, during the Dagstuhl seminar on SE, sparks of methodological derailment were starting to be evident. According to Basili et al. (2007), ESE faced several challenges, especially in data analysis. One of the major issues was the absence of standardized methods for aggregating and validating results across studies. Without systematic methods for meta-analysis, researchers cannot effectively corroborate findings and identify evidence gaps. Current methods, such as vote counting, are usually biased, especially in cases where the sample size is small or the effect size is minimal, Briand 2007. This limitation signifies the need to develop meta-analysis methods since those borrowed from other domains, such as medicine, have not been able to solve specific complexities of software engineering experiments (Oivo, 2007).\nData quality is another critical challenge. Researchers often have incomplete or noisy data, especially when working with software repositories. For instance, missing values or default entries can make certain attributes unreliable. These all pose serious challenges in requiring arduous validation procedures for the information to reflect correctly what it purports to reflect (Sj\u00f8berg, 2007).\nIn the case of survey studies, recollection bias, where faulty memories distort information, and \"suspicion of exposure\" bias, wherein previous knowledge of an outcome skews the analysis of the information, frequently leads to flawed results in retrospective studies of data (Kitchenham, 2007). A further limitation stems from the poor generalisability of the empirical findings. Most studies concern specific"}, {"title": "4 Methodology Overview", "content": "This section details the overall methodology of this empirical study, introducing the main goal and the adopted process. We design and conduct the empirical study according to the guidelines defined by W\u00f6hlin et al. (W\u00f6hlin et al., 2012).\nThe main goal of our study is two-fold, as follows:\nG1 Investigate the methodological statistical patterns for data analysis usage in ESE studies, distinguishing between adequate (\"good\") patterns that adhere to best practices and inadequate (\"bad\") patterns that compromise methodological rigor (as described in Section 2.3);\nG2 Evaluate the experts' inadequate methodological pattern detection and remediation capabilities.\nOur perspective is of researchers and practitioners in the ESE community.\nThe study draws on a rich corpus of ESE literature spanning the last 30 years (1994-2023). These studies encompass various domains and methodologies, reflecting the evolution of empirical data analysis practices in ESE.\nFigure 2 shows the overall study setup and data collection process. We modeled our process via the Business Process Model and Notation (BPMN) 2.0. BPMN is a business process modeling standard (Object Management Group, 2011), which provides a graphical notation to intuitively describe a method to technical users and business stakeholders for clear communication and collaboration. To lay a solid foundation for the investigation, according to Figure 2, we split the study into two steps:\n1. Pilot Study (Section 5): Large-scale analysis of patterns and trends characterized by the methods of data analyses in the literature on ESE;\n2. Workshop (Section 6): Involvement of ESE experts to verify and further inform findings from the pilot study."}, {"title": "5 Pilot Study", "content": "This section presents the methodology, results, discussions, and threats to the validity of the pilot study."}, {"title": "5.1 Study Design", "content": "This section details the goal, research questions, data collection, and analysis."}, {"title": "5.1.1 Goal and Research Questions", "content": "In the pilot study, we focus on the first goal (G1), which is to investigate the use of methodological, statistical patterns for data analysis in ESE studies. We distinguish between adequate (\"good\") patterns that adhere to best practices and inadequate (\"bad\") patterns that compromise methodological rigor. Our perspective is of researchers and practitioners in the ESE community.\nTherefore, we define two Research Questions (RQs):\nRQ1\nWhat is the historical trend of empirical data analysis methodology?\nRQ2\nWhat are the most common methodological statistical patterns in empirical software engineering data analysis?\nESE relies heavily on rigorous methodologies to produce reliable, actionable results. However, the complexity of software engineering practices and the diversity of empirical approaches often lead to inconsistencies in study designs. These methodological issues can significantly affect the quality and reproducibility of research findings (\u00c7arka et al., 2022; Esposito and Falessi, 2024). Stemming from our current peer-review experience and leveraging Dagstuhl seminar's legacy (Basili et al., 2007) with this question, we aim to conduct a pilot study on state-of-the-art data analysis methodologies applied to past ESE studies."}, {"title": "5.1.2 Study Context", "content": "This section describes the context in which our study is conducted. We first retrieved 504,062 articles from the Scopus database using a specific search query targeting a subset of the available ESE research based on their advertised empirical methodology (see Scope of the Search). After applying inclusion and exclusion criteria (detailed in Table 1), we refined the dataset to 27,140. These studies formed the basis for identifying and characterizing methodological patterns via Natural Language Processing (NLP) and large language models (LLMs)."}, {"title": "5.1.3 Study Setup and Data Collection", "content": "This section presents the data collection workflow for the pilot study.\nOverview. Due to the vast corpus of over 27k papers, we aimed to extract the relevant information leveraging NLP and LLMs automatically. To identify the variable types, which is a trickier task than looking for text matching the name of statistical tests, we used ChatGPT-4 with a self-reflection prompting technique. The rest of the paper's text was normalized to remove diacritical symbols to allow a more straightforward NLP application to infer preprocessing methods, computational models, performance metrics, and post hoc tests."}, {"title": "Scope of the Search", "content": "Our study is not a systematic literature review or mapping study. Thus, we deviate from the usual SLR guidelines (Kitchenham et al., 2004) while still safekeeping generalizability given the vast corpus of selected papers. Therefore, we chose only one search engine, Scopus, which allowed free access to its API to automate most of the literature search and papers gathering process. We employed the following search string:\nKEY (empirical) OR KEY (experiment) OR KEY (\u2018case stud*\u2019)\nWe limited the search to the SE domain and considered studies published between 1994 and 2023. We used the asterisk character (*) for the second term group to capture possible term variations, such as plurals and verb conjugations. We performed the data collection through official web APIs. We collected a total of 504,062 articles. We applied our inclusion and exclusion criteria (see Table 1), excluding 476,922, thus focusing on the remaining 27,140 research articles from major computer science publishers such as IEEE, ACM, Elsevier, and Springer.\nText Extraction. We extracted text from the research papers' methodological sections to avoid confusion in analyzing sections unrelated to the paper's methodology. These sections were identified using key terms commonly associated with research methodology, such as \"data analysis,\" \"method,\" \"design,\" \"result,\" \"discussion,\" and \"research question.\"\nDetermining Variable Type using LLM Prompting. SLR requires multiple authors to read, extract, and synthesize information from a usually large set of papers (Esposito and Falessi, 2024). In our context, reading and manually coding over 27k papers was deemed unfeasible for a pilot study. Therefore, we decided to leverage LLMs for specific data extraction tasks. Users provide input through a question or a prompt to interact with such models. This prompt can be iteratively refined to enhance the model's output and better align it with the user's goals. In the context of LLMs, prompt engineering refers to the deliberate crafting and optimization of prompts to guide models in performing desired tasks effectively (Esposito et al., 2024b). LLMs are pre-trained on extensive datasets, and prompting leverages this knowledge for task execution without requiring extensive additional training. Prompts translate tasks into structured inputs, enabling LLMs to generate relevant and accurate outputs. Prompting can be employed in zero-shot (Brown and et al., 2020), few-shot (Xia et al., 2020), or fully supervised scenarios, e.g., conversational agents (Lee et al., 2023; White et al., 2023), to achieve high performance across diverse NLP tasks. Specifically, we employed ChatGPT-4 to determine the variable type (e.g., text, nominal, binary, ordinal, interval, ratio, discrete, continuous, or time). We began with the following simple zero-shot prompt:\n{\n}\n\"role\": \"user\"\n\"Content\": \"[extracted paper's section]. \\n Question: Read the input text and determine the type of variable used for the data analysis in the research article?\""}, {"title": "Applying this prompt to the same research article multiple times often led to inconsistent results for the inferred variable type. These inconsistencies revealed a key limitation of the basic zero-shot prompting approach: a mechanism to encourage the model to evaluate or reflect on its reasoning critically. To overcome this challenge, we implemented a self-reflection-based prompting technique (Wang et al., 2023). This method explicitly guides the model to reflect on its reasoning process by:", "content": "Analyzing the input text thoroughly.\nInferring the variable type while critically assessing its alignment with the listed types.\nOffering a factual explanation grounded solely in the content of the research article, avoiding any speculative reasoning.\nThe updated self-reflective prompt was structured as follows:\n{ \"role\":\"user\"\n}\n\"Content\" : \"[extracted paper's section]. \\n Question: What is the type of variable used for the data analysis in the \\n\u2192research article? Infer the variable type, which must be one of text, nominal, binary, ordinal, interval, ratio,\ndiscrete, continuous, or time. Infer by self-reflecting on why/why not the variable type matches one of the listed types. Respond precisely based on the facts in the\n\u2192research article without hypothesizing. The response must be in a JSON format containing only variable_type 'along with its inferred value and 'explanation, containing a brief explanation of how the variable_type was inferred.\"\nReflection-based prompting introduces an additional layer of internal validation, prompting the model to critically evaluate its inferences before producing a response. This approach enhances robustness by prioritizing fact-based reasoning, ensuring that explanations are directly aligned with the input text, and reducing variability when the same content is analyzed multiple times. Leveraging this technique, we achieved consistent and factually grounded determinations of variable types across repeated evaluations.\nTo validate the model's output, two authors independently reviewed a subset of paper classifications, achieving a 95% confidence level with a 5% margin of error. Text Normalization. We perform text normalization once these sections relevant to data analysis are identified and extracted. First, we decompose the text containing diacritical accents into their simplest form, for instance, replacing \"\u00e9\" with \"e\" and the accent mark (') with ('). This critical step cleans and standardizes the text to ensure consistency, preparing it for accurate keyword detection. Then, remove any characters that are not ASCII.\nNLP Modeling. The primary goal of NLP modeling is to infer the components of data analysis used in a research article. For each data analysis step, at a high level, we identify the type of preprocessing (if any), the name of the computational/statistical model used for analysis, its performance metric for evaluation, and finally, posthoc tests to determine where significant differences exist between groups after the initial analysis using the computational/statistical model. To accomplish the NLP modeling, we rely on our taxonomy (see Section 2.2) containing a dictionary of keywords to categorize common preprocessing techniques, computational models, performance metrics, and post-hoc tests used in the research"}, {"title": "5.1.4 Data Analysis", "content": "This section presents our data analysis procedure to answer our RQs. To answer RQ1, we queried our database and hierarchically grouped the data. Therefore, we analyze the historical trends in empirical data analysis (EDA) methodology. To answer RQ2, i.e., the common methodology patterns in EDA, we leveraged Sankey Plots (SP) (Kennedy and Sankey, 1898). To analyze 30 years of data analysis methodologies and issues, SP can be a powerful tool for visualizing the evolution and distribution of methods, highlighting flows and transitions over time (Kennedy and Sankey, 1898). SP reveals patterns, significant shifts, and trends in data analysis practices by mapping the movement of methods from their inception to adoption or decline across decades. More specifically, we can show how paper falling into a specific category of the taxonomy is later distributed to the subsequent categories, like water in a pipe system."}, {"title": "5.2 Results", "content": "This section presents the early findings from our pilot and the workshop."}, {"title": "5.2.1 Historical Trends in Empirical Data Analysis Methodology (RQ1)", "content": "Figure 3 shows the ESE publication trend in the selected venues. According to Figure 3, interest in our field has increased steadily since the mid-90s, reaching more than 800 papers published in a year (2023). Following this preliminary trend, we are interested in analyzing in depth the trend of the methodological choices in terms of Variable type, data distribution checks, and hypothesis testing."}, {"title": "5.2.2 Common Methodology Patterns in Empirical Data Analysis (RQ2)", "content": "Figure 7 presents the Sanky Diagram, highlighting three examples of inadequate patterns. According to Figure 7, roughly 90% of the papers focusing on quantitative or qualitative data did not check the distribution of the collected data. We refer to this set of papers as A. About 10% of A proceeded with parametric tests, which require data to be normally distributed, and 90% of them did not employ any post hoc correction.\nSimilarly, About 10% of A proceeded with non-parametric tests, which do not require data to be normally distributed, and then circa 15% of these last ones use a parametric post hoc correction, which, in turn, required data normally distributed.\nFinally, 15% of A performed a distribution-specific test without being aware of the data distribution.\nTherefore, we can observe that different methodological issues affect the last 30 years of ESE."}, {"title": "5.3 Discussions", "content": "This section discusses our findings. This study is a pilot for our broader, ongoing evaluation of state-of-the-art literature across all digital libraries, extending beyond the current paper selection and the investigation scope.\nThe analysis of the publication trends in ESE, i.e., RQ2 gives several interesting insights about the development of methodological practices within the field. The interest of scholars has increased steadily over the years, culminating in a record number of publications in 2023, indicating a living and growing research community. However, taking a closer look at the methodological trends reveals huge gaps. Quantitative studies dominate the landscape, favoring measurable and numeric data, while qualitative studies are notably underrepresented. This may indicate a preference in the subject area for precision and generalizability over exploratory or contextual insights.\nThe trend realized in the checking of the distribution of data is worrying. Very few checked the distribution of their data; out of them, an even smaller fraction checked normality. Given that many statistical tests are based on the assumption of normal distribution, this step, if ignored, may lead to the failure of the results. Similarly, the trend in hypothesis testing shows that although some studies do this important thing, many do not, and most lean toward simpler analyses of correlation or non-parametric tests. This indicates a limited engagement with robust statistical methods that may reduce the robustness and reliability of conclusions made.\nThese trends indicate the need for more rigorous methodological thresholds in ESE research. Such checks on distribution should be comprehensive; encouraging a culture of robust hypothesis testing will increase the credibility and impact of future studies.\nMore specifically, RQ2 reveals that over the last 30 years, various methodological issues have impacted the ESE field. In particular, problems such as the blind application of tests without considering their prerequisites, i.e., testing the distribution and shape of the data, threaten the validity and generalizability of many ESE studies."}, {"title": "5.4 Threats to Validity", "content": "This section discusses the threats to the validity of our pilot study, categorized into construct, internal, external, and conclusion validity, following established guidelines (W\u00f6hlin et al., 2012).\nConstruct validity focuses on whether our measurements accurately reflect what we claim to measure (Runeson and H\u00f6st, 2009; Basili et al., 1994). One significant threat arises from the use of a single search engine, Scopus, and a specific search string that included terms like \"empirical,\" \"experiment,\" and \"case stud*.\" While this search string aimed to be broad enough to capture a wide range of ESE studies, it likely excluded relevant research that did not explicitly use these keywords excluding other empirical methodologies such as the ones presented in Section 2. This limitation could reduce the comprehensiveness of our dataset. Additionally, the predefined taxonomy used in our NLP modeling was developed with input from domain experts. However, this taxonomy may not fully capture the diverse methodologies employed in empirical software engineering studies, potentially leading to misclassification or oversight. Moreover, threats to construct validity may also arise from the behavior of the researchers conducting the study or the NLP tools themselves, potentially biasing the results (W\u00f6hlin et al., 2012). We will address those threats in the upcoming in-depth literature review on all the empirical study methodologies and the refinement of our taxonomy.\nInternal validity concerns how our study design identifies causal relationships between variables (W\u00f6hlin et al., 2012). Our reliance on ChatGPT-4 for determining variable types introduces a notable risk. Although we applied a self-reflection prompting technique to improve reliability, the inherent limitations of large language models may still result in errors or inconsistencies in the output. Similarly, the text normalization steps, such as removing diacritical marks, may have inadvertently altered or obscured key methodological details. While necessary to streamline the analysis, these preprocessing choices could compromise the results' accuracy. Potential instrumentation issues, such as errors in the tools or techniques used to extract and analyze the data, also represent a source of bias (W\u00f6hlin et al., 2012).\nExternal validity examines how well our findings generalize beyond the specific dataset used in the pilot study (W\u00f6hlin et al., 2012). While we analyzed over 27,000 articles, the decision to focus solely on Scopus-indexed studies published between 1994 and 2023 may have excluded important works from other databases, non-English publications, or gray literature. Consequently, our findings may not fully represent the broader landscape of ESE research. Moreover, although the dataset spans a wide range of methodological fields, some less common approaches may have been underrepresented, which could skew the observed trends and patterns. The small number of experts consulted during the taxonomy design also limits the generalizability of our results, although we sought to include a diverse set of empirical fields.\nConclusion validity addresses the reliability of the inferences drawn from the data (W\u00f6hlin et al., 2012). Since this study was a pilot, likely, not all methodological patterns were fully captured. This limitation may lead to premature conclusions about historical trends or prevalent practices in empirical data analysis. Furthermore, while the second and third authors independently reviewed a subset of classifications to validate the LLM outputs, the large corpus size prevented manual validation of the entire dataset. This reliance on automated processes could affect the accuracy and robustness of our conclusions. Our conclusions also rely heavily on the specific metrics chosen for accuracy, and alternative metrics might reveal additional insights (Esposito and Palagiano, 2024; Esposito and Falessi, 2024)."}, {"title": "6 ESE Experts' Evaluation", "content": "This section presents the methodology, results, discussions, and threats to the validity of the workshop."}, {"title": "6.1 Study Design", "content": "This sub-section details the goal, research questions, data collection, and analysis."}, {"title": "6.1.1 Goal and Research Questions", "content": "In the workshop, we focus on the second goal (G2) in which we evaluate the experts' inadequate methodological pattern detection and remediation capabilities. Our perspective is of researchers and practitioners in the ESE community. Therefore, we defined the last two RQs:\nRQ3\nHow accurate are the ESE experts at detecting inadequate methodological statistical patterns for data analysis?\nDespite the critical role of accurate statistical analysis in empirical research, the potential for misuse remains a significant challenge. RQ3 focuses on evaluating how well ESE experts can identify instances of statistical misuse in empirical data analysis. Finally, we must determine whether experts can suggest the correct remediation for the inadequate methodological statistical patterns for data analysis. Hence, we ask:\nRQ4\nHow accurate are the ESE experts in suggesting remediations to inadequate methodological statistical patterns for data analysis?\nIdentifying statistical misuse is only part of the solution; proposing effective remediation is equally crucial for upholding the integrity of research findings. RQ4 examines the ability of ESE experts to suggest accurate and practical solutions for correcting statistical issues in empirical analyses. This question assesses whether experts possess the diagnostic skills to identify problems and the prescriptive knowledge to guide improvements."}, {"title": "6.1.2 Study Context and Population", "content": "This section describes the context in which our study is conducted and the characteristics of the workshop participants.\nStudy Context Building on the insights from the pilot, we then conducted a workshop with 33 experts in ESE to test their capability to identify and correct methodological issues. Experts were selected based on their extensive experience in empirical research and review of methods within the software engineering domain. The data from the participants were collected in a way that is considered ethical and in line with the GDPR act on data collection and protection. The study also followed the ACM policy on human participant research (ACM, 2021).\nParticipant Demographics and Expertise. We identify three points, as follows:\nExperience: Participants included senior researchers, industry practitioners, and peer reviewers with 10-25 years of experience in ESE.\nFields of Expertise: The participants specialized in experimental design, statistical analysis, and methodological rigor within software engineering.\nGlobal Representation: Experts from diverse geographical regions were selected to reflect varied academic and industrial perspectives.\nEthical Considerations All participants were briefed on the study's objectives and provided informed consent before participation. The workshop involved anonymized research excerpts to mitigate bias, ensuring that neither author names nor publication details influenced the assessments."}, {"title": "6.1.3 Study Setup and Data Collection", "content": "According to Figure 2, regarding the workshop, we have selected 30 papers (that contain 15 adequate methodological patterns and 15 inadequate methodological patterns, as reported in Section 2.3) from the collected ones in the pilot and extracted only the methodology and data analysis sections with no reference to the authors and the actual paper to avoid bias. We prepared a task overview and gathered 33 experts split into seven groups. To each group, we provided four excerpts, two of which had inadequate practices and two of which had adequate practices. We asked the experts to read the excerpts and highlight any methodological issues they may find according to the provided taxonomy."}, {"title": "6.1.4 Data Analysis", "content": "This section presents our data analysis procedure to answer our RQs. To create the Ground Truth (GT) for our study, we classified the 30 selected paper excerpts using our taxonomy (see Section 2.2) to identify adequate and inadequate practices. We provided the experts with a copy of our taxonomy categories. Along with the taxonomy, we shared the selected paper excerpts and asked the experts to assess the practices described in these excerpts according to our taxonomy. Specifically, we requested that they evaluate each practice's adequacy or inadequacy based on the categories defined in our taxonomy. We needed to ensure their assessments aligned with the structure and criteria we used to create the GT."}, {"title": "To answer RQ3, we collected and manually coded the expert's answers in our workshop and compared them against the GT. To ensure consistency, the first two authors independently reviewed and refined the ESE expert's classifications through iterative coding, prioritizing more general keywords from the taxonomy when needed. In case of a disagreement, we discussed it and reached a consensus with the last author. To measure the expert's performance, we selected the Accuracy, Precision, Recall, F1-Score, and Matthews Correlation Coefficient (MCC), metrics that are commonly used in Information Retrieval (IR) tasks (Falessi et al., 2023) and have been successfully used in assessing human experts (Esposito and Palagiano, 2024).", "content": "Finally, to answer RQ4, from the collected answers, we classified as \"good suggestions\" [\"bad suggestions\"] an expert answer that refers to a specific keyword, model, or statistical test as correctly employed by the study, and the GT-backed up [did not back up] the expert's claim or the expert suggested a good practice that was indeed good [was not a good practice]. Moreover, we also added the keyword \"Other Suggestions\") and classified them accordingly for each category in the taxonomy for expert suggestions. We then compared the overall assessment of the paper against the ground truth to extract accuracy metrics."}, {"title": "6.2 Results", "content": "This section presents the early findings from our pilot and the workshop."}, {"title": "6.2.1 Accuracy of ESE Experts in Detecting Inadequate Methodological Patterns (RQ3)", "content": "Figure 8 presents the distribution of adequate and inadequate suggestions from the experts summarized by the taxonomy category. According to Figure 8, the experts did not reliably spot methodological flaws. On the one hand, the experts could not reliably identify the variable types, appropriate models or methods, and post hoc tests needed for the study. This led to misclassification, i.e., labeling poor analyses as good and vice versa. On the other hand, the experts reliably classified the data distribution checks correctly. Therefore, we can observe that, on average, ESE experts show limited precision in accurately"}]}