{"title": "Indiana Jones: There Are Always Some Useful Ancient Relics", "authors": ["Junchen Ding", "Jiahao Zhang", "Yi Liu", "Ziqi Ding", "Gelei Deng", "Yuekang Li"], "abstract": "This paper introduces Indiana Jones, an innovative approach to jailbreaking Large Language Models (LLMs) by leveraging intermodel dialogues and keyword-driven prompts. Through orchestrating interactions among three specialised LLMs, the method achieves near-perfect success rates in bypassing content safeguards in both white-box and black-box LLMs. The research exposes systemic vulnerabilities within contemporary models, particularly their susceptibility to producing harmful or unethical outputs when guided by ostensibly innocuous prompts framed in historical or contextual contexts. Experimental evaluations highlight the efficacy and adaptability of Indiana Jones, demonstrating its superiority over existing jailbreak methods. These findings emphasise the urgent need for enhanced ethical safeguards and robust security measures in the development of LLMs. Moreover, this work provides a critical foundation for future studies aimed at fortifying LLMs against adversarial exploitation while preserving their utility and flexibility.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have become an integral part of various industries, offering seamless integration into professional workflows and everyday internet usage (Chen et al., 2023; Zou et al., 2023). This rapid adoption is driven by extensive research and development efforts from leading organisations, resulting in a diverse array of tools such as ChatGPT by OpenAI (OpenAI et al., 2024), Gemini by Google (Hochmair et al., 2024), and Copilot by Microsoft (Peng et al., 2023), as well as white-box models like LLaMA (Dubey et al., 2024). These tools, commonly referred to as chatbots (Bahrini et al., 2023), are widely popular due to their intuitive interfaces and broad utility (Nicolescu and Tudorache, 2022).\nHowever, as LLMs continue to evolve (Kurokawa et al., 2024), concerns surrounding their \"security\" have intensified (Kumar et al., 2024; Xu et al., 2024). In this context, \"security\" refers to the ability of LLMs to generate content that adheres to ethical standards and societal moral values (Liu et al., 2025), a concept explored further in subsequent sections. A significant risk is the potential for LLMs to produce harmful or unethical content (Shen et al., 2024), which could inadvertently encourage illegal or immoral actions (Liu et al., 2023). For example, an LLM might inadvertently assist in planning a crime by generating detailed guidance, thereby facilitating harmful behaviours."}, {"title": "2 Tasks and Work", "content": "This paper undertakes several key objectives. First, it introduces a novel jailbreak method that targets the learning content of LLMs. Second, it evaluates the effectiveness and efficiency of the proposed approach on both mainstream black-box and white-box LLMs, providing robust evidence of its impact. Additionally, the study identifies significant security vulnerabilities present across all current mainstream LLMs, underscoring the urgent need for improved safeguards. Lastly, the paper proposes a defensive framework informed by these findings, offering practical solutions to enhance the security of LLMs.\nThe primary contribution of this research is the demonstration that the proposed method achieves a near-perfect jailbreak success rate across various directions in mainstream LLMs. This finding highlights a systemic issue in current models-the presence of illegitimate or irrational learning content.\nSubsequent sections will categorise and define jailbreak directions in detail, accompanied by a comprehensive analysis of the method's performance across each category."}, {"title": "3 Experiment Design", "content": "The experiment was designed to utilise Indiana Jones, which integrates three LLMs that interact across multiple rounds to achieve the jailbreak goal. Victim is the target model subjected to attack, Suspect generates prompts to guide the attack, and Checker evaluates the outputs from Victim to determine their validity. At each round, Checker decides whether the generated content aligns with the jailbreak objective while minimising hallucinations.\nThe process starts by prompting Checker to assess which direction is most suitable for the userprovided keyword to initiate the jailbreak. Once the direction is selected, Checker instructs Victim to produce relevant historical or contextual information, such as notable figures, events, or artistic works. The output is evaluated by Checker, and if deemed appropriate, it is passed to Suspect, which generates further prompts to refine or extend the content generated by Victim. This interaction continues iteratively for up to five rounds.\nSuccess is defined as the generation of jailbreak content that aligns with the keyword within these rounds. For stronger LLMs, a single round may suffice, whereas weaker models often require multiple iterations. In such cases, the intersection of outputs from different rounds is used to improve consistency and reduce hallucinations.\nIn our design, the number of rounds is set to a maximum of five, with each round marked by the end of a Victim-generated content marker. During these five rounds of dialogue, as long as the Victim produces jailbreak content, we consider the experiment successful for that keyword. Of course, success or failure should be considered a judgment call.\nIn each of the 12 identified jailbreak scenarios, at least three distinct keywords were tested to vali-"}, {"title": "4 Method", "content": "The method proposed in this paper, termed 'historical jailbreak,' leverages historical perspectives to bypass the safeguards of LLMs, prompting them to generate content that may be illegal or unethical. Jailbreaking occurs when LLMs produce such content, actively or passively, potentially causing harm to individuals or society. While this content contravenes universal human values, history reveals the existence of figures or methods that have pioneered such acts in various domains (Pavlidis, 2024).\nOur study observes that LLMs, when prompted directly about unethical or illegal actions, typically refuse to provide guidance and instead offer responsible advice, such as counselling. However, when asked indirectly-first about historical figures or precedents in unethical areas\u2014LLMs tend to respond with relevant information. Subsequent prompts about these figures' actions often lead to detailed explanations, including optimisations of harmful processes. This suggests that while LLMs reject explicit requests, they retain harmful information learned during training and can reproduce it when approached obliquely.\nTo exploit this vulnerability, we propose a multi-round dialogue approach using seemingly innocuous historical footage. That is, we have LLMs discuss the 'Grand Master' of a particular field and complete the jailbreak through these 'Masters'. Instead of explicitly selecting a jailbreak method, the user provides a harmful keyword and the LLM determines the most suitable approach. This method aims to demonstrate how historical framing can enable LLMs to bypass their safeguards and generate harmful outputs. The following sections explore the specific techniques employed in this process."}, {"title": "4.1 Figures, Artistic or Debate", "content": null}, {"title": "4.1.1 Figures Jailbreak", "content": "The Figures Jailbreak technique commences by prompting the LLM to identify notable historical figures associated with a user-provided keyword. The Victim model generates a list of relevant individuals, which is assessed for contextual appropriateness by the Suspect model. Once validated, the Victim selects the most pertinent figure and offers a detailed account of their actions, reframing these"}, {"title": "4.1.2 Artistic Jailbreak", "content": "In the Artistic Jailbreak, the user queries the LLM regarding historical artworks or creative outputs within a specified domain. The Victim produces a curated list of artworks, which is evaluated for relevance by the Suspect. Subsequent prompts guide the Victim to generate analogous artistic outputs, often imitating or reinterpreting the identified works. By framing the activity as an artistic exploration, this technique effectively persuades the LLM to provide detailed and potentially harmful instructions, all while maintaining the facade of creative inquiry."}, {"title": "4.1.3\nDebate Jailbreak", "content": "The Debate Jailbreak adopts a more discursive approach, initiating dialogue on figures or events associated with the given keyword. The Victim is tasked with presenting arguments in favour of certain actions or behaviours, emphasising their societal benefits and potential for contemporary application. The Suspect evaluates and refines these arguments through successive rounds of questioning, framing the process as an intellectual exercise in debate. This method leverages the LLM's perceived objectivity to elicit detailed and potentially"}, {"title": "4.2 Human Evaluation", "content": "Human evaluation played a crucial role in this study, enabling a nuanced assessment of the outputs generated by Large Language Models (LLMs) for ethical alignment, contextual relevance, and potential harmfulness. Evaluations were conducted independently by multiple reviewers, with ambiguous cases resolved through consensus.\nTo quantify inter-rater reliability, we employed Cohen's kappa (\u043a), a statistical measure that accounts for agreement occurring by chance. The formula is defined as:\n$\\\u041a=\\frac{P_o - P_e}{1- P_e}$ (2)\nwhere $P_o$ represents the observed agreement, or the proportion of evaluations where reviewers reached consensus, and $P_e$ denotes the expected agreement, which is the probability of agreement occurring by chance based on the distribution of categories.\nHigh scores across evaluation dimensions validated the consistency and reliability of the reviewers' assessments. This integration of Cohen's kappa ensured a statistically rigorous evaluation process, underscoring the importance of human oversight in identifying LLM vulnerabilities and enhancing the study's credibility."}, {"title": "5 Harmful Behaviour", "content": "In this section of the paper, harmful behaviours will be defined in detail. This paper summarises twelve harmful behaviours that occur because of inappropriate learning content in LLMs in Table 1.In response to this table, a description of the possible problems is given here.\nQ1: Why Fraud is separated into a separate Scenario rather than being subsumed under Illegal Activity?\nA1: We recognise that Fraud is indeed a subset of Illegal Activity. Fraud is biased towards the economic sphere and has an impact on the economic scenario of an individual or business. As Victims and participants of wire fraud have been on the rise in recent years, and the frequency of wire fraud in recent years has made it the most accessible criminal activity to the general public. This, coupled with the low cost of the offence of telecommunication fraud, makes it necessary for the general public to take precautions against it with greater care and more energy. The reason for this paper's separate item is to demonstrate that all of the current mainstream LLMs tools are potentially capable of generating content that contributes to fraud. We hope that companies and developers will put stricter controls on these tools so that they don't become accomplices to scammers.\nQ2: Why does this table not contain jailbreak directions for generating private content?\nA2: As mentioned in the previous section, the approach of this paper is based on the irrationality of the learning content of LLMs. All jailbreak methods and directions are based on what LLMS are not supposed to learn.In our experiments and research we have found that the learning content"}, {"title": "6 Analysis and Evaluation", "content": "This section evaluates the proposed jailbreaking method in Indiana Jones by comparing its performance with existing approaches, such as DeepInception (Li et al., 2024), PAIR (Chao et al., 2024), RedQueen (Jiang et al., 2024), and MM-SafetyBench (Liu et al., 2024). Metrics such as efficiency and robustness are analysed to demonstrate the effectiveness of our approach in both black-box and white-box LLMs."}, {"title": "6.1 Analysis of Results", "content": "In Table 2, we report the success rates of five LLMs-ChatGPT-40, Claude, Llama3.2, Qwen2.5, and Gemma2-across twelve jailbreak scenarios."}, {"title": "6.2 Evaluation of Results", "content": "In Figure 3, we compare the proposed method, Indiana Jones, with leading jailbreak approaches. The evaluation spans twelve scenarios across five models, with metrics including jailbreak success rates and the ARS (Attack Success Rate). Detailed data is provided in Appendix A.\nOn black-box models like GPT-40, Indiana Jones achieves a remarkable success rate of 98.9%, significantly surpassing DeepInception's 26.1%. Similarly, on white-box models, Indiana Jones consistently matches or outperforms DeepInception, demonstrating its adaptability to diverse architectures. The slightly reduced performance on Llama models is attributed to their exceptionally stringent safety mechanisms rather than any limitation of Indiana Jones. Unlike DeepInception, which relies on complex, multi-layered prompt engineering, Indiana Jones utilises inter-model dialogues and automated keyword analysis, ensuring efficient and reliable jailbreaks.\nRED QUEEN ATTACK, which employs multi-round dialogues and covert prompts, achieves competitive success rates, such as 64.8% on GPT-40 and 55.5% on Llama3.2. However, Indiana Jones surpasses it consistently, particularly in generat-"}, {"title": "7 Defence Test and Ablation Experiment", "content": "This section describes how the methods in this paper perform in the presence of additional defences, and what happens when one of the key prompts of the paper is removed.\nTable 3 shows the ARS of the different models in the presence of defence for the baseline tested in this paper. It is easy to find Indiana Jones remains resilient under advanced defences like selfreminders(Xie et al., 2023) or in-context(Zhou et al., 2024) adjustments. Indiana Jones's ARS seems to be dropping a lot but its still manageable"}, {"title": "8 Strengths and Limitations", "content": "Strengths:\nLow Resource Usage: The lightweight design ensures scalability and minimal computational costs.\nBroad Coverage: Addresses a wide range of jailbreak scenarios in both white-box and proprietary LLMs.\nRobustness: Performs consistently well against models with stringent safety measures.\nLimitations:\nLanguage Scope: Focused on English and Chinese, limiting applicability to other languages.\nSimulated Evaluations: Relies on controlled experiments, reducing direct real-world validation."}, {"title": "9 Conclusion", "content": "In this work, we introduced Indiana Jones, a robust and resource-efficient framework for exposing critical vulnerabilities in contemporary large language models (LLMs). Indiana Jones excels in several key aspects: its robustness enables consistent performance even against enhanced safeguard mechanisms; its optimised design significantly reduces computational overhead, making it more accessible to researchers with limited resources; and its intuitive, extensible architecture provides a solid foundation for further exploration of LLM security challenges.\nAs LLMs increasingly integrate into various facets of society, ensuring their security and ethical alignment is paramount. The success of Indiana Jones not only sheds light on the limitations of existing defence mechanisms but also underscores the pressing need for a collective effort from developers, researchers, and policymakers to prioritise security. We encourage the community to build upon this work, addressing the risks inherent in LLMs and establishing robust frameworks and regulations to guide their future development.\nBy fostering collaboration and maintaining a focus on safety, we can ensure that the advancement of AI technology aligns with societal values, paving the way for a secure and beneficial AI ecosystem."}, {"title": "A Table Appendix", "content": "A total of four baselines have been compared with this paper, and the table below maps the jailbreak"}, {"title": "B Figure Appendix", "content": "Shown next are the first three of five rounds of dialogue content using the bank robber as an example. Since this is an automated dialogue, the focus is on showing what Victim and Suspect output."}]}