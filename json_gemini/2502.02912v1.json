{"title": "MobiCLR: Mobility Time Series Contrastive Learning for Urban Region Representations", "authors": ["Namwoo Kim", "Takahiro Yabe", "Chanyoung Park", "Yoonjin Yoon"], "abstract": "Recently, learning effective representations of urban regions has gained significant attention as a key approach to understanding urban dynamics and advancing smarter cities. Existing approaches have demonstrated the potential of leveraging mobility data to generate latent representations, providing valuable insights into the intrinsic characteristics of urban areas. However, incorporating the temporal dynamics and detailed semantics inherent in human mobility patterns remains underexplored. To address this gap, we propose a novel urban region representation learning model, Mobility Time Series Contrastive Learning for Urban Region Representations (MobiCLR), designed to capture semantically meaningful embeddings from inflow and outflow mobility patterns. MobiCLR uses contrastive learning to enhance the discriminative power of its representations, applying an instance-wise contrastive loss to capture distinct flow-specific characteristics. Additionally, we develop a regularizer to align output features with these flow-specific representations, enabling a more comprehensive understanding of mobility dynamics. To validate our model, we conduct extensive experiments in Chicago, New York, and Washington, D.C. to predict income, educational attainment, and social vulnerability. The results demonstrate that our model outperforms state-of-the-art models.", "sections": [{"title": "1. Introduction", "content": "Urban regions are dynamic and complex spaces where diverse social and economic structures interact with human activity. Comprehending this intricate relationship is imperative for fostering the development of livable and sustainable cities [39]. In recent years, utilizing various urban data to learn the latent representations of urban regions has gained significant attention in the field of urban computing [12, 18, 16, 40]. Urban region representation learning involves transforming the diverse attributes of urban regions into a latent vector space, facilitating a deeper understanding of urban dynamics. The embeddings derived from this process have proven valuable for a range of applications, such as land-use classification [38], socio-economic and demographic indicator prediction [14], and crime prediction [29].\nWith advancements in sensing technology, the use of mobility data\u2014such as taxi trip records\u2014has proven effective in learning semantically meaningful embeddings of urban regions. Numerous studies have attempted to use trip record data to learn region embeddings [29, 34, 15, 38, 17, 37]. These studies can be divided into three categories: sequence modeling, OD (origin-destination) matrix reconstruction, and contrastive learning. The sequence modeling approach includes methods that capture OD co-occurrences to learn region representations [34] or generate random-walk-based sequences on spatial and flow graphs using mobility flow data [29]. The OD matrix reconstruction method involves reconstructing conditional trip distributions based on the influx and outflux of human mobility from a region over a specified period [15, 38]. Lastly, the contrastive learning approach generates influx and outflux embeddings from trip record data and applies a contrastive loss to obtain the region representations [37, 17].\nWhile previous methods have demonstrated effectiveness, they exhibit limitations in two key aspects. First, they often fall short in fully leveraging the rich temporal dynamics embedded in mobility data. In particular, the temporal patterns of inbound and outbound flows offer valuable insights into the functional dynamics of urban regions. For instance, regions with high morning outflux and evening influx typically serve as residential areas, while increased"}, {"title": "2. Related Work", "content": "Human mobility data has become a valuable resource for gaining insights into various aspects of urban dynamics, and numerous studies have integrated it to improve our understanding of urban environments [4, 6, 2, 33, 20, 19, 13]. Mobility data provides valuable insights into urban flow patterns [20, 5], hazard exposure [4, 33], region recommendation [19], and fine-grained socio-economic assessments [6, 2]. Recently, several studies have combined mobility data with deep learning techniques to deepen this understanding, focusing on three main approaches: sequence modeling [34, 29], OD matrix reconstruction [15, 38, 31], and contrastive learning [17, 37].\nIn sequence modeling methods, region embeddings are extracted by treating individual vehicle origin-destination trip chains as sequences and modeling the co-occurrence of origin and destination regions to learn region representations. [34]. Alternatively, inflow and outflow data from taxi trip records are used to weight the edges of a geo-spatial graph. A random walk on this weighted graph generates sequences that, combined with the skip-gram objective, produce region embeddings [29]. In OD matrix reconstruction methods, the OD pair and trip volume information are used to model inter-region interactions as a conditional trip distribution [15, 38]. Lastly, in the contrastive learning approach, inflow and outflow representations are averaged to obtain overall region representations, followed by the application of contrastive loss [17, 37]."}, {"title": "2.2. Contrastive Time Series Representation Learning", "content": "Supervised learning relies heavily on labeled data, which can be challenging and expensive to obtain in real-world scenarios. To overcome these limitations, contrastive learning has emerged as a promising approach for learning intrinsic patterns present in data without relying on external annotations or labels. It uses positive or negative pairs of data to learn representations. Through data augmentation, the positive pairs comprise two augmented versions of the same input data. In contrast, negative pairs are formed using different input samples. During the training process, the model maps input samples into the latent vector space, aiming to bring positive pairs geometrically closer, while the negative pairs are pushed further apart.\nBuilding on this foundation, contrastive learning techniques for time series data have shown promising results. For example, Franceschi et al. [11] employed time-based negative sampling to encourage the model to learn semantics similar to those of the smapled sub-series. Eldele et al. [9] employed augmentation techniques, including scaling and permutation, to extract representations invariant to transformations. Tonekaboni et al. [27] considered the temporal dependency of time series data by ensuring that neighboring timesteps are distinguishable from those that are not adjacent. Yue et al. [36] used hierarchical contrastive loss to extract a contextually invariant representation. Wickstr\u00f8m et al. [30] proposed a novel augmentation strategy to predict the mixing proportion of time series samples. Meng et al. [21] employed hierarchical clustering to construct contrastive pairs, thereby preventing the inclusion of instances with semantics similar to false-negative pairs."}, {"title": "3. Definition and Problem Statement", "content": "In this Section, we provide a formal definition for the mobility time series and present the problem statement."}, {"title": "3.1. Inbound and Outbound Time Series", "content": "The hourly counts of inbound and outbound trips for each location n were computed from the collected mobility data. This yields, the time series $x_n^i = \\{x_{n,t}^i\\}_{t=1}^T \\in \\mathbb{R}^{T\\times 1}$ and $x_n^o = \\{x_{n,t}^o\\}_{t=1}^T \\in \\mathbb{R}^{T\\times 1}$ for the inbound and outbound trips for $\\forall n \\in \\{1, 2, ..., N \\}$, respectively. Here, T denotes a given time interval, and N represents the number of areas of interests. Consequently, we obtain the mobility time series $x_n = [x_n^i, x_n^o] \\in \\mathbb{R}^{T\\times 2}$ by concatenating those two time series $x_n^i$ and $x_n^o$."}, {"title": "3.2. Problem Statement", "content": "For a set of mobility time series $X = \\{x_1,x_2,...,x_n,...,x_N\\}$ of N regions, the objective is to train a neural network $f_\\theta$ that maps each $x_n$ to their corresponding representation $h_n \\in \\mathbb{R}^D$, where D is the dimension of the latent vector space."}, {"title": "4. Methodology", "content": "In this Section, we introduce the MobiCLR framework. We first present an overview of the proposed model, followed by a explanation of the steps involved in learning region representations."}, {"title": "4.1. Model Architecture", "content": "The overall framework of MobiCLR is shown in Figure 1. The proposed framework involved the application of data augmentation to the original time series, followed by the extraction of transformation-invariant features using time series encoders. Subsequently, the model was trained to learn contextual information regarding urban regions related to both the origin and destination characteristics of human mobility.\nThe proposed model employs three types of encoders. The encoders $f^i_\\theta$, $f^o_\\theta$, and $f^{io}_\\theta$ take $x^i$, $x^o$, and $x^{io}$ as inputs, respectively. The encoders $f^i_\\theta$ and $f^o_\\theta$ capture the temporal context of the time series of outbound/inbound trips, respectively. On the other hand, $f^{io}_\\theta$ extracts overall semantics that encompass both inbound- and outbound-specific features. Thus $f^{io}_\\theta$ aims to capture the characteristics of both inbound and outbound activities, allowing for a more"}, {"title": "4.2. Data Augmentation", "content": "Data augmentation techniques were utilized in conjunction with contrastive learning to extract meaningful semantics from mobility time series data. In this study, a two-step data augmentation process involving jittering and shifting was implemented. The resulting representations from the encoder $f_\\theta$ of the timestamp t from two augmentations of $x_n$ are represented as $h_{n,t}$ and $h'_{n,t}$. The augmentation strategy is explained as follows.\n\u2022 Jitter: Independent and identically distributed Gaussian noise is added to each time step, sampled from a Gaussian distribution with a mean of 0 and a standard deviation of 0.2 (i.e., $\\epsilon_i \\sim \\mathcal{N}(0, 0.2)$). Each time step is now jittered as $x'_i = \\epsilon_i \\times x_i$.\n\u2022 Shift: The time series data was shifted by a single random scalar value, obtained by sampling from a Gaussian distribution with a mean of 0 and a standard deviation of 0.2 (i.e., $\\epsilon \\sim \\mathcal{N}(0, 0.2)$). Each time step was then shifted as $x'_t = \\epsilon + x_t$.\nTo enhance clarity, a graphical illustration of augmentation strategy is provided in Figure 2."}, {"title": "4.3. Learning Objectives", "content": "To extract information embedded within mobility patterns, we utilize instance-level contrastive loss and auxiliary regularizer. Through instance-level contrastive loss, the model learns flow-specific characteristics, while the auxiliary regularizer enables it to capture comprehensive regional mobility patterns."}, {"title": "4.3.1. Instance-level contrastive loss", "content": "To apply contrastive loss, the projection headers $g^i_\\theta$, $g^o_\\theta$, and $g^{io}_\\theta$ were employed on each of the representation vectors $h^i_n$, $h^o_n$, and $h^{io}_n$, respectively. This yielded $z^i_{n,t} \\in \\mathbb{R}^{T\\times F}$, $z^o_{n,t} \\in \\mathbb{R}^{T\\times F}$, and $z^{io}_{n,t} \\in \\mathbb{R}^{T\\times F}$ for $\\forall n \\in \\{1,2, ..., N\\}$.\nAn instance-level contrastive loss was applied to capture the distinct semantics of inbound and outbound trips within regions. From the data augmentation, the embeddings $z^i_{n,t}$, $z'^i_{n,t}$, $z^o_{n,t}$, and $z'^o_{n,t}$ were obtained, which were then used to compute the inbound-specific contrastive loss $\\mathcal{L}^i$ and outbound-specific contrastive loss $\\mathcal{L}^o$ by leveraging the normalized temperature-scaled cross entropy (NT-Xent) loss [25, 32, 23]. Let $sim (a, b)$ denote the cosine similarity between vectors a and b, and let $\\tau$ be the temperature hyperparameter. We denote the similarity between the representations $z_{n,t}$ and $z_{m,t}$ as $exp \\left( sim \\left(z_{n,t}, z_{m,t}\\right) /\\tau\\right)$. The contrastive losses for positive pairs $(z_{n,t}, z'_{n,t})$ and $(z^o_{n,t}, z'^o_{n,t})$ are formulated respectively as:\n$\\mathcal{L}^i = - \\log \\frac{\\exp \\left( sim \\left(z^i_{n,t}, z'^i_{n,t} \\right) /\\tau \\right)}{\\sum_{m \\in B} 1_{[m\\neq n]} \\exp \\left( sim \\left(z^i_{n,t}, z^i_{m,t} \\right) /\\tau \\right) + \\sum_{m \\in B} \\exp \\left( sim \\left(z^i_{n,t}, z^i_{m,t} \\right) /\\tau \\right)}$ \n$\\mathcal{L}^o = - \\log \\frac{\\exp \\left( sim \\left(z^o_{n,t}, z'^o_{n,t} \\right) /\\tau \\right)}{\\sum_{m \\in B} 1_{[m\\neq n]} \\exp \\left( sim \\left(z^o_{n,t}, z^o_{m,t} \\right) /\\tau \\right) + \\sum_{m \\in B} \\exp \\left( sim \\left(z^o_{n,t}, z^o_{m,t} \\right) /\\tau \\right)}$\nwhere B is a batch size and $1_{[m\\neq n]}$ is an indicator function that is 1 if $m \\neq n$ and 0 otherwise. Consequently, the inbound-specific contrastive loss $\\mathcal{L}^i$ and outbound-specific contrastive loss $\\mathcal{L}^o$ for a minibatch are obtained as\n$\\mathcal{L}^i = \\frac{1}{\\|B\\| \\times T} \\sum_{n \\in B} \\sum_{t=1}^{T} \\mathcal{L}^i (z^i_{n,t}, z'^i_{n,t})$ \n$\\mathcal{L}^o = \\frac{1}{\\|B\\| \\times T} \\sum_{n \\in B} \\sum_{t=1}^{T} \\mathcal{L}^o (z^o_{n,t}, z'^o_{n,t})$"}, {"title": "4.3.2. Auxiliary regularizer", "content": "An auxiliary regularizer was proposed to improve representation learning by aligning output features with inbound- and outbound-specific characteristics. This regularizer bridges these distinct representations, combining inbound and outbound dynamics into a unified and comprehensive view of regional mobility patterns. In contrast to Eqs. (2) and (3), the approach utilizes average pooling along the temporal dimensions of the $z^i_{n,t}$, $z^o_{n,t}$, and $z^{io}_{n,t}$ to generate summary vectors, $z^i_{n,*}\\in \\mathbb{R}^{F}$, $z^o_{n,*}\\in \\mathbb{R}^{F}$, and $z^{io}_{n,*}\\in \\mathbb{R}^{F}$, respectively. Subsequently, contrastive loss was applied to maximize the agreement between $z^i_{n,*}$ and $z^{io}_{n,*}$, as well as between $z^o_{n,*}$ and $z^{io}_{n,*}$.\nThe contrastive losses for the positive pairs $(z^i_{n,*}, z^{io}_{n,*})$ and $(z^o_{n,*}, z^{io}_{n,*})$ are defined as:\n$\\mathcal{L}^{io} \\left(z^{io}_{n,*}, z^{io}_{n,*}\\right): = - \\log \\frac{D_\\tau \\left(z^{io}_{n,*}, z^{io}_{n,*}\\right)}{\\sum_{m \\in B} 1_{[m\\neq n]} D_\\tau \\left(z^{io}_{n,*}, z^{io}_{m,*}\\right) + \\sum_{m \\in B} D_\\tau \\left(z^{io}_{n,*}, z^{io}_{m,*}\\right)}$\n$\\mathcal{L}^{ao} \\left(z^{io}_{n,*}, z^{io}_{n,*}\\right): = - \\log \\frac{D_\\tau \\left(z^{io}_{n,*}, z^{io}_{n,*}\\right)}{\\sum_{m \\in B} 1_{[m\\neq n]} D_\\tau \\left(z^{io}_{n,*}, z^{io}_{m,*}\\right) + \\sum_{m \\in B} D_\\tau \\left(z^{io}_{n,*}, z^{io}_{m,*}\\right)}$\nThen, the auxiliary regularizers for a mini-batch are computed as:\n$\\mathcal{L}^a = \\frac{1}{\\|B\\|} \\sum_{n \\in B} \\left[ \\mathcal{L}^{io} \\left(z^{io}_{n,*}, z^{io}_{n,*}\\right) + \\mathcal{L}^{ao} \\left(z^{io}_{n,*}, z^{io}_{n,*}\\right) + \\mathcal{L}^{ao} \\left(z^{io}_{n,*}, z^{io}_{n,*}\\right) \\right]$\nIncorporating regularizers into the model enhances its learning capacity by exposing it to a wide range of positive and negative sample variations. Consequently, the model generates more robust and informative representations."}, {"title": "4.4. Overall objective", "content": "Finally, the overall objective function $\\mathcal{L}$ is obtained during training as the combination of $\\mathcal{L}^i$, $\\mathcal{L}^o$, and $\\mathcal{L}^a$, where $\\mathcal{L}^i$ and $\\mathcal{L}^o$ enforce the learning of inbound- and outbound-specific features, respectively, while $\\mathcal{L}^a$ aligns output features with inbound- and outbound-specific features. The overall objective function $\\mathcal{L}$ is formulated as follows:\n$\\mathcal{L} = \\mathcal{L}^i + \\mathcal{L}^o + \\mathcal{L}^a$"}, {"title": "5. Experiments", "content": "This Section describes the datasets used in the experiment, as well as the baseline models, experimental setup, and performance comparison."}, {"title": "5.1. Experimental Setup", "content": "The entire codebase was implemented in PyTorch [24]. We trained the model exclusively on the training set, and the pre-trained model was applied to the test set to obtain representations. Specifically, 75% of the data were used for training, while the remaining 25% were used for testing. Results were reported as the average of 5 runs. The default batch size was set to 4, and the learning rate was fixed at 0.0001. The model was trained for 30 epochs and the representation dimension was set to 128. The kernel size for all one-dimensional convolution layers was set to 3, and each hidden dilated convolution had a channel size of 128. The temperature parameters $\\tau$ and $\\tau_a$ were set to 1 and 0.1, respectively. All the experiments were conducted on an NVIDIA GeForce RTX 3090GPU."}, {"title": "5.2. Datasets", "content": "We utilized taxi trip record data to extract region embeddings and evaluated the proposed model by predicting single indicators, including educational attainment and income, as well as a composite index, specifically the Social Vulnerability Index. Two weeks of taxi trip data were collected across three major US cities: New York, Washington D.C, and Chicago. The taxi trip data were acquired from the open data portal of each city1 2 3. The statistics of the mobility time series data are listed in Table 1. For single indicator prediction, educational attainment and per capita income data were retrieved from the American Community Survey (ACS) API 4. For the composite index prediction, social vulnerability index was used. The social vulnerability index employs a 16-factor such as unemployment, health insurance, age, and vehicle access. Social vulnerability index for the three cities were sourced from the Centers for Disease Control and Prevention (CDC) 5. A detailed description of the dataset is provided below:"}, {"title": "5.3. Baselines", "content": "We compared MobiCLR with the following six baseline methods: (1) GAT [28], which is a GNN model that aims to train a two-week static OD flow; (2) $x^o$, which is the raw hourly count of outbound trips; (3) $x^i$, which is the raw hourly count of inbound trips; (4) Mixing-up [30], which is a time series representation learning model that aims to predict the mixing proportion of two time series samples; (5) TS-TCC [9], which is a time series representation learning model that uses a transformer-based autoregressive model to capture contextual information; and (6) TS2vec [36], which is a time series representation learning model that incorporates contextual consistency through hierarchical contrastive loss. The reproduction details of the baselines can be found in the Appendix."}, {"title": "5.4. Prediction Results", "content": "To evaluate the performance of the MobiCLR model, we followed the protocol presented in [36]. After the pre-training phase, the network parameters were fixed, and a ridge regression model was implemented at the top of the network. Subsequently, the encoders $f^i_\\theta$ and $f^o_\\theta$ were discarded, and only $f^{io}_\\theta$ was utilized for downstream applications.\nTo obtain region-level representations, the pooling of $h^{io}$ across all timestamps was employed, resulting in $h^{io}$, which serves as the input for linear regression model. The regularization term of the linear regression model was selected through a grid search of {0.1, 0.2, 0.5, 1, 2, 5, 10}. The model's prediction performance for both single (educational attainment and income) and composite (social vulnerability index) index was evaluated using $R^2$.\nTable 2 presents the regression results for downstream applications in Chicago, New York, and Washington D.C. The results demonstrated that MobiCLR outperformed non-neural network models and state-of-the-art unsupervised methods across all downstream tasks in the three cities. It was observed that the performance of the non-neural network models were relatively poor compared to the neural networks models. The results also highlighted the disparate predictive capabilities of $x^o$ and $x^i$. This implies that semantic differences exist between inbound and outbound trips. Further, the GAT model, trained on aggregated OD flow information without considering temporal dynamics, exhibited lower performance than the time series representation learning models in most cases. This observation suggests that incorporating temporal dynamics into mobility data is effective. Additionally, it is worth noting that predicting the social vulnerability index, which account for the diverse elements of the urban environment, proved to be particularly challenging. However, MobiCLR demonstrates substantial performance gains in predicting social vulnerability across"}, {"title": "6. Analysis", "content": "This Section assesses the choice of data augmentation strategy, conducts ablation studies to determine the significance of the model components, and explores the transferability of the knowledge learned across different cities."}, {"title": "6.1. Data Augmentation", "content": "To analyze the impact of data augmentation systematically, several common augmentations, including scaling, jittering, shift, and dropout, were considered. The objective was to evaluate the performance of the framework when the augmentations were applied individually or in pairs. Figure 3 presents the social vulnerability index prediction results obtained by the framework for the three cities, showing the outcomes using individual transformations and the compositions of the transformations, along with their corresponding average scores.\nOur investigation revealed that there existed no universal augmentation strategy to achieve optimal prediction results for all cities, as demonstrated by the results shown in Figure 3a to 3c. The effectiveness of different data augmentation techniques varied depending on the specific dataset. Each dataset in the three cities may have unique characteristics and patterns, which may require different augmentation strategies to improve model's performance. Nonetheless, a data augmentation strategy that includes shift after the jitter consistently produced robust results across all cities, as evidenced by the average $R^2$ of social vulnerability prediction for the three cities presented in Figure 3d."}, {"title": "6.2. Ablation Study", "content": "The proposed model employs a joint learning objective, aiming to capture both inbound/outbound specific semantics and their correspondence. The hypothesis behind this approach suggests that combining these aspects"}, {"title": "6.3. Sensitivity Analysis", "content": "We investigate the robustness of the proposed model. To begin, we analyze the impact of three parameters: batch size (4, 8, 12) and embedding size (64, 128, 256) on the model's performance. Figure 4 presents the performance of the proposed model corresponding to different values of. In general, the results indicate that the model shows the improved prediction result when batch size is set to 4, and embedding dimension is set to 128. The proposed model outperforms state-of-the-art baselines across most settings, with the exception of the social vulnerability index prediction result in New York when the embedding size is 64."}, {"title": "6.4. Transferability Test", "content": "A transferability test was conducted to assess the applicability of the learned model parameters from the source dataset to the target dataset. A model trained on mobility data from one city was utilized to predict social vulnerability in another city. The encoder of the model $f^{io}_\\theta$ was kept fixed, and only the linear regressor was fine-tuned.\nExperiments were conducted by varying the source-target city pairs to evaluate the social vulnerability prediction performance, as shown in Figure 5. The diagonal line in the figure shows the highest correlation because it represents the same city transferred from the source to the target. Transfer experiments demonstrated competitive performances compared to the best baseline. For example, in the \"New York \u2192 Washington D.C.\" and \"Washington D.C. \u2192 Chicago\" scenarios, our model achieved an $R^2$ of 0.400 and 0.608, respectively, outperforming the second best approach in the non-transfer setting. These results validate our model's transferability for predicting social vulnerability across different cities, highlighting its broader applicability."}, {"title": "7. Conclusion", "content": "This study proposed a novel method for learning region representations using the hourly counts of inbound and outbound trips. The proposed method is based on time series representation learning, designed to uncover the intrinsic characteristics embedded within dynamically changing inflow and outflow mobility patterns. Specifically, the model learns separate representations for inflow and outflow patterns and aligns them to create a unified view of mobility dynamics, allowing for a comprehensive understanding of regional mobility patterns. To validate the proposed model, we conducted experiments to predict income, educational attainment, and social vulnerability index in three cities: Chicago, New York, and Washington D.C. Experimental results demonstrated that the proposed approach achieved accurate predictions of these indicators using only two weeks of mobility data. Furthermore, transferability tests indicated that our approach can provide valuable insights in areas where such indicators have not been assessed.\nWhile our study has effectively utilized mobility data to predict socio-economic indicators and social vulnerability, the use of taxi or ride-hailing data may not fully represent the entire human mobility. According to [26], about 17% of people use taxis or ride-hailing services for their daily commutes. This limited representation of transportation modes could introduce biases in our assessments. As we look ahead to future research endeavors, we aim to utilize various modes of transportation such as buses and subways. This expanded approach will lead to a more comprehensive understanding of human mobility, and consequently, we anticipate achieving better prediction results."}, {"title": "Appendix", "content": ""}, {"title": "Data Augmentation", "content": "The data augmentation strategies used in this study are as follows.\n\u2022 Shift: The time series data is shifted by a single random scalar value, obtained by sampling from a Gaussian distribution with a mean of 0 and a standard deviation of 0.2 (i.e., $\\epsilon \\sim \\mathcal{N}(0, 0.2)$). Each time step is then shifted as $x'_t = \\epsilon + x_t$.\n\u2022 Dropout: It randomly drops (i.e., sets to zero) a certain proportion of the input data. This forces the model to learn more robust and generalized representations of the input data because it prevents the model from relying too heavily on any one particular feature or input. The dropout probability was set to 0.1.\n\u2022 Jitter: Independent and identically distributed Gaussian noise is added to each time step, sampled from a Gaussian distribution with a mean of 0 and a standard deviation of 0.2 (i.e., $\\epsilon_i \\sim \\mathcal{N}(0, 0.2)$). Each time step is now jittered as $x'_i = \\epsilon_i \\times x_i$.\n\u2022 Scale: The time series data is scaled by a single random scalar value, obtained by sampling from a Gaussian distribution with a mean of 0 and a standard deviation of 0.2 (i.e., $\\epsilon \\sim \\mathcal{N}(0, 0.2)$). Each time step is then scaled as $x'_i = \\epsilon \\times x_i$."}, {"title": "Details of Baselines", "content": "\u2022 GAT [28] learns the static origin-destination flow matrix. Specifically, the model learns the conditional trip distribution using Kullback-Leibler divergence. The GAT model was configured with an embedding size of 48, and comprised two layers. The learning rate was set to 0.001 and optimization was performed using the Adam optimizer.\n\u2022 Inbound times series ($x^i$) represents raw hourly counts of outbound trips. The inbound time series is normalized using the z-score, such that the set of observations has zero mean and unit variance. This results in a matrix $x^i \\in \\mathbb{R}^{N\\times T}$ and it was directly used for downstream applications.\n\u2022 Outbound times series ($x^o$) represents the raw hourly count of inbound trips and is normalized using z-score normalization. The resulting matrix $x^o \\in \\mathbb{R}^{N\\times T}$ was used directly in downstream applications.\n\u2022 Mixing-up [30] employs a novel augmentation and pretext task framework that aims to predict the mixing proportion of two time series samples. This approach generates new samples by considering a convex combination of two existing time series samples using a mixing parameter drawn from a beta distribution. The model used a modified version of the NT-Xent loss function to predict the mixing proportion. For implementation, the same beta distribution and model parameters as those reported in the original study were adopted. The open source code was obtained from \"https://github.com/Wickstrom/MixupContrastiveLearning\".\n\u2022 TS-TCC [9] utilizes a transformer-based autoregressive model to capture contextual information and ensure transferability. Their augmentation strategy comprises jitter and scale and permutation and jitter. TS-TCC imposes a challenging pretext task to learn the transformation-invariant information of a time series. The TS-TCC is designed to learn robust and discriminative features from time series data through a two-stage process. In the first stage, an input time series was passed through a temporal contrast module that enforces a challenging cross-view prediction task, resulting in the learning of contextual information. In the second stage, a contextual contrast module was applied to the learned representations to further enhance their discriminative power. For implementation, we referred to their configuration on Sleep EDF datasets. The open-source code was obtained from \"https://github.com/emadeldeen24/TS-TCC\u201d.\n\u2022 TS2vec[36] is a time series representation learning method that incorporates contextual consistency through a hierarchical contrastive loss. The method utilizes both instance-wise and temporal-wise contrastive losses to learn both instance discrimination and dynamic trends of time series data. Subseries sampling and masking were applied to augment the input time series. The hierarchical contrastive loss was computed by gradually pooling contextual representations along the time dimension and applying a loss function based on contextual consistency. The open-source code was obtained from \"https://github.com/yuezhihan/ts2vec\u201d."}]}