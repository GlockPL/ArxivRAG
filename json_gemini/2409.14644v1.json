{"title": "zsLLMCode: An Effective Approach for Functional Code Embedding via LLM with Zero-Shot Learning", "authors": ["ZIXIANG XIAN", "CHENHUI CUI", "RUBING HUANG", "CHUNRONG FANG", "ZHENYU CHEN"], "abstract": "Artificial intelligence (AI) has revolutionized software engineering (SE) by significantly enhancing development\nefficiency. Regarding to SE tasks, Large language models (LLMs) have the capability of zero-shot learning,\nwhich does not require training or fine-tuning, unlike pre-trained models (PTMs). However, LLMs are primarily\ndesigned for natural language output, and cannot directly produce intermediate embeddings from source code.\nThey also face some challenges, for example, the restricted context length may prevent them from handling\nlarger inputs, limiting their applicability to many SE tasks, while hallucinations may occur when LLMs are\napplied to complex downstream tasks.\nMotivated by the above facts, we propose zsLLMCode, a novel approach that generates functional code\nembeddings using LLMs. Our approach utilizes LLMs to convert source code into concise summaries through\nzero-shot learning, which is then transformed into functional code embeddings using specialized embedding\nmodels. Furthermore, our approach is modular, which allows it to be decoupled into several components\nseamlessly integrated with any LLMs or embedding models. This unsupervised approach eliminates the\nneed for training and mitigates the issue of hallucinations encountered with LLMs. Moreover, our approach\nprocesses each code fragment one by one into code embeddings, eliminating the issue of limited context length\nof LLMs. Additionally, by processing each code fragment individually, our approach prevents the limitations\nimposed by the context length of LLMs. To the best of our knowledge, this is the first approach that combines\nLLMs and embedding models to generate code embeddings. We conducted a series of experiments to evaluate\nthe performance of our approach. The results demonstrate the effectiveness and superiority of our approach\nover state-of-the-art unsupervised methods, such as InferCode and TransformCode.", "sections": [{"title": "1 INTRODUCTION", "content": "Over the past decade, artificial intelligence (AI) has gained significant traction within the field of\nsoftware engineering (SE). The integration of AI into SE practices aims to enhance the efficiency\nand effectiveness of software development processes, thereby boosting productivity and fostering\ninnovation. A notable advancement in this integration has been the advent of pre-trained models\n(PTMs). PTMs such as CodeBERT [17] and CodeT5 [52] have been trained on extensive code\ndatasets, exhibiting a profound comprehension of programming languages. These models can be\nfined-tuned for various SE tasks [3, 17, 22]. However, fine-tuning is resource-intensive because PTMs\nhave a large number of parameters, requiring significant computational resources.\nTo address these challenges, Bui et al. proposed InferCode [11], which first splits the code abstract\nsyntax tree (AST) into subtrees of varying sizes, then encodes the subtree nodes with an enhanced\ntree-based convolutional neural network (TBCNN) [39]. This method leverages the hierarchical\nstructure of code to capture semantic information effectively. Similarly, Xian et al. introduced\nTransformCode [57], which employs a contrastive loss function to align the embeddings of original\ncode fragments with those of their AST-transformed versions. This alignment helps in capturing\nsemantic similarities between different code representations. Both approaches utilize unsupervised\nlearning methods to train smaller models, thereby reducing the computational resources required\ncompared to fine-tuning PTMs. Despite these advancements, it is important to note that these\nmethods still require training or fine-tuning, which can be time-consuming and resource-intensive.\nLLMs, which are primarily based on Transformer decoder architectures utilizing self-attention\nmechanisms to capture global dependencies between inputs and outputs [50], possess the capability\nfor zero-shot learning due to their extensive pre-trained knowledge. This allows them to perform\ntasks without additional training or fine-tuning, offering significant advantages over PTMs when\napplied to downstream SE tasks. Notable examples of such models include the GPT series developed\nby OpenAI, such as GPT-1 [40], GPT-2 [41], GPT-3 [9], and GPT-4 [1]. Additionally, the open-source\nGLM series from Tsinghua University and Zhipu AI [20], which includes the latest GLM3 and GLM4\nmodels, exemplifies advancements in this domain.\nResearchers have explored the application of LLMs in various SE tasks, leveraging zero-shot\nlearning techniques [7]. For instance, Khajezade et al. [31] utilized LLMs for code-clone detection.\nInstead of generating intermediate code embeddings, they employed prompt templates to directly\nquery LLMs whether two code fragments are clones. Despite their efforts to refine these prompt\ntemplates, they could not mitigate the hallucination issues inherent in LLMs. Additionally, LLMs\nhave a fixed context length, which limits their ability to process long code fragments. If the two\ncode fragments exceed this context length during code-clone detection, the LLMs may produce\nerroneous outputs due to the out-of-context problem.\nDespite the remarkable capabilities of LLMs for zero-shot learning, they encounter two signifi-\ncant issues when applied to downstream SE tasks as above: context-length limitations and LLM\nhallucinations. Firstly, each LLM has a predefined context length, which restricts the amount of\ninput it can process simultaneously. This limitation becomes particularly problematic for tasks\nsuch as code clustering and code-to-code search, which require the input of extensive code frag-\nments. LLMs may produce error messages or cease functioning when the input exceeds the context\nlength. Extending the context length of LLMs necessitates retraining or fine-tuning, which is a\ntime-consuming and resource-intensive process. Secondly, LLMs are prone to hallucination when\napplied to complex tasks like code-clone detection. In such cases, LLMs might incorrectly assess\nthe similarity between two code fragments. For instance, an LLM might determine that two code\nfragments have similar functionality but incorrectly respond with \u201cno\u201d, indicating that it did not"}, {"title": "2 RELATED WORK", "content": "The main goal of code-embedding learning is to convert source code into vector representations\n(code embeddings) that capture semantics and structural attributes. This conversion supports\nvarious subsequent tasks, with one of the most typical applications being code-clone detection\n[15, 35, 37]. Specifically, code-clone detection aims to identify code fragments that exhibit similarity\nor identity in functionality or syntax. By transforming code into vector embeddings, we can\nquantitatively evaluate the similarity between different code fragments, thereby determining the\nlikelihood of them being clones."}, {"title": "2.1 Methodology for Code Embedding", "content": "Code-embedding methods are used to represent source code in a vector space. These methods are\ntrained by different forms of input data (such as plain text, syntax trees, and graphs) to capture the\nsemantic attributes of the code. In this paper, we classify these traditional methods into three main\ntypes based on the form of code data: token-based, tree-based, and graph-based methods."}, {"title": "2.1.1 Token-based Methods", "content": "Token-based methods analyze code as sequences of lexical tokens or\nn-grams, and use natural language processing (NLP) techniques to capture lexical and syntactic\npatterns. A common approach is the term frequency-inverse document frequency (TF-IDF), which\nidentifies significant terms based on frequency. These frequency features can be fed into models like\nsupport vector machines (SVMs) [24] or extreme gradient boosting (XGBoost) [13] for classification\nor prediction. However, token-based methods may overlook the syntax and semantics crucial for\nunderstanding code. Researchers have explored deep-learning methods for more comprehensive\ncode representations to address this issue. CodeBERT [17] is a method trained on a basic language\nmodel BERT [14] with masked language modeling (MLM) and next sentence prediction (NSP).\nCodeAttention [60] translates code into natural language comments by leveraging structural\ninformation. Besides, Ahmad et al. [2] proposed a Transformer model with relative position encoding\nand copy attention mechanisms to generate natural language summaries of code."}, {"title": "2.1.2 Tree-based Methods", "content": "Tree-based methods parse code into abstract syntax trees (ASTs) or\nhierarchical structures, and then capture the syntactic and semantic of the specific code. These\nmethods effectively represent the nested and hierarchical nature of code, making them suitable for\ntasks requiring structural understanding. Tree-based methods improve the semantic and structural\naspects of code compared with the token-based methods. Mou et al. [39] introduced a tree-based\nconvolutional neural network (TBCNN) that uses ASTs to encode source code fragments for tasks\nlike functionality classification and pattern detection. Zhang et al. [59] developed ASTNN, which\nsplits code fragments into statement trees, encodes them with a tree-based neural network, and uses\na bidirectional recurrent neural network (BRNN) to generate final vector representations. Alon et al.\n[6] proposed code2vec to represent the code fragments as fixed-length vectors by decomposing codes\ninto AST paths and learning to aggregate AST paths. Code2seq [5] uses long short-term memory\nnetworks (LSTMs) to handle variable-length sequences and capture more syntactic information.\nBui et al. [12] introduced TreeCap, combining capsule networks and TBCNNs to learn code models\nfrom ASTs."}, {"title": "2.1.3 Graph-based Methods", "content": "Graph-based methods construct various graphs from code, such as\ncontrol flow graphs (CFGs), data flow graphs (DFGs), or other graph structures that represent the dy-\nnamic behavior and dependencies within the code. Similar to the tree-based methods, graph-based\ncode embedding is another possible approach to capture the semantic and execution aspects of code.\nFurthermore, graph-based methods are beneficial for capturing complex interactions and dependen-\ncies not easily represented by token-based or tree-based methods, such as the flow information of\nvariable data. Fang et al. [15] developed a joint code representation that combines AST embeddings\nwith CFG and DFG embeddings, utilizing various fusion methods such as concatenation, attention,\nand gated fusion. Guo et al. [22] introduced GraphCodeBERT, which integrates the program's data\nflow into the model, enabling it to learn from both the lexical and syntactic information of the code.\nMa et al. [36] presented a novel model called cFlow, which uses a flow-based gate recurrent unit\n(GRU) for feature learning from the source code CFG: The model leverages the program structure\nand the semantics of statements along the execution path, which reflects the flowing nature of\nCFGs."}, {"title": "2.2 LLMs and Sentence-Embedding Models", "content": "LLMs have revolutionized the field of NLP and have demonstrated remarkable capabilities in a\nwide range of tasks due to the ability to handle long-range dependencies. The advent of LLMs\nsuch as GPT [1, 9, 40, 41] and GLM [20] has significantly advanced the development in language\nunderstanding and generation."}, {"title": "3 MOTIVATING EXAMPLES", "content": "This section provides two motivating examples for our method: context-length limitations and LLM\nhallucinations."}, {"title": "3.1 Context-Length Limitations", "content": "While LLMs have significantly advanced the field of software engineering, they still encounter\nnotable limitations in specific tasks due to their restricted context length. The context length of an\nLLM is fixed and limited, and increasing it requires substantial GPU resources for retraining or\nfining. Context length refers to the maximum number of tokens (words, punctuation, etc.) the\nmodel can process at once, encompassing both the input and output of the LLMs.\nFor instance, the standard GPT-3.5 Turbo has a maximum context length of 4,096 tokens. This\nlimitation is insufficient for completing software engineering tasks such as classifying, clustering,\nand searching code. As illustrated in Figure 1, processing a single code fragment can consume a\nsignificant portion of the context length (Using GPT-3.5 Turbo as an example): Figure 1(a) requires\n164 tokens, while Figure 1(b) requires 202 tokens. Given the average token count of 160 per code\nfragment, GPT-3.5 Turbo can only handle approximately 25 code fragments simultaneously, which\nis inadequate for code datasets. Even with the extended GPT-3.5 Turbo 16k, which has a maximum\ncontext length of 16,385 tokens, the model can only process around 100 code fragments when the\naverage token count per fragment is 160. In this paper, we utilize the standard GPT-3.5 Turbo with\na maximum context length of 4,096 tokens. An example of an error message encountered due to\nexceeding the context length in GPT-3.5 Turbo is shown in Figure 2.\nSimilarly, in code-to-code search, the limited context length poses a challenge as it obstacles the\nmodel's ability to effectively match and retrieve relevant code segments. SE tasks often require the\nprocessing and analysis of large volumes of code fragments, which is constrained by the limited\ncontext length. A more effective approach for these SE tasks involves converting source code into\ncode embeddings, enabling the performance of various downstream tasks on these embeddings.\nHowever, current LLMs are not equipped to generate code embeddings directly; instead, they\nproduce natural language outputs, which are unsuitable for the aforementioned tasks. Furthermore,\nwhen applied to complex tasks, LLMs are prone to generating hallucinations \u2013 outputs that are"}, {"title": "3.2 LLM Hallucinations", "content": "LLMs frequently encounter issues with hallucinations, which can lead to incorrect outputs [58].\nFor instance, when asked to determine if two pieces of code are clones, LLMs might understand\nthe functionality of the codes but still fail to provide an accurate answer regarding code cloning\n[31]. Khajezade et al. [31] addressed this problem by proposing an improved prompt as follows.\nInstead of directly asking whether the codes are clones, they suggested inquiring if the two codes\nproduce the same inputs and outputs. This approach constrains the LLMs to respond with either\n\u201cYes\u201d for clone pairs or \u201cNo\u201d for non-clone pairs, thereby enhancing the accuracy of the responses:"}, {"title": "4 ZSLLMCODE", "content": "In this paper, we introduce a novel and efficient approach for code embedding that leverages\nzero-shot learning with LLMs, called zsLLMCode. Figure 3 presents the framework zsLLMCode,\nwhich mainly consists of four manageable steps: (1) uniform prompt design; (2) code summaries\nand storage; (3) functional code embedding generation; and support for (4) downstream tasks."}, {"title": "4.1 Uniform Prompt Design", "content": "There are two critical aspects when designing the prompt template: (1) the function of code should\nbe summarized in one sentence; and (2) avoid LLMs to provide any explanatory content. The first\naspect arises from the limitations of sentence-embedding models used in our approach: Sentence-\nembedding models are pre-trained on sentence pairs, typically formatted as single sentences.\nConsequently, LLMs must generate one-sentence code summaries to maintain compatibility with\nthe sentence-embedding models. The second aspect stems from the phenomenon that LLMs tend"}, {"title": "4.2 Code Summaries and Storage", "content": "zsLLMCode uses LLMs to generate concise code summaries based on the prompt designed. As\nillustrated in Section 4.1, we only extract the first sentence from the LLMs' responses as code\nsummaries. Figure 5 presents a sample of C code and its corresponding code summary generated by\nGPT-3.5 Turbo [21]. In code-clone detection tasks, we must combine two code fragments as input\ninto prompts for LLMs. If two code fragments are lengthy, the input may exceed the context-length\nlimitations of each session with LLMs, which could result in incomplete or incorrect responses.\nzsLLMCode can effectively address these two issues by summarizing each long code fragment\nindividually to avoid sending both code fragments to the LLM simultaneously, which provides\nmore accurate and reliable code summaries for the downstream tasks. It should be noted that we\nconducted an additional version that excludes stop words for the Chinese code summaries for"}, {"title": "4.3 Functional Code Embedding Generation", "content": "After storing the code summaries, we convert them into vector representations by language-specific\nsentence-embedding models. These sentence-embedding models are pre-trained on vast amounts\nof sentence pairs. In the above steps, zsLLMCode ensures LLMs can generate one-sentence code\nsummaries. Sentence-embedding models can effectively convert these concise code summaries\ninto functional-code embeddings. It is important to note that the sentence-embedding models only\nsupport code summaries in the same language. For example, some sentence-embedding models\nare exclusively pre-trained in English, thus limiting their application to English code summaries\nonly. After generating functional-code embeddings, we store all these vectorized embeddings for\ndownstream tasks."}, {"title": "4.4 Downstream Tasks", "content": "zsLLMCode can generate functional code embeddings without training or using labeled data,\nmaking it a valuable tool for a wide range of downstream tasks in software engineering. Specifically,\nzsLLMCode is practical and straightforward for various unsupervised or supervised learning-based\ntasks. It is important to note that zsLLMCode specifically targets functional-level code embeddings,\nmaking it particularly well-suited for code-level tasks like code-clone detection and code clustering."}, {"title": "5 EXPERIMENTAL DESIGN AND SETUP", "content": "In this section, we first present the research questions related to the performance of zsLLMCode\nand then formulate the tasks we conducted to answer them, from the perspectives of code-clone\ndetection tasks and code clustering tasks. We also outline the datasets, and independent and\ndependent variables used in our experiments. Additionally, we briefly introduce the experimental\nenvironment for our research."}, {"title": "5.1 Research Questions", "content": "To thoroughly evaluate the effectiveness of zsLLMCode, we aim to answer the following research\nquestions in the following experiments:\nRQ1: [Ablation Study]\n\u2022 RQ1.1: What is the impact of using different sentence-embedding models trained on different\nlanguages on the effectiveness of zsLLMCode?\n\u2022 RQ1.2: What is the impact of removing stop words from the LLM's response on the effective-\nness of zsLLMCode?\n\u2022 RQ1.3: What is the impact of using different LLMs for zsLLMCode?\nRQ2: [Generalization Evaluation] Does zsLLMCode also support the generation of functional\ncode embeddings for code fragments in other programming languages?\nRQ3: [Effectiveness Evaluation] How does the effectiveness of zsLLMCode compare to other\nunsupervised code embedding approaches?\nRQ4: [Quality Evaluation] How effective are the code embeddings generated by zsLLMCode\nwhen evaluated through visualization techniques, especially regarding boundary separation effects\nacross various LLM configurations?\nWe design a series of ablation experiments from different perspectives: RQ1.1 and RQ1.3 com-\npare the impact on the effectiveness of using different sentence-embedding models and LLMs,\nrespectively. RQ1.2 aims at evaluating the impact of stop words on the performance of sentence-\nembedding models. By comparing the performance of models with and without stop words, we can\ngain insights into the role of these words in embedding quality and overall model effectiveness.\nRQ2 and RQ3 respectively evaluates the generalization and effectiveness of zsLLMCode. RQ4\nevaluates the quality of zsLLMCode from the visualization perspective."}, {"title": "5.2 Task Formulation", "content": "We formulate two unsupervised downstream tasks to evaluate the performance of zsLLMCode:\ncode-clone detection; and code clustering."}, {"title": "5.2.1 Code-Clone Detection", "content": "Code-clone detection involves identifying code fragments that are\nsimilar or identical in syntax or semantics. This process is essential for maintaining code quality,"}, {"title": "5.2.2 Code Clustering", "content": "Code clustering involves the automatic grouping of similar code fragments\ninto clusters without any form of supervision. This process is crucial for various applications\nin software engineering and code analysis. However, directly asking LLMs to solve this task is\nnot feasible because of the context-length limitations and lack of inherent logic to process these\ncomplex tasks comprehensively. For this task, we first convert the code fragments into functional\ncode embeddings, which are numerical representations capturing the semantic attributes of the\ncode. These embeddings facilitate the comparison of code fragments in a high-dimensional space.\nFor effective clustering, we define a similarity metric based on the Euclidean distance between these\nembeddings: This metric quantifies the similarity between code fragments, enabling zsLLMCode to\ncluster the code accurately. Besides, we employ K-means [30], a widely used clustering algorithm\n[54-56], to organize the code fragments into meaningful clusters. The K-means algorithm iteratively\npartitions the data into K clusters by minimizing the variance within each cluster, thereby ensuring\nthat similar code fragments are grouped."}, {"title": "5.3 Dataset Selection", "content": "For our experiments, we utilize two prominent datasets: POJ-104 [39, 59] and BigCloneBench\n[48, 51]. POJ-104 is a dataset specifically designed for code-clone detection tasks. POJ-104 consists\nof 52,000 code fragments written in C, which are semantically equivalent but syntactically different\n[59]. This dataset is structured to facilitate the evaluation of models based on their ability to identify\nsemantically similar code fragments despite syntactic variations. The dataset is divided into training,\nvalidation, and test sets, with 32,000, 8,000, and 12,000 examples, respectively. BigCloneBench, a\nwidely used benchmark dataset [48, 51], includes projects from 25,000 Java repositories, covering\nten functionalities. It contains 6,000,000 true clone pairs and 260,000 false clone pairs. This extensive\ndataset comprehensively evaluates code-clone detection models, providing a robust benchmark for\nassessing their performance. Both datasets are available from the CodeXGLUE GitHub repository\u00b2.\nBesides, following the previous research [57], we employ the OJClone C, a dataset using code\npairs from POJ-104 based on pairwise similarity. This dataset involves 500 programs from each of\nthe first 15 POJ-104 problems, resulting in 1.8 million clone pairs and 26.2 million non-clone pairs.\nA comparison of all the pairs would be prohibitively time-consuming, so 5,000 clone pairs and 5,000"}, {"title": "5.4 Independent Variable", "content": "We focus on the LLMs, sentence-embedding models, and the baselines for code clustering tasks as\nthe independent variables of our experimental research."}, {"title": "5.4.1 LLM Selection", "content": "We employ three distinct LLMs for further evaluating the effectiveness of\nzsLLMCode: GPT-3.5 Turbo, GLM3, and GLM4. These LLMs were selected to provide diverse capa-\nbilities and performance characteristics, allowing for a comprehensive evaluation of zsLLMCode.\nTable 2 presents detailed information about the three LLMs. GPT-3.5 Turbo is a widely recognized\nand powerful LLM. GLM3 and GLM4 belong to the open-source GLM series LLMs.\nOur primary objective is to evaluate the performance of zsLLMCode across various LLM con-\nfigurations. While this study specifically examines these three specific LLMs, it is important to\nhighlight that zsLLMCode is flexible and can be adapted to integrate other closed-source LLMs that\nmay offer superior performance. This adaptability ensures that zsLLMCode remains relevant and\neffective as new and more advanced LLMs become available."}, {"title": "5.4.2 Sentence-Embedding Model Selection", "content": "We utilize the all-MiniLM-L6-v2\u00b3 and all-MiniLM-L12-\nv2\u2074 models from sentence transformers (SBERT) [42] for English code summaries. For Chinese\ncode summaries, we employ the sbert-base-chinese-nli\u2075 model. This dual-language implementation\nensures that the code summaries are accurately generated in the appropriate language, and enhances\nthe overall quality and relevance of the generated embeddings. Table 3 summarizes these three"}, {"title": "5.4.3 Approaches for Code-Clone Detection", "content": "We comprehensively compared zsLLMCode with\nother unsupervised code-clone detection methods that do not require labeled data. We intentionally\nexcluded comparisons with techniques that rely on supervised learning to construct clone classifiers,\nsuch as Oreo [43], CCD [15], ASTNN [59], and CCDLC [45, 46]. Additionally, we did not include\nthe work of Tufano et al. [49], who employed a supervised learning approach to training a neural\nnetwork for learning semantic similarities between code components based on a stream of identi-\nfiers. Our baseline for code-clone detection included several unsupervised methods: Deckard [28],\nSourcererCC [44], DLC [53], Code2vec [6, 29], InferCode [11], and TransformCode [57]. We also\nincorporated CodeBERT in an unsupervised setting for this experiment. To measure the similarity\nbetween two code fragments, we utilized cosine similarity to calculate the distance between their\ncode embeddings, without any training. It is important to note that CodeBERT was not trained\nwith a supervised clone-detection classifier, as this would have violated the unsupervised learning\nassumption. Both Code2vec and InferCode employ a similar prediction methodology to ours, where\nthe clone label is predicted based on the cosine similarity between two code fragments."}, {"title": "5.4.4 Approaches for Code Clustering", "content": "We select several baselines to evaluate the performance of\nzsLLMCode in the code clustering tasks. Firstly, we use Word2vec [38] and Doc2vec [34] to treat\ncode as text and generate embeddings. Word2vec employs a neural network model to learn word\nassociations from a large corpus of text, while Doc2vec extends this approach to learn document-\nlevel embeddings. Additionally, we introduce a Sequential Denoising Auto Encoder (SAE) [25],\nwhich encodes the text into embeddings and reconstructs the text from these embeddings. This\nmethod helps in capturing the underlying structure and semantics of the code. Other code-specific\nmodels have also been selected to benchmark our approach further. Firstly, we include Code2vec\n[6], which represents code fragments as continuously distributed vectors by learning from the\npaths in their abstract syntax trees (ASTs). Similarly, we also introduce Code2seq [5], which\ngenerates sequences from structured representations of code by leveraging the syntactic structure\nof programming languages. Lastly, we incorporate an unsupervised method, InferCode [11], which\nuses self-supervised learning to predict subtrees in the ASTs of code, thereby learning robust code\nrepresentations without the need for labeled data."}, {"title": "5.5 Dependent Variable", "content": "There are two dependent variables, relating to code-clone detection and code clustering tasks. We\nfollowed the original studies [11, 59] to guide the choice of evaluation metrics for these experiments."}, {"title": "5.5.1 Metric for Code-Clone Detection", "content": "Code-clone detection is a classification task that determines\nwhether or not two code fragments are identical. To evaluate the performance of code-clone\ndetection, we use the following metrics that are commonly used in classification tasks:"}, {"title": "5.5.2 Metric for Code Clustering", "content": "We employ the adjusted rand index (ARI) [23] to evaluate the\nperformance of models in code clustering tasks, which is a widely recognized and robust metric for\nassessing the quality of clustering algorithms [55, 56]. Unlike the traditional Rand Index, ARI adjusts\nfor the chance grouping of elements, providing a more accurate measure of clustering quality. This\nmetric computes the similarity between two clusterings by considering all pairs of samples and\ndetermining whether they are assigned to the same or different clusters in the predicted and true\nclusterings. The ARI score ranges from -1 to 1, where 1 indicates perfect agreement between the\nclusterings; 0 indicates random labeling; and negative values indicate less agreement than expected\nby chance. In our experiment, we employ the ARI to assess the effectiveness of various methods in\ncode clustering, ensuring a robust validation of clustering performance. The ARI is defined as:\n$ARI (C_{truth}, C_{pred}) = \\frac{\\Sigma_{i}\\Sigma_{j} \\binom{n_{ij}}{2} - [\\Sigma_{i} \\binom{a_{i}}{2} + \\Sigma_{j} \\binom{b_{j}}{2}] / \\binom{N}{2}}{[\\Sigma_{i} \\binom{a_{i}}{2} + \\Sigma_{j} \\binom{b_{j}}{2}] - [\\Sigma_{i}\\Sigma_{j} \\binom{n_{ij}}{2}] / \\binom{N}{2}}$\nwhere N represents the total number of data points in the dataset. $C_{truth}$ denotes ground-truth\nclustering and $C_{pred}$ denotes predicted clustering. $N_{ij}$ is the number of data points of the class label\n$C_{j} \\in C_{truth}$ assigned to cluster $C_{i}$ in partition $C_{pred}$. $N_{i}$ is the number of data points in cluster $C_{i}$ of\npartition $C_{pred}$, and $N_{j}$ is the number of data points in class $C_{j}$ of partition $C_{truth}$."}, {"title": "5.6 Experimental Environment", "content": "All experiments were conducted on a computer featuring an AMD Ryzen 7 5700X processor, dual\nNvidia RTX 3090 GPUs, and 64GB of DDR4 RAM. The algorithm was implemented with Python\n3.9.12. We utilized several Python libraries essential for our research, including PyTorch (torch),\nHugging Face's Transformers, and Sentence-Transformers."}, {"title": "6 EXPERIMENTAL RESULTS AND DISCUSSIONS", "content": "This section presents the experimental results of zsLLMCode in the code-clone detection and code\nclustering tasks, and answers to the research questions outlined in Section 5.1."}, {"title": "6.1 Results of Code-Clone Detection Tasks", "content": "We initially present the results of code-clone detection tasks, and answer the corresponding research\nquestions."}, {"title": "6.1.1 Answer to RQ1.1", "content": "We conducted the experiments on the original GPT-3.5 Turbo LLM (without\ntraining or fine-tuning) and the OJClone C dataset with both English and Chinese prompts. Besides,\nwe set a series of thresholds to compute the cosine similarity between code fragments. Table 4\nshows the evaluation results of zsLLMCode using sentence-embedding models that respectively\nsupport English and Chinese. From the results, we have the following observations:\n\u2022 Even though the same LLM (GPT-3.5 Turbo) was used, the selection of the sentence-embedding\nmodel and the setting of the threshold significantly impacted the results.\n\u2022 The MiniLM models for English consistently outperformed the sbert-base-chinese-nli model\nfor Chinese.\n\u2022 Within the MiniLM models, the 12-layer MiniLM model did not always yield the best results.\nWhen the threshold was set to 0.50, 0.55, or 0.60, the 6-layer MiniLM model performed better.\n\u2022 For English, when the all-MiniLM-L12-v2 sentence-embedding model under the similarity\nthreshold of 0.55 achieved the best performance, with an F1 score of 91.82%.\n\u2022 For Chinese, the sbert-base-chinese-nli sentence-embedding model achieved an F1 score of\n79.35% when the similarity threshold was set to 0.7, indicating superior performance.\nIt is important to note that our approach is not restricted to the sentence-embedding models\nutilized in these experiments. Moreover, we anticipate fine-tuning the sentence-embedding models\ncould further enhance the performance."}, {"title": "6.1.2 Answer to RQ1.2", "content": "To reveal the impact of removing stop words from Chinese code summaries\non the performance of zsLLMCode. We conducted an in-depth comparison with GPT-3.5 Turbo and\nthe sbert-base-chinese-nli sentence-embedding model on the OJClone C dataset. Table 4 presents\nthe results of whether the stop words are removed from the Chinese code summaries. Based on the\nresults, we have the following observations:\n\u2022 Removing stop words from the LLM outputs did not significantly impact performance."}, {"title": "6.1.3 Answer to RQ1.3", "content": "Table 4 presents the results of zsLLMCode conducted by GPT-3.5 Turbo. To\nbetter evaluate the performances of zsLLMCode using different LLMs", "observations": "n\u2022 For English sentence-embedding models, GLM4 achieved a higher F1 Score of 86.99% using\nthe all-MiniLM-L12-v2 embedding model with a threshold value of 0.5, indicating optimal\nperformance.\n\u2022 For Chinese models, GLM4 still achieved the best performance at a threshold value of 0.7,\nwith an F1 Score of 72.00%. The performance of zsLLMCode with GLM3 was worse than with\nGLM4, achieving only a 66.02% F1 Score.\n\u2022 In general, zsLLMCode with GLM3 yielded slightly lower performance than GLM4. Among\nall cases, GLM3 achieved the highest F1 Score of 78.46% at a threshold value of"}]}