{"title": "AI Guide Dog: Egocentric Path Prediction on Smartphone", "authors": ["Aishwarya Jadhav", "Jeffery Cao", "Abhishree Shetty", "Urvashi Priyam Kumar", "Aditi Sharma", "Ben Sukboontip", "Jayant Sravan Tamarapalli", "Jingyi Zhang", "Anirudh Koul"], "abstract": "This paper introduces AI Guide Dog (AIGD), a lightweight egocentric navigation assistance system for visually impaired individuals, designed for real-time deployment on smartphones. AIGD addresses key challenges in blind navigation by employing a vision-only, multi-label classification approach to predict directional commands, ensuring safe traversal across diverse environments. We propose a novel technique to enable goal-based outdoor navigation by integrating GPS signals and high-level directions, while also addressing uncertain multi-path predictions for destination-free indoor navigation. Our generalized model is the first navigation assistance system to handle both goal-oriented and exploratory navigation scenarios across indoor and outdoor settings, establishing a new state-of-the-art in blind navigation. We present methods, datasets, evaluations, and deployment insights to encourage further innovations in assistive navigation systems.", "sections": [{"title": "1 Introduction", "content": "Navigation assistance systems for visually impaired individuals have been researched for several decades (Dakopoulos and Bourbakis 2010), yet their widespread adoption remains limited due to (1) reliance on expensive, custom-built devices, (2) the lack of efficient and robust models that ensure user safety, and (3) limited user trust and convenience.\nExisting systems often rely on expensive devices with built-in sensors like LiDAR, or laser scanners (Wang et al. 2017; Wachaja et al. 2015; Hesch and Roumeliotis 2010), for 3D environmental mapping, or IMUs, gyroscopes, and pedometers (Lee and Medioni 2014; Hesch and Roumeliotis 2010) for localizing user position and orientation. While accurate, these systems are bulky and prohibitively expensive, limiting their accessibility. To address these challenges, we propose a lightweight system leveraging only video feed from a smartphone camera worn near the user's chest. This video stream is processed in real-time by an on-device model to predict navigation instructions, which are then translated into audio cues for the user. This facilitates easy user adoption while maintaining robust performance.\nMost prior work (Wang, Liu, and Kennedy 2024; Qiu et al. 2022; Soo Park et al. 2016; Yagi et al. 2018) on blind navigation formulates it as a trajectory forecasting problem, predicting precise 3D locations and orientations of the user. However, this approach typically reflects the behavior of sighted individuals, who navigate by dynamically avoiding obstacles and other pedestrians. Blind navigation is fundamentally different: visually impaired individuals typically prefer retaining canes (Ohn-Bar, Kitani, and Asakawa 2018), even when assisted by robotic systems. Canes help detect obstacles and signal others to yield space, facilitating smoother navigation. Thus, the user-environment dynamics of blind individuals differ significantly from those of sighted users.\nThis insight allows us to simplify the navigation task into an egocentric path prediction problem, where the system predicts all possible future user navigation actions such as continuing straight, turning left, or turning right. This abstraction avoids the uncertainties of precise trajectory prediction and instead focuses on the user's heading direction and actions relative to their environment. We adopt a multi-label classification approach to accommodate multiple possible navigation directions and enable easy translation of the model's outputs to clear and actionable commands for users. While limited prior work (Wang et al. 2017; Singh, Fatahalian, and Efros 2016) explored similar ideas, they were"}, {"title": "2 Related Work", "content": "Blind Navigation Assistance Systems\nPrevious blind assistance systems have primarily utilized either wearable devices or robotic helpers. Wearable navigation systems incorporate sensors placed on the body (Abidi et al. 2024) (e.g., feet, knees, or waist) and rely on standalone devices, such as laptops in backpacks (Lee and Medioni 2016), smartphones (Sato et al. 2017; Pawar et al. 2022) or tablets (Li et al. 2019) for computations. For instance, Lee and Medioni (2016) developed a head-mounted RGB-D camera system for egomotion estimation and obstacle-aware path planning, providing tactile feedback through a haptic vest. Similarly, Wang et al. (2017) introduced a wearable system with a structured light camera generating point clouds and conveying navigation instructions via vibration motors and Braille.\nRobotic helpers, such as smart canes (Hesch and Roumeliotis 2010; Gupta et al. 2015; Yang, Gao, and Choi 2024) or suitcase-mounted devices (Manglik et al. 2019), often act as robotic guide dogs. For example, Wachaja et al. (2015) proposed a robotic walker equipped with laser rangefinders and servo motors to estimate egomotion and detect obstacles, while ISANA (Li et al. 2019) employs a Google Tango tablet for computation and an onboard RGB-D camera for obstacle avoidance, offering multimodal feedback through speech, audio, and haptics.\nEgocentric Navigation\nEgocentric navigation comprises trajectory forecasting, which predicts future 2D/3D positions, and path prediction, which identifies discrete directional actions (e.g., left, right, forward), with related research extending into goal-based navigation. Trajectory forecasting, while extensively studied for vehicles, has seen limited attention for human navigation. Yagi et al. (2018) proposed a convolution-deconvolution network using pedestrian poses and egomotion. Other approaches have utilized GRU-CNN (Styles, Sanchez, and Guha 2020) and LSTM encoder-decoder models (Qiu et al. 2021) to predict human trajectories in indoor environments. More recent methods leverage multimodal transformers (Qiu et al. 2022) and diffusion models (Wang, Liu, and Kennedy 2024) to incorporate scene semantics and past trajectories for future position predictions.\nEgocentric path prediction methods include Lee and Medioni (2014), which generates 3D occupancy maps and utilizes D*-Lite planning for four directional cues (straight, left, right, stop); Singh, Fatahalian, and Efros (2016), which uses a fine-tuned CNN to predict discrete motion classes from single camera images; and Ohn-Bar, Kitani, and Asakawa (2018), which developed a dynamic path planning policy personalized to user reaction times, providing discrete localized guidance based on global pre-planned layouts.\nMost goal-based navigation research focuses on robotics (S\u00e1nchez-Ib\u00e1\u00f1ez, P\u00e9rez-del Pulgar, and Garc\u00eda-Cerezo 2021) and autonomous vehicles (Aradi 2022), relying on dynamic path planning and reinforcement learning. However, these approaches are unsuitable for modeling human behavior, particularly for blind users, who face unique social and reaction constraints.\nTo the best of our knowledge, AIGD is the first system to generalize navigation for blind users across diverse scenarios. Our approach is motivated by the unique requirements of this use-case, allowing us to scope the problem to a limited set of instruction classes while incorporating goal-based navigation and directional uncertainty, without relying on complex dynamic planning or reinforcement learning. This ensures a solution that is both practical and user-centric."}, {"title": "3 Method", "content": "3.1 System Overview\nOur system comprises a smartphone app that runs our lightweight prediction model on-device, in real-time. It primarily uses video input from the device's camera, and optionally, GPS data. For destination-based navigation, direc-"}, {"title": "3.2 Problem Definition", "content": "We model this task as a multi-label classification problem, where, for each frame sampled from the smartphone camera stream, the system predicts the user's heading direction one second into the future. Specifically, based on the current scene and, optionally, past frames, the model outputs classification scores for three possible directions (FRONT, LEFT, RIGHT) the user could take in the next second. The one-second future horizon is informed by studies on walking speeds (Liu et al. 2019) and average reaction times (Bhirud and Chandan 2017) specific to our blind user base.\nMultiple turn labels are generated only in scenarios without a destination intent, typically occurring at intersections or when pathways diverge. In such cases, the model must predict all potential directions one second before the turn begins. During the turn, it must predict the active turn direction (LEFT/RIGHT), and finally transition to predicting FRONT, with high confidence, as the turn concludes. Fig. 2 demonstrates this with frames sampled at 1 FPS. For free-space navigation without obstacles, our labeling scheme conditions the model to output only the FRONT direction."}, {"title": "3.3 Dataset", "content": "At the time of our study, existing open-source egocentric walking datasets lacked the diversity needed to encompass the variety of scenarios required for our use case. Most were confined to specific environments (e.g., labs, offices), relied on specialized cameras or hardware, or lacked the sensor and GPS data that we needed.\nTo better reflect the real-world conditions of our app's usage, we collected a custom dataset using smartphone cameras, aiming to capture the walking speeds and styles of visually impaired individuals as closely as possible. Eight participants, primarily graduate students and tech interns, were recruited for data collection. To constrain the study, all participants used IPhone 13 with the AIGD data collection app installed. Data was collected in semi-crowded social spaces containing stationary obstacles and people. To simulate social navigation interactions similar to those experienced by our target users, participants wore black glasses and carried the smartphone on a lanyard positioned near their chest, walking slowly and cautiously. This setup also allowed us to capture variations in first-person camera movements, camera positionings, and fields of view.\nData was collected for diverse indoor and outdoor scenes across Pittsburgh, Seattle, and the Bay Area, as described in Tab. 1, focussing on everyday venues that are not the users' home or office. For indoor environments, we selected well-lit settings and prioritized scenes with numerous aisles, such as grocery stores or building halls, to increase the frequency of left and right turns. Since there is no destination intent for these, participants were instructed to turn at every available opportunity. Outdoor data was collected in parks with winding pathways and city streets during daytime. All walking paths were unscripted and unplanned, with each video capturing a single scene and lasting 2 to 10 minutes.\nIn total, the dataset includes 57 hours of walking data captured at 30 fps. It includes video streams from smartphone cameras and sensor data (accelerometer, gyrometer, magnetometer, pedometer) captured at 0.1-second intervals. For outdoor scenes, GPS locations and directional data from the Google Maps API were also logged. Fig. 3 shows a raw example record.\nSensor data was processed through denoising, reference transformations, and windowing to generate ground truth labels. It was then timestamp-aligned with video frames that were downsampled to 2 fps, and converted to 128\u00d7128 grayscale. For outdoor scenarios, GPS and high-level destination directions were aligned with these records. Each data example consists of a frame, its corresponding label, the preceding 5 seconds (10 frames) as context, and associated GPS and direction features for all frames, resulting in a total of 392,580 examples. The dataset was split into training, validation, and test sets in a 60:20:20 ratio, ensuring no overlap of scenes across splits, as summarized in Table 1.\nGround Truth Labels Labels for each sampled frame were derived from sensor data. Various methods, including accelerometer, compass, GPS, and optical flow-based ap-"}, {"title": "3.4 Models", "content": "This section outlines the models used to validate our proposed problem formulation and intent methodology. We first set up multi-label classification models for no-destination (no-intent) navigation, including simple baselines such as CNN and ConvLSTM, commonly employed in prior work (Styles, Sanchez, and Guha 2020; Qiu et al. 2021; Singh, Fatahalian, and Efros 2016), as well as the more advanced PredRNN model (Wang et al. 2023). We then describe the models that exemplify our proposed methodology for integrating destination intent features and GPS signals into these no-intent base models to enable goal-conditioned predictions.\nCNN: Following Singh, Fatahalian, and Efros (2016), we finetuned a ResNet34 model with a linear classification head to encode individual frames. This baseline model, outlined in Fig. 4a, only considers per-frame information, disregarding the temporal context provided by preceding frames.\nConvLSTM: The ConvLSTM (SHI et al. 2015) architecture, described in Fig. 5a, designed for spatiotemporal prediction, serves as our temporal baseline. It replaces the input-to-state and state-to-state transitions of standard LSTMs with convolutional structures, which allow it to effectively leverage the visual information in the preceding context frames for improved predictions. However, it is computationally intensive and susceptible to overfitting, particularly with limited fine-tuning data.\nPredRNN: PredRNN (Wang et al. 2023) utilizes spatiotemporal LSTM units to model sequential dependencies in video data, and is widely used for future frame prediction tasks (Jadhav 2020; Ma, Zhang, and Liu 2022). We explore PredRNN's ability to model complex short-term dynamics for our future direction prediction use-case. However, like ConvLSTM, PredRNN is computationally demanding for both training and inference, with even higher latency due to its increased architectural complexity.\nIntent-based Navigation\nFor outdoor navigation, directions from Google or Apple Maps provide high-level guidance to destinations by localizing the user via GPS. However, GPS accuracy is limited to ~ 4.9 meters (GPS.gov 2024), making it insufficient for precise local navigation. To address this, the model must predict local directions, and the corresponding actions to take in the next second, that are aligned with the high-level directions from the Maps API and the user's GPS location history.\nIn this work, we leverage the Google Maps Directions API (Developers 2024), which provides step-by-step walking instructions for specified start and destination addresses. For the \"walking\" travel mode, the API returns an array of steps, each containing a start_location (latitude-longitude), end_location, and a maneuver or action to take at the end_location. Each step represents one segment of the travel. At each timestep, we pick the appropriate segment to retrieve the manueuver from based on the user's GPS position and the segment's start_location and end_location. Relevant maneuvers for walking include turn-slight-left, turn-left, turn-sharp-left, turn-slight-right, turn-sharp-right, turn-right, and straight. We convert the maneuver values into a one-hot-encoded vector and append it with the latitude and longitude from the start_location and end_location fields to form an Intent Feature for each step. These features are then combined with the user's current GPS coordinates and passed through a linear embedding layer to generate the Intent Embedding vector. This embedding vector serves as a high-level intent conditioning input for the model.\nThe following sections detail the modifications made to the baseline CNN and ConvLSTM architectures to incorporate Intent Embeddings. As discussed in the results in section 5, the performance improvements offered by PredRNN are outweighed by its high computational requirements and latency, which are critical factors for smartphone deployment. Hence, PredRNN is excluded from intent-based experiments. Instead, we implement a more efficient, hybrid CNN+LSTM architecture to capture both temporal and intent signals.\nCNN with Intent (Fig. 4b): Intent embeddings are concatenated with CNN-extracted frame embeddings, which are then passed through a 2-layer MLP for prediction.\nConvLSTM with Intent (Fig. 5b): Intent embeddings are projected down to two C-dimensional vectors, where C=3 corresponds to the number of channels in the video frame. These are then added as shift and scale vectors to the frame input channels before being passed through the ConvLSTM layers. While we explored other early fusion strategies, the current method demonstrated the best performance vs complexity tradeoff.\nCNN + LSTM with Intent (Fig. 6): This architecture enhances the Intent-based CNN model by replacing the MLP with a 2-layer LSTM in the final classifier. It combines past frame embeddings with intent embeddings, leveraging ResNet's powerful image feature extraction capabilities while modeling temporal relationships from past context with LSTMs. Compared to ConvLSTM, CNN+LSTM is more computationally efficient as the LSTM processes lower-dimensional embeddings instead of full image data."}, {"title": "4 Experiments", "content": "4.1 Label Imbalance\nDespite efforts to collect more turn-based data, the dataset remains significantly skewed toward the FRONT label, as seen in Fig. 7. However, predicting turns (LEFT and RIGHT) is more critical for navigation. To address this imbalance and improve turn prediction, we implemented the following during training:\n1. Minority Oversampling: Doubled LEFT/RIGHT class examples.\n2. Data Augmentation: Applied transforms described in Tab. 2 with a 20% probability.\n3. Loss Function:\nOversampling reduces class imbalance but does not fully address the more challenging, yet rarer, turn prediction cases near the start and end of a turn. To mitigate this, we employed Focal Loss (Lin et al. 2020), which emphasizes harder-to-classify samples by dynamically scaling their loss contribution. We also experimented with Weighted BCE Loss using class weights of 2:2:1 (LEFT:RIGHT:FRONT), which provided minor performance gains. These weights were integrated into our focal loss formulation:\n$FL(p_t) = \\sum_{c=1}^{C} w_c a (1 - p_t)^{\\gamma} log(p_t)$"}, {"title": "4.2 Experimental Setup", "content": "All models were fine-tuned using the label balancing settings described above. Best available public checkpoints were used to initialize the CNN and ConvLSTM components for both no-intent and intent-based models, as well as PredRNN. The final MLP in the CNN-based models, LSTM in the CNN+LSTM+Intent model and the Intent Embedding layers were trained from scratch. Training was conducted for 30 epochs with a batch size of 64, using the Adam optimizer with a weight decay of le-3. We used the CosineAnnealingLR scheduler with the learning rate initialized at 1e-4 for layers trained from scratch and at le-5 for finetuned layers.\nWe conducted ablation studies to assess the impact of different training data configurations. No-intent models are particularly well-suited for indoor scenarios, where GPS and high-level directions are unavailable, but multi-label supervision is provided. In contrast, intent-based models effectively leverage GPS and directional features in outdoor datasets. Accordingly, we trained the no-intent models exclusively on the indoor dataset and the intent-based models on the outdoor datasets, evaluating their performance on corresponding test sets. We then benchmarked these models against our generalized intent models trained on a combination of both indoor and outdoor datasets."}, {"title": "4.3 Evaluation Metrics", "content": "We evaluate the models using accuracy, AUC, Precision, Recall, and F1 score."}, {"title": "5 Results", "content": "Tab. 3 details the performance of the models described in section 3.4, trained on the combined indoor and outdoor datasets, and evaluated on a test set comprising both indoor and outdoor video segments. Tab. 4 further breaks down the AUC evaluations of these models for indoor and outdoor test videos separately. Finally, Tab. 5 reports the results of ablation experiments conducted with varying training datasets.\nFor all experiments, the performance of the FRONT label remains largely unaffected by the introduced modifications. Moreover, given that our use case requires a greater emphasis on turn (LEFT/RIGHT) classes, subsequent sections discuss the model performances of these classes only.\nPerformance of No-Intent Models\nWithout intent features, the ConvLSTM and PredRNN models outperform CNN due to their ability to leverage temporal context, which is particularly beneficial in the absence of high-level intent cues for path disambiguation. Temporal modeling enhances scene understanding, especially during turns, where past frames provide context about the ongoing action (e.g., turning), and the current frame helps decide whether to continue or conclude the turn.\nAmong the no-intent models, PredRNN achieves the best performance on the benchmark test dataset, surpassing ConvLSTM. This highlights PredRNN's ability to model egocentric future path predictions effectively, thanks to its advanced future frame prediction architecture. Interestingly, despite its greater complexity, PredRNN generalizes better than ConvLSTM, exhibiting less overfitting. However, the performance gains offered by PredRNN are not proportional to the computational complexity it introduces, making it an unsuitable candidate for on-device deployment.\nWe observe that no-intent models perform better on indoor video segments compared to outdoor scenes. This is expected, as indoor datasets provide multi-label supervision for ambiguous scenarios like intersections, while outdoor datasets use single-label annotations for each turn. Consequently, we observe many false positives at intersections in the outdoor test set because the no-intent models lack access to disambiguation through Intent Features.\nEffects of Adding Intent Features\nIncorporating intent features and GPS signals enhances the performance of CNN and ConvLSTM models over their no-intent counterparts, as seen in Tab. 3. Furthermore, Tab. 4 shows that the performance gains are much more pronounced for outdoor test videos, where intent and GPS signals provide explicit directional cues and help disambiguate turns.\nHowever, for the indoor test set, the performance difference between no-intent and intent-based models is negligible. A slight drop in performance is observed in the intent-based CNN model compared to its no-intent counterpart, likely due to the additional entropy introduced by zero-valued intent features in the indoor examples. Nonetheless, given the substantial gains observed for outdoor scenarios, the intent-based models represent a net positive improvement while supporting both scenarios.\nFinally, the CNN+LSTM+Intent model outperforms ConvLSTM+Intent in both evaluation metrics and computational efficiency. This mid-fusion approach surpasses the early fusion strategy used in ConvLSTM+Intent by independently extracting frame features while jointly modeling tem-"}, {"title": "5.1 Deployment", "content": "We deployed our best generalized model, CNN+LSTM+Intent, to an iPhone 13. Fig. 9 illustrates the deployment architecture. The model was quantized and converted to CoreML (Documentation 2024) for iOS deployment. Primary objectives during this phase were to minimize inference latency and on-device resource usage, including memory, GPU, and battery, while ensuring user data privacy.\nTo optimize these, we tuned the video frame rate and conducted quantization experiments, monitoring the device's resource consumption metrics. Fig. 10 summarizes the results. The final configuration utilized 16-bit quantization and a video frame rate of 2 FPS, which provided an effective balance between navigation performance and resource efficiency. Given the walking speeds and environments of our target users, 2 FPS is sufficient for reliable real-time navigation."}, {"title": "6 Discussion", "content": "In this work, we propose a novel intent-conditioning technique to extend egocentric path prediction models to handle scenarios with or without a destination. Our approach uses simple architectures commonly employed in the literature for egocentric path prediction, owing to the latency constraints of our system. Future research could explore the potential of this technique with more advanced architectures.\nThe relatively smaller size of our dataset also led us to focus on simpler models to mitigate overfitting. Expanding the dataset to include more scenarios could facilitate the exploration of larger and more complex architectures. Additionally, data from a broader range of smartphone cameras could improve the model's invariance and generalizability across different hardware configurations.\nWhile our current models predict three directional classes (LEFT, RIGHT, FRONT), they can be easily extended to support more nuanced directions, such as turning angles at 10-degree increments and start/stop walking instructions, given additional data and corresponding labels.\nOur research into the cane-walking patterns of blind users and the social constructs around them allowed us to scope the problem to predicting paths based on the limited scene information captured by a smartphone camera. However, explicitly modeling the behaviour of entities in the user's environment-such as obstacles, pedestrians, or vehicles could enable the generation of more nuanced navigation paths. Addressing these in future work could help bridge the gap between current capabilities and real-world needs."}, {"title": "7 Conclusion", "content": "This paper introduces AI Guide Dog (AIGD), an egocentric navigation system for visually impaired individuals. By introducing a novel intent-conditioning technique and a multi-label classification framework, we address key challenges such as goal-based navigation in outdoor settings and directional uncertainty in exploratory scenarios. Our approach effectively balances simplicity and performance, enabling generalization across diverse environments while remaining practical for real-time deployment on smartphones. Given the limited research in this domain, we hope this work inspires further advancements in assistive navigation technologies."}]}