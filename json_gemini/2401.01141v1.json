{"title": "Spiker+: a framework for the generation of efficient Spiking Neural Networks FPGA accelerators for inference at the edge", "authors": ["Alessio Carpegna", "Alessandro Savino", "Stefano Di Carlo"], "abstract": "Including Artificial Neural Networks (ANNs) in embedded systems at the edge allows applications to exploit Artificial Intelligence (AI) capabilities directly within devices operating at the network periphery, facilitating real-time decision-making. Especially critical in domains such as autonomous vehicles, industrial automation, and healthcare, the use of ANNs can enable these systems to process substantial data volumes locally, thereby reducing latency and power consumption. Moreover, it enhances privacy and security by containing sensitive data within the confines of the edge device. The adoption of Spiking Neural Networks (SNNs) in these environments offers a promising computing paradigm, mimicking the behavior of biological neurons and efficiently handling dynamic, time-sensitive data. However, deploying efficient SNNs in resource-constrained edge environments requires hardware accelerators, such as solutions based on Field Programmable Gate Arrays (FPGAs), that provide high parallelism and reconfigurability. This paper introduces Spiker+, a comprehensive framework for generating efficient, low-power, and low-area customized SNNs accelerators on FPGAs for inference at the edge. Spiker+ presents a configurable multi-layer hardware SNNs, a library of highly efficient neuron architectures, and a design framework, enabling the development of complex neural network accelerators with few lines of Python code. Spiker+ is tested on two benchmark datasets, the MNIST and the Spiking Heidelberg Dataset (SHD). On the MNIST, it demonstrates competitive performance compared to state-of-the-art SNN accelerators. It outperforms them in terms of resource allocation, with a requirement of 7,612 logic cells and 18 Block RAMs (BRAMs), which makes it fit in very small FPGAs, and power consumption, draining only 180mW for a complete inference on an input image. The latency is comparable to the ones observed in the state-of-the-art, with 780\u00b5s/img. To the authors' knowledge, Spiker+ is the first SNNs accelerator tested on the SHD. In this case, the accelerator requires 18,268 logic cells and 51 BRAMs, with an overall power consumption of 430mW and a latency of 54 \u00b5s for a complete inference on input data. This underscores the significance of Spiker+ in the hardware-accelerated SNN landscape, making it an excellent solution to deploy configurable and tunable SNN architectures in resource and power-constrained edge applications.", "sections": [{"title": "INTRODUCTION", "content": "Integrating Artificial Neural Networks (ANNs) at the edge\nis a pivotal computing advancement, enabling direct appli-\ncation of Artificial Intelligence (AI) capabilities in devices\nand systems at the periphery of networks, resulting crucial\nin domains like autonomous vehicles, industrial automa-\ntion, and healthcare [1]. Using ANNs at the edge allows\nsystems to process substantial data locally, reducing latency,\npower consumption, and reliance on external data centers or\ncloud services. Additionally, it enhances privacy and secu-\nrity by keeping sensitive data within the edge device. This\napproach boosts the effectiveness and agility of embedded\napplications, paving the way for innovative breakthroughs\nacross sectors and ushering in a new era of intelligent,\ndecentralized computing.\nNeural networks, tailored for diverse computational\ntasks, include feed-forward networks for pattern recogni-"}, {"title": "BACKGROUND", "content": "This section overviews foundational knowledge on SNNs,\nrequired to understand the remaining parts of the paper."}, {"title": "Spiking Neural Networks", "content": "SNNs distinguish themselves through their unique infor-\nmation encoding based on spikes, inspired by neuroscience.\nIndeed, this neuron model mimics biological neurons and\nsynaptic communication mechanisms based on action po-\ntentials. The information is thus represented as a flow of\nspikes with various neural coding techniques, shifting the\ncomputational complexity from the spatial dimension to\nthe temporal dimension. Spike encoding methods in SNNs\nrange from real current or voltage pulses in specialized\nanalog circuits to numerical representations in software\nor dedicated digital implementations. This paper focuses\nprimarily on the latter. Spikes convey information through\ntemporal organization, and in the digital domain, they can\nbe approximated as binary values: 'one' for received spikes\nand 'zero' otherwise. Essentially, neurons are translated into\ncompact computational units that exchange data, i.e., their\nactivations through binary bit streams."}, {"title": "Neuron models", "content": "Over the past decades, numerous neuron computational\nmodels have emerged, originating from nerve electrical\nconductance measurements and mathematical modeling.\nThe Hodgkin-Huxley model [10], inferred from squid giant\naxon measurements in 1952, is realistic but complex and\nhas evolved into the simplified Izhikevich model [11]. How-\never, for hardware implementations, balancing accuracy and\ncomplexity commonly leads to using the Leaky Integrate\nand Fire (LIF) model and its simplified version, Integrate\nand Fire (IF) [12]. These models aim to mathematically\ndescribe the behavior of a biological neuron focusing on its\nmembrane, which is crucial to defining the internal elec-\ntrical processes. The membrane selectively permits specific\nion passage, accumulating charge and creating a membrane\npotential ($V_m$) defining the neuron's state and behavior.\nThe LIF model in Spiker+ encompasses a family of\nneuron models with varying levels of simplification. Equa-\ntion 1 introduces the discrete-time formulation of a Synaptic"}, {"title": "Training", "content": "Training an SNN involves tuning it for specific problem-\nsolving, such as classifying input data. The training process\nadjusts synaptic weights (W) and internal neuron param-\neters like threshold ($V_{th}$) and time constants ($\\alpha$ and $\\beta$) to\nenhance model accuracy.\nTraining SNNs presents notable challenges, mainly due\nto the non-differentiability of the SNN activation func-\ntion. This paper adopts the Surrogate Gradient approach\n[6], replacing the non-differentiable gradient with a surro-\ngate function, like the spike function itself or a Gaussian\nfunction. This enables the application of standard super-\nvised learning techniques, such as BPTT, overcoming non-\ndifferentiability and facilitating effective SNN training.\nAlternative solutions, like e-prop [13] for Recurrent Spik-\ning Neural Network (RSNN) and STDP for unsupervised\nweight tuning based on spike timing, exist. However, as\nSpiker+ focuses on inference using pre-trained parameters,\nthese methods fall outside the paper's scope."}, {"title": "NEUROMORPHIC ACCELERATORS: RELATED WORK", "content": "In the past, SNNs were primarily implemented using soft-\nware frameworks like Brian/Brian2 [14]. However, their\nunique features, including high parallelism, temporal evolu-\ntion, and event-driven computation, are ill-suited for domi-\nnant Von-Neumann Central Processing Unit (CPU) architec-\ntures with one or a few powerful computational units. Un-\nfortunately, Single Instruction Multiple Data (SIMD) archi-\ntectures, such as General Purpose Graphic Processing Units\n(GPGPUs) and Tensor Processing Units (TPUs), optimized\nfor standard ANNs workloads, are also not well-equipped\nfor efficiently processing event-driven information across\nmultiple timesteps [15]. Furthermore, the binary spike en-\ncoding of SNNs does not align with the typical 64, 32, or\n16-bit numeric representations of these SIMD architectures.\nTherefore, dedicated neuromorphic hardware is crucial for\nensuring the widespread adoption of SNNs. Contributions in this domain span various design di-\nmensions, including application-driven solutions focused\non specific applications and those aimed at modeling bi-\nological neuron dynamics. However, this paper primarily\nemphasizes the hardware technology dimension. The re-\nsearch effort is divided between analog solutions based on\nemerging technologies and efficient digital implementations\n[17]. In the digital realm, presented solutions differ on\nthe target platform (ASIC or FPGA) and accelerator size,\ntailored for either large-scale systems or smal applications.\nExamining large network models, the SpiNNaker system\ndeveloped at Manchester University is implemented using\nstandard 32-bit ARM M4F CPUs simulating neuron activity.\nIt optimizes spike routing between units [18]. Promisingly,"}, {"title": "SPIKER+ ARCHITECTURE", "content": "This section presents the Spiker+ hardware architecture,\nwhich serves as the central component of the Spiker+ SNN\nhardware acceleration framework. The architecture is in-\ntroduced top-down, beginning with the high-level network\nmodel and then delving into the neurons and input/output\ninterfaces."}, {"title": "Network architecture", "content": "The SNN architecture presented here builds upon the initial\nSpiker architecture introduced in [5]. While our earlier work\nprovided a proof of concept tailored for inference on the\nMNIST dataset [7], derived from the SNN model by Diehl\nand Cook [45], Spiker+ focuses on a generic and fully\nconfigurable architecture adaptable to various problems.\nFigure 3 depicts the high-level architecture of a toy ex-\nample of a three-layer FF-FC architecture used to introduce\nthe three hierarchical levels of Control Units (CUs) that\ncharacterize Spiker+: (i) the network CU, responsible for\nsynchronizing the various components within the network;\n(ii) the layer CUs, orchestrating the update of the neurons of\na layer based on a set of input spikes; (iii) the neuron CU:\nthe accelerator core controlling the update of the membrane\npotential in each neuron. This organization represents a\nhighly optimized architecture in terms of performance and\nspace utilization.\nBlock communication is based on a simple two-signal\n(start/ready) handshake protocol to ensure high modu-"}, {"title": "Network CU: global synchronization", "content": "The primary function of the Network CU is to coordinate\nthe temporal evolution of the neurons of the different layers\nduring an inference. As previously mentioned, in an SNN,\ninformation is encoded as trains of spikes (i.e., sequences of\nbits) received on every input. Each train is characterized by a\ngiven duration (i.e., the number of transmitted bits) denoted\nas N_cycles. Thus, the network performs inference by\nevolving over N_cycles temporal steps to analyze the\ntemporal patterns."}, {"title": "Layer CU: deliver spikes to neurons", "content": "In a fully connected multi-layer SNN as the one proposed\nin Figure 3, a parallel update of each layer involves three\ndimensions: (i) the number of neurons, (ii) the number of\ninputs processed by each neuron, and (iii) the temporal\ndimension of each input, representing the number of cycles.\nThe last dimension is inherently sequential and cannot\nbe parallelized, as it depends on the temporal evolution\nof the inputs. This dimension is managed by the Network\nCU discussed in subsection 4.2, which manages the update\nduring each cycle.\nIf the network is sufficiently small, it could be feasible\nto update all neurons with their inputs in parallel within\na single cycle. However, the network and input data sizes\nare typically too high to achieve such a degree of paral-\nlelism. Consequently, Spiker+ exploits only one dimension\nto obtain parallelism, concurrently updating all neurons\nwithin a layer while sequentially providing inputs to each\nneuron. The Network CU, depicted in Figure 4b, oversees\nthis process.\nOnce again, this circuit operates based on a\nstart/ready protocol. The control unit receives a start\nsignal from the Network CU and enters a loop: it awaits the\nreadiness of the neurons composing the layer to process a\nnew spike (neurons ready signal), initiates the computa-\ntion (neurons start signal), and increments the internal\ncounter (CNT). The counter directly selects the spike to be\nprocessed from the sampled inputs. When all input spikes\nhave been provided to the neurons (N_INPUTS), the loop\nconcludes, and the control unit asserts the ready signal.\nAn additional component visible in the upper section\nof Figure 4b is an OR gate utilized to verify if there is\nat least one active spike among the inputs. Currently, no\nencoding or compression has been applied to the spikes.\nHowever, considering that SNNs typically exhibits sparse"}, {"title": "Neuron models", "content": "All the different LIF neuron models presented in subsec-\ntion 2.2 are translated into dedicated hardware implemen-\ntations in Spiker+, trying to minimize the required com-\nponents. Building upon the groundwork laid in [5], the\nproposed neuron functions as a Multiply and Accumulate\n(MAC) unit, augmented with additional components and\ncontrols to manage its various states. Figure 5 shows the\nobtained architectures, in order of increasing complexity\nfrom left to right (IF, I-order LIF and II-order LIF), with the\nsubtractive reset on top and the fixed one on the bottom.\nFrom a hardware perspective, the most critical factors of\nthe characteristic equation of the neurons are the multipli-\ncations. Four of them can be found in Equation 1:\n1) The synapses weighting: $W \\cdot Sin[n]$\n2) The reset: $V[n \u2013 1] \\cdot r$\n3) The exponential decays: $\\alpha \\cdot Isyn [n \u2013 1]$ and\n$\\beta \\cdot Vm [n-1]$\nFor the first one, exploiting the binary nature of the\nspikes reduces the operation to a simple selection: zero\nif there is no spike, W if a spike is present. This can be\nimplemented as a bitwise AND between the weight and the\nspike.\nThe reset operation can be applied in two ways, as\nshown in equations 3 and 4. The first case exploits the binary\nnature of the spike: either the membrane is kept at its value\nor reset to zero, so this is again a selection process more\nthan a multiplication. The hardware implementation is a bit\nmore general since it allows to explicitly choose the value\nof Vreset, which in this case can also be different from 0, as\nshown in figures 5d, 5e and 5f. The second reset method can\nbe obtained by simply subtracting the threshold voltage Vth\nfrom the computed value of Vm.\nAt this point, the last critical multiplication is the one\nrequired to compute the step-by-step exponential decay of\nthe membrane. The problem exists only for the two LIF\nmodels (in the IF model, the membrane is kept fixed with-\nout stimuli), with one multiplication needed in the I-order\nversion and two multiplications in the II-order one. The\ncriticality is solved once again, exploiting the characteristics\nof binary operations. If one of the operators is representable\nas a power of two, the multiplication can be reduced to a\nsimple bit-shift. Since there is no control on the values of\nIsyn and Vm, which evolve dynamically during the update\nof the network, the only parameters on which it is possible\nto act are the constant hyper-parameters a and B. The values\ncan vary between 0 and 1, with larger values corresponding\nto slower exponential decay. Generally, a value near to 1\nis observed. In this case a and \u1e9e can be approximated as\n$\\alpha = 1 - \\alpha'$ and $\\beta = 1 - \\beta'$, where a' and \u03b2' are negative\npowers of 2. As shown in [5], the overall accuracy has no"}, {"title": "Synapses", "content": "The primary advantage of implementing SNNs on dedi-\ncated hardware, alongside the execution parallelism, lies\nin the opportunity to integrate memory and computation.\nOn an FPGAs, this integration can be achieved through two\ndistinct methods.\nFor relatively small memory requirements, such as the\ninternal parameters of the neurons, the internal Look Up\nTables (LUTs) can serve as a viable memory solution. This\napproach offers superior speed, leveraging Flip Flops (FFs)\nand registers. However, the available space is limited, pri-\nmarily due to the necessity of accommodating the logical\nfunctions of the network within the LUTs.\nIn scenarios where a larger memory capacity is neces-\nsary, particularly for synaptic weights, many FPGAs grant\naccess to discrete units of Static Random Access Memory\n(SRAM) strategically positioned close to the computing\nelements, commonly referred to as Block RAM (BRAM).\nSpiker+ provides a synapse interface, implementing the\nstart/ready handshake protocol, and relies on an initial-ization file containing quantized weights. Weighs are stored\ninto BRAMs. Spiker+ expects all neurons to access their\nrespective weights in parallel upon activating the ready\nsignal by the synapse; therefore, it strongly relies on the\nhigh parallelism provided by on-board BRAMs. Spiker+\nalso permits storing weights in an external Dynamic Ran-\ndom Access Memorys (DRAMs) when on-board space is\ninsufficient. In such situations, the synaptic interface loads\nthe weights for the current cycle before asserting the ready\nsignal, impacting the accelerator's speed.\nA secondary configurable attribute concerning synapses\ninvolves incorporating feedback connections, such as inter-\nlayer inhibitory connections. Spiker+ can be configured to\ninclude or exclude these connections, depending on the\napplication requirements."}, {"title": "I/O interface", "content": "Spiker+ requires an input/output interface to receive data\nand transmit results. Spiker+ supports two scenarios. In\nthe simple scenario, inputs have already been encoded as\nspikes. For instance, these data may originate from neuro-\nmorphic sensors, such as a Dynamic Vision Sensor (DVS)\ncameras or a silicon cochlea. Alternatively, they could be\npre-encoded by an external block before being stored.\nIn a more complex scenario, data are stored in a raw\nnumeric format and converted on-board into spike streams.\nThere are different methods available for this conversion,\ndepending on the type of input data [43]: (i) firing rate\ncoding (i.e., information is encoded using the instantaneous\naverage firing rate), (ii) population rank coding (i.e., infor-\nmation is encoded using the relative firing time of a popu-\nlation of neurons), or (iii) temporal coding (i.e., information\nis encoded with the exact timing of individual spikes). An\nefficient rate encoding structure such as the one proposed\nin [5] can be directly connected to Spiker+. Furthermore,\nseveral possibilities exist concerning data transmission: data\nmay arrive as a continuous stream directly from a sensor"}, {"title": "CONFIGURATION FRAMEWORK", "content": "Spiker+ goes beyond being a mere hardware accelerator; it\nis a comprehensive design framework that facilitates easy\ncustomization of the SNN accelerator for specific applica-\ntions. As detailed in section 4, the platform encompasses\nsix distinct neuron models, a modular layer interface al-\nlowing instantiating any desired number of layers, and\ncustomizable inter-layer feedback connections. However,\nmanually defining the architecture at the Register Transfer\nLevel (RTL) requires substantial effort. To tackle this chal-\nlenge, Spiker+ incorporates a Python-based configuration\nframework, streamlining the customization process to just\na few lines of code. The customization and tuning flow for"}, {"title": "EXPERIMENTAL RESULTS", "content": "Spiker+ is evaluated using two widely recognized bench-\nmark datasets: (i) MNIST [7] and (ii) SHD [8].\nMNIST comprises grayscale images of handwritten dig-\nits from 0 to 9, commonly used to benchmark AI algo-\nrithms. This dataset is ideal for comparing Spiker+ with\nother SNN accelerators. Images are converted into spikes\nusing Poisson-distributed rate encoding. Due to the dataset\nsimplicity, a basic I-order LIF model with a FF-FC structure\nsuffices for accurate classification.\nSHD is explicitly designed as an SNN benchmark,\ncontaining recordings of people pronouncing numbers in\nEnglish and German. It requires a more complex neuron\nmodel, specifically a II-order LIF, and a network architecture\nwith inter-layer recurrent connections to account for the\nimportance of the time dimension in achieving acceptable\nclassification accuracy."}, {"title": "Benchmarking", "content": "Table 2 provides a comprehensive comparison of Spiker+\nwith recent state-of-the-art FPGA accelerators designed for\nSNNs on the MNIST dataset. The table is split into two\nsections. The upper section covers Spiking Convolutional\nNeural Networks (SCNN) accelerators, where spiking layers\nare strategically placed after or interleaved with standard\nconvolutional layers, gradually identifying key features in\ninput images. The lower section considers pure spiking\naccelerators with fully connected layers of either IF or\nLIF neurons. Notably, the comparison focuses on works\npublished from 2020 onward, while references such as [5]"}, {"title": "Performance vs input activity", "content": "As introduced earlier, the input spiking activity influenced\nby the encoding method significantly impacts the accelera-\ntor's performance. Before diving into this analysis, Figure 7\nvisually represents the average number of active cycles at\ndifferent network layers. A notable difference is observed\nbetween the two considered datasets. In MNIST, the activity\ndecreases monotonically across the network, while for SHD,\nthere is a peak of activity in the first hidden layer. This\ndifference may arise from the inter-layer feedbacks in SHD,\nleading to higher joint activity than the FF-FC architecture\nused in MNIST.\nSince all layers update in parallel and process inputs\nsequentially, latency is determined by the slowest layer\n(i.e., the layer handling the largest set of inputs). In the\narchitectures detailed in Table 1, the slowest layer is the\ninput layer, processing 784 inputs for MNIST and 700 for\nSHD. In this layer, 100% of time-steps contain at least one\nspike for MNIST, while for SHD, the percentage is around\n48%. Consequently, SHD, with the combination of a lower\nnumber of inputs and lower activity in the input layer,\nenables increased inference speed. Since both models use\nthe same number of time steps and clock frequency, and\nthe difference in the number of inputs is not significant,\none might expect about 48% inference time reduction for\nSHD compared to MNIST due to the reduced activity (i.e.,\nabout 0.37 ms). However, the observed value in Table 3 is\n0.54 ms. The higher latency is explained by the FC-R model\nused by SHD incorporating inter-layer feedback connec-\ntions, processed sequentially. Therefore, an additional set of\n200 feedback inputs must be processed for the first layer,\nwith an average of 93% active time steps.\nThe input activity not only impacts inference latency,\nas explained earlier. When the activity decreases, there is"}, {"title": "Performance vs quantization", "content": "As one of the key features of Spiker+ is the optimization of\nthe accelerator through quantization of weights and mem-\nbrane potentials, it is crucial to examine how these design\nchoices influence performance. Latency is not expected to\nchange, as it is independent of the chosen bit-widths. The\nprimary presumed impacts are on power consumption and\nnetwork accuracy. Figures 9 and 10 illustrate the results of\nquantization on MNIST and SHD, respectively."}, {"title": "Performance vs sizing", "content": "Finally, let us explore the model complexity achievable with\nSpiker+ on selected Xilinx\u2122\u2122\u2122 FPGA boards. Synthesis results\nfor three Xilinx\u2122\u2122\u2122 boards, particularly low-end ones suitable\nfor resource-constrained edge applications, are presented in\nTable 4.\nOn the Xilinx\u2122 XC7Z020/XA7Z020 boards discussed in\nsubsection 6.1, the largest FF-FC network possible using a I-\norder LIF consists of 1,220 neurons, utilizing 138 BRAMS\n(98.5%) and 42,430 LUTs (26.7%). The BRAM size of the\nFPGA emerges as the limiting factor. On the slightly more\nadvanced Xilinx\u2122 XCZU3EG board, a larger 1,900-neuron\narchitecture, utilizing 215 BRAMs (99.5%) and 62,989 LUTs\n(29.8%), can be implemented. Notably, the place-and-route\nalgorithm encounters no obstacles, reinforcing that BRAM\nlimitations govern the network size.\nFor FC-R architectures using a II-order LIF, where both\nfeed-forward and feedback weights need storage in BRAM,\nthe maximum network size is influenced. Specifically, it\nis capped at 550 neurons for Xilinx\u2122\u2122\u2122 XC7Z020/XA7Z020\nboards and 690 neurons for the Xilinx\u2122 XCZU3EG board.\nPotential issues such as place-and-route complexities\nor excessive power consumption due to the fully parallel\nnature of the accelerator are foreseeable. A prospective"}, {"title": "CONCLUSIONS", "content": "This paper introduced Spiker+, a versatile framework to de-\nsign low-power and resource-efficient hardware accelerators\nfor SNNs targeting edge inference on FPGA platforms. It\nfeatures a Python configuration framework that facilitates\neasy reconfiguration of the accelerator, allowing users to\nchoose from six neuron models (IF, I-order LIF, and II-\norder LIF, each with the option of a hard or subtractive reset)\nand two network architectures (FF-FC and FC-R). The tool\nenables the automatic selection of training and quantization\nparameters directly through Python. The results are signifi-\ncant, boasting a 93.85% accuracy on MNIST, with a classifi-\ncation latency of 780\u00b5s per image and power consumption\nof 180mW. Additionally, it achieves a 72.99% accuracy on\nSHD, corresponding to a 540\u00b5s latency and power con-\nsumption of 430mW. These metrics are highly competitive\ncompared to state-of-the-art FPGA accelerators for SNNs,\ndemonstrating high performance in both power efficiency\nand area. This work lays a solid foundation for deploying\nspecialized, low-power, and efficient SNN accelerators in\nresource and power-constrained edge applications. Spiker+\nis a live project, and ongoing work focuses on enlarging\nthe library of available neurons, input encoding blocs, and\nnetwork architectures. To encourage research in this field,\nSpiker+ is available as an open-source project\u00b2."}]}