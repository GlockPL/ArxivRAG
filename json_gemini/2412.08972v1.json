{"title": "RULEARENA: A Benchmark for Rule-Guided Reasoning with LLMs in Real-World Scenarios", "authors": ["Ruiwen Zhou", "Wenyue Hua", "Liangming Pan", "Sitao Cheng", "Xiaobao Wu", "En Yu", "William Yang Wang"], "abstract": "This paper introduces RULEARENA, a novel and challenging benchmark designed to evaluate the ability of large language models (LLMs) to follow complex, real-world rules in reasoning. Covering three practical domains\u2014airline baggage fees, NBA transactions, and tax regulations\u2014RULEARENA assesses LLMs' proficiency in handling intricate natural language instructions that demand long-context understanding, logical reasoning, and accurate mathematical computation. Two key attributes distinguish RULEARENA from traditional rule-based reasoning benchmarks: (1) it extends beyond standard first-order logic representations, and (2) it is grounded in authentic, practical scenarios, providing insights into the suitability and reliability of LLMs for real-world applications. Our findings reveal several notable limitations in LLMs: (1) they struggle to identify and apply the appropriate rules, frequently becoming confused by similar but distinct regulations, (2) they cannot consistently perform accurate mathematical computations, even when they correctly identify the relevant rules, and (3) in general, they perform poorly in the benchmark. These results highlight significant challenges in advancing LLMs' rule-guided reasoning capabilities in real-life applications.", "sections": [{"title": "1 Introduction", "content": "Recently, Large Language Models (LLMs) (Tou-vron et al., 2023; OpenAI, 2023; Team, 2023; Anthropic, 2024) have demonstrated remarkable capabilities across a range of real-world applications, including code synthesis (Rozi\u00e8re et al., 2023) and customer service (Shi et al., 2024). However, their limited domain-specific knowledge often leads to generating unfaithful or misleading information, which can cause significant risks and financial liabilities. For example, Canadian airline was recently required to compensate a customer who received in-correct guidance from the airline's chatbot\u00b9. These challenges highlight the need for robust, real-world benchmarks that assess how faithfully and accurately LLMs can follow real-life instructions and adhere to relevant regulations, thereby ensuring reliable and safe outputs for deployment.\nAlthough several studies have examined LLMs\u2019 instruction-following abilities (Chen et al., 2024a; Jiang et al., 2024; Wen et al., 2024), they have primarily focused on stylistic constraints, such as the expected format (Zhou et al., 2023), length, or topic of responses. Yet, the significance of instruction-following extends well beyond stylistic compliance. In many problem-solving scenarios, instructions function as rules: they impose logical constraints on the reasoning process and specify how answers should be derived from given inputs. However, limited attention has been devoted to LLMs' capacity to follow complex rules.\nExisting research (Mu et al., 2023; Sun et al., 2024) largely addresses only single-step, first-order logic reasoning or artificially synthesized logical tasks. In contrast, real-world rules frequently appear in diverse and nuanced natural language forms. They may involve intricate logical structures, including the need for parallel reasoning across multiple rules or navigating interdependent rule sets. For instance, to calculate the fees for checked luggage when taking flights, one needs to consider the base price for checking each item, the overweight and oversize charges, and how these charges should be aggregated together. The extent to which LLMs can accurately follow these complex, real-world rules\u2014an ability we term rule-guided reasoning\u2014remains unknown. To better understand the complexity and practical implications of rule-guiding reasoning, we introduce a new evaluation benchmark, RULEARENA, grounded in realistic scenarios."}, {"title": "2 Related Work", "content": "Complex Instruction-following Benchmarks A wide range of benchmarks has been designed to evaluate LLMs' instruction-following abilities from various perspectives, including semantics (Zheng et al., 2023; Li et al., 2023; Liu et al., 2023), format (Xia et al., 2024; Tang et al., 2023), and response length (Chen et al., 2024b; Sun et al., 2023). To further probe complexity, some works have introduced benchmarks that construct complex instructions through compositional methods. For example, WizardLM (Xu et al., 2023) generates intricate tasks by combining simpler instructions, while CELLO (He et al., 2024) uses task descriptions and input texts to create complex prompts grounded in real-world scenarios. ComplexBench (Wen et al., 2024) adopts multiple compositional structures to integrate atomic requirements into more challenging instructions. In contrast, our work focuses on instructions derived directly from real-life scenarios, where naturally occurring complexities arise from multifaceted constraints incurred by inputs on the set of instructions.\nLogical Reasoning Benchmarks Extensive research has explored benchmarks for mathematical (Koncel-Kedziorski et al., 2016; Ling et al., 2017; Amini et al., 2019; Cobbe et al., 2021; Hendrycks et al., 2021) and logical (Mao et al., 2019; Gupta et al., 2020; Tafjord et al., 2021; Zhong et al., 2022; Han et al., 2022; Zhang and Ding, 2024) reasoning, evaluating LLMs' abilities to solve math problems of varying difficulty, tackle coding challenges, and engage in deductive logic. Although these benchmarks test models' reasoning skills, their logical constraints are often represented in simplified, formal systems, such as propositional (Hua et al., 2024) or first-order logic (Zhu et al., 2023; Mu et al., 2023; Sun et al., 2024). In contrast, our benchmark deals with rules that arise in natural language, capturing a richer, more realistic set of constraints. Such natural language rules extend beyond neatly formalized logical representations, often express higher-order logic and more intricate relationships than typical propositional or first-order logic formalizations."}, {"title": "3 RuleArena", "content": "In this section, we present the RULEARENA benchmark and its construction process. We begin by describing the domains we have chosen and the corresponding regulations from which our rules are collected. We then describe how problems with varying difficulty levels are generated and how the ground-truth solutions are computed. Finally, we present the evaluation metrics we used to evaluate whether correct rules are correctly applied."}, {"title": "3.1 Domains and Rule Collection", "content": "We select three real-life domains that are both familiar in everyday life and demonstrate a high level of complexity:\nAirline. It requires LLM to calculate the total cost for one or more passengers, including their flight ticket and checked baggage fees. The regulations are extracted from policy of American Airlines. The complexity stems from the fact that baggage costs vary according to factors such as cabin class, flight origin and destination, the number of checked bags, and the size of each bag. Consequently, LLMs must carefully identify the correct baggage-related rules and apply them accurately to determine the final cost.\nNBA transaction. It requires LLMs to determine whether one or more specified transactions are allowed. The regulations are extracted from the 2023 NBA Collective Bargaining Agreements (CBA) and excerpt from the NBA Constitution and By-Laws. Complexity arises from the numerous factors influencing transaction eligibility, including the player's contract value, salary-matching constraints, and the specific transaction date. LLMs must accurately identify and apply the relevant rules from the agreement to determine whether a given transaction can proceed.\nTax. It requires LLMs to calculate the income tax for one person or family given their financial information. The regulations are collected from Internal Revenue Service. Although taxes are a common and universally encountered aspect of modern life, they are also known for their complexity. This complexity stems from a wide range of factors, including salary income, investment gains, gifts, home ownership and related expenses, as well as the jurisdiction in which income is earned. To arrive at the correct tax amount, LLMs must navigate and apply the appropriate rules drawn from these multifaceted conditions."}, {"title": "3.2 Problem Annotation", "content": "After gathering the relevant rules for each domain, we construct challenging test problems designed to evaluate whether LLMs can produce correct outputs from the provided rules.\nAirline. The problems are generated by randomly selecting passenger information (e.g., cabin class, itinerary, ticket price) and their checked baggage details (e.g., dimensions, weight). We convert each regulation into a corresponding rule-based script, enabling the direct calculation of ground-truth answers by executing these scripts. LLM performance is then assessed by comparing the model's computed solutions to the script-derived ground truths, step by step.\nNBA Transaction. The problems consist of proposed trades that may or may not comply with NBA regulations. Because these problems require a wide variety of operations and rule sets, fully automated generation and evaluation are difficult. Therefore, we employ annotators familiar with NBA transaction rules to curate complex test cases and identify all the relevant rules needed to resolve each case (further details in Appendix B.2). For each problem, we ask the LLM whether the transaction is legit or not based on the regulations. If LLM thinks the transaction is legit, it should generate \u201cYes\u201d; otherwise, it needs to identify the specific team and transaction that violates the rules.\nTax. The problems are randomly generated from hypothetical taxpayer profiles including information such as income levels, filing status, etc. IRS tax regulations are translated into rule-based scripts to compute ground-truth tax obligations. As with"}, {"title": "3.3 Difficulty control", "content": "To assess LLMs' capabilities under varying levels of complexity, we create problems with different degrees of difficulty. We define three levels of difficulties in each domain:\nAirline. The difficulty is controlled by adjusting the number of bags a passenger carries.\nNBA Transaction. Complexity is determined by increasing the number of teams, players, and transactions involved in a scenario.\nTax. The level of difficulty is raised by progressively introducing additional tax forms and thus relevant regulations."}, {"title": "3.4 Evaluation Metrics", "content": "To achieve a comprehensive evaluation of the rule-following abilities of Large Language Models (LLMs), we introduce a set of evaluation metrics. Unlike existing benchmarks (Hua et al., 2024; Fan et al., 2023; Zhu et al., 2023), which primarily rely on simple metrics such as answer accuracy or BLEU scores, our approach aims to conduct a more detailed analysis of the step-by-step rule-guided reasoning process. This analysis includes examining each rule application to determine whether the rule should be applied, whether any rules are missed, and whether the rule application computation process is accurate.\nFor each domain, assuming a set \\(T\\) of \\(N\\) problems and a set \\(R\\) of \\(M\\). For each problem \\(t_i = (q_i, a_i, R_i)\\), we have a query \\(q_i\\), an answer \\(a_i\\), and a set of relevant rules \\(R_i\\), together with a rule-usage matrix \\(U \\in \\mathbb{R}^{N \\times M}\\), where each item \\(U_{i,r} \\in \\{0,1\\}\\) indicates whether a ruler is used by an LLM in problem \\(t_i\\). Matrix \\(U\\) can be approximately obtained by parsing LLMs' responses using an LLM, which we will introduce in Section 4.1.\nNow we introduce two groups of metrics:\nThe first category focuses on problem-level evaluations: for each problem, we examine whether all necessary rules were applied, whether any extraneous rules were applied, and whether the final answer aligns with the ground-truth solution:\nProblem-wise Recall: denoted as \\(R(t)\\), measures whether LLMs apply all relevant rules for a problem \\(t\\). For each problem \\(t_i\\), \\(P(t_i)\\) is calculated as"}, {"title": "4 Experiments", "content": "This section presents the experiments on benchmark. We first introduce the LLMs and prompting strategies we use to evaluate, and then present the evaluation result."}, {"title": "4.1 Experiment Settings", "content": "LLMs. Our rules, which are prompted directly into LLMs, can be of a length up to 20,000 tokens. Therefore, we only consider LLMs that can handle such long contexts, including Llama-3.1 70B, Llama-3.1 405B (Dubey et al., 2024), Qwen-2.5 72B (Qwen Team, 2024), Claude-3.5 Sonnet (Anthropic, 2024), and GPT-40 (OpenAI, 2024) 6.\nPrompting Strategies. Since rule-guided reasoning can be an intricate multi-step reasoning process in our three real-world scenarios, we use Chain-of-Thought (CoT) (Wei et al., 2022; Kojima et al., 2022) reasoning by default. To further study if LLMs can learn to follow hard rules through in-context examples, we also compare 0-shot with 1-shot CoT given an example including a task of the lowest difficulty and its solution. Due to context limit, we do not further increase the number of in-context examples.\nOutput Parsing. To obtain the rule-usage matrix U we mentioned in Section 3.4, we utilize the structured output mode of GPT-40 (OpenAI, 2024) to parse the raw textual responses from LLMs. Specifically, for airline and tax problems we structuralize the ground-truth calculation process and ask GPT-4o to fill in problem-specific information according to an LLM's response, while for NBA problems we enumerate a list of all rules and ask GPT-4o to directly judge whether a specific rule is applied in an LLM's response. For details we refer our"}, {"title": "4.2 Main Results", "content": "This section provides a comprehensive analysis of benchmark results. The analysis is divided into two parts: problem-wise analysis and rule-wise analysis. The problem-wise analysis evaluates the performance of LLMs across different problems and difficulty levels; the rule-wise analysis delves into how effectively LLMs identify and apply specific rules, highlighting common failure modes and the impact of rule complexity and similarity."}, {"title": "4.2.1 Problem-wise Analysis", "content": "Table 2 presents the evaluation results7. Notice that the values of precision (P)(t), rule application (AC(t)), and recall (R(t)) are much higher than accuracy (Acc(t)). This is because solving a problem requires using multiple rules, hence one correct rule recall or application is insufficient for a correct answer. For example, if a problem requires 10 rules and only one rule is missed, R(t) is high as 0.9 while very probably leading to mistaken final answer (Acc(t) = 0).\nLow Accuracy. Overall performance in problem result accuracy (Acc(t)), as summarized in Table 2, remains unsatisfactory across all three scenarios. Under the 0-shot setting, even advanced models such as Llama 405B, Claude-3.5, and GPT-40 fail to produce correct answers for the simplest test problems. For more challenging problems, particularly in the airline and tax domains, Acc(t) rarely exceeds 10%. In 1-shot setting, we notice marked improvements on the easiest problems, yet the gains diminish as problem difficulty increases."}, {"title": "4.2.2 Rule-wise Analysis", "content": "Here, we provide a detailed examination of the rule-level evaluation. Figure 2 presents recall (R(r)) and application correctness (AC(r)) in airline domain and metrics for NBA transaction and tax domains are presented in Figure 7 and Figure 8 in Appendix. Table 3 summarizes the metric results by reporting the mean and variance of three key metrics across all rules: recall (R(r)), application correctness (AC(r)), and precision (P(r)). The low variance observed in metrics such as P(r) within the airline and tax domains suggests that certain performance aspects are largely independent of the specific rules being applied. In contrast, the high variance seen in metrics like R(r) implies that recall performance is significantly influenced by the particular rules in question. Detailed analysis is presented below."}, {"title": "5 What Impacts Rule Following?", "content": "In this section, we investigate the factors influencing LLM performance, as measured by Acc(t). We begin by examining the correlation between Acc(t) and other key metrics, including P(t), AC(t), and R(t). We then consider the effects of in-context examples, different rule representations, and the presence of distractors."}, {"title": "5.1 Correlation Between Accuracy and Other Metrics", "content": "To understand which factors most directly affect Acc(t), we visualize its correlation with other metrics in Figure 3 across all three domains on datapoints from all difficulty levels. From Figure 3a and Figure 3b, we observe an almost linear relationship between R(t) and Acc(t). Notice that in the tax domain, a recall lower than 0.95 immediately results in zero accuracy.\nIn contrast, the correlation between AC(t) and Acc(t) is highly non-linear, as seen in Figure 3a and Figure 3c. In many cases, a single computational error in rule application (thus reducing AC(t)) is sufficient to produce an incorrect final answer, indicating that only near-perfect AC(t) leads to significant Acc(t) improvements. For the NBA domain, we also compare P(t) and Acc(t); since P(t) is always 100% for the airline and tax domains, these correlations are not meaningful there. We find no clear relationship between P(t) and Acc(t) for the NBA problems."}, {"title": "5.2 Do In-Context Examples Help?", "content": "Table 2 presents the results with or without a level-1 1-shot example. LLMs generally provide better performances given 1-shot example on airline, tax, and (easy) NBA problems. Many studies have shown the benefit of in-context learning (Dong et al., 2022; Wei et al., 2023; Zhang et al., 2023), which conforms with our observation that Acc(t) gets higher in the 1-shot setting. This performance boost comes from both the enhancement of AC(t) as well as a better understanding of the reasoning process, indicated by higher R(t).\nHowever, when tackling more challenging NBA problems (Levels 2 and 3), providing an example increases P(t) and R(t) but leads to a counterintuitive decrease in overall Acc(t). This improvement in precision and recall primarily arises from the non-essential rules included in the in-context example, such as the \"Over 38 rule\u201d and \u201cSalary consumption of veteran free agent\". We compute the R(r) and P(r) for these two rules as in Table 7."}, {"title": "5.3 Does Rule Representation Matter?", "content": "In the airline and tax domains, some rules are represented as Markdown tables. To test whether representation format affects performance, we convert these tabular rules into textual \"if-then\" statements. Table 8 shows that converting tabular rules into text improves R(r), but has little impact on other metrics, including Acc(t)."}, {"title": "5.4 Do Distractive Rules Matter?", "content": "An essential aspect of rule-following involves identifying which rules are relevant to the current problem. In our experiments, all domain-specific rules are provided in the prompt, leaving it to the LLMs"}, {"title": "5.5 Intermediate Summary", "content": "In summary, various factors\u2014such as rule complexity, the presence of distractive information, and the difficulty gap between in-context examples and target problems\u2014can profoundly influence LLM performance. Even when LLMs succeed in simpler conditions, challenges like complex mathematical reasoning, large amounts of extraneous rules, and non-ideal in-context samples can severely limit"}, {"title": "6 Case Studies", "content": "To gain an intuitive understanding of how and why LLMs fail in complex rule-following problems, we present representative failure cases in Figure 5. These examples highlight three frequently observed failure modes:\nLLMs fail to recall certain rules. As discussed in Section 4.2, LLMs often neglect non-essential rules. In airline problems, for instance, a crucial requirement is to apply either the oversize fee or the overweight fee (whichever is higher) and not to sum them. However, LLMs frequently overlook this instruction and incorrectly combine both fees, resulting in an inflated, incorrect total cost.\nLLMs get confused by similar rules. When multiple rules appear similar but are applicable under different conditions, LLMs can misapply them. For example, in the NBA domain, teams under the Salary Cap should use the Mid-Level Exception for Room Teams, whereas teams above the Salary Cap should apply the Non-Taxpayer Mid-Level Exception. As illustrated in the second failure case of Figure 5, LLMs sometimes conflate these exceptions. Similar confusion also arises with various Traded Player Exceptions and differing types of Bird Rights.8\nLLMs compute incorrect results. Mathematical and logical operations present ongoing challenges. For example, in the tax scenario, LLMs must accurately compute a series of values related to income, tax brackets, and credits. Even a minor arithmetic mistake compromises the final result, as shown in"}, {"title": "7 Conclusions", "content": "In this paper, we introduce RULEARENA, a real-world benchmark designed to evaluate the abilities of LLMs on various rule-guided reasoning tasks. We observe that existing LLMs face significant challenges when they try to tackle problems on RULEARENA - even the strongest Claude-3.5 and GPT-40 models can hardly succeed on our hardest tasks. Our further analysis indicates that LLMs struggle to integrate multiple rules or facts cohesively and are prone to irrelevant distractions.\nRULEARENA's intricate design and emphasis on real-world rule-based reasoning represent an essential step in advancing LLMs' capabilities. By posing fundamental challenges in complex rule recall and multi-rule integration, this benchmark provides a valuable tool for understanding and enhancing the reasoning abilities of LLMs. We envision RULEARENA as a foundation for future research that strives to improve LLM performances in solving increasingly complex tasks."}, {"title": "8 Future Directions", "content": "While this study provides a comprehensive evaluation and analysis of LLMs' rule-guided reasoning capabilities, there remain numerous promising avenues for future research:\nAutomating Evaluation In this work, we rely on GPT-40 to parse textual responses into structured JSON, facilitating downstream analyses of rule application. A logical next step would be to investigate the use of LLMs for fully automated reasoning evaluations, including the identification of intermediate errors. This direction aligns with the concept of \"LLM-as-a-judge\u201d (Zheng et al., 2023), which, despite potential bias or inaccuracies (Xu et al., 2024), offers a scalable alternative to labor-intensive human evaluation and could improve the reliability and granularity of assessment metrics.\nTraining with Rule-Guided Reasoning Data Supervised fine-tuning has proven effective in enhancing LLMs' performance on tasks requiring substantial domain knowledge (Jeong, 2024; Fu et al., 2023; Wu et al., 2024). While we did not pursue fine-tuning in this study\u2014given the high cost of obtaining extensive rule-guided reasoning data and the limited generalizability to unseen domains\u2014it remains a worthwhile direction. Investigating whether training with datasets from related domains, such as mathematical or logical reasoning tasks or code generation problems, can bolster LLMs' rule-following performance is an open question worth exploring.\nImproving Rule Recall and Aggregation Our experiments reveal that LLMs frequently struggle with recalling and aggregating the correct rules, and that problem-wise recall strongly correlates with overall accuracy. Addressing these challenges may involve refining rule retrieval mechanisms or integrating structured reasoning frameworks. For instance, approaches that convert rules into structured data (Pan et al., 2023; Wang et al., 2024) have shown promise in reasoning tasks. Building on"}, {"title": "A Terminology Explanation in NBA", "content": "We briefly explain the NBA terminologies mentioned in this paper as follows:\nSalary Cap. The Salary Cap of NBA is a rule that limits how much money each team can spend on player salaries. It is designed to keep teams on a level playing field financially, so wealthier teams cannot just purchase all the best players. The league sets the cap based on its overall revenue.\n(Salary Cap) Exceptions. The NBA uses a \"soft\" Salary Cap, meaning teams can exceed the limit using certain Exceptions. Following are some commonly used Exceptions:\n\u2022 Mid-Level Exception (MLE) allows teams to sign free players even if they are above the salary cap. There are three types of MLEs, i.e. Non-Taxpayer MLE, Taxpayer MLE, and MLE for Room Teams, applicable to teams in different salary situations.\n\u2022 Traded Player Exceptions (TPE) is a tool that allows teams to make trades even if they are over the salary cap. When a team trades a player for less salary than it gives away (or for nothing), it creates a TPE, which is like a \"credit\" they can use later. If a team wants to acquire more salaries than it gives away in a trade, it can also use certain types of TPE to make such trade.\n\u2022 Veteran Free Agent Exception (Bird Rights) in the NBA allow teams to re-sign their own players even if they are over the salary cap. Named after Larry Bird, this rule encourages teams to keep their star players. There are three types of Bird Rights, i.e. Bird Rights, Early Bird Rights, and Non-Bird Rights, applicable to players that play for the same team for different numbers of consecutive seasons."}, {"title": "B Data Collection and Annotation", "content": "B.1 Rule Collection\nAirline. We collect the policy for bag and optional fees from American Airlines. Specifically, the rules in the policy mainly include: 1) the allowance of carry-on luggage; 2) the base price for checking each luggage on different routes and in different cabin classes; 3) the additional fees for luggage overweight or oversize to varying degrees on different routes and in different cabin classes; 4) when calculating fees for overweight and oversize luggage for each piece, only the higher of the two should apply. Many rules (base price, overweight/oversize fees) in this domain are represented in tabular forms, and we regard one entire table as one rule.\nNBA. We collect the regulations for NBA transactions from 2023 NBA Collective Bargaining Agreements (CBA) and excerpt from the NBA Constitution and By-Laws regarding the rules for trading first-round draft picks (i.e., the Stepien Rule). Since the complete CBA is too long (676 Pages PDF), we only aggregate the most commonly used rules such as the limits on salary and length of player contract, on team salary, and on player contract trade among teams. As applying rules of the same type but applicable under different conditions may result in completely different subsequent reasoning process, different from in airline domain, we depart such one paragraph including such similar rules into separate rules.\nTax. We collect tax forms and relevant instructions from Internal Revenue Service (IRS)12. Starting from the most famous Form 1040 (U.S. Individual Income Tax Return) and its basic Schedules 1-3, we consider more complex settings commonly happen in real-life, including using itemized deductions (Schedule A), self-employment (Schedule C and Schedule SE), education expenses and/or credits (Form 8863), and child and/or other dependent credits (Schedule 8812). We treat each line in these forms and its instructions as one rule, and convert the forms into line numbers and text for each line as LLM input."}, {"title": "B.2 NBA Data Annotation", "content": "For NBA tasks, we first survey famous rules and transactions that have happened in the NBA in recent 30 years and decide the 54 rules used in our annotation. To balance the task difficulty and annotation difficulty, we further simplify the rules by unifying different types of team salary (defined in different rules and calculated in different ways) into one simple \"Team Salary\". The process of annotating one problem is described as follows:\nCreating team and player situations. Our annotators are first required to create diverse valid scenarios involving one or more teams and players, as the following \u201cteam_situations\u201d and \u201cplayers_situations\u201d, and provide the number of teams (\u201cn_teams\") and players (\"n_players\") involved. Each item in the \u201cteam_situations\u201d list indicates the current salary of the team and its available first-round draft picks, while each item in the \u201cplayer_situations\u201d list tells the player's information including his draft year, age, and current (or last) contract.\nWritting transactions. Next, our annotators write \"n_operations\u201d sentences in the \"operations\u201d list, where each item corresponding to one team signing a player or several teams conducting a trade, and determine whether all these transactions are allowed according to the rules. The \u201canswer\" should be True if all transactions are allowed otherwise False. If \"answer\" is False, we ask our annotators to further provide \"illegal_team\" and \"illegal_operation\" as the specific team and transaction component that violates the rules.\nListing relevant rules. Finally, our annotators are told to provide a list of \u201crelevant_rules\" including all rules that they believe should be involved if humans need to consider the case comprehensively."}, {"title": "C Structured Rule Extraction in Each Scenario", "content": "As introduced in Section 4.1, we utilize the structured output mode of GPT-40 (OpenAI, 2024) to convert LLMs' textual output into structured data. Here we present the data structure we used in parsing.\nAirline. In airline domain we ask LLMs to parse the the list of checked luggage as well as provided basic information."}, {"title": "D More Experiment Results and Analysis", "content": "D.1 Rule-Wise Statistics\nWe visualize the rule-wise recall, precision, and correctness in Figure 6-8. Since precision is always 1.0 in airline and tax domains, we skip these two charts."}, {"title": "E LLM Prompts", "content": "Airline. The prompt template we use in airline domain is as follows.\nSystem Prompt: You are a helpful assistant at American Airlines.\nUser Prompt: You are given the information of a passenger, his / her items, his / her special needs, and the policies of American Airlines. You should compute the total cost (including the flight ticket fee, checked bag fees, cost of special needs) according to the policies for the passenger. The policies of American Airlines are as follows:\n<reference_rules >\n<user_query> Compute the total cost for him step by step (don't omit any bag) and end your response with \"The total cost is $xxx.\" (xxx is a number)\nYour response:\nNBA. The prompt template we use in NBA domain is as follows.\nSystem Prompt: You are a helpful NBA team consultant.\nUser Prompt: You are given rules in NBA Collective Bargaining Agreement and the information about some teams and players. Then you will be given a list of operations, each of which desribes how some teams conduct some transaction. You should determine whether each operation complies with the given rules.\nAssume:\n* the Salary Cap for the prior (2023-24) Salary Cap Year is $136,000,000;\n* the Average Player Salary for the prior (2023-24) Salary Cap Year is $9,700,000;\n* the Salary Cap for the current (2024-25) NBA Salary Cap Year is $140,588,000;\n* the Luxury Tax is $170,814,000;\n* the First Apron Level is $178,132,000;\n* the Second Apron Level is $188,931,000;\n* the Team Salary of each team listed under \"Team Situations:\" do not include the amount of contracts that expire at the end of 2023-2024 Salary Cap Year.\nReference Rules in NBA Collective Bargaining Agreement:\nDecide whether any operation by any team violate the rules:\nAnalyze the described operations and explicitly state the type of Salary Cap Exceptions if you think the exception should be involved. Conclude your response with:\n* \"Answer: False.\" if there is no violation to the rules;\n* \"Answer: True. Illegal Operation: X. Problematic Team: Y.\" if Team Y in Operation X violates the rules. Both X and Y should be a single capital letter as A/B/C/...\nYour response:\nTax. The prompt template we use in tax domain is as follows, where \u201c\" includes both form instructions and user query information.\nSystem Prompt: You are a helpful US taxation consultant.\nUser Prompt: You are given several forms used to report US income tax and the instructions or rules about how to fill the forms. Then you will be given the income and/or payment information about a tax payer According to the given information. You should calculate the income tax owed by this payer.\nIRS Forms for the tax payer:\nCalculate the tax owed by the payer step-by-step according to the information provided by the forms. You should calculate all fields marked with [__]. DO NOT round numbers without explicit instructions. End your response with:\n1. \"The total tax owed is $xxx.\" (xxx is a number) if there is tax owed.\n2. \"The total tax overpaid is $xxx.\" (xxx is a number) if there is tax overpaid (and should be refunded). Your response:"}]}