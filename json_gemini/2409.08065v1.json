{"title": "AI-accelerated discovery of high critical temperature superconductors", "authors": ["Xiao-Qi Han", "Zhenfeng Ouyang", "Peng-Jie Guo", "Hao Sun", "Ze-Feng Gao", "Zhong-Yi Lu"], "abstract": "The discovery of new superconducting materials, particularly those exhibiting high critical temperature (Tc), has been a vibrant area of study within the field of condensed matter physics. Conventional approaches primarily rely on physical intuition to search for potential superconductors within the existing databases. However, the known materials only scratch the surface of the extensive array of possibilities within the realm of materials. Here, we develop an AI search engine that integrates deep model pre-training and fine-tuning techniques, diffusion models, and physics-based approaches (e.g., first-principles electronic structure calculation) for discovery of high-Tc superconductors. Utilizing this AI search engine, we have obtained 74 dynamically stable materials with critical temperatures predicted by the AI model to be Tc \u2265 15 K based on a very small set of samples. Notably, these materials are not contained in any existing dataset. Furthermore, we analyze trends in our dataset and individual materials including B4CN3 and B5CN2 whose Tcs are 24.08 K and 15.93 K, respectively. We demonstrate that AI technique can discover a set of new high-Tc superconductors, outline its potential for accelerating discovery of the materials with targeted properties.", "sections": [{"title": "Introduction", "content": "Superconducting materials have numerous applications in modern society since it was discovered [1], particularly in magnetic resonance imaging [2], fueling advances in nuclear fusion technology [3]. Superconductor-based devices are demonstrating potential for achieving scalable quantum information processors, advanced sensors, and efficient communication systems [4\u20136]. Many of these devices use conventional Bardeen-Cooper-Schrieffer (BCS) superconductors [7], which demand costly helium-based cooling. Hence, searching superconductors with high superconducting critical temperature (Tc) is vital for propelling technological progress in these dynamic areas.\nOver the past decade, substantial advancements have been achieved in searching high-Tc superconductors. For example, a superconducting transition with T ~ 36 K was experimentally observed in high-pressured Scandium, which is the highest record for elemental superconductors [8]. The discovery of superconductivity in bilayer La3Ni2O7 under pressure raises superconducting Tc of nickelates to the liquid-nitrogen temperature zone [9]. And lots of theoretical work predicted superconductivity in hydrides [10\u201315], where superconductivity in H3S under pressure was experimentally confirmed [16].\nRecently, machine learning-based methods have become increasingly popular in searching potential high-Tc superconductors [17\u201322]. Wines et al [23]. have employed crystal diffusion variational auto-encoder (CD-VAE) [24] to generate data based on the JARVIS-DFT database [25], subsequently employing the atomistic line graph neural network (ALIGNN) [26] for Tc forecasting. Using high-throughput density functional theory (DFT) calculations, 34 dynamically stable 2D superconductors with Tc\u2265 5 K from over 1000 candidates in the JARVIS-DFT database were identified [18]. Moreover, Choudhary and Garrity [17] leveraged electron-phonon coupling (EPC) calculations, assistanted by deep-learning models for efficient prediction of superconducting properties, to identify 105 conventional superconductors with Tc \u2265 5 K from a pre-screened set of 1736 materials. While numerous studies have highlighted the application of machine learning in this field, these approaches primarily rely on chemical formulas or searches based on the existing datasets. They often lack the intricate atomic structure details crucial for understanding superconducting behavior and are limited in exploring crystal materials beyond known databases. To truly advance the discovery of new superconductors, it is essential to incorporate detailed structural information and broaden the scope beyond existing data. So far, the conventional methods (such as elemental substitution or physical insight) have limited success in finding new high-Tc superconductors among the existing data. The rise of AI technology brings a transformative approach, potentially reshaping our path to solving this challenge.\nIn this work, we developed an AI search engine to explore high-Tc BCS superconductors, integrating diffusion model, formation energy prediction model, ALIGNN, pre-training and fine-tuning technique, atom docking based on pre-trained model, active learning technique, and physics-based methods (e.g., first-principles electronic structure calculations), and meanwhile sufficiently incorporating detailed structural information. Leveraging a limited dataset of high-Tc BCS superconductors"}, {"title": "Overview", "content": "Our AI search engine utilizes multiple AI methods and DFT calculations (Fig 1(a)) for generating and screening high-Tc superconductors. Specifically, inspired by the DiffCSP model [27], we constructed a symmetry-constrained superconducting crystal generation model, based on diffusion generative models [28] and equivalent graph neural networks [29]. This model generates new superconducting structures. We also built a superconducting classification model using pre-training techniques [30], graph auto-encoder architectures [31], and optimal transport theory [32] to determine whether or not the generated crystals exhibit superconducting properties. To further assess the stability of the materials, we retrained a formation energy prediction model based on the MEGNET architecture [33] with improvements. Next, we used ALIGNN [26] to predict the superconducting transition temperatures of these materials and screened for high-Tc superconducting candidates. Finally, for the top-5 candidates, We validated our predictions with convergence tests and verified superconducting transition temperatures using DFT calculation. Adopting active learning, we incorporated the discovered superconductors into training set."}, {"title": "Symmetry-constrained crystal generation model", "content": "In crystal structures (Fig 1(b)), the atoms exhibit a periodic distribution, with the smallest repeating unit being the unit cell, denoted by M, which can be represented as M = (A, X, L). Here, A = [a_1, a_2, ..., a_N] \u2208 R^{h\u00d7N} denotes the atomic types within the unit cell, X = [x_1,x_2,...,x_N] \u2208 R^{3\u00d7N} represents the Cartesian coordinates of each atom, and L = [l_1,l_2,l_3] \u2208 R^{3\u00d73} is the lattice matrix used to describe the periodicity of the crystal. We employed an ab initio crystal generation approach to generate superconducting crystal structures. Specifically, this involves generating a superconducting crystal M from a given number of atoms N within the unit cell, with a sampling distribution defined as:\np(M, N) = p(N)p(M|N),\nwhere N remains unchanged during the generation process. The distribution p(N) is calculated from the training set, while p(M|N) is generated based on the model. Standard denoising diffusion probabilistic model (DDPM) [28] can be used to generate L and A, and their loss functions take the same form as:\nL_{L/A} = E_{\u20ac~N(0,I)} [||\u20ac \u2013 \u00ca_{L/A}(M_t, t)||^2].\nThe denoising terms \u00ca_L (M_t,t) and \u00ea_A(M_t,t) are predicted by an equivalent denoising graph neural networks (short as EDGNN Fig 1(b)), and N(0, I) represents a standard normal distribution. Given the periodicity of X, it is generated using a score-matching based framework [34]. Details are in the supplementary material (SM). Utilizing 105 BCS superconductors [17], we trained the model to generate novel crystal structures, excluding those in the training set and with overlapping compositions in the Materials Project (MP) database [35]. Since generative models often produce non-ground-state structures, we performed geometry optimization using the neural networks atomic simulation environment [36] and L-BFGS algorithm [37] to refine the generated structures."}, {"title": "Superconducting classification model", "content": "Initially, we extract 144,595 crystal data entries from the MP database [35]. We first classified the materials into two groups: magnetic and non-magnetic. Subsequently, we refined the non-magnetic category into conductors, semiconductors, and insulators. Then, we designated insulators and magnetic materials as negative samples, and conductors and semiconductors as positive samples, as illustrated in Fig 2a. The model is based on a pre-trained graph neural network (GNN) that utilizes material crystal structure information to predict materials [31, 38], consists of a graph convolutional network encoder and a decoder that reconstructs the graph features based on optimal transport theory (see Fig 2b and SM). To obtain hidden layer representations related to superconductivity, we pre-trained the model using the positive samples. During the fine-tuning stage, we employed the pre-trained encoder and used up-sampling techniques to balance the number of the BCS superconductors and negative samples for binary classification model. Subsequently, we obtained the classifier model that achieved a discrimination success rate of 99.04% for the 105 BCS superconductors. Utilizing this model, we evaluated the candidate structures generated by the generative model."}, {"title": "Formation energy prediction model", "content": "To further assess the stability of potential superconductors, we predict the formation energy of crystals as an indicator of their stability. The AI algorithms like CGCNN [31] and SchNet [39], while fast, lack the required precision for formation energy predictions. Inspired by MEGNET [33], we trained the model using 380,000 crystal structures from GNOME [40] and 60,000 crystal structures [35]. Next, we increased the cutoff radius for constructing atomic graphs from 5\u00c5 to 8\u00c5, enabling the model to capture more long-range atomic interactions to more accurately simulate atomic interactions. Recognizing the direct correlation between crystal formation energy and atomic bonding strength, we have incorporated eight new atomic features into our prediction model. This enhancement offers a more comprehensive representation of crystal data, as elaborated in the SM. The original MEG-NET benchmark reported a mean absolute error (MAE) of 28 meV per atom, while our improved model achieved an MAE of 21 meV per atom. Since we are particularly interested in high-Tc superconducting materials, we used ALIGNN [26] to predict the superconducting transition temperatures of these materials and applied a 15 K threshold, resulting in top-5 candidate high-Tc superconductors."}, {"title": "Predicted high-Te materials", "content": "By performing the DFT calculations, we studied the electronic structures, phonon properties, and EPC of B5CN2 and B4CN3 (See SM for crystal structures and additional results.). In Fig. 3, we show the band structure of B5CN2 and B4CN3 (5 GPa). The results of DFT calculations and Wannier projection show good consistence and suggest that B5CN2 and B4CN3 (5 GPa) are metallic. The atomic-orbital resolved density of states (DOS) shows that the 2p orbitals of B, C, and N atoms mainly contribute the Fermi surfaces.\nNext, we investigate the dynamical stability of B5CN2 and B4CN3. At ambient pressure, we find that B5CN2 is dynamically stable, while B4CN3 shows a maximum imaginary-frequency phonon of ~ -7.7 meV along the R-Z path. By applying pressure of 5 GPa, the imaginary phonons of B4CN3 disappear. Hence, we show the phonon spectrum of B5CN2 and B4CN3 (5 GPa) in Fig. 4(a) and (c) and further study the EPC of these two materials. The calculated Eliashberg spectral function a2F(w) and accumulated EPC constant \u03bb(w) are exhibited in Fig. 4(b) and (d). And the mode-resolved \u03bbqv is added in the phonon spectrum. The EPC constants \u03bb of B5CN2 and B4CN3 (5 GPa) are integrated to be 0.61 and 0.72, respectively. Using the McMillan-Allen-Dynes formula [41, 42]\nT_c = \\frac{w_{log}}{1.2} exp[-\\frac{1.04(1 + \\lambda)}{(\\lambda(1 - 0.62\\mu^*) - \\mu^*)}],\nthe superconducting Tc of B5CN2 and B4CN3 (5 GPa) is"}, {"title": "Discussion", "content": "Recently, several studies have utilized generative models to explore high-Tc superconductors [17, 18, 20, 23]. Wines et al. [23] employed CDVAE to generate data on the JARVES-DFT dataset [25], subsequently employing the ALIGNN [26] for Tc forecasting. Compared to existing methods, our proposed AI search engine has seen improvements in three aspects. Firstly, our method is capable of effective generation based on a few positive samples (i.e., 105 samples with Tc \u22655 K). Unlike CDVAE [24], which randomly generates chemical formulas before predicting structures, our approach directly generates structures. By directly generating structural configurations, our method adeptly navigates the spatial intricacies of superconductors, facilitating the genesis of plausible chemical entities. Secondly, we have integrated a sophisticated post-processing phase employing the DPA-2 model [43] for atom docking. This step meticulously circumvents atomic clashes, refines bond lengths to more rational values, and guarantees the equilibrium of forces exerted on each atomic constituent. Existing methods often predict superconducting transition temperatures without aforehand confirming the materials' superconductivity, which is problematic. We address this by introducing a superconducting classification model. We improved the formation energy prediction model under GNOME [40], increasing its precision from 28 meV to 21 meV. Lastly, active learning progressively expands the chemical space of high-Tc superconducting materials in iterative reinforcement generative learning. These refinements enhance our method's effectiveness and establish a new standard for exploring and predicting high-Tc superconductors.\nIn conclusion, our proposed AI search engine integrates a suite of advanced methodologies, including generative model, formation energy prediction model, pre-training and fine-tuning strategy, ALIGNN, and first-principles electronic structure calculations. This AI search engine has not only predicted 74 superconducting material candidates with T >15 K based on a modest set of positive samples (105 samples with T \u22655 K), but also identified two ideal high-Te candidates: B5CN2 (Tc=15.93 K) and B4CN3 (Tc=24.08 K). Notably, this engine is capable of discovering crystal structures that are not yet documented in existing material dataset, thereby opening up new horizons in the search for high-Tc superconductors. Moreover, the AI search engine's versatility allows it to be adapted for exploring a wide range of functional materials with various target properties, significantly expanding its potential applications in materials science."}, {"title": "CANDIDATE SUPERCONDUCTORS", "content": "The table I- II present the candidate superconducting formulas and their corresponding predicted superconducting transition stability values, obtained from our A\u0399 search engine."}, {"title": "SYMMETRY-CONSTRAINED CRYSTAL GENERATION MODEL DETAIL", "content": "Any atom in the crystal can be expressed by its Cartesian coordinates and type as {(a,x) a = ai, Xi = xi + Lk,\u2200k \u2208 Z^{3\u00d71}}. There is a relationship between Cartesian coordinates and fractional coordinates given by x = -1 fili. The following generation process employs the fractional coordinate system M = (A, F, L). Given the periodicity of F, it is generated using a score-matching (SM) based framework [34]. and its loss function is:\nL_F = E_{F_E~q_t} [\\frac{1}{t}||\u2207_{F_t} log q(F_t|F_o) \u2013 \u00ea_F(M_t,t)||^2].\nSimilar with DiffCSP [27], the distribution qt here uses the wrapped normal(WN) [46] distribution. where At is approximated via Monte-Carlo sampling. In this work, we employed EGNN [29] as an equivariant denoising model. The superconducting unit cell structure is represented as an atomic graph, where the encoding of atomic types and the diffusion time step t is processed through an MLP, serving as the model's input:\nh_i^{(0)} = MLP(f_{atom} (a_i), f_{pos}(t)). The representation of the i-th node at the s-th layer is denoted as h_i^{(s)}.\nh_i^{(s)} = h_i^{(s-1)} + O_h(\\frac{1}{N} \\sum_{j=1}^N( h_i^{(s-1)}, m_{ij}^{(s)})).\nOh denotes MLP, and m_{ij}^{(s)} represents the aggregated representation from all nodes to the i-th node.\nm_{ij}^{(s)} =  \\sum_{V_j EN_i, U_m EM_i,k } m_{ij}^{(s)}."}, {"title": "Algorithms for Training and Sampling", "content": "Algorithm 1 provides an overview of the forward diffusion process and the training procedure for the denoising model \u03c6. Meanwhile, Algorithm 2 details the backward sampling process. These algorithms are designed to preserve symmetries, provided that \u03c6 is carefully constructed. Specifically, the predictor-corrector sampler [50] is employed to sample Fo. In Algorithm 2, Line 8 represents the predictor step, while Lines 10-11 correspond to the corrector steps."}, {"title": "Hyper-parameters and training details", "content": "In this section, we detail the training process of the superconducting generative model. First, we perform an up-sampling operation on 105 BCS superconducting crystals, replicating the training dataset 100 times to create the superconductors dataset. We utilize a model architecture with 6 layers and 512 hidden units. The dimension of the Fourier embedding is set to k = 256. A cosine scheduler with s = 0.008 is applied to control the variance in the DDPM process on Lt, and an exponential scheduler with \u03c3\u2081 = 0.005 and \u03c3T = 0.5 is used to manage the noise scale in the score-matching process on Ft. The diffusion step is set to T = 1000. Our model is trained for 1000 epochs. For ab initio generation, we apply a scaling factor of \u03b3 = 5\u00d710\u207b\u2076. The model training was conducted on a GeForce RTX 3090 GPU."}, {"title": "SUPERCONDUCTING CLASSIFICATION MODEL", "content": "We developed a superconductor classification model inspired by MatAltMag [38]. We define the graph representation G(V, U, X) to encapsulate crystal structure information, where V represents the set of nodes, U the set of edges, and X the set of features. In this representation, atoms in the crystal structure are depicted as nodes vi, where i = 1, ..., |V|. Due to the consideration of periodic boundary conditions, equivalent nodes are merged, resulting in a set of irreducible nodes. For each node vi, we identify its neighboring nodes vj, where j = 1,..., |Ni|, and Ni denotes the set of neighbors of node vi. The connections between nodes vi and vj are represented by the"}, {"title": "Model architecture", "content": "edges u_{i,j)k, where k denotes the number of bonds between the nodes. The initial features of each node vi are denoted by {h_i^{(0)}}_{i=1}^{|V|} and are derived through one-hot encoding based on the atomic sequence in the crystal structure. The neighbor node features for node vi are represented as H_{Ni}. Each edge u_{(i,j)k} \u2208 U is associated with a feature vector u_{(i,j)k}, corresponding to the k-th bond between nodes vi and vj. Finally, each node vi \u2208 V is represented by a feature vector hi \u2208 X, which encodes the attributes of the atom corresponding to that node.\nThe encoder maps the input crystal structure information to a high-dimensional matrix, using n convolutional layers. In each layer t, the node feature vector h_i^{(t)} is updated through the convolution function h_i^{(t+1)} = Conv (h_i^{(t)}, H_{Ni}^{(t)}, u_{i,j}). The graph convolution function g iteratively refines the feature vector hi, feeding the output from one step as input to the next. The structure and length of hi remain consistent across steps. At each step t, the concatenated neighbor vector is defined as z_{(i,j)k}^{(t)} = h_i^{(t)} \u2295 h_j^{(t)} \u2295 u_{(i,j)}. The feature update is then performed via the convolution operation:\nh_i^{(t+1)} = h_i^{(t)} + \\sum_{V_j EN_i, U_m EM_i,k } \u00a9 g (z_{(i,j)kW_f^m}^{(t)}) o (W_S^{(t)}h_i + W_V^{(t)}H_{N_i}+b^{(t)}),\nwhere \u00a9 represents element-wise multiplication, and o is the sigmoid activation function. The term W_S^{(t)} accounts for the importance of magnetic atoms, crucial for superconducting properties. Residual connections via h_i^{(t)} are included to facilitate neural network training.\nThe decoder, denoted by \u03c6, reconstructs the input graph representation of a crystal from its embeddings. It consists of two main components: node feature reconstruction (\u03c6s) and adjacent node feature reconstruction (\u03c6p), with \u03c6 defined as \u03c8 = (\u03c6p + \u03c6s). Node features are reconstructed using \u03c6s = MLP(h_i^{(n)}), where MLP stands for a multilayer perceptron. The decoder block architecture follows the design in [49]. We utilize an n-hop neighboring Wasserstein decoder for graph feature reconstruction. First, we obtain the initial node features {h_i^{(0)}, H_{Ni}^{(0)}}. For each node vi \u2208 V, the GNN layer in the encoder updates the node representation h_i^{(t+1)} by aggregating information from h_i^{(t)} and its neighbors H_{Ni}^{(t)}, following the rule h_i^{(t+1)} = \u03a6^{(t)} (h_i^{(t)}, H_{Ni}^{(t)}). Here, H_{Ni}^{(t)} is assembled based on node adjacency. The network is trained by:\narg min_{\\Phi,\\psi} L(\\psi(h_i^{(0)}, H_{Ni}^{(0)})  ), H_{Ni}^{(0)}) ,(h_i^{(t+1)}, H_{Ni}^{(t+1)}))),"}, {"title": "Hyper-parameters and Training Details", "content": "Below are the hyperparameters used for the pre-training and fine-tuning of the superconductor classification model."}, {"title": "FORMATION ENERGY PREDICTION MODEL", "content": "Let V, E, and u represent the node, edge, and global state attributes of atoms in a crystal, respectively. Vi denotes the attribute vector of the i-th atom, and V is the set of all vi. The edges between atoms in the crystal are defined based on the geometric distance between atoms being less than a specified threshold (8\u00c5), with eij representing the edge attribute vector between atom i and atom j, and E being the set of all edges eij. The process of updating the graph from the input graph G = (E,V', u') to the output graph Gl+1 ="}, {"title": "Model architecture", "content": "contains a series of state update operations. The update for the edge state is given by:\ne_{ij}^{(l+1)} = \u03c6_e(v_i^{(l)} \u2295 v_j^{(l)} \u2295 e_{ij}^{(l)} \u2295 u).\nHere, \u03c6e represents a multi-layer perception (MLP), and \u2295 denotes the concatenation operation. The update for the node state is given by:\nv_i^{(l+1)} = \\frac{1}{N_e} \\sum_{j} e_{ij}^{(l+1)}.\nv_i^{(l+1)} = \u03c6_v(v_i^{(l)} \u2295 u).\nNe represents the total number of atoms bonded to atom i, and \u03c6v denotes a MLP. The update for the global state is given by:\nu_v^{(l+1)} = \\frac{1}{N_e} \\sum_{i,j} e_{ij}^{(l+1)}.\nu^{(l+1)} = \\frac{1}{N_u} \\sum_{i} v_i^{(l+1)}.\nu^{(l+1)} = \u03c6_u(u^{(l)} \u2295 u_v \u2295 u).\nNe represents the total number of edges, Nu represents the total number of atoms in the crystal, and \u03c6u denotes a MLP. Notably, we incorporated eight new atomic features into the model: electronegativity, group number, covalent radius, valence electrons, first ionization energy, electron affinity, atomic orbital, and atomic volume.\nThese features are considered as follows: Electronegativity and electron affinity impact atomic interactions and bonding strength, thereby affecting formation energy, with higher electronegativity typically leading to stronger bonds. Valence electrons and first ionization energy influence how easily an atom loses or gains electrons, directly impacting bond strength and formation energy. Covalent radius and atomic volume affect bond lengths between atoms, influencing stability and formation energy. Group number indicates an element's reactivity and bonding modes, indirectly affecting compound formation energy. Atomic orbitals relate to formation energy indirectly by affecting chemical bond properties. All these atomic features play a role in directly or indirectly influencing crystal formation energy."}, {"title": "Absolute error", "content": "Here are the formation energy prediction errors of some common methods, our model demonstrates strong competitiveness."}, {"title": "Hyper-parameters and training details", "content": "We detail the hyperparameter settings employed for model training. The training dataset size was set to 440,000 records, while the test dataset size was set to 4,500 records. The batch size was configured to 256. The number of training epochs was 1,000. The bond feature dimension was established at 100. The cutoff radius was defined as 8\u00c5. The Gaussian centers were evenly distributed from 0 to 8(\u00c5)+1, totaling 100 centers. The Gaussian width was set to 0.5."}, {"title": "ACTIVE LEARNING", "content": "Using our proposed AI search engine, 74 stable superconducting candidates with Tc \u2265 15 K were generated through three iterations of active learning. This approach progressively expanded the chemical space of high-Tc superconducting materials. Additionally, we observed that active learning improved the success rate of the DPA-2 model [43] from 15% to 53%."}]}