{"title": "Magnitude and Rotation Invariant Detection of Transportation Modes with Missing Data Modalities", "authors": ["Jeroen Van Der Donckt", "Jonas Van Der Donckt", "Sofie Van Hoecke"], "abstract": "This work presents the solution of the Signal Sleuths team for the 2024 SHL recognition challenge. The challenge involves detecting transportation modes using shuffled, non-overlapping 5-second windows of phone movement data, with exactly one of the three available modalities (accelerometer, gyroscope, magnetometer) randomly missing. Data analysis indicated a significant distribution shift between train and validation data, necessitating a magnitude and rotation-invariant approach. We utilize traditional machine learning, focusing on robust processing, feature extraction, and rotation-invariant aggregation. An ablation study showed that relying solely on the frequently used signal magnitude vector results in the poorest performance. Conversely, our proposed rotation-invariant aggregation demonstrated substantial improvement over using rotation-aware features, while also reducing the feature vector length. Moreover, z-normalization proved crucial for creating robust spectral features.", "sections": [{"title": "1 INTRODUCTION", "content": "Accurate and near-real-time detection of human transportation modes is crucial for applications such as automatic activity recognition on smartphones [8] or smartwatches [4], route recommendations, and context-aware service adaptations (e.g. service switching to car mode) [7]. Moreover, through ubiquitous devices such as"}, {"title": "2 SHL 2024 CHALLENGE DATASET", "content": "The 2024 SHL challenge, now in its sixth iteration, aims to detect transportation modes using multimodal sensor data from a single smartphone. Specifically, non-overlapping 5-second windows of the smartphone's accelerometer (Acc), gyroscope (Gyr), and magnetometer (Mag) data sampled at 100Hz are provided, requiring the prediction of the current locomotion mode at a sample-level (500 predictions per window). This year's challenge is particularly difficult due to four key aspects. First, the non-continuous (i.e., shuffled) test set makes post-processing predictions infeasible. This way, the organizers aim to evaluate the ability to detect locomotion modes in near-real time. Second, the test dataset does not provide the smartphone location (e.g., hands, hips, torso, or bag), necessitating a location-independent model. Third, as (exactly) one of the data modalities, i.e. Acc, Gyr, or Mag, is masked with zeros in both the validation and test datasets, the approach must be robust to missing data modalities. Fourth, the train dataset consists of data from a single user (subject 1), while the validation and test datasets are a mix of data from subjects 2 and 3, encouraging the development of user-independent models."}, {"title": "2.1 Exploratory Data Analysis", "content": "Figure 2 shows the distribution of transportation modes (i.e., the labels) and the number of unique modes per window in the train and validation datasets. Given that fewer than 0.4% of the 5s windows contain multiple labels, we adapted the objective to predicting a single activity label for each window.\nFigure 3 compares the distribution (standard deviation) of the signal magnitude vector (SMV) for each 5-second window across the different labels in the train and validation datasets. To ensure a fair comparison, the validation data was trimmed (per modality) to include only windows where the visualized modality was not missing. It is important to perform this comparison across different labels, as each locomotion mode can affect the feature distribution. From Figure 3, we observe a distinct distribution shift between the train and validation data, especially for the Acc and Gyr modalities. This shift is most noticeable for the walk, run, and bike locomotion modes. In contrast, the Mag data does not demonstrate such a shift. There may be several causes for this distribution shift, such as variability between recording devices (e.g., smartphone firmware [2]) or variability between subjects, as the train and validation datasets comprise different users. Consequently, solutions trained on solely the train data demonstrate limited generalizability. Particularly, Widhalm P. et al. [17] observed a significant drop in prediction accuracy between train and validation data. Addressing this distribution shift will therefore be a key aspect in this year's challenge.\nBy utilizing our plotly-resampler toolkit [10], we were capable of effectively analyzing all modalities of the train and validation data along with the labels, totaling \u00b14.5B data points, through interactive line chart visualization. This analysis provided insights into the relationship between the training/validation data and the labels. Note that this analysis could not be performed on the test set, as this data was shuffled, thereby removing the temporal aspect. Figure 4 presents an interesting excerpt from this analysis. The continuously labeled \"Bike\" data contains a segment of about 1 minute with nearly no movement (i.e., 17:11:30-17:12:30). This does not necessarily indicate mislabeled data as, for instance, the subject could be waiting at a stoplight. Evidently, postprocessing demonstrated substantial performance improvements (absolute 10% gain) on the SHL dataset [15]. However, relying solely on shuffled 5-second windows and being unable to use postprocessing - which demonstrated substantial performance improvements (absolute 10% gain) on the SHL dataset [15] \u2013 makes accurately classifying such small windows nearly impossible."}, {"title": "3 ALGORITHM PIPELINE", "content": "When devising our approach, we focused on two aspects: (i) rotation invariance due to the unknown phone position in the test set, and (ii) magnitude invariance to cope with the distribution shift observed between the train and validation set (see Figure 3). The resulting pipeline consists of 3 steps; (1) processing, (2) feature extraction, and (3) modeling."}, {"title": "3.1 Processing", "content": "During data loading, we scale each axis of the various modalities by fixed constants to convert them into more interpretable units. Specifically, accelerometer data is divided by 9.81 to convert m/s\u00b2 to g units, gyroscope data is divided by 27 to convert rad/s to Hz, and magnetometer data is divided by 100 to convert \u00b5T to Gauss. Figure 4 displays these scaled signals.\nNext, we aim to derive new signals (per modality) that are more robust to sensor placement and orientation. This involves computing the Signal Magnitude Vector (SMV) over the (transformed) x, y, and z axes for each modality $M\\in \\{Acc, Gyr, Mag\\}$ [15]. The SMV for each modality is computed using the formula:\n$SMV_{i,M}=\\sqrt{M_x^2+M_y^2+M_z^2}$\nAfter computing the SMV on the raw data, we apply several axis transformations: the first-order gradient, the second-order gradient, and the integral - after which the SMV is calculated on this transformed data.\nThe first and second-order gradients for each modality M and axis $a \\in \\{x, y, z\\}$ are computed using the numpy.gradient function:\n$dt^1_{t, M, a} = \\frac{M_{t+1,a}-M_{t-1,a}}{2}$\n$dt^2_{t, M, a} = \\frac{dt^1_{t+1, M, a}-dt^1_{t-1, M, a}}{2}$\nThe integral for each modality M and axis $a \\in \\{x, y, z\\}$ is computed via the scipy.cumtrapz function, based on the formula:\n$Integral_{t, M, a} = \\int_0^t M_{t,a} d\\tau$\nBy computing the SMV on both the raw data and the outputs of these three axis transformations, we derive 4 rotation-invariant signals per modality: SMV, SMV (dt\u00b9), SMV (dt\u00b2), and SMV (Integral).\nThese four signals, together with the raw (i.e., three-axial) signals, will be considered for feature extraction in the next step."}, {"title": "3.2 Feature Extraction", "content": "Table 1 provides an overview of the extracted features for each available signal. Our tsflex toolkit was used for convenient feature extraction [11]. Entropy, fractal, and Hjorth features were computed"}, {"title": "3.3 Model", "content": "We focused on traditional machine learning models, specifically investigating the performance of CatBoost, a gradient-boosted trees algorithm known for its strong performance without parameter tuning [5], making it suitable for an ablation study on feature subsets.\nWe limited the CatBoost model to 1,000 iterations (trees) and used \"Balanced\" as the auto_class_weight parameter. To cope with missing data, we created a new model for each missing modality configuration by excluding the missing modality from the (training) feature vector. Consequently, three different models were constructed, each using only two modalities."}, {"title": "3.4 Postprocessing", "content": "In contrast to most traditional approaches, we did not train on the entire train dataset to make final predictions on the test data. Instead, we leveraged K-fold cross-validation (CV), making predictions after fitting each fold, and then aggregated these predictions by utilizing majority voting (MV). We deliberately opted for a 3-fold CV, as using an odd number as K results in only 50% data overlap across the training folds. Using this approach, we effectively perform bootstrapping through CV on the training data."}, {"title": "3.5 Rotation Invariant Aggregation", "content": "To improve the robustness of the feature set computed on the raw axis signals (i.e., non-SMV-transformed), we proposed and examined three novel rotation-invariant statistical aggregation approaches. Specifically, the raw {x,y,z} signal features are condensed to summary statistics using: stat2: { mean, std }, stat3: { mean, std, skew }, or sort: { min, mid, max }. These approaches effectively discard axial information while retaining the overall feature information. Note that stat2 converts the three {x, y, z} features into two summary statistics (resulting in a 1/3 compression ratio), while stat3 and sort maintain the same input-output ratio.\nSensor orientation affects the sign of axial measurements. For instance, flipping a sensor along one axis produces opposite signs for the measurements along that axis. Consequently, rotation-invariant statistical aggregation is only relevant for sign-invariant features (e.g., kurtosis, mean crossing rate, and spectral domain features). As such, from Table 1, skewness is the only feature that is influenced by the sign, resulting in it being discarded when performing the rotation-invariant aggregation."}, {"title": "4 EXPERIMENTAL RESULTS AND DISCUSSION", "content": "Experiments were performed on a server computer (Arch Linux), with an AMD Ryzen 5 2600x CPU, 48GB of DDR4 RAM, and an Nvidia RTX 2070 GPU. Table 2 presents the out-of-fold (OOF) and validation (Val) macro F1 scores for the various investigated feature combinations. Each experiment (represented by a row in the table) involved training 12 models: 4 (3 folds + 1 full fit) x 3 (for the 3 missing modalities), taking a total of \u00b110 minutes to complete. Inference on the validation set took \u00b12 seconds, including 3-fold MV, with processing and feature extraction taking \u00b1 2 minutes.\nA first notable observation is that using only the SMV signal results in the worst validation performance. Yet, SMV has been commonly employed as the sole processing configuration in many studies [15, 17]. Second, our proposed rotation-invariant statistical aggregation approaches (i.e. rot_invstat2, rot_invstat3, and rot_invsort) all outperform the raw configuration (i.e., no feature aggregation) and the SMV) processing. These findings highlight the potential of statistical aggregation to effectively remove the axial (i.e., rotation-aware) information while preserving overall feature information. Third, we observe a consistent performance improvement when applying cross-fold-based majority voting (Valmv) compared to a model fully trained on the train set (Val).\nAnalyzing the performance of SMV computed on gradient and integral axis transformations reveals that incorporating the second-order gradient (dt2) yields the highest performance boost, followed by the first-order gradient (dt1), with the integral contributing the least. Interestingly, combining dt1 and dt2 provides a lower boost than combining either gradient transformation with the integral. This might be attributed to the high correlation between the gradient transformations, making their combination less informative."}, {"title": "4.1 Impact Distribution Shift", "content": "A consistent trend observed in Table 2 and in Figure 5 is that missing magnetometer data (i.e., MAG = 0) results in the poorest validation"}, {"title": "4.2 Final Model", "content": "For the final model, we selected the rot_invstat2 + SMV + SMVdt2 feature configuration, as the rot_invsort variant of this configuration resulted in the same Valmy score while having a larger feature vector (see Table 2)."}, {"title": "4.3 Improving the Test Score", "content": "To generate the final test predictions, we retrained our final models (3-fold CV) using several tricks to improve the test score over the above-reported validation score:\n\u2022 Exclude hand location: According to the challenge description, the test dataset comprises only torso, hips, and bag locations. Analysis of prediction errors per location indicated that hand location performed the worst, aligning with the observation of Gjoreski et al. [2]. Experimental results indicate an improvement in Valmv from 0.7255 to 0.7506.\n\u2022 Train on validation data: Since there were no restrictions on what data may be used for the training, including the validation during training should allow the model to better capture (any) remaining distribution shift and thereby learn the variability across multiple subjects."}, {"title": "5 CONCLUSION", "content": "This work presents the findings and final approach of the \"Signal Sleuths\" team for the 2024 Sussex-Huawei Locomotion-Transportation recognition challenge. During exploratory data analysis, we identified a substantial distribution shift between train and validation/test data, making this a key challenge in this year's edition. To address this, we utilized a traditional machine learning pipeline, enabling us to perform an ablation study on various magnitude and rotation-invariant feature configurations. In particular, we performed an ablation study on (i) including features from different processed signals as well as (ii) a novel approach in which we make rotation-aware signals rotation-invariant through statistical aggregation, and (iii) performing z-normalization prior to spectral feature computation. Results indicated that solely relying on SMV yields the poorest performance. Moreover, our proposed rotation-invariant statistical aggregation demonstrated a substantial improvement over using rotation-aware features or SMV alone, with the added benefit of reducing the feature vector length, underscoring the efficacy of this novel technique. Furthermore, z-normalization of the data proved to be beneficial when creating robust spectral features.\nOur final model consists of the best feature subset combination, retrained on both the training and validation data to better capture inter-user variability, while also excluding the hand data as this location is not present in the test data. The recognition result for the testing dataset will be presented in the summary paper of the challenge [14]."}]}