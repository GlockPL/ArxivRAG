{"title": "Augmenting the action space with conventions to improve multi-agent cooperation in Hanabi", "authors": ["F. Bredell", "H. A. Engelbrecht", "J. C. Schoeman"], "abstract": "The card game Hanabi is considered a strong medium for the testing and development of multi-agent reinforcement learning (MARL) algorithms, due to its cooperative nature, hidden information, limited communication and remarkable complexity. Previous research efforts have explored the capabilities of MARL algorithms within Hanabi, focusing largely on advanced architecture design and algorithmic manipulations to achieve state-of-the-art performance for a various number of cooperators. However, this often leads to complex solution strategies with high computational cost and requiring large amounts of training data. For humans to solve the Hanabi game effectively, they require the use of conventions, which often allows for a means to implicitly convey ideas or knowledge based on a predefined, and mutually agreed upon, set of \"rules\". Multi-agent problems containing partial observability, especially when limited communication is present, can benefit greatly from the use of implicit knowledge sharing. In this paper, we propose a novel approach to augmenting the action space using conventions, which act as special cooperative actions that span over multiple time steps and multiple agents, requiring agents to actively opt in for it to reach fruition. These conventions are based on existing human conventions, and result in a significant improvement on the performance of existing techniques for self-play and cross-play across a various number of cooperators within Hanabi.", "sections": [{"title": "1 Introduction", "content": "Reinforcement learning (RL) holds promising potential to address a large variety of problems where artificial agent operation offers a significant improvement over alter-native methods, such as hard-coded algorithms or solutions. For certain real-world problems, single-agent operation is not optimal (or even possible) and the incorporation of multiple agents would be beneficial (or necessary). Scenarios such as autonomous vehicles navigating terrain, guided drone swarms, mapping verbal instructions to executable actions, or the switching of railway lines benefit from the use of multiple agents. RL can help agents to effectively cooperate within these multi-agent scenarios by learning from past and or simulated experiences.\nUnfortunately, the introduction of multiple agents into an environment typically increases the complexity of the problem exponentially. It introduces a moving target learning problem, since all the agents must learn simultaneously. The reason for this is that each individual agent's policy changes over time, which in turn causes a non-stationary environment. This often inhibits all the agents from developing effective policies and can lead to undesired behaviour.\nOne of the most straightforward solutions to multi-agent reinforcement learn-ing (MARL) is the combination of deep Q-networks (DQNs) and independent Q-learning, where each agent independently and simultaneously learns its own action-value function (or Q-values) while interacting with the same environment. However, this strategy does not fair well when paired with partial observability. Hausknecht and Stone have shown that recurrent neural networks offer an improved solution to MARL problems containing partial observability. These networks incorpo-rate a built-in short term memory over which experiences are unrolled (or combined) to form longer sequences. This is often combined with deep Q-learning's feed forward neural network to produce deep recurrent Q-learning.\nVarious research efforts follow a similar path as Hausknecht and Stone, focusing on architectural advancements through the means of neural network layer manipulations, and complex algorithms to improve the estimation and assignment of action-values or policies. This often leads to convoluted solution strategies that are difficult to implement and computationally expensive. An alternative solution strategy would be to reconsider the problem dynamics and discover ways of incorporating existing domain knowledge into RL algorithms. Existing domain knowledge can be incorporated into RL and MARL using reward shaping, which focuses on manipulating the reward signal to encourage certain behaviour. Alternatively, agents can use state-space augmentations, such as auxiliary tasks, to imply or extract additional information (or features) of the problem setting. Sutton et al. have shown that options offer an additional strategy for incorporating domain knowledge into RL by changing the action space of an agent. Options allow an agent to solve problems on a higher level by extending the action space to include advanced temporal actions that range over multiple time steps (also referred to as macro-actions)."}, {"title": "1.1 Related Work", "content": "Games are often used as a medium for testing and evaluating RL algorithms, since they incorporate real world problems with well-defined rules and a clear metric for measuring performance, where examples include Go, Backgammon, and Dota 2. Hanabi is a cooperative card game containing partial observability and limited communication, requiring players to logically reason over the intentions and actions of their cooperators, a concept known as theory of mind. Human players require the development of conventions in order to solve the problem effectively. Conventions have evolved with humanity throughout the years and range from driving on a certain side of the road to social conventions or norms.\nMost research efforts on the Hanabi problem use complex architecture design with advanced algorithms, often focusing on estimating and calculating the beliefs of other agents. However, one avenue of the Hanabi problem yet to be explored is that of conventions, and how to incorporate existing human conventions into MARL solution strategies. In this paper, we propose a method to incorporate human conventions into MARL through the use of artificial conventions, which act as cooperative actions that span over multiple time steps and agents, and show how it significantly improves on the performance of MARL agents within Hanabi for self-play as well as cross-play scenarios. Our approach shares similarities with options, due to the multi-time step extension of actions, however it is fundamentally built on a different concept."}, {"title": "2 Multi-agent Reinforcement Learning and Options", "content": "Reinforcement learning aims at solving decision-making problems and is built on the concept of Markov decision processes (MDPs). Multi-agent reinforcement learning introduces more than one agent into the environment and often incorporates partial observability, and thus is built on the concept of decentralised partially observable Markov decision processes (Dec-POMDPs). The Dec-POMDP framework consists of\n(S, A, T, O, P, R, \u03b7, \u03b3),\nwhere S denotes the global state space and A represents the joint-action space of then agents at time t. The observation space O consists of the local observation of each agent i at time t within the global state S (O = O(S; i;t)). After A is sent to the environment, the global state transitions from S to S' given the state transition probability T, and similarly P represents the observation transition probability to transition from O to O'. This results in the environment producing R, which donates the immediate reward for each agent action that contributed to a global state change at t, with y denoting the discount factor for future reward. These rewards are either combined with equal weighting (summed), or can have their own discount factor Yr, and is referred to as the forward accumulated reward.\nIn value-based RL we can use (1) to calculate the value functions (Q) which acts as a quantitative measure for the desirability of a state. In tabular methods these values are represented using a lookup-table, while deep RL methods use powerful function approximators to estimate these values."}, {"title": "2.1 Independent Q-learning", "content": "Q-learning is an off-policy, temporal difference (TD) control algorithm that learns the action-value function Q(S, A) by directly approximating the optimal action-value function Q*(S, A), independent of the policy being followed. The update step for the action-value function is defined as\nQ(St, At) \u2190 Q(St, At) + \u03b1[Rt+1 + y max Q(St+1, a) \u2013 Q(St, At)],\nwhere a is the learning rate and y the discount factor of future rewards. This can be extended to MARL with independent Q-learning, where each agent has their own action-value function and uses their individual experiences in their update steps.\nThe pseudocode for independent Q-learning in a turn-based scenario is shown in Algorithm 1, where each agent i uses their own observations O\u012f to update their action-value functions Q. Note that due to the unique nature of turn-based settings, a special type of forward accumulated reward (FAR) must be used which consists of the one round return resulting from each agent's action, i.e. \u03a3=0 Rt+1+k\u2022\nInstead of distinct policies, independent Q-learning agents can also make use of a shared policy, especially when the environment contains symmetries. This allows the update step (2) to be updated more frequently by using the experiences of all the independent agents. This has proven to offer significant performance gain and allow agents to learn more effectively."}, {"title": "2.2 Deep Q-learning", "content": "Deep Q-learning is an extension of tabular Q-learning where artificial neural networks (referred to as deep Q-networks or DQNs) are used as non-linear function approximators. Deep Q-learning implements an experience replay memory to store the experiences e = (St, At, Rt+1, St+1) , which in turn is sampled in random batches b to remove correlation within the observation sequence, and thereby smoothing over the changes within the data distribution.\nDeep Q-learning usually incorporates a policy network and a target network. The policy network is used to select actions and utilises the update step, while the target network serves as a baseline when calculating the TD error. The target network is updated periodically with the policy network to reduce correlation with the target. The weights of the policy network are updated using backpropagation with the goal of minimizing the TD error, which is defined as\nLj (\u03b8j) = Eb[(Rt+1 + ymax Q(St+1, a; \u03b8j) \u2013 Q(St, At; \u03b8j))\u00b2],\nwhere \u03b8j and \u03b8j are the weights of the policy-and target network, respectively, at iteration j . In practice the backpropagation and calculation of the TD error is usually handled by an optimizer, such as the Adam optimizer. Similar to tabular Q-learning, deep Q-learning can be extended to MARL using independent deep Q-learning , and can also make use of a shared policy."}, {"title": "2.3 Options", "content": "Options are built on the concept of semi-MDPs, where actions can have varying lengths over multiple time steps t as the environment transitions from S to S'. When an agent choses an option w, the option executes stochastically according to a policy \u03c0\u03c9. This policy allows for existing domain knowledge to be incorporated into RL techniques. For example, in the case of a robot navigating terrain, a potential option can include: {move around boulder}, and once the agent choses that option, an object avoidance controller takes over executing policy \u03c0\u03c9.\nOnce policy \u03c0\u03c9 has concluded, the agent can choose a new option, effectively simplifying the problem at hand by solving it on a higher level, since the agent no longer has to focus on learning the primitive actions as well as the overarching problem simultaneously. Thus, an option w is defined as\n(\u0399\u03c9, \u03c0\u03c9, \u03b2\u03c9),\nwhere I is the initial condition that must be met for an option to become available in a certain state (St \u2208 \u0399\u03c9), \u03c0\u03c9 the policy according to which the option executes, and  the termination condition to which the option is executed . The policy \u03c0\u03c9 can also be defined using RL, a process known as intra-option learning . This has further lead to the development of Option Discovery, which uses a Laplacian framework to allow an agent to define and learn its own options as well as the intra-option policies while interacting with the environment. Options have also been extended to MARL with Amato et al. introducing their macro-action Dec-POMDP (MacDec-POMDP) and showing how options affect the Dec-POMDP structure.\nRecently, Chen et al. explored option discovery within MARL and introduced their own variation on the Laplacian framework using Kronecker graphs. Options within MARL poses unique challenges due to the temporal disconnect of the actions as a direct result of them having varying lengths. The actions are also chosen asynchronously, which further adds to the complexity and non-stationarity of the environment . Additionally, agents in a synchronous setting must often wait for other agents to complete their options before they can initiate a new option, especially if that new option is a cooperative one ."}, {"title": "3 Hanabi", "content": "The Hanabi problem along with how human conventions are used to solve the problem effectively, must first be discussed in order to give context for our novel approach. Hanabi is a 2-5 player card game, best described as a cooperative solitaire . Players have a hand of five cards for two and three player, and four cards for four and five player. Each card has a suit/colour (red, yellow, green, white, or blue) and a rank (1 to 5). The deck comprises 50 cards total, with 10 cards for each suit with a distribution of three 1's, two 2's, 3's, and 4's, and finally only one 5. The aim of the game is to stack these suits in numerical order from 1 to 5, however players cannot see their own cards, only the cards of their fellow players.\nThe players take consecutive turns, and on each turn a player can either play, discard, or hint. Hinting involves revealing all the cards in another player's hand matching a certain rank or a certain suit, and consumes one of the limited (and shared) hint tokens. In the centre is a stack for each suit where the players must play their cards, and a successful play entails playing a card that follows the current card on top of a stack (starting at 0). If the card played does not follow the current card on a stack, the play was unsuccessful (called a misplay) and the players lose one of their shared life tokens. Discarding involves removing a card from the current player's hand and adding it to the discard pile, effectively removing it from the game, while also replenishing a hint token. An example game is shown in Fig. 1."}, {"title": "3.1 Human Conventions in Hanabi", "content": "Due to the nature of Hanabi, players have too few hint tokens in order to effectively convey all the needed information (colour and rank) of the 25 playable cards . This becomes even more challenging as the number of players increase and hint tokens become more valuable, as well as the number of playable turns decreasing. Hints must therefore be used wisely, and must convey information about more than just one card. This is possible since more than one card with a similar rank or suit can be touched (or revealed) by a single hint, along with negative information being given about all the other cards in a hand. Unfortunately, this is not enough, and players who don't use additional strategies will struggle to reliably achieve a score above 15/25.\nPlayers, therefore, require a way to convey additional information through implicit communication, while still remaining within the rules of the game. As stated by Bard et al. : \"This implicit information is not conveyed through the impact that an action has on the environment (i.e., what happens) but through the very fact that another player decided to take this action (i.e., why it happened)\". Players can then reason over the actions of others, and implicitly communicate through them. This is done through the use of conventions, or mutually agreed upon \"rules\", that players develop outside the game. For example, in Fig. 1 the players have a convention where an ambiguous hint has the focus placed on their left-most card, therefore player 1 can hint to player 2 that they have two red cards in position one and three respectively, and player 2 will know (as a result of their convention) that the left-most card must be the playable red 1. Since the game's release, players have developed their own intricate and extensive conventions, often specific to their group of friends or cooperators, examples include the Board Game Arena and the H-Group, with the H-Group being the most widely used and adapted."}, {"title": "4 Augmenting the Action Space with Conventions", "content": "Existing MARL approaches specific to Hanabi naturally develop conventions, but these conventions are nonsensical and vary greatly from run to run . We propose incorporating existing human conventions into MARL algorithms using artificial conventions, which act as advanced cooperative actions that span over multiple time steps and agents. Conventions require that all the agents involved participate or \"subscribe\" in order for it to reach fruition, however the agents cannot communicate directly which convention is being started or followed.\nConventions can be divided into two categories, namely available conventions and active conventions. We define a convention C as\nC = (\u0399, \u03c0\u03b9, \u039c,\u03c0\u039c, F, \u03c0F),"}, {"title": "4.1 Architecture Design and Implementation", "content": "where the initial condition I must be met for a convention to become available (Ot \u2208 I), and if an agent chooses to initiate C, the policy \u03c0\u03b9 determines the corresponding environment action to start the convention. The convention will then become active, and another agent can \"subscribe\" to C if they meet the condition M, with \u03c0\u039c determining their environment action to continue C. Eventually an agent will have the opportunity to perform a unique continuation action to complete C, but only if they meet the final condition F, with TF determining their environment action to terminate Cand allow it to reach completion. For the sake of simplifying this discussion, and to be consistent with our implemented Hanabi conventions in later sections, we will focus on two-step conventions, where one agent initiates a convention C and another agent must opt in to complete the convention, i.e. C = (\u0399,\u03c01, F,\u03c0F). The concept behind conventions is best described at the hand of an example:\nConventions Example. In Fig. 1 each player is controlled by an agent, with the current player (referred to as the active player) being agent 1. The convention at hand, which we will refer to as c\u2081, states that if an ambiguous hint is given, the focus of that hint is the left-most (or newest) card. Since the condition Ic1 is met, i.e. player 2 has two cards in their hand that share the same colour or rank and the left-most one is playable, the action {start c\u2081} is available for agent 1. Agent 1 chooses to initiate c\u2081 which in turn gets translated by the policy TIC1 to an environment action of {hint red to player 2}, ending player 1's turn. It is now player 2's turn, and they have just received an ambiguous colour hint from player 1, and given their current observation they can derive that c\u2081 is currently active. This triggers the subscribing (and also completing) condition Fc1, where agent 2 can now opt in to continue (and complete) c\u2081. When agent 2 chooses to subscribe to c\u2081, their action is translated by the policy Fc1 to an environment action of {play card in position 1}, subsequently ending player 2's turn and completing c\u2081.\nIt is important to note that conventions do not have to be sequential, e.g. in the case of the example above, if agent 3 had their turn in between agent 1 and 2, agent 3 would be able to initiate, or subscribe to, a different convention (since they do not meet any conditions of c\u2081) and c\u2081 will remain active. When more conventions are introduced, the learning problem becomes apparent and crucial, since agents must learn which convention is applicable in certain scenarios where more than one convention is available. Furthermore, since agents have the ability to opt in to a convention, they can also choose not to, which could result in an active convention termination with-out a completing action. This allows agents to halt certain conventions based on their unique observations alluding to it being non-optimal. Thus, the agents must learn when to prioritise superior conventions over existing or active inferior ones."}, {"title": "4.2 Action Space Augmentation", "content": "However, there can exist scenarios where no conventions are available, and the agents will not be able to take any actions. To solve this problem, we propose augmenting the action space with primitive actions and conventions simultaneously, similar to the discussion by Sutton et al. on combining primitive actions and options. This requires that the conventions be appended to the existing primitive-action space (with size z), and results in an action-convention space consisting of\nC' = {ao, a1, ..., Az, C1, C2, ..., Cy } ."}, {"title": "4.3 Preliminary Results on Small Hanabi", "content": "Small Hanabi has a deck reduced to 20 cards, i.e. only having two suits, is limited to only two players, and the player hand size has been reduced to two cards. Furthermore, the life tokens have been reduced to one, and the hint tokens to three, otherwise the game shares all the core mechanics as regular Hanabi. This results in the problem having a significantly smaller state-action space and allows for initial testing and development of algorithms, with the aim of solving the full Hanabi problem. Even though this problem is considered simpler than Hanabi, it is by no means an easy problem to solve, with a perfect score of 10/10 being difficult to achieve. Player hands are often \"locked\" with important cards that must not be discarded (called critical cards), hint tokens run out almost immediately, and players are often forced into no-win scenarios where they must choose a bad action or risk losing their single life token.\nIn these tests, we use our in-house learning environment as well as the open-sourced learning environment developed by Bard et al. . We implement our own rudi-mentary conventions specific to Small Hanabi, which we developed through multiple human plays. Fig. 3a shows the result for independent Deep Q-learning (DQN) with a primitive-action space compared to independent Deep Q-learning with a pure convention space. DQN with a pure convention space performs significantly better than DQN with a primitive-action space, achieving a faster training time and improved converged performance. To validate these results, and improve on the performance further, we conduct an initial test using Rainbow and the open-sourced Hanabi learning environment with the Small Hanabi preset, the results are shown in Fig. 3b. Rainbow is able to perform better than independent Deep Q-learning achieving a score of 7,6/10 compared to 5/10, and when substituting the primitive-action space for a pure convention space, the agents achieve significantly faster training time along with a slight convergent performance increase.\nDue to Hanabi having a long list of intricate conventions, in addition to augmenting the action space with conventions, we experiment with the idea of simplifying the list of conventions during augmentation. This simplified list consists of the con-ventions that were the most straightforward and natural to develop, and were also the most common ones to occur in a game. The removed conventions can be considered edge-case or specific to certain game states that are very uncommon, and often results in uncertainty regarding their effectiveness. Fig. 3a and Fig. 3b shows the performance for DQN agents and Rainbow agents, respectively, with an augmented"}, {"title": "5 Performance Evaluation Using Hanabi", "content": "The preliminary tests for action augmentation with conventions in Small Hanabi has shown promising results, and allow the agents to achieve faster training time and improved policies. We now shift focus to the full Hanabi problem for 2-5 players and"}, {"title": "5.1 Experimental Setup", "content": "Before discussing the results, we will briefly highlight the architecture design for each deep RL technique. For a full list of the hyperparameters used in each method see Table A1 in Appendix A. All methods receive a one-hot encoded observation defined by the environment as input to their neural networks, notably this observation tuple also contains the most recent actions within the previous round. The Rainbow agents use a Multilayer Perceptron (MLP) consisting of two feed forward neural networks with 512 neurons each, with the ReLU activation function  applied. It implements the Adam optimizer  to calculate the TD error and perform backpropagations to update the network weights. Additionally, the Rainbow agents use distributional reinforcement learning to predict the value distributions which are approximated as a discrete distribution over 51 uniform atoms, along with prioritise replay memory sampling . Even though Rainbow uses n-step bootstrapping, in these experiments a value of n=1 was found to be optimal, additionally noisy nets have been disabled in favour for a traditional decaying epsilon-greedy approach.\nWhen applying conventions to Rainbow, we only augment the action space and leave the reward signal and observation space untouched. Additionally, we add the necessary convention layers to the network architecture, but keep the core algorithm unaltered, as illustrated by Fig. 2. Through testing, we found the optimal hyperpa-rameters to be similar to baseline Rainbow, as shown in Table A1 in Appendix A. During evaluation we compare each approach's exponential moving averages (with a weight value of 0.9995) and standard error of the mean. All agents are trained using an Intel Core i7 10 700K CPU and Nvidia RTX 3080Ti GPU. The reward type for the Hanabi learning environment is set to non-lenient, i.e. the agents will receive a large negative reward (equal to the score) when bombing out.\nDue to the symmetric nature of the environment, we apply a shared policy strategy during training, i.e. the agents share an action-value function that is updated based on a communal memory of each agent's experiences . It is important to note that this still restricts learning by only using individual experiences, i.e., there is no additional sharing of state information between each agent. During the cross-play evaluation of 2-5 player Hanabi, we chose samples from 10 separately trained agents for each player count, and show the results for the combination of agents that performed the best on average."}, {"title": "5.2 Self-play Performance", "content": "The learning curves for Rainbow with and without an augmented action-convention space for 2-5 player Hanabi are shown in Fig. 4. The two player scenario seen in Fig. 4a shows the smallest improvement when applying conventions, however the two player Hanabi problem can be considered a special case. This is due to the fact that the shared information between the two players are far more limited when compared to higher player counts.\nFor example, in two player Hanabi, half of the hands are hidden to a single player whereas in the three player scenario only a third of the hands are hidden, and the players always have a common hand visible. This allows for more advanced reasoning over another player's actions, and can be seen in two of the principle conventions (specifically the Finesse and the Prompt) which cannot be applied in two player Hanabi.\nDespite the fact that the complexity of the problem increases exponentially as more agents are added to the environment, the performance uplift of conventions become more apparent. In the three player setting seen in Fig. 4b, the agents are able to achieve a significantly faster training time as well as an improved convergent performance with a higher average score. The performance improvement becomes even more apparent when looking at the five player setting, seen in Fig. 4d, which is generally considered the most difficult problem to solve, and where the agents train roughly 10 times faster than the baseline Rainbow agents. In Fig. 5 the distribution of scores during evaluation of each agent over the course of 1 000 episodes is shown. In each scenario, conventions allow for a significant performance improvement, achieving a consistent grouping near a score of 21/25 and less overall variance when compared to baseline Rainbow. Rainbow with an augmented action-convention space also displays a robustness to increased player counts, with a remarkably consistent behaviour across all scenarios.\nTable 1 shows the average score for the best agent from each player count's various runs compared to the results from Bard et al. . Rainbow with an augmented action-convention space demonstrates an improved score for each scenario, with a higher average in the 3-5 player scenario and overall less deviation. Additionally, the four and five player results are competitive compared to other algorithms, such as ACHA, which requires 20 billion training steps to achieve a score of 16.8/25 in the five player setting.\nUpon investigation, the agents tend to choose conventions over primitive actions 70% of the time. This is the main reason for the Rainbow agents not achieving a higher average score, since they struggle to learn some of the core conventions available to them (specifically the Chop). We believe this to be a result of the Rainbow algorithm not having a memory of past observations, and therefore not being able to realise the importance of conventions that have a high payoff over extended time frames."}, {"title": "5.3 Cross-play Performance", "content": "Although self-play has historically led to the highest state-of-the-art agent perfor-mance within Hanabi , cross-play is also a crucial and widely applicable problem to solve. The ability for agents to cooperate across different training runs or regimes, or even across architectures, can greatly benefit MARL algorithms and their ability to cooperate in real world scenarios . Self-play agents are notorious for not being able to cooperate with never-before-seen partners, and often results in very poor performance .\nTable 2 shows the two player performance for various agents in cross-play Han-abi taken from existing literature, compared to the results of our cross-play Rainbow agents with an augmented action-convention space. Conventions are able to signifi-cantly improve on the performance of cross-play agents, allowing the baseline Rainbow agents to go from abysmal performance to an acceptable score. Furthermore, it is able to outperform SAD with only other-play applied for the two player scenario, with other-play and auxiliary task applied to SAD still holding the best score. How-ever, as mentioned previously, auxiliary tasks only benefit the two player scenario in Hanabi . When moving from self-play to cross-play, Rainbow with an augmented action-convention space only suffered a slight performance decrease and maintained remarkable consistency across various agent parings.\nTable 3 shows the results for cross-play Rainbow with an augmented action-convention space for all player counts of Hanabi. As seen in the self-play results for Rainbow conventions and confirmed in Table 3, the two player scenario is a spe-cial case, with the agents showing the biggest performance degradation when moving from self-play to cross-play, and overall worst score compared to the other cross-play results. This is again due to the more powerful conventions not being available in two player Hanabi, thus, it is important to look at all the player scenarios when evaluating cross-play performance.\nMost remarkable is the five player results, where the cross-play Rainbow agents with an augmented action-convention space are able to achieve a relatively high score and still cooperate effectively. The reason conventions are able to offer such a large performance gain for cross-play is due to the fact that the agents learn from a com-munal list of conventions that are taken from existing domain knowledge. Thus, if an agent is paired with a never-before-seen cooperator that has learned to play with the same list of conventions, they will still be able to cooperate effectively, since there is no uncertainty over another agent's intent and reasoning. This is not unexpected, since humans who learn from the same \"list\" of conventions (whether it is physical or passed down), and that have never before interacted, can cooperate and communicate effectively with relatively low uncertainty."}, {"title": "6 Conclusion", "content": "Hanabi offers a complex and intricate problem for multi-agent reinforcement learn-ing agents. It tests the ability to reason over another player's actions and intensions, while maintaining a limited communication channel and partial observability. Exist-ing MARL algorithms focus on complex architecture design and implement advanced algorithms capable of achieving remarkable performance. However, these algorithms are often computationally expensive, and require immense amounts of training data to learn convergent policies. In this paper, we focused on a different approach, explor-ing another core aspect of the Hanabi problem, namely conventions, and how to incorporate them into MARL. We showed how existing human conventions can be implemented in a MARL scenario using a special form of cooperative actions and action space augmentations.\nOur main results show that conventions are able to significantly improve on the training time for Rainbow agents in Hanabi, and in the case of three to five play-ers, also improve on the converged policy. Other Hanabi algorithms, such as SAD or MAPPO, require billions of training steps to reach convergent policies, whereas conventions reduced the number of training steps for Rainbow to below 30 million consistently. Additionally, conventions showed significant performance increase for cross-play agents, and allowed the Rainbow agents to go from not cooperating at all to being able to achieve decent performance across all player scenarios. For the most difficult scenario of five players, the cross-play Rainbow conventions agents trained on 50 million steps are able to outperform the ACHA self-play agents trained on 20 billion steps.\nThe biggest avenue for future research is exploring and developing convention dis-covery, similar to that of option discovery, which will allow agents to produce and define their own conventions as they train and learn. Our research serves as the basis for"}]}