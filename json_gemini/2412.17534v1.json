{"title": "CITEBART: LEARNING TO GENERATE CITATIONS FOR LOCAL CITATION RECOMMENDATION", "authors": ["Ege Yi\u011fit \u00c7EL\u0130K", "Selma TEK\u0130R"], "abstract": "Citations are essential building blocks in scientific writing. The scientific community is longing for support in their generation. Citation generation involves two complementary subtasks: Determining the citation worthiness of a context and, if it's worth it, proposing the best candidate papers for the citation placeholder. The latter subtask is called local citation recommendation (LCR). This paper proposes CiteBART, a custom BART pre-training based on citation token masking to generate citations to achieve LCR. In the base scheme, we mask the citation token in the local citation context to make the citation prediction. In the global one, we concatenate the citing paper's title and abstract to the local citation context to learn to reconstruct the citation token. CiteBART outperforms state-of-the-art approaches on the citation recommendation benchmarks except for the smallest FullTextPeerRead dataset. The effect is significant in the larger benchmarks, e.g., Refseer and ArXiv. We present a qualitative analysis and an ablation study to provide insights into the workings of CiteBART. Our analyses confirm that its generative nature brings about a zero-shot capability.", "sections": [{"title": "1 Introduction", "content": "Citations are essential building blocks in scientific writing. Their accurate placements indicate quality, as one should know the literature to claim contributions and put the current study in the context of the existing work from different aspects, such as background information, method, and result comparison [Cohan et al., 2019].\n\nThe first citation-related task in natural language processing (NLP) has been citation impact prediction, where a paper's future scientific impact is predicted on the basis of the number of times a paper gets cited after publication [Gehrke et al., 2003]. Unlike the first approaches that relied on paper metadata and abstract, the recent work (van Dongen et al. [2020], Huang et al. [2022]) exploit the whole content of scientific papers to achieve the goal.\n\nCitation prediction is defined as a two-step process where the former focuses on where in the sentence to place the citation [Buscaldi et al., 2024], while the latter (citation recommendation) obtains a set of candidate papers once there is a specified citation placeholder in a given context. In this sense, citation recommendation serves as a citation suggestion mechanism. For a given scientific text, it can suggest additional papers on a similar topic. These suggestions can be considered additional reading material alongside the targeted paper, corresponding to the ground-truth citation.\n\nThere are two levels of citation recommendation: the first, whom to cite, and the second, whom to cite in what context. The former is global citation recommendation, traditionally performed based on paper metadata such as author names, paper titles, abstracts, conference venues, publisher information, etc. Recently, custom citation-aware language models (SciBERT [Beltagy et al., 2019], SPECTER [Cohan et al., 2020]) learn good citation-aware embeddings for full papers to perform well in this task. The latter task is local citation recommendation (LCR), aiming to determine the target paper for a citation placeholder. Additionally, the local citation contexts can be leveraged for citation impact prediction. How a paper frames its work through citations is predictive of the citation count it will receive [Jurgens et al., 2018]."}, {"title": "2 Related Work", "content": "BERT [Devlin et al., 2019] is an encoder-only pretraining model that adopts the MLM objective. MLM masks tokens in a uniformly random fashion and predicts them, allowing the generation of learning signals bidirectionally. Some BERT variants were released to meet the requirements for masking a group of tokens. SpanBERT [Joshi et al., 2020] builds on this objective by masking random contiguous text spans. In the same direction, PMI-Masking [Levine et al., 2021] masks word n-grams based on their PMI (Pointwise Mutual Information) scores. Pretraining encoder decoders, e.g., BART [Lewis et al., 2020], combine the strengths of bidirectional learning of encoders with the autoregressive nature of decoders, capturing the local patterns of tokens within their generative capabilities.\n\nMost of the past works in citation prediction have focused on citation count prediction. Citation counts refer to the number of times a paper gets cited after publication. Brody et al. [2006] aim to predict the future citations of a paper using web usage statistics. NNCP [Abrishami and Aliakbary, 2019] uses a SimpleRNN model to predict long-term citations using short-term citations. Bai et al. [2019] propose the Paper Potential Index based on a combination of manually acquired features. SChuBERT [van Dongen et al., 2020] leverages the entire contents of papers to accomplish the task. FGCCP [Huang et al., 2022] performs a fine-grained analysis to attribute citation frequencies to individual parts of papers.\n\nYu et al. [2012] learn citation relations through a meta path-based approach. Their approach combines authorship metadata with discriminative term features to calculate citation probabilities on the DBLP network. Tanner and Charniak [2015] combine LDA-Bayes with metadata features under a logistic regression classifier to recommend citations.\n\nSimilar to citation recommendation, the recent work of Luo et al. [2023] predicts provisions of the U.S. Code by pretraining RoBERTa [Liu et al., 2019] and LegalBERT [Chalkidis et al., 2020] on the curated dataset (PACER [Luo et al., 2023]) of the US federal court documents where each provision source text is given with its associated target citation.\n\nIn scientific document understanding, learning better representations for scientific papers has been a focus. In such an effort, SciBERT [Beltagy et al., 2019] performs pretraining exclusively on scientific texts. Specifically, it is pre-trained on a randomly sampled dataset of 1.14M from the Semantic Scholar database. It is built upon an in-domain vocabulary (SCIVOCAB), which brings about superior performance compared to BERT in downstream tasks that involve scientific data.\n\nSPECTER [Cohan et al., 2020] learns citation-aware global representations for scientific papers using a citation-based pretraining objective. Starting from the initial SciBERT weights, the system adopts a triplet loss function based on document similarities. The first component in this objective is the source document's similarity to one of its citations. In contrast, the second component is its similarity to a negative paper not cited by the source, and finally, there is an additional term of loss margin hyperparameter. SPECTER-produced representations introduced remarkable results in the paper classification and global citation recommendation tasks.\n\nLCR has four benchmark datasets for evaluation. BERT-GCN [Jeong et al., 2020] introduced the FullTextPeerRead dataset, extended from the original PeerRead [Kang et al., 2018]. Throughout this paper, we refer to the FullTextPeer-Read dataset as PeerRead for brevity. An additional dataset is ACL-ARC [Bird et al., 2008], derived from the ACL Anthology Reference Corpus. We run our experiments on its ACL-200 subcategory, analogous to DualEnh [Medi\u0107 and Snajder, 2020] and HAtten [Gu et al., 2022]. Finally, Refseer [Huang et al., 2015] and ArXiv [Gu et al., 2022] are the largest benchmarks for this task.\n\nBERT-GCN [Jeong et al., 2020] utilizes two encoders for citation recommendation. The first encoder generates local context embeddings using BERT, while the second one creates the graph embeddings of citation networks using a GCN model [Kipf and Welling, 2017]. The approach combines these embeddings to produce representations for papers. It was evaluated exclusively on the PeerRead dataset.\n\nDualEnh [Medi\u0107 and Snajder, 2020] trains a Bi-LSTM model to leverage similarity between a target paper and its candidate papers. The target paper provides a context with a citation placeholder, and the model utilizes the titles"}, {"title": "3 Methodology", "content": "We propose CiteBART, a novel pre-training strategy designed to predict citations within the contexts of scientific papers. We mask placeholder tokens, which replace ground-truth citations in the parenthetical author-date style, for the continual pre-training of a vanilla BART-base to generate the correct parenthetical author-date citation for a given context. CiteBART is trained on benchmark datasets and learns to recommend citations during the decoding process."}, {"title": "3.1 Pre-training BART with Citation Objectives", "content": "BART [Lewis et al., 2020] is a sequence-to-sequence model with an encoder and a decoder. It introduces a set of document corruption (denoising) schemes and then optimizes a reconstruction loss, the cross-entropy between the original document and the decoder's outputs. The denoising transformations that are applied to the encoder during pre-training are as follows: Random token masking (similar to BERT), token deletion, text infilling (span masking with span lengths drawn from a Poisson distribution (X = 3)), sentence permutation, and document rotation with a randomly selected token leading the document.\n\nWe propose a citation learning strategy using BART. BART employs MLM similar to BERT. Additionally, to effectively reconstruct the masked contexts, it masks a span of k tokens with a single mask. In return, it can predict multiple tokens for a single mask. Thus, CiteBART can generate complex parenthetical author-date citations after custom pre-training for citation tokens without requiring further architectural modifications.\n\nWe propose two training schemes for CiteBART: Base and Global. In CiteBART-Base, the model gets the masked context with the ground-truth citation as input. This setting tests the model's performance in a local context-only situation (Table 1). With the underlying idea that good citation recommendation requires relating local citation contexts with the citing papers' global information, such as titles and abstracts, we devised an innovative way to accomplish it. Inspiring from pre-training under the REALM framework [Guu et al., 2020], in CiteBART-Global, we append the citing paper's title and abstract to the local context, allowing backpropagation through the global information that considers the pool of papers from the corpus. Specifically, we used the \"</s>\" token designated by the pre-trained BART-base model as the separator."}, {"title": "4 Experiments", "content": "We conducted our experiments on devices with NVIDIA RTX6000 Ada GPU and NVIDIA V100 GPU. The following hyperparameters were utilized in all our experiments. The number of epochs was set to 15, as the change in loss values between epochs became negligibly small beyond this point. Only the PeerRead Global dataset has been trained for 30 epochs since the generative model requires longer training for the relatively smaller PeerRead dataset. We employed a learning rate of 2e - 5 and an attention dropout rate of 0.12. Given that BART is a generative model, we adjusted its generation parameters to produce outputs that align with our requirements. Specifically, we utilized the grouped beam search with 20 beams and applied a diversity penalty of 1.5 to generate more diverse results. The maximum number of generated tokens was 25 since the generated citations should not exceed it. Apart from these specific modifications, we did not alter the architecture of the BART model."}, {"title": "4.1 Results", "content": "We report our results using Recall@10 (R@10), Exact Match (EM), and Mean Reciprocal Rank (MRR)6 and compare with the state-of-the-art approaches in Table 4. As can be seen from the table, CiteBART-Global outperforms others on the existing benchmarks except for the smallest PeerRead dataset, while the base scheme still performs as a good baseline, surpassing BERT-GCN on PeerRead, DualEnh, and HAtten on Refseer. The table includes the best-reported results of HAtten with 2k pre-fetched candidates. As for DualEnh [Medi\u0107 and Snajder, 2020], we chose their superior \"DualEnh-ws\" model for the comparison. BERT-GCN's [Jeong et al., 2020] results are available only on the PeerRead dataset. None of the past works have provided their Exact Match scores."}, {"title": "4.2 Qualitative Analysis", "content": "To provide insights into the working of CiteBART, we present some top 10 prediction examples. We analyze three different scenarios shown in Table 5. Since CiteBART is a generative model, it is prone to hallucination. In the examples, the hallucinated predictions are designated with the * symbol. Furthermore, we provide another qualitative analysis on the performance of LLMs in LCR in Appendix D."}, {"title": "4.3 Ablation Study", "content": "We conducted an ablation study to show different components' contributions to the overall results. The analysis was carried out on the ACL-200 dataset. Table 6 shows the results for CiteBART-Global with a model pre-trained on the ACL-200 Global dataset in 15 epochs.\n\nThe first three experiments test the contribution of the local context, title, and abstract to the overall performance. First, we remove the local context to see the performance due to the global information-only training (#1 in Table 6). We discard the title and abstract in the second and third configurations (#2 and #3 in Table 6). The results show that excluding the local context brings about a sharp reduction in the performance metrics (a drop from 0.739 to 0.588 in Recall@10), confirming its decisive role in generating citations. On the other hand, removals of title or abstract do not lead to a statistically significant decrease in performance.\n\nIn the fourth ablation study, we further expand the global information with the cited paper's title and abstract during pre-training (#4 in Table 6). The evaluation stays the same, feeding the local context with the citing paper's title and abstract during inference. Contrary to expectations, adding the ground-truth paper's global information during pre-training does not help; the model falls in its performance. This failure may be explained by the model learning to associate the citation token with the global information of both the citing and cited article in the training phase. However, lacking the cited paper's global information in the test phase confuses the model's predictions.\n\nThe previous studies (Medi\u0107 and Snajder [2020], Gu et al. [2022]) utilize an all-including training and inference configuration where citing and cited paper's global information is concatenated with the local citation context. Their"}, {"title": "5 Discussion and Conclusion", "content": "This work proposes CiteBART, a custom language model pre-training with citation objectives for LCR. In CiteBART-Base, we mask the citation tokens in the local contexts to make citation predictions. This local context-only pre-training performs as a good baseline, superior to BERT-GCN on PeerRead, and DualEnh-ws and HAtten on Refseer. CiteBART-Global concatenates the title and abstract of the citing paper to the local context during the citation-masked pre-training. Its superior performance proves the effectiveness of global information, such as titles and abstracts, in the citation learning task.\n\nCiteBART is distinctive as it performs LCR by end-to-end learning. On the other hand, the recent approaches adopt pre-fetch and re-rank pipelines where their system first retrieves a set of papers and then ranks the retrieved by matching queries (citing papers' titles and abstracts, local citation contexts) with candidate papers' representations (cited papers' titles and abstracts). While our model does not use global information about cited papers during testing, these systems require titles and abstracts of the cited papers for inference. In CiteBART-Base, we rely solely on local citation contexts, while CiteBART-Global incorporates the citing paper's global information to make predictions. Across all benchmark datasets except for the smallest PeerRead, CiteBART-Global outperforms the state-of-the-art.\n\nCompared to the existing approaches, CiteBART possesses zero-shot capabilities. As a generative language model, its predictions may craft a new group of tokens to form a citation. The crafted citation tokens exhibit some good propeties, e.g., they share the author names with the ground-truth citation token, the date part hallucinated.\n\nCiteBART can still be fine-tuned for any downstream task. We hypothesize that it should perform better in downstream tasks involving citations and scientific papers than other language models without citation-specific learning signals during pre-training, an area we intend to explore in future work. Furthermore, with the release of new citation recommendation datasets, it will be sufficient to continually pre-train the model to acquire knowledge about the new scientific papers with no need to pre-train from scratch.\n\nFinally, we comment on the advantages of using BART over encoder-based pre-training models such as RoBERTa. BART's MLM objective is flexible and allows the masking of all the tokens in the parenthetical author-date style. RoBERTa cannot add citation tokens to its vocabulary by its MLM. Moreover, in the prediction phase, constraining predictions to citation tokens for RoBERTa is not straightforward. While BART is prone to hallucination, its capabilities significantly enhance LCR performance.\n\nAs shown in our ablation study, extending the local citation context with both the citing and cited paper's title and abstract during the continual pre-training does not produce a better result, which can be evaluated counter-intuitive as one has all the information to learn a citation relationship. The missing global information for the cited paper in the test phase complicates finding out the associated citation token.\n\nFor future work, we plan to investigate further the all-including configuration given in the ablation study. Conceptually, exploiting the cited paper's title and abstract during the continual pre-training should have been complementary. However, the empirical evidence proves the contrary. More sophisticated masking strategies besides citation token masking should connect the dots by combining the information from the citing paper's title and abstract, local citation context, and the cited paper's title and abstract. Additionally, we should investigate the potential solutions to the citation-specific hallucinations and tackle a way to reduce the number of hallucinated recommendations in the top k."}, {"title": "Limitations", "content": "We recognize the following limitations in this study. First, CiteBART addresses the task of LCR, and given context with a citation placeholder, it predicts the best candidates for the placeholder. As a citation placeholder indicates that the context is worth citation, CiteBART builds upon the assumption of the citation worthiness of a local context.\n\nSecond, CiteBART necessitates pre-training on a specific dataset to recommend citations from the pool of papers in it. Thus, it may omit to cite some work or authors if they are not included in its training corpus. However, unlike the past works, as CiteBART is generative, it can recommend unseen papers, hallucinating. Although the fabricated citations in the top k predictions show that they capture the author names of the ground-truth citations, hallucination is still a problem."}, {"title": "Ethics Statement", "content": "CiteBART is a tool to support the scientific community in paper writing; it in no way replaces a researcher or alternates the thoughtful process of choosing the most appropriate references to cite in a local context."}, {"title": "Token Limits", "content": "Before pre-training with citation objectives, we ensured that each context has its \"<mask>\" token in its middle position after tokenization. Another critical aspect was the determination of correct lengths for citation contexts. We limited citation contexts in each dataset to an optimal number of tokens to avoid increasing time and memory costs. An exploratory analysis of context lengths shows that the contexts of ACL-200 and Peerread are significantly longer than those of the other datasets. After tokenization, we observed that 200 - 400 tokens were optimal for all base datasets. This limit allows sufficiently long contexts without a need for excessive amounts of padding tokens. As an exception, ACL-200 has 607 contexts that exceed the 400 limit. We have shortened them to the 400 token limit as they correspond to a small proportion of the whole number of contexts and also because the number of discarded tokens is negligible.\n\nFor each global dataset, we chose the token limit as 350. Since abstracts require a higher number of tokens, we limited the local context sizes to 100 for the global versions of the datasets. We also ensured that there are 50 tokens each on the left and right sides of the <mask> tokens. We used a token limit of 200 for abstracts for all datasets since most abstracts can fit into it."}, {"title": "Training and Evaluation Times", "content": "We conducted our experiments on devices with NVIDIA RTX6000 Ada GPU and NVIDIA V100 GPU for Global and Base datasets, respectively. For global datasets, the pre-training for Peerread and ACL-200 lasts for 2 and 6 hours,"}, {"title": "Metrics", "content": "To evaluate CiteBART, we used the Recall@10, Exact Match and Mean Reciprocal Rank metrics. The past works on citation recommendation have generally used Recall@10 and Mean Reciprocal Rank as evaluation metrics.\n\nRecall@10 is the ratio of the correctly predicted items in the top k recommendations. The benchmark datasets have only one actual target for each context. Therefore, recall@10 measures whether the target citation matches any recommendations in top k.\n\nExact match (EM) calculates whether the first prediction of the model is the same as the target citation. It is the same as accuracy since there is only one ground-truth citation for each context.\n\nMean Reciprocal Rank (MRR) considers the position of the ground-truth label in a top-k ranked recommendation list. It is the mean of the reciprocal rank of the correctly recommended citation in the recommendation list. Thus, in Equation 1, U corresponds to the total number of contexts in the dataset (test set size), and i is the position of the ground-truth citation for context u in the top-k results. We used k as 10 in our experiments.\n\n$MRR = \\frac{1}{U} \\sum_{u=1}^U \\frac{1}{rank_i}$ (1)"}, {"title": "Qualitative Analysis on Large Language Models' Performances in LCR", "content": "We conducted experiments on a Large Language Model (LLM) to evaluate its performance in local citation recommendation. We prompted the open-source \"Llama-2-70b-chat\" model for our trials. In each prompt, we first list a set of citation tokens (200, due to the limits of chat windows) from our dataset, followed by a few examples of masked contexts with the corresponding ground truth mask values. Subsequently, we ask the model to fill in the mask for a new context by selecting a citation from the initially provided list.\n\nWe present four examples in Figures 1 and 2 to illustrate the workings of the base and global pre-training schemes, respectively. Due to space constraints, we partially display the list of citations, example contexts, and citing abstracts in the prompts. Each example consists of three parts: the prompt, the LLM's answer, and the ground truth value of the masked citation token provided at the end of the prompt.\n\nFigure 1 includes a correct prediction in Part (a) and an incorrect one in (b). Indeed, the correct prediction is the only successful example in several trials using the base approach. The model responds to the prompt by \"Shwartz et al., 2016\" explaining its choice. On the other hand, the model fills in the mask by \"Bahdanau et al., 2016\" in Part (b), where \"Bluche, 2016\" is expected. Its reasoning sheds light on its wrong choice as it strongly associates the term \"attention-based mechanisms\" in the local context with Bahdanau et al.'s seminal paper on attention-based sequence modeling.\n\nIn Figure 2, Part (a) presents a successful example based on the global dataset where the prompt includes the citing paper's title and abstract with the local citation context. The LLM generates the correct citation without an explanation, unlike other predictions. The second example in Part (b) belongs to an incorrect prediction, yet the LLM makes a plausible choice here, judging from its grounding. We can conclude from the observed behavior that LLMs need custom pre-training for the citation tokens to perform well in the task of local citation recommendation.\n\nOur further trials with LLMs demonstrate that they tend not to restrict their predictions to the provided list of citations but to recommend the best choice based on their prior knowledge. They also exhibit a known deficiency. They"}]}