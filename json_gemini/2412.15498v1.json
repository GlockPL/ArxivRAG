{"title": "The First Multilingual Model For The Detection of Suicide Texts", "authors": ["Rodolfo Zevallos", "Annika Schoene", "John E. Ortega"], "abstract": "Suicidal ideation is a serious health problem affecting millions of people worldwide. Social networks provide information about these mental health problems through users' emotional expressions. We propose a multilingual model leveraging transformer architectures like mBERT, XML-R, and mT5 to detect suicidal text across posts in six languages - Spanish, English, German, Catalan, Portuguese and Italian. A Spanish suicide ideation tweet dataset was translated into five other languages using SeamlessM4T. Each model was fine-tuned on this multilingual data and evaluated across classification metrics. Results showed mT5 achieving the best performance overall with F1 scores above 85%, highlighting capabilities for cross-lingual transfer learning. The English and Spanish translations also displayed high quality based on perplexity. Our exploration underscores the importance of considering linguistic diversity in developing automated multilingual tools to identify suicidal risk. Limitations exist around semantic fidelity in translations and ethical implications which provide guidance for future human-in-the-loop evaluations.", "sections": [{"title": "1 Introduction", "content": "According to data published by the World Health Organization (WHO), over 700,000 people die by suicide each year, with an additional 10 to 20 million attempting to take their own lives. Suicidal behavior typically begins with thoughts and ideations of death, eventually leading to suicide attempts - conscious acts with the purpose of ending one's existence. In this context, social networks have become spaces where individuals often disclose emotions and information that they don't feel comfortable sharing with healthcare providers. Early identification of signs of suicidal ideation in these online environments poses a major challenge. This is where Natural Language Processing (NLP) and Deep Learning (DL) can play a crucial role in the automatic detection of suicidal thoughts in computational settings. Furthermore, these computational approaches may contribute to the development of tools for harm reduction and prevention. However, the majority of research on suicidal ideation detection has been conducted on English language data, resulting in a scarcity of linguistic resources (e.g.: datasets, lexicons, and Language Models) for most other languages.\nPrevious computational approaches to identifying suicidal ideation have relied heavily on hand-engineered features and domain expertise. For example, some studies have used structural and emotional features to train statistical prediction models on suicide text. Additionally, conventional machine learning algorithms like Logistic Regression (LR), Decision Tree (DT), Naive Bayes (NB), Support Vector Machine (SVM), K-nearest neighbor algorithm (KNN) and Extreme Gradient Boost (XGBoost) have been applied. While achieving some success, these methods depend on costly feature engineering and professional knowledge.\nRecently, deep learning has emerged as a promising approach that can automatically learn representations from data. Moreover, deep learning techniques like CNNs, LSTMS, BiLSTM have been applied in detecting suicidal ideation, with competitive performance.\nOn the other hand, with the increasing use of pre-trained language models such as BERT, RoBERTa, and mT5, the landscape of suicide ideation detection has evolved significantly. These pre-trained models, developed through massive unsupervised learning on diverse linguistic tasks, offer a powerful foundation for understanding intricate nuances of language. Researchers are now exploring the adaptation of those models for the detection of suicidal ideation. Leveraging the contextual understanding encoded in these pre-trained models, studies have reported promising results in discerning subtle and complex expressions related to suicidal thoughts, contributing to the advancement of automated detection systems. This shift towards pre-trained language models signifies a paradigmatic enhancement in the field, as it allows for a more nuanced comprehension of linguistic patterns associated with suicidal ideation, thereby enhancing the overall accuracy and sensitivity of detection algorithms.\nIn our approach, we emphasize the importance of addressing the detection of suicidal texts in the context of using multilingual language models. Translating a corpus from Spanish into five different languages and fine-tuning a multilingual language model allows us to classify suicidal texts in various languages, thus expanding the applicability of our approach.\nIn our research, we aim to explore the effectiveness of multilingual pretrained language models such as mBERT, XML-R and mT5 for detecting suicidal text on social media. The main focus of our study is to leverage these multilingual pre-trained models to translate and recognize suicidal text in six languages: Spanish, English, German, Catalan, Portuguese and Italian.\nOur primary contribution lies in the implementation of a multilingual language model with the capability to detect suicidal text in these six distinct languages. Additionally, to address the lack of labeled suicidal text in other languages, we utilize a labeled corpus and translate it into five different languages using an automatic translation model (SeamlessM4T). Therefore, this approach enables us to effectively tackle the linguistic diversity present on social media and provides a valuable tool for the early identification of suicidal content in various cultural and linguistic contexts.\nThe main objectives of our study are:\n1. Prediction of Suicidal Text in Six Languages: The model focuses on predicting posts with suicidal content by analyzing the words or phrases written by users, utilizing multilingual pretrained language models such as mBERT, XML-R and mT5.\n2. Improvement of Prediction Accuracy in Various Languages: We aim to enhance the accuracy of predicting suicidal text by incorporating attention mechanisms from multilingual pretrained language models. These attention mechanisms highlight crucial aspects within the obtained information, providing effective detection in six different languages.\nThe significant contributions of our work include:\n1. Detection of Suicidal Texts Using Multilingual Pretrained Language Models (mBERT, XML-R, mT5): We propose a model that integrates multilingual pretrained language models, including mBERT, XML-R amd mT5, for effective detection of suicidal texts in social media posts in six different languages.\n2. Prediction of User-Specific Suicidal Tendencies in Various Languages: The model examines the posts of specific users to determine if they exhibit suicidal tendencies, leveraging the capabilities of the mentioned multilingual pretrained language models."}, {"title": "2 Related Work", "content": "The initial approaches to automatic suicide risk detection were based on identifying specific language features present in psychiatric literature. For instance, in the LIWC dictionary was used to extract emotional and cognitive markers, while a set of emotional features such as feelings of loneliness, helplessness, and hopelessness. Additionally,"}, {"title": "3 Experiments", "content": "In this section, we delineate the setup of diverse experiments aimed at exploring the feasibility of a multilingual model capable of classifying suicidal texts across six different languages."}, {"title": "3.1 Dataset", "content": "The dataset we utilized in our experiments is the set of 2,068 Spanish tweets introduced in. This dataset was compiled by the authors through targeted keyword searches on expressions of suicidal ideation. The tweets were then manually annotated by humans, labeling each as either containing suicidal intent or not \u2013 a binary classification scheme. After annotation, the dataset contains 498 tweets (24%) expressing suicidal ideas, with example phrases like \"I want to disappear\" or \"I can't stand life anymore.\" The remaining 1,570 Spanish tweets do not express suicide risk.\nWe split the full dataset into training, validation. 80% of the data, encompassing 1,654 Spanish tweets, was used for model training to learn signals of suicidal intent. The validation set makes up 20% of the data, with 414 tweets, which was leveraged during model development for hyperparameter tuning and performance checks. Moreover, we used as test set the Lexicography Saves Lives (LSL) .\nWe leveraged this dataset by machine translating the entire corpus of 2,068 Spanish tweets into five other languages: Catalan, English, German, Italian, and Portuguese. The translations were produced using Facebook's SeamlessM4T model, allowing us to obtain versions of the suicide texts dataset across multiple languages stemming from the original Spanish source data."}, {"title": "3.2 Pre-trained language models", "content": "The recent advances in neural network-based language models have demonstrated substantial improvements across a wide range of natural language processing tasks. In particular, the introduction of Transformer architectures led to unprecedented progress in semantic and syntactic modeling capabilities. Unlike previous recurrent models such as LSTMs, Transformer networks apply a purely attention-based mechanism to learn intricate context representations. By utilizing multiple attention heads in parallel, these architectures can capture both local and global dependencies in a sequence of tokens.\nThe original authors of the Transformer introduced a specific implementation called BERT, which laid the groundwork for a new generation of contextualized language models. Through pre-training objectives such as predicting subsequent sentences and token masking, BERT achieves a deep syntactic and semantic understanding of language. However, the initial version of BERT was limited to the English language. Subsequent research focused on extending these models to a multilingual context to enable cross-lingual learning.\nAdaptations such as mBERT emerged, incorporating shared vocabularies and subword segmentation to represent a wide range of languages. Then, XML-R enhanced the multifaceted approach by adding byte-level tokenization and techniques like Whole-Word Masking. Finally, mT5 adopted an encoder-decoder architecture instead of the exclusively encoder format. Considering the rapid progress in multilingual language models, this work aimed to evaluate three transformative alternatives for the automatic detection of suicidal ideation: mBERT, XML-R, and mT5. Through thorough experimentation, the goal is to determine their capabilities in both language and semantics.\nEach chosen model presents unique characteristics, as described earlier, that could positively impact their performance for the given task. Additionally, all of them were pretrained in various languages, incorporating millions of trainable parameters and state-of-the-art techniques to enhance cross-linguistic transfer. In combination, this diversity allows addressing the problem from multiple perspectives, enabling a comprehensive evaluation of the relative advantages of different cutting-edge approaches for such a sensitive scenario as the expression of suicidal intentions.\nFor this study, three pre-trained language models were utilized and we oultine below further details about the architecture, hyperparameters, and training datasets for each."}, {"title": "3.3 Suicide phrase recognition", "content": "In the pursuit of robust multilingual performance, our experiments enlisted the capabilities of four cutting-edge language models: mBERT, XML-R and mT5. To fortify their adaptability, each model underwent a meticulous fine-tuning process. Leveraging the Spanish dataset, as previously detailed, and its translations into six languages-Catalan, English, German, Italian, and Portuguese-we aimed to comprehensively capture the nuances of suicidal text across linguistic variations.\nThe initial configurations for fine-tuning were aligned with the recommended settings provided by each language model. Subsequently, recognizing the intricate interplay of hyperparameters in influencing model performance, we conducted an exhaustive search to identify the most effective and contextually relevant hyperparameter sets for each individual model. This process was undertaken with a dual purpose: ensuring optimal performance across languages and tailoring the models to the specific intricacies of suicidal text classification.\nWe fine-tune mBERT, XML-R and mT5 on 1 NVIDIA 4070 GPUs with FP32. Model hyperparameters are tuned on the validation set, where learning rate {2e-5, 3e-5, 3e-5}, batch size {16, 16, 32}, a dropout rate of {0.3, 0.5, 0.5}, a weight decay of 0.01, a warmup proportion of 0.01. For clarity and replicability, the detailed configurations for all models, including the identified hyperparameter sets, are meticulously documented in Table 1."}, {"title": "4 Results", "content": "In this section, we present an analysis of the results obtained from our fine-tuned language models-mBERT, XML-R and mT5 deployed in the task of suicidal text classification across six languages. Our objective is to scrutinize the models' performance intricacies, assess their multilingual adaptability, and glean insights into the efficacy of our approach."}, {"title": "4.1 Classifiers Performance Analysis", "content": "We delve into the nuanced evaluation of our language models' performance across six languages: Spanish, Catalan, English, German, Italian, and Portuguese. Table 2 shows that, mT5 displays superior performance over the other two models across all metrics and for all languages. The precision, recall, F1, and AUC scores are consistently high, surpassing 85% in most cases.\nThis indicates that mT5 is exceptionally good at both positively detecting relevant cases (high recall) as well as minimizing false positives (high precision). It also maintains an adequate balance between both goals, as shown by its high F1-score. There is clearly substantial superiority of mT5 at this task compared to more generic BERT models.\nOn the other hand, we see that mBERT obtains the lowest scores, although still decent (around 80-83% for key metrics). XML-R improves upon mBERT's results across all languages, suggesting that language-specific pretraining can be beneficial. Regarding languages, English and Spanish consistently achieve the top scores across all models, followed by German and Catalan. Italian and Portuguese appear to be the most difficult. This could be due to several factors: data availability, similarity to English, etc.\nAn interesting finding is that the relative gaps between models remain remarkably stable across languages. This implies that the inherent strengths of each model transcend linguistic particularities. While some languages are more complex, all benefit from mT5's architectural improvements over BERT models.\nIn summary, mT5 is better suited to suicide text detection, especially excelling for English and Spanish. mBERT may perform adequately as a baseline, but there is clear room for improvement with more advanced models such as XML-R and especially mT5."}, {"title": "4.2 Model Validation", "content": "To delve deeper into understanding the learning mechanism, we implemented k-fold cross-validation to determine the mean accuracy in our three models: mBERT, XML-R and mT5. Cross-validation is a widely used data resampling strategy to assess the generalization capabilities of predictive models and estimate the true estimation error. In k-fold cross-validation, the learning set is divided into k subgroups of approximately equal length, and the number of subgroups produced is referred to as 'fold'. This partition is achieved by randomly selecting examples from the learning set without replacement. Our language models, including mBERT, XML-R and mT5, were fine-tuned using k = 10 subsets representing the entire training set. Each model was then applied to the remaining subset, known as the validation set, and its performance was evaluated. This process was repeated until all k subsets had served as validation sets.\nSubsequently, we proceeded to conduct additional tests in our six languages since our models are multilingual. This variant involves applying our models in scenarios with various languages, adding an additional level of complexity and versatility to the evaluation of their performance in detecting suicidal text."}, {"title": "5 Translation analysis", "content": "For the translation of the Spanish dataset into the other 5 target languages (English, Catalan, German, Italian and Portuguese), this study employed the SeamlessM4T model developed by Facebook .\nSeamlessM4T is based on the Transformer architecture, demonstrating the effectiveness of cross-lingual model pretraining and transfer learning. In particular, it leverages a sequence-to-sequence model with encoder-decoder structure trained on large-scale data across multiple languages (100 languages).\nThe key advantages of this specific architecture include:\n\u2022 Attention-based interactions model both global and local dependencies in input and output sequences. This provides greater context and reduces reliance on recurrence.\n\u2022 Multi-head self-attention combines representations from different positional offsets, learning synergistic features.\n\u2022 Masked language modeling and denoising objectives during pretraining further enhance context modeling.\nIt was pretrained on a variety of language pairs, including Spanish, English, Catalan, Italian and Portuguese. It demonstrated excellent BLEU metrics on translations between these languages, corroborating its suitability for the present cross-lingual research task."}, {"title": "5.1 Evaluation Metrics", "content": "Perplexity serves as an indicator to quantify the quality of each translation. We employed monolingual language models specific to each target language, assessing their ability to predict word sequences in the translated texts."}, {"title": "5.2 Results", "content": "The perplexity scores for each translation are presented in the Table 3:"}, {"title": "6 Discussion", "content": "This study explores the use of pre-trained multilingual language models, including mBERT, XML-R, and mT5, for the automatic detection of suicidal texts in social media posts across six languages: Spanish, English, German, Catalan, Portuguese, and Italian. The results show mT5 achieving the best performance overall, with F1 scores above 85%, highlighting capabilities for cross-lingual transfer learning.\nAn interesting finding is that the relative gaps between models remain remarkably stable across languages. This implies that the inherent strengths of each model transcend linguistic particularities. While some languages are more complex, all benefit from mT5's architectural improvements over BERT models.\nRegarding limitations, direct extrapolation of the results to other languages must be approached cautiously, given the wide linguistic diversity and potential impact of cultural nuances on interpreting suicidal texts. Furthermore, the quality of translations and, consequently, the predictive model, is inherently tied to the effectiveness of pre-trained models, indicating a constant need for improvements in this area.\nWhile this study presents a promising model for multilingual detection of suicidal texts, there are several directions to extend and strengthen this line of research. Some of these include: expanding linguistic scope by incorporating a broader spectrum of languages; enriching training data with more instances and diversity of sources; using specialized metrics to quantify the usefulness of the early detection model; and implementation of a user-friendly interface enabling integration into healthcare settings.\nIn summary, the focus on multilingual translation emerges as a crucial step in constructing an effective predictive model for suicidal texts across six languages. The identified conclusions and limitations provide guidance for future developments, emphasizing the need for linguistic and cultural considerations."}, {"title": "7 Conclusion", "content": "In the pursuit of a predictive model for suicidal texts in six languages, our exploration into multilingual translation yields critical insights. We observe that translations into English and Portuguese excel, showcasing the ability to preserve intent and coherence in sensitive contexts such as suicidal content.\nSensitivity to linguistic diversity emerges as a pivotal element in this process. While synchronization in translations into Catalan was acceptable, adaptations into German and Italian posed challenges, underscoring the importance of considering linguistic nuances in constructing a robust predictive model. The versatility of multilingual models, especially mT5, proves to be a valuable resource in this scenario. These models demonstrate a remarkable ability to maintain the integrity of suicidal content across diverse languages, providing a solid foundation for building a multilingual predictive model. Automated evaluation, though guided by objective metrics such as perplexity, does not replace human assessment for sensitivity and semantic fidelity in suicidal content. The implementation of human evaluations in subsequent phases is essential to ensure the appropriateness and ethical considerations of the model.\nIn summary, our focus on multilingual translation emerges as a crucial step in constructing a predictive model for suicidal texts in six languages. The identified conclusions and limitations provide guidance for future developments, emphasizing the need for linguistic and cultural considerations, as well as continuous improvements in pre-trained models and human evaluations to achieve an effective and ethical model."}, {"title": "8 Ethical Considerations", "content": "There are a number of aspects to consider when using pretrained language models to automatically translate suicide related language, especially given the sensitive nature of the data. Firstly, we have to consider user privacy and be aware of the impact online surveillance, collection of sensitive data and people's health. Furthermore, there are concerns around linguistic, cultural and contextual accuracy when automatically translating suicide-related tweets, where there can be issues around accurate translations and misrepresentation of cultural or conceptual concepts. Finally,"}, {"title": "9 Limitations and Future Work", "content": "Direct extrapolation of our results to other languages must be approached cautiously, given the wide linguistic diversity and the potential impact of cultural nuances on the interpretation of suicidal texts. Furthermore, the quality of translations and, consequently, the predictive model, is inherently linked to the effectiveness of pre-trained models, indicating a constant need for improvements in this area.\nWhile this study presents a promising model for multilingual detection of suicidal ideation, there are several directions to extend and strengthen this line of research:\n\u2022 Expansion of Linguistic Scope Incorporating a broader spectrum of languages would be key to achieving a globally impactful tool. Languages with limited use of digital technologies like Hindi, Arabic or Chinese pose challenges due to scarce representation in training data. Techniques such as small-scale automatic translation of annotated data and adaptation of models to new languages through transfer learning could help bridge this gap.\n\u2022 Enrichment of Training Data Having more instances and diversity of sources in the initial Spanish dataset would enhance derived models. Collecting content from platforms like Reddit and Facebook with a higher prevalence of mental health themes could be beneficial. Expanding labels to capture emotional nuances, linguistic subtleties and a more granular view of suicide-realted content (e.g.: moving beyond binary classification) could also contribute.\n\u2022 Specialized Metrics To more precisely quantify the utility of the early detection model, metrics like average latency to high-risk posts or rate of early false negatives should be incorporated. Establishing how these indicators vary across dialectal and sociocultural differences is essential.\n\u2022 Implementation for Healthcare Institutions Developing a user-friendly interface for models that enables integration in healthcare settings would ease the transition of this technology into real-world applications. Achieving integration with existing clinical record systems and care workflows could further its adoption.\nAddressing these extensions would provide a comprehensive system with superior accuracy, broad multilingual reach and significant impact on the timely detection and prevention of suicidal behaviors through computing."}]}