{"title": "Generating Realistic Tabular Data with Large Language Models", "authors": ["Dang Nguyen", "Sunil Gupta", "Kien Do", "Thin Nguyen", "Svetha Venkatesh"], "abstract": "While most generative models show achievements in image data generation, few are developed for tabular data generation. Recently, due to success of large language models (LLM) in diverse tasks, they have also been used for tabular data generation. However, these methods do not capture the correct correlation between the features and the target variable, hindering their applications in downstream predictive tasks. To address this problem, we propose a LLM-based method with three important improvements to correctly capture the ground-truth feature-class correlation in the real data. First, we propose a novel permutation strategy for the input data in the fine-tuning phase. Second, we propose a feature-conditional sampling approach to generate synthetic samples. Finally, we generate the labels by constructing prompts based on the generated samples to query our fine-tuned LLM. Our extensive experiments show that our method significantly outperforms 10 SOTA baselines on 20 datasets in downstream tasks. It also produces highly realistic synthetic samples in terms of quality and diversity. More importantly, classifiers trained with our synthetic data can even compete with classifiers trained with the original data on half of the benchmark datasets, which is a significant achievement in tabular data generation.", "sections": [{"title": "I. INTRODUCTION", "content": "Recently, tabular data generation has attracted a significant attention from the research community as synthetic data can improve different aspects of training data such as privacy [40], quality [43], fairness [34], and availability [25]. Particularly noteworthy is its capacity to overcome usage restrictions while preserving privacy e.g. synthetic data replace real data in training machine learning (ML) models [7], [44].\nTo evaluate an image generation method, we often generate synthetic images \u00ee, and visualize \u00ee to assess whether they are naturally good looking [18]. However, since it is hard to say whether a synthetic tabular sample looks real or fake with bare eyes, evaluating a tabular generation method often follows the \"train on synthetic, test on real (TSTR)\" approach [12], [19]. It is a common practice to generate synthetic samples \u00ee and their labels y, then use them to train ML predictive models and compute performance scores on a real test set. A better score means a better tabular generation method. \nAmong existing approaches, generative adversarial network (GAN)-based methods are widely used for tabular data gener-ation [3], [10], [21], [30], [37], [44]. Recently, large language models (LLM)-based methods are proposed, and they show promising results [7], [48]. Compared to GAN-based methods, LLM-based methods have some advantages. First, since they do not require heavy data pre-processing steps e.g. encoding categorical data or normalizing continuous data, they avoid information loss and artificial introduction. Second, since they represent tabular data as text instead of numerical, they can capture the context knowledge among variables e.g. the relationship between Age and Marriage.\nTo generate a sample that has M features {X1, ..., XM} and a label \u0177, most existing methods treat the target variable Y as a regular feature XM+1 [7], [21], [37], [44]. However, these methods may not correctly capture the correlation between X and Y, which is very important for training predictive models in the evaluation phase (see Figure 1). Some methods generate only \u00ee, and use an external classifier trained on the real dataset to predict \u0177 [48]. However, this method is cumbersome since it needs two standalone models - a generative model and a predictive model. As shown in Figure 2, we compute the importance level of each feature for each class on the dataset Cardiotocography using the Shapley values [24]. We compare the feature importance on the original data with the feature importance on the synthetic data generated by various methods. Existing state-of-the-art methods such as TVAE [44], CTGAN [44], Great [7], and TapTap [48] cannot capture the ground-truth feature-class correlation on the real dataset. This suggests that the data generated with these methods does not closely mimic the real data.\nTo address the problem, we propose a LLM-based method"}, {"title": "II. RELATED WORK", "content": "A. Machine learning for tabular data\nSince ML methods have shown lots of successes on other data types e.g. image, text, graph, they are recently being applied to tabular data. These works can be categorized into five groups. Table question answering provides the answer for a given question over tables [17], [46]. Table fact verification assesses if an assumption is true for a given table [9], [47]. Table to text describes a given table in text [1], [4]. Table structure understanding aims to identify the table character-istics e.g. column types, relations [38], [39]. Table prediction is the most popular task that predicts a label for the target variable (e.g. Income) based on a set of features (e.g. Age and Education). Different from other tasks where deep learning methods are dominant, traditional ML methods like random forest, LightGBM [20], and XGBoost [8] still outperform deep learning counterparts in tabular prediction tasks.\nB. Generative models for tabular data\nInspired by lots of successes of generative models in image data generation, researchers have explored different ways to adapt them to tabular data generation. Most tabular generation methods are based on GAN models [13], including MedGAN [10], VeeGAN [37], TableGAN [30], CTGAN [44], Copula-GAN [31], TabGAN [3], and OCTGAN [21]. Some methods are based on other generative models e.g. TVAE [44] uses Variational Autoencoder (VAE) [22], Great [7] and TapTap [48] use Large Language Models (LLM) [5].\nCTGAN [44] is one of the most common methods, which has three contributions to improve the modeling process. First, it applies different activation functions to generate mixed data types. Second, it normalizes a continuous value corresponding to its mode-specific. Finally, it uses a conditional generator to address the data imbalance problem in categorical columns. Although these proposals greatly improve the quality of the synthetic data, most of them relate to the pre-processing tasks while the core training process of CTGAN is still based on the Wasserstein GAN (WGAN) loss [2]. Many methods were extended from CTGAN e.g. TabGAN [3] and OCTGAN [21].\nC. Large language models for tabular data\nThe biggest weakness of GAN-based methods is that they require heavy pre-processing steps to model data efficiently. These steps may cause the loss of important information or the introduction of artifacts. For example, when the categorical variables are encoded into a one-hot encoding (i.e. a numeric vector), it implies an artificial ordering among the values [6]. To overcome these problems, LLM-based methods are recently proposed for tabular data generation [7], [48]. Compared to other methods, LLM-based methods offer three great benefits: (1) preventing information loss and artificial characteristics, (2) capturing problem specific context, and (3) supporting arbitrary conditioning.\nExisting LLM-based methods can generate realistic tabular data that have various applications such as data privacy [15], data augmentation [35], class imbalance [45], and few-shot classification [16]. However, as shown in Figure 2, they may not capture the correlation between the features X and the target variable Y. We are the first to propose a LLM-based method for tabular data generation, which accurately captures the correlation between X and Y."}, {"title": "III. FRAMEWORK", "content": "A. Problem definition\nGiven a real tabular dataset Dreal = {xi, Yi}i=1N, each row is a pair of a sample xi with M features {X1, ..., XM} and a label yi (a value of the target variable Y). Our goal is to learn a data synthesizer G from Dreal, and then use G to generate a synthetic tabular dataset Dfake = {xi, Yi}i=1N. Following other works [7], [21], [44], we evaluate the quality of Dfake by measuring the accuracy or MSE of a ML predictive model trained on Dfake and tested on a held-out test dataset Dtest (shown in Figure 1). A better score means a better Dfake.\nB. The proposed method\nWe propose a LLM-based method (called Pred-LLM) to generate a synthetic dataset that mimics the real dataset and captures the correlation between X and Y accurately. As shown in Figure 3, our method has three phases: (1) fine-tuning a pre-trained LLM with tabular data, (2) generating samples \u00ee conditioned on each feature X\u2081, and (3) constructing prompts based on \u00ee to query labels \u0177.\n1) Fine-tuning: In the first phase fine-tuning (top plot), we have three steps: (a) textual encoding, (b) permutation, and (c) fine-tuning a pre-trained LLM.\n(a) Textual encoding. As our method uses LLM to generate tabular data, following other works [7], [48] we convert each sample xi and its label yi into a sentence. Although there are several methods to serialize a tabular row [16], we use a simple strategy that transforms the ith row ri = [X1 = Vi,1,..., XM = Vi,M,Y = yi] into a corresponding sentence si = \"X1 is Vi,1,..., XM is Vi, M, Y is yi\", where {X1,..., XM} are feature names, vi,j is the value of the jth feature of the ith sample, Y is the target variable and yi is its value. For example, the first row [Age = 25, Edu = Bachelor, Job = Admin, Income = Low] is converted into \"Age is 25, Edu is Bachelor, Job is Admin, Income is Low\".\n(b) Permutation. Recall that existing LLM-based methods permute both the features X and the target variable Y. We call this strategy permute_xy. Formally, given a sentence si = \"X1 is vi,1,..., XM is vi,M, Y is yi\u201d, we re-write it in a short form si = [ai,1, ..., Ai,M, Ai,M+1], where ai,j = \"Xj is vi,j\u201d with j\u2208 {1, ..., M}, ai,M+1 = \"Y is yi\u201d, and [\u00b7] denotes the concatenation operator. The permutation strategy permute_xy applies a permutation function P to randomly shuffle the order of the features and the target variable. This step results in a permuted sentence si = [ai,k1, ..., Ai,km, ai,kM+1], where [k1, ..., \u043a\u043c, \u043a\u043c+1] = P([1, ..., M, M + 1]). For example, the sentence \"Age is 25, Edu is Bachelor, Job is Admin, Income is Low\" is permuted to \"Income is Low, Edu is Bachelor, Job is Admin, Age is 25\".\nIn contrast, we only permute the features X while fixing the target variable Y at the end. We call our strategy permute_x. Formally, given a sentence si = [ai,1,..., ai,M, ai,M+1], we apply the permutation function P to M features only, which results in a permuted sentence si = [Ai,k1, \u2026, Ai,km, ai,M+1], where [k1, ..., k\u043c] = P([1, ..., M]). For example, the sentence \"Age is 25, Edu is Bachelor, Job is Admin, Income is Low\" is permuted to \"Age is 25, Job is Admin, Edu is Bachelor, Income is Low\". Note that the target variable \"Income\" and its value \"Low\" are at the end of the permuted sentence.\n(c) Fine-tuning. While other LLM-based methods use the permute_xy version of the dataset to fine-tune the LLM, we fine-tune our LLM with the dataset permuted with our strategy permute_x. As we also want to keep the original order of the features, we augment the permuted data with the original data when fine-tuning the model, as shown in step (c). This step is necessary because in a later step where we construct a prompt based on a generated sample 2 to query its label \u0177, we follow the original order of the features in the real dataset.\nWe fine-tune our LLM following an auto-regressive manner i.e. we train it to predict the next token given a set of observed tokens in each encoded sentence. Given a textually encoded tabular dataset S = {si}i=1N, for each sentence si \u2208 S, we to-kenize it into a sequence of tokens (c1, ..., c\u2081) = tokenize(si), where {C1,...c} are required tokens to describe the sentence Si. Following an auto-regressive manner, we factorize the probability of a sentence si into a product of output prob-abilities conditioned on previously observed tokens:\n$p(s_i) = p(c_1, ..., c_r) = \\prod_{k=1}^{l} P(c_k | c_1, ..., c_{k-1})$ (1)\nWe train the model to maximize the probability $\\prod_{s_i\\in S} P(s_i)$ of the entire training dataset S. With this fine-tuning process, we can use any pre-trained auto-regressive LLM e.g. GPT [33], Transformer-XL [11], and Reformer [23].\n2) Sampling : In the second phase sampling (middle plot), we generate synthetic samples \u00ee. Given an input sequence of tokens (C1, ..., Ck\u22121), our fine-tuned LLM Q returns logits over all possible follow-up tokens:\nz = Q(C1, ..., Ck-1)\nThe next token ck is sampled from a conditional probability distribution defined by a softmax function that is modified by a temperature T > 0 to soften the output distribution:\n$p(c_k | c_1, ..., c_{k-1}) = \\frac{e^{(z_{c'}/T)}}{\\sum_{c' \\in C} e^{(z_{c'}/T)}}$ (2)\nwhere e() is an exponential function and C is the complete set of all unique tokens i.e. the vocabulary.\nTo generate \u00ee, we need to initialize the sequence of tokens (C1,..., Ck-1) and use it as a condition for the sampling step. As we employ permutation during the fine-tuning phase, our method also supports arbitrary conditioning like other LLM-based methods. In other words, we can use any set of features as a condition. As mentioned in the literature, there are several ways to create a condition. The most popular way is to use a pair of the target variable and its value as a condition [7], [48]. In particular, a label yi is first sampled from the distribution of the target variable Y i.e. Yi ~ p(Y). The condition is then constructed as \"Y is yi\" e.g. \"Income is Low\". We call this strategy class-conditional sampling.\nIn contrast, our method follows a feature-conditional sam-pling style. We first uniformly sample a feature X\u2081 from the list of features {X1,..., \u0425\u043c}. We then sample a value from the distribution of Xi i.e. vi ~ p(Xi). Finally, we construct our condition as \"Xi is vi\u201d e.g. \"Age is 40\u201d.\nOur sampling strategy has an advantage. As we generate the features X before the target variable Y, our LLM can use all learned attention links from X to Y to generate Y more accurately. However, it also has a weakness. As we cannot control when Y is generated, the correlations between Y and some features may be missed. For example, assume that we use X1 as a condition. From X1, using Equation (2), we may generate X3. Conditioned on (X1, X3), we may generate Y. In this case, we generate Y with only attentions coming from two features X1 and X3, and we may miss the attentions from the other features. We address this problem in the next step.\n3) Querying \u0177: In the final phase querying (last plot), we construct prompts to query our LLM for labels. After generating a synthetic sample \u00cei = [X1 = Vi,1,..., XM = Vi,M] with all features, we construct a prompt based on \u00eei as \"X1 is vi,1, ..., XM is vi,M\" and use it as a condition to sampling \u0177i. Using this way, our LLM can utilize the learned attention links from all features to better generate the target variable Y. Moreover, our LLM behaves like a predictive model, and we generate \u0177 from a conditional distribution p(\u0177 | 2). This strategy works very well thanks to the accurate correlation between X and Y captured by our LLM.\n4) Algorithm: Algorithm 1 presents the pseudo-code of our method Pred-LLM. It has three phases: fine-tuning, sampling, and querying. In the fine-tuning phase (lines 4-8), we textually encode each row to a sentence, permute it using our permu-tation strategy permute_x, and create a new dataset Dreal that contains permuted sentences and original sentences (line 7). Finally, we use DrO real to fine-tune a pre-trained LLM Q.\nIn the sampling phase (lines 17-22), we generate synthetic samples using our feature-conditional sampling approach. For each feature X\u2081, we use it as a condition to sample data from our fine-tuned LLM Q. Note that the number of synthetic samples conditioned on each Xi is sampled equally (line 16). In the querying phase (lines 24-27), we construct prompts based on generated samples \u00ee to query their labels \u0177. In this case, we use our LLM as a predictive model to generate labels."}, {"title": "IV. EXPERIMENTS", "content": "We conduct extensive experiments to show that our method is much better than other methods under different qualitative and quantitative metrics.\nA. Experiment settings\n1) Datasets: We evaluate our method on 20 real-world tabular datasets. They are commonly used in predictive and generation tasks [7], [21], [28], [44], [48]. The details of these datasets are provided in Table I.\n2) Evaluation metric: To evaluate the performance of tabu-lar generation methods, we use the synthetic data in predictive tasks (Figure 1) similar to other works [7], [21], [44]. For each dataset, we randomly split it into 80% for the real set Dreal and 20% for the test set Dtest. We train tabular generation methods on Dreal to generate the synthetic set Dfake, where |Dfake |=|Dreal |. Finally, we train XGBoost on Dfake and"}, {"title": "V. CONCLUSION", "content": "In this paper, we address the problem of synthesizing tabular data by developing a LLM-based method (called Pred-LLM). Different from existing methods, we propose three important contributions in fine-tuning, sampling, and querying phases. First, we propose a novel permutation strategy during the fine-tuning of a pre-trained LLM, which helps to capture the corre-lation between the features and the target variable. Second, we propose the feature-conditional sampling to generate synthetic samples, where each feature can be conditioned on iteratively. Finally, instead of leveraging an external classifier to predict the labels for the generated samples, we construct the prompts based on the generated data to query the labels. Our method offers significant improvements over 10 SOTA baselines on 20 real-world datasets in terms of downstream predictive tasks, and the quality and the diversity of synthetic samples. By generating more synthetic samples, our method can help other applications such as few-shot knowledge distillation [27] and algorithmic assurance [14] with few samples."}]}