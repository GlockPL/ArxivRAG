{"title": "E-CAR: Efficient Continuous Autoregressive Image\nGeneration via Multistage Modeling", "authors": ["Zhihang Yuan", "Yuzhang Shang", "Hanling Zhang", "Tongcheng Fang", "Rui Xie", "Bingxin Xu", "Yan Yan", "Shengen Yan", "Guohao Dai", "Yu Wang"], "abstract": "Recent advances in autoregressive (AR) models with continuous tokens for image\ngeneration show promising results by eliminating the need for discrete tokenization.\nHowever, these models face efficiency challenges due to their sequential token gen-\neration nature and reliance on computationally intensive diffusion-based sampling.\nWe present E-CAR (Efficient Continuous Auto-regressive Image Generation via\nMultistage Modeling), an approach that addresses these limitations through two\nintertwined innovations: 1 a stage-wise continuous token generation strategy that\nreduces computational complexity and provides progressively refined token maps\nas hierarchical conditions, and a multistage flow-based distribution modeling\nmethod that transforms only partial-denoised distributions at each stage comparing\nto complete denoising in normal diffusion models. Holistically, E-CAR operates\nby generating tokens at increasing resolutions while simultaneously denoising the\nimage at each stage. This design not only reduces token-to-image transformation\ncost by a factor of the stage number but also enables parallel processing at the token\nlevel. Our approach enhances computational efficiency and aligns naturally with\nimage generation principles by operating in continuous token space and following\na hierarchical generation process from coarse to fine details. Experimental results\ndemonstrate that E-CAR achieves comparable image quality to DiT [Peebles &\nXie, 2023] while requiring 10\u00d7 FLOPs reduction and 5\u00d7 speedup to generate a\n256x256 image.", "sections": [{"title": "1 Introduction", "content": "Autoregressive (AR) Large Language Models (LLMs) [Vaswani, 2017, Achiam et al., 2023, Touvron\net al., 2023a,b, Jiang et al., 2023] have demonstrated remarkable capabilities in natural language\ngeneration, driven by scaling laws [Kaplan et al., 2020, Hoffmann et al., 2022]. Inspired by these\nadvancements, the computer vision community has been striving to develop large autoregressive and\nworld models [Zhou et al., 2024, Team, 2024, Liu et al., 2024a] for visual generation. This effort is\ninitiated by VQ-GAN [Esser et al., 2021] and further advanced by its successors [Lee et al., 2022,\nSun et al., 2024, Tian et al., 2024, Team, 2024], showcasing the potential of AR models in visual\ngeneration.\nTraditional AR image generation models employ a visual tokenizer to discretize continuous images\ninto grids of 2D tokens, which are then flattened into a 1D sequence for AR learning (see Fig. 2.b).\nHowever, recent research [Li et al., 2024, Tschannen et al., 2023, Zhou et al., 2024, Fan et al., 2024]\ndemonstrates that discrete tokenization is not only unnecessary but potentially detrimental to AR\nmodels' generation capabilities. These works show that AR image generation can be formulated\ncontinuously, modeling per-token probability distributions on continuous-valued domains. For\ninstance, several approaches [Li et al., 2024, Zhou et al., 2024, Fan et al., 2024] leverage diffusion\nmodels for representing arbitrary probability distributions, where the model autoregressively predicts"}, {"title": "2 Related Work", "content": "Autoregressive Visual Generation models initially operated on sequences of pixels [Gregor et al.,\n2014, Van Den Oord et al., 2016, Chen et al., 2018a, 2020]. While early implementations primarily\nutilized RNNs and CNNs as base models, Transformers [Vaswani, 2017] have recently become the\ndominant architecture for these tasks [Esser et al., 2021]. VQGAN [Esser et al., 2021] pioneered\nthe use of Transformers for autoregressive image generation. Specifically, it employs a GPT-2\ndecoder-only Transformer [Radford, 2018] to generate tokens in a raster-scan order, similar to\nhow ViT [Dosovitskiy, 2020] serializes 2D images into 1D patches. Building upon this paradigm,\n[Razavi et al., 2019, Lee et al., 2022] extend the approach by incorporating multiple scales or\nstacked codes. More recently, LlamaGen [Sun et al., 2024], based on the popular open-source\nLLM architecture Llama [Touvron et al., 2023a,b], scales the Transformer to 3B parameters and\ndemonstrated impressive results in text-to-image synthesis. In parallel, recent advancements have"}, {"title": "3 E-CAR: Efficient Continuous Auto-regressive Image Generation via Multistage Modeling", "content": "We first review the basic pipeline of continuous autoregressive (AR) visual generation models\n(Sec.3.1). Next, we introduce our Efficient Continuous AR (E-CAR) image generation model. The\ndevelopment and optimization of E-CAR include three key advancements: (1) a stage-by-stage\ncontinuous token generation AR module (Sec.3.2); (2) an multistage flow model for fast continuous\ntoken recovery (Sec. 3.3); and (3) the multistage loss that enhances training stability (Sec.3.4). Finally,\nwe provide a straightforward explanation of why E-CAR can be both efficient and effective (Sec.4.3)."}, {"title": "3.1 Preliminaries: Continuous Autoregressive Visual Generation", "content": "The review of continuous AR visual generation models focuses on their main components and training\nprocedures. In this review, we highlight the inefficiencies associated with existing continuous AR\nmodels.\nImage Tokenization and Detokenization. Images are inherently 2D continuous signals. Pioneering\nAR studies [Esser et al., 2021, Sun et al., 2024, Tian et al., 2024, Yu et al., 2023a,b] apply AR\nmodeling to images via next-token prediction, mimicking AR in language models [Vaswani, 2017].\nThis approach requires two key steps: (1) tokenizing an image into discrete tokens, and (2) modeling\nthe tokens' unidirectional generation process.\nFor tokenization, a quantized autoencoder such as VQGAN [Esser et al., 2021] is often used to\nconvert the image feature map $f \\in \\mathbb{R}^{(h\\cdot w)\\times C}$ to discrete tokens $z \\in [V]^{(h\\cdot w)}$:\n$\\qquad f = E(x), z = Q(f), \\qquad \\qquad(3.1)$\nwhere $x \\in [R^{h'\\times w'\\times 3}$ denotes the raw image, $E(\\cdot)$ is an encoder, and $Q(\\cdot)$ is a quantizer. The\nquantizer typically includes a learnable $Codebook \\in R^{V\\times C}$ containing V vectors with C dimensions.\nThe quantization process $z = Q(f)$ maps each feature vector $f^i$ to the code index $z^i$ of its nearest"}, {"title": "Autoregressive Token Generation", "content": "model predicts tokens sequentially, by factorizing the likelihood\nof a sequence $z = (z_1, ..., z_T)$ as:\n$\\qquad P(z_1, z_2, ..., z_T) = \\prod_{t=1}^T p(z_t|z_1, z_2, ..., z_{t-1}).\\qquad\\qquad(3.4)$\nIn the traditional AR image generation models [Esser et al., 2021, Sun et al., 2024, Tian et al.,\n2024, Yu et al., 2023a,b] (see Fig.2), ${z_i}_{i=1}^T$ are discrete tokens. However, the discrete tokens\n${z_i}_{i=1}^T$ are not directly generated by the AR model. In fact, the autoregressive model produces\na continuous-valued C-dim vector $z_i' \\in \\mathbb{R}^C$, which is then projected by a V-way classifier matrix\n$W\\in \\mathbb{R}^{C\\times V}$ to $z_i \\in [V]$. In other words, one can use Eq.3.4 and an additional projector to generate a\nsequence of categorical tokens, and then use Eq.3.3 to decoder back to the image space.\nContinuous AR. Recent work [Li et al., 2024, Tschannen et al., 2023] formulate the probability\ndistribution $p(f_i|z_i)$ to model the detokenization for continuous tokens, where $f_i \\in \\mathbb{R}^C$ is the i-th\nimage patch's feature map and $z_i \\in \\mathbb{R}^D$ is the corresponding AR model's output continuous token.\nMAR [Li et al., 2024] uses diffusion models to define the loss function and sampler, as shown in\nFig.2 (c). MAR and its follow-up work [Fan et al., 2024] achieve state-of-the-art performance in\nimage generation, showing the promising potential of continuous AR in the visual generation.\nEfficiency Bottlenecks of Continuous AR Models. Continuous AR models face two main efficiency\nchallenges: Token-by-token generation: Similar to LLMs, the sequential nature of generation\nlimits efficiency [Yuan et al., 2024]. In these models, each token is produced based on all the\npreviously generated tokens, which means that the process cannot be easily parallelized. This\nsequential dependency results in longer computation times, especially for generating lengthy tokens\n(corresponding to high-resolution images or videos). Diffusion sampling: [Song et al., 2020a]\ndiffusion models require a large number of iterative denoising steps to generate a single sample, each\nof which must be executed sequentially [Song et al., 2020a].\nWe propose a hierarchical framework for addressing these issues, in which continuous token genera-\ntion and continuous detokenization are realized in a hierarchical manner. By doing so, we are able to\nimprove the efficiency of the continuous AR model as well as maintain its effectiveness."}, {"title": "3.2 Multistage Autoregressive Token Generation", "content": "Targeting the efficiency challenge, we introduce a stage-by-stage continuous tokens AR generation\nmodel to enhance the efficiency of token generation. Our method reimagines autoregressive image\nmodeling by transitioning from a \u201cnext-token prediction\u201d paradigm to a \u201cnext-stage prediction\"\nstrategy, as illustrated in Figs.1 and 2. In this framework, the autoregressive unit evolves from a single\ntoken to an entire token map. The process begins by encoding a feature map $M = [z_1,......, z_{(h\\cdot w)}] \\in$\n$\\mathbb{R}^{(h\\cdot w)\\times C}$ into a sequence of S-stage token maps ($M_1, M_2, . . . , M_S$). Each successive map increases\nin resolution from $h_1 \\times w_1$ to $h_S \\times w_S$, with the final map $m_S$ matching the original feature map's\ndimensions ($h. w$). The autoregressive likelihood can be formulated as:\n$\\qquad p(M_1, M_2, ..., M_S) = \\prod_{s=1}^S p(M_s | M_1, M_2, ..., M_{s-1}).\\qquad \\qquad(3.5)$\nHere, each autoregressive unit $M_s \\in \\mathbb{R}^{(h_s.w_s)\\times C}$ represents the token map at stage s, encompassing\n$h_s.w_s$ continuous tokens. The preceding sequence $(M_1, M_2, ..., M_{s-1})$ functions as a \u201cpre-\nfix\" [Yuan et al., 2024] for $m_S$. During the s-th autoregressive step, our model generates distributions\nfor all $h_s.w_s$ tokens in $m_S$ concurrently. This parallel generation is conditioned on both the prefix\nand the stage-specific positional embedding map. We term this \"next-stage prediction\" approach\nMulti-stage Continuous Autoregressive, as depicted in Fig. 2.\nEfficiency Benefit. The designed multi-stage continuous token generation offers significant com-\nputational advantages over the vanilla continuous AR. Theoretically, our method achieves a time\ncomplexity of O($n^2$), where n is the number of tokens representing an image. In contrast, the vanilla\ncontinuous AR method has a time complexity of O($n^3$). This improvement results in our approach\nbeing n times faster than the vanilla method. More details can be found in the Supplementary\nMaterials. We would like to highlight that this efficiency gain becomes increasingly significant as we\nmove towards long-sequence token generation (e.g., higher-resolution images or video generation)."}, {"title": "3.3 Multistage Flow Matching for Fast Sampling from Continuous Token", "content": "After we get the set of continuous tokens ${M_s}_{s=1}^S$, we hope to get a sampler that can draw samples\nfrom the distribution $F_s \\sim p(F_s|M_s)$ at inference time, i.e., generate image patches' embeddings\nbased on the output tokens. Targeting the efficiency challenge, we model this sampler under the\nframework using multistage flow [Liu et al., 2022, Lipman et al., 2022, Liu et al., 2024b, Xie et al.,\n2024] to improve the continuous detokenization efficiency.\nProblem Formulation. Consider the continuous-valued token map at S-th stage $M_S \\in \\mathbb{R}^{(h_S\\cdot w_S)\\times D}$,\nand the ground-truth image patch embeddings $F_1^{(S)} \\in \\mathbb{R}^{(h_S.w_S)\\times C}$ to be predicted at the this stage.\nOur goal is to model a probability distribution of $F_s$ conditioned on $M_S$, that is, $p(F_1^{(S)}|M_S)$. In the\ncontext of optimal transport, we hope to find a velocity model, optimal transport from $\\eta|M_S \\sim \\pi_0$ to\n$F_1^{(S)}|M_S \\sim \\pi_1$. For simple sampling, $\\pi_0$ commonly be set as a normal distribution.\nFlow Matching [Liu, 2022, Liu et al., 2022, Ma et al., 2024, Jin et al., 2024] is a unified ODE-based\nframework for generative modeling and domain transfer. It provides an approach for learning a\ntransport mapping T between two distributions $\\pi_0$ and $\\pi_1$. Specifically, flow Matching learns to\ntransfer $\\pi_0$ to $\\pi_1$ via an ordinary differential equation (ODE), or flow model\n$\\qquad\\frac{dF_t^{(S)}}{dt} = v_{\\theta}((F_t^{(S)}, t | cond = M_s),$\\ninitialized from $F_0^{(S)} \\sim \\pi_0 = \\mathcal{N}(0, I)$, s.t.$F_1^{(S)} \\sim \\pi_1,\\qquad\\qquad(3.6)$\nwhere cond is the condition variable, it can be a class or a text embedding, and $v_{\\theta} : \\mathbb{R}^d \\times [0, 1] \\times \\mathbb{R}^D \\rightarrow$\n$\\mathbb{R}^d$ is a velocity field, learned by minimizing a mean square objective:\n$\\qquad\\min_{\\upsilon_{\\theta}} \\mathbb{E}_{(F_0^{(S)}, F_1^{(S)}) \\sim \\gamma, z \\sim \\mathcal{D}_{cond}} \\bigg[ \\int_0^1 |\\frac{d}{dt} F_t^{(S)} - \\upsilon_{\\theta} (F_t^{(S)}, t | M_s)|^2 dt \\bigg],\\qquad\\qquad(3.7)$\nwith $F_t^{(S)} = \\phi(F_0^{(S)}, F_1^{(S)}, t)$ where $\\mathcal{D}_{cond}$ is the collection of conditions, $F_0^{(S)} = \\phi(F_0^{(S)}, F_1^{(S)}, t)$\nis any time-differentiable interpolation between $F_0^{(S)}$ and $F_1^{(S)}$, with $\\frac{d}{dt}F_t^{(S)} = \\phi(F_0^{(S)}, F_1^{(S)}, t)$.\nThe $\\gamma$ is any coupling of $(\\pi_0, \\pi_1)$. A simple example of $\\gamma$ is the independent coupling $\\gamma = \\pi_0 \\times \\pi_1$,\nwhich can be sampled empirically from unpaired observed data from $\\pi_0$ and $\\pi_1$. Usually, $\\upsilon_{\\theta}$ is\nparameterized as a deep neural network and Eq.3.7 is solved approximately with stochastic gradient\nmethods. Different specific choices of the interpolation process $F_t^{(S)}$ result in different algorithms.\nAs shown in [Liu, 2022], the commonly used denoising diffusion implicit model (DDIM) [Song et al.,\n2020a] and the probability flow ODEs of [Song et al., 2020b] correspond to $F_t^{(S)} = a_tF_0^{(S)} + b_tF_1^{(S)}$,\nwith specific choices of time-differentiable sequences $a_t$, $b_t$ (see [Liu, 2022, Liu et al., 2024b] for\ndetails). In rectified flow [Liu, 2022], the authors suggested a simpler choice of\n$\\qquad F_t^{(S)} = (1-t) F_0^{(S)} + t F_1^{(S)}\\qquad \\frac{d}{dt}F_t^{(S)} = F_1^{(S)} - F_0^{(S)}. \\qquad\\qquad(3.8)$\nSimple Flow Sampler. Once we obtain a well-trained velocity model $v_{\\theta}$ through optimization of\nEq. 3.7, we can sample image patches's embedding $F_s)$ conditioned on the continuous token map\n$M_s)$. The sampling process involves approximating the ODE in Eq. 3.6 using numerical methods.\nThe most common approach for this approximation is the forward Euler method, which discretizes\nthe continuous ODE into finite steps:\n$\\qquad F_{t+\\Delta t}^{(S)} = F_t^{(S)} + \\Delta t v_{\\theta} (F_t^{(S)}, t | cond = M_s),\\qquad\\qquad(3.9)$\nwhere $t \\in \\{0, \\frac{1}{N}, ..., \\frac{N-1}{N} \\}$ and $\\Delta t = \\frac{1}{N}$ is the step size, N is the total number of simulation\nsteps, $F_t^{(S)}$ represents the state at time t, and $v_{\\theta}(F_t^{(S)}, t)$ is the velocity predicted by our trained\nmodel. The sampling process starts at t = 0 and iteratively applies this update equation N times until\nreaching t = 1, thereby transforming the initial noise distribution into the desired sample distribution.\nHowever, directly using full-size token map $M_s$ to generate full-size feature map $F_1^{(S)}$ in the final\nstage is not optimal for efficiency, specifically in our stage-wise token generation case. We hope to\nfully leverage the low-resolution token maps. Therefore, to leverage our multi-stage token generation\nframework and accelerate sampling, we propose a multi-stage sampling strategy that aligns with the\nhierarchical nature of our token generation process."}, {"title": "3.4 Training Loss", "content": "The training of E-CAR involves optimizing both the multistage autoregressive token generation model\nand the multistage flow matching model. At each stage, our goal is to reconstruct the image's\nlatent representations at varying resolutions, ensuring that the model learns to generate accurate\nrepresentations progressively. Specifically, for each stage s: Continuous Token Generation: We\ngenerate the continuous token map $M_s$ using the multistage autoregressive (AR) model, as described\nin Sec. 3.2. This token map serves as the conditioning information for the flow model at stage s.\nImage Latent Reconstruction: We reconstruct the image's latent representation $F_1$, at resolution\nlevel s using the multistage flow model. The reconstruction is conditioned on the continuous token\nmap $M_s$. Specifically, we sample $\\hat{F_1}^{(s)}$ by solving the ODE defined in Eq. (3.9) using the velocity\nmodel $v_{\\theta}$:\n$\\qquad \\hat{F_1}^{(s)} = ODE[v_{\\theta}](\\hat{F_0}^{(s)} | M_s),\\qquad\\qquad(3.13)$\nwhere $\\hat{F_0}^{(s)}$ is the initial noise sample at stage s, typically a downsampled noisy image. Flow\nMatching Loss Computation: We compute the flow matching loss $l_{flow}$ at stage s using the time-\ndependent interpolated latent representations $F_t$ and their derivatives:\n$\\qquad l_{flow} = \\mathbb{E}_{\\hat{F_0}^{(s)}, \\hat{F_1}^{(s)}, t \\in [t_{s-1}, t_s]} \\bigg[ |\\frac{d F_t^{(s)}}{dt} - v_{\\theta} (\\hat{F_t}^{(s)}, t | M_s)|^2 \\bigg],\\qquad\\qquad(3.14)$\nwhere $\\hat{F_1}^{(s)}$ is the ground-truth image latent at stage s, and $\\frac{d F_t^{(s)}}{dt} = F_1^{(s)} - F_0^{(s)}$ due to linear\ninterpolation (see Fig.3).\nThis multi-stage loss encourages the model to generate accurate representations at various resolutions,\nultimately leading to high-quality final outputs. To dynamically balance the contributions of different\nstages to the overall loss, we employ the GradNorm technique [Chen et al., 2018b]. This adaptive\nweighting method automatically adjusts the stage-specific weights $w_s$ during training."}, {"title": "4 Experiments", "content": "We detail the experimental setup and model configurations in Sec. 4.1, followed by an analysis of\nthe quantitative and qualitative performance of our approach in Sec. 4.2 and Sec. 4.4. Finally, we\ndemonstrate the effectiveness of each component in our model in Sec.4.5."}, {"title": "4.1 Experimental Setup", "content": "We train the image generation models on ImageNet [Deng et al., 2009] train dataset at 256\u00d7256\nresolution. The image tokenizer is from SDXL [Podell et al.], which is a VAE that transform the\nimage to the latent space with 8x reduction. The latent resolution is 32\u00d732 when the image resolution\nis 256x256. All models are trained with the settings: AdamW optimizer with learning rate of 1e-4,\nno weight decay, and the batch size of 256. We use 8 NVIDIA A100 GPUs to train each model and\nthe TF32 dataformat to accelerate the training process."}, {"title": "4.2 Quantitative Results", "content": "To quantitatively evaluate the performance of E-CAR image generation, we trained the E-CAR S,B,L\nmodels for 400K iterations. We choose two widely used model series to compare, including DiT [Pee-\nbles & Xie, 2023] and MAR [Li et al., 2024].\nThe results are shown in Table 2. The E-CAR-L model achieves a computational performance of\n5.8 TFLOPs for generating 256\u00d7256 images with classifier-free guidance (CFG), which is a mere\n9.7% of the capabilities seen in both the MAR-H and DiT-XL/2 models. E-CAR-L achieve highest\ninception score and precision among three types of model. Though E-CAR-L has a higher FID\ncompared with MAR-H and DiT-XL/2, it's important to highlight that due to time constraints, our\nE-CAR models have only undergone 400K training iterations, significantly fewer than the 7 million\niterations typical for DiT and MAR models. As shown in fig 6, at 400K iterations, the model loss\nis continuously decreasing, suggesting that further training could potentially result in a lower FID.\nOur findings demonstrate that E-CAR-L significantly diminishes computational expenses while\nmaintaining a high standard of image generation quality."}, {"title": "4.3 Inference Efficiency Analysis", "content": "To evaluate the inference efficiency of E-CAR, we deploy the model on different devices including\nNVIDIA 4090 24G GPU, Intel i9-13900K CPU with 32G memory, and Apple M2 16G on Macbook\nAir. The batchsize of inference is set to 1. The result is shown in Table 3. The generation time of\nE-CAR-L model on 4090 GPU is less than 1 second. In comparison to MAR models, E-CAR models\nachieve a substantial speedup from 63\u00d7 to 107\u00d7 on NVIDIA 4090 GPUs, and 25\u00d7 to 37\u00d7 on Intel\ni9-13900K CPUs. MAR results for the Apple M2 are not listed, as the platform is not supported and\nencountered errors during testing. When compared with DiT models, despite having a slightly higher\nnumber of model parameters than DiT models, our models still deliver considerable acceleration on\nall test devices. E-CAR models achieve 3.6\u00d7 - 4.0\u00d7 speedup on 4090 GPUs, 4.1\u00d7 - 5.5\u00d7 speedup\non i9-13900K CPU, and 4.1\u00d7 - 4.9\u00d7 speedup on Apple M2 chips.\nEfficiency of E-CAR. Here, we provide a straightforward explanation of why E-CAR is both more\nefficient and effective than traditional AR image generation models. The efficiency of E-CAR stems\nfrom three key factors: (1) Hierarchical Continuous Token Generation: As described in Sec. 3.2, our\nstage-wise approach reduces the computational complexity from O(n\u00b3) to O(n\u00b2), where n is the\nnumber of tokens. (2) Efficient Detokenization: The multistage flow in Sec.3.3 offers a more efficient\nalternative to diffusion-based approaches for transforming tokens into images. Diffusion-based\ndetokenizer needs (3) Parallel Processing: Unlike token-by-token generation, E-CAR can generate the\nentire stack of tokens for an image simultaneously. This enables parallel processing during token\nrecovery, further enhancing efficiency.\nEffectiveness in Image Generation. The effectiveness of E-CAR in image generation is rooted\nin two principles: (1) Continuous Representation: Images are inherently continuous signals. By\noperating in a continuous token space, E-CAR aligns more closely with the natural structure of images,\na widely accepted inductive bias in computer vision. It echoes the finding in [Yu et al., 2024] that\nrepresentation space is important for generative models' training. (2) Hierarchical Architecture: The\nstage-wise generation process of E-CAR mirrors the hierarchical nature of visual information, from\ncoarse structures to fine details. This approach allows the model to generate images at multiple scales\neffectively."}, {"title": "4.4 Qualitative Results", "content": "We show the generated images of E-CAR models with three different model size in Figure 4. We can\nobserve that our method is able to generate high quality images with 400K training rounds. Among"}, {"title": "4.5 Ablation Study", "content": "To show the effective of the E-CAR architecture, we replace the layers in auto-regressive model\nwith layers in denoising model, that is, only the low resolution image is sent to the next stage.\nThe results are shown in Figure 5. It shows that by using the auto-regressive model, the loss can\ndecease significantly quickly. This indicate that the auto-regressive can plan the generation process in\nhigh-level, and the denoising model is only focused on low-level denoising process."}, {"title": "5 Conclusion", "content": "We introduce E-CAR, a method enhancing continuous AR image generation efficiency with two\ninnovations: 1) stage-wise progressive token map generation that reduces computation by generating\ntokens at increasing resolutions in parallel, and 2) multistage flow-based distribution modeling that\ntransforms only partial-denoised distributions at each stage comparing to complete denoising in\nnormal diffusion models. Experiments show that E-CAR achieves comparable image quality to prior\nwork while requiring 10\u00d7 fewer FLOPs and providing a 5\u00d7 speedup for 256\u00d7256 image generation."}, {"title": "A Appendix", "content": null}, {"title": "A.1 Training More Iterations", "content": "We have extended the training of E-CAR-L to 800,000 iterations, resulting in a Fr\u00e9chet Inception\nDistance (FID) of 4.9. This represents a significant improvement compared to the 400,000 iterations,\nwhich achieved an FID of 6.5. However, we have observed that the loss has not yet converged and\ncontinues to decrease steadily. Therefore, we are continuing the training of this model for more\niterations, as we believe the results will improve further."}, {"title": "A.2 Token Generation Efficiency Analysis", "content": "For a standard self-attention transformer, the time complexity of AR generation is O($n^3$), where n is\nthe total number of image tokens.\nIn AR generation, tokens are generated one at a time. For the i-th token (1 \u2264 i \u2264 n), the model\ncomputes attention scores between the new token and all previously generated i 1 tokens. The\nattention computation scales quadratically with the sequence length, so the time complexity for\ngenerating the i-th token is O($i^2$).\nThe total time complexity is the sum over all n tokens:\n$\\qquad T_{AR} = \\sum_{i=1}^n O(i^2)$"}, {"title": null, "content": "$\\qquad = O(\\sum_{i=1}^n i^2)$\n$\\qquad = O(\\frac{n (n + 1)(2n + 1)}{6})$\n$\\qquad = O(n^3)$.\nFor a standard self-attention transformer using the multistage method with a resolution schedule, the\ntime complexity is O($n^2$).\nIn Multistage AR generation, tokens are generated in stages with increasing resolutions. Define a\nsequence of stages ($n_1, n_2, ..., n_K$), where $n_K$ is the number of tokens at stage k, and K is the total\nnumber of stages.\nAssume that the number of tokens doubles at each stage (for simplicity), starting from $n_1$ = 1:\n$\\qquad n_k = 2^{k-1}, for 1 \\le k \\le K$.\nThe cumulative number of tokens up to stage k is:\n$\\qquad S_k = \\sum_{i=1}^k n_k = 2^k - 1$.\nSince the total number of tokens is n, we have:\n$\\qquad n = S_K = 2^K - 1 \\quad K = log_2(n + 1)$.\nAt each stage k, the time complexity is proportional to $S_k^2$ (due to the quadratic scaling of attention\ncomputation):\n$\\qquad T_{MultiAR} = \\sum_{k=1}^K O(S_k^2).$"}, {"title": null, "content": "Substitute $S_k = 2^k - 1$:\n$\\qquad T_{MultiAR} = O(\\sum_{k=1}^K (2^k-1)^2)$\n$\\qquad = O(\\sum_{k=1}^K (2^{2k} - 2^{k+1} + 1))$\n$\\qquad = O(\\sum_{k=1}^K 4^k - \\sum_{k=1}^K 2^{k+1} + \\sum_{k=1}^K 1).$\nCompute each term separately and combine the sums:\n$\\qquad T_{MultiAR} = O(\\frac{4^{K+1} - 4}{3} - \\frac{2^{K+3} - 4}{1} + K + constants).\nSince K = $log_2(n - 1)$, we have:\n$\\qquad 4^{K+1} = 2^{2(K+1)} = 2^{2(log_2(n+1)+1)} = 4(n + 1)^2$.\nSimilarly:\n$\\qquad 2^{K+2} = 2^{log_2(n+1)+2} = 4(n + 1)$.\nTherefore, the dominant terms in $T_{Multiar}$ are:\n$\\qquad T_{MultiAR} = O(\\frac{4(n + 1)^2}{3} - 4(n + 1) + log_2(n + 1))$\n$\\qquad = O(n^2)$.\nComparing the time complexities:\n$\\qquad \\frac{T_{AR}}{T_{MultiAR}} = \\frac{O(n^3)}{O(n^2)} = O(n).$\nThus, the multistage AR method is faster than the AR method by a factor of O(n) when generating\nan image with n tokens. The modeling and proof are inspired by VAR [Tian et al., 2024]."}, {"title": "A.3 Upsample-Renoise in Multi-stage Flow", "content": "In the inference phase"}]}