{"title": "Speechworthy Instruction-tuned Language Models", "authors": ["Hyundong Cho", "Nicolaas Jedema", "Leonardo F. R. Ribeiro", "Karishma Sharma", "Pedro Szekely", "Alessandro Moschitti", "Ruben Janssen", "Jonathan May"], "abstract": "Current instruction-tuned language models are\nexclusively trained with textual preference\ndata and thus are often not aligned with the\nunique requirements of other modalities, such\nas speech. To better align language models\nwith the speech domain, we explore (i) prompt-\ning strategies grounded in radio-industry best\npractices and (ii) preference learning using a\nnovel speech-based preference data of 20K\nsamples, generated with a wide spectrum of\nprompts that induce varying dimensions of\nspeech-suitability and labeled by annotators\nwho listen to response pairs. Both human and\nautomatic evaluation show that both prompting\nand preference learning increase the speech-\nsuitability of popular instruction-tuned LLMs.\nInterestingly, we find that prompting and pref-\nerence learning can be additive; combining\nthem achieves the best win rates in head-to-\nhead comparison, resulting in responses that\nare preferred or tied to the base model in 76.2%\nof comparisons on average. Lastly, we share\nlexical, syntactical, and qualitative analyses to\nshowcase how each method contributes to im-\nproving the speech-suitability of generated re-\nsponses.", "sections": [{"title": "Introduction", "content": "Speech is one of our primary means of communi-\ncation and a convenient and popular mode for inter-\nacting with virtual assistants (Yang, 2004). Virtual\nassistants are a prime application of instruction-\ntuned language models (ITLM), as both seek to pro-\nvide helpful responses to user requests (Peng et al.,\n2023; Chung et al., 2022; Wang et al., 2022a,b; Wei\net al., 2021; Sanh et al., 2022; Zhou et al., 2023).\nHowever, current ITLMs are fine-tuned on textual\ninstructions (Peng et al., 2023; Chung et al., 2022;\nWang et al., 2022a,b; Wei et al., 2021; Sanh et al.,\n2022; Zhou et al., 2023) and preferences obtained\nWe hypothesize that user preferences for speech\nand text are different and that the shift from gen-\nerating written text to spoken language may pose\na challenge for ITLMs (Gonz\u00e1lez et al., 2021).\nSpeech is serial and transient; speech processing\nis strictly linear (Flowerdew, 1994) and requires\nhigher cognitive load than reading (Thompson and\nRubin, 1996; Osada, 2004). Concise and simple\nsentences, as seen on the right side of Figure 1, are\nthus often preferred in speech (Kern, 2008; Abel,\n2015), yet current ITLMs optimized with textual\npreference datasets (Stiennon et al., 2020; Sing-\nhal et al., 2023) produce verbose responses with\nnon-vocalizable content (left side of Figure 1).\nWe test the hypothesis that current ITLMs are"}, {"title": "Speech-Suitability of ITLM Responses", "content": "We initiate our study on the speech-suitability of\nITLM responses by answering these questions:\n(i) \"Are ITLM responses suitable for speech-based\ninteractions?\u201d and (ii) \"If not, what makes them\nunsuitable?\u201d"}, {"title": "Radio Industry Best Practices", "content": "Kern (2008) and Abel (2015) provide a glimpse\nto the answers for these questions by illustrating\nnumerous examples of how news reporting and\nstorytelling in text differs from audio. Through\ndecades of radio experience, they establish rules-\nof-thumb to improve information delivery specific\nto the audio modality.\nWe highlight rules that generalize beyond the\nnews domain below, including: (i) use simple\nwords and sentence structures: allot a sentence\nto each idea and put the subject at the beginning\nas much as possible; (ii) do not use atypical syn-\ntax, such as \u201cPresident Bush today told mem-\nbers of congress\u201d and \u201cI today went shopping\u201d;\n(iii) avoid hyphenated adjectives (e.g. mineral-rich, tech-heavy); (iv) avoid too many names and\nnumbers; and other minor ones such as (v) avoid\ntongue twisters and (vi) avoid too much alliter-\nation. Similar principles have been echoed in\npodcasting (Dowling and Miller, 2019), multime-\ndia journalism (Kolodzy, 2012), and literature on\nlistenability (Chall and Dial, 1948; Fang, 1966;\nMesserklinger, 2006)."}, {"title": "Text vs. Speech for ITLMS", "content": "Speech guidelines from applications like radio and\npodcasting may or may not generalize to virtual as-\nsistant application, as the latter is more interactive\nand involve a nonhuman interlocutor. Therefore,\nwe verify whether best practices from radio also\napply for speech-based interactions with ITLMs by\ncomparing how listeners perceive response suitabil-\nity in different modalities."}, {"title": "Adapting ITLMs for Speech", "content": "Having established the shortcomings of ITLM re-\nsponses, now we explore how to adapt ITLMs\nto generate more speech-suitable responses. In\nthis section, we describe our approaches with two\nmain directions: prompt engineering and prefer-\nence learning."}, {"title": "Prompt Engineering", "content": "Arguably the simplest method to generate speech-\nsuitable responses is to specify an instruction or\nsystem prompt that induces the ITLM's generation\nprocess towards the desiderata of speech (Raffel\net al., 2020; Wang et al., 2022b). This is effective\nbecause most recent language models are further\nfined-tuned to follow instructions (Wei et al., 2021;\nSanh et al., 2021) and is appealing because it has\nzero or very low data requirements. Guided by\nthe rules-of-thumb from the radio industry (\u00a72.1)\nand our findings from \u00a72.2, we iteratively refined\nsystem prompts using 20 randomly sampled input\nprompts (\u00a72.2) until LLMs generated more consis-\ntently speech-worthy responses. We share our final\ndetailed system prompt in Table 4, annotated with\nmappings to desiderata listed from \u00a72.\nA simple extension to a system prompt is to\ninclude examples that enable in-context learning\n(ICL), which have shown to further improve per-\nformance across many tasks (Dong et al., 2023;\nGupta et al., 2023b). While our system prompt\nis descriptive, it can benefit a model to see actual\nexamples of speech-suitable responses. To get a\ndiverse set of in-context examples, we sample a\nfew user prompts from each prompt category in\nDolly-15K and generate responses using our de-\ntailed system prompt. Then, we edit these to use\nsimpler, more colloquial language, as well as re-\nmoving any non-vocalizable content if necessary.\nThe set of in-context examples we used are shared"}, {"title": "Preference Learning", "content": "With prompt engineering, we are prescribing what\naccounts for speech-suitability. Although these\nprompts are grounded in our findings from \u00a72.2, it\nis unlikely to an exhaustive guideline that general-\nizes to all types of prompts and audiences. There-\nfore, we investigate whether we can take a data-\ndriven approach with user preferences to learn to\ngenerate speech-worthy responses. To this end, we\ncollect a speech-based preference dataset and use\npreference learning algorithms to fine-tune models."}, {"title": "Response Sampling", "content": "Creating a reward model that robustly approx-\nimates speech-suitability requires collecting re-\nsponses of varying quality across the different di-\nmensions of speech-suitability. To diversify re-\nsponse quality, we empirically compile a set of sys-\ntem prompts that can diversify generated responses.\nWe pair these system prompts with various ITLM s\nand generation hyperparameters to synthetically\ncollect responses that follow the insights from \u00a72.2\nat varying levels of detail. We cycle through each\npair of configurations to generate responses that\nwill be compared to one another using the filtered\nuser prompts from Dolly-15K described in \u00a72.2.\nWe repeat this process until we collected 20K re-\nsponse pairs. We share further technical details of\nour sampling process in Appendix B."}, {"title": "Annotating Speech-based Preferences", "content": "Similar to the annotation setup for \u00a72.2, we only\nallow the annotators to listen to the user prompt\nand responses in order to accurately capture speech-\nbased preferences. They must listen to both com-\npletely before they're able to indicate preferences.\nTo minimize any bias introduced by listening order,\nthe compared responses are presented in random"}, {"title": "Fine-tuning", "content": "To fine-tune models with SPEECHPREF, we lever-\nage two popular preference learning methods:\nPPO (Schulman et al., 2017) and DPO (Rafailov\net al., 2024).\nFor PPO, we train a reward model that generates\na scalar score for speech-suitability given a single\nuser prompt and response pair. Following the pro-\ncedure of Ouyang et al., we add a projection layer\nto the same model architecture and model weights\nthat we plan to adapt with reinforcement learning\nand train with the pairwise binary ranking loss:\n$L_{ranking} = -log(\\sigma(r_{\\theta}(x, y_c)) \u2013 r_{\\theta}(x, y_r))$,\nwhere $r_{\\theta}(x, y)$ is the reward model's score for the\nuser prompt $x$ and the generated response $y$, given\nmodel weight $\\theta$. $y_c$ is the response chosen by the\nannotator and $y_r$ is the rejected one. We use this\ntrained reward model for PPO.\nDPO is an RL free method that learns from pref-\nerence data by learning to maximize the difference\nbetween the language model's predicted probabil-\nities of the chosen texts and those of the rejected\ntexts. It is a much simpler method than PPO, whose\ntraining stability is highly sensitive to hyperpa-\nrameters and checkpoint selection strategies. An\noverview of these methods are shown in Figure 2.\nLastly, since there is no theoretical constraints\nthat prevent us from appending the prompts that we\ndeveloped earlier in \u00a73.1, we can easily combine\nprompting and preference learning by appending\nthe prompts while fine-tuning with preferences. We\nonly need to set the fixed system prompt shown in\nthe bottom half of Figure 2 with the detailed prompt\nor the ICL prompt. Therefore, we also examine this\ncombined approach."}, {"title": "Experimental Setup", "content": null}, {"title": "Models", "content": "We conduct our experiments with Falcon 7B In-\nstruct (Almazrouei et al., 2023) and OLMO 7B In-\nstruct (Groeneveld et al., 2024) as our base models;\nhenceforth, we refer to these as Falcon and OLMO\nrespectively. We chose these models because they\nwere the best-performing ITLMs with an Apache\n2.0 license at the time of our study.\nTo simplify notations going forth, we add a suf-\nfix -PROMPT for models using the detailed prompt\nin Table 4 and ICL for those that also include\nthe in-context learning examples. Models trained\nwith SPEECHPREF using PPO/DPO add the suf-"}, {"title": "Data", "content": "The user prompts that we use for sampling re-\nsponses for preference annotations and the PPO\nstep are from the filtered version of Dolly-\n15K (Conover et al., 2023), described in \u00a72.2. This\ninstruction dataset does not have a predefined train-\ntest split, and therefore we take a 9:1 train-test split.\nIt is one of the first open source, human-written\ninstruction dataset with a permissive creative com-\nmons license. SPEECHPREF is used to train the\nreward model for PPO and for DPO training."}, {"title": "Evaluation", "content": "Our primary goal is to generate responses that are\nmore frequently and significantly preferred than\nthose from the base model. The base model refers\nto the setup of generating response from the non-\nfinetuned model with our simplest prompt \"You\nare a helpful, respectful, and honest voice assistant.\"\nWe also compare our best-performing approaches\nwith one another to discover which configuration\nleads to the best results. Another desired outcome\nis that responses from one of our approaches will be\npreferred over for the original responses (denoted\nas Original response) as the latter were collected\nthrough a textual interface that does not consider\nspeech suitability.\nSimilar to the human eval-\nuation in Zhou et al. (2023), we perform a head-\nto-head comparison of responses from the target\nmodels using the same setup as the preference an-\nnotations for SPEECHPREF (\u00a73.2.2). We count\nnegligibly better as ties while all others\nlead to a win or loss. For both OLMO and Fal-\ncon, -PROMPT, -ICL, DPO, and -DPO-ICL\nvariants are compared to the base model to mea-\nsure how much stronger each is against the base\nmodel. To further solidify the relative performance,"}, {"title": "Results and Discussion", "content": null}, {"title": "Human evaluation", "content": "All techniques achieve significant win rates\nand the combined technique performs the best.\nOur human evaluation results with OLMo and Fal-\ncon are shown in Figure 3, respectively. The first\nfour rows in both figures clearly show that our tech-\nniques achieve a significantly higher win rate com-\npared to the base model, with at minimum 13.5%\nhigher win rate and a maximum of 56.7%. While\nmost results are similar, OLMO DPO-ICL gets a\nhigher win rate over the base model than OLMO-\nICL does, but the opposite is true for Falcon. To\nevaluate their relative performance, we conduct a\nhead-to-head comparison between DPO-ICL and\nICL. With this setup, we find that DPO-ICL is more"}, {"title": "Automatic evaluation", "content": "Speech-worthiness is not simply about generat-\ning shorter and more readable responses. Au-\ntomatic evaluation results are shown in Table 5,\nand we find that most results align with our ex-\npectations in that the more preferred responses are\neither shorter or easier to understand, and contains\nfewer nonvocalizable content. This trend is more\npronounced for OLMo, which originally generates\nlonger responses compared to Falcon, and thus\nhave higher chances of containing nonvocalizable\ncontent. On the other hand, Falcon already gener-"}, {"title": "Experiments with GPT-4", "content": "We also experiment with GPT-4 (gpt-4-turbo)\nto examine whether our prompts can improve"}, {"title": "Related work", "content": null}, {"title": "Language models and speech", "content": "Language models have been widely adopted for\nmodular components of a voice assistant, mainly\nfor automatic speech recognition (Yu and Deng,\n2016; Wang et al., 2020; Chiu et al., 2018), re-\nsponse generation (Cho and May, 2020; Zhou et al.,\n2022; Liu et al., 2023), response selection (Humeau\net al., 2019; Gao et al., 2020; Cho et al., 2021), and\nspeech synthesis (Tan et al., 2022; Wang et al.,\n2017; Le et al., 2023). While speech synthesis\nfocuses on how to translate text to speech such\nthat it sounds natural, our work explores how to\nbest compose the response itself for speech-based"}, {"title": "Preference learning for ITLMs", "content": "While the paradigm of pre-training and then fine-\ntuning has become the defacto status quo, there\nis still active research in how to go about fine-\ntuning to get the best results and do it efficiently.\nOne of the central methods for fine-tuning ITLMs\nis through preference learning, which is a pro-\ncess of fine-tuning models with human prefer-\nence data (Bai et al., 2022; Ethayarajh et al.,\n2022; Ouyang et al., 2022; Touvron et al., 2023).\nPPO (Schulman et al., 2017) was one of the first\nto be successfully applied to natural language pro-\ncessing in a summarization task (Stiennon et al.,\n2020), but it is difficult to apply because of the\nchallenge of attaining a reward model that is ro-\nbust to distribution shifts. DPO (Rafailov et al.,\n2024) has been proposed as an theoretically justi-\nfied RL-free alternativee and has become popular\ndue to its simplicity. Since then, many variants of\nDPO have appeared, such as SimPO (Meng et al.,\n2024), IRPO (Pang et al., 2024), KTO (Ethayarajh\net al., 2024), but examining their effectiveness for\nSPEECHPREF and developing a new preference\nlearning method is not the scope of this work."}, {"title": "Conclusion", "content": "We explore an important yet overlooked challenge\nof adapting ITLMs to compose responses that are\nspecifically designed to be verbalized, i.e., speech-\nsuitable. With rules-of-thumb of the radio indus-\ntry and through our surveys that compare the suit-\nability of a response for both text and audio, we"}, {"title": "Limitations", "content": "In this work, we focused on what ITLMs should\ngenerate for responses that will be delivered via\nspeech. However, we recognize that another inter-\nesting line of research is how the response should\nbe verbalized, where factors related to speech, such\nas timber, pitch, and speed, are important. In ad-\ndition, our examination is focused on single-turn\ninteractions, but another intriguing realistic dimen-\nsion to suitability of a response in speech is multi-\nturn interactions. It would be interesting to com-\npare the effect of delivering information at various\ngranularities and how different types of follow-up\nquestions on the user experience with a ITLMs in\nspeech-based interactions. We leave these lines of\nresearch to future work.\nIn addition, we were limited to the set of models\nthat we investigated due to legal constraints that\nprevented us from experimenting with open-source\nmodels without an Apache 2.0 license. We look\nforward to the community without these constraints\nto the next steps in this line of research for further\nimproving speech suitability in ITLMs."}, {"title": "Broader Impact", "content": "Since those who cannot read due to illiteracy or\nblindness rely on voice assistants to interface with\nmodern technology, expanding their capabilities\ncan directly lead to improvements in their standard\nof living. However, current voice assistants are\nnot as generally useful as current state-of-the-art\nITLMs. The likes of Siri and Alexa tend to fulfill\nsimple routine tasks and are brittle when facing\ncomplex requests. Therefore, adopting ITLMs as\nthe main backbones of voice assistants and adapt-"}, {"title": "Appendix", "content": null}, {"title": "In-context examples", "content": "The in-context examples that we used in our exper-\niments are shown in Table 6."}, {"title": "Response sampling details", "content": "We share the full set of system prompts that we\nuse and the process for devising them in Appendix"}, {"title": "System prompts", "content": "Our set of system prompts are presented in Table 7.\nSimple is the simplest prompt that simply adds\n\"voice\" in front of assistant. Medium and Easy are\nslightly more complex variants intended for cov-\nering different levels of voice suitability between\nSimple and Detailed. Medium focuses on\ngenerating more conversational and compact re-\nsponses while Easy is geared towards generating\ntext with high readability, hence listenability, by\nusing simple vocabulary and sentence structures.\nDetailed is the prompt where we place the most\neffort in trying to get the most consistent perfor-\nmance with GPT-4 to generate responses that we\ndeemed as most suitable for speech based on the\nlessons from audio reporting outlined in Section\n2.1."}, {"title": "Annotation Details", "content": null}, {"title": "Annotators", "content": "Before inviting annotators to a larger batch of an-\nnotations, each annotator was asked to complete\n10 annotations that were also completed by the\nauthors, which were manually evaluated for consis-\ntency and accuracy according to the given guide-\nlines. If their explanations were valid and consis-\ntent with their annotations, and the Cohen's Kappa\ncoefficient was greater than 0.6, they were invited\nto a larger batch for which quality was monitored\non a small sample for every 100 annotations that\nwere completed. If not, we provided feedback to\nthe annotators and asked them to do another 10,\nrepeating this process until the annotations met our\nstandards.\nWe paid our annotators a rate that converts to an\nhourly wage that exceeds the minimum wage from\nwhere this study was conducted."}, {"title": "Annotation interface and guidelines", "content": "The annotation interface for text vs. voice anno-\ntations is shown in Figure 6. In order to calibrate\nannotation results in this comparison task, we re-\nquired each annotator to complete both text-based\nand audio-based task, given in random order. For\nthe audio-based task, annotators can only see the\nsurvey after they completely listen to the audio-"}, {"title": "DPO vs PPO", "content": "DPO better than PPO for OLMo, tied for Falcon\nFigure 9 summarizes the result of 60\npairwise comparison results, following the same\nmethodology mentioned in \u00a74.3. Due to financial"}, {"title": "Implementation and Technical Details", "content": "For training our reward model and finetuning Fal-\ncon Instruct 7B with DPO and PPO, we use 8\nA100 GPUs. We use the LLaMA-Factory reposi-\ntory (Zheng et al., 2024), which is built on top of\npopular HuggingFace frameworks such as TRL and\nPEFT. We use the default training hyperparameters\nfor both reward modeling and PPO."}, {"title": "Qualitative Analysis", "content": "Where do our finetuned models fall short? To\nunderstand the gap between our models and state-\nof-the-art ITLMs, we examine the Falcon DPO-ICL\nvs. GPT-4-Prompt/ICL comparisons. We notice\nthat even after controlling for factual knowledge,\nthere were still instances where the responses from"}, {"title": "Falcon DPO-ICL vs. Falcon-ICL", "content": "We share sample comparisons between Falcon\nDPO-ICL and Falcon-ICL in Table 9. We show\nboth cases where Falcon DPO-ICL was preferred\nand where Falcon-ICL was preferred, categorizing\neach with a reason, and indicating how often each\nreason was mentioned as part of the evaluator's\nexplanation for their preference or was observed in\nthe responses. Both models were considered more\nsuccinct, but for different instances. Falcon DPO-\nICL was considered more conversational, which\naccounts for 28% of the preferred cases for Speech-\nFalcon. It also provided better follow-up questions,\nwhich occurred 37% of the times it was the pre-\nferred choice."}, {"title": "Falcon DPO-ICL vs. GPT-4-ICL", "content": "We share sample comparisons between Falcon\nDPO-ICL and GPT-4-ICL in Table 10. The most\nnoticeable reason for GPT-4-ICL's dominance over\nFalcon DPO-ICL was in that it was better at bal-\nancing length and actually addressing the user's\nrequest, while Falcon DPO-ICL was good at gener-\nating natural sounding text without non-vocalizable\ncontent but sometimes would not answer the re-\nquest and go directly to a follow up question."}, {"title": "Is prompt engineering with larger models sufficient for speech-suitable responses?", "content": "The dominant performance of GPT-4-ICL shown in\nFigure 4 raise the question whether larger ITLMs\nwith only prompt engineering is sufficient for reli-\nably producing speech-suitable responses. There\nare several practical arguments for why this ap-\nproach is not sufficient.\n(i) Inconsistency in following instructions: even\nGPT-4-ICL, the model with the best win rate\nover Falcon DPO-ICL, generated lists or non-\nvocalizable content for 13.6% of its responses,\ndespite being explicitly told not to do so. On\nthe other hand, Falcon DPO-ICL had only 1.2%\nsuch responses. (ii) Speech suitability is context-\ndependent: as illustrated in Section 5.1 and 5.2,\nspeech-suitability is context-dependent and not al-\nways about being more simple or concise. A naive\nsolution would be to expand the system prompt to\ncatch as many edge cases as possible, but recent\nwork has found that detailed system prompts can\nlead to worse reasoning skills, which could lead to\nworse performance in unexpected use cases (Gupta\net al., 2023a). In conclusion, these practical issues\nendorse speech-centric finetuning approaches such\nas SpeechFalcon to develop ITLMs that can more\nreliably generate responses that capture the nuances\nof speech preferences without relying on extensive\nprompt engineering."}, {"title": "Use of AI assistants", "content": "Github Copilot was used throughout the coding\nprocess for this work. None was used to assist with\nresearch or writing."}]}