{"title": "Reducing Hallucinations in Vision-Language Models via Latent Space Steering", "authors": ["Sheng Liu", "Haotian Ye", "James Zou"], "abstract": "Hallucination poses a challenge to the deployment of large vision-language models (LVLMs) in applications. Unlike in large language models (LLMs), hallucination in LVLMs often arises from misalignments between visual inputs and textual outputs. This paper investigates the underlying mechanisms of hallucination, focusing on the unique structure of LVLMs that distinguishes them from large language models (LLMs). We identify that hallucinations often arise from the sensitivity of text decoders to vision inputs, a natural phenomenon when image encoders and text decoders are pre-trained separately. Inspired by this, we introduce Visual and Textual Intervention (VTI), a novel technique designed to reduce hallucinations by steering latent space representations during inference to enhance the stability of vision features. As a task-agnostic test-time intervention, VTI can be easily applied to any problem without additional cost. Extensive experiments demonstrate that it can effectively reduce hallucinations and outperform baseline methods across multiple metrics, highlighting the critical role of vision feature stability in LVLMs.", "sections": [{"title": "Introduction", "content": "Large Vision-Language Models (LVLMs) (Liu et al., 2023b; Zhu et al., 2023; Ye et al., 2023; Li et al., 2023a; Dai et al., 2023; Gong et al., 2023; Bai et al., 2023b) have demonstrated impressive performance across various tasks such as image captioning (Li et al., 2023b; Wang et al., 2023b), visual question answering (Lee et al., 2024; Wang et al., 2024a), medical treatment planning (Liu et al., 2024b), and many more. LVLMs take advantage of both powerful vision encoders such as CLIP (Radford et al., 2021) and large language models, and can present sophisticated understanding of vision information by mapping them to the language domain where LLMs can process.\nDespite their remarkable success, LVLMs still encounter numerous challenges that impede their applications in real-world tasks, among which hallucination (Liu et al., 2023a; Lovenia et al., 2023; Leng et al., 2023b; Liu et al., 2024a; Deng et al., 2024; Zhu et al., 2024) is one prominent concern (Gunjal et al., 2023; Li et al., 2023c). Hallucination in LVLMs emerges when the generated textual responses include inaccurate descriptions of the input image (Li et al., 2023c), such as mentioning non-existing objects or characters, or providing incorrect spatial relations. Such kind of cross-modality inconsistency arises from a different mechanism from that in standard LLMs, where vision information is missing and hallucination comes solely from the linguistics level. In LVLMs, the sequential relation of information flow from the vision encoder to the text decoder can distinctly contribute to hallucination, a potentially essential question to be answered.\nWhile a few existing studies have explored the underlying mechanism in this regard, leading to different conclusions such as statistical pre-training bias (Agarwal et al., 2020; Agrawal et al., 2016;"}, {"title": "Related Work", "content": "Large Vision-Language Models. The success of Large Language Models (LLMs) (Gilardi et al., 2023; Touvron et al., 2023; Tay et al., 2022; Raffel et al., 2020; Brown et al., 2020; Chowdhery et al., 2022; Taori et al., 2023; Chiang et al., 2023; Bai et al., 2023a) has fueled significant advancements in Large Vision-Language Models (LVLMs)(Liu et al., 2023b; Dai et al., 2023; Zhu et al., 2023; Li et al., 2023a; Ye et al., 2023; Bai et al., 2023b). By incorporating visual encoders and feature projectors (Li et al., 2019; Sun et al., 2019; Wang et al., 2022; Li et al., 2022), LVLMs have achieved notable improvements across a variety of multimodal tasks, including image captioning (Li et al., 2023b), visual question answering (Zhang et al., 2023a), and image segmentation (Lai et al., 2024). However, similar to LLMs, LVLMs are susceptible to generating hallucinations (Li et al., 2023c), which hinder their robustness and reliability in real-world applications. Our work seeks to address this by mitigating hallucinations in LVLMs, thereby enhancing their utility and dependability in practical settings.\nHallucination in Vision-Language Models and LVLMs. In natural language processing, hallucination refers to the generation of false or nonsensical content (Ji et al., 2023; Zhang et al., 2023b; Shi et al., 2023). This issue has recently drawn attention in multimodal models, where hallucinations can significantly impair performance (Wang et al., 2023a; Zhao et al., 2023; Huang et al., 2023; Yue et al., 2024). To counter this, various methods have been proposed, primarily involving additional training to align models with ground truth (Gunjal et al., 2023; Liu et al., 2023a; Sun et al., 2023; Yin et al., 2023; Zhou et al., 2023; Zhai et al., 2023; Yue et al., 2024). However, such approaches often suffer from practical limitations, such as the need for extensive training and additional data. In response, training-free methods have gained traction. These approaches rely on techniques like self-feedback correction (Lee et al., 2023; Yin et al., 2023), leveraging auxiliary models for knowledge integration (Wan et al., 2024; Deng et al., 2024; Zhao et al., 2024; Yang et al., 2024; Kim et al., 2024), and refining the model's decoding process (Huang et al., 2023; Leng et al., 2023a; Favero et al., 2024; Zhang et al., 2024; Wang et al., 2024b; Liu et al., 2023c). These methods typically focus on adjusting the text decoder but are limited in addressing hallucinations originating from visual components.\nIn contrast, our approach targets hallucination reduction by directly steering the latent feature"}, {"title": "Mechanism of Hallucination in LVLMS", "content": "A large vision language model typically consists of a vision encoder and a large language model as a text decoder. Given an image v, a text input x such as a question or prompt about the image, the vision encoder first extracts visual information from the image as vision feature vectors V = {v1, v2, ..., Vn }. These vision features are then projected into the same embedding space as the text input, and both the vision and text embeddings are concatenated and passed to the text decoder to generate text outputs auto-regressively. While LVLMs are known to hallucinate, the underlying causes of this phenomenon remain unclear, particularly in relation to the vision encoder's role in the process. In this section, we explore how the stability of vision features impacts hallucination.\nWe begin by highlighting a fundamental relationship between the vision and text components of large vision-language models (LVLMs): the output of the vision encoder serves as the input to the language decoder. This sequential connection implies that the stability of the vision features plays a crucial role in the model's outputs and can influence the occurrence of hallucinations. To examine the connection between feature stability and object hallucination, we perturb raw images with various types of noise and analyze the variance in the resulting feature distributions. Ideally, noise that does not alter an image's semantic content should have minimal impact on the vision features or the model's output, provided the vision encoder is robust and trained to capture semantic information effectively.\nHowever, as shown in the left figure of Figure 2, while most vision features remain stable, approximately 15% exhibit significant variance, resulting in a long-tailed distribution. These unstable features are closely tied to hallucinations, as the model becomes overly sensitive to these features, leading to inaccuracies in its outputs.This connection is further demonstrated in the second figure"}, {"title": "Method", "content": "The experiments in the previous section demonstrate that features averaged across mildly corrupted images are robust and effective in reducing hallucinations, despite the side effects. To avoid this, inspired by works on representation engineering for LLMs (Liu et al.; Cao et al., 2024; Luo et al., 2024; Zou et al., 2023), where LLM's behavior is altered by editing the features in the latent space during inference, we develop a computationally efficient algorithm called visual and textual intervention (VTI) that can improve vision feature stability as well as text to image dependancy to reduce hallucination of LVLMs. In particular, we pre-compute the \u201cdirection\u201d of more stable features and then apply them consistently to all query examples during inference to reduce hallucination, without introducing additional training or inference cost. As sometimes hallucination rise from the text decoder, i.e. the LLM, we further obtain a textual direction and apply it to the text decoder to maximize the performance. The overview of the proposed method is illustrated in Figure 3.\nSpecifically, given a vision input v, assume that the latent states that the vision encoder takes v as the input are $h^l_it$, where l \u2208 {1, 2, . . ., L} represent the layer in the encoder, and t \u2208 {1, 2, . . ., T} represent the index of vision tokens.\nSimilar to feature averaging, we randomly mask a few patches of the input image v with m different random masks Ci, i = 1,...,m, resulting in $C_i(v)$, which are m corrupted copy of v. Intuitively, the robust latent embedding about this vision input is the averaged embedding of different random masked images, i.e. $\\hat{h}^l_t = \\frac{1}{m}\\sum_{i=1,t}^m h^l_{C_i(v)}$. Therefore, the visual direction is obtained by\n$\\Delta^l_{it} = \\hat{h}^l_{it} - h^l_{it}$\nAs the desirable visual direction should be readily applied to new image queries. To remove image-specific information in the direction vector and only keep the change that features averaging brings, we compute $\\Delta^l_{it}$ for a few examples vi and use the first principle direction of the matrix [\u0394v, \u0394v,\u00b7\u00b7\u00b7, \u0394N] (denoted as $d^{vision}_{l,t}$ dvision) to capture the main difference that averaging leads to, where N is the number of samples used. Notice that for each image, the random mask can be different.\nApart from the vision token shifting, we further introduce the textual shifting vector that steers the latent states of the text decoder when generating model outputs. Obtaining the textual shifting vector is as simple as what previous work proposed in aligning the style of LLMs; Following (Zhou et al., 2023), we curated N image captions without hallucination, denoted as x and adopted GPT model to generate the hallucinated version \u017e. As a result, we obtain paired captions with and without hallucination. We"}, {"title": "Experiments", "content": "In this section, we empirically investigate the effectiveness of VTI in reducing hallucinations. Remarkably, we use 50 examples with paired images, hallucinated and non-hallucinated responses to pre-computed the visual and textual direction and apply them to all tasks, datasets, and queries. This ensures the universality and generalizability of our results. We aim to answer the following questions: (1) Can visual intervention effectively reduce hallucination in LVLMs? (2) Can textual intervention effectively reduce hallucinations in LVLMs? (3) What is the benefit of combining them?\nExperimental Settings\nDatasets. We evaluate our model on both discriminative and generative datasets as listed below. More details about the datasets are provided in the appendix. (a) POPE: The Polling-based Object Probing Evaluation (Li et al., 2023c) contains 27,000 Yes/No questions about object existence in MSCOCO (Lin et al., 2014), where the task is to judge whether the given object is in the given image (examples are provided in Figure 7). Following existing works, we compute accuracy, precision, recall, and F1 score for each method. (b) CHAIR: Caption Hallucination Assessment with Image Relevance (Rohrbach et al., 2018) quantifies object hallucinations in image captions by comparing generated objects to ground-truth objects. Following previous works (Huang et al., 2023; Yue et al., 2024), we randomly select 500 images from the MSCOCO dataset (Lin et al., 2014) and use CHAIR1, CHAIRS, and Recall as evaluation metrics. (c) MMHAL-Bench (Sun et al., 2023): This benchmark evaluates LVLMs beyond object hallucination and contains eight question types: object attributes, adversarial objects, comparisons, counting, spatial relations, environment, holistic description, and others. We evaluate the hallucination rate and response informativeness using GPT-4.\nImplementation Details. We evaluate the effectiveness of our model on three mainstream large vision-language models, including InstructBLIP (Dai et al., 2023), LLaVA 1.5 (Liu et al., 2023b) and Qwen-VL (Bai et al., 2023b) with beam search as the default decoding strategy (denoted as Regular). We also compare our model with state-of-the-art baseline methods: OPERA (Huang et al., 2023) and VCD (Leng et al., 2023b). These methods focus on mitigating object hallucinations by improving the decoding strategy. We perform a grid search for the strength of vision and text vectors where \u03b1, \u03b2 \u2208 {0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 10.9, 1.0}. For baseline methods, we followed the settings in their papers and released code to ensure a fair comparison. More details are provided in the Appendix A."}, {"title": "Experimental Results", "content": "Results on POPE. We begin with the most extensively used benchmark for object hallucination. In Table 1, we compare various decoding-based hallucination mitigation methods on the POPE benchmark (Li et al., 2023c), where \u201cVanilla\u201d stands for the original model. Obviously, VTI outperforms the regular decoding strategies across all LVLMs consistently, resulting in the best accuracy and F1 score. In addition, VTI surpasses state-of-the-art contrastive decoding methods, demonstrating its effectiveness in mitigating object hallucinations. Notice that we average three different sub-tasks in POPE, and the comprehensive results can be found in Section B.\nResults on Open-ended Generation with CHAIR Evaluation. Beyond the \u201cYes-or-No\u201d discriminative evaluations on POPE, we validate our model on open-ended caption generation using the CHAIR benchmark (Rohrbach et al., 2018). The results in Table 2 demonstrate consistent improvement over the compared methods. Remarkably, visual shifting and textual shifting demonstrate effectiveness on different assessment dimensions: visual shifting is more effective in reducing CHAIR1 that calculates on image-level, while textual shifting is more effective in reducing CHAIR, that calculates on sentence-level. Combining them (VTI, the last row) helps incorporate their benefits and effectively reduces object hallucinations in generated captions, as evidenced by lower CHAIRS and CHAIR\u00bf scores. In addition, VTI also maintains the comprehensiveness of the generated captions, as indicated by higher recall scores.\nResults on MMHAL-Bench. To extensively test our method on various vision tasks, we benchmark VTI on the MMHAL dataset with the evaluations of eight different hallucination types. Comparison with existing methods are shown in Table 3, and a fine-grained ablation is presented in Figure 4, VTI achieves a much higher score (corresponds to less hallucination) across all categories. This underscores its effectiveness in addressing a broader range of multimodal hallucination challenges beyond objects. We also observe that textual and visual shifting are complementary. Visual shifting performs better on vision-centric tasks such as recognizing and comparing attributes of objects. In contrast, textual shifting is good at reducing hallucination of textual-centric tasks such as answering adversarial questions and counting, where relatively more complex language reasoning is required."}, {"title": "Analysis", "content": "Can visual shifting improve feature stability? As we discussed in Section 3, simply averaging vision embeddings across multiple perturbed images can reduce hallucination. VTI serves as a \u201csoft\u201d alternative to the naive averaging, and it remains unclear whether the algorithm indeed improve feature stability. To show this, we apply various types of pertubations to images, including random mask, Gaussian blur, random noise, random brightness adjustment, and random elastic transform. For each of the settings, we compute the feature variance (across 100 perturbations) and average"}, {"title": "Conclusion", "content": "In conclusion, our exploration into mitigating object hallucinations in LVLMs through latent space steering has yielded promising results. By implementing visual and textual intervention, we have significantly reduced hallucinations without compromising the models' ability to generate detailed and contextually accurate outputs. Our findings underline the importance of robust feature representation and its pivotal role in enhancing model reliability. As LVLMs continue to evolve, we believe the strategies outlined in this paper will serve as foundational steps toward creating more accurate, trustworthy, and efficient systems, fostering broader applicability in real-world scenarios."}, {"title": "Detailed Experimental Settings", "content": "In all experimental setups, the mask ratio to compute visual direction is set to 0.99, and we average across 50 random masks. For experiments on CHAIR, to maintain similar lengths of generations, we set a = 0.4 for visual intervention only, similarly \u03b2 = 0.4 for textual intervention only. For VTI, \u03b1 = 0.2 and \u03b2 = 0.4. For other experiments, we fix a = 0.9 and \u03b2 = 0.9.\nPOPE\u00b9 We utilize the official benchmark from Li et al. (2023c), which includes 3,000 question-answer pairs for each of the random, popular, and adversarial settings. We use the query template 'Is there a [object] in the image?'. Here, [object] is selected randomly, from the most frequent objects in the dataset, or from objects that frequently co-occur with [object], corresponding to the random, popular, and adversarial settings respectively. We evaluate the performance based on whether the model-generated output contained the ground truth ('Yes' or 'No') using accuracy, precision, recall, and average F1-score.\nCHAIR2 We select 500 random images from the COCO Lin et al. (2014) validation set and generate the output using the query \"Please Describe this image in detail.\". Due to the computational complexity, we restrict the max new tokens to 64. Following the M3ID Favero et al. (2024), we report two assessment metrics, CHAIR, and CHAIR\u00bf, which calculate the hallucination ratio per sentence and instance as follows:\n$\\text{CHAIR}_S = \\frac{{\\text{{sentences with hallucinated objects}}}}{{\\text{{all sentences}}}}$\n$\\text{CHAIR} = \\frac{{\\text{{hallucinated objects}}}}{{\\text{{all objects mentioned}}}} \\text{.} $\nMMHAL-Bench\u00b3 In MMHAL-Bench dataset, 96 image-question pairs, ranging in 8 question categories \u00d7 12 object topics are included. The question categories are\n\u2022 Object attribute: LVLMs incorrectly describe the visual attributes of invididual objects, such as color and shape.\n\u2022 Adversarial object: LVLMs answers questions involving something that does not exist in the image, instead of pointing out that the referred object cannot be found.\n\u2022 Comparison: LVLMs incorrectly compare the attributes of multiple objects.\n\u2022 Counting: LVLMs fail to count the number of the named objects.\n\u2022 Spatial relation: LVLMs fail to understand the spatial relations between multiple objects in the response.\n\u2022 Environment: LVLMs make wrong inference about the environment of the given image.\n\u2022 Holistic description: LVLMs make false claims about contents in the given image when giving a comprehensive and detailed description of the whole image.\n\u2022 Others: LVLMs fail to recognize the text or icons, or incorrectly reason based on the observed visual information"}, {"title": "Additional experiment results", "content": "Following Sun et al. (2023), we use GPT-4 to evaluate different methods on MMHAL-BENCH and to analyze and rate responses. Prompts we used for MMHAL-BENCH evaluation can be found in (Sun et al., 2023). The rating scales are\n\u2022 6, very informative with good analysis or reasoning, no hallucination\n\u2022 5, very informative, no hallucination\n\u2022 4, somewhat informative, no hallucination\n\u2022 3, not informative, no hallucination\n\u2022 2, very informative, with hallucination\n\u2022 1, somewhat informative, with hallucination\n\u2022 0, not informative, with hallucination\nand if the rating < 3, the generation is considered with hallucination."}]}