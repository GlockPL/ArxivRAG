{"title": "Text3DAug \u2013 Prompted Instance Augmentation for LiDAR Perception", "authors": ["Laurenz Reichardt", "Luca Uhr", "Oliver Wasenm\u00fcller"], "abstract": "LiDAR data of urban scenarios poses unique chal- lenges, such as heterogeneous characteristics and inherent class imbalance. Therefore, large-scale datasets are necessary to ap- ply deep learning methods. Instance augmentation has emerged as an efficient method to increase dataset diversity. However, current methods require the time-consuming curation of 3D models or costly manual data annotation. To overcome these limitations, we propose Text3DAug, a novel approach leveraging generative models for instance augmentation. Text3DAug does not depend on labeled data and is the first of its kind to generate instances and annotations from text. This allows for a fully automated pipeline, eliminating the need for manual effort in practical applications. Additionally, Text3DAug is sensor agnostic and can be applied regardless of the LiDAR sensor used. Comprehensive experimental analysis on LiDAR segmentation, detection and novel class discovery demonstrates that Text3DAug is effective in supplementing existing methods or as a standalone method, performing on par or better than established methods, however while overcoming their specific drawbacks. The code is publicly available.", "sections": [{"title": "I. INTRODUCTION", "content": "LiDAR sensors enable the 3D perception of environments and are crucial for applications such as autonomous naviga- tion, robotics, mapping and various industrial applications. While deep learning applications have become the de facto standard for many tasks such as LiDAR detection and segmentation, the data still poses unique challenges.\nFirstly, LiDAR data is heterogeneous, with characteristics highly dependend on the sensor. Point cloud structure and distribution varies with the number of scanlines, field of view, rotation frequency, mounting height, etc. This leads to a significant decline in performance, when deep learning methods trained on data from one sensor are applied to data from another sensor. The magnitude of this so called sensor domain gap is unique to 3D point clouds with ongoing research on how to pre-train networks on different datasets or enable multi-dataset training [1], [2].\nSecondly, data-imbalance is inherent to LiDAR point clouds, due to multiple factors. In the case of urban scenarios, large objects such as buildings are represented by more points compared to smaller objects or individuals. Due to the radiating alignment of the vertical LiDAR scanlines, point cloud density decreases with increased object distance, meaning that small objects are represented by few or no points beyond a certain distance. This results in adverse effects on network performance for long range perception [3]. This is exasperated by the fact that some objects, especially road participants such as motorcycles, are rare. Such factors result in data-imbalance in large scale datasets. For example, the SemanticKITTI dataset contains building points exceeding those representing people by a factor 709 and for motorcyclist by a factor of 16,205.\nThese challenges impose the need for large scale and diverse datasets in order to apply deep learning methods to LiDAR data, in order to obtain sufficient points for all classes. Data augmentation is a standard technique to artificially increase data diversity and in the context of LiDAR scans, instance augmentation has emerged as an effective approach to tackle data-imbalance. Specifically, training data is enriched by \u201ccut and pasting\u201d object instances (e.g. road participants for SemanticKITTI) from different scans. However, the practical application of this concept has extensive requirements. Creating instance cut-outs necessi- tate semantic and instance labels, however labeling point clouds is significantly more time-consuming when compared to image data due to additional dimensions involved [4].\nThe labeled data use for cut-out instances also has to exhibit sufficient objects of the desired class, which can be challenging due to data-imbalance, possibly requiring further data collection. Moreover, these instances retain the point structure and remission values specific to their original position, LiDAR sensor and possible occlusion. Additional factors such as the sensor domain gap, different semantic classes, or missing instance labels, mean that objects from other datasets can rarely be employed.\nIn this work, we tackle the above mentioned limitations, presenting Text3DAug as the first fully-automated and label-"}, {"title": "II. RELATED WORK", "content": "The natural imbalance in LiDAR point clouds requires extensive and varied datasets for deep learning. Simulated data has emerged as a viable alternative to real-world data acquisition. Additionally, data augmentation, including the particularly effective instance augmentation, has become a standard method for enhancing data diversity.\nA. Data Simulation\nData simulation has emerged as a natural alternative to the time-consuming and expensive process of data acquisition and labeling. Urban simulators such as SYNTHIA [5] and Carla [6] are based on game engines, while others extend existing video games [7], [8]. Simulators allow for data generation under various lighting and weather conditions, with differing dynamic object behaviour and new viewpoints, enabling the collection of diverse data for different sensor modalities. Nonetheless, simulated scenes are created with significant manual effort for 3D asset creation, realistic placement, dynamic animation and rendering. VirtualKITTI [9] and LiDARsim [10] instead use real world LiDAR scans to initialize digital twins, but again rely on manually labeled data in order to transfer object classes and positions.\nDespite its apparent benefits, a large domain gap remains between synthetic and real world data, resulting in a signif- icant performance gap [11], [12], [13]. Currently, methods using synthetic data lag behind those using a modest amount of real labeled data and even further behind those using large scale datasets [12]. So called \"sim2real\" methods attempt to map real LiDAR characteristics to synthetic data [11], [10], [14] or mix real data into the training process [13], [15]. LiDAR-Aug [16] inserts synthetic CAD models into real LiDAR pointclouds, followed by ray casting. However, LiDAR-Aug requiring the costly curation or manual creation to obtain such models. Moreover, CAD models themselves might vary in quality (poly count and detail) and in factors such as coordinate system definition. For example, the mesh axis of a CAD model does not necessary align with the real center point, varying by object class and standard (e.g. ISO8855 [17] defining the rear axle of a car as the vehicle center), requiring post processing after curation. Lastly, most simulation methods, including LiDAR-Aug do not account for the LiDAR-remission values absent from CAD models.\nB. Instance Augmentation\nInstance augmentation has been a significant step towards increasing 3D point cloud data diversity, especially of under- represented classes, an aspect crucial for safety critical ap- plications such as autonomous driving. The pioneering work of Yan et al. [18] laid the foundation, creating a database of \u201ccut and paste\u201d ground-truth instances for integration into LiDAR scans. Zhou et al. [19] extend this concept by oversampling rare class instances and adding local instance transformation in order to maximize data variance. However, \u201ccut and paste\u201d methods preserve the point distribution and structure specific to the instances original position relative to the LiDAR sensor. Due to the radiating scanlines, the point density of an object decreases with increasing distance to the sensor. Placing an instance closer or further from the sensor, results in a different point density when compared to its surroundings. These issues, in combination with the random placement of instances, leads to unrealistic representations in a LiDAR scan. Subsequent works [20], [21] add some realism by removing points occluded by the added instances, based on the range-view representation of point clouds. However, depending on the grids angular resolution, the range-view representation can lead to data loss.\nReal3DAug [22] instead precomputes placement and oc- clusion maps to identify suitable scene positions, but with significant limitations. These maps are prohibitively time-consuming and computationally expensive. Because of this, the dataset is modified once prior to training, meaning that augmentation remains identical between epochs. Besides trajectories and labelled instances, Real3DAug also requires further semantic labels for its maps, regardless of the Li- DAR perception task, as its placement strategy differentiates between different ground types such as road and sidewalks. During placement, instance orientation is computed based on estimated bounding boxes, an approximation since LiDAR data only covers the sensor-facing sides of objects [10]."}, {"title": "III. APPROACH", "content": "We identify three key observations regarding the current state-of-the-art in LiDAR instance augmentation. Firstly, \u201ccut and paste\u201d instance augmentation methods [18], [19], [20], [21], [22] depend on the availability of point-wise semantic and instance labels. Secondly, the amount of instances avail- able to these methods is limited by the size and variance of the dataset. Thirdly, obtaining CAD models instead of instance cut-outs can be time consuming and expensive, with models potentially requiring post processing and varying in quality.\nThus, we propose Text3DAug, a fully automated and label free instance augmentation pipeline, the first leveraging generative models for 3D content creation. Section III-A describes our standardized prompt recipe which we use to generate object meshes from text-to-3D models. These are then automatically post-processed, evaluated and labeled. Our instance generation engine is described in Section III- B and in Figure 2. For augmentation, meshes are randomly selected, then placed and rendered as instances in LiDAR point clouds using the algorithms described in Section III-C. This process is further elaborated in Figure 1. Text3DAug is designed to be modular, allowing its components to be modified and extended with future research developments.\nA. Prompting\nInitially we explored the generation of meshes from text- to-3D models [24], [23], [25], [26], [27] using prompts derived from the Q&A output of a Large Language Model (LLM) [28]. However, we observed that LLM generated prompts with multiple attributes such as \u201cA man wearing a hat, walking and holding an umbrella\u201d led to the generation of indistinguishable meshes across a variety of generative models. In contrast, simple prompts such as \u201ca man walking\u201d produced plausible results. Based on this observation and with efficiency in mind, we deemed the use of LLMs excessive and instead designed a fixed prompt recipe. For a desired object class, we define a data store of synonyms and (if applicable) brand-names, e.g. for the class car: sportscar, convertible, sedan, SUV, Ford, etc. From this, we build the prompt instruction and incorperate attributes like size and color to provide context. Utilizing this recipe, we randomly sample prompts such as \u201cGenerate a large purple sports car\u201d for the class car.\nOur observation that simple prompts perform better is backed by current research, in that generative models strug- gle with prompts containing concepts or multiple attributes. Specifically, this issue stems from coarse textual annotations in public datasets, with only a few attributes such as color and size [23], [25], [26], or from drawbacks in the mesh optimization process resulting in imprecise geometries [29], [30], [27]. With increased research and data in the space of generative models for 3D content, more complex prompt strategies will become possible.\nB. Instance Generation\nBy employing the aforementioned prompt recipe, we uti- lize pre-trained text-to-3D models to generate object meshes for the desired classes. These meshes are added to a database and later sampled for instance augmentation of the LiDAR scans. By pregenerating the meshes, we enable efficient augmentation during training. A notable advantage of text- to-3D models is that meshes are generated with a specific orientation. Contrary to methods such as Real3DAug [22], the orientation does not have to be estimated from partial LiDAR data. We exploit this characteristic to derive precise bounding box annotations. This is achieved by axis aligning and transferring meshes into a common coordinate system, followed by fitting the bounding box based on the axis bounds. Mesh vertices are scaled to a maximum height of one. This later allows for the randomization of instance height during placement, during which the bounding box is transformed accordingly (refer to Section III-C). For the semantic label we assign the object class that was used for prompting.\nThe mesh quality is automatically evaluated based on the CLIP score [31]. Since color information is not necessary for LiDAR data, we substitute mesh textures with shading derived from estimated surface normals, and then render the mesh from four views (front, back, both sides). The removal of textures encourages CLIP to focus on shape. CLIP similarity is determined for these four views in relation to the desired classes, as well as for the void classes such as none of these or invalid. The highest score corresponding to the prompted class is assigned as the quality value. This approach is based on the understanding that certain classes have characteristic views that are more representative or identifiable. For example, a person is best recognized from the front, while a car might be best viewed from its side. We evaluate the effectiveness of CLIP for this task in Section IV. Our instance generation engine is depicted in Figure 2."}, {"title": "C. Placement and Local Augmentation", "content": "Our Text3DAug pipeline introduces a systematic approach to instance placement in 3D point clouds. For further detail on this procedure we refer to Alg. 1. First, n random object meshes are sampled from the previously created database D, according to the desired classes C, ensuring a diverse representation of object classes. To add additional realism, we process the LiDAR data by mapping remission values according to their range, denoted as R. This approach enables the unsupervised acquisition of realistic remission values. Remission values are assigned to the mesh vertices by sampling random values from R corresponding to the range of each vertex relative to its position during placement.\nEach mesh undergoes a random local transformation, which includes height scaling within a class-dependent ap- propriate range, followed by rotation, in order to reflect the stochastic nature of real-world data. This is followed by a free-space analysis, which transforms a point cloud P and the mesh's vertices into polar coordinates. A random placement distance r of the object to the sensor is chosen, and the relative azimuth span \u2206\u03a6(r) is derived from the mesh. Our algorithm filters the polar coordinates of the point cloud P based on AP and the mesh height, delineating viable regions. Then, one of these regions is randomly chosen and the mesh is positioned within it. If no regions are found, the process is repeated with a different object distance. After a suitable region is found, we consider all points of P above and below the mesh and take their minimum z-coordinate Zmin as the estimated ground level. If no points are found, we also search within the area around the mesh, until Zmin is determined. The mesh z-positioning is then corrected with Zmin, anchoring it to the ground. We found this approach to be particularly effective, enabling a seamless integration into urban landscapes, e.g. accurately placing objects on sidewalks or inclined roads. Through this placement algo- rithm, meshes are inserted into the point cloud respecting its existing structure and spatial constraints. For the last step of realistic placement, we calculate \u03a6min and \u03a6max for each vertical scanline (ring) and use these as a limit to remove points in front of the mesh or occluded by it.\nAfter placement, instances are rendered from the meshes using ray casting based on the LiDAR sensor parameters, crucial for simulating an authentic point distribution. Re- alism is enhanced by introducing noise with the weight Wnoise and point dropout with probability Pdrop, emulating imperfections found in real-world data. Annotations L are adjusted accordingly, with the mesh class being assigned as the semantic label. For detection tasks, the bounding boxes of the instances are adjusted based on the instance's position, rotation, and height. The augmented point cloud Paug and the updated annotations Laug are passed on for network training."}, {"title": "IV. EXPERIMENTS", "content": "We conduct a series of experiments to assess the capa- bilities of our prompted Text3DAug pipeline and its com- ponents. In Section IV-A and Section IV-B, we evaluate Text3DAug as an instance augmentation method for LiDAR perception, comparing it to relevant methods on the tasks of LiDAR segmentation and detection. The label-free nature of our method lends itself to novel class discovery and we investigate whether text-generated meshes are sufficient for this task in Section IV-C. Our choice of a generative text-to- 3D model and CLIP scoring is examined in Section IV-D. Also, we investigate the scalability of our method through a trade-off between mesh quality and quantity in Section IV-F. Finally, placement and local augmentation for realistic LiDAR rendering are assessed in Section IV-E.\nFor our experiments, we identify eight instance classes (car, person, bicycle, bicyclist, motorcycle, motorcyclist, truck, and bus) which are shared or can be remapped between the SemanticKITTI [34] and NuScenes semantic segmentation datasets [35], as well as the KITTI [32] and NuScenes [36] detection datasets. We also use these for our evaluation. Unless otherwise stated, we use the original implementations for generative models, segmentation models and augmentation pipelines. Detection experiments are based on the OpenPCDet [37] framework. We use CLIP [31] with ViT-L/14 [38] for mesh evaluation. Training configurations and settings will be made available along with our code release."}, {"title": "V. CONCLUSION", "content": "In this work, we propose Text3DAug, exploiting the recent advent of generative text-to-3D models for LiDAR instance augmentation. Current instance augmentation methods rely on large datasets, labels or manual effort. Tackling these limi- tation, we design an automatic engine to generate meshes and annotations from text prompts. These are then realistically placed into LiDAR scans and rendered as instances according to the sensor characteristics. The number of instances avail- able to our method is not limited by dataset size, offering a practical and scalable approach. We show the effectiveness of Text3DAug through comprehensive evaluation on the tasks of LiDAR semantic segmentation and detection, out- performing or performing on-par with comparable methods, however without their drawbacks. We further experiment with Text3DAug for novel class discovery, demonstrating the capability to learn new classes solely from text, potentially enhancing label-efficient or unsupervised research. In future work, our modular pipeline can accommodate new generative networks or prompting techniques, developing with progress in the state of the art. Furthermore, we see potential in expanding Text3DAug to different sensor modalities such as radar [43] and various methods of depth sensing."}]}