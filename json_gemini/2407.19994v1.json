{"title": "A Study on the Implementation Method of an Agent-Based Advanced RAG System Using Graph", "authors": ["Cheonsu Jeong"], "abstract": "This study aims to improve knowledge-based question-answering (QA) systems by overcoming the limitations of existing Retrieval-Augmented Generation (RAG) models and implementing an advanced RAG system based on Graph technology to develop high-quality generative AI services. While existing RAG models demonstrate high accuracy and fluency by utilizing retrieved information, they may suffer from accuracy degradation as they generate responses using pre-loaded knowledge without reprocessing. Additionally, they cannot incorporate real-time data after the RAG configuration stage, leading to issues with contextual understanding and biased information.\nTo address these limitations, this study implemented an enhanced RAG system utilizing Graph technology. This system is designed to efficiently search and utilize information. Specifically, it employs LangGraph to evaluate the reliability of retrieved information and synthesizes diverse data to generate more accurate and enhanced responses. Furthermore, the study provides a detailed explanation of the system's operation, key implementation steps, and examples through implementation code and validation results, thereby enhancing the understanding of advanced RAG technology. This approach offers practical guidelines for implementing advanced RAG systems in corporate services, making it a valuable resource for practical application.", "sections": [{"title": "I. Introduction", "content": "Recent advancements in AI technology have brought significant attention to Generative AI. Generative AI, a form of artificial intelligence that can create new content such as text, images, audio, and video based on vast amounts of trained data models (Jeong, 2023d), is being applied in various fields, including daily conversations, finance, healthcare, education, and entertainment (Ahn & Park, 2023). As generative Al services become more accessible to the general public, the role of generative AI-based chatbots is becoming increasingly important (Adam et al., 2021; Przegalinska et al., 2019; Park, 2024). A chatbot is an intelligent agent that allows users to have conversations typically through text or voice (S\u00e1nchez-D\u00edaz et al., 2018; Jeong & Jeong, 2020). Recently, generative AI chatbots have advanced to the level of analyzing human emotions and intentions to provide responses (Jeong, 2023a). With the advent of large language models (LLMs), these chatbots can now be utilized for automatic dialogue generation and translation (Jeong, 2023b). However, they may generate responses that conflict with the latest information and have a low understanding of new problems or domains as they rely on previously trained data (Jeong, 2023c). While 2023 was marked by the release of foundational large language models (LLMs) like ChatGPT and Llama-2, experts predict that 2024 will be the year of Retrieval Augmented Generation (RAG) and AI Agents (Skelter Labs, 2024).\nHowever, there are several considerations for companies looking to adopt generative AI services. Companies must address concerns such as whether the AI can provide accurate responses based on internal data, the potential risk of internal data leakage, and how to integrate generative Al with corporate systems. Solutions include using domain-specific fine-tuned LLMs and enhancing reliability with RAG that utilizes internal information (Jung, 2024). When domain-specific information is fine-tuned on GPT-4 LLM, accuracy improves from 75% to 81%, and adding RAG can further increase accuracy to 86% (Angels et al., 2024). RAG models are known for effectively combining internal knowledge retrieval and generation to produce more accurate responses. They offer the advantages of source-based fact provision and addressing data freshness issues through the integration of internal and external knowledge bases. However, the effectiveness of RAG models heavily depends on the quality of the database, directly impacting model performance (Kim, 2024). Traditional RAG models load knowledge once and generate responses without reprocessing, leading to accuracy degradation and an inability to reflect real-time data after the RAG configuration. This process can result in inaccurate responses, particularly when generating answers to complex questions, as the initial vectorized knowledge is used without updating with new information. Furthermore, traditional RAG models struggle to handle various types of questions and may suffer from unrelated documents being used in response due to poor retrieval strategies, along with the hallucination issues observed in LLMs.\nThe purpose of this study is to improve the traditional RAG model-based knowledge-based QA system (Jeon et al., 2024) and overcome its limitations by accessing real-time data and verifying whether the retrieved documents are genuinely relevant to the questions. By implementing an enhanced RAG system capable of addressing questions about recent events and real-time data, and being less susceptible to hallucinations, this study aims to improve the quality and performance of generative AI services.\nThe introduction of this paper explains the research background and objectives, the limitations of existing RAG models, the importance and contributions of the study, and the structure of the paper. The theoretical background reviews the overview of RAG models, advanced RAG approaches, and case studies of existing research improvements. The design of the advanced RAG model covers the composition flow of advanced RAG, the configuration of Agent RAG, and other enhanced features. The implementation of the advanced RAG system details the overview and application of LangGraph, the system implementation process, and the results. The testing section presents the improved results of the implemented code. Finally, the conclusion summarizes the research findings, discusses the limitations, and outlines directions for future research."}, {"title": "II. Related Work", "content": "For this study, recent key research papers, journals, and articles related to the RAG model were reviewed. This section provides an overview of the RAG model and describes the advancements leading to the development of the Advanced RAG.\n2.1. Overview of the RAG Model\nThe RAG (Retrieval-Augmented Generation) model combines retrieval and generation to produce answers by integrating document retrieval and generation models (Lewis et al., 2020). To generate an answer to a question, the model first retrieves relevant documents and then uses them to produce the response. This process helps in generating accurate answers to questions. The RAG model can handle various types of questions effectively, even when there is a lack of specific domain knowledge. Consequently, it enhances the accuracy and consistency of information compared to traditional generative models.\nThe RAG model consists of two main stages:\nRetrieval Stage: Information relevant to the given question is retrieved through a search engine.\nGeneration Stage: Answers are generated based on the retrieved information.\n2.1.1. RAG Model Implementation Flow\nThe RAG model performs text generation tasks by retrieving information from a given source data and using that information to generate the desired text. The data processing for using RAG involves dividing the original data into smaller chunks, embedding the text data by converting it into numerical vectors, and storing these vectors in a vector store (Microsoft, 2023). The implementation flow of a generative AI service based on the RAG model"}, {"title": "2.2. Prior Research on Advanced RAG", "content": "2.2.1. Methods to Enhance RAG Performance\nThe performance of RAG (Retrieval-Augmented Generation) is influenced by the quality of the data that can be composed into prompts based on the results of question processing from external repositories. Recently, various Advanced RAG (Advanced Retrieval-Augmented Generation) methods have been proposed to address the limitations of conventional RAG. Advanced RAG represents an evolved form of the traditional RAG technique, incorporating various optimization methods to overcome its limitations. Recent research by Yunfan G. et al. introduces an optimization strategy that divides the retrieval process into Pre-Retrieval, Retrieval, and Post-Retrieval stages, significantly enhancing information accuracy and processing efficiency through optimization at each stage (Yunfan G. et al., 2024). Additionally, various improvement methods, such as re-ranking based on relevance to enhance accuracy, have been proposed as strategies for improving the quality of RAG systems (Jang Dong-jin, 2024)."}, {"title": "2.2.2. Research on Advanced RAG Types", "content": "Notable advanced RAG approaches currently being researched include the following:\nSelf-RAG: This method involves re-searching generated responses to find relevant information and using it to refine the answers. This approach can enhance the accuracy and fluency of the responses (Asai A. et al., 2023).\nCorrective RAG: This approach employs a Corrective Agent to rectify errors in generated responses. The Corrective Agent identifies errors in the responses and retrieves information to correct them, thereby improving the reliability of the answers (Yan, S.Q. et al., 2024).\nAdaptive RAG: This method involves selecting the appropriate RAG approach based on the type of question. For instance, Self-RAG may be used for factual questions, while Corrective RAG could be employed for opinion-based questions. By choosing the appropriate method according to the question type, the accuracy of the responses can be improved (Jeong, S. et al., 2024)."}, {"title": "III. Design of Advanced RAG Models", "content": "This chapter reviews various advanced RAG approaches proposed in existing research and designs an enhanced RAG system based on these methods. Specifically, Self-RAG, Corrective RAG, and Adaptive RAG approaches are closely analyzed, and improvements derived from these analyses are used to guide the implementation design.\nThe implementation of the Agent RAG system is based on Corrective RAG, with references to Self-RAG and Adaptive RAG. To enhance a typical RAG system, the workflow generally involves searching document chunks from a vector database, then using an LLM to verify whether each retrieved document chunk is relevant to the input question. If all retrieved document chunks are relevant, the system proceeds to generate a response using an LLM, similar to the standard RAG pipeline. However, if some retrieved documents are assumed to be irrelevant to the input question, the system reformulates the input query, searches for new information related to the input question, and then uses the LLM to generate a response. This workflow implements an advanced RAG system based on a graph-based approach.\n3.1. Design of Advanced RAG Execution Procedures\nThe Advanced RAG model maintains the basic flow of a traditional RAG model while incorporating additional processes after the search stage to enhance the accuracy and consistency of responses. The composition and flow of the Advanced RAG model are designed as follows:\nQuery Processing: Input the user's query, and analyze and understand the precise meaning of the query through intent recognition and analysis.\nSearch: Utilize search engines to explore and retrieve information from various sources related to the query.\nCandidate Selection: Select information from the search results that is highly relevant and reliable to the query as candidates.\nCandidate Ranking: Rank the selected candidates based on their relevance to the query, the reliability of the information, and diversity."}, {"title": "3.2. Application of Agent RAG", "content": "To apply the enhanced execution procedures, the Agent RAG framework incorporates the concept of an \"Agent\" into the answer generation process, thereby further improving the accuracy and consistency of responses. The Agent serves as a core element in the answer generation process, fulfilling the following roles:\nAnswer Evaluation: Assessing the accuracy, fluency, and reliability of the generated responses.\nAnswer Improvement: Enhancing the responses based on the evaluation results.\nInformation Retrieval: Searching for necessary information to improve the responses."}, {"title": "3.3. Application of the LangGraph Module", "content": "LangGraph is a module released by LangChain designed to build stateful multi-actor applications using LLMs. It is utilized to create Agent and multi-Agent workflows, allowing for the definition of flows that include essential cycles for most Agent architectures, and providing detailed control over the application's flow and state, which is critical for creating reliable Agents (LangGraph, 2024).\nBuilt on top of LangChain, LangGraph facilitates the development of AI agents driven by LLMs by creating essential cyclic graphs. LangGraph treats Agent workflows as cyclic graph structures. Specifically, the LangGraph Conversational Retrieval Agent offers various functionalities essential for the development of language-based AI applications, including language processing, Al model integration, database management, and graph-based data processing. It is composed of states, nodes, and edges and performs the following roles:\nComplex Workflow Management: Useful for structuring and managing state-based workflows, with clear transitions and branching for each stage.\nClear Flow Control: Allows for precise definition of relationships between nodes and edges, facilitating the implementation of complex conditional logic and state transitions.\nScalability: Enables easy addition of new nodes and edges to expand the workflow, accommodating complex systems with various conditional logic.\nThis study proposes using LangGraph, which offers a range of functionalities, as a suitable tool for implementing Agent-based Advanced RAG systems."}, {"title": "IV. Implementation Results of the Advanced RAG System", "content": "This chapter presents the implementation of the RAG model and LangChain framework based on the Advanced RAG concepts introduced in Chapter 3, utilizing data suitable for internal corporate use. The implementation approach and considerations for using LangGraph, which is well-suited for Agent implementation, are demonstrated through practical examples.\n4.1. Development Environment\nThe solutions and development platforms applied in this case are based on the framework and the implementation method utilizing LangGraph and OpenAI LLM is described. The process involves chunking and embedding documents, storing them in ChromaDB, and then transforming them into retrievers for document content search. The results are evaluated, and an Agent RAG Graph is defined and implemented accordingly. The development was carried out using Python, which provides a range of libraries necessary for Al development.\nThe development environment for each implementation component is as follows:\nOrchestration Framework: LangChain\nAgent Graph Workflow: LangGraph\nWorkflow Trace: LangSmith\nData Extraction and Chunking: LangChain Modules\nEmbedding: OpenAI\nVector Database: Chroma\nLLM: OpenAI GPT-4-turbo Model\nPython Development Environment: Google Colab\n4.2. Results of the step-by-step implementation\n4.2.1. Installation of Basic Libraries and API Key Setup\nBasic libraries, including LangChain for overall orchestration of tasks such as data splitting, OpenAI for API access, ChromaDB for storing RAG knowledge, and a web search library, were installed. To facilitate easy management of knowledge files, Google Drive was integrated with Colab. To ensure security, the keys for various modules were registered in a .env file at a specific location."}, {"title": "4.2.2. Retriever Implementation", "content": "For managing internal documents, such as 'Dress Code Standards.pdf\", the PyPDFLoader is used to load the document from the specified location. Given the document's characteristics, which include tables, the TextSplitter is adjusted. Instead of using the CharacterTextSplitter with a single delimiter (e.g., newline), the RecursiveCharacterTextSplitter is employed by adjusting the 'chunk_size' and 'chunk_overlap' parameters to efficiently maintain context. The split documents are stored in the Chroma vector store, embedded using OpenAIEmbeddings, and then converted into retrievers for search purposes."}, {"title": "4.2.3. Evaluation of Search Results", "content": "To assess whether the retrieved documents are relevant to the given question, the implementation is as shown in Figure 5."}, {"title": "4.2.4. Definition of the Agent RAG Graph", "content": "To enhance answer retrieval, the Tavily API is used for web searches, and the connection to this API is established. The Graph State of the Agent is defined, where the state object is passed to each node in the graph. Nodes such as Retrieve, generate_answer, grade_documents, and web_search_add are defined"}, {"title": "4.2.5. Implementation of the Agent RAG Graph", "content": "In the implementation phase of the Agent RAG Graph, LangGraph is used to build the Agent into a graph by utilizing the functions developed in the previous section. This involves placing the Agent into relevant nodes and connecting them with defined edges according to the specified workflow. The Agent performs an action that calls the Retrieve function and then adds output information to the state before invoking the Agent. The StateGraph class is used to define and manage the state-based graph. The provided code sets up the workflow to define the process for retrieving documents or performing other tasks based on the Agent's decisions."}, {"title": "4.3. Test Results", "content": "The implemented Agent RAG workflow was tested with various questions to improve the accuracy of the answers, and the process of streaming responses to questions can be confirmed through the stream method.\n4.3.1. Verification of Questions in RAG Knowledge Information\nWhen the question \"Tell me the things to consider when choosing a work uniform\" was input for the document 'Dress Code Standards' , which is RAG information, the question was rewritten to \"What are the main factors to consider when choosing a work uniform?\" to improve the accuracy of the answer. This resulted in a more accurate response. This process can be confirmed through the streaming of responses to questions using the stream method as shown in Figure 18."}, {"title": "4.3.2. Verification of Questions Not in RAG Knowledge Information", "content": "When the question \"Tell me what is the capital of the country where BTS is located\" was input, which is not in the RAG knowledge information, it was determined to \"perform a web search because all Vector RAG documents are irrelevant to the question.\" The incomplete initial question \"Tell me what is the capital of the country where BTS is located\" was rewritten to a complete and rewritten question \"What is the capital of South Korea, the country where BTS is from?\" and the query was made, resulting in the correct answer \"The capital of South Korea is Seoul.\""}, {"title": "V. Conclusion and Discussion", "content": "This study has reviewed various methods to enhance the accuracy of RAG and explored the theoretical background of Advanced RAG models aimed at improving knowledge-based QA systems. Through the implementation of a graph-based Agent RAG system, along with specific implementation code and validation results, this research has demonstrated the feasibility of an enhanced RAG system.\nThe proposed graph-based Advanced RAG system offers a novel approach that significantly improves RAG performance, addressing the limitations of existing RAG models. The experimental results show that this system markedly enhances the accuracy and relevance of responses to user queries. The utilization of LangGraph's graph technology has effectively assessed the reliability of information, contributing to the improvement of information quality through question rewriting and web search optimization. By enhancing real-time data accessibility and strengthening the system's ability to handle various types of questions, the LangGraph-based method has expanded the potential applications of AI-driven customer support and information retrieval. These findings provide a crucial foundation for the advancement of RAG-based generative Al services.\nHowever, several limitations remain. The LangGraph-based system is optimized for specific domains, which may result in performance degradation when applied to other fields. Additionally, the system's complexity may require additional resources for implementation and maintenance. Further validation processes are necessary to ensure the accuracy and reliability of real-time data, which could impact overall system performance.\nFuture research should focus on improving the generalizability of graph-based RAG systems. Expanding the system's applicability through testing and optimization across various domains, as well as developing and validating algorithms to enhance real-time data reliability, will be essential for further performance improvements. Lastly, given the rapid advancements in RAG technology, it is crucial to not only keep pace with technological progress but also to deeply understand and continually improve how information is retrieved and how accurate and reliable answers are generated."}]}