{"title": "Towards Reinforcement Learning for Exploration of Speculative Execution Vulnerabilities", "authors": ["Evan Lai", "Wenjie Xiong", "Edward Suh", "Mohit Tiwari", "Mulong Luo"], "abstract": "Speculative attacks such as Spectre can leak secret information. Speculative execution vulnerabilities are finicky. To exploit a speculative execution attack, it requires intensive manual reverse engineering and intimate knowledge of the hardware. In this paper, we introduce SpecRL, a framework that utilizes reinforcement learning to explore speculative execution leaks in commercial-of-the shelf microprocessors.", "sections": [{"title": "I. INTRODUCTION AND BACKGROUND", "content": "Speculative execution is a powerful technique implemented in the microarchitecture of many modern pipelined micro- processors to enhance performance. By speculatively execute instructions instead of waiting, more instructions can be ex- ecuted in the processor pipeline, which improves instruction- level parallelism. One example of the performance benefit of speculative execution can be seen in the case of branch pre- dictions for conditional branches. With conditional branches, the instruction following a conditional branch is only known after the conditional branch has been resolved. In the case of a vanilla pipelined processor, the processor must stall, which is a big hit to performance. If instead, the processor speculatively executes the instructions following the conditional branch by predicting the outcome of the branch, the processor can get work done while waiting for the branch to be resolved. If the wrong path was predicted, the processor flushes the pipeline and starts executing the correct path. Instructions that are executed during a mispredicted path are called transient instructions, and the execution of transient instructions is called transient execution.\nWhile speculative execution and branch prediction are good for processor performance, they also create a critical security vulnerability. Transient instructions are problematic, as they can make unintended accesses to secrets in protected memory. From an instruction set architecture (ISA) level of abstraction, this is legit, as transient execution leaves no trace in the architectural state, making the secret unleakable. From a microarchitectural view of the processor, however, transient execution leaves traces in various parts of the microarchi- tectural state, and can thus be transmitted through a variety of side-channels. This vulnerability has been exploited most notably with the famous Spectre [1], and Meltdown [2] attacks in 2018.\nDetecting speculative vulnerabilities: vulnerability detection for speculative execution leaks can occur during one of two stages: pre-silicon (white box), or post-silicon (black box).\nFor white box testing [3]-[5], vulnerability detection is relatively straightforward, as the detector deals with the pro- cessor's RTL description, and thus has complete knowledge and control of the entire microarchitectural state.\nFor black box testing [6], on the other hand, vulnerability detection is far more convoluted, as there is no direct way to tell whether a speculative leak has occurred. Guarnieri et. al [7] developed the hardware-software contract as a method to detect speculative leaks in black box processors through relational testing. A short definition of a speculative leak using contracts is given below.\nDefinition 1: A speculative leak occurs when, given input i and input j,\n$CTrace_i = CTrace_j$ and $HTrace_i \\neq HTrace_j$,\nwhere $CTrace_i$ and $HTrace_j$ are the contract and hard- ware traces generated given a program P, an input i, and a speculation contract S, and a CTrace consists of a list of observations observable given a speculation contract S, and a HTrace consists of a list of observations observable through side-channels.\nExisting methods: The first speculative execution vulnera- bilities, Spectre and Meltdown, were found manually. This method of generating leaks is a slow and expensive process, as it requires a deep and expansive knowledge of both the instruction set architecture as well as the microarchitecture of a given processor. Since then, automated tools have been developed to search for speculative leaks, with some looking for the leak pre-silicon, and others looking post-silicon. In this paper, we focus on searching for leaks post-silicon. When looking for speculative leaks post-silicon, most of the promi- nent methods, such as Revizor [6], or Medusa [8], rely on a fuzzing-based approach in order to generate test cases. These methods, however, are limited by the huge search space and sequential nature of many speculative leaks. This is because fuzzers generate in one-shot, and scale exponentially with program size.\nThis paper: We introduce SpecRL, a novel way to ex- plore speculative execution vulnerabilities using reinforcement learning. Reinforcement learning has recently been used in computer science and security problems. Unlike traditional empirical methods which rely on expert knowledge, learning based methods generally only rely on data and training."}, {"title": "II. REINFORCEMENT LEARNING FORMULATION", "content": "Reinforcement learning typically involves an agent and an environment. The agent interacts with the environment through actions, and receives feedback through observations and rewards. Training follows the general flow below during one \"step\":\n1. An action is chosen by the agent from the action space.\n2. The learning environment responds to the action, returning a corresponding observation and reward.\n3. The agent uses the returned observation and reward to update its policy.\nFor our application, each action represents an assembly instruction, and the agent aims to build an assembly program that will trigger a speculative leak. SpecRL's environment follows the Gymnasium standard and is formulated as such:\nAction Space: The action space is a vector of instructions {10, 11, ..., in-1}, of length n, where n is the number of instructions the agent can choose from.\nObservation Space: Due to the black box nature of our environment, the observation space for SpecRL cannot simply consist of the microarchitectural state of the processor.\nInstead, we rely on performance counters and side-channels to help us glean helpful observations about our environment. More specifically, we can observe, for a given instruction sequence:\n1. HTrace, implemented with Prime+Probe.\n2. CTrace, implemented with a QEMU-based simulator.\n3. Number of branch misses (#ofBRMisses), implemented with the INT_MISC.RECOVERY_CYCLES performance counter.\n4. Number of transient micro-operations (#ofTranUOps), im- plemented as the difference between the performance counters UOPS.ISSUED_ANY and UOPS.RETIRED_SLOTS.\nIn order to find the effect each additional instruction has, we must iteratively observe the desired instruction sequence. More formally, given an instruction sequence\n$P = \\{a_0, a_1, ..., a_{n-1}\\}$\nof length n, we produce a sequence of instruction subse- quences of length 1 to n,\n$S = \\{p_0, p_1, ..., p_{n-1}\\}$,\nwhere $p_i = \\{a_0, a_1, ..., a_i\\}$. For each instruction subsequence Pi, we return the associated HTrace, Ctrace, #ofBRMisses, and #ofTranUOps for each input, as well as the index of ai in the action space. Specifically, an instruction subsequence\nPi gives $obs_i = (H, C, B, T)$,\nwhere $H = \\{h_1, h_2, ..., h_n\\}$, $C = \\{c_1, c_2, ..., c_n\\}$, $B = \\{b_1, b_2, ..., b_n\\}$, and $T = \\{t_1, t_2, ..., t_n\\}$, where hi, Ci, bi, and ti are the HTrace, CTrace, #ofBRMisses, and #ofTranUOps for the ith input, respectively. Thus, we can define the observation space as\n$OBS = \\{obs_0, obs_1, \u2026\u2026\u2026, obs_{m-1}\\}$,\nwhere m is the max instruction sequence length.\nReward Function: The reward function for SpecRL is formu- lated with the idea in mind that the agent should be rewarded for actions that cause observable transient execution. A very large reward is given if a speculative leak occurs. A smaller, but still substantial reward is given if any misspeculation occured, and a slightly larger reward compared to the mis- speculation reward is given if this misspeculation is observable through a side-channel. Corresponding negative rewards are given if the program either doesn't induce misspeculation, or the misspeculation isn't observable. A small negative reward is also given at every step to encourage conciseness.\nComplications: There are three complications that arise from this particular reinforcement learning formulation.\n1. The first complication is infinite loops. As we are allow- ing the agent to sequentially add instructions that by definition should include control flow instructions, we need to check if a given step creates an infinite loop before gathering the corresponding traces. This, however, cannot be done statically, as noted by Alan Turing with his Halting Problem [10]. We circumvent this problem dynamically, by giving the program with the added step (instruction) first to a child process, measuring how long it takes to simulate the program, and throwing away the instruction if the child process takes too long. This method also ensures that each step has a maximum trace generation time, guaranteeing that training is not stalled by an overly long loop.\n2. The second complication is the microarchitectural state of the processor. Ideally, we would like to reset the entire microarchitectural state between each obsi. This is because each obsi comes from the same instruction sequence, and we would like to make sure that each obsi agrees with each other. That is, we should be able to find instruction i's effect by looking at the difference between obsi and obsi-1. Resetting the microarchitectural state, however is not feasible with a black box processor. Thus, we only reset the most impactful microarchitectural components for our purposes, the branch predictor and the cache. By executing the instruction WBINVD and running a program filled with 50 million conditional branches before each obsi, we effectively clobber the branch predictors pattern history table (PHT) to a deterministic state and flush the cache, resetting the relevant microarchitectural components between different observations of the same program.\n3. The third complication relates to memory accesses. The instruction sequence is observed on hardware in the kernel"}, {"title": "III. CASE STUDY", "content": "The focus of this case study was to: (1) see if SpecRL could find a leak at all, and (2) examine how detection time scales with program size when using SpecRL versus a more conventional fuzzer. If SpecRL shows better scalability compared to a fuzzer, it implies a significant advantage in detecting longer, more complex leaks. We expect this to be true, as SpecRL adds instructions one at a time, as opposed to fuzzers which must generate the entire program size at once.\nSetup: We implement the reinforcement learning formulation that we have specified on an AMD Matisse CPU, the AMD Ryzen 5 3600. To do this, we created a custom environment based on the Gymnasium standard, called SpecEnv, which borrows some of the hardware and simulator infrastructure from Revizor [6]. SpecEnv was then interfaced with Ray's RLlib library, a reinforcement learning library widely used across the industry.\nAs a proof of concept, for this case study we focused on exploiting Spectre V1 type vulnerabilities, which rely on mistraining a branch predictor. Our action space reflects this goal, with a simple instruction set that consists of a subtraction instruction (SBB), a signed multiplication instruction (IMUL), a conditional branch (JNS), and an unconditional branch (JMP). As there are multiple choices for each operand for each instruction (registers, labels), the action space of the agent consists of 40 different instructions. Additionally, 20 random inputs were randomly generated at the beginning of training. Memory operation instructions are sandboxed, with all reads and writes done relative to R14, with any action that takes longer than 1 second to simulate being thrown away.\nWe use RLlib's Proximal Policy Optimization (PPO) algo- rithm formulation to train the agent using the default algorithm configuration. The only notable change in the training config- uration is the number of environment runners, which is set to 1. Typically this number is much larger, as Ray is a distributed framework. In our use case, however, the number of environ- ment runners was kept to 1 to ensure accurate observations, as multiple environments training on a single CPU will ruin any useful microarchitectural observations generated.\nAs a baseline for a state of the art fuzzer, we tested Revizor with a similar action space. For both SpecRL and Revizor, program size ranged from 10 to 150 in increments of 10.\nResults: On all program sizes, SpecRL found a leak on av- erage within 7 minutes. At larger program sizes, SpecRL was also still able to find leaks with longer instruction sequence lengths of up to 60. Additionally, as seen in Figure 2, the detection time appears only loosely correlated to program size, and at worst scales linearly, as expected. Revizor, on the other hand, has its detection time scaling roughly exponentially with program size, as seen in Figure 2. This makes sense, as given a perfectly random fuzzer, the number of test cases before a leak (which is proportional to detection time) is $a^{n-l+1}$, where n is the program size, a is the action space size, and l is the length of the leak instruction sequence. These results show that SpecRL is able to find leaks more efficiently at larger program sizes."}, {"title": "IV. FUTURE WORK", "content": "One of the more straightforward spaces for future work is in the action space. The case study was done with a very small action space that only consisted of 4 unique instructions. In reality, however, the x86 instruction set has hundreds of instructions. Reinforcement learning algorithms have historically struggled with large action spaces, so there are a number of challenges related to expanding the action space to a larger instruction set.\nAdditionally, the action space currently does not give the agent any control over the inputs that are given to the program. Currently, inputs are randomly generated, and then boosted at run time to check for speculative leaks. Although this method does work, the boosting process is quite expensive, as the program must first be analyzed to find dependencies. Including inputs in the action space could possibly speed up training, and it would also expose more of the execution environment to the agent, potentially allowing it to fully learn how to \"mistrain\u201d a branch predictor. One of the potential downsides to giving the agent control over the inputs is once again the resultant enormous action space.\nFuture work could also explore enabling distributed training, which is one of the core advantages of Ray. While the current setup performs training on a single machine, leveraging the distributed nature of Ray could parallelize training and significantly accelerate the learning process. This would allow the agent to find leaks faster, and may also enable it to find more sophisticated leaks in a reasonable time."}, {"title": "V. RELATED WORK", "content": "Related work includes Revizor [6]. Revizor also uses Software-Hardware Contracts, the main difference being that Revizor uses a fuzzer with some speculative filters, whereas SpecRL uses an agent, which has the concept of sequential decision making. We believe this is a more intuitive approach to speculative execution leaks that require the \"training\" of a branch predictor. As we have shown, Revizor does not scale well with large program sizes, while SpecRL does.\nAutoCAT [11] was the first work to explore microarchi- tectural attacks using reinforcement learning. It only focused on cache timing attacks, however, and does not consider the branch predictor or any speculative aspects of the microar- chitecture. MACTA [12] uses multiagent RL for attack and detection of cache side-channel, but it also only considers cache timing channel mainly on a simulators."}, {"title": "VI. CONCLUSION", "content": "In conclusion, in this paper we have introduced SpecRL, a novel method to find speculative execution leaks using reinforcement learning. We present SpecRL's reinforcement learning formulation, and we also conduct a case study exam- ining its scalability. As speculative execution vulnerabilities continue to evolve, SpecRL gives credence to a promising direction for automating and improving microarchitectural security analysis."}]}