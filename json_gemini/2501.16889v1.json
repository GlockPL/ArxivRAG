{"title": "Extending Information Bottleneck Attribution to Video Sequences", "authors": ["Veronika Solopova", "Lucas Schmidt", "Dorothea Kolossa"], "abstract": "We introduce VIBA, a novel approach for explainable video classification by adapting Information Bottlenecks for Attribution (IBA) to video sequences. While most traditional explainability methods are designed for image models, our IBA framework addresses the need for explainability in temporal models used for video analysis. To demonstrate its effectiveness, we apply VIBA to video deepfake detection, testing it on two architectures: the Xception model for spatial features and a VGG11-based model for capturing motion dynamics through optical flow. Using a custom dataset that reflects recent deepfake generation techniques, we adapt IBA to create relevance and optical flow maps, visually highlighting manipulated regions and motion inconsistencies. Our results show that VIBA generates temporally and spatially consistent explanations, which align closely with human annotations, thus providing interpretability for video classification and particularly for deepfake detection.", "sections": [{"title": "Introduction", "content": "The rising need for explainability in video classification models has prompted research into extending interpretability methods traditionally designed for image-based models to the temporal and spatial complexities of video sequences. Existing approaches in explainable AI (XAI) often focus on static image analysis, leaving a gap in interpretability for models that incorporate dynamic, time-dependent information critical in video applications. In this study, we adapt the Information Bottleneck for Attribution (IBA) [Schulz et al., 2020] method to video sequences, offering a new VIBA (Video Information Bottleneck Attribution) approach to produce visual explanations that address both spatial and temporal dimensions. To illustrate our method, we apply VIBA in a deepfake detection task. Here, a significant challenge arises from the need to catch both subtle spatial manipulations and temporal inconsistencies, since detecting deepfakes with the human eye is becoming increasingly difficult as the quality of fake media improves.\nWe test VIBA with two model architectures: Xception for capturing spatial features and a VGG11-based optical flow model for temporal motion dynamics. Using a dataset that incorporates recent deepfake generation techniques, we demonstrate the ability of VIBA to generate relevance and optical flow maps, effectively highlighting keyframes and motion patterns relevant to model predictions. We then analyse the effect of IBA on the model performance and the consistency of the highlighted regions over the length of the video. We also compare the IBA explanations to human annotations we collected from lay and expert annotators. Our code is available in an anonymous GitHub repository 1. The trained models will be made available upon acceptance. This contribution provides a pathway to interpretable video models, supporting applications"}, {"title": "Related Work", "content": "In recent years, deep learning models have significantly improved tasks like object detection and medical imaging, yet their \u201cblack-box\u201d nature complicates interpretability, especially in sensitive areas like healthcare [Montavon et al., 2017]. Explainable AI (XAI) seeks to address this by enhancing transparency in model decision-making.\nRelevance maps are central to explainable image processing, highlighting the areas most influential to model predictions. Grad-CAM [Selvaraju et al., 2019], a widely used technique, generates heatmaps based on gradient information from a model's final layers, helping identify key features in classifier decisions. Another popular method, Layer-wise Relevance Propagation (LRP) [Binder et al., 2016], back-propagates relevance scores to assign pixel importance, clarifying the contribution of different layers [Lapuschkin et al., 2015]. Integrated Gradients [Sundararajan et al., 2017] computes attributions by integrating gradients from a baseline to the actual input, offering precise relevance insights. Perturbation-based techniques, like Occlusion Sensitivity [Zeiler and Fergus, 2013], evaluate importance by masking input regions, making them applicable as black-box methods. Information Bottlenecks for Attribution (IBA) [Schulz et al., 2020], which we used as a basis for this study, uses information bottlenecks, selectively limiting input information to assess regional importance, providing more robust explanations.\nXAI methods for video sequences are, however, a less explored field. Chen et al. [2021] propose a space-time attention network for multi-modal settings that uncovers the synergistic dynamics of audio and visual data over both space and time, by localizing where the relevant visual cues appear, and when the predicted sounds occur in videos. Lee [2024] use a transformer-based, spatiotemporal attention network (STAN) for gradient-based time-series explanations for video classification, producing salient frames of time-series data. In Vanneste et al. [2024], the authors adapt Grad-CAM to generate heatmaps highlighting regions of the video that significantly influence the model's predictions.\nAn application attracting a lot of attention for explainable models has been deepfake detection. Malolan et al. [2020] adapted model-agnostic LRP and LIME approaches for this task. Using an adversarial attacks framework, Tsigos et al. [2024] compared the performance of Grad-CAM++ [Chattopadhay et al., 2018], SHAP [Lundberg and Lee, 2017], LIME [Ribeiro et al., 2016], and SOBOL [Hart et al., 2016], where for this task LIME showed the best performance. Gowrisankar and Thing [2024] evaluate XAI for deepfakes using an adversarial attack approach. To the best of our knowledge, the benefits of IBA explanations for videos have not yet been investigated. However, multiple studies have shown that it provides a more stable estimation for images and text than similar attribution methods [Schulz et al., 2020; Demir et al., 2021; Jiang et al., 2020b]. IBA's key advantage is its ability to quantify relevance in bits, providing a clear, interpretable measure for attribution, something gradient-based methods can struggle with due to numerical instability and sensitivity to model architecture [Adebayo et al., 2020]. Additionally, IBA is a post-hoc method, applicable to any pre-trained black-box model without needing training data or internal parameters, while IBA's iterative process enhances its robustness to over-attribution and ensures that only the most important features are emphasized, while also slightly improving the generalization of the explained model [Samek et al., 2015]."}, {"title": "Methods", "content": "In the following section, we describe the processing pipeline, detail the construction of the dataset used and the trainings we carried out, and explain the proposed adaptations to IBA for motion analysis."}, {"title": "Model Architecture and Training", "content": "We structured the process into two main analysis paths, as depicted in Figure 1: the spatial analysis relies on a pre-trained Xception model, while our newly proposed temporal analysis is made possible by training a motion-artefact detection model using a VGG11-based optical flow model. This setup allows us to test IBA for both types of detection, comparing how well it explains features across different aspects of the videos.\nThe Xception model architecture was initialized with publicly available pre-trained weights from Liu [2023]. The artefact detection was trained using the VGG11 architecture on optical flow maps derived from manipulated and authentic video frames. To avoid overfitting, an early stopping mechanism was implemented based on the validation loss. The training process involved using over 10,000 pairs of frames, consisting of randomly selected keyframes and their subsequent frames from both real and deepfake videos. These pairs of frames were then converted into ca. 5000 optical flow maps."}, {"title": "Dataset Construction", "content": "For our use case implementation, we created a deepfake detection dataset\u00b2 combining many types of manipulated and authentic videos from established sources such as FaceForensics++ [R\u00f6ssler et al., 2019], Celeb-DF [Li et al., 2019], Deepfake Detection Challenge (DFDC) [Dolhansky et al., 2020], Deepfake Detection Dataset (DFD) [Dufour and Gully], DeeperForensics [Jiang et al., 2020a], FakeAVCeleb [Khalid et al., 2022], AV-Deepfake1M [Cai et al., 2023] and the Korean Deepfake Detection Dataset (KoDF) [Kwon et al., 2021]. Approximately 50 videos were sampled from each dataset, ensuring diversity across manipulation methods. The dataset was refined with 30 state-of-the-art deepfakes from flagged and attested fake YouTube videos and short clips from the TV show \"Deep Fake Neighbour Wars\". The dataset was randomly split into three parts: 70% was used to train the VGG11-based model (Train dataset), and 15% for its validation. The remaining 15% of the dataset was used to test both deepfake detection models and to evaluate the VIBA explanations (further referred to as the Evaluation dataset).\nPreprocessing followed a pipeline to standardize input data for both models. Videos were first split"}, {"title": "IBA for Motion Analysis", "content": "IBA applies the Information Bottleneck principle from Tishby et al. [2000] to enhance explainability in neural networks [Schulz et al., 2020]. The main concept is to insert a bottleneck into the intermediate layers of the network and iteratively reduce the information flow through these layers. This is done by adding noise to feature maps, limiting the effect of certain input regions, and observing how the model's output is altered. By assessing the sensitivity of the output to various input regions, IBA identifies the areas that have the greatest influence on the final decision in the following way:\nIn a typical deep neural network, let R represent the feature map output of a given layer, with dimensions (N, C, H, W), where N is catch size, C is number of channels (or features), and H and W are height and width of the feature map. Let \\( \\epsilon \\sim N(\\mu_R, \\sigma_R) \\) be Gaussian noise with the same dimensions as R added to this feature map. \\( \\mu_R \\) and \\( \\sigma_R \\) are mean and standard deviation of R, calculated over specific axes. The modified representation Z is calculated as a linear combination of the original feature map R and the noise \\( \\epsilon \\) with the combination weighted by an attention parameter \\( \\lambda(X) \\):\n\\[ Z = \\lambda(X)R + (1 - \\lambda(X))\\epsilon, \\]\nwhere X is the input data, \\( \\lambda(X) \\) has the same dimensionality as R and is applied element-wise, controlling how much information from R is retained and \\( (1 \u2013 \\lambda(X)) \\) represents the amount of noise introduced.\nThe objective is to find the optimal \\( \\lambda(X) \\) that best retains the model's prediction capability while keeping the information flow minimal.\n\\[ \\min_{\\lambda(X)} L(\\theta, \\lambda(X)) + \\beta I(R; Z), \\]\nwhere \\( L(\\theta, \\lambda(X)) \\) is the model's loss function (e.g., cross-entropy loss); \\( I(R; Z) \\) is the mutual information between R and Z, which quantifies how much information about R is retained in Z. Smaller \\( I(R; Z) \\) implies more noise is added and \\( \\beta \\) is a trade-off parameter controlling the balance between prediction capability and information flow.\nInjection Layer Identification\nWe identified the appropriate layer for bottleneck injection based on a balance between feature abstraction and spatial detail, guided by the methodology in Schulz et al. [2020]. The selection process was primarily qualitative, focusing on the clarity and interpretability of relevance maps generated at different stages of the model's architecture. We systematically tested several injection points within the Xception model, including block 4, bn3, conv3, block 12, and bn4 (see Table 5 and Table 6 in Appendix B for layer definitions and sizes of the Xception architecture), each representing progressively deeper layers, where the feature maps evolve from detailed spatial patterns to highly abstract representations.\nThis assessment involved generating relevance maps for the same set of real and deepfake images across these layers and visually comparing their clarity and ability to highlight subtle manipulation artefacts. Consistently, block 4 produced the most informative and human-interpretable heatmaps, as it retained fine-grained spatial details such as edges, textures, and local patterns critical for deepfake detection. Deeper layers like bn3 and block 12 continued to capture relevant information, but showed increasing abstraction of the feature maps, making the heatmaps less spatially detailed but still useful for highlighting broader patterns. Conversely, layers such as conv3 and bn4, closer to the network's final decision layers, produced highly abstract and diffused relevance maps, losing essential spatial information and becoming less informative for localized deepfake detection. You can find sample frame relevance maps from each injection block in Appendix E, Figures 4 to 8.\nIn repeating these experiments with the VGG11 model, we saw similar outcomes, where earlier layers (Layer 9 and 12) also retained finer temporal details, while deeper layers, like Layer 16, became too abstract for effective visual analysis (see sample optical flow maps from each layer in Appendix F, Figures 9 to 11). These observations reinforce the general pattern that earlier convolutional layers, where spatial and temporal details remain rich, strike a better balance between preserving interpretability and capturing essential patterns for manipulation detection.\nTherefore, based on these qualitative comparisons, we selected block 4 in the Xception model and layer 9 for VGG11 as the optimal bottleneck injection points for all subsequent experiments.\nBottleneck type selection\nA key decision for IBA-based attribution is whether to use a per-sample bottleneck or a readout bottleneck. The per-sample bottleneck fine-tunes noise injection for individual inputs, generating relevance maps tailored to specific examples, while the readout bottleneck uses a pre-trained neural network to predict noise for new inputs and is more scalable. We tested both methods using the Xception model, since both architectures share a hierarchical convolutional design, aligning with the principles of feature abstraction in CNNs, and both models also exhibited a similar pattern of optimal bottleneck placement in earlier layers.\nAfter training with Train dataset for 10 epochs, the relevance maps generated by the readout bottleneck lacked consistency and clarity in comparison to those produced by the per-sample bottleneck."}, {"title": "Evaluation", "content": "We evaluate the consistency of our explanations using three tests:\n1.  Comparative baseline testing. We verify the performance of the models on the task of our choice, with and without IBA injection on the test set, to see if the injected noise has a negative influence on the predictive accuracy. We use Accuracy, Precision, Recall and Expected Calibration Error (ECE) [Guo et al., 2017].\n2.  Saliency map consistency. To analyze the consistency of the saliency maps we produce, we chose three metrics: Intersection over Union (IoU), Temporal Consistency Score (TCS), and Region Persistence Index (RPI). We compute the mean of these metrics over the generated VIBA explanations for the test set. IoU measures the spatial overlap between two binary masks, showing how similar the highlighted regions are between consecutive frames. Scores closer to 1 indicate greater consistency.\nTCS measures the proportion of frames where regions stay consistently highlighted over time. Scores closer to 1 indicate higher consistency throughout the entire video sequence.\n\\[ TCS = \\frac{1}{N} \\sum_{i=1}^{N} \\frac{ \\sum_{x,y} M_i(x,y) }{ W \\times H }, \\]\nwhere N is the total number of frames; \\( M_i(x, y) \\) is the binary mask value for pixel (x, y) in frame i; W and H are the width and height of the image.\nRPI measures the average movement (in pixels) of the centroid of the highlighted region across frames.\n\\[ RPI = \\frac{ \\sum_{i=2}^{N} \\sqrt{ (C_{i_x} - C_{i-1_x})^2 + (C_{i_y} - C_{i-1_y})^2 } }{ \\sqrt{W^2 + H^2} }, \\]\nwhere N is the number of frames; \\( C_i \\) is the centroid of the binary mask for frame i; \\( ||C_i - C_{i-1}|| \\) is the Euclidean distance between the centroids of consecutive frames; As we have a stable size of frames (924, 924), we calculate the upper limit of the centroid movement and normalize the values from 0 to 1, with lower values indicating more consistency, and 0 - perfect stability.\n3.  Ablation testing with known artefacts. To see whether VIBA and the models are picking up on essential cues or just spurious correlations, we asked 8 annotators (two senior researchers with PhDs in Engineering, five PhD students in Computer Science, and a trained journalist) to annotate 27 deepfake videos of the Evaluation dataset with regions they found most indicative when identifying a deepfake. We used the top three most frequent answers to calculate the F1-score (macro), Precision, Recall and Overlap (Szymkiewicz-Simpson Coefficient). Additionally, we measured the percentage of cases where the most commonly selected region by human annotators (majority vote) was present in the VIBA explanation. We did not offer the annotators any real images, as they do not contain artefacts to annotate."}, {"title": "Results", "content": "Our results, presented in Table 2, show that VIBA barely alters the overall accuracy, but improves the quality of attributions. We also observe a slight improvement in the ECE of the Xception model, where VIBA might reduce overconfidence by ensuring the model's predictions are based only on the most relevant information. The Xception model shows slightly better performance than the VGG model across metrics, but overall results are comparable, which allows us to optimally evaluate VIBA explanations for two different architectures. Overall, saliency maps of VIBA explanations show slightly better consistency for real examples (see Table 3). While highlighted regions of Xception are on average more consistent over consecutive frames, the temporal consistency score of VGG11 with its optical flow maps is more than twice higher over the whole video sequence. At the same time, it is natural for different regions of deepfake manipulations to become more pronounced throughout the course of the video, and this may indicate that Xception is more sensitive to such changes.\nIn terms of the centroid movement of the highlighted regions, the explanation generated for both models seems rather stable with numbers close to 0, with slightly less consistency observable for deepfakes in the case of the VGG-based model.\nAs we can observe in Table 4, both models show moderate agreement with human annotations, overlapping with human labels in more than 50% of videos. The region most important by the majority vote of human annotations was also identified by the model in 40% of the cases for the VGG model and around 63% for Xception. Based on the higher recall of the models compared to precision, they cover a considerable amount of the regions humans marked as indicative but also highlight other regions. For instance, the most frequently identified areas by human annotators were lips and mouth area as well as brows, eyes and forehead, which is also true for explanations produced for Xception outputs.\nFor VGG11 the most frequently highlighted area is the eyes, brows and forehead, which is also the second most important for Xception. IBA explanations for VGG matched well the brows, eyes and forehead area, and visible deepfake edges. It did not attribute any importance to the chin and neck area, even when human annotators did. In terms of explanations for the Xception model, lips and mouth were identified well and better than in the case of the VGG model, while eyes, brows and forehead were also identified moderately well. It was also slightly better at identifying when regions outside of the face were suggestive of manipulations. In contrast to the VGG model, Xception explanations often focused on the chin and neck, but this often does not coincide with human annotations. Explanations for both models similarly often identified nose, ears and cheek areas, but less frequently than human annotators. In terms of qualitative findings within the defined face parts, Xception often focused on the nose and forehead area, and almost never on the eyes. Eyes and ears are highly frequent for the VGG model. Explanations for real videos and deepfakes focus on similar areas. For fake videos, the Xception model tends to focus on the forehead when analyzing the \"eyes, brows, and forehead\" zone. In contrast, for real videos, the explanations provided by the VIBA highlight the eyes more precisely. Similarly, in the \u201cnose, cheeks, and ears\" zone, the model emphasizes the nose and ears for fake videos. For real videos, however, it focuses more on the cheeks. Interestingly, in the case of the VGG model, which never focuses on the nose and chin when it comes to fake videos, VIBA does highlight these regions in real videos."}, {"title": "Discussion", "content": "In this study, we demonstrated how information bottleneck attribution can be adapted to video sequence explanations. We showed the application of this proposed VIBA approach to the timely task of deepfake detection by providing explanations fitted to two substantially different architectures. Neither of the models that we implemented to illustrate VIBA\u2014the standard Xception model and an optical-flow-based VGG11 architecture\u2014showed a statistically significant drop in performance. The near-identical recall and precision values suggest the implementation is stable and does not introduce randomness or performance degradation. The ECE even improved slightly with VIBA, indicating the model is better calibrated with more reliable probability outputs. Our analysis shows that VIBA explanations produced for both models show high temporal and spatial consistency of the saliency maps. The regions important for the human annotators are moderately well picked up by the model, while indicative features outside of the face are not prevalent, indicating that features that are important for the model but not for human annotators, are still found in the face and might still pick up artefacts the human eye misses. In our qualitative analysis, we also observed notable differences in the prevalence of important regions between real videos and deepfakes, indicating the presence of class-specific features. The optical flow maps for the VGG11-based model capture the motion patterns between frames. This approach is effective in identifying subtle temporal inconsistencies in fake videos. However, as these relevance maps focus more on motion dynamics, they are less intuitive for human interpretation than the static frame-based relevance maps obtained for the Xception model. Comparing VIBA to popular Grad-CAM explanations, Grad-CAM is limited to the final convolutional layer, which can lead to coarse explanations, as higher layers in CNNs often leverage more abstract features, while overreliance on backpropagated gradients leads to incomplete or misleading visual explanations, especially in high confidence predictions [Schulz et al., 2020]. IBA directly controls the amount of information retained in the explanation by introducing a bottleneck, ensuring that only the most relevant features for the decision are highlighted, while not being tied to CNN architectures, and thus applicable to a broader range of deep learning models. Its mechanism of perturbing activations and focusing on how information flows through the network rather than relying purely on gradients makes it additionally generalizable, producing more fine-grained heatmaps, and reducing the risk of gradient saturation. Despite its strengths, IBA has certain limitations, namely its computational complexity, and its reliance on noise to restrict information flow. While this approach provides a strong mechanism for attribution, it may not always align with human intuition of relevance, as we can see by only moderate agreement with human annotators in our case. For instance, regions containing detailed information that are not directly related to the prediction may be down-weighted by IBA, even though humans might perceive these details as important for understanding the overall frame [Hendricks et al., 2018]. XAI methods are particularly effective in human-in-the-loop (HIL) scenarios because they enable human experts to interpret model outputs, identify potential errors, and make informed decisions. This is especially relevant for deepfake detection, where nuanced judgments and context are often required to distinguish subtle manipulations."}, {"title": "Conclusion", "content": "We extended the Information Bottleneck for Attribution (IBA) method to explainable video classification, applying it to spatial (Xception) and temporal (VGG11 optical flow) models for deepfake detection. VIBA generated consistent and detailed relevance and optical flow maps, enhancing interpretability without affecting model performance, with results highlighting its versatility across architectures and potential for broader applications in video analysis tasks."}, {"title": "Ethical Statement", "content": "Our models should not be used as a stand-alone fact-checking solution, but they can be used as weak annotation tools, or as a help for human fact-checkers, with the final decision to be made by a human."}]}