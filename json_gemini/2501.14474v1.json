{"title": "The Pseudo-Dimension of Contracts", "authors": ["Paul D\u00fctting", "Michal Feldman", "Tomasz Ponitka", "Ermis Soumalias"], "abstract": "Algorithmic contract design studies scenarios where a principal incentivizes an agent to exert\neffort on her behalf. In this work, we focus on settings where the agent's type is drawn from an\nunknown distribution, and formalize an offline learning framework for learning near-optimal contracts\nfrom sample agent types. A central tool in our analysis is the notion of pseudo-dimension from\nstatistical learning theory. Beyond its role in establishing upper bounds on the sample complexity,\npseudo-dimension measures the intrinsic complexity of a class of contracts, offering a new perspective\non the tradeoffs between simplicity and optimality in contract design. Our main results provide\nessentially optimal tradeoffs between pseudo-dimension and representation error (defined as the loss\nin principal's utility) with respect to linear and bounded contracts. Using these tradeoffs, we derive\nsample- and time-efficient learning algorithms, and demonstrate their near-optimality by providing\nalmost matching lower bounds on the sample complexity. Conversely, for unbounded contracts, we\nprove an impossibility result showing that no learning algorithm exists.\nFinally, we extend our techniques in three important ways. First, we provide refined pseudo-\ndimension and sample complexity guarantees for the combinatorial actions model, revealing a novel\nconnection between the number of critical values and sample complexity. Second, we extend our\nresults to menus of contracts, showing that their pseudo-dimension scales linearly with the menu\nsize. Third, we adapt our algorithms to the online learning setting, where we show that, a polynomial\nnumber of type samples suffice to learn near-optimal bounded contracts. Combined with prior work,\nthis establishes a formal separation between expert advice and bandit feedback for this setting.", "sections": [{"title": "1 Introduction", "content": "Contract theory plays a foundational role in understanding markets for services, much like mechanism\ndesign and auctions underpin our understanding of markets for goods. The importance of this framework\nis reflected in its widespread application and recognition in economics, with the 2016 Nobel Prize awarded\nto Hart and Holmstr\u00f6m [44]. As more of the classic applications are moving online and growing in scale,\nand as new applications and opportunities arise in online markets, this mandates an algorithmic approach\nto contract theory (see, e.g., [3, 40, 24, 52]).\nA prime example of the growing relevance of contract theory in the online era is the creator economy,\nprojected to surpass half a trillion dollars by 2027 [36]. The creator economy refers to the ecosystem\nof individual content creators such as YouTubers, Twitch streamers, TikTok influencers, and indepen-\ndent writers who monetize their work through platforms that generate value for users and revenue for\nthemselves. Online platforms aim to maximize long-term growth by encouraging creators to produce\nhigh-quality content. However, creators often prioritize minimizing production costs (e.g., by producing\ncheap, click-bait content or fake news) leading to a misalignment of incentives.\nScenarios of this nature are captured by the classic hidden-action principal-agent model [41, 37], where\na principal (the platform) delegates a task (producing content) to an agent (the creator). In this model,\nthe agent chooses from a set of actions (e.g., levels of effort in content production), each associated with\na cost and a distribution over outcomes (e.g., engagement metric and audience growth) which benefit the\nprincipal. Since the principal observes only these outcomes, not the agent's actions, an asymmetry of\ninformation arises, creating a moral hazard problem: the agent has no inherent motivation to undertake\ncostly actions that align with the principal's goals. Hence, the principal has to design a contract (e.g., a\nrevenue-sharing scheme) that specifies transfers\u2014monetary rewards on observed outcomes incentivizing\nagents to exert effort."}, {"title": "1.1 Main Results: Bounds on Pseudo-Dimension and Sample Complexity", "content": "We consider the hidden-action principal-agent model with n actions and m outcomes. The agent is\ncharacterized by a private type, encoding their costs and associated outcome distributions for all potential\nactions. We assume that the agent's type is drawn from an unknown distribution D. The principal has\nsample access to D but does not know the agent's type or the actual distribution D. Our goal is to learn\na (single) contract from a prespecified contract class C that maximizes the principal's expected utility,\nwith the expectation taken over the randomness of the agent's type."}, {"title": "1.2 Extensions of the Main Results", "content": "In this section, we present extensions of our core results to other settings, highlighting the novel challenges\naddressed in this paper, the flexibility of our techniques, and their applicability to more complex and\nless-explored settings."}, {"title": "1.3 Our Techniques", "content": "Upper Bounds and Delineability. The key technique for our pseudo-dimension upper bounds is to\napply the delineability framework introduced by Balcan, Sandholm, and Vitercik [7]. They observed that,\nin related settings in mechanism design, the objective function is linear over a bounded number of regions\nof the type space and demonstrated how to leverage this structure to derive pseudo-dimension bounds.\nIn this work, we extend this idea to the contract design problem, showing that our model exhibits a\nsimilar structure to that of mechanism design. Notably, while we demonstrate that this structure holds\nfor general contracts, we find that in the case of linear contracts, the boundaries between those regions\ncorrespond precisely to the critical values [25]. While previous work has established a crucial connection\nbetween the number of critical values and the time complexity of finding optimal (linear) contracts\n[25, 29], in this work, we uncover a novel connection between the number of critical values and the\nsample complexity of learning near-optimal linear contracts.\nPseudo-Dimension Lower Bounds. One of our main technical achievements is showing that the\npseudo-dimension upper bounds from the delineability framework are near-tight. Our lower bound\nconstruction for bounded contracts proceeds in two steps: First, by considering suitable distributions\nover agent types, we prove that any e/m-approximation with 1/\u20ac < n contains a structured grid of\n(1/6)m-1 contracts (Lemma 4.8). Then, by carefully adjusting the outcome distributions and costs of\nthe different actions, we construct agent types that have high utility on any chosen subset of the grid\nand low utility elsewhere (Lemma 4.9). These observations directly yield a shattering construction.\nSample Complexity Lower Bounds. Beyond the pseudo-dimension lower bounds discussed above,\nour second technical contribution is demonstrating that pseudo-dimension yields nearly optimal bounds\non sample complexity. Our lower bounds are established through a reduction of the contract design\nproblem to distinguishing between a set of sufficiently close distributions of agent types based on samples.\nNotably, while the distributions of agent types are close in total variation distance, the corresponding\noptimal contracts differ significantly. Leveraging standard techniques from statistical learning theory, we\nestablish lower bounds for distinguishing between these distributions (Lemmas 5.8 and 5.10)."}, {"title": "1.4 Related Literature", "content": "Over several decades, a rich body of literature has developed around contract theory (see, e.g., Holmstr\u00f6m\n[41], Grossman and Hart [37]). Recently, there has been a significant focus on the computational aspects\nof contract design, as summarized in the survey of D\u00fctting, Feldman, and Talgam-Cohen [33]. For\ninstance, Babaioff, Feldman, and Nisan [3] examine the computational challenges involved in incentivizing\na group of agents to coordinate on a task. Additional work on this and other combinatorial contract\nproblems includes [26, 25, 27, 34, 29, 21, 32, 30].\nAnother important line of research explores how well simple contracts approximate the optimal utility.\nD\u00fctting, Roughgarden, and Talgam-Cohen [24] find that the best linear contracts can closely match the\nperformance of the optimal contracts under specific conditions, while Carroll [12] shows that linear\ncontracts are robust even when actions are unknown.\nA rich body of work, initiated by Castiglioni, Marchesi, and Gatti [13], Guruganesh, Schneider, and\nWang [38], and Alon, D\u00fctting, and Talgam-Cohen [1] explores the problem of optimal contract design\nfor a known distribution over agent types. Additional work on this topic includes [1, 14, 39, 2, 15, 38]. A\nkey finding of this line of work is that computing optimal deterministic contracts is typically NP-hard,\nwhile computing an (almost) optimal randomized contract is feasible. The main differentiation of our"}, {"title": "2 Contract Design Preliminaries", "content": "Throughout this work, we use [k] to denote the set {0,...,k - 1} and $\\Delta^k$ to denote the set of all\nprobability distributions over [k]."}, {"title": "2.1 The Principal-Agent Model", "content": "In this section, we define the hidden-action principal-agent model [41, 37]. The principal delegates\nthe execution of a costly project to an agent, who can choose an action from a set of actions [n] =\n{0,...,n - 1}. The set of possible outcomes is [m] = {0, ..., m \u2013 1} for m \u2265 2. Every outcome $j \\in [m]$\nis associated with a reward $r_j \\in \\mathbb{R}_{\\geq 0}$, known to the agent and the principal. We assume that the reward\nfor outcome 0 is $r_0 = 0$, and that the is some outcome $j \\in [m]$ with positive reward $r_j > 0$. The action\nchosen by the agent is hidden from the principal, who can observe only the final outcome.\nEvery action $i \\in [n]$ is associated with a private probability distribution $f_i = (f_{i,0},..., f_{i,m-1}) \\in \\Delta^m$\nover the outcomes, and also with a private cost $c_i \\in \\mathbb{R}_{>0}$ that the agent incurs for taking action i. We\nrefer to the ensemble of probability distributions $f = (f_0, ..., f_{n-1})$ as the agent's production function,\nand to $c = (c_0,..., c_{m-1})$ as the agent's costs. The agent's type is the tuple $\\theta = (f, c)$ and is known only\nby the agent. The principal observes the final outcome, but not the agent's type $\\theta$ nor the action taken\nby the agent.\nTo motivate the agent to exert effort, the principal designs a contract $t = (t_0,...,t_{m-1}) \\in \\mathbb{R}^m_{\\geq 0}$\ndetermining the agent's payment for each outcome. For a given contract t, if the agent with type\n$\\theta = (f, c)$ takes action i resulting in outcome j, the agent's utility is $t_j - c_i$ and the principal's utility is\n$r_j - t_j$. We define the agent's and principal's expected utilities for action i in contract t as:\n$u_a((f, c),t,i) = \\sum_{j\\in[m]} f_{i,j}t_j - c_i$\n$u_p((f, c), t, i) = \\sum_{j\\in[m]} f_{i,j} (r_j - t_j)$"}, {"title": "2.2 Contract Classes", "content": "An important special case of the contracts defined above is linear contracts, where the principal transfers\na fixed fraction of her reward to the agent."}, {"title": "2.3 Binary Outcome Model", "content": "A notable special case of the contracting model occurs when there are only two outcomes, i.e., m = 2,\nwith rewards r0 = 0 and r1 > 0. We refer to this case as the binary outcome model, and we say that\noutcome 0 corresponds to failure, and outcome 1 corresponds to success. This special case encompasses\nmany significant scenarios and has been extensively studied (see, e.g., [25, 29]).\nPrevious work (see, e.g., [25, 33]) made two important observations: first, that linear contracts are\nalways optimal in the binary outcome model; and second, that when restricting attention to linear\ncontracts, it is without loss of generality to consider the binary outcome model. We formalize and prove\nthese observations as Lemma A.1 in Appendix A."}, {"title": "3 Learning Theory Preliminaries", "content": "In this section, we describe a general model for learning a parameterized class of real-valued functions\nwith input space I and parameter space P. The class of functions is denoted as $F : I \\times P \\to \\mathbb{R}$, and for\nany parameter $p \\in P$, $F(\\cdot, p)$ is the function parameterized by p.\nWe describe the learning model in terms of a general class of functions F for clarity, but our focus is\non the principal's utility as a function of the agent's type parameterized by chosen contracts. Specifically,\nwe define $F = u_p$ with the input space $I = \\Theta_{all} = (\\Delta^m)^n \\times (\\{0\\} \\times \\mathbb{R}_{>0}^n)$ representing the agent's type\nspace and the parameter space $P \\subseteq C_{unbounded}$ representing the contract space.\nOur primary objective is to solve the following problem: There is some unknown distribution D\nover the agent types. We want to design efficient learning algorithms that take a limited number of\nindependent samples from Das input and return some contract t that nearly maximizes the principal's\nexpected utility $\\mathbb{E}_{\\theta \\sim D}[u_p(\\theta,t)]$ with high probability.\nIn the general model, this corresponds to the following problem: There is a distribution D over the\ninput space I, and we aim to find a parameter $p\\in P$ that nearly maximizes $\\mathbb{E}_{i\\sim D}[F(i,p)]$. Slightly\nabusing the notation, we denote $OPT(F,D) = max_{p \\in p} \\mathbb{E}_{i \\sim D}[F(i,p)]$"}, {"title": "3.1 Uniform Learnability", "content": "The first crucial concept for establishing our result is uniform learnability."}, {"title": "3.2 Pseudo-Dimension", "content": "In order to establish uniform learnability guarantees, we use the notion of the pseudo-dimension. Before\nwe define pseudo-dimension, we need to define the concept of shattering."}, {"title": "3.3 Delineability", "content": "The main technical tool that we use to derive pseudo-dimension bounds is the notion of delineability due\nto Balcan, Sandholm, and Vitercik [7]."}, {"title": "4 Pseudo-Dimension Results", "content": "In this section, we provide a tight analysis of the pseudo-dimension of the main contract classes ($C_{linear}$,\n$C_{bounded}$, $C_{unbounded}$). Moreover, by considering suitable discretizations of linear and bounded contracts\n($L_e$ for $C_{linear}$ and $B_e$ for bounded contracts), we derive the Pareto frontier between the pseudo-dimension\nand the representation error with respect to both $C_{linear}$ and $C_{bounded}$.\nFirst, in Section 4.1, we provide upper bounds on the pseudo-dimension of the full contract classes\n($C_{linear}$, $C_{bounded}$ and $C_{unbounded}$) and their discretizations ($L_e$ and $B_e$). Then, in Section 4.2, we establish\nnearly matching lower bounds, demonstrating the tightness of our results."}, {"title": "4.1 Upper Bounds on the Pseudo-Dimension", "content": "Linear Contracts. We first state the upper bound on the pseudo-dimension of the class of linear\ncontracts. Crucially, our upper bound depends on the number of critical values (see Definition 2.2),\nwhich is at most n 1 in general (see Lemma 2.3). Moreover, better bounds on the number of critical\nvalues are known for important special cases, which we discuss in Section 6.1.\nDiscretizing Linear Contracts. Our first pseudo-dimension bound in this section (Theorem 4.1)\ndepends on the number of actions, n, which can be problematic in settings with large or continuous\naction spaces. To address this, we restrict oursearch space to a suitably discretized subclass of linear\ncontracts. We begin by defining this discretized class.\nFirst, we establish a representation error guarantee (see Definition 2.5), showing that there is always\nan e-discrete linear contract at most e away from the optimal contract."}, {"title": "4.2 Lower Bounds on the Pseudo-Dimension", "content": "In this section, we establish nearly matching lower bounds for the pseudo-dimension upper bounds we\ndeveloped in Section 4.1.\nFirst, in Lemma 4.8, we show that any contract class with low representation error relative to Cbounded\nmust include contracts with a specific structured form."}, {"title": "5 Sample Complexity Results", "content": "First, in Section 5.1, we use the pseudo-dimension upper bounds established in Section 4.1 to establish\nupper bounds on the sample complexity of learning linear and bounded contracts, and demonstrate\nhow those results can be translated to sample- and time-efficient algorithms. Then, in Section 5.2,\nwe provide almost matching lower bounds for those problems by a careful reduction to the problem of\ndistinguishing between sufficiently close distributions. Thus, we demonstrate that the upper bounds\non sample complexity that we obtained in Section 5.1 using the pseudo-dimension are essentially tight.\nFinally, for the class of unbounded contracts, we establish a negative result, proving that no learning\nalgorithm can achieve finite sample complexity in this setting."}, {"title": "5.1 Upper Bounds on the Sample Complexity", "content": "In this section, we use the pseudo-dimension upper bounds established in Section 4.1 to derive sample-\nand time-efficient algorithms for learning near-optimal linear and bounded contracts.\nTo derive sample- and time-efficient algorithms, we introduce the concept of an approximation oracle,\nwhich provides a near-optimal contract for an empirical distribution of agent types. This approach\ndecouples the learning problem from the computational challenge of identifying the optimal contract\nfor a given (known) distribution. For many contract classes and distributions, such oracles can be\nconstructed [e.g., 13, 38, 1], and we will demonstrate how to leverage them effectively.\nUsing the approximation oracle of Definition 5.1, we can establish a sample complexity bound for\nany subclass of bounded contracts."}, {"title": "5.2 Lower Bounds on the Sample Complexity", "content": "In this section, we establish nearly matching lower bounds for the sample complexity of finding near-\noptimal contracts, demonstrating that the upper bounds derived using the pseudo-dimension in Sec-\ntion 5.1 are essentially tight. Finally, we prove that for the class of unbounded contracts, no learning\nalgorithm with finite sample complexity exists.\nLinear Contracts. We will reduce the problem of learning an almost optimal linear contract to that of\ndistinguishing between two sufficiently close Bernoulli distributions. We will use the following well-known\nresult, see [e.g. 51, Chapter 4]. We also provide a proof in Appendix D.\nWe establish the following sample complexity lower bound by reducing the problem of learning an\napproximately optimal linear contract to the fundamental hypothesis testing problem above.\nBound"}, {"title": "6 Extensions to Other Settings", "content": "In this section, we extend our results to the combinatorial actions setting introduced in D\u00fctting, Ezra,\nFeldman, and Kesselheim [25]. We demonstrate how pseudo-dimension enables us to leverage structural\ninsights specific to these settings, achieving improvements over the naive extension of our results to this\ndomain."}, {"title": "6.1 Combinatorial Contracts", "content": "Combinatorial Actions Model. In this section, we define the combinatorial actions model. In this\nsetting, there is a ground set of actions [\u00f1] = {0, 1, . . ., \u00f1 - 1}, and the agent can take any subset $S \\subseteq [\\tilde{n}]$\nof those actions. The agent's type (f, c) in this model comprises of a production function $f : 2^{[\\tilde{n}]} \\to \\Delta^m$\nand a cost function $\\tilde{c} : 2^{[\\tilde{n}]} \\to [0, 1]$. If the agent with type (f,c) chooses to take action set S, outcome j\noccurs with probability $f_j(S)$ and the agent incurs cost equal to $\\tilde{c}(S)$.\nThe agent with type $\\theta = (f,c)$ in the combinatorial actions model, with a set of $\\tilde{n}$ combinatorial\nactions, is equivalent to the agent with type $\\theta = (f, c)$ in the non-combinatorial setting with $n = 2^{\\tilde{n}}$\nactions, where each of the $2^{\\tilde{n}}$ possible sets of actions in the combinatorial setting corresponds to a single\naction S in the non-combinatorial setting. The transition probabilities are related as $f_{S,j} = f_j(S)$ for\nevery outcome $j \\in [m]$ and corresponding costs are $c_S = \\tilde{c}(S)$.\nGiven the equivalence of the two models via the correspondence above, we extend all definitions\nfrom the model described in Section 2 to the combinatorial actions model. In particular, we can restate\nTheorem 4.1 for the setting with combinatorial actions.\nTighter Bounds with Structure. In the combinatorial actions setting it is natural to impose some\nform of \"decreasing marginal returns\" on the structure of the agent's production function f, which in\nturn imply \"decreasing marginal returns\" on the principal's reward function."}, {"title": "6.2 Menus of Contracts", "content": "In this section, we show how pseudo-dimension can be used to derive sample-efficient algorithms for\nlearning menus of contracts. This highlights the flexibility of our approach and its applicability beyond\nthe scenarios studied in the previous sections.\nFirst, we define menus of contracts of a bounded size.\nAn agent with type $\\theta = (f,c)$ selects a contract $t^{k^*}$ and corresponding action $i^*$ to maximize her\nexpected utility. This is formalized by solving:\n$(k^*, i^*) \\in argmax \\space u_a (\\theta,t^k, i), where \\space u_a (\\theta,t^k, i) = \\sum_{j\\in[m]} f_{ij} t_j - c_i$\n$k\\in[K],i\\in[n]$\nAs is standard in the literature, the agent is assumed to break ties in favor of the principal."}, {"title": "6.3 Online Learning", "content": "In this section, we show how our results for offline learning of contracts can be extended to online learning.\nWe first define the online learning variant of our model.\nWe consider a sequence of T rounds\nfor some T> 0. In each round i = 1,..., T, the following events happen in the given order:\nThe algorithm commits to a contract $t_i \\in C$.\nAn agent type $\\Theta_i$ is drawn independently from D.\nThe algorithm observes the type $\\theta_i$.\nThe algorithm receives a reward equal to $u_p(\\theta_i, t_i)$.\nThe chosen contract in round i can depend only on the types observed in the previous round, i.e.,\n$\\theta_1,..., \\theta_{i-1}$. We define the regret of an online learning algorithm as:\n$R_T = sup_D \\frac{1}{T}(\\sum_{i=1}^T OPT(C, D) - E_{\\Theta_1,..., \\Theta_T \\sim D}  \\sum_{i=1}^T u_p (\\Theta_i, t_i))$.\nThat is, the regret is the difference between the total reward of the optimal contract for D and the total\nexpected reward of the algorithm for the worst-case distribution D."}, {"title": "7 Conclusion and Future Work", "content": "In this work, we formalize a framework for learning nearly-optimal contracts in an offline setting, where\nsamples are drawn from the agent type distribution. A central tool in our analysis is the pseudo-\ndimension, which measures the intrinsic complexity of a contract class. We leverage structural insights\non optimal contract design to derive essentially tight tradeoffs between representation error, pseudo-\ndimension and sample complexity for linear, bounded and unbounded contracts. This enables the prin-\ncipal to balance contract expressivity with sample complexity.\nWe also extend our results to the combinatorial actions model. We show refined bounds that link\nthe pseudo-dimension (and consequently sample complexity) of optimal contracts in that model to the\nnumber of critical values. Prior work had linked the number of critical values to the computational\ncomplexity of finding (near-)optimal contracts. Our work establishes a formal link between this concept\nand pseudo-dimension. As another extension we consider menus of contracts, and show that the pseudo-\ndimension scales linearly with menu size. This shows a very benign scaling behavior, when going from\nsingle contracts to menus of contracts. Last but not least, we adapt our offline framework to the online\nlearning setting under expert advice, and show polynomial sample complexity. This is in contrast to\nprior work, which showed that, with bandit feedback, exponentially many samples are required, even\nif the agent is of a fixed but unknown type. This reveals a key distinction between the two feedback\nmodels: while bandit feedback focuses on learning the distribution over outcomes for the actions of a\nsingle agent, we focus on learning the distribution over agent types.\nOur work suggests several directions for future work. A significant challenge lies in scenarios where\neven a polynomial number of samples from the agent's type distribution is infeasible. Examining the\nfew-sample regime, where the principal has access to only a constant number of samples, could provide\nmeaningful insights; see [e.g., 22, 19, 31] for an analysis of this regime in related settings. It remains\nan open question how, and under what conditions, learning algorithms can be designed to guarantee a\nconstant-factor approximation of the optimal contract with such limited data.\nAnother avenue for future research involves refining our results for the pseudo-dimension of approxi-\nmations of the class of bounded contracts. While our bounds are tight in the region where \u0454 < 1/(24\u00b7m\u00b2),\na gap remains between our lower and upper bounds in the case where \u0454 > 1/(24\u00b7 m\u00b2), leaving room for\nfurther improvement."}]}