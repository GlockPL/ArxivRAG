{"title": "Bypassing DARCY Defense: Indistinguishable Universal Adversarial Triggers", "authors": ["Zuquan Peng", "Yuanyuan He", "Jianbing Ni", "Ben Niu"], "abstract": "Neural networks (NN) classification models for Natural Language Processing (NLP) are vulnerable to the Universal Adversarial Triggers (UAT) attack that triggers a model to produce a specific prediction for any input. DARCY borrows the \"honeypot\" concept to bait multiple trapdoors, for effectively detecting the adversarial examples generated by UAT. Unfortunately, we find a new UAT generation method, called IndisUAT, which produces triggers (i.e., tokens) and uses them to craft the adversarial examples whose feature distribution is indistinguishable from that of the benign examples in a randomly-chosen category at the detection layer of DARCY. The produced adversarial examples incur the maximal loss of predicting results in the DARCY-protected models. Meanwhile, the produced triggers are effective in black-box models for text generation, text inference, and reading comprehension. Finally, the evaluation results under NN models for NLP tasks indicate that the IndisUAT method can effectively circumvent DARCY and penetrate other defenses. For example, IndisUAT can reduce the true positive rate of DARCY's detection at least 40.8% and 90.6%, and drop the accuracy at least 33.3% and 51.6% in the RNN and CNN models, respectively. IndisUAT reduces the accuracy of the BERT's adversarial defense model by at least 34.0%, and makes the GPT-2 language model to spew racist outputs even when conditioned on non-racial context.", "sections": [{"title": "1 Introduction", "content": "Textual Neural Networks (NN) classification models used in Natural Language Processing (NLP) are vulnerable to be fooled and forced to output specific results for any input by attackers with adversarial examples carefully crafted by perturbing original texts (Ebrahimi et al., 2018). It is noticeable that adversarial examples have successfully cheated the NN classification models in a large number of applications, such as fake news detection (Le et al., 2020), sentiment analysis (Pang and Lee, 2004), and spam detection (Erdemir et al., 2021).\nThe early methods of adversarial example generation are instance-based search methods, which search adversarial examples for specific inputs, but they can be easily identified by spelling detection and semantic analysis. The current methods mainly rely on learning models that learn and generate adversarial examples for various unknown discrete textual inputs, e.g., HotFlip (Ebrahimi et al., 2018), Universal Adversarial Triggers (UAT) (Wallace et al., 2019), and MALCOM (Le et al., 2020). The learning-based methods are attractive, since \u2460 they have high attack success rates and low computational overhead; \u2461 they are highly transferable from white-box models to black-box models, even if they have different tokenizations and architectures; and \u2462 they are usually effective to fool other models, e.g., reading comprehension and conditional text generation models. UAT (Wallace et al., 2019), as one of powerful learning-based attacks, can drop the accuracy of the text inference model from 89.94% to near zero by simply adding short trigger sequences (i.e., a token or a sequence of tokens) chosen from a vocabulary into the original examples. Besides, the adversarial examples generated by UAT for a Char-based reading comprehension model are also effective in fooling an ELMO-based model.\nTo defend against UAT attacks, DARCY (Le et al., 2021) has been firstly proposed. It artfully uses the \"honeypot\" concept and searches and injects multiple trapdoors (i.e., words) into a textual NN for minimizing the Negative Log-Likelihood (NLL) loss. A binary detector is trained for identifying UAT adversarial examples from the examples by using the binary NLL loss. Therefore, adversarial examples can be detected when the features of the adversarial examples match the signatures of the detection layer where the trapdoors are located.\nThe literature (Le et al., 2021) introduced two"}, {"title": "2 Background", "content": "2.1 Related work\nAdversarial Attacks in NLP. The concept of adversarial examples was first introduced by Goodfellow et al. (2015). Later, Jia and Liang (2017) found that even minor perturbations of target answers can have negative impacts on reading comprehension tasks. Thus, many generation methods of adversarial examples were proposed for different attack levels (i.e., character-level, word-level, sentence-level, and multi-level) and in different models (e.g., DNN models and pre-trained models). For example, Textfooler (Jin et al., 2020) in BERT (Devlin et al., 2019) and TextBugger (Li et al., 2019) for multi-level attacks can significantly change the outputs of DNN models. However, these methods are not universal (input-agnostic), which means that they have poor transferability. To improve the transferability, Wallace et al. (2019) propose the UAT attack that is an universal attack method for many NLP tasks such as text classification, reading comprehension, text generation, and text inference. The UAT attack is independent of the victim classification models and the position of triggers, and it only needs original data and a model that has similar effects on a victim classification model to generate word-level and character-level triggers. Thus, the UAT attack is highly transferable and resource-efficient. Subsequently, Song et al. (2021) added a semantic information processing step during the UAT generation to make UAT more consistent with"}, {"title": "2.2 Analysis of DARCY's detection", "content": "The detection performance of DARCY is outstanding due to the following reasons: \u2460 the pertinent adversarial examples drop into trapdoors and activate a trapdoor when the feature of the adversarial example matches the signature of the trapdoor, so that the adversarial examples can be captured; \u2461 the signature of each trapdoor is different from that of benign examples in the target category, and the signatures are also different between trapdoors to guarantee a low false-positive rate and the effectiveness of trapdoors; and \u2462 the detector is built from a single network, and its detection rate increases with the number of trapdoors.\nIn IndisUAT, the features of the trigger-crafted adversarial examples are similar to those of the benign examples. Therefore, these adversarial examples do not activate the trapdoors located on the"}, {"title": "3 Indistinguishable UAT", "content": "3.1 Detection Layer Estimation\nThe IndisUAT attacker can perform the following steps to estimate the distribution of outputs corresponding to benign examples on the detection layer of DARCY.\n(1) Randomly select the candidate examples from the benign examples detected by DARCY to form a set, i.e., $D_f$, where L is the randomly-chosen target class. For each example-label pair $(x_i, y_i) \\in D$, example $x_i \\& D_L$ and label $y_i \\notin L$, where $|D| = N, D^\\perp$ is a dataset belonging to $L$.\n(2) Feed the chosen data $D_f$ into $F_g$, where $F_g$ is the binary detector trained in Sec. A.1.2.\n(3) Estimate the feature distribution of the outputs on the detection layer for benign examples that do not belong to the class L, i.e., $F_{tgt} \\sim [E[F_g(x_1)],\\cdots, E[F_g(x_n)]]$, where $E[F_g(x_i)]$ is the expected output of $F_g$ with an input $x_i \\in D_f$.\n3.2 Generation of Candidate Triggers\nThe IndisUAT attacker can perform the following steps to generate candidate triggers.\n(1) Set the vocabulary set V as described in Sec. A.4.3. Set the length of a trigger (a sequence of words) N, an initial token $t_{init} \\in V$, the number of candidate triggers k, and the threshold of the cosine similarity $\\tau$. A trigger $T_1$ is initialized on line 1, Alg. 1.\n(2) For each batch in $D$, run the HotFlip method (Ebrahimi et al., 2018) on line 3 of Alg. 1 to generate the candidate tokens that are as close as possible to the class L in the feature space. The technical details are presented in Sec. A.1.3.\n(3) For each candidate token, replace $T[0]$ with the candidate token on line 4 of Alg. 1 by executing Alg. 2, and obtain an initial set of k candidate triggers. For each $i \\in [1, N - 1]$ and each initial candidate trigger, run Alg. 2 to return a set of tuples and finally get a set $T_{cand}$. Each tuple contains a candidate trigger $T$, the loss for the target prediction L, and the cosine similarity between detecting results before and after adding candidate trigger $c_{tgt}$. The key steps in Alg. 2 are as follows: \u2460 replace the id-th word of the trigger with a token to"}, {"title": "3.3 Triggers Selection and Update", "content": "The IndisUAT attacker can perform the following steps to use a two-objective optimization and select triggers that can bypass DARCY's defense and successfully attack the class L.\n(1) Filter out the candidate triggers satisfying $c_{tgt} > \\tau$ in each iteration on line 11, Alg. 1, and obtain the set of final remaining candidate triggers $T_{cand}$. It indicates that the detecting results of adversarial examples generated by adding triggers in $T_{cand}$ are similar to those of benign examples in $D_f$ for the class L, so the adversarial examples can circumvent the DARCY's trapdoors."}, {"title": "4 Principle Analysis", "content": "IndisUAT searches and selects the adversarial examples indistinguishable to benign examples in the feature space without sacrificing their attack effects, so that IndisUAT deviates from the convergence direction of adversarial examples in original UAT method and keeps the adversarial examples away from DARCY's trapdoors. Thus, the detection layer of DARCY is inactive to the adversarial"}, {"title": "5 Experimental Evaluation", "content": "5.1 Settings\nDatasets and Threshold setting. We use the same datasets as DARCY did, including Movie Reviews (MR) (Bo Pang and Lillian Lee, 2005), Binary Sentiment Treebank (SST) (Wang et al., 2018), Subjectivity (SJ) (Pang and Lee, 2004), and AG News (AG) (Zhang et al., 2015). Their detailed information is shown in Table A1, Sec. A.3. We split each dataset into $D_{train}$, $D_{attack}$, and $D_{test}$ at the ratio"}, {"title": "5.2 Effect of IndisUAT on DARCY Defense", "content": "We choose the clean model as a baseline. Table 1 shows that IndisUAT circumvents the detection of DARCY with a high probability. For the RNN and CNN models, IndisUAT has lower ACC than other attack methods. IndisUAT incurs the ACC of the RNN model at least 33.3% on all datasets below the baseline, and meanwhile reduces the TPR of the DARCY's detector at least 40.8% on all datasets. For the BERT model, the ACC drops at least 27.3%,"}, {"title": "5.3 Effect of IndisUAT on Adversarial Defense", "content": "Table 2 shows that the IndisUAT attack is at work for the adversarial defenses based on PGD, FreeAt,"}, {"title": "5.4 Effect of IndisUAT on Other Tasks", "content": "IndisUAT can be used to attack the models for text generation, text inference, and reading comprehension in addition to the text classification task. A custom attack dictionary is used to make the models much more risky and vulnerable to unknown attacks. We target many pre-trained models, adversarial trained models, and trained models to illustrate that IndisUAT is still highly transferable.\nText Generation. IndisUAT is used to generate triggers for racist, malicious speech on the GPT-2 (Radford et al., 2019) model with 117M parameters. Applying the triggers to the GPT-2 with 345M parameter model is able to generate malicious or racially charged text as shown in Table 5. The detailed results refer to Sec. A.3.\nReading Comprehension. The SQUAD dataset is used for the questions about why, who, where, when. The F1 score of the result from BiDAF (Seo et al., 2017) is set as a metric, and only a complete mismatch indicates a successful attack (Wallace et al., 2019). Table 6 shows the results,"}, {"title": "6 Conclusion", "content": "We propose a novel UAT attack that can bypass the DARCY defense called IndisUAT. IndisUAT estimates the feature distribution of benign examples and produces adversarial examples to be similar enough to the distribution estimates at the DARCY's detection layer. Meanwhile, the adversarial examples with the maximal loss of predicted results of the original model are selected to attack the model with a high success rate. Extensive experiments show that IndisUAT circumvents the DARCY defense even with decades of injected trapdoors, while reducing the accuracy of the original model, adversarial training model, and pre-training model. Beside the text classification tasks, IndisUAT is at work for other tasks, e.g., text generation, text inference, and reading comprehension. Therefore, IndisUAT is powerful and raises a warning to model builders and defenders. It is challenging to propose approaches to protect the textual NN"}, {"title": "Limitations", "content": "IndisUAT generally outperforms other attack methods for many reasons. First, IndisUAT, as an universal attack method, does not require the white-box (gradient) access and the access to the target model at the inference stage. The widespread existence of trigger sequences lowers the barrier for attackers to enter into the model. Second, the trigger search is bath-oriented in the IndisUAT method, while other attacks rely on the results of a single example, so the overall attack effect of IndisUAT is stronger than that of others. Third, the trigger search can be extended to find more powerful trigger sequences in an extended vocabulary. The time complexity of searching triggers increases linearly with the size of the vocabulary. However, this increased complexity is negligible, since Top-K, beam search, and KDTree methods can be used to speed up the search process by discarding trigger sequences with low impact on the results. If the information of the detector is fully obtained, IndisUAT is highly transferable to attack even the black-box defense models with different tokenizations and architectures."}, {"title": "Broader Impact Statement", "content": "IndisUAT inspired by FIA (He et al., 2021) uses the cosine similarity to build adversarial examples against honeypot-injected defense models. Although the IndisUAT attack is specifically designed to bypass the DARCY defense, it also provides effective ideas of adversarial examples generation to circumvent similar detection and defense mechanisms. The vulnerability of the learning model can be found using adversarial attack methods, and its robustness can be improved using adversarial defense methods. Meanwhile, it is necessary for researchers to design novel methods that can filter out potential adversarial examples to improve the robustness of learning models."}, {"title": "A Appendix", "content": "A.1 Preliminaries\nA.1.1 UAT Attack\nGiven a textual DNN model F parameterized by 0, an attacker adds a perturbation d to the original data x, and obtains a perturbed example $x' = x+d$. $x'$ is an adversarial example, if the addition of $x'$ results in a different classification output, i.e., $F_\\theta(x') \\neq F_\\theta(x)$. UAT attack (Wallace et al., 2019) consists of two steps.\n(1) Trigger Search. The task loss L for the target class L is minimized to search the best trigger S, i.e., $\\min_g L = \\mathbb{E}_{x}logF_\\theta(x + S, L)$. Trigger S is a fixed phrase consisting of k tokens (original example tokens). $\\oplus$ is token-wise concatenation.\n(2) Trigger Update. UAT method updates the embedding value e to minimize its influence on the average gradient of the task loss over a batch $\\nabla_e L_{adv}$, i.e.,\n$\\arg \\min_{e'} [e; - e_{adv}]^T \\nabla_eL_{adv}$,\nwhere V is the set of all token embeddings in the model's vocabulary, and T is the first-order Taylor approximation. The embeddings are converted back to their associated tokens, and the tokens that alter the corresponding classification results are selected as the updated triggers.\nA.1.2 DARCY\nDARCY (Le et al., 2021) consists of the following three steps.\n(1) Trapdoor Search. To defend attacks on a target label L of model F, DARCY performs a multiple-greedy-trapdoor search algorithm H with the inputs of (K, Dtrain, L) to select K trapdoors $S_\\perp = {w_1, w_2,\\cdots,w_\\kappa}$. \u0397 has the properties of fidelity, robustness, and class-awareness.\n(2) Trapdoor Injection. DARCY injects $S_\\perp$ into F by populating a set of trapdoor-embedded examples, and obtains a new dataset $D_{trap} \\leftarrow {(S \\oplus x, L) : (x, y) \\in D_{y\\neq L}}$, where $D_{y\\neq L} \\leftarrow {D_{train} : y \\neq L}$. DARCY baits $S_\\perp$ into F by training F to minimize the NLL loss on both original examples and trapdoor-embedded examples.\n(3) Trapdoor Detection. DARCY trains a binary classifier $F_g$ using the binary NLL loss, i.e., $\\min_{\\theta_{F_g}}, L_{F_g} = \\mathbb{E}_{x \\in D_{train}} -log(F_g(x))-log(1-F_g(x'))$, where $\\theta_{F_g}$ denotes the parameters of $F_g$, and $x' = x \\oplus S_\\perp$."}, {"title": "A.1.3 HotFlip", "content": "In the HotFlip method (Ebrahimi et al., 2018), the attacker inputs the adversarial examples into the original model, and then uses the back-propagation learning process of the model to obtain the gradients of the trained triggers. The attacker calculates the model product of the gradient vectors corresponding to the triggers and the trained triggers at the embedding layer. The trigger-involved dimension of the model product matrix can be denoted as a vector. All components of the vector are sorted to select the k-highest components, and the attacker gets the words in V corresponding to these k components as the k candidate tokens.\nA.2 More Detailed Analysis\nA.2.1 Threshold Analysis\nThe threshold $\\tau$ is critical to adaptively circumvent the DARCY defense with k trapdoors.\nWhen k is small, e.g., k < 5, $\\tau$ can ensure that the features of the adversarial examples are as similar as possible to the target class and they are not matched with the signature of the detection layer. When k is large, e.g., k > 10, the detector is extremely sensitive. Thus, $\\tau$ should be large for"}, {"title": "A.2.2 Trigger Analysis", "content": "In the process of generating triggers, the smaller length of the trigger has higher concealment. The default length of triggers in IndisUAT is 3.\nIndisUAT uses the beam search and pruning method to accelerate searches and achieve a low time complexity O(|V|), where V is the vocabulary set. Thus, the speed of searching triggers in the IndisUAT method is fast.\nThe searched triggers are effective, because of the constraints on the similarity part of Eq. (1) and the HotFlip method. For example, even if the length of a trigger is small, e.g., 3, it can successfully compromise the DARCY's detector with 20 trapdoors.\nThus, the IndisUAT method produces effective and imperceptible triggers.\nA.3 Further Details of Experiments"}]}