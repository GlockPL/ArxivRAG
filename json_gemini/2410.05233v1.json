{"title": "SIMO LOSS: ANCHOR-FREE CONTRASTIVE LOSS FOR FINE-GRAINED SUPERVISED CONTRASTIVE LEARNING", "authors": ["Taha Bouhsine", "Imad El Aaroussi", "Atik Faysal", "Wang Huaxia"], "abstract": "We introduce a novel anchor-free contrastive learning (AFCL) method leveraging our proposed Similarity-Orthogonality (SimO) loss. Our approach minimizes a semi-metric discriminative loss function that simultaneously optimizes two key objectives: reducing the distance and orthogonality between embeddings of similar inputs while maximizing these metrics for dissimilar inputs, facilitating more fine-grained contrastive learning. The AFCL method, powered by SimO loss, creates a fiber bundle topological structure in the embedding space, forming class-specific, internally cohesive yet orthogonal neighborhoods. We validate the efficacy of our method on the CIFAR-10 dataset, providing visualizations that demonstrate the impact of SimO loss on the embedding space. Our results illustrate the formation of distinct, orthogonal class neighborhoods, showcasing the method's ability to create well-structured embeddings that balance class separation with intra-class variability. This work opens new avenues for understanding and leveraging the geometric properties of learned representations in various machine learning tasks.", "sections": [{"title": "INTRODUCTION", "content": "The pursuit of effective representation learning (Gidaris et al. (2018); Wu et al. (2018); Oord et al. (2019)) has been a cornerstone of modern machine learning, with contrastive methods emerging as particularly powerful tools in recent years. Despite significant advancements, the field of supervised contrastive learning (Khosla et al. (2021); Balestriero et al. (2023)) continues to grapple with fundamental challenges that impede the development of truly robust and interpretable models.\nOur research unveils persistent challenges in embedding methods (Wen et al. (2016); Grill et al. (2020); Hjelm et al. (2019)), notably the lack of interpretability and inefficient utilization of embedding spaces due to dimensionality collapse (Zbontar et al. (2021); Jing et al. (2022)). Certain techniques, such as max operations in loss functions (e.g., max(0, loss)) (Gutmann & Hyv\u00e4rinen (2010)) and triplet-based methods (Sohn (2016b); Tian et al. (2021)), introduce complications like non-smoothness, which disrupt gradient flow. These approaches also suffer from biases in hand-crafted triplet selection and necessitate extensive hyperparameter tuning, thereby limiting their generalizability (Rusak et al. (2024)). Contrastive loss functions, including InfoNCE (Chen et al. (2020)), often rely on large batch sizes and negative sampling, leading to increased computational costs and"}, {"title": "RELATED WORK", "content": ""}, {"title": "ANCHOR-BASED CONTRASTIVE LEARNING", "content": "In contrastive learning, anchor-based losses have evolved from simple pairwise comparisons to more sophisticated multi-sample approaches (Khosla et al. (2021)). The triplet loss (Coria et al. (2020)), which compares an anchor with one positive and one negative sample, has found success in applications like face recognition (Chopra et al. (2005)), despite its tendency towards slow convergence. Building on this foundation, the (N+1)-tuplet loss extends the concept to multiple negatives, approximating the ideal case of comparing against all classes. Further refinement led to the development of the multi-class N-pair loss, which significantly improves computational efficiency through strategic batch construction, requiring only 2N examples for N distinct (N+1)-tuplets (Sohn (2016a)). Recent theoretical work has illuminated the connections between these various loss functions. Notably, the triplet loss can be understood as a special case of the more general contrastive loss. Moreover, the supervised contrastive loss (Khosla et al. (2021)), when utilizing multiple negatives, bears a close resemblance to the N-pairs loss. Nevertheless, anchor-based methods are sensitive to negative sample quality, which can lead to inefficiencies in small datasets and struggle with false negatives. It also relies heavily on effective data augmentations and large batch size, with a risk of overlooking global relationships."}, {"title": "DIMENSIONALITY COLLAPSE IN CONTRASTIVE LEARNING METHODS", "content": "Dimensionality collapse, a significant challenge in contrastive learning, occurs when learned representations converge to a lower-dimensional subspace, thereby diminishing their discriminative power and compromising the model's ability to capture data structure effectively (Jing & Tian (2020)). To address this issue, researchers have proposed several innovative strategies. The NT-Xent loss function (Chen et al. (2020)) implements temperature scaling to emphasize hard negatives, promoting more discriminative representations . Another approach involves the use of a nonlinear projection head, which enhances representation quality through improved hypersphere mapping (Grill et al. (2020)). The Barlow Twins method (Zbontar et al. (2021)) takes a different tack, focusing on redundancy reduction by minimizing correlations between embedding vector components through optimization of the cross-correlation matrix. Architectural innovations have also played a crucial role in combating dimensionality collapse. Methods like BYOL and SimSiam employ asymmetric architectures to prevent the model from converging to trivial solutions (Chen & He (2021)). The use of stop gradient in these methods ensures that the models do not converge to produce the same outputs over time. Additionally, the use of batch normalization (Ioffe (2015)) has been empirically shown to stabilize training and prevent such trivial convergence, although the precise mechanisms underlying its effectiveness remain an area of active research (Peng et al. (2023))."}, {"title": "EXPLAINABILITY OF CONTRASTIVE LEARNING", "content": "The underlying mechanisms driving the effectiveness of contrastive learning remain an active area of investigation. To shed light on the learned representations, Zhu et al. (2021) introduced attribution techniques for visualizing salient features. Cosentino et al. (2022) explored the geometric properties of self-supervised contrastive methods. They discovered a non-trivial relationship between the encoder and the projector, and the strength of data augmentation with increasing complexity. They provided a theoretical framework for understanding how these methods learn invariant representations based on the geometry of the data manifold. Furthermore, Steck et al. (2024) examined the implications of cosine similarity in embeddings, challenging the notion that it purely reflects similarity and suggesting that its geometric properties may influence representation learning outcomes. Wang & Liu (2021) investigate the behavior of unsupervised contrastive loss, highlighting its hardness-aware nature and how temperature influences the treatment of hard negatives. They show that while uniformity in feature space aids separability, excessive uniformity can harm semantic structure by pushing semantically similar instances apart. Wang & Isola (2020) identified alignment and uniformity as key properties of contrastive learning. Alignment encourages closeness between positive pairs, while uniformity ensures the even spread of representations on the hypersphere. Their work demonstrates that optimizing these properties leads to improved performance in downstream tasks and provides a theoretical framework for understanding contrastive learning's effectiveness in repre-"}, {"title": "PRELIMINARIES", "content": ""}, {"title": "METRIC SPACE", "content": "A metric space is a set X together with a distance function d : X \u00d7 X \u2192 R (called a metric) that satisfies the following properties for all x, y, z \u2208 X:\n1. Non-negativity: d(x, y) \u2265 0\n2. Identity of indiscernibles: d(x, y) = 0 if and only if x = y\n3. Symmetry: d(x,y) = d(y, x)\n4. Triangle inequality: d(x, z) \u2264 d(x, y) + d(y, z)"}, {"title": "SEMI-METRIC SPACE", "content": "A semi-metric space is a generalization of a metric space where the triangle inequality is not required to hold. It is defined as a set X with a distance function d: XXX R that satisfies:\n1. Non-negativity: d(x, y) \u2265 0\n2. Identity of indiscernibles: d(x, y) = 0 if and only if x = y\n3. Symmetry: d(x,y) = d(y,x)"}, {"title": "METHOD", "content": ""}, {"title": "SIMILARITY-ORTHOGONALITY (SIMO) LOSS FUNCTION", "content": "We propose a novel loss function that leverages Euclidean distance and orthogonality (through the squared dot product) for learning the embedding space. This function, which we term the Similarity-Orthogonality (SimO) loss, is defined as:\n$L_{Simo} = y \\frac{\\sum_{i,j} d_{ij}}{\\epsilon + \\sum_{i,j} O_{ij}} + (1 - y) \\frac{\\sum_{i,j} O_{ij}}{\\epsilon + \\sum_{i,j} d_{ij}}$\n- Vi, j, i \u2260 jandi < jare indices of the embedding pairs within a batch\n- y is a binary label for the entire batch, where y = 1 for similarity and y = 0 for dissimilarity\n- $d_{ij} = ||e_i - e_j||^2$ is the squared Euclidean distance between embeddings $e_i$ and $e_j$\n- $O_{ij} = (e_i \\cdot e_j)^2$ is the squared dot product of embeddings $e_i$ and $e_j$\n- \u20ac is a small constant to prevent division by zero\nSimO loss function presents a novel framework for learning embedding spaces, addressing several critical challenges in representation learning. Below, we highlight its key properties and advantages:\n\u2022 Semi-Metric Space function: The SimO loss function operates within a semi-metric space, as formalized in the SimO Semi-Metric Space Theorem. This allows for a flexible representation of distances between embeddings, particularly useful for high-dimensional data where traditional metrics may fail to capture complex relationships (Theorem A.1).\n\u2022 Preventing Dimensionality Collapse: The orthogonality component of the SimO loss plays a pivotal role in preventing dimensionality collapse, a phenomenon where dissimilar classes become indistinguishable in the embedding space. By encouraging orthogonal embeddings for distinct classes, SimO ensures that the learned representations remain well-separated and span diverse regions of the embedding space, preserving class distinctiveness (Theorem A.2)."}, {"title": "ANCHOR-FREE CONTRASTIVE LEARNING", "content": ""}, {"title": "RESULTS", "content": "Our extensive experiments on the CIFAR-10 dataset demonstrate the effectiveness of SimO in learning discriminative and interpretable embeddings. We present a multifaceted analysis of our results, encompassing unsupervised clustering, supervised fine-tuning, and qualitative visualization of the embedding space.\nTo assess the transferability and discriminative capacity of our learned representations, we conducted a supervised fine-tuning experiment. We froze the SimO-trained encoder and attached a simple classifier head, fine-tuning only this newly added layer for a single epoch. This minimal fine-tuning yielded impressive results:\nThe rapid convergence to high accuracy with minimal fine-tuning underscores the quality and transferability of our SimO-learned representations. It's worth noting that this performance was achieved with only 1 epoch of fine-tuning, demonstrating the efficiency of our approach."}, {"title": "ABLATION STUDY", "content": "Our ablation studies provide crucial insights into the effectiveness of SimO's key components.\nOrthogonality Leaning Factor When removing the orthogonality constraint from our loss function, we observed a significant degradation in performance. The model failed to learn representations for all classes, instead converging to a state where only 4 out of 10 classes from CIFAR-10 were distinguishable, with the remaining 6 classes grouped together. This result underscores the critical role of the orthogonality leaning factor in SimO, acting as a regularizer that encourages the model to utilize the full dimensionality of the embedding space and prevent the clustering of multiple classes into a single region which validate. In our initial experiments with batch composition, we encountered an interesting phenomenon where certain classes dominated the loss function and, consequently, the embedding space. This dominance prevented the model from learning adequate representations for the other classes, resulting in unstable learning. To address this issue, we implemented a class sampling strategy inspired by techniques used in meta-learning, randomly selecting less than 50% of the total number of classes for each batch. This approach led to more balanced learning across all classes increasing stability in the learning process."}, {"title": "DISCUSSION AND LIMITATIONS", "content": "In our proposed framework, the embedding space generated by the SimO loss exhibits notable geometric properties (Figure 2, Figure 3) that can be interpreted through the lenses of stratified spaces, quotient topology, and fiber bundles. Specifically, we can view the overall embedding space as a stratified space, where each stratum corresponds to a distinct class neighborhood. This structure is facilitated by the orthogonality encouraged by our loss function, promoting clear separations between classes while maintaining cohesive intra-class relationships. Furthermore, we propose considering a quotient topology in which points within the same class neighborhood are identified, simplifying the representation of the embedding space to a point for each class. This transformation not only highlights the distinctness of classes but also emphasizes their orthogonality in the learned space. Additionally, our method generates a structure reminiscent of a fiber bundle, where each fiber corresponds to a specific class and is orthogonal to other fibers. This fiber bundle-like organization allows for a rich representation of class relationships and facilitates a more interpretable understanding of the learned embeddings. Collectively, these geometric interpretations underscore the robustness and effectiveness of our SimO loss with our AFCL framework in structuring embeddings that balance class separation with interpretability requiring small batch sizes unlike other loss functions.\nWhile SimO demonstrates significant advancements in contrastive learning, our extensive experimentation has revealed several important limitations and areas for future research.\nRedefinition of Similarity Metrics: A key finding of our work is that embeddings learned through SimO no longer adhere to traditional similarity measures such as cosine similarity. This depar-"}, {"title": "Sensitivity to Data Biases", "content": "Our method's ability to learn fine-grained representations comes with increased sensitivity to biases present in the training data. This is particularly evident in the case of background biases in object recognition tasks. For instance, our model struggled to separate the neighborhoods of Dog and Cat classes even though it learned from the 120,000th iteration to the 1 millionth iteration, despite having learned most other classes effectively. This sensitivity necessitates robust data augmentation techniques to mitigate the impact of such biases. While this requirement for strong augmentation can be seen as a limitation, it also highlights SimO's potential for detecting and quantifying dataset biases, which could be valuable for improving dataset quality and fairness in machine learning models."}, {"title": "The Orthogonality Learning Factor", "content": "The performance of SimO is notably influenced by the orthogonality learning factor, a hyperparameter that balances the trade-off between similarity and orthogonality objectives. Finding the optimal value for this factor presents a challenge we term \"the curse of orthogonality.\" Too low a factor leads to insufficient separation between class neighborhoods, while too high a factor can result in overly rigid embeddings that fail to capture intra-class variations. Our experiments show that this factor often needs to be tuned specifically for each dataset and task, which can be computationally expensive. Developing adaptive methods for automatically adjusting this factor during training represents an important direction for future research."}, {"title": "Computational Complexity", "content": "While not unique to SimO, the computational requirements for optimizing orthogonality in high-dimensional spaces are substantial. This can limit the applicability of our method to very large datasets or in resource-constrained environments. Future work should explore approximation techniques or more efficient optimization strategies to address this limitation.\nDespite these limitations, we believe that SimO represents a significant step forward in contrastive learning. The challenges identified here open up exciting new avenues for research in representation learning, similarity metrics, and bias mitigation in machine learning models. Addressing these limitations will not only improve SimO but also deepen our understanding of the fundamental principles underlying effective representation learning."}, {"title": "CONCLUSION", "content": "Our AFCL method introduces the SimO loss function as a novel approach to contrastive learning, effectively addressing several critical challenges related to embedding space utilization and inter-operability. By optimizing both the similarity and orthogonality of embeddings, SimO prevents dimensionality collapse and ensures that class representations remain distinct, even in lower dimensions, requiring smaller batch sizes and embedding dimensions.\nOur experimental results on the CIFAR-10 dataset demonstrate the efficacy of SimO in generating structured and discriminative embeddings with minimal computational overhead. Notably, our method achieves impressive test accuracy as early as the first epoch. Although there are limitations, such as sensitivity to data biases and dependence on specific hyperparameters, SimO paves the way for future advancements in enhancing contrastive learning techniques and managing embedding spaces more effectively."}]}