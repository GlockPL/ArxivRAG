{"title": "How to build trust in answers given by Generative AI for specific and vague financial questions", "authors": ["Alex Zarifis", "Xusen Cheng"], "abstract": "Purpose - Generative artificial intelligence (GenAI) has progressed in its ability and has seen explosive growth in adoption. However, the consumer's perspective on its use, particularly in specific scenarios such as financial advice, is unclear. This research develops a model of how to build trust in the advice given by GenAI when answering financial questions.\nDesign/methodology/approach \u2013 The model is tested with survey data using structural equation modelling (SEM) and multi-group analysis (MGA). The MGA compares two scenarios, one where the consumer makes a specific question and one where a vague question is made.\nFindings - This research identifies that building trust for consumers is different when they ask a specific financial question in comparison to a vague one. Humanness has a different effect in the two scenarios. When a financial question is specific, human-like interaction does not strengthen trust, while (1) when a question is vague, humanness builds trust. The four ways to build trust in both scenarios are (2) human oversight and being in the loop, (3) transparency and control, (4) accuracy and usefulness and finally (5) ease of use and support.\nOriginality/value \u2013 This research contributes to a better understanding of the consumer's perspective when using GenAI for financial questions and highlights the importance of understanding GenAI in specific contexts from specific stakeholders.", "sections": [{"title": "1. Introduction", "content": "Financial technology (Fintech) is reshaping business models and the relationships between organisations and their consumers. Generative artificial intelligence (GenAI) is still in the early stages of its adoption in finance, but its role and the opportunities and challenges it creates are starting to take shape. As with other technologies that are used in processes that involve some risk, trust is one of the challenges that need to be overcome (Mou, Shin, & Cohen, 2017). There are privacy concerns, as more information is shared and the ability to process personal information increases. Trust is particularly important when receiving financial advice from Al due to the financial risk involved when acting on it (Dietzmann, Jaeggi, & Alt, 2023; Dupont & Karpoff, 2020). There isn't a direct \u201cmechanical\u201d effect between a technology that offers value and a user adopting it. There is the interim step of them building sufficient\ntrust in it. It is therefore helpful to have a model of how to build trust in the advice given by GenAI when answering financial questions.\nGenAI can be defined as algorithms that can analyse vast volumes of data and create human-like content as output such as text, images or sound (Salah, Al Halbusi, &\nAbdelfattah, 2023). GenAI apps are different from other AI technologies because they have the ability to generate or create something. They are also pre-trained so that they can respond quickly. These characteristics result in a technology that is closer to human intelligence and has the ability to offer financial insight. This increased humanness can, potentially, create an affinity with the GenAI. Popular examples include Baidu's Ernie Bot, Tencent's Hunyuan,\nChat GPT, Dall-E 2, Google Bard and Jasper AI.\nGenAI brings with it some risks that are common across many information systems such as the accuracy of the data used, but it also brings some specific risks such as \u201challucinations", "does stock X usually give a higher dividend than stock Y": "r vague such as \u201chow can my investments make me happier", "bandwidth": "it is important to identify the most effective approaches to focus on. The actions that need to be taken involve both making changes to how GenAI operates and how its value and trustworthiness are communicated.\nStrengthening trust is not just needed to increase the rate of adoption of GenAI for\nfinancial decisions, but it will also influence the nature of this relationship. For example,\nhigher trust will reduce the likelihood of several systems being used in parallel in financial\ndecision-making. It may also reduce indecision and second-guessing every move, which can\nbe time consuming and inefficient.\nWhile it should be clear that trust is important, the issues that challenge trust and how to\nbuild trust are not entirely clear. The nature of trust is often different when the contexts and\nrelationships are different. It is therefore important to clarify the processes by which GenAI is\nused to advise financial decisions first, and then clarify the challenges to trust within that\nspecific process. Therefore, the research questions are:\nRQ1. How is trust built into the answers given by GenAI to financial questions?\nRQ2. Is building trust in the answers given by GenAI the same for specific and vague\nfinancial questions?\nThis research identified four methods to build trust in GenAI in both scenarios \u2013 specific and\nvague questions \u2013 and one method that only works for vague questions. Humanness has a\ndifferent effect on trust in the two scenarios. When a question is specific, humanness does not\nincrease trust, while (1) when a question is vague, human-like GenAI increases trust. The four\nways to build trust in both scenarios are: (2) human oversight and being in the loop, (3)\ntransparency and control, (4) accuracy and usefulness and, finally, (5) ease of use and support.\nFor best results, all identified methods should be used together to build trust. These variables\ncan provide the basis for guidelines for organisations in finance utilising GenAI.\nThe next section identifies dimensions of trust from more mature related areas that are\nrelevant. The methodology covers how a survey was implemented and analysed\nquantitatively with structural equation modelling (SEM) and multi-group analysis (MGA).\nThe analysis presents many tests that are used to evaluate the model of building trust in\nadvice given by GenAI in answers to financial questions. Lastly, the discussion and\nconclusion identify the theoretic and practical implications of the model validated."}, {"title": "2. Theoretic foundation", "content": "The literature review covers (1) the role of GenAI in finance, (2) the process of using GenAI to answer a user's financial questions, (3) humanness and creepiness in GenAI and finally (4) the user's trust in this context.\n2.1 Generative AI in finance\nThe capabilities of GenAI to answer complex financial questions are revolutionising finance\n(Chui et al., 2023). Financial institutions, such as retail banks, investment banks, micro- investing platforms and insurers, have vast internal data collected from their interactions with the user, and they also have access to extensive data from external sources. These data can be characterised as a \u201cgoldmine\u201d, as they can be utilised for better insights with GenAI (Izard, 2023). Financial institutions that generate huge volumes of useful data can use GenAI to gain a competitive advantage against other competitors with inferior data, such as start- ups. This technology can not only add new capabilities but also reduce costs. GenAI can contribute to coordinating the financial ecosystem, understanding and generating language and understanding and generating emotion and creativity (Chui et al., 2023). All these different abilities of GenAI in finance also influence how the person asking for financial advice experiences the interaction and forms beliefs about it.\nThis is not a typical case of technology adoption, but it is a disruption with some\nuncertainty and some challenges. While the application of this technology to finance is often\nreferred to as Fintech, automatic investment advice by AI is also being referred to as\nWealthTech (Sloan, 2023). GenAI can bring wealth management tools to more people.\n2.2 The process of using Generative AI to answer user's financial questions\nThe ability of GenAI to replicate humans' intelligence and communication, in some ways, also changes how financial questions are put to GenAI. Questions do not have to be narrow and specific, such as a car owner entering their car's details and asking for an insurance quote. Questions can be vague or be a combination of several questions made at once. For example, a user may describe their current job, what they earn, when they expect a promotion and ask for investment advice. illustrates the stages a user typically takes when asking GenAI a\nfinancial question. A user will initially make a specific narrow question or a vague general\nquestion, receive an answer, assess the answer and possibly make a follow-up question.\n2.3 Humanness and creepiness in Generative AI\nDespite the ability of AI, it can be seen negatively by a consumer, both externally to an organisation and internally by a user in the workplace (Hornung & Smolnik, 2021). GenAI can display human-like characteristics both in its appearance and in what it says (Sullivan, Nyawa, &\nWamba, 2023). A hardware device using GenAI can have an anthropomorphised appearance and facial expressions (Song, Tao, & Luximon, 2023). GenAI can mimic human-like characteristics by having a conversation and providing relevant and sophisticated responses (Chandra, Shirish, & Srivastava, 2022). GenAI can even go beyond human-like characteristics and understand, create and convey emotions (Chui et al., 2023). AI can mimic empathy, joy and sadness. Whether the emotions shown are appropriate usually depends on understanding the users' emotions they are responding to. Understanding users' emotions has been a challenge for GenAI, although it is improving (Demmer, K\u00fchnapfel, Fingerhut, & Pelowski, 2023).\nPeople using AI can feel a strange feeling, sometimes called creepiness. This is a feeling users do not feel with information systems that do not use AI (Rajaobelina, Prom Tep,\nArcand, & Ricard, 2021). This creepiness does not happen when there are no human-like\ncharacteristics, but only when there are some human-like characteristics (Mori, 2012).\nTherefore, while GenAI can display human-like behaviour, even emotions, there is a lack of\nclarity about when this is useful and when it is counterproductive.\n2.4 Trust in Generative AI in finance\nThe role of a user's trust in a technology is not always immediately apparent as soon as that technology is made available. It often takes some time for users' beliefs to be formed,"}, {"title": "3. Conceptual framework and hypotheses", "content": "Human-like interaction: Human-like characteristics can create negative feelings such as creepiness, so they are not always appropriate (Hyun Baek & Kim, 2023; Mori, 2012). When a question is specific, a human may expect the information system to act like a machine without\nhuman-like characteristics. In this scenario, GenAI should avoid human-like behaviour and emotion when responding to the user's question. Attempting human-like behaviour with\ntechnology can create a sense of creepiness. When a user makes vague questions, they are\nmore open to human-like behaviour and emotion in GenAI. Therefore, the two parts of the\nfirst hypothesis are:\nHla. GenAI with higher humanness in the interaction will reduce trust, if the question\nasking for financial advice is specific.\nH1b. GenAI with higher humanness in the interaction will increase trust, if the question\nasking for financial advice is vague.\nHuman oversight, being in the loop: Using GenAI to inform financial decisions involves some risks. Several steps are needed to mitigate these risks. Firstly, it is necessary to keep expert practitioners in the loop and communicate their role clearly (Gr\u00f8nsund & Aanestad, 2020). Regular audits of performance will identify problems early or even prevent them. For a human to feel comfortable using GenAI, it must conform to the (human) norms and culture of the financial sector. As with most professional financial advice, institutions such as regulators should have oversight in an effective and visible way (Chen, Wu, & Zhao, 2023;\nMoin et al., 2015). Therefore, the second hypothesis is:\nH2. Visible human oversight in how GenAI is used to answer a question asking for\nfinancial advice will strengthen trust.\nTransparency and control: GenAI can learn and act in a less-predictable way than software did in the past. Therefore, for trust to be built in GenAI for finance, some steps are needed. Firstly, there should be transparency that GenAI is used (Augustine, 2012; Felzmann et al.,\n2019). The user must have the sense that they are in control when getting the information\nthey need (Pavlou & Fygenson, 2006). It may weaken trust if this is not disclosed to the user\nand they discover it on their own. Secondly, some explanation as to how GenAI is used can be\nprovided. The term explainable AI, or Explainable AI (XAI), is often used to describe this\n(Weber, Carl, & Hinz, 2023). It must be clear on what dataset the AI was trained on, and it\nmust be clear how personal information provided by the consumer is used. Therefore, the\nthird hypothesis proposed is:\nH3. Transparency in how GenAI is used to answer a financial question will\nstrengthen trust.\nAccuracy and usefulness: While the nature of the interaction plays a role, the value of the\ninformation provided to answer the financial question is also important, as this is the primary\nusefulness of this action (Pavlou & Fygenson, 2006; Thatcher et al., 2011). GenAI needs to\nprovide an accurate result consistently. Nevertheless, an accurate result might not be enough,\nif the user does not believe it is accurate. Therefore, it is important to provide simple and clear\nevidence to support the financial advice provided. GenAI often has a stronger ability to\ncustomise solutions, so a well-tailored solution is expected (Sison, Daza, Gozalo-Brizuela, &\nGarrido-Merch\u00e1n, 2023). Therefore, we propose to test:\nH4. GenAI that provides accurate information in the answers given to financial\nquestions will increase trust.\nEase of use and support: There is a long history of technology adoption that points to ease of\nuse being important in adoption and trust. GenAI must have an interaction with the user that\nis easy to use when making a question, receiving financial advice and resolving any\nsubsequent issues. Reliable support and correcting errors in a process involving technology\nbuild trust (Pavlou & Fygenson, 2006; Thatcher et al., 2011). Therefore, we propose to test:\nH5. GenAI that is easy to use and has support will increase trust in the answers given to\nfinancial questions."}, {"title": "4. Research design", "content": "4.1 Research instrument and data analysis\nThe theoretic foundation provided by the literature suggests that there may be a difference in\nhow humanness influences trust in two different scenarios: specific and vague financial\nquestions. As the model of building trust in GenAI when receiving financial information\nneeds to be tested in two different scenarios, partial least squares-structural equation\nmodelling (PLS-SEM) and MGA are applied (Hair, Hult, Ringle, & Sarstedt, 2022). The first\nscenario is when the question asking for financial advice is specific, and the second scenario is\nwhen the question asking for financial advice is vague. Apart from the need to compare two\ngroups, an additional reason to use PLS-SEM is that the priority here is to explore a model and\nto identify some key drivers rather than to test a mature model (Hair et al., 2022). The\nreflective model's latent and measured variables are listed in Table 2.\n4.2 Sample and data collection\nThe minimum sample size required was calculated with three popular methods that gave\nsimilar results. Based on the most arrows pointing at a variable being five, a statistical power\nof 80% and a significance level of 1%, the minimum sample size recommended by a popular\nguide is 205 (Hair et al., 2022). A second method was used to evaluate the minimum sample\nwith the G*Power 3.1.9.7 software. Using the G*Power software with a statistical power of\n95% the minimum sample recommended is 220. The third method, a common rule of thumb to\nhave ten participants per variable, suggests 210 as there are 5 latent and 16 measured\nvariables (Chin, 1998). The survey questions in the main section provided a Likert scale\nranging from one to seven for participants to respond to. A trial was done with eight\nparticipants to ensure that the questions were clear. The first section of the survey includes\nsome questions to ensure the participants are adults, and the typical demographic questions\nsuch as age, education and gender. The main section of the survey includes 21 questions\nbased on the measured variables that capture the latent variables. The survey was\ndisseminated online for a period of two months. The number of survey submissions in the\nsystem was 486. Some checks were carried out to ensure the responses were valid and\ngenuine attempts. Based on the validity checks, 44 submissions were taken out, leaving 442"}, {"title": "5. Analysis and results", "content": "5.1 Measurement model\nAs part of the SEM-PLS process, some tests are made to evaluate the relationship between the\nmeasured and latent variables. The results are presented in Table 4. Convergent validity is\ntested in two ways. Firstly, for the factor loading, the lowest is 0.922, which is above the"}, {"title": "5.2 Structural model", "content": "The results of the structural model and MGA are presented in Table 7. For the endogenous variable, \u201ctrust Generative AI for financial decisions\u201d, the coefficient of determination\nR-square is 0.928, which is substantial (Chin, 1998). The probability that the groups are different based on the MGA is in the last column of Table 7. There is no significant difference between the two groups if the value is between 0.05 and 0.95 (Hair et al., 2022).\nAs hypothesised, there is a significant difference between the path \u201cHuman oversight (in the"}, {"title": "6. Discussion", "content": "6.1 Theoretical contribution\nThis research tested and verified the relevance to GenAI of several theories from information\nsystems. This research supports the concept that higher humaneness of technology does not\nnecessarily create a higher affinity and trust and may reduce affinity and trust in technology\nin some contexts (Mori, 2012). The five ways to build trust in GenAI for responses to financial\nquestions identified are as follows: (1) for vague and general questions, human-like\ninteraction builds trust. Human-like interaction does not build trust when specific questions\nare asked. The final four methods build trust for both specific and vague questions: (2) human\noversight and being in the loop, (3) transparency and control, (4) accuracy and usefulness and\n(5) ease of use and support. The model of building trust in advice given by GenAI when\nanswering financial questions for specific and vague questions is illustrated in Figure 3.\n(1) Human-like interaction: When a question is specific, such as \u201cdoes stock X usually\ngive a higher dividend than stock Y\u201d, a response from GenAI with a high humanness\ndoes not increase the user's trust. This is in line with research in other contexts for\nfinancial questions that find that humanness is not always appropriate (Hyun Baek &\nKim, 2023; Mori, 2012). In this scenario, the response of GenAI should avoid human-\nlike behaviour and emotion. When a user makes a vague question, such as \u201cHow can\nmy investments make me happier?\u201d, the user is more open to a response from GenAI\nwith human-like behaviour and emotion. In response to a vague question, humanness\ndoes build trust.\n(2) Human oversight and being in the loop: Asking GenAI to inform financial decisions\ninvolves some risks, and human oversight can build trust. This research finds that it is\nbeneficial for trust to keep expert practitioners in the loop, have regular audits of\nperformance and conform to (human) norms and culture of the financial sector\n(Gr\u00f8nsund & Aanestad, 2020).\n(3) Transparency and control: Transparency and control build trust in the financial advice from GenAI. This is in agreement with research that finds that transparency in\nGenAI is beneficial (Augustine, 2012; Felzmann et al., 2019). It also extends research\nthat found similar results for AI giving financial advice (Dietzmann et al., 2023) to\nGenAI giving financial advice. There needs to be explanation how GenAI is used and\nclarity about what dataset the AI was trained on and how data provided by the consumer are used. This supports the importance of XAI in finance (Weber et al.,\n2023). However, given that several variables were needed to build trust, it also\nsupports that XAI is not sufficient on its own.\n(4) Accuracy and usefulness: The accuracy and usefulness of the answers GenAI gives\nbuild trust. The customisation of the answer to the specific user must be tailored well\nfor it to be useful (Sison et al., 2023). This finding is in line with research on other\ntechnologies where the usefulness in relation to the primary purpose of the technology\nis important (Pavlou & Fygenson, 2006; Thatcher et al., 2011). Providing evidence that\nthe answer is accurate will make it more likely that trust is built.\n(5) Ease of use and support: Finally, the analysis suggests ease of use and support when\nusing GenAI to answer financial questions, which builds trust. This is in line with\nsimilar findings when using technology in other contexts (Pavlou & Fygenson, 2006;\nThatcher et al., 2011). Reliable support and correcting errors in a process involving\ntechnology build trust (Pavlou & Fygenson, 2006; Thatcher et al., 2011).\nThis research also contributes to the body of literature on whether more technology\ncharacteristics or human characteristics influence trust more (Lankton, McKnight, & Tripp,\n2015). The findings here suggest that more technology-based characteristics build trust in\nGenAI for financial decisions. While this research focuses on GenAI when responding to\nfinancial questions, most of these variables are expected to be relevant, to some degree, in\nsimilar contexts where there is financial risk.\n6.2 Practical contribution\nThe topics identified can inform specific steps to build trust now in existing implementations\nof GenAI, but there are also guidelines to follow in the future to maintain and continue\nbuilding trust. Ideally, various stakeholders can move towards addressing the five dimensions of building trust identified in a synergistic way. This research should aid in creating some common priorities for businesses and regulators in the financial sector. Trust can be built across the value chain or with specific trust tools (Lin & Loten, 2023).\nDespite the ability of GenAI to learn and adapt to support various financial services, the most successful organisations will be in the loop, utilising the unique knowledge of their experts to guide and optimise GenAI. Knowing when GenAI should respond to financial questions with a human-like response and emotion is important in building trust. A financial service can go through the list of typical questions a user asks and decide whether a human- like, emotional response is appropriate.\nWhile trust must be built throughout the use of GenAI, it is particularly important to build it when the financial advice received leads to unsuccessful investments. Financial advice, whether directly from a human, GenAI or some other statistical analysis or data aggregation, will inevitably lead to unsuccessful decisions sometimes. It is important for the business providing the financial advice to be as clear as possible about why the GenAI did work reliably even if, for other reasons out of the company's control, there was a negative outcome.\nIf trust at some point is too high and unrealistic expectations are created, this may only increase disappointment at a later stage. \u201cOver-trust\u201d may backfire. Therefore, the level of trust must be built on the solid foundations of its actual capabilities.\nLastly, the findings here encourage providers of GenAI for financial decisions to have vigilance moving forward to understand and adapt to changes in technology, data and regulation."}, {"title": "7. Conclusion", "content": "This research developed a model with five ways to build trust in GenAI responses to financial\nquestions. The model was tested and validated by analysing the data from a survey with\nSEM and MGA.\nThe five ways to build trust in GenAI for responses to financial questions are as follows:\n(1) for vague, general questions, human-like interaction builds trust. Human-like interaction\ndoes not build trust when specific questions are asked. The final four methods build trust for\nboth specific and vague questions: (2) human oversight and being in the loop, (3)\ntransparency and control, (4) accuracy and usefulness, and (5) ease of use and support.\nA business providing GenAI for financial decisions must be clear what it is being used for.\nFor example, analysing past financial performance to attempt to predict future performance\nis very different from analysing social media activity. The advice from GenAI needs to feel\nlike a fully integrated part of the financial community, not just a system. Trust must be built\nsufficiently to overcome the perceived risk. The findings suggest that the consumer will not\nfollow the \"pied piper\u201d blindly, however alluring \u201ctheir song\u201d of automation and efficiency is.\n7.1 Limitations and future research\nThis research finds that humanness and emotions are useful in some financial contexts and\ncounterproductive in others. Future research can explore how different users' personalities\naffect the degree of humanness and emotion they would benefit from.\nThe sample was from one European country, France, and therefore, it may not fully apply\nto other countries, particularly outside the European Union where its common rules do not\napply. Similarly, the model applies to GenAI answers to financial questions and may not\napply fully to other questions. Future research can explore the relevance of the model\nidentified here to other responses of GenAI where risk is high."}]}