{"title": "A Review of Bayesian Uncertainty Quantification in Deep Probabilistic Image Segmentation", "authors": ["M.M.A. Valiuddin", "R.J.G. van Sloun", "C.G.A. Viviers", "P.H.N. de With", "F. van der Sommen"], "abstract": "Advancements in image segmentation play an integral role within the greater scope of Deep Learning-based computer vision. Furthermore, their widespread applicability in critical real-world tasks has given rise to challenges related to the reliability of such algorithms. Hence, uncertainty quantification has been extensively studied within this context, enabling expression of model ignorance (epistemic uncertainty) or data ambiguity (aleatoric uncertainty) to prevent uninformed decision making. Due to the rapid adoption of Convolutional Neural Network (CNN)-based segmentation models in high-stake applications, a substantial body of research has been published on this very topic, causing its swift expansion into a distinct field. This work provides a comprehensive overview of probabilistic segmentation by discussing fundamental concepts in uncertainty that govern advancements in the field as well as the application to various tasks. We identify that quantifying aleatoric and epistemic uncertainty approximates Bayesian inference w.r.t. to either latent variables or model parameters, respectively. Moreover, literature on both uncertainties trace back to four key applications; (1) to quantify statistical inconsistencies in the annotation process due ambiguous images, (2) correlating prediction error with uncertainty, (3) expanding the model hypothesis space for better generalization, and (4) active learning. Then, a discussion follows that includes an overview of utilized datasets for each of the applications and comparison of the available methods. We also highlight challenges related to architectures, uncertainty-based active learning, standardization and benchmarking, and recommendations for future work such as methods based on single forward passes and models that appropriately leverage volumetric data.", "sections": [{"title": "I. INTRODUCTION", "content": "IMAGE segmentation entails pixel-wise classification of data, effectively delineating objects and regions of interest [1]. With the rapid development of Convolution Neural Networks (CNNs), Deep-Learning based image segmentation has seen major advancements and gained significant interest over time [2]-[4], obtaining impressive scores with large-scale segmentation datasets [5]-[7]. Nevertheless, such methodologies rely on extensive assumptions and relaxations on the Bayesian learning paradigm, omitting crucial information on the uncertainty associated with the model predictions. This ignorance diminishes the reliability and interpretability of such models. For example, difficult distinction between classes in real-time automotive scenarios can result in disastrous consequences or uncertain lesion malignancy prediction may significantly impact the decision-making of invasive treatments. Extensive efforts have been made to align modern neural network optimization with Bayesian Machine Learning [8]- [11], such as learning parameter densities, rather than point estimates, to include a notion of epistemic uncertainty. Furthermore, explicitly modeling the output likelihood distribution enables expressing the aleatoric uncertainty. Notably, literature mention that determining the nature of uncertainty is not often straightforward. For example, H\u00fcllermeier et al. [12] mentioned that \"by allowing the learner to change the setting, the distinction between these two types of uncertainty will be somewhat blurred\". This sentiment is also shared by Kiureghian and Ditlevsen [13], noting that \"In one model an addressed uncertainty may be aleatory, in another model it may be epistemic\". Sharing similar views, we highlight the necessity of careful analyses and possible subjective interpretation regarding the topic.\n\nThe merits of uncertainty quantification have fortunately been well-recognized in the field of CNN-based segmentation and underscore the importance of a rigorous literature overview. Nonetheless, most surveys take perspective from the medical domain [14], [15], often for specific modalities [16]\u2013 [19]. There is a notable absence of a comprehensive overview in this field, relating theoretical foundation to the multitude of applications. Furthermore, the abundance of available works can often be overwhelming for both new-coming and seasoned researchers. This study seeks to fill this gap in literature and contributes to the subject area by providing a curated overview, where various concepts are clarified through standardized notation. After reading this work, readers will be able to discern various forms of uncertainty with their pertinent applications to segmentation tasks. Additionally, they will have developed comprehensive understanding of challenges and unexplored avenues in the field.\n\nThis review paper is structured as follows. Past work with significant impact in general image segmentation is presented in Section II, where closely related surveys are also referenced. Then, the theoretical framework and notation that governs the remainder of the paper is introduced in Section III. The role of these concepts in image segmentation will be treated in Section IV and V, which includes all architectures and approaches with significant impact on the field. We then consider the applications that use these uncertainty estimates in Section VI. Finally, this overview will be further discussed together with key challenges and future recommendations in Section VII and we conclude in Section VIII."}, {"title": "II. BACKGROUND", "content": "As summarized by Minaee et al. [20], semantic segmentation has been performed using methods such as threshold-ing [21], histogram-based bundling, region-growing [22], k-means clustering [23], watersheds [24], to more advanced algorithms such as active contours [25], graph cuts [26], conditional and Markov random fields [27], and sparsity-based methods [28], [29]. After the application of CNNs [30], the domain of image segmentation underwent rapid developments. Notably, the Fully Convolutional Network (FCN) [3] adapted the AlexNet [31], VGG16 [32] and GoogLeNet [33] architectures to enable end-to-end semantic segmentation. Furthermore, other CNN architectures such as DeepLabv3 [34], and the MobileNetv3 [35] have also been commonly used.\n\nAs the research progressed, increasing success has been observed with encoder-decoder models [2], [4], [36], [37]. Initially developed for the medical applications, Ronneberger et al. [2] introduced the U-Net, which successfully relies on residual connections between the encoding-decoding path to preserve high-frequency details in the encoded feature maps. To this day, the U-Net is still often utilized as the default backbone model for segmentation architectures. In fact, reports in recent research indicate that the relatively simple U-Net and nnU-Net [38] still outperform more contemporary and complex models [39], [40].\n\nSemantic Segmentation focuses solely on assigning a class label(s) to each pixel and is particularly suitable for amorphous or uncountable subjects of interest. In contrast, Instance Segmentation not only detects, but also delineates individual objects within the image. This form of segmentation is more applicable when identifying and outlining countable instances of objects. The third category, Panoptic Segmentation, combines both class and instance level classification [20]."}, {"title": "III. PROBABILISTIC IMAGE SEGMENTATION", "content": "Let random-variable pairs $(Y,X) \\sim P_{Y,X}$ take values in $Y \\in \\mathbb{R}^{K \\times H \\times W}$ and $X \\in \\mathbb{R}^{C \\times H \\times W}$, respectively, where $Y$ can be considered as the ground-truth of a K-class segmentation task and X as the query image. The variables H, W and C correspond to the image height, width and channel depth, respectively."}, {"title": "A. Bayesian inference", "content": "Conforming to the principle of maximum entropy, the optimal parameters given the data (i.e. posterior) subject to the chosen intermediate distributions can be inferred through Bayes Theorem as\n\n$p(\\theta|y,x) = \\frac{p(y|x,\\theta)p(\\theta)}{p(y|x)},$ (1)\n\nwhere $p(\\theta)$ represents the prior belief on the parameter distribution and $p(y|x)$ the conditional data likelihood (also commonly referred to as the evidence). After obtaining a posterior with dataset $\\mathcal{D} = \\{x_i, y_i\\}_{i=1}^{N}$ containing N images, the predictive distribution from a new datapoint $x^*$ can be denoted as\n\n$p(Y|x^*,\\mathcal{D}) = \\int \\underbrace{p(Y|x^*, \\theta)}_{Model} \\underbrace{p(\\theta|\\mathcal{D})}_{Data} d\\theta.$ (2)\n\nAs evident in Equation (2), both the variability in the empirical data and the inferred parameters of the model influence the variance of the predictive distribution. Hence, uncertainties stemming from the conditional likelihood distribution are classified as either aleatoric, implying from the statistical diversity in the data, or epistemic, which stems from the posterior, i.e. the variance of the model parameters. A straightforward approach to quantify any of these uncertainties is achieved through obtaining the predictive entropy defined as\n\n$H[Y|x^*,\\mathcal{D}] = E_{Y} [-\\log p(Y|x^*, \\mathcal{D})]$ (3)\n\nor variance\n\n$\\text{Var}[Y|x^*,\\mathcal{D}] = E_{Y} [p(Y|x^*, \\mathcal{D})^2] - E_{Y} [p(Y|x^*, \\mathcal{D})]^2.$ (4)"}, {"title": "B. Conventional segmentation", "content": "The so-called \"deterministic\" segmentation networks are trained by Maximum Likelihood Estimation (MLE) as\n\n$\\theta_{MLE} = \\text{arg max}_{\\theta} \\log p(y|x, \\theta),$ (5)\n\nwhich simplifies the training procedure by taking a point estimate of the posterior. This approximation improves as the"}, {"title": "IV. ALEATORIC UNCERTAINTY", "content": "Aleatoric uncertainty quantification reconsiders the non-deterministic relationship between $x \\in \\mathcal{X}$ and $y \\in \\mathcal{y}$, which implies that\n\n$p(Y|X) = \\frac{p(Y, X)}{p(X)} \\neq \\delta(Y - F(X)),$ (9)"}, {"title": "A. Pixel-level sampling", "content": "A valid approach employs direct pixel-level uncertainty in the annotations. As discussed before, uncertainty can be quantified in case the independence assumption holds. In this case, it is important to ensure proper calibration, which is discussed in Section IV-A1. In another method, the spatial correlation is explicitly modeled across the pixels of the segmentation mask and results in spatially coherent samples, which will be treated in Section IV-A2."}, {"title": "1) Independence:", "content": "The normalized confidence values that result from SoftMax activation can only be interpreted as probabilities after proper validation, which is referred to as model calibration. Ideally, the empirical accuracy of a model should approximately equal to the provided class confidence $c_k$ for class $k$, i.e. $P(Y = k | c_k) = c_k$. Calibration is typically visualized with a reliability diagram, where miscalibration and under-/over- confidence can be assessed by inspecting the deviation from the graph diagonal. Different methods can be used to measure calibration, but often introduce their own biases. A fairly straightforward metric, the Expected Calibration Error (ECE), determines the normalized distance between accuracy and confidence bins as\n\n$ECE = \\sum_{b=1}^{B} \\frac{n_b}{N} |\\text{acc}(b) - \\text{conf}(b)|,$ (10)\n\nwith $n_b$ the number of samples in bin $b$ and N being the total sample size across all bins. The ECE is prone to skew representations if some bins are significantly more populated with samples due to over-/under- confidence. Furthermore, Maximum Calibration Error (MCE) is more appropriate for high-risk applications, where only the worst bin is considered.\n\nContemporary SoftMax-activated neural networks often portray a significantly incongruous reflection of the true data uncertainty because of negative log-likelihood optimization and techniques such as batch normalization, weight decay and other forms of regularization [9]. Most calibration techniques are post-hoc, i.e. they occur after training and thus require a separate validation set. For example, Temperature Scaling [9] has been applied in a pixel-wise manner for segmentation problems [43]. Nonetheless, some methods, such as Label Smoothing [44], [45] or using Focal loss [46] can directly be applied on the training data. Furthermore, overfitting has often been considered to be the cause of overconfidence [47], [48] and erroneous pixels can therefore be penalized through regularizing low-entropy outputs [49]."}, {"title": "2) Spatial correlation:", "content": "To explicitly model spatial correlation within pixel space of the likelihood distribution, Monteiro et al. [50] propose the Stochastic Segmentation Network (SSN), which models the output logits as a multivariate normal distribution parameterized by the neural networks $f_{\\mu}$ and $f_{\\sigma}$ as\n\n$p(a|x, \\theta) = \\mathcal{N}(a; \\mu = f_{\\mu}(x), \\Sigma = f_{\\Sigma}(x)),$ (11)\n\nwhere the covariance matrix has low-rank structure\n\n$\\Sigma = PP^T + \\Lambda.$ (12)\n\nHere, P has dimensionality $((H \\times W \\times K) \\times R)$, with R being a hyperparameter that controls the parameterization rank and $\\Lambda$ a diagonal matrix. This results in a more structured and expressive distribution, while retaining reasonable efficiency. As the SoftMax transform on this low-rank multivariate normal distribution pertains an intractable integral, Monte Carlo sampling is employed. SSNs can be augmented as an additional layer to any existing CNN.\n\nAnother method uses an autoregressive approach to predict pixel $Y_i$ based on the preceding pixels. In this case, we can rephrase Equation (8) to\n\n$p(Y|X) = \\prod_{i=1}^{K \\times D} p(Y_i|Y_1, ..., Y_{i-1}, X).$ (13)\n\nThe PixelCNN remains a popular method to model such relationship due to its substantial receptive field [51], [52]. Zhang et al. [53] suggest to use this to predict a downsampled segmentation mask $\\tilde{y}$ with $p_{\\theta}(\\tilde{y}|x)$, and fuse this with a conventional CNN to predict the full resolution mask with $p_{\\phi}(y|\\tilde{y}, x)$. Fusing the two masks is done through a resampling module, containing a series of specific transformations to improve quality and diversity of the samples. Notably, PixelCNNs employ a recursive sampling process, which also enables completion/inpainting of user-given inputs."}, {"title": "B. Latent-level sampling", "content": "Directly learning the conditional data distribution is a challenging task. Hence, generative models often employ a simpler latent (unobserved) variable $Z \\sim p_z$ with $Z \\in \\mathbb{R}^d$, to then learn the approximate joint density $p(Y,Z|X)$. The marginalized distribution is obtained through decomposition\n\n$P_{\\theta,\\psi} (Y|X) = \\int P_{\\theta} (Y|z, X) p_{\\psi} (z|X) dz,$ (14)\n\nwith parameters $\\theta,\\psi$. Conditioning the latent density on the input images is not a necessity but usually preferred for smooth optimization trajectories [54]. As such, the spatial correlation is not explicitly modeled but rather induced through mapping"}, {"title": "1) Generative Adversarial Networks:", "content": "A straightforward approach is to simply learn the decomposition in Equation (14) through sampling from an unconditional prior density\n\n$p_z = \\mathcal{N}(\\mu = 0, \\Sigma = \\sigma \\cdot I),$ (15)\n\nand mapping this to segmentation $Y$ through a generator $G_{\\phi} : \\mathcal{X} \\times \\mathcal{Z} \\rightarrow Y$. Goodfellow et al. [55] show that this approach can notably enhanced through the incorporation of a discriminative function (the discriminator), denoted as $D_{\\psi} : \\mathbb{R}^{C \\times D} \\rightarrow [0,1]$. In this way, $G_{\\phi}$ learns reconstruct realistic looking images, guided by the discriminative capabilities of $D_{\\psi}$, making sufficient resistance from $D_{\\psi}$ to $G_{\\phi}$ imperative. We can denote the cost of $G_{\\phi}$ in the GAN as the negative cost of $D_{\\psi}$ as\n\n$J_{G} = -J_{D} =\\mathbb{E}_{p_{D}}[\\log D_{\\psi}(y)]$\n\n$- \\mathbb{E}_{p_{z}} \\mathbb{E}_{p_{D}} [\\log (1 - D_{\\psi}(G_{\\phi}(z,x)))].$ (16)\n\nWhile conditional GANs had been used for semantic segmentation before [57], Kassapis et al. [56] explicitly contextualized the architecture within aleatoric uncertainty quantification using their proposed Calibrated Adversarial Refinement (CAR) network. The calibration network, $F_{\\theta} : \\mathbb{R}^{C \\times D} \\rightarrow \\mathbb{R}^{K \\times D}$, initially provides a SoftMax activated prediction as $F_{\\theta}(x) = c$, with (cross entropy) reconstruction loss\n\n$L_{rec} = - \\mathbb{E}_{p_{D}} [\\log p_{\\theta}(c|x)].$ (17)\n\nThen, the conditional refinement network $G_{\\phi}$ uses c together with X and latent samples $Z_i \\sim p_z$ injected at multiple decomposition scales i, to predict various segmentation maps. Furthermore, the refinement network is subject to adversarial objective\n\n$L_{adv} = - \\mathbb{E}_{p_{D}} \\mathbb{E}_{p_{z}} [\\log D_{\\psi}(G_{\\phi}(F_{\\theta}(x), z), x),x)],$ (18)\n\nwhich is argued to elicit superior structural qualities compared to relying on cross-entropy alone. At the same time, the discriminator opposes the optimization with\n\n$L_{D} = - \\mathbb{E}_{p_{z}} \\mathbb{E}_{p_{D}} [1 - \\log D_{\\psi}(G_{\\phi}(F_{\\theta}(x), z), x),x)]$\n\n$\\mathbb{E}_{p_{D}} [\\log D_{\\psi}(y)].$ (19)\n\nFinally, the average of the N segmentation maps generated from $G_{\\phi}$ are then compared against the initial prediction of $F_{\\theta}$ through the calibration loss, which is the analytical KL-divergence between the two categorical densities denoted as\n\n$L_{cal} = \\mathbb{E}_{p_{D}} \\mathbb{KL}[p_{\\phi}(y|c, x)||p_{\\theta}(c|x)].$ (20)\n\nIn this way, the generator loss can be defined as\n\n$L_{G} = L_{adv} + \\lambda \\cdot L_{cal},$ (21)\n\nwith hyperparameter $\\lambda \\geq 0$. The purpose of the calibration network is argued to be three-fold. Namely, it (1) sets a calibration target for $L_{cal}$, (2) provides an alternate representation of X to $G_{\\phi}$, and (3) allows for sample-free aleatoric uncertainty quantification. The refinement network can be seen as modeling the spatial dependence across the pixels, which enables sampling coherent segmentation maps through latent variable Z."}, {"title": "2) Variational Autoencoders:", "content": "Techniques such as GANs rely on implicit distributions and are void of any notion of data likelihoods. An alternative approach estimates the Bayesian posterior w.r.t. the latent variables, $p(Z|Y, X)$, with an approximation $q_{\\theta} (Z|Y, X)$ obtained my maximizing conditional Evidence Lower Bound (ELBO)\n\n$p(Y|X) = \\mathbb{E}_{q_{\\theta} (Z|Y,x)} [\\logp(Y|X)]$\n\n$= \\mathbb{E}_{q_{\\theta} (Z|Y,X)} \\log \\frac{p(z, Y|X)}{p(z|Y, X)}$\n\n$\\geq \\mathbb{E}_{q_{\\theta} (Z|Y,X)} [\\log p_{\\phi}(Y|z, X)]$\n\n$- \\mathbb{KL}[q_{\\theta} (z|Y, X)||q_{\\psi}(z|X)].$ (22)\n\nThis is also known as Variational Inference (VI). The first term in Equation (22) represents the reconstruction cost of the decoder subject to the latent code Z and input image X. The second term is the Kullback-Leibler (KL) divergence between the approximate posterior and prior density. As a consequence of the mean-field approximation, all involved densities are modeled by axis-aligned Normal densities and amortized through neural networks parameterized by $\\theta, \\phi$ and $\\psi$. The predictive distribution after observing dataset $\\mathcal{D}$ is then obtained as\n\n$p(Y|x^*) = \\int [P_{\\phi}(Y|z, x^*)q_{\\theta} (z|\\mathcal{D})]dz$ (23)\n\nImplementing the conditional ELBO in Equation (22) can be achieved through a VAE-like architecture [10]. A few additional design choices specific for uncertainty quantification for segmentation result in the The Probabilistic U-Net (PU-Net) [58]. Firstly, the latent variable z is only introduced at the final layers of a U-Net conditioned on X, where the vector is up-scaled through tiling and then concatenated with the feature maps of the penultimate layer, which is followed by a sequence of 1\u00d71 convolutions. When involving conditional latent variables in this manner, it is expected that most of the semantic feature extraction and delineation is performed in the U-Net, while the information Z provides is almost exclusively regarding the segmentation variability. Therefore, relatively smaller values of d are feasible than what is commonly used in image generation tasks."}, {"title": "a) Density parameterization:", "content": "Augmenting a Normalizing Flow (NF) to the posterior density of a VAE is a commonly used tactic to improve its expressiveness [63]. This phenomena has also been successful for CVAE-like models such as the PU-Net [64], [66]. NFs are a class of generative models that utilize k consecutive bijective transformations $f_k : \\mathbb{R}^D \\rightarrow \\mathbb{R}^D$ as $f = f_k \\circ ... \\circ f_k \\circ ... \\circ f_1$, to express exact log-likelihoods of arbitrarily complex distributions $\\log p(x|z)$. These are often denoted as $\\log p(x)$ for simplicity and can be determined through\n\n$\\log p(x) = \\log p_z (z_0) - \\sum_{k=1}^{K} \\log \\det \\frac{df_k(Z_{k-1})}{dz_{k-1}},$ (24)\n\nwhere $z_k$ and $z_{k-1}$ are intermediate variables from intermediate densities and $z_0 = f^{-1}(x)$. Equation (24) can be substituted in the conditional ELBO objective in Equation (22) to obtain\n\n$p(Y|X) \\geq \\mathbb{E}_{q_{\\theta} (Z|Y,X)} [\\log p_{\\phi}(Y|z, X)]$\n\n$- \\mathbb{KL}[q_{\\theta} (z_0|Y, X)||q_{\\psi}(z|X)]$\n\n$- \\mathbb{E}_{q_{\\theta} (z_0|Y,X)} \\sum_{k=1}^{K} \\log \\det \\frac{df_k(Z_{k-1})}{dz_{k-1}},$ (25)\n\nwhere the objective consists of a reconstruction term, sample-based KL-divergence and a likelihood correction term for the change in probability density induced by the NF.\n\nBhat et al. [67], [68] compare this approach with other parameterizations of the latent space including a mixture of Gaussians and low-rank approximation of the full covariance matrix. Valiuddin et al. [79] show that the latent space can converge to contain non-informative latent dimensions, undermining the capabilities of the latent-variable approach,"}, {"title": "b) Multi-scale approach:", "content": "Learning latent features over several hierarchical scales can provide expressive densities and interpretable features across various abstraction levels [82]\u2013 [86]. Such models commonly fall under hierarchical VAE umbrella term. Often, an additional Markov assumption of length T is placed on the posterior as\n\n$q_{\\theta} (Z_{1:T}|Y, X) = q_{\\theta} (Z_1|Y, X) \\prod_{t=2}^{T} q_{\\theta} (Z_t|Z_{t-1}).$ (28)\n\nConsequently, the conditional ELBO is denoted as\n\n$p(Y|X) \\geq \\mathbb{E}_{q_{\\theta} (Z|Y,X)} [\\log p_{\\phi}(Y|z, X)]$\n\n$- \\sum_{t=2}^{T} \\mathbb{KL}[q_{\\theta}(z_t|Y, X, Z_{1:t-1})||q_{\\psi} (Z_t|Z_{1:t-1})]$\n\n$- \\mathbb{KL}[q_{\\theta}(Z_1|Y, X)||q_{\\psi}(Z_1|X)].$ (29)\n\nThis objective is implemented in the Hierarchical PU-Net [71] (HPU-Net). Simply stated, the architecture learns a latent representation at multiple decomposition levels of the U-Net. Furthermore, residual connections in the convolutional layers are necessary to prevent degeneracy of uninformative latent variables with the KL-divergence rapidly approaching zero. For similar reasons, the Generalized ELBO with Constrained Optimization (GECO) objective was employed, which extends on Equation (29) as\n\n$L_{GECO} = \\lambda \\cdot (\\mathbb{E}_{q_{\\theta} (Z|Y,x)} [\\log p_{\\phi}(Y|z, X)] - \\kappa)$\n\n$- \\sum_{2}^{T} \\mathbb{KL}[q_{\\theta} (Z|Y, X, Z_{1:t-1})||q_{\\psi} (Z_t|Z_{1:t-1})]$\n\n$- \\mathbb{KL}[q_{\\theta} (Z_1|Y, X)||q_{\\psi}(Z_1|X)].$ (30)\n\nHyperparameter $\\lambda$ is the Lagrange multiplier update through the Exponential Moving Average of the reconstruction, which is constrained to reach target value $\\kappa$ set beforehand to an appropriate value. Finally, online negative hard mining is used to"}, {"title": "3) Denoising Diffusion Probabilistic Models:", "content": "Recent developments in generative modeling have resulted a family of models known as Denoising Diffusion Probabilistic Models (DDPMs) [90]\u2013[92]. Such models are especially renowned for their expressive power by able to encapsulate large and diverse datasets. While several perspectives can be used to introduce the the DDPMs, we build upon the earlier discussed HVAE to maintain cohesiveness with the overall manuscript. In specific, let us introduce three additional modifications to the HVAE. Firstly, the latent dimensionality is set equal to the data dimensions, i.e. d= D. As a consequence, redundant notation of Z is removed and Y is instead subscripted with t \u2208 {1, ..., T}, indicating the encoding depth, where $Y_0$ is the initial segmentation mask. Secondly, the encoding process (or forward process) is predefined as a linear Gaussian model such that\n\n$q(Y_T| Y_0) = p(y_0) \\prod_{t=1}^{T} q(Y_t| Y_{t-1})$ (31)\n\nand\n\n$q(y_t| y_{t-1}) = \\mathcal{N}(y_t; \\mu = \\sqrt{a_t}y_{t-1}, \\Sigma = (1 - a_t) \\cdot I),$ (32)\n\nwith noise schedule a = {$a_t$}=$_{t=1}^T$. Then, the decoding or reverse process can be learned through $p_{\\phi}(y_{t-1}| y_t, x)$. The ELBO for this objective is defined as\n\n$p(y|x) \\geq \\mathbb{E}_{q(y_1|y_0)} [\\log P_{\\phi} (y_0|y_1, X)]$\n\n$+ \\mathbb{E}_{q(y_t/Y_0)} \\mathbb{KL}[q(y_{t-1}|Y_t)||P_{\\phi}(Y_{t-1}|y_t, x)]$\n\n$+ \\mathbb{E}_{q(y_T|y_0)} \\log \\frac{p(y_T)}{q(Y_T| Y_0)}.$ (33)\n\nAs denoted, the regularization term is assumed to be zero, since we assume that a sufficient amount of steps T are taken such that $q(y_T|y_0)$ is approximately normally distributed. With the reparameterization trick, the forward process is governed by random variable $\\epsilon \\sim \\mathcal{N}(0,1)$. As such, the KL-divergence in the second term can be interpreted as predicting $Y_0$, the source noise $\\epsilon$ or the score $\\nabla_{y_t} \\log q(y_t)$ (gradient of the data log-likelihood) from $Y_t$ depending on the parameterization, and is in almost all instances approximated with a U-Net.\n\nIt has also be proposed to model Bernoulli noise instead of Gaussian [93]\u2013[96]. However, most methodologies vary in the conditioning of the reverse process on the input image [97]- [100]. For instance, Wolleb et al. [97] concatenate the input image with the noised segmentation mask. Wu et al. [98] insert encoded image features to the U-Net bottleneck. Additionally, information on predictions $Y_t$ at a time step t is also provided in the intermediate layers of the conditioning encoder. This is performed by applying the Fast Fourier Transform (FFT) on the U-Net encoding, followed by a learnable attentive map and"}, {"title": "V. EPISTEMIC UNCERTAINTY", "content": "The crucial difference between epistemic and aleatoric uncertainty is that the former is related to model ignorance, while the latter reflects statistical ambiguity inherent in the data. Epistemic uncertainty can be further categorized into two distinct types [12]. The first type pertains uncertainty related to the capacity of the model. For example, under-parameterized models or approximate model posteriors can become too stringent to appropriately resemble the true posterior. The ambiguity on the best parameters given the limited capacity induces uncertainty in the learning process, this is also known as model uncertainty. Nevertheless, given the complexity of contemporary parameter-intensive CNNs, the model uncertainty is often assumed to be negligible. A more significant contribution to the epistemic uncertainty is due to the limited data availability, known as approximation uncertainty, and can often be reduced by collecting more data. Both model and approximation uncertainty contribute to the epistemic uncertainty."}, {"title": "A. Variational Inference", "content": "Consider a simpler, tractable density $q(\\theta|\\eta)$, parameterized by $\\eta$, to approximate $p(\\theta|y, x)$. Then, we can achieve Variational Inference (VI) w.r.t. to the parameters by minimizing the Kullback-Leibler (KL) divergence between the true and approximated Bayesian posterior can be written as\n\n$\\eta^* = \\text{arg min} \\mathbb{KL} [q(\\theta|\\eta) || p(\\theta|y, x)]$\n\n$= \\text{arg min} \\int q(\\theta|\\eta) \\log \\frac{q(\\theta|\\eta)}{p(\\theta|y, x)} d\\theta$\n\n$= \\text{arg min} \\int q(\\theta|\\eta) \\log \\frac{q(\\theta|\\eta) p(y|x)}{p(Y|x, \\theta)p(x, \\theta)} d\\theta$\n\n$= \\text{arg min} \\mathbb{KL} [q(\\theta|\\eta)||p(\\theta)] - \\mathbb{E}_{q(\\theta|\\eta)} [\\log p(y|x, \\theta)],$ (34)\n\nwhere the parameter-independent terms are constant and therefore excluded from notation. A popular choice for the approximated variational posterior is a the Gaussian distribution, i.e. a mean $\\mu$ and covariance $\\sigma$ parameter for each element of the convolutional kernel, usually with zero-mean unitary Gaussian prior densities. However, the priors can also be learned through Empirical Bayes [117]. Furthermore, backpropagation is possible with the reparameterization trick [10] and within this context, the procedure is referred to as Bayes by Backprop (BBB) [8]. During testing, a sample-based approach is utilized to approximate the posterior. Since using several parameter permutations effectively enriches the hypothesis space due to the model combining effect and can already improve performance [118]-[120]."}, {"title": "B. Monte Carlo Dropout", "content": "Dropout, a common technique used to regularize neural networks [121], can mimic sampling from an implicit parameter distribution, $q(\\theta|0, p)$, defined as\n\n$\\eta \\sim Bernoulli(p)$ (35a)\n\n$\\theta \\sim \\theta \\cdot \\eta,$ (35b)\n\nwith probability p and $\\eta$ operating element-wise on the parameters. Using Dropout can also be interpreted as a first-order equivalent $L_2$-regularization with additional transforming the input by the inverse diagonal Fisher information matrix [121]."}, {"title": "VI. APPLICATIONS", "content": "This section explores literature that employs uncertainty-based downstream tasks on segmentation models. These include estimating the segmentation mask distribution subject to observer variability (Section VI-A), model introspection (i.e. ability to self-assess, Section VI-B), improved generalization (Section VI-C) and reduced labeling costs using Active Learning (Section VI-D)."}, {"title": "A. Observer variability", "content": "After observing sufficient data, the variability in the predictive distribution is often considered to be negligible and is therefore omitted. Nevertheless, this assumption becomes excessively strong in ambiguous modalities, where its consequence is often apparent with multiple varying, yet plausible annotations for a single image. Additionally, such annotations can also vary due to differences in expertise and experience of annotators. This phenomenon of inconsistent labels across annotators is known as the inter-observer variability, while variations from a single annotator is referred to as the intra-observer variability.\n\nTo contextualize this phenomenon within the framework of uncertainty quantification, annotators can be treated as models themselves. For example, consider K separate annotators modeled through parameters $ \\theta_k $ with k = 1, 2, ..., \u039a. For a simple segmentation task, it can be expected that $\\text{Var}[p(\\theta_k)", "text{Var}[p(\\phi)": ""}]}