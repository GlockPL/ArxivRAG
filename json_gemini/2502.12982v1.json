{"title": "Sailor2: Sailing in South-East Asia with Inclusive Multilingual LLMs", "authors": ["Longxu Dou", "Qian Liu", "Fan Zhou", "Changyu Chen", "Zili Wang", "Ziqi Jin", "Zichen Liu", "Tongyao Zhu", "Cunxiao Du", "Penghui Yang", "Haonan Wang", "Jiaheng Liu", "Yongchi Zhao", "Xiachong Feng", "Xin Mao", "Man Tsung Yeung", "Kunat Pipatanakul", "Fajri Koto", "Min Si Thu", "Hynek Kydl\u00ed\u010dek", "Zeyi Liu", "Qunshu Lin", "Sittipong Sripaisarnmongkol", "Kridtaphad Sae-Khow", "Nirattisai Thongchim", "Taechawat Konkaew", "Narong Borijindargoon", "Anh Dao", "Matichon Maneegard", "Phakphum Artkaew", "Zheng-Xin Yong", "Quan Nguyen", "Wannaphong Phatthiyaphaibun", "Hoang H. Tran", "Mike Zhang", "Shiqi Chen", "Tianyu Pang", "Chao Du", "Xinyi Wan", "Wei Lu", "Min Lin"], "abstract": "Sailor2 is a family of cutting-edge multilingual language models for South-East Asian (SEA) languages, available in 1B, 8B, and 20B sizes to suit diverse applications. Building on Qwen2.5, Sailor2 undergoes continuous pre-training on 500B tokens (400B SEA-specific and 100B replay tokens) to support 13 SEA languages while retaining proficiency in Chinese and English. Sailor2-20B model achieves a 50-50 win rate against GPT-40 across SEA languages. We also deliver a comprehensive cookbook on how to develop the multilingual model in an efficient manner, including five key aspects: data curation, pre-training, post-training, model customization and evaluation. We hope that Sailor2 model (Apache 2.0 license) will drive language development in the SEA region, and Sailor2 cookbook will inspire researchers to build more inclusive LLMs for other under-served languages.", "sections": [{"title": "1 Introduction", "content": "Large language model (LLM) technology has driven significant innovations but remains predominantly focused on major languages like English and Chinese, leaving many others underrepresented. As a linguistically diverse region with 11 countries and 675 million people, Southeast Asia presents a unique opportunity for advancing multilingual NLP research. In this paper, we introduce Sailor2, a contribution to the advancement of language technology in the SEA region. Sailor2 offers improved open models, open tools, a transparent training recipe, and valuable insights to drive progress in multilingual LLMs.\nTo optimize Sailor2, we apply the following techniques:\n\u2022 Rigorous data deduplication with six layers.\n\u2022 Model expansion to mitigate language degeneration.\n\u2022 Two-stage continual pre-training with varying language compositions.\n\u2022 Two-stage instruction tuning with reward-aware and ppl-aware data selection.\n\u2022 Preference tuning on both off-policy and on-policy data.\nWe devoted significant effort to evaluation, which includes: (1) few-shot evaluation for the base model, (2) chat performance comparison with GPT-4, and (3) cultural understanding about SEA cuisine and traditions. The results indicate that Sailor2 excels at both basic language tasks (e.g., question answering and translation) and advanced tasks (e.g., mathematics and creative writing).\nOverall, the Sailor2 project contributes to the following outcomes:\n\u2022 A family of open models, optimized for Southeast Asian (SEA) languages.\n\u2022 A comprehensive cookbook detailing the process of building multilingual LLMs, covering data curation, model training, and thorough evaluation."}, {"title": "2 Related Works", "content": "Open science has gained increasing attention, particularly with the thriving efforts in developing open language models. While notable initiatives like OLMo, LLM360, and MAP-Neo have made significant contributions, they primarily focus on dominant languages on the Internet, such as English and Chinese. The Aya model serves as a massively multilingual language model, supporting 101 languages, beats previous multilingual models such as BloomZ, yet not particularly expert in South-East Asian (SEA) languages. Although there has been some recent progress in creating SEA language models, open initiatives such as the SeaLLM series and Sea-LION series still fall short of achieving performance levels comparable to commercial models, such as GPT-40.\nStarting in March 2024, we have continuously released both Sailor and Sailor2. We are committed to building a fully open pipeline for the entire LLM ecosystem while striving to achieve top-tier SEA language performance. In the future, we will continue refining the Sailor series models to advance open language models for more low-resource languages."}, {"title": "2.2 Open SEA Language Resources", "content": "Resources for SEA languages remain underdeveloped. Pre-training: Even the recent Fineweb2 Dataset, which scales the pre-training corpus to over 1,000 languages, provides a significantly smaller data volume for SEA languages compared to others, falling short of the 100B tokens. Moreover, directly translating English resources into local languages often leads to an overestimation of performance, as these translations typically lack culturally nuanced content. Post-training: The Aya dataset is the largest multilingual instruction fine-tuning resource, containing 513 million instances across 114 languages. It comprises mainly machine-translated data with a small, essential human-curated subset. Evaluation: Although benchmarks such as SeaBench, SeaCrowd, and SeaEval have been introduced, they remain limited in either language coverage, primarily focusing on Thai, Indonesian, Vietnamese, and Malay, or in dataset quality due to reliance on machine translations.\nIn the Sailor2 project, we open source the SailCraft scripts for SEA-language-specific data cleaning, the instruction tuning dataset covering 17 SEA languages, SailCompass evaluation suit for base model evaluation, and the SEA-WildBench for chat model evaluation."}, {"title": "2.3 Cookbook for Multilingual Language Models", "content": "There have been swift advancements and many explorations in multilingual large language models. FinGPT builds on BLOOM through continual pretraining (CPT), primarily targeting Finnish and other low-resource languages, while incorporating English data for optimization. MAP-Neo is a recently released 7B bilingual Chinese-English Bilingual model, designed with a from-scratch approach. Notably, it offers full transparency, particularly in pretraining corpus collection, processing, and cleaning, providing detailed records and rigorous data curation rules. Jais, an Arabic-centric multilingual model, is trained from scratch on Arabic and English data, with followup safety tuning, offering a structured guidance recipe for optimizing model safety. BritLLM is a UK-centric LLM initiative, aiming to develop open pipelines tailored to UK-specific needs, including law, finance, healthcare, and multilingual diversity.\nThe Sailor2 project also actively explores multilingual LLM development, offering a cookbook while addressing key challenges such as English performance degradation, multilingual data collection and cleaning, optimal language mixing strategies, multi-stage training, post-training techniques, inference acceleration, and more."}, {"title": "3 Data Curation", "content": "Sailor2 showcases substantial improvements in pre-training data quality over its predecessor Sailor, driven by several key factors:\n1. Better data sourcing.\n2. Better data filtering.\n3. Data recall for low-resource languages.\n4. Swift data-mixture in multilingual training.\nWith these enhancements, we have a larger and high-quality continual pre-training corpus, expanding from 150 Billon SEA tokens in Sailor to 400 Billion SEA tokens, covering 13 SEA languages as listed in Table 2."}, {"title": "3.1 Web Data Curation", "content": "All the data used for Sailor2 is sourced from publicly available resources.\nFor the replay data employed during continual pre-training to prevent model degeneration, we select Fineweb-Pro for English, Chinese-Fineweb-Edu for Chinese, and Open-Web-Math-Pro for math. Since our current focus is on general multilingual LLMs rather than coding models, we deliberately avoid including code data in the replay to safeguard multilingual performance.\nFor SEA language data that provide local text and knowledge, we extract content from 96 CommonCrawl snapshots spanning from summer 2013 to April 2024. Additionally, to extract high-quality and professional text, we also leverage publicly available PDFs.\nFor the bilingual data used to organize the code-switch dataset, we follow the Sailor approach by selecting Open Subtitles and open translation data\u2074. Subtitles typically consist of brief, conversational sentences. To generate longer, more coherent documents, we employ a sliding window of 100 to concatenate adjacent subtitle segments."}, {"title": "3.2 Synthetic Data Curation", "content": "To address challenges in selecting high-quality datasets for low-resource languages, we leverage the NLLB-3.3B model to translate high-quality English documents into local languages. For each language, we train a FastText classifier following the approach of Li et al. (2024) to identify high-quality text. Specifically, we generate a training set comprising 10,000 positive examples and 10,000 negative examples. The positive examples are obtained by machine-translating high-quality English datasets, 40% from Cosmopedia, 40% from MADLAD, and 20% from UltraChat. The negative examples are randomly sampled from the CommonCrawl corpus for each language. Once trained, the classifiers rank documents in the CommonCrawl corpus based on their likelihood of being a positive example. We then select the top 20% as the high-quality subset for annealing."}, {"title": "3.3 Data Cleaning", "content": "We leverage SailCraft for comprehensive data processing consisting of six layers filtering5. It employs rule-based cleaning, model-based filtering, near deduplication, exact deduplication, URL deduplication, and frequent line removal. During URL deduplication, we prioritize documents with more content, effectively reducing total tokens by nearly 50%. As for the frequent line removal, following the Llama3 approach, we remove lines appearing more than 5 times in 10M document buckets, successfully eliminating nearly 5% of total tokens, most of which were determined to be meaningless content.\nTable 3 (with tokens counted using the Qwen2.5 tokenizer) presents the raw tokens used for Sailor2 training after data cleaning and deduplication. We subsequently downsample or upsample portions of this data to achieve a more balanced training set (see Section 3.4)."}, {"title": "3.4 Data Mixture", "content": "We employe RegMix to optimize the data mixture, with the primary objective of maximizing the log sum across all languages considered in stage 1. Unlike our previous practices in Sailor that used 0.5B models as proxy models for data mixture, we follow RegMix and utilize 1M samll models as our proxy model, even for the scenario of continual pre-training. Our underlying assumption is that if a model can be trained over an extended period, the converged or equivalent data mixture should remain relatively consistent. Please refer to RegMix for more implementation details."}, {"title": "4 Model Continual Pre-Training", "content": "The Sailor2 model comes in three sizes, 1B, 8B, and 20B, which are expanded from the Qwen2.5 base models of 0.5B, 7B, and 14B, respectively. The decision was made to perform model expansion prior to continual pre-training in order to mitigate the potential for forgetting of English and Chinese language capabilities, while also enhancing the model's capacity for further improvements in SEA languages.\nIn practice, the approach draws inspiration from LlamaPro, leveraging a block-expansion mechanism in the original Qwen2.5 model. This approach significantly enhances the model's performance in SEA languages while maintaining stable capabilities in English and Chinese. By utilizing the strategy, the newly introduced layers are able to store the additional SEA knowledge from the continually pre-trained tokens, rather than overwriting the existing linguistic information of the other languages."}, {"title": "4.2 Model Parallel Optimization", "content": "We leverage key Megatron-LM optimizations to accelerate training."}, {"title": "4.2.1 Zero Bubble Pipeline Parallelism", "content": "Zero Bubble Pipeline Parallelism minimizes GPU idle time by splitting the backward pass into input and weight components, prioritizing the former. While ZB-2P or ZBV could fully eliminate pipeline bubbles for better throughput, we opt for the simpler ZB-H1, which reduces bubbles to 1/3 with just 80 lines of code changes in Megatron-LM."}, {"title": "4.2.2 Large Vocabulary Optimization", "content": "As vocabulary size increases, placing vocabulary layers in the first or last pipeline stage leads to imbalanced computation and memory usage. For Sailor2-8B, a single vocabulary layer is roughly equivalent to four transformer layers, increasing memory usage and GPU idle time, often resulting in out-of-memory (OOM) errors. Moreover, Zero Bubble Pipeline Parallelism further exacerbates this by delaying weight gradient computation, making vocabulary activations long-lived and a memory bottleneck. While Vocabulary Parallelism proposed in Yeung et al. (2024) proposes a perfect balance, we take a simpler approach: redistributing transformer layers from the last stage to other stages (excluding the first) based on FLOP calculations, which also eliminates the last stage's extra memory overhead."}, {"title": "4.3 Intra-Document Training", "content": "We employ intra-document masking to disable cross-document attention within a packed sequence. It has been shown in previous studies that it improves pretraining compared to fully-open attention by a large margin, especially when the documents are randomly concatenated with each other. It has also been shown to be effective in large-scale pretraining. Specifically, during pretraining, we replace the attention module in Megatron with the flash_attn_varlen function and pass the length information of the documents in the pretraining corpus to ensure that attention is computed only within the same document, avoiding the calculation of cross-document scores."}, {"title": "4.4 Two-Stage Continual Pre-Training", "content": "We adopt a two-stage pre-training approach inspired by MiniCPM. In stage one, we train on comprehensive datasets at a high learning rate (1e-4) and 1,024 global batch size, introducing high-resource languages such as English, Chinese, Vietnamese, Indonesian, Thai, Malay, Burmese, Tagalog, and Khmer. In stage two, we shift to high-quality tokens with a lower learning rate (1e-5) and 4,096 global batch size, and expand to include both high-resource and low-resource languages, adding Cebuano, Lao, Javanese, Waray, Sundanese, and Ilocano. This strategy automatically mixes data in stage one and seamlessly integrates high-quality low-resource tokens in stage two without adjusting mixing ratios."}, {"title": "4.4.1 Stage 1: Pre-training with Balanced Data Mixture", "content": "In stage 1, we select a subset of languages that could provide sufficiently enough tokens for Regmix data mixture optimization. After conducting 1,000 runs of data mixture optimization using 1M models, we observed a subtle shift from the original token distribution. Notably, the optimized data mixture resulted in upsampling languages like Khmer, Malay, Burmese, Thai, and Tagalog, while simultaneously downsampling Indonesian and Vietnamese. The final data mixture of Stage 1 is shown in Table 4 (tokens counted in the tokenizer of Qwen2.5)."}, {"title": "4.4.2 Stage 2: Annealing with High-Quality Tokens", "content": "In stage 2, we lower the learning rate to 1e-5 (1/10 of the original learning rate), and take 20% of the stage 1 dataset to make sure the model still behaves well on the original distribution. As for the remaining 80% training budget, we allocate them to high-quality SEA tokens, where all low-resource languages are added, and the token distribution of high-resource languages is maintained as similar to the stage 1. In addition, we also added some English instruction tuning datasets and some datasets contributed by the Sailor2 community."}, {"title": "5 Model Post-Training", "content": "Sailor2 employs the following post-training techniques: (1) two-stage instruction tuning using 4.8M examples from SEA-UltraChat, covering 14 SEA languages; and (2) two-stage preference tuning on both off-policy data from SEA-UltraFeedback and on-policy preference data. Table 22 summarizes the statistics for SEA-UltraChat and SEA-UltraFeedback."}, {"title": "5.1 Instruction Tuning", "content": "As described in Section 2.2, existing instruction tuning datasets for SEA languages are limited in both quality and quantity. To address this, we translate UltraChat, a high-quality and diverse English instruction dataset, into 15 SEA languages using GPT-40-0803, resulting in 4.4 million multilingual examples. Translating code and math data into multiple languages remains particularly challenging. To mitigate this, we developed a novel multi-round translation prompt6.\nData Cleaning. The dataset is first partitioned by language, and each entry is assigned a MinHash signature using 128 permutations. These signatures are then compared using a Locality-Sensitive Hashing (LSH) index with a Jaccard similarity threshold of 0.8, enabling efficient identification of near-duplicate entries. The data entries are also verified against a strict message format specification: (1) a system prompt, if present, must appear as the first message; (2) user queries and assistant responses must strictly alternate, with the assistant's response being the final message; and (3) all messages must contain non-empty content. Through this process, the deduplication phase eliminated 1.4% of the original data rows7, while the verification filtered out about 1K invalid samples. Finally, SEA-UltraChat comprises 4.8 million examples across 14 Southeast Asian languages, as detailed in Table 22."}, {"title": "5.1.2 Two-Stage Instruction Tuning", "content": "In developing multilingual models, maintaining balance across languages and domains is crucial. However, our supervised fine-tuning dataset exhibits significant imbalances in both dimensions, as shown in Figure 3: language distribution ranges from 34.8% for English to merely 0.6% for low-resource languages like Acehnese, while domain coverage shows a substantial difference in percentage, with creative tasks significantly greater than coding and mathematical content.\nTo address these imbalances, we employ the two-stage instruction tuning inspired by Huang et al. (2024). Stage 1 establishes a broad foundation by processing the bulk of the training data with a large batch size of 4096 over a single epoch. To optimize learning, the learning rate is gradually decreased from 7 \u00d7 10\u22126 to 7 \u00d7 10\u20137. Building upon this base, Stage 2 then focuses on a carefully selected subset of data balanced across both languages and domains, employing a small batch size of 512 over 3 epochs. This strategic approach maximizes the use of instruction data while ensuring the model maintains balance across dimensions."}, {"title": "5.1.3 Instruction Data Selection for Stage 2", "content": "To select high-quality data for stage 2, we annotate each sample with two metrics: (1) a reward score from a reward model, and (2) the perplexity computed by Sailor2-8B. Both metrics are normalized by computing their percentiles within each language and category. Figure 4 displays the distribution of English instruction data in the Creative Task category. Our case study in Table 6 demonstrates that instruction data with both high reward scores and high perplexity are particularly valuable for stage 2 training. In general, a high reward score indicates a high-quality response, while high perplexity suggests that such responses are under-trained. Based on this analysis, we rank the instruction data using the harmonic mean (i.e., the product divided by the sum) of their reward and perplexity percentiles.\nAfter ranking, we apply an embedding-based deduplication step to select a fixed number of final candidates for each category and language. Specifically, we utilize the jinaai/jina-embeddings-v3 model from HuggingFace to generate embeddings and filter out any data point whose cosine similarity with an already selected item exceeds 0.6."}, {"title": "5.2 Preference Tuning", "content": "In Sailor2, we perform the preference tuning to enhance model performance beyond supervised fine-tuning. This section first introduces the problem formulation of reinforcement learning from human feedback and the learning algorithms examined in this work (Sec. 5.2.1). We then describe the pipeline for constructing preference data in SEA languages (Sec. 5.2.2) and present the full recipe of the preference tuning (Sec. 5.2.3). In addition, we provide extensive ablation study results on preference data construction in Sec. 8.3."}, {"title": "5.2.1 Background", "content": "In preference tuning, the preference data typically takes the form of pairwise preferences. Each prompt x is paired with two possible responses, y\u2081 and y2. The human annotator or AI annotator provides the preference feedback 0(y1 > y2|x) \u2208 {0,1}, indicating whether y\u2081 is preferred over y2. The preferred response is denoted as yw, while the other is denoted as y\u2081.\nPolicy optimization algorithms. DPO is introduced to optimize the policy model in an offline manner. Rafailov et al. (2024) demonstrates that it directly optimizes the RLHF objective using the following equivalent formulation:\n$L_{DPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D} \\left[ log \\sigma \\left(\\beta log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} \\right) \\right]$   (1)\nUnlike the classic RLHF pipeline which first trains a reward model and then optimizes the policy using the trained RM, DPO optimizes the policy while simultaneously training an implicit reward model. This approach allows DPO to directly optimize the policy using preference pairs, thereby simplifying the preference-tuning pipeline. Recently, many variants have been proposed to improve the vanilla DPO algorithm In this work, we explored three promising approaches including SimPO, length-normalized DPO (LN-DPO), and length-regularized DPO (LR-DPO). Our experiment results indicate that LR-DPO achieves a favorable balance between performance and verbosity. The objective of LR-DPO is defined as follows:\n$L_{LR-DPO}(\\pi_\\theta; \\pi_{ref}) = -E_{(x,y_w,y_l)\\sim D} \\left[ log \\sigma \\left(\\beta log \\frac{\\pi_\\theta(y_w|x)}{\\pi_{ref}(y_w|x)} - \\beta log \\frac{\\pi_\\theta(y_l|x)}{\\pi_{ref}(y_l|x)} + a y_w - y_l \\right) \\right]$   (2)\nThe additional length difference term serves as a regularizer, down-weighting the gradients of preference pairs in which the preferred response is longer, and vice versa. This mitigates length exploitation in preference tuning."}, {"title": "5.2.2 Preference Data", "content": "Our preference tuning consists of two stages, training with off-policy responses generated by Llama-3-8B-Instruct and training with on-policy responses generated by Sailor2 suite. Additionally, we conduct the preference distillation from our 20B model to smaller models.\nOff-policy Data. To construct the off-policy dataset, we first translate the UF-Llama3 preference dataset into SEA languages. Low-quality translations are filtered based on perplexity scores obtained from the Sailor2-8B base. The resulting off-policy dataset is a mixture of SEA languages and English. Note that GPT-40 struggles with translating extremely low-resource languages such as Lao and Khmer, often producing outputs with excessive emojis and improper formatting. We find these cases using a simple script and translate them into the target language using Deepseek-V3, which has demonstrated superior performance as evaluated by Huang et al. (2025).\nOn-Policy Data. At this stage, we use the prompts from the off-policy dataset to generate responses with the corresponding model. These responses are scored by the open-source"}, {"title": "5.2.3 Preference Tuning Recipe", "content": "Due to the absence of a high-quality reward model for PPO-based algorithms, we explore different direct alignment algorithms, such as DPO and its variants (SimPO, LN-DPO, LR-DPO). LN-DPO optimizes the length-averaged log-probabilities, while LR-DPO explicitly introduces the response length as the regularizer in their objective. We extensively tuned hyperparameters and conducted ablation studies to optimize model performance. Table 7 summarizes the hyperparameter search space, and Table 8 lists the final preference tuning settings. LR-DPO, offering a good balance between performance and verbosity, was chosen to train our final models.\nAll experiments were conducted with the training framework, Oat, which enables large-scale and flexible training."}, {"title": "6 Model Customization", "content": "A 128K token context window allows large language models (LLMs) to handle complex tasks such as multi-document question answering, repository-level code comprehension, and many-shot learning by capturing long-range dependencies, leading to more coherent and contextually relevant outputs.\nThe Sailor2 series employs AnchorAttention to extend its maximum context length from 4K to 128K. In particular, Sailor2 masks out cross-document attention to prevent the model from aggregating irrelevant information across irrelevant documents. Note, this strategy that aligns with the approach used during pretraining. By maintaining a consistent masking paradigm in both pretraining and long-context training, Sailor2 mitigates potential conflicts that could arise from shifting between different attention mechanisms.\nUnlike approaches such as LLaMA3, which rely solely on cross-document attention masking, Sailor2 introduces an anchor token that serves as a stable reference point. Specifically, the first token in each training sequence (the <eos> token of each sample) retains a fixed positional ID and is therefore visible to all documents within the training context. This design helps reduce numerical instability and provides the model with a consistent anchor across the extended sequence. Furthermore, instead of resetting the positional IDs to 0 for each new document, Sailor2 maintains continuous positional indexing across the entire sequence, allowing the model to fully utilize the entire position range in training.\nWith AnchorAttention, Sailor2 efficiently achieves long-context capabilities while training on a relatively small amount of data. Specifically, Sailor2 uses a total of 4 billion tokens in 1,000 steps (4 million tokens per step) at a learning rate of 2 \u00d7 10\u20135, with the first 200 steps designated as warm-up. Despite the limited token budget, Sailor2 effectively extends its context length, as demonstrated by the model's performance on the RULER benchmark , as shown in Table 9."}, {"title": "6.2 Speculative Decoding", "content": "To accelerate model inference, we adopted speculative decoding, a technique designed to reduce the computational cost of autoregressive generation. Specifically, we customized a one-layer draft model, GliDe, for Sailor 8B and 20B.\nBackground. GliDe is a draft model based on a transformer decoder-only architecture that retains standard components\u2014self-attention, cross-attention, and feed-forward networks (FFNs). In GliDe, the conventional self-attention layer is applied first, where each token in the sequence attends only to its preceding tokens. This is immediately followed by a cross-attention layer, which reuses precomputed and cached cross-attention outputs from the target LLM instead of recomputing the keys and values for each draft token. This approach yields a more precise token representation while reducing redundant computations. Finally, the cross-attended outputs pass through position-wise FFNs to further refine token representations. The processing sequence follows: self-attention \u2192 cross-attention \u2192 FFN.\nImplementation Details. Unlike GliDe, we share the weights of the embedding layer and LM head between the target and draft models, significantly reducing memory consumption, especially for large-vocabulary LLMs. Moreover, to improve the stability and robustness of the draft model, we employed a flash noise training technique to replace the cape mask in the original GliDe, which can not only solve the problem of training-inference discrepancy but also be compatible with Flash Attention (Dao et al., 2022). Specifically, for the cross-attention query Qt in the draft model, we can only ensure access to the corresponding key-value states $K_{<t\u2019}$, $V_{<t\u2019}$ that satisfy $1 < |t\u2019 \u2013 t| < \\gamma$, where y denotes the number of speculative steps. During training, we randomly shift the indices of queries and key-value states within the range $1 < j < \\gamma$. If the sequence length is l, we then compute $O_{zj}$ = flash_attn(Q_{>j}, K_{<1-j}, V_{<1-j})$. This approach effectively enforces the same visibility constraints as those in the inference phase, i.e., $1 \\le |t' - t| < \\gamma$, thereby ensuring that the training process aligns with inference behavior.\nDuring speculative decoding inference, we first generate tokens autoregressively using the one-layer draft model, followed by parallel verification with Sailor 2. This approach effectively reduces the number of autoregressive steps required for decoding. Since tree-based speculative decoding is incompatible with Flash Attention, we opted for a straightforward sequential speculative decoding strategy. We set the speculation length y to 4 based on empirical observations.\nPerformance. The performance of our GliDe model is illustrated in Figure 5 and Figure 6, demonstrating an approximate 2\u00d7 acceleration. Notably, for Burmese (mya), our approach achieves an accept length exceeding 3 and a speedup of approximately 2.5\u00d7. We attribute this improvement to the high tokenization granularity of Burmese, which provides a greater margin for speculative decoding to optimize token generation."}, {"title": "6.3 Model Pruning", "content": "By leveraging existing pre-trained models, the pruning method enables the rapid generation of smaller-scale models, which avoids the high costs of training from scratch. In this study, we apply the Sheared LLaMA method(Xia et al., 2023) to prune the Sailor2-20B and Sailor2-8B models, resulting in Sailor2-14B and Sailor2-3B respectively12. The pruning stage takes 6B tokens. Subsequently, we performed continual training using a dataset of 180B tokens to recover the models' performance.\nBackground. The Sheared LLaMA method focuses on structured pruning to produce smaller yet competitive models from pre-trained larger models. It employs two main techniques: targeted structured pruning and dynamic batch loading. Targeted structured pruning compresses a model into a target architecture via Lo-regularized binary mask learning. Lagrange multipliers are applied to enforce constraints on the target architecture, ensuring the pruned model adheres to the desired configuration while optimizing performance. Dynamic batch loading adjusts training data batches based on domain-specific loss reduction rates, enhancing data efficiency and accelerating convergence.\nImplementation Details. In contrast to the Sheared LLaMA method, we introduce several optimizations in our approach. First, instead of pruning Multi-Head Attention as in the original method, we retain the Key and Value Heads in the Grouped Query Attention structure, pruning only an equal number of Query Heads from each Query Head Group corresponding to the KV Heads. Second, we do not prune the layer dimension, as our preliminary experiments have shown that pruning the layer dimension leads to convergence difficulties. Instead, we focus on optimizing other dimensions (i.e., hidden dimension, head number, and intermediate dimension). Third, to maintain consistency between the hidden dimensions and the number of attention heads, the pruning options are limited. We recommend conducting ablation studies with minimal continual training to identify optimal configurations. Finally, during continual training, we have not used the dynamic batch loading strategy, as it is complex to divide the pretraining data into several domains explicitly. Instead, we directly sample from the Sailor2 Stage-2 training dataset, achieving promising results.\nPerformance. To obtain the final chat models, we train two pruned models using the Sailor2 post-training pipeline, resulting in Sailor2-3B-Chat and Sailor2-14B-Chat. The experimental results in Table 12 demonstrate that these pruned models significantly outperform the baseline Qwen2.5 in low-resource languages such as Khmer and Lao."}, {"title": "7 Evaluation", "content": "For base model evaluation, we focus on the basic language understanding task like sentence classification, and language generation task like question answering and machine trans-lation. Specially, we evaluate Sailor2 on SailCompass evaluation suite and FLoRes-200 translation suite. To expand the evaluated language coverage, we choose the dataset in Indonesian, Thai, Vietnamese, Malay and Javanese."}, {"title": "7.2 Evaluation on Chat Model", "content": "We aim to comprehensively evaluate the performance of our Chat Model by using WildBench as the primary evaluation dataset. WildBench covers five tasks: Coding & Debugging, Information Seeking, Math & Data, Reasoning & Planning, and Creative Tasks. We employ GPT-40-0806 to translate WildBench into eight SEA languages (Thai, Vietnamese, Indonesian, Tagalog, Burmese, Khmer, Lao, and Malay), thereby creating a new benchmark named SEA-WildBench (SWB).\nDetailed results are presented in Table 14 (task-level) and Table 15 (language-level). We use the SWB Score as our evaluation metric, which is calculated based on the win-rate against GPT-40-0806 (the same model serves as the judge). We selected the most representative open models, including both general-purpose and SEA language-optimized variants. For improved visualization, we use Llama-3.1-70B-Instruct as the baseline, with a SWB Score of 30. Our results indicate that both Sailor2-20B-Chat and Sailor2-8B-Chat achieve superior performance across various tasks and languages. As shown in Table 15, Sailor2 models excel in low-resource languages. Notably, Sailor2-20B-Chat achieves nearly a 50% win rate against GPT-40-0806 on SeaWildBench, demonstrating GPT-40-level performance in local chat scenarios for Southeast Asian languages.\nNote that the overall SWB Score can be higher than the scores for individual subsets. For example, although Llama-2-7B-Chat scores below 0.05 on each subset, its overall SWB Score is 0.05. We follow the WildBench score calculation 15. This method may overestimate scores in cases where parse errors occur."}, {"title": "8 Analysis", "content": "This section presents our insights on building multilingual LLMs during both the continual pre-training and post-training stages. We also examine Sailor2's capabilities in translation and cultural understanding, two core components of practical multilingual applications."}, {"title": "8.1 Effect of Model Expansion", "content": "For both Sailor and Sailor2, we adopt a continual pre-training (CPT) approach to efficiently develop multilingual LLMs by reusing computational resources. Unlike Sailor, Sailor2 incorporates model expansion, which creates additional capacity for learning new knowledge from the multilingual corpus.\nWe analyze the perplex"}]}