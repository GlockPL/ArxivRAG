{"title": "Rebalanced Vision-Language Retrieval Considering Structure-Aware Distillation", "authors": ["Yang Yang", "Wenjuan Xi", "Luping Zhou", "Jinhui Tang"], "abstract": "Vision-language retrieval aims to search for similar instances in one modality based on queries from another modality. The primary objective is to learn cross-modal matching representations in a latent common space. Actually, the assumption underlying cross-modal matching is modal balance, where each modality contains sufficient information to represent the others. However, noise interference and modality insufficiency often lead to modal imbalance, making it a common phenomenon in practice. The impact of imbalance on retrieval performance remains an open question. In this paper, we first demonstrate that ultimate cross-modal matching is generally sub-optimal for cross-modal retrieval when imbalanced modalities exist. The structure of instances in the common space is inherently influenced when facing imbalanced modalities, posing a challenge to cross-modal similarity measurement. To address this issue, we emphasize the importance of meaningful structure-preserved matching. Accordingly, we propose a simple yet effective method to rebalance cross-modal matching by learning structure-preserved matching representations. Specifically, we design a novel multi-granularity cross-modal matching that incorporates structure-aware distillation alongside the cross-modal matching loss. While the cross-modal matching loss constraints instance-level matching, the structure-aware distillation further regularizes the geometric consistency between learned matching representations and intra-modal representations through the developed relational matching. Extensive experiments on different datasets affirm the superior cross-modal retrieval performance of our approach, simultaneously enhancing single-modal retrieval capabilities compared to the baseline models.", "sections": [{"title": "I. INTRODUCTION", "content": "In re-examining current vision-language retrieval tasks [1, 2, 3], approaches typically focus on searching images for given texts or retrieving texts from image queries [4]."}, {"title": "II. RELATED WORK", "content": "To learn matching representations of heterogeneous modalities, a large number of vision-language retrieval models are proposed [23, 24, 25, 26]. Traditional approaches [1, 6, 8, 9, 27] always utilize dual architecture, where the image and text modalities are separately embedded into a common space and then maximize the cross-modal representation similarity. For example, [8] built two independent modal encoders (i.e., VGG19 for image and GRU for text), and incorporated hard negatives in the triplet loss function; [6] further utilized the Faster R-CNN for image modality, and discovered the full latent alignments using both image regions and words in a sentence as context; CLIP [28] employed a large-scale contrastive pre-training approach to effectively align vision and language modalities; [29] applied contrastive learning to cross-modal hashing using a momentum-based optimizer and a cross-modal ranking learning loss. These approaches can efficiently"}, {"title": "A. Vision-Language Retrieval", "content": "To learn matching representations of heterogeneous modalities, a large number of vision-language retrieval models are proposed [23, 24, 25, 26]. Traditional approaches [1, 6, 8, 9, 27] always utilize dual architecture, where the image and text modalities are separately embedded into a common space and then maximize the cross-modal representation similarity. For example, [8] built two independent modal encoders (i.e., VGG19 for image and GRU for text), and incorporated hard negatives in the triplet loss function; [6] further utilized the Faster R-CNN for image modality, and discovered the full latent alignments using both image regions and words in a sentence as context; CLIP [28] employed a large-scale contrastive pre-training approach to effectively align vision and language modalities; [29] applied contrastive learning to cross-modal hashing using a momentum-based optimizer and a cross-modal ranking learning loss. These approaches can efficiently"}, {"title": "B. Imbalanced Multi-Modal Learning", "content": "The important assumption behind cross-modal methods is the modal balance, i.e., it typically assumes that each modality can well represent other modalities [17]. However, in real-world scenarios, factors such as data noise multi-modal data [16], heterogeneity of multi-modal data and missing multi-modal data can lead to modality imbalance. For instance, the noise can impact the model's ability to learn effective information, resulting in the sufficiency of different modalities is various, leading to the existence of strong and weak modalities [18, 19]. In traditional cross-modal matching constraints, strong and weak modalities will interact with each other, and it is difficult to control the strong modal structure from being affected by the weak modality when exiting a large cross-modal divergence. This problem leads to multi-modal joint training being hard and has been researched in multi-modal fusion for classification tasks, [32, 33, 34] observed that the best single-modal network often outperforms the multi-modal networks. Therefore, [32] proposed to estimate the single-modal generalization and overfitting speeds to calibrate the learning through loss re-weighing; [33] promoted reliability and robustness by integrating evidence that explained the prediction of each modality; [34] chose to slow down the learning rate of the mighty modality by online modulation to lessen the inhibitory effect on the other modality. Nevertheless, these methods only focus on multi-modal fusion tasks by learning complementary information, which is different from the cross-modal retrieval for learning matching representations."}, {"title": "III. PROPOSED METHOD", "content": "To rebalance the vision-language retrieval, we aim to transfer the optimal instances' structure from teacher networks to cross-modal model in learning matching representations. Therefore, the overall architecture consists of two networks: 1). a cross-modal student network, and 2). two-modal independent teacher networks. In this paper, we concentrate on the image and text modalities, considering the effectiveness and feasibility, we adopt the cross-modal network following [35] and utilize the Swin Transformer [21]/BERT [20] for image/text modalities. Notably, our method can be used as a plug-and-play module, so we can transform any state-of-the-art cross-modal network and independent networks in our framework, and more analyses are provided in the experiments part. In the following subsections, we will first provide the exploration of modal imbalance, then introduce our framework and learning objectives."}, {"title": "A. Exploration of Modal Imbalance", "content": "We investigate the phenomenon of imbalance between image and text modalities, specifically focusing on differences in modality sufficiency. Proposition 2 in [16] indicates that modality sufficiency is correlated with the performance of the optimal model: greater insufficiency leads to poorer performance in single-modal models. Hence, we assess modality sufficiency based on single-modal retrieval performance. Specifically, we use the ResNet101 [36] and Swin Transformer (SwinT) [21] for training the image modality, and LSTM [37] and BERT [20] for the text modality. The results are shown in Table I, which shows that text consistently outperforms images in single-modal retrieval across all datasets, indicating that the sufficiency of the text modality is greater than that of the image modality. Consequently, we categorize text as the strong modality and image as the weak modality. Furthermore, Swin Transformer and BERT exhibit the best retrieval performance for images and text, respectively. Additionally, to further study whether modals can sufficiently represent each other, we conducted experiments where"}, {"title": "B. Cross-Modal Student Model", "content": "We use X-VLM [35] without the bounding box module as the cross-modal student model.\nVision and Language Encoders: Vision encoder produces fine-grained visual concept representations based on the SOTA Swin Transformer. As shown in Fig. 3, Swin Transformer splits an image $I \\in \\mathbb{R}^{C\\times H\\times W}$ into patches and flats to $I\\in \\mathbb{R}^{(C\\times P^2)\\times L_1}$, where $P \\times P$ represents the patch resolution and $L_1 = HW/P^2$ represents the number of patches following [38]. On the other hand, the language encoder maps the input text $T \\in \\mathbb{R}^{L_T\\times d}$ to the same dimension subspace of the image with BERT, $L_T$ is the words number, and $d$ is the dimension of the common subspace. We recommend referring to [21, 38] for more detailed information.\nCross-Modal Encoder: It refers to the cross-attention transformer, i.e., the text-oriented cross-attention transformer, which aims to harness the efficacy of interaction layers to process visual and textual representations. As shown in Fig. 3, the cross-attention can be formulated as:\n$Att(T) = \\text{softmax}(\\frac{QTK_I}{\\sqrt{d_M}})V_I,$"}, {"title": "C. Multi-Granularity Distillation", "content": "We employ single-modal teacher models to guide the cross-modal model, implementing multi-granularity distillation that incorporates structural distillation [39] on top of representational distillation. This approach preserves the structural consistency of cross-modal representations within the latent space, functioning as a rebalancing mechanism to enhance cross-modal retrieval performance.\nSingle-Modal Teacher Models: We construct advantaged teacher models for two modalities, which aim to transfer structural knowledge for the cross-modal network in learning matching representations. Note that traditional single-modal contrastive learning usually pre-trains models pairwisely, leading to some semantically similar instances being regarded as antagonistic pairs. Therefore, inspired by [40], we employ unsupervised prototype-aware contrastive learning for preserving the single-modal structure, which can distinguish intra- and inter-cluster pairs with a distance regularization. Given that labels may be available in practical scenarios, we investigate the impact of the teacher model under supervised learning. Furthermore, we examine the performance of pre-trained models, DINO [41] and T5 [42]. The results of both experiments are provided in Section IV of the supplementary material.\nRepresentation-Level Distillation: At the single-modal representation level, we employ contrastive learning to narrow the expression gap between the student model and the teacher model, thereby mitigating the influence on single-modal representations during cross-modal matching. Specifically, with the randomly sampled batch pairs J, we calculate the similarity between the output of the visual model and the visual teacher model, as well as the output of the language model and the language teacher model. Each anchor instance's single-modal representations, paired with the corresponding teacher model output, form positive pairs, while the instances from"}, {"title": "IV. EXPERIMENTS", "content": "To verify the effectiveness of our method, we conduct experiments on three datasets. In detail, MS-COCO [43] contains 123,287 images, including 82,783 training images and 40,504 validation images, each labeled with 5 captions. Following [44], we use the splits of 5,000 images for validation, 1,000/5,000 images for testing, and the rest for training. FLICKR30K [45] consists of 31,000 images, and each image is associated with 5 captions. The dataset is split into 29,000 training images, 1,000 validation images, and 1,000 testing"}, {"title": "A. Data Description", "content": "To verify the effectiveness of our method, we conduct experiments on three datasets. In detail, MS-COCO [43] contains 123,287 images, including 82,783 training images and 40,504 validation images, each labeled with 5 captions. Following [44], we use the splits of 5,000 images for validation, 1,000/5,000 images for testing, and the rest for training. FLICKR30K [45] consists of 31,000 images, and each image is associated with 5 captions. The dataset is split into 29,000 training images, 1,000 validation images, and 1,000 testing"}, {"title": "B. Baselines and Evaluation Protocol", "content": "For comparison methods, we evaluate four types of state-of-the-art approaches: 1). Dual-encoder retrieval methods, including VSE++ [8], SCAN [6], IMRAM [47], SGRAF [9], GSMN [48], VSRN [49], and NAAF [50]. 2). Transformer-based retrieval methods, including ALBEF [31], BLIP [14], and X-VLM [35]. 3). Single-modal models, e.g., Swin Transformer [21] and BERT [20]. 4). Imbalance multi-modal learning methods, i.e., CYCLIP [51] and UMT [52]. Note that CY-CLIP incorporates regularizers about single-modal and cross-modal structural information into the cross-modal contrastive item in the form of a multi-task loss, and UMT optimizes the cross-modal model by distilling the instance' relations learned by the single-modal model. Since the baselines utilize different backbones, we conducted an experiment that integrates our method with these baselines in a plug-and-play manner to ensure a fair comparison. The detailed results are provided in Section III of the supplementary."}, {"title": "C. Implementation Details", "content": "Our multi-granularity distillation approach can be seen as a plug-and-play module, and we can incorporate other cross-"}, {"title": "D. Retrieval Results", "content": "Cross-Modal Retrieval: I2T and T2I retrievals are considered to evaluate cross-modal retrieval performance. Table III records the results on four public datasets. To ensure experimental fairness, we exclusively utilized the given datasets for model training, rather than pre-training additional data as the large-scale models (i.e., ALBEF, X-VLM, CYCLIP, and BLIP) do. So we retrained the ALBEF and other large models from scratch (marked with \"*\"), thus causing different results compared to the original paper. Results indicate: 1). Our X-VLM*+ outperforms the best cross-modal retrieval method, i.e., X-VLM, on I2T Recall@1 and T2I Recall@1 by 6.2/11.4/7.5/6.2 and 3.1/5.1/7.9/6.5 respectively, on four datasets. This phenomenon reveals that structure preservation can also enhance the learning of cross-modal consistent representations by bringing image (i.e., weak modality) representations closer to the text (i.e., strong modality) ones without"}, {"title": "E. Ablation Study", "content": "Furthermore, we conduct ablation studies to validate the effectiveness of each module: cross-modal matching (lite and litm), representation-level distillation losses (liic and lttc) and the structure-aware distillation module (lsa). Due to the large size of the MS-COCO dataset, we focus on the smaller FLICKR30K and Vizwiz datasets in subsequent experiments. Analyzing the results in Table V reveals the following observations: 1). The removal of litm leads to a significant decline in cross-modal retrieval performance, although single-modal retrieval performance improves, further confirming the negative impact of cross-modal consistency learning on single-modal retrieval. 2). Compared to the removal of lite and litm, the elimination of lsa results in the poorest retrieval performance, indicating that preserving instance structure information more effectively promotes representation learning both across modalities and within each modality."}, {"title": "F. Sensitivity to Parameters", "content": "To verify the influence of parameters, we conduct more experiments by tuning several important parameters: 1). batch size J; 2). temperature parameter \u0442; 3). hyper-parameter \u03bb."}, {"title": "G. Influence with Different Distillation Measurements", "content": "To explore the effectiveness of graph-matching criteria, we conducted more experiments. To be specific, we replace the MAE distance with other distance measurements, i.e., the Mean Squared Error (MSE), Kullback-Leibler divergence (KL), and Wasserstein distance (WD). The results in Table VI reveal that our graph-matching criteria, independent of specific distance metrics, demonstrate excellent flexibility, as observed in the MAE, MSE, and WD outcomes. In contrast, the KL method is not conducive to cross-modal retrieval."}, {"title": "H. Generalization of Multi-Granularity Distillation", "content": "To assess the generalization ability of our multi-granularity distillation strategy, we extended experiments to include the retrieval methods SCAN and VSRN based on dual encoders, along with the transformer-based retrieval method ALBEF. These experiments were conducted on the FLICKR30K and Vizwiz datasets. SCAN adopts a local representation matching approach, while VSRN introduces a global semantic reasoning module alongside attention to region relationships. As dual encoder methods do not learn modal fusion representations, we introduced the structure-aware distillation module for each single modality, aiming to enhance instance-level modality matching by leveraging the representation structure information from the single-modal teacher model. Results in Table VIII show a significant performance improvement when"}, {"title": "I. Exploration on Large-Scale Multi-modal Model", "content": "To explore the effectiveness of our proposed coordinate optimization strategy for large-scale multi-modal models, we apply our method to the fine-tuning process (marked with \u201c+\u201d). Table VII takes X-VLM as an example on the FLICKR30K dataset. Results demonstrate that our method not only improves the performance of single-modal retrieval in the fine-tuning stage of the pre-trained large-scale model, for both the weak and strong modalities but also maintains excellent performance on most metrics in cross-modal retrieval and mixed retrieval tasks."}, {"title": "J. Case Study", "content": "To analyze the retrieval visualizations, we randomly sample cross-modal and mixed retrieval cases from the FLICKR30K dataset to validate the effectiveness of our proposed method. The visualization examples are exhibited in Fig. 6 and Fig. 7, in which green ticks/boxes/values represent exactly aligned instances, red forks/boxes/values denote unaligned instances, the mixed retrieval instances are with ROUGE-L value (the larger the better). Considering the superior performance, we adopt our method here. Fig. 6 shows the qualitative results of cross-modal retrieval using our method and VSRN. First, most of the retrieved cross-modal instances using our method are correct (shown as green ticks) on both the I2T and T2I retrieval. Some outputs are mismatched (shown as red forks), but reasonable, for example, (a) 5 contains similar semantic meanings to the image. On the other hand, our method outperforms the VSRN on both the I2T and T2I retrieval considering the same query. For example, (b) and (d) share the same image query, our method can retrieve the most aligned sentences, while VSRN fails, the reason that better structure-preserving can promote consistent representation learning in return. Fig. 7 shows the qualitative results of mixed retrieval using our method compared with the SCAN. We find that our method can not only find accurate cross-modal instances but also find semantically similar intra-modal instances. For example, in the image query case (a), our method can retrieve similar images and exactly aligned cross-modal instances, while SCAN only retrieves images with lower similarities (i.e., lower ROUGE-L values), and several unaligned sentences. This phenomenon further validates the effectiveness of our proposed method in mixed retrieval. Moreover, we can intuitively find the differences between consistent representations learned by different cross-modal approaches in (d). Using the \"harp player\" case as an example, we can find the clustered instances in our method are all with \"harp player\" semantics, whereas the clustered instances in SCAN are with outliers."}, {"title": "V. CONCLUSION", "content": "In this paper, we find that the learned consistent representations from existing vision-language retrieval methods may affect single-modal retrieval performance. To explain this phenomenon, we identify the main cause as modal sufficiency, i.e., there exist weak and strong modalities, and hard cross-modal consistency may bring negative representation learning to strong modality, leading to the destruction of instance structure. To address this problem, we develop a different way inspired by multi-task learning, which aims to learn two modal representations that can simultaneously ensure cross-modal consistency and single-modal structure. Extensive experiments on different datasets validate our method can achieve better single-modal retrieval accuracy whilst maintaining cross-modal retrieval capacity compared with the baselines. In future work, we aspire to theoretically elucidate the relationship between the degree of modality imbalance and retrieval performance. Additionally, addressing the challenges posed by modality imbalance in cross-modal retrieval tasks involving more than two modalities represents a significant area for further investigation."}]}