{"title": "An investigation into the performances of the Current state-of-the-art Naive Bayes, Non-Bayesian and Deep Learning Based Classifier for Phishing Detection: A Survey", "authors": ["Tosin Ige", "Christopher Kiekintveld", "Aritran Piplai", "Amy Wagler", "Olukunle Kolade", "Bolanle Hafiz Matti"], "abstract": "Phishing is one of the most effective ways in which cybercriminals get sensitive details such as credentials for online banking, digital wallets, state secrets, and many more from potential victims. They do this by spamming users with malicious URLs with the sole purpose of tricking them into divulging sensitive information which is later used for various cybercrimes. In this research, we did a comprehensive review of current state-of-the-art machine learning and deep learning phishing detection techniques to expose their vulnerabilities and future research direction. For better analysis and observation, we split machine learning techniques into Bayesian, non-Bayesian, and deep learning. We reviewed the most recent advances in Bayesian and non-Bayesian-based classifiers before exploiting their corresponding weaknesses to indicate future research direction. While exploiting weaknesses in both Bayesian and non-Bayesian classifiers, we also compared each performance with a deep learning classifier. For a proper review of deep learning-based classifiers, we looked at Recurrent Neural Networks (RNN), Convolutional Neural Networks (CNN), and Long Short Term Memory Networks (LSTMs). We did an empirical analysis to evaluate the performance of each classifier along with many of the proposed state-of-the-art anti-phishing techniques to identify future research directions, we also made a series of proposals on how the performance of the under-performing algorithm can improved in addition to a two-stage prediction model", "sections": [{"title": "I. INTRODUCTION", "content": "Phishing is a type of cybercrime in which an individual is lured to divulging sensitive information details through text message, email, or phone conversation by someone posing either as a legitimate institution or a member of a legitimate institution, some of these commonly requested sensitive details which are social security number, password, credit, and bank-ing card details etc are later used to access more sensitive information for a different type of cybercrime which often results in financial loss or identity theft as about 76% of the phishing attacks were credential-harvesting in 2022 according to Digital Information world. A California teenager was able to get sensitive information to access credit card details and withdraw money from his victim's account through his fake \"America Online\" website which resulted in the first lawsuit filed in 2004. Efficient phishing detection has been challenging as attackers continue to advance their tactics as technologies evolve. To defraud personnel, all an attacker needs to do is simply clone a legitimate website to create a new website (SCAM Website) which is then used to defraud computer users.\nEmail phishing is responsible for 90% of ransomware attacks and for which the average ransom payment in those instances is can be as high as $200,000 (\u00a3161,000), and in addition to the fact that organizations that fall victim of ransomware attacks lose a couple of weeks as downtime [55]. The UK Government's Cyber Security Breaches Survey of 2022 had revealed that cyberattacks rose by 38% in 2022 alone compared to 2021 as 83% of businesses and organizations have suffered at least one data breach with Over 3.4 billion phishing emails sent daily. According to the U.S Federal Bureau of Investigation, 2 billion dollars were stolen due to phishing in 2018 alone, 5 billion dollars was stolen in 2019, and 4.7 billion in 2021 [?], [5], [66]. In 2019, insights Business E-mail Compromise (BEC) announced that about 4.8 million dollars were lost as a result of phishing attacks in 2022, while a cybersecurity research group reported that a whopping 1.6 million dollars were lost in 2019 4.7 billion dollars in 2021\nThe ever-evolving ways attacker tries to improve their phishing techniques to bypass existing state-of-the-art anti-phishing detection and prevention method poses a mountain of challenge to researchers in both industry and academia. Thus, the constant evolvement and innovation in phishing techniques adopted by attackers are the reason why all existing anti-phishing methods remain vulnerable to phishing attacks. All existing methods of detecting phishing attack which are based on machine learning [2], [10], [14], [37]\u2013[41], black-lists/whitelists [25], natural language processing [44], visual similarity [44], rules [43], remains vulnerable to attack due to the following reasons;\n\u2022 Very small or minute changes to the uniform Resource Locator (URL) of a blacklisted URL will make the black-list/ whitelist phishing detection method to fail. Also, the fact that there is no worldwide centralize database for whitelisted or blacklisted URL make this method even more vulnerable, and so if company X blacklisted my phishing URL on their internal server, I can try it with company Y and be successful.\n\u2022 In machine learning phishing detection which uses rel-evant features such as URL, webpage content, website traffic, search engine, WHOIS record, and Page Rank has their own vulnerabilities because firstly, such classifier will make a phishing URL that is hosted on a hacked or compromise server to be false classify as benign leading to false negative, secondly using domain age as a feature to train a model will always lead to higher false positive simply because the URL of a newly registered legitimate company website will be misclassify because the domain name was recently register, page rank is zero, and with low traffic, and thirdly the fact that parameters for those features are gotten from third party website is another concern. What will happen if the third party website is having a downtime?\n\u2022 The issue with visual similarity-based heuristic method which compares both the pre-stored signature such as images, font styles, page layout, and screenshot and so on of the new website with the old website will have general difficulty in detecting anomaly in a newly hosted phishing site.\n\u2022 The fact that the majority of the existing machine learning models are trained based on textual features such as \"#\",\".\", Internet Protocol address, URL Length, domain levels, and so on from the Uniform Resource Locator (URL) does not help simply because any phisher or attacker with little web technologies can develop what we called \"friendly URL\" depending on the programming language adopted whether JAVA, C#, Python, PHP or framework to avoid all those features. With a friendly URL, such models are bound to misclassify leading to an increment in false negative rate.\nFor any Machine learning-based phishing detection method to be effective in real-time combat against phishing attacks, it must address each of the stated reasons above for which existing state-of-the-art anti-phishing techniques continue to be vulnerable as phishing methods continue to evolve in a more sophisticated and innovative way. It is worth noting that past reviews on phishing have been largely based on approaches, classification, and so on. RASHA ZIENI et al. [89] focus their review on list-based, similarity-based, and machine learning-based categories of approaches for phishing detection to identify pending research gap, Angad et al. [58] focus theirs on the advantages and limitations of existing approaches to phishing detection, while also using discussion of related application scenarios as guidance to propose a new method of anti-phishing detection, Yifei Wang [83] categorizes widely used phishing detection methods into seven categories and summarizes them.\nIn this work, we did an extensive review of some of the most recent works on phishing detection, and state-of-the-art algorithms from the past 5 years in order to investigate the performance of the Naive Bayes algorithm relative to other state-of-the-art algorithms for phishing detection task, and the factors behind those performances to uncover future research direction. Our first strategy was to Isolate Naive Bayes from other algorithms, hence, we categorized state-of-the-art phishing detection classifiers into Naive Bayes-based, Machine learning-based, and Deep learning-based for better analysis. The contributions of our research are stated below;\n1) Comparative study of the performance of Naive Bayes relative to other machine learning and deep learning-based state-of-the-art algorithms for phishing detection tasks through a survey of the recently published research works.\n2) Investigating and analyzing possible factors behind our findings on the performance of Naive Bayes relative to other machine learning and deep learning-based state-of-the-art algorithms for phishing detection\n3) Proposing possible solutions so as to identify future"}, {"title": "II. BACKGROUND STUDY", "content": "Since it is easier for attackers to exploit human weakness to easily bypass the most advanced state-of-the-art defense system by extracting sensitive credentials and information through phishing. Attackers therefore focused their effort on getting sensitive credentials through phishing emails which are mistaken for legitimate emails by unsuspecting victims. Hence, it is imperative to understand how different phishing technique works in order to proffer a strategic defense solution to effectively detect, prevent or mitigate phishing impact in case of a successful attack. In this section, we analyse the process of the major phishing attack."}, {"title": "A. Email Phishing", "content": "Email phishing is a phishing type in which unsuspecting victim is tricked into divulging credential or sensitive in-formation through email [10], [52]. Here the attacker sends phishing code either through email containing a phishing link or malware attachment in such a way that as soon as the victim clicks on the link [21], it will either redirect it to a phishing site or get the system infected by malware. Sensitive credentials getting by this mean can then be use by the attacker to commit series of cybercrimes against the victim or target organization including but not limited to remote malware installation, instigate Denial of service attack, Cyberstalking, identity theft, and can even be sold in the dark market."}, {"title": "B. Spear Phishing", "content": "Statistic from Barracuda data shows that a typical orga-nization receives 5 customized spear phishing email each day targeting an individual, and despite the fact that only 0.1% of all emails are spear phishing attacks, 66% of all organization breaches are caused by spear phishing. In this type of attack, the attacker keep tracks of the prospective victim activities [30], [72] in the social media such as X formally Twitter, Linkedin, Facebook, Instagram and so on so as to gather substantial information about the targeted victim. With this newly gathered information, the attacker is able to compose email messages which will seems to come from the organization's manager account and typically requesting for sensitive information belonging to the organization."}, {"title": "C. Voice Phishing (Vishing)", "content": "It is a type of cybercrime in which attacker make automated phone call by a seemingly legitimate phone number from an organization to get confidential detail from unsuspecting victim [22]. An instance is a customer who get a warning call from an attacker who posed to be bank staff claiming u usual activities on the victim's account and requesting for recently generated one-time password (OTP) or Personal Identification Number (PIN) of the account. The fact that the phisher was able to make scam call from an organization which the victim has connection with makes gives this type of attack a high success rate as experienced in 2021 when 59.49 million which is a whopping 23% of the America population lost an estimated 29.8 billion US Dollar to voice phishing according to earthweb."}, {"title": "D. SMS Phishing (Smishing)", "content": "It is a type of cybercrime in which a bait message is sent by an attacker to a set of targeted audience through text message. Messages in a smishing attack usually contains either an email to contact, phone number to call, or link to click where the potential victim is then to provide person credential information such as credit card details, password etc for later use by the attacker on legitimate website to commit series of cybercrime. The SMS uses series of social engineering tactics to ensure potential victim follow the instruction by calling the phone number, contacting the email, or clicking on the link which will lead to the actual phishing website with a form to collect their personal data."}, {"title": "III. CATEGORY OF CURRENT STATE OF THE ART PHISHING DETECTION MODEL", "content": ""}, {"title": "A. Bayesian-Based-Classifier", "content": "Naive Bayes is a family of probabilistic-based algorithms that is based on the Bayes rule. It is based on the fact that, if B has occurred, we can find the probability that A will occur. B is taken to be the evidence while the hypothesis is A and with a strong assumption that each of the features is independent. It uses the prior probability distribution to predict the posterior probability of a sample that belongs to a class. In this process, the class with the highest probability is then selected as the final predicted class [84]. Naive Bayes updates prior belief of an event occurring given that there is new information. Hence, given the availability of new data, the probability of the selected sample occurring is given by;\n$P(class/features) = \\frac{P(class) * P(features/class)}{P(features)}$\nWhere\n\u2022 P(class/features): Posterior Probability\n\u2022 P(class): Class Prior Probability\n\u2022 P(features/class) : Likelihood\n\u2022 P(features): Predictor Prior Probability\nIt has a very strong assumption of independency which affects its performance for classification tasks [36] as the strong assumption of independence among features is not always valid in most of the dataset that is used to train the current state-of-the-art model for several classification tasks. The strong assumption of the Naive Bayes classifier is one reason why it usually underperforms when compared with its peers for similar classification tasks. Naive Bayes classifier has different variants with each variant having its own individual assumption which also impacts its performance in addition to the general assumption of independence which is common to all variants of the Naive Bayes classifier, and so each variant is suitable for different classification tasks.\nMultinomial Naive Bayes is a variant of Naive Bayes, It assumes multinomial distribution among features of dataset in addition to the general assumption of independency, and so its performance is affected if the actual distribution is not multinomial or partially multinomial. Multinomial Naive Bayes is the suitable variant for natural language processing classification task [35] but still underperforms when compared with non-bayesian and deep learning-based classifiers for the same NLP classification task.\nGaussian Naive Bayes is the suitable Bayesian variant for anomaly detection in network intrusion which could be used to detect Distributed Denial of Service (DDOS) attacks [36]. It assumes the normal distribution among features in dataset in addition to the general assumption of independence which is common to all variants of Naive Bayes.\nDespite being a suitable Naive Bayes variant for anomaly detection, it still underperforms when compared with its suitable peer for detection of Distributed Denial of Service (DDOS) attack as evident in the work done by Rajendran [65] where Gaussian Naive Bayes have the least accuracy of 78.75% compared with other non-bayesian based for attack detection classification task.\nBernoulli Naive Bayes assumes Bernoulli distribution in addition to the assumption of independence. Its main feature is that it only accepts binary values such as success or failure, true or false, and yes or no as input while complement Naive Bayes is used for imbalance datasets as no single variant of Naive Bayes can do the task of all the variants. Both the suitability and performance of each variant are determined by their individual assumption in addition to the general assump-tion of independence which impacts their performance when compared with their suitable peer for the same classification task."}, {"title": "B. Non-Bayesian Based Classifier", "content": "1) Decision Tree: A decision Tree is a Supervised learning technique whose operation is based on a tree-structured clas-sifier, with features in the dataset being represented by an in-ternal node, each decision rule is represented by the branches, while the internal nodes represent the features of a dataset, branches represent the decision rules and each leaf node represents the decision outcome is represented by the leaf node and so does not have further branches. It makes a decision-based graphical representation of all possible solutions to a problem. It uses the Classification and Regression Tree algorithm (CART) [88] to construct a decision tree starting with the root node whose branch keeps expanding further to construct a tree-like structure. It is a non-parametric and the ultimate goal is the creation of a machine learning model capable of making prediction by learning simple decision rules that are inferred from data features.\n2) Random Forest: It is an ensemble-based learning algo-rithm that could be used for classification, regression task, and other similar tasks that operates based on the construction of multiple decision trees [33]. Since the algorithm works by constructing multiple decision trees during training, the output of a classification model trained with a random forest algorithm is the class selected by most of the trees, while the mean or average prediction of individual trees is returned as the output for a regression task. This system of aggregating and ensemblement with multiple trees for prediction makes it possible for a random forest-trained model to outperform the decision tree-trained model and also avoid overfitting which is a peculiar problem for decision tree classifiers.\n3) Logistic Regression: Logistic regression is the modeling of the probability of a discrete outcome by having the event log-odds be a linear combination of one or more independent variables given an input variable [23]. Logit transformation is applied to the bounded odds which is the division between the probability of success and probability of failure based on it's linear regression that could be used for both classification and regression tasks and since the output is a probability, the dependent variable is bounded between 0 and 1 values, it uses logistic function to model binary output for classification prob-lems. The difference between linear regression and logistic regression is that the range in logistic regression is bounded by 0 and 1, and also that logistic regression does not require a linear relationship between input and output.\n4) XGBoost: It is a supervised learning algorithm that is gradient boosting based. It is extremely efficient and highly scalable, the algorithm works by first creating a series of individual machine learning models and then combining each of the previously created models to form an overall model that is more accurate and efficient than any of the previously created individual models in the series. This system of creating a series of models and combining them to create a single model [26] makes XGBoost perform better than other state-of-the-art machine learning algorithms in many classification, ranking, several user-defined prediction problems, and regression tasks across several domains. XGboost uses gradient descent to add additional individual models to the main model for prediction, hence it is also known as stochastic gradient boosting, gradient boosting machines, or multiple additive regression trees.\n5) K-Nearest Neighbor (KNN): k-nearest neighbors (kNN) algorithm is a non-parametric supervised learning algorithm that uses the principle of similarity to predict the label or"}, {"title": "C. Deep Learning Based Classifier", "content": "1) Convolutional Neural Network (CNN): CNN is a deep learning model with a grid pattern for processing data that is designed to automatically and adaptively learn spatial hi-erarchies of features, from low- to high-level patterns [?]. It is a mathematical construct that is composed of convolution, pooling, and fully connected layers as three types of layers or building blocks responsible for different tasks for predictions. While convolution and pooling layers, perform feature extrac-tion, the fully connected layer, maps the extracted features into the final output usually known as classification. The convolution layer is composed of mathematical operations (convolution) which plays a very crucial role in Convolutional Neural Networks as in a kind of linear operation. The CNN architecture is a combination of several building blocks like convolution layers, pooling layers, and fully connected layers, and so, a typical architecture consists of repetitions of a stack of many convolution layers and a pooling layer, and then followed by one or more fully connected layers. It stored digital images, and pixel values as a two-dimensional (2D) grid which is an array of numbers along with some parameters called the kernel before an optimizable feature extractor is finally applied at each image position. This makes CNNs a highly efficient classifier for image processing classification tasks, since a feature may occur anywhere in the image. extracted features can hierarchically and progressively become more complex as each layer progressively feeds its output to the next layer, the main task is the minimization of differences between output and ground truth by backward propagation and gradient descent which is an optimization algorithm. This process of optimizing parameters like kernels to minimize the difference between outputs and ground truth is called training.\n2) Recurrent Neural Network (RNN): Recurrent Neural Networks (RNNs) is a type of Neural Network in which output from the previous step is fed to the current step as input, It introduce the concept of memory to neural networks through the addition of the dependency between data points. This addition of dependency between data points ensured that RNNs could be trained to remember concepts by able able to learn repeated patterns. The main difference between RNN and the traditional neural network is the concept of memory in RNN which is made possible as a result of the feedback loop in the cell. Here, it is the feedback loop that enables the possibility of passing information within a layer unlike in feedforward neural networks where information can only be passed between layers. While input and output are independent of each other in a traditional neural network, It is a different ball game in RNN where sequence information is to be remembered, this was made possible in RNN by its Hidden state also known as the memory state through which it remembers previous input to the network, and so it is safe to conclude that the most important features of RNNs is the Hidden state by which it remembers some information in a sequence. In terms of architecture, RNN architecture is the same as that of other deep neural networks, the main difference lies in how the information flows from the input to the output. While the weight across the network in RNN is the same, deep neural network has different weight matrices for each dense network. The Hidden state in the RNNs which enables them to remember sequence information makes it suitable for natural language processing tasks.\n3) Long Short-Term Memory (LSTM): Long short-term memory (LSTM) network is a recurrent neural network (RNN) that is specifically designed to handle sequential data, such as speech, text, and time series, it is aimed at solving the problem of vanishing gradient in traditional RNNs. It is insensitive to gap length which gives it an advantage over hidden Markov models, hidden Markov models, and other RNNs. It provides"}, {"title": "d(x, z) = \\sqrt{\\sum_{i=1}^{n}(X_i - Z_i)^2}$", "content": "value of a new data point by considering values of its K-nearest neighbors in the training dataset based on a distance metric like Euclidean distance.\ndist(x, z) \u2264 dist(x, y) + dist(y, z)   (1)\nfor which the distance between x and z could be calculated by\n$d(x, z) = \\sqrt{\\sum_{i=1}^{n}(X_i - Z_i)^2}$ (2)\nThe prediction of the new data point is based on the average or majority vote of its neighbor, this method allows the classifier to adapt its prediction according to the local structure of the data which ultimately helps to improve its overall accuracy and flexibility. Since KNN can be used for both classification and regression tasks, its prediction output depends on the type of task (classification or regression). In the case of a classification task, it uses class membership as the output by using the plurality vote of its neighbor to assign the input to the class that is most common among its k nearest neighbors, but when KNN is being used for a regression task, it uses the average of the values of k nearest neighbors as the prediction output, the value of k has an impact on the overall accuracy [16] of the model."}, {"title": "IV. DISCUSSION AND ANALYSIS", "content": "Our initial decision was to use a combination of f1 score, precision, and accuracy as a performance evaluation metric, but when searching for an appropriate evaluation metric that we could use, we observed that the overwhelming majority of the authors rely solely on accuracy as a measure of evaluation, this shaped our decision to use evaluation as a criterion for performance measurement, and as we all know that there is no single perfect evaluation metric meaning that accuracy alone is not a perfect evaluation metric because different factors and condition can affect the accuracy like imbalance in the dataset which could tilt the accuracy in favor or against a classifier, preprocessing (where roles containing null values were removed or replaces), bias in dataset, possible mistake or negligence on the part of the author and so on. To ensure fairness and a true picture of the performance of individual state-of-the-art algorithms for phishing detection tasks, we decided to use mean accuracy both at individual and categorical levels.\nHaving adopted mean accuracy as a measure of performance evaluation to counter the effect of (i) uncertainty in the quality of dataset since they come from a different source in which some are internally generated in certain cases and not available as a public dataset (ii) dataset imbalance or bias that can tilt the result in favor or against a target (iii) series of processing tasks such as complete removal of rows with null values that can cause massive reduction in dataset or replacing them with the mean value which makes the data distorted and not exact (iv) unintended mistake or negligence as every researcher is different in terms of professionalism, ethical level, attention to details. We tried to look at the reason why phishing is still very effective despite the accuracy and performances of machine learning models, hence, we observed the following;\n(1) Overwhelming reliance on the Uniform Resource Locator(URL) dataset\nIt is worth noting that current state-of-the-art machine-learning phishing detection models are trained based on the properties of the URL such as length of URL, length of the hostname, average words in URL, longest words, character repetition, average path, who is registered domain, domain with copyright, domain age, web traffic, DNS record, google index, PageRank and so on. To better understand why successful phishing attack remains high despite the level of accuracy from state-of-the-art machine learning-based phishing detection model, we will classify the properties of the phishing URL on which ML models are being trained into Controllable Properties and Uncontrollable Properties.\n(a) Controllable Property:\nWe classified controllable properties of URLs as properties or characteristics of URLs that could be controlled by attackers.URL characteristics such as length of URL, length of the hostname, average URL, longest word, character repetition, average word, average path, etc can easily be defeated by using SEARCH ENGINE FRIENDLY URL."}, {"title": "V. CONCLUSION AND FUTURE RESEARCH DIRECTION", "content": "In this work, we did an extensive review of some of the most recent works on phishing detection, and state-of-the-art algorithms from the past 5 years in order to investigate the performance of Naive Bayes algorithm relative to other state-of-the-art algorithms for phishing detection task, and the factors behind those performances to uncover future research direction. In our comparative study of the performance of Naive Bayes relative to other machine learning and deep learning-based state-of-the-art algorithms for phishing detec-tion tasks through a survey of the recently published research papers, Random Forest, Decision Tree, CNN, XGBoost with an individual mean accuracy of 97.1%, 95.2%, 94.2%, and 94.1% respectively have the top 4 performance for URL properties-based phishing detection task while Naive Bayes, SVM, RNN with individual mean accuracy of 80.4%, 89.4%, and 91.6% respectively have the worst 3 performance for URL properties-based phishing detection classification task.\nIn our effort to improve the performance of current state-of-the-art phishing detection methods that rely on the properties of the phishing URLs, especially to counter the ever-evolving phishing methods in which attackers are now using images as text to avoid detection, we proposed a two-stage prediction model where random forest makes the first prediction base on the properties of the URL, and if the first stage is successful i.e the site is predicted as legitimate, then the model goes to the next stage of prediction where the content of the URL is web scrapped and fed to a Convolutional Neural Network to make the final prediction. We chose Random Forest for the first stage because it outperforms other classifiers for phishing detection based on URL properties while CNN was chosen due to its effectiveness in natural language processing and image classification tasks.\nLooking at the poor performance of the Naive Bayes clas-sifier both at the individual and categorical levels for which it has the least performance for phishing detection classification task, we propose regularization to the current Bayes Rule that will put both the level of correlation or dependency among the features as well as the underlying nature of feature distribution in a dataset into perspective as a way to improve the performance of Naive Bayes-based algorithms instead of just ignoring them or merely replacing the marginal probability density function with joint probability density function as seen in non-naive Bayes."}]}