{"title": "Leveraging Taxonomy and LLMs for Improved Multimodal Hierarchical Classification", "authors": ["Shijing Chen", "Mohamed Reda Bouadjenek", "Shoaib Jameel", "Usman Naseem", "Basem Suleiman", "Flora D. Salim", "Hakim Hacid", "Imran Razzak"], "abstract": "Multi-level Hierarchical Classification (MLHC) tackles the challenge of categorizing items within a complex, multi-layered class structure. However, traditional MLHC classifiers often rely on a backbone model with n independent output layers, which tend to ignore the hierarchical relationships between classes. This oversight can lead to inconsistent predictions that violate the underlying taxonomy. Leveraging Large Language Models (LLMs), we propose novel taxonomy-embedded transitional LLM-agnostic framework for multimodality classification. The cornerstone of this advancement is the ability of models to enforce consistency across hierarchical levels. Our evaluations on the MEP-3M dataset - a Multi-modal E-commerce Product dataset with various hierarchical levels- demonstrated a significant performance improvement compared to conventional LLMs structure.", "sections": [{"title": "1 Introduction", "content": "The increasing complexity of real-world datasets has led to the widespread adoption of multi-level hierarchical structures, making Multi-level Hierarchical Classification (MLHC) a critical tool in modern data analysis (Zhang et al., 2024). For instance, large-scale e-commerce platforms like Amazon manage extensive product catalogs through complex taxonomies, which include nested categories, subcategories, and filters (e.g., electronics \u2192 laptops \u2192 2-in-1 laptops). These taxonomies help users efficiently navigate and refine their search by narrowing down options based on attributes like brand, price range, and features (Zhang et al., 2024). In such scenarios, MLHC plays an important role by accurately classifying items within these hierarchies, using the taxonomy to infer relationships and ancestors among categories (Boone-Sifuentes et al., 2022b). This ability to leverage taxonomy-based classification not only enhances user experience but also improves data organization, making MLHC indispensable in sectors dealing with large-scale hierarchical datasets.\nTo illustrate and evaluate the benefits of MLHC, we refer to Figure 1, which shows (1a) An data entry of an apple classified by three independent classifiers across three hierarchy levels, and (1b) the proportion of correctly classified data enties at each level of the taxonomy for the sampled MEP-3M dataset (shown in dark color), as well as the proportion of data entries that were misclassified at one level but correctly identified at other levels within the taxonomy (shown in light color). Several key insights can be drawn from this analysis: (1) First, MLHC facilitates the structure of vast amounts of information using a hierarchical taxonomy, which is particularly useful for capturing the relationships between classes via the \u201csubclass-of\" concept. (2) Second, as demonstrated in Figure 1a, providing the final classification layer with information that the data belongs to higher-level categories such as \"Food\" and \"Fruit\" could have enhanced its ability to correctly classify the image at l3, or at the very least, ensure consistency by selecting a subclass of \u201cFruit\u201d. (3) Finally, the results depicted in Figure 1b show that 4.08% of images incorrectly classified at l\u2081 were correctly classified at either l2 or l3, 11.92% of images misclassified at 12 were accurately classified at l\u2081 or l3, and 16.06% of images incorrectly classified at l3 were correctly classified at l\u2081 or l2. This shows and motivates the potential benefit of an MLHC that embeds the taxonomy structure with a top-down or a bottom-up classification approach.\nSeveral methods have been proposed for MLHC (Boone Sifuentes et al., 2024), which can be classified based on how they utilize the hierarchical structure. Specifically, we distinguish between three primary approaches: (i) the flat classification approach, where the class hierarchy is completely ignored. In this approach, predictions are made solely for the bottom levels, with the assumption that all ancestor classes are implicitly attributed to the instance as well; (ii) the local classification approach, which involves training a separate multi-class classifier at each parent node in the hierarchy to distinguish between its child nodes; and (iii) the global classification approach (Zhang et al., 2024; Bettouche et al., 2024; Liu et al., 2024a), where a single classifier is responsible for handling the entire class hierarchy. In this paper, we argue that flat classifiers, by ignoring the hierarchical relationships between class levels, often result in inconsistent classifications. For instance, as shown in Figure 1a, the data entry of an apple is correctly classified as \u201cFood\u201d and \u201cFruit\", but incorrectly as \"Pearl\" at the leaf node. Furthermore, we argue that it is impractical to train and maintain n separate networks for local classification approaches, which can be redundant and costly in real-world applications. As a result, we favor global classification approaches, which address the limitations of flat and local methods. However, existing methods still face several key challenges: (i) they do not inherently embed the taxonomy structure, (ii) they often rely on complex neural network architectures with n independent output layers that do not interact, (iii) they frequently produce predictions that are inconsistent with the taxonomy, and (iv) they typically operate with a fixed n, limiting flexibility and requiring extensive hyperparameter tuning to optimize n for different scenarios.\nThis paper addresses the aforementioned shortcomings by introducing a novel Taxonomy-based Transitional Classifier (TTC) for MLHC. Specifically, we propose an LLM-agnostic output layer that can be used in conjunction with any LLM integrating taxonomy information. Our output layer employs a top-down divide-and-conquer strategy, attending to the taxonomy relationships at each level of the classifier to ensure predictions remain consistent with the hierarchical structure. Focusing on a multimodal dataset, we evaluate the effectiveness of our approach on the MEP-3m dataset (Liu et al., 2023a) and use different LLMs as backbone models. Experimental results demonstrate that TTC improves the performance of various backbone LLMs compared to when they are applied as flat classifiers."}, {"title": "2 Related Work", "content": "Hierarchical classification is a well-established area of research, with a broad spectrum of approaches developed across various domains. Below, we review the most prominent approaches.\nFlat Classification methods: which ignore the hierarchical structure, typically predicting only classes at the leaf nodes and considering that all its ancestor classes are also implicitly assigned to that instance. Although these methods are simple and computationally efficient, they often fail to utilize the inherent relationships between classes, resulting in suboptimal performance, particularly in complex taxonomies (Silla and Freitas, 2011; Valentini, 2010). While commonly used as baselines in empirical studies, their lack of hierarchical awareness limits their effectiveness in domains where class relationships are crucial.\nLocal Classifier Approaches: have been widely adopted to address the limitations of flat classification by training classifiers at different levels of the hierarchy. These approaches can be further divided into three types: Local Classifier per Node (LCN), Local Classifier per Parent Node (LCPN), and Local Classifier per Level (LCL). The LCN approach, proposed by Koller and Sahami (Koller and Sahami, 1997), is perhaps the most common, where a classifier is trained for each node in the hierarchy. However, it is prone to inconsistencies in predictions across levels (Silla and Freitas, 2011; Dumais and Chen, 2000). The LCPN approach trains classifiers for each parent node to distinguish among its children, which can reduce inconsistencies but may still propagate errors down the hierarchy (Secker et al., 2007). The LCL approach, though less commonly used, involves training classifiers at each level of the hierarchy, but it can suffer from the challenge of discriminating among a large number of classes at deeper levels (de Carvalho and Freitas, 2009; Costa et al., 2007).\nGlobal Classifier Approaches: in contrast, treat the entire hierarchy as a single unit during training. These approaches integrate hierarchical information into the learning process, ensuring consistency across levels but often at the cost of increased computational complexity and reduced modularity (Vens et al., 2008). Notable examples include the Clus-HMC algorithm, which uses predictive clustering trees to handle the hierarchical structure (Kiritchenko et al., 2005; Vens et al., 2008). Global classifiers are advantageous in that they avoid the error propagation issues inherent in local approaches, but they require significant computational resources and are often specific to the underlying flat classifier being adapted (Silla and Freitas, 2011).\nGraph Neural Networks (GNNs): have become a central tool in hierarchical classification, particularly for handling complex dependencies between labels. These networks model the entire hierarchy as a graph, where nodes represent labels and edges represent hierarchical relationships. Recent works have shown that GNNs are particularly effective in capturing both horizontal and vertical dependencies within the hierarchy, leading to improvements in classification performance across multiple levels such as models like Hierarchy-Aware Graph Models (HiAGM) (Liu et al., 2023b).\nSpecialized loss functions: have emerged as a robust method for ensuring consistency in hierarchical multi-label classification. This approach is designed to handle the intricacies of pre-defined class hierarchies by incorporating a max constraint loss (MCLoss) that enforces hierarchical dependencies during training. The Coherent Hierarchical Multi-Label Classification Networks (C-HMCNN) (Giunchiglia and Lukasiewicz, 2020) are proposed with such MCLoss to ensure that the predictions across the hierarchy remain coherent, meaning that a child node can only be activated if its parent node is also activated. This method effectively maintains logical consistency across hierarchical levels, significantly improving classification accuracy where adherence to the hierarchy is critical.\nAdvances and Challenges: Recent advancements in hierarchical classification have focused on integrating deep learning techniques and graph-based approaches, particularly for tasks involving multi-level taxonomies, such as document categorization and other NLP tasks. Despite progress, challenges remain in scaling models to handle large, complex hierarchies consistently (Boone-Sifuentes et al., 2022a). The Taxonomy-based Transitional Classifier (TTC) proposed in this paper addresses these issues by embedding hierarchical information directly into the classification process and leveraging LLMs for multi-modal data. As a model-agnostic layer, TTC enhances flexibility and accuracy across various backbone models, offering a more consistent solution for complex hierarchies."}, {"title": "3 Taxonomy-based Transitional Classifier", "content": "This section formally defines the MLHC problem and introduces our proposed Taxonomy-based Transitional Classifier, designed to enforce hierarchical consistency across classification levels."}, {"title": "3.1 Notation and problem definition", "content": "Classification: Most classification problems in the literature focus on flat classification, where each instance is assigned to a single class from a finite set of independent, non-hierarchical classes. Formally, given a dataset $D = \\{(x^{(1)}, y^{(1)}), (x^{(2)}, y^{(2)}), ..., (x^{(m)}, y^{(m)})\\}$ with m instances, where each $x^{(i)} \\in X \\subseteq R^n$ is an n-dimensional input feature vector of the instance i and $y^{(i)} \\in Y = \\{Y_1, Y_2,\\dots, Y_k\\}$ represents its class, a classification algorithm must learn a mapping function $f : X \\rightarrow Y$, which assigns to each feature vector $x^{(i)}$ its correct class $y^{(i)}$.\nMultimodal classification: It extends the flat classification paradigm by incorporating multiple data modalities, such as text, images, or audio, into the classification process. Formally, given a multimodal dataset $D = \\{((x_1^{(i)}, x_2^{(i)},...,x_p^{(i)}),y^{(i)}) | i = 1,2,\\dots,m\\}$, where $x_j^{(i)} \\in X_j \\subseteq R^{n_j}$ represents the feature vector of modality j for instance i, and $y^{(i)} \\in Y$ represents the class, a multimodal classification algorithm learns a mapping function $f: (X_1 \\times X_2 \\times ... \\times X_p) \\rightarrow Y$. This function assigns the correct class label $y^{(i)}$ by leveraging information from all available modalities to improve prediction accuracy and robustness.\nHierarchical classification: In contrast to flat classification in which classes are considered unrelated, in a hierarchical classification problem classes are organized in a taxonomy. The taxonomy is often organized as a tree, where classes have a single parent each, or a directed acyclic graph (DAG), where classes can have multiple parents. Given a set of classes y, Wu et al. (Wu et al., 2005) defined a taxonomy as a pair (V, \u3145), where \u4eba is the \u201csubclass-of\u201d relationship with the following properties (Wu et al., 2005; Silla and Freitas, 2011): (i) asymmetry (yi, Yj \u2208 V, ifyi < Yj then yj K Yi), (ii) anti-reflexivity (\u2200y \u2208 V, Yi Kyi), and (iii) transitivity (\u2200yi, Yj, Yk \u2208 V, Yi < yj and Yj \u4eba Yk implies Yi Yk).\nIn this paper, we consider only tree taxonomies, which are organized with a hierarchy structure of n levels li, such that li CV, l\u2081 U l2. U ln = Y, \u2200yj \u2208 l1, Yi < \u00d8, and \u2200yj \u2208 li+1,\u2203!yk \u2208 li s.t. Yj < Yk for i \u2265 1 (see Figure la for a three-level taxonomy). Finally, we encode the relationship between two successive levels li and li+1 in a taxonomy using an |li|\u00d7 |li+1| matrix M[li,li+1], where the binary value Mili+1] \u2208 {0(YjkYk), 1 (yj \u4eba Yk)}, with Yk \u2208 li and yj \u2208 li+1\u00b7\nProblem definition: The multimodal multi-level hierarchical classification problem addressed in this paper is defined as the task of learning a mapping function $f : (X_1 \\times X_2 \\times \\cdots \\times X_p) \\rightarrow Y$, which assigns to each instance-represented by a combination of feature vectors from p different modalities- a prediction vector $y^{(i)} = \\{y^{[l_1]}, y^{[l_2]}, ..., y^{[l_n]}\\}$. Here, $y^{[l_i]} \\in l_i$ represents the class assigned by the function f at each hierarchical level li, ensuring accurate predictions across all taxonomy levels."}, {"title": "3.2 TTC Model Description", "content": "As mentioned earlier, our proposed TTC addresses the limitations of existing methods, which often result in inconsistent predictions, by enforcing consistency throughout the prediction process. Leveraging the detailed taxonomic information at each hierarchical level, the TTC layer guides its predictions by restricting them to labels that are appropriate for the corresponding level in the hierarchy. This approach avoids inconsistent classifications that span unrelated categories. By integrating the hierarchical structure directly, the TTC layer promotes consistency and aims to enhance the logical soundness of predictions in a multimodal context, potentially achieving better overall accuracy than conventional LLMs.\nFigure 2 illustrates the architecture of the proposed TTC layer, an LLM-agnostic component designed to leverage the taxonomy and ensure that predictions adhere to the hierarchical structure of the data. Several independent classifiers are used to predict the categories on different levels in the same way as local approaches. However, to maintain consistency, the relation information of upper levels is incorporated into the next level in the same way as attention is. The output probabilities from the upper level are multiplied by a transition matrix, where each entry represents the relationship between classes at successive levels in the taxonomy (i.e., 1 if the class in the column is a \u201csubclass of\" the class in the row, and 0 otherwise). The product can be considered as the attention score that incorporates the hierarchical information as well as the relation between classes and can be applied to the output probability for the next level. The prediction of the classifiers can be formulated as $z^{[l_i]} = W^{[l_i]} a + b^{[l_i]}$, where a is the joint output latent feature of backbone multimodal LLMs, and $W^{[l_i]}, b^{[l_i]}$ are learnable parameters that trained on the trainset regarding each li of the hierarchies. The prediction of the first classifier is obtained by applying a temperature-scaled softmax normalization, as $\\hat{y}^{[l_1]} = softmax(z^{[l_1]})$. For each subsequent level, we compute an attention score to incorporate relational information into the predictions, ensuring consistency across levels (i.e., $\\hat{y}^{[l_{i+1}]} \\\u4eba \\hat{y}^{[l_{i}]}$). This is achieved by injecting hierarchical relations as follows:\n$m^{[l_{i+1}]} = \\hat{y}^{[l_i]} \\times M^{[l_i,l_{i+1}]} $(1)\nwhere $M^{[l_i,l_{i+1}]}$ is our $|l_i| = |l_{i+1}|$ transitional matrix which encodes the relationship between two successive levels li and li+1 in a taxonomy (i.e., the binary value $M_{jk}^{[l_i,l_{i+1}]} \\in \\{0 (if y_j \\nsubclass y_k), 1 (if y_j \\\u4eba y_k)\\}$, with yk \u2208 li and yj \u2208 li+1). Referring to the example illustrated in Figure la, consider the l2 labels, which include Jewel and Fruit, and the l3 labels, comprising K gold, Pearl, Apple, and Pear. The corresponding transition matrix M[l2,13] is:\n$M^{[l_2,l_3]} = \\begin{pmatrix} 1 & 1 & 0 & 0 \\\\ 0 & 0 & 1 & 1 \\end{pmatrix}$\nin which the first row corresponds to the l2 class Jewel, where a value of 1 indicates that the 13 class (e.g., K gold or Pearl) is a subclass of Jewel, and a value of 0 indicates no such relationship. Similarly, the second row refers to the l2 class Fruit, where the values reflect whether the l3 classes are subclasses of Fruit. In this manner, the hierarchical structure of the taxonomy is fully encapsulated within the transition matrix.\nEach attention score is applied using an element-wise product on the probability output of each classifier from a lower level as:\n$\\hat{y}^{[l_{i+1}]} = softmax_\\tau (z^{[l_{i+1}]} \\odot m^{[l_{i+1}]}) $(2)\nAttention scores and classifications in Equations 1 and 2, respectively, are processed sequentially for all hierarchical levels. The loss function is also adjusted as follows:\n$\\frac{1}{m} \\sum_{j=1}^{m} \\sum_{i=1}^{n} \\pi^{[l_i]} L(\\hat{y}\\[l_i], y\\[l_i])$ (3)\nwhere L(,) denotes the cross-entropy function and $\u03c0^{[l_i]}$ are a set of importance factors that can be tuned to changing the weight of losses for different li.\nContinuing with the example provided earlier, given the transition matrix M[l2,13], and assuming the probability output from the 12 classifier is $\\hat{y}\\[2] = \\{0.9,0.1\\}$, the attention scores are calculated as: $m^{[3]} = \\hat{y}\\[2] \\cdot M^{[l_2,l_3]} = \\{0.9, 0.9, 0.1, 0.1\\}$. Assuming the output from 13 is z[3] = {-0.2, 0.5, 1.3, 0.3}, applying the attention scores m[3] and a softmax function to normalize the result gives the prediction probability output: $\\hat{y}\\[3] = \\{0.182, 0.342, 0.249, 0.225\\}$.\nCompared to a flat classifier for 13 which would have applied directly softmax to z[3], TTC's prediction produces more consistency with upper-level prediction. Additionally, from a taxonomic perspective, tree-like hierarchical classification leverages general-to-specific relationships, where general categories have better data separability. This indicates that they possess wider margins in their decision boundaries, making it easier for classifiers to distinguish them. As a result, general classes at higher levels contribute to higher classification accuracy at the top (Cortes, 1995). By enforcing the consistency across hierarchical levels, the LLM is further guided to make more accurate predictions at deeper, more specific levels with greater granularity."}, {"title": "4 Experiments and Results", "content": "This section evaluates the performance of a TTC in MLHC tasks using the MEP-3 dataset, a large-scale multimodal e-commerce product dataset containing over 3 million entries. Due to computational constraints and a significant portion of entries lacking third-level hierarchical labels, we focused on the third-largest food subset, chosen for its diversity in product types. To maintain consistency in the experiments, we excluded entries that do not include third-level hierarchies. As a result, the final dataset used for these experiments consisted of 177,195 data points. Figure 3 illustrates the distribution of data across all classes at both hierarchies."}, {"title": "4.1 Experimental Details", "content": "Experimental Setting: The preprocessing steps were as follows: Textual Data: Product descriptions were tokenized using Byte Pair Encoding (BPE) or Wordpiece tokenization, which are commonly used methods for handling both Chinese and multilingual text. Stop words were also removed to reduce noise, improving the model's focus on relevant content. Image Data: The majority of images in the dataset were already 220 \u00d7 220 pixels, but a minority were smaller, with sizes like 64 \u00d7 50, 75 x 75, 60 \u00d7 60, and 54 \u00d7 54. To ensure uniformity across the dataset, all images were resized to 220x220 pixels. This resizing helped maintain consistent input dimensions for the model. Additionally, pixel values were normalized to fall within a common range to improve model stability and convergence during training. Hierarchical Labels: The dataset's hierarchical structure consists of three levels: 1 top-level class, 4 second-level sub-classes, and 40 third-level sub-sub-classes. Labels were encoded to ensure that each product was accurately categorized across the relevant levels of hierarchy.\nAn 80/20 split was applied to this food subset, with 80% of the data (141,756 inputs) used for training. We split the whole process into two stages to speed up the training procedure: Fine-tune LLMs: We applied Low-Rank Adaptation (LoRA) (Hu et al., 2021) to the LLMs and fine-tuned them on the training set to improve the models' representation capabilities. Since bottom-level labels offer greater granularity, the model learned finer distinctions between similar classes within broader categories. To achieve this, we sampled 1,000 data entries from each l2 label (40,000 in total). This balanced sampling provided sufficient data for the model to learn detailed and specific features for each class, which is essential for fine-grained classification. By ensuring that each 12 class had an equal number of samples, we avoided overrepresentation of certain classes, leading to more stable and reliable performance across all categories. We utilized the Parameter-Efficient Fine-Tuning (PEFT) library (Mangrulkar et al., 2022) to efficiently apply LoRA to the candidate backbone LLMs. Train TTC: Aligned with the hierarchical levels of the dataset we used, the TTC also consists of 2 levels of classifiers to predict both levels of categories. After fine-tuning, the backbone models are integrated with TTC with frozen parameters to train the classifier further on the training set. 20% (35,439 inputs) of the subset is used for testing. The input of the dataset, which contains (v,t), where v refers to the representation from the image and t refers to the textual representation, is fed into the backbone Multimodal LLMs to generate a joint implicit feature x for the taxonomy-based transitional classifier to make predictions on each level of hierarchy.\nBackbone Multimodal LLMs: For backbone models, we have adopted different LLMs including: LLAVA-1.5 (13B) (Liu et al., 2024b), Visual-Bert (Li et al., 2019), mPLUG-Owl (7B) (Ye et al., 2023), OpenFlamingo(9B) (Awadalla et al., 2023), Fuyu(8B) (Bavishi et al., 2023), InstructBLIP(7B) (Dai et al., 2024), MiniGPT-4 (Zhu et al., 2023), and Llama (7B) (Touvron et al., 2023). Notably, for Llama, we manually integrated ResNet as the visual encoder to enable multimodal functionality. ResNet extracts visual features from images, which are then projected to align with Llama's embedding dimensions and fused with textual inputs. This integration allows Llama to process and generate responses based on both visual and textual information. Each model is assessed with and without the proposed method, denoted by (TTC). The hyperparameters for the experiments are set as Table 1.\nEvaluation Metrics: For evaluating the MLHC task, we have adopted the Hierarchical F1-Score (HF1- score) (Kosmopoulos et al., 2015), which assesses model performance in predicting classes across different hierarchy levels. Similar to the F1-score, the HF1-Score is defined as:\nHF1- Score = $\\frac{2 \\cdot (H-Precision \\cdot H-Recall)}{H-Precision + H-Recall}$\nwhere H-Recall and H-Precision are analogous to Recall and Precision but evaluate the proportion of correctly predicted classes among all actual/predicted classes. In addition to the HF1-Score, we also use consistency and Exact Match as evaluation metrics. Consistency ensures that predicted labels adhere to hierarchical structures, meaning predictions across all levels remain within the same hierarchy. Exact Match is a stricter criterion requiring that predictions not only stay within hierarchy but also exactly match true labels at all levels."}, {"title": "4.2 Experimental Results", "content": "We present the results of applying the proposed taxonomy-based transitional classifier (TTC) to various large multimodal LLMs for a comparative analysis. Figure 4 provides an overall comparison of the different LLMs with and without the integration of the model-agnostic TTC layer (see Table 2 for details, and refer to Appendix A for further information). Across the board, integrating the hierarchical layer shows a clear improvement in performance for most models, confirming the efficacy of the TTC approach. In particular, while HF1-Score remains relatively high across models\u2014indicating a strong ability to capture hierarchical relationships\u2014there is a slight decrease in this metric for some models when TTC is introduced. This suggests that TTC's emphasis on enforcing consistency between layers can result in a trade-off with general performance. However, the hierarchical layer consistently leads to improvements in Consistency, Exact Match, and Accuracy at 13, highlighting its strength in producing more coherent and fine-grained predictions. These enhancements underline the effectiveness of TTC in addressing complex hierarchical classification tasks, ensuring predictions align better with structured taxonomy.\nFor detailed comparison, Figure 5 further highlights the impact of TTC by illustrating how it significantly boosts key metrics such as Consistency and Exact Match across models. For example, while applying TTC to LLAVA-1.5 (13B) resulted in a slight reduction in HF1-Score (from 0.8356 to 0.8003), it led to a substantial improvement in Consistency, increasing from 0.6305 to 0.7268, indicating more coherent predictions aligned with the hierarchical structure. Similarly, mPLUG-Owl (7B) saw a remarkable improvement in Exact Match (from 0.3881 to 0.6218) and Consistency (from 0.4538 to 0.7096), demonstrating TTC's ability to enhance alignment with taxonomical classifications. OpenFlamingo (9B), which experienced improvements across all metrics, particularly in Consistency (from 0.4903 to 0.8485) and 13 Accuracy (from 0.6102 to 0.8404), further reinforces the effectiveness of TTC in producing more precise and reliable predictions. These results collectively showcase TTC's capacity to significantly improve LLMs' ability to handle hierarchical classification tasks, leading to more accurate and consistent model outputs, especially for tasks that require deeper, fine-grained distinctions.\nOverall, the hierarchical classification method generally enhances the consistency of predictions and, in many cases, improves the exact match metric. These results highlight the potential of our method to improve the performance of multimodal large language models in MLHC tasks. Figure 6 further supports this by demonstrating a strong positive correlation between Consistency and 13 Accuracy, indicating that models which align their predictions with the taxonomy tend to perform better on detailed classification tasks. This correlation suggests that the TTC layer is not only effective for hierarchical classification but can also be extended to traditional classification tasks. By constructing labels from the bottom up and applying the TTC layer in a top-down manner, the divide-and-conquer approach has the potential to enhance performance in a wide range of classification problems."}, {"title": "5 Conclusion", "content": "In conclusion, the proposed taxonomy-based transitional classifier (TTC) demonstrates significant potential in enhancing the performance of large multimodal language models, particularly in hierarchical classification tasks. Across all evaluated models, the TTC layer led to notable improvements in key metrics such as Consistency, Exact Match, and l3 Accuracy, as seen in both the grouped bar chart and the Consistency vs l3 Accuracy diagram. While some trade-offs, such as slight reductions in HF1-Score, were observed, these were offset by the substantial gains in consistency and fine-grained accuracy, underscoring the efficacy of TTC in aligning model predictions with the underlying hierarchical structure. The strong positive correlation between Consistency and l3 Accuracy further suggests that TTC can be extended beyond hierarchical tasks to traditional classification problems, where it could serve as a top-down, divide-and-conquer approach to boost performance. Overall, these results emphasize the versatility and effectiveness of TTC in improving both hierarchical and standard classification tasks, making it a promising addition to model-agnostic strategies for enhancing multimodal LLMs."}, {"title": "Limitations", "content": "Though TTC-aided LLMs demonstrated significantly better performance across various metrics compared to traditional LLMs, and can be applied to many classification tasks, they rely on an inherent hierarchical structure in the data and require manual annotation to create multiple levels of classes. For large-scale datasets with deep hierarchies, this manual annotation incurs high labor costs, and calculating the transition matrix becomes increasingly complex. Additionally, the current approach only considers top-down transitions, ignoring bottom-up information that could enhance prediction consistency across levels. This restricts the model's ability to capture interdependencies between lower and higher-level predictions.\nMoreover, the sequential nature of the TTC design limits its parallelizability, as predictions for different levels must be processed one after another. This sequential processing increases computational costs and reduces efficiency, particularly for large datasets, making the method less suited for real-time applications where speed is crucial."}]}