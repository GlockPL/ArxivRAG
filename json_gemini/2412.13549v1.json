{"title": "EscapeBench: Pushing Language Models to Think Outside the Box", "authors": ["Cheng Qian", "Peixuan Han", "Qinyu Luo", "Bingxiang He", "Xiusi Chen", "Yuji Zhang", "Hongyi Du", "Jiarui Yao", "Xiaocheng Yang", "Denghui Zhang", "Yunzhu Li", "Heng Ji"], "abstract": "Language model agents excel in long-session planning and reasoning, but existing benchmarks primarily focus on goal-oriented tasks with explicit objectives, neglecting creative adaptation in unfamiliar environments. To address this, we introduce EscapeBench-a benchmark suite of room escape game environments designed to challenge agents with creative reasoning, unconventional tool use, and iterative problem-solving to uncover implicit goals. Our results show that current LM models, despite employing working memory and Chain-of-Thought reasoning, achieve only 15% average progress without hints, highlighting their limitations in creativity. To bridge this gap, we propose EscapeAgent, a framework designed to enhance creative reasoning through Foresight (innovative tool use) and Reflection (identifying unsolved tasks). Experiments show that EscapeAgent can execute action chains over 1,000 steps while maintaining logical coherence. It navigates and completes games with up to 40% fewer steps and hints, performs robustly across varying difficulty levels, and achieves higher action success rates with more efficient and innovative puzzle-solving strategies. All the data and codes are released\u00b9.", "sections": [{"title": "1 Introduction", "content": "Building robust language model (LM) agents for planning and reasoning has always been challenging. Recent efforts have explored how agents could compress and utilize memory (Wang et al., 2023a; Hu et al., 2023; Liu et al., 2023b; Liang et al., 2023b; Wang et al., 2024d,c; Zhong et al., 2024), perform complex reasoning (Wei et al., 2022; Kojima et al., 2022; Zhou et al., 2023a; Lin et al., 2024; Yao et al., 2023), planning (Wang et al., 2023b; Liu et al., 2023a; Hao et al., 2023; Yao et al., 2024; Zhou et al., 2024a), and reflection"}, {"title": "2 Related Work", "content": "Creativity in Language Models. Creativity is a cornerstone of human intelligence and a growing focus in Al research (Legg and Hutter, 2007; Lake et al., 2017). LMs have demonstrated notable creative capabilities across domains - they excel at generating narratives and poetry (Brown et al., 2020; Akoury et al., 2020), show effectiveness in tool creation and design (Qian et al., 2023; Cai et al., 2024), and augment human creativity through interactive ideation (Mialon et al., 2023). In scientific discovery, research has also found that LM-generated ideas tend to be more novel but slightly less feasible than those from human experts (Si et al., 2024; Wang et al., 2024b).\nHowever, research on LM creativity still remains nascent, emphasizing novelty, surprise, and practical value through psychological assessments like the Alternative Uses Test (AUT) (Guilford, 1967) and Torrance Tests of Creative Thinking (TTCT) (Boden, 1998). Creativity in LMs is categorized as combinatorial, exploratory, or transformational, with transformational being the most challenging (Franceschelli and Musolesi, 2023). A TTCT study found GPT-4 performing in the top 1% of human fluency and originality, but adapting such assessments to other LMs faces limitations like sample randomness and high evaluation costs (Guzik et al., 2023). Similarly, a modified Torrance Test (Zhao et al., 2024) identified strengths in elaboration and originality but highlighted gaps influenced by prompts and role-play. Notably, most research evaluates backbone models, whereas our work explores creativity within an LM agent-based setting that requires complex reasoning and planning.\nAgent Evaluation in Simulated Environment. Existing work in agent evaluation primarily focuses on text-based and sandbox-like environments that assess an agent's cognitive and behavioral abilities in goal-oriented tasks (Zhou et al., 2023b; Chen et al., 2022, 2024; Yu et al., 2024; Deng et al., 2024), with emerging research exploring the integration of LM/VLM-enabled agents in robotics to address challenges in real-world, embodied settings (Liang et al., 2023a; Huang et al., 2023b, 2024b; Rana et al., 2023). Text-based environments (Yuan et al., 2018; C\u00f4t\u00e9 et al., 2019), such as interactive fiction games (Lin et al., 2024) or conversational agents (Qiao et al., 2023), evaluate natural language understanding, reasoning, and decision-making consistency (Uluda\u011fl\u0131 and O\u011fuz, 2023; Qi et al., 2024). Games like Zork (Infocom, 1980) and TextWorld measure narrative comprehension and problem-solving in structured contexts. In contrast, sandbox environments (Lin et al., 2023; Gan et al., 2021; Fan et al., 2022) like Minecraft (Zhu et al., 2023) and Roblox (Rospigliosi, 2022) provide more open-ended settings that test spatial reasoning, planning, and collaboration (Carroll et al., 2019; Agashe et al., 2023). Evaluation of agents in these environments typically relies on task-specific, cognitive, behavioral, and learning metrics to assess their ability to achieve objectives, reason adaptively, and collaborate. However, existing benchmarks often neglect creative adaptation in unfamiliar environments requiring unconventional tool use and proactive problem-solving. To address this gap, we introduce EscapeBench which evaluates agents' creative reasoning and strategic problem-solving through navigating uncertain goal-achieving pathways, offering a new dimension of agent evaluation."}, {"title": "3 EscapeBench Construction", "content": "3.1 Motivation\nMost current agent benchmarks emphasize explicit, goal-oriented tasks, where decisions rely on commonsense knowledge encoded within the agent model's parametric space. For instance, a typical task in AgentBench (Liu et al., 2024b) might ask: \"What musical instruments did the Minnesota-born Nobel prize winner play?\u201d Using the Freebase search API, the agent can chart a clear pathway to the goal, identifying relevant data and filtering out irrelevant results. This process is deterministic and fact-based, with each step reflecting the agent's analytical skill in processing search outputs. While such benchmarks effectively test an agent's information-seeking and retrieval capabilities, they often overlook its ability to engage in creative problem-solving\u2014an area that extends beyond analytical and practical intelligence.\nThis leads to our core research question: How to build an environment that benchmarks an agent's creative intelligence? To answer this, an agent must demonstrate out-of-the-box thinking in its reasoning. Since the primary function of agents lies in their tool-use abilities, we aim to design a benchmarking environment that emphasizes creative tool use. Room escape game scenarios, by nature, are well-suited for this purpose, as they require a player to creatively make use of all the"}, {"title": "3.3 Action Space", "content": "The model agent could take five different actions in each scene. While the action space is well-defined, the parameter space-regarding the scenes, items, or tools involved in these actions\u2014is high-dimensional, thus enabling dynamic interactions.\n\u2022 Move (Scene): Move to an adjacent scene.\n\u2022 Click (Item): Click to simply interact with an item in the scene.\n\u2022 Apply (Tool, Item): Apply a tool in the bag to an item in the scene.\n\u2022 Input (str, Item): Input an arbitrary string to an item in the scene.\n\u2022 Craft (Tool, Tool): Use two tools in the bag to craft a new one.\nFigure 2 illustrates the connections between the components of the game engine and the model agent's action space. Among all the actions, \u201cApply\u201d and \u201cCraft\u201d stand out as the most creativity-driven, as they require the agent to think innovatively about how to use or craft tools in an unseen way during its training. We delve into specific examples in Section 3.5."}, {"title": "3.4 Annotation and Statistics", "content": "Building on existing online room escape games and puzzle-solving logic\u00b9, we present EscapeBench, featuring 12 scenarios and 36 settings across three difficulty levels. Each scenario, such as a laundry"}, {"title": "3.5 Preliminary Study", "content": "To verify the creative reasoning required in our designed games and assess the current LMs' ability to perform such reasoning, we conduct a preliminary case study in Table 2. We sample diverse real scenarios from EscapeBench's game design and test them on closed-source GPT-40 (Hurst et al., 2024) and open-source LLama-3.1-70B (Dubey et al., 2024). For each model, we instruct it to think creatively about what it might achieve with the provided Items and Tools in room escape scenarios.\nOur results reveal that: i) EscapeBench offers a wide range of creative reasoning challenges, encompassing unconventional tool usage, implicit numerical puzzle solving, innovative tool crafting strategies, etc. ii) Both closed- and open-source models demonstrate significant limitations in creativity and imagination, particularly in identifying implicit goals and devising creative strategies. These findings emphasize the complexity and necessity of our benchmark as well as the current gaps in models' creativity."}, {"title": "4 EscapeAgent Design", "content": "Our preliminary study highlights that current LMs often struggle with identifying implicit goals and engaging in the creative reasoning needed to solve complex puzzles. To overcome these challenges and maximize the LM's potential, we present EscapeAgent, a novel framework designed to uncover tasks dynamically and reason creatively about tool use. EscapeAgent addresses two core challenges identified by EscapeBench:\n\u2022 Uncertain Goal Pathways: We introduce a Reflection module, which dynamically maintains a task list by adding, updating, or removing tasks after each trial-and-error action. This approach fosters proactive task discovery and sharpens the agent's focus on specific goals (Section 4.2).\n\u2022 Creative Tool Use: We design a Foresight module that facilitates explicit reasoning about creative tool applications and their alignment with specific tasks. It enables the agent to hypothesize and evaluate strategies before execution, thus promoting problem-solving (Section 4.3).\nBy integrating both modules with a BaseAgent, EscapeAgent excels at managing Super-long Reasoning Chains, effectively performing over 1,000 consecutive actions while maintaining logical coherence. This robust design greatly enhances the language model's capabilities in creativity, problem-solving, and strategic thinking."}, {"title": "4.1 BaseAgent", "content": "The BaseAgent serves as the foundation of EscapeAgent, as shown in the middle section of Figure 4. It takes actions based on the scenario context provided by the game engine and updates its working memory with environment feedback after each step. The working memory stores the agent's previous actions and corresponding feedback.\nThe BaseAgent employs Chain-of-Thought (Wei et al., 2022) reasoning to decide its next action. Note that it serves as a strong baseline and standard evaluation method for EscapeBench, as it already incorporates all the essential components an agent needs for reasoning. For more implementation details, please refer to Appendix C.1."}, {"title": "4.2 Reflection Module", "content": "The Reflection Module enables the agent to maintain a structured task list, updated through:\n\u2022 New: Add a newly identified, unsolved task.\n\u2022 Update: Record attempted but failed actions.\n\u2022 Delete: Remove a task when its goal is achieved. Each task list entry includes the task name, the target item, and previously failed actions. This organization prevents the agent from repeating ineffective actions, thus promoting efficiency.\nThe reflection module is triggered after every non-move action, using environment feedback to guide task list updates. For example, in Figure 4 (right), Task 1 is deleted once the machine successfully starts. By encouraging targeted problem-solving rather than random exploration, the reflection module significantly improves task-solving efficiency. See Appendix C.2 for more details."}, {"title": "4.3 Foresight Module", "content": "The Foresight Module enhances creative reasoning by explicitly evaluating tool use and problem-solving strategies. It activates in two cases:\n\u2022 When a new task is identified: The agent evaluates existing tools it has and hypothesizes potential actions to achieve the task."}, {"title": "5 Experiments", "content": "We divide our experiments into two parts: i) Benchmarking the creativity of current models using the BaseAgent (Section 5.2), and ii) Demonstrating the effectiveness of EscapeAgent (Section 5.3)."}, {"title": "5.1 Settings", "content": "Environment. Experiments are conducted on 36 game settings. An agent is considered to be making progress if it either achieves a key step (bottleneck actions defined in Figure 3) or collects a new tool. Agents will receive help if they fail to make progress for 50 consecutive actions (see Appendix D.1), thus ensuring full completion of the game. The working memory length is set to 10 due to context window limitations.\nModels. We evaluate both closed- and open-source models: GPT-40, GPT-40-mini (Hurst et al., 2024), Claude-3.5 (Anthropic, 2024), Gemini-1.5 (Team et al., 2024), Llama-3.1 (Dubey et al., 2024), Qwen-2.5 (Team, 2024), DeepSeek-LLM (Liu et al., 2024a), Phi-3.5 (Abdin et al., 2024), Yi (Young et al., 2024), Ministral (MistralAI, 2024). Models with fewer than 7B parameters are excluded due to near-random behavior. For consistency, we set the sampling temperature to T = 0 and n = 1.\nMetrics. We use two main metrics including:\n\u2022 Hints Used: Total hints used in a game.\n\u2022 Total Steps: Total actions taken in a game.\nAuxiliary metrics for analysis include:\n\u2022 Early Exit Progress: Proportion of key steps and tools collected before needing a hint (game progress before needing a hint for the first time)."}, {"title": "5.2 Benchmarking Results", "content": "We benchmark current models using the BaseAgent framework, with results in Table 3 showing that large-scale closed-source models, such as GPT-40, Claude-3.5, and Gemini-1.5, consistently outperform smaller models. Key insights include:\n\u2022 Most hints are used on key steps, which require creative reasoning, while models can often collect tools through random exploration.\n\u2022 Models require significantly more action steps and hints than the average human player, with some using up to 20x more steps than the most efficient action chain.\nComplex environment causes challenges. We show in Figure 5 and Figure 6 that, generally, harder game settings require more steps and hints for an agent to solve. Since our difficulty setting depends solely on the granularity and usefulness of descriptions and feedback (see Table 1), our results demonstrate that the way the environment is presented can impact difficulty, even when the core"}, {"title": "5.3 EscapeAgent Results", "content": "The introduction of the Reflection and Foresight modules in EscapeAgent, as shown in Table 5, leads to a marked reduction in hint usage and total steps, with large-scale models benefiting the most. Key insights include:\n\u2022 Larger models like GPT-40 consistently outperform smaller models, suggesting that while the new modules aid creative reasoning, the core model's capabilities remain a crucial factor.\n\u2022 Early exit progress improves significantly across models, despite the exponentially increasing difficulty of maintaining consecutive meaningful progress without hints. Notably, GPT-40 achieves near-human-level performance, demonstrating the effectiveness of the enhanced framework.\nEscapeAgent progresses more efficiently. Figure 9 demonstrates EscapeAgent's more efficient progress, as the slopes of the dotted lines are consistently steeper. We further show in Figure 8 that EscapeAgent requires fewer actions to achieve the"}, {"title": "6 Discussions and Future Directions", "content": "LM's creativity for benchmarking. Our experiments with EscapeBench reveal key insights into the capabilities and limitations of current language models in creative reasoning and tool use. Even the most advanced model, EscapAgent, requires more hints and takes twice as many steps as the average human to solve tasks. The benchmark demonstrates that these advanced models struggle with implicit goal identification and creative problem-solving, often relying heavily on extensive hints. This underscores a significant gap in the evaluation of AI agents: while analytical and practical intelligence are well-assessed, creative intelligence remains largely underexplored. Addressing this gap may require fundamentally improving how LMs organize knowledge, linking objects not just by words but by their affordances-how their properties and functions relate to potential uses.\nHuman-AI Collaboration. Human-AI collaboration in escape room puzzles holds the potential to significantly enhance problem-solving efficiency and introduce a new paradigm of learning. Humans contribute intuitive insights and creative ideas that language models are unlikely to generate independently, while AI excels in systematic tasks such as data analysis and information aggregation. Furthermore, this collaboration leverages the strength of chain-of-thought reasoning, a process through which Al organizes and evaluates information in a logical, step-by-step manner. Integrating human creativity with AI's structured problem-solving capabilities together in the future might not only foster innovative strategies but also enable deeper learning experiences, making the problem-solving process more dynamic and efficient.\nTheoretical Foundations for AI Creativity. Understanding the cognitive mechanisms behind human creativity is essential for designing AI systems that emulate or surpass human creative processes. Human creativity is a multifaceted phenomenon involving the generation of novel and valuable ideas, problem-solving, and adaptation to complex situations (Dainys, 2024). Neurologically, it arises from the interplay between stochastic"}, {"title": "7 Conclusion", "content": "In this work, we introduce EscapeBench, the first benchmark for advancing LM's creativity. Our results highlight that current LMs still lag in creative reasoning. However, the EscapeAgent framework effectively lowers the barrier to creative problem-solving and implicit goal identification, resulting in significant performance improvements. While these advancements are promising, challenges remain in further enhancing the creative capabilities of the language model itself. Future work may focus on integrating multi-modal perception and developing new reinforcement learning algorithms"}, {"title": "Appendix", "content": "A Engine Design Details\nThe game engine involves scenes, tools, and items as three main components. We will introduce in detail each of them in the following.\nA scene typically includes a description, its connections to other scenes, and the tools and items it contains. A typical scene example in the game configuration looks like the following:\nIn this example, the name of this scene is \u201challway\". It leads to nearby scenes including \"blocked path close-up\" and \"cabinet close-up\", where the model could reach through action \u201cmove(To the blocked path close-up)\u201d and \u201cmove(To the cabinet close-up)\".\nEach tool has various states and a visibility status. In each state, a tool is either awaiting the application of another tool or ready to be applied to another tool or item. A typical tool example in the game configuration looks like the following:\nIn this example, a rusty key (tool, state 1) hidden in the chest (item) won't be visible to the user until the chest is opened, and after applying lubricant (tool), it may change to a non-rusty functional key (tool, state 2) that could be applied to open a safe. Item is an upgraded version of the tool, as each state may await multiple inputs or tools in order to trigger certain changes, including item and tool's visibility, state, etc. A typical item example in the game configuration looks like the following:\nB Data Annotation Details\nWe recruited eight human annotators, all with prior Room Escape game experience (offline and online) and at least a bachelor's degree. To ensure a smooth annotation process, all annotators were U.S.-based students with computer science backgrounds. Each annotator received detailed guidelines to ensure objective annotations and was tasked with extracting game logic and object descriptions (scenes, items, and tools) based on the official guide. The foundational data logic will be released with the software, and all annotators consented to the data collection.\nC Agent Design Details\nFor both BaseAgent and EscapeAgent, we apply the same system prompt for its action-taking to ensure fairness:"}, {"title": "C.1 BaseAgent Details", "content": "At each step, the BaseAgent receives information from the game engine. This information typically includes an environment description, a list of interactable objects in the scene, and the tools available in the agent's bag. A typical environment description appears as follows:\nThe scene typically includes a general description, while each item within the scene is accompanied by a detailed description. Possible actions specify which items or aspects of the scene the agent can interact with, and tools in the bag indicate which tools are available for use.\nThis environment description will also be coupled with working memory of previous steps. Each step's memory contains the following fields:"}, {"title": "C.2 Reflection Module Details", "content": "The Reflection module is integrated as a downstream component after BaseAgent within the EscapeAgent design. This module is responsible for maintaining a task list that is updated solely based on the agent's current actions and the feedback received. Each task in the list generally includes the following fields:"}, {"title": "C.3 Foresight Module Details", "content": "The Foresight module serves as an upstream component preceding the BaseAgent in the EscapeAgent design. This module is activated when a new tool is collected during the last action step or a new task is added during the previous reflection step. When a new tool is collected, the agent is provided with the current task list and instructed to propose potential applications for the tool within the context of these tasks and their specific scenarios. Additionally, the agent is given a list of all existing tools in its bag and encouraged to creatively evaluate whether the new tool could be combined with others to craft something useful. The following system prompt is employed to guide the model:"}, {"title": "D More Experiment Details", "content": "D.1 Help Setting\nWe provide help to the agent through explicit instructions, focusing on two aspects: i) identifying the next action location that could help the model make progress, and ii) specifying the action the model should take. These objectives are addressed by providing the instruction below to the model during the action-taking step:\nD.2 Resource Setting\nFor closed-source models, we utilize standard APIs for testing. Under the BaseAgent framework, the average number of API calls per game setting is approximately 800, resulting in a total cost of $50-60 per model test. While the EscapeAgent framework introduces two additional modules, these are not always triggered. Consequently, the total number of API calls increases to roughly 1.2 times that of BaseAgent, raising the cost to approximately $60-80 per model test.\nD.3 More Study Results\nIn Figure 12, we present additional results on the action step intervals with respect to two types of progress-making ways, achieving Key Step or Tool Collection. It can be observed that EscapeAgent consistently requires fewer steps to perform the next bottleneck Key Step. In contrast, for Tool Collection, the difference between the two bars is less pronounced. Despite this, the findings still demonstrate the effectiveness of our design. Tool Collection typically occurs after the successful application or crafting of tools, meaning the reasoning challenge associated with it is significantly lower. Since EscapeAgent focuses primarily on creative reasoning, it is reasonable that it excels in identifying the next Key Action more efficiently.\nIn Figure 13, we present the results of action trial counts for a specific item across four additional models. We observe that EscapeAgent achieves a higher success rate within 10 trials, even though it does not always succeed on the first attempt. Furthermore, for smaller models like Phi-3 and Ministral, EscapeAgent occasionally outperforms BaseAgent even in terms of one-trial success rates. This highlights how our framework effectively lowers the barriers for creative reasoning, even for less capable language models.\nIn Figure 15, we showcase eight additional pairs of progress-making maps for eight more models. These case studies illustrate two key points: i) There are significant disparities in creativity among models. For instance, models like GPT-40-mini and Qwen-2.5-72B require only two-thirds of the total steps than others to achieve success, while smaller models such as Qwen-2.5-7B heavily rely on hints to make progress, even with the EscapeAgent framework. ii) When combining these results with Table 5, we observe that EscapeAgent's performance improvement relative to BaseAgent is more pronounced for larger-scale models like"}]}