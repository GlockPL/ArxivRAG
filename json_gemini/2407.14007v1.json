{"title": "Multi-modal Relation Distillation for Unified 3D Representation Learning", "authors": ["Huiqun Wang", "Yiping Bao", "Panwang Pan", "Zeming Li", "Xiao Liu", "Ruijie Yang", "Di Huang"], "abstract": "Recent advancements in multi-modal pre-training for 3D point clouds have demonstrated promising results by aligning heterogeneous features across 3D shapes and their corresponding 2D images and language descriptions. However, current straightforward solutions often overlook intricate structural relations among samples, potentially limiting the full capabilities of multi-modal learning. To address this issue, we introduce Multi-modal Relation Distillation (MRD), a tri-modal pre-training framework, which is designed to effectively distill reputable large Vision-Language Models (VLM) into 3D backbones. MRD aims to capture both intra-relations within each modality as well as cross-relations between different modalities and produce more discriminative 3D shape representations. Notably, MRD achieves significant improvements in downstream zero-shot classification tasks and cross-modality retrieval tasks, delivering new state-of-the-art performance.", "sections": [{"title": "1 Introduction", "content": "Recently, 3D shape understanding has garnered increasing attention due to its wide range of applications, such as space calculation [2, 27, 30, 42], autonomous driving [23, 53] and robotic perception [3,46]. Despite notable advances in 3D visual analysis, the limited availability of 3D data, characterized by constraints in scale and scarcity of annotations, remains a significant barrier.\nTo tackle this challenge, many researchers have delved into integrating auxiliary modalities within the self-supervised learning framework. Some attempts utilize priors from the image modality to craft more instructive pretext tasks [9,58], thereby enhancing the discriminative power of the learned 3D representations. Meanwhile, others focus on distilling knowledge from pre-trained models in either image or text modalities to facilitate conceptual understanding within the 3D modality [57,60].\nAmong these endeavors, tri-modal-based methodologies [16,26,49,50,59] have shown exceptional prowess by aligning point cloud representations with the pre-aligned image-text feature space. They employ a synthesized set of triplet data"}, {"title": "2 Related Work", "content": "Self-Supervised Learning (SSL) for point cloud understanding initiates with the 3D modality. The researchers diligently craft a range of pretext tasks such as geometric reconstruction [22, 39, 40, 43], mask auto-modeling [6,31,54,56], normal estimation [33], and contrastive learning [18,48,51] to enhance shape representations of deep learning backbones of 3D point clouds [34, 36, 44, 54]. Contemporary endeavors seek to augment 3D SSL by importing insights from other modalities. Attempts leveraging image-guided strategies [58], multi-modal reconstruction [9, 14,35], and weight-sharing [5,37] are integrated into conventional frameworks such as Masked Autoencoder [15] or contrastive learning, yielding significant improvements over single-modality methods. Despite the success in enhancing the performance of downstream applications, these methods are limited by their inability to forge more comprehensive cross-modal correlations."}, {"title": "2.1 3D Self-supervised Learning", "content": "Self-Supervised Learning (SSL) for point cloud understanding initiates with the 3D modality. The researchers diligently craft a range of pretext tasks such as geometric reconstruction [22, 39, 40, 43], mask auto-modeling [6,31,54,56], normal estimation [33], and contrastive learning [18,48,51] to enhance shape representations of deep learning backbones of 3D point clouds [34, 36, 44, 54]. Contemporary endeavors seek to augment 3D SSL by importing insights from other modalities. Attempts leveraging image-guided strategies [58], multi-modal reconstruction [9, 14,35], and weight-sharing [5,37] are integrated into conventional frameworks such as Masked Autoencoder [15] or contrastive learning, yielding significant improvements over single-modality methods. Despite the success in enhancing the performance of downstream applications, these methods are limited by their inability to forge more comprehensive cross-modal correlations."}, {"title": "2.2 Relation Modeling in Contrastive Learning", "content": "Previous research demonstrates that a thorough understanding of the complex relations among samples significantly improves the acquisition of structural intricacies within representations [32]. Recently, this detailed relation modeling has been integrated into the realm of contrastive learning. RINCE [17] enhances contrastive learning by establishing a partial order among samples, thus contributing to more precise representations of 2D images. Similarly, Soft-InfoNCE [21] refines the margins of the sample distribution based on sample similarities, leading to a notably more distinct feature space.\nIn the context of bi-modal contrastive learning, CyCLIP [13] introduces the constraints based on pairwise Euclidean distances, applicable both within single modality and across different modalities. Meanwhile, CLIP-PSD [1] adopts similarity-based connections to mitigate the requirement of rigid one-to-one correspondence during training. Some alternative methodologies prioritize modeling complex relations through low-level similarities [11, 12], coherence of local patches [52], and diversity augmentation [55]. Similar enhancements also appear in the scenarios like downstream distillation [20] and CAD representation learning [29]. Both the strategies outperform the standard baseline, indicating their effectiveness."}, {"title": "3 Method", "content": "Previous studies align 3D shape representations with the pre-trained CLIP embedding space of images and texts by applying the contrastive learning loss. Given a set of synthesized image-text-3D triplet $\\mathcal{X} = \\{(x_i^I, x_i^T, x_i^P)\\}_{i=1}^N$ as training samples, dedicated encoders $E^I, E^T, E^P$ for each modality process inputs in their respective domains to produce the corresponding feature embeddings $f_i^I, f_i^T, f_i^P$. Notably, $E^I$ and $E^T$, derived from the extensive pre-training of CLIP on a large-scale image-text dataset, cover a data volume substantially greater than the number of available synthesized triplets. To circumvent potential model collapse, the weights for the image branch $E^I$ and the text branch $E^T$ are kept frozen, with only the point cloud branch $E^P$ being actively trained to align 3D representations with corresponding image-text pairs. Upon acquiring the feature embeddings for all samples in the batch, alignment between 3D-to-Image and 3D-to-Text is executed following the contrastive learning procedure in CLIP:"}, {"title": "3.1 Preliminaries", "content": "Previous studies align 3D shape representations with the pre-trained CLIP embedding space of images and texts by applying the contrastive learning loss. Given a set of synthesized image-text-3D triplet $\\mathcal{X} = \\{(x_i^I, x_i^T, x_i^P)\\}_{i=1}^N$ as training samples, dedicated encoders $E^I, E^T, E^P$ for each modality process inputs in their respective domains to produce the corresponding feature embeddings $f_i^I, f_i^T, f_i^P$. Notably, $E^I$ and $E^T$, derived from the extensive pre-training of CLIP on a large-scale image-text dataset, cover a data volume substantially greater than the number of available synthesized triplets. To circumvent potential model collapse, the weights for the image branch $E^I$ and the text branch $E^T$ are kept frozen, with only the point cloud branch $E^P$ being actively trained to align 3D representations with corresponding image-text pairs. Upon acquiring the feature embeddings for all samples in the batch, alignment between 3D-to-Image and 3D-to-Text is executed following the contrastive learning procedure in CLIP:\n$$\n\\begin{cases}\n    L_{P2T} = -\\sum_{i=1}^N \\log(\\frac{\\exp(f_i^P \\cdot f_i^T / \\tau)}{\\sum_{j=1}^N \\exp(f_i^P \\cdot f_j^T/\\tau)})\n    \\\\\n    L_{P2I} = -\\sum_{i=1}^N \\log(\\frac{\\exp(f_i^P \\cdot f_i^I/ \\tau)}{\\sum_{j=1}^N \\exp(f_i^P \\cdot f_j^I/\\tau)})\n\\end{cases}\n$$\nwhere i, j represent the indices of the samples, and $\\tau$ denotes a learnable temperature parameter. The training minimizes the triplet contrastive loss for the weights $\\Theta^P$ in the 3D branch:\n$$\n\\Theta^P = \\argmin_{\\Theta^P} (L_{P2T} + L_{P2I})\n$$"}, {"title": "3.2 Multi-modal Relation Representation (MRD)", "content": "To distill structural knowledge from the multi-modal unified representation space, it is essential to first investigate the form of mutual relation representations between samples within this multi-modal context.\nWith the image encoder $E^I$ and the text encoder $E^T$ fixed during the alignment phase, the learning of the 3D representation $\\{f_i^P\\}_{i=1}^N$ targets identifying the optimal distribution, based on the image features $\\{f_i^I\\}_{i=1}^N$ and text features $\\{f_i^T\\}_{i=1}^N$, extracted from the training set $\\mathcal{X}$. This process strives to ensure the maximal preservation of the inherent priori knowledge within the pre-aligned image-text embedding spaces.\nTherefore, delineating two primary types of relations becomes crucial. The first type is the intra-modality mutual relation, symbolized as $\\mathcal{M}(\\cdot)$, concentrating on the sample distribution within a specific modality $\\mathcal{M}$. This singular relation underscores the representational significance of various samples within the same modality. The second type, i.e. the cross-modal mutual relation, is denoted as $\\mathcal{M}_{1,M_{2}}(\\cdot)$, delving into the binary relation to reveal the semantic links between samples across modalities $\\mathcal{M}_1$ and $\\mathcal{M}_2$.\nIn this study, we explore three widely acknowledged methods for representing mutual relations: Euclidean Distance, Normalized Similarity, and Partial Order.\nEuclidean Distance serves to quantify the relative difference between two samples. For intra-modal mutual relations, this difference is captured by the pairwise distance among all samples within the same modality, denoted as:\n$$\n\\Psi_{\\mathcal{M}}(\\mathcal{X}) = \\{\\|f_i^{\\mathcal{M}} - f_j^{\\mathcal{M}}\\|\\}_{ (f_i^{\\mathcal{M}}, f_j^{\\mathcal{M}})\\in \\mathcal{M}^2}\n$$\nSimilarly, cross-modal relations between samples from different modalities can be expressed as follows, where $\\mu$ denotes the normalization constant.\n$$\n\\Psi_{\\mathcal{M}_1, \\mathcal{M}_2}(\\mathcal{X}) = \\{\\|f_i^{\\mathcal{M}_1} - f_j^{\\mathcal{M}_2}\\|\\}_{ (f_i^{\\mathcal{M}_1}, f_j^{\\mathcal{M}_2})\\in \\mathcal{M}_1\\times \\mathcal{M}_2}\n$$\nNormalized Similarity assesses correlation of features of samples within a single batch, emphasizing the overall distribution of correlation between samples rather than the absolute distance between individual sample pairs, as seen with Euclidean distance modeling. This approach offers a perspective that prioritizes relational dynamics within and across modalities. For intra-modal relations, the normalized similarity can be articulated as follows:\n$$\n\\mathcal{S}_{\\mathcal{M}}(\\mathcal{X}) = \\{ \\frac{\\exp(f_i^{\\mathcal{M}} \\cdot f_j^{\\mathcal{M}}/\\tau)}{\\sum_{k=1}^N \\exp(f_i^{\\mathcal{M}} \\cdot f_k^{\\mathcal{M}}/\\tau)} \\}_{ (f_i^{\\mathcal{M}}, f_j^{\\mathcal{M}})\\in \\mathcal{M}^2}\n$$\nSimilarly, the normalized similarity of inter-modal relations can be described as:\n$$\n\\mathcal{S}_{\\mathcal{M}_1, \\mathcal{M}_2}(\\mathcal{X}) = \\{ \\frac{\\exp(f_i^{\\mathcal{M}_1} \\cdot f_j^{\\mathcal{M}_2}/\\tau)}{\\sum_{k=1}^N \\exp(f_i^{\\mathcal{M}_1} \\cdot f_k^{\\mathcal{M}_2}/\\tau)} \\}_{ (f_i^{\\mathcal{M}_1}, f_j^{\\mathcal{M}_2})\\in \\mathcal{M}_1\\times \\mathcal{M}_2}\n$$\nPartial Order captures the relative ordering between samples, differentiating itself from Euclidean distance and normalized similarity by not imposing strict metric distance constraints. Instead, it is defined through binary relations among samples. For intra-modal relations, partial order can be represented as:\n$$\n\\varphi_{\\mathcal{M}}(\\mathcal{X}) = \\{ r(f_i^{\\mathcal{M}}, f_j^{\\mathcal{M}}) \\}_{(f_i^{\\mathcal{M}}, f_j^{\\mathcal{M}})\\in \\mathcal{M}^2}\n$$\nLikewise, cross-modal relations can be depicted as:\n$$\n\\varphi_{\\mathcal{M}_1, \\mathcal{M}_2}(\\mathcal{X}) = \\{ r(f_i^{\\mathcal{M}_1}, f_j^{\\mathcal{M}_2}) \\}_{(f_i^{\\mathcal{M}_1}, f_j^{\\mathcal{M}_2})\\in \\mathcal{M}_1\\times \\mathcal{M}_2}\n$$\nwhere $r(f_i^{\\mathcal{M}_1}, f_j^{\\mathcal{M}_2})$ indicates the rank of $f_j^{\\mathcal{M}_2}$ after sorting all samples in $\\{f_j^{\\mathcal{M}_2}\\}_{j=1}^N$ by the Euclidean distance."}, {"title": "3.3 Dynamic Relation Distillation", "content": "After establishing the structural relational representations, we proceed to distill intra-modality and cross-modality mutual relations. Specifically, in the context of intra-modality relations, our objective is to align the mutual relations within the 3D modality as closely as possible with those observed in the image and text modalities. Conversely, concerning cross-modal mutual relations, we aim for the 3D-to-Image and 3D-to-Text relations to emulate the correlations observed between Image-to-Text or Text-to-Image.\nTo tackle the issue of inconsistent mutual relations across different modalities, we introduce learnable weights for both the processes of intra-modal and cross-modal mutual relation distillation. These parameters are designed to dynamically adjust the balance between various distillation objectives in the learning phase, thus aiding in more effective network convergence. The losses associated with intra- and cross-modality distillation are articulated as follows:\n$$\n\\begin{aligned}\n\\mathcal{L}_{intra} &= \\alpha \\mathcal{L}(\\psi_P(\\mathcal{X}), \\psi_I(\\mathcal{X})) + (1 - \\alpha) \\mathcal{L}(\\psi_P(\\mathcal{X}), \\psi_T(\\mathcal{X})) \\\\\n\\mathcal{L}_{P2T}^{Cross} &= \\beta \\mathcal{L}(\\phi_{P,T}(\\mathcal{X}), \\phi_{I,T}(\\mathcal{X})) + (1 - \\beta) \\mathcal{L}(\\phi_{P,T}(\\mathcal{X}), \\phi_{T,I}(\\mathcal{X})) \\\\\n\\mathcal{L}_{P2I}^{Cross} &= \\gamma \\mathcal{L}(\\phi_{P,I}(\\mathcal{X}), \\phi_{I,T}(\\mathcal{X})) + (1 - \\gamma) \\mathcal{L}(\\phi_{P,I}(\\mathcal{X}), \\phi_{T,I}(\\mathcal{X}))\n\\end{aligned}\n$$\nSince directly incorporating parameters such as $\\alpha$ and $1 - \\alpha$ can result in unstable optimization due to their non-smoothness, we draw inspiration from the architecture parameter settings in neural architecture search algorithms [25].\nFor each weight, we introduce three pairs of learnable parameters: $[w_{\\alpha}^1, w_{\\alpha}^2]$, $[w_{\\beta}^1, w_{\\beta}^2]$, $[w_{\\gamma}^1, w_{\\gamma}^2]$. During training, the values of $\\alpha$ and $1 - \\alpha$ are derived from softmax of their corresponding learnable weights $[w_{\\alpha}^1, w_{\\alpha}^2]$. Similarly, this is applied to $\\beta$ and $\\gamma$, with these parameters being optimized iteratively. $\\mathcal{L}(.,.)$ denotes a loss function that penalizes the discrepancies between two mutual relations.\nGiven the distinct nature of the approaches used to characterize structural relations in Sec. 3.2, the required form of the loss function varies accordingly. Specifically, to model by calculating Euclidean distances, we employ the Mean Squared Error (MSE) form for distillation. The intra-modal loss can be denoted as\n$$\n\\mathcal{L}(\\Gamma_1, \\Gamma_2) = \\frac{1}{N^2} \\sum_{i,j} ||\\Gamma_1^{i,j} - \\Gamma_2^{i,j}||^2, \\quad i, j \\in (1,N)\n$$\nwhere $\\Gamma_1, \\Gamma_2$ represent two distinct mutual relations, and $\\Gamma_{\\kappa}^{i,j}$ denotes the Euclidean distance between the i-th and j-th samples in the relation modeling of $\\Gamma_{\\kappa}$.\nIn the case of the normalized similarity form, we apply the Jeffrey divergence for imposing constraints.\n$$\n\\mathcal{L}(\\Gamma_1, \\Gamma_2) = \\frac{1}{N} \\sum_{i \\in (1,N)} (KL(\\gamma_i^1 || \\gamma_i^2) + KL(\\gamma_i^2 || \\gamma_i^1))\n$$\nwhere $KL(.)$ represents the Kullback-Leibler Divergence, and $\\gamma_i$ denotes the normalized similarity of the i-th sample with others in the relation modeling of $\\Gamma_{\\kappa}$.\nLastly, for the partial order relation form, we utilize a margin-based ranking loss:\n$$\n\\mathcal{L}(\\Gamma_1, \\Gamma_2) = \\frac{1}{N^2} \\sum_{i,j \\in (1,N)} max(0, -r_{i,j} * (f_{M_1} - f_{M_2}) + \\eta)\n$$\nwhere $r_{ij}$ is the sign function indicating whether the rank order between i and j is consistent in $\\Gamma_1$ and $\\Gamma_2$, and $\\eta$ represents the margin constraint.\nWe summarize the three relational representations and their respective constraints in Fig. 2, and assess their impacts in the subsequent experimental section. Finally, we select the normalized similarity-based relational representation and Jeffrey divergence to distill both intra- and inter-modal relations."}, {"title": "3.4 Framework", "content": "The entire framework of MRD is depicted in Fig. 3. Its pre-training process involves two main components. Initially, it encodes triplet Image-Text-3D data using the pre-trained CLIP model in conjunction with a trainable 3D foundation model. Following this, both intra- and cross-modal relations between samples within each batch across the three modalities are calculated as described in Sec. 3.2. These structural mutual relations from the image and text modalities are dynamically distilled into the 3D modality as explained in Sec. 3.3, thereby facilitating representation learning.\nDuring training, our objective is to minimize both the instance-level contrastive loss and the relational distillation loss across modalities. Thus, the overall loss function can be articulated as:\n$$\n\\mathcal{L} = \\min_{\\mathcal{P}} \\mathcal{L}_{Align} + \\lambda (\\mathcal{L}_{intra} + \\mathcal{L}_{Cross}^{P2T} + \\mathcal{L}_{Cross}^{P2I})\n$$\nwhere $\\lambda$ is a tunable hyper-parameter used to balance the loss terms."}, {"title": "4 Experiments", "content": "Training Settings. In alignment with prior research [26, 50, 59], we compile triplets comprising 3D point clouds, 2D images, and textual descriptions from two extensive datasets of 3D objects. The first integrates the comprehensive 3D collection featured in both OpenShape and Uni3D, encompassing the datasets from Objaverse [8], ShapeNet [4], 3D-FUTURE [10], and ABO [7], culminating in a total of 876K training samples. The second, ShapeNet, contains 52.5K 3D objects across 55 annotated categories. Image data for these datasets are generated through 12 preset camera angles, meticulously arranged to uniformly cover the entire spatial domain, while textual data originate from diverse sources, including curated descriptions, captions crafted by LLMs, and retrieval data. Consistent with preceding methodologies [26], we employ OpenCLIP ViT-BigG-14 as the foundational pre-trained model in this work.\nImplementation Details. PointBERT is taken as our fundamental model to pre-train MRD, given its status as a transformer-based backbone that has demonstrated robust performance in previous studies. To assess the impact of scaling up, we incrementally increase its size from 5M (T) to 22M (S), 32M(M), 88M (B) and 307M (L) parameters and evaluate its performance under the same settings, following the way adopted in the counterparts.\nWe utilize a learning rate of 0.001 for all versions of PointBERT. The batch size is configured to 192 when the model is trained on ShapeNet and increased to 512 on other datasets. Specifically, for ShapeNet, the training duration is set to 70 epochs, while for Objaverse, we extend training to 300 epochs to ensure adequate convergence. We use a cosine annealing schedule with a 15-epoch warm-up period. The weight decay is set at 0.05, $\\lambda$ is set to 3, and the random seed is fixed at 0. The experiments are conducted on eight A800 GPUs, and the whole training process takes about 16 GPU-days."}, {"title": "4.1 Setup", "content": "Training Settings. In alignment with prior research [26, 50, 59], we compile triplets comprising 3D point clouds, 2D images, and textual descriptions from two extensive datasets of 3D objects. The first integrates the comprehensive 3D collection featured in both OpenShape and Uni3D, encompassing the datasets from Objaverse [8], ShapeNet [4], 3D-FUTURE [10], and ABO [7], culminating in a total of 876K training samples. The second, ShapeNet, contains 52.5K 3D objects across 55 annotated categories. Image data for these datasets are generated through 12 preset camera angles, meticulously arranged to uniformly cover the entire spatial domain, while textual data originate from diverse sources, including curated descriptions, captions crafted by LLMs, and retrieval data. Consistent with preceding methodologies [26], we employ OpenCLIP ViT-BigG-14 as the foundational pre-trained model in this work.\nImplementation Details. PointBERT is taken as our fundamental model to pre-train MRD, given its status as a transformer-based backbone that has demonstrated robust performance in previous studies. To assess the impact of scaling up, we incrementally increase its size from 5M (T) to 22M (S), 32M(M), 88M (B) and 307M (L) parameters and evaluate its performance under the same settings, following the way adopted in the counterparts.\nWe utilize a learning rate of 0.001 for all versions of PointBERT. The batch size is configured to 192 when the model is trained on ShapeNet and increased to 512 on other datasets. Specifically, for ShapeNet, the training duration is set to 70 epochs, while for Objaverse, we extend training to 300 epochs to ensure adequate convergence. We use a cosine annealing schedule with a 15-epoch warm-up period. The weight decay is set at 0.05, $\\lambda$ is set to 3, and the random seed is fixed at 0. The experiments are conducted on eight A800 GPUs, and the whole training process takes about 16 GPU-days."}, {"title": "4.2 Zero-Shot Shape Classification", "content": "For the zero-shot classification task, our evaluation considers three standard datasets: ModelNet40 [47], ScanObjectNN [41], and Objaverse [8]. ModelNet40 and ScanObjectNN feature 2,468 and 2,890 test samples across 40 and 15 target classes, respectively, while Objaverse includes 46,205 test samples spanning 1,156 target classes. We calculate and compare the top-1, top-3, and top-5 accuracies with those achieved by other 3D zero-shot classification approaches. For evaluation on ModelNet and Objaverse, the input consists of sampled coordinates and color information from 10,000 points on the mesh surface of each sample. For the ScanObjectNN dataset, the input is composed of 2,048 point-cloud data points derived from the OBJ_ONLY version.\nWe compare the performance of the proposed MRD framework with the representative state-of-the-art methods such as ULIP-2, OpenShape, and Uni3D. The results are presented in Tab. 1. From Tab. 1 we observe that MRD achieves highly competitive results in all the settings. Notably, despite that OpenShape, Uni3D, and MRD utilize exactly the same training data, MRD significantly surpasses both OpenShape and Uni3D when operating with backbones of comparable parameter scales. It is worth noting when scaled to 88M, MRD delivers the top-1 accuracies of 53.2% on Objaverse and 88.8% on ModelNet40, out-performing Uni3D-L, which employs a considerably larger backbone of 307M,"}, {"title": "4.3 Cross-Modality Retrieval", "content": "Considering that previous research assess cross-modal retrieval capabilities primarily relying on qualitative sample visualizations, lacking quantitative comparisons, we propose a new quantitative evaluation protocol to fill this gap for validating the effectiveness in text-to-3D retrieval. Utilizing the recently proposed Cap3D [28] method, which is capable of generating detailed textual descriptions for 3D data, we produce precise descriptions for the samples in the Objaverse test set. These descriptions then serve as the foundation for retrieving the most relevant samples within the Objaverse test set. Subsequently, we compute both the retrieval accuracy for individual instances and that for corresponding categories, providing a robust metric to assess the external-text retrieval capabilities of multi-modal 3D representation learning frameworks. We compare our MRD with recent state-of-the-art methods, including OpenShape and Uni3D, with the results presented in Tab. 2.\nFrom Tab. 2, we can find that MRD consistently outperforms its counterparts across both metrics, delivering the best performance. Benefiting from its structural distillation capability, MRD effectively learns the relations within the textual feature space. This, in turn, enhances its ability to accurately retrieve corresponding instances when processing external text descriptions, thereby demonstrating its effectiveness."}, {"title": "4.4 Ablation Study", "content": "Candidate Relation Representation. We evaluate the effects of different candidate relation representations and their associated distillation losses, as outlined in Sec. 3.2, on various test sets, with the outcomes presented in Tab. 3. From Tab. 3, we can find that incorporating the relational distillation mechanism leads to improvements in accuracy for most settings. Notably, the most significant enhancement is achieved when modeling intra- and inter-modal relations using normalized similarities. This superiority may stem from the ability of normalized similarity-based relation representations, which strikes a more effective balance between the flexibility of the sample distribution and the consistency of relational constraints, in contrast to the stricter Euclidean distance and the more lenient partial order relation, thereby enhancing the performance.\nRelation Distillation Strategies. Leveraging the normalized similarity and its corresponding distillation loss, we conduct ablation experiments on dynamic distillation, with the results presented in Tab. 4. As observed from Tab. 4, simultaneously introducing both intra-modal and cross-modal relation distillation leads to the performance that is less favorable compared to independently implementing either of the intra-modal or cross-modal constraints, due to the inconsistency arising from the complex spectrum of mutual relations. Notably, upon introducing the Dynamic Distillation (DD) mechanism, there is a significant gain in performance, demonstrating the efficacy of this strategy.\nTo quantify the improvements provided by relational distillation more precisely, we compute the Mean Absolute Error (MAE) between the similarity matrices across both intra- and cross-modal samples on ShapeNet to examine the effects of incorporating various relations. In Tab. 5, it is evident that IR leads to a decrease in MAE between intra-modal similarities, while CR slightly reduces the divergence of cross-modal similarities. When DD is introduced, MRD achieves a better balance between multiple intra- and cross-modality relations.\nVisualization of Dynamic Weights. We visualize the value changes of the dynamic weights of MRD-M when trained on ShapeNet and Objaverse in Fig. 4. As shown in Fig. 4, the intra-modal relations tend to mimic the image-image relations, as they share common information such as textures and geometries between the 3D and image modalities. In contrast, the cross-modal relations lean towards synthesizing the text-image relations, likely due to the inherent differences between textual descriptions and visual appearnaces. Meanwhile, because of the difference of data distributions of ShapeNet and Objaverse, the dynamic weights converge to distinct values."}, {"title": "5 Conclusion and Discussion", "content": "We introduce MRD, an innovative approach to acquiring a unified multi-modal representation of 3D shapes. By distilling both intra-modal and cross-modal relations from the pre-aligned image and text modalities into the 3D modality, MRD facilitates the achievement of more discriminative representations. Our findings demonstrate notable improvements in the zero-shot shape classification and cross-modal retrieval tasks over previous research, underscoring the superior capability of MRD in 3D shape understanding.\nLooking ahead, several avenues merit further investigation: a) developing advanced methods for relationship representation and distillation for improved 3D and multi-modal features; b) enhancing the granularity of relationship characterization to extract richer semantic insights for more robust 3D representations; and c) improving conflict removal mechanisms to integrate diverse relational representations for deeper insight into representational relations across modalities."}, {"title": "A More Ablation Studies", "content": "Hyper-parameter. We perform an ablation study to examine the impact of the tunable hyper-parameter $\\lambda$ in Eq. (13). The results are depicted in Tab. 7.\nFrom Tab. 7, we find that the optimal performance across all three benchmarks is achieved when $\\lambda$ is set to 3. Consequently, we adopt $\\lambda$ = 3 for all the experiments within this study.\nDownstream Finetuning. We follow the same fine-tuning protocols as ULIP on standard 3D classification and present the results in Tab. 8. MRD largely surpasses the baseline and other counterparts in both the settings."}, {"title": "B Details in Cross-Modal Retrieval", "content": "For quantitative assessment in Sec. 4.3, we use the Objaverse test set, denoted as $\\{(p_i,l_i)\\}_{i=1}^N$ as our evaluation set. The encoded 3D point clouds are employed as cross-modal queries to retrieve the corresponding detailed text descriptions $\\{t_i\\}_{i=1}^N$ generated by Cap3D. A retrieval is deemed a success at the instance level if the retrieved item is the i-th item itself. Additionally, a more lenient criterion is applied to category-wise retrieval: if the retrieved item belongs to the same category $l_i$ as the target, it is also considered a successful case.\nFor fair comparison, we utilize the released OpenShape-PointBert and Uni3D-B models as well as MRD-B, which are all on a comparable scale in terms of parameters. The retrieval accuracy is reported in Tab.2 in the main text. Additionally, more qualitative comparison is visualized in Fig. 7, wherer we can find that MRD achieves the superior performance compared to the counterparts.\nWe further visualize additional image-query 3D shape results generated by MRD in Fig. 8."}]}