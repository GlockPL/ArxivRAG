{"title": "Multi-modal Relation Distillation for Unified 3D Representation Learning", "authors": ["Huiqun Wang", "Yiping Bao", "Panwang Pan", "Zeming Li", "Xiao Liu", "Ruijie Yang", "Di Huang"], "abstract": "Recent advancements in multi-modal pre-training for 3D point clouds have demonstrated promising results by aligning heterogeneous features across 3D shapes and their corresponding 2D images and language descriptions. However, current straightforward solutions often overlook intricate structural relations among samples, potentially limiting the full capabilities of multi-modal learning. To address this issue, we introduce Multi-modal Relation Distillation (MRD), a tri-modal pre-training framework, which is designed to effectively distill reputable large Vision-Language Models (VLM) into 3D backbones. MRD aims to capture both intra-relations within each modality as well as cross-relations between different modalities and produce more discriminative 3D shape representations. Notably, MRD achieves significant improvements in downstream zero-shot classification tasks and cross-modality retrieval tasks, delivering new state-of-the-art performance.", "sections": [{"title": "Introduction", "content": "Recently, 3D shape understanding has garnered increasing attention due to its wide range of applications, such as space calculation [2, 27, 30, 42], autonomous driving [23, 53] and robotic perception [3,46]. Despite notable advances in 3D visual analysis, the limited availability of 3D data, characterized by constraints in scale and scarcity of annotations, remains a significant barrier.\nTo tackle this challenge, many researchers have delved into integrating auxiliary modalities within the self-supervised learning framework. Some attempts utilize priors from the image modality to craft more instructive pretext tasks [9,58], thereby enhancing the discriminative power of the learned 3D representations. Meanwhile, others focus on distilling knowledge from pre-trained models in either image or text modalities to facilitate conceptual understanding within the 3D modality [57,60].\nAmong these endeavors, tri-modal-based methodologies [16,26,49,50,59] have shown exceptional prowess by aligning point cloud representations with the pre-aligned image-text feature space. They employ a synthesized set of triplet data\nto draw point cloud representations closer to their respective image-text pairs, simultaneously distancing them from non-associated examples. The strategic use of the impressive discriminative capabilities of CLIP [38] markedly boosts the zero-shot performance and effectively creates a unified representation spanning different modalities, which highlights the potential of harnessing complex, multi-modal interactions to improve understanding of 3D shapes.\nAs illustrated in Fig. 1, these methodologies concentrate exclusively on aligning point cloud representations with corresponding features in image and text modalities at the individual instance level, thereby neglecting the intricate structural relations within the pre-aligned image-text feature space. Unfortunately, these mutual relations between data samples are crucial for developing a discriminative representation [32]. For instance, intra-modal relations emphasize similarities such as shapes or textures in images, or connections among scene compositions in texts. Meanwhile, cross-modal relations reveal semantic relevance across modalities, underscoring the connections between diverse entities. The failure to consider these multifaceted relations results in the underutilization of the extensive priors learned by CLIP, culminating in a fragmented comprehension of multi-modal representations.\nIn this study, we aim to transfer mutual relational knowledge from the pre-aligned image-text domain into the 3D modality, thus improving the effectiveness of current multi-modal contrastive 3D pre-training approaches. Achieving this involves addressing two pivotal challenges. On the one hand, the method, which accurately models mutual relations across various modalities, has not yet been sufficiently explored. Unlike the dual-distribution scenarios of prior studies [12, 13,32], adding the 3D modality to the pre-aligned image-text framework incurs a complex array of mutual relations that demands substantial analysis and elaborate modeling. On the other hand, inherent semantic variances within each modality create a pronounced gap between image and text modalities [24, 45], potentially leading to conflicts when aligning 3D representations to pre-trained image or text features. Identifying an effective strategy to mitigate such conflicts for improved convergence is an area that warrants further efforts.\nTo address the issues aforementioned, we commence with an in-depth analysis of existing representations and associated constraints of mutual relations, examining both within individual modalities (intra-modal) and across different modalities (cross-modal). Following this, we develop a data-driven mechanism to dynamically reconcile these conflicting intra-modal and cross-modal discrepancies. Integrating both parts, we propose a Multi-modal Relation Distillation pre-training framework, namely MRD, for 3D representation learning. Since MRD adeptly transfers structural relational knowledge from the image-text modality to the 3D one, it successfully enhances 3D representations by effectively merging information from diverse modalities, consequently leading to improved performance in downstream tasks.\nTo sum up, our contributions can be summarized as follows:\nWe introduce MRD, an innovative self-supervised learning framework, which effectively distills both intra-modal and cross-modal relations, significantly improving the discriminative capability of 3D point cloud representations.\nWe perform comprehensive analysis and comparison of mutual relation representations within multi-modal 3D representation learning, and propose a data-driven strategy to resolve the discrepancies in the pre-aligned image-text space of CLIP.\nWe present state-of-the-art pre-trained models across diverse applications and deliver superior performance on ModelNet40 [47], ScanObjectNN [41] and Objaverse [8]."}, {"title": "Related Work", "content": "Self-Supervised Learning (SSL) for point cloud understanding initiates with the 3D modality. The researchers diligently craft a range of pretext tasks such as geometric reconstruction [22, 39, 40, 43], mask auto-modeling [6,31,54,56], normal estimation [33], and contrastive learning [18,48,51] to enhance shape representations of deep learning backbones of 3D point clouds [34, 36, 44, 54]. Contemporary endeavors seek to augment 3D SSL by importing insights from other modalities. Attempts leveraging image-guided strategies [58], multi-modal reconstruction [9, 14,35], and weight-sharing [5,37] are integrated into conventional frameworks such as Masked Autoencoder [15] or contrastive learning, yielding significant improvements over single-modality methods. Despite the success in enhancing the performance of downstream applications, these methods are limited by their inability to forge more comprehensive cross-modal correlations.\nLeveraging the groundbreaking achievements in learning visual concepts directly from textual descriptions through contrastive learning methods, some efforts are made to capitalize on the impressive zero-shot classification capability of CLIP [38] to facilitate the comprehension of 3D shapes. PointCLIP [57] converts 3D point clouds into multi-view images for zero-shot classification by using the pre-trained visual-text encoders of CLIP and PointCLIP-2 [60] advances the methodology by refining the image projection strategies and employing Large Language Models (LLMs) to optimize prompt design, reporting improved performance. However, these techniques suffer the challenges related to information loss during 3D to 2D projection and increased computational demands due to the use of images as an intermediary, which restrict their pervasion.\nSimultaneously, some alternatives directly integrate multi-modal representation learning into the 3D domain. CG3D [16] generates ternary pairs of image-text-points from ShapeNet and aligns point cloud features with the text and image features of CLIP, aiming to distill image and text modalities into 3D representations. ULIP [49] enhances this approach by incorporating more intricate multi-view renderings and varied text descriptions. Subsequent developments like ULIP-2 [50] and OpenShape [26] expand the ternary dataset for training, leading to superior outcomes. Uni3D [59] scales up the approach by adapting a Vision Transformer (ViT) pre-trained on image datasets to 3D models, reaching a milestone of 1 billion parameters. While these advances markedly improve the zero-shot classification performance, they primarily align features at the instance level across modalities and neglect the potential of exploring inter-sample relations.\nPrevious research demonstrates that a thorough understanding of the complex relations among samples significantly improves the acquisition of structural intricacies within representations [32]. Recently, this detailed relation modeling has been integrated into the realm of contrastive learning. RINCE [17] enhances contrastive learning by establishing a partial order among samples, thus contributing to more precise representations of 2D images. Similarly, Soft-InfoNCE [21] refines the margins of the sample distribution based on sample similarities, leading to a notably more distinct feature space.\nIn the context of bi-modal contrastive learning, CyCLIP [13] introduces the constraints based on pairwise Euclidean distances, applicable both within single modality and across different modalities. Meanwhile, CLIP-PSD [1] adopts similarity-based connections to mitigate the requirement of rigid one-to-one correspondence during training. Some alternative methodologies prioritize modeling complex relations through low-level similarities [11, 12], coherence of local patches [52], and diversity augmentation [55]. Similar enhancements also appear in the scenarios like downstream distillation [20] and CAD representation learning [29]. Both the strategies outperform the standard baseline, indicating their effectiveness.\nNevertheless, relational distillation has not yet been well-explored in the field of multi-modal unified representation learning for 3D point clouds. The integration of the 3D modality with the pre-aligned image-text modality incurs two significant challenges. Firstly, the expansion in the number of modalities results in an increase of inter-sample relations that need to be articulated. The methodology for modeling these intra-modality and cross-modality sample relations as well as establishing the corresponding constraints remains unclear. Secondly, the inconsistency in feature distribution of the image-text modality [24,45] leads to conflicting relations among these distributions, making it a dilemma to leverage them for proficient representation learning in the 3D modality."}, {"title": "Method", "content": "Previous studies align 3D shape representations with the pre-trained CLIP embedding space of images and texts by applying the contrastive learning loss. Given a set of synthesized image-text-3D triplet \\(X = \\{(x_i^I,x_i^T,x_i^P)\\}_{i=1}^N\\) as training samples, dedicated encoders \\(E^I, E^T, E^P\\) for each modality process inputs in their respective domains to produce the corresponding feature embeddings \\(f_i^I, f_i^T, f_i^P\\). Notably, \\(E^I\\) and \\(E^T\\), derived from the extensive pre-training of CLIP on a large-scale image-text dataset, cover a data volume substantially greater than the number of available synthesized triplets. To circumvent potential model collapse, the weights for the image branch \\(E^I\\) and the text branch \\(E^T\\) are kept frozen, with only the point cloud branch \\(E^P\\) being actively trained to align 3D representations with corresponding image-text pairs. Upon acquiring the feature embeddings for all samples in the batch, alignment between 3D-to-Image and 3D-to-Text is executed following the contrastive learning procedure in CLIP:\n\n\\begin{equation}\n\\begin{cases}\nL_{P2T} = -\\sum_i \\frac{exp(f_i^P \\cdot f_i^T/\\tau)}{\\sum_j exp(f_i^P \\cdot f_j^T/\\tau)}\\\\\nL_{P2I} = -\\sum_i \\frac{exp(f_i^P \\cdot f_i^I/\\tau)}{\\sum_j exp(f_i^P \\cdot f_j^I/\\tau)} \\\\\n\\end{cases}\n\\end{equation}\n\nwhere \\(i, j\\) represent the indices of the samples, and \\(\\tau\\) denotes a learnable temperature parameter. The training minimizes the triplet contrastive loss for the weights \\(\\Theta^P\\) in the 3D branch:\n\n\\begin{equation}\n\\Theta^P = \\underset{\\Theta^P}{\\text{min}} (L_{P2T} + L_{P2I})\n\\end{equation}\n\nTo distill structural knowledge from the multi-modal unified representation space, it is essential to first investigate the form of mutual relation representations between samples within this multi-modal context.\nWith the image encoder \\(E^I\\) and the text encoder \\(E^T\\) fixed during the alignment phase, the learning of the 3D representation \\(\\{f_i^P\\}_{i=1}^N\\) targets identifying the optimal distribution, based on the image features \\(\\{f_i^I\\}_{i=1}^N\\) and text features \\(\\{f_i^T\\}_{i=1}^N\\), extracted from the training set \\(X\\). This process strives to ensure the maximal preservation of the inherent priori knowledge within the pre-aligned image-text embedding spaces.\nTherefore, delineating two primary types of relations becomes crucial. The first type is the intra-modality mutual relation, symbolized as \\(\\mathcal{M}(\\cdot)\\), concentrating on the sample distribution within a specific modality \\(M\\). This singular relation underscores the representational significance of various samples within the same modality. The second type, i.e. the cross-modal mutual relation, is denoted as \\(\\phi^{(M_1,M_2)}(\\cdot)\\), delving into the binary relation to reveal the semantic links between samples across modalities \\(M_1\\) and \\(M_2\\).\nIn this study, we explore three widely acknowledged methods for representing mutual relations: Euclidean Distance, Normalized Similarity, and Partial Order.\nEuclidean Distance serves to quantify the relative difference between two samples. For intra-modal mutual relations, this difference is captured by the pairwise distance among all samples within the same modality, denoted as:\n\n\\begin{equation}\n\\Psi^M(X) = \\{\\frac{1}{N^2}\\sum_{i,j}{\\parallel f_i^M - f_j^M \\parallel^2\\}}(f_i^M, f_j^M) \\in M^2\n\\end{equation}\n\nSimilarly, cross-modal relations between samples from different modalities can be expressed as follows, where \\(\\mu\\) denotes the normalization constant.\n\n\\begin{equation}\n\\phi^{M_1,M_2}(X) = {\\frac{1}{\\mu^2}\\parallel f_i^{M_1} - f_j^{M_2}\\parallel^2\\}}(f_i^{M_1}, f_j^{M_2}) \\in M_1XM_2\n\\end{equation}\n\nNormalized Similarity assesses correlation of features of samples within a single batch, emphasizing the overall distribution of correlation between samples rather than the absolute distance between individual sample pairs, as seen with Euclidean distance modeling. This approach offers a perspective that prioritizes relational dynamics within and across modalities. For intra-modal relations, the normalized similarity can be articulated as follows:\n\n\\begin{equation}\n\\mathcal{S}^M(X) = {\\frac{exp(f_i^M \\cdot f_j^M /\\tau)}{\\sum_{k=1}^N exp(f_i^M \\cdot f_k^M /\\tau)}\\}}(f_i^M, f_j^M) \\in M^2\n\\end{equation}\n\nSimilarly, the normalized similarity of inter-modal relations can be described as:\n\n\\begin{equation}\n\\mathcal{S}^{M_1,M_2}(X) = {\\frac{exp(f_i^{M_1} \\cdot f_j^{M_2} /\\tau)}{\\sum_{k=1}^N exp(f_i^{M_1} \\cdot f_k^{M_2} /\\tau)}\\}}(f_i^{M_1}, f_j^{M_2}) \\in M_1XM_2\n\\end{equation}\n\nPartial Order captures the relative ordering between samples, differentiating itself from Euclidean distance and normalized similarity by not imposing strict metric distance constraints. Instead, it is defined through binary relations among samples. For intra-modal relations, partial order can be represented as:\n\n\\begin{equation}\n\\mathcal{PO}^M(X) = {r(f_i^M, f_j^M)}(f_i^M, f_j^M) \\in M^2\n\\end{equation}\n\nLikewise, cross-modal relations can be depicted as:\n\n\\begin{equation}\n\\mathcal{PO}^{M_1,M_2}(X) = {r(f_i^{M_1}, f_j^{M_2})}(f_i^{M_1}, f_j^{M_2}) \\in M_1XM_2\n\\end{equation}\n\nwhere \\(r(f_i^{M_1}, f_j^{M_2})\\) indicates the rank of \\(f_j^{M_2} \\in \\{f_j^{M_2}\\}_{i=1}^N\\) after sorting all samples in \\(\\{f_i^{M_1}\\}_{i=1}^N\\) by the Euclidean distance.\nAfter establishing the structural relational representations, we proceed to distill intra-modality and cross-modality mutual relations. Specifically, in the context of intra-modality relations, our objective is to align the mutual relations within the 3D modality as closely as possible with those observed in the image and text modalities. Conversely, concerning cross-modal mutual relations, we aim for the 3D-to-Image and 3D-to-Text relations to emulate the correlations observed between Image-to-Text or Text-to-Image.\nTo tackle the issue of inconsistent mutual relations across different modalities, we introduce learnable weights for both the processes of intra-modal and cross-modal mutual relation distillation. These parameters are designed to dynamically adjust the balance between various distillation objectives in the learning phase, thus aiding in more effective network convergence. The losses associated with intra- and cross-modality distillation are articulated as follows:\n\n\\begin{equation}\n\\begin{aligned}\n\\mathcal{L}_{intra} &= \\alpha \\mathcal{L}(\\Psi^P(X), \\Psi^I(X)) + (1 - \\alpha) \\mathcal{L}(\\Psi^P(X), \\Psi^T(X)) \\\\\n\\mathcal{L}_{cross}^{P2T} &= \\beta \\mathcal{L}(\\phi^{P,T}(X), \\phi^{I,T}(X)) + (1 - \\beta) \\mathcal{L}(\\phi^{P,T}(X), \\phi^{T,I}(X))\\\\\n\\mathcal{L}_{cross}^{P2I} &= \\gamma \\mathcal{L}(\\phi^{P,I}(X), \\phi^{I,T}(X)) + (1 - \\gamma) \\mathcal{L}(\\phi^{P,I}(X), \\phi^{T,I}(X))\\\\\n\\end{aligned}\n\\end{equation}\n\nSince directly incorporating parameters such as \\(\\alpha\\) and \\(1 - \\alpha\\) can result in unstable optimization due to their non-smoothness, we draw inspiration from the architecture parameter settings in neural architecture search algorithms [25].\nFor each weight, we introduce three pairs of learnable parameters: \\([w_{\\alpha}, w_{\\alpha}^1; w_{\\beta}, w_{\\beta}^1; w_{\\gamma}, w_{\\gamma}^1]\\). During training, the values of \\(\\alpha\\) and \\(1 - \\alpha\\) are derived from softmax of their corresponding learnable weights \\([w_{\\alpha}, w_{\\alpha}^1]\\). Similarly, this is applied to \\(\\beta\\) and \\(\\gamma\\), with these parameters being optimized iteratively. \\(\\mathcal{L}(\\cdot,\\cdot)\\) denotes a loss function that penalizes the discrepancies between two mutual relations.\nGiven the distinct nature of the approaches used to characterize structural relations in Sec. 3.2, the required form of the loss function varies accordingly. Specifically, to model by calculating Euclidean distances, we employ the Mean Squared Error (MSE) form for distillation. The intra-modal loss can be denoted as\n\n\\begin{equation}\n\\mathcal{L}(\\Gamma_1, \\Gamma_2) = \\frac{1}{N^2} \\sum_{i,j}{\\parallel \\Gamma_{1_{i,j}} - \\Gamma_{2_{i,j}} \\parallel^2}\n\\end{equation}\n\nwhere \\(\\Gamma_1, \\Gamma_2\\) represent two distinct mutual relations, and \\(\\Gamma_{\\kappa_{i,j}}\\) denotes the Euclidean distance between the i-th and j-th samples in the relation modeling of \\(\\Gamma_{\\kappa}\\).\nIn the case of the normalized similarity form, we apply the Jeffrey divergence for imposing constraints.\n\n\\begin{equation}\n\\mathcal{L}(\\Gamma_1, \\Gamma_2) = \\sum_{i}{\\frac{1}{N} (KL(y_i^1||y_i^2) + KL(y_i^2||y_i^1))}\n\\end{equation}\n\nwhere \\(KL(\\cdot)\\) represents the Kullback-Leibler Divergence, and \\(y_i\\) denotes the normalized similarity of the i-th sample with others in the relation modeling of \\(\\Gamma_{\\kappa}\\).\nLastly, for the partial order relation form, we utilize a margin-based ranking loss:\n\n\\begin{equation}\n\\mathcal{L}(\\Gamma_1, \\Gamma_2) = \\frac{1}{N^2} \\sum_{i,j}{max(0, -r_{i,j} * (f^1_{i,j} - f^2_{i,j}) + \\eta)}\n\\end{equation}\n\nwhere \\(r_{i,j}\\) is the sign function indicating whether the rank order between i and j is consistent in \\(\\Gamma_1\\) and \\(\\Gamma_2\\), and \\(\\eta\\) represents the margin constraint.\nWe summarize the three relational representations and their respective constraints in Fig. 2, and assess their impacts in the subsequent experimental section. Finally, we select the normalized similarity-based relational representation and Jeffrey divergence to distill both intra- and inter-modal relations."}, {"title": "Framework", "content": "The entire framework of MRD is depicted in Fig. 3. Its pre-training process involves two main components. Initially, it encodes triplet Image-Text-3D data using the pre-trained CLIP model in conjunction with a trainable 3D foundation model. Following this, both intra- and cross-modal relations between samples within each batch across the three modalities are calculated as described in Sec. 3.2. These structural mutual relations from the image and text modalities are dynamically distilled into the 3D modality as explained in Sec. 3.3, thereby facilitating representation learning.\nDuring training, our objective is to minimize both the instance-level contrastive loss and the relational distillation loss across modalities. Thus, the overall loss function can be articulated as:\n\n\\begin{equation}\n\\mathcal{L} = \\underset{\\Theta^P}{\\text{min}} \\mathcal{L}_{Align} + \\lambda (\\mathcal{L}_{intra} + \\mathcal{L}_{cross}^{P2T} + \\mathcal{L}_{cross}^{P2I})\n\\end{equation}\n\nwhere \\(\\lambda\\) is a tunable hyper-parameter used to balance the loss terms."}, {"title": "Experiments", "content": "Training Settings. In alignment with prior research [26, 50, 59], we compile triplets comprising 3D point clouds, 2D images, and textual descriptions from"}, {"title": "Conclusion and Discussion", "content": "We introduce MRD, an innovative approach to acquiring a unified multi-modal representation of 3D shapes. By distilling both intra-modal and cross-modal relations from the pre-aligned image and text modalities into the 3D modality, MRD facilitates the achievement of more discriminative representations. Our findings demonstrate notable improvements in the zero-shot shape classification and cross-modal retrieval tasks over previous research, underscoring the superior capability of MRD in 3D shape understanding.\nLooking ahead, several avenues merit further investigation: a) developing advanced methods for relationship representation and distillation for improved 3D and multi-modal features; b) enhancing the granularity of relationship characterization to extract richer semantic insights for more robust 3D representations; and c) improving conflict removal mechanisms to integrate diverse relational representations for deeper insight into representational relations across modalities."}]}