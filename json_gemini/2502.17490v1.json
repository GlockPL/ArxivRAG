{"title": "A generalized dual potential for inelastic Constitutive Artificial Neural Networks: A JAX implementation at finite strains", "authors": ["Hagen Holthusen", "Kevin Linka", "Ellen Kuhl", "Tim Brepols"], "abstract": "We present a methodology for designing a generalized dual potential, or pseudo potential, for inelastic Constitutive Artificial Neural Networks (iCANNs). This potential, expressed in terms of stress invariants, inherently satisfies thermodynamic consistency for large deformations. In comparison to our previous work, the new potential captures a broader spectrum of material behaviors, including pressure-sensitive inelasticity.\nTo this end, we revisit the underlying thermodynamic framework of iCANNs for finite strain inelasticity and derive conditions for constructing a convex, zero-valued, and non-negative dual potential. To embed these principles in a neural network, we detail the architecture's design, ensuring a priori compliance with thermodynamics.\nTo evaluate the proposed architecture, we study its performance and limitations discovering visco-elastic material behavior, though the method is not limited to visco-elasticity. In this context, we investigate different aspects in the strategy of discovering inelastic materials. Our results indicate that the novel architecture robustly discovers interpretable models and parameters, while autonomously revealing the degree of inelasticity.\nThe iCANN framework, implemented in JAX, is publicly accessible at https://doi.org/10. 5281/zenodo.14894687.", "sections": [{"title": "1. Introduction", "content": "Constitutive material modeling plays a crucial role in understanding the behavior of history-dependent materials, as it allows for accurate predictions of their response under various loading conditions. The selection of an appropriate material model requires extensive expertise, particularly when addressing both elastic and inelastic behaviors which are often strongly coupled. A flawed choice can lead to a focus on optimizing material parameters rather than identifying the most suitable model for the material's behavior.\nIt is important to recognize that the expertise of professionals is often related to their practical experience, which can lead to biases in the modeling process. This subjectivity has the potential to influence the selection of models and parameters, thereby constraining the exploration of alternative approaches.\nIn this context, neural networks which are grounded in physical principles \u2013 whether strongly or weakly \u2013 can provide significant assistance, besides model-free approaches [1, 2]. Such neural networks are particularly valuable in the field of finite deformations, where complexity is a major challenge. By using advanced computational methods, we can improve our ability to capture the complex responses of materials under different conditions, ultimately leading to more reliable and effective engineering solutions and paving the way for novel advanced materials."}, {"title": "1.1. State-of-the-art", "content": "Neural networks for inelastic materials. In recent years, the use of neural networks has surged across various scientific fields, particularly for analyzing large data sets beyond human analytical capabilities. For instance, Kepner et al. [3] demonstrate the societal impact of the internet using hypersparse neural networks on a data set of 50 billion packets.\nIn medical science, neural networks significantly enhance the understanding of complex brain connectivity essential for identifying neurodegenerative diseases, as shown in [4] with data from 2,702 subjects. Additionally, Girardi et al. [5] utilize an attention-based neural network trained on 600,000 medical notes to detect critical warning symptoms. Moreover, Ferle et al. [6] combine Long Short-Term Memory networks with Conditional Restricted Boltzmann Machines to predict multiple myeloma events up to twelve months in advance, improving patient care. In the field of mechanics, Nagle et al. [7] trained a neural network on 1,000 individual virtual subjects to predict skin growth.\nIn continuum mechanics, access to extensive experimental data is often limited, prompting the question of how neural networks can be beneficial in this field. Expert-based neural network architectures offer distinct advantages, particularly when modeling the diverse behaviors of inelastic materials. The phenomena of interest, such as visco-elasticity and plasticity, exhibit variability based on factors like pressure-sensitivity.\nTypically, experts evaluate experimental data to determine both the inelastic phenomenon and the corresponding constitutive model equations. These decisions are often influenced by prior experience, introducing bias. Given the complexity of constitutive equations, it is challenging for humans to formulate highly nonlinear relationships accurately.\nNeural networks address this limitation by accommodating a wide range of material behaviors and approximating complex nonlinear functions with high accuracy through scaling their depth and width. Recent research has focused on integrating neural networks with machine learning to enhance modeling capabilities in continuum mechanics.\nOne of the pioneering thermodynamics-based networks was introduced in [8], focusing specifically on inelastic multiscale modeling [9] and the inelastic evolution process [10]. Further advancements by [11] employed proper orthogonal decomposition to extract macroscopic internal state variables, thereby enriching the data set for training the network. A hierarchical discovery framework that aligns with thermodynamic principles has been proposed in [12].\nAs'ad and Farhat [13] proposed a mechanics-informed neural network [14] aimed at elucidating the behavior of visco-elastic solids. Physics-augmented neural networks [15, 16] exploit fundamental principles from continuum mechanics, such as polyconvexity of the Helmholtz free energy, facilitating discoveries related to visco-elasticity [17] and thermo-elasticity [18]. In this context, [19] proposed a methodology that integrates deformation invariants with the architecture of neural ordinary differential equations.\nTo uncover inelastic materials characterized by an inelastic potential, the idea of input convex neural networks [20] is particularly helpful due to their ability to ensure thermodynamically consistent designs. For example, physics-informed neural networks developed for elasto-viscoplasticity discovery align with this network architecture [21].\nThe unsupervised learning framework EUCLID incorporates Generalized Standard Materials (GSM) \u2013 a method also used by [22] \u2013 into its architecture to autonomously identify plasticity under small strain conditions [23, 24]. This framework has recently been extended to address non-associative pressure-sensitivity under small strains [25]. Additionally, [26] utilized a dissipation potential based on the rate of the right Cauchy-Green tensor to characterize finite visco-elastic behavior within a GSM framework.\nIn scenarios involving damage, a built-in physics neural network was proposed by [27], while fracture problems were addressed through generalizable symbolic regression techniques in [28]. Moreover, a deep neural network capable of automatically locating and inserting regularized discontinuities for modeling brittle fractures was introduced in [29].\nWe adopt the approach established by Constitutive Artificial Neural Networks (CANNs) [30, 31], which were extended to visco-elasticity based on a Prony series in [32]. Recently, the idea of CANNs served as foundation for constitutive Kolmogorov Arnold networks [33]. Its generalization to inelastic behavior (iCANN) under finite strains was proposed in [34, 35] and investigated in studies on visco-elasticity and elasto-plasticity with kinematic hardening [36], as well as applications to biological tissue growth [37]. Although this custom-designed architecture inherently satisfies thermodynamic consistency for inelastic materials, it has certain limitations regarding scalability concerning depth and width and fails to incorporate pressure sensitivity effectively.\nTherefore, in this study, we explore methodologies for transitioning traditional feed-forward architectures onto the iCANN framework. Our novel architecture has similarities to recent contributions from Jadoon et al. [38], who pertain to finite elasto-plasticity and share conceptual foundations with iCANNs. However, unlike Jadoon et al. [38], we fully custom-design our feed-forward network based on comprehensive discussions aimed at achieving a thermodynamically consistent yet generalized potential."}, {"title": "1.2. Hypothesis", "content": "This overview of neural networks applied within the realm of thermodynamically consistent discovery of inelastic materials is not exhaustive; indeed, data-driven mechanics is an expanding field. Therefore, interested readers are encouraged to consult recent survey articles such as [39, 40, 41], which discuss methodologies for integrating physics into neural network architectures.\nDiscovery of inelastic neural networks. As mentioned by [42], different constitutive models can describe specific mechanical behaviors depending on parameter variations. The authors proposed a Bayesian-based procedure that combines model selection with parameter identification. Similarly, neural networks in continuum mechanics face challenges; as their architectures become more generalized, encompassing various inelastic phenomena, the complexity of the discovery process increases.\nWhile highly generalizable, dense networks offer numerous advantages, they pose challenges in uniquely identifying the network's weights. In this context, obtaining a sparse network during the discovery process is preferred over a dense one, as it can be considered \u2018unique' to a certain extent. Regularizing the network's weights in thermodynamics-based architectures has proven to be a valuable tool in this regard [43]. This emphasis on sparsity not only enhances the model's interpretability but also aligns with the objective of extracting meaningful insights from the underlying physical processes.\nThis consideration becomes increasingly critical in scenarios where data is subject to uncertainties [44], as establishing a deterministic relationship between stresses and strains may often be unrealistic. Strategically, incrementally increasing the network's complexity could facilitate the identification of a unique network configuration [45]\nThe challenge of creating a 'rich' data set that enables optimal training of neural networks is gaining importance. On a structural level, this raises questions about how to design test specimens that maximize information extraction, which has been explored for one-shot identification in [46]. Additionally, microstructural simulations serve as powerful tools for enriching macroscopic data sets [47]. In this regard, the work presented in [48] provides methodologies for reconstructing microstructures from extremely limited data sets.\n1.2. Hypothesis\nWe hypothesize that leveraging the depth and width of neural networks within a dual potential framework enables the modeling and discovery of a broad spectrum of inelastic material behaviors, including pressure-sensitive inelastic flow. By scaling the complexity of the neural network architecture, we aim to capture increasingly sophisticated material responses. Furthermore, we anticipate that utilizing sufficiently\u2018rich' data sets will lead to accurate model development and reliable training outcomes.\nIn this initial phase of our work, we simplify the problem by neglecting both intrinsic and induced directional influences. Consequently, we do not incorporate preferred directions (i.e. anisotropy) in"}, {"title": "1.3. Outline", "content": "the formulation of the Helmholtz free energy, we disregard hardening effects, and we assume that the potential depends solely on the driving force.\nTo ensure full accessibility of the source code and to facilitate reproducibility, we implement the entire framework and training scheme using JAX [49].\n1.3. Outline\nWe begin with a description of the constitutive framework for inelastic materials at finite strains, applicable to a wide range of materials, in Section 2. There, we formulate the dual/pseudo potential in terms of stress invariants and comment on its connection to Generalized Standard Materials in Section 2.1. In Section 3, we present the novel neural network architecture of the generalized dual potential embedded in a recurrent context, discussing time discretization schemes and our regularization approach. Therefore, in Section 3.1.1, we briefly discuss mathematical properties to obtain a potential that satisfies thermodynamics a priori. In Section 3.1.3, we show how well-known models, such as the von Mises or Drucker-Prager models, are included in the architecture. Afterwards, in Section 4, we train our network on artificial and experimentally obtained data and evaluate its performance. Our results are critically assessed in Section 5, including a discussion on currently observed limitations of the approach. Finally, in Section 6, we conclude on the proposed method and outline possible future investigations."}, {"title": "2. Constitutive framework for finite strain inelasticity", "content": "In this section, we briefly outline the underlying constitutive framework for general inelastic materials at finite strains, which are modeled using the multiplicative decomposition of the deformation gradient. For this, we introduce two fundamental scalar-valued quantities: The Helmholtz free energy, \u03c8, as well as a dual potential, w. All constitutively dependent quantities can be derived from these thermodynamic potentials. This dual potential approach is certainly not the only method for modeling inelastic behavior. We will elucidate its close relationship to Generalized Standard Materials [50], another well-established framework for characterizing inelasticity, in Section 2.1.\nKinematics. We employ the multiplicative decomposition of the deformation gradient, $F = F_eF_i$, into an elastic part, $F_e$, and an inelastic part, $F_i$, cf. [51, 52, 53, 54]. Both determinants of the individual parts are greater than zero. Conceptually, we introduce an intermediate configuration, relative to which the elastic response is characterized. Unfortunately, the multiplicative decomposition is non-unique, i.e. we may superimpose any rotation $F = F_eQ^*Q^*F_i = FF_i$ where $Q^* \\in SO(3)$ with $SO(3)$ denoting the special orthogonal group. By employing the singular value decomposition, we recognize that $F_i$ and $F_i$ share the same singular values, and thus, the same stretch tensor $U_i$ resulting from the polar decomposition $F_i = R_iU_i$ with $R_i \\in SO(3)$. Thus, we find $U_i$ to be unique, and further, $F = RU_i$ where $R= Q^*R_i$. Lastly, we introduce an appropriate stretch measure of the elastic stretches $C_e = F_e^TF_e = Q^*F_e^TF_eQ^*$, which however, is"}, {"title": "Clausius-Planck inequality", "content": "non-unique.\nClausius-Planck inequality. Any constitutive framework for solids must satisfy the Clausius-Planck inequality $D := - \\dot{\\psi} + 1/2 S : \\dot{C} \\geq 0$ where $S$ denotes the second Piola-Kirchhoff stress, while $C = F^TF$ refers to the right Cauchy-Green tensor. For the time being, we assume the Helmholtz free energy to be a scalar-valued isotropic function [55, 56] depending solely on $C_e$, i.e. $\\psi = \\phi(C_e)$. Hence, we obtain the following, cf. [57]\n$D = (S - 2F_e\\frac{\\partial \\phi}{\\partial C_e}F_e^T):\\dot{L_i} + \\frac{1}{2} \\underbrace{2C_e:\\frac{\\partial^2 \\phi}{\\partial C_e \\partial C_e}:\\dot{C_e F_e^TF_e^T}}_{\\geq 0} \\geq 0\\qquad (1)$"}, {"title": "Co-rotated intermediate configuration", "content": "where we introduce the elastic Mandel-like stress $\\Sigma$, which is symmetric since $\\phi$ is an isotropic function of $C_e$, cf. [58]1. Noteworthy, since $\\Sigma$ solely depends on $C_e$, the elastic Mandel-like stress is also non-unique, i.e. $\\Sigma = Q^*\\Sigma Q^*$. Following the arguments of [59, 60, 61], we assume the term in brackets in Inequality (1) to be zero, revealing the state law for $S$. Consequently, as $\\Sigma$ is symmetric, we can reduce the dissipation inequality to\n$D_{red} := \\Sigma : D_i \\geq 0\\qquad (2)$\nwhere $D_i := sym(L_i)$ is the symmetric part of $L_i$. To satisfy the reduced dissipation inequality for arbitrary processes, we will introduce a dual potential, $w = \\hat{w}(\\Sigma)$, which is assumed to be a scalar-valued isotropic function in order to be independent of the superimposed rotation $Q^*$.\nCo-rotated intermediate configuration. We have observed that the relevant constitutive quantities, such as $C_e$ and $\\Sigma$, suffer from an inherent rotational non-uniqueness. This poses challenges in computing these quantities and derivatives with respect to those, for instance $\\frac{\\partial \\psi}{\\partial C_e}$. To address this issue in our numerical implementation, we adopt the approach suggested by [34] and introduce a co-rotated intermediate configuration. In short, this approach pulls all non-unique quantities back by either $R_i$ or $R_i^*$, i.e. $(\\bullet) := R_i^*(\\bullet)R_i = R_i^*(\\bullet)R_i$. Consequently, we obtain the following unique quantities\n$C_i = U_iC_eU_i^{-1}, \\qquad S = -2U_i\\frac{\\partial \\psi}{\\partial \\hat{C}_e}U_i^{-1}, \\qquad \\Sigma = 2C_e\\frac{\\partial \\psi}{\\partial \\hat{C}_e},\\qquad  D_i = sym(\\dot{U}_iU_i^{-1}).\\qquad (3)$"}, {"title": "Potential-based evolution equation", "content": "Noteworthy, the co-rotated pullback preserves both the symmetry as well as the eigenvalues, which is considered an advantage.\nPotential-based evolution equation. It remains to introduce an evolution equation for $D_i$ in a thermodynamic consistent way such that Inequality (2) is satisfied for arbitrary processes. Therefore, we postulate the existence of a pseudo potential [62], which we may identify as the dual potential resulting from the Legendre-Fenchel transformation of the \u2018classical' dissipation potential (see Section 2.1) known from Generalized Standard Materials [50], viz.\n$D_i = \\frac{\\partial \\hat{w} (\\Sigma)}{\\partial \\Sigma}. \\qquad (4)$\nAccording to Germain et al. [63], the dissipation inequality is naturally fulfilled if w is convex, zero-valued, and non-negative with respect to $\\Sigma$ [37]2.\nInvariant representation. As discussed above, the dual potential is assumed to be an isotropic function of $\\Sigma$, and can, thus, be expressed in terms of its invariants. Here, we choose the common stress invariants $I_{\\Sigma} := tr(\\Sigma), J_{\\Sigma} := 1/2tr(dev(\\Sigma)^2)$, and $J_{\\Sigma} := 1/3tr(dev(\\Sigma)^3)$. With these invariants at hand, the evolution equation reduces to\n$\\frac{\\partial w^*}{\\partial \\Sigma} = \\frac{\\partial w^*}{\\partial I_{\\Sigma}}I +  \\sqrt{J_{\\Sigma}}\\frac{\\partial w^*}{\\partial J_{\\Sigma}}dev(\\Sigma) +  \\sqrt[3]{J_{\\Sigma}}\\frac{\\partial w^*}{\\partial J_{\\Sigma}}dev(dev(\\Sigma)^2)\\qquad (5)$\nwhere $w^* = w^*(I_{\\Sigma}, J_{\\Sigma}, J_{\\Sigma})$. The square and cubic roots are calculated to ensure that all invariants share the same unit. If we plug (5) into the co-rotated version of Equation (2)\n$D_{red} = \\nabla w^*(z) \\cdot z \\geq 0, \\qquad z = \\begin{pmatrix}  \\sqrt{I_{\\Sigma}} \\\\ \\sqrt[3]{J_{\\Sigma}} \\\\ \\sqrt[4]{J_{\\Sigma}}  \\end{pmatrix} \\qquad (6)$\nwe observe that the inequality is satisfied if $w^*$ is convex, zero-valued, and non-negative with respect to its arguments; however, this does not guarantee its convexity with respect to $\\Sigma$, cf. [65]. The reason for this lies in the indefinite Hessian of $\\frac{\\partial w^*}{\\partial \\Sigma}$ with respect to the Mandel-like stress. Nevertheless, non-convex yield surfaces, which are typically modelled as a potential subtracted by a threshold such as the yield stress, are not only of significant practical relevance [66, 67, 68] but also amenable to numerical treatment [69, 70]. As $w^*$ includes the special case of being convex with respect to $\\Sigma$, we consider this framework advantageous."}, {"title": "2.1. Relation to Generalized Standard Materials for solids", "content": "2.1. Relation to Generalized Standard Materials for solids\nIn the following, we will explain the intrinsic relationship between the present modeling framework for iCANNs and the classical framework of Generalized Standard Materials, the latter of which is well-known in the literature (see e.g. [50, 63, 24]). We start again with the Helmholtz free energy and assume, for the same reasons as explained above, that it is a scalar-valued isotropic function of quantities in the co-rotated intermediate configuration. Specifically, we assume a dependence of \u03c8 on $\\hat{C}_e$ only, i.e. $\\psi = \\phi(\\hat{C}_e)$. More general cases in which 4 additionally depends on further internal state variables or structural tensors are, of course, possible. However, since this does not lead to additional insights in the presentation that follows, we will not consider this case for simplicity.\nExploiting again the Clausius-Planck inequality and the chain rule of differentiation, we may arrive at\n$D = -\\dot{\\psi} + \\frac{1}{2} \\hat{S} : \\dot{\\hat{C}} \\geq 0 \\qquad \\rightarrow \\qquad D = -2\\frac{\\partial \\phi}{\\partial \\hat{C}_e} : \\frac{1}{2} \\dot{\\hat{C}} + \\hat{S} : \\dot{\\hat{C}} \\geq 0.\\qquad (7)$\nConsidering $\\hat{C}_e = U_i^{-1}C_eU_i^{-1}$, the relation $\\dot{C_e} = \\dot{U}_i \\hat{C}U_i + U_i\\dot{\\hat{C}_e}U_i + U_i\\hat{C}\\dot{U}_i$, and well-known properties of the scalar product of two second-order tensors, Inequality (7) can directly be rewritten as\n$D = (\\hat{S} - 2\\frac{\\partial \\phi}{\\partial \\hat{C}_e}) : \\frac{1}{2} \\dot{\\hat{C}} + \\hat{\\Sigma}: L_i \\geq 0,\\qquad (8)$\nHere, $\\hat{S} := U_i^T S U_i$ is the second Piola-Kirchhoff stress tensor relative to the co-rotated intermediate configuration and $\\Sigma = \\hat{C}\\hat{S}$ denotes the (up to this point generally unsymmetric) Mandel-like stress tensor in the very same configuration. With $L_i = sym(L_i) + skew(L_i) = D_i + W_i$, Expression (8) is finally rewritten as\n$D = \\hat{S}^{dis} : \\frac{1}{2} \\dot{\\hat{C}} + sym(\\hat{\\Sigma}) : D_i + skew(\\hat{\\Sigma}) : W_i \\geq 0\\qquad (9)$\nwhere $\\hat{S}^{dis} := (\\hat{S} - 2\\frac{\\partial \\phi}{\\partial \\hat{C}_e})$ can be considered the irreversible or dissipative part of the stress $\\hat{S}$. To fulfill dissipation inequality (9), it is now customary in the framework of Generalized Standard Materials to assume a scalar-valued dissipation potential3 $\\Omega = \\Omega(\\dot{\\hat{C}}, D_i, W_i)$, expressed in terms of the strain-like rate quantities $\\dot{\\hat{C}}, D_i$, and $W_i$, which is convex, non-negative, and zero-valued at the origin, i.e. $\\Omega(0, 0, 0) = 0$. This potential is conveniently used to derive complementary laws for the thermodynamic conjugate forces, i.e.4:\n$\\hat{S}^{dis} = \\frac{\\partial \\Omega}{\\partial \\dot{\\hat{C}} },\\qquad sym(\\hat{\\Sigma}) = -\\frac{\\partial \\Omega}{\\partial D_i}, \\qquad skew(\\hat{\\Sigma}) = -\\frac{\\partial \\Omega}{\\partial W_i}.\\qquad (10)$\nAs can be shown, with the above definitions, thermodynamic consistency of the formulation is naturally ensured.\nStrain-rate independent dissipation potential. Further consequences for the dissipation"}, {"title": "3. Architecture of generalized iCANN", "content": "potential arise when dealing with solid materials, for which it is usually assumed that purely elastic, but otherwise arbitrary deformations ($\\dot{\\hat{C}} \\neq 0, D_i = W_i = 0$) do not cause any dissipation. In this case, it can be inferred from Inequality (9) that\n$D = \\hat{S}^{dis} : \\frac{1}{2} \\dot{\\hat{C}} = 0 \\qquad \\rightarrow \\qquad \\hat{S}^{dis} : \\frac{\\partial \\Omega}{\\partial \\dot{\\hat{C}}} = 0.\\qquad (11)$\nIn other words, it can be concluded that the dissipation potential cannot be a function of the elastic strain rate $\\dot{\\hat{C}}$ in this case5. Furthermore, as the second Piola-Kirchoff stress tensor relative to the intermediate configuration becomes $\\hat{S} = 2C\\frac{\\partial \\phi}{\\partial \\hat{C}}$, this immediately leads to a symmetric Mandel-like stress tensor $\\Sigma = C\\hat{S} = 2C\\frac{\\partial \\phi}{\\partial \\hat{C}}$ (see also (3)3), i.e. sym($\\hat{\\Sigma}$) = $\\hat{\\Sigma}$ and skew($\\hat{\\Sigma}$) = 0. The Clausius-Planck inequality therefore reduces to\n$D_{red} = \\hat{\\Sigma} : D_i \\geq 0,\\qquad (12)$\nsuch that the dissipation potential needs to be a function of $D_i$ only, i.e. $\\Omega = \\hat{\\Omega}(D_i)$ with $\\Omega(0) = 0$. The complementary law for $\\hat{\\Sigma}$ is then obtained as\n$\\hat{\\Sigma} = -\\frac{\\partial \\Omega}{\\partial D_i}.\\qquad (13)$\nDual dissipation potential in terms of stress-like quantities. Finally, by means of a Legendre-Fenchel transformation of $\\Omega$, a dual dissipation potential w in terms of the stress-like quantity $\\Sigma$ can be derived:\n$\\hat{w}(\\hat{\\Sigma}) = sup_i \\left(  \\hat{\\Sigma} : D_i - \\hat{\\Omega}(D_i)\\right).\\qquad (14)$\nThe latter potential is also convex, non-negative, and zero-valued at the origin, i.e. $\\hat{w}(0) = 0$. It can be employed to define a thermodynamically consistent evolution equation (or complementary law) for the strain-like internal state variable $D_i$ via\n$D_i = \\frac{\\partial \\hat{w}}{\\partial \\Sigma}.\\qquad (15)$\nRelation to iCANN framework. The above shows that iCANNs perfectly fit into the framework of Generalized Standard Materials. The only difference to the more classical approach is that, in the iCANN modeling framework presented here, the dual dissipation potential (14) in terms of the stress-like variable $\\Sigma$ is constructed and identified directly. However, this alternative procedure is by no means unusual or disadvantageous, and has been proposed as an equally valid approach by other authors in the past (among many others, [71, 72, 73, 74]). One potential can always be constructed\n3. Architecture of generalized iCANN\nRecurrent architecture. History-dependent materials require sequential data processing. The iCANN framework integrates two independent feed-forward networks \u2013 one for dual potential discovery and another for the Helmholtz free energy \u2013 within a recurrent architecture. Although distinct, these networks are strongly coupled via their derivatives, governing inelastic stretch and thermodynamic driving force. Their architectures are detailed in Sections 3.1 and 3.2. Schematic 1 depicts the overall structure. As typical in recurrent networks, the current output, S, depends not only on the current input, C, but also on propagated states, Ui, across sequential time data. The time integration scheme, whether implicit or explicit, relies on the time increment \u2206t. Explicit methods require propagating the previous step's right Cauchy-Green tensor, Cn, while implicit methods solve for inelastic stretch iteratively via an equality constraint.\nLoss function. Defining the loss function is critical for accurate neural network training. While neural networks effectively learn complex input-output relationships, their flexibility can lead to overfitting. To mitigate this, we apply weight regularization, a well-established method also effective in training Constitutive Artificial Neural Networks (CANNs) [43, 45]. For finite elasticity, studies have demonstrated successful regularization strategies for CANNs. For inelasticity, this still poses an open research question. In contrast to elasticity, where sparsity leads to a reasonably \u2018unique' solution, inelasticity demands not only a sparse network but also the discovery of the underlying inelastic phenomena hidden in the data. Ideally, if the material behaves for instance purely elastically, the network should assign zero weights to the dual potential.\nWe define the loss as follows\n$L(\\boldsymbol{S}; \\boldsymbol{W}, \\boldsymbol{b}) = \\frac{1}{N_{exp}} \\sum_{a=1}^{N_{exp}}\\frac{1}{N_{data}} \\sum_{\\beta=1}^{N_{data}} ||\\boldsymbol{S}^\\beta(C_B, U_{ia}; \\boldsymbol{W}, \\boldsymbol{b}) - \\boldsymbol{\\hat{S}^\\beta}||_2^2 + R_{\\psi}(W_{\\psi}, b_{\\psi}) + R_{w^*}(W_{w^*}, b_{w^*})\\qquad (16)$\nwhere (\u2022)T refers to Voigt's notation and ||(\u2022)||2 refers to computing the mean of the sum of squared element-wise differences, known as L2 loss. Further, nexp denotes the number of experiments used for training, while ndata is the number of data points per experiment. The experimentally measured stress is denoted by S. In addition, $R_\\psi$ and $R_{w^*}$ account for the regularization of the weights and biases of the feed-forward networks of the Helmholtz free energy and the dual potential, respectively. These weights and biases are summarized in W and b.\nTime integration schemes. To solve the evolution Equation (5) numerically, we employ two different discretization schemes for the interval $t \\in [t_n, t_{n+1}]$6. The exponential integrator map is"}, {"title": "3.1. Feed-forward network: Dual potential", "content": "well suited in the finite strain regime as it, for instance, preserves the volume in case of an absence of hydrostatic pressure. Moreover, as shown in [37], the exponential integrator satisfies that the isochoric invariants of Ui are preserved if the deviator of $\\Sigma$ is equal to zero.\nFirst, we employ an explicit time integration scheme [34]\n$C_{i} = U_{in} exp \\left( 2 \\Delta t \\hat{D}_i\\right)U_{in}^T, \\qquad U_i = +\\sqrt{C_{i}}\\qquad (17)$\nwith \u2206t := tn+1 \u2212 tn denoting the time increment. Although an explicit integration is numerically efficient as no iterative solution is required, it might be less stable than an implicit scheme. Further, we have to compute the square root of $C_{i}$. As JAX does not provide an implementation of the matrix square root at the current time, we use the generating function proposed in [75]. Due to these reasons, we additionally introduce the following residual of an implicit integration scheme, cf. [76], [34]\n$log \\left( U_i^{-1} C_{i,n} U_i^{-T} \\right) + 2 \\Delta t \\hat{D}_i = 0\\qquad (18)$\nwhere log (\u2022) refers to the matrix logarithm, which is again computed using a generating function [75]. To solve Equation (18) in an iterative manner, we employ Broyden's method (see Ap-pendix A.1).\n3.1. Feed-forward network: Dual potential\nDeep feed-forward neural networks, i.e. multiple hidden layers between in- and outputs, serve as universal approximators [77] and can approximate functions to any desired degree of accuracy if complexity of the network is increased. Multilayer networks achieve this by recursively applying linear transformations\n$y_{k+1} = W_{k+1}x_k + b_{k+1}\\qquad (19)$\nfrom one layer, k, to the next layer k+1. Subsequently, a nonlinear activation function, f, is applied on the linear transformation\n$x_{k+1} = f(y_{k+1}).\\qquad (20)$\nThis activation function can vary between layers and can even differ between neurons in the same layer, although this is not common. In the following, we discuss the architecture of a generalized dual potential resulting in a convex, zero-valued, and non-negative function. Specifically, we constrain the layer stacking, permissible activation functions per layer, and the domains of weights, W, and biases, b, between layers. To this end, we briefly recapture some mathematical properties of convexity, zero-valueness, and non-negativity of composed functions in Section 3.1.1. Our discussions inspire the design of the iCANN's dual potential in Section 3.1.2, which is capable of recovering some classical potentials from the literature, see Section 3.1.3."}, {"title": "3.1.1. Convex, zero-valued, and non-negative dual potential", "content": "3.1.1. Convex", "consistency.\nConvexity": "Composition of a convex function and linear combination. Suppose we have a function $g : \\mathbb{R"}, "n \\rightarrow \\mathbb{R}$ (linear combination)\n$g(x_1,...,x_n) = a^Tx, \\qquad x = \\begin{pmatrix}  x_1 \\\\  \\vdots \\\\  x_n  \\end{pmatrix}, a \\in \\mathbb{R}^n,\\qquad (21)$\nas well as $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ which is convex. The composition $h(x) = (f \\circ g)(x)$ is convex, since the Hessian, $H_h(x)$, is a rank-1 matrix of the outer product\n$H_h(x) = \\frac{\\partial^2 f}{\\partial g^2}aa^T.\\qquad (22)$\nThus, its only non-zero eigenvalue is $\\frac{\\partial^2 f}{\\partial g^2}a^Ta$, which is greater or equal to zero since $\\frac{\\partial^2 f}{\\partial g^2} \\geq 0$ for a convex function f.\nConvexity: Positive sum of convex functions. Let us introduce a family of convex functions $f_1, f_2, ..., f_n : \\mathbb{R}^m \\rightarrow \\mathbb{R}$ and $g : \\mathbb{R}^m \\rightarrow \\mathbb{R}$ be the positive sum of these functions, i.e.,\n$g(x) = \\sum_{i=1}^n b_i f_i(x), \\qquad x \\in \\mathbb{R}^m, b_i \\geq 0\\qquad (23)$\nthen g(x) is also convex, as each fi is convex and"]}