{"title": "Thing2Reality: Transforming 2D Content into Conditioned Multiviews and 3D Gaussian Objects for XR Communication", "authors": ["Erzhen Hu", "Mingyi Li", "Jungtaek Hong", "Xun Qian", "Alex Olwal", "David Kim", "Seongkook Heo", "Ruofei Du"], "abstract": "During remote communication, participants often share both digital and physical content, such as product designs, digital assets, and environments, to enhance mutual understanding. Recent advances in augmented communication have facilitated users to swiftly create and share digital 2D copies of physical objects from video feeds into a shared space. However, conventional 2D representations of digital objects restricts users' ability to spatially reference items in a shared immersive environment. To address this, we propose Thing2Reality, an Extended Reality (XR) communication platform that enhances spontaneous discussions of both digital and physical items during remote sessions. With Thing2Reality, users can quickly materialize ideas or physical objects in immersive environments and share them as conditioned multiview renderings or 3D Gaussians. Thing2Reality enables users to interact with remote objects or discuss concepts in a collaborative manner. Our user study revealed that the ability to interact with and manipulate 3D representations of objects significantly enhances the efficiency of discussions, with the potential to augment discussion of 2D artifacts.", "sections": [{"title": "1 INTRODUCTION", "content": "Shared artifacts, including digital resources (e.g., text, images, videos), and physical objects (e.g., prototypes, printouts), play a crucial role in facilitating effective communication and idea generation. They provide common spatial reference points that bridge gaps between collaborators, enhancing creative exploration and ideation [4]. Besides physical artifacts, designers frequently use online platforms like Pinterest and Google to source relevant digital artifacts that can support their design processes [13]. However, using shared artifacts in remote meetings often pose challenges, especially in scenarios that require quick and spontaneous sharing, such as brainstorming sessions. First, artifacts shared via remote meetings are typically in 2D, whether they are captured via camera or retrieved from online repositories, limiting the understanding compared to interactions with physical objects or 3D models. Second, in physical meetings, participants can easily observe and interact with tangible artifacts, which facilitates creative exploration and idea generation processes [4]. However, in remote meetings, this level of interaction is often unavailable or limited.\nSeveral methods have attempted to address these challenges, such as preparing 3D models in advance via CAD or 3D scanning [21], or employing specialized real-time 3D capture setups [40, 55]. While effective, these approaches have limitations: pre-made 3D assets do not support spontaneous sharing, and specialized setups are often impractical for general use. Recent advances in AI-driven text-to-3D and image-to-3D technologies [54] present an accessible and efficient alternative, lowering barriers to 3D content creation and enabling broader participation in collaborative efforts.\nIn this paper, we aim to understand how image-to-3D AI can mediate users in XR communication and how the design of such a system can benefit users and integrate interactive image-to-3D workflows within various Extended Reality (XR) meeting context. We introduce Thing2Reality, an XR communication platform that enables fluid interactions with 2D and 3D artifacts. Thing2Reality allows users to segment content from any source (video streams, shared digital screens) within the XR environment (Figure 1a), generate multi-view renderings (Figure 1b), and transform them into shared 3D Gaussians for interactive manipulation (Figure 1c). We evaluated Thing2Reality in a user study involving three tasks: avatar decoration, furniture arrangement, and workspace organization. Our findings suggest that while 3D objects facilitate intuitive explanations and hands-on collaboration, 2D representations are more often used in final pitch deliverables, suggesting a trade-off between the two formats depending on the context and purpose of the task. We demonstrate applications of Thing2Reality in various workspace and social scenarios, highlighting how on-the-fly 3D generation can enrich interaction and social connectedness, augmenting human-human communication regarding shared digital and physical artifacts.\nIn summary, we contribute:\n\u2022 Thing2Reality, an XR communication system that provides on-the-fly AI-mediated 3D objects generation by enabling users to present and share spontaneous thoughts, and augment their shared digital and physical artifacts with remote peers."}, {"title": "2 RELATED WORK", "content": "Our work is inspired by prior art on vision-language interfaces, distributed communication, and task-space collaboration."}, {"title": "2.1 Vision-Language Interfaces", "content": null}, {"title": "2.1.1 Generative Models.", "content": "Text-to-3D and image-to-3D methods, such as DreamFusion [43], focus on score distillation sampling (SDS) that utilizes pretrained 2D diffusion models to generate 3D content, but faces problems with speed and diversity. Recent advances in large reconstruction models [16, 29] use non-SDS methods. Large Gaussian Models (LGM) [54] use similar methods to [25], with algorithms to convert 3D Gaussian into meshes. Advances using multi-view diffusion models as a prior have also made generation of complex, textured 3D models possible. These generative models provide a foundation for transforming 2D visuals into 3D representations."}, {"title": "2.1.2 Text-Based and Spatial-Oriented Prompting.", "content": "Prompting has been enabled by large language models primarily as a natural language-oriented way of interaction. However, text-based prompts are constrained by the level of control they can provide, especially in terms of the spatial aspects of images [27]. Modern prompting in computer vision and machine learning has evolved towards spatial-oriented prompting. For example, ControlNet [66] enables users to condition image generation using additional images such as depth maps and human skeletons. Segment-Anything (SAM) [26] enables the use of points and boxes in addition to text as a way of prompting mask segmentation.\nTo enable users to grab content from shared multimedia artifacts and transform them into 3D generated objects, Thing2Reality utilizes a controllable pipeline that allows users to explicitly identify areas of interest using spatial-oriented prompts, such as points and strokes."}, {"title": "2.1.3 Vision and Language in Computer Interfaces.", "content": "Recent work has explored vision-language methods, such as text-to-image generation [32], in 3D design workflows and image generation for news illustration [31], and in mediating human-human co-creation with creativity support tools [6, 9, 53]. For communication-related areas, Visual Captions [33] utilized language input to retrieve relevant images as visual aids to augment human-human communication. These images can be used as shared media between users to facilitate communication. However, the existing visuals of shared media and artifacts have not been fully exploited in these interfaces."}, {"title": "2.2 Distributed Communication and Task Space Collaboration", "content": "People increasingly use remote conferencing platforms [17\u201319, 36] for workplace meetings, education, entertainment, and social interaction with families and friends. The field of computer-mediated cooperative work has investigated the importance of shared media [8, 34, 35, 42] during in-person and remote communications, especially in understanding how people use, create, and share multimedia artifacts. Shared task spaces are essential for scenarios such as education [37, 37, 45], creativity support [1], tabletop and tablet games [7, 64, 67], video editing [38], and physical task demonstration [28, 55]."}, {"title": "2.2.1 Al-Augmented 2D Shared Task Space.", "content": "IllumiShare [24] enabled users to share physical and digital objects on arbitrary surfaces. Recent work such as ThingShare [19] enabled the digital copies of physical objects with deep neural network to mediate remote communication and collaboration. Visual Captions [33] also supported shared media by augmenting language as visual aids between users in remote conferencing. Other work has explored mobile sharing [23] and reconstruction of mobile phone video streams [58], but providing a stable view can be challenging.\nThe advances in generative AI have enabled new opportunities for blending reality and enabling coarse-grained and fine-grained customization of environments. For example, BlendScape [46] enabled a blended virtual environment by meaningfully merging people's virtual backgrounds using in-painting and image-to-image techniques with selective regeneration of portions in the 2D video-conferencing environment. However, prior work has either focused on 2D visual aids with images retrieved by language to augment communication or 2D and 3D reconstruction/blending of capturing physical shared task spaces for remote collaboration. The potential of text-to-3D and image-to-3D workflows to transform generic artifacts into interactive 3D objects for communication tasks remains under-explored."}, {"title": "2.2.2 Enabling Spatiality in Remote and Video Conferences.", "content": "2.5D and 3D video-conferencing has been focused primarily on reconstructing and displaying of life-sized talking heads [51], gaze-aware 3D photos [12], and space-aware scene rendering for avatar placement (e.g., VirtualCube [68], ChatDirector [44]).\nDifferent from 2.5D and 3D avatars, SharedNeRF [47] leverages photo-realistic and view-dependent rendering with NeRF and point clouds, which enables an on-the-fly volumetric task space. Spatiality in video conferencing [11] has been explored by combining person videos with immersive desktop collaborative virtual environments. These spatial interfaces were found to positively influence social presence and co-presence compared to 2D, while potentially compromising task focus and efficiency. Sousa et al. [49] explored ways to mediate ambiguity in workspace awareness when interacting with 3D digital assets and found an increase in mental demand when converting coordinates between frames. However, the creation, interaction, and sharing of AI-generated 3D objects to mediate human-human communication in remote settings remain under-explored.\nFurthermore, prior work has used extended reality to assist remote physical task guidance using virtual replicas [39, 56], which was found to be more efficient than 3D annotations in mixed-reality collaboration scenarios [56]. However, these studies focused more on the manipulation and interaction of pre-designed 3D virtual objects and the support of virtual replicas for spatial referencing rather than how the spontaneity of 3D object creation can enable better delivery of human thoughts and ideas."}, {"title": "3 DESIGN CONSIDERATIONS", "content": "We present three dimensions in articulating our design space (Figure 2) and situate Thing2Reality into prior literature of distributed communication and demonstration of 2D and/or 3D artifacts. Prior work also explores different ways of creating pre-made or catalog assets, such as using gestures to approximate and imitate the object [15] among a database of known objects, or understanding the role of virtual replicas in communication and remote assistance [39, 56]. We did not include this line of work because we focused on the spontaneity of sharing things during the communication phase."}, {"title": "3.1 Design Space", "content": null}, {"title": "3.1.1 Methods: Capturing versus Generating.", "content": "The distinctions between capturing and generating methods are represented in Figure 2: Methods).\nCapturing as Virtual Replicas. This line of work supports spontaneity of 2D and 3D artifacts sharing via snapshot-based, 3D reconstruction, or search-based methods, which aims to capture and reconstruct the physical reality as virtual replicas [19, 20, 30, 47]. For example, ThingShare [19] explores snapshot-based interactions to facilitate object-focused collaboration. Some other work utilized real-time 3D reconstruction with sophisticated camera setups (e.g., [30]) or single camera with NeRF (e.g., [47]). However, this line of research focuses on cloning the physical space for physical tasks or remote assistance, whereas shared information artifacts such as digital images and videos in the web were not explored.\nGenerating as On-the-Fly Assets. Different from capturing or reconstructing surrounding scenes or objects as replicas, GenAI enabled new opportunities for remote human-human communication, specifically the task space communication in both digital and physical space. For example, digital images, and sketches [33] can be used for augmenting spontaneous communication with LLM-enabled digital search. BlendScape [46] used stable diffusion and in-painting techniques to blend virtual backgrounds of users together as a meaningful cohabited space. Different from these methods that enabled 2D visual aids or 2D virtual background, Thing2Reality enables both 2D and 3D with (primarily) image-to-3D methods. Furthermore, we separate these artifacts as digital vs. physical for the data source - as these artifacts can be either searched, or drawn in the digital information space (as visual aids like [33]), or image streams captured directly from the physical environment (for physical tasks or object-focused collaboration)."}, {"title": "3.1.2 Representation.", "content": "The column (Figure 2: Representation) differentiates the object representations (2D-only, 2D+3D, 3D-only) supported by the system. However, most objects explored by prior individuals or collaborative work and remote conferencing are represented as 2D [19, 24, 33]. Remixed-reality supports real-time 3D reconstruction and provided scene modifications [30]. Some recent work support 3D scene sharing during remote meetings [47].\nThe bidirectional interactions between 2D images and 3D models have been explored in augmented reality [69], yet the 3D models are pre-loaded rather than spontaneously identified by the user, and are thus not included in the design space. Furthermore, this line of work either explores pre-made bidirectional 2D and 3D object transformation from either the physical environment [10, 61] or from digital assets [62, 69]. Furthermore, most of them emphasized individual editing rather than collaborative interactions. An exception is Loki [55] that provides different data modalities (both 2D videos and 3D point-cloud scenes) of physical spaces for scene-level (SL) interactions with point clouds and videos, whereas Thing2Reality focuses on object-level (OL) interactions."}, {"title": "3.2 Design Goal", "content": "Based on the design space, we formulated the following three objectives to direct the design of Thing2Reality to enable efficient and flexible discussion around artifacts in XR communication."}, {"title": "DG1 Spontaneity:", "content": "Enable Spontaneous Communication Using Digital and Physical 3D Artifacts As Visual Aids. Acknowledging the importance of both physical and digital artifacts in professional discussions, we aim to facilitate a seamless conversion of 2D artifacts from diverse data sources into 3D representations. These inputs from a variety of sources includes digital files (such as images, and sketches) and physical objects captured via camera feeds."}, {"title": "DG2 Cohabitation:", "content": "Support for Co-Habitation of 2D and 3D Objects During Communication. Conventional remote conferencing approaches often rely on a single modality of capture and presentation data (e.g., 2D images, 2D videos) to teach or guide remote participants [2]. Loki [55] demonstrates the potential benefit of incorporating multiple data modalities. To facilitate efficient discussions and collaborative sessions, Thing2Reality should allow for multiple data representations. By supporting interaction with multiple data modalities, users can choose the most appropriate representation for their current communication needs, leading to more effective collaboration."}, {"title": "DG3 Transition:", "content": "Enable Flexible Bi-Directional Transformations Among Digital Media Forms (i.e., 2D images, videos, and 3D). XR workspaces can be more dynamic and open compared to traditional videoconferencing, and the presence of multiple data modalities may introduce friction for users. Recognizing the diverse needs of remote collaboration with multiple data modalities in D2, it is also essential to allow users to frictionlessly switch between different forms of these representations (2D images, videos, multi-view representations, and 3D models) according to the context of their discussion. This requirement would imply that Thing2Reality should not only store and organize various forms of media but also allow for their easy retrieval and transformation during discussions. By enabling flexible bi-directional transitions between digital media forms (2D-to-3D, 3D-to-2D), users can adapt their communication style to the specific requirements of the task at hand, leading to more efficient and effective collaboration."}, {"title": "4 THING2REALITY SYSTEM OVERVIEW", "content": "A key takeaway from our design space highlighted the effectiveness of integrating 3D object affordances with the spatial organization advantages of 2D artifacts. The workspace encompasses not just flat artifacts but also three-dimensional things. We developed Thing2Reality to capitalize on the strengths of 3D artifacts to facilitate communication between individuals, focusing on the utilization of surrounding information surfaces (e.g., tables, whiteboards). This system introduces the capability to seamlessly transition 3D artifacts from 2D digital or physical counterparts. Before delving into the system's design, it is crucial to clarify the definition of \"Thing\" in the context of our work."}, {"title": "4.1 What the \u201cThing\"? Exploring the Role of User-Generated 3D Assets in Spontaneous Communication", "content": "In bringing user-generated 3D assets into distributed human-human communication, bridging 2D and 3D counterparts may shape a new way of communication in the immersive information space. We aim to outline the typical approaches individuals take when spontaneously incorporating various artifacts (i.e., sketches, searched images, and physical objects) into discussions as a source of inspiration, explanation, or clarification (Figure 3).\n\u2022 Text-based content (Figure 4-1): Text-based content uses words and language to convey ideas, including written descriptions, transcribed speech, and notes. Text-based contents can be transformed into 2D images use text-to-image methods like Gemini Imagen.\n\u2022 Hand-created visual content (Figure 4 - 2): Hand-created visual content encompasses manually produced images, diagrams, or visual representations, either physical or digital. This includes sketches, drawings, and hand-drawn diagrams, providing intuitive and spontaneous representations of ideas, spatial relationships, or abstract concepts in communication. Current methods such as ControlNet [66] use sketches as one of the ways for controlling image generation.\n\u2022 Digital visual content (Figure 4 - 1): Images found through online searches like Google images or Pinterest, screenshots, and digital artwork stock photos to find images that closely align with their discussion topics, utilizing these images as a reference point [13].\n\u2022 Captured real-world content (Figure 4 - 3): Photographs or scans of physical objects and environments, which can serve as a powerful means of conveying ideas, but their integration poses challenges for distributed users [4, 19], who might opt to digitally capture and share these items. It is important to note that these digital 3D representations of real-world content do not always capture the specific details of an object as accurately as a virtual replica (e.g., NeRF). Instead, they serve as a proxy for the original object. Furthermore, this can be beneficial for items when part of an object's side is not easily capturable, or when it's difficult to photograph at close range (e.g., a large shelf).\nTransforming these variations of 2D content into 3D objects can help enhance the immediacy and tangible engagement with abstract concepts, such that users can gain a deeper mutual understanding during discussions. Furthermore, text-to-image and sketch-to-image generation methods often produce less predictable results due to the vagueness of the input in describing expected images, making precise control challenging. In contrast, searching for existing images or capturing real-world content allows for more direct selection and accuracy. This difference in control stems from the interpretative nature of AI-based generation versus the specificity of human-curated or directly captured visual content. Recognizing this difference in control between Al-based generation and human-curated or directly captured visual content, Thing2Reality's design primarily focuses on digital visual content and captured real-world content with image-to-3D approaches. This ensures more precise control over the images used in communication, enhancing the system's reliability and user experience."}, {"title": "Our Focus on Spontaneous Human-Human Communication in XR:.", "content": "In light of these insights from our design space and prior literature, our research focuses on the exploration of user-generated 3D assets. This focus is driven by the unique potential of these assets to disseminate the essence of abstract concepts to distributed XR users.\nWe aim to explore the spontaneity of user-generated 3D objects from any sources in facilitating distributed XR communication."}, {"title": "4.2 Interaction Workflow", "content": "Here we show the default interaction workflow with the example of digital search.\nInteractive Object Segmentation. The user can identify object of interest by holding the grip button of the controller while capturing a pointdown and point up event with the trigger button, which results in three point positions identified on the data source image (e.g., a web browser view, or a camera feed). The object of interest will then be segmented from the image source, and provide the user the segmented results besides their hands. The user can then confirm to send the images for multi-view rendering and 3D Gaussian creation.\n2D-to-3D: Creation of Multi-views and 3D Generated Objects. After the user confirms the object of interest, the multiple conditioned views will be rendered on a 2D Pie Menu (Figure 4b) attached to the user's left controller. The center of the Pie Menu shows the original image being cropped from the data source (webviews, images from physical space) or generated from the image generation model. The four orthogonal views, generated with conditioned diffusion models, will be displayed on the top (front view), left (side views), right (side views) and bottom (back views) of the outer ring of the Pie Menu. Selecting the central image also displays a 360\u00b0 video of the object. The user can show or hide it by pressing the \"X\" button on the controller. The generated 3D object will then become a shared object in the environment, which can be moved, grabbed, and re-scaled by all the users via the semi-transparent Sphere Proxy (Figure 4c) as a collider. The orthogonal views of a 3D object on the 2D Pie Menu are private to the user who created it, but it can be converted from any shared 3D objects generated in the environment.\n3D-to-2D: Projecting Things to Surrounding Whiteboard and Table for Workspace Communication. The user can take a snapshot from any angles of the generated 3D objects, under the field of view of the user, and project the point of view on collaborative surfaces like a whiteboard or a table (Figure 5), which is different from the discrete orthogonal views due to the continuous perspectives a 3D object presents.\nUsers can use raycasting to drag things around the whiteboard, rescaling things by selecting the object and using the y axis of the thumbstick to make it larger or smaller. They can also delete the object by selecting it and then pressing on the \"B\" button on the controller. Users can also select the discrete orthogonal views on their private 2D menu, which can be projected on the whiteboard. The central image can be projected on the whiteboard to show 360-degree videos of the object."}, {"title": "5 IMPLEMENTATION", "content": "The virtual environment was developed using Unity 2022.3.19f1 and the following SDKs: Oculus Interaction Toolkit, Meta Avatar SDK for rendering avatars, gestures, and lip-syncing, and Photon Fusion and Voice SDK for voice streaming between avatars, and ZED SDK for physical space sensing.\nSystem Setup. The study program ran on a desktop PC with an Intel Core i7-13700K processor and an NVIDIA RTX 4070 Ti GPU for MobileSAM [65], text-conditioned [48] and image-conditioned [60] multi-view diffusion models, and Large Gaussian Models [54] to fuse multiview renderings into interactive 3D Gaussians.\nDue to the privacy issues of capturing data from physical environments via current passthrough technologies of VR/MR HMD, a ZED Mini camera was attached in front of the Meta Quest 3.\n3D Object Rendering Pipeline. The enhancement of extended reality communication involves a blend of digital and physical elements, captured through both web-based sources and external camera feeds.\nUtilizing a webview feature within a virtual space allows for the seamless incorporation of digital content into 3D artifacts. This is achieved by implementing a Unity web browser plugin [59]. To merge physical reality into the XR experience, image frames are captured using the ZED Mini camera\u00b9. These frames are then integrated into the Unity environment, providing a real-world context that users can interact with alongside digital content.\nHence, users can identify objects in both web-based content and live camera feeds capturing physical spaces via ZED and turn the identified 2D images into 3D. This includes the ability to perform actions such as making strokes or taking snapshots. Interactions within the virtual environment are managed through a custom event listener via a Python Flask server. The original input (Figure 7:3) is the selected image frame with the three points on the selected image frame filtered from the user's stroke interaction using the raycaster of the Meta Quest 3 controller.\nThe Unity application communicates with the Python Flask Server via HTTP POST requests (Figure 7:4), activating models for quick, gesture-prompted segmentation. This process identifies objects of interest based on user interactions. Once segmented, objects are visualized within the Unity environment for user confirmation. Multi-view rendering and 3D Gaussian modeling techniques are employed to create more interactive representations of these objects. The 3D Gaussian output was a \".ply\" format, but can also be represented as a 2D video, or a visual effect in the Unity environment. The 3D Gaussian were then imported and visualized in Unity as Gaussian splats, surrounded by a semi-transparent sphere around it as a proxy collider to enable interactions like Grab, Scale, and Move with hands or controllers."}, {"title": "6 APPLICATION SCENARIOS", "content": "Thing2Reality can be used in a diverse set of applications to elevate the experiences of the human-human communication across workspace and social gatherings."}, {"title": "6.1 Workspace Discussion", "content": null}, {"title": "6.1.1 Collaborative Concept Explanation.", "content": "Thing2Reality enhances concept explanation by providing tangible 3D explorations of ideas that may be difficult to convey with static 2D images. For example, in a product pitch meeting (Figure 8a), a designer could quickly generate a 3D model from a sketch or reference image. This allows stakeholders to interact with the concept from various angles, facilitating more informed feedback and effective brainstorming."}, {"title": "6.1.2 3D Design Discussion and Co-Creativity.", "content": "In early prototyping stages, Thing2Reality can transform 2D references into 3D models, enhancing design discussions. For instance, an interior designer could create 3D furniture models or room layouts from reference images (Figure 1). This enables clients to better visualize proposed designs and collaborate more effectively on decisions about materials, colors, and spatial arrangements, fostering co-creativity between designer and client."}, {"title": "6.2 Customization for XR Avatars, Virtual Try-ons, and 3D Emojis", "content": null}, {"title": "6.2.1 Avatar Decoration and Virtual Try-ons.", "content": "In XR meetings, personalizing avatars and environments is crucial for immersive experiences. Traditional GUI-based customization can be limiting. Thing2Reality enables users to quickly create or customize 3D objects for avatars to wear or interact with (Figure 8c), such as hats, bags, or coffee mugs. This fosters a more natural and engaging environment for various social gatherings in XR spaces such as coffee chats, family meetings, and gatherings around friends."}, {"title": "6.2.2 Personalized 3D Emojis and Memes.", "content": "While 2D emojis, GIFs, and memes are widely used in online communication, Thing2Reality allows users to transform online images or memes into the 3D version (Figure 8d). This feature enhances social gatherings, and VR live streams, moving beyond pre-designed 3D emojis like those in Meta Workroom. Different from Apple's newest Genmojis\u00b2 that enabled 2D text-to-emoji in text-based digital interactions, Thing2Reality enabled more intuitive expressions during vr meetings."}, {"title": "6.2.3 Magic Book.", "content": "Thing2Reality can be used to create a MagicBook effect [3], augmenting book contents to aid discussions and enhance storytelling. For example, in a children's book about dinosaurs, Thing2Reality could bring illustrations to life as interactive 3D models, allowing young readers to explore the creatures' anatomy and interact with them in a virtual environment. This approach creates a more engaging and immersive reading experience for long distance social play [22, 64]."}, {"title": "7 COMPARATIVE USER STUDY", "content": "We conducted a comparative user study to understand how Thing2Reality improves communication around the shared information artifacts (both physical and digital items), compared to 2D objects."}, {"title": "7.1 Study Design", "content": "The study employed a within-subject design that compared 3D (Thing2Reality) and 2D (Baseline) shared artifacts during impromptu discussions. Both conditions allowed for manipulation of objects (2D vs. 3D) upon confirmation of 2D interactive segmentation from data input sources. Two data sources were tested in all tasks: capturing things from a private screen with digital search, and from the video camera feed (DG1). The design of the 2D condition was inspired by ThingShare [19], with a difference that the 2D shared segmented artifacts could be manipulated in the 3D space using VR controllers. In the 3D condition, users can manipulate 3D shared Gaussian objects with VR controllers. The study was IRB-approved.\nRQ1: How do the differences between the generated 3D objects and 2D virtual replica affect users' understanding, exploration, and trust of an object when both can be similarly manipulated in a VR environment?\nRQ2: How do creating and using 3D Gaussian objects from digital and physical sources affect the effectiveness of discussions about the objects?\nFor dependent variables, we used a questionnaire to assess users' experiences with Thing2Reality in shared task spaces. Key measures included satisfaction with the format, comfort with object control, trust in object representation, ease of communication, and effectiveness in conveying and understanding object details. We evaluated these factors for both sharing (as the presenter) and understanding (as the inquirer) object information. The questionnaire also addressed the system's impact on clarifying complex concepts and improving overall subject understanding. Two authors collaboratively analyzed the qualitative data using Affinity Diagramming to identify main themes in users' responses and statistically analyzed the survey data using a paired-t test between 2D and 3D condition (a = 0.05)."}, {"title": "7.2 Participants", "content": "We recruited 12 participants (4 female, 8 male) as pairs from the university via email lists. Participants reported medium familiarity with VR (Median=3, IQR=1; scale 1-5). Their VR/AR experiences primarily involved gaming (N = 4), online browsing (N = 2), and research participation (N = 2). Two participants noted prior social meetup experience in AR/VR platforms. The in-person study took around 90 minutes per session. Each participant was compensated with $20 USD."}, {"title": "7.3 Study Setup and Procedure", "content": "Pairs of participants were invited to two rooms (see Figure 9) so that they could not see each other nor the physical objects in the other room. All the sessions were video-recorded.\nAfter completing each condition and context, participants responded to an intermediate survey about the drawbacks and benefits of the tried 2D or 3D format and the effectiveness of the system in completing the tasks using a 5-point Likert scale and open-ended questions. After completing all tasks, a final survey collected user preferences over different conditions and subjective feedback about the features."}, {"title": "7.4 Findings", "content": "7.4.1 Questionnaire Data (Figure 10). Overall", "Q1": 3, "D": "Median = 5"}, {"Q1": 2, "D": "Median = 4.5", "Q3": 2}, {"D": "Median = 4", "3D": "Median = 4", "Q4": 2}, {"D": "Median = 5", "1;3D": "Median = 5", "Q2": "Median = 5", "Q7": 2}, {"D": "Median = 2.5", "3D": "Median = 2", "Q8": 2}, {"D": "Median = 4", "3D": "Median = 5", "Q6": 2}, {"D": "Median = 4", "3D": "Median = 5", "Q5": 2}, {"D": "Median = 4", "2;3D": "Median = 5", "Q9": 2}, {"D": "Median = 4.5", "Q9": 3}, {"D": "Median = 4", "2D": "Median=4", "3D": "Median=5"}, {"2D": "Median=4", "3D": "Median=4", "Q12": 3, "D": "Median = 5"}, {"Q12": 2, "D": "Median = 4, IQR = 3). This could be attributed to the comprehensive 3D view"}]}