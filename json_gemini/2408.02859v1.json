{"title": "Multistain Pretraining for Slide Representation Learning in Pathology", "authors": ["Guillaume Jaume", "Anurag Vaidya", "Andrew Zhang", "Andrew H. Song", "Richard J. Chen", "Sharifa Sahai", "Dandan Mo", "Emilio Madrigal", "Long Phi Le", "Faisal Mahmood"], "abstract": "Developing self-supervised learning (SSL) models that can learn universal and transfer-able representations of H&E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the potential to advance critical tasks such as few-shot classification, slide retrieval, and patient stratification. Existing approaches for slide representation learning extend the principles of SSL from small images (e.g., 224x224 patches) to entire slides, usually by aligning two different augmentations (or views) of the slide. Yet the resulting representation remains constrained by the limited clinical and biological diversity of the views. Instead, we postulate that slides stained with multiple markers, such as immunohistochemistry, can be used as different views to form a rich task-agnostic training signal. To this end, we introduce MADELEINE, a multimodal pretraining strategy for slide representation learning. MADELEINE is trained with a dual global-local cross-stain alignment objective on large cohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney transplant samples (N=12,070 WSIs across four stains). We demonstrate the quality of slide representations learned by MADELEINE on various downstream evaluations, ranging from morphological and molecular classification to prognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple medical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) with multimodal pretraining is increasingly adopted in medical AI for constructing universal image representations that can be used for diagnosis, prognosis, and treatment response prediction [2, 16,46]. The core idea is to align an image (e.g., a histology region-of-interest of a tumor) with other corresponding modalities (e.g., the morphological text description of the tumor) into a shared latent space via contrastive learning or other similarity matching losses [67]. Intuitively, the richer the contrasting modality employed, the more detailed and nuanced the image representations can become, enabling better generalization and transferability to downstream tasks.\nIn computational pathology [76], multimodal pretraining has mostly focused on building visual-language models of small images [24,32], capitalizing on their success in computer vision [67,84]. However, the scale of whole-slide images (WSIs), often exceeding 100,000 \u00d7 100,000 pixels at 20\u00d7 magnification (0.5 \u00b5m/pixel), presents a significant challenge for adapting such techniques to pathology. To address this issue, most intra-modal and multimodal SSL methods focus on embedding small patches (e.g., 224 \u00d7 224-pixel images), which can then be aggregated using multiple instance learning (MIL) for downstream tasks [33,60,72]. Alternatively, the aggregation stage can also be pretrained with SSL to create a slide embedding from the patch embeddings [15,48,61,77]. The hierarchical construction from patches to patch embeddings to a slide embedding in a two-stage training pipeline enables self-supervised slide representation learning, without utilizing labels from pathologists or learning task-specific representations.\nHowever, most existing slide representation learning methods are intra-modal, thus limiting the richness and diversity of the training signal to learning visual invariances within the slide [15, 48]. Instead, we propose to leverage additional modalities that naturally form clinically and biologically relevant pairs suitable for pretraining. In this study, we hypothesize that WSIs stained with various markers, such as immunohistochemistry (IHC), can constitute a strong task-agnostic training signal for multimodal pre-training. Each stain can be seen as a different view of the H&E slide by highlighting spatially-resolved expression levels of relevant markers. In addition, unlike bulk gene expression data or text captions [37], H&E and other stains offer fine-grained morphological correspondences, which can be leveraged for en-hanced representational power.\nTo this end, we introduce MADELEINE, an SSL approach for multistain-guided slide representation learning. MADELEINE uses a multihead attention-based MIL [33,60] to encode pre-extracted patch em-beddings into a slide embedding. MADELEINE is pretrained on large collections of multistain tissue using a dual global-local cross-stain objective. The global objective, based on a symmetric contrastive loss [18], learns slide-level correspondences between the H&E slide and the other stains. This alignment guides the H&E embedding to encapsulate the global morphological composition of the tissue. The local objective, based on the Graph Optimal Transport framework [14, 64], learns patch-level correspondences between the H&E and the other stains, thereby enabling cross-stain matching of fine-grained morphological fea-tures. The resulting latent space (i) can encode all stains encountered during pretraining, as the same network is employed for encoding each stain, and (ii) can be used for diverse downstream applications, as the training signal and resulting model are task-agnostic.\nTo summarize, our contributions are (1) MADELEINE, a multimodal pretraining strategy for slide representation learning in computational pathology; (2) a large-scale demonstration of MADELEINE pre-training on two organs, breast (N=4,211 slides, five stains) and kidney (N=12,070 slides, four stains); and (3) extensive evaluation of MADELEINE across 21 tasks including morphological subtyping, molecular subtyping, survival prediction, and IHC quantification, tested in various scenarios for few-shot learning (using linear probing and prototyping) and model fine-tuning."}, {"title": "2 Related work", "content": ""}, {"title": "2.1 Vision representation learning", "content": "Training a Vision Transformer (ViTs) [21,80] with self-supervised learning (SSL) [12, 98] is now the preferred approach for learning task-agnostic image representations, such as based on visual-language models [4,39,52-54, 67, 73, 84, 95]. Visual-language models are usually based on contrastive learning [67], where the objective is to maximize the similarity between an image and its textual description, or as recently proposed, using Optimal Transport (OT) for fine-grained cross-modal alignment [19,44,65]. This approach is framed as a distribution matching objective, where the aim is to minimize the cost associated with a transport plan to match a token distribution of one modality to the other. Differently, multimodal training can leverage other spatial modalities, such as depth maps or bounding box annotations [8]. Drawing on these methodologies, our model, MADELEINE, integrates various high-resolution \"views\" of the same tissue stained with different markers, such as estrogen or progesterone receptor stainings."}, {"title": "2.2 Representation learning of histology images", "content": "SSL for learning representations of histology images is an active field with efforts in (i) developing models that can extract embeddings from small patches, typically 256\u00d7256 in size, and (ii) creating models designed to derive representations from entire WSIs, a task we denote as slide representation learning, and which constitutes the central contribution of our study.\nPatch representation learning Using SSL to encode histology patches has so far been the main focus with increasingly large models trained on larger datasets [7, 16, 23, 41, 45,81,86,94] (e.g., [11] used 3 billion patches from 423,000 slides). Simultaneously, vision-language models for histopathology have been developed using large datasets from sources such as social media and educational textbooks [24, 32,59]. Similar to MADELEINE, [31] proposed multimodal fine-tuning by aligning H&E and IHC patches. However, their method focuses on encoding patches, whereas MADELEINE focuses on encoding WSIS.\nSlide representation learning Developing pretrained encoders that extend beyond simple regions of interest to gigapixel whole slides is the next frontier in representation learning of histology images. Several works [6, 15, 40, 48, 61, 74, 75, 77,82,96] proposed hierarchical slide pretraining, first by transforming each patch into a patch embedding and then into a slide embedding (or region embedding). The slide encoder is typically trained using image augmentation techniques to define different views of the slide followed by contrastive or reconstruction objectives. Concurrent to this work, multimodal pretraining for slide representation learning was explored using bulk transcriptomics [37] and pathology reports [70]."}, {"title": "2.3 Beyond H&E staining", "content": "While H&E staining remains the gold standard in standard-of-care, it is often complemented with im-munohistochemistry (IHC) and special stains. Several works have been proposed for automatic IHC quantification [26, 43, 66, 79], often leveraging cell segmentation networks. Differently, IHC status can be predicted from H&E slides, such as for HER2 (human epidermal growth factor receptor 2) status prediction in invasive breast cancer or EGFR (epidermal growth factor receptor) prediction in lung cancer [3, 5, 20, 22, 27, 42, 62, 68, 71, 83]. Stain transfer, also known as virtual staining has also been proposed [9, 10, 29, 47, 56-58,69,97], in particular based on unpaired style transfer techniques [34,63,99]."}, {"title": "3 Methods", "content": "We introduce MADELEINE for multistain-guided slide representation learning (Fig. 1). MADELEINE is composed of (1) a stain-agnostic patch encoder that transforms histology patches into patch embeddings (Sec. 3.1 and 3.2), (2) a multihead attention-based MIL to learn a slide embedding (Sec. 3.3), and (3) a cross-stain alignment module based on a dual global-local objective (Sec. 3.4)."}, {"title": "3.1 Pre-processing and notation", "content": "Given a histology slide Xi \u2208 \\(R^{dx \\times dy \\times 3}\\) (H&E or another stain) for the ith patient, we follow the MIL paradigm [33,49,50,60,72], which consists of tessellating the slide into small patches, using a pretrained vision encoder to extract patch embeddings, and pooling the resulting patch embeddings into a slide embedding. We use sk to refer to the kth stain with \\({sk}\\)^{K}_{1} collectively referring to all non-H&E stains, e.g., in breast cases, sk \u2208 {ER, PR, HER2, KI67} with K = 4 denoting estrogen receptor, progesterone receptor, human epidermal growth factor receptor 2, and antigen kiel 67, respectively. We start by detecting and segmenting tissue regions to discard any background information. We use the CLAM toolbox [60] to detect H&E tissue and employ a deep learning-based tissue detector trained on mask annotations to detect non-H&E tissue. We then extract non-overlapping 256\u00d7256 patches on all stains."}, {"title": "3.2 Patch encoding", "content": "AS MADELEINE is trained on multiple stains, this renders most SSL models for patch feature extraction trained on H&E suboptimal [81,85]. Instead, we use CONCH, the image encoder of a visual-language model pretrained on 1M histology image-caption pairs curated from existing publications, which includes various histology stains [59]. We obtain the H&E patch embeddings \\(H^{HE} \\in R^{N_{HE} \\times d}\\), with \\(N_{HE}\\) and d = 512 denoting the number of H&E patches and the embedding dimension, respectively. The jth row entry, \\(H^{HE}_{j,:}\\), corresponds to the jth patch embedding. We perform the same procedure for other non-H&E stains \\({sk}\\)^{K}_{1} to obtain patch embeddings, i.e., \\(H^{k} \\in R^{N_{sk} \\times d}\\)."}, {"title": "3.3 Slide encoding", "content": "Pre-attention & stain encoding The patch embeddings \\(H^{HE}\\) are first passed through a pre-attention network, \\(f_{pre}: R^{d} \\rightarrow R^{d}\\), resulting in \\(\\hat{H}^{HE} \\in R^{d \\times d}\\). As the same pre-attention module is used for encoding all stains, having a stain-specific signature in the input can be beneficial. To do so, we define a learnable stain-specific encoding (denoted as SE, 32 dims) that is concatenated to each patch token before pre-attention, with d = d +32. This is inspired by modality-specific token augmentation schemes in multimodal fusion [35,55,75].\nMulti-head attention-based MIL We subsequently pass the resulting patch embeddings \\(\\hat{H}^{HE}\\) to a multihead (MH) attention network with M heads [33], resulting in an attention score \\(a^{m}_{i,j}\\) \u2208 [0,1] for each patch (Appendix Equation 2). Using multiple attention heads allows each head to focus on different yet morphologically important regions, similar to multi-head attention in Transformers [21, 80]. Once computed, we form head-specific slide embeddings by taking the weighted average of the transformed patch embeddings, i.e., \\(h^{HE}_{i,m} = \\sum_{j=1}^{N_{HE}} a^{m}_{i,j} H^{HE}_{j,:}\\). The resulting slide embedding \\(h^{HE}\\) is formed by concatenating the M slide embeddings and passing it through a post-attention network for dimension reduction, \\(f_{post}: R^{Md} \\rightarrow R^{d}\\),\n\\[h^{HE} = f_{post} ([h^{HE}_{i,1}, ..., h^{HE}_{i,M}]).\\]"}, {"title": "3.4 Loss", "content": "MADELEINE is trained using a combination of two cross-modal objectives: (1) a global objective to align slide embeddings of all stains in a shared latent space, and (2) a local objective for matching cross-stain patch embeddings. We optionally complement these two objectives with an intra-modal loss.\nCross-modal global alignment (INFONCE) We align the latent space induced by each stain through a global symmetric cross-modal contrastive learning objective, commonly referred to as INFONCE [18]. This is a widely employed representation learning formulation [67], especially in visual-language pretrain-ing. This objective enforces slide embeddings from the same case to be closer to each other while pushing away slide embeddings from different cases. Each term maximizes the dot-product similarity between embeddings from the same pair normalized (with Softmax) by negative pairs, which can be seen as other \"classes\".\nCross-modal local alignment (GOT) We also perform local alignment by matching the empirical dis-tributions of patch embeddings of all stains. Intuitively, as the local morphological structure is preserved"}, {"title": "5 Results", "content": "We showcase the performance of MADELEINE and MADELEINE-SE (i.e., with stain encoding) on few-shot classification (Sec. 5.1), and full classification (Sec. 5.2), that we complement by a series of ablations (Sec. 5.3). We benchmark MADELEINE against four MIL methods: single head ABMIL [33], Trans-MIL [72], IB-MIL [51] and ILRA [91]; four intra-modal SSL methods: INTRA, HIPT [15], GigaSSL [48], and GigaPath [94] (work concurrent to MADELEINE); and mean pooling (MEAN and GigaPath-MEAN constructed using the average of GigaPath patch embeddings)."}, {"title": "5.1 Few-shot results", "content": "Fig. 2 highlights the few-shot classification of MADELEINE against baselines (for each task: best MIL, best intra-modal, and MEAN). Detailed morphological subtyping performance is reported in Table 1, molecular subtyping in Supplementary Table 3, and kidney transplant rejection in Supplementary Table 7.\nMADELEINE vs. rest MADELEINE outperforms all baselines in 13/13 tasks, in some cases by a significant margin, e.g., +10.1% over INTRA in TCGA Breast (k=10, prototyping classification), or +9.0% over ABMIL in BWH Breast (k=5, linear probing). This performance is achieved using simple downstream models based on linear probing or prototyping classification, whereas MIL methods are trained from scratch for each task."}, {"title": "5.2 Full classification", "content": "Beyond few-shot classification, we assess MADELEINE in a supervised setting using 5-fold cross-validation, where we directly use MADELEINE embeddings for survival prediction and molecular subtyping.\nSurvival. We perform survival outcome prediction on TCGA Breast using a 5-fold site-stratified cross-validation. MADELEINE and other slide-level models (HIPT, GigaSSL and INTRA) are trained using a Cox proportional hazards objective from the slide embedding. All MIL models are trained with a survival NLL objective following prior work [17,38]. MADELEINE leads to the best survival prediction reaching 0.71 c-index outperforming all baselines (Fig. 3.b and Supplementary Table 5).\nMolecular subtyping. In addition, we use logistic regression to predict HER2 status in AIDPATH and KI67 status in AIDPATH and BCNB from H&E. In AIDPATH, MADELEINE-SE leads to +11.4% performance boost over the best MIL in HER2, and +1.8% in KI67 (Fig. 3.c and Supplementary Table 6."}, {"title": "5.3 Ablation", "content": "All ablations were run using MADELEINE pretrained on breast cancer slides, evaluated using AUC, and benchmarked using prototyping classification (k=25) on a set of three representative tasks: (1) BWH Breast subtyping, (2) TCGA PR classification, and (3) BCNB ER classification. In addition, we benchmark TCGA Breast survival using a Cox model trained using 5-fold cross-validation. Prototyping"}, {"title": "6 Conclusion", "content": "In this study, we present MADELEINE, a method exploring multimodal pretraining for slide representation learning based on multistain alignment. Our method utilizes extensive datasets of multistain slides, where we consider each stain as a unique perspective of the standard H&E-stained slide, each revealing different aspects of the tissue's biological state. We demonstrate that MADELEINE slide encoder outperforms multiple instance learning and intra-modal pretraining models in few-shot and full classification scenarios across various tasks, ranging from morphological and molecular subtyping to prognosis prediction to IHC quantification. Our method currently incorporates four to five different stains per sample, yet in clinical practice, more stains can be available to assist pathologists. This opens up promising avenues for expanding MADELEINE pretraining to include a broader range of stains. Furthermore, while our focus has been on multimodal pretraining with multiple stains, there exists a potential to explore other spatial modalities, such as those based on immunofluorescence, mass spectrometry, or spatial transcriptomics [36] for slide representation learning."}, {"title": "1.1 Contrastive loss", "content": "Formally, we define a batch of B cases, where each case includes K pairs \\((h^{HE}_{i}, h^{Sk}_{i}\\), where sk represents a non-H&E stain. The objective \\(L_{INFONCE}\\) is given by:\n\\[L_{INFONCE} = \\frac{1}{2K} \\sum_{k=1}^{K} \\left( \\frac{1}{B} \\sum_{b=1}^{B} - \\log \\frac{exp \\left( \\left( h^{HE}_{i} \\right)^T h^{Sk}_{i} / \\tau \\right)}{ \\sum_{b'=1}^{B} exp \\left( \\left( h^{HE}_{i} \\right)^T h^{HE}_{i} / \\tau \\right)} + \\frac{1}{B} \\sum_{b=1}^{B} - \\log \\frac{exp \\left( \\left( h^{Sk}_{i} \\right)^T h^{HE}_{i} / \\tau \\right)}{ \\sum_{b'=1}^{B} exp \\left( \\left( h^{Sk}_{i} \\right)^T h^{Sk}_{i} / \\tau \\right)} \\right)  (Supp. Eq. 1)\\]\nwhere the first and second terms represent the H&E-to-sk and sk-to-H&E contrastive loss, respectively. \\(\\tau\\) represents the Softmax temperature parameter."}, {"title": "1.2 Multi-head attention architecture", "content": "MADELEINE uses a multi-head attention-based Multiple Instance Learning (MIL) architecture. Before applying each head, the patch embeddings are passed through a common pre-attention network consisting of 3 layers with 512 hidden units, layer normalization, GELU activation, and 0.1 dropout. Each attention head comprises a gated-attention network, consisting of a 2-layer MLP with 512 hidden units with Softmax activation and 0.25 dropout. The attention score \\(a^{m}_{i,j}\\) for each patch derived from the mth attention head of M total heads are defined as:\n\\[a^{m}_{i,j} = \\frac{exp \\left( w^{m} \\left( tanh \\left( V^{m} H^{HE}_{j,:} \\right) \\odot sigm \\left( U^{m} H^{HE}_{j,:} \\right) \\right) \\right)}{\\sum_{l=1}^{NHE} exp \\left( w^{m} \\left( tanh \\left( V^{m} H^{HE}_{j,:} \\right) \\odot sigm \\left( U^{m} H^{HE}_{j,:} \\right) \\right) \\right)}    \\forall m.      (Supp. Eq. 2)\\]\nThe output of each head is concatenated, and a post-attention network consisting of two linear layers with 2048 and 512 units is applied to get a slide embedding for each stain."}, {"title": "1.3 Additional information on the GOT objective", "content": "Additional details of the Graph Optimal Transport objectives are as follows,\n1) Graph building: Each stain-specific graph is defined by instantiating 256 randomly sampled patches as nodes from the slide (sampling is done as each slide can have > 10,000 patches, making it computationally infeasible to calculate the complete optimal transport-based loss). Then, an edge is built between two nodes (i.e., two patches) if the cosine similarity between their patch embeddings is larger than a threshold. The threshold is dynamically constructed and is set at the lowest similarity value, increasing by 0.1 times the difference between the highest and lowest similarity values."}, {"title": "1.4 MADELEINE pretraining", "content": "We pretrained MADELEINE with AdamW optimizer and a batch size of 90 for 120 epochs. The learning rate is linearly ramped up during a 5-epoch warmup from 1e-9 to 1e-4. Then, we employed a cosine scheduler to reach the final learning rate of 1e-8 after 120 epochs. To increase training diversity and simplify batch processing, we sample a fixed and random subset of patches per slide, specifically 2048 patch embeddings. In slides with fewer patches, we perform random over-sampling. All training settings are summarized in Appendix Table 1."}, {"title": "1.5 Early stopping with rank analysis", "content": "Following [25], we use the rank as a measure of the quality of the underlying latent space learned during MADELEINE pretraining and save the model weights from the highest rank iteration (no models are saved during the first 20 epochs of training). We compute the rank as the entropy of the d (assuming d < n) L1-normalized singular values of the slide embedding matrix \\(H \\in R^{n \\times d}\\). Specifically, we have:\n\\[RankMe(H) = exp \\left( - \\sum_{k=1}^{d} p_{k} \\log (p_{k}) \\right),      (Supp. Eq. 5)\\]\n\\[p_{k} = \\frac{\\sigma_{k}(H)}{\\|\\sigma(H)\\|_{1} + \\epsilon},        (Supp. Eq. 6)\\]\nwhere \\(\\sigma_{k}\\) denotes the k-th singular of H (sorted from large to low), and \\(\\epsilon\\) is small constant set to 1e-7 for numerical stability."}, {"title": "8 Limitations", "content": "MADELEINE is a multimodal pre-training strategy for slide representation learning. It operates under the assumption that representation learning of H&E images can be guided by other stains (immuno-histochemistry and special stains). This premise is directly inspired by the standard practice in clinical settings, where H&E staining is routinely performed as the gold standard procedure, along with comple-mentary stains. Though this approach is principled, we highlight some limitations of our study and this methodology more broadly.\nData scaling Clinical practice is complex and ever evolving. Every year, new IHC and special stains become available, some of which are integrated in the workflow and can be used on a case-by-case basis. In breast cancer, our study focuses on four IHC stains (the most common ones), whereas many more can be employed, such as Epidermal Growth Factor Receptor (EGFR), P53, and E-Cadherin. As each stain offers a different view of a biomarker, increasing the number of stains would make the training signal richer and the resulting representation potentially better.\nLack of large public datasets Acrobat is the only large-scale public dataset with H&E and IHC stains. Therefore, without relying on proprietary data, such method cannot be scaled to more stains and other types of cancer. While the NADT-Prostate [89] cohort includes H&E and IHC, it remains limited by its size; for example, 14/18 stains provided have less than 100 examples, preventing efficient pre-training in prostate adenocarcinoma. In addition, TCGA includes known limitations such as site-specific biases [30] and demographic biases [78]. Despite these limitations, TCGA remains the largest public resource for cancer prognostication and survival analyses.\nModel scaling MADELEINE is trained using a combination of a global objective using contrastive learn-ing and a local objective using graph optimal transport (GOT). Using our current hardware (3\u00d7 3090 GPUs), we are limited by the maximum batch size for contrastive learning, even using efficient paral-lelization and bfloat16 quantization. In addition, computing GOT is computationally expensive, with significant memory requirements. Because of this constraint, we must use 256 patch embeddings (or to-kens) per stain for computing GOT. Scaling to more tokens would allow finer-grained matching between stains. Local alignment through GOT also requires morphological overlap between tissue sections used for different stains."}, {"title": "2) LWD:", "content": "Denoting \\(T \\in \\mathbb{R}^{+ N_{HE} \\times N_{sk}}\\) as the transport plan, we can minimize the Wasserstein Distance (WD) between distributions \\(p_{HE}\\) and \\(p_{sk}\\) by finding the optimal transport plan,\n\\[L_{WD} (p_{HE}, p_{Sk}) = \\min_{T} \\sum_{j} \\sum_{m} T_{j,m} C(\\nu^{HE}_{j}, \\nu^{sk}_{m}).     (Supp. Eq. 3)\\]"}, {"title": "3) LGWD:", "content": "such that \\(\\sum_{m} T_{j,m} = 1/N_{sk}, \\forall m\\) and \\(\\sum_{j} T_{j,m} = 1/N_{HE}, \\forall j\\). The cost between cross-modal embeddings \\(C(\\nu^{HE}_{j}, \\nu^{sk}_{m})\\) is computed with the cosine distance metric.\n  In addition to the node-matching with LWD, we also wish to match the graph topology via comparing the edge distance between stain-specific graphs. Denoting \\(T \\in \\mathbb{R}^{+ N_{HE} \\times N_{sk}}\\) as the transport plan as before,\n  \\[L_{GWD} (p_{HE}, p_{Sk}) = \\min_{T} \\sum_{j,j',m,m'} T_{j,m}T_{j',m'} C((\\nu^{HE}_{j}, \\nu^{HE}_{j'}), (\\nu^{sk}_{m}, \\nu^{sk}_{m'})).     (Supp. Eq. 4)\\]"}, {"title": "3) LGWD: part 2", "content": "such that \\(\\sum_{m} T_{j,m} = 1/N_{sk}, \\forall m\\) and \\(\\sum_{j} T_{j,m} = 1/N_{HE}, \\forall j\\). The cost between the pairs \\((\\nu^{HE}_{j}, \\nu^{HE}_{j'})\\) and \\((\\nu^{sk}_{m}, \\nu^{sk}_{m'})\\) is given as \\(C(E^{HE}, E^{Sk}) = \\|c(\\nu^{HE}_{j}, \\nu^{HE}_{j'}) - c(\\nu^{sk}_{m}, \\nu^{sk}_{m'})\\|\\), with \\(c(.,.)\\) representing the cosine similarity metric."}, {"title": "1.4 MADELEINE pretraining", "content": "We pretrained MADELEINE with AdamW optimizer and a batch size of 90 for 120 epochs. The learning rate is linearly ramped up during a 5-epoch warmup from 1e-9 to 1e-4. Then, we employed a cosine scheduler to reach the final learning rate of 1e-8 after 120 epochs. To increase training diversity and simplify batch processing, we sample a fixed and random subset of patches per slide, specifically 2048 patch embeddings. In slides with fewer patches, we perform random over-sampling. All training settings are summarized in Appendix Table 1."}, {"title": "1.5 Early stopping with rank analysis", "content": "Following [25], we use the rank as a measure of the quality of the underlying latent space learned during MADELEINE pretraining and save the model weights from the highest rank iteration (no models are saved during the first 20 epochs of training). We compute the rank as the entropy of the d (assuming d < n) L1-normalized singular values of the slide embedding matrix \\(H \\in R^{n \\times d}\\). Specifically, we have:\n\\[RankMe(H) = exp \\left( - \\sum_{k=1}^{d} p_{k} \\log (p_{k}) \\right), \\tag{Supp. Eq. 5}\\]\n\\[p_{k} = \\frac{\\sigma_{k}(H)}{\\|\\sigma(H)\\|_{1} + \\epsilon}, \\tag{Supp. Eq. 6}\\]\nwhere \\(\\sigma_{k}\\) denotes the k-th singular of H (sorted from large to low), and \\(\\epsilon\\) is small constant set to 1e-7 for numerical stability."}]}