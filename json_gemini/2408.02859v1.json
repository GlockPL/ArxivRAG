{"title": "Multistain Pretraining for Slide Representation Learning in Pathology", "authors": ["Guillaume Jaume", "Anurag Vaidya", "Andrew Zhang", "Andrew H. Song", "Richard J. Chen", "Sharifa Sahai", "Dandan Mo", "Emilio Madrigal", "Long Phi Le", "Faisal Mahmood"], "abstract": "Developing self-supervised learning (SSL) models that can learn universal and transferable representations of H&E gigapixel whole-slide images (WSIs) is becoming increasingly valuable in computational pathology. These models hold the potential to advance critical tasks such as few-shot classification, slide retrieval, and patient stratification. Existing approaches for slide representation learning extend the principles of SSL from small images (e.g., 224x224 patches) to entire slides, usually by aligning two different augmentations (or views) of the slide. Yet the resulting representation remains constrained by the limited clinical and biological diversity of the views. Instead, we postulate that slides stained with multiple markers, such as immunohistochemistry, can be used as different views to form a rich task-agnostic training signal. To this end, we introduce MADELEINE, a multimodal pretraining strategy for slide representation learning. MADELEINE is trained with a dual global-local cross-stain alignment objective on large cohorts of breast cancer samples (N=4,211 WSIs across five stains) and kidney transplant samples (N=12,070 WSIs across four stains). We demonstrate the quality of slide representations learned by MADELEINE on various downstream evaluations, ranging from morphological and molecular classification to prognostic prediction, comprising 21 tasks using 7,299 WSIs from multiple medical centers. Code is available at https://github.com/mahmoodlab/MADELEINE.", "sections": [{"title": "1 Introduction", "content": "Self-supervised learning (SSL) with multimodal pretraining is increasingly adopted in medical AI for constructing universal image representations that can be used for diagnosis, prognosis, and treatment response prediction [2, 16,46]. The core idea is to align an image (e.g., a histology region-of-interest of a tumor) with other corresponding modalities (e.g., the morphological text description of the tumor) into a shared latent space via contrastive learning or other similarity matching losses [67]. Intuitively, the richer the contrasting modality employed, the more detailed and nuanced the image representations can become, enabling better generalization and transferability to downstream tasks.\nIn computational pathology [76], multimodal pretraining has mostly focused on building visual-language models of small images [24,32], capitalizing on their success in computer vision [67,84]. However, the scale of whole-slide images (WSIs), often exceeding 100,000 \u00d7 100,000 pixels at 20\u00d7 magnification (0.5 \u00b5m/pixel), presents a significant challenge for adapting such techniques to pathology. To address this issue, most intra-modal and multimodal SSL methods focus on embedding small patches (e.g., 224 \u00d7 224-pixel images), which can then be aggregated using multiple instance learning (MIL) for downstream tasks [33,60,72]. Alternatively, the aggregation stage can also be pretrained with SSL to create a slide embedding from the patch embeddings [15,48,61,77]. The hierarchical construction from patches to patch embeddings to a slide embedding in a two-stage training pipeline enables self-supervised slide representation learning, without utilizing labels from pathologists or learning task-specific representations.\nHowever, most existing slide representation learning methods are intra-modal, thus limiting the richness and diversity of the training signal to learning visual invariances within the slide [15, 48]. Instead, we propose to leverage additional modalities that naturally form clinically and biologically relevant pairs suitable for pretraining. In this study, we hypothesize that WSIs stained with various markers, such as immunohistochemistry (IHC), can constitute a strong task-agnostic training signal for multimodal pre-training. Each stain can be seen as a different view of the H&E slide by highlighting spatially-resolved expression levels of relevant markers. In addition, unlike bulk gene expression data or text captions [37], H&E and other stains offer fine-grained morphological correspondences, which can be leveraged for en- hanced representational power.\nTo this end, we introduce MADELEINE, an SSL approach for multistain-guided slide representation learning. MADELEINE uses a multihead attention-based MIL [33,60] to encode pre-extracted patch em- beddings into a slide embedding. MADELEINE is pretrained on large collections of multistain tissue using a dual global-local cross-stain objective. The global objective, based on a symmetric contrastive loss [18], learns slide-level correspondences between the H&E slide and the other stains. This alignment guides the H&E embedding to encapsulate the global morphological composition of the tissue. The local objective, based on the Graph Optimal Transport framework [14, 64], learns patch-level correspondences between the H&E and the other stains, thereby enabling cross-stain matching of fine-grained morphological fea- tures. The resulting latent space (i) can encode all stains encountered during pretraining, as the same network is employed for encoding each stain, and (ii) can be used for diverse downstream applications, as the training signal and resulting model are task-agnostic.\nTo summarize, our contributions are (1) MADELEINE, a multimodal pretraining strategy for slide representation learning in computational pathology; (2) a large-scale demonstration of MADELEINE pre- training on two organs, breast (N=4,211 slides, five stains) and kidney (N=12,070 slides, four stains); and (3) extensive evaluation of MADELEINE across 21 tasks including morphological subtyping, molecular subtyping, survival prediction, and IHC quantification, tested in various scenarios for few-shot learning (using linear probing and prototyping) and model fine-tuning."}, {"title": "2 Related work", "content": "2.1 Vision representation learning\nTraining a Vision Transformer (ViTs) [21,80] with self-supervised learning (SSL) [12, 98] is now the preferred approach for learning task-agnostic image representations, such as based on visual-language models [4,39,52\u201354, 67, 73, 84, 95]. Visual-language models are usually based on contrastive learning [67], where the objective is to maximize the similarity between an image and its textual description, or as recently proposed, using Optimal Transport (OT) for fine-grained cross-modal alignment [19,44,65]. This approach is framed as a distribution matching objective, where the aim is to minimize the cost associated with a transport plan to match a token distribution of one modality to the other. Differently, multimodal training can leverage other spatial modalities, such as depth maps or bounding box annotations [8]. Drawing on these methodologies, our model, MADELEINE, integrates various high-resolution \"views\" of the same tissue stained with different markers, such as estrogen or progesterone receptor stainings.\n2.2 Representation learning of histology images\nSSL for learning representations of histology images is an active field with efforts in (i) developing models that can extract embeddings from small patches, typically 256\u00d7256 in size, and (ii) creating models designed to derive representations from entire WSIs, a task we denote as slide representation learning, and which constitutes the central contribution of our study.\nPatch representation learning Using SSL to encode histology patches has so far been the main focus with increasingly large models trained on larger datasets [7, 16, 23, 41, 45,81,86,94] (e.g., [11] used 3 billion patches from 423,000 slides). Simultaneously, vision-language models for histopathology have been developed using large datasets from sources such as social media and educational textbooks [24, 32,59]. Similar to MADELEINE, [31] proposed multimodal fine-tuning by aligning H&E and IHC patches. However, their method focuses on encoding patches, whereas MADELEINE focuses on encoding WSIS.\nSlide representation learning Developing pretrained encoders that extend beyond simple regions of interest to gigapixel whole slides is the next frontier in representation learning of histology images. Several works [6, 15, 40, 48, 61, 74, 75, 77,82,96] proposed hierarchical slide pretraining, first by transforming each patch into a patch embedding and then into a slide embedding (or region embedding). The slide encoder is typically trained using image augmentation techniques to define different views of the slide followed by contrastive or reconstruction objectives. Concurrent to this work, multimodal pretraining for slide representation learning was explored using bulk transcriptomics [37] and pathology reports [70]."}, {"title": "3 Methods", "content": "We introduce MADELEINE for multistain-guided slide representation learning (Fig. 1). MADELEINE is composed of (1) a stain-agnostic patch encoder that transforms histology patches into patch embeddings (Sec. 3.1 and 3.2), (2) a multihead attention-based MIL to learn a slide embedding (Sec. 3.3), and (3) a cross-stain alignment module based on a dual global-local objective (Sec. 3.4)."}, {"title": "3.1 Pre-processing and notation", "content": "Given a histology slide Xi \u2208 Rdxxdy\u00d73 (H&E or another stain) for the ith patient, we follow the MIL paradigm [33,49,50,60,72], which consists of tessellating the slide into small patches, using a pretrained vision encoder to extract patch embeddings, and pooling the resulting patch embeddings into a slide embedding. We use sk to refer to the kth stain with {sk}Kk=1 collectively referring to all non-H&E stains, e.g., in breast cases, sk \u2208 {ER, PR, HER2, KI67} with K = 4 denoting estrogen receptor, progesterone receptor, human epidermal growth factor receptor 2, and antigen kiel 67, respectively. We start by detecting and segmenting tissue regions to discard any background information. We use the CLAM toolbox [60] to detect H&E tissue and employ a deep learning-based tissue detector trained on mask annotations to detect non-H&E tissue. We then extract non-overlapping 256\u00d7256 patches on all stains."}, {"title": "3.2 Patch encoding", "content": "AS MADELEINE is trained on multiple stains, this renders most SSL models for patch feature extraction trained on H&E suboptimal [81,85]. Instead, we use CONCH, the image encoder of a visual-language model pretrained on 1M histology image-caption pairs curated from existing publications, which includes various histology stains [59]. We obtain the H&E patch embeddings HHE \u2208 RNHE\u00d7d, with NHE and d = 512 denoting the number of H&E patches and the embedding dimension, respectively. The jth row entry, HHEj, corresponds to the jth patch embedding. We perform the same procedure for other non-H&E stains {sk}Kk=1 to obtain patch embeddings, i.e., Hk \u2208 RNsk\u00d7d."}, {"title": "3.3 Slide encoding", "content": "Pre-attention & stain encoding The patch embeddings HHE are first passed through a pre-attention network, fpre : Rd \u2192 Rd, resulting in HHE \u2208 Rd\u00d7d. As the same pre-attention module is used for encoding all stains, having a stain-specific signature in the input can be beneficial. To do so, we define a learnable stain-specific encoding (denoted as SE, 32 dims) that is concatenated to each patch token before pre-attention, with d = d +32. This is inspired by modality-specific token augmentation schemes in multimodal fusion [35,55,75].\nMulti-head attention-based MIL We subsequently pass the resulting patch embeddings HHE to a multihead (MH) attention network with M heads [33], resulting in an attention score ami,j \u2208 [0,1] for each patch (Appendix Equation 2). Using multiple attention heads allows each head to focus on different yet morphologically important regions, similar to multi-head attention in Transformers [21, 80]. Once computed, we form head-specific slide embeddings by taking the weighted average of the transformed patch embeddings, i.e., hHEi,m = \u2211Nj=1 am i,jHHEj. The resulting slide embedding hHE is formed by concatenating the M slide embeddings and passing it through a post-attention network for dimension reduction, fpost : RMd \u2192 Rd,\nhHE = fpost ([hHEi,1,...,hHEi,M])."}, {"title": "3.4 Loss", "content": "MADELEINE is trained using a combination of two cross-modal objectives: (1) a global objective to align slide embeddings of all stains in a shared latent space, and (2) a local objective for matching cross-stain patch embeddings. We optionally complement these two objectives with an intra-modal loss.\nCross-modal global alignment (INFONCE) We align the latent space induced by each stain through a global symmetric cross-modal contrastive learning objective, commonly referred to as INFONCE [18]. This is a widely employed representation learning formulation [67], especially in visual-language pretrain- ing. This objective enforces slide embeddings from the same case to be closer to each other while pushing away slide embeddings from different cases. Each term maximizes the dot-product similarity between embeddings from the same pair normalized (with Softmax) by negative pairs, which can be seen as other \"classes\".\nCross-modal local alignment (GOT) We also perform local alignment by matching the empirical dis- tributions of patch embeddings of all stains. Intuitively, as the local morphological structure is preserved"}, {"title": "4 Study design", "content": "To assess the representative power of MADELEINE pretraining, we design two distinct scenarios: (1) MADELEINE pretraining on breast cancer cases (Sec. 4.1) and (2) MADELEINE pretraining on kidney transplant cases (Sec. 4.2). We then perform downstream evaluations based on public and private cohorts (Sec. 4.3). The evaluation was designed to encompass the variability of tasks found in pathology. We emphasize that MADELEINE pretraining does not involve datasets used for downstream tasks, precluding any data leakage. A detailed description is provided in Supplementary 2.\n4.1 Breast\nAcrobat (multi-stain, pretraining) We pretrain MADELEINE using data from the Automatic reg- istration of breast cancer tissue MICCAI challenge (Acrobat) [87, 88]. Acrobat is a multi-stain dataset comprising 4,211 WSIs from 1,153 primary breast cancer cases. Every case includes an H&E-stained WSI, along with one to four WSIs of tissue from the same tumor that have been stained with immuno- histochemistry, either ER (N=844 WSIs), PR (N=837), HER2 (N=534), or KI67 (N=843), such that K=4. The entirety of Acrobat was used for pretraining, with all slides processed at 10\u00d7 magnification.\nTCGA Breast (H&E, downstream) We use the public TCGA Breast cohort for (1) morphological subtyping (N=1,041) into invasive ductal carcinoma (IDC) and invasive lobular carcinoma (ILC); (2) binary molecular subtyping for predicting ER status (N=996), PR status (N=993), and HER2 status (N=693), and (3) survival prediction (N=1,049).\nBCNB (H&E, downstream) We use the public BCNB cohort [93] for binary molecular subtyping by predicting ER, PR, HER2 and KI67 status (N=1,058)."}, {"title": "4.2 Kidney", "content": "BWH Kidney (multi-stain, pretraining) We collected an in-house renal transplant cohort com- prising kidney biopsies from 1,069 renal transplant cases. Each case includes one to three tissue blocks, where each block consists of one to two H&E-stained (N=4,638) and one to two periodic acid-Schiff (PAS) (N=4,630) slides, one Jones-stained slide (N=2,326) and one Trichrome-stained slide (N=2,328), such that K = 3. In total, each case includes 6 to 18 slides. We hold out 20% of this cohort (210 cases, 1,852 slides across all stains, out of which 463 are H&E slides) as an independent test set and used the rest for MADELEINE pretraining. Slides were processed at 20x.\nRenal allograft rejection (downstream) We use H&E slides of the held-out cohort (N=463) to screen for Antibody-mediated rejection (AMR, 2 class) and quantify Interstitial Fibrosis and Tubular Atrophy (IFTA, 3 classes) (N=210 cases, N=1,852 WSIs across all stains). As each case includes several H&E slides, we define two sub-tasks: \"single-slide\" prediction, where we use a single slide per case (N=463 H&E slides), and \"all-slides\" prediction, where we use all available slides per case (N=305 H&E slides)."}, {"title": "4.3 Evaluation framework", "content": "Few-shot classification Following the standard practice in SSL evaluation [12, 16,98], we benchmark MADELEINE and baselines with k-shot classification (k = 1, 5, 10, 25 examples per class) using (1) linear probing and (2) prototyping. All experiments are repeated ten times by randomly sampling k examples per class. Linear probing was conducted without hyper-parameter search using default parameters of the sklearn package.\nSurvival prediction We assess MADELEINE in survival outcome prediction, where slide embeddings are passed to a Cox proportional hazards loss predicting survival. Following prior work, MIL models are trained using survival negative log-likelihood (NLL) loss [17]. We use site- and survival-stratified five-fold cross-validation evaluated with concordance-index (c-index) [38].\nFine-tuning We assess the performance of MADELEINE encoder for downstream tasks when fine-tuned, compared to when trained from scratch. Evaluations follow a 5-fold label-stratified train-test strategy."}, {"title": "5 Results", "content": "We showcase the performance of MADELEINE and MADELEINE-SE (i.e., with stain encoding) on few-shot classification (Sec. 5.1), and full classification (Sec. 5.2), that we complement by a series of ablations (Sec. 5.3). We benchmark MADELEINE against four MIL methods: single head ABMIL [33], Trans- MIL [72], IB-MIL [51] and ILRA [91]; four intra-modal SSL methods: INTRA, HIPT [15], GigaSSL [48], and GigaPath [94] (work concurrent to MADELEINE); and mean pooling (MEAN and GigaPath-MEAN constructed using the average of GigaPath patch embeddings).\n5.1 Few-shot results\nFig. 2 highlights the few-shot classification of MADELEINE against baselines (for each task: best MIL, best intra-modal, and MEAN). Detailed morphological subtyping performance is reported in Table 1, molecular subtyping in Supplementary Table 3, and kidney transplant rejection in Supplementary Table 7.\nMADELEINE vs. rest MADELEINE outperforms all baselines in 13/13 tasks, in some cases by a significant margin, e.g., +10.1% over INTRA in TCGA Breast (k=10, prototyping classification), or +9.0% over ABMIL in BWH Breast (k=5, linear probing). This performance is achieved using simple downstream models based on linear probing or prototyping classification, whereas MIL methods are trained from scratch for each task."}, {"title": "5.2 Full classification", "content": "Beyond few-shot classification, we assess MADELEINE in a supervised setting using 5-fold cross-validation, where we directly use MADELEINE embeddings for survival prediction and molecular subtyping.\nSurvival. We perform survival outcome prediction on TCGA Breast using a 5-fold site-stratified cross- validation. MADELEINE and other slide-level models (HIPT, GigaSSL and INTRA) are trained using a Cox proportional hazards objective from the slide embedding. All MIL models are trained with a survival NLL objective following prior work [17,38]. MADELEINE leads to the best survival prediction reaching 0.71 c-index outperforming all baselines (Fig. 3.b and Supplementary Table 5).\nMolecular subtyping. In addition, we use logistic regression to predict HER2 status in AIDPATH and KI67 status in AIDPATH and BCNB from H&E. In AIDPATH, MADELEINE-SE leads to +11.4% performance boost over the best MIL in HER2, and +1.8% in KI67 (Fig. 3.c and Supplementary Table 6."}, {"title": "5.3 Ablation", "content": "All ablations were run using MADELEINE pretrained on breast cancer slides, evaluated using AUC, and benchmarked using prototyping classification (k=25) on a set of three representative tasks: (1) BWH Breast subtyping, (2) TCGA PR classification, and (3) BCNB ER classification. In addition, we benchmark TCGA Breast survival using a Cox model trained using 5-fold cross-validation. Prototyping"}, {"title": "5.4 MADELEINE attention visualization", "content": "By visualizing head-specific attention weights, we can gain insights into the internal behavior of MADELEINE (Fig. 4). We show that different heads learn to focus on morphologically distinct regions, e.g., Head-3 focuses on tumor while Head-4 focuses on non-tumor stroma. This is a remarkable finding as MADELEINE was not given any morphological labels like tumor grade or subtype during training. Additional example heatmaps are provided in Supplementary 7."}, {"title": "6 Conclusion", "content": "In this study, we present MADELEINE, a method exploring multimodal pretraining for slide representation learning based on multistain alignment. Our method utilizes extensive datasets of multistain slides, where we consider each stain as a unique perspective of the standard H&E-stained slide, each revealing different aspects of the tissue's biological state. We demonstrate that MADELEINE slide encoder outperforms multiple instance learning and intra-modal pretraining models in few-shot and full classification scenarios across various tasks, ranging from morphological and molecular subtyping to prognosis prediction to IHC quantification. Our method currently incorporates four to five different stains per sample, yet in clinical practice, more stains can be available to assist pathologists. This opens up promising avenues for expanding MADELEINE pretraining to include a broader range of stains. Furthermore, while our focus has been on multimodal pretraining with multiple stains, there exists a potential to explore other spatial modalities, such as those based on immunofluorescence, mass spectrometry, or spatial transcriptomics [36] for slide representation learning."}, {"title": "8 Limitations", "content": "MADELEINE is a multimodal pre-training strategy for slide representation learning. It operates under the assumption that representation learning of H&E images can be guided by other stains (immuno- histochemistry and special stains). This premise is directly inspired by the standard practice in clinical settings, where H&E staining is routinely performed as the gold standard procedure, along with comple- mentary stains. Though this approach is principled, we highlight some limitations of our study and this methodology more broadly.\nData scaling Clinical practice is complex and ever evolving. Every year, new IHC and special stains become available, some of which are integrated in the workflow and can be used on a case-by-case basis. In breast cancer, our study focuses on four IHC stains (the most common ones), whereas many more can be employed, such as Epidermal Growth Factor Receptor (EGFR), P53, and E-Cadherin. As each stain offers a different view of a biomarker, increasing the number of stains would make the training signal richer and the resulting representation potentially better.\nLack of large public datasets Acrobat is the only large-scale public dataset with H&E and IHC stains. Therefore, without relying on proprietary data, such method cannot be scaled to more stains and other types of cancer. While the NADT-Prostate [89] cohort includes H&E and IHC, it remains limited by its size; for example, 14/18 stains provided have less than 100 examples, preventing efficient pre-training in prostate adenocarcinoma. In addition, TCGA includes known limitations such as site-specific biases [30] and demographic biases [78]. Despite these limitations, TCGA remains the largest public resource for cancer prognostication and survival analyses.\nModel scaling MADELEINE is trained using a combination of a global objective using contrastive learn- ing and a local objective using graph optimal transport (GOT). Using our current hardware (3\u00d7 3090 GPUs), we are limited by the maximum batch size for contrastive learning, even using efficient paral- lelization and bfloat16 quantization. In addition, computing GOT is computationally expensive, with significant memory requirements. Because of this constraint, we must use 256 patch embeddings (or to- kens) per stain for computing GOT. Scaling to more tokens would allow finer-grained matching between stains. Local alignment through GOT also requires morphological overlap between tissue sections used for different stains."}]}