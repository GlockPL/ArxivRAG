{"title": "A Survey: Spatiotemporal Consistency in Video Generation", "authors": ["Zhiyu Yin", "Kehai Chen", "Xuefeng Bai", "Ruili Jiang", "Juntao Li", "Hongdong Li", "Jin Liu", "Yang Xiang", "Jun Yu", "Min Zhang"], "abstract": "Video generation, by leveraging a dynamic visual generation method, pushes the boundaries of Artificial Intelligence Generated Content (AIGC). Video generation presents unique challenges beyond static image generation, requiring both high-quality individual frames and temporal coherence to maintain consistency across the spatiotemporal sequence. Recent works have aimed at addressing the spatiotemporal consistency issue in video generation, while few literature review has been organized from this perspective. This gap hinders a deeper understanding of the underlying mechanisms for high-quality video generation. In this survey, we systematically review the recent advances in video generation, covering five key aspects: foundation models, information representations, generation schemes, post-processing techniques, and evaluation metrics. We particularly focus on their contributions to maintaining spatiotemporal consistency. Finally, we discuss the future directions and challenges in this field, hoping to inspire further efforts to advance the development of video generation.", "sections": [{"title": "1 Introduction", "content": "Artificial Intelligence Generated Content (AIGC) [Yang et al., 2023a; Li et al., 2024; Liu et al., 2024] has become a mainstream application of AI, leveraging algorithms and models to create new content that meets user needs, greatly impacting people's production and daily life. Among them, video, as an important form of visual content, rapidly displays a series of images or frames in succession, creating the perception of motion. It offers a rich and dynamic medium for expressing and transmitting information. In recent years, there have been transformative advancements in video generation fields [Yang et al., 2023a; Li et al., 2024; Liu et al., 2024; Melnik et al., 2024; Xiong et al., 2024]. Major AI institutions and companies have invested substantial resources into developing video generation products that meet basic level, such as Runway's Gen series 123 [Esser et al., 2023], Google DeepMind's Veo45 and OpenAI's Sora6. Video can be viewed as a sequence of images [Ho et al., 2022; Hong et al., 2022], with each image referred to as a video frame. These frames are arranged in a sequence that maintains the continuity in time. Therefore, the video generation task can be viewed as modeling a sequence of frames with temporal relationships. In this task, researchers mainly focus on two types of consistency: spatial consistency and temporal consistency [Tran et al., 2015; Zhou et al., 2019; Yang et al., 2023a; Bar-Tal et al., 2024; Li et al., 2024; Liu et al., 2024; Xiong et al., 2024]. Spatial consistency concerns the details and elements within each frame (such as the color, shape, position of objects, etc.), which should remain consistent across frames to prevent spatial distortions, video flickering, and other issues in the generation process. Temporal consistency, on the other hand, studies the changes between consecutive frames. The changes in elements such as objects, scenes, lighting and motion should be smooth and coherent, without abrupt jumps or unnatural variations. In this survey, we comprehensively review the works that aimed to maintain spatiotemporal consistency in various aspects of video generation, including foundation mod-els [Melnik et al., 2024], information representations [Yang et al., 2023a], generation schemes [Liu et al., 2024], post-processing techniques [Liu et al., 2024], and evaluation metrics [Li et al., 2024]. Specifically, the foundation models ensure the spatiotemporal consistency of video generation from a mathematical perspective. The information representations utilize extraction methods to obtain an efficient spatiotemporal representation. The generation schemes, combining video characteristics with foundation models and representations, allow for the creation of high-quality, coherent videos. Post-processing techniques optimize along the spatial or temporal dimensions to enhance the quality and improve visual effects of the generated videos. Evaluation metrics guide and assess the generated content from different perspectives.\nContributions: The contributions of this survey are primarily in three aspects. (1) We review the research progress in video generation from the perspective of spatiotemporal consistency, which is novel compared to existing video generation surveys [Li et al., 2024; Liu et al., 2024; Xiong et al., 2024]. (2) We summarize current advances in video generation and introduces their contributions to maintaining spatiotemporal consistency. (3) We discuss future promising directions and challenges in this field with respect to spatiotemporal consistency. Relevant reference works are listed in Appendix Table 1. We hope that this survey will contribute to the development of advanced video generation technologies in the future.\nSections Structure: In Section 2, we summarize the principles and applications of four foundation generation models. In Section 3, we introduce several effective visual information representation methods. In Section 4, we review four video generation schemes based on video generation concepts. In Section 5, we present commonly used post-processing techniques that enhance video performance. In Section 6, we outline various video evaluation metrics. In Section 7, we discuss the potential future directions and ongoing challenges in this field. In Section 8, we provide a summary of our work and express our aspirations for the advancement of future research."}, {"title": "2 Foundation Models", "content": "In this section, we summarize the foundation models related to video generation, including Generative Adversarial Network Model (GAN), Autoregressive Model, Diffusion Model, and Mask Model. For each model, we briefly present its basic mathematical principles and provide an explanation of how to maintain spatiotemporal consistency."}, {"title": "2.1 Generative Adversarial Network Model (GAN)", "content": "GAN is an unsupervised neural network model that typically consists of a generator and a discriminator [Tulyakov et al., 2018; Skorokhodov et al., 2022]. The generator is responsible for producing data that is difficult to distinguish from real data, while the discriminator's role is to differentiate between real and generated data. The model is trained through a zero-sum game between these two components to generate high-quality data. The target optimization loss function between the generator G and the discriminator D can be formulated as:\nmin(max(V(G, D))),\nwhere V (G, D) is equal to\nEx~Pdata(x) [logD(x)] + Ez~pz(z)[log(1 \u2013 D(G(z)))].\nIn this setup, real data x is sampled from the data distribution Pdata(x), and random data z is sampled from a prior distribution pz(z) (usually a Gaussian distribution) and then input into the generator to produce generated data. The adversarial loss function [Tulyakov et al., 2018] aims to continuously enhance the capabilities of the generator and discriminator through adversarial training, allowing the generator to produce data that closely resembles real data.\nTo ensure the spatiotemporal consistency of the generated video, Saxena and Cao [2019] proposed a Deep GAN (D-GAN) that learns spatiotemporal features for more accurate spatiotemporal prediction. Some researchers have improved the discriminator and loss functions [Zhang et al., 2022], optimizing temporal modeling while considering the quality of each frame."}, {"title": "2.2 Autoregressive Model", "content": "According to the previous description, video data can be regarded as a sequence of frames. Based on the mathematical definition of autoregressive models, the generation of the current frame is conditioned on the preceding frames. [Hong et al., 2022; Xiong et al., 2024].\nN\np(x) = [P(xi|X0, X1, X2, ..., Xi\u22121; 0),\n2=1\nwhere p(xixo, X1,X2, ..., Xi\u22121;0) represents the condi- tional probability of generating the current frame based on the previous frames, and 0 denotes the model parameters. The optimization objective is to minimize the negative log-likelihood (NLL) loss, which is given as:\nN\nL(0) = \u2211log(p(xi|X0, X1, X2, ..., Xi\u22121; 0)).\ni=1\nThe autoregressive model [Hong et al., 2022; Wu et al., 2022b; Li et al., 2024; Xiong et al., 2024] ensures that generating each frame with high quality, while also captures the dependencies between frames, thus maintaining the spatiotemporal consistency of the generated video."}, {"title": "2.3 Diffusion Model", "content": "The diffusion model was initially used in the image generation domain and later transferred into video generation field [Yang et al., 2023a; Melnik et al., 2024]. The diffusion model defines a Markov chain of diffusion steps, where random noise is gradually added to the original data until it becomes pure Gaussian noise. The formulation is defined as:\nq(xt|xt\u22121) = N(xt; \u221a\u221a1 \u2013 Btxt\u22121, \u1e9eti\nT\nq(x1:Txo) = [q(xt|xt\u22121),\n2=1\nwhere \u1e9et \u2208 (0,1) controls the variance of the Gaussian distribution, I is the identity matrix.\nThen, the model learns the reverse diffusion process to generate data, where the process progressively denoises to recover the original data distribution. The specific formula is as follows:\nPo(Xt-1|Xt) = N(xt\u22121; \u03bc\u03bf(xt, t), Eo(xt,t)),\nT\n\u0420\u04e9 (\u0445\u043e:\u0422) = \u041f\u0440\u043e(Xt\u22121|Xt),\ni=1\nwhere \u03bc\u03bf(xt, t), Do(xt, t) are the mean and variance estimated by a neural network.\nThis neural network is optimized by minimizing the distance between the real noise et and the predicted noise \u20ac\u03b8.\nLt(0) = Et~[1,t],xt,et || Et - Eo(xt, t)||2.\nSome methods have been proposed to enhance the spatiotemporal consistency of the generated videos. For example, the Latent Diffusion Model [He et al., 2022] maps the original data into a latent feature space, enabling a more efficient spatiotemporal representation. Moreover, noise is a crucial component in diffusion models, which determines the generated content. Choosing random noise can easily lead to suboptimal results. Blattmann et al. [2023] addressed this by aligning the diffusion model's upsamplers in the temporal domain, transforming them into temporally consistent super-resolution models. The noise estimation network (usually U-Net network) determines the model's denoising capability and generation quality. Peebles and Xie [2023] replaced the U-Net backbone with a transformer (DiT), showing impressive capabilities in generating high-quality video."}, {"title": "2.4 Mask Model", "content": "Mask models were initially designed for natural lan-guage modeling and later applied to image and video domains [Liu et al., 2024]. Given a time sequence (X0, X1, X2, ... Xi,...,xT), where xi can represent the basic element of language or visual data. The element of the sequence will be randomly masked, then the model is tasked with predicting the masked portions as accurately as possible to reconstruct the original sequence. By focusing on specific regions or components of the videos, the model shows efficacy in training.\nOne notable study was the Gupta team study [2022], where they proposed a variable masking ratio mask model for video prediction. In the inference stage, the model gradually reduces the masking rate according to the mask scheduling function, thereby iteratively refining the generation of all the marked elements. Yu et al. [2023] presented a condition video generation model using a multivariate mask. Hu and Ommer [2024] leveraged the mask strategy to bridge masked generation and non-autoregressive diffusion models as well as generative and discriminative tasks."}, {"title": "3 Information Representations", "content": "Video data typically have high-dimensional structures and contain a large amount of redundant information. Directly storing and processing this data can impose significant computational and storage burdens. Therefore, we need to extract deep spatiotemporal information from the original data and represent it in a low-dimensional form to facilitate model learning [Zhang et al., 2020; Yang et al., 2023a; Zhang et al., 2023b; Liu et al., 2024]. In this section, we summarize some commonly used methods for information representation."}, {"title": "3.1 Spatiotemporal Convolution", "content": "Convolution Network (CNN) [Tran et al., 2015; Liu et al., 2024] is a classic technique for extracting information from vision data. Compared to 2D image data, video data extends the time dimension. Therefore, traditional 2D CNN methods need to be modified to accommodate video information representation [Tran et al., 2015]. For instance, using a convolution kernel of size kh \u00d7 kw \u00d7 kd, where ka represents the time dimension, meaning that ka consecutive frames are treated as a group for convolution.\nTran et al. [2015] proposed that the standard CNN 3D network (C3D) can learn the joint spatial and temporal features of videos. Carreira and Zisserman [2017] accelerated the training by initializing 3D convolutional layers with pre-trained 2D CNN weights from large-scale image datasets like ImageNet, thereby improving video representation performance (I3D). Huang et al. [2021] proposed R (2+1) D, which decomposes the 3D convolution operation into two consecutive subconvolution blocks: a 2D spatial convolution and a 1D temporal convolution, each responsible for extracting spatial and temporal information, respectively."}, {"title": "3.2 Spatiotemporal Patch", "content": "In convolution operations, the filter extracts information from a spatiotemporal region of the size of the convolution kernel. Related works have adopted this idea and introduced the concept of patch [Melnik et al., 2024]. Spatial patch [Li et al., 2024], originally proposed for the image domain, involves dividing the input image into patches based on pixels or feature regions. Spatiotemporal patch [Liu et al., 2024] is improved specifically for video tasks, such as action recognition, event detection, etc. It divides the input data both spatially and temporally, enabling the representation of visual information in local spatiotemporal regions.\nPatch-based learning is a novel training technique [Wang et al., 2024] which trains on patches instead of full images reducing the computational burden. The Vision Transformer (ViT) [Girdhar et al., 2023] was the first to apply patches to process visual information, marking a milestone for Transformer [Vaswani et al., 2017] applied in the Computer Vision (CV) field. Patch can also serve as the basic unit in video generation. Meta AI introduced OmniMAE [Girdhar et al., 2023], which uses a patch-based masked autoencoding approach to predict both images and videos."}, {"title": "3.3 Self-Attention Mechanism", "content": "As Transformer has become the mainstream architecture, self-attention mechanism has become one of the effective strategies adopted by various models. There are mainly four commonly used self-attention mechanisms [Villegas et al., 2022; Liu et al., 2024]: spatial self-attention, temporal self-attention, spatiotemporal self-attention and causal self-attention.\n\u2022 Spatial Self-Attention: It is a classic patch-based self-attention mechanism that calculates the similarity between a query patch and other patches in the same frame [Girdhar et al., 2023].\n\u2022 Temporal Self-Attention: It focuses on the temporal dimension, computing the similarity between patches at the same position across a sequence of consecutive frames [Singer et al., 2022]."}, {"title": "3.4 Variational Autoencoder (VAE)", "content": "Autoencoders use unsupervised learning-based networks to achieve data extraction and representation. Among them, VAE [Yan et al., 2021; Hong et al., 2022; Song et al., 2024] is a mainstream type of autoencoder that introduces variational inference and optimization of latent space distribution. It includes two parts: the Encoder and the Decoder. The Encoder maps the raw data to a low-dimensional latent feature space, while the Decoder reconstructs the latent feature representation back to the original input.\nYan et al. [2021] introduced VideoGPT, which uses a Vector Quantized-VAE (VQ-VAE) based on 3D CNN and axial self-attention to learn discrete representations of videos. Polyak et al. [2024] proposed a temporal autoencoder (TAE), which maps raw image and video data to a spatiotemporal latent space. The model takes sampled noise and user prompts as inputs to generate latent output. In the technical analysis of the Sora model [Liu et al., 2024], some researchers believed that its ability to handle different forms of visual input lies in learning a unified latent representation. The video is first transformed into a low-dimensional latent space and then decomposed into a series of spatiotemporal patches. Using such latent patches helps the model better accommodate the diversity of inputs."}, {"title": "3.5 Visual Encoder", "content": "A visual encoder [Yin et al., 2023] is a commonly used network structure in the field of CV, designed to transform input images, videos, or other visual data into feature representations. Some works combine the previously introduced techniques with the visual encoder [Melnik et al., 2024], making it an important component of the model.\nCommon visual encoders include ViT and CLIP. ViT [Girdhar et al., 2023]is a Transformer-based encoder that divides images into patches to effectively capture the global dependencies of the image. CLIP [Ge et al., 2023; Zhang et al., 2023a] is a joint learning model of a visual encoder and a text encoder, which uses contrastive learning between images and text to map data into a shared feature space. It is suitable for cross-modal understanding and generation tasks. Moreover, Phenaki [Villegas et al., 2022] adopts the C-ViViT encoder architecture, which combines both spatial and causal attention mechanisms to effectively represent video data."}, {"title": "4 Generation Schemes", "content": "The generation schemes refer to the procedure of video generation given certain foundation models and information representation methods. In this section, we summarize four generation schemes designed on the basis of different conceptual approaches, which can be viewed in Figure 1: the decoupled scheme based on the idea of decomposition and composition, the hierarchical scheme based on the top-down hierarchical approach, the multi-stage scheme based on gradual quality enhancement, and the latent model scheme based on latent space."}, {"title": "4.1 Decoupled Scheme", "content": "In video generation, there are elements of both variability and invariability [Liu et al., 2024]. Invariability mainly refers to the objects or backgrounds in the video that remain almost consistent across several frames. Variability, on the other hand, refers to changes such as the motion trajectories, lighting and other dynamic changes. To leverage this characteristic, decoupled scheme [Liu et al., 2024] has been proposed for video generation. It sets up different modules to analyze and process elements of both variability and invariability independently, and then merge them to form a complete video.\nThe two-stream network [Simonyan and Zisserman, 2014] introduced the concepts of spatial content stream and temporal motion stream, which have shown good results in video understanding and action recognition. MoCoGAN and StyleGAN-V [Tulyakov et al., 2018; Skorokhodov et al., 2022]separated the processed latent features into content encode and motion trajectory, which are then passed through content mapping network and motion mapping network, respectively. The two parts are synthesized into final outputs by a synthesis network. Liu et al. [2023] combined diffusion models with a two-stream network (DSDN) to achieve alignment between the content domain and motion domain. It improved the consistency of content changes during video generation. This method is also effective in human video generation. Text2Performer [Jiang et al., 2023]introduced two novel designs: decomposed human representation and diffusion-based motion sampler, which together enable the generation of flexible and diverse human videos."}, {"title": "4.2 Hierarchical Scheme", "content": "In the field of video generation, some researchers have borrowed the idea of divide-and-conquer and proposed hierarchical architectures for video generation [Hong et al., 2022; Li et al., 2024]. First, the global model roughly outlines the storyline of the video, such as the key frame sequences. Then the local model focuses on the details, performing alignment, inpainting, and refinement.\nWu et al. [2022a] presented NUWA-Infinity, an autoregressive-over-autoregressive video generation architecture. In this architecture, the global autoregressive model considers the dependencies between patches, while the local autoregressive model focuses on the dependencies of visual tokens within each patch, enabling the generation of globally consistent and locally detailed high-quality videos. Yin et al. [2023] proposed NUWA-XL, which adopts a diffusion-over-diffusion architecture. The global diffusion model is first used to generate key frames, and then local diffusion is applied iteratively to complete intermediate frames, allowing the video length to increase exponentially. Skorokhodov et al. [2024] introduced a novel hierarchical strategy, Patch Diffusion Models (PDMs), which do not operate on full-resolution inputs but instead propagate context information from low-scale to high-scale patches in a hierarchical manner, ensuring global consistency."}, {"title": "4.3 Multi-Staged Scheme", "content": "Directly generating high-resolution, high-quality videos typically requires significant computational and time costs [Li et al., 2024]. Therefore, some researchers have proposed stage-wise schemes to alleviate this issue. In these schemes, video generation is a multi-staged process [Ho et al., 2022]. The first stage generates a set of low-quality initial frames, and subsequent stages gradually improve the video's quality, resolution, and frame rate based on these initial frames and user prompts.\nCascade model [Singer et al., 2022; Ge et al., 2023] is an effective application of stage-wise scheme. In this model, the base module generates a sparse and low-resolution frame sequence. Then, a series of refinement modules, such as super-resolution generation modules for spatial and temporal dimensions, are applied to enhance the resolution and frame rate of the output frames. Zhang et al. [2023a] proposed I2VGen-XL, a cascade model that decouples video semantic accuracy and exceptional quality to enhance performance. Specifically, the base stage uses a CLIP visual encoder to ensure high semantic alignment. Then, the refinement stage improves the video's resolution and enhances the spatiotemporal continuity and clarity of the video."}, {"title": "4.4 Latent Model Scheme", "content": "The current mainstream video generation architectures do not directly generate videos based on pixels or patches, but instead use information representation techniques (e.g., VAE) to map the original data to a latent feature space [Yan et al., 2021; He et al., 2022; Liu et al., 2024; Polyak et al., 2024]. This scheme generally consists of three parts: the encoder module, the decoder module, and the backbone network module. The encoder and decoder module apply the visual information representation techniques mentioned earlier to represent and reconstruct the video data. The backbone network module is often combined with models such as GAN models, autoregressive models, and diffusion models [He et al., 2022; Skorokhodov et al., 2022; Wu et al., 2022a; Skorokhodov et al., 2024] to process the latent features. Latent models are better at maintaining the spatiotemporal consistency of videos, especially in tasks involving long videos and complex scenes [Li et al., 2024]. By modeling in the latent space, they can effectively capture the dependencies between video frames and temporal features, thereby generating smoother and more coherent video content.\nFor conditional generation tasks, such as the Text-to-Video Generation (T2V) task, a control module will be added to the scheme [Zhang et al., 2023a]. It maps the text condition into a shared latent space with visual information by using a pre-trained language model like CLIP [Xiong et al., 2024]. This allows for the alignment of text semantics and visual content, enabling video content generation with better flexibility and controllability."}, {"title": "5 Post-processing Techniques", "content": "The post-processing techniques for video generation primarily aim to improve the quality of the generated video or modify its style and lighting to enhance its visual effects. This section summarizes some post-processing techniques, including frame interpolation, video super-resolution, video stabilization, deblurring, video stylization and relighting."}, {"title": "5.1 Frame Interpolation", "content": "Frame interpolation [Ge et al., 2023; Singer et al., 2022] is a common CV algorithm used to insert frames into a video to increase the frame rate and enhance the coherence and smoothness of the video. Common frame interpolation techniques include Optical Flow methods and Deep Learning methods [Ho et al., 2022].\nThe basic principle of the Optical Flow method is to predict the position of pixels in the next frame by analyzing the pixel changes between adjacent frames. Xue et al. [2019] proposed task-oriented flow, a motion representation, which is learned through joint training to adapt to specific tasks like frame interpolation and super-resolution.\nDeep Learning-based frame interpolation is typically based on CNNs or GANs. CNNs [Ho et al., 2022; Ge et al., 2023] learn the features of adjacent frames to generate high-quality interpolation frames. GANs [Zhang et al., 2022], on the other hand, use adversarial learning to continually optimize the generator's performance, reducing distortions and blurring, thus improving the interpolation results."}, {"title": "5.2 Video Super-Resolution", "content": "Super-resolution technology [Ho et al., 2022], also known as super-sampling, enhances the resolution of video frames using hardware or software methods, converting low-resolution images into high-resolution ones. In this field, super-resolution techniques are generally classified into two categories: spatial-based and temporal-based [Ho et al., 2022; Singer et al., 2022; Ge et al., 2023].\n\u2022 Spatial-based super-resolution (SSR). [Ho et al., 2022] This method focuses on enhancing the resolution of each frame in the video. It aims to improve the spatial detail in each frame by using techniques such as CNNs.\n\u2022 Temporal-based super-resolution (TSR). [Singer et al., 2022] This approach leverages the information across multiple frames over time to improve the resolution. Temporal-based super-resolution methods typically enhance the frame rate and detail by utilizing motion information and temporal coherence, thus providing smoother high-resolution outputs."}, {"title": "5.3 Video Stabilization", "content": "Video jittering is one of the common issues in video generation, referring to unstable frame variations in the video [Tulyakov et al., 2018; Skorokhodov et al., 2022]. It typically manifests as sudden displacements, rotations, or distortions, which disrupt the coherence between frames and reduce the overall video quality. As a result, video stabilization techniques are proposed to mitigate the jittering effects in videos.\nZhang et al. [2022] conducted a study on video generation across different time spans (short-range, medium-range, and long-range), finding noticeable periodic jittering in long-video generation from previous works. To address this, they introduced B-spline controlled interpolation and low-rank constraints, which help alleviate the jittering phenomenon. Yang et al. [2023b] combined various advanced techniques to improve video rendering and enhancement functions. The work employed motion estimation and compensation algorithms to identify and eliminate jitter in videos, thereby improving video stability."}, {"title": "5.4 Deblurring", "content": "Video deblurring [Liu et al., 2024] is a method to improve video quality and enhance video resolution, with the main goal of recovering clear images from blurred video frames. Video deblurring is widely utilized in high-resolution imaging applications [He et al., 2025], such as medical imaging, film generation, and other fields, to improve the viewing experience.\nZhou et al. [2019] proposed the Spatiotemporal Filter Adaptive Network (STFAN), which extracts blur features from the previous blurred and restored images, and aligns them with the current frame to remove the blur caused by changes in the feature space. He et al. [2025] proposed a domain-adaptive deblurring model approach, which extracts relatively clear regions as pseudo-clear images. The model then applies the deblurring technique in combination with the pseudo-clear images to generate blurred images, achieving domain adaptation effects for unseen domains."}, {"title": "5.5 Video Stylization", "content": "Generating stylized videos [Skorokhodov et al., 2022] involves using deep learning and CV techniques to transfer an artistic style to the video content, creating visually appealing effects. Stylized videos must maintain consistency in the style across each frame while ensuring temporal consistency throughout the video to avoid abrupt jumps or discontinuities in the visual flow.\nSong et al. [2024] proposed a unified framework for localized video style transfer. The work used a training-free video style transfer mechanism that operates mainly in the latent and attention layers, which reduces detail loss while ensuring content consistency. Ye et al. [2024] proposed a video stylization generation and transformation method called StyleMaster. This approach simultaneously considers global style and local textures, enhancing the consistency of the style. For video content, a gray-block control network was introduced to achieve content control."}, {"title": "5.6 Relighting", "content": "Video relighting [Liu et al., 2024] is also an emerging direction in the field of CV, especially in portrait videos. It involves performing 3D perception on portrait videos and transforming 2D facial features into a 3D relightable representation. This enables re-rendering portrait videos under different viewing angles and lighting conditions. The process also requires maintaining spatiotemporal consistency. Choi et al. [2023] developed a personalized video relighting network architecture that can effectively separate intrinsic appearance features, such as facial shape, from the source lighting. These features are then combined with target lighting to generate relighted images. Cai et al. [2024] proposed the first real-time 3D perception-based portrait video relighting method. Their approach uses a three-plane dual encoder to encode geometric materials and portrait lighting effects separately, and then a special network optimizes the temporal consistency of the video."}, {"title": "6 Evaluation Metrics", "content": "This section summarizes some evaluation metrics for video generation and divides them into three categories: generation quality assessment, video smooth assessment, and user subjective evaluation [Jiangkuo et al., 2024; Liu et al., 2024]. Among these, generation quality assessment focuses more on spatial consistency, smooth assessment emphasizes temporal consistency, and user evaluation centers on subjective user experience."}, {"title": "6.1 Generation Quality Assessment", "content": "PSNR (Peak Signal-to-Noise Ratio): PSNR [He et al., 2025] is a widely used metric that measures the quality of the generated image by comparing it with the original image. It is generally represented as the ratio of the maximum pixel value of the original image to the difference in pixel values between the original and generated images.\nSSIM (Structural Similarity Index): SSIM [Li et al., 2024] is an image quality metric that considers the changes in brightness, contrast, and structural information between the original and generated images.\nFr\u00e9chet Inception Distance (FID): FID [Blattmann et al., 2023] uses a network to extract abstract features of images and analyzes the distance between the generated and original images in feature space, reflecting the distance between the generated distribution and the original distribution.\nFVD (Fr\u00e9chet Video Distance): FVD [Bar-Tal et al., 2024] is an extension of FID in the video domain, replacing the feature extraction network with a video feature extraction network, evaluating the distance between the generated video and the original video.\nInception Score (IS): IS [Bar-Tal et al., 2024] measures the diversity and authenticity of generated images."}, {"title": "6.2 Video Smoothness Assessment", "content": "Temporal Consistency: Temporal consistency [Cai et al., 2024] assessment introduces Time Change Consistency (TCC) and Temporal Motion Consistency (TMC), which means maintaining the consistency of changes and motions (optical flow) between consecutive frames in the generated video, aligning with the corresponding true depth.\nFPS (Frames Per Second): FPS [Bar-Tal et al., 2024] measures the frame rate of the video. The generation process typically needs to meet certain frame rate requirements to achieve a smooth video."}, {"title": "6.3 User Subjective Evaluation", "content": "Mean Opinion Score (MOS): MOS [Li et al., 2024] is the arithmetic average of the user's subjective ratings for the generated video."}, {"title": "7 Future Directions and Challenges", "content": "Recently, with the growing user demand, video generation is increasingly moving towards more complex and diverse directions, bringing both opportunities and challenges.\nLong Video Generation. Extending the length of gen-erated videos is a growing trend in development [Li et al., 2024]. As the length increases, the videos will contain more redundant and variable visual information. For example, generating movie or game videos requires addressing consistency issues across long time spans, which brings significant challenges in terms of both computational time and space consumption.\nPersonalized Video Generation. Personalized genera-tion [Ye et al., 2024] needs to align with the user's specific needs or preferences, controlling the details of the generated video. This requires the models to better understand user prompts and generate more detailed content. It will bring more complicated issues of consistency.\nVideo Emotion Expression. For users, video is not only a medium for conveying information but also a way of expressing emotions [Liu et al., 2024]. Whether the generated video can truly convey human emotions and evoke emotional resonance from the audience will determine whether AIGC can genuinely replace humans in the generation task.\nVideo Generation Evaluation. Current evaluation metrics for video generation [Li et al., 2024] are mostly borrowed from the image field. They overlook the temporal information in videos and are unable to evaluate dynamic content. Therefore, a complete and comprehensive video evaluation system is urgently needed in this field."}, {"title": "8 Conclusions", "content": "Spatial and temporal consistency is one of the key challenges in the video generation process. In this survey", "following": 1}]}