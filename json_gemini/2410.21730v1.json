{"title": "Efficient Reprogramming of Memristive Crossbars for DNNs: Weight Sorting and Bit Stucking", "authors": ["Matheus Farias", "H. T. Kung"], "abstract": "We introduce a novel approach to reduce the number of times required for reprogramming memristors on bit-sliced compute-in-memory crossbars for deep neural networks (DNNs). Our idea addresses the limited non-volatile memory endurance, which restrict the number of times they can be reprogrammed. To reduce reprogramming demands, we employ two techniques: (1) we organize weights into sorted sections to schedule reprogramming of similar crossbars, maximizing memristor state reuse, and (2) we reprogram only a fraction of randomly selected memristors in low-order columns, leveraging their bit-level distribution and recognizing their relatively small impact on model accuracy. We evaluate our approach for state-of-the-art models on the ImageNet-1K dataset. We demonstrate a substantial reduction in crossbar reprogramming by 3.7x for ResNet-50 and 21x for ViT-Base, while maintaining model accuracy within a 1% margin.", "sections": [{"title": "I. INTRODUCTION", "content": "Resistive compute-in-memory (CIM) crossbars offer a promising computing architecture for deep neural networks (DNNs), using analog accelerators to achieve fast, power-efficient matrix multiplication. Central to this architecture are memristors, which enable simultaneous data storage and processing, overcoming the data movement limitations of traditional von Neumann systems [1]\u2013[7].\nDNN weights are encoded into memristor conductances by applying sequential voltage pulses, which significantly increases programming time. In conventional 1 transistor-1 memristor (1T1R) crossbar architectures, each memristor is programmed individually, with the transistor acting as a switch to isolate the target cell for precise conductance adjustment [8]\u2013[14].\nMemristors, based on non-volatile memory technologies [4], [5], [15], offer high retention but have limited endurance, degrading after a finite number of reprogramming cycles. In CIM architectures, where write operations are more frequent, endurance is especially critical; for instance, a 1024x1024 resistive RAM crossbar used for 32-bit multiplication may fail within minutes [16]. This issue is intensified when implementing large DNNs, as they require partitioned sections to fit onto relatively small, fixed-size crossbars. Consequently, each crossbar requires multiple reprogramming cycles to accommodate each partition, further straining endurance. We propose programming sorted weight sections to reduce reprogramming frequency, thereby extending the lifespan of memristor-based architectures without sacrificing efficiency.\nThe use of sorted sections minimizes memristor state mismatch during reprogramming by grouping crossbars of similar weights. To the best of our knowledge, this is the first implementation of sorted weight sectioning (SWS) [17] for crossbar reprogramming. Moreover, SWS enhances parallelization by distributing programming across multiple crossbars based on weight magnitude, thereby balancing the workload across threads and further optimizing reprogramming.\nWe further propose a reprogramming method based on the distribution of memristor states, leveraging the statistical properties of bitline-represented numbers. As bitwidth increases, the probability of the least significant bit being one approaches 50%, reflecting an inherent randomness in lower-order bits. From a CIM perspective, this reveals a lack of correlation between the bell-shaped distribution of DNN [18]\u2013[21] and the uniform distribution of active memristors in low-order columns, which are farther from the input and contribute minimally to the overall weight magnitude. Recognizing their limited impact, we selectively reprogram only a small fraction of memristors in the lowest-order column, reducing the overall reprogramming workload without sacrificing performance.\nSee Figure 1 for the summary of the approach. The main contributions of this paper are:\n\u2022 The use of sorted weight sectioning to (1) minimize reprogramming and (2) efficient scheduling of multiple crossbar programming without penalizing model accuracy\n\u2022 A bit stucking procedure based on the uniform distribution of memristors in low-order columns to further reduce reprogramming with minimal DNN accuracy loss"}, {"title": "II. BACKGROUND AND RELATED WORK", "content": "Past works addressed crossbar programming efficiency in terms of time and accuracy. Feedback strategies [14], [22] uses multiple read/write operations to approximate the output of multi-level memristors. Variable amplitude pulses [8], [11], [23] implement finer transitions between adjacent conductance levels. Emmanuelle et. al. [13] combines feedback and variable pulses and is considered as the state-of-the-art implementation of sequential programming. Recent papers addressed parallel memristor programming with sequential fine-tuning [14] and neural networks assisting memristor programming [24].\nOur work targets bit-sliced crossbars, where each row corresponds to a single weight value and each column represents a power-of-two multiplier. For example, in a 128x128"}, {"title": "III. OPTIMIZING REPROGRAMMING WITH SORTED WEIGHT SECTIONING", "content": "In bit-sliced crossbars, reprogramming is restricted to memristors changing states. To minimize these transitions, a weight allocation method called sorted weight sectioning (SWS) is used, where weights are sorted by magnitude and grouped into sections to reduce the frequency of state changes. This sorting is done once offline, but inference requires a buffer to batch data and apply index matching for correct dot-product calculations. Building on SWS, this approach reduces reprogramming and enhances energy efficiency as discussed in [17]."}, {"title": "A. Reprogramming Crossbars", "content": "Consider we have two crossbars $A, B \\in \\mathbb{R}^{m \\times n}$, with memristors $a_{ij}, b_{ij} \\in \\{0,1\\}$ representing, respectively, inactive and active states, for $i \\in \\mathbb{R}^{m}$ and $j \\in \\mathbb{R}^{n}$. The reprogramming cost to convert from A to B is\n$R_{AB} = \\sum_{i,j} |a_{ij} - b_{ij}|.$ (1)\nIf $a_{ij} - b_{ij} = 0$, the memristor $a_{ij}$ does not face a transitional state. On the other hand, if $a_{ij} - b_{ij} = 1$, we switch states. Mathematically, we want to find B* such that\n$B^* = \\arg \\min_{B^{m \\times n} \\in W} R_{AB},$ (2)\nwhere W is the set of all weight matrix sections of size m\u00d7n. Without SWS, optimal scenario is achieved by iterating through all sections and calculating the cost to see which one"}, {"title": "B. Extending to Multiple Crossbars", "content": "Now suppose we have a set C of L programmable crossbars. Our mathematical optimization model becomes\n$\\{B_1, \\dots, B_L\\} = \\arg \\min_{A^{m \\times n} \\in C, B^{m \\times n} \\in W} \\sum R_{AB}.$ (3)\nThat is, we want to find a collection of matrix sections $\\{B_1, \\dots, B_L\\}$ that minimizes reprogramming of L crossbars. We propose the stride L and the stride 1 scheduling.\nStride L Scheduling. We select the L first crossbars in the sorted list, At each reprogramming time, we program the crossbar i + L to the crossbar i.\nStride 1 Scheduling. We select L evenly spaced crossbars in the sorted list, At each reprogramming time, we program the crossbar i + 1 to the crossbar i.\nThe stride L efficiency depends on the skip length L and the similarity of sections in the sorted list. Although stride 1 initially incurs higher costs by programming the first L crossbars, it only skips one crossbar at each step, making it advantageous over multiple reprogramming tasks, as demonstrated in Figure"}, {"title": "C. Multiple Crossbar Programming Thread Imbalance", "content": "If we program L crossbars in parallel, we expect to achieve speedup of L times. Realistically, depending on how we balance the work of each thread, it might result in significant delay. For instance, if we group crossbars of small and large reprogramming costs within the same thread, the programming time will be bottlenecked by the largest reprogramming cost. To optimize parallel programming of multiple crossbars, we propose a greedy approach based on SWS. We group crossbars of similar reprogramming costs on each thread (see Figure 4)."}, {"title": "IV. BIT STUCKING-BASED REPROGRAMMING", "content": "Now we leverage bit-level weight distribution to further reduce reprogramming with minimal DNN accuracy loss.\nConsider a random sample of numbers in bitline representation. The probability that a number has a digit one on its least significant bit (LSB) tends to 50% as we increase the bitwidth: the distribution of values in the LSB is approximately uniform.\nIn bit-sliced crossbars, weights are mapped in bitline form on each row. This way, the LSB is the lowest-order column memristor. Since each column is a power-of-two multiplier,"}, {"title": "V. EXPERIMENTS", "content": "We assess performance using speedup (ratio of memristors that needed to switch states) and model accuracy, benchmarking against the state-of-the-art unsorted scenarios from CASCADE [26] and ISAAC [3]. Crossbar computations were simulated in PyTorch on ImageNet-1K [27] on all model layers (ResNets and VGGs from PyTorch, and ViTs and DeITs from timm), trained in 32-bit floating point precision. Simulations utilized 128x10 crossbars unless specified otherwise."}, {"title": "A. Sorted Weight Sectioning on a Single Crossbar", "content": "The DNN bell-shaped distribution [18]\u2013[21] results in many crossbars with low-magnitude weights, which require fewer transitional states. The gradual transition from small to large-magnitude crossbars is essential to understanding how SWS optimizes reprogramming. In Figure 5, DeIT-Tiny, with its sharp weight distribution, achieved the lowest speedup (1.47x), while VGGs, with smoother distributions, saw higher improvements, reaching 1.87x on VGG16. Notably, SWS enhanced programming speed for all models."}, {"title": "B. Sorted Weight Sectioning on Multiple Crossbars", "content": "We compare the average speedup per crossbar in two stride methods with 16 reprogrammable crossbars, we observed that increasing the stride reduced SWS speedup, as shown in Figure 6. This decline in speedup occurs because larger strides require skipping multiple (L) crossbars per reprogramming task. As"}, {"title": "C. Bit Stucking-Based Reprogramming Analysis", "content": "The bit stucking-based reprogramming experiments, shown in Figure 8, compare p = 1 (full reprogramming of necessary memristors in the lowest-order column) with p = 0.5 (only half are reprogrammed). Speedups ranged from 19% for AlexNet to 27% for DeIT-Base, with minimal accuracy loss (less than 1%). A further sweep of p values for ViT-Base and ResNet-50 in Figure 9 demonstrates that reducing p down to 0-essentially stucking the last column-maintains accuracy within 1%. Overall, tuning p between 0 and 1 effectively balances speedup and accuracy.\nFinally, we fix p = 0.5 and sweep the number of columns (see Figure 10). Sweeping columns provided almost constant"}, {"title": "VI. CONCLUSION", "content": "We showed that sorting pretrained DNN weights for bit-sliced CIM crossbars and leveraging memristor distribution in low-order columns significantly save reprogramming time by minimizing transitional resistance states. The method's effectiveness depends on how parameters are distributed in crossbars: bit-level sparsity, models with similar matrices, and large-sized crossbars provide better results.\nWe validate these results on ImageNet-1K dataset. We achieved substantial speedups\u20143.7x for ResNet-50 and 21x for ViT-Base\u2014while maintaining less than 1% accuracy drop. This work suggests a new research direction on efficient crossbar reprogramming based on weight allocation."}]}