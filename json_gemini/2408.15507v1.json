{"title": "What Machine Learning Tells Us About the Mathematical Structure of Concepts", "authors": ["Jun Otsuka"], "abstract": "This paper examines the connections among various approaches to understanding concepts in philosophy, cognitive science, and machine learning, with a particular focus on their mathematical nature. By categorizing these approaches into Abstractionism, the Similarity Approach, the Functional Approach, and the Invariance Approach, the study highlights how each framework provides a distinct mathematical perspective for modeling concepts. The synthesis of these approaches bridges philosophical theories and contemporary machine learning models, providing a comprehensive framework for future research. This work emphasizes the importance of interdisciplinary dialogue, aiming to enrich our understanding of the complex relationship between human cognition and artificial intelligence.", "sections": [{"title": "Introduction", "content": "What are concepts? This is arguably one of the fundamental questions in philosophy, with nearly as many answers as there are philosophers. Aristotle's Categories, Locke's compositional theory of ideas, Kant's transcendental logic, Hegel's concrete universals, Cassirer's Funktionsbegriff, and Wittgenstein's discussion of family resemblance are just a few notable examples of philosophical theories on concepts. Since the last century, philosophical inquiries into concepts have been further accelerated and enriched through interaction with cognitive science, which added an empirical dimension to the discussion. Various models of concepts, such as prototype theory, exemplar theory, and the so-called theory-theory, have been proposed, oftentimes in opposition to or drawing inspiration from existing philosophical theories, and brought to empirical scrutiny (Margolis and Laurence, 1999; Murphy, 2004). At the same time, the discussions highlighted the multifaceted nature of what we call concepts, prompting skepticism about the general theory of concepts that would unify all the empirical findings and the theoretical roles we expect for \u201cconcepts\u201d (Machery, 2009).\nOn the other hand, the nature of concepts has also been a main topic in the machine learning and artificial intelligence literature since the first generation of AI scholars set out to seek appropriate knowledge representation to build expert systems (cf. McCarthy and Hayes, 1969). Today, the main impetus of machine learning has shifted from rule-based algorithms to statistical inference powered by deep neural networks (DNNs), which spurs an interest on how these models are extracting appropriate representations from data and using them in various tasks including object classifications and texts or image generations, to name a few (e.g. Buckner, 2019, 2023). The swift advancement in AI research has developed hand in hand with the mathematical modeling of representations (cf., Goodfellow et al., 2016). The \"Good-Fashioned Old AI\" or GOFAI employed a lattice-like structure to represent the knowledge base of expert systems. Contemporary natural language processing (NLP) and object recognition embed words and objects in some metric space (e.g. vector space). Generative models (such as variational auto-encoder or VAE; Kingma and Welling 2013) are thought to map data onto manifolds (intuitively, curved hypersurfaces) in the latent space. Recent studies use group theory to formulate the invariant and compositional features of concepts (Cohen and Welling, 2016; Higgins et al., 2018; Wang et al., 2024; Weiler, 2024). These mathematical approaches aim to explicate the formal nature of machine representations from different aspects, with a view toward unlocking insights into the remarkable capabilities of DNNs.\nThe aim of this paper is to put together insights from these different studies on concepts in philosophy, cognitive science, and machine learning-in one place and draw connections among them. In particular, the paper will categorize these works into the following four types and characterize each type in terms of the corresponding theories of different traditions:\n1.  The Abstractionism: is a classical Aristotelian view that concepts are formed through abstraction. This is the basis of the classical theory in cognitive science and formal ontology in expert systems, and is suitably represented by lattice algebra.\n2.  The Similarity Approach: underlies the Wittgensteinian theory of family resemblance (philosophy), prototype and exemplar theories (cognitive theory), and vector space representations in machine learning (NLP, object recognition).\n3.  The Functional Approach: dates back to Lotze and Cassirer's funktionsbegriff, and captures some aspects of the theory theory or knowledge-based approaches in cognitive science. A similar idea underlies various kinds of manifold learning and generative models (such as VAE), where representations are analyzed in terms of manifolds.\n4.  The Invariance Approach: draws on the idea that concepts must be invariant under certain groups of transformations (Cassirer, 1944; Jantzen,\nThrough this (admittedly partial and incomplete) classification, the present paper aims to reveal hitherto unrecognized commonalities among philosophical, psychological, and computational theories. This will facilitate researchers working in different disciplines to learn from each other and extrapolate findings in one domain to another. For instance, modeling practices in cognitive science and machine learning suggest how a priori philosophical ideas may be implemented empirically and computationally. In turn, philosophical considerations may be instrumental in both exploring and refining scientific models, by setting desiderata or providing their interpretations. Such interdisciplinary exchange is becoming increasingly important in the context of the recent explosive advancements in AI, which raises the critical question of how we can comprehend the thought processes of machines (Otsuka, 2022). Since representations are expected to play key roles in both the generalizability and interpretability of deep learning models, explicating their structures and comparing them with human concepts should provide a deeper understanding of the mechanisms underlying AI and contribute to the development of more transparent and robust models.\nWith that said, this paper does not purport to make any empirical claim about the psychological mechanism underlying human reasoning. It is not the goal of this paper to identify the conceptual model that best captures how we humans think, for such a question should be answered not by philosophy but by empirical studies. Nor is the comparison meant to imply the similarity between machine and human. I remain completely agnostic about whether human brains are really like neural networks (or vice versa). The focus of this paper is not on empirical implementations, but rather exclusively on models themselves. That is, its sole aim is to compare and analyze the (mathematical) nature of various theories of concepts, regardless of their empirical veracity. In this sense, the present paper is meant to serve as a prolegomenon for empirical studies, by taking stock and pulling together different approaches to modeling concepts.\nThis paper unfolds as follows: Section 2 begins with an exploration of the classical Aristotelian theory of Abstractionism and its modern adaptations in cognitive science, expert systems, and lattice theory. Section 3 discusses the Similarity Approach, tracing its origins to Wittgenstein's family resemblance and its application in prototype and exemplar theories within cognitive science, as well as in vector space models in machine learning. Section 4 examines the Functional Approach, which draws on Lotze and Cassirer's work, and its parallels in manifold learning and generative models like VAEs. Section 5 introduces the Invariance Approach, highlighting the role of group theory in understanding invariant and equivariant representations in both philosophy and contemporary machine learning, particularly in the context of disentangled representations. Finally, Section 6 provides a comparative analysis of these approaches, discussing their implications for future research in the interdisciplinary study of concepts"}, {"title": "The Abstractionism", "content": "According to abstractionism, concepts are formed through abstractions from individual data. For instance, the concept human is formed from individual humans by abstracting away idiosyncratic differences, say in height, hair color, or any physical and psychological characteristics. The abstraction process can be done in stages. The concept mammal may be formed by abstracting away differences among human, tiger, dog, etc., and by repeating a similar process one can obtain more general concepts such as animal, organism, and so on. The resulting upside-down tree-like structure is called the Porphyrian tree. Going up and down through this tree respectively correspond to abstraction and specification. Each downward branching of the tree represents logical disjunction, such as mammal = human V tiger V  Disjunction amounts to ignoring differences among the terms. Conversely, one may also create a concept through conjunction, like centaur = human  horse, whereby the concept of a centaur is formed by combining the features possessed by humans and horses. Conjunction is represented by upward branching, which yields a lattice-like structure.\nThis way of looking at concepts, which Heis (2007) dubbed as \u201cAristotelian abstractionism,\" dates back to Aristotle's Categories and reached a certain level of perfection in the modern period, as witnessed in Arnauld and Nicole's Port Royal Logic and Kant's Logik (Igarashi, 2023). Modern logic made an explicit distinction between extents and intents. The extent is a set of objects to which a given concept applies, while its intent is a set of properties that defines the concept. For instance, the extent of human includes Socrates, Caesar, etc., while its intent would include such properties as being bipedal, rational, etc. A concept is fully identified by its intent, i.e., conditions that are separately necessary and jointly sufficient for being its instance, or in other words, the properties shared by all and only members of the corresponding extent.\nThe notion reminiscent of abstractionism extends beyond the boundaries of philosophy. Because concepts are essential building blocks of human thought and understanding, cognitive scientists have developed several models of concepts to characterize their cognitive functionalities in perception, memory, reasoning, language processing, and decision-making. The Classical Theory is one such theory that has often served as a foil for other views, and identifies a concept with the corpus of information pertaining to its defining attributes (Margolis and Laurence, 1999; Murphy, 2004; Machery, 2009). Along with its name, the resemblance to abstractionism should be clear.\nThe abstractionist conceptualization of concepts has also laid the foundational framework for initial AI research, notably influencing the development of the knowledge base for expert systems (e.g., Sowa, 2000). Expert systems aim to emulate the decision-making ability of human experts (e.g. medical doctors) by applying inferential rules to information stored in the knowledge base. The standard form of knowledge representation, often called ontology, stores infor-"}, {"title": "The Similarity Approach", "content": "As we have seen above, one major criticism against abstractionism was targeted at its essentialism that a concept is definable by a set of necessary and sufficient conditions, or its essence. Most real concepts seem to lack such essence. What, for example, are properties that are shared by all instances of game, which would presumably include football, Go, Tetris, and so on? Wittgenstein famously pointed out that what unites these games are not the shared properties but rather family resemblance, i.e., they are related via loose similarity relationship among objects. Similarity is symmetric but not necessarily transitive: I might resemble my maternal grand father and paternal grand mother, but they do not need to resemble each other. Likewise, an arbitrary pair of games, say football and Tetris, need not be similar or have shared properties. What make them game is the fact they resemble to other items that together form a cluster of objects we call game.\nSimilarity judgments depend on properties or standards under considerations. Two people may have similar body heights but not weights. To measure the similarity between objects, therefore, one first needs to determine a set of relevant properties or dimensions, which can take either continuous, discrete, or binary (yes or no) values, and then plot objects according to their value in each dimension. Suppose we have n relevant properties and let $x_i(a)$ be the value of the i-th property/dimension of an object a. One straightforward similarity measurement of a pair (a, b) of objects is a weighted sum of the absolute difference\n$\\sum_{i=1}^{n} w_i|x_i(a) \u2013 x_i(b)|$ \n or the Euclidean distance\n$\\sqrt{\\sum_{i=1}^{n} w_i\u00b7 (x_i(a) \u2013 x_i(b))^2}$,\nwhere the weight $w_i$ gives the relative importance of the i-the property toward calculating similarity. A pair similar in this sense will be mapped closely in the n-dimensional property space. A concept, then, can be defined as a cluster in the high-dimensional space. Such a cluster may be \"cloud-like\" and need not have clear-cut boundaries or threshold values, capturing the idea that concepts are usually not definable by sharp necessary and sufficient conditions.\nThe similarity-based view of concepts goes by different names in different fields. In philosophy of science, the Homeostatic Property Cluster or HPC Theory has argued that property clusters can better account for natural kinds, especially those appearing in biological sciences such as biological species, than the definitional approach (Boyd, 1991, 1999). The same idea underlies prototype theory and exemplar theory in cognitive science, which posit that humans classify objects based on their similarity to representative items, referred to as prototypes or exemplar (cf. Murphy, 2004, ch. 3). According to prototype theory, an object is classified into a category, say cat, if it is more similar to the prototype of cat than to other prototypes, where a prototype is defined as a statistical center, such as the mean, derived from past instances of the same kind. Exemplar theory, on the other hand, suggests that the standard for classification is set by a representative instance, such as an actual sparrow or robin in the case of birds, rather than a statistical construct. These views excel in handling the typicality effects we saw above by explaining the time lag and unreliability of classification judgments as a function of the distance from the prototype or exemplar, and became widely accepted in cognitive science.\nThe notion of similarity has also played the crucial role in the recent development of machine learning, more specifically in the construction of good representations. Deep neural networks (DNNs) process complex data by representing them as vectors in a latent space, which are then used for the downstream tasks such as image classifications, word predictions, and so on. Such representations are expected to be aligned, i.e., reflect the actual relationships among objects so that similar things are embedded into a neighboring area (Wang and Isola, 2020). For instance, natural language processing (NLP) embeds words in a high-dimensional vector space so that word-vectors with similar meanings cluster together. The similarity between these vectors is measured by the angle formed between them, known as cosine similarity. It has been known that this task is accomplished with a simple three-layer network, such as word2vec, and a corpus of modest size (Mikolov et al., 2013). More generally, the tech- nique called contrastive learning is used to learn the similarities and differences between data points in a wide range of tasks (van den Oord et al., 2018). It compares given data points with both similar (positive) and dissimilar (negative) examples, learning representations that minimize the distance between positive pairs and maximize the distance between negative pairs, thus capturing the underlying semantic structure of the data. Representations learned through contrastive learning have proven highly effective in image classification, NLP, and speech recognition, excelling at distinguishing categories, understanding semantic relationships, and identifying different speakers and phonetic patterns.\nUnderlying all the above approaches is the mathematical notion of metric space, a space equipped with a metric function that specifies the distance between any two points in the space. The absolute distance, Euclidean distance, and cosine similarity are typical examples of metric functions, but there are plenty of other choices. Moreover, the metric does not need to be defined glob-"}, {"title": "The Functional Approach", "content": "Above we have seen that the similarity approach was prompted by the criticism on the essentialist aspect of abstractionism. The functional approach is inspired by another issue of abstractionism that permits an arbitrary concept formation through the abstraction of random objects, say the concept of red juicy food from cherry and raw meat (Lotze, 1874; Heis, 2007). A similar point may be raised against the similarity approach. Recall that in the similarity approach concepts are represented as regions or \"chunks\" in the metric space. Now consider abstracting these concepts. The straightforward way to do this is to combine all the regions corresponding to these concepts to form a greater region: for instance, mammal could be formed by combining all such clusters like human, dog, whale, etc. But obviously not any amalgamation would do: gerrymandered or isolated patches, like the notorious \"grue\" formed by all green things observed before a certain time t and blue things observed there-after, should not make a bona fide concept (G\u00e4rdenfors, 1990). This means that there must be a certain restriction on the shape of a region or cluster for it to count as a concept.\nSuch a constraint can be represented by a certain functional relationship. According to this idea, a concept is not a mere combination of attributes or items but rather embodies a certain functional relationship among its possible features. Heis (2007) attributes this \"functional\" view of concept to Lotze:\nAs a rule, the marks of a concept are not coordinated as all of equal\nvalue, but they stand to each other in the most various relative\npositions, offer to each other different points of attachment, and so\nmutually determine each other; an appropriate symbol for the\nstructure of a concept is not the equation S = a + b + c + d, etc,\nbut such an expression as S = F(a, b, c, etc.) indicating merely that,\nin order to give the value of S, a, b, c, etc, must be combined in a\nmanner precisely definable in each particular case, but extremely\nvariable when taken generally. (Lotze 1874 \u00a728, quoted from Heis\nLotze's point is that a concept cannot be created by combining arbitrary fea- tures willy-nilly. It is rather characterized by its internal functional relation- ship, through which its possible attributes are constrained by each other. For instance, each concept that belong to animal should specify as its attributes the means for locomotion, reproduction, and respiration, etc, like dog = f(walking, viviparous, pulmonary, ...) (Asano, 2020). Moreover, these attributes are not independent from each other: e.g., the respiratory system of an organism should match its locomotive means. Lotze's proposal is that the concept animal should be understood as such an internal relationship that determines possible combinations of animal traits.\nThe functional relationship embodied by a concept reflects our theoretical knowledge about the objects and world. That flying animals usually have lungs instead of gills is a part of our folk biology.4 In this sense, the functional view shares the spirit with the so-called theory theory or knowledge approach in cognitive science that sees concepts as anchored by our overall understanding of the world (Margolis and Laurence, 1999, pp. 60-1). According to this view, concepts are not mere labels of objects, but rather carry with them significant information about what they are, which is exploited by our reasoning and inference. Although there is much room of interpreting what it counts as a \"theory\" behind each concept, one (but obviously by no means only) possibility is to represent it as a functional relationship. Take as an example what Barsalou (1985) calls goal-derived categories. healthy food is one such category, which may include items such as kale, whole grains, and lentils. However, its content are not determined merely by a disjunction of these items. Rather, we consider a vector-valued function that maps food to its nutritional components such as calories, fiber, proteins, etc., and defines healthy food as an inverse image of certain regions (e.g. a low calorie and high fiber and protein region) in the codomain of such a function. Thus the concept healthy food is based on the function that presumably reflects our knowledge in nutritional science.\nLet's examine how this functional perspective of concepts intertwines with the geometric landscape discussed in the previous section. As seen in graphs described by equations, a function serves to specify a certain region or hyper-surface in the space. For instance, $f(x, y) = x^2 + y\u00b2 = 1$ in the two-dimensional Euclid space $R^2$ defines the origin-centered circle with radius 1. In general, a real-valued function f on a space determines a region of the space by its inverse image $f^{-1}(r)$ for $r \\in R$ (which, in the case of the above function, is the circle with radius r). Moreover, if it is a smooth function $f: M\\rightarrow R$ on a manifold M, it determines a submanifold $M' \\subset M$ as its inverse image $M' = f^{\u22121}(r)$ for $r \\in R$ under some regularity condition.5 Such a submanifold M' represents possible combinations of features under the functional constraint f. The functional concept, therefore, can be represented as a certain region in the conceptual space, which in cases of smooth regular function constitutes a submanifold.6\nIn the machine learning literature, the Manifold Hypothesis postulates that observed data can be represented in low-dimensional regions or submanifold within the high-dimensional representational space, reflecting the fact that not all possible combinations of features are realizable (cf. Goodfellow et al., 2016, Sec. 5.11.3).7 If the hypothesis is correct, different types of objects or concepts"}, {"title": "The Invariance Approach", "content": "The generative capability of concept manifolds, which allows one instance to morph into another, highlights a dynamic aspect of concepts, enabling various forms of inductive reasoning about hypothetical changes. When we classify an object under a certain concept, e.g. identify it as a face, we are at the same time attributing to it a various projections about how its appearance would change or would not change under possible transformations, say in perspective or the passing of time. As Cassirer puts it, \"the apprehension of the particular qua 'existence' involves apprehension of the possibilities of transformation which it contains within itself\" (Cassirer, 1944, p. 15). In the context of psychological research, the same idea underlies Gibson's pioneering work on affordance, which posits that animal perceptions encompass not merely sensory stimuli but also rich information about what the environment offers or affords the organism in terms of possibilities for action. This includes, for example, the organism's anticipation of how visual perception might alter upon moving through its surroundings (Gibson, 1979). This is echoed by Barsalou's perceptual theory of knowledge, which identifies a concept to be a \"simulator\" that allows the cognitive argent to mentally simulate different aspects and instances of a given category, say chair, in many different circumstances (Barsalou, 1999, p. 587).\nThe key insight here is that objects and their appearances change in a systematic fashion, and thus our recognition system must be robust enough to identify the object under different guises, while also being flexible enough to track these changes. These features are respectively captured by invariance or equivariance with respect to group actions (Cassirer, 1944; Hoffman, 1966; Dodwell, 1983; Jantzen, 2015). This framework first models various changes in objects or their appearance as transformations, represented by functions g: X \u2192 X where X is a set of items in question. For instance, if X is the set of visual images formed on the retina, a shift g in perspective changes one image x \u2208 X to another g(x) \u2208 X. The premise is that the set G of all such transformations forms a group, so that (i) it contains an identity or \"do nothing\" transformation e \u2208 G such that e.x = x for all x \u2208 X; (ii) for every transformation g \u2208 G there is an inverse \u201ccancellation\u201d $g^{\u22121} \u2208 G$ such that $g. (g^{-1}.x) = g^{\u22121}.(g.x) = x$ for all x \u2208 X; and (iii) transformations are associative, so that $(g_i\u00b7g_j)\u00b7g_k = g_i\u00b7(g_j\u00b7g_k)$ for any $g_i, g_j,g_k \u2208 G$. Changes in perspective, for instance, arguably satisfy these rules and thus present group actions on X.\nConceptualization can be understood as a function  : X \u2192 R that maps ob- jects or stimuli x \u2208 X to their representations \u03c6(x) \u2208 R. Such a representation map is called invariant with respect to group actions G on X if actions do not affect representation, such that $(x) = \u03c6(g(x))$ for all x \u2208 X and g \u2208 G (Fig. 4 left). This means that however one transform an object x within the range of G, still identifies them as the \"same thing.\" This is arguably a desirable feature of conceptual recognition, as the apprehension of \"what it is\" such as identifying whose face it is should not depend on a certain range of transformations like perspectival shifts. In other words, the ability to classify objects or stimuli into one concept, or \"the apprehension of the particular qua 'existence' \" as Cassirer puts it, involves determining the range of group actions G' CG with respect to which representation  remains invariant.\nOn the other hand, equivariance refers to the aspect of representations that \"tracks\" changes in objects. Perspectival shifts, for instance, though leaves in-"}, {"title": "Discussion", "content": "The preceding sections have examined various theories on concepts and representations in philosophy, cognitive science, and machine learning, classifying them into four categories: abstractionist, similarity, functional, and invariance approaches. This section compares these views from a meta-perspective, exploring their connections and deriving implications for further studies. By identifying the intersections and differences among these approaches, this discussion section aims to provide a clearer framework for interdisciplinary studies of concepts and representations.\nThe first axis for comparison is the role of concepts. Arguably, concepts play various roles in classification, learning, communication, problem solving, and so on; but these roles can be further understood through the lens of two major functionalities: descriptive and inferential. In the descriptive use, concepts serve to characterize and summarize data in various formats. Among the approaches discussed so far, the abstractionist and similarity approaches are particularly motivated by these descriptive tasks. The major goal of abstractionists is to classify objects in the hierarchical manner, by constructing a conceptual lattice from a data table or \"context\" that lists items and their observed properties (Section 2). Given a similar (but possibly continuous) dataset, the similarity approach visualizes mutual relationships between items in a two or more di- mensional space based on their attributes and delineates concepts as clustered points that are close to each other (Section 3). These outcomes\u2014lattices or plots-reveal the inherent structure of data not visible in the original format.\nConcepts also play a central role in inferential tasks, e.g., for the purpose of predicting future events or possible behaviors of objects. By identifying an object in front of me as a dog, I can anticipate what it will and will not do: it may bark, chase after balls, and sniff around, but probably not climb trees or purr like a cat. Though very simplistic, it is nonetheless a bona fide act of prediction: based on some observable cues like floppy ears, I derive its behavioral characteristics that I have not yet seen, at least with respect to this particular dog in front of me. Such inferential role of concepts is the primary focus of the func- tional perspective (Section 4). Inferences from one set of attributes to another exploit the information about internal constraints on the possible combinations of attributes. This information, encoded in the functional form, constitutes our understanding of concepts. Alternatively, the invariance approach captures inference from a different perspective, by identifying a concept as a set of trans- formation rules that govern possible changes in the object (Section 5). Such equivariant group actions highlight the inferential role of concepts that allows us to simulate how objects or their perceptions transform in accordance with changes in the environment or the perceiving subject.\nThe above discussion should not imply that models are inherently connected to or designed for particular roles. Indeed, there is no contradiction in taking a conceptual lattice as representing internal constraints of concepts or using the similarity space for the purpose of prediction, as is quite common in natural language processing. The descriptive/inferential contrast rather concerns the modeling purpose: namely whether one is interested in data themselves, or in some underlying structure of \"uniformity of nature\" from which data are sampled. In the former context, mathematical models are taken as a sort of descriptive statistics that serve for the economy of thought; while in the latter, they are interpreted as representing a generative mechanism that lies beyond any particular data (Otsuka, 2022). These two desiderata are often in conflict: the more one tries to adjusts one's concepts as faithful as possible to given data, the more likely they are to overfit the data, resulting in a loss of predictive ability (Forster and Sober, 1994). A good descriptive model needs not be a good inferential model, and vice versa. Therefore, when evaluating a particular model, it is essential to clearly specify the criteria by which it is being evaluated.\nOur second point of consideration concerns the relationship between concepts and their defining features. In the philosophical and cognitive science literature, concepts have been defined or characterized in terms of familiar and explicit fea- tures we use in everyday life, such as color, size, shape, and so on. Accordingly, conceptual hierarchies or similarity maps have been build and evaluated on the basis of a pre-specified set of mostly ostensible features. This represents a \"feature-first\" approach, where features act as the raw material for thoughts, with concepts being formed by combining or mixing these features. In contrast, features in representation learning are not fixed beforehand, but rather are extracted from data as axes or dimensions that constitute the latent space. In addition, the meaning of these extracted features is not given a priori, but must be determined by unpacking the internal intricate structure of the trained neural network through detailed analysis. This can be seen as a \"representation-first\" approach, considering representations to be epistemologically prior to features. These two approach faces different challenges. The main challenge of the feature-first approach lies in its empirical adequacy. A common criticism against abstractionism or the classic view of concepts points out that it is simply im- possible to define concepts, such as game or human, through combinations of existing features or traits (Wittgenstein, 1953; Boyd, 1991). Moreover, it is not clear which features should be considered to define concepts. Outcomes of formal concept analysis heavily depend on the list of attributes used to charac- terize data (Section 2). And Feigenbaum's bottleneck highlights the difficulty of determining the relevant attributes for the problem at hand. A similar issue arises for prototype and exemplar theorists in determining the appropriate at- tributes/dimensions and their relative importance in constructing the similarity space, which significantly affects similarity judgments (Murphy, 2004).\nWith its successful applications to a range of empirical problems, the representation- first approach seems to be overcoming all the above issues: deep learning mod- els are able to learn robust representations (with some reservations discussed shortly) that automatically extract relevant features from data, capture complex patterns, and generalize well to new situations. The problem, however, is that these features generally resist intuitive interpretation. One thus needs to read off meanings from trained models, but to do so requires a clear understanding of what one is looking for that is, what are meaningful features? The idea of disentangled representation discussed in Section 5 is one attempt to explicate what we consider meaningful features of a representation in terms of indepen- dent group actions. Understanding features is crucial not just for interpretability but also to enhance model performance, ensure robustness, and improve fair- ness in machine learning applications (Lipton, 2016). It is pointed out that the well-known phenomenon of adversarial attack, where deep learning models mis- classify objects due to the addition of small, often imperceptible perturbations to the input data, is a consequence of the model's using complex, high-dimensional features that are not necessarily aligned with human-perceptible features (Ilyas et al., 2019). Also, understanding features allows data scientists to identify and mitigate biases in machine learning models by examining how different features influence predictions (Ribeiro et al., 2016). These efforts can be understood as attempts to identify the concepts (representations) used by machines and analyze them into components (features) amenable to human understanding.\nThe feature-first and representation-first approaches can thus be understood as akin to digging a tunnel from opposite sides. The former starts with a set of explicit features and builds complex concepts in a bottom-up way, while the latter aims to break down given representations into understandable pieces in a top-down fashion. The important challenge in contemporary concept research is to make these approaches meet halfway.\nThe final, but not least, point concerns the relationship among the four approaches discussed in this paper. In his influential book, Edouard Mach- ery (2009) argued for the disunity of concepts, challenging the traditional view that concepts are a unified phenomenon within cognitive science. His view is that what are labeled \"concepts\" actually involve heterogeneous mental kinds with different functionalities, purposes, and empirical bases. Be that as it may, mathematical considerations naturally suggest the logical relationships between different conceptual models. Indeed, we have already seen some of such attempts in the present paper. The group-theoretic analysis of disentangled representa- tions can be thought as an attempt to integrate the theoretic aspect of concepts (encoded by group operations) and their similarity-based aspect (represented by a manifold). In Section 3, we have seen some recent works in natural lan- guage processing that aim to encode the hierarchical structure of concepts into the word vector space by using non-Euclidean (hyperbolic) spaces (Nickel and Kiela, 2017) or representing words by boxes instead of vectors (Vilnis et al., 2018). If successful, this line of research will reconcile the abstractionist and similarity approaches, which have been considered rivals in both philosophy and cognitive science literature.\nUnderlying these studies is the overarching theme of the relationship be- tween geometry and algebra. Lattices and groups are algebraic in nature, while metric spaces and manifolds have a clear geometric character. Hence the four approaches discussed in this paper can each be seen as shedding light on the geometric or algebraic aspects of concepts. Kant (1999) was the first to make a clear distinction between, and propose a unification of, these two aspects of human cognition-namely sensibility equipped with a geometric form, and un- derstanding that follows logical and algebraic principles. Over two centuries after Kant, the contemporary machine learning research is trying to integrate the both components to understand and improve the performance of neural networks, with the aid of much more advanced mathematical machinery than those available to Kant, including non-Euclidean geometry, topology, and group or gauge theory (Sanborn et al., 2024). Just as Kant was inspired by Newtonian physics in his time, these developments in machine learning will provide new insights into the philosophical understanding of concepts."}, {"title": "Conclusion", "content": "This paper has explored the connections among various approaches to understanding concepts in philosophy, cognitive science, and machine learning, with a particular focus on their mathematical nature. By categorizing these approaches into Abstractionism, the Similarity Approach, the Functional Approach, and the Invariance Approach, we have highlighted the distinct yet interconnected ways in which concepts are represented, organized, and learned across different disciplines.\nEach approach offers unique insights into the nature of concepts. Abstrac- tionism provides a structured, hierarchical framework that has influenced both philosophy and early AI research. The Similarity Approach, with its focus on resemblance and metric spaces, has been instrumental in both psychological the- ories and modern machine learning techniques, such as word embeddings. The Functional Approach introduces a dynamic perspective, emphasizing the inter- nal relationships and constraints that govern concept formation, which aligns with the manifold learning methods used in generative models. Finally, the Invariance Approach, rooted in group theory, underscores the importance of understanding how concepts remain stable under transformations, a principle that is central to the robustness and generalizability of deep learning models.\nThis paper has also underscored the importance of interdisciplinary ex- change. Philosophical insights into concepts can guide the development and refinement of computational models, while empirical findings in cognitive science and machine learning can offer new perspectives on longstanding philosophical questions. As AI continues to advance, the need for a deeper understanding of the representations used by machines becomes increasingly critical. By integrat- ing the mathematical rigor of machine learning with the conceptual analysis of philosophy, we can move towards more transparent, interpretable, and robust models.\nNeedless to say, the exploration presented in this paper is only a prelimi- nary sketch of the vast landscape of concept representation across disciplines. Notably, the implications of advanced machine learning models such as Atten- tion mechanisms and Diffusion models have not been fully addressed, leaving significant avenues for further inquiry. Additionally, the relationships among the four categories-Abstractionism, the Similarity Approach, the Functional Approach, and the Invariance Approach-require deeper examination, as there may be further connections and insights to uncover. Consequently, systematic and comprehensive investigation remains necessary to deepen our philosophical and mathematical understanding of these interrelated approaches to concepts."}]}