{"title": "RILQ: Rank-Insensitive LoRA-based Quantization Error Compensation\nfor Boosting 2-bit Large Language Model Accuracy", "authors": ["Geonho Lee", "Janghwan Lee", "Sukjin Hong", "Minsoo Kim", "Euijai Ahn", "Du-Seong Chang", "Jungwook Choi"], "abstract": "Low-rank adaptation (LoRA) has become the dominant\nmethod for parameter-efficient LLM fine-tuning, with LoRA-\nbased quantization error compensation (LQEC) emerging\nas a powerful tool for recovering accuracy in compressed\nLLMs. However, LQEC has underperformed in sub-4-bit\nscenarios, with no prior investigation into understanding\nthis limitation. We propose RILQ (Rank-Insensitive LoRA-\nbased Quantization Error Compensation) to understand fun-\ndamental limitation and boost 2-bit LLM accuracy. Based\non rank analysis revealing model-wise activation discrep-\nancy loss's rank-insensitive nature, RILQ employs this loss to\nadjust adapters cooperatively across layers, enabling robust\nerror compensation with low-rank adapters. Evaluations on\nLLaMA-2 and LLaMA-3 demonstrate RILQ's consistent im-\nprovements in 2-bit quantized inference across various state-\nof-the-art quantizers and enhanced accuracy in task-specific\nfine-tuning. RILQ maintains computational efficiency com-\nparable to existing LoRA methods, enabling adapter-merged\nweight-quantized LLM inference with significantly enhanced\naccuracy, making it a promising approach for boosting 2-bit\nLLM performance.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) like GPT-4 (OpenAI 2023)\nand LLaMA-3 (Meta 2024) have revolutionized various do-\nmains, demonstrating human-level performance in complex\ntasks such as question answering (Kamalloo et al. 2023),\ncode auto-completion (Rozi\u00e8re et al. 2024), and summariza-\ntion (Zhang et al. 2024b). However, adapting these models\nto specialized domains requires efficient fine-tuning tech-\nniques (Wei et al. 2022a; Wang et al. 2023). Low-rank\nadaptation (LoRA) (Hu et al. 2022) has emerged as a\nleading solution, efficiently reparameterizing weight matri-\nces with low-rank adapters to incorporate task-specific in-\nformation. By fine-tuning only a small, adaptable exten-\nsion to the frozen base model, LoRA significantly reduces\nthe memory footprint while effectively specializing LLMs.\nThis cost-effective fine-tuning approach has expanded to en-\nable autonomous module composition (Huang et al. 2023),\nmultiple-task adaptation (Sheng et al. 2023), and long-\ncontext inference (Chen et al. 2024). LoRA has emerged\nas a powerful tool for quantization error compensation in\nlarge language models (LLMs), addressing the challenges\nof reduced inference and fine-tuning costs (Dettmers et al.\n2023). While weight quantization techniques (Ashkboos\net al. 2024a; Frantar et al. 2023; Lee et al. 2023; Yao et al.\n2022; Lin et al. 2023) mitigate LLMs' memory footprint,\nthey introduce errors due to reduced-precision representa-\ntion. Therefore, LoRA has been adopted to help compen-\nsate quantization error. Fig. 1 illustrates two LoRA-based\nquantization error compensation (LQEC) approaches: (1)\nDirect error correction, where adapters offset quantization\nerrors in weights. For instance, ZeroQuant-v2 (Yao et al.\n2023) employs a low-rank adapter per linear module, which\ncan be merged into quantized weights (Xu et al. 2024; Liu\net al. 2024) for efficient inference (Fig. 1(a)). (2) Task-\nspecific fine-tuning, exemplified by LoftQ (Guo et al. 2024),\nwhich combines LoRA with the base model quantized and\nfrozen for memory-efficient fine-tuning, using LQEC-tuned\nadapters as initialization (Fig. 1(b)). Notably, LQEC inte-\ngrates into existing structures without requiring additional\nadapters, offering a promising method to address LLM's\nmemory bottleneck while preserving accuracy.\nDespite advances in LQEC, achieving high compres-"}, {"title": "Related Work", "content": "LLM Weight Quantization. Weight quantization is a\npromising technique to reduce the memory footprint of\nLLMs by lowering the bit-precision of weight values (Fran-\ntar et al. 2023; Lee et al. 2023; Yao et al. 2022; Lin et al.\n2023; Wei et al. 2022b, 2023). Activation-aware weight\nquantization methods (Lin et al. 2023; Lee et al. 2024; Kim\net al. 2023b; Guo et al. 2023; Heo et al. 2023) have suc-\ncessfully reduced weight precision to 4-bit or lower, but\nthese approaches incur mixed-precision overhead and strug-\ngle with poor accuracy at 2-bit precision. Parallel efforts\nhave focused on developing advanced quantizers for 2-bit\nLLMs (Yao et al. 2022; Shao et al. 2024; Chee et al. 2023;\nTseng et al. 2024; Guan et al. 2024; Egiazarian et al. 2024).\nFor instance, OmniQuant (Shao et al. 2024) introduced\nlearnable quantization parameters to adjust weight ranges\nand transformations, QuIP (Chee et al. 2023; Tseng et al.\n2024) employed grouped vector quantization with a shared\ncodebook for non-uniform weight group representation, and\nQuaRot (Ashkboos et al. 2024b) rotated the weight matrix\nto redistribute outliers. While these non-uniform quantiza-\ntion schemes achieve robust 3~4-bit LLM inference accu-\nracy, they still face challenges in closing the accuracy gap\nfor 2-bit LLM inference.\nLORA for Fine-tuning and Compensation. Low-rank\nadaptation (LoRA) (Hu et al. 2022) has emerged as the\nleading parameter-efficient fine-tuning technique to en-\nhance the capabilities of foundational large language mod-\nels (LLMs) (Hu et al. 2022; Huang et al. 2023; Xia,\nQin, and Hazan 2024; Hu et al. 2023; Ding et al. 2022;\nHan et al. 2024; Chen et al. 2023a; Sheng et al. 2023).\nLoraHub (Huang et al. 2023) aggregates LoRA modules\ntrained on different tasks to autonomously compose com-\npatible modules, while SLORA (Sheng et al. 2023) facil-\nitates multiple-LoRA blocks for various tasks, and Lon-\ngLoRA (Chen et al. 2024) enables context extension for\nlong-context LLM inference. LoRA has also been adapted\nfor error compensation, with ZeroQuant-v2 (Yao et al. 2023)\nemploying a low-rank adapter to compensate for weight\nquantization errors and similar approaches used for pruning\nerror compensation (Li, Tang, and Zhang 2024; Zhang et al.\n2023; Chen et al. 2023b). Despite these advances, LoRA-\nbased error compensation methods still face challenges in\nachieving high compression rates, such as 2-bit quantization,\nwithout compromising model accuracy.\nQuantization Error Compensation. Quantization error\ncompensation (QEC) has been extensively explored in the\ncontext of quantization-aware training (QAT), which sup-\nports aggressive sub-4bit quantization while preserving task-\nspecific fine-tuned accuracy (Kim et al. 2023a; Liu et al.\n2023). For example, TSLD (Kim et al. 2023a) uses full-\nparameter tuning combined with token-wise scaled knowl-\nedge distillation to enable robust 2-bit LLM inference,\nthough it requires significant memory for full-parameter ad-\njustments, similar to pre-training. To reduce the memory de-\nmands of QAT, LORA-based parameter-efficient QEC have\nbeen developed (Dettmers et al. 2023; Xu et al. 2024; Kim\net al. 2024; Guo et al. 2024; Li et al. 2024; Liao and Monz\n2024; Chai et al. 2023). Notably, QLORA (Dettmers et al.\n2023) applies LoRA on top of frozen quantized weights to\nminimize memory usage, while QA-LoRA (Xu et al. 2024)\nintegrates adapters into quantized weights to enhance LLM\ninference efficiency. Despite these advancements, signifi-\ncant accuracy losses still occur with extremely low-bit quan-\ntization. Recent quantization-aware LoRA methods, such\nas LoftQ (Li et al. 2024) as well as (Guo et al. 2024;\nZhang et al. 2024a; Kim et al. 2024; Huang et al. 2024b)\ntackle these challenges by using singular value decompo-"}, {"title": "Background and Challenges", "content": "LoRA-based Quantization Error Compensation\nLLMs typically comprise multiple Transformer decoder lay-\ners, an Embedding layer for token-to-activation conver-\nsion, and an LM-Head for converting Transformer out-\nputs into vocabulary logits (Fig.2(a)). Each Transformer\nlayer contains several linear modules with weight param-\neters (WQKV, WOut,WFFN1, WFFN2) for matrix multi-\nplication with input (X) to compute output activation (Y).\nSince the number of parameters exceeds billions, weight\nquantization, a prominent model compression technique,\nrepresents a pre-trained weight matrix $W \\in \\mathbb{R}^{d_1\\times d_2}$ using\na limited bit width b. The quantized weight Qt is formulated\nby Eq. 1:\n\n$Q_b = s \\cdot \\text{clamp}( \\frac{W}{s} - z, 0, 2^b-1) + z$\n$s=\\frac{\\gamma max(W) - \\beta min(W)}{2^b-1}$, $z = \\lfloor{\\frac{\\beta min(W)}{s}} \\rceil$,\n(1)\n\nwhere $\\lfloor \\cdot \\rceil$ indicates the rounding function. The choice of $\\gamma$\nand $\\beta$ varies depending on the quantizers; they can be a con-\nstant ($\\gamma = 1, \\beta = 1$) for the round-to-nearest quantization\n(RTN) or represent the learnable clipping strengths for the\nupper and lower bounds of quantized weights as in Omni-\nQuant (Shao et al. 2024).\nLORA introduces learnable low-rank adapter modules,\n$L_1 \\in \\mathbb{R}^{d_1\\times r}$ and $L_2 \\in \\mathbb{R}^{d_2\\times r}$ ($r < \\text{max}(d_1, d_2)$), with\nfrozen pre-trained W, formulating the forward operation for\na linear module in the Transformer layer as $Y = X(W +$\n$L_1 L_2^T)$. For LoRA-based QEC, rank-r adapters are updated\nto compensate for the impact of weight quantization. For ex-\nample, LoftQ (Li et al. 2024) updates the adapters to mini-\nmize the weight discrepancy (Weight-SVD, Fig. 2(b)):\n\n$\\underset{L_1,L_2}{ \\text{arg min}} ||W - (Q_b + L_1L_2^T)||_F,$\n(2)\n\nwhere $Q_b = \\text{Quant}(W - L_1L_2^T)$, and $L_1 L_2^T$ is iteratively\nupdated via SVD. However, weight discrepancy optimiza-\ntion does not consider the combined impact of W and X to\nthe matrix multiplication output Y. Thus, (Liao and Monz\n2024) proposed the discrepancy optimization on the output\nof linear modules (Linear-Loss, Fig. 2(c)):\n\n$\\underset{L_1,L_2}{ \\text{arg min}} ||Y - Y^q||_F,$\n(3)\n\nwhere $Y = WX$ and $Y^q = (Q_b + L_1L_2^T)X$. QLLM (Liu\net al. 2024) further extended the optimization scope to a\nTransformer layer by updating $L_1$ and $L_2$ parameters within\nit via gradient from the loss between $Y_n$ and $Y_n^q$ defined as :\n\n$Y_n = G(Y_{n-1}, {W_{n,i}}{i=1}^{L})$\n(4)\n$Y_n^q = G(Y_{n-1}^q, {Q_{b,n,i}, L_{1,n,i}, L_{2,n,i}}{i=1}^{L}),$\n\nwhere n is a layer index and G represents a group of L linear\nmodules within a Transformer layer which are sequentially\nprocessed according to the Transformer structure. As de-\nscribed in Fig. 2(d), this layer-wise discrepancy loss (Layer-\nLoss) asserts the alignment of quantized output activation to\nthe FP16 output at each Transformer layer.\nLQEC's Challenge for 2-bit LLM\nDespite efforts to compensate for quantization errors at var-\nious scopes, LQEC has shown limited success with aggres-\nsive sub-4-bit quantization. Fig. 3(a) demonstrates that ex-\nisting discrepancy minimization approaches (Weight-SVD,\nLinear-Loss, Layer-Loss) suffer growing accuracy loss as\nranks decreases when LLaMA-2-7B is quantized to 2-bit,\ncontradicting LoRA's fundamental assumption of low-rank\nfine-tuning updates. To understand these limitations for 2-\nbit LLM inference, we investigate quantization error char-\nacteristics. Fig. 3(b) reveals that weight discrepancy be-\ntween full-precision and quantized weights increases signif-\nicantly as bit-precision decreases from 4-bit to 2-bit, with"}, {"title": "Methodology", "content": "In this section, we first propose the rank sensitivity analy-\nsis to reveal the impact of the discrepancy scope on LQEC\nperformance. Based on this intriguing finding, we propose\na simple yet effective loss, model-level discrepancy loss\n(Model-Loss), as a new objective for LQEC that overcomes\nthe rank sensitivity of 2-bit quantization errors.\nRank Sensitivity Analysis\nThe main limitation of existing LQEC methods is their in-\ndiscriminate use of low-rank adapters for QEC without con-\nsidering quantization error characteristics. To address this,\nwe introduce a new metric called rank sensitivity, which\nmeasures the relative error ($E = |(Y - Y^q)/Y|$) of log-\nits at the LM-Head output. Lower rank sensitivity (smaller\nrelative error) indicates more accurate inference, as logits di-\nrectly influence token prediction accuracy. Using this metric,\nwe analyze how the discrepancy scope affects LQEC per-\nformance. We extend the discrepancy scope to encompass\nall Transformer layers, proposing a new discrepancy loss at\nthe output activation of the final (N'th) Transformer layer\n(Model-Loss, Fig. 2(e)):\n\n$\\underset{L_1,L_2}{ \\text{arg min}} ||Y_N - Y_N^q||_F.$\n(5)"}, {"title": "Rank-Insensitive LQEC", "content": "Building on insights from the rank-insensitive characteris-\ntics of Model-Loss, we introduce Rank-Insensitive LoRA-\nbased Quantization error compensation (RILQ), a novel\nmethod for compensating quantization errors in 2-bit LLMs.\nRILQ significantly improves accuracy by implementing\nModel-Loss and optimizing LoRA adapters in Transformer\nlayers' linear modules using a Model-Loss. As shown in\nEq. 5, RILQ uses gradient descent to collectively tune all\nadapters, minimizing the discrepancy between full-precision\nand quantized activation outputs ($Y_N - Y_N^q$) of the final layer.\nThis approach effectively addresses inter-weight inconsis-\ntencies arising from 2-bit quantization by learning global\ndiscrepancy loss from a holistic model perspective. Notably,\nRILQ is particularly advantageous when adapter shapes are\nconstrained to merge with quantized weights for efficient in-\nference (like QA-LORA), offering a comprehensive solution\nto LQEC challenges.\nTo further enhance the language modeling capabilities of\nLLMs during autoregressive token generation, RILQ incor-\nporates a causal language modeling objective with Ground\nTruth (GT), in the optimization of low-rank adapters (GT-\nLoss):\n\n$\\underset{\\{L_1,L_2\\} \\in \\Theta}{ \\text{arg max}} \\sum_{t=1}^{T} P(x_t | x<t; \\Theta),$\n(6)\n\nwhere x represents a token and T the sequence length. While\nthis approach has been utilized in previous QAT meth-\nods (Kim et al. 2023a), we found it particularly effective in\nguiding low-rank adapters to improve the model's genera-\ntion of coherent and contextually appropriate text sequences.\nThis enhancement further aligns 2-bit quantization adjust-\nments with the calibration data. The additional benefits of\nthis method are demonstrated in Table 7 through an ablation\nstudy, and the entire procedure for RILQ is detailed in the\nAppendix."}, {"title": "Experiments", "content": "Experimental Setup\nTasks and Models. We evaluate our proposed method\nfor common-sense QA tasks (WinoGrande (Sakaguchi\net al. 2019), PIQA (Bisk et al. 2019), Hellaswag (Zellers\net al. 2019), ARC_challenge (ARC-C) (Clark et al. 2018),\nARC_easy (ARC-E) (Clark et al. 2018)) and arithmetic rea-\nsoning task (GSM8K (Cobbe et al. 2021)). We focus on two\nrecent open-source pre-trained LLMs, LLaMA-2-7B (Tou-\nvron et al. 2023) and LLaMA-3-8B (Meta 2024), for eval-\nuating RILQ. Additional model scales are in Table 9 in the\nablation study.\nQuantization Methods. For a comprehensive perfor-\nmance comparison, we employ RILQ alongside state-of-\nthe-art weight quantization techniques, including Omni-\nQuant, QuIP#, QuaRot, as well as LoftQ (Weight-SVD\nbased LQEC). Each method is sourced from its respective\nrepository\u00b9. For OmniQuant and QuaRot, we set the group\nsize to 64 (QuIP# employs a codebook). Other implementa-\ntion details of each quantization method are specified in the\nAppendix.\nRILQ Implementation Details. The RILQ implementa-\ntion includes a calibration process for initializing LoRA\nadapters on quantized models using the C4 dataset (Raffel\net al., 2019). Perplexity is evaluated on both the WikiText-\n2 (Merity et al., 2016) and C4 datasets. For this purpose,\na sequence length 512 is employed, and 256 sentences are\nrandomly sampled from the C4 training dataset. During opti-\nmization, the Model-Loss and the GT-Loss are applied, with"}, {"title": "Impact of Scope for Discrepancy Loss", "content": "Table 7 presents\nan ablation study on LLaMA-2-7B, examining how loss\ntypes and optimization granularities affect quantization er-\nror compensation. Increasing scope from linear module to\nmodel level improves accuracy, highlighting the importance\nof inter-layer interactions within model. Incorporating GT-\nLoss with Model-Loss, which provides richer information\ncompared to using either loss individually, further enhances\nperformance, surpassing the effectiveness of GT-Loss alone\nand serving as an effective optimization guide. This im-\nprovement in accuracy aligns with findings of QAT (Kim\net al. 2023a). The proposed RILQ method, which combines\nthese loss objectives at the model level, achieves the best\noverall performance, highlighting the benefits of global op-\ntimization and diverse loss functions for robust error com-\npensation."}, {"title": "QuIP# end-to-end FT with RILQ", "content": "We additionally eval-\nuate the cross effects of end-to-end fine-tuning in QuIP#\n(QuIP#-FT) and RILQ on LLaMA-3-8B in Table 8. As\nshown in Table 8, RILQ improves QuIP#-FT CSQA accu-\nracy by 2%, highlighting RILQ 's ability to enhance fine-\ntuning beyond standard end-to-end approaches."}, {"title": "Experiments on Large Models", "content": "To evaluate the scalabil-\nity of RILQ, we conduct experiments across the LLaMA-\n2 family, scaling from 7B to 70B parameters. As shown in\nTable 9, RILQ consistently enhances the perplexity of the"}, {"title": "Conclusion", "content": "In this work, we propose RILQ, a novel LoRA-based quanti-\nzation error compensation method that effectively addresses\nthe challenges of 2-bit weight quantization in large language\nmodels. By explicitly employing LoRA for quantization er-\nror compensation and utilizing a model-wise activation dis-\ncrepancy loss, RILQ enables robust quantization-error com-\npensation while maintaining computational efficiency. Ex-\nperiments on LLaMA-2 and LLaMA-3 demonstrate the su-\nperiority of RILQ in improving 2-bit quantized LLM infer-\nence accuracy and fine-tuning performance."}, {"title": "Procedure of RILQ", "content": "RILQ can be applied in two ways. First, it can be used for\ndirect error correction to restore the base model's genera-\ntive capability. Alternatively, RILQ can serve as initialization\nmethod for LoRA, enabling further task-specific fine-tuning.\nCase 1: Direct Error Correction with RILQ\nGenerate Quantized Model. Apply an existing PTQ\nmethod (e.g., RTN, OmniQuant) to obtain a weight-\nquantized student model from the full-precision model\n(teacher).\nAdd LoRA Module. Freeze both models, and add a\ntrainable LORA module to each linear layer of the stu-\ndent model.\nInitialize LoRA Parameters. Initialize LoRA parame-\nters using gradient descent on Model-Loss (Eq. 5) and\nGT-Loss (Eq. 6) on small calibration samples.\nNote: Step 3 varies with LQEC methods; LoftQ uses SVD\nwhile RILQ uses gradient descent on Model (+GT)-Loss.\nThe resulting student model with LoRA is then used for in-\nference.\nCase 2: Task-Specific LORA Fine-Tuning with RILQ\nSetup Fine-Tuning. With RILQ-initialized LoRA, freeze\nstudent model parameters and make LoRA trainable.\nTask-Specific Fine-Tuning. Update the LoRA param-\neters using gradient descent on GT-Loss with a task-\nspecific dataset.\nThis approach is similar to Apple's on-device\nmodel (Gunter et al. 2024), (Sec. 5, accuracy-recovery\nadapter), but improves on 2-bit LQEC (vs. 3~4 bit) with\n200\u00d7 sample efficiency (~50K tokens vs. 10B tokens)."}, {"title": "Experimental Details", "content": "Quantization Settings. We quantize all weights of the\nLLM decoder layer to 2-bit to evaluate how effectively RILQ\ncompensates for the residual quantization error via LoRA.\nFollowing the original works of four existing quantization\nmethods, we apply their respective quantization techniques\nto the weights. The details of the quantization for each\nmethod are outlined below.\nLoftQ (Li et al. 2024). Following the official GitHub\nrepository of LoftQ2, we quantize the base weights to\n2-bit NormalFloat (NF2) (Dettmers et al. 2023) with a\ngroup size of 64 and initialize LoRA using five iteration\nsteps. We exclude 3-bit quantization results for LoftQ\nfrom our evaluation due to the absence of 3-bit quanti-\nzation support in the official implementation.\nOmniQuant (Shao et al. 2024). OmniQuant employs 2-\nbit integer (INT2) quantization for weights, optimizing\nscales and zeros through learnable weight clipping with\na calibration dataset. In our experiments, we use Omni-\nQuant's official GitHub repository and scripts\u00b3, except"}, {"title": "Comparison with BRECQ", "content": "BRECQ (Li et al. 2021) also addresses the impact of\nquantization loss granularity the in CNN domain, propos-\ning that block-wise updates enhance quantization error re-\nconstruction compared to linear-wise optimization. How-\never, a key difference between BRECQ and our analysis is\nthat BRECQ (Li et al. 2021) does not establish the unex-\npected effectiveness of model-loss for 2-bit LQEC. Instead,\nBRECQ argues that model-wise reconstruction is less effec-\ntive than block-wise reconstruction as shown in Table 1 of\n(Li et al. 2021), a point we challenge in this paper. Fig. 4(b)\ndemonstrates that block-loss results in diverged outputs. The\ncrucial distinction lies in BRECQ's application of compen-\nsation to the entire weight matrix through full-rank compen-\nsation, adjusting quantization parameters such as rounding"}]}