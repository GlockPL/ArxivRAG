{"title": "Activation Function Optimization Scheme for Image Classification", "authors": ["Abdur Rahman", "Lu He", "Haifeng Wang"], "abstract": "Activation function has a significant impact on the dynamics, convergence, and performance of deep neural networks. The search for a consistent and high-performing activation function has always been a pursuit during deep learning model development. Existing state-of-the-art activation functions are manually designed with human expertise except for Swish. Swish was developed using a reinforcement learning-based search strategy. In this study, we propose an evolutionary approach for optimizing activation functions specifically for image classification tasks, aiming to discover functions that outperform current state-of-the-art options. Through this optimization framework, we obtain a series of high-performing activation functions denoted as Exponential Error Linear Unit (EELU). The developed activation functions are evaluated for image classification tasks from two perspectives: (1) five state-of-the-art neural network architectures, such as ResNet50, AlexNet, VGG16, MobileNet, and Compact Convolutional Transformer which cover computationally heavy to light neural networks, and (2) eight standard datasets, including CIFAR10, Imagenette, MNIST, Fashion MNIST, Beans, Colorectal Histology, CottonWeedID15, and TinyImageNet which cover from typical machine vision benchmark, agricultural image applications to medical image applications. Finally, we statistically investigate the generalization of the resultant activation functions developed through the optimization scheme. With a Friedman test, we conclude that the optimization scheme is able to generate activation functions that outperform the existing standard ones in 92.8% cases among 28 different cases studied, and -x \u00b7 er f(e\u207b\u02e3) is found to be the best activation function for image classification generated by the optimization scheme.", "sections": [{"title": "1. Introduction", "content": "The choice of activation function plays a pivotal role in determining the learning dynamics, ability to converge, convergence speed, and ultimate performance of a deep neural network. Activation functions determine whether a neuron should be active or not during the training of a neural network (ZahediNasab and Mohseni, 2020). Activation function can be divided into three types: (1) linear activation function, (2) nonlinear monotonic activation function, and (3) nonlinear non-monotonic activation function (Zhu et al., 2021). The first three linear activation functions are binary step function (Sharma et al., 2017), sign function (Huang and Babri, 1998), and identity function. Due to the poor classification ability of these functions, researchers have introduced some nonlinear monotonic functions such as Sigmoid (Narayan, 1997), Tanh (Kalman and Kwasny, 1992), ReLU (Nair and Hinton, 2010; Glorot et al., 2011), and their variant functions (Courbariaux et al., 2015; Gulcehre et al., 2016; Arjovsky et al., 2016). Sigmoid and Tanh are the first two nonlinear functions used in deep neural networks (Apicella et al., 2021). However, both of them have the same limitation of gradient vanishing problem (Hochreiter, 1998). The emergence of ReLU has paved the way to a better activation function by addressing the gradient vanishing problem (Hu et al., 2018). Though ReLU has a monotonically increasing part in the positive x-axis, it becomes saturated on the negative side, which suppresses every negative signal to zero and leads to significant information loss from the neural network. The saturation on the negative side is typically considered as the dying ReLU problem (Lu et al., 2019) and Leaky ReLU (LReLU) (Maas et al., 2013) was thus proposed. LReLU allows a small degree of negative input information by adding a slight slope of 0.1. In addition to that, different variants of ReLU were proposed over the years, such as PReLU (He et al., 2015), RReLU (Xu et al., 2015), and SReLU (Jin et al., 2016). ELU (Exponential Linear Unit) (Clevert et al., 2015) is another extension of the ReLU, designed to address the \u201cdying ReLU\u201d problem. ELU introduces an"}, {"title": null, "content": "exponential term for negative inputs, which helps maintain a small, non-zero gradient and improves the learning process. An update of ELU with self-normalizing properties was proposed as SELU (Klambauer et al., 2017). The nonlinear non-monotonic functions are gaining more popularity for their well-generalized performance across different neural architectures and over complex datasets. Swish (Ramachandran et al., 2017), GeLU (Hendrycks and Gimpel, 2016), and Mish (Misra, 2019) are the most widely used non-monotonic activation functions. The log-Softplus ERror activation Function (SERF) (Nag and Bhattacharyya, 2021) and Logish (Zhu et al., 2021) have been proposed very recently, and they also perform well due to their non-monotonicity. These non-monotonic functions have similar properties, making them better performing than other linear and nonlinear monotonic functions. These functions are unbounded above (x-axis), bounded below, possess a non-monotonic region, demonstrate differentiability almost everywhere, and show a smooth curve at almost all points allowing better information flow through the neural network. The search for an activation function that possesses the mentioned desirable properties and also outperforms the state-of-the-art activation functions has been a point of interest for the researcher over the years. A comparative study of activation functions for convolutional neural networks (CNN) has been carried out by Vargas et al. (2021). The authors (Vargas et al., 2021) proposed two manually developed activation functions for CNN and compared their performance with existing activation functions using CIFAR10 (Krizhevsky, 2009), CIFAR100 (Krizhevsky et al., 2009), CINIC10 (Darlow et al., 2018), and ImageNet (Russakovsky et al., 2015) datasets. In another study, Hu et al. (2022) proposed a parameterized activation function called AReLU to fit the parameters during model training adaptively. Hu et al. (2022) compared the activation function across a range of datasets including CIFAR10 (Krizhevsky, 2009), CIFAR100 (Krizhevsky et al., 2009), miniImageNet (Shaban et al., 2017), PASCAL VOC (Everingham et al., 2015), and COCO (Lin et al., 2014). Task-specific activation function, e.g., a discontinuous hard-limiting activation function, was proposed by Liu and Wang (2008) to solve quadratic programming problems. Except for the Swish, most activation functions described above are hand-crafted. The development of Swish using reinforcement learning has opened the door to search for better activation functions. The superiority of the performance of Swish over ReLU convinced us to take a step forward and search for a more versatile activation function. Evolutionary algorithms have greatly succeeded in optimizing"}, {"title": null, "content": "deep neural network architectures in recent years. This application is not limited to optimizing the architectures but also has significant potential in finding the most effective loss functions, the highest performing activation functions, and the optimal combination of hyper-parameters. Motivated by this, we proposed an evolutionary optimization-based scheme to develop the highest-performing activation function for image classification tasks. We also performed experiments to statistically compare the performance of the developed activation functions with the existing standard ones. Dubey et al. (2022) recently presented a comprehensive survey of activation functions and evaluated their performance on 6 different deep neural networks with CIFAR10 and CIFAR100 datasets. The key difference between our work and this study (Dubey et al., 2022) is that apart from studying the performance of state-of-the-art activation functions across different datasets and neural networks, we have proposed a series of high-performing activation functions for image classification tasks. While Dubey et al. (2022) only focused on CIFAR10 and CIFAR100, we compared the performance of these proposed activation functions with the existing state-of-the-art ones across eight different image classification datasets including two large-scale benchmark datasets, TinyImageNet (200 classes) (Le and Yang, 2015), and CottonWeedID15 (15 classes) (Chen et al., 2022). The structure of this paper is organized as follows. Section II provides an overview of related algorithms proposed to optimize activation functions. Section III describes the proposed activation function optimization scheme. Experimental results are demonstrated in Section IV. The research findings and future work are concluded in Section V."}, {"title": "2. Related Works", "content": "NeuroEvolution (NE) refers to the application of evolutionary algorithms in optimizing neural network architectures and topology. Although the initial application of NE was limited to evolving the weights of a fixed architech-ture (Morse and Stanley, 2016), recently, NE has been utilized to optimize the whole neural network and the network topology (Elsken et al., 2019; Wis-tuba et al., 2019). Real et al. (2017) developed an evolutionary algorithm to discover the optimum network architecture through a novel intuitive mutation. The output of their approach is a fully trained model. In another work, Real et al. (2019) proposed a novel evolutionary approach by introducing an age property and obtained better classification accuracy on the ImageNet"}, {"title": null, "content": "dataset than other hand-designed architectures. Hagg et al. (2017) extended the NE of Augmenting Topologies (NEAT) algorithm (Stanley and Miikkulainen, 2002) to evolve activation functions, weights, and topology of the network. Apart from NE, some other approaches have also been proposed in this pursuit, such as Reinforcement Learning (RL) (Ramachandran et al., 2017; Baker et al., 2016) and Monte Carlo tree search (Negrinho and Gordon, 2017). In addition to optimizing neural network architectures, some studies have also focused on discovering improved activation functions. A summary of the related works can be found in Table 1. Basirat and Roth (2018) proposed a Genetic Algorithm (GA) approach for the first time to search for activation functions, motivated by NEAT algorithm (Stanley and Miikkulainen, 2002). However, they have limited their search space to the existing activation functions as candidate elements for generating new activation functions. For example, the candidate function list included ReLU, ELU, SeLU, Swish, Softplus, Sigmoid, etc. Lapid and Sipper (2022) employed similar search space but with Cartesian Genetic Programming (CGP). It is noteworthy that restricting the search space to only existing activation functions may result in suboptimal outcomes. Bingham et al. (2020) adopted the search space used by Ramachandran et al. (2017) and proposed a GA-based activation function search algorithm for the ResNet20 architecture. They concluded that it is"}, {"title": null, "content": "feasible to evolve activation functions using smaller architecture and datasets and then transfer them to larger models and difficult datasets within the same domain (e.g., image classification). ZahediNasab and Mohseni (2020) developed a neural network with adaptive activation functions to investigate how this adaptation can improve image classification accuracy. Nader and Azar (2021) have utilized GA to develop activation functions for different tasks such as multi-variate classification, regression, and image classification. Similar to Basirat and Roth (2018), they have also limited their search space to the existing activation functions as candidate elements for generating new activation functions (Nader and Azar, 2021). Marchisio et al. (2018) introduced a methodology to improve overall Deep Neural Network (DNN) performance by using a layer-wise activation function. In another study, Singh and Bist (2020) proposed a genetic algorithm and used basic mathematical functions and existing activation functions as candidate elements. Ramachandran et al. (2017) proposed another activation function development framework using Reinforcement Learning (RL) and concluded that carefully selecting the candidate elements of the activation function can result in developing high-performing activation functions. For example, a common structure shared by top activation functions generated by them is b(x, u(x)), where b & u are binary and unary functions, respectively. Besides choosing the search space, defining an effective fitness function for the GA is also crucial. Nader and Azar (2021) and Ramachandran et al. (2017) used validation accuracy as a fitness function and reward function, respectively, while Bingham et al. (2020) used both of the validation loss and validation accuracy separately. Bingham et al. (2020) concluded that the accuracy-based fitness function favors exploration over exploitation. Here exploration refers to the fact that activation functions with poor validation accuracy may still have a reasonable chance of surviving to the next generation. For instance, an activation function that achieves 90% validation accuracy is only 2.2 times more likely to be selected for the next generation than a function with only 10% validation accuracy since e\u2070\u22c5\u2079/e\u2070\u22c5\u00b9 \u2248 2.2 (Bingham et al., 2020). On the other hand, loss-based fitness functions favor exploitation by penalizing the poor activation functions and encouraging the current best ones to move forward to the next generation. For instance, an activation function with a validation loss of 10 is 21,807 times less likely to be chosen for the next generation compared to a function with a validation loss of 0.01 because e\u207b\u2070\u22c5\u2070\u00b9/e\u207b\u00b9\u2070 ~ 21807 (Bingham et al., 2020). A noticeable similarity among studies mentioned in the literature is the"}, {"title": null, "content": "use of tree-based search space that was first introduced by Ramachandran et al. (2017). Among the GA operators, crossover and mutation appeared to be prevalent. While crossover inspires evolution to discover better activa-tion functions more quickly than random search, mutation enables evolution to explore the search space (Bingham et al., 2020). Moreover, mutation helps prevent better-performing activation functions from skewing the early generations of the activation function search (Bingham et al., 2020). Ramachandran et al. (2017) concluded that less intricate activation functions generally outperform the intricate ones. The intricacy of the developed acti-vation function can be defined as the number of core units needed to construct the activation function. A core unit can be formed as b(u\u2081(x), u\u2082(x)) where u\u2081 and u\u2082 represent two unary functions. Ramachandran et al. (2017) also observed that the best-performing activation mostly has 1 or 2 core units. In light of the literature, our study contributes to the following points:\n1. Improved evolutionary search:\n\u2022 Enhanced search space Instead of using only the existing acti-vation functions as candidate elements for the evolutionary algo-rithm as done in previous works, we have incorporated a compre-hensive list of candidate functions (See Section 3.1.1), resulting in a search space with more than 170 million combinations of candi-date solutions. We have added several modified binary functions to the search space e.g., x1 \u00b7 er f(x2), which eventually helped in finding high-performing activation functions.\n\u2022 Efficient fitness function - We proposed a ratio-based fitness func-tion by taking the ratio of validation accuracy and validation loss due to its high likelihood of selecting better chromosomes for the next generation.\n2. Novel activation functions: A series of high-performing activation functions called EELU for image recognition tasks have been proposed.\n3. Study of generalization: We studied the generalization of the pro-posed activation functions across eight image recognition datasets and five state-of-the-art neural network architectures."}, {"title": "3. Methodology", "content": "This paper proposes an Activation Function Optimization Scheme (AFOS) for image classification tasks in deep neural networks. The following sections include the specific details of the implementation of the optimization scheme. First, we have introduced the notations used to describe the AFOS."}, {"title": "Notations", "content": "\u03a9 Search space\n\u03a6 Base neural network\n\u03a8 Mating pool\n\u03c1 Number of classes in the datasets\nT Cross site for crossover\n\u0398 Termination criterion\nAF Activation function\nC Chromosome\nDTe Test dataset\nDTr Training dataset\nDv Validation dataset\nF Fitness function\nf Fitness score\nJ Number of chromosomes in each population\nK Number of times Genetics Algorithm is repeated\nM Mutation\nm Probability of mutation\nN Number of generations"}, {"title": null, "content": "Nm Number of chromosomes generated by mutation\nNx Number of chromosomes generated by crossover\n\u03b7\u03c8 Number of chromosomes generated by selection\nNrand Number of chromosomes generated by random initialization\nP Population\nPo Initial population\nPnext Population of the next generation\nS Selection\nVa Validation accuracy\nVi Validation loss\nX Crossover\nX Probability of crossover"}, {"title": "3.1. Activation Function Optimization Scheme (AFOS)", "content": "This work employed the Genetic Algorithm (GA) as the optimization method. GA is a meta-heuristic and a subclass of evolutionary algorithms inspired by natural selection. Although GA, as a meta-heuristic, does not guarantee to find the optimal solution, it evolves and eventually generates a near-optimal solution. We first developed a chromosome representation of the activation function for the population P in GA. Then, the initial population Po was generated and evolved toward better solutions. Next, a fitness function F was evaluated to find the fitness score f for Po. Then the algorithm checks the termination criteria \u0398 and terminates if the criteria are fulfilled. Otherwise, the algorithm applies the selection S, crossover X, and mutation M to generate population for the next generation (Pnext). The number of chromosomes in each population is J. We repeated the GA for K times. Figure 1 and Algorithm 1 illustrate the framework of AFOS."}, {"title": "3.1.1. Chromosome Representation (C)", "content": "In this study, we used the tree-based encoding similar to the encoding used by Ramachandran et al. (2017). Tree-based encoding is generally suit-able for evolving expressions or functions (Kumar, 2013). We represented each activation function as a tree consisting of unary (u) and binary func-tions (b). While u takes a single input, b takes two inputs as illustrated in Figure 2. We defined our enhanced search space (\u03a9) by using the following list of unary and binary functions:\n\u2022 Unary functions: x, -x, ex, |x|, e\u207b\u02e3, min(x,0), max(x,0), sin(x), cos(x), sinh(x), tanh(x), sin\u207b\u00b9(x), tan\u207b\u00b9(x), sinh\u207b\u00b9(x), tanh\u207b\u00b9(x), er f (x), \u03c3(x), xer f(x), ln(1 + ex)\n\u2022 Binary functions: x\u2081+x\u2082, x\u2081\u00b7x\u2082, max(x\u2081, x\u2082), min(x\u2081, x\u2082), max(max(x\u2081,0), x\u2082), max(min(x\u2081,0), x\u2082), min(max(x\u2081,0), x\u2082), min(min(x\u2081, 0), x\u2082), x\u2081\u00b7e\u02e3\u00b2, x\u2081\u03c3(x\u2082), x\u2081 \u00b7 er f (x\u2082)\nwhere, \u03c3(x) = 1/(1+e\u207b\u02e3) is the sigmoid function and erf(x) = \u221a(2/\u03c0) \u222b\u2080\u02e3 e\u207b\u1d57\u00b2 dt is the gaussian error function. Although we constrained the maximum number of"}, {"title": "3.1.2. Initial Population (Po)", "content": "The initial population represents the individual candidate solutions for the problem. The population size depends mainly on the nature of the prob-lem. For activation function development, previous studies have used a pop-ulation size ranging from 30 to 100. Moreover, Chen et al. (2012) concluded that a large population size is not always helpful for evolutionary algorithms under several conditions. Therefore, in this study, we set the population size J to 30 following the work of (Li et al., 2022) and initiated the popula-tion randomly. The advantage of a random initial population is that it can potentially initiate values from the entire range of the search space."}, {"title": "3.1.3. Fitness Function (F)", "content": "To calculate the fitness score f, the algorithm first decodes the chromo-some C. For each chromosome C, the algorithm replaced the ReLU acti-vation function in the base neural network \u03c6 as shown in Table 2 with the decoded activation function. The base neural network \u03c6 was trained with"}, {"title": "3.1.4. Selection (S)", "content": "The process of choosing two parents from the population for generating offspring is referred to as selection. The notion of selection is to prioritize fitter individuals so that their offspring can have higher fitness. Roulette, tournament, and rank selection are the most popularly used selection meth-ods. In our study, we focused on ranking the chromosomes according to their fitness score f and selecting the ones with higher scores. Through the selection operator, a mating pool \u03a8 consisting of n\u03c8 number of chromosomes was generated for crossover X."}, {"title": "3.1.5. Crossover (X)", "content": "Crossover involves taking two chromosomes as parents and generating offspring from them. The crossover process typically includes three steps: first, selecting two random chromosomes from the mating pool \u03a8; second, choosing a random crossover point \u03c4; and third, swapping the position values along the chromosome string based on a crossover probability \u03c7. Figure 3(a) illustrates an example of this crossover process. In our study, new nx chromosomes were produced using a single-point crossover method."}, {"title": "3.1.6. Mutation (M)", "content": "Mutation involves the random alteration of a gene within a chromosome. While crossover refines existing solutions to discover superior ones, mutation serves as a mechanism to explore the entire search space. This process is crucial for preventing the algorithm from becoming stuck in local optima. In our algorithm, we randomly selected a chromosome from the entire popula-tion and applied a single-point mutation with a probability of m. Figure 3(b) illustrates a single-point mutation, where a dark gray block (gene) is replaced with a light gray block (gene) at random. To maintain the consistency of the activation function, the algorithm replaced unary functions with other unary"}, {"title": "3.1.7. Population for Next Generation (Pnext)", "content": "From the crossover of the n\u03c8 highest-ranked chromosomes, we generated nx new evolved chromosomes. Additionally, we applied single-point mutation to nm chromosomes. Given that the population size for each generation is J, the remaining nrand chromosomes were created randomly. Thus, nrand = J \u2212 (n\u03c8 + nx + nm), which induced the population diversity."}, {"title": "4. Experiments and Results", "content": "In this section, we describe the experimental setup and results from the AFOS and investigate the robustness of the results by analyzing the proper-ties of the evolved activation functions. We conducted our experiments using GPU NVIDIA GeForce RTX 2080 SUPER on Intel Core i9-10900, 32.0 GB RAM, and 64-bit Windows 10 Operating System. We have made the code available at: https://github.com/abdurrahman1828/AFOS."}, {"title": "4.1. Experimental Setup", "content": "In the AFOS, we utilized a base neural network \u03c6 as outlined in Table 2 for calculating the fitness score. This neural network features a typical deep learning architecture, consisting of consecutive convolutional, max-pooling,"}, {"title": null, "content": "and dropout layers, followed by flatten and dense layers in the final stages. The purpose of selecting this base network was to evaluate how the activation functions derived from it perform across different neural architectures. Following the prevalent trends in the literature, we selected the CIFAR-10 dataset (Krizhevsky, 2009) as the primary dataset for running experiments on AFOS. Each chromosome was trained for 15 epochs with a batch size of 64. The optimizer used was Stochastic Gradient Descent (SGD) with a learn-ing rate of 0.01 and momentum set at 0.9. We employed the he_uniform initializer for the kernel and maintained the padding option as same. The loss function utilized was categorical_crossentropy. The detailed parameters for the optimization algorithm are provided in Appendix A. As illustrated in the convergence graph in Figure 4, significant improvements were not ob-served beyond 40 generations. Hence, we set the termination criterion \u0398 for AFOS to be 40 generations. The AFOS method required 1 to 1.5 GPU days under varying initial conditions. To optimize runtime, we implemented two stopping criteria: first, halting evaluation if the loss function value became undefined, and second, stopping if the accuracy after the first epoch did not exceed a threshold of 0.25. This allowed us to proceed efficiently to the next chromosome. For different initial conditions, we initialized AFOS with various random seeds to create the initial populations for five replications. This approach revealed a consistent prevalence of the er f (x) unary function and the x \u00b7 f(x) binary function among the top-performing activation functions in these repli-cations. We selected the top 15 activation functions from AFOS based on the validation accuracy of the model on the CIFAR10 dataset. Then, we trained the base neural network \u03c6 with the developed activation functions"}, {"title": null, "content": "from scratch for 50 epochs and evaluated them on the test set (DTe) of CIFAR10. In Table 3, we mentioned the expressions of the developed activation functions and their test accuracy (mean and standard deviation) of 5 replications."}, {"title": "4.2. Results of Activation Function Optimization Scheme", "content": "4.2.1. Superior Properties of the Developed Activation Functions\nActivation functions of the EELU series are non-linear, bounded below, and unbounded above, as shown in Figure 5(b) to Figure 5(e). Being bounded below helps in ensuring the strong regularization effects (Misra, 2019). On the contrary, the unboundedness above is desired because it avoids satu-ration (significantly slow training due to near-zero gradients) (Glorot and Bengio, 2010). The negative bounds for the EELU series are approximately -0.3985, -0.3985, -0.1898, and -0.2755, respectively. Furthermore, the EELU series shows a non-monotonic bump in the positive x-axis region (x > 0) for EELU-2 and a similar bump in the negative x-axis region (x < 0) for the oth-ers. To be well-suited for gradient-based optimization, avoiding singularity"}, {"title": null, "content": "(especially due to the lack of differentiability) is crucial. We investigated and found that each activation of the EELU series is continuously differentiable. In terms of smoothness and convergence, the EELU series shows better at-tributes than ReLU. A smooth transition in the output landscape effectively indicates a smoother loss landscape which eventually leads to a lower number of local minimums and faster convergence. To obtain the output landscape of the ReLU and EELU series, we employed a dense neural network of 4 layers following the work of Li et al. (2018). These landscapes as shown in Figure 6 are evident that the EELU series shows smoother transitions which can be a potential reason for the high performance of the EELU series in general. A summary of the properties of the EELU series compared to the ReLU is demonstrated in Table 5. All these attributes are evidence that the EELU series is suitable for image classification applications in deep neural networks."}, {"title": "4.2.2. Performance on Different Neural Networks and Datasets", "content": "The high performance of an activation function on a particular neural network with a specific dataset may not always secure the best performance"}, {"title": null, "content": "in other architectures and datasets. To check the generalization over various neural architectures, we considered computationally heavy to light neural networks such as ResNet50 (He et al., 2016), AlexNet (Krizhevsky et al., 2012), VGG-16 (Simonyan and Zisserman, 2014), MobileNet (Howard et al., 2017), and the base network \u03c6 using the CIFAR10 dataset. Then again, we tested the performance of the activation functions on different image classifi-cation datasets such as MNIST (LeCun et al., 2010), Fashion MNIST (Xiao et al., 2017), and Imagenette (Howard, 2019) to validate the generalization over datasets. We used the base neural network \u03c6 mentioned in Table 2 for understanding the performance across different datasets. To check how the activation function performs in different domains such as agriculture, and medical science, we also tested the Beans dataset (Lab, 2020) and Col-orectal Histology dataset (Kather et al., 2016) respectively. Furthermore, we evaluated the performance with large-scale benchmark datasets including TinyImageNet (Le and Yang, 2015) and CottonWeedID15 (Chen et al., 2022) on ResNet50 (He et al., 2016), VGG-16 (Simonyan and Zisserman, 2014), and MobileNet (Howard et al., 2017). Finally, we compared the proposed activa-tion functions using a Vision Transformer (ViT) model (Hassani et al., 2021) with three recently proposed activation functions. Therefore, our experiment included testing datasets of sizes ranging from 1,295 to 100,000 images. A summary of the datasets used in this study is outlined in Appendix B. All of these datasets are balanced (the number of images for each class is reason-ably equal), except for the CottonWeedID15 (Chen et al., 2022), and publicly available."}, {"title": "4.2.3. Statistical Comparison", "content": "We conducted a Friedman Test (Friedman, 1940) to investigate the sig-nificance of differences in the performance of the activation functions. The null hypothesis Ho states that there is no difference between the performance of all activation functions, while the alternative hypothesis Ha states that there is. The Friedman statistics is calculated by the following equations (1) & (2):\nRi = (\u2211q=1^NM V\u2081q)/NM\n\u03c7\u00b2 = (12NM/(NA(NA+1))) * (\u2211(Ri - (NA(NA+1)/4))^2)"}, {"title": null, "content": "where, NA denotes the number of activation functions compared, and NM represents the number of neural networks (or the number of datasets for comparing the performance among different datasets). In our study, Na and NM are 10 and 5 (16 for datasets), respectively. Ri is the i-th activation function's average rank and riq is the i-th activation function's rank on the q-th neural network (or dataset). For each neural network (or dataset), the activation functions were ranked from one to ten. A ranking of one represents the lowest accuracy, and conversely, ten represents the highest accuracy. Table 10 and Table 11 show the Friedman test results for accuracy at a significance level of 0.05. For large values of NA and NM, Friedman statistics follows \u03c7\u00b2 distribution (Iman and Davenport, 1980), therefore we calculated the \u03c7\u00b2 value for both neural network and datasets. In this study, the degree of freedom is (NM-1) where NM is the number of individual cases considered for performance evaluation. Therefore, the degree of freedom is 4 and 15 for neural networks and datasets, respectively. The X\u00b2Critical values for neural networks and datasets were approximately 9.49 and 25.00, respectively. Since the X\u00b2Critical value is lower than \u03c7\u00b2 statistics in both cases, we reject the null hypothesis. Therefore, we conclude that performance is not the same for all ten activation functions. Additionally, higher rank and significant difference in the performance indicate the superior performance of the proposed activation function series. The average rank (Ri) of EELU-2 is the highest in terms of different neural networks and different datasets among all ten activation functions, which ultimately indicates the superiority of EELU-2 over the state-of-the-art activation functions. The properties of EELU-2 such as non-linearity, differentiability, non-monotonicity, and lower-boundedness are similar to the other activation functions of the EELU series. The only dissimilarity, a rather unique property of EELU-2 is in the direction of the upper bound, which is negative. Even with this unique characteristic, EELU-2 outperformed the state-of-the-art activation functions in almost all cases. Another observation from the Friedman test is about the generalization of the performance of the EELU series. The \u03c7\u00b2 value for the dataset-based ranking is more significant than that of the neural network-based ranking. This is because the p-value associated with the \u03c7\u00b2 value of 81.84 is less than 0.001, while the p-value associated with the \u03c7\u00b2 value of 17.86 is significant but not as strong, as it is greater than 0.001 but less than 0.05. This lower p-value indicates the consistent performance of the EELU series across differ-ent datasets, in other words, \u2018generalization'. Conversely, the generalization"}, {"title": null, "content": "across different neural networks is less strong compared to different datasets."}, {"title": "4.2.4. Tests on Large-scale Benchmark Datasets", "content": "To evaluate the effectiveness of the EELU series on large-scale and chal-lenging datasets, we conducted experiments with the TinyImageNet bench-mark dataset (200 classes, balanced) (Le and Yang, 2015) and the Cotton-WeedID15 dataset (15 classes, imbalanced) (Chen et al., 2022) as shown in Table 12. For the CottonWeedID15 dataset (Chen et al., 2022), we per-formed 5-fold cross-validation since it does not have any specified test set. Moreover, since this dataset has class imbalance i.e., the number of images in each class is not the same, we reported the mean and standard devia-tion of the F1-scores in Table 12. For the TinyImageNet dataset (Le and Yang, 2015), we trained the respective models with the training set and then evaluated the performance on the validation set. Due to the longer training time and limited resources, we evaluated the ReLU activation function as the baseline along with the EELU series."}, {"title": "4.2.5. Tests on Vision Transformer", "content": "Vision transformers (Dosovitskiy et al., 2020) are widely used unique com-puter vision models with high performance. Therefore, we also tested the per-formance of our developed activation functions on"}]}