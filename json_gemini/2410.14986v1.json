{"title": "NeuralMAG: Fast and Generalizable Micromagnetic Simulation with Deep Neural Nets", "authors": ["Yunqi Cai", "Jiangnan Li", "Dong Wang"], "abstract": "Micromagnetics has made significant strides, particularly due to its wide-ranging applications in magnetic storage design and the recent exciting advancements in spintronics research. Numerical simulation is a cornerstone of micromagnetics research, relying on first-principle rules to compute the dynamic evolution of micromagnetic systems based on the renowned LLG equation, named after Landau, Lifshitz, and Gilbert. However, simulations are often hindered by their slow speed, primarily due to the global convolution required to compute the demagnetizing field, which involves full interaction among any two units in the sample. Although Fast-Fourier transformation (FFT) calculations reduce the computational complexity to O(NlogN), it remains impractical for large-scale simulations. In this paper, we introduce NeuralMAG, a deep learning approach to micromagnetic simulation. Our innovative approach follows the LLG iterative framework but accelerates demagnetizing field computation through the employment of a U-shaped neural network (Unet). The Unet architecture comprises an encoder that extracts aggregated spins at various scales and learns the local interaction at each scale, followed by a decoder that accumulates the local interactions at different scales to approximate the global convolution. This divide-and-accumulate scheme achieves a time complexity of O(N), significantly enhancing the speed and feasibility of large-scale simulations. Unlike existing neural methods, NeuralMAG concentrates on the core computation rather than an end-to-end approximation for a specific task, making it inherently generalizable. To validate the new approach, we trained a single model and evaluated it on two micromagnetics tasks with various sample sizes, shapes, and material settings: (1) basic LLG dynamic evolution, and (2) MH curve estimation. The results show that the model maintains reasonable accuracy and is significantly faster than the conventional FFT-based method, achieving a sixfold speedup for large-size models. NeuralMAG has been published online and is available for users to download.", "sections": [{"title": "Main", "content": "Born in the early 20th century to address the issues of magnetic domain and hysteresis, micromagnetics has evolved into the fundamental methodology for understanding the magnetic behavior of materials from a microscopic view [1, 2]. The practicality of micromagnetics has earned itself popularity within the communities of magnetic storage and permanent magnets, alongside growing demands at the forefront of spintronics research [3-12]. The theoretical framework, established by L. Landau and further developed by W. F. Brown [1, 13], aims to account for various forms of magnetic energies and to determine the system's evolutionary path based on the energy landscape. This evolutionary process is captured by the renowned Landau-Lifshitz-Gilbert (LLG) dynamics [14], as illustrated in Fig. 1(a) and detailed below:\n$\\frac{dm}{dt} = -\\gamma m \\times H_{eff} - \\frac{\\alpha}{M_s} m \\times (m \\times H_{eff})$\nwhere $m$ is the magnetization vector, $H_{eff}$ the effective field, $\\gamma$ the electron gyromagnetic ratio, and $\\alpha$ a phenomenological damping parameter. The effective field $H_{eff}$ serves as a consolidated representation of magnetic energies, encompassing the external magnetic field $H_{ext}$ derived from Zeeman energy, the anisotropy field $H_{aniso}$ from magnetocrystalline energy, the demagnetizing field $H_{demag}$ due to magnetostatic interaction, and the exchange field $H_{exch}$ from the Heisenberg exchange interaction, as follows:\n$H_{eff} = H_{ext} + H_{aniso} + H_{demag} + H_{exch}$\nThe above equation is typically solved using numerical computation methods, among which the finite-differential method (FDM) and the finite-element method (FEM) are most commonly employed. However, a significant challenge arises from the computation of the demagnetizing field $H_{demag}$ (also referred to as the magnetostatic field), which is notoriously difficult. In the context of the FDM, for any specific cell denoted as $(i, j, k)$, the primary computation of $H_{demag}(i, j, k)$ requires summing up the interactions between this cell and every other cell in the model, represented as $(l,m,n)$:"}, {"title": null, "content": "$H_{demag}(i, j, k) = M_s \\sum_{lmn} \\Omega(l - i, m - j, n - k) \\cdot m_{lmn}$\nwhere $M_s$ represents the magnitude of magnetization, known as saturation magneti- zation, and $\u03a9$ represents the location-invariant magnetostatic interaction tensor [15]. Crucially, the strength of magnetostatic interactions adheres to a 1/r law, indicating that long-distance interactions might be sufficiently strong and so cannot be simply disregarded, resulting in a computational complexity of $O(N^2)$. This high complexity has emerged as a significant concern within micromagnetic studies, leading to simula- tions being limited to small-scale, simplified tasks, which notably curtails their utility in real-world applications. In 1988, Zhu et al. introduced Fast-Fourier transformation (FFT) calculations into the FDM scheme [16], by noticing that Eq. 3 is essentially a convolution between $m$ and $\u03a9$. This method dramatically reduced the complexity of magnetostatic calculations from $O(N^2)$ to $O(Nlog(N))$, quickly elevating micromagnetic simulation into a powerful tool for designing hard-disk drivers (HDDs) [16]. However, despite these improvements, FFT computations, as illustrated in Fig. 1(b), still struggle to scale to large-size problems. For instance, the size of a feasibly modeled permanent magnet remains commonly below one micron, a scale even smaller than a single grain in actual materials[17-19]. The enhancement of computation efficiency is eternally required in micromagnetic research.\nRecently, the rapid evolution of deep learning (DL) methods has brought a sig- nificant revolution in traditional computational physics[20-31]. In micromagnetics, significant efforts have been devoted to training DL models as alternative tools for magnetostatic calculation. Khan et al. trained a convolutional neural network (CNN) to calculate the demag distribution in electromagnetic motors [32]. Kovacs et al. implemented a physics-informed neural network (PINN) to substitute the calcula- tion of magnetostatic interaction in the FEM micromagnetic scheme [33]. The high accuracy of PINN on the \u00b5MAG standard problem #3 was later demonstrated by Schaffer et al [34]. Additionally, some research efforts focused on learning the direct time evolution of the magnetization state. For instance, Kovacs et al. trained a neu- ral network to simulate the magnetic dynamic process [35] and demonstrated their approach on the \u00b5MAG standard problem #4. Chen et al. used neural ordinary dif- ferential equations (NODE) to reproduce the trajectory of magnetic skyrmions [36]. An acceleration factor exceeding 200 times was reported in comparison to traditional micromagnetic simulation. Despite these promising results, the reported DL models for micromagnetics were typically designed to solve specific problems, with little focus on generalization across various scales and configurations. Consequently, none of them can yet be considered a general computational tool for micromagnetic simulations.\nIn this study, we introduce NeuralMAG, a general and generalizable computa- tion framework for micromagnetic simulation by integrating deep learning methods and the LLG dynamics. The core concept involves using a deep neural network to approximate $H_{demag}$, the most computationally demanding component of the effective field, and utilizing this approximation to conduct the LLG simulation. As illustrated in Fig. 1(c), NeuralMAG utilizes a Unet architecture comprising an encoder and a decoder, with corresponding layers of the encoder and decoder interconnected by skip"}, {"title": null, "content": "connections[37, 38]. The encoder consists of a series of convolution layers that aggregate the input magnetization vectors into a hierarchy of granularity levels, where each layer's convolution kernels encode the local interaction rules at the respective granularity level. The local interactions at various levels are subsequently integrated by the decoder, leveraging the skip connections and the upsampling capabilities of the deconvolution layers. The output of the Unet, an estimation of $H_{demag}$, serves to conduct subsequent LLG iterations, establishing a Unet/LLG iterative computation scheme.\nIntuitively, the Unet decomposes the large-scale convolution in Eq. 3 into a hierarchy of small-scale convolutions, yielding an approximation for $H_{demag}$. We demonstrate that this approximation, despite the inevitable accuracy loss, results in an algorithm with a complexity of O(N) under specific conditions, significantly out- performing the FFT approach whose complexity is $O(Nlog(N))$. This offers a valuable opportunity to balance efficiency and accuracy, enabling simulations for large-size"}, {"title": "Results", "content": "The Unet model of NeuralMAG was trained using 140k fully random samples for each of the three sizes: 32, 64, and 94. Two-thirds of these samples were 'masked' by ran- dom shapes at arbitrary locations, and the masked areas were set to zero. A random external magnetic field was also applied to these 'masked' samples. Utilizing these random samples, conventional FFT/LLG simulations were performed, and the pairs $(m, H_{demag})$ from every step of the simulation were collected as training data for the Unet model. After training, the Unet was employed to replace the FFT to conduct Unet/LLG simulations. We emphasize that this model is generalizable, meaning that the single model can be applied across all micromagnetic tasks based on LLG dynamics, and suitable for samples of varying sizes, shapes, and magnetic materials.\nWe evaluated the Unet/LLG simulation on two tasks: (1) Basic dynamics sim- ulation to assess the accuracy of LLG iterations using the Unet approximation; (2) MH curve prediction employing LLG to determine the relaxed magnetization under a sequentially changing external magnetic field. Finally, the computation speed was analyzed to gauge the efficiency improvements."}, {"title": "Basic dynamics", "content": "The first experiment assesses the Unet/LLG approach's accuracy in simulating basic magnetization dynamics, by comparing the trajectories and ground states derived from the Unet/LLG iterations against those from conventional FFT/LLG iterations. The experiment utilizes a soft magnetic thin film as the material system, where magnetic topologies, vortex and anti-vortex [40, 41] emerge typically from a random magneti- zation configuration. The dynamic process primarily involves the repulsion between vortices of the same type and the attraction and subsequent annihilation between vortex/anti-vortex pairs [42]. Given the absence of the magnetocrystalline energy and the external field ($K_u = 0$ and $H_{ext} = 0$), the dynamics are governed by a competi- tion between $H_{demag}$ and $H_{exch}$, making them particularly sensitive to the accuracy of $H_{demag}$. A set of benchmarks was established to evaluate the prediction accuracy of the ground states. These benchmarks focus on two critical characteristics of the magnetic thin film's ground state: (1) the number of vortices and (2) the orientation and polarization of these vortices. The specific methodologies employed are detailed in the Methods section.\nFig. 3 showcases an example of the simulation with both the sample's shape and initial state randomized. We found that Unet-based simulation on fully randomized ini- tial states often results in large prediction errors. A cooling process has been designed to solve the problem, which runs the FFT-based simulation until a relatively stable state is achieved, as illustrated in Fig. 3(a)-(c). After the cooling process, the Unet can then replace FFT to continue with the simulation. Fig. 3(d)-(i) demonstrate how the Unet-based simulation closely aligns with the outcomes of the FFT-based process.\nFor quantitative analysis, three test groups were constructed, each consisting of 100 random samples: (1) The Square group, comprising square samples with sizes ranging from 32 to 128. Importantly, the size of 128 was not included in the model's training set, acting as a test for cross-scale generalizability. (2) The Random Shape group,"}, {"title": "MH curve", "content": "The second experiment involves simulating the magnetization-field (MH) curve of a magnetic thin film. Contrary to the first experiment which began with randomized initial conditions, this experiment starts from a saturated state, meaning all magne- tization vectors are aligned in one direction due to a strong external magnetic field ($H_{ext}$). Subsequently, the external field is sequentially weakened, then reversed in direction and increased again, at each step with the state stabilized before the field is changed. The average magnetization along the field direction is plotted against the field strength, illustrating the magnetic material's response to changes in its environment. MH curves hold significant importance in both practical applications and theoretical analysis. In practical applications, experimental measurements and numerical simula- tions of MH curves are crucial for designing magnetic recording media and rare-earth permanent magnets[43, 44]. Theoretically, the MH curve gives insights into the energy landscape of the magnetic system under varying external fields ($H_{ext}$), showcasing how magnetization configurations adapt to remain at the energy minimum [45]. Thus, distinct from the first experiment's emphasis on LLG dynamics, this MH curve exper- iment assesses the accuracy of the Unet model in predicting the energy contributions of magnetostatic interactions.\nAn example of the MH prediction is presented in Fig.4(a)-(c). Generally, two aspects of the MH curves are mostly interesting to researchers: (1) a smooth transition from saturation to $H_{ext} = 0$, and (2) a sharp magnetization reversal within a narrow"}, {"title": null, "content": "external field range (magnetization reversal). Two critical points on the curve, $M_r$ (remanence) and $H_c$ (coercivity), are usually adopted to characterize these two aspects, respectively [46]. $M_r$, or remanence, is the magnetization at $H_{ext} = 0$, and $H_c$, or coercivity, is the external field required to reduce the magnetization to zero. These two points are used to quantify the prediction accuracy of the Unet/LLG approach for MH tasks relative to the FFT/LLG approach. As in the previous section, this experiment also emphasizes the cross-scale, cross-shape, and cross-material generalizability.\nThe outcomes of the MH curve experiment are depicted in Fig.4(d)-(o), with addi- tional details provided in Fig.8 within the Extended Data. Of the 60 simulation cases, 49 showed a strong agreement between Unet/LLG and FFT/LLG, with no signif- icant discrepancies in either coercivity or remanence (defined as $\\Delta H \\geq 25 Oe$ or $\\Delta M \\geq 0.03$), resulting in an overall accuracy of 82%. Focusing solely on one of the features, the prediction accuracies are 92% for remanence and 85% for coercivity. These findings confirm the Unet/LLG method's capacity in MH tasks, in particular its generalizability as the test involves complex settings on sample size, shape and material.\nA notable observation is the improved generalizability with a large magnetocrys- talline energy $K_u$. Severe mismatch exists when $K_u$ is small, in particular with small sizes. When $K_u$ increases to $3.0\\times10^5 erg/cc$, severe mismatch is suppressed, remaining only in the size of 32, and it is further suppressed in all sample sizes when $K_u$ increases to $4.0 \\times 10^5 erg/cc$. The behavior can be attributed to the increasing contribution of the size-invariant $H_{aniso}$ when $K_u$ is large. This characteristic underscores the feasi- bility of our Unet/LLG approach for MH tasks involving larger sizes and nonzero $K_u$ values, which holds practical significance for the study of full-size magnetic devices, such as reading sensors or storage components."}, {"title": "Computational Efficiency Evaluation", "content": "Micromagnetic simulation, which explores the magnetic behavior of materials at the microscale, is notably resource-intensive, particularly for large-scale samples. Consequently, there is a critical need to enhance computational efficiency and reduce memory consumption. Our study compares the performance of three simulation methodolo- gies: FFT/LLG, Unet/LLG, and Unet/LLG accelerated by TensorRT, with results evaluated on a Nvidia RTX-3090 GPU.\nTable 2 presents a comparative analysis of the conventional FFT/LLG approach versus the TensorRT-accelerated Unet/LLG model, focusing on computation speed and GPU memory usage. With increasing sample size, the TensorRT-accelerated Unet/LLG approach is significantly faster than the FFT/LLG method. Remarkably, for a sample size of 2048, the Unet/LLG method realized a 6.7-fold enhancement in computation speed, alongside a reduction to 0.53 times its original memory consumption.\nThe improved speed and decreased memory requirements of the TensorRT- accelerated Unet/LLG model compared to the FFT/LLG approach stem the lower computational complexity O(N), as well as the advanced optimization capabilities of the TensorRT framework integrated with the PyTorch ecosystem. TensorRT's opti- mization techniques, including mixed-precision inference, layer fusion, and kernel auto-tuning, facilitate the efficient execution of deep learning models. These methods contribute to significant enhancements in computational speed and memory efficiency.\nTable 6 in the Extended Data provides a more detailed comparison of the com- putational efficiency of three simulation methodologies: FFT/LLG, Unet/LLG, and Unet/LLG accelerated by TensorRT. The analysis reveals that the Unet/LLG model, even without TensorRT, offers a significant speed advantage over the conventional FFT/LLG approach, especially when the sample size is large. This advantage demon- strates the benefit of the lower computational complexity O(N) with UNet compared to $O(N log(N))$ with FFT. It is further magnified by incorporating TensorRT accelera- tion. This is particularly evident in the marked reduction in the $H_{demag}$ computation time per iteration, showcasing the considerable efficiency gains achieved through Unet and TensorRT acceleration. The advancements on speed and resource consumption enable simulations of large samples."}, {"title": "Discussion", "content": "A key strength of the Unet/LLG approach compared to existing DL-based methods is its exceptional generalizability, i.e., a single model is capable of handling diverse tasks across samples of varying sizes, shapes, and material properties, establishing it as a versatile tool for micromagnetic simulation. Theoretically, this generalizability originates from the universal physical rule governing the cross-cell interaction dur- ing demagnetization. More specifically, regardless of the cell's location, the cross-cell interaction follows the same principle and so can be calculated with the same pro- cedure, conditioned on a set of parameters determined by the material's physical characteristics.\nTechnically, this capability is derived from two pivotal designs within Neural- MAG: the Unet/LLG nested framework and the implementation of local learning in Unet. In the Unet/LLG framework, the Unet component is specifically responsible for modeling the cross-cell interactions, while the evolutionary dynamics and material- specific settings are managed by the LLG iterations. This arrangement implies that if the physical rule governing cross-cell interaction is universally applicable, then Unet/LLG framework possesses inherent generalizability concerning material settings and can be applied to any task that relies on LLG dynamics, i.e., task and material generalizability.\nThe ability to generalize across sample sizes and shapes is rooted in its local learn- ing feature of the Unet model. Let's delve into the Unet architecture to understand how it computes the demagnetizing field. In the first convolution layer, the interac- tion among primary cells is represented by the convolution kernels, with a limited kernel size of 3 capturing only local interactions. To address long-range interactions, the encoder progressively increases the receptive field of the neurons through down- sampling, enabling the learning of local interactions on a broader scale, as depicted in Fig. 1(c). Within each hidden layer, 2 x 2 neighboring cells are consolidated into a single cell, establishing a new granularity level that differs in cross-cell interactions from previous levels. The convolution kernels, operating at this enlarged granularity, are expected to capture larger-scale interactions while still maintaining local learning"}, {"title": "Generalizability", "content": "due to the limited kernel size. The decoder aggregates local interactions across various granularities by extensively utilizing skip connections. Our fundamental assumption is that by aggregating local interactions at different levels of granularity, the Unet model can approximate global cross-cell interactions. The accuracy of this approximation will be discussed later; for now, our focus is on elucidating how the Unet architecture enables size and shape generalizability.\nFirstly, it's important to note that the local interaction rule is independent of shape, making the Unet model inherently capable of generalizing across different shapes. Additionally, since samples of any size can be broken down into small blocks, and all these blocks adhere to the same physical rules for local interaction, the entire demag- netization field can be calculated on a block-by-block basis, ensuring generalizability across different sizes. This block-by-block processing is illustrated in the fourth hidden layer of the Unet architecture, as depicted in Fig. 1(c), where the size of the feature maps scales linearly with the size of the input samples."}, {"title": "Bound of accuracy", "content": "As previously mentioned, Unet is designed to approximate global inter-cell interac- tions in the computation of the demagnetizing field by aggregating local interactions across multiple granularity levels, from coarse to fine. Firstly notice that deep CNN models are universal approximators[47], which means that with a sufficiently complex Unet structure, the accuracy of approximation is assured, at least for sample sizes rep- resented in the training dataset. However, to maintain efficiency, the model structure needs to be compact, for instance the Unet architecture depicted in Fig. 1(c). We will discuss the upper bound of accuracy achievable with this design.\nThe first scenario we will discuss occurs when the size of the input sample does not exceed the receptive field of a single neuron in the bottleneck layer, specified as 16 in Fig. 1(c). Under these conditions, the dependencies between any two cells within the sample can be accurately represented, leading to effectively modeling the global inter-cell interaction. Therefore, prediction accuracy is assured, provided that the con- volutional kernels are adequately comprehensive and the learning process perfectly converges.\nThe second scenario arises when the sample size is larger than the receptive field of neurons in the bottleneck layer. Under these conditions, the feature maps in the bottleneck layer do not consist of singular points but rather contain multiple neurons. The interactions among these bottleneck neurons are inadequately represented (even though some neighboring neurons may interact via the deconvolution kernel), indi- cating a lack of capacity to accurately depict the interactions between any two cells in the sample. Therefore, Unet serves as an imperfect approximator in this scenario, with the approximation deteriorating as the sample size increases.\nTo enhance accuracy for large samples, one possible approach is to increase the depth of the encoder. This approach is feasible as the computational complexity is O(N) if the receptive field of the pooling operations is sufficiently large, a point that will be elaborated on shortly. However, this approach requires the generation of corresponding training samples via traditional FFT-based simulation, a process cur- rently not feasible due to its high cost. A potential solution could involve developing a"}, {"title": "Complexity", "content": "We can begin by analyzing the computational complexity of the encoder in the Unet model. Let's assume that all the convolution kernels are of sizer with a stride of 1. Additionally, suppose the size of the input samples is N = n x n, and the first hidden layer has c channels. As we progress through each subsequent convolution layer, the resolution decreases by a factor of 1/a, while the number of channels increases by a factor of $B$. This sequential reduction in feature map size leads to a single 1 x 1 map over $log_{\\alpha^2} N$ steps. The number of multiplication operations involved in this encoding process can be calculated as follows:\n$\\begin{aligned} \\\\ &6r^2N \\times c & [Input] \\\\ +&r^2c \\frac{N}{\\alpha^2} \\times Bc & [H1] \\\\ +&r^2Bc \\frac{N}{\\alpha^4} \\times B^2c & [H2] \\\\ +&... \\\\ +&r^2 \\frac{N}{\\alpha^{2k}} \\times B^{2k-1}c & [Hk] \\\\ +&... \\\\ +&r^2 c \\frac{N}{\\alpha^{2 log_{\\alpha^2} N}} \\times B^{2 log_{\\alpha^2} N-1}c & [H_{log_{\\alpha^2} N}] \\end{aligned}$\nSimple calculation shows that the computation amounts to:\n$6r^2N \\times c + r^2c^2 B \\frac{N}{\\alpha^2} (1 + \\frac{B^2}{\\alpha^2} + ... + \\frac{B^{2log_{\\alpha^2}N-1}}{\\alpha^{2log_{\\alpha^2} N}})$\nIn the scenario where $\\alpha = \\beta$, the computational complexity can straightforwardly be demonstrated as $O(Nlog N)$, given that the summation includes $log N/log \\alpha^2$ items. For cases where $\\alpha \\neq B$, the calculations within the brackets form a geometric series with a common ratio $q = B^2/\\alpha^2$, resulting in the computation being expressed as follows:\n$6r^2N \\times c + r^2c^2 B \\frac{N}{\\alpha^2} \\frac{1-q^{log_{\\alpha^2} N}}{1-q}$\nIf $\\beta <\\alpha$, the above computation is bounded by\n$6r^2N \\times c + r^2c^2 B \\frac{N}{\\alpha^2} \\frac{1}{1-q}$"}, {"title": null, "content": "which means the complexity is O(N). If $\\beta > \\alpha$, the computation is:\n$\\begin{aligned} \\\\ &6r^2N \\times c + r^2c^2 B \\frac{N}{\\alpha^2} \\frac{q^{log_{\\alpha^2} N} - 1}{q - 1} \\\\ <& 6r^2N \\times c + r^2c^2 B \\frac{N}{\\alpha^2} \\frac{q^{log_{\\alpha^2} N}}{q - 1} \\\\ =&6r^2N \\times c + r^2c^2 \\frac{B}{\\alpha^2} \\frac{[(\\frac{\\beta}{\\alpha})^2]^{log_{\\alpha^2} N}}{(\\frac{\\beta}{\\alpha})^2 - 1} \\\\ =&6r^2N \\times c + r^2c^2 \\frac{B}{\\alpha^2} \\frac{[(\\frac{\\beta}{\\alpha})^{log_{\\alpha} N}]^2}{(\\frac{\\beta}{\\alpha})^2 - 1} \\\\ =&6r^2N \\times c + r^2c^2 \\frac{B}{\\alpha^2} \\frac{[(\\frac{\\beta}{\\alpha})^{log_{\\beta} N}]}{(\\frac{\\beta}{\\alpha})^2 - 1} \\\\ =&6r^2N \\times c + r^2c^2 \\frac{B}{\\alpha^2} \\frac{N^{log_{\\alpha} \\frac{\\beta}{\\alpha}}}{(\\frac{\\beta}{\\alpha})^2 - 1} \\end{aligned}$\nTherefore, the computational complexity is $O(N^{log_{\\alpha}}\\frac{\\beta}{\\alpha}))$, which is higher than O(N) with $\\beta > \\alpha$.\nFinally, the decoder's complexity is twice that of the encoder's, a consequence of concatenating the feature maps through skip connections. In total, the computational complexity of the Unet model aligns with that of the encoder.\nThe derivation previously discussed utilizes a pyramid architecture, which system- atically reduces feature maps to singular points, creating a 'complete' structure. This design guarantees interaction between each pair of cells. More commonly, a truncated architecture, as illustrated in Fig. 1, is employed, offering a straightforward O(N) complexity, irrespective of the values of a and $\\beta$. This truncated architecture trades a degree of accuracy for increased computational speed. Additionally, considering that FFT's complexity is $O(Nlog N)$, our method whether adopting the truncated or the pyramid version with $\\alpha > \\beta$, theoretically surpassing the FFT-based approach in speed, particularly as N increases."}, {"title": "Methods", "content": "Following the Landau-Lifshitz-Gilbert (LLG) dynamical equation, as presented in Eq.(1), the evolutionary process of a micromagnetic system can be numerically simulated by partitioning the material into small units and calculating magneti- zation within each unit. This computation unfolds iteratively, yielding a detailed temporal evolution of the system. Two popular simulation methods are the finite- differential scheme[48] and the finite-element scheme [49]. In this study, we adopt the finite-differential scheme.\nIn the finite-differential scheme, a magnetic material sample is discretized into a regular lattice, with each cell assigned a magnetic moment m, i.e., the magnetiza- tion vector M divided by its saturation magnetization $M_s$. Determining the state of a sample means knowing the directions of $m_{ijk}$ for every cell. The evolution of"}, {"title": "Micromagnetic simulation", "content": "$M_{ijk}$ is primarily governed by the local energies and their interactions, including Zee- man energy $E_z$ as a response to the applied field $H_{ext}$, magnetocrystalline anisotropy energy $E_{aniso}$, Heisenberg exchange interaction $E_{exch}$ among neighbors, and mag- netostatic/demagnetizing energy $E_{demag}$ between pairs of $m_{ijk}$. Following Landau's suggestion, energy variation in each cell creates an effective field $H_{eff} = - \\delta E / \\delta m M_s$, and the temporal evolution of the magnetic moment can be described by the LLG equation, as shown in Eq.(1) or reformed below:\n$dm/dt = \\gamma H_{eff} - \\frac{\\alpha}{M_s} m \\times (H_{eff} \\cdot m)$\nThe effective field $H_{eff}$ is calculated as the sum of several components: the external field $H_{ext}$, the exchange field $H_{exch}$, the anisotropy field $H_{aniso}$, and the demagnetizing field $H_{demag}$, as presented in Eq.(2) and relisted below:\nThe effective field $H_{eff}$ is computed by summing up the external field $H_{ext}$, the exchange field $H_{exch}$, the anisotropy field $H_{aniso}$, and the demagnetizing field $\\hat{H}_{demag}$, formulated by Eq.(2) and reproduced as below:\n$\\hat{H_{eff}} = \\hat{H_{ext}} + \\hat{H_{exch}} + \\hat{H_{aniso}} + \\hat{H_{demag}}$\nwhere\n$\\hat{H_{aniso}} = \\frac{H_k}{M_s} (m_{ijk} \\cdot k) k$ for uniaxial anisotropy\n$\\hat{H_{exch}} = H_A \\cdot \\sum (m_{imn} - m_{ijk}), l = i \\pm 1  m = j \\pm 1  n = k \\pm 1$\n$\\hat{H_{demag}} (i, j, k) = M_s \\cdot \\sum \\Omega(l - i, m - j, n - k) \\cdot m_{imn}$\nThe exchange field constant, denoted by $H_A = \\frac{2A_x}{M_s D^2}$, involves $A_x$ as the exchange stiffness, $M_s$ as the saturation magnetization, and $D$ as the size of the finite- differential cell. The anisotropy field constant, expressed as $H_K = \\frac{2K_u}{M_s}$, involves $K_u$ as the uniaxial anisotropy energy density. The demagnetizing tensor $\\Omega(.,.,.)$ rep- resents the interaction between cells. Convolution of this tensor with the magnetic moment m yields the demagnetization field $H_{demag}$, constituting the most compu- tationally demanding phase in determining $H_{eff}$. The straightforward computation approach exhibits a time complexity of O(N2). To speed up the convolution process, the fast Fourier transformation (FFT) algorithm is typically employed, reducing the computational complexity to O(Nlog(N)).\nAfter calculating the gradient dm/dt based on the LLG equation, we update the magnetic moment m for each cell. For the update process in our implementation, we utilize the fourth-order Runge-Kutta (RK4) method[50]. For an in-depth explanation of this workflow, please see Algorithm 1."}, {"title": "Unet Structure", "content": "In our research, we employed a CNN-based Unet, traditionally utilized in image segmentation [37], to approximate the convolution function detailed in Eq.3, specifi- cally, $H_{demag} = Unet(m)$. As depicted in Fig. 1 (c), the Unet architecture includes an encoder, consisting of a series of CNN layers designed to learn local interactions across various scales, and a decoder that aggregates these interactions to approximate the global convolution. The specifics of the model are outlined as follows.\nEncoder module\nUnless explicitly stated otherwise, the stride for all convolution kernels is set to 1. The Unet model's encoder initiates with a 3x3 convolution layer (s0) that processes the"}, {"title": "Symmetric Logarithmic Activation", "content": "To address the challenges posed by the vast value range of the demagnetizing field ($H_d$), spanning from $10^{-4}$ to $10^{4}$, we propose a novel activation function called Symmetric Logarithmic Activation (SLA) as follows:\n$\\begin{aligned} \\\\ &For x \\geq 0: y = ln(x + 1), \\\\ &For x < 0: y = -ln(x + 1). \\end{aligned}$\nNote that SLA is an invertible function, and its inverse is given by:\n$\\begin{aligned} \\\\ &For x > 0: x = e^y - 1, \\\\ &For x<0: x = -e^{-y} + 1. \\end{aligned}$\nUtilizing SLA allows us to compress the demagnetizing field's values into a narrower range, thereby simplifying the Unet training process. In contrast, employing the inverse Symmetric Logarithmic Activation (ISLA) enables us to expand the Unet's output back to the actual scale of the demagnetizing field necessary for LLG simulation.\nWeight Initialization\nAll layers within the Unet architecture are initialized utilizing the Kaiming normal- ization technique[56]. The convolutional layers adopt the \u2018fan_out' initialization mode, whereas the transposed convolutional layers are initialized using the 'fan in' mode. Batch normalization layers are specifically initialized with ones for weights and zeros for biases."}, {"title": "Data Generation", "content": "In our research", "follows": "n\u2022 Saturation magnetization $M_s$ 1000 emu/cc,\n\u2022 Exchange constant $A_x = 0.5 \\times 10^{-6} erg/cm"}]}