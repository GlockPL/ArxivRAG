{"title": "ENSEMBLE OF PRE-TRAINED LANGUAGE MODELS AND DATA\nAUGMENTATION FOR HATE SPEECH DETECTION FROM ARABIC\nTWEETS", "authors": ["Kheir Eddine Daouadi", "Yaakoub Boualleg", "Kheir Eddine Haouaouchi"], "abstract": "Today, hate speech classification from Arabic tweets has drawn the attention of several researchers. Many systems and\ntechniques have been developed to resolve this classification task. Nevertheless, two of the major challenges faced\nin this context are the limited performance and the problem of imbalanced data. In this study, we propose a novel\napproach that leverages ensemble learning and semi-supervised learning based on previously manually labeled. We\nconducted experiments on a benchmark dataset by classifying Arabic tweets into 5 distinct classes: non-hate, general\nhate, racial, religious, or sexism. Experimental results show that: (1) ensemble learning based on pre-trained language\nmodels outperforms existing related works; (2) Our proposed data augmentation improves the accuracy results of hate\nspeech detection from Arabic tweets and outperforms existing related works. Our main contribution is the achievement\nof encouraging results in Arabic hate speech detection.", "sections": [{"title": "1 Introduction", "content": "Nowadays, the use of social media has substantially increased in Arab countries, which has allowed more freedom for\nspeech in different domains. Twitter for example is one of the leading social media where users can share short text of\nup to 280 characters optionally followed by a link, video, or photo, known as a tweet. This free micro-blogging allows\nusers to subscribe, follow other users, share content, like other tweets, repost another tweet and reply to another tweet.\nToday, Twitter is booming, the service has more than hundreds of millions of users producing over five hundred million\ntweets per day 2. The population of Saudi Arabia and Oman are the most countries that used the platform. The Arab\nusers generate 27.4 million tweets per day Al-Hassan et al. (2021). From that big number, we can assume that hate\nspeech can spread easily and quickly through this platform.\nThe General Policy Recommendation no. 15 of the European Commission\u00b3 has described Hate Speech (HS) as \"the\nadvocacy, promotion or incitement, in any form, of the denigration, hatred or vilification of a person or group of persons,\nas well as any harassment, insult, negative stereotyping, stigmatization or threat in respect of such a person or group of\npersons and the justification of all the preceding types of expression, on the ground of race, color, descent, national or\nethnic origin, age, disability, language, religion or belief, sex, gender, gender identity, sexual orientation, and other\npersonal characteristics or status\".\nToday, through Twitter, researchers propose approaches for Arabic HS detection. Nevertheless, two major challenges\nfaced in this context are the limited performance and the problem of imbalanced data, which make them suffer from\nsevere over-fitting Founta et al. (2018). Automatic HS detection from Arabic tweets using traditional machine learning\nclassifiers like Na\u00efve Bayes (NB), Support Vector Machine (SVM) and Random Forest (RF) have shown reasonable"}, {"title": "2 Related Works", "content": "The emergence of the Twitter platform has encouraged a multitude of research avenues including topic detection\nDaouadi et al. (2023, 2021, 2024), organization detection Daouadi et al. (2019a, 2018a,b), and bot detection Daouadi\net al. (2019b, 2020). Thanks to its importance, HS detection from Arabic tweets has drawn the attention of several\nresearchers. In the literature, many systems have been developed to resolve this classification problem. They follow two\nmain principal approaches: a traditional approach, and a deep learning approach."}, {"title": "2.1 Traditional Approaches", "content": "In this context, researchers have focused on manual features engineering based on local metadata and rely on the\npresence of tweet features like hashtags. Some examples of traditional approaches are briefly described in the following.\nBesides, authors in Mubarak et al. (2021) focus on the problem of detecting offensive tweets. They use features from\nthe Arabic word2vec model with SVM, this yielded an Accuracy result of 88.6%.\nLikewise, authors in Husain et al. (2020a) classify tweets being offensive or not. The best experimental results are\nobtained using the trained ensemble learning in offensive language detection, and this yielded F1 score of 88%, which\nexceeds the score obtained by the best single learner classifier by 6%.\nFurthermore, authors in Husain et al. (2020c) examine the effect of the preprocessing step on offensive language and\nHS classification. They claimed that an intensive preprocessing technique demonstrates its significant impact on the\nclassification rate. The optimal experimental results are obtained using BoW and SVM, yielding F1 score results of\n89% and 95% for offensive language and HS classification.\nIn a different strategy, authors in Mubarak et al. (2017) classify user accounts being abusive or not. The best experimental\nresults are obtained using list-based methods (Sweed Words + Log Odds Ratio), and this yielded F1 score of 60%."}, {"title": "2.2 Deep Learning Approaches", "content": "In this context, researchers follow two principal approaches are deep learning from scratch approaches and fine-tuning\napproaches."}, {"title": "2.2.1 Deep Learning from Scratch Approaches", "content": "In this context, researchers have focused on automatic feature engineering. They used deep learning models such as\nCNN and LSTM. Some examples of deep learning approaches are briefly described in the following.\nAuthors in Alharbi et al. (2020) classify Arabic tweets being offensive or not. They use features from n-gram and word\nembedding model. The best experimental results are obtained using LSTM. This yielded Accuracy result of 74%.\nIn a similar, authors in Farha et al. (2020) investigate the impact of using Multitask learning on offensive and HS\ndetection. The showed that Multitask learning based on CNN-LSTM improve the accuracy results, yielded macro\nF1-score of 90.4% and 73.7% for classifying offensive and HS tweets, respectively."}, {"title": "2.2.2 Transfer Learning Approaches", "content": "In this context, researchers have focused on transfer learning based on fine-tuning. The majority of transfer learning\napproaches focus mainly on the Arabic versions of BERT. Some examples of transfer learning approaches are briefly\ndescribed in the following:\nAuthors in Elmadany et al. (2020) evaluate Arabic BERT on offensive language and HS classification tasks. This\nyielded a macro F1-score of 82.31% and 70.51% for classifying HS and offensive language classification task.\nLikewise, authors in Abdellatif et al. (2020) evaluate the Universal Language Model Fine-tuning on the offensive\nlanguage and HS classification task, this yielded macro F1-score of 77% and 58% for classifying offensive and HS\ntweets."}, {"title": "2.3 Data Augmentation Methods", "content": "In this context, researchers have focused on generating synthetic data in order to face the challenge of imbalanced\nlearning. Some examples of data augmentation methods are briefly described in the following:\nAuthors in Husain et al. (2020c) investigate the impact of upsampling technique (duplicate tweets of the minority class\nto balance the class label) on offensive and HS classification tasks. They showed that upsampling decrease the result of\nF1-score results.\nIn similar, the authors in Haddad et al. (2020) propose two data augmentation methods for offensive and HS classification.\nThe first one uses an external augmenting technique by adding some offensive tweets from another labeled data. While\nthe second one uses the random oversampling method by shuffling the words into HS tweets to create new samples.\nLikewise, authors in Elmadany et al. (2020) investigate the impact of seed words and tweet emotion for automatic\nannotate tweets for offensive and HS classification tasks. They showed that the examined method has a positive and\nnegative impact on the offensive and HS classification tasks, respectively.\nIn a different strategy, authors in Alsafari et al. (2021) explore the role of semi-supervised built on unlabeled tweets for\nclassifying tweets being offensive, hateful, and normal. They showed that an improvement of up to 7% was achieved\nfrom using additional pseudo-labeled tweets."}, {"title": "3 Proposed Approach", "content": "To overcome the challenges previously mentioned in our related works, we propose a novel approach for classifying\nhate speech from Arabic tweets. As shown in Figure 1, our proposed approach consists of four main steps are data\naugmentation, tweets pre-processing, transfer learning and ensemble learning."}, {"title": "3.1 Data Augmentation", "content": "The imbalance nature in data often has been disregarded in the major related works, which might have a substantial\nimpact on the classification results. This problem has occurred in the HS detection where most labeled datasets are\nhighly imbalanced. To face this issue, we propose semi-supervised learning built on previously manually labeled tweets.\nFirstly, we train the model using the state-of-the-arts datasets Al-Hassan et al. (2021) labeled as (Non-hate, General HS,\nSexism, Racial and Religious HS). The tweets labeled as religious HS in Albadi et al. (2019); Alsafari et al. (2020c) are\nadded directly to the to the datasest of Al-Hassan et al. (2021). While the tweets previously labeled as HS in Sun et\nal. (2019); Mulki et al. (2019); Haddad et al. (2019); Alshaalan et al. (2020); Alsafari et al. (2020c); Ousidhoum et al.\n(2019) are classified with the trained model, and the new label is used balance the first datasets."}, {"title": "3.2 Tweets Preprocessing", "content": "The inputs of our proposition are the textual content of tweets composed of raw tweet content. In this step, our\nmain objective is to clean the textual content produce a more consistent and standard tweet. We performed some\npre-processing tasks as follows:\n\u2022 Removing the tweet features: user mentions '@', URLs, the word RT, #, punctuation, special characters\n(emoticons), and numerical characters.\n\u2022 Removing repeated characters, Arabic stop words, non-Arabic letters, new lines as well as diacritics.\n\u2022 Arabic letters normalization: in the Arabic language, they are different variations for representing some letters\nwhich are:\n\u2013 Letter which can be mistaken and written as 5, we normalized it to o.\n\u2013 Letter which has the forms 1, \u0623 ,1 ,\u0622(, all these four letters are normalized into 1.\n\u2013 The Arabic dash that is used to expand the word \u0645\u0631\u062d\u0628\u0627( to \u0645\u0631\u062d\u0628\u0627( has been removed.\n\u2013 Letter \u06cchas been normalized to \u064a."}, {"title": "3.3 Transfer Learning", "content": "The second step aims to use the pre-trained language models (AraBert-Large, AraBert-Base, and MarBert).\nTraditional machine learning technology has matured to a point where it may be used in a variety of practical applications\nwith considerable success. However, it have certain limits in some real-world circumstances. For example, gathering\nsufficient training data is prohibitively expensive, time-consuming, or impossible, necessitating the use of transfer\nlearning methods. Transfer learning is a method of training artificial neural networks that depend on pre-trained models\non specific tasks and data. Transfer learning assumes that if a pre-trained network solves one issue well, it may be\nutilized to tackle a similar but distinct problem with a little extra training. The reliance on a large amount of training\ndata can be reduced in this way.\nTransfer learning is undeniably one of the most important aspects of language models. In the Natural Language\nProcessing, the advent of Bidirectional Encoder Representation from Transformers (BERT) Devlin et al. (2018) sparked\na revolution. BERT is a deep learning model that has produced best-in-class performance on a wide range of natural\nlanguage processing tasks. BERT was taught using a technique known as Masked Language Modeling (MLM). The\nMLM operates by randomly concealing part of the existing tokens from the input. As a result, the masked word's\noriginal vocabulary id may be anticipated from the word's context. As a result, the MLM will be able to merge the left\nand right contexts. In addition to the MLM, BERT employs the Next Sentence Prediction task to train a competent\nlanguage model that recognizes and understands sentence relationships. The BERT model architecture is a multi-layered\nbidirectional transformer encoder based on the Devlin et al. (2018) original implementation. The transformers, it is\nsuggested, are a collection of numerous nested layers (or blocks). An 'attention' layer is present in each layer/block.\nFor each block, BERT uses twelve (12) different attention mechanisms, allowing tokens from the input sequence (e.g.,\nsentences made up of word or sub-word tokens) to focus on the other token. In their paper, Devlin et al. (2018) Vaswani\net al. (2017) offered the following two architectures:\n\u2022 BERT Base: 12 layers (Transformer blocks), 12 attention heads, and 110 million parameters.\n\u2022 BERT Large: 24 layers, 16 attention heads, and 340 million parameters.\nA preprocessing phase, consisting of tokenization, should be conducted before feeding a raw sentence to BERT. The\nsub-word tokenization technique WordPiece employs a vocabulary initialization to cover all characters in the training\ndata. WordPiece added the appropriate merge rules for tokenization later on. WordPiece learns positional embeddings\nwith a sequence length of up to 512 tokens and creates embeddings with a 30,000 character vocabulary. English and\nBooksCorpus are used to train BERT models. The single sentence categorization using the BERT model is shown in\nFigure 2. In Figure 2, E stands for input embedding, Ti for contextual representation of token i (Tok I), and [CLS] for\nclassification output, which summarizes all the hidden states outputted from all tokens in the input sentence."}, {"title": "3.4 Ensemble Learning", "content": "Ensemble learning is a broad machine learning meta-approach that tries to improve predictive performance by mixing\npredictions from several models. In our research, we compared two ensemble learning methods are majority voting and\naverage voting described as follows.\nMajority voting (also known as Hard Voting), each classifier votes for a class, and the class with the most votes win.\nThe ensemble's anticipated target label is the mode of the distribution of individually predicted labels in statistical terms.\nThis strategy is intentionally used to increase model performance, to out performs any one model in the ensemble.\nAverage voting (also known as Soft voting), where each classifier assigns a probability value to each data point that it\nbelongs to. The predictions are totaled and weighted according to the relevance of the classifier. The vote is then cast\nfor the target label with the largest sum of weighted probability."}, {"title": "4 Experimental results and evaluation", "content": "To fine-tune the transfer learning model, authors in Sun et al. (2019) have recommended selecting from the values of\nthe following parameters: learning rate, batch size, number of epochs described as follows:\n\u2022 The learning rate is a hyper-parameter that controls how much the model changes each time.\n\u2022 The number of epochs is a hyper-parameter that specifies how many times the learning algorithm will iterate\nover the whole training dataset."}, {"title": "4.1 Fine-tuning", "content": "The first set of our experiment is dedicated to determining the best hyper-parameter. The initial parameters used in our\nexperiment are 2 epochs, 8 batch size, and le-5 learning rate.\nThe experiment presented in Table 3 shows the effect of the number of epochs. The F1-score metric is increased when\nthe number of epochs is increased from 2 to 4 by 4.99% in bert-base-arabertv02-twitter, but it is decreased when the\nnumber of epochs is further increased. Then, using bert-large-arabertv02-twitter, The F1-score metric is increased\nfrom 2 epochs, but it is decreased when the number of epochs is further increased. While when using MARBERT, the\nF1-score is increased by 0.16 when the number of epochs is increased from 2 epochs to 4 epochs, but it is decreased\nwhen the number of epochs is further increased"}, {"title": "4.2 The Effect of Ensemble Learning", "content": "The second set of our experiments is dedicated to evaluating the ensemble learning methods (majority voting & average\nvoting). Table 7 present a comparison between single and ensemble models. It is important to note that ensemble\nlearning models achieve the highest results, yielding weighted-average F1-score results of 85.48 and 85.10 using\nmajority voting and averaged voting, respectively. In a summary, the Macro-Averaged, Micro-Averaged, and F1-score\nare reached 72.66%, 85.47% and 85.48%, respectively."}, {"title": "4.3 The Effect of Data Augmentation", "content": "The third set of our experiments is delegated to evaluating our proposed method for data augmentation. Firstly, the\ntweets labeled as religious hate speech by Albadi et al. (2019); Alsafari et al. (2020c) are directly added to the training\ndata. Second, we used semi-supervised learning using the trained model with the datasets presented in Table 2. Table\n8 present the results of the classification task.\nThe experiments presented in Table 9 shows a comparison of the proposed approach using data augmentation and\nwithout data augmentation. However, the F1-score result is increased from 85.48 to 85.65 when the training data is\naugmented through or proposed method."}, {"title": "4.4 Comparison with Existing Data Augmentation Methods", "content": "The fourth set of our experiments is dedicated to comparing our proposed methods with the relevant related works.\nNotably, from the experiment shown in Table 10, the F1-score results of different classes is presented as follows:\n\u2022 Normal: The F1-score of the latest state-of-the-art data augmentation methods is ranging from [91.65% \u2013\n92.36%] while the F1-score of our proposed data augmentation methods yielded 92.41%.\n\u2022 General hate speech: The achieved results from our proposed data augmentation method is increased by\nabout 3.19% when compared with the latest state-of-the-art.\n\u2022 Sexism: The F1-score of the latest state-of-the-art data augmentation methods ranges from [66.38% \u2013 70.52%],\nwhereas our proposed data augmentation method achieved 72.58%.\n\u2022 Racial: Our proposed data augmentation method increases the F1-score from [44.15% - 50.99%] in comparison\nwith the latest state-of-the-art, which yielded 57.25%.\n\u2022 Religious hate speech: The F1-score of the latest state-of-the-art data augmentation methods is ranging from\n[78.72% - 79.89%] while our proposed data augmentation method yielded F1-score of 83.50%.\nIn a summary, the Macro-Averaged, Micro-Averaged, and Micro-Averaged F1-scores are reached 73.80%, 85.60%, and\n85.65%, respectively."}, {"title": "4.5 Comparison with Existing Classification Approaches", "content": "The last set of our experiment is dedicated to comparing our proposed classification approach with the latest related\nworks.\nWhile comparing our proposed approach with the other baselines in Table 11, the accuracy rate of our proposition is\nthe highest. This is due to our use of ensemble learning and the the data augmentation method. When compared with\nCNN, LSTM, current DL hate speech classification models require to huge amount of labeled data to achieve good\nperformance results. In contrast, does not require a huge amount of labeled data to achieve good performance results.\nWhen compared to SVM, NB, and Bagging, ours has the benefit of not requiring handcrafted features to be designed\nand extracted."}, {"title": "5 Conclusion", "content": "Today, the increase in the spread of HS on social media is a global alarm. Efforts are now taking place to control and\navoid HS. This research investigates the impact of ensemble learning that incorporates three different pre-trained Arabic\nlanguage models with semi-supervised learning built on previously labeled datasets. The leveraged models are used to\ncover classical Arabic texts and their dialects. Moreover, using pre-trained language model will facilitate the process of\nlearning content representations. Experiments results show that: (1) ensemble learning based on pre-trained language\nmodels outperforms existing related works; (2) Our proposed data augmentation improves the accuracy results of hate\nspeech detection from Arabic tweets and outperforms existing related works."}]}