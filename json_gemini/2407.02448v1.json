{"title": "ENSEMBLE OF PRE-TRAINED LANGUAGE MODELS AND DATA AUGMENTATION FOR HATE SPEECH DETECTION FROM ARABIC TWEETS", "authors": ["Kheir Eddine Daouadi", "Yaakoub Boualleg", "Kheir Eddine Haouaouchi"], "abstract": "Today, hate speech classification from Arabic tweets has drawn the attention of several researchers. Many systems and techniques have been developed to resolve this classification task. Nevertheless, two of the major challenges faced in this context are the limited performance and the problem of imbalanced data. In this study, we propose a novel approach that leverages ensemble learning and semi-supervised learning based on previously manually labeled. We conducted experiments on a benchmark dataset by classifying Arabic tweets into 5 distinct classes: non-hate, general hate, racial, religious, or sexism. Experimental results show that: (1) ensemble learning based on pre-trained language models outperforms existing related works; (2) Our proposed data augmentation improves the accuracy results of hate speech detection from Arabic tweets and outperforms existing related works. Our main contribution is the achievement of encouraging results in Arabic hate speech detection.", "sections": [{"title": "1 Introduction", "content": "Nowadays, the use of social media has substantially increased in Arab countries, which has allowed more freedom for speech in different domains. Twitter for example is one of the leading social media where users can share short text of up to 280 characters optionally followed by a link, video, or photo, known as a tweet. This free micro-blogging allows users to subscribe, follow other users, share content, like other tweets, repost another tweet and reply to another tweet. Today, Twitter is booming, the service has more than hundreds of millions of users producing over five hundred million tweets per day 2. The population of Saudi Arabia and Oman are the most countries that used the platform. The Arab users generate 27.4 million tweets per day Al-Hassan et al. (2021). From that big number, we can assume that hate speech can spread easily and quickly through this platform.\nThe General Policy Recommendation no. 15 of the European Commission\u00b3 has described Hate Speech (HS) as \"the advocacy, promotion or incitement, in any form, of the denigration, hatred or vilification of a person or group of persons, as well as any harassment, insult, negative stereotyping, stigmatization or threat in respect of such a person or group of persons and the justification of all the preceding types of expression, on the ground of race, color, descent, national or ethnic origin, age, disability, language, religion or belief, sex, gender, gender identity, sexual orientation, and other personal characteristics or status\".\nToday, through Twitter, researchers propose approaches for Arabic HS detection. Nevertheless, two major challenges faced in this context are the limited performance and the problem of imbalanced data, which make them suffer from severe over-fitting Founta et al. (2018). Automatic HS detection from Arabic tweets using traditional machine learning classifiers like Na\u00efve Bayes (NB), Support Vector Machine (SVM) and Random Forest (RF) have shown reasonable"}, {"title": "2 Related Works", "content": "The emergence of the Twitter platform has encouraged a multitude of research avenues including topic detection Daouadi et al. (2023, 2021, 2024), organization detection Daouadi et al. (2019a, 2018a,b), and bot detection Daouadi et al. (2019b, 2020). Thanks to its importance, HS detection from Arabic tweets has drawn the attention of several researchers. In the literature, many systems have been developed to resolve this classification problem. They follow two main principal approaches: a traditional approach, and a deep learning approach."}, {"title": "2.1 Traditional Approaches", "content": "In this context, researchers have focused on manual features engineering based on local metadata and rely on the presence of tweet features like hashtags. Some examples of traditional approaches are briefly described in the following.\nBesides, authors in Mubarak et al. (2021) focus on the problem of detecting offensive tweets. They use features from the Arabic word2vec model with SVM, this yielded an Accuracy result of 88.6%.\nLikewise, authors in Husain et al. (2020a) classify tweets being offensive or not. The best experimental results are obtained using the trained ensemble learning in offensive language detection, and this yielded F1 score of 88%, which exceeds the score obtained by the best single learner classifier by 6%.\nFurthermore, authors in Husain et al. (2020c) examine the effect of the preprocessing step on offensive language and HS classification. They claimed that an intensive preprocessing technique demonstrates its significant impact on the classification rate. The optimal experimental results are obtained using BoW and SVM, yielding F1 score results of 89% and 95% for offensive language and HS classification.\nIn a different strategy, authors in Mubarak et al. (2017) classify user accounts being abusive or not. The best experimental results are obtained using list-based methods (Sweed Words + Log Odds Ratio), and this yielded F1 score of 60%."}, {"title": "2.2 Deep Learning Approaches", "content": "In this context, researchers follow two principal approaches are deep learning from scratch approaches and fine-tuning approaches."}, {"title": "2.2.1 Deep Learning from Scratch Approaches", "content": "In this context, researchers have focused on automatic feature engineering. They used deep learning models such as CNN and LSTM. Some examples of deep learning approaches are briefly described in the following.\nAuthors in Alharbi et al. (2020) classify Arabic tweets being offensive or not. They use features from n-gram and word embedding model. The best experimental results are obtained using LSTM. This yielded Accuracy result of 74%.\nIn a similar, authors in Farha et al. (2020) investigate the impact of using Multitask learning on offensive and HS detection. The showed that Multitask learning based on CNN-LSTM improve the accuracy results, yielded macro F1-score of 90.4% and 73.7% for classifying offensive and HS tweets, respectively."}, {"title": "2.2.2 Transfer Learning Approaches", "content": "In this context, researchers have focused on transfer learning based on fine-tuning. The majority of transfer learning approaches focus mainly on the Arabic versions of BERT. Some examples of transfer learning approaches are briefly described in the following:\nAuthors in Elmadany et al. (2020) evaluate Arabic BERT on offensive language and HS classification tasks. This yielded a macro F1-score of 82.31% and 70.51% for classifying HS and offensive language classification task.\nLikewise, authors in Abdellatif et al. (2020) evaluate the Universal Language Model Fine-tuning on the offensive language and HS classification task, this yielded macro F1-score of 77% and 58% for classifying offensive and HS tweets."}, {"title": "2.3 Data Augmentation Methods", "content": "In this context, researchers have focused on generating synthetic data in order to face the challenge of imbalanced learning. Some examples of data augmentation methods are briefly described in the following:\nAuthors in Husain et al. (2020c) investigate the impact of upsampling technique (duplicate tweets of the minority class to balance the class label) on offensive and HS classification tasks. They showed that upsampling decrease the result of F1-score results.\nIn similar, the authors in Haddad et al. (2020) propose two data augmentation methods for offensive and HS classification. The first one uses an external augmenting technique by adding some offensive tweets from another labeled data. While the second one uses the random oversampling method by shuffling the words into HS tweets to create new samples.\nLikewise, authors in Elmadany et al. (2020) investigate the impact of seed words and tweet emotion for automatic annotate tweets for offensive and HS classification tasks. They showed that the examined method has a positive and negative impact on the offensive and HS classification tasks, respectively.\nIn a different strategy, authors in Alsafari et al. (2021) explore the role of semi-supervised built on unlabeled tweets for classifying tweets being offensive, hateful, and normal. They showed that an improvement of up to 7% was achieved from using additional pseudo-labeled tweets."}, {"title": "3 Proposed Approach", "content": "To overcome the challenges previously mentioned in our related works, we propose a novel approach for classifying hate speech from Arabic tweets. As shown in Figure 1, our proposed approach consists of four main steps are data augmentation, tweets pre-processing, transfer learning and ensemble learning."}, {"title": "3.1 Data Augmentation", "content": "The imbalance nature in data often has been disregarded in the major related works, which might have a substantial impact on the classification results. This problem has occurred in the HS detection where most labeled datasets are highly imbalanced. To face this issue, we propose semi-supervised learning built on previously manually labeled tweets. Firstly, we train the model using the state-of-the-arts datasets Al-Hassan et al. (2021) labeled as (Non-hate, General HS, Sexism, Racial and Religious HS). The tweets labeled as religious HS in Albadi et al. (2019); Alsafari et al. (2020c) are added directly to the to the datasest of Al-Hassan et al. (2021). While the tweets previously labeled as HS in Sun et al. (2019); Mulki et al. (2019); Haddad et al. (2019); Alshaalan et al. (2020); Alsafari et al. (2020c); Ousidhoum et al. (2019) are classified with the trained model, and the new label is used balance the first datasets."}, {"title": "3.2 Tweets Preprocessing", "content": "The inputs of our proposition are the textual content of tweets composed of raw tweet content. In this step, our main objective is to clean the textual content produce a more consistent and standard tweet. We performed some pre-processing tasks as follows:\n\u2022 Removing the tweet features: user mentions '@', URLs, the word RT, #, punctuation, special characters (emoticons), and numerical characters.\n\u2022 Removing repeated characters, Arabic stop words, non-Arabic letters, new lines as well as diacritics.\n\u2022 Arabic letters normalization: in the Arabic language, they are different variations for representing some letters which are:\n\u2013 Letter which can be mistaken and written as 5, we normalized it to o.\n\u2013 Letter which has the forms 1, \u0623 ,1 ,\u0622(, all these four letters are normalized into 1.\n\u2013 The Arabic dash that is used to expand the word \u0645\u0631\u062d\u0628\u0627( to \u0645\u0631\u062d\u0628\u0627( has been removed.\n\u2013 Letter \u06cchas been normalized to \u064a."}, {"title": "3.3 Transfer Learning", "content": "The second step aims to use the pre-trained language models (AraBert-Large, AraBert-Base, and MarBert).\nTraditional machine learning technology has matured to a point where it may be used in a variety of practical applications with considerable success. However, it have certain limits in some real-world circumstances. For example, gathering sufficient training data is prohibitively expensive, time-consuming, or impossible, necessitating the use of transfer learning methods. Transfer learning is a method of training artificial neural networks that depend on pre-trained models on specific tasks and data. Transfer learning assumes that if a pre-trained network solves one issue well, it may be utilized to tackle a similar but distinct problem with a little extra training. The reliance on a large amount of training data can be reduced in this way.\nTransfer learning is undeniably one of the most important aspects of language models. In the Natural Language Processing, the advent of Bidirectional Encoder Representation from Transformers (BERT) Devlin et al. (2018) sparked a revolution. BERT is a deep learning model that has produced best-in-class performance on a wide range of natural language processing tasks. BERT was taught using a technique known as Masked Language Modeling (MLM). The MLM operates by randomly concealing part of the existing tokens from the input. As a result, the masked word's original vocabulary id may be anticipated from the word's context. As a result, the MLM will be able to merge the left and right contexts. In addition to the MLM, BERT employs the Next Sentence Prediction task to train a competent language model that recognizes and understands sentence relationships. The BERT model architecture is a multi-layered bidirectional transformer encoder based on the Devlin et al. (2018) original implementation. The transformers, it is suggested, are a collection of numerous nested layers (or blocks). An 'attention' layer is present in each layer/block. For each block, BERT uses twelve (12) different attention mechanisms, allowing tokens from the input sequence (e.g., sentences made up of word or sub-word tokens) to focus on the other token. In their paper, Devlin et al. (2018) Vaswani et al. (2017) offered the following two architectures:\n\u2022 BERT Base: 12 layers (Transformer blocks), 12 attention heads, and 110 million parameters.\n\u2022 BERT Large: 24 layers, 16 attention heads, and 340 million parameters.\nA preprocessing phase, consisting of tokenization, should be conducted before feeding a raw sentence to BERT. The sub-word tokenization technique WordPiece employs a vocabulary initialization to cover all characters in the training data. WordPiece added the appropriate merge rules for tokenization later on. WordPiece learns positional embeddings with a sequence length of up to 512 tokens and creates embeddings with a 30,000 character vocabulary. English and BooksCorpus are used to train BERT models. The single sentence categorization using the BERT model is shown in Figure 2. In Figure 2, E stands for input embedding, Ti for contextual representation of token i (Tok I), and [CLS] for classification output, which summarizes all the hidden states outputted from all tokens in the input sentence."}, {"title": "3.4 Ensemble Learning", "content": "Ensemble learning is a broad machine learning meta-approach that tries to improve predictive performance by mixing predictions from several models. In our research, we compared two ensemble learning methods are majority voting and average voting described as follows.\nMajority voting (also known as Hard Voting), each classifier votes for a class, and the class with the most votes win. The ensemble's anticipated target label is the mode of the distribution of individually predicted labels in statistical terms. This strategy is intentionally used to increase model performance, to out performs any one model in the ensemble.\nAverage voting (also known as Soft voting), where each classifier assigns a probability value to each data point that it belongs to. The predictions are totaled and weighted according to the relevance of the classifier. The vote is then cast for the target label with the largest sum of weighted probability."}, {"title": "4 Experimental results and evaluation", "content": "To fine-tune the transfer learning model, authors in Sun et al. (2019) have recommended selecting from the values of the following parameters: learning rate, batch size, number of epochs described as follows:\n\u2022 The learning rate is a hyper-parameter that controls how much the model changes each time.\n\u2022 The number of epochs is a hyper-parameter that specifies how many times the learning algorithm will iterate over the whole training dataset.\n\u2022 The batch size is a hyper-parameter that specifies how many samples must be processed before the internal model parameters are updated.\nThe tenfold cross-validation approach was used to evaluate the performance of our proposed approach. The dataset was split into ten equal-sized segments while maintaining the balance of each class in the corresponding datasets. One of these parts was used as a testing and the remainder were used as training. This procedure was repeated 10 times and the averaging of the performance results was obtained across the ten repetitions of cross-validation. The performance measures used to evaluate our proposed approach are as described follows.\n\u2022 Recall (R), because it concentrates on the good examples, we should pay greater attention to that measure. For each class label, recall refers to the proportion of properly classified tweets to the number of tweets that belonged to that class but were mistakenly classified by the model. For example, the Recall of the Racial class is calculated as described in the following equation:\n$$R_{Racial} = \\frac{correctly\\_classified\\_as\\_Racial}{Total\\_number\\_of\\_Racial\\_tweets}$$\n\u2022 Precision (P), this metric is likewise useful in our instance; it reflects the proportion of relevant retrieved tweets from a specific class. For example, the Precision of religious class is calculated as described in the following equation:\n$$P_{Religious} = \\frac{correctly\\_classified\\_as\\_Religious}{Tweets\\_classified\\_as\\_\"Religious\"}$$\n\u2022 F1-Score is more useful than accuracy in our instance of unequal class distribution since it is the weighted average of precision and recall.\n$$F1 Score = 2 \\times \\frac{Precision \\times Recal}{Precision + Recall}$$\n\u2013 Micro: Calculate measures globally by counting the total true positives, false negatives, and false positives."}, {"title": "4.1 Fine-tuning", "content": "The first set of our experiment is dedicated to determining the best hyper-parameter. The initial parameters used in our experiment are 2 epochs, 8 batch size, and le-5 learning rate.\nThe experiment presented in Table 3 shows the effect of the number of epochs. The F1-score metric is increased when the number of epochs is increased from 2 to 4 by 4.99% in bert-base-arabertv02-twitter, but it is decreased when the number of epochs is further increased. Then, using bert-large-arabertv02-twitter, The F1-score metric is increased from 2 epochs, but it is decreased when the number of epochs is further increased. While when using MARBERT, the F1-score is increased by 0.16 when the number of epochs is increased from 2 epochs to 4 epochs, but it is decreased when the number of epochs is further increased"}, {"title": "4.2 The Effect of Ensemble Learning", "content": "The second set of our experiments is dedicated to evaluating the ensemble learning methods (majority voting & average voting). Table 7 present a comparison between single and ensemble models. It is important to note that ensemble learning models achieve the highest results, yielding weighted-average F1-score results of 85.48 and 85.10 using majority voting and averaged voting, respectively. In a summary, the Macro-Averaged, Micro-Averaged, and F1-score are reached 72.66%, 85.47% and 85.48%, respectively."}, {"title": "4.3 The Effect of Data Augmentation", "content": "The third set of our experiments is delegated to evaluating our proposed method for data augmentation. Firstly, the tweets labeled as religious hate speech by Albadi et al. (2019); Alsafari et al. (2020c) are directly added to the training data. Second, we used semi-supervised learning using the trained model with the datasets presented in Table 2. Table 8 present the results of the classification task.\nThe experiments presented in Table 9 shows a comparison of the proposed approach using data augmentation and without data augmentation. However, the F1-score result is increased from 85.48 to 85.65 when the training data is augmented through or proposed method."}, {"title": "4.4 Comparison with Existing Data Augmentation Methods", "content": "The fourth set of our experiments is dedicated to comparing our proposed methods with the relevant related works. Notably, from the experiment shown in Table 10, the F1-score results of different classes is presented as follows:\n\u2022 Normal: The F1-score of the latest state-of-the-art data augmentation methods is ranging from [91.65% \u2013 92.36%] while the F1-score of our proposed data augmentation methods yielded 92.41%.\n\u2022 General hate speech: The achieved results from our proposed data augmentation method is increased by about 3.19% when compared with the latest state-of-the-art.\n\u2022 Sexism: The F1-score of the latest state-of-the-art data augmentation methods ranges from [66.38% \u2013 70.52%], whereas our proposed data augmentation method achieved 72.58%.\n\u2022 Racial: Our proposed data augmentation method increases the F1-score from [44.15% - 50.99%] in comparison with the latest state-of-the-art, which yielded 57.25%.\n\u2022 Religious hate speech: The F1-score of the latest state-of-the-art data augmentation methods is ranging from [78.72% - 79.89%] while our proposed data augmentation method yielded F1-score of 83.50%.\nIn a summary, the Macro-Averaged, Micro-Averaged, and Micro-Averaged F1-scores are reached 73.80%, 85.60%, and 85.65%, respectively."}, {"title": "4.5 Comparison with Existing Classification Approaches", "content": "The last set of our experiment is dedicated to comparing our proposed classification approach with the latest related works.\nWhile comparing our proposed approach with the other baselines in Table 11, the accuracy rate of our proposition is the highest. This is due to our use of ensemble learning and the the data augmentation method. When compared with CNN, LSTM, current DL hate speech classification models require to huge amount of labeled data to achieve good performance results. In contrast, does not require a huge amount of labeled data to achieve good performance results. When compared to SVM, NB, and Bagging, ours has the benefit of not requiring handcrafted features to be designed and extracted."}, {"title": "5 Conclusion", "content": "Today, the increase in the spread of HS on social media is a global alarm. Efforts are now taking place to control and avoid HS. This research investigates the impact of ensemble learning that incorporates three different pre-trained Arabic language models with semi-supervised learning built on previously labeled datasets. The leveraged models are used to cover classical Arabic texts and their dialects. Moreover, using pre-trained language model will facilitate the process of learning content representations. Experiments results show that: (1) ensemble learning based on pre-trained language models outperforms existing related works; (2) Our proposed data augmentation improves the accuracy results of hate speech detection from Arabic tweets and outperforms existing related works."}]}