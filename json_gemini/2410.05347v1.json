{"title": "ResTNet: Defense against Adversarial Policies via Transformer in Computer Go", "authors": ["Tai-Lin Wu", "Ti-Rong Wu", "Chung-Chin Shih", "Yan-Ru Ju", "I-Chen Wu"], "abstract": "Although AlphaZero has achieved superhuman levels in Go, recent research has highlighted its vulnerability in particular situations requiring a more comprehensive understanding of the entire board. To address this challenge, this paper introduces ResTNet, a network that interleaves residual networks and Transformer. Our empirical experiments demonstrate several advantages of using ResTNet. First, it not only improves playing strength but also enhances the ability of global information. Second, it defends against an adversary Go program, called cyclic-adversary, tailor-made for attacking AlphaZero algorithms, significantly reducing the average probability of being attacked rate from 70.44% to 23.91%. Third, it improves the accuracy from 59.15% to 80.01% in correctly recognizing ladder patterns, which are one of the challenging patterns for Go AIs. Finally, ResTNet offers a potential explanation of the decision-making process and can also be applied to other games like Hex. To the best of our knowledge, ResTNet is the first to integrate residual networks and Transformer in the context of AlphaZero for board games, suggesting a promising direction for enhancing AlphaZero's global understanding.", "sections": [{"title": "1 Introduction", "content": "AlphaZero [1] has achieved superhuman playing levels in various board games, such as Go, chess, and Shogi. Following the advent of AlphaZero, many researchers have successfully reproduced its algorithm, achieving superhuman levels in the game of Go. Examples include ELF OpenGo [2], one of the strongest open-source computer Go programs developed by Facebook in 2019, and KataGo [3], the current state-of-the-art open-source Go program trained through volunteer crowdsourcing. However, despite achieving superhuman performance, these computer Go programs remain vulnerable in scenarios requiring a more comprehensive understanding of the entire board, specifically for the circular pattern and ladder pattern.\nRecent research [4] has developed a specialized program, named cyclic-adversary, specifically to find the weaknesses in KataGo. During the game, the cyclic-adversary subtly induces KataGo to form a circular pattern, as illustrated by the marked black stones in Figure 1a, while simultaneously encircling and capturing these stones. These circular patterns usually cover a wide range of the Go board, requiring a global understanding to handle these patterns effectively. While human players can easily recognize these patterns, they lead to confusion in KataGo's judgment, resulting in incorrect decisions. When training an adversary program specifically targeted at certain network models of KataGo, the cyclic-adversary successfully attacks KataGo with a success rate exceeding 90%."}, {"title": "2 Background", "content": ""}, {"title": "2.1 Transformers in Computer Vision", "content": "Transformers have been widely used and have achieved significant results in natural language processing [6, 11] by leveraging attention mechanisms for handling sequential data. In recent years, this network architecture has been further extended to the field of computer vision. Unlike textual data, images heavily rely on positional and local information, presenting unique challenges for traditional Transformers. To tackle these differences, the Vision Transformer (ViT) [7] divides an image into patches where each patch is treated as a word token in a sentence. This allows the network to apply the Transformer's sequential data processing capabilities to visual data. The Swin Transformer [12] took this a step further by incorporating shifted attention windows, a method that improves local receptive fields. This enhancement enables the network to more effectively capture local features while still maintaining the global context.\nRecent trends in computer vision show that integrating convolutional operations within the Transformer significantly enhances the performance [8, 13, 14, 9]. A prominent example is the Convolution vision Transformer (CvT) [8], which first translates image tokens into a 2D-reshaped token map and then incorporates convolution within the self-attention mechanism to extract local features. Subsequently, the 2D-reshaped token map is converted back into image tokens, enabling Transformer layers to focus on global attention. Remarkably, CvT outperforms other Vision Transformers on ImageNet-1k, achieving higher performance with fewer parameters. Moreover, CoAtNet [9] proposes a distinct approach by using convolutional and Transformer layers in separate stages. The network architecture begins with several convolutional blocks, followed by a series of Transformer blocks with relative attention [15]. CoAtNet has demonstrated state-of-the-art performance on image classification tasks, even under conditions of low data availability."}, {"title": "2.2 AlphaZero", "content": "AlphaZero [16] is a reinforcement learning-based algorithm that combines deep neural networks with search algorithms to master board games without the need for any human knowledge. The network architecture is composed of several residual blocks with convolutional layers, followed by two head outputs: a policy head for predicting the probability distribution of the next move, and a value head for estimating the win rate. The training process consists of self-play and optimization phases. In the self-play phase, AlphaZero uses the latest neural network model to play games against itself. To generate high-quality self-play games, a Monte Carlo Tree Search (MCTS) [17, 18] is executed to conduct planning and generate moves during the game. In the optimization phase, self-play games are uniformly sampled from the recently generated self-play games and used to optimize the neural network model. The policy network aims to learn the move distribution that reflects the search results of the MCTS, while the value network aims to predict the game outcomes. The newly optimized network models are then used to generate self-play games. Repeating this process allows AlphaZero to progressively enhance its performance.\nWhile the Transformer architecture has demonstrated remarkable capabilities in various domains, researchers have recently explored the potential of using Transformers in the game of Go. Li et al. proposed replacing the residual network with EfficientFormer and trained the policy and value network by learning human game records. Their findings show that using EfficientFormer can result in a higher win rate compared to residual networks, particularly in scenarios where only CPU is available. However, their experiments only focus on supervised learning without further evaluating performance in the AlphaZero training, leaving the results at a preliminary stage."}, {"title": "3 ResTNet", "content": ""}, {"title": "3.1 Network Design", "content": "This paper proposes ResTNet, a novel architecture that combines residual and Transformer blocks, as shown in Figure 2. It consists of several blocks, with each block being either a residual block or a Transformer block. For any given board, the network outputs a policy distribution and a value, as depicted in Figure 2a, where the architectures for the residual block and Transformer block are described in more detail in Figure 2b and 2c respectively. The residual block follows the same"}, {"title": "3.2 Feature Conversion", "content": "In ResTNet, the residual and Transformer blocks process different input features. The residual blocks utilize convolutional neural networks to process 2D feature maps, whereas the Transformer blocks, designed for sequences handling, use 1D tokens as input. In board games like Go, precise and accurate positional representation is crucial, as each position on the board is unique and can differ significantly from neighboring positions. Therefore, we present a feature conversion method for transferring features between residual and Transformer blocks in ResTNet. Figure 2d illustrates feature conversion from residual to Transformer blocks. Note that the feature conversion from Transformer to residual blocks simply mirror Figure 2d left-right. The method converts the 2D feature maps into 1D tokens through a one-to-one mapping to preserve positional information, ensuring no positional information is lost, as described as follows.\nThe input/output representations of the residual block are consistent with the board representation, which is defined by dimensions $C \\times H \\times W$, where $C$ represents the number of channels, and both $H$ and $W$ denote the height and width respectively. Hence, the initial board representation can be viewed as a specification of the residual block to the first conversion. Conversely, for Transformer"}, {"title": "4 Experiments", "content": ""}, {"title": "4.1 ResTNet in 9x9 Go", "content": "Given that the proposed ResTNet allows for various combinations, we first evaluate different 6- block network configurations in 9x9 Go. Each residual block consists of 256 filters, whereas each Transformer block comprises 81 (9 \u00d7 9) tokens with an embedding size of 256. Detailed settings are provided in the appendix A.1. Table 1 lists the 6-block networks we evaluated, along with their inference time and the number of parameters. These networks can be further categorized into four types. First, the AlphaZero-like networks, comprising solely of convolutional neural networks, i.e., 6R. Second, the purely Transformer-based networks, similar to ViT, which rely entirely on attention mechanisms, i.e., 6T. Third, the CoAtNet-like networks which begin with a series of residual blocks and followed by Transformer blocks, such as 5R1T, 4R2T, etc. Finally, the networks with interleavings of residual and Transformer blocks, such as RTRRRT, RRTRRT, etc. Considering the inference time, we only explore permutations that include two Transformer blocks and four residual blocks, with one Transformer block at the end. Note that these four categories are all encompassed within our proposed ResTNet."}, {"title": "4.2 Circular Patterns in 19x19 Go", "content": "Next, we examine whether ResTNet can learn global knowledge and identify circular patterns. These patterns require a global understanding of the entire board that current AlphaZero-like Go AIs struggle to handle. We first evaluate the performance of different network configurations of ResTNet by directly playing against the cyclic-adversary in 19x19 Go, an adversary program tailor-made for attacking AlphaZero algorithms. Then, we provide an analysis to explain the observed results."}, {"title": "4.2.1 Defending Cyclic-adversary", "content": "We select two 10-block network models based on the experimental results from the previous subsection. These models are 10R, serving as an AlphaZero-like baseline model; and R3 (RRT), a model with a repeating pattern of interleaved residual and Transformer blocks. Note that R3 (RRT), also represented as RRRTRRTRRT, indicates to begin with R followed by a sequence repeating three RRT. Since training an AlphaZero model for 19x19 Go requires numerous computing resources, we simply trained these models using supervised learning on the human game collection. The collection contains a total of 1 million games played by amateur human players ranked between 7 to 9 Dan. After training the models until saturation, we compare the win rate between R3 (RRT) and 10R. The results show that R3 (RRT) achieves a win rate of 56.70% \u00b1 3.98% against 10R, suggesting that the repeating pattern of RRT discovered in 9x9 Go also improves the playing strength in 19x19 Go.\nNext, to assess the models' ability to handle circular patterns, both models utilize MCTS to play against the cyclic-adversary with specific openings. These openings include 24 games, provided by Wang et al., all containing a circular pattern. The model is required to recognize these circular patterns and play correctly with a sequence of moves to avoid being attacked; otherwise, it will lose the game. Given the inherent randomness of the cyclic-adversary, each opening is played by 30 games, resulting in a total of 720 evaluation games."}, {"title": "4.2.2 Recognizing Circular Pattern", "content": "We integrated a board evaluation head [22, 23] to further investigate how the models recognize circular patterns. The board evaluation head, commonly used in current Go AIs to investigate the model's understanding, is designed to predict the ownership of each position on the Go board at the endgame. The output values are bounded within the range of [-1, 1], where 1 represents black ownership and -1 represents white ownership. Figure 3a illustrates an example of circular patterns from the games provided Wang et al.. The cyclic-adversary, playing as the white player, successfully establishes a circular pattern and deceives the target program, KataGo, leading to the capture of the marked black stones. Namely, the ownership of these marked stones becomes white, as shown in 3b. We observe significant differences between the 10R and R3(RRT), as depicted in Figure 3c and 3d. The 10R model does the opposite by predicting the ownership of the marked stones as belonging to a black player. In contrast, the R3 (RRT) model accurately predicts the ownership of the marked stones (as white), aligning with the ground truth and demonstrating its robustness in recognizing circular patterns. The results offer insight into the R3 (RRT) model's ability to interpret complex circular patterns, a challenging task for conventional models."}, {"title": "4.3 Ladder in 19x19 Go", "content": "This subsection investigates whether ResTNet can effectively address the long sequence challenge, i.e., Ladder. Unlike circular patterns, which can be generated using cyclic-adversary, there are no established strategies to induce ladder patterns. Therefore, we collected ladder patterns from the human game collection and trained another network, named ladder network, to determine whether the network could correctly recognize ladder patterns. As a result, we collect 1,655,000 games containing ladder patterns. In each game, only one training data is kept, specifically the ladder sequences at the beginning. Figure 4a and 4c show two examples in our ladder dataset. The stones marked with triangles indicate the player attempting to escape, serving as the defender, while the opponent is the attacker, trying to capture the marked stone. A ladder head is incorporated to the network output, aiming to predict whether the marked stone can successfully escape, with a value of 1 representing escape success as shown in Figure 4b, and -1 representing escape failure, illustrated in Figure 4d. This approach provides a straightforward analysis to determine whether the network is capable of recognizing ladder patterns. Note that the training data includes an equal number of escape successes and escape failures to ensure fairness.\nWe use the same models as previous experiments in subsection 4.2.1 to analyze ladder patterns, including 10R and R3 (RRT). In addition, we incorporate the ladder head into the model and train"}, {"title": "4.4 Attention Map in 19x19 Go", "content": "We examine the attention values from the Transformer blocks of R3 (RRT), which was trained as described in Subsection 4.2, to understand the model's focus in 19x19 Go. In the Transformer block, each token corresponds to an attention map where the values indicate the relative importance to other tokens. By visualizing the attention values, we can explore the patterns or strategies utilized by the network, providing a possible explanation for its behavior and decision-making process. Specifically, we select two Go games, including a circular pattern and a ladder pattern. The attention maps represent the relative importance of all positions corresponding to the position marked in green and the redder color on the board represents a higher level of importance relative to it."}, {"title": "4.5 ResTNet in 19x19 Hex", "content": "We further apply ResTNet to Hex, another game that also requires understanding global information for long sequences, to verify its generality for other games. Given that the game complexity of 19x19 Hex is lower than that of 19x19 Go, we train two 6-block models, 6R and RRTRRT, in 19x19 Hex using similar training settings in 9x9 Go. Then, we evaluate the playing strength of both models against MoHex [24], a well-known Hex program that has won championships in computer Olympiads. The result shows that RRTRRT outperforms 6R in matches against MoHex, achieving win rates of 56.00% and 42.00% respectively, consistent with the results in 9x9 Go.\nNext, we examine the attention maps of RRTRRT in 19x19 Hex. Figure 6a shows that the attention map to marked-green stones highlights the positions of black stones and possible positions to achieve a win for Black. Note that the attention map only labels the relevant black stones for winning while leaving the irrelevant ones, marked by blue hexagons, unlabeled. In Figure 6b, the attention map highlights virtual connections\u00b3 [25], which can lead Black to win. The above two attention maps show that ResTNet successfully learns global knowledge concepts in Hex, similar to the findings in subsection 4.4, demonstrating its effectiveness in both Go and Hex."}, {"title": "5 Discussion", "content": "This paper proposes ResTNet, a network architecture specifically designed for AlphaZero algorithms, which interleaves the residual and Transformer blocks. Experiments show that ResTNet improves playing strength and significantly addresses two challenging patterns in Go: circular patterns and ladder patterns. For circular patterns, ResTNet lowers the average probability of being attacked by cyclic-adversary from 70.44% to 23.91%, reducing a factor of 2.95. For ladder patterns, ResTNet improves the accuracy of successfully recognizing ladder patterns from 59.15% to 80.01%. However, a limitation of our work is that while ResTNet successfully defends against the cyclic-adversary, it is uncertain whether ResTNet can defend against other types of attacks, such as those that do not rely on global information. We leave the study of designing new attacks for ResTNet as future work."}, {"title": "A Training Experiments in More Detail", "content": "In this section, we present training experiments in more detail conducted on both Go and Hex games."}, {"title": "A.1 9x9 Go", "content": "In 9x9 Go training, we employed the Gumbel AlphaZero algorithm [20], which can guarantee policy improvement even with a small number of simulations (e.g., 16, 32, 64) during training.\nFor the 6-block network, the training process is 100,000 training steps in total. Initially, the learning rate is set to 0.02 for the first 70,000 steps. Then, it is reduced to 0.01 for the subsequent 20,000 steps, from step 70,001 to 90,000. For the final 10,000 steps, the learning rate is further decreased to 0.005. For the simulation count, 64 is used. Other training hyperparameters are listed in Table 3.\nTo measure the performance of the models, we select three KataGo models as baselines and conduct 500 games for each model. The simulation count is set to 2,000 simulation counts for each player. The details of the selected KataGo models are listed in Table 4. Each experiment evaluates a total of 1,500 games, with both models playing 750 games as Black and 750 games as White for fairness."}, {"title": "A.2 19x19 Go", "content": "In 19x19 Go training, the models are pre-trained from a dataset, which contains 1,000,000 games from 7-dan to 9-dan human Go players in Tygem [26], which is a popular online Go platform for human players. Based on the pre-trained model, we add an extra board evaluation head for predicting the ownership of each position on the board. Then, we can use these models to test whether they can recognize circular patterns by evaluating the ownership among the 24 games [4].\nFor the models of recognizing ladder patterns, which are more common in Go than circular patterns, we identified another dataset, which is also from Tygem [26]. In our experiment, the training dataset consists of 1,665,000 unique ladder patterns, while the evaluation dataset contains a total of 166,500 ladder patterns. During the evaluation, the model predicts escape success if the output of the ladder head is greater than 0.5, and escape failure if the output is less than -0.5. Values within the range of (-0.5, 0.5) are classified as unknown and considered incorrect predictions."}, {"title": "A.3 19x19 Hex", "content": "Two models, 6R and RRTRRT, are trained for a total of 100,000 training steps in 19x19 Hex. The training settings for 19x19 Hex are similar to 9x9 Go except following differences. The learning rate is set to 0.02 and the simulation count is 32.\nTo evaluate the performance, each model uses the same simulation count of 2,000 playing against MoHex [24], which serves as a baseline opponent model. Each experiment evaluates a total of 100 evaluation games, with both models playing 50 games as Black and 50 games as White for fairness."}, {"title": "B Additional Results of Circular Patterns", "content": "This section provides detailed results for each game in Section 4.2, including the probability of defending cyclic-adversary in subsection B.1 and board evaluation results in B.2."}, {"title": "B.1 Defending Cyclic-adversary", "content": "The openings are extracted from the cyclic-adversary papers, which include 24 games\u00b9. The 24 openings can be found in Figure 7. For each opening, both 10R and R3 (RRT) play against the cyclic-adversary with a total of 30 games. Table 7 presents the probability of being attacked by cyclic-adversary for each opening. A lower rate indicates a more robust model to defend against the cyclic-adversary."}, {"title": "B.2 Recognizing Circular Pattern", "content": "Table 8 lists the MSE of board evaluation results for 24 games with circular patterns. These games are the same as the previous subsection, but we use the end game to evaluate to ensure that the ownership is confirmed. All circular patterns are shown in Figures 8-31, where the stones marked in red in (a) are those we are interested in. We used 10R and R3 (RRT) to predict the likelihood of board evaluation as shown in (c) and (d) of these figures, while the ground truth is in (b). The model's performance is evaluated by the mean squared error (MSE) between the ground truth and the network prediction, where a lower MSE indicates better performance."}]}