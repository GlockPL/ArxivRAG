{"title": "Fundamental Risks in the Current Deployment of General-Purpose AI Models: What Have We (Not) Learnt From Cybersecurity?", "authors": ["Mario Fritz"], "abstract": "General Purpose AI - such as Large Language Models (LLMs) - have seen rapid deployment in a wide range of use cases. Most surprisingly, they have have made their way from plain language models, to chat-bots, all the way to an almost \"operating system\u201d-like status that can control decisions and logic of an application. Tool-use, Microsoft co-pilot/office integration, and OpenAIs Altera are just a few examples of increased autonomy, data access, and execution capabilities.\n\nCybersecurity Risk of Application Integrated General-Purpose AI Models. Unfortunately, it turns out that the current technology is vulnerable to attacks like prompt and in-direct prompt injection. This means that a message sent to the AI by a user or even an attacker injecting a message into the AI, can alter the behavior and lead to malicious and harmful outcomes. The more powerful the Al's capabilities, the greater the potential harm.\n\nBefore the actually deployment in such scenarios, we have predicted such vulnerabilities and have hypothesized as well as demonstrated basically the whole range of known cybersecurity vulnerabilities that are induced by these issues  \u2013 as illustrated figure. In the meanwhile, this has been recognized as a core issue of the current technology that poses sever cybersecurity risks and has been reflected in threat taxonomies published e.g. by NIST and OWASP.\n\nMisinformation\n\nThreats to Information Society by Shift in the Information Eco-System.\nWhile our paper goes into great detail on these risks, it should be highlighted that the implications are very likely to be significant to the changing information ecosystem that in turn makes us even more vulnerable to disinformation. As illustrated in the figure to the right, we see a transition from consuming the \"raw\" information content on the web to increasingly relying AI assistants such as bing co-pilot or google Gemini to query and summarize information. This puts these information assistants in a central spot, as they are used to mediate the information", "sections": [{"title": "Core Issue: Missing Data and Instruction Separation in General Purpose AI Models.", "content": "It turns out the core issues of such attacks like indirect prompt injection [Greshake et al., 2023], is rooted in a fundamental cybersecurity requirement for IT system: Data-Instruction-Separation. As we argue and demonstrate [Zverev et al., 2024], the current technology lacks such a separation and it is also not clear how to impose such constraints. Basically, a message sent to a general purpose AI / LLM can be interpreted as \"data\" or a new \u201cinstruction\" that can drastically alter the behavior of the AI.\n\nWe have formalized this issue and also proposed a benchmark SEP [Zverev et al., 2024] in order to quantify the capabilities of current LLMs to adhere to this essential Cybersecurity demands. Unfortunately, the current state of technology struggles with this issue - which makes it vulnerable when used in an application integrated context."}, {"title": "ELSA Benchmarking Platform.", "content": "In the ELSA - European Lighthouse on Secure and Safe AI, we have worked quite extensively on benchmarking and auditing of AI models in particular w.r.t. robustness, privacy, and human agency/oversight. The ongoing benchmarks are available at https:\n//benchmarks.elsa-ai.eu, which also features a related LLM-Capture-the-Flag challenge that was also hosted at the SATML conference [Debenedetti et al., 2024]."}, {"title": "Need for Mode Dynamic Test and Auto Red Teaming.", "content": "While most of the current benchmarks and auditing tools are \"static\", there is a clear need to make them more dynamic and adaptive. Relying on a secret AI validation set as the \"root password to AI safety\", seem highly dangerous. We would be running the risk of such tools being evaded and not representative of actual safety measures. In order to counter this, we have taken inspiration from red teaming practices in cybersecurity to come up with with auto red teaming mechanisms that draw from prior experiences - similar to a \"red team play book\" [Chin et al., 2024]. We also see opportunities to provide additional safety nets by detecting potential over-fitting - deliberately or indeliberately - to safety checks and benchmarks."}, {"title": "Future Challenges of Multi-Agent Negotiations in General Purpose AI Models.", "content": "Another future direction, is the safety and reliability of multi-agent behavior. There is a lot of promise and interest to perform tasks like negotation and deliberation in a multi-agent setup, but we have shown for the first time, that these are equally vulnerable to adversarial behavior. Beyond this, we also show how to re-create tests, so that test contamination is reduced in order to arrive at more sustainable testing frameworks [Abdelnabi et al., 2024]. In addition, we also see an increasing need to evaluate the cybersecurity implications of AI code generation agents. We have contributed an initial benchmark that systematically finds vulnerabilities of state of the art code generation models [Hajipour et al., 2024]. Lastly, in AI for Science, chemical and bio security of general purpose AI models are emerging challenges that are particularly difficult due the context dependent delineation between useful and harmful outputs."}, {"title": "Auditing and Benchmarking vs. Foundational Approaches and Guarantees.", "content": "While bench-marking, testing, auditing play an important role, it has to be noted that this should not undermine the importance of foundational solutions and guarantees that rule out certain attacks and issues completely. Differential Privacy [Dwork and Roth, 2014] and Robustness Certification [Li et al., 2023] are two excellent examples where research has progressed to a level that also for application relevant scenarios strong and rigorous statements about AI methods can be made. These statements hold and cannot be broken in the future. This is preferable - when ever possible - to empirical approaches."}, {"title": "Vision for Secure and Safe AI in Europe \u2013 A Strategic Research Agenda.", "content": "The ELSA - European Lighthouse on Secure and Safe AI has described a Strategic Research Agenda for Secure and Safe AI in Europe [Angelov et al., 2024] that highlights challenges as well as opportunities. Key and strategic investments not only in the capabilities but also in the properties that will make AI comply with European values is a cornerstone of this joint vision."}]}