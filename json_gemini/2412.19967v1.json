{"title": "MobileNetV2: A lightweight classification model for\nhome-based sleep apnea screening", "authors": ["Hui Pan", "Yanxuan Yu", "Jilun Ye", "Xu Zhang"], "abstract": "Obstructive Sleep Apnea (OSA) is a prevalent sleep disorder linked to severe complications such as\nhypertension, stroke, and epilepsy, posing significant health risks. Despite its high prevalence, early\ndiagnosis of OSA remains challenging due to the high cost of screening and limited accessibility to\nspecialized sleep centers, leading to a large number of undiagnosed cases. While neural networks\nhave demonstrated high classification accuracy in OSA screening, their large size often limits their\napplicability to wearable devices, constraining practical use.\nThis study proposes a novel lightweight neural network model leveraging features extracted from\nelectocardiogram (ECG) and respiratory signals for early OSA screening. ECG signals are used\nto generate feature spectrograms to predict sleep stages, while respiratory signals are employed to\ndetect sleep-related breathing abnormalities. By integrating these predictions, the method calculates\nthe apnea-hypopnea index (AHI) with enhanced accuracy, facilitating precise OSA diagnosis.\nThe method was validated on three publicly available sleep apnea databases: the Apnea-ECG\ndatabase, the UCDDB dataset, and the MIT-BIH Polysomnographic database. Results showed an\noverall OSA detection accuracy of 0.978, highlighting the model's robustness. Respiratory event clas-\nsification achieved an accuracy of 0.969 and an area under the receiver operating characteristic curve\n(ROC-AUC) of 0.98. For sleep stage classification, in UCDDB dataset, the ROC-AUC exceeded 0.85\nacross all stages, with recall for Sleep reaching 0.906 and specificity for REM and Wake states at\n0.956 and 0.937, respectively.\nThis study underscores the potential of integrating lightweight neural networks with multi-signal\nanalysis for accurate, portable, and cost-effective OSA screening, paving the way for broader adoption\nin home-based and wearable health monitoring systems.", "sections": [{"title": "1 Introduction", "content": "Obstructive sleep apnea(OSA) is a common sleep disorder characterized by repeated cessation of\nbreathing during sleep[1]. The prevalence of OSA in the population ranges from 9% to 38% and increases\nwith age, with the probability of moderate to severe OSA being as high as 49% in the senior population[2].\nOSA is emerging as a major health problem that cause complications like hypertension, stroke, diabetes\nand epilepsy[3, 4]. Despite its high prevalence, OSA has reported to be undiagnosed in 85% of potential\npatients[5], due to the high costs of screening and the limited availability of sleep centers. Traditional\nPSG screening method is performed in sleep centers with sensors including electroencephalogram(EEG),\nelectocardiogram (ECG), nasal airflow, pulse oximeter(SpO2), respiratory effort chest belts, and others."}, {"title": "2 Materials and Methods", "content": "The proposed method was validated using three publicly available datasets:\n1. Apnea-ECG Database[10]: Contains 70 ECG recordings with a sampling rate of 100 Hz. Selected\nrecords (a01-a04, b01, c01-c03) include both ECG and blood oxygen data.\n2. UCDDB Dataset[11]: Comprises overnight PSG data from 25 patients with a sampling rate of\n128 Hz, used for sleep stage differentiation.\n3. MIT-BIH Polysomnographic Database[12]: Includes 18 multi-channel recordings with a sampling\nrate of 128 Hz, broadening the model's predictive scope."}, {"title": "2.2 Data preprocessing", "content": "As reported in the literature[13], 1-minute data segments are more advantageous for screening\nrespiratory events, while sleep stages are typically divided into 30-second segments. In this study, ECG\ndata is uniformly divided into 1-minute segments, with a sliding window of 30 seconds, like Fig 1.\nThis approach encompasses sleep states while preventing respiratory events from being split due to the\nsegmentation. To more accurately calculate the AHI index, sleep periods in this study are classified into\ntwo states: Sleep (S) and Non-Sleep (NS). Segments labeled as N1, N2, N3, and N4 are considered as\nSleep (S) states.\nAfter re-segmenting the signals, we applied bandpass filter (0.3-45Hz) to the single-lead ECG to\nremove baseline offsets and power disturbances. Observing fluctuations and artifacts in the signals in two\ndatasets, we applied the following two treatments to the ECG and SpO2, respectively:\nFor SpO2, it in healthy humans is chronically stable at 97% or higher, while in critically ill patients\nundergoing surgery it is between 80% and 100%[14]. Therefore, we only retained the 60-second epochs\nwith SpO2 in the 70% to 100% range. You can see those non-compliant SpO2 data in Fig 2.\nFor ECG, we divided each 60-second segment into six 10-second subsegments and extracted the\nmaximum and minimum values from each. The median of the six maximum values was set as threshold"}, {"title": "2.3 Model architecture", "content": "The MobileNet model is based on depth-separable convolution, which is a form of decomposed\nconvolution. This form decomposes the standard convolution into a depth convolution and a 1x1 con-\nvolution called point-by-point convolution. Fig 7 illustrates the difference in feature extraction between\ndepthwise and pointwise convolutions. This design reduces parameters and computational load, making\nit well-suited for mobile applications.\nAccording to existing study [18, 19], we usd MobileNet to build an optimal OSA detection model, as\nshown in Fig 8. The first layer is a standard convolutional layer with 32 filters of size 3x3x3, which undergo\nbatch normalization and ReLU activation before passing to the second layer. The second layer utilizes\nan inverted residual structure: initially, a pointwise convolution expands the dimensionality, activated by\nReLU6, followed by a depthwise convolution with ReLU6 activation, and finally a pointwise convolution\nreduces the dimensionality with a linear activation. This layer includes 17 inverted residual blocks, also\nknown as \"bottlenecks.\" The addition of ReLU6 minimizes the loss of high-dimensional information post-\nconvolution. In the third layer, feature map expansion is achieved via pointwise convolution, followed\nby dimensionality reduction using an average pooling layer to prevent overfitting. A final 1x1 pointwise\nconvolution adjusts the number of channels in the feature map. In the fourth layer, softmax activation is\napplied to the fully connected layer for classification predictions, where all neurons are fully connected\nand learning occurs through forward and backpropagation algorithms.\nNote that after each convolution (either normal convolution, deep convolution or point-by-point\nconvolution) a batch normalization and activation function operation is performed, which is omitted in\nFig 8."}, {"title": "3 Results", "content": null}, {"title": "3.1 Experiment setup", "content": "The deep learning framework selected for this experiment is Huawei's self-developed MindSpore.\nDuring model training, the Adam optimizer is used. The batch size is set to 32, and the number of epochs\nis set to 20. The ECG data from 25 subjects in the UCDDB dataset is uniformly classified into two states:\nSleep (S) and Non-Sleep (NS), and then input into the MobileNetV2 model for training. Subsequently,\nthe overnight data from each subject in the Apnea-ECG and MIT-BIH datasets is fed into the model\nfor sleep duration prediction. Finally, a rule-based screening algorithm designed for respiratory signals\nin the MATLAB platform is used to compute the AHI index for each subject, which is then compared\nwith the AHI values provided in the datasets. More details can be found in Table 1."}, {"title": "3.2 Performance evaluation", "content": "We use statistical values (like accuracy, precision, recall, specificity and Roc) to quantify the per-\nformance of our model. The formulas for these parameters are shown in Equation 1, 2, 3, 4, 5, where\nTP, FP, TN, and FN are true positive, false positive, true negative, and false negative, respectively.\nAccuracy = $\\frac{TP+TN}{TP+FP+TN+FN}$ (1)\nPrecision = $\\frac{TP}{TP + FP}$ (2)\nRecall = $\\frac{\u03a4\u03a1}{TP+FN}$ (3)\nSpecificity = $\\frac{TN}{TN + FP}$ (4)\nF1 score = 2 \u00d7 $\\frac{Precision \u00d7 Recall}{Precision + Recall}$ (5)"}, {"title": "3.3 Classification performances", "content": "In this study, we performed the detection of respiratory abnormal events using data from the Apnea-\nECG dataset, specifically focusing on two different sample sizes: a small sample of 8 individuals and\na larger sample of 50 individuals. For each frame, which corresponds to a 1-minute segment of the\nElectrocardiogram-derived Respiratory (EDR) signal spectrogram, the classification results of the model\nare shown in Fig 9. Since the Apnea-Hypopnea Index (AHI) is considered the gold standard for diagnosing\nsleep apnea syndromes (SAS) and other sleep-related breathing disorders, its accurate computation is\ncrucial. The calculation of AHI relies heavily on the precise measurement of sleep duration, as defined\nby Equation 6.\nAHI = $\\frac{num}{SleepTime(/h)}$ (6)\nTo enhance the prediction accuracy, we employed the UCDDB dataset, applying our model to\nperform sleep stage classification and normal/abnormal respiratory status identification during sleep.\nSpecifically, we performed a three-class prediction to differentiate the stages of sleep and a binary classi-\nfication to distinguish between normal and abnormal respiratory events. The results of these predictions\nare presented in Fig 10. The model demonstrated strong performance in recognizing respiratory states\nin each frame, as well as accurately identifying sleep stages."}, {"title": "4 Conclusion", "content": "The proposed machine learning model demonstrates promising results for the classification of AHI\nindices in the context of sleep apnea screening. The confusion matrices reveal that the model is generally\neffective at identifying Normal and Severe classes, but it encounters more difficulty in distinguishing\nbetween the Mild and Moderate categories, likely due to the similarities in these intermediate states."}]}