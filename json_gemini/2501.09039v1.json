{"title": "Playing Devil's Advocate:\nUnmasking Toxicity and Vulnerabilities in Large\nVision-Language Models", "authors": ["Abdulkadir Erol", "Trilok Padhi", "Agnik Saha", "Ugur Kursuncu", "Mehmet E. Aktas"], "abstract": "The rapid advancement of Large Vision-Language Models (LVLMs) has enhanced\ncapabilities offering potential applications from content creation to productivity\nenhancement. Despite their innovative potential, LVLMs exhibit vulnerabilities,\nespecially in generating potentially toxic or unsafe responses. Malicious actors can\nexploit these vulnerabilities to propagate toxic content in an automated (or semi-\n) manner, leveraging the susceptibility of LVLMs to deception via strategically\ncrafted prompts without fine-tuning or compute-intensive procedures. Despite\nthe red-teaming efforts and inherent potential risks associated with the LVLMs,\nexploring vulnerabilities of LVLMs remains nascent and yet to be fully addressed\nin a systematic manner. This study systematically examines the vulnerabilities of\nopen-source LVLMs, including LLaVA, InstructBLIP, Fuyu, and Qwen, using ad-\nversarial prompt strategies that simulate real-world social manipulation tactics\ninformed by social theories. Our findings show that (i) toxicity and insulting are\nthe most prevalent behaviors, with the mean rates of 16.13% and 9.75%, respec-\ntively; (ii) Qwen-VL-Chat, LLaVA-v1.6-Vicuna-7b, and InstructBLIP-Vicuna-7b\nare the most vulnerable models, exhibiting toxic response rates of 21.50%,\n18.30% and 17.90%, and insulting responses of 13.40%, 11.70% and 10.10%,\nrespectively; (iii) prompting strategies incorporating dark humor and multimodal\ntoxic prompt completion significantly elevated these vulnerabilities. Despite be-\ning fine-tuned for safety, these models still generate content with varying degrees\nof toxicity when prompted with adversarial inputs, highlighting the urgent need\nfor enhanced safety mechanisms and robust guardrails in LVLM development.", "sections": [{"title": "Introduction", "content": "Digital platforms provide rich opportunities for general users to create, consume, and\nengage with multimodal content. Yet, the misuse of these online resources facilitates\nand perpetuates the spread of online social harms in the form of toxicity (Kursuncu,\nPurohit, Agarwal, & Sheth, 2021), such as hate speech (Arya et al., 2024), cyber-\nbullying (Kim, Razi, Stringhini, Wisniewski, & De Choudhury, 2021; Wijesiriwardene\net al., 2020), extremism (Kursuncu et al., 2019) and adverse mental health impacts\n(Holland & Tiggemann, 2016). According to the PEW Research Center, half of U.S.\nteens have experienced bullying (PEWCyberbullying, 2022), and four in ten Ameri-\ncan adults have faced online harassment linked to various factors, including politics,\nsexual harassment, and stalking, while the severity of such toxicity has only increased\nsince 2017 (PEWHarassment, 2021). Research shows that frequent exposure to harm-\nful media (i.e., violent) is associated with increased aggression, as it desensitizes people\nto such content (Anderson et al., 2010; Bartholow, Bushman, & Sestir, 2006; DeWall,\nAnderson, & Bushman, 2011). The advent and advancement of Large Language Mod-\nels (LLMs) and Large Vision-Language Models (LVLMs) have further complicated\nthese challenges. These models are capable of generating realistic multimodal content\nthat can mirror and amplify human intentions, potentially serving both benevolent\nand malevolent purposes.\nThe Microsoft 2024 Global Online Safety Survey revealed that 87% of 16,795 re-\nspondents expressed concerns about at least one problematic generative AI (Gen-AI)\nscenario\u00b9, highlighting issues such as sexual or online abuse, AI hallucinations, the am-\nplification of biases, scams, deepfakes, and data privacy. More specifically, the younger\ngenerations, including Gen Z and millennials, have shown greater excitement about\nGen-AI tools, leading to increased usage and potential exposure to unknown risks.\nSome recent red-teaming studies have demonstrated LLMs' vulnerabilities (Ganguli\net al., 2022; Microsoft, 2023; Perez et al., 2022), although safety mechanisms were\nincorporated to prevent the generation of harmful content. These studies show that\ndespite such safety guardrails, LLMs can still be manipulated to produce undesirable\noutcomes consistently, potentially exacerbating online toxicity. In contrast to these\nobservations on LLMS, LVLMs remain to be explored for their vulnerabilities.\nLVLMs were designed to make digital social interactions more intuitive, richer,\nand naturally aligned with human communicative intentions, as they were visual in-\nstruction fine-tuned to interpret and execute multimodal complex user instructions\n(Achiam et al., 2023; D. Chen, Liu, Dai, & Wang, 2024; Dai et al., 2024; H. Liu, Li,\nWu, & Lee, 2024). Despite the implementation of safety guardrails through instruction\ntuning (Ouyang et al., 2022; Wei et al., 2021) and reinforcement learning from human\nfeedback (RLHF) (Y. Bai et al., 2022; Ouyang et al., 2022) to promote responsible\nuse, vulnerabilities persist. These vulnerabilities have manifested as various forms of\ntoxicity, which malicious actors and groups have exploited to spread hate speech, ex-\ntremism, disinformation, and discriminatory content (P. Liang et al., 2022). Carefully\ncrafted adversarial prompts (i.e., manipulative and deceptive) can exploit the inherent\nbiases encoded in these models, leading to unmasking the undesired and unforeseen"}, {"title": "Background", "content": "Toxic behavior in communication often appears as expressions of anger, stress, or\nfrustration, conveyed through profanity, insults, and slurs (Jay, 1992). The semantic\nmeaning and impact of these expressions can change significantly depending on con-\ntext and intention, highlighting the necessity for models to accurately interpret and\nnavigate these nuances (Kursuncu et al., 2021; Sheth, Shalin, & Kursuncu, 2022). The\nprimary goal of using toxic language is to elicit specific emotional responses, such as\nanger or insult (Montagu, 2001). Psycholinguistics literature provides insights suggest-\ning that the use of toxic language is a learned behavior that LVLMs may inadvertently\nlearn and mimic. According to Jean Piaget's theory of cognitive development, during\nthe formal operational stage, individuals develop the capacity for abstract thinking\nand sophisticated reasoning (Piaget, 1964), a capacity that LVLMs might emulate or\nmisuse if not properly regulated.\nWe explore the vulnerabilities of LVLMs through the lens of social theories that\nhelp characterize toxic behaviors in online communications. Specifically, our approach\nincorporates four key dimensions informed by: confirmation bias, dark humor, malevo-\nlent creativity, and social identity. Each of these dimensions offers a unique perspective\non how and why LVLMs may generate harmful content, informing our approach to\nexamining their susceptibility to generating toxic outputs. These dimensions provide\na comprehensive framework for understanding the complex interplay between LVLMs\nand the toxic behaviors they may exhibit, guiding our investigation."}, {"title": "Confirmation Bias", "content": "Confirmation bias rooted in social science literature describes the tendency to fa-\nvor information that aligns with one's own pre-existing beliefs (Pohl, 2004) while\ndismissing or undervaluing contradictory evidence (Nickerson, 1998). These biases of-\nten can manifest in various settings, from social media platforms, where algorithms\nand user interactions can create echo chambers that reinforce biased views (Alsaad,\nTaamneh, & Al-Jedaiah, 2018; Stibel, 2018), to more formal and structured settings,\nsuch as scientific research and political discourse. In research, it can lead to selective\ndata interpretation that supports hypotheses while disregarding contradictory findings\n(Dahlgren, 2020; Hergovich, Schott, & Burger, 2010). In political discourse, it can am-\nplify existing polarized views by leading individuals to engage with information that\naligns with their pre-existing entrenched viewpoints (Knobloch-Westerwick, Mothes,\n& Polavin, 2020; Pearson & Knobloch-Westerwick, 2019).\nConfirmation bias also presents significant challenges for the LLMs and LVLMs.\nDespite efforts to mitigate harmful biases and unfairness in these models, which were\ntrained on diverse and often biased internet corpora, they can still produce biased\n(Goyal et al., 2024) and toxic outputs (Howard, Bhiwandiwalla, Fraser, & Kiritchenko,\n2024). These pre-existing biases usually arise during model pre-training, where learn-\ning is optimized to identify statistically frequent patterns in content. As they learn"}, {"title": "Dark Humor", "content": "In social science literature, dark humor is characterized by its treatment of grim sub-\njects, such as death, disease, and warfare with bitter amusement (Willinger et al.,\n2017), associated with dark personality traits, such as Machiavellianism, psychopathy,\nand narcissism (Martin, Lastuk, Jeffery, Vernon, & Veselka, 2012; Veselka, Schermer,\nMartin, & Vernon, 2010). Individuals with these traits often use humor to manipu-\nlate or degrade others (Dionigi, Duradoni, & Vagnoli, 2022; Zeigler-Hill, McCabe, &\nVrabel, 2016) and attempt to explain why it is funny (Ferguson & Ford, 2008). As\nhumans mature, they develop a sense of humor relying on the repetition of simple but\nincongruent stimuli based on intellectual, psychological, and social factors, often incor-\nporating social, political, and sexual elements (Jay, 1992). This development can lead\nto the use of toxic language and behavior depending on the environmental visual cues\nand contextual language-based memories (McGhee & Pistolesi, 1979), which manifests\nin various forms, including insults, ethnic slurs, and online trolling (Navarro-Carrillo,\nTorres-Mar\u00edn, & Carretero-Dios, 2021; Sanfilippo, Fichman, & Yang, 2018; Voisey\n& Heintz, 2024; Volkmer, Gaube, Raue, & Lermer, 2023), particularly under condi-\ntions of frustration (Montagu, 2001) or for self-entertainment (Bishop, 2013; Cook,\nSchaafsma, & Antheunis, 2018). Dark humor establishes social connections among\nlike-minded individuals (i.e., in-group) and a disconnect with those who are laughed\nat (i.e., out-group) (Kuipers, 2015). Ferguson and Ford (2008) argues that this type\nof humor can be reciprocated as a come-back as out-groups may feel threatened by\nsuch behavior. Research shows that this dynamic can deepen the social divides and\npolarization further (Davis, Love, & Killen, 2018; Gal, 2019). As the technological\naffordances with social media have accelerated the spread of such humor through\nmemes (Attardo, 2023), they provided a major means to disseminate potential social\nharms, given memes' unique effectiveness in cross-modal contextual representation of\nhumorous information.\nIn the context of LLMs and LVLMs, the use of dark humor poses potentially\nsignificant challenges. Research indicates that exposure to dark humor can foster a\ntolerance toward moral violations, especially when these violations are self-serving\n(Brigaud & Blanc, 2021). Consequently, when these models generate content that\nincorporates dark humor, they may inadvertently promote such tolerance, leading to"}, {"title": "Social Identity Attacks", "content": "Social Identity Theory describes how individuals' social identities shape behaviors\nand interactions seeking to enhance their self-image by favoring their in-group, often\nleading to prejudice against out-groups. This theory highlights the significant influ-\nence of social identities on both personal and collective actions in group dynamics and\nintergroup relations (Harwood, 2020; Jenkins, 2014). Through this lens, the mecha-\nnisms behind the social divide, polarization, and depolarization dynamics across online\ncommunities can be explored by examining the organizational structures (Phillips &\nCarley, 2024) and common attributes of the polarized groups (Agarwal, Liu, Murthy,\nSen, & Wang, 2009).\nPerceived threats to a group's social identity by out-groups can trigger defensive or\naggressive reactions, potentially escalating further support for conflict (Zeigler, 2023).\nThese dynamics are increasingly relevant in online platforms, where LLMs and LVLMs\ncan simulate social identity attacks at scale, potentially deepening existing social\ndivides and polarization. These identity attacks often involve derogatory comments\ntargeting individuals based on race, gender, religion, or sexual orientation, perpetuat-\ning harmful stereotypes and fostering discriminatory ideologies (Google, 2024). Online\nplatforms, with their anonymity and broad reach, facilitate the spread of identity-\nbased harassment (Casta\u00f1o-Pulgar\u00edn et al., 2021). This is exacerbated by the capacity\nof language models to weaponize identity attacks through persona modulation strate-\ngies. These strategies manipulate models to adopt certain personalities or identities,\nenabling malicious actors to generate content that appears to originate from specific\nindividuals or groups (Shah et al., 2023). Such tactics can have detrimental effects,\nparticularly in political contexts, by undermining public figures, manipulating opin-\nions, or inciting societal divisions. Identifying vulnerabilities of language models for\nsocial identity threats will provide insights into robust mechanisms to mitigate these\nrisks in deploying language technologies."}, {"title": "Malevolent Creativity", "content": "Malevolent creativity involves the generation of innovative ideas intended to cause\nharm or selfish outcomes (Hunter, Walters, Nguyen, Manning, & Miller, 2022), dis-\ntinct from benevolent creativity which seeks to advance societal benefits (Bower, Acar,\n& Kursuncu, 2023; Kapoor & Kaufman, 2022). While creativity is often celebrated\nfor its positive impact and force for good, promoting integrity and societal advance-\nment (D.H. Cropley & Cropley, 2019), it can also be harnessed for unethical purposes\n(D.H. Cropley, 2016) through original, often ingenious methods. This form of creativity\nhas been shown to lead to severe consequences, such as war atrocities or environmental\ndamage (McLaren, 1993)."}, {"title": "Related Work", "content": "The safety of language models is defined by their ability to generate accurate, non-\nharmful content, safeguarding against misinformation, bias, and potential misuse\n(Y. Bai et al., 2022; Hugging Face, 2023). While these models may show remarkable\nperformance in various tasks, they remain vulnerable in generating responses with\nproblematic information. Hence, it is critical to gain a better understanding of these\nvulnerabilities and the safety mechanisms through systematic evaluations (P. Liang\net al., 2022). In prior research, researchers performed systematic adversarial attacks\nagainst LLMs to find their potential vulnerabilities that might be harmful to individ-\nals or society, termed as red-teaming (Ganguli et al., 2022; Kaddour et al., 2023).\nBelow, we provide a summary of the work related to both LLMs and LVLMs.\nPrior work examined LLMS and LVLMs to uncover and address the vulnerabilities.\nAs the GPT family of models pioneered the advent and advancement of these models,\nresearchers identified vulnerabilities to prompt injection, bias in programming, and\nthe dissemination of misinformation (i.e., hallucination) (Ray, 2023; Zhuo et al., 2023).\nTo enhance the scope of testing, the Curiosity-Driven Red Teaming (CRT) technique\nwas introduced, which expanded the range of test cases, moving beyond reliance on\nhuman testers or reinforcement learning alone due to lack of diversity in the test cases\n(Hong et al., 2024). An integrated framework was proposed by Deng et al. (2023)\nto attack and defend LLMs. The attack strategy combined manual and automated"}, {"title": "Methods", "content": "In this study, we selected models from the most prominent open-source Large\nVision-Language Model (LVLM) families with state-of-the-art performances: LLaVA,\nInstructBLIP, Fuyu, and Qwen, as depicted in Figure 1. Four prompting strategies"}, {"title": "Dataset Creation", "content": "Our aim is to condition LVLMs using multimodal prompts that incorporate a social\nthreat, exploiting the potentially harmful information embedded within these models\n(Branch et al., 2022; Ganguli et al., 2022; Y. Liu et al., 2023). We utilize datasets\nconsisting of toxic memes and textual prompts, creating multimodal prompts with\ntoxic image and text pairs to trigger the models to generate toxic outputs. Memes\noften employ humor and visual elements to convey messages that range from be-\nnign to harmful. We specifically selected the Hateful Memes (Kiela et al., 2020) and\nHVVMemes (Sharma et al., 2023) datasets because they include examples that illus-\ntrate the potential harm that memes can inflict, such as hate speech, misinformation,\nand perpetuating stereotypes. These datasets allow us to simulate realistic scenarios\nin which LVLMs are exposed to adversarial inputs designed to exploit their vulner-\nabilities. Moreover, the inclusion of politically and socially charged content ensures\nthat the models face challenging and realistic examples of toxic content. Additionally,\nwe created the Multimodal-RealToxicityPrompts dataset, building on the RealToxici-\ntyPrompts dataset (Gehman et al., 2020). This allows us to comprehensively assess the\nsusceptibility of various LVLMs to generating or amplifying harmful messages when\nprompted with multimodal adversarial inputs, from subtle political biases to overt\nhate speech. These datasets were chosen to align with the objectives of our prompting\nstrategies.\nThe Hateful Memes dataset, developed by Meta Research, is a source of multi-\nmodal data combining images and text designed specifically to address hateful content.\nThis dataset includes over 10,000 images, with a subset labeled as 'hateful' according\nto a stringent definition of hate speech; \"A direct or indirect attack on people based\non characteristics, including ethnicity, race, nationality, immigration status, religion,\ncaste, sex, gender identity, sexual orientation, and disability or disease. We define at-\ntack as violent or dehumanizing (comparing people to non-human things, e.g., animals)\nspeech, statements of inferiority, and calls for exclusion or segregation. Mocking hate\ncrime is also considered hate speech\" (Kiela et al., 2020). In our study, we selectively\nused the memes labeled 'hateful' from this dataset (3,756 out of 10,000), focusing not\non detecting the toxicity but rather on leveraging these toxic examples to probe the\nvulnerabilities of LVLMs. These examples are a form of social threat with which we\naim to condition the models to generate toxic responses.\nThe HVVMemes dataset comprises approximately 7,000 memes, covering top-\nics related to COVID-19 and U.S. Politics (Sharma et al., 2023). For our study, we\nspecifically utilized the U.S. Politics segment of this dataset (3,552 out of 7,000) to"}, {"title": "Creating Multimodal-RealToxicityPrompts Dataset", "content": "We developed a new dataset by mapping visual elements from the HVVMemes and\nHateful Memes datasets with the textual prompts from the RealToxicityPrompts\ndataset. This approach aims to exploit potential confirmation bias for toxic behavior\nin these models through auto-completion, specifically for our multimodal toxic prompt\ncompletion strategy. To ensure the multimodal prompts were contextually coherent\nacross both modalities, we performed semantic mapping between meme images and\ntext prompts using the pretrained CLIP model (Radford et al., 2021) to extract and\nalign image features and text embeddings based on their similarity.\nThe sets of images from Hateful Memes and HVVMemes and texts from RealTox-\nicityPrompts are defined as:\n$U = {u_1, u_2,... u_n}, T={t_1, t_2,... t_m}$\nwhere $u_i$ and $t_j$ represent each image and text, respectively, for computing the cosine\nsimilarity scores between them:\n$cosine(u_i, t_j) = \\frac{u_i t_j}{||u_i||||t_j||}$\nUsing the semantic similarity scores obtained, we identified the text prompt most\nclosely related to $u_i$, with the highest similarity score. This process creates image and\ntext pairs to construct our dataset, Multimodal-RealToxicityPrompts.\nIn Figure 2, we present a multimodal prompt from our dataset comprising pairs of\nimages and text to explore how LVLMs react to potentially manipulative inputs. The\nfeatured image is a meme on taxation, with the top section labeled \"Taxation is theft\"\nand the bottom section stating \"I know.\" This image, along with an accompanying text\nprompt, is processed by our model, specifically LLaVA-v1.6-Vicuna-7b. The model's\nresponse to this multimodal prompt was notably toxic, receiving a toxicity score of\n0.911 from the Perspective API (Google, 2024), challenging the safety guardrails."}, {"title": "Prompting Strategies", "content": "To assess the resilience of LVLMs against adversarial toxic inputs, we have devised\nfour distinct prompting strategies, outlined in Table 1. These strategies are designed\nto test how LVLMs will react to inputs that emulate the characteristics of toxic be-\nhavior, drawing from various social theories discussed in Section 2. More specifically,"}, {"title": "Measuring Toxicity", "content": "We used the Perspective API to quantify various forms of toxic content, including\ntoxicity, threat, insult, severe toxicity, identity attack, and profanity (Google, 2024).\nThe authors defined toxicity as a \"rude, disrespectful, or unreasonable comment, likely\nto make people leave a discussion.\" For a comprehensive understanding of these at-\ntributes and their definitions, please refer to Appendix Section D, which details the\ncriteria outlined in the Perspective API documentation. The API provides a score be-\ntween 0 and 1 where 1 is the most and 0 is the least toxic. In our analyses, we consider\na response toxic if it has a score \u2265 0.5, and non-toxic otherwise (Gehman et al., 2020;\nP. Liang et al., 2022). A higher score does not necessarily indicate greater severity\nbut rather a higher probability of being perceived as toxic for the respective metric.\nWe recognize that our measure of toxicity, which may be biased towards lexical cues,\nis imperfect despite its wide usage (Koh, Kim, Lee, & Jung, 2024). Hence, we ran-\ndomly selected 200 responses for manual validation of scores produced by the API.\nThe manual annotations revealed high levels of agreement among annotators for both\ntoxicity (Cohen's Kappa score \u043a = 0.91) and insults (Cohen's Kappa score \u043a = 0.82),\nindicating perfect agreement (Hallgren, 2012), and is detailed in Appendix Section\nD.1."}, {"title": "Statistical Analysis", "content": "To address our research questions, we performed statistical testing to evaluate and\ncompare, determining whether differences in mean toxicity scores across LVLMs and\nprompting strategies are statistically meaningful. We organized the results (e.g., six\ntoxicity metrics) into four distinct groups aligned with our research questions (see\nTable 2), facilitating a systematic ranking through relative comparisons across toxicity\nmetrics, among LVLMs as well as prompting strategies, indicating relative vulnerabil-\nities with respect to each other. Each of the four groups was divided into subgroups\nbased on the grouping criteria specified in this table. Grouping criteria can influence\nassumptions of the statistical tests, such as homogeneity of variances and distribution\nshapes. For example, when subgroups display unequal variances, this violates the as-\nsumptions of traditional ANOVA and necessitates the use of more robust methods like\nWelch's ANOVA, which does not assume equal variances across groups. This approach\nallows us to quantitatively assess the prevalence and variation of toxicity across dif-\nferent models and prompting strategies based on their statistically significant mean\ndifferences (Welch, 1951).\nFor each group, we first conducted Levene's test to assess the equality of vari-\nances among the different subgroups of data, which reveals unequal variances across\nthese subgroups. Due to this finding, we utilize Welch's ANOVA (Welch, 1951). This\nmethod is particularly critical given the range of subgroups tested and the variabil-\nity in their responses, ensuring that our analysis accurately reflects the significance\nof the observed differences. For post-hoc analysis, the Games-Howell test was used to\naccommodate unequal variances and unequal sample sizes, conditions prevalent in our\ndataset (Games & Howell, 1976). Unlike other post-hoc tests, such as Tukey's Hon-\nestly Significant Difference (HSD) that require equal variances, the Games-Howell test"}, {"title": "Thematic Analysis of Toxic Responses", "content": "To gain a thematic understanding of the toxic content and contextual cues, we per-\nformed topic modeling over the toxic responses using BERTopic (Grootendorst, 2022)\nand conducted a qualitative study to understand the most dominant themes in the\ngenerated responses. The optimal number of topics from BERTopic was determined\nas 64 by calculating coherence scores for topic numbers ranging from 4 to 100 in\nincrements of 4 (see Appendix Figure F4)."}, {"title": "Results", "content": "The results of our toxicity analysis are presented in Table 6 for toxicity and\ninsult, where we quantitatively assess the vulnerabilities of LVLMs, including\nFuyu-8b, LLaVA-v1.6-Mistral-7b, LLAVA-v1.6-Vicuna-7b, Qwen-VL-Chat, and\nInstructBLIP-Vicuna-7b (see Appendix Table A1 for full list). We evaluated these\nmodels across six metrics (via Perspective API): toxicity, threat, insult, severe toxicity,\nidentity attack, and profanity. The analysis reveals varying toxicity levels among the\nmodels, as shown by the scores for each metric, model, and strategy. We provided key\ndescriptive statistics such as mean, median, third quartile (Q3), and maximum (max)\nvalues to focus on the distributions of toxic responses. Additionally, we reported the\npercentage of responses that exceeded the toxicity threshold of 0.5, offering a quanti-\ntative measure of how frequently each model generated responses deemed toxic. This\napproach allows us to systematically compare the performance and safety of each\nmodel under different prompting strategies.\nMore specifically, among the LVLMs, Qwen-VL-Chat consistently shows higher\ntoxicity scores across most metrics, particularly under Multimodal Toxic Prompt Com-\npletion and Dark Humor strategies, indicating a greater vulnerability to generating\ntoxic content in these contexts. On the other hand, LLAVA-v1.6-Mistral-7b exhibit\nrelatively lower toxicity levels, suggesting more robustness against generating harm-\nful content under similar conditions. For prompting strategies, the Multimodal Toxic\nPrompt Completion strategy seems to generate significantly higher toxic responses,\nas evidenced by higher mean and max scores across several models. Dark Humor and\nMalevolent Creativity also effectively induce toxic outputs, with the latter showing\nsubstantial impact in prompting models, such as Qwen-VL-Chat, to generate responses\nwith severe toxicity and profanity.\nOverall, the variation in responses (as shown by Q3 and max values) under each\nprompting strategy suggests a considerable disparity in how each model handles toxic\nprompts, pointing towards intrinsic differences in their pretraining or inherent design\nbiases. The presence of higher max values, even where mean scores are moderate,\nindicates outlier responses that could be extremely toxic, highlighting the importance\nof considering the range and not just central tendencies in toxicity assessment."}, {"title": "Ranking Results", "content": "As discussed in the methods section, we ranked the subgroups of generated content\nin each of the four groups by adjusting their ranking scores based on the effect size\nderived from pairwise Games-Howell comparisons. This adjustment process increased\nthe rank of less toxic and lowered the rank of those that were more toxic, but only"}, {"title": "Prevalent Toxic Behaviors (RQ1):", "content": "Among the six toxic behaviors analyzed, tox-\nicity and insult were found to be the most\nprevalent, while severe toxicity was the least\ncommon (See Table 3). This prevalence likely\noriginates from their predominance in the\ndatasets we used, which may condition the\nmodels for generating harmful content. The\nhigh prevalence of these behaviors is particu-\nlarly concerning for language models designed\nfor use in general as well as more sensitive\ndomains, such as healthcare, indicating a sig-\nnificant risk of producing content that could\nerode user trust and compromise safety. The\nANOVA test indicates a statistically signifi-\ncant effect of metrics ($F = 9005.24, p < 0.001$) with a moderate effect size ($\u03b7^2 = 0.144$). The\nGames-Howell tests further reveal that metrics such as Toxicity and Insult consistently\ndemonstrate higher mean scores (see Games-Howell test results in Appendix Table\nE5), indicating their stronger discriminatory ability in detecting harmful content com-\npared to others like Profanity and Identity Attack. Ranking results emphasize that\nToxicity and Insult score higher, whereas Severe Toxicity scores lower, highlighting\ntheir effectiveness and alignment with model sensitivity."}, {"title": "Which LVLMs are More Vulnerable? (RQ2):", "content": "To assess vulnerabilities of various models and prompting strategies for toxicity, we\nfirst confirmed the homogeneity of variances using Levene's test, followed by Welch's\nANOVA and the Games-Howell post-hoc test to evaluate and rank our five LVLMs and\nfour prompting strategies. Our goal was to measure their susceptibility to generating\ntoxic behaviors, specifically focusing on toxicity and insult. The results highlighted that\nthe Qwen-VL-Chat and LLAVA-v1.6-Vicuna-7b models showed a higher propensity\nfor generating toxic content, marking them as particularly vulnerable (See Table 4).\nAmong the prompting strategies, Multimodal Toxic Prompt Completion was found to\nbe the most effective in eliciting toxic responses. This pattern suggests that certain\nmodel-prompt combinations might be especially likely to produce undesirable outputs.\nModel-specific analysis demonstrates statistically significant differences (F >\n50, p < 0.001) with small effect sizes (\u03b7 < 0.01). Models including Qwen-VL-Chat and\nLLAVA-1.6-Vicuna-7b exhibit higher levels of toxicity and insult than others. These\nresults are supported by the Games-Howell test, showing Qwen-VL-Chat consistently\nranked highest for both tasks (see Games-Howell test results in Appendix Tables E6\nand E7)."}, {"title": "Which Prompting Strategies are More Effective? (RQ3):", "content": "For strategy comparisons, significant differences (F > 600, p < 0.001) and moderate\neffect sizes (\u03b7 > 0.04) are observed. The Multimodal Toxic Prompt Completion strat-\negy consistently achieves higher scores across toxicity and insult tests, indicating its\nrobustness, while Social Identity Attacks strategy ranks lowest, implying it is less ef-\nfective in mitigating harmful outputs (See Table 5). These results are supported by\nthe Games-Howell test, showing Multimodal Toxic Prompt Completion consistently\nranked highest for both tasks (see Games-Howell test results in Appendix Tables E8\nand E9)."}, {"title": "Thematic Analysis of Toxic Responses", "content": "As we identified the vulnerable LVLMs and effective prompting strategies, we gath-\nered 3,287 generated responses with scores \u2265 0.5. To gain a better understanding of\ncontextual cues and thematic insights in the generated toxic responses, we performed\ntopic modeling using BERTopic. This process revealed predominant themes, including\nPolitical Discourse and Criticism, Cultural and Social Critique, Racism and Social Is-\nsues, Misinterpretation and Humor, and Offensive and Explicit Content. For instance,\nthe responses with political discourse and criticism referenced public political figures,\nsuch as Trump and Biden, ideological debates, partisan conflicts, and political humor."}, {"title": "Discussion", "content": "Our study identified the Qwen-VL-Chat and InstructBLIP-Vicuna-7b models as par-\nticularly vulnerable to producing responses with high toxicity levels. This finding\nemphasizes a critical need to enhance the safety mechanisms within these models.\nTheir relatively high propensity to generate toxic responses to adversarial prompts\nhighlights an urgent requirement for these models to improve their ability to manage\nof harmful behavior and minimize the generation of toxic outputs. Our analysis across\ntoxicity metrics and prompting strategies shed light on the significant risks associated\nwith the use and misuse of these technologies. Given that LVLMs increasingly produce\nhuman-like interactions, it is crucial to better understand their vulnerabilities to pro-\nducing toxic responses. This understanding is essential for mitigating potential risks\nand safeguarding against the exploitation of these vulnerabilities by malicious actors.\nBuilding on such understanding, it is important to contextualize the broader land-\nscape of online toxicity. Toxic behavior, though prevalent across online platforms\nand perhaps in broader society, often comprises a minority of interactions. Previous\nresearch has found that toxic occurrences can range from 3% to 41% (Beknazar-\nYuzbashev, Jim\u00e9nez Dur\u00e1n, McCrosky, & Stalinski, 2022; Park, Seering, & Bernstein,\n2022). This distribution pattern holds true across various forms of harmful behavior,\nincluding harassment (PEWHarassment, 2021)"}]}