{"title": "SlimGPT: Layer-wise Structured Pruning for Large Language Models", "authors": ["Gui Ling", "Ziyang Wang", "Yuliang Yan", "Qingwen Liu"], "abstract": "Large language models (LLMs) have garnered significant attention for their remarkable capabilities across various domains, whose vast parameter scales present challenges for practical deployment. Structured pruning is an effective method to balance model performance with efficiency, but performance restoration under computational resource constraints is a principal challenge in pruning LLMs. Therefore, we present a low-cost and fast structured pruning method for LLMs named SlimGPT based on the Optimal Brain Surgeon framework. We propose Batched Greedy Pruning for rapid and near-optimal pruning, which enhances the accuracy of head-wise pruning error estimation through grouped Cholesky decomposition and improves the pruning efficiency of FFN via Dynamic Group Size, thereby achieving approximate local optimal pruning results within one hour. Besides, we explore the limitations of layer-wise pruning from the perspective of error accumulation and propose Incremental Pruning Ratio, a non-uniform pruning strategy to reduce performance degradation. Experimental results on the LLaMA benchmark show that SlimGPT outperforms other methods and achieves state-of-the-art results.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) [1, 2, 3] have made significant strides in various natural language processing tasks, leading to the emergence of novel applications such as AI agents [4]. One of the factors contributing to the exceptional capabilities of LLMs is their massive parameter scales. However, these extensive parameters also introduce increased inference costs and deployment challenges, hindering the widespread application and adoption of LLMs. Accelerating inference for LLMs has become a focal point of current research. Model compression [5], as one of the strategies for inference acceleration, including techniques like pruning and quantization [6, 7], has been extensively researched. Nevertheless, earlier model compression techniques, particularly model pruning, typically rely on heavy post-training to recover the model's capabilities, which typically involves retraining with the entire training dataset. Given the constraints of current computational resources, the above approaches are not feasible for LLMs.\nIn the domain of LLM pruning, recent studies have largely focused on unstructured (or semi-structured) pruning [8], a method that shrinks models by selectively zeroing out weights considered non-critical. Despite its advancements, unstructured pruning falls short in substantially reducing parameter count, which is crucial for accelerating LLM inference as it is often bottlenecked on memory bandwidth and communication [9]. To accelerate inference speed, unstructured pruning models are often paired with specialized frameworks or hardware solutions. Conversely, structured pruning [10, 11] effectively decreases the model's parameter count by systematically eliminating columns or rows from weight matrices, enabling significant improvements in inference speed, and reduce deployment cost on conventional hardware. Yet, structured pruning often entails more pronounced compromises in model performance, which poses a greater challenge.\nRecently, researchers have applied the classic Optimal Brain Surgeon (OBS) framework to the compression of LLMs. This approach includes parameter compensation which can mitigate the loss incurred during compression and reduce the dependence on post-training. The OBS framework is currently applied in the areas of unstructured pruning [12] and quantization [13] for LLMs. However, there exist some challenges in its application to structured pruning:\n\u2022 The OBS is a fine-grained compression framework that compresses one parameter at each iteration, whereas structured pruning has a minimum granularity of either a column or head. Directly applying the OBS framework will result in high numerical errors, impairing model performance.\n\u2022 The OBS is essentially a layer-wise compression method. It focuses on each individual layer, thus failing to allocate pruning ratios for each layer rationally using global information (such as global gradients). This is crucial for LLM structured pruning, which relies on a non-uniform strategy to reduce the impact on performance.\nTo address these issues, we propose a new structured pruning method for LLMs. We introduce Batched Greedy Pruning to achieve low-cost and rapid pruning for LLMs. Specifically, for attention heads, we propose grouped Cholesky decomposition to select nearly optimal heads for pruning in each iteration, thereby maintaining an approximately locally optimal pruning result. For Feed-Forward Networks (FFNs), we achieve near-optimal and efficient pruning results through Dynamic Group Size. Furthermore, since the OBS is essentially a layer-wise compression framework, we investigate the error accumulation phenomenon in layer-wise pruning and propose pruning by Incremental Pruning Ratio, a straightforward non-uniform strategy to control the pruning rate of each layer, further mitigating performance loss under a given overall pruning ratio.\nContribution. In this paper, we propose SlimGPT, a layer-wise pruning approach that extends the classical OBS framework to structured pruning for LLMs. The characteristics of SlimGPT can be summarized as follows: (i) Task-agnostic pruning scheme. Only a random sample of data from generic pre-training corpora is needed as a calibration set, and we can obtain a compressed model with most performance preserved; (ii) Low-cost, low-resource, and time-efficient compression scheme. The model can be compressed using just a single GPU, a few hundred of calibration data, and about one hour; (iii) A universal pruning method for Transformer-based models. It has good transferability and, theoretically, is applicable to all large models based on the conventional Transformer architecture. We employ LLaMA models for pruning and conduct evaluations on wikitext2 and Commonsense Reasoning tasks. The results indicate that SlimGPT substantially retains the performance of the pruned models, surpassing state-of-the-art methods."}, {"title": "Related Work", "content": "Compression methods with regularization. Before the era of LLMs, using the scaling factors from Batch Normalization layers as indicators of channel importance made pruning based on regularization a very popular method [14, 15]. Notably, Louizos et al. [16] implemented the non-differentiable LO penalty in a differentiable form, a technique frequently used for pruning in large models. Compresso [17] combines LO regularization with LoRA training [18], effectively preserving model performance at a low cost. In a similar vein, Sheared LLaMA [19] employs augmented L0 regularization on inserted masks for structured pruning, using extensive data to restore performance and deliver compact yet powerful pruned models.\nGlobal gradient-based compression methods. NVIDIA's works [20, 21] involve a Taylor expansion of the global loss. By eliminating higher-order terms, it is revealed that the impact of a weight on the loss can be assessed using the magnitude of the weight combined with gradient information. Based on this, LLM-Pruner [11] employs a first-order importance estimation to gauge the importance of weights. LORAPrune [22] measures the importance of weights based on the gradients of the LORA parameters rather than the model's parameters, achieving commendable results.\nOutliers-dependent compression methods. Dettmers et al. [23] identifies an attribute unique to LLMs, where a small subset of activation values in the data features have magnitudes significantly larger than the others. And removing corresponding weights impacts model performance substantially. Building upon this, Wanda [24] proposes a simple yet effective unstructured pruning method, using the product of a weight's L1 norm and the L2 norm of eigenvalues to gauge its importance, achieving impressive pruning results. OWL [25] determines layer-wise sparsity ratios based on Layerwise Outlier Distribution (LOD), obtaining substantial performance gains at high sparsity levels.\nLayer-wise compression methods. The early works [26, 27] provide a layer-wise compression framework with a locally optimal solution named Optimal Brain Surgeon (OBS). And then OBC [28] reduces the computational burden by converting layer-wise pruning into row-wise pruning and updating the inverse Hessian using a proposed formula. Furthermore, GPTQ [13] accelerates the process with Lazy Batch-Updates and Cholesky Reformulation, enabling the application of this method to the quantization of LLMs. SparseGPT [12] also adapts this approach for unstructured pruning of LLMs. However, there appears to be no existing research that has implemented OBS in structured pruning for LLMs.\nStructured Pruning vs. Other Techniques. Given that OBS has previously been used in both quantization and unstructured pruning, and is now being applied to structured pruning, there is an inherent consistency across these three compression schemes. These methods actually compress the model at varying levels of granularity. Quantization, which \"trims\" floating-point precision, represents the finest granularity and delivers excellent compression outcomes. Structured pruning, on the other hand, involves trimming weight vectors and represents the coarsest granularity, naturally resulting in higher performance losses compared to other methods, which poses significant challenges. For small models, it is possible to recover most of the performance with post-training, but this is challenging to achieve in LLMs due to resource constraints. Nonetheless, structured pruning effectively reduces the number of parameters without needing special inference framework support and is compatible with the other two methods, thus still holding considerable potential for application."}, {"title": "Preliminary", "content": "Layer-Wise Pruning. Consider the scenario of pruning on a well-optimized model, known as post-training pruning, a prevalent approach involves decomposing the global model pruning challenge into layer-wise subproblems (i.e., Layer-wise pruning), which are typically modeled as issues of minimizing L2 error. Specifically, let $W_l$ represent the weights of the l-th layer of a pretrained model and $X_l$ be the input features for layer l. The goal is to determine pruned weights $\\tilde{W}_l$ that achieve a predefined pruning ratio while minimizing the squared error:\n$\\operatorname{argmin}_{\\tilde{W}_l} ||W_l X_l - \\tilde{W}_l X_l||_2^2$.\nOptimal Brain Surgeon (OBS) Framework. As Equation 1 can be rewritten as the sum of square error of each row of the weights to be pruned, the layer-wise pruning can be further split into row-wise pruning [28]. Consider the removal of a single weight from a row in $W_l$, Equation 1 has a closed-form solution [27]. Let $w_p$ denote a specific weight in a row of $W_l$, and let p be its corresponding index. Given that our optimization objective is to minimize row-wise squared error, the Hessian of this objective with respect to the weight row of layer l is given by $H_l = 2X_l X_l^T$. The weight to be pruned, $w_p$, as well as the necessary update $\\delta_p$ applied to the remaining weights of the same row to counterbalance the removal, can be determined through the following calculation:\n$\\underset{w_p}{argmin} \\frac{w_p^2}{H_{p,p}^{-1}}, \\; \\delta_p = - \\frac{w_p}{H_{p,p}^{-1}} H_{p, :}^{-1}$\nwhere $H_{p,p}^{-1}$ denotes the p th diagonal entry of the inverse Hessian, and $H_{p,:}^{-1}$ is its p th column. By iteratively using Equation 2 to remove one weight and update the remaining weights in the same row, one can obtain a locally optimal compressed model. After each iteration, $H^{-1}$ will be updated by removing the p row and column, which is represented by $H_{[-p]}$, here we use [-p] to indicate the removal of p row and column of the matrix. As $H^{-1}$ cannot be updated by simple removal as $(H_{[-p]})^{-1} \\neq (H^{-1})_{[-p]}$, to avoid the expensive full recomputations of $H^{-1}$, the following formula is proposed to quickly update $H^{-1}$ [28]:\n$(H_{[-p]})^{-1} = (H^{-1} - \\frac{H^{-1}_{:,p} H^{-1}_{p,:}}{H^{-1}_{p,p}})_{[-p]}$.\nThis framework can be practically applied to medium-sized models. However, for models with billions of weights, the iterative pruning becomes exceedingly time-consuming."}, {"title": "Methodology", "content": "In this section, by extending the OBS framework to structured pruning, we introduce SlimGPT from two aspects: (1) By employing Batched Greedy Pruning to reduce error computation, we minimize the performance degradation caused by pruning while also accelerating the pruning speed; (2) By analyzing the limitation of layer-wise pruning from the perspective of error accumulation, we introduce Incremental Pruning Ratio, a non-uniform pruning strategy."}, {"title": "Structured Pruning with OBS Framework", "content": "As mentioned above, the pruning between different rows is independent, making it possible to prune all rows simultaneously [29]. We extend the OBS framework to structured column pruning, i.e., pruning one column at a time and compensating the rest columns using the following formula:\n$\\text{W}_{:,p} = \\text{argmin}_{w_{:,p}} \\frac{\\sum w_{:,p}^2}{H_{p,p}^{-1}}, \\; \\Delta = - \\frac{W_{:,p}}{H_{p,p}^{-1}} H_{p,:}^{-1}$ ,\nwhere $H_{p,:}^{-1}$ denotes the p-th row of $H^{-1}$, and the obtained $\\Delta$ is a compensation matrix of the same size as $W$. We following previous works employ attention blocks and FFNs as the smallest units for pruning. By pruning the columns of the output matrix in attention blocks and the dimensionality reduction matrix in FFN blocks, we reduce the number of attention heads and FFN channels, thereby decreasing the model's parameter count.\nHowever, the above formula cannot be applied directly, as iteratively finding and pruning the column with the minimum error is time-consuming. More critically, the structural dependency in attention blocks imposes additional constraints on column pruning, making it impossible to evaluate the importance of a head based solely on information from a single column."}, {"title": "Batched Greedy Pruning", "content": "Given that the calculation of the pruning error requires only the diagonal elements of $H^{-1}$ (see Equation 4), which are updated after each iteration, computing these elements in advance allows for calculating the head-wise error. With the observation that the sequential row removal via Equation 3 for the symmetric $H^{-1}$ essentially corresponds to taking a Cholesky decomposition [13], we can obtain the elements in advance with Cholesky decomposition.\nHoewever, the matrix obtained by Cholesky decomposition is triangular, and the elements of the current row (column) are calculated based on the elements of all the previous rows (columns), which means the Cholesky decomposition breaks the comparability between rows (columns). So it is hard to obtain all the required information in advance through the Cholesky decomposition like [12, 13], whose error comparison is usually within the same column but structured pruning requires the comparison of different columns.\nSince structured pruning only requires traversing the columns that need to be removed, by rearranging the rows and columns corresponding to a head that is to be pruned in H to the front, and then invert the matrix followed by Cholesky decomposition, we can calculate the head error column-wise. However, repeated rearrangement followed by matrix inversion and Cholesky decomposition is highly time-consuming, and this is just to find one head to be pruned.\nWe accelerate the above process through two common lemmas (proofs are provided in the Appendix): (i) For symmetric H, the inverse matrix after permutation can be obtained by the same permutation of $H^{-1}$; (ii) The principal submatrix of symmetric $H^{-1}$ after Cholesky decomposition is equivalent to the Cholesky decomposition of its principal submatrix. Thus we can calculate the pruning error of all the heads at once through grouped Cholesky decomposition. Specifically, we inverse H once and split it into $n_{head}$ matrices along the main diagonal, with each remains definite and symmetric, and decompose them in parallel:\n$H^{-1} = \\text{Cholesky}(\\text{Stack}([H_{0:d,0:d}, H_{d:2d,d:2d}, ..., H_{(n-1)d:nd,(n-1)d:nd}]))$\nwhere decomposed $\\hat{H}^{-1}$ is a matrix of size $n_{head} \\times d_{head} \\times d_{head}$, $N_{head}$ and $d_{head}$ represent the head number and head dimension, respectively. Utilizing GPU acceleration, we can quickly calculate the value of the diagonal element in advance and calculate the head-wise error. Note that during error computation, we only update the diagonal elements of $H^{-1}$ and skip the update of W, which is small and does not dominate the ordering of errors.\nAfter determining the head to be pruned, we rearrange the corresponding columns of W and the corresponding rows and columns of $H^{-1}$ to the front, and again use the global Cholesky decomposition on reordered $H^{-1}$ to prune the head column by column until the first head is pruned. In this way, we can avoid traversing columns that do not need pruning and only traverse necessary columns to improve pruning efficiency further."}, {"title": "Incremental Pruning Ratio", "content": "Through Batched Greedy Pruning, we can obtain near-optimal structured pruning results for each layer. However, finding a suitable pruning ratio for each layer is difficult, as considering global information is quite challenging for layer-wise pruning, which only provides optimal pruning results for the current layer. Maintaining a uniform pruning ratio across all layers is unreasonable and will impact model performance, especially when the pruning ratio is high. Existing works have different approaches to the problem. For example, LLM-Pruner [11] avoids pruning in the initial and final layers while maintaining a consistent ratio in the intermediate layers to manually implement non-uniform pruning. OWL [25] adjusts sparse ratios dynamically for each layer based on the proportion of feature outliers, which is applied to unstructured pruning.\nWe find that layer-wise pruning, particularly structured layer-wise pruning, suffers from error accumulation due to its locality. Errors introduced during pruning in one layer can be amplified in subsequent layers, resulting in significant discrepancies between the final model output and the original. Based on this observation, we propose a straightforward pruning strategy for layer-wise pruning, termed Incremental Pruning Ratio, which can effectively minimize pruning losses without any additional operation.\nIn Incremental Pruning Ratio, without loss of generality, we employ a logarithmically increasing strategy to control the layer-wise pruning ratio. Specifically, for an n-layer model with the first and last layer pruning ratios denoted as $r_0$ and $r_{n-1}$ respectively, the pruning ratio for the i-th layer is defined as follows:\n$r_i = r_0 + (r_{n-1} - r_0) \\frac{\\text{log}(i + 1)}{\\text{log}(n)}, \\quad (0 \\leq i < n)$\nwhere $r_i$ represents the pruning ratio for the i-th layer. This formula ensures that the pruning ratio from the first layer to the last layer transitions smoothly as a logarithmic curve. The strategy mitigates the pruning error accumulation in shallow layers while avoiding the issue of excessive pruning in the deeper layers, allowing for further reduction in performance loss."}, {"title": "Experiment", "content": "Implementation details. We use C4 dataset [30] as the calibration set. From the first shard of C4, we randomly select 256 2048-token sequences for pruning. To restore performance, we following LLM-Pruner [11] finetune the pruned model with LORA [18]. We tune with Alpaca datsets [31] for one epoch and utilize the AdamW optimizer with an initial learning rate set to 1e-4, coupled with a cosine annealing schedule for the learning rate. The global batch size is set to 64 and the sequence length is truncated to 256. All pruning experiments are conducted on a single A100, while finetuning is performed using two A100s.\nModels and Metrics. To assess the effectiveness and generality of SlimGPT, We carry out a series of experiments on the LLaMA families [2]. And to measure the effectiveness of our pruned models in the task-agnostic setting, we follow previous pruning works to evaluate language modeling performance and commonsense reasoning capabilities. The language modeling performance is evaluated on the WikiText2 [32] validation set with sequence length truncated to 128, and the commonsense reasoning capabilities is carried out under a zero-shot setting on the Commonsense Reasoning datasets, which encompass seven diverse subtasks: BoolQ [33], PIQA [34], HellaSwag [35], WinoGrande [36], ARC-easy [37], ARC-challenge [37], and OpenbookQA [38]. We utilize the lm-eval-harness framework [39] to conduct these evaluations."}, {"title": "Main Result", "content": "Performance Evaluation\nTo facilitate a more effective comparison of the evaluated results with prior works, we prune the LLaMA-7B model using four distinct pruning ratios\u201420%, 25%, 33%, and 50%\u2014resulting in four smaller models with parameter counts of 5.4B, 5B, 4.5B, and 3.4B, respectively.  Compared to other approaches, SlimGPT demonstrates superior performance in language modeling and commonsense reasoning across most subtasks. Under a pruning condition of 20%, SlimGPT achieves a slightly better perplexity score than the best existing results (16.68 vs. 16.80) and shows a 3.6-point improvement in zero-shot average (65.07 vs. 61.50). As the pruning ratio increases to 50%, the advantages of SlimGPT become even more pronounced. SlimGPT without post-training represents an approximately 8% improvement over the baseline LLM-Pruner in average performance (52.23 vs. 48.35), and with post-training, the average performance improvement reaches up to 11% (53.76 vs. 48.35). Specifically, on a dataset like Hellaswag, the improvement soars up to 25% (59.94 vs. 47.86).\nMoreover, we observe that although SlimGPT affects different subtasks to varying degrees, its impact is relatively balanced across different tasks, eliminating the occurrence of disproportionately large losses in particular tasks. At lower pruning ratios, some tasks such as BoolQ can even outperform the original unpruned model. Additionally, the effects of fine-tuning also differ among tasks, significantly improving tasks like HellaSwag and ARC-easy, while potentially causing negative side effects for tasks such as BoolQ and WinoGrande. This phenomenon is likely closely associated with the datasets used for fine-tuning.\nFor larger-scale models such as LLaMA-13B and LLaMA-30B, previous works have not provided pruning results for these models. Therefore, we solely compare our results to the LLM-Pruner baseline, concentrating on two specific pruning settings: a lower pruning ratio (20%) and a higher pruning ratio (50%). The replication of LLM-Pruner is consistent with the method described in the paper, where the pruned models by LLM-Pruner are finetuned with LORA.\nTable 2 presents the pruning results of LLaMA-13B and LLaMA-30B, and we can draw similar conclusions: SlimGPT outperforms LLM-Pruner in terms of both PPL and zero-shot average scores even without post-training. Note that as the scale of the model increases, the performance loss due to pruning becomes smaller, suggesting a higher degree of parameter redundancy in larger models. At a low pruning ratio of 20%, the LLaMA-13B model's average performance in commonsense reasoning is nearly on par with that of the original, unpruned model (68.06 vs. 68.16). Similarly, the pruned LLaMA-30B model slightly outperforms the unpruned version (72.56 vs. 71.92). For the perplexity task, even though SlimGPT exhibits gaps compared to the original model, it still performs better than baseline, even at low pruning ratios.\nBesides, we can find that the performance of LLaMA-13B pruned by 50% falls short compared to LLaMA-7B pruned by 20%. This highlights the limitations of low-cost fine-tuning, where resource constraints and training with techniques like LoRA result in limited performance recovery for the model. Therefore, using lower pruning ratios to compress smaller LLMs yields better returns."}, {"title": "Efficiency Analysis", "content": "The pruning runtime and memory usage for LLaMA-7B and LLaMA-13B are detailed in Table 3. Memory usage fluctuates based on the model size and the calibration scale, while the pruning speed is additionally affected by the pruning ratio. We demonstrate the pruning efficiency results derived from our experimental setup. Utilizing SlimGPT, which operates on a layer-wise basis, there is no need to load the entire model at once. Instead, we only load the parameters of the current layer along with the corresponding input features, significantly reducing memory consumption. For instance, to prune the 7B model by 20%, approximately 7 GB of GPU memory and 18 minutes are required to complete the process. Similarly, pruning the 13B model by 50% necessitates around 12 GB of GPU memory and 41 minutes to finalize.\nTable 4 illustrates the inference latency and memory usage of the pruned LLaMA-7b models. We prune LLaMA-7b by 20% and 50% respectively. The maximum output limit is set to 512 and the presented values are the average derived from 50 inference trials. When pruning 50% of the parameters, the memory usage of the model during inference decreases to approximately 51% (14297MB vs. 27737MB), and the inference latency is reduced to about 69% (9.21ms vs. 13.51ms)."}, {"title": "Ablation Study", "content": "We systematically analyze the influence of several key parts of SlimGPT on the pruning effect, including the Batched Greedy Pruning and Incremental Pruning Ratio strategy. Within the calibration dataset, we conduct thorough experiments with sample sizes and sequence lengths. Unless specifically stated otherwise, all the following experiments are conducted under the condition of pruning 50% of LLaMA-7b without further post-training, to eliminate potential confounding effects. Supplementary ablation experiments can be found in the Appendix."}, {"title": "Impact of Batched Greedy Pruning Strategy", "content": "We leverage grouped Cholesky decomposition to enhance the accuracy of head-wise error computation in attention blocks. Similarly, for FFNs, our proposed Dynamic Group Size substantially increases pruning efficiency while preserving near-optimal pruning results. To validate the effectiveness of these two strategies, we start with the complete SlimGPT algorithm and first remove the Dynamic Group Size (denoted as \u2018-DGS'), setting the group size for FFN pruning to a fixed value of 128. Then, we remove the grouped Cholesky decomposition (denoted as \u2018-GCD') and use the initial $H^{-1}$ to calculate head-wise errors.  For attention blocks, the grouped Cholesky decomposition strategy plays a key role in language modeling capabilities by improving the accuracy of error compensation. Replacing it with ordinary Cholesky decomposition results in a significant increase in PPL . In comparison to the naive fixed group size scheme for FFNs, the Dynamic Group Size strategy proposed contributes to maintaining the model's commonsense reasoning performance."}, {"title": "Impact of Incremental Pruning Ratio Strategy", "content": "The Incremental Pruning Ratio is a strategy specifically proposed for addressing the issue of layer-wise pruning. To maintain generality, we selected various common non-uniform strategies for comparative experiments, including logarithmic and linear increase strategies, as well as their corresponding decrease strategies. Among these, the logarithmic increase strategy is the default configuration for SlimGPT. Additionally, we conduct experiments under the setting of uniform pruning. From an overall perspective, the increase strategy for the pruning ratio has a clear advantage over uniform, and likewise, uniform shows a distinct advantage over decrease. Such results further verify the phenomenon of layer-wize error accumulation. As for the increase strategies of logarithmic and linear changes, due to disparities in model sizes, their results are not entirely comparable. The former performs best in language modeling (38.83), while the latter shows better performance in common sense reasoning tasks (53.45)."}, {"title": "Effects of Calibration Samples & Sequence Length", "content": "We delve further into the impact of calibration samples and sequence length, and we choose C4 dataset for our experiments as it has a longer average sequence length. In exploring the effects of the sample scale, we fix the sequence length at 256 and test five scales ranging from 128 to 2048; similarly, when investigating the impact of sequence length, the sample scale is set to 256, with choices of sequence length varying from 64 to 2048. As the number of samples increases, the PPL and zero-shot averages show a positive overall trend. Furthermore, after the sample count reaches 2048, the PPL does not bottom out, and there is room for further reduction. Similar phenomena can be observed in experiments on sequence length. With more sufficiently high-quality datasets with longer sequences, we believe SlimGPT can achieve better pruning effects."}, {"title": "Conclusion", "content": "In this work, we introduce a fast, structured pruning method for large-scale models within resource-constrained scenarios, based on the OBS framework, termed SlimGPT. Leveraging the novel Batched Greedy Pruning, we enhance the accuracy of pruning error estimation, thereby minimizing performance degradation from pruning. Moreover, we analyze the limitations of layer-wise pruning from the perspective of error accumulation and propose a non-uniform strategy named Incremental Pruning Ratio, which effectively improves the pruned model's performance. Evidence from open-source experiments affirms the efficacy of our approach.\nLimitations. Even though SlimGPT achieves SOTA results in the structured pruning of LLMs, the model performance degradation at high pruning ratios (e.g., 50%) or on more complex tasks (e.g., LongBench) is still significant. How to enhance the model compression effectiveness under low-resource conditions remains a challenge. Moreover, we utilized a naive logarithmic change strategy in the Incremental Pruning Ratio, which, while ensuring generality, is not the optimal solution. The most suitable non-uniform approach requires further exploration. Lastly, similar to many large-scale open-source models available today, the model obtained through pruning by SlimGPT poses risks in terms of ethical safety and requires cautious handling."}, {"title": "About Structural Dependency", "content": "The structural dependency problem in attention blocks happens when a column of weights in an attention head is removed, elements in other positions in the attention matrix are also affected because of the softmax function. Directly summing the errors across all columns of a head may result in significant numerical inaccuracies, as Equation 4 applies only to single-column pruning instead of multiple columns. To achieve multi-column pruning, we need to iterate using Equation 4 and update $H^{-1}$ with Equation 3, which makes it difficult to assess the pruning error of a total attention head in advance."}, {"title": "Layer-wise Pruning Ratio at Pruning Stage", "content": "To more conveniently present the details of the logarithmic increase variation in Incremental Pruning Ratio, we illustrate the layer-wise pruning ratios for SlimGPT's logarithmic increase and LLM-Pruner's heuristic setting at a 50% pruning rate. SlimGPT starts with a lower initial pruning rate, with a rapid increase in the shallower layers followed by a slower change in the deeper layers, eventually approximating the fixed pruning ratio of LLM-Pruner. Their biggest difference lies in the handling of the last two layers. LLM-Pruner lacks parameter compensation, so the layers pruned at the output end have a larger impact on the final results, whereas SlimGPT reduces their impact on the model through parameter compensation."}, {"title": "Training Loss at Recovery Stage", "content": "To figure out whether overfitting has occurred during the finetuning phase, potentially affecting the performance evaluation of the pruned models, we plot the loss curve of the model during the fine-tuning stage. We train for one epoch on the Alpaca dataset while using Wikitext2 as the evaluation set. The figure illustrates the train loss on Alpaca and the evaluation loss on Wikitext2. As is shown, the training loss is decreasing and converging normally, with no significant fluctuations in the evaluation loss on Wikitext2, indicating that fine-tuning is conducted on general data without specific optimization for Wikitext2, and there is no occurrence of overfitting."}]}