{"title": "Learning World Models for Unconstrained Goal Navigation", "authors": ["Yuanlin Duan", "Wensen Mao", "He Zhu"], "abstract": "Learning world models offers a promising avenue for goal-conditioned reinforcement learning with sparse rewards. By allowing agents to plan actions or exploratory goals without direct interaction with the environment, world models enhance exploration efficiency. The quality of a world model hinges on the richness of data stored in the agent's replay buffer, with expectations of reasonable generalization across the state space surrounding recorded trajectories. However, challenges arise in generalizing learned world models to state transitions backward along recorded trajectories or between states across different trajectories, hindering their ability to accurately model real-world dynamics. To address these challenges, we introduce a novel goal-directed exploration algorithm, MUN (short for \"World Models for Unconstrained Goal Navigation\"). This algorithm is capable of modeling state transitions between arbitrary subgoal states in the replay buffer, thereby facilitating the learning of policies to navigate between any \"key\" states. Experimental results demonstrate that MUN strengthens the reliability of world models and significantly improves the policy's capacity to generalize across new goal settings.", "sections": [{"title": "Introduction", "content": "Goal-conditioned reinforcement learning (GCRL) has emerged as a powerful framework for learning diverse skills within an environment and subsequently solving tasks based on user-specified goal commands, without requiring further training( Mendonca et al. (2021); Andrychowicz et al. (2017)). Given that specifying dense task rewards for GCRL requires domain expertise, access to object positions, is time-consuming, and is prone to human errors, rewards in GCRL are typically sparse, signaling success only upon reaching goal states. However, sparse rewards pose a challenge for exploration during training. To address this challenge, several previous methods, e.g., Hafner et al. (2019a); Hansen et al. (2023); Mendonca et al. (2021) have proposed learning a generative world model of the environment using a reconstruction (decoder) objective, an instantiation of Model-based Reinforcement Learning (MBRL), visualized in Fig. 1. This approach is appealing because the world model can provide a rich learning signal( Yu et al. (2020); Georgiev et al. (2024)). For example, world models allow agents to plan their actions or exploratory goals without directly interacting with the real environment for more efficient exploration( Hu et al. (2023); Sekar et al. (2020)).\nExisting MBRL techniques train world models to capture the dynamics of the environment from the agent's past experiences stored in a replay buffer. The richness of the data stored in the agent's replay buffer directly impacts the quality of a World Model. It is expected that the world model generalizes reasonably well to the state space surrounding the trajectories recorded in the replay buffer. However, the world model may not generalize well to state transitions backward along recorded trajectories or to states across different trajectories, which impedes the world model's learning of the real-world dynamics."}, {"title": "Problem Setup and Background", "content": "We consider the problem of goal-conditioned reinforcement learning (GCRL) under a Markov Decision Process (MDP) parameterized by (S, A, P, G, \u03b7, R, po). S and A are the state and action spaces, respectively. The probability distribution of the initial states is given by po(s), and P(s'|s, a) is the transition probability. \u03b7 : S \u2192 G is a mapping from the state space to the goal space, which assumes that every state s can be mapped to a corresponding achieved goal g. The reward function R is defined as R(s, a, s', g) = 1{n(s') = g}. We assume that each episode has a fixed horizon T. For ease of presentation, we further assume S = G and \u03b7 is an identify function in this paper.\nA goal-conditioned policy is a probability distribution \u03c0 : S \u00d7 G \u00d7 A \u2192 R+, which gives rise to trajectory samples of the form \u03c4 = {so, ao, 9, 81, . . ., ST}. The purpose of the policy \u03c0 is to learn how to reach the goals drawn from the goal distribution pg. With a discount factor \u03b3 \u2208 (0,1), it\nmaximizes $J(\\pi) = E_{g \\sim p_g, \\tau \\sim \\pi(g)} [\\sum_{t=0}^{T-1} \\gamma^t .R(S_t, a_t, S_{t+1}, g)]$.\nIn the context of model-based reinforcement learning (MBRL), a world model M is trained over trajectories sampled from the agent's interactions with the real environment, which are stored in a replay buffer, to predict the dynamics of the real environment. Fig. 1 illustrates the general MBRL framework. We use the world model structure M of Dreamer (Hafner et al. (2019a,b, 2020, 2023)) to learn real environment dynamics as a recurrent state-space model (RSSM). We provide a detailed explanation of the network architecture and working principles of the RSSM in Appendix A.1. Our study focuses on tackling the world model learning problem in goal-conditioned model based reinforcement learning settings. Particularly, we consider GC-Dreamer (goal-conditioned Dreamer) as an important baseline with the following learning components:\nWorld Model: $M(s_t | s_{t-1}, a_{t-1})$\nActor: $\\pi (a_t | s_t,g)$\nCritic: $V(s_t, g)$                                                                                                                                                                (1)"}, {"title": "Training World Models for Unconstrained Goal Navigation", "content": "In this section, we introduce MUN, our main approach to addressing the core challenge in GCRL: efficient exploration in long-horizon, sparse-reward environments. Our approach focuses on enhancing the agent's understanding of the real-world environment through improved dynamic (world) modeling and latent space representation. As the quality of the world model improves, the goal-conditioned policy developed from it generalizes more effectively to the real environment. By closing the generalization gap between the policy's behavior in the real environment and the world model, MUN effectively guides the agent's exploration towards the desired goal region."}, {"title": "Training Generalizable World Models", "content": "Fig 1 illustrates the general framework of Model-based RL, where world models are trained using agent's experiences stored in a replay buffer populated with observed environment transitions (St, at, St+1) linking the environment's future states st+1 and past states st along with the corresponding control actions at. The richness of the environment space and dynamic transitions captured by the replay buffer define the extent of what a world model can learn about the real environment. Through supervised learning, the model can generalize reasonably well within the state space moving forward along the trajectories recorded in the replay buffer. However, it may be inaccurate for the state transitions moving backward along the recorded trajectories or across different trajectories.\nConsider the task of stacking blocks using a robot manipulator in Fig. 2(a). When humans learn to stack blocks, they also understand how to reverse the process to unstack the blocks or return to the initial state. In contrast, a world model trained solely on data from policies under training for stacking"}, {"title": "Key Subgoal Generation through Distinct Action Discovery (DAD)", "content": "Having set up the main learning algorithm, we seek to address: how should we pick exploration-inducing goals from the replay buffer at training time to help learn generalizable world models? A straightforward strategy is to sample trajectories from the replay buffer and select subgoal states at fixed time intervals along these trajectories. We improve this simple approach with a practical method called DAD (Distinct Action Discovery) for identifying key subgoal states, which represent the pivotal milestones necessary to complete a complex task. Consider the block stacking task as an example. The robotic arm must be able to move its gripper to the vicinity of a block, close the gripper, lift the block, move to the top of another block, release the block, and finally open the gripper. These key subgoal states are essential for completing the task. We illustrate the roles of key states in a 3-Block Stacking task in Fig. 2(a). For an agent to learn this task, it must master reaching and navigating between the key states. By training world models for unconstrained transitions between these key states, MUN can develop models that more accurately capture the task structure and learn policies capable of adapting to novel goal scenarios e.g. block unstacking.\nThere exist methods for identifying key\nstates( Zhang et al. (2021); Paul et al.\n(2019)). However, these methods often\ntend to be overly complex, leading to in-\nsufficient generalization across different\nenvironments and requiring adjustments\nto the methods' components or parameters\nfor various tasks. Our approach is based"}, {"title": "Experiments", "content": "We evaluate MUN across various robotic manipulation and navigation environments, aiming to address the following three research questions: (RQ1) Does MUN outperform other goal-conditioned model-based reinforcement learning baselines with advanced exploration strategies? (RQ2) Can DAD effectively identify key subgoal states along trajectories to the environment goal region? (RQ3) Does MUN successfully leverage the bi-directional replay buffer to train a generalizable policy for navigating effectively between arbitrary subgoals?"}, {"title": "Environments", "content": "We conducted experiments on six challenging goal-conditioned tasks to evaluate MUN. In Ant-Maze, an ant-like robot is tasked to learn complex 4-legged locomotion behavior and navigate around the"}, {"title": "Baselines", "content": "We compare MUN with the following baselines. The GC-Dreamer baseline is discussed in Sec. 2. We include two baselines based on the Go-Explore strategy( Ecoffet et al. (2019)) that has been proved efficient in the GCRL setting: MEGA( Pitis et al. (2020)) and PEG( Hu et al. (2023)). A Go-Explore agent firstly uses its goal-conditioned policy $\\pi_G$ to approach a sampled exploration-inducing goal command g, referred to as the Go-phase. In the Explore-phase, it activates an exploration policy $\\pi_E$ to explore the environment from the terminal state of the Go-phase. In contrast, MUN improves the generalization of world models to facilitate effective real-world environment exploration. During training, MUN collects trajectories that navigate between two goal states sampled from its candidate subgoal set, essentially replacing the \"Explore-phase\" in \"Go-Explore\" with another \"Go-phase\". MEGA commands the agent to rarely seen states at the frontier by using kernel density estimates (KDE) of state densities and chooses low-density goals from the replay buffer. PEG selects goal commands to guide an agent's goal-conditioned policy toward states with the highest exploration potential given its current level of training. This potential is defined as the expected accumulated exploration reward during the Explore-phase. Similar to MUN, our baseline methods, named PEG-G and MEGA-G, augment GC-Dreamer with the PEG and MEGA Go-Explore strategies, respectively. In these methods, the replay buffer D contains not only trajectories sampled by the GCRL policy $\\pi_G$ commanded by environment goals but also exploratory trajectories sampled using the Go-Explore strategies. The exploration policy $\\pi_E$ in PEG-G and MEGA-G is the Plan2Explore policy from Sekar et al. (2020), which encourages the agent to actively search for states that induce disparities among an ensemble of world models.\nWe note that MUN and the baselines are all implemented based on the Dreamer framework as realized in GC-Dreamer\u00b9."}, {"title": "Results", "content": "Fig. 4 shows the evaluation performance of MUN and all baselines across training. MUN demonstrates superior performance compared to the baseline models, excelling in both the final success rate and the speed of learning. MUN outperforms the Go-Explore baselines (MEGA-G and PEG-G) across all tasks, demonstrating the effectiveness of the exploration strategy in MUN over the alternative Go-Explore strategies. In the most challenging tasks-block stacking, block rotation, and pen rotation-MUN shows a significant margin of superiority. For example, MUN achieves over 95%"}, {"title": "Ablation study", "content": "We conducted the following ablation studies to investigate MUN's exploration goal selection mechanism. First, we investigated the effect of the number of subgoal states (Ns) in our algorithm. MUN sequentially traverses N\u2083 = 2 goal states sampled from the replay buffer to explore the environment during each training episode. We introduced an ablation MUN-Ns-3 that sets N\u2083 = 3. This ablation aims to investigate whether increasing Ns leads to improved learning performance. Second, we considered an ablated version of MUN, named MUN-noDAD, which replaces the goal sampling strategy DAD (Algorithm 2) with a simple method that chooses goal states with fixed time interval in trajectories sampled from the replay buffer. This ablation investigates the importance of identifying key subgoal states, which represent pivotal milestones necessary to complete a complex task. It seeks to determine whether training world models from state transitions between these key states in MUN is essential, or if using any states from the replay buffer would suffice. Lastly, we explored an alternative key subgoal discovery strategy. MUN identifies key subgoals for exploration as states in the replay buffer that result in distinct actions within the action space. We introduced an ablation, MUN-KeyObs, which directly discovers key subgoals from the state space by identifying centroids of (latent) state clusters in the replay buffer, following the strategy in Zhang et al. (2021).\nThe results are depicted in Fig. 8. MUN outperforms all ablated versions. Setting N\u2083 = 3 slows down the training performance, supporting our claim it suffices to set N\u2083 = 2. The performance of MUN-noDAD and MUN-KeyObs does not match MUN, especially in the 3 Block Stacking environment, highlighting that discovering key subgoals in the action space (the DAD strategy) indeed contributes to higher performance and efficiency. It is noteworthy that the ablation methods achieve a relatively small gap in success rates compared to MUN in the challenging Block Rotation and Pen Rotation environments. This suggests that MUN's approach to learning a world model from state transitions between any states in the replay buffer (whether tracing back along recorded trajectories or transitioning across separate trajectories) alone is effective in bridging the generalization gap between the model and the real environment."}, {"title": "Related Work", "content": "Model-based reinforcement learning (MBRL) is a promising approach to reinforcement learning that learns a model of the environment and uses it to plan actions(Sutton (1991); Deisenroth and Rasmussen (2011); Oh et al. (2017); Chua et al. (2018)). It has achieved remarkable success in numerous control tasks and games, such as chess(Silver et al. (2017); Schrittwieser et al. (2020); Xu et al. (2022)), Atari games(Hafner et al. (2020); Schrittwieser et al. (2020); Oh et al. (2017)), continuous control tasks(Kurutach et al. (2018); Buckman et al. (2018); Hafner et al. (2019b); Janner et al. (2019)), and robotic manipulation tasks(Lowrey et al. (2018); Luo et al. (2018)). The dynamic model serves as a pivotal component of model-based reinforcement learning, primarily fulfilling two key roles: planning actions(Deisenroth and Rasmussen (2011); Oh et al. (2017); Chua et al. (2018); Lowrey et al. (2018); Hafner et al. (2019b)) or generating synthetic data to aid in the training"}, {"title": "Conclusion", "content": "In summary, we introduce MUN, a novel goal-directed exploration algorithm designed for effective world modeling of seamless transitions between arbitrary states in replay buffers, whether retracing along recorded trajectories or transitioning between states on separate trajectories. As the quality of the world model improves, MUN demonstrates high efficacy in learning goal-conditioned policies in sparse-reward environments. Additionally, we present a practical strategy DAD for identifying pivotal subgoal states, which act as critical milestones in completing complex tasks. The experimental results underscored the effectiveness of MUN in strengthening the reliability of world models and learning policies capable of adapting to novel test goals."}, {"title": "Reproducibility Statement", "content": "The code of MUN is provided on https://github.com/RU-Automated-Reasoning-Group/MUN. For hyperparameter settings and baseline pseudocode, please refer to Appendix D and Appendix E.3."}, {"title": "Appendix", "content": "A Extended Background"}, {"title": "Dreamer World Model", "content": "The RSSM consists of an encoder, a recurrent model, a representation model, a transition predictor, and a decoder, as formulated in Equation 3. And it employs an end-to-end training methodology, where its parameters are jointly optimized based on the loss functions of various components, including dynamic transition prediction, reward prediction, and observation encoding-decoding. These components often operate in a latent space rather than the original observation space, as encoded by the World Model. Therefore, during end-to-end training, the losses of all components indirectly optimize the latent space.\nThe encoder fe encodes the input state xt into a embed state et, which is then fed with the deterministic state ht into the representation model q$ to generate the posterior state zt. The transition predictor po predicts the prior state 2t based on the deterministic state ht without access to the current input state xt. Using the concatenation of either (ht, zt) or (ht, 2t) as input, the recurrent transition function f iteratively updates the deterministic state ht with given action at.\nEncoder:\n$e_t = f_e(e_t/x_t)$\nRecurrent model:\n$h_t = f(h_{t-1}, z_{t-1}, a_{t-1})$\nRepresentation model:\n$z_t \\sim q_\\$(z_t|h_t, e_t)$\nTransition predictor:\n$\\hat{z}_t \\sim p(\\hat{z}_t/h_t)$\nDecoder:\n$x_t \\sim f_D(x_t | h_t, z_t)$                                                                                                                                                                                  (3)"}, {"title": "Temporal Distance Training in LEXA", "content": "The goal-reaching reward rG is defined by the self-supervised temporal distance objective (Mendonca et al. (2021)) which aims to minimize the number of action steps needed to transition from the current state to a goal state within imagined rollouts. We use bt to denote the concatenate of the deterministic state ht and the posterior state z\u0142 at time step t.\n$b_t = (h_t, z_t)$                                                                                                                                                                           (4)\nThe temporal distance Dt is trained by sampling pairs of imagined states bt, bt+k from imagined rollouts and predicting the action steps number k between the embedding of them, with a predicted embedding \u00eat from bt to approximate the true embedding et of the observation xt.\nPredicted embedding:\n$emb(b_t) = \\hat{e} \\approx e_t$, where\n$e_t = f_E(x_t)$                                                                                                                                                                           (5)\nTemporal distance:\n$D_t(\\hat{e}_t, \\hat{e}_{t+k}) \\approx k/H$ where $\\hat{e}_t = emb(b_t)$\n$\\hat{e}_{t+k} = emb(b_{t+k})$                                                                                                                                                                           (6)\n$r^G(b_t, b_{t+k}) = -D(\\hat{e}_t, \\hat{e}_{t+k})$                                                                                                                                                                   (7)"}, {"title": "Limitations and Future Work", "content": "The MUN has provided powerful guidance in enhancing world model learning by repeatedly studying the transitions between various key states. This allows the acquisition of richer dynamic transitions and deepens the world model's understanding of the real world. However, such a framework requires an efficient strategy for discovering key states, as evidenced by the comparative results of the MUN and MUN-noDAD. We found that although DAD excels in discovering key states with its simple and efficient method, it will identify ineffective and task-irrelevant states in tasks with highly complex"}, {"title": "Environments", "content": "In this task, the robot must stack three blocks in different colors into a tower shape. While PEG assesses goals of varying difficulty levels: 3 easy goals (picking up a single block), 6 medium goals (stacking two blocks), and 6 hard goals (stacking three blocks), our evaluation is focused solely on the 6 hard goals, and we use only 3 hard goals of them as the guiding goals from the training environment. Training and evaluating with only the hardest goals imposes a significant challenge for the MUN. However, we observed that the MUN can spontaneously discover additional easy and medium goals through DAD, as these serve as critical transitional states toward the hard goals. The environment is characterized by a 14-dimensional state and goal space. The first five dimensions capture the gripper's state, while the remaining nine dimensions correspond to the xyz positions of each block. The action space is 4-dimensional, with three dimensions dedicated to the gripper's xyz movements and the fourth dimension controlling the gripper's finger movement. Success is defined by achieving an L2 distance of less than 3 cm between each block's xyz position and its target position. This environment is a modified version of the FetchStack3 environment from Pitis et al. (2020), designed to better test the robot's precision in stacking."}, {"title": "Walker", "content": "In this environment, a 2D walker robot is trained and evaluated on its ability to move across a flat surface. The environment's implementation is based on the code from Mendonca et al. (2021). To thoroughly assess the agent's capability and precision in covering longer distances, we expanded the evaluation goals to 12 (\u00b113, \u00b116, \u00b119, \u00b122, \u00b125, \u00b128) along the x axis from the initial position. In our training setup for the MUN, we only use the goals at \u00b113 and \u00b116 provided by the environment, but we evaluate the agent's performance across all 12 goals. Success is measured by verifying whether the agent's x position is within a small margin of the target x position. The state and goal space in this environment are nine-dimensional, comprising the walker's xz positions and its joint angles. This configuration ensures a comprehensive evaluation of the walker's locomotion capabilities."}, {"title": "Ant Maze", "content": "This environment builds upon the Ant Maze from Pitis et al. (2020), incorporating a few modifications. The state and goal spaces in the Ant Maze environment are highly complex, totaling 29 dimensions. These dimensions include the ant's xyz position, joint angles, and velocities. The first three dimensions account for the xyz position, the next 12 dimensions capture the joint angles of the ant's limbs, and the remaining 14 dimensions represent the velocities of the joints and the ant's movements in the xy plane. The action space consists of 8 dimensions, controlling the hip and ankle movements of the ant's legs. We matched the goal space to the state space, which includes the ant's"}, {"title": "Fetch Slide", "content": "In this task, a robotic arm with a two-fingered gripper must push an object along a flat surface to a specific goal location. We use the \"FetchSlide-v1\" environment from Gymnasium, where the robot operates in a 25-dimensional state space that includes the robot's joint states, object position, and goal information. The goal space is 3-dimensional, representing the target coordinates for the object. Each episode presents a unique random goal location within a bounded area, requiring the agent to adjust its pushing strategy accordingly. A key challenge in Fetch Slide lies in the indirect manipulation of the object. The agent must accurately control the force and direction of its push while accounting for physical properties like friction, surface irregularities, and object momentum. Unlike grasping or lifting tasks, sliding demands precise force calibration and anticipation of the object's response to contact. For evaluation, the agent's learned policy is tested across 50 episodes with different goal locations, assessing its ability to generalize over varied configurations. Training goals are randomly generated from the environment, helping the agent explore diverse sliding trajectories to improve robustness across different scenarios."}, {"title": "Block and Pen Rotation", "content": "In this task, a robotic hand must manipulate either a thin pen or a block to achieve specified target rotations. We use \"HandManipulatePenRotate-v1\" and \"HandManipulateBlockRotateXYZ-v1\" versions of the gymnasium environments. Both tasks feature a state space of 61 dimensions, encompassing the robot's joint states, object states, and goal information. The goal space is 7-dimensional, representing the target pose details. Each episode will have randomized target rotations goal for all axes of the block and for the x and y axes of the pen. The pen is more challenging to handle due to its tendency to slip, requiring more precise control compared to the block. For evaluation, the latest policy is tested 50 episodes for each task, with each episode having a unique random goal. In our framework, training goals are also randomly generated from the environment."}, {"title": "Baselines", "content": "We first present our overall training framework for goal-conditioned model-based reinforcement learning (MBRL). It is important to note that all baselines utilize this training framework, differing only in the strategy employed for collecting trajectories within the real environment. Our training framework is based on the implementation of LEXA paper(Mendonca et al. (2021))."}, {"title": "Go-Explore", "content": "Our baselines utilize the state-of-the-art Go-Explore exploration framework, following the implementation detailed in the PEG paper(Hu et al. (2023)). This approach initially employs a goal-conditioned policy $\\pi_G$ to get as close as possible to a specified goal g, a process referred to as the \"Go phase.\" Subsequently, an explorer policy $\\pi_E$ is used to further explore the environment starting from the final state of the Go phase, known as the \"Explore phase.\"\nThe quality of the trajectories generated by the Go-Explore strategy largely depends on the selection of the goal g during the Go phase. Therefore, establishing an effective mechanism for selecting the Go phase goals is crucial. If the chosen goal g is too simple, the explorer will not sufficiently explore the environment. Conversely, if the goal g is too difficult, the goal-achieving policy $\\pi_G$ will fail to approach it effectively. Thus, the baselines MEGA-G and PEG-G employ different goal selection strategies to determine g, guiding the agent to areas with high exploration potential during the Go phase. MEGA-G and PEG-G enhance the agent's exploration efficiency by crafting robust exploration strategies, enabling faster learning of the world model with respect to new dynamic transitions and environmental areas."}, {"title": "GC-Dreamer", "content": "GC-Dreamer is the goal-conditioned version of Dreamer(Hafner et al. (2019a,b, 2020)), without incorporating any exploration or goal-directed strategies. It only uses a goal-conditioned policy to collect trajectories, with goals provided by the training environment."}, {"title": "PEG-G", "content": "PEG uses a world model to simulate exploration trajectories and evaluates the exploration potential(PE(g)) to identify areas worth exploring.\n$PE(g) = E_{\\pi_G(s_\\tau | s_0,g)}[V^E(s_\\tau)]$                                                                                                                                                                (8)\n$V^E(s_\\tau) = E_{\\pi_E} [\\sum_{t=\\tau+1}^{\\tau+T_E} \\gamma^{t-\\tau-1} r_t^E]$                                                                                                                                                                             (9)"}, {"title": "MEGA-G", "content": "MEGA (Pitis et al. (2020)) employs kernel density estimates (KDE) to assess state densities and selects goals with low densities from the replay buffer. For the implementation of MEGA, we adopt the model-based MEGA methodology described in the PEG paper without modifications. The PEG paper has illustrated that their adaptation of MEGA outperforms the original MEGA implementation. This entails integrating MEGA's KDE model and incorporating a goal-conditioned value function into the LEXA framework to filter goals based on reachability. Similar to PEG-G, MEGA-G switches between utilizing goals from the environment and employing the MEGA goal selection strategy."}, {"title": "MUN-noDAD", "content": "We consider the Time-sample Hindsight Waypoints Sampling Strategy from Hindsight Planner (Lai et al. (2020)) as an alternative to the subgoal selection mechanism in MUN. MUN-noDAD selects subgoals at fixed time intervals along trajectories, providing a simple and effective strategy for defining subgoals. MUN-noDAD can still benefit from the framework of MUN in navigating between different subgoals."}, {"title": "Implementation Details", "content": "Farthest Point Sampling (FPS) Algorithm"}, {"title": "Runtime", "content": "We conduct each experiment on GPU Nvidia A100 and require about 3GB of GPU memory. See table in Table 1 for specific running time of MUN for different task. Most of the runtime is consumed by the neural network updates for the policy and the world model, while the time taken by DAD to filter subgoals is minimal."}, {"title": "Hyperparameters", "content": "We use the default hyperparameters of the LEXA backbone MBRL agent (e.g., learning rate, optimizer, network architecture) and keep them consistent across all baselines. MUN primarily requires hyperparameter tuning in the following: 1) the number of candidate subgoals stored Nsubgoals; 2) the number of subgoals used for navigation when sampling in the environment Ns; and 3) the total"}, {"title": "Additional Experiments", "content": "More subgoals found by DAD"}, {"title": "Navigation Experiments", "content": "We do the extend navigation experiments on 3-Block Stacking, Ant Maze, and Walker environments to see if the MUN can learn a better world model to navigate to unconstrained goals from unconstrained start state compared to other baselines. In the 3-Block Stacking task, we use a set of 15 goals that represent various critical states in the block-stacking process. These goals serve as candidates for both initial states and endpoint goals, resulting in a total of 225 unique combinations of initial states and endpoint goals for each evaluation episode. For each combination, we conduct 10 repeated evaluations, ultimately computing the average success rate across 2250 evaluation trajectories. Our goal is to assess whether MUN can effectively achieve a random goal when the agent starts from an arbitrary state. This evaluation inherently includes both the forward and reverse processes of stacking blocks, determining whether an agent that can stack blocks is also capable of returning the stacked blocks to an intermediate state. In the Ant Maze environment, we use 32 different positions within the maze as a candidate set for starting and goal positions. Evaluating navigation between these positions allows for a comprehensive assessment of the Ant Robot's world model learning for the maze structure. This evaluation not only measures its ability to reach the final room but also its capability to return to previous rooms from intermediate positions. We evaluate 1024 combinations of starting and goal positions, conducting 10 evaluations for each combination, resulting in an average success rate computed over 10,240 experiments. In the Walker environment, we use all evaluation goals (\u00b113, \u00b116, \u00b119, \u00b122, \u00b125, \u00b128) as a candidate set for starting and goal positions. This set can form a total of 144 different combinations of starting and goal positions, providing a thorough assessment of the Walker robot's ability to move forward and backward, as well as its precision in position judgment."}, {"title": "World Model Assessment", "content": "Table 6 shows the single-step prediction error of learned world models. We randomly sample 1e4 state transition tuples (si, ai, Si+1) within the replay buffers from all of our baselines (MUN, MUN-noDAD, GC-Dreamer, MEGA-G, and PEG-G) to form a validation dataset. Table 6 reports the mean squared error on this dataset.\nTable 7 shows the compounding error (multistep prediction error) of learned world models for evaluation when generating the same length simulated trajectories. More specifically, assume a real trajectory of length h is denoted as (so, ao, S1, ..., sh). For a learned model M, we sample from 80 and generate forward rollouts (\u015do, ao, \u015c1, ..., \u015dh) where $ \\hat s_0 = s_0$ and for i \u2264 0, $ \\hat s_{i+1} = M(s0,a_i)$. Then the corresponding compounding error of M is defined as $ \\frac{1}{h} \\sum_{i=1}^h ||s_i - \\hat s_i||^2$. We set h to be the maximum number of timesteps in our environments. We evaluated the compounding prediction error of the learned world models by generating 500 trajectories for each benchmark, simulated on both the models and the real environments.\nIn Tables 6 and 7, we used the final world models trained by all methods after the same number of environment interaction steps. These results provide a quantitative comparison of the world model prediction quality between MUN and the baselines across our benchmarks. The world models trained by MUN show a much smaller generalization gap to the real environment compared to goal-conditioned Dreamer (and the other baselines). Consequently, MUN can effectively leverage these world models to train control policies that generalize well to the real environment. This explains the superior task success rates of MUN compared to the baselines in our experiment."}]}