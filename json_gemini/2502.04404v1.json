{"title": "Step Back to Leap Forward: Self-Backtracking for Boosting Reasoning of Language Models", "authors": ["Xiao-Wen Yang", "Xuan-Yi Zhu", "Wen-Da Wei", "Ding-Chu Zhang", "Jie-Jing Shao", "Zhi Zhou", "Lan-Zhe Guo", "Yu-Feng Li"], "abstract": "The integration of slow-thinking mechanisms into large language models (LLMs) offers a promising way toward achieving Level 2 AGI Reasoners, as exemplified by systems like OpenAI's o1. However, several significant challenges remain, including inefficient overthinking and an over-reliance on auxiliary reward models. We point out that these limitations stem from LLMs' inability to internalize the search process, a key component of effective reasoning. A critical step toward addressing this issue is enabling LLMS to autonomously determine when and where to backtrack, a fundamental operation in traditional search algorithms. To this end, we propose a self-backtracking mechanism that equips LLMs with the ability to backtrack during both training and inference. This mechanism not only enhances reasoning ability but also efficiency by transforming slow-thinking processes into fast-thinking through self-improvement. Empirical evaluations demonstrate that our proposal significantly enhances the reasoning capabilities of LLMs, achieving a performance gain of over 40% compared to the optimal-path supervised fine-tuning method. We believe this study introduces a novel and promising pathway for developing more advanced and robust Reasoners. The code is available at https://github.com/LAMDASZ-ML/Self-Backtracking.", "sections": [{"title": "1. Introduction", "content": "The incorporation of slow-thinking mechanisms has become a pivotal path for large language models (LLMs) to attain Level 2 AGI Reasoners 1. OpenAI has proposed five steps towards AGI: Chatbots, Reasoners, Agents, Innovators, and Organizations.\nNevertheless, do these techniques represent the ultimate paradigm for Reasoners? Our analysis uncovers several critical challenges in current approaches: 1) 01-like models frequently suffer from inefficient overthinking, even for simple problems (Chen et al., 2024b), resulting in considerable computational resource wastage; 2) Some o1-like methods (Choi et al., 2023; Zhang et al., 2024a; Qin et al., 2024) exhibit a heavy reliance on auxiliary reward models for state evaluation, which not only incurs significant inefficiencies but also risks reward hacking (Chen et al., 2024a).\nThe core issue lies in the fact that LLMs have yet to internalize the capability of search. Currently, search functions more as an external component (Gandhi et al., 2024; Zhang et al., 2024a), integrated with LLMs only at a superficial level, with limited fusion between the two. This externalized approach constrains the performance of LLMs in more complex reasoning tasks, while simultaneously inducing excessive thinking in simpler tasks. As the saying goes, \"To err is human, to backtrack divine.\u201d Backtracking (Van Beek, 2006) plays a critical role in many efficient search algorithms, particularly in reasoning problems, where most classical search algorithms employ backtracking strategies. Through backtracking, algorithms can learn from suboptimal paths, reassess, and seek optimal solutions. Therefore, we have strong reasons to believe that if LLMs could internalize the backtracking mechanism, they would potentially mitigate overthinking and reduce reliance on external reward models, paving the way toward becoming stronger reasoners.\nIn this paper, we propose a novel Self-Backtracking technique that equips language models with the ability to learn when and where to perform backtracking during the training phase. Specifically, the model is trained to recognize when its initial predictions or reasoning paths are suboptimal and to backtrack to earlier states in order to explore alternative possibilities. During the testing phase, the model leverages this learned backtracking capability to conduct dynamic search processes, systematically revisiting prior actions and exploring multiple reasoning paths. The model ultimately utilizes the improved results for expert iteration to achieve self-improvement. This process facilitates a transition from slow thinking to fast thinking, significantly enhancing the model's capability for fast reasoning in complex tasks.\nOur proposed method effectively addresses the limitations of 01-like models through the internalization of the backtrack process within the LLMs. Firstly, the model intelligently avoids unnecessary backtracking in simpler problems by learning optimal conditions for backtracking, thereby effectively mitigating the risk of overthinking. Secondly, it implicitly integrates the state evaluation mechanism within the model itself, obviating the need for external reward models. We compare ol-like models and our method in Figure 1. Experiments are conducted on the Countdown task (Gandhi et al., 2024) to evaluate our proposed method's advantages across models of different parameter scales. Our method demonstrates an accuracy enhancement exceeding 40% compared to the SFT approach that solely relies on the optimal reasoning solutions. Results show that the self-backtracking technique significantly enhances the model's reasoning flexibility and overall performance while exhibiting test-time scaling capabilities. We believe it provides the potential to make a advancement toward achieving Level 2 AGI Reasoners.\nOur contributions are summarized as follows:\n\u2022 Problem: Existing slow thinking techniques face significant challenges, including inefficient overthinking and excessive reliance on auxiliary reward models. We highlight that enabling LLMs to internalize the search process is a promising direction for enhancing LLM's reasoning capabilities.\n\u2022 Method: We introduce Self-Backtracking, a novel technique that enables LLMs to internalize the backtracking ability during both training and inference. This approach not only mitigates inefficient overthinking and reduces dependencies on external reward models but also enhances reasoning efficiency by transforming slow-thinking processes into fast-thinking capabilities through self-improvement.\n\u2022 Evaluation: Extensive experimental results on Countdown demonstrate that the proposal can significantly enhance LLMs' reasoning performances, achieving a performance gain of over 40% compared to the optimal-path SFT method."}, {"title": "2. Related Work", "content": "Learn from Search Trajectories Recently, several studies have explored using symbolic search algorithms to construct trajectory data and train transformer models to learn these search strategies, with the aim of enabling models to perform reasoning tasks. For instance, Yang et al. (2022) employs Monte Carlo Tree Search (MCTS) or BFS to construct reasoning trajectories. Searchformer (Lehnert et al., 2024) and DualFormer (Su et al., 2024) utilize traces from A* search to train language models, with each trace containing state information, A* heuristic values, and search history. Stream of Search (SoS) (Gandhi et al., 2024) constructs trajectories using various search algorithms to help language models learn the commonalities across different search strategies, facilitating the discovery of new search strategies. GSOS (Moon et al., 2024) further extends SoS by integrating optimal solutions into the process of learning search trajectories. DeepSeek R1 (Guo et al., 2025) employs end-to-end reinforcement learning to autonomously acquire the capability of search in language. However, training these model to learn exploration trajectories may conflict with guiding it to generate optimal trajectories, leading to inefficient overthinking when solving easy problems.\nLearn from Mistakes Numerous recent studies have focused on exploring whether language models possess the ability to learn from their previous mistakes and subsequently correct them. One line of techniques (An et al., 2023; Tong et al., 2024; Zhang et al., 2024b) introduces the external verifier to evaluate the reasoning paths generated by LLMs. This evaluation is then used to construct preference training data for RLHF, with training conducted"}, {"title": "3. Preliminary", "content": "3.1. Problem Setup\nWe adopt the formal definition of reasoning problems as proposed in Stream of Search (Gandhi et al., 2024), where a reasoning problem is modeled as a Markov Decision Process (MDP). An MDP is characterized by the following components: a state set $S$, representing all possible states within the problem domain; an action set $A$, denoting all permissible actions; a transition function $T : S \\times A \\rightarrow S$, which defines how states transition based on actions; and a reward function $R : S \\rightarrow \\mathbb{R}$, which assigns a numerical reward to each state. A typical reasoning task involves an initial state $s_0 \\in S$ and a goal state $s_g \\in S$, where the goal state $s_g$ is associated with a reward of 1 ($R(s_g) = 1$), while all other states yield a reward of zero ($R(s) = 0,\\forall s \\neq s_g$). The solution $P$ to the reasoning problem is represented as a trajectory\u2014a sequence of states and actions\u2014($s_0, a_0, s_1, a_1, ..., s_{g-1}, a_{g-1}, s_g$), where each successive state $s_{i+1}$ is determined by the preceding state $s_i$ and a valid action $a_i$ via $s_{i+1} = T(s_i, a_i)$. The trajectory must terminate at the goal state $s_g$.\nLet $\\Sigma$ represent an alphabet, which is a finite, non-empty set of symbols. A string is defined as a finite sequence of symbols drawn from $\\Sigma$. In this work, we focus on reasoning tasks that can be expressed purely in the form of language, where both the state set $S$ and the action set $A$ consist of strings. Each state $s \\in S$ could represent an intermediate conclusion or partial solutions and each action $a \\in A$ represents a permissible operation that can be performed on the current state to advance the reasoning process. The transition function is defined as $T(s, a) = s \\circ a$, where $\\circ$ denotes string concatenation.\n3.2. Backtracking\nBased on the formalization above, we can extend the state-action pairs into a tree, allowing the search process on the tree to be naturally integrated. Backtracking is a classic searching technique that incrementally constructs a solution by exploring various options and retracting decisions when encountering a dead end. This approach is particularly effective in scenarios that require exploring multiple possibilities to solve a reasoning problem, such as navigating a maze or solving puzzles like Sudoku. When the algorithm encounters a dead end, it backtracks to the previous decision point to explore alternative paths, continuing this process until a solution is found or all possibilities are exhausted. Backtracking forms the foundation for many well-known algorithms, including DFS, BFS and MCTS. The general backtracking algorithm is summarized in Algorithm 1.\nThis study focuses on enabling LLMs to learn backtrack-"}, {"title": "4. Self-Backtracking in Language Models", "content": "In this section, we introduce our self-backtracking technique. First, during the training phase, we design a specific optimization goal and a tailored dataset format to help the language model learn when and where to backtrack (\u00a74.1) Second, during the inference phase, we use the learned backtracking ability to create an efficient search algorithm, without the need for additional tools or reward models (\u00a74.2) Finally, through a self-improvement process, we feed the search results back into the model, further enhancing its fast-thinking performance (\u00a74.3). Figure 2 illustrates the comprehensive framework of our proposed method.\n4.1. Learn to Backtrack\nUnder the standard Supervised Fine-Tuning (SFT) framework, we typically employ a dataset $D_{op} = \\{(x_i, y_i)\\}_{i\\in[n_{op}]}$, where for reasoning tasks, $y_i$ represents the natural language reasoning path representing the optimal solution. To enable the model to backtrack at appropriate times and positions, we introduce a backtracking dataset:\n$D_{back} = \\{(x_j, prefix(y_j) \\circ a_{err} \\circ \\langle backtrack \\rangle)\\}_{j\\in[n]}$\nHere, $prefix(y_j)$ denotes the prefix of the optimal solution $y_j$, representing a partial solution; $a_{err}$ signifies an erroneous action extended from the partial solution, which cannot lead to the correct answer; and $\\langle backtrack \\rangle$ is a special token indicating that the model needs to backtrack from the current state. The final dataset is $D = D_{op} \\cup D_{back}$. For different tasks, there are various methods for constructing $D_{back}$. In our experimental setup, the questions for both $D_{op}$ and $D_{back}$ are configured to be identical. This configuration allows us to effectively model the dataset as a preference dataset, so we can compare more baselines.\nThrough this data construction, if the model learns to recognize $a_{err}$ and utilize the $\\langle backtrack \\rangle$ token for backtracking, it acquires the ability of when to backtrack, fulfilling the requirement of $valid\\_state$. Simultaneously, this dataset format implicitly contains information on where to backtrack, indicating that the model should revert to the state represented by $prefix(y_j)$. Although this design superficially appears to support only single-step backtracking, the recursive nature of the backtracking algorithm allows"}, {"title": "4.2. Inference with Backtracking", "content": "Upon learning when and where to backtrack, the backtracking algorithm (see Algorithm 1) can be integrated into the inference search process. We further propose a self-backtracking inference algorithm that consider both depth and breadth search, which primarily consists of three steps: Expansion, Backtracking, and Selection.\nExpansion. In the expansion phase, given the current state $s$, the algorithm samples $N$ predictions from the language model. These predictions are then categorized into two groups: those containing the $\\langle backtrack \\rangle$ token and those that do not. Predictions without the $\\langle backtrack \\rangle$ token are directly added to the candidate set, while those containing the token are processed further in the next phase.\nBacktracking. During the backtracking phase, the algorithm selects $\\nu N$ predictions containing the"}, {"title": "4.3. Self-Improvement", "content": "In this stage we aim to transfer the model's slow thinking abilities to fast thinking through the self-improvement method. To achieve this, we employ an expert iteration strategy, which primarily consists of three steps: First, during the slow thinking data generation phase, we utilize the self-backtracking inference model to produce high-quality reasoning path data. Subsequently, in the expert screening phase, experts evaluate the generated data to select training samples suitable for the fast thinking model. In our experiment, we quantify the model's accuracy using an evaluator. Finally, in the fast thinking model training phase, the selected high-quality data is used to train the fast thinking model by SFT. Through this iterative optimization, we get continuous enhancement in the performance of the fast thinking model. The process is shown in Algorithm 2."}, {"title": "5. Experiments", "content": "In this section, we evaluate the capability of the self-backtracking algorithm to enhance the reasoning performance of language models on the Countdown (Gandhi et al., 2024; Moon et al., 2024) task. This task requires the language model to exhibit robust reasoning abilities, also posing a substantial challenge even for humans. We also analyze that our method exhibits the test-time scaling law and possesses the ability to self-improvement, demonstrating significant advantages over other approaches.\n5.1. Experimental Setup\nDataset. We employ the Countdown task (Gandhi et al., 2024) as our principal experimental framework to rigorously evaluate the reasoning capabilities of our self-backtracking approach. This task extends the traditional 24 Game (Yang et al., 2022) by necessitating that LLMs strategically combine a provided set of input numbers using fundamental arithmetic operations\u2014addition, subtraction, multiplication, and division-to achieve a predefined target number. The complexity of the task stems from its expansive search space, which rigorously tests the models' ability in reasoning the correct path. We construct datasets focusing on problem instances with four input numbers and target values < 100. The training set consisted of 500,000 samples, balanced between optimal solutions and backtracking data. The test set was systematically partitioned into two distinct categories: one comprising seen targets paired with novel input combinations (denoted as Seen Targets), and the other incorporating entirely new targets (denoted as New Targets), consistent with the setting outlined in SoS (Gandhi et al., 2024). Each category contained 5,000 instances.\nData Construction. In the construction of optimal solutions, we first randomly generate the target value and then select four operands within a possible range. To solve the mathematical problem, we use a recursive exhaustive method to systematically explore all potential solutions. The construction of backtracking data is categorized into three types based on different types of error patterns: 1) Exploration Errors: For a given set of target numbers and operands, we employ a DFS strategy to generate search steps. If the generated steps do not match the correct solution steps, subsequent searches are terminated. 2) Computational Errors: These errors are constructed by inserting invalid mathematical equations within the solution steps. 3) Rule Violations: This type of error is created by deliberately using operands not in the predefined list of available operands in the solution steps, thereby violating the solution rules. After appending $\\langle backtrack \\rangle$ tokens immediately following the erroneous steps in all backtracking samples, the final training dataset is formed, with the respective proportions of the above error modes being 1:2:2.\nComparison Methods In the countdown task, we employ Llama3.2-1B (Dubey et al., 2024) and Llama3.2-3B (Dubey et al., 2024) as the base models. The comparative experiments primarily consist of three categories of methods: The first category involves supervised fine-tuning (SFT) using only optimal solution dataset $D_{op}$, and compares two typical sampling strategies, namely greedy search and beam search (beams=16). The second category models the data as preference data pairs and compares various RLHF algorithms,"}, {"title": "5.2. Main Results", "content": "5.2.1. SELF-BACKTRACKING BOOSTS REASONING\nIn Table 1, we present the accuracy of various methods across different models for the countdown task. Overall, the self-backtracking technique demonstrates a significant improvement over the baseline of greedy search after SFT, with enhancements of approximately 40% on Llama3.2-1B and over 30% on Llama3.2-3B. Notably, our method exhibits considerable advantages even when b = 0, i.e., without backtracking, suggesting that the $\\langle backtrack \\rangle$ token can implicitly assess the quality of the current state, effectively substituting the function of the reward model. Additionally, when b = 1, we find that performance further improves, indicating that backtracking to previous states enables the model to leverage search mechanisms to explore correct answers more effectively. The experimental results yield a strange finding: our algorithm's Llam3.2-3B underperforms compared to Llam3.2-1B, although it is larger. We observe that while the 3B model demonstrates superior computational accuracy, it always fails to achieve the target values. This phenomenon, which will be further examined in subsequent sections through the error type analysis.\nIn addition, we compared the best performance of our method (Llama3.2-1B, b = 1, N = 32) with the classical symbolic algorithm BFS (with a backtracking budget) and search-augmented methods, as shown in Table 2. The results demonstrate that our approach exhibits significant advantages over these algorithms in such reasoning task, even those already utilizing search frameworks.\n5.2.2. SELF-BACKTRACKING CAN SELF-IMPROVE\nFurther experiments demonstrate that our algorithm achieves self-improvement through expert iteration. Employing self-backtracking with configurations b = 0, N = 16 and b = 1, N = 16 on two base models and datasets"}, {"title": "5.3. Analysis", "content": "Analysis for different $b$ and $N$. We conduct experiments by varying the $b$ and $N$, and generate performance curves under different $b$ values as $N$ increases, as illustrated in Figure 4. The results demonstrate that the performance of BoN initially increases and then decreases with larger $N$, which we attribute to the reward hacking. On the contrary, our method exhibits a consistent improvement with increasing $N$, eventually stabilizing, indicating a clear test-time scaling law in breadth. Furthermore, when backtracking is permitted (b = 1), the performance improves more rapidly with $N$ and achieves a higher overall performance, underscoring the necessity of backtracking. Surprisingly, increasing $b$ does not result in a significant scaling phenomenon in depth. Our case study reveals that the diversity of outputs from secondary backtracking significantly decreases, leading to only marginal improvements compared to $b$ = 1.\nAnalysis for error types. We conduct experiments to analyze the error types for different $b$ and $N$. There are four error types: not reached target, invalid step format, incorrect result in step and unknown numbers in step. The experimen-"}, {"title": "6. Conclusion", "content": "In this study, we propose a novel Self-Backtracking technique that addresses critical limitations in current reasoning models by enabling them to internalize the search process, particularly the ability to autonomously determine when and where to backtrack. This approach not only mitigates inefficient overthinking and reduces dependencies on external reward models but also enhances reasoning efficiency by transforming slow-thinking processes into fast-thinking capabilities through self-improvement. Empirical evaluations on the Countdown task demonstrate that our method achieves a performance gain of over 40% compared to the optimal-path supervised fine-tuning baseline, highlighting its effectiveness in improving reasoning capability.\nLimitations and future work. This study has several limitations. For instance, the method has not been adapted to a broader range of general reasoning tasks, and need further scaling up. We plan to demonstrate the advantages of our method in more general reasoning tasks on larger LLMs in subsequent research.\n7. Impact Statements\nThis work advances LLMs reasoning by enabling the model to backtrack during both the training and inference phases. Our method has the potential to enhance a wide range of applications that require reliable artificial intelligence reasoning, such as mathematical problem-solving. There are many potential societal consequences of our work, none which we feel must be specifically highlighted here."}, {"title": "A. Training Details", "content": "All baseline methods are trained on a cluster with four NVIDIA A800 GPUs. Model-specific precision is applied: FP32 for Lama3.2-1B and BF16 for Lama3.2-3B. Input sequences are truncated to 128 tokens.\n\u2022 Hyper-parameters:\nLearning rate: $1 \\times 10^{-5}$\nWarmup steps: 1\nBatch size: 16\nLearning rate scheduler: Cosine\nTraining epochs: 3\n\u2022 For DPO: $\\beta = 0.5$, RPO $\\alpha = 1.0$.\nWe also demonstrate the loss curve in Figure 6 of the training process of our self-backtracking method."}, {"title": "B. Case Study", "content": "We present two illustrative examples in Figure 7, both derived from our self-backtracking approach. Our analysis reveals that the proposed model is capable of identifying novel solutions distinct from the reference solutions, thereby demonstrating the superior reasoning capabilities of our algorithm."}, {"title": "C. More Results", "content": "C.1. Self-Improvement\nWe supplement the experimental results of expert iteration on Llama3.2-3B in Figure 8.\nC.2. Analysis for Error Types\nWe demonstrate a more comprehensive analysis of error types, with results for Llama3.2-1B on Seen Targets shown in 9, results on New Targets in 10, results for Llama3.2-3B on Seen Targets shown in 11, and results on New Targets shown in 12. Through empirical observation, we identify that the Llama3.2-3B model demonstrates superior computational accuracy. However, it frequently commits errors that prevent it from reaching the target. We hypothesize that this phenomenon is attributable to the model's robust foundational capabilities, which prioritize computational precision. A potential solution to mitigate this issue could involve the incorporation of more training data."}, {"title": "C.3. Analysis for Temperatures", "content": "In previous experiments, we consistently set the temperature parameter to 0.7. To further investigate the sensitivity of our algorithm to temperature variations, we conduct additional experiments, with the results presented in Figure 13. The findings indicate that our method exhibits strong stability across different temperature settings, with only excessively low temperatures causing minor adverse effects specifically under the condition of b = 1. Consequently, we recommend using the conventional temperature value of 0.7, which proves to be a reasonable choice."}, {"title": "C.4. Analysis for Different Ratio Between Dop and Dback", "content": "We supplement the experimental results on the test split of New Targets across varying ratios between $D_{op}$ and $D_{back}$ using the Llama3.1-1B model in Table 4."}]}