{"title": "Transfer Learning for Diffusion Models", "authors": ["Yidong Ouyang", "Liyan Xie", "Hongyuan Zha", "Guang Cheng"], "abstract": "Diffusion models, a specific type of generative model, have achieved unprecedented performance in recent years and consistently produce high-quality synthetic samples. A critical prerequisite for their notable success lies in the presence of a substantial number of training samples, which can be impractical in real-world applications due to high collection costs or associated risks. Consequently, various finetuning and regularization approaches have been proposed to transfer knowledge from existing pre-trained models to specific target domains with limited data. This paper introduces the Transfer Guided Diffusion Process (TGDP), a novel approach distinct from conventional finetuning and regularization methods. We prove that the optimal diffusion model for the target domain integrates pre-trained diffusion models on the source domain with additional guidance from a domain classifier. We further extend TGDP to a conditional version for modeling the joint distribution of data and its corresponding labels, together with two additional regularization terms to enhance the model performance. We validate the effectiveness of TGDP on Gaussian mixture simulations and on real electrocardiogram (ECG) datasets.", "sections": [{"title": "Introduction", "content": "Diffusion models have achieved remarkable success in modeling data distributions and generating various types of synthetic data, such as images [13; 36; 17], videos [14], vision language [32; 33; 30], and time series [39]. However, their success heavily relies on the availability of a large number of training samples. In real-world applications, acquiring ample samples for specific tasks can be challenging due to the high costs associated with data collection or labeling, or the potential risks involved. Therefore, an important research question is how to effectively transfer knowledge from a pre-trained generative model in the source domain (using existing large-scale datasets) to a target domain (for specific tasks) where data is limited.\nTraining a generative model directly or finetuning a pre-trained generative model on limited data from the target domain often results in significant performance degradation due to overfitting and memorization. To address these issues, numerous studies have proposed methods in generative domain adaptation, including the GAN-based models [46; 45; 48; 1; 50; 28; 47; 9; 15; 43; 22; 49], diffusion-based model [25; 51; 44], etc. Specifically, approaches using diffusion models can be divided into two categories: finetuning lightweight adapters [25; 44] and finetuning with regularization [51]. Approaches involving finetuning lightweight adapters focus on adjusting only a subset of parameters in a pre-trained model. The primary challenge here is identifying which parameters to finetune. This process is typically heuristic and requires preliminary experiments to identify the most efficient parameters for adjustment. Additionally, the specific parameters to be finetuned can vary across different neural network architectures. On the other hand, the challenge in incorporating regularization during the finetuning process is the heuristic design of the regularization term, which"}, {"title": "Problem Formulation and Preliminaries", "content": ""}, {"title": "Transfer Learning Problem Setup", "content": "Let X denote the data space and Y the label space. A domain corresponds to a joint distribution over X and Y, denoted as pxy for the source domain and qxy for the target domain. The marginal distribution of data in the source and target domains are px and qx, respectively. Suppose we have access to m (labeled) samples from the source domain S = {(xi, Yi)}i=1m ~ pxy and n (labeled) samples from the target domain T = {(x, y)}i=1n ~ qXY. Typically, the source domain contains significantly more samples than the target domain, i.e., n \u00ab m. This setup reflects the common scenario where there is limited data available for specific tasks in the target domain, while abundant data is readily accessible and stored in the source domain.\nThe problem of interest is as follows. Given a pre-trained generative model pe for the data distribution px in the source domain, and a relatively small number of samples from the target domain, generative domain adaptation approaches aim to obtain a generative model that can generate"}, {"title": "Preliminaries of Diffusion Model", "content": "synthetic samples following the target data distribution qx. We will focus on diffusion generative models, given their great success in synthetic data generation. We first present the key idea of a carefully designed guidance network for the generation of x values only. Then, we extend the method to facilitate conditional generations so that we can generate paired samples with labels, (x, y), and can incorporate downstream classification tasks on the target domain."}, {"title": "Preliminaries of Diffusion Model", "content": "Diffusion models are characterized by their forward and backward processes. For illustrative purposes, we discuss the diffusion model trained on the source domain. The forward process involves perturbing the data distribution px (x) by injecting Gaussian noise, as described by the following continuous-time equation [36]:\n$dxt = f(xt, t)dt + g(t)dw, t \u2208 [0, T]$,\nwhere w is the standard Brownian motion, $f(., t) : R^d \\rightarrow R^d$ is a drift coefficient, and $g(\u00b7) : R \\rightarrow R$ is a diffusion coefficient. The marginal distribution of $x_t$ at time t is denoted as $p_t(x_t)$, and $p_0$ is the distribution of the initial value $x_0$, which equals the true data distribution $p_x(x)$. For notational simplicity and provided it does not cause further confusion, we will refer to this diffusion process as p in the following, and we define p(xt|xs), \u2200s, t, as the conditional distribution of xt given the value xs. Similarly, for initial value x following the target domain distribution, we denote the corresponding probability measure induced by the above diffusion process (1) as q.\nThen, we can reverse the forward process (1) for generation, defined as:\n$dxt = [f(xt, t) - g(t)^2\u2207x log pt(x)] dt + g(t)dw$,\nwhere w is a standard Brownian motion when time flows backwards from T to 0, and dt is an infinitesimal negative time step. The key of the backward process is to estimate the score function of each marginal distribution, \u2207 log pt(x), then the generation can be performed by discretizations of (2) [13; 36]. Score Matching [16; 40; 35] are proposed to train a neural network s\u00f8(xt, t) (parameterized by ) to estimate the score:\n$ \\phi^* = arg min_{ \\Phi} E_{t \\sim U(0,T)}E_{xt \\sim p_t(x_t)} [||S_{\\phi}(x, t) \u2013 \\nabla_{x_t} log p_t(x_t)||_2^2]$,\nwhere x(t) : [0, T] \u2192 R>o is a positive weighting function, t is uniformly sampled over [0,T]. One commonly adopted forward process is choosing an affine $f(x,t) = -{\\frac{1}{2}}\\beta(t)x$ and $g(t) = \\sqrt{ \\beta(t)}$, which yields the Gaussian transition distribution $p(x_t|x_s) = N(x_t; \\sqrt{1 - \\beta(t)}x_s, \\beta(t)\u2161)$, t > s, with \u03b2(t) : [0,T] \u2192 (0,1) as a variance schedule. This is the Variance Preserving Stochastic Differential Equation (VP SDE) that we use in the numerical Section 4.\nSeveral works on image generation [4; 5] and inverse problem [7] extends Score Matching to Conditional Score Matching, i.e.,\n$ \\phi^* = arg min_{ \\Phi} E_{t \\sim U(0,T)}E_{(x,y) \\sim p_t(x,y)} [||S_{\\phi}(x, y, t) \u2013 \\nabla_{x_t} log p_t(x_t|y)||_2^2]$,\nwhere pt(xt|y) is the conditional distribution of perturbed data x\u0165 given corresponding label y."}, {"title": "Transfer Guided Diffusion Process", "content": "In this section, we introduce the proposed Transfer Guided Diffusion Process (TGDP) that leverages a pre-trained diffusion model - trained on the source domain data - to generate data in the target"}, {"title": "Methodology Formulation", "content": "domain. The proposed approach is orthogonal to and different from the existing fine-tuning type methods. We introduce the additional guidance in Section 3.1. The methods for calculating the guidance are provided in Section 3.2. We extend our framework to the conditional diffusion model in Section 3.3 and we propose two regularization terms for enhancing the performance of our method in Section 3.4. All proofs are deferred to Appendix B."}, {"title": "Methodology Formulation", "content": "This subsection outlines the process of transferring knowledge from a diffusion generative model pre-trained using the source domain data S for generating samples that match the underlying distribution of target domain sample T. The simplest non-transfer type approach involves directly training a diffusion model on samples T from the target domain by denoising Score Matching as described by Eq (3) or Eq (4). However, since we assume only a limited amount of data is accessible on the target domain, directly learning from the target domain is unlikely to yield an effective generative model.\nSeveral studies propose to finetune the pre-trained diffusion model to alleviate the challenges caused by limited data and make use of acquired knowledge [25; 42; 52]. These methods typically design different strategies, such as adapters, to avoid finetuning all weights in a pre-trained model. However, these approaches generally use the pre-trained diffusion model from the source domain only as initial weights. Our method offers a different way for better utilization of the acquired knowledge.\nOur proposed method is inspired by the key observation detailed in the following Theorem 3.1. Intuitively, the score function \u2207xt log qt (xt) for the target domain differs from the score function \u25bdxt log pt (xt) of the source domain by a term related to the density ratio function qx/px. We refer to this differing term as a guidance term in the following Theorem.\nTheorem 3.1. Consider two diffusion models on the source and target domain, denoted as p and q, respectively. Let the forward process on the target domain be identical to that on the source domain, q(xt|x0) = P(xt|x0), and sp* (xt,t) is the score estimator in the target domain:\n$ \\phi^* = arg min_{ \\Phi} E_{t \\sim U(0,T)}E_{xt \\sim q_t(x_t)} [||S_{\\phi}(x, t) \u2013 \\nabla_{x_t} log q_t(x_t)||_2^2]$,\nthen we have\n$S_{ \\phi^*}(x_t, t) = \\nabla_{x_t} log p_t(x_t) + \\nabla_{x_t} log E_{p(x_0 \\mid x_t)}[ \\frac{q(x_0)}{p(x_0)} ].$\nBased on Eq (6), instead of solving s\u00f8* from the limited training samples on the target domain, we construct s\u00f8* by combing the pre-training score estimator and the guidance based on a binary classifier of source and target domain samples (detailed in Section 3.2). We comment on some potential advantages of this simple yet effective idea. First of all, we do not need to fine-tune the pre-trained diffusion model on the source domain, with the corresponding computation shifted to training the guidance network which is essentially a classifier. Second, the guidance network can be effectively estimated by a domain classifier using data from both the source and target domains. There is also great flexibility in constructing this guidance network due to the extensive literature on classification problems and density ratio estimation approaches. Additionally, the sample complexity for training a generative model could be much larger than a discriminative model, since the generative model needs to recover the full spectrum of target data distribution, while a domain classifier only needs to distinguish whether the sample is from the source or target distribution."}, {"title": "Learning Guidance Network", "content": ""}, {"title": "Learning Guidance Network", "content": "We calculate the guidance for the diffusion model on the target domain as defined in the second term of Eq (6) via two steps. In the first step, we estimate the density ratio q(x0)/p(x0) by training a classifier c\u025b(x) : X \u2192 [0,1] to distinguish samples from the source and target domains. We adopt the typical logistic loss as follows:\n$ \\omega^* = arg min_{ \\omega} {\\frac{1}{m} \\sum_{x_i \\sim p} log c_\\omega (x_i) - \\frac{1}{n} \\sum_{x \\sim q} log(1 - c_\\omega(x)) }$.\nThen, the density ratio q(x0)/p(x0) can be estimated as $(1 - C_{\\omega^*}(x_0))/C_{\\omega^*}(x_0)$, and it can be shown that the optimal solution to the population counterpart of Eq (7) is exactly the true likelihood ratio [38]. It is worthwhile mentioning that we may only use a subset of source domain samples to learn the classifier to alleviate the unbalanced sample sizes, and we could also adopt modern density ratio estimators to improve the accuracy [31]. After learning the density ratio q(x0)/p(x0), the second step is to calculate the expectation Ep(xo|xt)[q(x0)/p(x0)] using Monte Carlo simulation. Since it is hard to sample from q(x0|xt), we use the following equivalent formulation to get the value instead. This trick has also been used in previous work such as the Appendix H in [23].\nLemma 3.2. For a neural network $h_\\psi (x_t,t)$ parameterized by \u03c8, define the objective\n$L_{guidance}(\\psi) := E_{p(x_0,x_t)} [ h_\\psi (x_t, t)  {\\frac{q(x_0)}{p(x_0)}} ]^2$,\nthen its minimizer \u03c8* = arg min Lguidance(\u03c8) satisfies:\n$h_{\\psi^*}(x_t,t) = E_{p(x_0 \\mid x_t)} [\\frac{q(x_0)}{p(x_0)}].$\nBy Lemma 3.2, we estimate the value Ep(x0)xt) [q(x0)/p(x0)] using the guidance network hay*\nsolved by minimizing the objective function Lguidance(\u03c8), which can be approximated by easy\nsampling from the joint distribution p(x0, xt). Combine the above steps together, the estimated\nscore function for the diffusion generative model on target domain qx can be calculated as follows:\n$S_{ \\phi^*}(x_t,t) = \\nabla_{x_t} log p(x_t) + \\nabla_{x_t} log h_{\\psi^*}(x_t, t) $."}, {"title": "Extension to the Conditional Version", "content": "The approach outlined above is for generating the sample x in the target domain. In this section, we extend the idea to the conditional generation task. Such extension is essential when the label sets in the source and target domain are different since, in such cases, we usually rely on the conditional diffusion model for sampling [18; 21]. We first present the following theorem, which is an analog to Theorem 3.1 within the context of conditional score matching.\nTheorem 3.3. Assume xt and y are conditional independent given Xo in the forward process, i.e., p(xt|x0,y) = p(xt|x0), \u2200t \u2208 [0,T], and let the forward process on the target domain be identical to that on the source domain q(xt|x0) = p(xt|xo), and $* is the optimal solution for the conditional diffusion model trained on target domain q(x0, y), i.\u0435.,\n$\\phi^* = arg min_{ \\Phi} E_{t \\sim U(0,T)}E_{q_t(x,y)} [||S_{\\phi}(x_t, y, t) \u2013 \\nabla_{x_t} log q_t(x_t|y)||_2^2]$,"}, {"title": "Additional Regularizations in Practical Implementations", "content": ""}, {"title": "Additional Regularizations in Practical Implementations", "content": "In this subsection, we provide two additional regularization terms in our final objective function, to enhance the performance of the proposed scheme.\nCycle Regularization In the approaches described above, after obtaining the classifier network cw*,\ncalculation of the additional guidance \u2207xt log Ep(xo/xt) [q(x0)/p(xo)] (or \u2207xt log Ep(x0)xt,y) [q(x0, y)/p(x0, y)]\nfor conditional generation) only utilizes the data from source domain. In this section, we provide an\nenhancement in which the limited data from the target domain can also be utilized to improve the\ntraining of the guidance network hy.\nNotice that (with detailed derivation given in Appendix B.4)\n$E_{p(x_0 \\mid x_t)} [\\frac{q(x_0)}{p(x_0)}] =  {\\frac{q_t(x_t)}{p_t(x_t)}}$,\nwhere recall pt(xt) and qt(xt) are the marginal distribution at time t for source and target distributions,\nrespectively. A similar idea to Theorem 3.2 implies that we can learn the guidance network by\nsolving the following optimization problem as well:\n$ \\psi^* = arg min_{ \\psi} L_{cycle} := E_{q(x_0,x_t)}  [ h_\\psi (x_t, t)  {\\frac{q_t(x_t)}{p_t(x_t)}} ]^2$,\nMoreover, in order to estimate the density ratio for marginal distributions at time t between\nthe target and source data distribution, we train a time-dependent classifier cw(x, t) to distinguish\nsamples from source domain pt(x) and target domain qt(x) by the logistic loss as follow:\n$ \\omega^* = arg min_{ \\omega}  {\\frac{1}{m} \\sum_{x_0 \\sim p, x_t} log c_\\omega(x_t, t) - \\frac{1}{n} \\sum_{x_0 \\sim q, x_t} log(1 - c_\\omega(x_t, t)) }$,\nwhere m, n are the number of training samples in source and target domains. The density ratio\n$q_t(x_t)/p_t(x_t)$ can then be calculated by $(1 - C_{\\omega^*}(x_t, t))/(C_{\\omega^*}(x_t, t))$.\nConsistency Regularization Motivated by the fact that an optimal guidance network should\nrecover the score in the target domain, we further use score matching in the target domain as the\nConsistency Regularization Lconsistence to learn the guidance network better.\n$ \\psi^* = arg min_{ \\psi} L_{consistence} := E_t {x(t)E_{q(x_0)}E_{q(x_t \\mid x_0)} [||\\nabla_{x_t} log p(x_t|x_0) + \\nabla_{x_t} log h_\\psi (x_t, t) - \\nabla_{x_t} log q(x_t|x_0)||_2^2]}$."}, {"title": "Experiments", "content": "In this section, we present empirical evidence demonstrating the efficacy of the proposed Transfer\nGuided Diffusion Process (TGDP) on limited data from a target domain. In Section 4.1, we conduct\nproof-of-concept experiments using a Gaussian mixture model to showcase that the guidance network\nof TGDP can successfully steer the pre-trained diffusion model toward the target domain. In Section\n4.2, we illustrate the effectiveness of TGDP using a real-world electrocardiogram (ECG) dataset."}, {"title": "Simulation Results", "content": "Experimental setup We begin with a Gaussian mixture model where X = Rd and y = {\u22121,1}.\nOn both domains, the marginal distribution for label y is uniform in Y, and the conditional\ndistribution of features is x|y ~ N(y\u03bc, \u03c3\u00b2\u2161d), where \u03bc\u2208 Rd is non-zero, and Ia is the d dimensional\nidentity covariance matrix. We let \u03bc = \u03bcs on the source domain and \u03bc = \u03bc\u03c4 on the target\ndomain, with (\u03bc\u03c2)\u03a4\u03bc\u03c4 = 0. Under such case, the marginal distribution of x on the source\ndomain px is a Gaussian mixture, for convenience we denote it as 0.5N (\u03bc\u03c2, \u03c3\u00b2\u2161) + 0.5N(\u2212\u03bc\u03c2, \u03c32\u2161),\nand the marginal feature of target distribution qx is 0.5N (\u03bc\u03c4, \u03c3\u00b2\u2161) + 0.5N(\u2212\u03bc\u03c4, \u03c3\u00b2\u2161). We let\nd = 2, \u03bc\u03c2 = [0.5, 0.5], \u03bc\u03c4 = [0.5, \u22120.5], \u03c3\u00b2 = 0.1, and draw m = 10000 samples from source domain\npx, and n = 10, 100, 1000 samples from target domain qx, respectively.\nImplementation details and Baselines We adopt the default Variance Preserving (VP) SDE\nin [36] with a linear schedule, i.e., q(xt|xo) = p(xt|x0) = N (xt|atxo, \u03c3\u03b5\u0399) with at and ot being:\n$\\alpha_t = {\\frac{ \\beta_1 - \\beta_0}{4}}t ,\\sigma_t = \\sqrt{1 - \\alpha_t^2}$,"}, {"title": "ECG Data", "content": "In this section, we demonstrate the effectiveness of the proposed guidance on the benchmark of\nelectrocardiogram (ECG) data. We first provide the standard synthetic quality and diversity\nevaluation in Section 4.2.1. Then, we utilize downstream classification tasks to further evaluate the\neffectiveness of TGDP in Section 4.2.2. We follow the setup of existing benchmarks on biomedical\nsignal processing [37] that regard PTB-XL dataset [41] as the source domain and ICBEB2018 dataset\n[27] as the target domain. PTB-XL dataset contains 21,837 clinical 12-lead ECG recordings of 10\nseconds length from 18,885 unique patients. Moreover, the PTB-XL dataset is a multi-label dataset\nwith 71 different statements (label). ICBEB2018 dataset [27] comprises 6877 12-lead ECGs lasting\nbetween 6 and 60 seconds. Each ECG record is categorized into one of nine classes, which is a subset\nof labels in the PTB-XL dataset. We randomly select 10% samples as limited target distribution by\nstratified sampling preserving the overall label distribution in each fold following [41]. We include\nmore implementation details in Appendix C.3."}, {"title": "Synthetic Quality and Diversity Evaluation", "content": "Baseline method We compare TGDP with the following baseline methods to demonstrate the\neffectiveness of TGDP. 1) Learn a generative model directly (Vanilla Diffusion): The vanilla way\nis to learn a generative model directly on limited samples from the target domain. 2) Leveraging\nthe pre-trained generative model from source domain (Finetune Generator): Since the label set of\nthe target domain is a subset of that in the source domain, a preliminary solution is to utilize the\npre-trained diffusion model to generate samples with labels in the target domain."}, {"title": "TGDP for Downstream Task", "content": "In Section 4.2.1, we illustrate that TGDP is capable of generating samples that adhere to the joint\ndistribution of data and labels in the target domain and is diverse enough. In this subsection, we\nfurther investigate whether utilizing TGDP to acquire a generative model for the target domain\nyields superior performance compared to existing transfer learning pipelines.\nBaseline method First of all, we can utilize the generative model learned in Section 4.2.1 to\ngenerate sufficient samples. Incorporated with the original limited sample from the target domain,\nwe can train the classifier, which we still denoted as Vanilla Diffusion, Finetune Generator, and\nTGDP, respectively. Moreover, we have the following baseline methods. Directly train a classifier\non target domain (Vanilla Classifier): Utilizing the limited data from the target domain, a vanilla\nclassifier can be obtained. Finetune pre-trained classifier (Finetune Classifier): Instead of training a\nclassifier from scratch on the target domain, the parameters of the classifier trained on the source\ndomain are adjusted by using the limited data from the target domain."}, {"title": "Conclusion", "content": "In this work, we propose a novel framework, Transfer Guided Diffusion Process (TGDP), for\ntransferring a source-domain diffusion model to the target domain which consists of limited data.\nInstead of reducing the finetuning parameters or adding regularization for finetuning, TGDP proves\nthe optimal diffusion model on the target domain is the pre-trained diffusion model on the source\ndomain with additional guidance. TGDP outperforms existing methods on Gaussian mixture\nsimulations and electrocardiogram (ECG) data benchmarks.\nLimitations and broader impact Overall, this research presents a promising direction for\nleveraging pre-trained diffusion models to tackle new tasks. The proposed method, TGDP, has\npotential applications in a wide range of tasks where domain shift exists. A limitation of this study\nis the lack of empirical validation regarding TGDP's performance on language vision tasks, which\nwe have earmarked for future exploration. Since we propose a generic algorithm for transferring\nknowledge to new tasks, this technique could enable people to train Deepfakes for disinformation\nbetter. Our approach hinges on the efficacy of detection methods in mitigating negative societal\nconsequences."}, {"title": "More Discussion on Related Work", "content": "Finetune diffusion model on limited data Directly finetuning the pre-trained generative\nmodel on limited data from the target domain may suffer from overfitting and diversity degradation.\nMoon et. al. [25] introduce a time-aware adapter inside the attention block. Since the attention\nmodules take about 10% of parameters in the entire diffusion model, they significantly reduced the\nturning parameters and alleviated the overfitting. While in [44], the authors only finetune specific\nparameters related to bias, class embedding, normalization, and scale factor. Zhu et. al. [51] found\nout the images generated by directly finetuned diffusion models share similar features like facial\nexpressions and lack ample high-frequency details. Therefore, they introduce two regularization\nterms, pairwise similarity loss for diversity and high-frequency components loss to enhance the\nhigh-frequency feature.\nThe main drawback of finetuning the pre-trained diffusion model is the sample complexity is\nlarger compared with training a classifier since modeling the distribution is a tougher task. In\nour work, we decompose the diffusion model on the target domain as the diffusion model on the\nsource domain plus a guidance network. Since training a guidance network (essential as a classifier\ndemonstrated in section 3) requires smaller sample complexity, we believe this novel framework\nmight provide a new way for diffusion-based domain adaptation.\nText-to-image diffusion model and learning with human feedback Numerous studies\non Text-to-Image diffusion models focus on optimizing the diffusion model to align with human\npreferences and personalize its performance for specific tasks. These endeavors commonly involve\nstrategies such as text-guided zero-shot finetuning [34; 29] or finetuning diffusion model (or its\nadaptor) through reward-weighted objectives [32; 19; 11; 20; 10]. We acknowledge the significant\npotential in these approaches, given that language models inherently encapsulate rich semantic\ninformation, thereby endowing text-to-image diffusion models with zero-shot transferability. However,\nit is noteworthy that in domains lacking a substantial amount of paired data for learning semantic\nmappings, such as biomedical signal processing and electrocardiogram (ECG) data, we refrain from\nconsidering these methods as the primary benchmarks in our comparative analysis.\nNon-diffusion based approaches in generative domain adaptation Numerous works in\ngenerative domain adaptation (or few-shot generative adaptation) study how to improve the trans-\nferability of the generative model on limited data from the target domain. Since we mainly focus on\nthe diffusion model, we summarize the primary GAN-based domain adaptation there. They mainly\npropose to add different kinds of regularization to avoid model collapse [28; 50; 48; 47; 9; 15; 43] or\nfinetune subset of the parameter (adaptor) [1; 45; 22; 49]."}, {"title": "Theoretical Details for Section 3", "content": ""}, {"title": "Proof of Theorem 3.1", "content": "Proof. To prove Eq (6), we first build the connection between Score Matching on the target domain\nand Importance Weighted Denoising Score Matching on the source domain in the following Lemma.\nLemma B.1. Score Matching on the target domain is equivalent to Importance Weighted Denoising"}, {"title": "Proof of Theorem 3.1", "content": "Score Matching on the source domain, i.e.,\n$\\phi^* =argmin_{ \\Phi} E_{t \\sim U(0,T)}E_{x_t \\sim p_t(x_t)} [||S_{\\phi}(x_t, t) \u2013 \\nabla_{x_t} log q_t(x_t)||_2^2]$\n$=argmin_{ \\Phi} E_{t \\sim U(0,T)}E_{p(x_0)}E_{p(x_t \\mid x_0)} [||S_{\\phi}(x_t, t) \u2013 \\nabla_{x_t} log p(x_t|x_0)||_2^2 \\frac{q(x_0)}{p(x_0)} ].$\nProof of Lemma B.1. We first connect Score Matching objective in the target domain to Denoising\nScore Matching objective in target distribution, which is proven by [40], i.e.,\n$\\phi^* =argmin_{ \\Phi} E_{t \\sim U(0,T)}E_{x_t \\sim q_t(x_t)} [||S_{\\phi}(x_t, t) \u2013 \\nabla_{x_t} log q_t(x_t)||_2^2]$\n$=argmin_{ \\Phi} E_{t \\sim U(0,T)}E_{q(x_0)}E_{q(x_t \\mid x_0)} [||S_{\\phi}(x_t, t) \u2013 \\nabla_{x_t} log q(x_t|x_0)||_2^2] $.\nThen, we split the mean squared error of Denoising Score Matching objective on target distribution\ninto three terms as follows:\n$E_{q(x_0)}E_{q(x_t \\mid x_0)} [||S_{\\phi}(x_t, t) \u2013 \\nabla_{x_t} log q(x_t|x_0)||_2^2]$\n$=E_{q(x_0,x_t)} [||S_{\\phi}(x_t, t)||_2^2] - 2E_{q(x_0,x_t)} [ \\langle S_{\\phi}(x_t, t),  \\nabla_{x_t} log q(x_t|x_0) \\rangle] + C_1,$\nwhere $C_1 = E_{q(x_0,x_t)} [||\\nabla_{x_t} log q(x_t|x_0)||_2^2]$ is a constant independent with . We can similarly split\nthe objective function in the right-hand side (RHS) of Eq (16) as follows:\n$E_{p(x_0)}E_{p(x_t \\mid x_0)} [||S_{\\phi}(x_t, t) \u2013 \\nabla_{x_t} log p(x_t|x_0)||_2^2 \\frac{q(x_0)}{p(x_0)}]$\n$=E_{p(x_0,x_t)} [||S_{\\phi}(x_t, t)||_2^2 \\frac{q(x_0)}{p(x_0)}] - 2E_{p(x_0,x_t)} [ \\langle S_{\\phi}(x_t, t),  \\nabla_{x_t} log p(x_t|x_0) \\rangle \\frac{q(x_0)}{p(x_0)}] + C_2,$\nwhere $C_2$ is a constant independent with 4. It is easy to show that the first term in Eq (17) is equal\nto the first term in Eq (18), i.e.,\n$E_{p(x_0,x_t)} [||S_{\\phi}(x_t, t)||_2^2 \\frac{q(x_0)}{p(x_0)}]$\n$= \\int_{X_0} \\int_{X_t} p(x_0)p(x_t|x_0) ||S_{\\phi}(x_t, t)||_2^2 \\frac{q(x_0)}{p(x_0)} dx_0dx_t$\n$= \\int_{X_0} \\int_{X_t} p(x_0)q(x_t|x_0) ||S_{\\phi}(x_t, t)||_2^2 \\frac{q(x_0)}{p(x_0)} dx_0dx_t$\n$= \\int_{X_0} \\int_{X_t} q(x_0)q(x_t|x_0) ||S_{\\phi}(x_t, t)||_2^2 dx_0dx_t$\n$=E_{q(x_0,x_t)} [||S_{\\phi}(x_t, t)||_2^2],$\nwhere the equality (i) is due to $q(x_t|x_0) = p(x_t|X_0)$."}, {"title": "Proof of Theorem 3.1", "content": "Next", "equivalent": "n$E_{p(x_0,x_t)} [ \\langle S_{\\phi}(x_t, t),  \\nabla_{x_t} log p("}]}