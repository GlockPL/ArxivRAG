{"title": "Eliciting Instruction-tuned Code Language Models' Capabilities to Utilize Auxiliary Function for Code Generation", "authors": ["Seonghyeon Lee", "Suyeon Kim", "Joonwon Jang", "Heejae Chon", "Dongha Lee", "Hwanjo Yu"], "abstract": "We study the code generation behavior of instruction-tuned models built on top of code pre-trained language models when they could access an auxiliary function to implement a function. We design several ways to provide auxiliary functions to the models by adding them to the query or providing a response pre- fix to incorporate the ability to utilize auxiliary functions with the instruction-following capability. Our experimental results show the effectiveness of combining the base models' auxiliary function utilization ability with the instruction following ability. In particular, the performance of adopting our approaches with the open-sourced language models surpasses that of the recent powerful proprietary language models, i.e., gpt-40.", "sections": [{"title": "Introduction", "content": "Generating codes based on natural language re- quirements, i.e., code generation, becomes an ap- pealing application for natural language process- ing community due to the recent advance of code pre-trained language models (Singh et al., 2023; Zhou et al., 2023; Wang et al., 2023; Zhang et al., 2023). Pre-training on large-scale code corpora enables a language model to implement correct functions based on their requirements written in the docstrings. Also, tuning a code pre-trained language model to follow instructions has been re- leased due to the effectiveness of instruction-tuned language models on natural language tasks (Luo et al., 2024; Wei et al., 2023; Song et al., 2024; Lei et al., 2024).\u00b9 These instruction-tuned models boost up the code generation ability. In code generation tasks, leveraging an auxiliary function reduces the implementation difficulty of a target function compared to that of implement- ing them from scratch. The auxiliary function is a function that helps implement a target function by inspiring novel mechanism or handling compli- cated subroutines for the target function through function calls (Lee et al., 2024). Therefore, prop- erly utilizing the given auxiliary function becomes important for the instruction-tuned models. However, limited research has been conducted on providing auxiliary functions to make the instruction-tuned models utilize the auxiliary func- tions effectively. Lee et al. (2024) initially in- cluded the auxiliary function in the prompt, but it showed inferior results compared to just prompting the corresponding base pre-trained models. Also, the instruction-tuned models' ability to incorporate the code content with their natural language text has not been fully explored, except that the model providers showcase some qualitative examples in their appendix (Rozi\u00e8re et al., 2024). In this work, we comprehensively explore the instruction-tuned models' code generation behav- ior when they can access an auxiliary function. To do this, we design several prompts that are likely to elicit the ability to utilize auxiliary functions by leveraging the query-response structure employed in the instruction-tuned models. To be specific, we provide detailed information about the auxiliary function in the query and provide an incomplete codeblock to the prefix in the response to complete the remaining response. Then, we evaluate their effectiveness across several competitive instruction- tuned models. Our evaluation results show that our proposed prompts perform efficaciously on the instruction-tuned models compared to the corre- sponding base models, and even surpass gpt-40, which is purportedly known as the most powerful language model. Finally, we perform an in-depth analysis to demonstrate that incorporating auxiliary function utilization ability already encoded in their base model with instruction-following capability"}, {"title": "Methods", "content": "We design several prompts for pre-trained models and instruction-tuned models to show their abil- ity to generate code with and without auxiliary functions. To do this, we build up two different approaches to naturally fuse the information inside the auxiliary functions into the instruction format used during instruction tuning."}, {"title": "Preliminary", "content": "We briefly explain how the existing work uses instruction-tuned models to implement codes. In general, the instruction-tuned models comply with a query-response format. Within this format, the models are trained to respond to that query. There- fore, we simply write a query with the given re- quirements to induce them to implement the code. The query consists of (1) the objective statement, (2) a description of a function, and (3) a formatting guideline. The objective statement commands them to implement a function, and a description of the function is followed to explain their functionality"}, {"title": "Incorporating the function utilization ability with instruction-following ability", "content": "On top of the existing approach, we propose simple and effective ways to enhance the instruction-tuned models' ability to utilize other functions."}, {"title": "Inserting auxiliary function information in the query-side", "content": "The first approach (Figure 1, Green) is to insert the information about auxiliary function to the query. Assuming that the instruction-tuned models understand the codes in their query, we can expect that information about the auxiliary function, e.g., declaration, docstring, and their implementation, provided in the query would be comprehended by the models and lever- aged when generating their response."}, {"title": "Inserting auxiliary function definition in the response-side", "content": "Another approach (Figure 1, Purple) is to attach an informative start- ing text to the response to guide the models to naturally complete the remaining response. One similar approach that has been conducted in gen- eral tasks is to elicit language models' reasoning capability by attaching a starting text, e.g., let's think step by step, to the response (Kojima et al., 2022). Motivated by this work, we attach the in- complete codeblock that models should generate within their response. Specifically, we open a code- block with an appropriate tag and function declara- tion requested to be implemented in the query with proper import statements. Using this approach also guarantees that the codeblock is properly opened, so the task for the models becomes easier as they just properly finalize the codeblock. In addition, we insert an auxiliary function into the codeblock for the models to leverage the auxiliary function during the generation."}, {"title": "Experiments", "content": "We explore the effectiveness of our proposed strat- egy with recent competitive instruction-tuned mod- els and analyze their behavior from various angles."}, {"title": "Experimental setup", "content": "We list the recent competitive instruction-tuned models and their corresponding base models in Table 1. We adopt the Humanex- tension benchmark consist of 151 relevant function pairs specially designed for measuring the language models' ability to utilize other functions (Lee et al., 2024). We follow the widely used decoding strat- egy for generating code: 0.2 for temperature and 0.95 for top p, and generate at most 512 tokens per prompt (Ben Allal et al., 2022). We evaluate the generated implementations using functional cor- rectness by measuring the proportion of correct implementations that pass whole test cases among 20 generations, which is known as pass@1 score."}, {"title": "Measuring the base models' auxiliary function utilization capability", "content": "We measure the pre- trained base models' ability to utilize other func- tions using the Humanextension benchmark with the prompt where the detailed prompt for the base models can be found in the Appendix. In doing so, we disentangle the strength of the instruction-tuned models from our experimental results by consider- ing the improvement already observed in the base models. We compare this score to evaluate whether the proposed approaches with the instruction-tuned models could surpass this simple baseline."}, {"title": "Results", "content": "We demonstrate that our ap- proaches successfully elicit the instruction-tuned models' ability to utilize auxiliary functions. We"}, {"title": "In-depth analysis for response codeblock", "content": "We perform an in-depth analysis on the effective- ness of appending an incomplete codeblock to the response by dissecting the code-block into sev- eral components and observing the performance change as we remove them sequentially. We re- port the performance in Table 3. For CodeLlama, removing the docstring for the target function in the codeblock significantly drops the perfor- mance. We conclude that CodeLlama understands the given requirements in the query with the doc- string through the response codeblock. On the other hands, CodeGemma preserves the performance after removing the docstring to some extent. In this case, providing a function signature is much more crucial for CodeGemma. From this results, we find that the instruction-tuned models focus on different code components and we further investigate this phenomena in future work."}, {"title": "Conclusion", "content": "In this work, we study the instruction-tuned mod- els' behavior when they can access the auxiliary function. Through various prompting approaches, i.e., providing an auxiliary function in the query or response, we discover effective prompting ap- proaches that enhance the probability of implement- ing correct codes using open-sourced models and surpass the recent powerful proprietary models, i.e., gpt-40. Our further investigation identifies that providing docstring or function signature to the response code-block is the major reason to boost performance. We believe that incorporating the"}, {"title": "Limitation", "content": "There are a few limitations that have not been fully addressed in this work. Due to the limited control of the proprietary models to the user, we could not report the score of gpt-3.5-turbo and gpt-40 when appending a prefix to the response which is verified as effective in the open-sourced mod- els. Also, whether the improvement made by the proposed approaches could be transferred through fine-tuning does not explore in this work, which will be our main future work."}]}