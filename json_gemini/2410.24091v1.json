{"title": "3D-ViTac: Learning Fine-Grained Manipulation with Visuo-Tactile Sensing", "authors": ["Binghao Huang", "Yixuan Wang", "Xinyi Yang", "Yiyue Luo", "Yunzhu Li"], "abstract": "Tactile and visual perception are both crucial for humans to perform fine-grained interactions with their environment. Developing similar multi-modal sensing capabilities for robots can significantly enhance and expand their manipulation skills. This paper introduces 3D-ViTac, a multi-modal sensing and learning system designed for dexterous bimanual manipulation. Our system features tactile sensors equipped with dense sensing units, each covering an area of $3mm^2$. These sensors are low-cost and flexible, providing detailed and extensive coverage of physical contacts, effectively complementing visual information. To integrate tactile and visual data, we fuse them into a unified 3D representation space that preserves their 3D structures and spatial relationships. The multi-modal representation can then be coupled with diffusion policies for imitation learning. Through concrete hardware experiments, we demonstrate that even low-cost robots can perform precise manipulations and significantly outperform vision-only policies, particularly in safe interactions with fragile items and executing long-horizon tasks involving in-hand manipulation.", "sections": [{"title": "1 Introduction", "content": "Humans heavily rely on both visual and tactile sensing to perform everyday manipulation tasks. Consider grasping an egg or a grape: we start by visually locating the object, and then extract more information from tactile interactions with the object to determine the appropriate amount of force to apply. When eating with a spoon, our eyes estimate the global position and geometric information of the spoon, while our sense of touch provides detailed contact information during interactions with the food. Vision and touch complement each other, enhancing our interaction with the environment and significantly increasing our flexibility and robustness, especially in tasks involving large occlusions and in-hand manipulation.\nHowever, building such a multi-modal platform for robots brings two major challenges: (i) Tactile Hardware and System. Due to requirements for minimal range and viewing directions, many existing optical tactile sensors can be either overly bulky or overly rigid for fine-grained manipulation or tasks that require compliant interactions [1-3]. Many commercial tactile sensors can also be expensive due to customized manufacturing and electronic components [4, 5], while some low-cost tactile sensors are usually too sparse to convey effective information [2, 6\u20138]. (ii) Distinct Nature of Tactile and Visual Modalities. Tactile signals are typically local and physical, while visual data are more global and semantic. This disparity poses significant challenges for models to process and interpret the data effectively. Successfully fusing these distinct modalities requires careful design of the tactile sensor and the learning algorithm.\nTo address these challenges, we present 3D-ViTac, a novel multi-modal sensing and learning system for contact-rich manipulation tasks. (i) For the tactile sensing hardware system, rather than using existing optical tactile sensors [2, 5, 9, 10], we propose an alternative dense, flexible tactile sensor array that covers a larger area of the robot end-effector. Our tactile sensors, inspired by the STAG glove [11], are cost-effective, flexible, and capable of producing stable continuous signals during manipulation. As illustrated in Fig. 1, the sensor array has a resolution of 16\u00d716 on each soft finger, totaling 1024 tactile sensing units across our bimanual tactile sensing system. This dense, continuous tactile sensor array provides effective feedback, including the presence of contact, the amount of applied normal force, and local contact patterns. (ii) On the algorithm side, given the multi-modal sensory information, instead of separately inputting visual and tactile data into the policy [12], we propose a unified 3D visuo-tactile representation that fuses these two modalities for imitation learning. This representation integrates 3D visual points and 3D tactile points (calculated using robot kinematics) into a unified 3D space, which explicitly accounts for the 3D structure and spatial relationship between vision and touch. This approach enables effective imitation learning through diffusion policy [13], allowing the system to react to nuanced force changes and overcome significant visual occlusions.\nWe conducted comprehensive evaluations on four challenging real-world tasks (e.g., manipulating fragile objects like eggs and fruit, and in-hand manipulation of tools and utensils, as shown in Fig. 1). The results demonstrate that our 3D visuo-tactile representation significantly enhances the performance of contact-rich manipulations by providing more detailed contact states and local geometry or position information. We observed that tactile information is especially critical when there is heavy visual occlusion. We also conducted detailed ablation studies regarding the sensing characteristics, comparing performance across different tactile resolutions, and showed the importance of continuous tactile reading. Additionally, the inclusion of tactile feedback during the data collection process enables the operator to gather higher-quality data, making the final policy more robust."}, {"title": "2 Related Work", "content": "Bimanual Manipulation. Dual-arm robotic setups present a wealth of opportunities for broad applications [13-17]. Traditionally, approaches to bimanual manipulation were based on a classical model-based control perspective using known environmental dynamics [18\u201325]. However, these methods depend on ground truth environment models that are not only time-consuming to construct but also typically require full-state estimation, which is often difficult to obtain, particularly for objects with complex physical properties, such as deformable items. In recent years, many researchers in the robotics community have increasingly shifted their focus to learning-based methods, such as reinforcement learning [15, 26-32], and imitation learning [16, 17, 33-41]. However, most bimanual manipulation methods still primarily rely on visual inputs [13, 16, 42-46], limiting the robot's ability to achieve human-level flexibility and dexterity due to the sensing gap between humans and robots.\nTo overcome these limitations, recent works [47, 48] employ RGB images from optical tactile sensors. However, due to the minimal range of the cameras in these sensors, the robot fingers are typically very bulky and overly rigid, limiting their effectiveness in more complicated dexterous tasks. In contrast, the tactile sensor introduced in this work is based on piezoresistive materials, allowing for larger-scale, compliant coverage of flexible thin fingers.\nVisuo-Tactile Manipulation. Tactile information plays a crucial biological role [49], and the integration of vision and touch is fundamental for humans to successfully interact with their environment [50]. Vision provides a broad perspective of the environment but often lacks detailed contact information and suffers from visual occlusion, which can be effectively complemented by tactile sensing [51-62]. Integrating visual and tactile information is also crucial for robotic manipulation. Lin et al. [14] propose a visuo-tactile policy that leverages human demonstrations within a bimanual system. However, their tactile sensor is low-resolution, and their approach lacks an explicit account of the spatial relationship between vision and touch. To address this limitation, Yuan et al. [27] proposed Robot Synesthesia, which combines visual and tactile data as a single input to the policy network. However, this approach only considers low-resolution binary tactile signals, which are limited in visuo-tactile sensing capacity. In contrast, our method employs a dense continuous tactile sensing system that delivers comprehensive information about the contact area. We also provide explicit accounting of the scene structure by integrating both modalities into a unified 3D representation space, leading to a more effective policy learning process."}, {"title": "3 Visuo-Tactile Manipulation System", "content": "3.1 Sensor and Gripper Design\nFlexible Tactile Sensors. Our sensor pads consist of resistive sensing matrices that convert mechanical pressure into electrical signals. Designed with a total thickness of less than 1mm, the tactile sensor pads can be easily integrated onto various robotic manipulators, including the surfaces of robot arms. In this paper, we install the tactile sensors on a soft and adaptable fin-shaped gripper, as shown in Fig. 2(b). These flexible sensor pads bend with the soft gripper and continue to provide effective signal transmission, making the system versatile across a wide range of robotic applications.\nAs illustrated in Fig. 2(b), each finger of the manipulator is equipped with a sensor pad containing 256 sensing units (a 16 by 16 sensor array). The size, density, and spatial sensing resolution of the sensor pads can be customized; in our current design, the resolution is set at 3mm\u00b2 per sensor point. Similar to [11], the tactile sensing pads leverage a triple-layer design, where a piezoresistive layer (Velostat) is sandwiched between two sets of orthogonally aligned conductive yarns serving as electrodes. These layers are then encapsulated between two shaped Polyimide films using a high-strength adhesive (3M 468MP), ensuring robust electrical contact between the electrodes and the piezoresistive film, which is crucial for reliable signal acquisition. The sensor characteristics are repeatable across multiple devices and reliable over multiple cycles. More detailed information on tactile sensor manufacturing is available in the Appendix.\nThe resistance of the piezoresistive layer changes in response to applied pressure, enabling each sensor point to convert mechanical pressure into an electrical signal. These analog signals are captured by an Arduino Nano and transmitted to a computer via serial communication. We use a customized electrical readout circuit to acquire data at a frame rate of up to approximately 32.2 FPS. The total cost of one sensor pad and the reading board (without Arduino) is about $20. We are committed to releasing comprehensive tutorials for hardware manufacturing.\nIntegration of Flexible Tactile Sensor and Soft Gripper. We install our tactile sensors on the surface of a fully 3D-printed soft gripper made from TPU material (Fig. 2(b)). The tactile sensor pads integrate well with the flexible soft gripper. Our new gripper design offers several advantages. First, the soft nature of the gripper significantly increases the contact area between the sensors and the target objects. This not only helps stabilize the manipulation process but also ensures consistent reflection of the contacting patterns and geometry of the objects. Second, while our visuo-tactile policy provides certain levels of action compliance, the softness of the gripper adds mechanical compliance [42], enabling us to handle fragile objects more effectively."}, {"title": "3.2 Multi-Modal Sensing and Teleoperation Setup", "content": "We employ a bimanual teleoperation system using two master robots to control two puppet robots [16]. During the data collection process, we gather synchronized multimodal information at a consistent frequency of 10 Hz from various sensors, including tactile sensors, multi-view RGBD cameras (Realsense), and data on robot target actions and current joint states. Synchronization among the sensing information is critical for maintaining the temporal consistency of the multimodal dataset, allowing for accurate alignment between tactile feedback and visual data. We also implement real-time tactile information feedback by visualizing it on the screen, as shown in Fig. 2(a). This enables the human operators to assess the adequacy of contact for secure grasping, which enhances the quality of the collected data, as will be detailed in Sec. 5."}, {"title": "4 Learning Visuo-Tactile Dexterity", "content": "4.1 Problem Formulation\nIn our study, we address the challenge of learning contact-rich robot skill trajectories through imitation learning. Fig. 3 provides an overview of the integration of visuo-tactile data and the subsequent action generation processes. Specifically, we introduce a visuo-tactile policy, denoted as \u03c0 : O \u2192 A. This policy maps combined visual and tactile observations o \u2208 O to actions a \u2208 A. Our method consists of two critical parts: (1) Dense Visuo-Tactile Representation: Fig. 3(b) shows the integration of visual and tactile data within a unified coordinate system, which includes: (i) 3D Visual Point Cloud (visualized by): Captured by the camera, formatted as $P_{visual} \u2208 R^{N_{vis} \u00d74}$, including an additional empty channel to match the shape of the tactile data. (ii) 3D Tactile Point Cloud (visualized by): This tactile point cloud includes all the points of the tactile sensing units and uses the sensing value as a feature channel, formatted as $P_{tactile} \u2208 R^{N_{tac} \u00d74}$. (2) Policy Learning: Fig. 3(c) indicates the imitation learning process. Conditioned on our 3D dense visuo-tactile representation, we leverage the diffusion policy [13] to generate actions as a sequence of robot joint states."}, {"title": "4.2 Dense Visuo-Tactile Representation", "content": "In our approach, instead of separately processing tactile and visual modalities for feature extraction [14], we integrate tactile and visual data by projecting them into the same 3D space. As illustrated in Fig. 3(b), the top row demonstrates the processing of visual observations, while the bottom row depicts the processing of dense 3D tactile points using tactile signals and robot proprioception.\n3D Visual Point Cloud. We implement a series of data preprocessing procedures on the point cloud captured by the camera, denoted as $P_{visual} \u2208 R^{N_{vis} \u00d74}$. The process involves four steps: (i) Merge: We combine point clouds from multi-view depth observations to ensure comprehensive coverage of the observed environment. (ii) Crop: The point cloud is cropped to the designated work region using a manually defined bounding box. (iii) Down-Sample: To enhance the efficiency of our visual data processing, we down-sample the point cloud using Farthest Point Sampling (FPS) [63] to ensure a more uniform coverage of the 3D space (compared to uniform sampling). Here, we set $N_{vis} = 512$ to maintain a balance between geometric detail and computational efficiency. (iv) Transform: The point cloud is transformed into the robot's base frame.\n3D Tactile Point Cloud. The tactile-based point cloud, $P_{tactile} \u2208 R^{N_{tac}\u00d74}$, represents the positions and the continuous tactile readings from the tactile units in 3D space. To determine the position of each sensor, we calculate the real-time grippers' positions using forward kinematics based on the robot's joint states. We set $N_{tac} = 256 \u00d7 N_{finger}$ for the tactile point cloud, where $N_{finger}$ represents the number of robot fingers equipped with the tactile sensor pads. Each sensor pad consists of 256 tactile points. We set $N_{finger} = 2$ for the single arm task and $N_{finger} = 4$ for the bimanual task.\n3D Visuo-Tactile Points. We then integrate the two types of point clouds, $o = P_{tactile} \u222a P_{visual}$, into the same spatial coordinates, as visualized in Fig. 3(c). Each point is also assigned a one-hot encoding to indicate whether it is a visual point or a tactile point. This unified 3D visuo-tactile representation provides the policy network with a detailed and explicit accounting of the spatial relationships between tactile and visual data. This integration introduces an inductive bias that enhances the effectiveness of manipulation in contact-rich tasks, particularly those requiring a comprehensive consideration of both modalities."}, {"title": "4.3 Training Procedure", "content": "As shown in Fig. 3(c), the decision module in our method is formulated as a conditional denoising diffusion model [64]. It uses the PointNet++ [65] architecture as the backbone and is conditioned on the 3D visuo-tactile representation o to denoise random Gaussian noise into the actions a."}, {"title": "5 Experiments", "content": "5.1 Tactile Sensor Hardware Characterization\nWe evaluated the physical characteristics of our tactile sensors to better understand their performance and signal range. As depicted in Fig. 2(c), the top figure illustrates how the reading signals of our tactile sensors vary with the applied force. We selected 10 sensor units from one sensor pad, and the data from each of them were used to fit individual curves. The bottom figure in Fig. 2(c) shows the consistency of our tactile sensor pad under different loads. We divided our sensor into an 8\u00d78 grid and summed the readings from each unit. The consistency of the 8\u00d78 grid under different loads is shown using a box plot. To demonstrate the capabilities of our dense, continuous tactile sensors, we also conducted additional experiments for 6 DoF pose estimation using only 3D tactile information and models of the manipulated objects. More details can be found in the Appendix."}, {"title": "5.2 Experiment Setup for Imitation Learing", "content": "We evaluate our multi-modal sensing and learning system on four challenging real-world robotic tasks, each divided into four steps for a more fine-grained assessment of performance (Fig. 4). Tasks are categorized based on how they benefit from the incorporation of tactile signals. Below are the basic descriptions and evaluation metrics for all tasks:\n(1) Tasks Requiring Fine-Grained Force Information:\nEgg Steaming. The robot uses its right hand to open the egg tray first. Then the robot must grasp and place an egg into an egg cooker. Subsequently, the left hand is used to relocate and secure the cooker's cover over the egg. Evaluation Metrics: The task is considered successful if the egg is placed without damage and the cooker's cover is properly positioned over the egg. A handling failure occurs if the egg falls due to insufficient grip force or cracks under excessive force.\nFruit Preparation. The robot uses its left hand to grasp the plate and place it on the table. Subsequently, both robot arms collaborate to open the plastic bag. Then, the right arm grasps a grape or several grapes and places them on the plate. Evaluation Metrics: The task is considered successful if the grapes are placed on the plate without sustaining any damage.\n(2) Tasks Requiring In-Hand State Information:\nHex Key Collection. The right hand is required to grasp the Hex Key, and then the left hand needs to grasp the Hex key followed by an in-hand adjustment of the Hex Key's position using its left hand. Subsequently, the robot is required to accurately insert the Allen wrench into the hole in the box. Evaluation Metrics: A successful insertion of the Allen wrench into the hole is considered a successful operation. Additionally, any failure to properly adjust the Hex Key's position may result in the inability when inserting it into the hole.\nSandwich Serving. First, the right hand is required to grasp the serving spoon. Second, the left hand needs to hold the pot handle and then tilt the pot. The right hand should then retrieve the fried egg from the pot and serve it on the bread. Evaluation Metrics: The robot must successfully retrieve the fried egg from the pot and serve it on the bread."}, {"title": "5.3 Qualitative Analysis", "content": "Our policy integrates three modalities: vision, tactile sensing, and robot proprioception, and has proven to be effective through four challenging, long-horizon tasks. We observe three key benefits of integrating touch. (1) Tactile sensors provide critical feedback on the presence of contact and the appropriate amount of force to apply. For example, a common issue observed with the baseline during the egg-steaming task is the application of insufficient force, which often results in the egg falling or not being successfully grasped from the tray. Similarly, during the grape-handling task, a typical error occurs when the gripper attempts to grasp multiple grapes at once, applying excessive force that damages the fruit. (2) Our policy leverages detailed contact patterns provided by touch to address visual occlusions effectively. A common failure in the baseline during the Hex Key collection task is the inability to adjust the hex key's position in hand, leading to failures in subsequent insertion attempts. In the sandwich serving task, tactile feedback is crucial for understanding the in-hand states and the spatial relationship between the contact area and all tools, especially when tools may passively rotate as the spoon interacts with the pot a frequent cause of failures in baseline approaches. (3) Tactile feedback provides the confidence needed to transition between task stages. This is especially important in scenarios where the visual information is noisy or highly occluded. During our experiments on grasping grapes in a bag, the visual-only policy frequently failed, often getting stuck in the bag and unable to progress to the next stage. More details and videos about failure cases can be found in the Appendix.\nAblation Study on Varying Levels of Visual Information. Visual occlusion presents a significant challenge in manipulation tasks, especially in bimanual tasks. To determine whether tactile sensing can compensate for visual occlusion, we varied the levels of occlusion by changing the number of cameras used both in the training set and during the policy rollout. As detailed in Table 2, Cam refers"}, {"title": "6 Conclusion and Limitation", "content": "Conclusion. In this paper, we develop a multi-modal sensing and learning system for contact-rich robotic manipulation. We introduce a dense, flexible tactile sensor array that covers a larger area of thin, soft robot grippers. We also propose a unified 3D visuo-tactile representation, which explicitly accounts for the 3D structure and spatial relationship between vision and touch. We demonstrate the effectiveness of these innovations in a range of challenging manipulation tasks. We are committing to release our code and hardware setup.\nLimitations. We acknowledge that collecting real data with a multi-modal perception system is expensive, due to the data collection itself and the addition of tactile sensing modality. One missing piece of our study is the simulation of our tactile sensor, limiting our ability to introduce randomization or scale up the data. In future work, we aim to enhance the system's capabilities to better exploit advances in physical simulation, further improving the generalizability and robustness of the policy."}]}