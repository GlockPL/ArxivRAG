{"title": "Adaptive Reasoning and Acting in Medical Language Agents", "authors": ["Abhishek Dutta", "Yen-Che Hsiao"], "abstract": "This paper presents an innovative large language model (LLM) agent framework for enhancing diagnostic accuracy in simulated clinical environments using the AgentClinic benchmark. The proposed automatic correction enables doctor agents to iteratively refine their reasoning and actions following incorrect diagnoses, fostering improved decision-making over time. Experiments show that the implementation of the adaptive LLM-based doctor agents achieve correct diagnoses through dynamic interactions with simulated patients. The evaluations highlight the capacity of autonomous agents to adapt and improve in complex medical scenarios. Future enhancements will focus on refining the algorithm and expanding its applicability across a wider range of tasks and different large language models.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have emerged as powerful statistical tools capable of predicting the next word, phrase, or even entire paragraphs based on the given input [1]. The effectiveness of these models can significantly depend on the prompts they receive [2]. One notable feature of LLMs is in-context learning, allowing them to grasp new tasks from a few examples provided within the prompt during inference [3]. This leads to the practice known as prompt engineering, which involves crafting and refining input prompts to elicit the desired responses from these models [4].\n\nThe application of large language models (LLMs) in healthcare has demonstrated significant potential, with models achieving remarkable results on tasks such as the GPT-4 [5] achieves the average accuracy of around 83.15 from the United States Medical Licensing Examination (USMLE) self assessment dataset in [6]. However, in real-world clinical practice, diagnosis is a dynamic process involving continuous patient interaction, ordering of medical tests, and decision-making under uncertainty. Simulated clinical environments offer a valuable way to evaluate these models in more interactive, adaptive settings that reflect the realities of patient care.\n\nIn this paper, we leverage AgentClinic [7], a multimodal benchmark designed to simulate clinical environments, to assess the performance of LLM agents in diagnosing patients through iterative doctor-patient dialogue, medical test interpretation, and bias management. AgentClinic [7] features four agents: the Doctor Agent, responsible for gathering information and making diagnoses; the Patient Agent, which simulates real-world patient interactions; the Measurement Agent, which provides test results; and the Moderator Agent, which evaluates the accuracy of the diagnosis. This"}, {"title": "Simulated clinical environment", "content": "The AgentClinic benchmark [7] is a simulated clinical environment designed to evaluate the per-formance of AI models, particularly large language models (LLMs), in tasks that require real-time decision-making and patient interaction, mimicking the complexities of clinical settings. Unlike traditional static medical question-answering tests, this benchmark incorporates a more dynamic and interactive approach by simulating dialogues between patient and doctor agents, along with medical exams and tests, through multimodal agents.\n\nIn AgentClinic [7], four main agents simulate the clinical environment: (1) Doctor Agent: The model being evaluated for its diagnostic abilities. This agent begins with minimal context about a patient's condition and must interact with the patient agent to gather relevant information. It can ask a limited number of questions, request specific medical tests via the measurement agent, and ultimately provide a diagnosis. This setup simulates the process of sequential medical decision-making, requiring the doctor agent to operate under realistic clinical constraints, such as finite time and limited diagnostic resources. (2) Patient Agent: The patient agent holds information about symptoms, medical history, and lifestyle but does not know the final diagnosis. Its role is to provide responses that emulate real patient behavior during doctor-patient consultations. The patient agent can exhibit cognitive and implicit biases, affecting its interaction with the doctor agent. These biases emulate real-world patient biases, such as self-diagnosis based on internet research or distrust of the doctor based on implicit factors. (3) Measurement Agent: This agent simulates diagnostic tests, providing realistic medical readings based on the patient's condition. For example, it can deliver results from an electrocardiogram, blood pressure readings, or imaging tests like X-rays. The doctor agent can request specific tests, and the measurement agent responds with results that match the patient's simulated condition, contributing to the decision-making process. (4) Moderator Agent: This agent evaluates the doctor agent's performance, determining whether the correct diagnosis has been made based on the information gathered during the interaction. The moderator ensures the dialogue is parsed correctly and compares the diagnosis with the actual medical condition to assess the accuracy of the doctor agent.\n\nAgentClinic [7] also includes biases in the behavior of both patient and doctor agents, allowing researchers to study the impact of cognitive and implicit biases on medical decision-making. The benchmark introduces various patient types, with 107 patient agents having unique family histories, age groups, diseases, and lifestyle habits."}, {"title": "Autonomous Agent Architecture", "content": "Let a simulated clinical environment be denoted as a function \\(f\\) that maps a state \\(s \\in V\\) and an action \\(a \\in V\\) to an observation \\(o \\in V\\), where \\(V\\) is a set of vocabulary. Let \\(\\pi_{\\theta}\\) be an LLM agent over a"}, {"title": "Experimental results", "content": "We conducted experiments on the MedQA dataset from [7], utilizing 15 scenarios with a maximum of 20 inferences, without bias or image requests, employing GPT-4 [5] as the patient, measurement, and moderator language agent. In the first experiment, we used both GPT-4 [5] and GPT-3.5 [8] as the doctor language agent policy \\(\\pi_{\\theta}\\) to compare the diagnostic results from these two different models. In each step of the sequence of play, the doctor agent, based on the given context state \\(s_0\\), takes action \\(a_0\\) which can be either to consult the patient agent or invoke the measurement agent, whose replies become the observation \\(o_1\\). Now based on this added context, the doctor agent takes the next action \\(a_1\\) and the cycle continues till the doctor makes the diagnosis or fails, which is the return \\(R(\\tau)\\). This medical self-adaptive language agent is presented in Figure 2.\n\nTherefore, per our algorithm, a correction or adaptation \\(\\tau_0\\) is added to the initial context \\(s_0\\) as an exemplar and the cycle is repeated. As shown in Figure 5, by adding the reflection, \"If the patient has symptoms such as double vision, difficulty climbing stairs, and upper limb weakness, perform an Acetylcholine Receptor Antibody Test instead of an MRI of the brain and spine.\", to the system prompt of the doctor agent, the \\(\\pi_{gpt-3.5}\\) doctor agent can correctly diagnose Myasthenia Gravis from the patient with 1 test and 12 questions, which is less than the \\(gpt-4\\) doctor agent in Figure 3 with 1 test and 19 questions."}, {"title": "Conclusion", "content": "In this paper, we have explored the capabilities of large language model (LLM) agents in a simulated clinical environment through the MedQA simulated clinical environment in AgentClinic [7]. By leveraging the power of in-context learning together with reason/act and observe, we introduced an automatic correction mechanism for doctor agents, enabling them to enhance their diagnostic accuracy after initial failures. Our experiments demonstrated that this framework can help the LLM doctor agent to achieve correct diagnoses over time, even in the face of complex patient interactions and decision-making scenarios.\n\nThe results from our evaluations highlight the significant potential of autonomous agents in healthcare settings, particularly in mimicking the dynamic nature of clinical practice. As we advance the field of"}], "equations": ["\\alpha\u03bf ~ \u03c0\u03bf(\u03b1\u03cc\u03c2\u03cc),", "o\u2081 = f(so, ab).", "s1 = {so, ad, o1}.", "s = {R(T)},", "\u03c4\u03bf ~ \u03c0\u03bf(ts),", "\u03b1\u03be ~ \u03c0\u03bf(\u03b1\u03c2, \u03c4\u03bf)."]}