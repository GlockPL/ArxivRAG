{"title": "Knowledge Graph Modeling-Driven Large Language Model Operating System (LLM OS) for Task Automation in Process Engineering Problem-Solving", "authors": ["Sakhinana Sagar Srinivas", "Vijay Sri Vaikunth", "Venkataramana Runkana"], "abstract": "We present the Process Engineering Operations Assistant (PEOA), an AI-driven framework designed to solve complex problems in the chemical and process industries. The framework employs a modular architecture orchestrated by a meta-agent, which serves as the central coordinator, managing an action generator and instruction-tuned small-scale language models (expert models). The action generator decomposes complex problems into sub-tasks and identifies suitable expert models to execute each, delivering precise solutions for multi-step problem-solving. Key techniques include advanced knowledge modeling using property graphs for improved information retrieval, facilitating more accurate and contextually relevant solutions. Additionally, the framework utilizes a teacher-student transfer-learning approach with GPT-4 (Omni) to fine-tune the action generator and expert models for domain adaptation, alongside an iterative problem-solving mechanism with sophisticated error handling. Custom datasets were developed to evaluate the framework against leading proprietary language models on various engineering tasks. The results demonstrate the framework's effectiveness in automating calculations, accelerating prototyping, and providing AI-augmented decision support for industrial processes, marking a significant advancement in process engineering capabilities.", "sections": [{"title": "Introduction", "content": "In recent years, significant advancements have been made in retrieval-augmented generation techniques (RAG), which combine the capabilities of large language models (LLMs) with external knowledge sources to enhance information retrieval and question-answering tasks. However, while traditional RAG techniques excel at localized information retrieval, they struggle with global questions requiring a holistic understanding of knowledge bases. Recently, there has been a surge of interest in the Graph RAG approach (Sci-PhiAI 2024b; Edge et al. 2024; Hu et al. 2024), which integrates the strengths of property graph-based knowledge modeling from unstructured data and graph-based indexing with the retrieval and generation capabilities of LLMs. By leveraging these strengths, the Graph RAG approach aims to overcome the limitations of traditional RAG techniques. The combined market capitalization of the Oil and Gas, Semiconductor, Fast-moving consumer goods (FMCG), Pharmaceuticals, Automobile, Aviation, and Energy sectors amounts to approximately $20 trillion USD. These industrial/manufacturing sectors involve complex chemical and process engineering challenges from a broader perspective. By enhancing problem-solving capabilities in these major industries with Graph RAG approaches for both process knowledge graph modeling and retrieval for question-answering (Q&A) tasks, we have the potential to contribute to economic growth, technological advancement, and improved competitiveness on a global scale. The rapidly evolving landscape of chemical and process engineering presents numerous complex challenges that necessitate innovative solutions for design, optimization, and troubleshooting. To address these challenges, we present the Process Engineering Operations Assistant (PEOA) framework\u2014a modular, AI-driven Large Language Model Operating System (LLM OS) designed to tackle intricate problems in the chemical and process industry by automating key steps in the problem-solving process. The framework architecture revolves around a central orchestrator, or meta-agent, which coordinates the framework's various components. The meta-agent works in tandem with an action generator, which plays a key role in breaking down complex problems into manageable sub-tasks and identifying the most appropriate tools (or expert models) for each step in solving the sub-tasks. To execute these sub-tasks with high precision, the action generator employs a collection of expert models, each specialized in different aspects. In essence, the framework utilizes a two-stage pipeline that iteratively decomposes complex problems into manageable sub-tasks, selects and chains together suitable tools, and executes solutions. The (subject-matter) expert models include small-scale language models (SLMs) for code generation, mathematical reasoning, and structured information retrieval from property graphs, enabling the framework to leverage external knowledge and solve diverse problems by decomposing, executing, and refining multi-step problem-solving trajectories. The framework incorporates an advanced error-handling mechanism. When a runtime error occurs, it uses a reflection procedure to identify the faulty step and associated tool (expert model). An expert model then generates a revised solution, considering both the immediate error and the broader problem context. This procedure iterates until a successful solution is achieved or a predefined limit is reached. The debugging mechanism functions in two phases: error identification and solution revision. It allows the framework to dynamically adapt its problem-solving strategy, refine solutions iteratively, and tackle increasingly complex tasks that may require multiple rounds of adjustment, thereby improving its robustness and effectiveness in real-world engineering scenarios. The proposed framework addresses the limitations of current language models and problem-solving approaches in the industry, which are hindered by a lack of domain-specific knowledge and expertise, an inability to integrate diverse tools and data sources, and a limited capacity for complex, multi-step reasoning. These limitations result in inefficient and time-consuming problem-solving workflows that impede innovation and progress in the chemical and process industry. The proposed framework serves as a decision support tool, enabling process engineers to focus on high-level decision-making and innovation, accelerate design cycles through rapid prototyping and testing, and optimize chemical processes to enhance yield, efficiency, and safety."}, {"title": "Proposed Method", "content": "We aim to address the complex challenges faced by chemical and process engineers in designing, optimizing, and troubleshooting industrial processes. To this end, we are developing the Process Engineering Operations Assistant (PEOA), a task automation framework conceptualized as a Large Language Model Operating System (LLM OS). This modular framework combines AI-driven capabilities with computational tools to streamline problem-solving in chemical and process engineering. At its core, the PEOA framework leverages the LLM OS to manage and orchestrate foundational language models, automating key steps in the problem-solving process. The objective is to allow process engineers to focus on high-level decisions and innovation while accelerating design cycles through faster prototyping and testing and optimizing chemical processes to identify optimal conditions that maximize yield, efficiency, and safety. Small-scale language models for code (SLMs) such as Google Code Gemma (Google 2024) and Meta Code Llama (MetaAI 2023) often lack extensive pre-trained knowledge related to domain-specific tasks, such as specialized mathematical reasoning and problem-solving skills for the chemical and process industry, compared to proprietary large-scale models (LLMs) like GPT-4 (Omni) (Achiam et al. 2023). Additionally, SLMs are not designed to effectively incorporate and utilize external knowledge from various domain-specific tools (e.g., vector similarity search on knowledge graph databases of code repositories/documentation, or retrieval-augmented generation with Stack Overflow APIs) for more accurate and efficient problem-solving beyond their pre-trained knowledge (Zhang et al. 2024a). These limitations hinder the performance of SLMs in specialized domains. Instruction-tuning SLMs to access external information offers a promising solution, improving their use of relevant background knowledge for more accurate outputs. We utilize Instruction Tuning with Graph Retrieval-Augmented Code Generation (GRACG), allowing the proposed framework utilizing SLMs combined with the ReAct (Reason + Act) (Yao et al. 2022) prompting technique, to generate 'solution trajectories' structured, step-by-step problem-solving sequences that break down complex tasks, integrate various tools, and produce coherent solutions. Unlike traditional RAG, which relies on linear text retrieval, GRACG techniques utilize knowledge graph databases that preserve graph topology. Graph-based representation of relationships and hierarchies between entities and concepts is more effective than flat text, providing richer contextual information, enhancing multi-hop reasoning, and reducing hallucinations. The solution trajectory consists of multiple steps, executed sequentially to systematically and incrementally solve complex chemical and process industry problems. Each step in a trajectory includes a high-level step description (a subtask), a specific tool to use from a predefined set, and the tool-executed output reformulated in natural language. While this approach shows promise, a significant challenge remains in its implementation. There isn't a pre-existing tool-integrated problem-solving solution trajectory (curated dataset) relevant to the domain that illustrates the comprehensive process of solving complex, multi-step reasoning tasks step-by-step through the integration of various tools. Such a trajectory would provide a structured approach for instruction-tuning SLMs by enhancing domain-specific knowledge and computational tool usage to generate code for solving process engineering calculations. To overcome this limitation, we utilize a teacher-student learning paradigm (Kim et al. 2024) for adapting SLMs to domain-specific tasks with similar performance to proprietary LLMs. It involves a foundational LLM, such as GPT-4 (Omni), serving as a robust teacher (subject-matter expert) to generate high-quality, tool-integrated solution trajectories that serve as synthetic, instruction-tuning datasets demonstrating effective problem-solving strategies. The machine-generated datasets are used to develop a robust and customizable student model-PEOA-for solving process engineering calculations. The teacher model is prompted with few-shot examples to generate step-by-step solutions that involve calling specific tools to solve domain-specific tasks. Each solution trajectory consists of a sequence of steps (i.e., subtasks to perform), with corresponding tools to be used at each step, and outputs (i.e., the result of executing the tool on the given step) reformulated into natural language. Our method efficiently transfers knowledge from the large teacher model by distilling its advanced mathematical reasoning and problem-solving capabilities to a smaller student model. The student model learns effective strategies for performing complex multi-step reasoning, breaking down complex tasks into smaller, more manageable steps, integrating diverse tools, and producing coherent step-by-step solutions (tool-specific outputs). Tools are specialized components or services that enhance the capabilities of language models, enabling them to handle complex and diverse tasks. These include code generators for creating executable snippets, math problem solvers for mathematical reasoning, and vector-search retrieval on knowledge graph databases for structured information access. By integrating these tools, language models can expand their problem-solving abilities. Integrating external tools with automatic tool chaining (Shi et al. 2024) allows SLMs to execute tasks beyond their pre-trained knowledge, augmenting their problem-solving abilities. Tool learning involves four stages: it begins with task planning, where the SLM analyzes a user's query and decomposes it into sub-tasks with tuning-free methods, like few-shot prompting with ReACT techniques. Next, in tool selection, the SLM identifies the most appropriate tools for each sub-task. During tool calling, the SLM extracts and formats the necessary parameters from the user's query to invoke the selected tools. Finally, in response generation, the SLM synthesizes the tool outputs with its own pre-trained knowledge to provide a comprehensive and coherent response. Tool learning can follow two paradigms: one-step task solving, where SLMs plan sub-tasks upfront and generate responses without adjusting for errors, and iterative task solving, where SLMs interact with tools iteratively, correcting tool outputs based on feedback. In this work, we use iterative task solving to enable SLMs to handle complex queries more effectively by leveraging external tool chaining. Given a natural language query Q, we begin by decomposing it into smaller, manageable sub-tasks. Let S = {81, 82, ..., Sn} be the set of sub-tasks derived from Q. The aim is to enable the proposed framework to use a sequence of tools from the set T = {t1, t2,...,t|7|} to solve the task. For each sub-task Si, the most appropriate tool ti is selected from the set of tools T. The framework first determines if tool usage is necessary to solve the sub-task. If so, the program chains them together to complete the task. When tools are not required, the framework relies on its internal pre-trained knowledge to solve the task. The tool protocols provide meta-information to understand each tool's purpose and usage. The tool protocols D = {d1, d2, ...,d|D|} consist of documented protocols di corresponding to each tool ti \u2208 T. Each protocol di E D offers detailed information about its associated tool ti, including an overview of functionality and use cases, argument requirements specifying necessary inputs, and a response schema outlining expected output structure and type. The detailed tool protocols allow the framework to learn tool usage, understand the input-output schema and capabilities of various tools, and manage data flow dependencies, enabling it to chain together and utilize multiple tools to solve the end-user task. Tool learning is a crucial component of the proposed framework, supporting its core objective of streamlining workflows and automating complex problem-solving tasks in process engineering. The PEOA framework consists of a meta (top-level) agent orchestrating a specialized action generator (A) and expert models (tools) (Mt) modeled by SLMs. The meta agent delegates the input question and the solution history to the action generator to predict the next high-level sub-task and select the appropriate tool needed to solve the sub-task, which expert models then execute precisely, updating the solution state. The framework iterates over a two-stage pipeline to solve multi-step reasoning tasks using various expert models as tools, combining the generation of sub-tasks and tool selection followed by invoking the specialized expert models to efficiently address complex problems. Tool-integrated solution trajectories generated by the teacher model fine-tune the action generator and expert models. The framework employs a diverse set of expert models to execute actions based on the tool chosen by the action generator. These models include Mc (DeepSeek-Coder-7B-Instruct (Guo et al. 2024)) for generating executable code snippets, and Mm, a RAG technique variant that combines the mathematical reasoning of DeepSeek-Math-7B-Instruct (Shao et al. 2024) with the computational power of Wolfram Alpha's (Hindin 2010) API for advanced problem-solving. Additionally, Mq (Meta Llama or Google Gemma) is used for crafting search queries, translating sub-tasks into understandable formats to retrieve information from web search engines like Duck-DuckGo or Stack Overflow APIs, and parsing their outputs. Lastly, MKQ (Meta Llama or Google Gemma), a graph-RAG variant, is employed for conducting structured information retrieval through similarity searches from knowledge graph databases of scholarly sources such as numerical libraries/code documentation. The action generator (A) is realized with Meta Llama or Google Gemma. Finally, the top-level agent integrates the results, potentially with its own knowledge, to craft a coherent, human-friendly response that provides context, explanations, and insights, allowing the framework to tackle a wide range of complex tasks by leveraging specialized tools as needed. The action generator A takes the task instruction x and the concatenated solution history hi-1 up to the previous step and predicts the next step si and the associated tool ti to solve the sub-task, described as follows:\n$A(Ia, x, h_{i-1}) = A(Ia, x, [S_1 || O_1 || ... || S_{i-1} || O_{i-1}])\\rightarrow [t_i, S_i]$\nwhere hi-1 is the solution history up to step i - 1 and Ia indicates a concise instruction prompt provided to the action generator to predict the next step si and the tool ti. The step si is the high-level description of the action to be taken at each stage in the tool-integrated solution trajectory. The expert model $M_{t_i}$ associated with the tool ti generates the output oi for the step si as follows:\n$M_{t_i}(I_m, x, h_{i-1}, S_i) \\rightarrow O_i$\nwhere $M_{t_i}$ is the expert model corresponding to the tool ti, oi is the output of the current step, and $h_i = h_{i-1} U (S_i, O_i)$ is the updated solution history including si and oi. Im serves as a concise instruction prompt provided to the expert model to generate the output for a given step in the solution trajectory. The output oi is generated by executing the tool's action. For example, a code snippet ci generated by Mc is executed by a code interpreter to produce oi. The iterative process continues until the action generator A identifies the final answer to x in the solution history h. A and Mt are trained on tool-integrated solution trajectories generated by a teacher LM (GPT-4 (Omni)). At inference time, the proposed framework uses A to predict steps (sub-tasks) and tools, and Mt to execute these steps until it finds the final answer. \n The framework employs a sophisticated error-handling (code debugging) (Gou et al. 2023) and adaptive problem-solving mechanism, utilizing a dynamic interplay between an action generator A and specialized expert models Mt, which work in tandem to decompose, execute, and refine multi-step problem-solving trajectories. When encountering a runtime error, the action generator A employs a reflection mechanism to identify both the faulty step s and the associated tool t as follows:\n$[s^\\u2020, t^\\u2020] = A(I_e, x, h_{i-1}, o^\\u2020)$\nwhere Ie represents the error identification instruction, x denotes the original task, $h_{i-1}$ is the cumulative solution history up to the previous step, and $o^\\u2020$ is the faulty output that triggered the error. This error localization process leverages the system's understanding of tool protocols, input-output schemas, and the interdependencies between various computational tools and steps in the problem-solving sequence. Once the error is localized, the corresponding expert model $M_{t^\\u2020}$ generates a revised step and output prediction as follows:\n$O_i = M_{t^\\u2020}(I_r, h_{i-1}, s,o^\\u2020)$\nwhere Ir is a specially crafted revision instruction. This revision process not only corrects the immediate error but also considers the broader context of the problem, ensuring that the revised step aligns with the overall solution strategy. This error-correction and refinement process iterates until successful execution is achieved or a predefined iteration limit is reached. With each iteration, the solution history is updated, creating a comprehensive record of the problem-solving trajectory, including both successful steps and addressed challenges. This iterative approach enables the framework to tackle increasingly complex tasks that may require multiple rounds of refinement. Combined with the proposed framework's ability to chain multiple tools and parse their outputs, this approach significantly enhances its problem-solving capabilities. The framework's ability to dynamically generate, execute, and refine both individual steps and overarching action sequences is particularly noteworthy. In summary, the proposed framework, PEOA, is a novel approach for automating complex problem-solving in process engineering, enabling it to accelerate design cycles, optimize chemical processes, and support high-level decision-making. It operates in two intertwined phases analogous to localization and repair: error identification and solution revision. In the error identification phase, the framework leverages a reflection mechanism within its action generator (A) to analyze runtime errors (off) and pinpoint faulty steps (s) and tools (t) within the tool-integrated solution trajectory. During solution revision, the corresponding expert model (Mt), guided by a revision instruction (Ir), proposes a revised output (0\u2082), considering the error and solution history (hi-1). This revised solution is integrated into the solution trajectory, and the process iterates until a satisfactory solution is achieved, enabling the framework to dynamically adapt its problem-solving strategy for complex calculations."}, {"title": "Knowledge Modeling : Document Parsing/Indexing for Graph-Based Semantic Search and Retrieval", "content": "For graph-retrieval augmented code generation (GRACG), we perform document parsing (LlamaIndex 2023b) to extract structured information from unstructured PDFs. This involves reading the PDF, analyzing its structure, and extracting content like text, images, tables, and code. We store and query this information using a production-grade graph database, such as Neo4j, which supports property graphs and vector searches. The graph database organizes parsed document elements and their metadata into nodes, relationships, and properties, preserving contextual relationships and enabling efficient, context-aware retrieval. Text chunking divides large texts into smaller, manageable segments (chunks) to preserve context, improve processing efficiency, and enhance document-specific KG search engines indexing and retrieval. We use a sliding window technique, moving a fixed-size window across the text with a predefined stride, ensuring overlapping chunks that maintain contextual continuity. Text segments are stored as chunk nodes with metadata, including title, page numbers, summaries, and keywords. Text embedding models generate dense semantic vector representations of text segments, stored as additional metadata to enable semantic search and context-aware vector retrieval. Parsed text segments, stored as chunk nodes, are processed by an LLM like GPT-4 (Omni), which infers and generates knowledge graph triples by identifying entities and relationships. It outputs single-hop paths in the format (subject(entity) relation object(entity)). This approach dynamically constructs an ontology-a formal representation of domain concepts (e.g., entities, attributes, and categories) and their relationships (e.g., associations and hierarchies)-while developing a schema that defines the database structure. Entity nodes represent specific concepts or objects in text chunks. Entity nodes link to related chunk nodes via 'MENTIONS' relationships. In summary, each text chunk in the property graph store has two node types: chunk nodes and entity nodes, capturing various attributes and metadata associated with the text segment. The knowledge graph serves as both an ontology and a schema, providing a flexible, semantically rich framework for organizing and querying the extracted knowledge. Each table is represented as a node with metadata properties such as table ID, title, source page, and summary description. We use text embedding techniques to create vector representations of the table content, facilitating efficient similarity searches. Each row in a table is represented as a row node with properties corresponding to the column values. Relationships between each row node and its respective table node use a relationship type such as 'BELONGS', facilitating efficient querying. Similarly, for images, we store metadata related to scholarly image data as image nodes with properties such as page number, resolution, format, and summary descriptions generated by LLMs like GPT-4 (Omni). These descriptions provide high-level scene interpretation and content analysis. We also use CLIP embeddings to convert images into low-dimensional embeddings that capture the semantic content of the images. These vector representations are stored as node metadata properties, enabling efficient similarity searches and facilitating the retrieval of semantically similar images from a local file system. Each image node is connected to its top-K visually similar nodes through visually similar relationships, enabling the retrieval of visually similar images. In summary, each image in the property graph store is represented by a single node that stores metadata and semantic content representations (generated by CLIP embeddings) and is connected to other nodes through visual similarity relationships. We use a code hierarchy parser (LlamaIndex 2023a) to break down long code files from Github repositories into manageable segments by creating a hierarchical structure. This process, called skeletonization (e.g., using abstract syntax trees), replaces code blocks with comments that reference specific nodes for detailed context. The parser organizes code into nodes based on scope (e.g., functions, methods, classes, modules) and links these nodes to their parent and child nodes, enhancing readability and accelerating KG vector retrieval. The parser also handles comments, import statements, and variable declarations. For metadata extraction, we gather information on project structure, dependencies, and version information. Finally, we perform entity de-duplication by addressing duplicate entities in KGs. This involves identifying similar nodes using cosine similarity and Levenshtein distance, merging overlapping groups of similar nodes, filtering subsets to retain comprehensive node groups, and ultimately merging nodes within each group to discard redundancies and preserve the most descriptive identifiers. Entity de-duplication merges duplicates to maintain graph accuracy, reduce noise, and ensure searches and analyses are performed on unique data. Graph retrieval involves selecting the top-k entity nodes based on vector similarity to the user query, traversing to retrieve adjacent triples (one-hop neighbors) and corresponding parent nodes. In summary, we transform unstructured data into structured, searchable knowledge, covering the workflow from parsing PDFs to constructing and querying knowledge graphs. These graphs extract, organize, and utilize information from complex documents to assist with code generation tasks. This approach emphasizes LLMs (such as GPT-4 (Omni)) for dynamic ontology creation, graph databases for semantic searches, and context preservation for enhanced performance. The expert model (Google Gemma or Meta Llama) interprets the user's query, integrates retrieved information from the knowledge graph, finds relevant information from a structured knowledge base with its pre-existing knowledge, and generates a coherent, contextually appropriate response. This combination leverages the expert model's language understanding and generation capabilities while grounding its outputs in external, structured knowledge, resulting in more accurate and informative answers."}, {"title": "Experiments", "content": "Benchmark Datasets: We developed two custom benchmark datasets to train and evaluate our framework for solving complex chemical and process engineering problems: the mathematical and computational tuning (MathComp) dataset and the chemical process tuning (ChemProc) dataset. The MathComp dataset contains over 8,500 question-answer pairs, focusing on mathematical modeling and numerical algorithms. It is designed to customize the framework for using computational tools for tasks such as solving differential equations, linear algebra, optimization, and related mathematical tasks. The ChemProc dataset includes over 7,000 question-answer pairs, covering topics specific to chemical engineering such as mass and energy balances, thermodynamics, heat transfer, reaction kinetics, fluid mechanics, separation processes, and process control. These high-quality datasets were essential for adapting the framework to handle specialized engineering problems by providing domain-specific knowledge and enabling it to leverage computational tools. We compiled these datasets from publicly available scholarly sources, including textbooks ranging from basic to advanced levels, ensuring a comprehensive and diverse collection of problems and solutions. The datasets were divided into training (70%), validation (15%), and test (15%) sets to facilitate rigorous evaluation. In summary, these diverse datasets provide the domain-specific knowledge, computational problem-solving skills, and rigorous evaluation framework absent in existing, more general datasets. MathComp focuses on mathematical modeling and numerical algorithms, while ChemProc covers core chemical and process engineering principles.\nExperimental Settings: In our experimental setup, we leveraged the custom MathComp and ChemProc datasets to train and evaluate the proposed framework. A key innovation in our approach was the implementation of a sophisticated knowledge modeling technique using property graphs. We developed a custom document parsing pipeline to extract structured information from complex, unstructured PDFs of scholarly articles. This process involved analyzing document structure, extracting various content types, and retrieving metadata. To store and query this information effectively, we utilized enterprise-level graph databases like Neo4j, allowing us to create a rich, interconnected representation of domain knowledge. We structured the data as a labeled property graph, with nodes representing different elements (text, images, tables, and code) and edges capturing the relationships. The resulting knowledge graph served dual purposes\u2014as both an ontology and a schema\u2014providing a flexible framework for organizing and querying the extracted knowledge. For benchmarking, we compared the framework against leading proprietary models like GPT-4, Claude-3 Opus, and Google Gemini Pro. We fine-tuned smaller language models (DeepSeek-Coder-7B-Instruct and DeepSeek-Math-7B-Instruct) using the Hugging Face PEFT library, employing techniques like QLoRA. Our hyperparameter configuration included a batch size of 24, a learning rate of 1e-4, and 50 training epochs, among other settings. Training was conducted on NVIDIA GPUs, with multiple independent runs to ensure robustness. We reported ensemble averages of the results to provide a comprehensive evaluation of the framework's performance in handling complex chemical and process engineering tasks.\nEvaluating Tool Proficiency: Our study employs various evaluation metrics to assess the effectiveness of the proposed framework tool learning (Qu et al. 2024) across different stages: task planning, tool selection, tool calling, and response generation. We evaluate the task planning capabilities of the framework through several key metrics: Tool Usage Awareness, Pass Rate, and Accuracy. Tool Usage Awareness measures the ability of the framework to correctly identify if a query requires an external tool, expressed as $Awareness = \\frac{\\text{Number of Correct Identifications}}{\\text{Total Number of Queries}}$. The Pass Rate assesses the effectiveness of the proposed task planning in addressing the query, calculated by $Pass Rate = \\frac{\\text{Number of Successfully Completed Tasks}}{\\text{Total Number of Tasks}}$. Accuracy evaluates the precision of the plan generated by the framework by comparing it to a gold standard solution, calculated as $Accuracy = \\frac{\\text{Number of Correct Plans}}{\\text{Total Number of Plans}}$. Additionally, the values of these metrics range from 0 to 1, where 0 indicates the worst performance and 1 indicates the best performance. The evaluation metrics used for tool selection include Recall, NDCG, and COMP. Recall@K measures the proportion of selected top-K tools that are present in the set of ground-truth tools, formulated as $Recall@K = \\frac{\\sum_{q\\in Q} |T_q \\cap T^K|}{\\sum_{q\\in Q} |T_q|}$, where Q is the set of queries, Tq is the set of relevant tools for the query q, and $T^K$ is the top-K tools for the query q selected by the framework. Normalized Discounted Cumulative Gain (NDCG@K) considers the proportion and positions of positive tools, with Discounted Cumulative Gain (DCG@K) calculated as $DCG_q@K = \\sum_{i=1}^{K}\\frac{g_i}{log_2(i+1)}$ and NDCG@K as $NDCG@K = \\frac{1}{|Q|} \\sum_{q=1}^Q \\frac{DCG_q@K}{IDCG}$, where gi is the graded relevance score (assigned by human evaluators) at position i and IDCG is the ideal DCG. '@K' indicates that the cumulative gain is considered up to the K-th item in the ranked list. COMP@K assesses whether the top-K selected tools form a complete set with respect to the ground-truth set, defined as $COMP@K = \\sum_q I(\\Phi_q \\subseteq \\Psi^K)$, where I, is the ground-truth tool set for query q, $I$ is the top-K tools retrieved. The indicator function I(\u00b7) in COMP@K checks if the ground-truth set Iq is a subset of the top-K retrieved set, returning 1 for true ($I(\\Phi_q \\subseteq ) = 1$) and 0 for false ($I(\\Phi_q \\subseteq ) = 0$). The subset condition ensures that all relevant tools are included in the retrieved top-K results. The evaluation metrics for tool selection\u2014Recall@K, NDCG@K, and COMP@K\u2014each range from 0 to 1, where higher values indicate better performance. In evaluating tool calling, we assess the framework using three metrics: Consistency with Stipulations, Correctness of Parameter Extraction, and Error Handling. Consistency with Stipulations measures how well the provided parameters match the tool's documentation requirements, calculated as $\\frac{\\text{(Number of parameters consistent with the stipulations)}}{\\text{Total number of parameters required}} \\times 100%$. Correctness of Parameter Extraction evaluates the accuracy in extracting the correct parameters from the user query, defined as $(\\frac{\\text{Number of correctly extracted parameters}}{\\text{Total number of parameters}}) \\times 100%$. Error Handling assesses the system's ability to manage errors during tool calling, measured as $(\\frac{\\text{Number of errors handled successfully}}{\\text{Total number of errors encountered}}) \\times 100%$. These metrics are expressed as percentages to quantitatively assess the effectiveness of the framework in tool calling, with values ranging from 0% (worst performance) to 100% (best performance). A value of 0% for any metric indicates complete failure (e.g., no parameters meet the stipulations, no parameters correctly extracted, or no errors managed), while 100% indicates perfect performance (e.g., all parameters meet the stipulations, all parameters correctly extracted, or all errors managed effectively). The evaluation metrics used for response generation include BLEU, ROUGE-L, and Exact Match. BLEU (Bilingual Evaluation Understudy) is calculated using the formula: $BLEU = BP \\cdot exp(\\frac{1}{N} \\sum_{n=1}^{N} W_n log p_n)$, where BP is the brevity penalty, Wn is the weight for n-gram precision, and pn is the modified n-gram precision. ROUGE-L (Recall-Oriented Understudy for Gisting Evaluation) focuses on the longest common subsequence (LCS) and its formula is: $ROUGE-L = F_B = \\frac{(1+B^2) \\cdot LCS-precision \\cdot LCS-recall}{B^2 \\cdot LCS-precision + LCS-recall}$, where \u00df is usually set to 1.0. In ROUGE-L, LCS-Precision is the ratio of the length of the LCS to the total number of words in the candidate response, LCS-Recall is the ratio of the length of the LCS to the total number of words in the reference response, and the F-measure balances these using the harmonic mean. Exact Match measures the percentage of responses that are exactly the same as the reference answer, and its formula is: $Exact Match = \\frac{\\text{Number of Exact Matches}}{\\text{Total Number of Responses}}$. The metrics BLEU, ROUGE-L, and Exact Match all range from 0 to 1 (or 0 to 100%), with 0 indicating the worst performance (no match or overlap with the reference response) and 1 (or 100%) indicating the best performance (perfect match or complete alignment with the reference response). These metrics provide a comprehensive evaluation of the quality of generated responses by assessing them against machine-generated (Gold-LLM such as GPT-4 (Omni)) reference responses in terms of precision, recall, and exact match. In summary, evaluation metrics are crucial in tool learning to ensure the framework can effectively plan tasks, select and call tools, and generate accurate and useful responses. These metrics help in instruction-tuning the action generator and expert models (tools) and improving the framework's performance in handling complex tasks with the aid of external tools.\nUser-Centric Evaluation: We present a comprehensive human evaluation approach for assessing the effectiveness of tool learning with the framework, going beyond metrics. Our approach involves eight key aspects, all rated by humans: user satisfaction, usability, task completion, response quality, context awareness, adaptability, error handling, and qualitative feedback. User satisfaction and usability are gauged through Likert-scale surveys, with scores ranging from 1 (minimum) to 5 (maximum). Task completion is measured by whether specific tasks are successfully completed (Yes/No). Response quality is evaluated based on four criteria: relevance, clarity, completeness, and accuracy, each scored from 1 to 5. Context awareness is evaluated by presenting a series of related queries to check if the framework maintains coherence, while adaptability is tested using various query types, with both aspects scored from 1 to 5. Error handling is examined by introducing deliberate errors to see how well the framework corrects itself, also scored from 1 to 5. Qualitative feedback is categorized as High, Medium-High, or Medium, providing deeper insights into user experiences. This multi-faceted evaluation ensures a thorough understanding of the framework's performance from a human-centric perspective, highlighting its strengths.\nExperimental Results\nThe experimental results on the evaluation of the PEOA framework in task planning, tool selection, tool calling, and response generation are detailed in several tables. In task planning,  compares state-of-the-art proprietary LLMs using metrics such as Tool Usage Awareness (TUA), Pass Rate (PR), and Accuracy (Acc), all expressed as percentages, where TUA ranges from 0% (failure) to 100% (perfect identification), PR from 0% (none correct) to 100% (all correct), and Accuracy from 0% (none correct) to 100% (all correct).  for tool selection uses Recall, NDCG, and COMP metrics, with Recall@K measuring the proportion of relevant tools in the top-K selected (0% to 100%), NDCG@K assessing ranking quality (0 to 1), and COMP@K verifying if the selected tools form a complete set (0% to 100%). For tool calling,  employs Consistency with Stipulations (Cons), Correctness of Parameter Extraction (PE), and Error Handling (EH), with Cons ranging from 0% (none meet requirements) to 100% (all meet requirements), PE from 0% (none correct) to 100% (all correct), and EH from 0% (ineffective) to 100% (effective). The experimental results for response generation are shown in  using BLEU, ROUGE-L, and Exact Match (EM), where BLEU measures n-gram precision (0 to 1), ROUGE-L focuses on the longest common subsequence (0 to 1), and EM assesses exact matches between generated and reference responses (0% to 100%). The experimental results show that the proposed framework performs effectively across"}, {"title": "Ablation Studies", "content": "We conducted several ablation studies to thoroughly evaluate the contributions and effectiveness of various components of the PEOA framework, particularly focusing on its instruction-tuning, graph-based retrieval methods, and iterative problem-solving mechanisms for solving complex chemical and process engineering calculations. The ablation study aims to isolate and evaluate the contributions of each major component in the framework. By systematically disabling key components, we can better understand their roles and optimize the framework for improved performance in real-world process engineering applications. The ablation study evaluates four key variants of the framework. The first variant (W/o GRACG) uses instruction-tuning of expert models (tools). 'W/"}]}