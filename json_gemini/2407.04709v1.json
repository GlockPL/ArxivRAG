{"title": "Efficient 4D Radar Data Auto-labeling Method using\nLiDAR-based Object Detection Network", "authors": ["Min-Hyeok Sun", "Dong-Hee Paek", "Seung-Hyun Song", "Seung-Hyun Kong"], "abstract": "Focusing on the strength of 4D (4-Dimensional)\nradar, research about robust 3D object detection networks\nin adverse weather conditions has gained attention. To train\nsuch networks, datasets that contain large amounts of 4D\nradar data and ground truth labels are essential. However,\nthe existing 4D radar datasets (e.g., K-Radar) lack sufficient\nsensor data and labels, which hinders the advancement in this\nresearch domain. Furthermore, enlarging the 4D radar datasets\nrequires a time-consuming and expensive manual labeling\nprocess. To address these issues, we propose the auto-labeling\nmethod of 4D radar tensor (4DRT) in the K-Radar dataset.\nThe proposed method initially trains a LiDAR-based object\ndetection network (LODN) using calibrated LiDAR point cloud\n(LPC). The trained LODN then automatically generates ground\ntruth labels (i.e., auto-labels, ALs) of the K-Radar train dataset\nwithout human intervention. The generated ALs are used to\ntrain the 4D radar-based object detection network (4DRODN),\nRadar Tensor Network with Height (RTNH). The experimental\nresults demonstrate that RTNH trained with ALs has achieved\na similar detection performance to the original RTNH which is\ntrained with manually annotated ground truth labels, thereby\nverifying the effectiveness of the proposed auto-labeling method.\nAll relevant codes will be soon available at the following GitHub\nproject: https://github.com/kaist-avelab/K-Radar", "sections": [{"title": "I. INTRODUCTION", "content": "As one of the fundamental modules of autonomous driving\nsystems, perception modules enable vehicles to identify vari-\nous types of driving information [1]. Among perception mod-\nules, object detection identifies the type, shape, and position\nof objects around the vehicle. Robust 3D (3-Dimensional)\nobject detection under all weather conditions is required\nto ensure safe driving since autonomous driving systems\nsubsequently perform path planning and control based on\nthe outputs of 3D object detection [2].\nCamera and LiDAR are primary sensors for object de-\ntection. Camera can provide high-resolution RGB images\nfor accurate 2D object detection [3]. However, camera lacks\ndepth information, which is essential for precise 3D object\ndetection. Furthermore, since camera measures visible light,\nit is difficult to robustly detect objects under adverse illu-\nmination or weather conditions. LiDAR measures the time-\nof-flight of infrared pulse signals and outputs precise 3D\nspatial information of the surrounding environment through\nLiDAR point cloud (LPC), which enables accurate 3D object\ndetection [4]. In addition, since LiDAR utilizes infrared pulse\nsignals, they are less affected by illumination conditions\nthan camera. However, LiDAR sensors are susceptible to\nraindrops or snowflakes, which degrades the detection perfor-\nmance of LiDAR-based object detection networks (LODNs)\nin adverse weather conditions [5].\nRecently, 4D radar has emerged as an alternative sensor\nfor robust object detection under adverse illumination and\nweather conditions [6]. As radar utilizes long-wavelength\nradio waves, 4D radar is less affected by illumination and\nweather conditions than camera or LiDAR [7]. Further-\nmore, unlike 3D radar which only can measure along the\nazimuth, range, and Doppler dimensions, 4D radar addi-\ntionally can measure along elevation dimension. Therefore,\n4D radar can recognize the 3D shapes of objects similar\nto LiDAR. Focused on the application of 4D radar to\nautonomous driving systems, studies [8]\u2013[13] published 4D\nradar dataset or proposed 4D radar-based object detection\nnetworks (4DRODNs). Among them, [12] and [13] released\na large-scale multi-modal 4D radar dataset (i.e., K-Radar),\nwhich contains 4D radar tensor (4DRT) captured under var-\nious illumination, weather, and road conditions. The studies\nalso proposed Radar Tensor Network with Height (RTNH),\none of the most practical 4DRODNs trained with the K-\nRadar dataset. RTNH demonstrated the robust detection\nperformance in adverse weather conditions such as sleet and\nheavy snow than LODN, which verified the strength of the\napplication of 4D radar in autonomous driving systems."}, {"title": "II. RELATED WORKS", "content": "Recent advancements in radar hardware technology has led\nthe active commercialization of 4D radar sensors [19]. The\nactive commercialization has enabled the use of 4D radar\nin autonomous driving perception, and several 4DRODN\nstudies and 4D radar datasets have been published.\nASTYX [8] is the first published 4D radar dataset that\nincludes 0.5K frames of the 4D radar point cloud (4DRPC)\nand ground truth labels. Study [9] proposed RPFA-net, which\nis similar to LODN PointPillars [20]. The authors trained\nRPFA-net with ASTYX dataset and demonstrated the first\n4D radar-based 3D object detection. Although these studies\nare notable for pioneering the utilization of 4D radar in\nautonomous driving systems, the dataset was too small to\ndevelop and train advanced networks. Studies [10] and [11]\npublished VoD and TJ4DRadSet datasets that contain 8.7K\nand 7.8K frames of 4DRPC, respectively, and performed 4D\nradar based 3D object detection using PointPillars network.\nHowever, they only captured 4DRPC under good weather\nconditions. Therefore, these studies can not explore the\nstrength of 4DRODNs, which can robustly detect objects in\nadverse weather conditions. In addition, these datasets are\nstill insufficient to train practical 4DRODNs for autonomous\ndriving systems.\nIn contrast, study [12] released a K-Radar dataset that\nprovides 35K frames of 4D radar data captured in diverse\nroad, illumination, and weather conditions. In addition, the\nK-Radar dataset offers 4D radar tensor (4DRT) instead of\n4DRPC. 4DRT is a raw-level radar measurement before\nConstant False Alarm Rate (CFAR) process. Therefore,\n4DRT can provide denser 3D spatial information than the\n4DRPC. The study also proposed a practical and real-time\noperating 4DRODN, RTNH. The experiment results in [12]\ndemonstrated that RTNH has superior detection performance\nin challenging weather conditions compared to LODNs. A\nfollow-up study [13] introduced a pre-processing method to\nfilter 4DRT and achieved enhanced detection performance.\nHowever, the overall detection performance of RTNH is\nnot enough to ensure safe autonomous driving. One primary\nreason is that the K-Radar train dataset does not contain\nsufficient labeled objects compared to the LiDAR datasets.\nFor example, K-Radar has 100K annotated objects, whereas\nKITTI [15], nuScenes [16], and Waymo [17] provide 200K,\n1.4M, and 12M labeled objects, respectively. In general, a\nlarge amount of labeled objects is crucial to train advanced\nobject detection networks [21]. Therefore, efficient ways to\nexpand the K-Radar dataset are necessary."}, {"title": "III. PROPOSED AUTO-LABELING METHOD", "content": "4DRT contains raw power measurements of the radio\nsignal reflected from objects, but there are also invalid mea-\nsurements from the sidelobe, noise, interference, and clutter.\nTherefore, as shown in Fig.1, 4DRT is not intuitive for\nhuman eyes to distinguish various types of objects, such as\nvehicles, pedestrians, road signs, or curbs [12]. On the other\nhand, LPC can represent the detailed boundaries of objects,\nand is easy to distinguish object types in the data. There-\nfore to generate HLs of K-Radar dataset, [12] accurately\ncalibrated the 4DRT to align with the LPC. Subsequently,\nthey used the LPC object representations as a reference and\ngenerated ground truth labels for 4DRT manually. Similar\nto the manual labeling process, after training LODN to\nrecognize objects on the LPC, the inference outputs of the\ntrained LODN can serve as ALs for the 4DRT."}, {"title": "B. Generation and Refinement of ALs", "content": "After training PVRCNN++, The proposed method gener-\nates ALs of the K-Radar train dataset with the inference\noutput of PVRCNN++. When generating ALs, the confi-\ndence score threshold significantly affects the accuracy of\nthe generated ALs. The confidence score represents the\nconfidence level for each detected object in the inference\noutput of the trained object detection network. Therefore, a\nlower confidence score threshold reduces miss detections but\nincreases false alarms in the generated ALs and vice versa.\nTo determine the optimal threshold, the method generates\nmultiple ALs with three different thresholds and compares\nthe accuracy of ALs based on the Fl-score metric. The\ncomparison result in Table I demonstrates that a threshold\nof 0.3 is the optimal."}, {"title": "C. Training 4DRODN with Generated ALS", "content": "The proposed method introduces RTNH [13] to train\n4DRODN with generated ALs. In [13], RTNH is trained with\n4D sparse radar tensor (4DSRT), which includes only the top\n10% measurements of the 4DRT, and the experiment results\ndemonstrate the enhanced detection performance. Similarly,\nwe train RTNH (i.e., RTNH-AL) using 4DSRT and ALs, to\ncompare with the original RTNH and verify the effectiveness\nof the proposed auto-labeling method in subsection IV-A.\nAdditionally, we hypothesize that the performance degra-\ndation of LODN in adverse weather conditions would neg-\natively impact the training of RTNH-AL. LiDAR cannot"}, {"title": "IV. EXPERIMENT RESULTS", "content": "In this section, we present the experiment results of\ntraining RTNH using the proposed auto-labeling method. In\nsubsection IV-A, we compare the detection performance of\nRTNH-AL (trained with ALs) with that of the original RTNH\n(trained with HLs) and demonstrate the effectiveness of the\nproposed auto-labeling method. Subsequently, in subsection\nIV-B, we present the results of three RTNH-AL models\ntrained with three divided subsets and discuss the generalized\ntraining of RTNH-AL. Finally, we conduct ablation studies\nin subsection IV-C to explore ways to improve the training\neffectiveness of RTNH using the auto-labeling method fur-\nther based on quantitative experimental results.\nAll the training environments and hyper-parameters are\nset identically to those in [13] (implemented with PyTorch\n1.11.0 on Ubuntu 20.04 machines equipped with RTX 3090).\nWe evaluate the both bird's-eye view (BEV) and 3D detection\nperformance of trained RTNH models with the metric of IoU-\nbased average precision (\\(AP_{BEV}\\) and \\(AP_{3D}\\) respectively) for\nsedan objects with a 0.3 IoU threshold, which follows the\nmetric in [12]."}, {"title": "A. Comparison of RTNH-AL and RTNH", "content": "Table II presents the \\(AP_{BEV}\\) and \\(AP_{3D}\\) for car objects of\nRTNH and RTNH-AL. As shown in the table, the RTNH-AL\ndemonstrates almost the same level of detection performance\nas that of RTNH. This result verifies that the proposed auto-\nlabeling method that utilized LODN trained with LPC is\ndesirable to train RTNH efficiently. The example of inference\noutput is shown in Fig.3 where the HLs, inference output\nof RTNH, and RTNH-AL of example frames under various\nweather conditions are presented."}, {"title": "B. Impact of Inaccurate ALs on RTNH-AL Detection Perfor-mance", "content": "In this experiment, we aim to examine the effect of\ninaccurate ALs in adverse weather conditions on RTN\u0397-\nAL performance. Further, we explore the generalized training\nof RTNH-AL by analyzing three RTNH-AL models trained\nwith the subsets of the K-Radar, as mentioned in subsection\nIII-C. The \\(AP_{BEV}\\) and \\(AP_{3D}\\) of trained RTNH-AL models are\npresented in Table III."}, {"title": "C. Ablation Studies", "content": "1) Comparison of RTNH-AL models based on detection\nperformance of LONDs for auto-labeling: Since ALs are\ngenerated based on the inference output of trained LODN,\nthe detection performance of the LODN may affect the\nALs accuracy and RTNH-AL detection performance. We\nexamined the impact of LODN detection performance on\nRTNH-AL training using ALs generated by two different\nLODNs (i.e., SECOND [23] and PVRCNN++ [22]). The\nexperiment result is shown in Table V."}, {"title": "V. CONCLUSION", "content": "In this paper, we have proposed an auto-labeling method\nfor 4DRT using a LODN trained with calibrated LPC. The\nproposed method generated ground truth labels for RTNH\ntraining without additional human aid or expense. The ex-\nperiment results have demonstrated the effectiveness of the\nproposed method as the RTNH trained with ALs achieved\nsimilar detection performance to the RTNH trained with HLs.\nIn addition, we have provided the strategies for efficient\nexpansion of the K-Radar dataset based on the results of\nthe experiment using generated ALs."}]}