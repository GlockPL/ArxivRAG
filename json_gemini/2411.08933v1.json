{"title": "Confidence-aware Denoised Fine-tuning of Off-the-shelf Models for Certified Robustness", "authors": ["Suhyeok Jang", "Seojin Kim", "Jinwoo Shin", "Jongheon Jeong"], "abstract": "The remarkable advances in deep learning have led to the emergence of many off-the-shelf classifiers, e.g., large pre-trained models. However, since they are typically trained on clean data, they remain vulnerable to adversarial attacks. Despite this vulnerability, their superior performance and transferability make off-the-shelf classifiers still valuable in practice, demanding further work to provide adversarial robustness for them in a post-hoc manner. A recently proposed method, denoised smoothing, leverages a denoiser model in front of the classifier to obtain provable robustness without additional training. However, the denoiser often creates hallucination, i.e., images that have lost the semantics of their originally assigned class, leading to a drop in robustness. Furthermore, its noise-and-denoise procedure introduces a significant distribution shift from the original distribution, causing the denoised smoothing framework to achieve sub-optimal robustness. In this paper, we introduce Fine-Tuning with Confidence-Aware Denoised Image Selection (FT-CADIS), a novel fine-tuning scheme to enhance the certified robustness of off-the-shelf classifiers. FT-CADIS is inspired by the observation that the confidence of off-the-shelf classifiers can effectively identify hallucinated images during denoised smoothing. Based on this, we develop a confidence-aware training objective to handle such hallucinated images and improve the stability of fine-tuning from denoised images. In this way, the classifier can be fine-tuned using only images that are beneficial for adversarial robustness. We also find that such a fine-tuning can be done by merely updating a small fraction (i.e., 1%) of parameters of the classifier. Extensive experiments demonstrate that FT-CADIS has established the state-of-the-art certified robustness among denoised smoothing methods across all 12-adversary radius in a variety of benchmarks, such as CIFAR-10 and ImageNet.", "sections": [{"title": "1 Introduction", "content": "Despite the recent advancements in modern deep neural networks in various computer vision tasks (Radford et al., 2021; Rombach et al., 2022; Kirillov et al., 2023), they still suffer from the presence of adversarial examples (Szegedy et al., 2013) i.e., a non-recognizable perturbation (for humans) of an image often fools the image classifiers to flip the output class (Goodfellow et al., 2014). Such adversarial examples can"}, {"title": "2 Preliminaries", "content": "Adversarial robustness and randomized smoothing. We assume a labeled dataset $D = \\{(x_i, y_i)\\}_{i=1}^n$ sampled from $P$, where $x \\in X \\subset \\mathbb{R}^d$ and $y_i \\in Y := \\{1, ..., K\\}$, and aim to develop a classifier $f: X \\rightarrow Y$ which correctly classifies a given input $x$ into the corresponding label among K classes, i.e., $f(x_i) = Y_i$.\nAdversarial robustness refers to the worst-case behavior of $f$; given a sample $x \\in X$ and the corresponding label $y \\in Y$, it requires $f$ to produce a consistent output under any perturbation $\\delta \\in \\mathbb{R}^d$ which preserves the original semantic of $x$. Here, $\\delta$ is commonly assumed to be restricted in some $l_2$-norm in $\\mathbb{R}^d$, i.e., $|\\|\\delta|\\|_2 \\leq \\epsilon$ for some positive $\\epsilon$. For example, Moosavi-Dezfooli et al. (2016); Carlini et al. (2019) quantify adversarial robustness as average minimum distance of the perturbations that cause $f$ to flip the originally assigned label $y$, defined as:\n$R(f; P) := \\mathbb{E}_{(x,y) \\sim P}  \\underset{f(x')\\neq y}{\\text{min}} ||x' - x||_2 $ (1)\nThe primary obstacle in achieving adversarial robustness lies in the difficulty of evaluating and optimizing for it, which is typically infeasible because $f$ is usually modeled by a complex, high-dimensional neural network. Randomized smoothing (Cohen et al., 2019; Lecuyer et al., 2019) addresses this challenge by constructing a new robust classifier $g$ from $f$, instead of directly modeling robustness with $f$. In particular, Cohen et al. (2019) models $g$ by selecting the most probable output of $f$ under Gaussian noise $N(0, \\sigma^2I)$, defined as:\n$g(x) := \\underset{c \\in Y}{\\text{arg max}} \\mathbb{P}_{\\delta \\sim N(0,\\sigma^2I)} [f(x + \\delta) = c] $. (2)"}, {"title": "3 Method", "content": "In Section 3.1, we present a description of our problem and the main idea. In Section 3.2, we provide descriptions of our selection strategy for non-hallucinated samples. In Section 3.3, we outline our overall fine-tuning framework."}, {"title": "3.1 Problem description and Overview", "content": "In this paper, we investigate how to effectively elaborate an off-the-shelf classifier $f_{c1f}$ within a denoised smoothing scheme. We remark that the robustness of the smoothed classifier $g$ from denoised smoothing of $f_{c1f}$ depends directly on the accuracy of the denoised images (see Eq. (3) and (4)). Therefore, one may expect that improving $f_{c1f}$ for clean images is sufficient to improve the generalizability and robustness of $g$ (Carlini et al., 2023), assuming that the denoised images follow the pre-training distribution with clean images (Salman et al., 2020), i.e., the denoised images preserve the semantics of the original clean images. However, this assumption is not true; the noise-and-denoise procedure of denoised smoothing often suffers from distribution shifts and hallucination issues so that the resulting denoised images have completely different semantics from the original labels (see Figure 2a)."}, {"title": "3.2 Confidence-aware denoised image selection", "content": "We propose a confidence-aware selection strategy to identify hallucinated images and non-hallucinated images within a set of denoised images. Consider the denoised images $D_x = \\{denoise(x + \\delta_1), ..., denoise(x+\\delta_M)\\}$ for a given clean image $x$ and the number of noises $M$. We aim to find non-hallucinated images within $D_x$ that an off-the-shelf classifier $f_{c1f}$ classifies as the assigned label $y$, i.e., $f_{c1f}$ shows the highest confidence for $y$ among all possible classes. Conversely, if $f_{c1f}$ classifies denoised images as a label other than $y$, we define such denoised images as hallucinated images, i.e., samples that no longer preserve the core semantic of $y$. Accordingly, the set of non-hallucinated images $D_{x,nh} \\in D_x$ is defined as follows:\n$D_{x,nh} = \\{x|f_{clf} (denoise(x + \\delta_i)) = y, i \\in [1, ..., M]\\} $. (7)\nWe remark that the off-the-shelf classifier $f_{c1f}$ is pre-trained with clean images, rather than denoised images. Thus, at the beginning of the fine-tuning, $f_{c1f}$ often fails to correctly assign $D_{x,nh}$ due to the distribution shift from clean images to denoised images. Thus, we update $D_{x,nh}$ at each training iteration using Eq. (7) for a more accurate assignment of non-hallucinated images."}, {"title": "3.3 Fine-tuning with confidence-aware denoised image selection", "content": "Our main goal is to improve both the generalizability and the robustness of the smoothed classifier $g$, through the fine-tuning of the off-the-shelf classifier $f_{c1f}$ based on our confidence-aware denoised image selection in"}, {"title": "4 Experiments", "content": "We verify the effectiveness of our proposed training scheme for off-the-shelf classifiers by conducting comprehensive experiments. In Section 4.1, we explain our experimental setups, such as training configurations and evaluation metrics. In Section 4.2, we present the main results on CIFAR-10 and ImageNet. In Section 4.3, we conduct an ablation study to analysis the component-wise effect of our training objective."}, {"title": "4.1 Experimental setup", "content": "Baselines. We mainly consider the following recently proposed methods based on denoised smoothing framework. We additionally compare with other robust training methods for certified robustness based on randomized smoothing. Following the previous works, we consider three different noise levels, $\\sigma\\in \\{0.25, 0.50, 1.00\\}$, to obtain smoothed classifiers.\nCIFAR-10 configuration. We follow the same classifier and the same denoiser employed by Carlini et al. (2023). Specifically, we use the 86M-parameter ViT-B/16 classifier which is pre-trained and fine-tuned on ImageNet-21K and CIFAR-10, respectively. We use the 50M-parameter 32\u00d732 diffusion model from Nichol & Dhariwal (2021) as the denoiser. We provide more detailed setups in Appendix B.2.\nImageNet configuration. We use the 87M-parameter ViT-B/16 classifier which is pre-trained on LAION-2B image-text pairs using OpenCLIP (Cherti et al., 2023) and fine-tuned on ImageNet-12K and then ImageNet-1K. Compared to the previous state-of-the-art method, diffusion denoised (Carlini et al., 2023) based on BEiT-large model with 305M parameters, we use a much smaller off-the-shelf classifier (30% parameters). We also adopt parameter-efficient fine-tuning with LORA (Hu et al., 2022), i.e., the number of parameters updated through fine-tuning is only 1% of the total parameters. We use the same denoiser employed by Carlini et al. (2023), i.e., 552M-parameter 256\u00d7256 unconditional model from Dhariwal & Nichol (2021). We provide more detailed setups in Appendix B.2.\nEvaluation metrics. We follow the standard metric in the literature for assessing the certified robustness of smoothed classifiers: the approximate certified test accuracy at r, which is the fraction of the test set that CERTIFY, a practical Monte-Carlo-based certification procedure, classifies correctly with a radius larger than r without abstaining. Throughout our experiments, following Carlini et al. (2023), we use N = 100,000 noise samples to certify robustness for entire CIFAR-10 test set and N = 10,000 samples for 1,000 randomly selected images from the ImageNet validation set (note that RS methods in Table 1b use N = 100,000). We use the hyperparameters from Cohen et al. (2019), specifically $n_0$ = 100 and $\\alpha$ = 0.001. In ablation study, we additionally consider another standard metric, the average cerified radius (ACR)"}, {"title": "4.3 Ablation study", "content": "In this section, we conduct an ablation study to further analyze the design of our proposed losses, the impact of updating the set of non-hallucinated images, and the component-wise effectiveness of our method. Unless otherwise stated, we report the test results based on a randomly sampled 1,000 images from the CIFAR-10 test set.\nEffect of overall loss design. Table 3 presents a comparison of variants of , including: (a) removing the non-hallucinated condition of in Eq. (8), (b) removing the masking condition of in Eq. (9), and (c) training with cross-entropy loss LCE only. In summary, we observe that (a) using only non-hallucinated images for  achieves better ACR and effectively balances between accuracy and robustness. Additionally, we find that (b) the mask \"Dx,nh = M\" in  is crucial for stable training, as it prevents the optimization of adversarial images that have lost the semantic of the original image; and (c) FT-CADIS"}, {"title": "5 Related Work", "content": "Certified adversarial robustness. Recently, various defenses have been proposed to build robust classifiers against adversarial attacks. In particular, certified defenses have gained significant attention due to their guarantee of robustness. Among them, randomized smoothing shows the state-of-the-art performance by achieving the tight certified robustness guarantee over l2-adversary. This approach converts any base classifier, e.g., a neural network, into a provably robust smoothed classifier by taking a majority vote over random Gaussian noise. To maximize the robustness of the smoothed classifier, the base classifier should be trained with Gaussian-augmented images. For instance, Salman et al. (2019) employed adversarial training within the randomized smoothing framework, while Jeong & Shin (2020) suggested training a classifier using simple consistency regularization. Moreover, Jeong et al. (2023) introduced sample-wise control of target robustness, motivated by the accuracy-robustness trade-off in smoothed classifiers. However, training base classifiers specifically for Gaussian-augmented images requires large training costs and thus these methods suffer from scalability issues in complex datasets, e.g., the accuracy drops severely in the ImageNet dataset.\nDenoised smoothing. Denoised smoothing alleviates the aforementioned scalability issue of randomized smoothing by introducing \"denoise-and-classify\" strategy. This approach allows randomized smoothing to be applied to any off-the-shelf classifier trained on clean images, i.e., not specifically trained on Gaussian-augmented images, by adding a denoising step before feeding Gaussian-augmented images into the classifier. In recent years, diffusion probabilistic models have emerged as an ideal choice for the denoiser in the denoised smoothing scheme. In particular, Lee (2021) have initially explored the applicability of diffusion models in denoised smoothing, and Carlini et al. (2023) further observe that combining the latest diffusion models with an off-the-shelf classifier provides a state-of-the-art design for certified robustness. Meanwhile, Jeong & Shin (2024) investigate the trade-off between robustness and accuracy in denoised smoothing, and proposed a multi-scale smoothing scheme that incorporates denoiser fine-tuning.\nOur work aims to improve the certified robustness of smoothed classifiers in denoised smoothing, which is determined by the average accuracy of the off-the-shelf classifiers under denoised images. We improve such robustness by addressing hallucination and distribution shift issues of denoised images. Specifically, we focus on filtering out hallucinated images based on the confidence of off-the-shelf classifiers, and then fine-tune off-the-shelf classifiers with non-hallucinated images."}, {"title": "6 Conclusion", "content": "We propose FT-CADIS, a scalable fine-tuning strategy of off-the-shelf classifiers for certified robustness. Specifically, we propose to use the confidence of off-the-shelf classifiers to mitigate the intrinsic drawbacks of the denoised smoothing framework, i.e., hallucination and distribution shift. We also demonstrate that this can be achieved by updating only 1% of the total parameters. We hope that our method could be a meaningful step for the future research to develop a scalable approach for certified robustness."}, {"title": "Limitation and future work", "content": "In this work, we apply an efficient training technique for off-the-shelf classifiers based on LORA (Hu et al., 2022). Nevertheless, certification remains a bottleneck for throughput, due to its majority voting process involving a large number of forward inferences, i.e., N = 100,000. An important future work would be to accelerate the certification process for a more practical deployment of our method. In addition, certain public vision APIs do not allow us to access the underlying off-the-shelf classifiers, i.e., black-box. In such cases, our method is not directly applicable, and further research on training methods that are independent of model parameters, such as prompt-tuning (Jia et al., 2022), will be necessary."}]}