{"title": "Diffusion-Based Visual Art Creation: A Survey and New Perspectives", "authors": ["Bingyuan Wang", "Qifeng Chen", "Zeyu Wang"], "abstract": "The integration of generative Al in visual art has revolutionized not only how visual content is created but also how Al interacts\nwith and reflects the underlying domain knowledge. This survey explores the emerging realm of diffusion-based visual art creation,\nexamining its development from both artistic and technical perspectives. We structure the survey into three phases, data feature and\nframework identification, detailed analyses using a structured coding process, and open-ended prospective outlooks. Our findings\nreveal how artistic requirements are transformed into technical challenges and highlight the design and application of diffusion-based\nmethods within visual art creation. We also provide insights into future directions from technical and synergistic perspectives,\nsuggesting that the confluence of generative Al and art has shifted the creative paradigm and opened up new possibilities. By\nsummarizing the development and trends of this emerging interdisciplinary area, we aim to shed light on the mechanisms through\nwhich Al systems emulate and possibly, enhance human capacities in artistic perception and creativity.", "sections": [{"title": "1 INTRODUCTION", "content": "As an emerging concept and evolving field, Artificial Intelligence Generated Content (AIGC) has made significant\nprogress and impact over the past several years, especially since the diffusion model was proposed [58]. On the other\nhand, visual art, encompassing a wide variety of genres, media, and styles, possesses high artistic value and diverse\ncreativity, sparking widespread interest. However, compared to general method innovations [59, 132] and specific model\ndesigns [42, 117], relatively limited research focuses on diffusion-based methods for visual art creation. Fewer works\nthoroughly examine the problem, summarize frameworks, or provide trends and insights for future research.\nRelevant surveys approach this problem from both technical and artistic perspectives. Some recent surveys focus\non the intersection of artificial intelligence with content generation, examining data modalities and tasks [44, 80] to\nmethodological progressions and applications [9, 16]. These surveys reviewed a series of work on artistic stylization [76],\nappearance editing [124], text-to-image transitions [165], and the newfound applications of AI across multiple data\nmodalities [173]. Methodologically, they span neural style transfer [67], GAN inversion [153] to attention mecha-\nnisms [52] and diffusion models [23]-each contributing to the state of the art in their own right. From an application\nperspective, they explore the transformative integration of AIGC across various domains, and while remarkable, they\nalso highlight challenges that call for further development and ethical consideration [93, 155]. Meanwhile, surveys with\nan artistic focus unravel the interplay between arts and humanities within the AIGC era, probing into the processing\nand understanding of art through advanced computational methods [7, 18, 89], the generative potential of AI in creating\nnovel art forms [36, 168], and the applicability of its integration in enhancing educational and therapeutic experiences"}, {"title": "2 BACKGROUND", "content": "Prior to the emergence of diffusion models, the field of machine learning in visual art creation had already gone through\nseveral significant developments. These stages were marked by various generative models that opened new chapters in\nimage synthesis and editing. One of the earliest pivotal advancements was the introduction of Generative Adversarial\nNetworks (GANs) by Goodfellow et al. [50], which introduced a new framework where a generator network learned to\nproduce data distributions through an adversarial process. Following closely, CycleGAN by Zhu et al. [184] overcame\nthe need for paired samples, enabling image-to-image translation without paired training samples. These models\ngained widespread attention due to their potential in a variety of visual content creation tasks. Simultaneously with\nthe development of GANs, another important class of models was the Variational Autoencoder (VAEs) introduced\nby Kingma and Welling [71], which offered a method to generate continuous and diverse samples by introducing a\nl atent space distribution. This laid the groundwork for controllable image synthesis and inspired a series of subsequent\nworks. With enhanced computational power and innovation in model design, Karras et al. pushed the quality of image\ngeneration further with StyleGAN [69], a model capable of producing high-resolution and lifelike images, driving\nmore personalized and detailed image generation. The incorporation of attention mechanisms into generative models\nsignificantly improved the relevance and detail of generated content. The Transformer by Vaswani et al. [138], with its\npowerful sequence modeling capabilities, influenced the entire field of machine learning, and in visual art generation, the\nsuccessful application of Transformer architecture to image recognition with Vision Transformer (ViT) by Dosovitskiy\net al. [39], and further for high-resolution image synthesis with Taming Transformers by Esser et al. [42], showed the\nimmense potential of Transformers in visual generative tasks. Subsequent developments like SPADE by Park et al. [110]\nand the time-lapse synthesis work by Nam et al. [107] marked significant steps towards more complex image synthesis\ntasks. These methods provided richer context awareness and temporal dimension control, offering users more powerful"}, {"title": "3 RELATED WORK", "content": "In this section, we provide an overview of the scope of AIGC and contributions of pertinent surveys that concentrate\non fields and topics relevant to diffusion-based visual art creation. We first collected 42 surveys and filtered out 30\nby relevance. These surveys are primarily categorized by their focus on either technical (17) or artistic (13) aspects.\nCollectively, they establish the paradigm of this interdisciplinary field and create a platform for our discussion."}, {"title": "3.1 Relevant Surveys with Technical Focus", "content": "From a technical view, a tier of surveys focus on the advancements and implications of artificial intelligence in content\ngeneration. For example, Cao et al. [16] provides a detailed review of the history and recent advances in AIGC,\nhighlighting how large-scale models have improved the extraction of intent information and the generation of digital\ncontent such as images, music, and natural language. We further break down this view into data and task, method, and\napplication perspectives."}, {"title": "3.1.1 Data and Task Perspectives.", "content": "A series of surveys inspect AIGC from data and modality and highlight the evolution\nand challenges in various tasks, including artistic stylization, appearance editing, text-to-image generation, text-to-3D"}, {"title": "3.1.2 Method Perspective.", "content": "A main body of recent surveys in generative Al and computer vision has been on the\nevolution of methodologies for style transfer, GAN inversion, attention mechanisms, and diffusion models, which have\nbeen instrumental in driving forward the state-of-the-art. Neural Style Transfer (NST) has evolved into a field of its\nown, with a variety of algorithms aimed at improving or extending the seminal work of Gatys et al. [67] provides a\ntaxonomy of NST algorithms and compares them both qualitatively and quantitatively, also highlighting the potential\napplications and future challenges in the field. In the realm of GANs, the survey on GAN inversion [153] details the\nprocess of inverting images back into the latent space to enable real image editing and interpreting the latent space of\nGANs. It outlines representative algorithms, applications, and emerging trends and challenges in this area. The survey\non attention mechanisms in computer vision [52] categorizes them based on their approach, including channel, spatial,\ntemporal, and branch attention. This comprehensive review links the success of attention mechanisms in various visual\ntasks to the human ability to focus on salient regions in complex scenes, and it suggests future research directions.\nDiffusion-based image generation models have seen significant progress, paralleling advancements in large language\nmodels like ChatGPT. [173] examines the issues and solutions associated with these models, particularly focusing on the\nstable diffusion framework and its implications for future image generation modeling. Text-to-image diffusion models\nare also reviewed [165], offering a self-contained discussion on how basic diffusion models work for image synthesis.\nThis includes a review of state-of-the-art methods on text-conditioned image synthesis, applications beyond, and existing\nchallenges. Retrieval-Augmented Generation (RAG) for AIGC is discussed in a survey that classifies RAG foundations\nand suggests future directions by illuminating advancements and pivotal technologies [178]. The survey provides a\nunified perspective encompassing all RAG scenarios, summarizing enhancement methods, and surveying practical\napplications across different modalities and tasks. Finally, an overview of diffusion models addresses their applications,\nguided generation, statistical rates, and optimization [23]. It reviews emerging applications and theoretical aspects of"}, {"title": "3.1.3 Application Perspective.", "content": "From an application perspective, recent surveys have explored the integration and impact\nof AIGC across different domains such as brain-computer interfaces, education, and mobile networks, emphasizing its\ntransformative potential. Mai et al.'s survey [99] introduces the concept of Brain-conditional Multimodal Synthesis\nwithin the AIGC framework, termed AIGC-Brain. This domain leverages brain signals as a guiding condition for content\nsynthesis across various modalities, aiming to decode these signals back into perceptual experiences. The survey\nprovides a detailed taxonomy for AIGC-Brain decoding models, task-specific implementations, and quality assessments,\noffering insights and prospects for research in brain-computer interface systems. Chen's systematic literature review [25]\naddresses AIGC's application in education, highlighting the profound impact of technologies like ChatGPT. The review\nidentifies key themes such as performance assessment, instructional applications, and the advantages and risks of AIGC\nin education. It delves into the research trends, geographical distribution, and future agendas to integrate AI more\neffectively into educational methods, tools, and innovation. Xu et al. [155] survey the deployment of AIGC services\nin mobile networks, focusing on providing personalized and customized content while preserving user privacy. The\nsurvey examines the lifecycle of AIGC services, collaborative cloud-edge-mobile infrastructure, creative applications,\nand the associated challenges of implementation, security, and privacy. It also outlines future research directions for\nenhancing mobile AIGC networks.\nThese technically oriented surveys characterize remarkable advancements in the field of generative AI, emphasizing\nthe innovative algorithms and interaction paradigms that enable the creation of diverse content across various data\nmodalities. However, they also point out the existing challenges, including the need for further technical development,\nthe consideration of ethical issues, and the imperative to address potential negative impacts on society."}, {"title": "3.2 Relevant Surveys with Artistic Focus", "content": "Another tier of work adopts an artistic view by specifically focusing on arts and humanities in the AIGC era. For\nexample, Liu et al. [93] explore the transformational impact of artificial general intelligence (AGI) on the arts and\nhumanities, addressing critical concerns related to factuality, toxicity, biases, and public safety, and proposing strategies\nfor responsible deployment. We further break the view into processing and understanding, generation, and application\nperspectives."}, {"title": "3.2.1 Processing and Understanding Perspectives.", "content": "The surveys with an artistic focus shed light on the intersection of\nart and technology, where advanced processing techniques and computational methods are employed to understand\nand enhance the appreciation of visual arts. Depolo et al.'s review [36] discusses the mechanical properties of artists'\npaints, emphasizing the importance of understanding paint material responses to stress through tensile testing data and\nother innovative techniques. The study highlights how new methods allow for the investigation of historic samples\nwith minimal intervention, utilizing techniques such as nanoindentation, optical methods like laser shearography,\ncomputational simulations, and non-invasive approaches to predict paint behavior. Castellano et al. [18] provides an\noverview of deep learning in pattern extraction and recognition within paintings and drawings, showcasing how these\ntechnological advances paired with large digitized art collections can assist the art community. The goal is to foster\na deeper understanding and accessibility of visual arts, promoting cultural diffusion. Zhang et al.'s comprehensive\nsurvey on the computational aesthetic evaluation of visual art images [168] tackles the challenge of quantifying\naesthetic perception. It reviews various approaches, from handcrafted features to deep learning techniques, and explores"}, {"title": "3.2.2 Generation Perspective.", "content": "Surveys focused on the generation of art through AI technologies underscore the\ntransformative role AI plays in both understanding and creating visual arts. Cetinic et al. [19] offer an integrated\nreview of Al's dual application in art analysis and creation, including an overview of artwork datasets and recent\nworks tackling various tasks such as classification and computational aesthetics, as well as practical and theoretical\nconsiderations in the generation of AI Art. Shahriar et al. [127] examine the potential of GANs in art creation, exploring\ntheir use in generating visual arts, music, and literary texts. This survey highlights the performance and architecture of\nGANs, alongside the challenges and future recommendations in the field of computer-generated arts. Liu's overview of\nAl in painting [90] reveals the field's current status and future direction, discussing how Al algorithms can produce\nunique art forms and automate tasks in traditional painting, thereby promising a revolution in the digital art world\nand traditional painting processes. Ko et al. [72] delve into Large-scale Text-to-Image Generation Models (LTGMs) like\nDALL-E, discussing their potential to support visual artists in creative works through automation, exploration, and\nmediation. The study includes an interview and literature review, offering design guidelines for future intelligent user\ninterfaces using LTGMs. Lastly, Maerten et al.'s review on deep neural networks in AI-generated art [98] examines\nthe evolution of these architectures, from classic convolutional networks to advanced diffusion models, providing\na comparison of their capabilities in producing AI-generated art. This review encapsulates the rapid progress and\ninteraction between art and computer science."}, {"title": "3.2.3 Application Perspective.", "content": "The surveys with an artistic focus on application delve into the transformative potential\nof integrating art with other disciplines, particularly science education and therapy, to foster holistic learning and healing\nexperiences. Turkka et al. [137] investigates how art is integrated into science education, revealing through a qualitative\ne-survey of science teachers (n=66) that while the incorporation of art can enhance teaching, it is infrequently applied\nin classroom practices. The study presents a pedagogical model for art integration, which characterizes integration\nthrough content and activities, and suggests that teacher education should provide more consistent opportunities for art\nintegration to enrich science teaching. The study on art therapy [62] surveys the clinical applications and outcomes of\nart therapy as a non-pharmacological intervention for mental disorders. The systematic review of 413 literature pieces\nunderscores the clinical effectiveness of art therapy in alleviating symptoms of various mental health conditions, such\nas depression, anxiety, and cognitive impairments, including Alzheimer's and autism. It emphasizes the therapeutic\npower of art in assisting patients to express emotions and providing medical specialists with complementary diagnostic\ninformation."}, {"title": "4 RESEARCH SCOPE AND CONCEPTS", "content": "In this section, we first define the survey's research scope and explain relevant concepts. Then, we summarize our\nresearch goals and target questions. Together, they establish a coherent context and lay a foundation for the following\nsections."}, {"title": "4.1 Research Scope", "content": "Based on the surveys discussed in the previous section, we identify two independent taxonomies in the technical and\nartistic realms. The first taxonomy, typical in surveys with a technical focus, categorizes diffusion-based generative\ntechniques as one of the generative methods and art as an application scenario [16, 44, 80, 93]. On the other hand,\nsurveys with an artistic stance commonly adopt historical or theoretical perspectives, categorize relevant research by\napplication scenarios and artistic categories (in Sec. 5.1, we correspond them to different data modalities or applications),\nand focus more on the implications of generated results [17, 36, 89, 90, 168]."}, {"title": "4.2 Relevant Concepts", "content": "To clearly define our research scope and differentiate it from similar work, we provide an explanation and categorization\nmethod for the two most relevant concept realms and their sub-concepts."}, {"title": "4.2.1 Diffusion Model.", "content": "In Jan. 2020, Ho et al. proposed Denoising Diffusion Probabilistic Models [58] and tested its\nperformance on multiple image synthesis tasks, proclaiming the advent of the post-diffusion era. Ten months later, Song\net al. adapted the denoising process to the latent space and significantly improved the generative performance, which is\ncalled Denoising Diffusion Implicit Models [132]. In 2021, different researchers optimized the method by integrating\nadvanced text-image encoders (e.g., CLIP [115]) and conditioning methods (e.g., ILVR [28]). Another series of work\nsystematically framed the generative task [108] and established relevant benchmarks [37], demonstrating surpassing\nperformance than previous state-of-the-art methods.\nIn early 2022, many technical companies released respective diffusion-based generative frameworks, including\nDALL-E-2 [116], Imagen [121], Stable Diffusion [118], etc. These methods feature extensive training and can generate\nhigh-quality, artistic images to meet commercial needs. From late 2022, the field has shifted from a common focus to\ndifferent sub-tracks and downstream applications, by diversifying multiple tasks, introducing different methods, and\nadapting to various scenarios. Meanwhile, within the AIGC framework, the field of Natural Language Processing (NLP)\nhas also witnessed significant breakthroughs. Researchers proposed foundational models (e.g., the GPT series [109]),\ndesigned adaptation methods (e.g., LoRA [61]), and achieved comparable performance with humans in NLP tasks [14].\nCombined with these advancements, the field of Diffusion Model increased in both width and inclusiveness, becoming\nmore expanded and more interconnected with other fields."}, {"title": "4.2.2 Visual Art.", "content": "We break up the topic by three different perspectives of visual art 1) as a conceptual realm under art,\n2) as visual contents created by artists, and 3) as generated results with the quality of artistic. Such perspectives will be\nrevisited in the Discussion section (Sec. 6)."}, {"title": "4.3 Research Goals and Questions", "content": "Following the previous discussion and prior work, we summarize two research goals of our paper:\nG1 Analyze how diffusion-based methods have facilitated and transformed visual art creation. How are\ndiffusion-based generative systems and models used for different Visual Art applications?\nG2 Provide frameworks, trends, and inspirations for future research in relevant fields. How may human\nand generative Al inspire each other in Diffusion-Based Visual Art Creation?\nBased on the two research goals, we further propose four research questions as the basis of this survey.\nQ1 What are the most attended topics in diffusion-based Visual Art Creation? This is the basic step to identify\nhot issues and construct a consistent framework. The question also concerns contrasts between diffusion-based\nand non-diffusion-based methods, and the temporal features and evolution of this field.\nQ2 What are current research problems/needs/requirements in diffusion-based Visual Art creation? This\nquestion ranges from an artistic/user perspective to a technical/designer perspective. In the following sections,\nwe further break it down to artistic requirements and technical problems, featured by application scenarios,\ndata modalities, and generative tasks, and attempt to establish connections between them.\nQ3 What are the methods applied in diffusion-based Visual Art creation? For each technical problem, we\nfocus on diffusion-based method design according to its modalities and tasks. As DDPM, DDIM, and their\nextensions follow similar model structures, we can further categorize and organize the methods based on the\nunified structure of an extended diffusion model.\nQ4 What are the frontiers, trends, and future works? We are interested in the following questions: Are there\nany further problems to solve? How may we leverage the development of a diffusion model and its application\nin relevant fields to cope with the problems?"}, {"title": "5 FINDINGS", "content": "In this section, we aim to fulfill G1 by answering questions Q1-Q3."}, {"title": "5.1 Structural Analysis and Framework Construction", "content": "5.1.1 Data Classification. We focus on the first question: What are the currently most attended topics in diffusion-based\nVisual Art Creation? (Q1) We first summarized different paper codes proposed in Sec. ?? along with the index terms of\neach selected paper. Among them, a major part is closely related to method design, while others concern data, modalities,\nartistic genres, and application scenarios. We found that these terms basically form three categories and thus applied a\nVenn Chart to characterize different works, as shown in Fig. 3. The three categories include:\n\u2022 Application. Different application scenarios (e.g., different art genres [48, 114], visualization [154, 164])\n\u2022 Understanding. Different data forms and corresponding modalities (e.g., image series [12, 134, 135], 3D\nscenes [38, 53, 166]). From an artistic perspective, the first two categories characterize different art forms/genres\nwith corresponding features.\n\u2022 Generation. Different generative tasks (e.g., style control [84, 152], style transfer [176, 177], image editing [10,\n55]) and Different generative methods (e.g., ControlNet [95, 156], Textual Inversion [1, 175], LoRA [24, 126])\nWith such a categorization method, we can approach the dataset from different perspectives and identify correspond-\ning hot topics. As shown in Fig. 3, most of the selected works lie in four subsets of the seven areas, including:\n\u2022 Generation (125). Generation and editing with controllable style, subject, and layout (e.g., personalization [106,\n151], stylization [136, 148], layout control [32, 157])\n\u2022 Generation \u2229 Application (55). Application-oriented generation (e.g., art therapy [185], visual art educa-\ntion [35], computational arts metaverse [78])"}, {"title": "5.1.2 Framework Construction.", "content": "Since most of the work is concentrated at the pole of generation, we dived into the\ngenerative part and paid more attention to the intersection areas. We found that 1) Research in Diffusion-Based Visual\nArt Creation is typically characterized by different artistic scenarios and technical methods. 2) the artistic requirements\nand technical problems are basically connected by specifying data modality and extracting generative tasks. As a result,\nwe summarized a new framework that can better characterize the current research paradigm (Fig. 4).\nBased on the framework, we can further break down Q1 into a series of consequential questions and approach a\ngenerative problem from different perspectives:\n\u2022 Scenario. What are the common features and requirements of different artistic scenarios?\n\u2022 Modality. What are the data modalities applied, including training dataset, input, conditions, and output?\n\u2022 Task. What are popular research problems in generating Visual Arts, including their technical statements and\nclassification?\n\u2022 Method. What are the methods used to augment and adapt diffusion models?\nIn the following sections, we will refer to this structure to analyze the relationships represented by each red dashed\nline."}, {"title": "5.2 Temporal Analysis and Trend Detection.", "content": "In this part, we investigate how the number of publications, categories, and keywords in different dimensions evolve\nover time in our dataset. We specifically focus on the difference between pre-diffusion and post-diffusion eras."}, {"title": "5.2.1 Data Distribution.", "content": "Fig. 5 displays the temporal distribution of our dataset, including the time when major\ndiffusion-based models are proposed. According to the figure, most of the work is published after Jun. 2022, especially\nafter Feb. 2023, when a series of landmark methods were introduced. We also calculate the proportions of different\ncategories of work and demonstrate the result in Fig. 6. We found that while the proportion of generation research\nremains essentially the same, understanding research keeps growing in the four years of 2020-2023, and application\nresearch witnessed a surge in 2023. On the other hand, the proportion of diffusion-based methods increased steadily,\nfrom around 20% in 2020 to over 70% in 2023."}, {"title": "5.2.2 Topic Evolution.", "content": "In examining the dataset, we noticed that most methods and some tasks change and upgrade over\ntime, while application scenarios are generally constant. We thus provide a Word Cloud Chart (Fig. 7) to compare methods\nand tasks in pre-diffusion and post-diffusion eras and illustrate the main application scenarios, artistic categories,\nmethod features, and user requirements. According to the figure, most generative tasks stay the same, while some\ntraditional tasks (e.g., NPR) are less mentioned. However, generative methods have undergone a major shift, from SBR\n(stroke-based rendering), rule-based generation, and physical-based simulation, to a series of modifications based on\ndiffusion models."}, {"title": "5.2.3 Qualitative Comparison.", "content": "From a microscopic perspective, we are also interested in how the development of\ndiffusion-based models introduces new methods for solving traditional problems. We thus selected five artistic genres\nor scenarios, including robotic painting, Chinese landscape painting, ink painting, story visualization, and artistic font\nsynthesis, to compare different approaches for similar problems and tasks."}, {"title": "5.2.4 A Brief Summary and Outlook.", "content": "In the previous section, we have compared methods before and after the Diffusion\nEra, to borrow frameworks and ideas from the Pre-Diffusion Era and inspire new method design. Here we are further\ninterested in identifying research gaps from temporal trends and task-method relationships in Diffusion-Based Visual\nArt Creation."}, {"title": "5.3 From Artistic Requirements to Technical Problems", "content": "In this section, we focus on the upper half of the Rhombus framework (Fig. 4), to summarize current research prob-\nlems/needs/requirements in Diffusion-Based Visual Art creation (Q2). Specifically, we start from application scenarios"}, {"title": "5.3.1 Application Domain and Artistic Category.", "content": "Among our 143 selected papers, 70 are coded as application/scenario-\noriented. Within this subset, 55 papers focus on specific artistic categories (e.g., traditional paintings, human portraits,\nand specific art genres), and 17 focus on relevant domains (e.g., story visualization, replication prevention, human-AI\ncollaboration). We summarize representative work in different application scenarios, focusing on how they formulate\nand tackle the domain issues.\nThe first series of works view visual art (or digital art, fine art) as a general category. Abrahamsen et al. [?] introduce\ninnovative methods to invent art styles using models trained exclusively on natural images, thereby circumventing\nthe issue of plagiarism in human art styles. Their approach leverages the inductive bias of artistic media for creative\nexpression, harnessing abstraction through reconstruction loss and inspiration from additional natural images to forge\nnew styles. This holds the promise of ethical generative AI use in art without infringing upon human creators' originality.\nIn a similar vein, Zhang et al. [177] address the limitations of existing artistic style transfer methods, which either fail\nto produce highly realistic images or struggle with content preservation, by proposing ArtBank. This novel framework,\nunderpinned by a Pre-trained Diffusion Model and an Implicit Style Prompt Bank (ISPB), adeptly generates lifelike\nstylized images while maintaining the content's integrity. The added Spatial-Statistical-based self-attention Module\n(SSAM) further refines training efficiency, with their method surpassing contemporary artistic style transfer techniques\nin both qualitative and quantitative evaluations. Meanwhile, Qiao et al. [113] explore the use of image prompts in\nconjunction with text prompts to enhance subject representation in multimodal AI-generated art. Their annotation\nexperiment reveals that initial images significantly improve subject depiction, particularly for concrete singular subjects,\nwith icons and photos fostering high-quality, aesthetically varied generations. They provide valuable design guidelines\nfor leveraging initial images in Al art creation. Furthermore, Huang et al. [63] present the multimodal guided artwork\ndiffusion (MGAD) model, a novel approach to digital art synthesis that leverages multimodal prompts to direct a\nclassifier-free diffusion model, thereby achieving greater expressiveness and result diversity. The integration of the CLIP\nmodel unifies text and image modalities, with substantial experimental evidence endorsing the efficacy of the diffusion\nmodel coupled with multimodal guidance. Lastly, Liao et al. [85] contribute to the field by introducing ArtBench-10,\na class-balanced, high-grade dataset for benchmarking artwork generation. It stands out with its clean annotations,\nhigh-quality images, and standardized dataset creation process, addressing the skewed class distributions prevalent in\nprior artwork datasets. Available in multiple resolutions and formatted for seamless integration with prevalent machine\nlearning frameworks, ArtBench-10 facilitates comprehensive benchmarking experiments and in-depth analyses to\npropel generative model research forward. Collectively, these works illustrate the dynamic intersection of AI and art,\nwhere innovative methodologies and datasets are expanding the frontiers of artistic creation, opening avenues for novel\nstyles, ethical considerations, and enhanced representation in the digital art sphere.\nThe second series of works focus on specific artistic genres or historical contexts, among which traditional Chinese\npainting is most frequently visited. Wang et al. [149] introduce CCLAP, a pioneering method for controllable Chinese\nlandscape painting generation. By leveraging a Latent Diffusion Model, CCLAP consists of a content generator and\nstyle aggregator that together produce paintings with specified content and style, evidenced by both qualitative and\nquantitative results that showcase the model's artful composition capabilities. A dedicated dataset, CLAP, has been\ndeveloped to evaluate the model comprehensively, and the code has been made accessible for broader use. Addressing"}, {"title": "5.3.2 Representing Scenarios as Modalities and Tasks.", "content": "Next", "modalities": "n\u2022 Thread/Brushstroke. The first series of work focus on brush stroke generation. The problem has been\nlong-studied and technically attended since around 2000 and can be well solved by traditional rendering and\nrule-based methods", "159": ".", "183": ".", "141": ".", "169": ".", "147": ".", "Creation": "n\u2022 Quality Enhancement. As the baseline task in content generation and the basic requirement in visual art\ncreation, the generated content should possess higher resolution and better quality. This is commonly realized\nby aesthetic training data, advanced model structure, more parameters, and result optimization designs. In the\npost-diffusion era, these methods are integrated into training foundation models [22, 54, 108, 118, 176", "179": "."}]}