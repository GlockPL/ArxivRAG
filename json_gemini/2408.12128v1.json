{"title": "Diffusion-Based Visual Art Creation: A Survey and New Perspectives", "authors": ["BINGYUAN WANG", "QIFENG CHEN", "ZEYU WANG"], "abstract": "The integration of generative Al in visual art has revolutionized not only how visual content is created but also how Al interacts\nwith and reflects the underlying domain knowledge. This survey explores the emerging realm of diffusion-based visual art creation,\nexamining its development from both artistic and technical perspectives. We structure the survey into three phases, data feature and\nframework identification, detailed analyses using a structured coding process, and open-ended prospective outlooks. Our findings\nreveal how artistic requirements are transformed into technical challenges and highlight the design and application of diffusion-based\nmethods within visual art creation. We also provide insights into future directions from technical and synergistic perspectives,\nsuggesting that the confluence of generative Al and art has shifted the creative paradigm and opened up new possibilities. By\nsummarizing the development and trends of this emerging interdisciplinary area, we aim to shed light on the mechanisms through\nwhich Al systems emulate and possibly, enhance human capacities in artistic perception and creativity.", "sections": [{"title": "1 INTRODUCTION", "content": "As an emerging concept and evolving field, Artificial Intelligence Generated Content (AIGC) has made significant\nprogress and impact over the past several years, especially since the diffusion model was proposed [58]. On the other\nhand, visual art, encompassing a wide variety of genres, media, and styles, possesses high artistic value and diverse\ncreativity, sparking widespread interest. However, compared to general method innovations [59, 132] and specific model\ndesigns [42, 117], relatively limited research focuses on diffusion-based methods for visual art creation. Fewer works\nthoroughly examine the problem, summarize frameworks, or provide trends and insights for future research.\nRelevant surveys approach this problem from both technical and artistic perspectives. Some recent surveys focus\non the intersection of artificial intelligence with content generation, examining data modalities and tasks [44, 80] to\nmethodological progressions and applications [9, 16]. These surveys reviewed a series of work on artistic stylization [76],\nappearance editing [124], text-to-image transitions [165], and the newfound applications of AI across multiple data\nmodalities [173]. Methodologically, they span neural style transfer [67], GAN inversion [153] to attention mecha-\nnisms [52] and diffusion models [23]-each contributing to the state of the art in their own right. From an application\nperspective, they explore the transformative integration of AIGC across various domains, and while remarkable, they\nalso highlight challenges that call for further development and ethical consideration [93, 155]. Meanwhile, surveys with\nan artistic focus unravel the interplay between arts and humanities within the AIGC era, probing into the processing\nand understanding of art through advanced computational methods [7, 18, 89], the generative potential of AI in creating\nnovel art forms [36, 168], and the applicability of its integration in enhancing educational and therapeutic experiences"}, {"title": "2 BACKGROUND", "content": "Prior to the emergence of diffusion models, the field of machine learning in visual art creation had already gone through\nseveral significant developments. These stages were marked by various generative models that opened new chapters in\nimage synthesis and editing. One of the earliest pivotal advancements was the introduction of Generative Adversarial\nNetworks (GANs) by Goodfellow et al. [50", "184": "overcame\nthe need for paired samples, enabling image-to-image translation without paired training samples. These models\ngained widespread attention due to their potential in a variety of visual content creation tasks. Simultaneously with\nthe development of GANs, another important class of models was the Variational Autoencoder (VAEs) introduced\nby Kingma and Welling [71"}]}