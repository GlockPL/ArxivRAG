{"title": "Curriculum Learning with Quality-Driven Data Selection", "authors": ["Biao Wu", "Fang Meng", "Ling Chen"], "abstract": "The remarkable multimodal capabilities demonstrated by OpenAI's GPT-4 have sparked significant interest in the development of Multimodal Large Language Models (MLLMs). Visual instruction tuning of MLLMs using machine-generated instruction-following data has been shown to improve zero-shot capabilities on many tasks, but there has been less exploration of controlling the instruction data quality. Current methodologies for data selection in MLLMs often rely on single, unreliable scores or use downstream tasks for selection, which is time-consuming and can lead to potential overfitting on the chosen evaluation datasets. To mitigate these limitations, we propose a novel data selection methodology that utilizes image-text correlation and model perplexity to evaluate and select data of varying quality. This approach leverages the distinct distribution of these two attributes, mapping data quality into a two-dimensional space that allows for the selection of data based on their location within this distribution. By utilizing this space, we can analyze the impact of task type settings, used as prompts, on data quality. Additionally, this space can be used to construct multi-stage subsets of varying quality to facilitate curriculum learning. This multiple training strategy not only utilizes a minimal amount of data but also maintains data quality diversity, significantly enhancing the model's fine-tuning performance. Our research includes comprehensive experiments conducted on various datasets. The results emphasize substantial enhancements in five commonly assessed capabilities compared to using the complete dataset. Our codes, data, and models are publicly available at: https://anonymous.4open.science/r/EHIT-31B4", "sections": [{"title": "Introduction", "content": "Instruction-following Multimodal Large Language Models (MLLMs) excel in multi-modality tasks [25, 39, 29]. Their effectiveness largely comes from using Large Language Models (LLMs) to generate synthetic data for visual instruction tuning. SELF-FILTER [8] emphasizes that visual instruction tuning is a straightforward alignment process in MLLMs training. It only needs a small amount of tuning data to activate the pre-trained capabilities and align them with the target interaction format. To improve this process, dataset selection tasks have been proposed to choose high-quality instruction-tuning data, enhancing the performance of these models [8].\n\nDespite the central role that datasets play in training large language models, exploring data quality for instruction tuning in vision-and-language models remains challenging. Many existing data selection methods use simple rules based on the characteristics of images and texts separately, such as the"}, {"title": "Related Work", "content": "Multimodal Instruction Tuning Multimodal Instruction Tuning is pivotal in advancing the capa-bilities of models like LLaVA [26], MiniGPT-4 [41], and InstructBLIP [9], which thrive on intricately paired image-text data. This technique refines the models' performance beyond what is achievable with conventional VQA datasets [12, 15], which often provide limited, short-answer data that can impair model performance. Recognizing this, the MiniGPT-4 [41] team curated a dataset of 3,500 image-text pairs, refined through interactions with ChatGPT, to enhance the models' ability to gener-ate nuanced responses. Similarly, LLaVA [26] set a benchmark by creating LLaVA-Instruct-150K, a dataset generated by prompting GPT-4 with rich annotations from the COCO dataset [24], including image captions and object details, to produce detailed questions and answers. Expanding the scope, LLaVAR [38] addressed the challenges of interpreting text-rich images by assembling over 422,000 pieces of instruction-following data through OCR technology, supplemented by an additional 16,000 high-quality entries processed by GPT-4. Furthermore, InstructBLIP [9] incorporated a diverse array of 26 public datasets, including LLaVA-Instruct-150K, to create a more comprehensive visual instruction tuning dataset. This effort, however, highlighted the prevalence of brief, perceptually focused content in existing datasets. Meanwhile, M\u00b3IT [20] transformed 40 distinct datasets into a unified vision-to-text framework, utilizing ChatGPT to rephrase and enrich the context of the responses, broadening the scope of training data suitable for deep learning models. This collective endeavor to enrich multimodal datasets [18, 22] illustrates a strategic pivot towards generating a larger, more varied corpus of visual instruction data. These datasets now cover an extensive range of tasks from basic visual recognition to complex reasoning and planning, setting a new standard for training sophisticated multimodal systems."}, {"title": "Methods", "content": "We define our data selection task in the context of instruction fine-tuning. Given an instruction tuning dataset \\(D = \\{x_i\\}_{i=1}^S\\), where each \\(x_i = (x^v_i, x^t_i)\\) represents a pair of input image and text, our objective is to select a subset of size \\(m\\) from \\(D\\). The goal is to prune \\(D\\) such that the resulting subset, \\(D' \\subset D\\), enables the pre-trained vision-language model \\(f\\) to achieve optimal performance on downstream tasks \\(\\{T_i\\}_{i=1}^n\\). Here, \\(|D'| = m\\).\n\nWe propose to select a subset of the dataset based on 1) clip score for image-text feature similarity and 2) model loss for data perplexity. We use these two data attributes to create a representation space for all data instances. By employing this method, we select the data using the region of the representation space that exhibits higher or lower values for both attributes. The vision-language model \\(f\\) is pre-trained. We denote its total loss as \\(l\\) and the loss on visual instruction data \\(x_i\\) as \\(l_i\\). Additionally, We denote its clip score as \\(s\\) and the correlation on visual instruction data \\(x_i\\) as \\(s_i\\). By maximizing the \\(l\\) and \\(s\\), we can obtain a relatively high-quality subset of data \\(D^r\\).\n\nTo evaluate the similarity between an input image \\(x^v\\) and text \\(x^t\\), we use the CLIP model to extract features from both. Specifically, we apply the image encoder of the CLIP, defined as \\(I(\\cdot)\\), to obtain the feature vector from the image, and the text encoder of the CLIP, defined as \\(T(\\cdot)\\), to derive the feature vector from the text. We then compute the dot product of both features to generate a clip score, which we define as \\(s_j\\).\n\\[ s_j = I(x^v_i) \\cdot T(x^t_i) \\]\nWe partition the data subset by identifying the upper bounds \\(S_{max}\\) and lower bounds \\(S_{min}\\) of the \\(s_j\\). Using these bounds, we select the sample data \\(d_j\\) to obtain the corresponding subset, which we refer to as the Data of Intermediate Similarity (DIS):\n\\[ DIS = \\{d_j | S_{min} \\leq s_j \\leq S_{max} \\]\nClip score reflects how well the image features correspond to the text features, allowing us to identify and select high-quality data where the image and text are closely related."}, {"title": "Data selection", "content": "Data selection is a developing field in the instruction-tuning of large language models, focused on identifying high-quality data and removing harmful information that could lead to errors [7, 4]. In this area, [7] introduced Alpagasus, a method that automates data selection by assessing instruction quality via queries to ChatGPT, thereby improving training efficiency. [21] sug-gested using the IFD score as an indicator of data difficulty, while [4] developed Instruction Mining, which evaluates sample quality through a linear combination of various indicators. Concurrently, [23] proposed assessing data by the one-shot learning performance on specific tasks. Finally, [36] in their study on InstructionGPT-4, apply a combination of multimodal scores and a regression model trained on predefined tasks for data selection, although their application is confined to MiniGPT-4 [41], which includes just 3,400 instructions."}, {"title": "Data Curriculum", "content": "Curriculum Learning Curriculum Learning has emerged as an effective strategy in machine learning, allowing models to start with simpler tasks and gradually progress to more complex ones. This method, inspired by the way humans learn, has been applied across various domains such as natural language processing and computer vision [2, 35]. In this context, [2] pioneered the concept by showing how a progressive learning schedule can improve performance in neural networks. More recently, [35] proposed a dynamic curriculum learning approach that adjusts the difficulty of the data based on the model's performance during training. Additionally, [28] introduced an automatic curriculum learning framework that utilizes reinforcement learning to dynamically select training samples, optimizing the learning process. Lastly, [34] explored self-paced learning, a variation where the model self-assesses and chooses the appropriate learning pace, thereby aligning with curriculum learning principles to improve overall training efficacy."}, {"title": "Intermediate Data Loss", "content": "The loss produced by the model, which is also a measure of perplexity, reflects the difference between the target text and the model's internal preferences. A higher loss makes the learning process more challenging for the model. Following a standard LLaVA architecture, the image encoder provides latent encoded features \\(X_i\\). Concurrently, the text decoder is tasked with maximizing the conditional likelihood of the paired text \\(Y_i\\) under the forward autoregressive factorization:\n\\[ l_j = - \\sum_{t=1}^T \\log P_\\theta(Y_{j,t} | Y_{j,<t}, X_j) \\]\nWe partition the data subset by detecting the upper bounds \\(L_{max}\\) and lower bounds \\(L_{min}\\) of the loss. Using these bounds, we select the sample data \\(d_j\\) to obtain the corresponding subset, which we refer to as the Data of Intermediate Loss (DIL):\n\\[ DIL = \\{d_j | L_{min} \\leq l_j \\leq L_{max} \\]\nWhen each piece of data has clip score and loss, we can construct a two-dimensional representation space based on these two attributes. Therefore, we select the sample data \\(d_j\\) and set both related upper bounds and lower bounds to select the high-quality subset, which we refer to as the Data of Intermediate Quantity (DIQ) :\n\\[ DIQ = \\{d_j | L_{min} \\leq l_j \\leq L_{max}, S_{min} \\leq s_j \\leq S_{max} \\]\nWe propose a data curriculum framework that starts training with simpler tasks and progressively advances to more complex ones. Based on our DIQ, we divide the region into unified blocks and use \\(\\Delta L\\) and \\(\\Delta S\\), corresponding to model loss and clip score respectively. By employing data selection methods, we can control the quality of a subset of data by gradually increasing clip score thresholds and loss thresholds. Consequently, we divide the learning process into several phases \\(k\\), and we select the sample data in each phase with different quantities :\n\\[ C_k = \\{d_j | L_p \\leq l_j, S_p \\leq s_j \\}\\]\nwhere\n\\[ L_p = L_{min} + k \\Delta L \\text{ and } S_p = S_{min} + k \\Delta S. \\]\nAs \\(k\\) increases, the learning process can be divided into multiple phases: Initialization, Intermediate, and Advanced.\n\nInitialization Phase (k=0): The model starts with a distribution of high-quality data, focusing on underlying patterns without being overwhelmed by complexity.\n\nIntermediate Phase (k=1): Data quality is improved by increasing the thresholds for clip score and loss, narrowing the candidate region of high-quality data.\n\nAdvanced Phase (k=2): The model is exposed to the most challenging data, characterized by higher clip score and model loss, testing its ability to handle complex and less consistent relationships.\n\nThis phased approach ensures progressive learning, better generalization, reduced overfitting, and enhanced robustness. By systematically organizing and presenting data based on quality metrics, the data curriculum ensures the model develops a solid foundation before tackling more complex data, leading to improved performance on multimodal tasks."}, {"title": "Experiments", "content": "In this section, we first detail our settings and the chosen base models. Then we introduce the different train scenarios and evaluation benchmarks used in our experiments and the baseline methods. We show that our proposed method achieves better performance on multiple tasks using less data."}, {"title": "Experimental Setup", "content": "VL instruction data. We use the core set, SVIT-core-157K, as our raw data, totaling 157,712 samples. SVIT [39] extends visual instruction tuning data to present a large-scale dataset containing 4.2 million command adjustment data. These data include dialog Q&A pairs, complex inference Q&A pairs, referring Q&A pairs, and detailed descriptions. More details can be found in the Appendix.\n\nBase models. We use the LLaVA-v1.5-7B [25] model architecture and its pre-training weights as our base models. The entire LLaVA training process is divided into two stages. For the first stage of pretraining, LLaVA-1.5-558k [26] selected from CC3M data are used, which have been converted into instruction-following data by GPT-4. For the second stage of visual instruction tuning, LLaVA-1.5-mix-665k [25] has been used.\n\nsetting We consider LoRA finetuning for the new instruction data. We define the state where LLaVA-1.5-mix-665k [25] has been used for instruction tuning as scenario 1, and the state where this data has not yet been used for instruction tuning as scenario 2. And, to verify the effectiveness of the data selection strategy for LLaVA model training, we mainly consider these two scenarios.\n\nBenchmarks We assess our methods using a mix of academic-task-oriented benchmarks and new benchmarks tailored for instruction-following LMMs, covering a total of 5 benchmarks. For academic-focused benchmarks, VQA-v2 [12] and GQA [15] test the model's visual perception abilities with open-ended questions. VizWiz [13] includes 8,000 images to evaluate the model's zero-shot generalization on visual queries from visually impaired individuals. In line with Instruct-BLIP [10], we use the image subset of ScienceQA [27] with multiple-choice questions to gauge zero-shot performance in scientific question answering. TextVQA [33] involves text-rich visual question answering."}, {"title": "Scenario 1: Training from LLaVA", "content": "We use the LLaVA-v1.5-7B [25] architecture with model weights fully fine-tuned using LLaVA-1.5-mix-665k data. Subsequently, we fine-tune this model with LoRA [14] during the follow-up experiments. In training, we keep the visual encoder, projector, and LLM weights frozen, and maximize the likelihood of with trainable parameters of LoRA only. We keep the rest of the training protocol the same to allow for a fair comparison. Scenario 1, which only includes LoRA tuning, takes approximately 16 hours on an NVIDIA Tesla A100 GPU with 40GB of memory, using DeepSpeed ZERO Stage 3. We use the SVIT-core-157K [39] dataset for continuous fine-tuning to establish a baseline. And the same method is applied to fine-tune our data."}, {"title": "Exploring Different Data Selection", "content": "Effectiveness of DIS and DIL. To verify the effectiveness of DIS and DIL separately, we first verified the data selection results of individual methods, in scenario 1 of the LLaVA training program. As shown in Figure 2, both DIS and DIL, using only the top 5% (around 7000 samples) of the selected data, significantly outperform the results using all the data. The model performance gradually decreases as the amount of data increases."}, {"title": "What Makes Selected Data Quality Different?", "content": "Visual instruction data generated via unimodal LLM exhibit different properties on the epistemic evidence space of clip score and loss by forming image-text pairs with their corresponding images. We try to understand what causes this problem with visual instruction data and how to control the distribution and quality of visual instruction data. The existing methods for generating data for visual instruction-tuning primarily use single-mode LLMs to adjust the text format in the data. This approach can lead to inconsistencies between the images and the corresponding text content, causing mismatches or failing to accurately capture the main elements of the images. Additionally, the design of prompts often influences the visual instruction data, altering the generation process to suit different tasks. This variation in text generation methods for different tasks exacerbates the issue of data quality divergence. To better compare the distribution of data for different tasks in space, we visualize the space."}, {"title": "Conclusion", "content": "In this paper, we introduce a curriculum learning method that imitates the human learning process. By gradually improving the quality of training data from easy to difficult stages, our method enhances performance while requiring less training data. In addition, we demonstrate the effectiveness of utilizing a dual-attribute representation space in controlling the quality of multimodal training data that divides data subsets based on clip score and model loss. We find that not only do data with higher dual-attribute values lead to better performance, but we also found a correlation between the task type used during visual instruction data creation and the distribution of positions in the dual-attribute space. At the same time, we found that different selection strategies for the subset of high-quality data are needed at different stages of training. When MLLMs have completed instruction fine-tuning tasks, incorporating curriculum learning can significantly improve fine-tuning performance.\n\nLimitation Regarding the limitations, the scope of our experiments was constrained by com-putational costs, limiting our focus to a single model. We conducted all related experiments on LLaVA-v1.5[25]. Nevertheless, this approach allowed us to achieve significant results within our computational constraints. To address these limitations, future research could explore more powerful and advanced models as computational resources allow."}]}