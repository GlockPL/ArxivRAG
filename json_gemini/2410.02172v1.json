{"title": "Abstract Reward Processes: Leveraging State Abstraction for Consistent Off-Policy Evaluation", "authors": ["Shreyas Chaudhari", "Ameet Deshpande", "Bruno Castro da Silva", "Philip S. Thomas"], "abstract": "Evaluating policies using off-policy data is crucial for applying reinforcement learning to real-world problems such as healthcare and autonomous driving. Previous methods for off-policy evaluation (OPE) generally suffer from high variance or irreducible bias, leading to unacceptably high prediction errors. In this work, we introduce STAR, a framework for OPE that encompasses a broad range of estimators-which include existing OPE methods as special cases\u2014that achieve lower mean squared prediction errors. STAR leverages state abstraction to distill complex, potentially continuous problems into compact, discrete models which we call abstract reward processes (ARPs). Predictions from ARPs estimated from off-policy data are provably consistent (asymptotically correct). Rather than proposing a specific estimator, we present a new framework for OPE and empirically demonstrate that estimators within STAR outperform existing methods. The best STAR estimator outperforms baselines in all twelve cases studied, and even the median STAR estimator surpasses the baselines in seven out of the twelve cases.", "sections": [{"title": "1 Introduction", "content": "Within reinforcement learning (RL), off-policy evaluation (OPE) is the foundational challenge of evaluating the performance, $J(\\pi)$, of policies $\\pi$ that are different from the ones used to generate data. OPE methods are a general-purpose tool that can be used as part of a local policy search algorithm [45] to provide insight about policies similar to the current policy, or as a tool to evaluate policies without requiring their actual deployment for high-risk applications like those in healthcare [38], education [35, 15], and recommendation systems [5, 7]. Despite many recent advances in OPE, existing methods struggle to give accurate predictions for many real-world applications [52], showing the need for new perspectives on OPE.\nExisting methods can be broadly divided into two categories: importance sampling (IS) based and model-based [58]. IS-based methods are typically consistent (i.e., their predictions converge probabilistically to the correct value in the limit as the amount of data approaches infinity), but have variance that increases exponentially with the horizon [29, 31]. Model-based methods have lower variance but often introduce bias due to model class mismatch and are not generally guaranteed to be consistent [36, 10]. A third set of methods, which we call mixture methods, combine the predictions obtained from both of these categories [22, 53]. However, in some cases, combining the predictions also combines the drawbacks-high variance and bias. This leads us to ask: Can we develop a framework for OPE that yields predictions that are both consistent and low variance?"}, {"title": "2 Background and Notation", "content": "An MDP is a tuple $M := (\\mathcal{S}, \\mathcal{A}, p, r, \\gamma, \\eta)$ where $\\mathcal{S}$ is the set of states, $S_t$ is the state at time $t\\in \\{0,1,...\\}$, $\\mathcal{A}$ is the set actions, $A_t$ is the action at time $t$, $p : \\mathcal{S} \\times \\mathcal{A} \\times \\mathcal{S} \\rightarrow [0,1]$ is the transition function that characterizes state transition dynamics according to $p(s, a, s') := \\text{Pr}(S_{t+1}=s'| S_t=s, A_t=a)$, $r : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\mathbb{R}$ is the reward function that characterizes rewards according to $r(s,a) := \\mathbb{E}[R_t | S_t=s, A_t=a]$, $\\gamma\\in [0,1]$ is the reward discount parameter, and $\\eta : \\mathcal{S} \\rightarrow [0,1]$ characterizes the initial state distribution according to $\\eta(s) := \\text{Pr}(S_0=s)$.\u00b9 A policy $\\pi : \\mathcal{S} \\times \\mathcal{A} \\rightarrow [0, 1]$ characterizes how actions can be selected given the current state according to $\\pi(s, a) := \\text{Pr}(A_t=a| S_t=s)$. We consider finite horizon MDPs [49] where episodes terminate by some (unspecified) time $T \\in \\mathbb{N}$\u2014which is common in practical applications of OPE. For simplicity, we set $\\gamma = 1$, allowing us to omit $\\gamma$ terms.\nFor OPE, a dataset $\\mathcal{D}_{\\pi_b}^{(n)}$ is collected by deploying a behavior policy $\\pi_b$ on the MDP $M$. The dataset of $n$ logged trajectories is denoted by $D^{(n)} := \\{H^i\\}_{i=1}^n$ where each $H^i := (S_0^i, A_0^i, R_0^i, S_1^i, ...)$ represents an independent trajectory generated by executing $\\pi_b$. The performance of an evaluation policy $\\pi_e$ is its expected return, denoted by\u00b2\n$J(\\pi_e) := \\mathbb{E} \\Big[\\sum_{t=0}^{T} R_t ; \\pi_e\\Big].$ (1)\nThe problem of off-policy evaluation entails estimating $J(\\pi_e)$ with access only to data $\\mathcal{D}_{\\pi_b}^{(n)}$, generated by a behavior policy $\\pi_b$, without additional interaction with the MDP. To ensure that samples in $\\mathcal{D}_{\\pi_b}^{(n)}$ are sufficiently informative, we make the common assumption that any outcome under $\\pi_e$ has non-negligible probability of occurring under $\\pi_b$.\nAssumption 2.1. There exists an (unknown) $\\epsilon > 0$ such that for all $s \\in \\mathcal{S}$ and $a \\in \\mathcal{A}$, $(\u03c0b(s, a) < \\epsilon) \u21d2 (\u03c0e(s, a) = 0)$.\nBackground: For a detailed review of OPE methods, we refer the reader to surveys by Voloshin et al. [58] and Uehara et al. [56]. Concepts fundamental to this approach are briefly introduced here.\n1. Importance Sampling: Importance sampling [25] enables unbiased estimation of the expected value, $\\mathbb{E}[f(X)]$, of a function $f$ applied to a random variable $X \\sim p$, given samples of a different random variable $Y \\sim q$. The importance sampling estimator is $\\frac{p(Y)}{q(Y)}f(Y)$, where"}, {"title": "2.1 Markov Reward Processes", "content": "A Markov reward process (MRP) extends the idea of a Markov chain by associating states with rewards. Formally, an MRP is a tuple $(\\mathcal{X}, p, r, \\gamma, \\eta)$ where $\\mathcal{X}$ is the set of states of the MRP, $X_t$ is the state at time $t$, $p : \\mathcal{X} \\times \\mathcal{X} \\rightarrow [0, 1]$ is the transition function where $p(x, x') := \\text{Pr}(X_{t+1}=x'| X_t=x)$, $r : \\mathcal{X} \\rightarrow \\mathbb{R}$ is the reward function where $r(x) := \\mathbb{E}[R_t| X_t=x]$, $\\gamma\\in [0, 1]$ is the discount factor, and $\\eta : \\mathcal{X} \\rightarrow [0, 1]$ is the starting state distribution. We consider finite horizon MRPs where episodes terminate by some (unspecified) timestep and set $\\gamma = 1$.\nA specific MRP is induced by the use of a fixed policy $\\pi$ on an MDP $M$, where $\\mathcal{X} = \\mathcal{S}$. The resulting transition and reward functions, denoted by $p^{\\pi}$ and $r^{\\pi}$ respectively, are:\n$p^{\\pi} (x,x') = \\frac{\\sum_{t} \\text{Pr}(S_{t+1} = x', S_t = x; \\pi)}{\\sum_{t} \\text{Pr}(S_t = x; \\pi)}, r^{\\pi}(x) = \\frac{\\sum_{t} \\mathbb{E} [R_t| S_t = x; \\pi] \\text{Pr}(S_t = x; \\pi)}{\\sum_{t} \\text{Pr}(S_t = x; \\pi)}.$ (2)\nThe Markov property [37] allows for further simplification of the above expressions (detailed in Appendix A.1), but this form is most conducive to our subsequent discussion. In this work, we focus on a specific instantiation of an MRP, described in the next section, where the set of states $\\mathcal{X}$ of the MRP are outputs of a state abstraction function $\\Phi \\in \\Phi$."}, {"title": "3 Abstract Reward Processes", "content": "An abstract reward process is a Markov reward process\u2014derived from MDP $M$ and policy $\\pi$ and defined over abstract states\u2014that we use to evaluate $\\pi$. The ARP provides two primary benefits for policy evaluation: (1) it preserves sufficient information to exactly evaluate the policy $\\pi$, and (2) the ARP can be consistently estimated from data. In this section, we formalize the concept of an ARP, and highlight the theoretical and practical benefits of using ARPs for policy evaluation.\nGiven a state abstraction function $\\phi : \\mathcal{S} \\rightarrow \\mathcal{Z}$, the ARP $\\mathcal{R}^\\pi$ is defined such that $\\mathcal{X} = \\mathcal{Z}$. Formally, $\\mathcal{R}^\\pi$ is an MRP $(\\mathcal{Z}, P^\\pi, R^\\pi, \\eta_\\phi)$, with $\\gamma = 1$ (see Appendix A.1 for a discussion on termination in ARPs and MRPs). The components of the ARP are defined over abstract states as:\n$P^\\pi(z, z'):= \\frac{\\sum_{t} \\text{Pr}(\\phi(S_{t+1})=z', \\phi(S_t)=z; \\pi)}{\\sum_{t} \\text{Pr}(\\phi(S_t)=z; \\pi)}, R^\\pi(z) := \\frac{\\sum_{t} \\mathbb{E}[R_t|\\phi(S_t)=z; \\pi]\\text{Pr}(\\phi(S_t)=z; \\pi)}{\\sum_{t} \\text{Pr}(\\phi(S_t)=z; \\pi)},$ (3)\nand $\\eta_\\phi(z) := \\text{Pr}(\\phi(S_0) = z)$. These expressions cannot be simplified further, unlike the case of an MRP [2]. Since $\\mathcal{Z}$ is a finite set, i.e., the abstract states are discrete, the components of the ARP can be represented by matrices (we use uppercase letters to emphasize this). The expected return of $\\mathcal{R}^\\pi$ can be computed efficiently using a linear solver to evaluate the expression $J(\\pi; \\mathcal{R}^\\pi) := (I-P^\\pi)^{-1}R^\\pi\\eta_\\phi$, or via Monte Carlo rollouts of the reward process.\nARPS are Performance Preserving: The expected return of an ARP has a surprising property: even though some state information is abstracted away to create simple discrete abstract states, the finite"}, {"title": "Theorem 3.1.", "content": "$\\forall \\phi \\in \\Phi$, the performance of a policy $\\pi$ is equal to the expected return of the abstract reward process $\\mathcal{R}^\\pi$, defined from MDP $M$, i.e., $J(\\pi; \\mathcal{R}^\\pi) = J(\\pi; M)$."}, {"title": "Lemma 3.2.", "content": "$\\forall \\phi \\in \\Phi$, the expected return of the maximum likelihood estimate $\\widehat{\\mathcal{R}}_n^{\\pi, \\phi}$ converges almost surely to the expected return of the policy $\\pi$, i.e., $J(\\pi; \\widehat{\\mathcal{R}}_n^{\\pi, \\phi}) \\xrightarrow{a.s.} J(\\pi; M)$."}, {"title": "3.1 Estimation from Off-Policy Data: Weighted Maximum Likelihood Estimation", "content": "We present a method for consistent estimation of the ARP corresponding to the evaluation policy $\\pi_e$ from off-policy data $\\mathcal{D}_{\\pi_b}^{(n)}$. It relies on the following intuition:"}, {"title": "Lemma 3.3.", "content": "Under Assumption 2.1, the weighted maximum likelihood estimate $\\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi}$ converges almost surely to the ground-truth ARP $\\mathcal{R}^{\\pi_e}$, i.e., $\\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi} \\xrightarrow{a.s.} \\mathcal{R}^{\\pi_e}$."}, {"title": "4 Off-Policy Evaluation with ARPs", "content": "The expected return of $\\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi}$ is a consistent estimate of the performance of policy $\\pi_e$ since $\\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi} \\xrightarrow{a.s.} \\mathcal{R}^{\\pi_e}$, as per Lemma 3.3."}, {"title": "Theorem 4.1.", "content": "The expected return of the ARP $\\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi}$ (built from off-policy data) converges almost surely to the expected return of $\\pi_e$, i.e., $J (\\pi_e; \\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi}) \\xrightarrow{a.s.} J(\\pi_e; M)$."}, {"title": "4.1 Variance Reduction: Leveraging Markovness of the State Abstraction", "content": "A common technique for mitigating the variance of IS-based methods for OPE is clipping the importance weights to the $c$ most recent ratios [18, 3, 20], i.e., $p_{(t-c+1)_+:t} := \\prod_{j=(t-c+1)_+}^t \\frac{\\pi_e(S_j, A_j)}{\\pi_b(S_j, A_j)}$, where $(t - c + 1)_+:= \\text{max}(t - c + 1, 0)$. This is often a bad approximation for classical IS-based methods, as it implies that only the $c$ most recent actions affect the reward distribution at any timestep, which rarely holds true in practice. In STAR, importance weights are incorporated into model estimation, resulting in a more reasonable implication of weight clipping.\nBy importance weighting the abstract-state occurrences as described in Equation (5), clipping importance weights, in this case, implies that the $c$ most recent abstract states are sufficient to determine the current abstract-state transition and reward distributions. This allows actions from the distant past to influence the current reward, as the effects of actions propagate through the abstract state transitions, unlike in IS-based methods. This condition, that a recent history of abstract states is sufficient to predict the current abstract state transition distribution, often approximately holds"}, {"title": "Definition 4.2 (c-th order Markov).", "content": "The abstraction function $\\phi$ is c-th order Markov if $\\text{Pr}(\\phi(S_{t+1})| \\phi(S_{t}),\u00b7\u00b7\u00b7,\\phi(S_{(t\u2212c+1)_+}); \\pi)=\\text{Pr}(\\phi(S_{t+1})| \\phi(S_{t}),\u00b7\u00b7\u00b7, \\phi(S_{0}); \\pi)$ for $\\pi \\in \\{\\pi_b, \\pi_e\\}$."}, {"title": "Theorem 4.3.", "content": "Given a c-th order Markov $\\phi$, the expected return of the abstract reward process $\\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi,c}$ converges almost surely to the expected return of $\\pi_e$, i.e., $J (\\pi_e; \\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi,c}) \\xrightarrow{a.s.} J(\\pi_e; M)$."}, {"title": "4.2 Fantastic $'s and Where to Find Them", "content": "Discrete abstraction functions that are c-th order Markov with small values of $c$ represent the most suitable abstractions for enabling asymptotically correct, low-variance off-policy evaluation using STAR. An automated approach to discovering such abstraction functions, however, remains elusive. In a manner reminiscent of the options framework [50], wherein one might consider the usefulness of options before having methods for constructing options automatically, this work emphasizes the remarkable effectiveness of state abstractions used in abstract reward processes for OPE. It motivates a research area akin to option discovery: abstraction discovery for OPE.\nWe expect the following factors to play an important role in the search for good abstraction functions: (a) state-visitation distributions of $\\pi_b$ and $\\pi_e$, determining the granularity of abstraction in different parts of the state set, and (b) the distribution shift in abstract state visitation induced by the two policies, determining the extent of weight clipping that can be applied. Both of these are affected by properties of the underlying MDP, in particular the transition function, and in our initial analyses, we observe varying effects of similar abstractions across different MDPs (Appendix C.2).\nWe observe that a simple approach of randomly initializing centroids and applying k-means clustering [34, 33], where each cluster denotes a discrete abstract state, results in abstractions that provide competitive OPE performance, often significantly outperforming existing methods. We call this naive clustering-based abstraction method CluSTAR, and use it for our experiments. In some cases, abstraction by aggregation of states can increase the difficulty of estimation of the transition function. For example, aggregation of two states with deterministic transitions\u2014which can be estimated perfectly from a single observation of those transitions-creates stochastic transitions between abstract states. However, in general, state aggregation tends to simplify estimation by increasing the effective sample size [21]."}, {"title": "Recovering Existing OPE Methods from STAR:", "content": "Different configurations of $(\\phi, c)$ induce different ARPs, $\\widehat{\\mathcal{R}}_n^{\\pi_b \\rightarrow \\pi_e, \\phi,c}$. For certain configurations of $(\\phi, c)$:\n$\\bullet$ $|\\mathcal{Z}| = 1$ and no weight clipping: Mapping all states to a single abstract state yields the weighted per-decision importance sampling (WPDIS) estimator [42].\n$\\bullet$ $|\\mathcal{Z}| = |\\mathcal{S}|$ and $c = 1$: Amounts to no state abstraction, and yields the maximum likelihood estimate of the MRP over states. The MRP is a combination of the approximate-model estimator [40] that directly estimates the model dynamics with the evaluation policy."}, {"title": "5 Empirical Analysis", "content": "In this section, we (A) analyze the performance of the set of estimators (ARPs) induced by STAR across different configurations of $(\\phi, c)$, and (B) compare the performance of the best and median ARPs from this set against existing OPE methods to demonstrate that estimators encompassed by STAR often outperform prior OPE methods. We use the following domains for OPE: (1) CartPole [49]: A classic control domain in OpenAI Gym [6]. (2) ICU-Sepsis [8]: An MDP that simulates treatment of sepsis in the ICU. ICU-Sepsis is built from real-world medical records obtained from the MIMIC-III dataset [24], using a modified version of the process described by Komorowski et al. [26]. (3) Asterix from the MinAtar testbed [61]: A miniaturized version of the Atari game Asterix. Details about each domain, and about the behavior and evaluation policies can be found in the Appendix C. The code is available at: https://github.com/shreyasc-13/STAR."}, {"title": "(A) Estimator Selection:", "content": "Estimator selection presents a significant challenge for OPE [48] due to the unavailability of a validation set. To select STAR estimators to compare against existing methods, we first report the performance of the set of the ARPs induced by STAR, across a range of configurations of $(\\phi, c)$. We highlight the performance of the best and median estimators from this set. For reference, we compare the mean squared prediction errors from the estimated ARPs against WPDIS and approximate-model estimator (MBased), the two endpoints of STAR, as shown in Figure 2 on the CartPole domain. The state abstraction is performed with CluSTAR with the number of centroids $|\\mathcal{Z}| \\in \\{2, 4, 8, 16, 32, 64, 128\\}$ and the weight clipping factor $c\\in \\{1, 2, 3, 4, 5\\}$ defining 35 ARPs, where the performance of each is indicated by. This range of $(\\phi, c)$ is picked based on the intuition that (a) larger values of $c$ are expected to introduce high variance, and (b) this range of $|\\mathcal{Z}|$ covers a variety of granularities of state abstraction (for a continuous problem). The results are averages across 200 trails. The prediction errors for the estimated ARPS are competitive with baselines. This suggests that an average estimator in STAR is competitive with existing OPE methods, and even better performance may be attained by using specialized methods for abstraction discovery and estimator selection [55]."}, {"title": "(B) ARPs Can Outperform Existing Methods for OPE:", "content": "We compare against representative methods from the main categories of OPE methods: (a) IS-based methods: Vanilla IS, Per-Decision IS, Weighted IS, Weighted Per-Decision IS [42]; (b) Model-based methods: that approximate a model of the MDP\u2014MBased [49]\u2014and directly estimate off-policy Q-values\u2014FQE and MRDR [27, 11]; and (c) Mixture methods: DR [22] and MAGIC [53], which blend estimates from the methods in the aforementioned categories. We use implementations from the Caltech OPE Benchmarking Suite (COBS) [58] for all methods. The representative method for minmax style estimators, IH [31], is designed for the infinite horizon setting and performs poorly with $\\gamma = 1$ [63]. Due to the instability of IH estimates in our experiments, we excluded it from comparison."}, {"title": "7 Discussion and Conclusion", "content": "In this paper, we have introduced a new framework for consistent model-based off-policy evaluation. This framework leverages state abstraction to prevent model class mismatch along with importance sampling to consistently learn models from off-policy data. Unlike traditional model-based methods, our approach eliminates the need for model class assumptions and provides theoretical guarantees for the obtained performance estimates. Moreover, using state abstraction increases the effective sample size [21], which is particularly beneficial in limited data regimes. Importantly, this work presents a framework with a new approach to OPE, rather than a specific new method. Estimators that lie within this framework significantly outperform existing OPE methods, with the best estimator consistently outperforming all baselines, as demonstrated in our empirical evaluation.\nThe framework has two main limitations: it requires knowledge of the probabilities of observed actions under the behavior policy, which may not always be available, and a principled method for selecting well-performing configurations of the abstraction function and the weight clipping factor remain elusive. Combining this work with regression IS [19] would be a practical extension that addresses the first limitation. Additionally, a data-driven approach to automated estimator selection based on characteristics of the domain, dataset sizes, and other factors, as suggested by Su et al. [48], would enhance its practical application.\nOur findings indicate that even a simple class of abstraction functions can provide competitive OPE performance. We theoretically demonstrate the existence of certain abstraction functions that may offer better performance. Investigating the properties of abstraction functions and developing automated approaches to abstraction discovery for ARPs are promising directions for future work on creating high-performing OPE methods."}]}