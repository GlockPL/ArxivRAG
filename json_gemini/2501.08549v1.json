{"title": "The Devil is in Temporal Token: High Quality Video Reasoning Segmentation", "authors": ["Sitong Gong", "Yunzhi Zhuge", "Lu Zhang", "Zongxin Yang", "Pingping Zhang", "Huchuan Lu"], "abstract": "Existing methods for Video Reasoning Segmentation rely heavily on a single special token to represent the object in the keyframe or the entire video, inadequately capturing spatial complexity and inter-frame motion. To overcome these challenges, we propose VRS-HQ, an end-to-end video reasoning segmentation approach that leverages Multi-modal Large Language Models (MLLMs) to inject rich spatiotemporal features into hierarchical tokens. Our key innovations include a Temporal Dynamic Aggregation (TDA) and a Token-driven Keyframe Selection (TKS). Specifically, we design frame-level <SEG> and temporal-level <TAK> tokens that utilize MLLM's autoregressive learning to effectively capture both local and global information. Subsequently, we apply a similarity-based weighted fusion and frame selection strategy, then utilize SAM2 to perform keyframe segmentation and propagation. To enhance keyframe localization accuracy, the TKS filters keyframes based on SAM2's occlusion scores during inference. VRS-HQ achieves state-of-the-art performance on ReVOS, surpassing VISA by 5.9%/12.5%/9.1% in F&F scores across the three subsets. These results highlight the strong temporal reasoning and segmentation capabilities of our method. Code and model weights will be released at VRS-HQ.", "sections": [{"title": "1. Introduction", "content": "Reasoning segmentation [9, 15, 25, 28, 36], which aims to generate segmentation results from complex query texts, has advanced with multimodal foundation models [16, 19, 27]. However, existing methods [9, 15, 25, 28] focus primarily on image-level segmentation, leaving the more challenging domain of video-level segmentation, which requires temporal reasoning of object relations and attributes, relatively unexplored. To address this gap, Video Reasoning Segmentation (VRS) [1, 37, 42] has recently emerged as a promising approach. Unlike Referring Video Object Segmentation (RVOS) methods [34, 35, 38], which rely on explicit descriptive phrases like \"a person skateboarding\", VRS leverages the extensive world knowledge and temporal reasoning capabilities of Multimodal Large Language Models (MLLMs) to transform implicit intent-based expressions into precise object masklets.\nDespite recent advancements in Video Reasoning Segmentation (VRS), such as VISA [37] and VideoLISA [1], significant challenges still exist. (i) Limited Temporal Context: Existing methods [1, 37] typically rely on a single segmentation token from an MLLM for keyframe-based segmentation (cf. Fig. 1 (a)), resulting in limited temporal context and hindering the effective capture of inter-frame variations and spatiotemporal features. (ii) Suboptimal Keyframe Detection: The LLaMA-VID [17] model, used by VISA for keyframe detection, can produce inaccurate keyframes, particularly in videos requiring complex temporal reasoning. (iii) Decoupled Segmentation and Propagation: VISA's reliance on separate, pre-trained models for keyframe segmentation (SAM [14]) and mask propagation (XMem [6]) prevents end-to-end training and inference.\nTo address the above obstacles, we introduce several strategies to strengthen the VRS model's proficiency in perceiving spatial information and interpreting temporal dynamics. Firstly, current limitations arise from insufficient single-token representation capacity, which restricts the model's abilities to capture intra-frame spatial features and maintain inter-frame temporal coherence. We hypothesize that encoding frame-level and temporal-level information into hierarchical tokens separately via MLLM and integrating them could effectively unify spatial details with temporal dynamics, enhancing perceptual capability. Secondly, the occlusion score introduced by SAM2 [27] inspires us to incorporate the target confidence of each sampled frame as a criterion for keyframe determination. We thus employ the temporal token in conjunction with SAM2 to generate the occlusion scores, applying temporal information for precise keyframe selection. As shown in Fig. 1 (b), our approach locates the cat more accurately compared to VISA, boosting inference efficiency and segmentation performance. Thirdly, leveraging SAM2's integrated seg-"}, {"title": "2. Related Works", "content": "2.1. Referring Video Object Segmentation\nReferring Video Object Segmentation [2, 35, 38] (RVOS), which focuses on segmenting and tracking prominent objects in video frames using explicit textual descriptions, has significantly advanced through the integration of visual and linguistic cues. A large proportion of these studies have leveraged attention mechanisms to merge multimodal information. For instance, Wang et.al. [31] uses asymmetric cross-guided attention to enhance sentence representations"}, {"title": "3. Methodology", "content": "Task Defination. The task of Video Reasoning Segmentation can be briefly outlined as follows. Given a video clip consisting of T frames $X_v \\in \\mathbb{R}^{T \\times 3 \\times H \\times W}$, where H and W denote the height and width of each frame respectively, along with a high-level textual instruction $X_{txt}$, VRS aims to design a model M to interpret and transform $X_{txt}$ into the binary segmentation mask sequence $X_M \\in \\mathbb{R}^{T \\times H \\times W}$ for each frame. In contrast to RVOS tasks providing explicit descriptions like \u201cthe person skateboarding,\u201d VRS typically employs expressions incorporating world knowledge like \"tool(s) for holding garbage\" or temporal logic like \"the ship(s) moving at the highest speed.\", raising higher demands on the model's capabilities of temporal relationship comprehension and complex scenario reasoning."}, {"title": "Overall Architecture.", "content": "Fig. 2 illustrates the VRS-HQ architecture, which comprises Chat-UniVi [12] as the Multi-modal Large Language Model (MLLM) for temporal token encoding (\u00a73.1), a Temporal Dynamic Aggregation (\u00a73.2), a Token-driven Keyframe Selection (\u00a73.3), and Mask Decoding and Propagation (\u00a73.4). The MLLM encodes the textual prompt into multi-level tokens representing spatial and semantic target information. These tokens drive both temporal dynamic aggregation and keyframe selection via cosine similarity scores. SAM2 then segments the keyframes and tracks the target object throughout the video. During inference, the Token-driven Keyframe Selection computes object occlusion scores for each sampled frame by merging it with the fused temporal token. Filtering keyframes based on these scores improves the accurate localization of the target object and subsequently segmentation process."}, {"title": "3.1. Temporal Token Encoding", "content": "Current VRS approaches [1, 37] typically use specified language prompts to guide MLLMs in embedding target information from keyframes or entire videos into a single specialized token, struggling to capture rich spatiotemporal dynamics essential for fine-grained video understanding. To equip the MLLM with both frame-level and video-level contextual awareness for video segmentation, we propose encoding intra-frame spatial information and inter-frame temporal relations into hierarchical tokens.\nHierarchical Token Generation. Instead of previous single-token encoding strategy, the vocabulary of the MLLM is initially augmented with two new special tokens: <SEG> and <TAK>. Then, we design a structured conversational template as \"USER: Please find\n{expression} in the Reference Video and\nsegment it in each frame and the entire\nvideo respectively.\u201d. Here, \u201c{expression}\" denotes the target object description. The tokenized prompts $X_{txt}$ and sampled video frames $X_v$ are input into the MLLM, which generates a response $Y_{txt}$ containing multiple frame-level <SEG> tokens and a temporal-level <TAK> token through autoregressive encoding."}, {"title": "Token Extraction and Mapping.", "content": "Subsequently, the <SEG> token embeddings $\\hbar_{seg} \\in \\mathbb{R}^{T'\\times d'}$ and the <TAK> token embedding $h_{tak} \\in \\mathbb{R}^{1\\times d'}$ are extracted from the MLLM's final layer, where d' denotes the MLLM's embedding dimension and T' denotes the length of sampled frames. These embeddings are then projected into the same feature space as SAM2 using a multi-layer perceptron:\n$h_{seg},h_{tak} = MLP(\\hbar_{seg}), MLP(h_{tak})$ (1)\nHere, $h_{seg} \\in \\mathbb{R}^{T'\\times d}$ and $h_{tak} \\in \\mathbb{R}^{1\\times d}$ represent the sparse embeddings for segmentation mask activation, and d denotes the feature dimension of SAM2. Finally, the frame-"}, {"title": "3.2. Temporal Dynamic Aggregation", "content": "Building on the strong temporal encoding of MLLM, <SEG> and <TAK> embeddings encapsulate the spatial priors and temporal semantic signals of the targets respectively, providing rich contextual information for the segmentation model. Accordingly, we propose the Temporal Dynamic Aggregation to facilitate the fusion of positional and semantic information of the targets.\nKeyframes as Token Similarity. The cosine similarity between each frame-level <SEG> token and the temporal-level <TAK> token reflects the semantic alignment between individual frames and the overall video context. We hypothesize that high-similarity frames are more representative of the video's content and thus suitable as keyframes. This motivates our use of token similarity for keyframe selection during training, which also facilitates weighted fusion of <SEG> and <TAK> tokens."}, {"title": "Similarity-based Weighted Fusion.", "content": "To enhance the inter-frame consistency while achieving more precise discernment of the object's positional shifts across frames, we fuse the frame-level embeddings $h_{seg}$ with the video-level embedding $h_{tak}$ using normalized cosine similarity scores as attention weights (cf. Fig. 2 (b)). During backpropagation, frame-level features $h_{seg}$ are updated alongside the temporal-level embedding $h_{tak}$ to enable more comprehensive learning of local and global video context:\n$h'_{tak} = h_{tak} + \\alpha \\sum_{i=1}^{T'} a_i h_{seg}[i]$, (2)\nwhere $a_i$ represents the normalized cosine similarity between $h_{seg}[i]$ and $h_{tak}$, and $\\alpha$ is the fusion coefficient. The resulting fused embedding $h'_{tak}$ is then used for keyframe selection during inference and mask generation."}, {"title": "3.3. Token-driven Keyframe Selection", "content": "VISA [37] relies on an external model (LLaMA-VID [17]) for keyframe selection during inference, hindering end-to-end processing and potentially degrading performance due to inaccuracies in the external model's output. To address this limitation, we introduce the Token-driven Keyframe Selection that leverages the temporal information encoded within the integrated <TAK> embedding. This approach eliminates the need for complex prompt engineering and significantly improves the reliability of keyframe selection.\nInstead of uniform video sampling during training, we adopt the CLIP model [23] to find the frame most aligned with the expression $X_{exp}$ for inference. The anchor frame is used for global sampling, resulting in sampled frames"}, {"title": "3.4. Mask Decoding and Propagation", "content": "Following temporal token fusion and keyframe selection, the mask embeddings enriched with positional and semantic information can be yielded by the segmentation model. In contrast to prior methods [1, 37, 42] depending on image-level segmentation models or external object trackers for target trajectories prediction, we utilize SAM2 to perform segmentation and propagation concurrently.\nSAM2 for Masklets Decoding. Given the keyframe $X_k$, we first extract its features using the image encoder E, providing the conditional input for SAM2. The integrated temporal embedding $h_{tak}$ then interacts with these keyframe features within the mask decoder MD (cf. Fig. 2 (d)) to generate the keyframe mask:\n$\\mathcal{X}_M = MD(E(X_k), h'_{tak}),$ (4)\nwhere $\\mathcal{X}_M^k$ represents the predicted mask for the keyframe. Next, we propagate this mask to two adjacent frames, $X_{k-1}$ and $X_{k+1}$, treated as non-conditional frames, using the memory storage and interaction mechanism. This yields the mask sequence $\\mathcal{X}_M$. During inference, all remaining frames are processed as non-conditional frames, facilitating mask propagation throughout the entire video."}, {"title": "3.5. Training Objectives", "content": "Our method is trained end-to-end using a combined text generation loss and mask loss to optimize the <TAK> and <SEG> embeddings. The mask loss combines binary cross-entropy (BCE) and DICE loss:\n$\\mathcal{L}_{total} = \\lambda_{txt} \\mathcal{L}_{txt}(Y_{txt}, \\hat{Y}_{txt}) + \\lambda_{mask}\\mathcal{L}_{mask},$ (5)\n$\\mathcal{L}_{mask} = \\lambda_{bce} \\mathcal{L}_{bce}(\\mathcal{X}_M, \\mathcal{Y}_M) + \\lambda_{dice} \\mathcal{L}_{dice} (\\mathcal{X}_M, \\mathcal{Y}_M),$ (6)\nwhere $\\hat{y}_{txt}$ and $\\mathcal{Y}_M$ represent the ground truth text and mask, respectively, and $y_{txt}$ and $\\mathcal{X}_M$ are their corresponding predictions. The weighting coefficients $\\lambda_{txt}, \\lambda_{mask}, \\lambda_{bce}, and \\lambda_{dice}$ are set to 1, 1, 2 and 0.5, respectively."}, {"title": "4. Experiments", "content": "4.1. Datasets and Metrics\nDatasets. Our model is trained and evaluated on extensive image and video segmentation datasets, LLaVA-Instruct-150k [19], as well as the Video Question-Answering datasets from Video-ChatGPT [20]. Specifically, the image segmentation datasets comprise semantic segmentation: ADE20K [44], COCO-Stuff [3], PACO-LVIS [24], and PASCALPart [5], referring segmentation: refCLEF, refCOCO, refCOCO+ [13], and refCOCOg [21], and reasoning segmentation: ReasonSeg [15]. While the video segmentation datasets encompass the RVOS datasets: Ref-Youtube-VOS [30], Ref-DAVIS17 [22] and MeViS [7], and the VRS benchmark ReVOS [37].\nEvaluation Metrics. We evaluate video segmentation using region similarity (J), contour accuracy (F), and their mean (J&F). Image segmentation accuracy is measured via Generalized Intersection over Union (gIoU) [29] and Complete Intersection over Union (cIoU) [43]. Model hallucination is assessed using the robustness score R [37]."}, {"title": "4.2. Implementation Details", "content": "We fine-tuned Chat-UniVi [12] (our chosen MLLM) using LoRA [10] (rank 8), optimizing the mask decoder and MLP projection layer while freezing other parameters. Training used AdamW (learning rate 0.0003, no weight decay) with a WarmupDecayLR scheduler (100-iteration warmup). We set the fusion coefficient $\\alpha$ to 0.1 and sampled two non-keyframes per video. Trained for 7500 iterations on a hybrid image/video dataset using four A800 GPUs with DeepSpeed [26] (batch size 1, gradient accumulation 32, total batch size 128), the model employed TDA for video data (token fusion, segmentation, and propagation) and the <TAK> token for direct segmentation on image data."}, {"title": "4.3. Comparison Results", "content": "To showcase the robust pixel-level perception and generalization of VRS-HQ, we conduct evaluations across diverse benchmarks, including ReVOS, RVOS, and image-based referring and reasoning segmentation datasets."}, {"title": "VRS-HQ [Ours]", "content": "Tab. 2 compares VRS-HQ with state-of-the-art RVOS methods. On Ref-YouTube-VOS and Ref-DAVIS17, VRS-HQ-13B surpasses VideoLISA [1] and VISA-13B [37], achieving F&F improvements of 7.3% and 5.6%, respectively. Furthermore, on the motion-intensive MeViS dataset, VRS-HQ-13B yields substantial gains of 6.2%, 6.1% and 6.4% in J, F and J&F, respectively. demonstrating its effective in capturing mo-"}, {"title": "4.4. Ablation Studies", "content": "Fusion Coefficient Ablation. The ablation analysis of the fusion coefficient $\\alpha$ in the TDA is illustrated in Tab. 4. An $\\alpha$ of 0.1 yields optimal performance. Setting $\\alpha$ to 0 leads to a sharp drop in the model's performance as <TAK> fails to capture fine-grained intra-frame details, and the <SEG> token cannot be jointly optimized with <TAK> through backpropagation (e.g., 62.1%\u219258.8% J&F on the referring subset, 56.1%\u219254.1% J&F on the reasoning subset). As $\\alpha$ increases beyond 0.1, the metrics decline slightly, likely due to excessive frame-level noise being introduced into the temporal token at a higher fusion coefficient (e.g., 62.1%\u219260.6% J&F on the referring subset, 56.1%\u219254.0% J&F on the reasoning subset).\nToken-driven Keyframe Selection. Tab. 5 analyzes the impact of different score combinations for keyframe selection within the TKS. Results indicate that the occlusion score is the most influential of the three considered, improving J&F by 1.8% and 1.6% on the referring and reasoning subsets, respectively (first two rows). This improvement stems from the occlusion score's ability to reflect the target object's presence within each frame. The model achieves its best performance when keyframe selection is"}, {"title": "D. Failure Case Analysis", "content": "Fig. 6 presents a detailed analysis of several failure cases, offering a deeper understanding of the limitations of VRS-HQ. The top row highlights two specific challenges. First, VRS-HQ struggles with keyframe localization when presented with queries based on motion, such as identifying the fastest-moving boat within a video sequence. This suggests a potential weakness in analyzing and interpreting dynamic visual information. Second, the model exhibits difficulty segmenting targets with minimal temporal presence, as exemplified by the gorilla visible only in the last two frames of the video. This points to a possible limitation in effectively capturing and utilizing short-duration visual cues. The bottom row reveals further limitations. VRS-HQ demonstrates a lack of comprehension when faced with nuanced or implicitly phrased prompts, such as recognizing a \u201chigh bar\" within the context of gymnastics performance evaluation. This suggests a need for improved understanding of complex semantic relationships within video content. Furthermore, the model occasionally exhibits hallucinatory behavior, generating segmentations for non-existent objects, particularly when dealing with empty targets or scenes where the requested object is absent.\nWe hypothesize that several strategies could mitigate these limitations. Improving the video comprehension capabilities of the Multimodal Large Language Model (MLLM) could enhance the ability to interpret complex scenes and queries. Enabling the model to process a larger number of sampled frames simultaneously might improve its sensitivity to subtle temporal changes and short-duration events. Finally, designing specialized tokens specifically for representing empty masks could address the observed hallucinations in such scenarios. We leave a thorough investigation of these potential improvements to future research."}, {"title": "E. More Qualitative Comparison", "content": "In addition to the visual comparisons presented in the main document, we provide further comparisons across more diverse settings in Fig. 7-9 to demonstrate the model's reasoning and segmentation capabilities. As illustrated in Fig. 7, VISA demonstrates reduced sensitivity to color-related expressions (e.g., \"white\" and \"brown\") when provided with explicit textual instructions. Furthermore, the example on the left demonstrates VISA's tendency to misidentify visually similar objects with complex spatial variations. In contrast, VRS-HQ effectively aggregates temporal information, capturing inter-frame motion dynamics and leading to improved segmentation accuracy."}]}