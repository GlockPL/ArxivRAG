{"title": "GRAPHBPE: Molecular Graphs Meet Byte-Pair Encoding", "authors": ["Yuchen Shen", "Barnab\u00e1s P\u00f3czos"], "abstract": "With the increasing attention to molecular machine learning, various innovations have been made in designing better models or proposing more comprehensive benchmarks. However, less is studied on the data preprocessing schedule for molecular graphs, where a different view of the molecular graph could potentially boost the model's performance. Inspired by the Byte-Pair Encoding (BPE) algorithm, a subword tokenization method popularly adopted in Natural Language Processing, we propose GRAPHBPE, which tokenizes a molecular graph into different substructures and acts as a preprocessing schedule independent of the model architectures. Our experiments on 3 graph-level classification and 3 graph-level regression datasets show that data preprocessing could boost the performance of models for molecular graphs, and GRAPHBPE is effective for small classification datasets and it performs on par with other tokenization methods across different model architectures.", "sections": [{"title": "1. Introduction", "content": "Tokenization (Sennrich et al., 2016; Schuster & Nakajima, 2012; Kudo, 2018; Kudo & Richardson, 2018) is an important building block that contributes to the success of modern Natural Language Processing (NLP) applications such as Large Language Models (LLMs) (Brown et al., 2020; Touvron et al., 2023a; Almazrouei et al., 2023; Touvron et al., 2023b). Before being fed into a model, each word in the input sentence is first tokenized into subwords (e.g., \u201clowest\u201d \u2192 \u201clow\u201d, \u201cest\u201d), which may not necessarily convey meaningful semantics but facilitates the learning of the model. Among different tokenization methods, Byte-Pair Encoding (BPE) (Gage, 1994; Sennrich et al., 2016) is a popularly adopted mechanism. Given a text corpus containing numerous sentences and thus words, BPE counts the appearance of two consecutive tokens (e.g., a subword \u201ces\u201d, an English letter \u201ct\u201d) in each word at each iteration, and merges the token pair with the highest frequency and treats it as the new token (e.g., \u201ces\u201d, \u201ct\u201d \u2192 \u201cest\u201d) for next round. A vocabulary containing a variety of subwords is then learned after some iterations, and later used to tokenize sentences fed to the model.\nIt is easy to observe that this \u201ccount-and-merge\" schedule has the potential to generalize beyond texts into arbitrary structures such as molecular graphs. Indeed, we can view words as line graphs, where each character in the word is the node, and the edges are defined by whether two characters are contiguous in the word. This observation naturally motivates us to explore the following questions: a). \u201cCan graphs be tokenized similarly to that of texts?\u201d b). \u201cWill the tokenized graphs improve the model performance?\u201d\nTo investigate whether molecular graphs can be tokenized similarly to texts, we develop GRAPHBPE, a variant of the BPE algorithm for molecular graphs, which counts the co-occurrence of contextualized (e.g., neighborhood-aware) node pairs (e.g., defined by edges) and merges the most frequent pair as the new node for next round. Compared with other methods (Jin et al., 2020; Li et al., 2023) that require external knowledge (e.g., functional groups, a trained neural network) to mine substructures, our algorithm relies solely on a given molecular graph corpus and is model agnostic. After each round of tokenization, the resulting new graph is still connected with its nodes being subsets of the nodes of the previous graph, which provides a view to construct both simple graphs and hypergraphs (Section 3.2) that can be used by Graph Neural Networks (GNNs) (Kipf & Welling, 2017; Veli\u010dkovi\u0107 et al., 2018; Xu et al., 2019; Hamilton et al., 2018) and Hypergraph Neural Networks (HyperGNNs) (Feng et al., 2019; Bai et al., 2020; Dong et al., 2020; Gao et al., 2023).\nTo explore whether tokenization helps with model performance, we compare GRAPHBPE with other tokenization methods on various datasets with different types of GNNs and HyperGNNs. We observe that tokenization in general helps across different model architectures, however, there exists no tokenization method that performs universally well over different datasets, models, and configurations. Our GRAPHBPE algorithm tends to provide more improvements"}, {"title": "3. Preliminary", "content": "In this section, we introduce the Byte-Pair Encoding (Gage, 1994; Sennrich et al., 2016) algorithm, which is widely used for NLP tasks, and the notion of hypergraphs."}, {"title": "3.1. Byte-Pair Encoding", "content": "Byte-Pair Encoding (BPE) is first developed by Gage (1994) as a data compression technique, where the most frequent byte pair is replaced with an unused \"placeholder\" byte in an iterative fashion. Sennrich et al. (2016) introduce BPE for machine translation, which improves the translation quality by representing rare and unseen words with subwords from a vocabulary produced by BPE.\nThe core of BPE can be summarized as a \u201ccount-and-merge\u201d paradigm. Starting from a character-level vocabulary derived from a given corpus, it counts the co-occurrence of two contiguous tokens, and merges the most frequent pair into a new token. Such a process is carried out iteratively until a desired vocabulary size is reached or there are no tokens to be merged."}, {"title": "3.2. Hypergraph", "content": "Compared with a N-node simple graph G = (V, E), with V = {v\u2081, V\u2082, ..., UN} and E \u2286 V \u00d7 V denoting the vertex set and edge set, N(v) representing the 1-hop neighbors of v, a N-node M-hyperedge hypergraph is defined as Gh = (V, E, W), including a vertex set |V| = N, a hyperedge set |E| = M, and a diagonal weight matrix W \u2208 R^{M\u00d7M}"}, {"title": "4. GRAPH\u0392\u03a1\u0395", "content": "In this section, we motivate our algorithm by showing a performance boost via ring contraction compared with no tokenization on molecules, followed by the details of the proposed GRAPHBPE tokenization algorithm."}, {"title": "4.1. A Motivating Example", "content": "To show that tokenization can potentially yield better performance for molecules, we compare the performance of GNNs learned on the original molecules and tokenized ones. Specifically, we contract rings in the original molecules into hypernodes (e.g., a benzene ring is viewed as 1 hypernode instead of 6 carbons), and use the summation (Xu et al., 2019) of the node features within a hypernode as its representation to be fed into GNNs.\nWe evaluate on two graph-level tasks, with MUTAG (Morris et al., 2020) for classification and FREESOLV (Wu et al., 2018) for regression, and choose GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2018), GIN (Xu et al., 2019), and GraphSAGE (Hamilton et al., 2018) as the GNNs, with the implementations detailed in Appendix A."}, {"title": "4.2. Algorithm", "content": "Given a collection of graphs D = {Gi = (Vi, E\u2081)}D, at each iteration t, our algorithm aims to tokenize each graph G\u00b9 into a collection of node sets V = {N}|N \u2208 2V}, where 2Vi denotes the power set over V\u00bf and each node set N is viewed as a hypernode, and constructs the next tokenized graph as G = (V, E), with E = {(N,N)|\u2203vm \u2208 N, vn \u2208 N, (Um, Un) \u2208 Ei}.\nAlgorithm 1 shows the proposed GRAPHBPE, which consists of a preprocessing stage and the tokenization stage. We use G to denote a general space for graphs, T to represent the space for different types of topology (e.g., rings), and S as the space for text strings. We explain the functions used in Algorithm 1 in detail as follows.\n Find():G\u00d7 T \u2192 2V, a function that finds a certain topology \u0442\u2208T of a graph G = (V,E) \u2208 G, and returns the node set N\u00ba \u2208 2V presenting that topology.\n Context(): G \u00d7 2V \u2192 S, a function that contextualizes a node set N\u00ba \u2208 2V of a graph G = (V, E), mapping it to a identifiable string s \u2208 S.\n Contract(): G \u00d7 2V \u00d7 S \u2192 G, a function that contracts a graph G = (V, E) \u2208 G on a node set N\u2208 2V and its identifiable string s \u2208 S, and returns a new graph G' = (V', E') \u2208 G with N being its hypernode, and construct the edge set E' such that E' = {(N, N')|\u2203vm \u2208 N, vn \u2208 N', (vm, Un) \u2208 E}.\n Smap () : S \u00d7 G \u2192 2V, a function that keeps track of the mapping between a graph G = (V,E) \u2208 G, an identifiable string s \u2208 S and the corresponding node set N \u2208 2V."}, {"title": "5. Experiment", "content": "In this section, we introduce the datasets, tokenization methods for comparison, and models for simple graphs and hypergraphs, and then present the experiment results."}, {"title": "5.1. Dataset", "content": "We conduct experiments on graph-level classification and regression datasets, and show their statistics in Table 3. We detail the train-validation-test split in Appendix A.\nClassification For graph classification tasks, we choose MUTAG, ENZYMES, and PROTEINS from the TUDataset (Morris et al., 2020). MUTAG is for binary classification where the goal is to predict the mutagenicity of compounds. ENZYMES is a multi-class dataset that focuses on classifying a given enzyme into 6 categories, and PROTEINS aims to classify whether a protein structure is an enzyme or not.\nRegression For graph regression tasks, we use FREESOLV, ESOL, and LIPOPHILICITY from the MoleculeNet (Wu et al., 2018), where FREESOLV aims to predict free energy of small molecules in water, ESOL targets at predicting water solubility for common organic small molecules, and LIPOPHILIC-ITY focuses on octanol/water distribution coefficient."}, {"title": "5.2. Tokenization", "content": "Given a simple graph G = (V, E), GRAPHBPE translates it into another graph G' whose vertices are node sets in 2V, which can be then used to construct a hypergraph as defined in Section 3.2. We introduce three other hypergraph construction strategies as follows.\nCentroid Following Feng et al. (2019), we construct the hyperedges by choosing each vertex together with its 1-hop neighbors. This is domain-agnostic and requires no extra knowledge, and we refer to it as CENTROID.\nChemistry-Informed We can construct hyperedges such that the nodes within which represent functional groups (Li et al., 2023). Specifically, we use RDKit (Landrum et al., 2006) to extract functional groups, and construct each hyperedge based on the nodes that belong to the same functional group. For a node that does not belong to any functional groups, we treat its edges as the respective hyperedges. This method requires domain knowledge in chemistry and we refer to it as CHEM.\nHyper2Graph Jin et al. (2020) introduce a motif extraction schedule for molecules based on chemistry knowledge and heuristics. We treat the extracted motifs, which are not necessarily meaningful substructures such as functional groups, as a type of tokenization and refer to this method as H2G."}, {"title": "5.3. Model", "content": "We choose two types of models for evaluation, with GNN for (untokenized) simple graphs, and graphs tokenized by GRAPHBPE at each iteration, and HyperGNN for hypergraphs defined by the tokenization of GRAPHBPE at each iteration, and constructed by other algorithms. We detail the model implementations in Appendix A.\nGNN We choose GCN (Kipf & Welling, 2017), GAT (Veli\u010dkovi\u0107 et al., 2018), GIN (Xu et al., 2019), and GraphSAGE (Hamilton et al., 2018) for (untokenized) simple graphs and graphs specified by the tokenization of GRAPHBPE at each iteration.\nHyperGNN For hypergraphs constructed by GRAPHBPE and other tokenization methods, we choose HyperConv (Bai et al., 2020), HGNN++ (Gao et al., 2023), which shows improved performances in metrics and standard deviation over HGNN (Feng et al., 2019) in our preliminary study, and HNHN (Dong et al., 2020) as our three backbones."}, {"title": "5.4. Result", "content": "We present experiment results on both classification datasets, with accuracy reported, and regression datasets, with RMSE reported, as suggested by Wu et al. (2018), where for each configuration we run experiments 5 times and report the mean and standard deviation of the metrics. For GRAPHBPE, we present the results on preprocessing with 100 steps of tokenization. We also report the results on the number of times GRAPHBPE is statistically (with p-value < 0.05) / numerically better / the same / worse compared with the baselines. Due to space limits, we present the rest of the results in Appendix C.\nGNN We present the test accuracy for 3-layer GNNs on MUTAG, ENZYMES, and PROTEINS in Figure 2 and the results on performance comparison in Table 4.\nIn Figure 2, we can observe that on the MUTAG dataset, GRAPHBPE performs better in general across different GNN architectures, especially for GCN and GraphSAGE, where at different time steps our algorithm consistently outperforms the untokenized molecular graphs in terms of mean\u00b1std. This suggests that tokenization could potentially help the performance of GNNs on molecular graphs. For ENZYMES and PROTEINS, GRAPGBPE does not consistently perform better than untokenized graphs, where both the tokenization step and the choice of the model will affect the accuracy. For example, approximately the first 20 tokenization steps are favored by GAT on both ENZYMES and PROTEINS, and the performance begins to degenerate as the tokenization step increases, while for GIN on ENZYMES, our algorithm is outperformed in all time steps."}, {"title": "6. Conclusion", "content": "In this work, we explore how tokenization would help molecular machine learning on classification and regression tasks, and propose GRAPHBPE, a count-and-merge algorithm that tokenize a simple graph into node sets, which are later used to construct a new (simple) graph or a hypergraph. Our experiment across various datasets and models suggests that tokenization will affect the test performance, and our proposed GRAPHBPE tends to excel on small classification datasets, given a limited number of tokenization steps.\nWe explore the simple idea of how different views of molecular graphs would benefit graph-level tasks, and we hope our results can inspire more discussions and attract attention to the data preprocessing schedules for molecule machine learning, which is less studied compared with innovations on models and benchmarks."}, {"title": "Limitation", "content": "Types of Tokenization We include two types of tokenization baselines where one is based on chemistry knowledge and the other is based on pre-defined rules. However, there exist more sophisticated tokenization methods, such as deriving tokenization rules from an off-the-shelf GNN, which are not discussed in this work.\nTypes of Task & Dataset We focus on graph-level tasks and exclude node-level tasks. For the classification and regression task, although we include three datasets for each and consider both binary and multi-class classification datasets, the size of our datasets (e.g., ~ 10\u00b3) is relatively small compared with those usually used for molecular graph pre-training (e.g., ~ 10\u00ba)."}, {"title": "A. Implementation", "content": "For all the models, we use {1, 2, 3}-layer architecture with a hidden size of {32, 64} and a learning rate of {0.01, 0.001}. For both classification and regression tasks, we apply a 1-layer MLP with a dropout rate of 0.1. We use a batch size of the form 2^N. 10^M where N, M are chosen such that the batch size can approximately cover the entire training set, and we further apply BatchNorm (Ioffe & Szegedy, 2015) to stabilize the training. We train the model for 100 epochs, and report the mean \u00b5 and standard deviation \u03c3 over 5 runs for the test performance on the model with the best validation performance.\nFor datasets with a size smaller than 2000, we adopt a train-validation-test split of 0.6/0.2/0.2, and use 0.8/0.1/0.1 for larger datasets. We ignore the edge features and use the one-hot encodings as the node features. For the tokenized graphs from GRAPHBPE, we use the summation of the node features as the representation for that hypernode for our experiments on GNNs. For classification datasets, we make the validation and test set as balanced as possible, as suggested in our preliminary study, that using a random split validation set might favor models that are not trained at all (e.g., models always predict positive for binary classification task). For regression datasets, we follow Wu et al. (2018) and use random split. For MUTAG, FREESOLV, ESOL, and LIPOPHILICITY, we set the topology to be contracted as rings in the preprocessing stage, and we set that for ENZYMES and PROTEINS as cliques.\nNote that from untokenized graphs to the last iteration T, we can track how the nodes merge into node sets in the graph and thus develop a tokenization rule for unseen graphs. However, for simplicity and efficiency, we first tokenize the entire dataset before we split them into train/validation/test sets. Our code is available at"}, {"title": "B. Discussion on Contextualizer", "content": "For the Principal Subgraph Extraction (PSE) algorithm proposed by Kong et al. (2022), we can recover it from GRAPHBPE by skipping the preprocessing stage, while setting the contextualizer as Algorithm 3. The only difference between the PSE contextualizer and ours is that in Algorithm 3, the name mapper Nmap () returns an empty string for any node sets, while ours returns the string representation for the neighborhood, meaning PSE does not take the neighbors/context into consideration during tokenization. For the Connection-Aware Motif Mining algorithm proposed by Geng et al. (2023), where the connection among the nodes is considered to mine common substructures (e.g., as illustrated in Figure 2 of Geng et al."}, {"title": "Algorithm 3 PSE-CONTEXTUALIZER", "content": "Input: a graph G = (V, E), a set of nodes V \u2208 2V, a name mapper Nmap () : V \u00d7 2V \u2192 \u00d8\nOutput: a string representation s for Ve\n# initialize s to be an empty list\ns\u2190[]\nfor v in Ve do\ns \u2190 s + Nmap (\u03c5, \u039d(v))\nend for\ns\u2190 Sort (s)\ns\u2190 Concat (s)"}, {"title": "C. Result", "content": "We include the visualization of the test performance, and the performance comparison based on p-value and metric-value for MUTAG, ENZYMES, PROTEINS, FREESOLV, ESOL, and LIPOPHILICITY in Section C.1, C.2, C.3, C.4, C.5 and C.6. For better visualization, we plot both the mean and the standard deviation as \u03bc\u00b1 \u03c3 for our experiments on GNNs, and exclude the standard deviation for the CHEM, H2G baselines on HyperGNNs. For the performance comparison, we use the triplet a:b:c to denote the number of times our algorithms are better / the same / worse compared with the baseline, and use red to mark the best within the triplet based on p-value comparison, and black to mark that for metric value comparison. We can observe that in general, given the 100 tokenization steps, GRAPHBPE tend to perform well on small datasets, which we suspect is due to the reason that larger datasets contain richer substructures to learn; thus, may need more tokenization rounds. Compared with regression tasks, GRAPHBPE tends to provide more boosts for classification tasks."}, {"title": "C.1. MUTAG", "content": "For GNNs, we include the performance comparison results in Table 10, and the visualization over different tokenization steps in Figure 4, 5, 6, and 7 for GCN, GAT, GIN, and GraphSAGE.\nFor HyperGNNs, we include the performance comparison results in Table 20, and the visualization over different tokenization steps in Figure 8, 9, and 10 for HyperConv, HGNN++, and HNHN."}, {"title": "C.2. ENZYMES", "content": "For GNNs, we include the performance comparison results in Table 25, and the visualization over different tokenization steps in Figure 11, 12, 13, and 14 for GCN, GAT, GIN, and GraphSAGE.\nFor HyperGNNs, we include the performance comparison results in Table 29, and the visualization over different tokenization steps in Figure 15, 16, and 17 for HyperConv, HGNN++, and HNHN."}, {"title": "C.3. PROTEINS", "content": "For GNNs, we include the performance comparison results in Table 34, and the visualization over different tokenization steps in Figure 18, 19, 20, and 21 for GCN, GAT, GIN, and GraphSAGE.\nFor HyperGNNs, we include the performance comparison results in Table 38, and the visualization over different tokenization steps in Figure 22, 23, and 24 for HyperConv, HGNN++, and HNHN."}, {"title": "C.4. FREESOLV", "content": "For GNNs, we include the performance comparison results in Table 43, and the visualization over different tokenization steps in Figure 25, 26, 27, and 28 for GCN, GAT, GIN, and GraphSAGE.\nFor HyperGNNs, we include the performance comparison results in Table 53, and the visualization over different tokenization steps in Figure 29, 30, and 31 for HyperConv, HGNN++, and HNHN."}, {"title": "C.5. ESOL", "content": "For GNNs, we include the performance comparison results in Table 58, and the visualization over different tokenization steps in Figure 32, 33, 34, and 35 for GCN, GAT, GIN, and GraphSAGE.\nFor HyperGNNs, we include the performance comparison results in Table 68, and the visualization over different tokenization steps in Figure 36, 37, and 38 for HyperConv, HGNN++, and HNHN."}, {"title": "C.6. LIPOPHILICITY", "content": "For GNNs, we include the performance comparison results in Table 73, and the visualization over different tokenization steps in Figure 39, 40, 41, and 42 for GCN, GAT, GIN, and GraphSAGE.\nFor HyperGNNs, we include the performance comparison results in Table 83, and the visualization over different tokenization steps in Figure 43, 44, and 45 for HyperConv, HGNN++, and HNHN."}]}