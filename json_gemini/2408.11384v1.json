{"title": "Data-Centric Machine Learning for Earth Observation: Necessary and Sufficient Features", "authors": ["Hiba Najjar", "Marlon Nuske", "Andreas Dengel"], "abstract": "The availability of temporal geospatial data in multiple modalities has been extensively leveraged to enhance the performance of machine learning models. While efforts on the design of adequate model architectures are approaching a level of saturation, focusing on a data-centric perspective can complement these efforts to achieve further enhancements in data usage efficiency and model generalization capacities. This work contributes to this direction. We leverage model explanation methods to identify the features crucial for the model to reach optimal performance and the smallest set of features sufficient to achieve this performance. We evaluate our approach on three temporal multimodal geospatial datasets and compare multiple model explanation techniques. Our results reveal that some datasets can reach their optimal accuracy with less than 20% of the temporal instances, while in other datasets, the time series of a single band from a single modality is sufficient.", "sections": [{"title": "1 Introduction", "content": "The abundance of Earth Observation (EO) data presents a significant opportunity for leveraging Machine Learning (ML) to train highly accurate models for EO-related applications in real-world scenarios. The data used is usually derived from various sources such as satellites, aerial imagery, and ground-based sensors, including time series and static features. This diversity provides rich datasets that can be harnessed by multi-modal learning techniques to improve the predictive capacities of ML models [10].\nMost recent work in the EO research community has focused on enhancing model architectures and training strategies to boost performance, and significant advancements have been made. However, as a complementary component to these model-centric efforts, research is increasingly diverging towards a more data-centric approach. This shift aims to better address challenges faced during\ndata acquisition and data curation stages. Subsequently, the enhancement of the quality of the input data can improve the performance and reliability of the models. Furthermore, incorporating a feedback loop that includes model evaluation results can provide valuable insights for refining both the data and the models.\nWithin the scope of data-centric machine learning, this work focuses on feature engineering techniques. In particular, feature selection methods aim at identifying the most useful and necessary features relevant to the task at hand. The objective is to avoid supplying the model with redundant information or extraneous features in the available data that may cause the model to learn spurious correlations and hinder its capacity to generalize. The prevailing belief that \"more data is better\" does not always hold true, and often comes at the expense of immense computational resources usage and a heavy impact on the environment [4].\nTo enhance the feature selection process, we employ eXplainable AI (\u03a7\u0391\u0399) techniques to guide the selection approach. In particular, we leverage feature attribution methods to estimate how much each feature contributes to the final predictions. By employing an incremental deletion approach, we iteratively remove less important features until an optimal set of predictive features is identified. This method ensures an active optimization of the data at each cycle, leading to a final model that efficiently utilizes the available data.\nTraditional feature selection methods, such as filter, wrapper, and embedded approaches, often involve exhaustive search strategies that can be computationally prohibitive for large datasets. Filter methods are preprocessing steps independent of the model training. They select features based on their statistical properties and relevance to the target variable. Wrapper methods evaluate feature subsets based on the performance of a specific machine learning algorithm. One prominent work in this domain is the Recursive Feature Elimination (RFE) algorithm, which recursively removes the least significant features based on their importance weights, as determined by a support vector machine (SVM) [7]. In EO, Zhang et al. [18] use a similar technique based on feature importance extracted from a random forest model to identify the most relevant features in marine data. A major limitation of this approach is that the feature importance is extracted from a model different from the main one, while these scores are usually model dependent. Embedded methods perform feature selection during the model training process, often through regularization techniques. Examples include the Least Absolute Shrinkage and Selection Operator (LASSO) method [16], which performs feature selection by enforcing sparsity through L1 regularization, and implicitly removes less important features during model training.\nThe incremental deletion approach is also used in the field of XAI to evaluate the correctness of feature importance scores, i.e. how faithful these scores are to the model [11]. Given that deleting features by setting them to zero, for instance, can lead to out-of-distribution samples, an alternative is to retrain the model on the modified data. Specifically, Hooker et al. [9] propose the RemOve And"}, {"title": "2 Methodology", "content": "We apply the incremental deletion framework to the following three EO datasets, which are composed of different modalities and span multiple years, including both regression and classification tasks.\nCropHarvest. The CropHarvest dataset is a multi-source temporal dataset designed for crop classification tasks [17]. The input modalities include satellite data from Sentinel-2 and Sentinel-1, weather time series, and static topographic information. The temporal modalities are provided on a monthly basis over multiple years (2016 - 2022). We utilize a multi-crop version of this dataset, which includes data for 10 classes.\nCrop Yield. For crop yield estimation, we use a dataset specifically aimed at predicting cereal crop yield [12]. This regression task involves predicting the amount of crop (in tonnes per hectare, t/ha) grown in a particular location during the growing seasons from 2017 until 2021. The input modalities for this dataset include multispectral satellite data from Sentinel-2 and weather time series. The weather data is aligned with the satellite temporal resolution of five days, covering the period from seeding to harvesting.\nChina PM2.5. The China PM2.5 dataset provides data for the prediction of PM2.5 concentration levels across various regions in China to track air pollution [2]. This dataset covers the years from 2010 until 2015 and includes multiple modalities, namely weather information and ground-based air quality measurements. The task is to predict particulate matter concentrations using hourly measurements from the preceding 7 days leading up to the date of the target value."}, {"title": "2.2 Modeling", "content": "Each dataset is split into three subsets: the training set comprises data from all years except the last two, which are uniformly divided into validation and test sets. This strategic split aims at reproducing the case where real-life application can only train the models on data from previous years to be deployed for upcoming seasons.\nTo identify the most effective architecture for processing multivariate time series data, we conduct a comparative evaluation of seven different model architectures. Each architecture is tested under multiple hyperparameter settings and evaluated on the validation set to determine its performance. The models compared in this study include a multilayer perceptron (MLP), a basic recurrent neural network (RNN) and two variants: Long short-term memory (LSTM) [8] and Gated Recurrent Unit (GRU) [3], a Temporal Convolutional Neural Networks (TempCNN), and finally two attention-based models: Temporal Attention Encoder (TAE) [6] and Lightweight-TAE (L-TAE) [5]."}, {"title": "2.3 Attribution estimators", "content": "Feature attribution methods are XAI tools that aim to quantify the contribution of input features to the output of a machine learning model. These methods provide insights into which features are influential in making predictions. In this study, we compare perturbation-based and gradient-based methods to infer feature attributions, namely Shapley Value Sampling (SVS) [15] and Guided Backprop (GB) [14] methods. Additionally, two ensemble-based variants of each base estimator are implemented: SmoothGrad-Squared (SG-SQ) [13] and VarGrad (VAR) [1]. As shown in [9], these ensembling methods can strongly improve the correctness of the attributions of the base estimators.\nTo ensure that the analysis captures the overall influence of each feature, the attributions are considered in their absolute values. A random selection of 5000 samples from the training set is used to estimate the attributions, which are averaged to derive the ranking of the features. The same selection of samples is used across all experiments of the same dataset.\nWe employ a feature grouping strategy applicable only on the SVS method to estimate temporal and band importance, by considering collective contributions to the model's predictions. Temporal importance is estimated by grouping the bands at each time step, perturbing them together to infer the significance of that particular time step. Similarly, band importance is assessed by treating the time series of each band as a group."}, {"title": "2.4 Incremental Deletion", "content": "Baseline Model - After evaluating multiple architectures as described in 2.2, the model with the best metric score on the validation set is selected as the baseline model for the incremental deletion cycles. The metric score is defined based on the optimization task, such as the accuracy for classification or the coefficient of determination (R2) for regression. The validation loss is used for early stopping."}, {"title": "3 Results", "content": "We trained each model architecture with various configurations and compared their performance on the validation set. The metrics reported in Table 1 include accuracy for the classification task and the R2 for regression tasks, for the best-performing configuration of each model architecture.\nDeletion Order - We conduct two types of feature deletions: either based on the most important features or based on the least important features. Progressively deleting the most important features can reveal the necessity of the key features to reach the baseline performance. Conversely, eliminating features that do not significantly contribute to the predictions can reduce noise in the input data, potentially enhancing model performance. This method can also identify a subset of features that are sufficient to reach the baseline performance after all extraneous features have been eliminated. The number of features deleted in each step depends on the dataset. By default, the deletion process addresses a single feature at a time. For long time-series, a larger step is used.\nCycles After training the baseline model, feature attributions are estimated using six different estimators, grouped by time-steps or bands, and the corresponding features are ranked accordingly. A new copy of the dataset is created by deleting the most or least important features, and a new model instance is trained with this modified data. The model architecture remains consistent, except for the modifications necessary to handle the new input size. For instance, removing a spectral band from the satellite modality would require adjusting the number of input channels in the first convolutional layer. Post-training, the new model is explained, and the attribution scores averaged over the selected samples are used to rank the features and decide which ones to delete in the next cycle. This process is repeated, updating the feature attribution estimates and modifying the input data after each training, until only a single (or a set of) feature(s) is left in the addressed dimension at the final cycle."}, {"title": "3.2 Incremental deletion", "content": "We apply the incremental deletion approach on each dataset using the TempCNN architecture. We evaluate the model after each cycle and report its performance results for band and time-step deletion on the validation set, as shown in Figures 1 and 2, respectively. A horizontal line indicates the baseline performance in each plot.\nIn the top row of Figure 1, the most important bands are deleted first. We observe that the performance progressively declines in the CropHarvest and CropYield datasets. The decrease in performance is more significant for the PM2.5 dataset, especially when using the SVS attribution estimators to rank the features. This observation has two implications: first, the correctness of the attributions returned by SVS exceeds those provided by the GB method; and second, the baseline performance in the PM2.5 dataset relies on the two most important features (wind speed and direction), with the remaining features being insufficient for the model to achieve the baseline performance. In contrast, in the agricultural datasets, many important features can be dropped before a significant decline in model performance is observed. This also indicates that these features are not necessary for achieving a performance comparable to the baseline.\nDeleting the least important bands, as shown in the second row of Figure 1, reveals additional insights. In the CropHarvest dataset, removing up to 70% of the least important bands does not reduce the model accuracy below 60%. In the CropYield dataset, more than 80% of the bands can be removed according to the SVS method, and the model can still recover its baseline performance. Using the same methods, the model trained on the PM2.5 dataset maintains its baseline performance even when 65% of the features are deleted. In this case, wind speed, wind direction, and humidity conditions were sufficient for achieving the baseline optimal performance.\nThe results of the time-step deletion analysis provide insights into the time periods whose absence significantly impacts model performance and those which are sufficient to achieve baseline accuracy. In Figure 2, removing the most important instances first, as shown in the top row, results in a consistent decline in performance in the CropHarvest dataset, particularly when using the SVS estimator. For the PM2.5 dataset, the R\u00b2 scores with the SVS estimator are lower than those achieved with the GB estimator, especially when more than 30% of the time-steps are removed. The behavior of the two estimators is mixed in the CropYield dataset. In all datasets, the moderate slope of the curves indicates that the information required by the model for accurate prediction is distributed across multiple instances, rather than being concentrated in a few critical time-steps.\nThe results of removing the least important instances first in the second row in Figure 2 show that the model can still perform similarly to the baseline after deleting more than 30% and 40% of the time-steps in CropHarvest and Crop Yield, respectively. In PM2.5, a performance comparable to the baseline can be achieved even when more than 80% of the instances are deleted, according\nto the feature ranks provided by SVS. Taking a closer look at the time-steps left at this point revealed that the hourly instances from the last two days were sufficient to reach a high R\u00b2 score.\nWe repeated these experiments using the ensemble-based variants. The results of the incremental deletion based on SG-SQ and VAR methods applied to each estimator are illustrated in Figures 3 and 4.\nIn Hooker et al. [9], these variants significantly improved the correctness of gradient-based attribution estimator. In our experiments, the only noticeable improvement is observed in the PM2.5 dataset when removing the most important bands first. Specifically, GB (VAR) shows a significant decline in model performance, reflecting a correct estimation of the feature importance ranking. Another noteworthy improvement is observed in the CropYield dataset, where removing the least important time-steps first, as displayed in Figure 4.(b), shows a high accuracy even in the last cycle of using the SVS (SG-SQ) method. This suggests a filtering of the least important features that is faithful to the model and its reasoning. Concretely, in this dataset, the first short wave infrared band from Sentinel-2 satellite data was sufficient for achieving baseline performance."}, {"title": "4 Conclusion", "content": "Inspired by the ROAR framework, we propose an approach to identify a small subset of bands and time-steps in geospatial temporal data sufficient to reach the model's baseline performance, i.e. the accuracy reached when providing the\nmodel with all available modalities and instances. We evaluated this approach on three datasets, and showed how many features can be removed before a significant drop in accuracy is observed. Additionally, we found that in some datasets, performance declines immediately after a few features identified as the most important are removed. This suggests that these features are necessary for the baseline performance and that the information they encode is absent in the remaining features.\nFurthermore, the expected behavior regarding the decline in performance when starting with the deletion of the most or least important features also revealed a higher correctness of the attributions estimated by SVS compared to GB. The ensemble-based variants only improved the faithfulness of GB estimates in a few cases.\nThis work can be enhanced by comparing additional feature attribution methods. The faithfulness of the chosen method enhances its ability to identify a minimal feature subset necessary to achieve the model's baseline performance. Moreover, comparing these results across various model architectures might help identify the features necessary and sufficient for predicting the target regardless of the model employed."}]}