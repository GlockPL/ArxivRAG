{"title": "MetaLLM: A High-performant and Cost-efficient Dynamic Framework for Wrapping LLMS", "authors": ["Quang H. Nguyen", "Duy C. Hoang", "Juliette Decugis", "Saurav Manchanda", "Nitesh V. Chawla", "Khoa D. Doan"], "abstract": "The rapid progress in machine learning (ML) has brought forth many large language models (LLMs) that excel in various tasks and areas. These LLMs come with different abilities and costs in terms of computation or pricing. Since the demand for each query can vary, e.g., because of the queried domain or its complexity, defaulting to one LLM in an application is not usually the best choice, whether it is the biggest, priciest, or even the one with the best average test performance. Consequently, picking the right LLM that is both accurate and cost-effective for an application remains a challenge. In this paper, we introduce MetaLLM, a framework that dynamically and intelligently routes each query to the optimal LLM (among several available LLMs) for classification tasks, achieving significantly improved accuracy and cost-effectiveness. By framing the selection problem as a multi-armed bandit, MetaLLM balances prediction accuracy and cost efficiency under uncertainty. Our experiments, conducted on popular LLM platforms such as OpenAI's GPT models, Amazon's Titan, Anthropic's Claude, and Meta's LLaMa, showcase MetaLLM's efficacy in real-world scenarios, laying the groundwork for future extensions beyond classification tasks\u00b9.", "sections": [{"title": "Introduction", "content": "Large language models have shown extraordinary zero-shot capabilities in various tasks and domains, such as text classification, summarization, question answering, and chatbot [22, 1, 25, 32, 21, 29]. Recent works [13, 5, 10] suggest exhaustively scaling the model size and training data size to improve the performance of language models and provoke their emergent abilities; for example, GPT-4, with over 1.74 trillion parameters, achieves superior performance in several tasks but also incur high economic costs. While this trend of scaling language models will continue in the future, there is also a growing diversification in recent models in terms of task or (sub-)domain specialization and computational costs. Consequently, for model users, identifying which LLM is best suited for their applications has become a daunting task. When factoring in the cost constraints, either computational resources or API service pricing, this task becomes even more complex.\nIn this paper, we imagine a world with several LLM providers, such as OpenAI2 or Amazon's Bedrock\u00b3; each provides service access to a diverse suite of LLMs with heterogeneous capabilities and cost structures. Here, an LLM user asks an important question: How do I select an LLM i (out of k LLMs) for optimal performance and usage cost in my application? One option is combining"}, {"title": "Related Works", "content": "Large Language Models. The emergence of large language models (LLMs) has fundamen-\ntally transformed several domains, including natural language processing, computer vision, and\ne-commerce, and on diverse tasks such as (zero-shot) classification, question-answering, and recom-\nmendation [20, 15, 16]. The impressive effectiveness and versatility of these LLMs have come at the\nprice of a drastic increase in LLM sizes, along with significant computational costs and data required\nto train, and expensive computational resources to perform inference with them. Consequently,\nseveral companies or services are now offering users access to LLMs with diverse capabilities and\nheterogeneous cost structures. For instance, the costs for processing 10 million tokens are $15, $30,\n$75, and $80 for Titan Lite, Cohere Command-Light, Llama 2 7B, and Claude Instant, respectively,\nusing Amazon Bedrock APIs. This abundance of API choices significantly burdens their users\nwith the decision \"which LLM should I use?\", as different LLM APIs are known for their diverse\ncapabilities for the prediction tasks [14, 18, 19].\nPrompt Optimization and Mixture of Experts. Fine-tuning is a standard option to improve the\nperformance of LLMs for specific tasks. Mixture-of-Experts (MoE) [8, 26, 7, 28] trains a routing\noperator within the large model to enhance its performance; essentially, MoE assumes the model as a\ncollection of \"experts\u201d (modules) and learn to route the input to the best expert. These approaches\nrequire training the LLMs, which is challenging for many users, while their single-LLM enhancements"}, {"title": "MetaLLM Framework", "content": "3.1 Preliminaries\nZero-shot Classification. In this paper, we employ LLMs for text classifications without additional\ntraining. Given an input sentence $x \\in X$, we create a prompt to ask a language model M for a label.\nThe model gives a correct prediction if the answer $M(x)$ matches the corresponding ground-truth\nlabel y.\nLLM APIs. We consider the case where the user has access to a set K of k different LLM APIs;\neach LLM $M_i$ has a different capability, determined by how well $M_i$ can answer a query x, and a\ncost $c_i$. For zero-shot classification, we represent the capability $a_i(x) \\in {0, 1}$ of an LLM $M_i$ on a\nsample x by comparing the answer to its corresponding ground-truth label: $a_i(x) = 1 \\text{ if } M_i(x) = y$.\nUsually, the model with a higher cost has better capability. If the user has a limited budget, they can\nchoose less expensive models for their application. In contrast, if they require better performance,\nmore expensive models will be more desirable. However, in general, the more expensive model is not\nalways the best-performing choice for all queries [12, 30]\nProblem Formulation. The goal of MetaLLM is to learn a routing function $f : X \\rightarrow K$ that\ndispatches a query x to an appropriate LLM to achieve a good response with a lower cost. For\nexample, given a subset of LLMs, $K' \\subseteq K$, that can give good responses for x, MetaLLM's objective\nis to return $arg \\text{ } \\min_{i \\in K'} C_i$. In practice, the user wants the ability to balance the performance and the\nusage cost, depending on the needs of their application. More specifically, given a budget b, the user"}, {"title": "The optimal solution of routing objective", "content": "Let $S \\in {0,1}^{N \\times k}$, $A \\in R^{N \\times k}$, where S is a matrix that has $S_{i,j} = 1$ if and only if $f(x_i) = j$,\nand $A_{i,j} = a_j(x_i)$ is the performance of the j-th API on the sample $x_i$. The objective in 1 can be\nreformulated as follows:\n$\\arg \\max_S \\sum_{i=1}^N \\sum_{j=1}^k A_{i,j} S_{i,j} \\text{ S.t. }  \\sum_{i=1}^N c_{f(x_i)} \\le b$.\nWe can relax the constraint of S to $S \\in R^{N \\times k}$ and $\\sum_{i=1}^k S_{i,j} = 1$, and its dual problem can be\nderived as\n$\\arg \\min_{\\rho \\in R, Q \\in R^N}  \\rho + \\sum_{i=1}^N Q_i$ s.t. $pc_j + q_i \\ge a_j(x_i) + 1$.\nChen et al. [3] study similar formulation for model cascading and suggest solving the dual form 3. In\nthe case where we have an exact accuracy matrix A, we can optimize 2 to find the optimal S and\nchoose the suitable API j such that $S_{i,j} = 1$.\n3.3 The Proposed MetaLLM\nWhen deploying an application, we cannot know the exact accuracy of an LLM's API on a test sample\nbefore sending the test sample through this LLM. Previous works [3, 2] learn a model to predict the\nperformance of each API. These methods cascade multiple machine learning models and query them\niteratively until the response has high confidence; consequently, they are highly expensive, especially\nwhen there are a substantial number of queries.\nIn this work, we approach this problem from a different perspective. Instead of training an accuracy\npredictor, we reformulate this problem as a multi-armed bandit. Specifically, for each input query, we"}, {"title": "The Proposed MetaLLM", "content": "define an LLM as an \u201carm\u201d, and obtain a reward expressing the performance of the LLM and the\ncost for that query. The benefit of this formulation is twofold: first, it allows the modeler to focus\non designing the reward function to capture their application needs, making the framework more\nversatile; second, we can take advantage of the extensive and well-developed research on multi-arm\nbandit, including their rigorous theoretical foundations and practical solutions. The MetaLLM\nframework is depicted in Figure 1.\nThe remaining question will discuss our design of the reward function for the zero-shot classification\ntask. Chen et al. [3] prove that if a cost scaling $p \\in R$ is the solution of 3, the routing function\n$f(x) = arg \\max_i a_i(x) - pc_i$ will be the optimal solution of 1. Intuitively, this strategy prefers the\nLLM with high performance and low-cost value. Motivated by that theoretical result, we propose the\nfollowing reward function for training the multi-armed bandit:\n$r(x,i) = a(x) - pc_i$.\nDuring training, we compute the expected reward $Q_{ji} (x; \\theta)$ of each arm j' for a training sample x\nand choose the arm j with the highest expected reward and minimize the objective\n$\\theta = arg \\min_\\theta || Q_{j} (x_i, \\theta') - (a_j(x_i) - p(c_j))||^2$.\nDuring inference, MetaLLM returns the expected reward of each arm for a query and dispatches that\nquery to the LLM with the highest reward. The training and inference algorithm for MetaLLM is\nillustrated in Algorithm 1."}, {"title": "Experiments", "content": "In this section, we provide empirical results of MetaLLM on popular APIs and benchmark datasets.\n4.1 Experimental Setup\nLLM Services. We conduct our experiments with LLMs provided by popular API services, in-\ncluding OpenAI and Amazon's Bedrock. We chose four models from OpenAI: text-ada-001, text-\nbabbage-001, text-curie-001, and text-davinci-002. These models have different costs and different\ncapabilities, giving the users diverse options. The costs of these models are shown in Table 1.\nGiven a sample SENT, we query OpenAI models with the following prompt:\nFor the sentence: SENT, is the sentiment in this sentence positive or negative?\nFor Amazon's Bedrock APIs, we evaluate MetaLLM with four different LLMs: Titan Lite, Cohere\nCommand-Light, Llama 2 7B, and Claude Instant. Since these models come from different families,\nthis evaluation setting exhibits more heterogeneity than in the setting with the OpenAI models. We\nsummarize the cost of each APIs in Table 2.\nGiven a sample SENT, we query Bedrock APIs with the following prompt:\nFor the paragraph: 'SENT', is the sentiment in this paragraph positive or negative?\nAnswer in one word.\nDatasets. We conduct experiments on two text classification datasets: SST-2 and IMDB. SST-2 is a\nbinary sentiment analysis dataset consisting of movie reviews; the task here is to classify whether a\nreview is positive or negative. This dataset has 67, 439 training samples and 872 test samples; we\nselect 100 samples from the test set to form the validation set. IMDB dataset has 25,000 training\nsamples and 25,000 test samples. We also extract 2,000 samples from the test set to build the\nvalidation set.\nTraining MetaLLM. For each input query, we utilize Sentence-BERT [23] model to extract an\nembedding vector. MetaLLM is a linear model that maps the embedding vector to the reward\nexpectation, which is optimized with the true reward 4 and mean square loss. For a budget b, we train\nMetaLLM with the scaling p five times such that the cost of MetaLLM on the validation set is not\nhigher than b and compute the accuracy of the text classification task. We normalize the cost of each\nLLM in the reward function such that the highest value is 1.\nEvaluation. To evaluate, we compute the cost and the accuracy of MetaLLM on the test set of the\nclassification task and compare them to each LLM candidate. We report the average cost per 10,000\nqueries in all experiments."}, {"title": "Performance of MetaLLM on OpenAI Models", "content": "Table 3 shows the accuracy and cost of each OpenAI LLM and MetaLLM with different p on SST-2.\nAs can be observed, the LLM with higher cost has better accuracy. Noticeably, the differences in\ncost between LLMs can be very high; for example, text-davinci-002 is ten times more expensive than\ntext-curie-001 but only has 0.4% better performance in terms of accuracy. Consequently, querying\nonly the most expensive LLM is not an optimal choice, unless the usage budget of the application is\nsufficiently high."}, {"title": "The Heterogeneous Capabilities of Different LLMs", "content": "The fact that MetaLLM can achieve better accuracy than even the most expensive OpenAI model\nmeans that an LLM's performance can vary across queries, and cannot always determined by its usage\ncost (or the size of the LLM). We provide a more rigorous analysis of this observation in this section.\nIn Figure 2, the value at position (i, j) is the number of samples that are correctly classified by the\ni-th model but not the j-th model. As we can observe, there are many queries that can be answered\nby smaller models such as text-ada-001 and text-babbage-001, while text-curie-001 and text-davinci-\n002 give incorrect answers. If MetaLLM can learn this property and determine the suitable LLM for\na query, it will boost the accuracy of using a single model. Our analysis in Section 4.6 confirms this\nhypothesis."}, {"title": "Reward Function With Dynamic Cost", "content": "The cost of an LLM is not fixed for every input; for the zero-shot classification task, it primarily\ndepends on the length of the query. We perform an experiment where we use the exact cost of each\ntraining input in the reward function 4 and train MetaLLM with different p. The results for this\ndynamic cost setting and the fixed cost are provided in Figure 3. As we can observe, using dynamic\ncosts does not lead to significant improvements, although both strategies can achieve better accuracies\nand lower costs than defaulting to a single LLM. When we set a high budget, dynamic cost setting\ncan even has lower accuracy. We hypothesize that using dynamic cost imposes greater penalties on\nlong queries, thereby encouraging MetaLLM to route them to cheaper models; these queries are,\nhowever, more complicated and may not be effectively solved by these cheaper models. Therefore,\nwe recommend training MetaLLM with a fixed cost in the reward function for every training query."}, {"title": "Performance of MetaLLM on Bedrock APIs", "content": "This section studies the scenario where the LLMs are heterogeneous, i.e., the less expensive models\ncan perform better on some queries than the more expensive ones. We perform this experiment using\nthe LLMs provided by Amazon's Bedrock and the IMDB dataset. Table 4 provides the accuracy\nand the cost of each approach, including defaulting to the same LLM (the first four rows) and our\nMetaLLM. As we can observe, when defaulting to a single LLM, a more expensive option does not\nguarantee a better performance; for example, defaulting to Claude Instant, the most expensive LLM,\nresults in lower accuracies compared to other single-LLM options.\nOn the other hand, training MetaLLM with p = 0 yields the best performance on the text classification\ntask. Setting a positive value to p decreases the cost substantially while having a minor degradation in\nthe accuracy. Notably, MetaLLM with p = 0.5 achieves better performance than the best LLM, Llama\n2 7B, but only incurs a minimal cost ( 42% less expensive). Compared to the cheapest defaulting\noption, i.e., Titan Lite, MetaLLMachieves more than 3% better accuracy while a having comparable\ncost."}, {"title": "Analysis of MetaLLM's Cost Scaling Parameter", "content": "In this section, we study the characteristics of MetaLLM with different values of p in the reward\nfunction. First, we consider the scenario where the user only optimizes for the performance (i.e.,\np = 0). Figure 4 shows the histogram or the frequency each OpenAI's LLM is selected by MetaLLM,\nseparated by the number of correct and incorrect predictions, in the SST-2 dataset. As we can\nobserve, MetaLLM indeed can learn whether an LLM can accurately answer a query. As discussed in\nSection 4.3, there exist many queries that can be correctly answered by less expensive models such"}, {"title": "Conclusion", "content": "In this paper, we study the problem of selecting a model in a set of LLMs for optimal performance\nwith cost efficiency. We propose MetaLLM that employs the multi-armed bandit algorithm to learn\nthe reward of querying the correct model with a low price. Our approach is lightweight and applicable\nto any set of off-the-shelf LLMs and thus versatile in practical use cases. Empirical results show\nthat MetaLLM can improve the accuracy of the best API by around 1% while reducing the cost of\nzero-shot text classification tasks by 60% and 40% on OpenAI and Bedrock APIs, respectively."}, {"title": "Limitations and Societal Impacts", "content": "6.1 Limitations\nAs mentioned in the previous sections, we only study MetaLLM 's framework on zero-shot text\nclassification tasks as these are important in NLP applications and increasingly utilize LLMs as the\nbase predictors. Another reason is that it is straightforward to determine the correct LLM outputs\nand set up the reward function accordingly. As our paper aims to to demonstrate the potential of the\nMetaLLM's framework, this is sufficient.\nHowever, the MetaLLM framework can be extended to arbitrary language tasks, such as as question\nanswering or text generation, by modifying the reward function to incorporate suitable metrics\nassessing the quality of the responses. Due to the complexities of designing such reward function,\nthese directions deserve independent studies. We leave them to future work.\nMetaLLMalso only trains a simple linear model whose input is the extracted feature of the query,\nwhich can ignore more fine-grained features. Building a more complex reward model and utilizing\nother information from the query, such as the domain of the input and the demand of the user, may\nfurther facilitate better the needs of the applications and improve the performance of MetaLLM.\nFinally, we optimize MetaLLM with two values in the reward function: the performance and the cost\nof querying the API. However, several aspects to evaluate the model in practice could be incorporated\ninto the reward, such as the inference time, the robustness of the model, emergent abilities, or even\nthe information on the training distribution. Combining those factors can help build a more powerful\nand reliable AI system for diverse purposes.\n6.2 Societal Impacts\nLarge language models, with their emergent abilities, have transformed our lives by assisting in several\ntasks. However, large models come with high inference costs; therefore they are very expensive to\ndeploy and may cause harmful effects to the environment by high power consumption. Our framework\nhelps reduce the cost of querying LLMs substantially by routing the input to cheaper models that\nmay return the correct answer and can even improve performance by utilizing the combination of\nmany LLMs. MetaLLM is applicable to any set of off-the-shelf LLMs, being useful for future AI\nsystems with more modern language models."}]}