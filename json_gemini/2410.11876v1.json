{"title": "Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating Privacy Trade-offs in LLM-Based Conversational Agents", "authors": ["JIJIE ZHOU", "ERYUE XU", "YAOYAO WU", "TIANSHI LI"], "abstract": "The proliferation of LLM-based conversational agents has resulted in excessive disclosure of identifiable or sensitive information. However, existing technologies fail to offer perceptible control or account for users' personal preferences about privacy-utility tradeoffs due to the lack of user involvement. To bridge this gap, we designed, built, and evaluated Rescriber, a browser extension that supports user-led data minimization in LLM-based conversational agents by helping users detect and sanitize personal information in their prompts. Our studies (N=12) showed that Rescriber helped users reduce unnecessary disclosure and addressed their privacy concerns. Users' subjective perceptions of the system powered by Llama3-8B were on par with that by GPT-4. The comprehensiveness and consistency of the detection and sanitization emerge as essential factors that affect users' trust and perceived protection. Our findings confirm the viability of smaller-LLM-powered, user-facing, on-device privacy controls, presenting a promising approach to address the privacy and trust challenges of AI.", "sections": [{"title": "1 INTRODUCTION", "content": "The emergence of large language models like OpenAI's GPT [36], Google's Gemini [17], and Meta's Llama [48] has enabled LLM-based conversational agents that understand the context and can generate human-like responses. As products with high capacity for handling natural language tasks and user-friendly interfaces, such as ChatGPT, come into play, people increasingly rely on them for a variety of tasks in both personal and professional contexts [5, 10, 55]. Consequently, a large amount of personal or sensitive disclosure has been observed in users' conversations with ChatGPT-like LLM-based conversational agents [30, 55], which leads to severe privacy threats in the event of database breaches and data extraction attacks targeting the memorization vulnerabilities of LLMs [8, 9, 33, 53]. This includes personally identifiable information (PII) like names and email addresses, as well as non-PII sensitive topics such as sexual preferences and drug use [30]. Notably, parts of the disclosure are unnecessary to the main task. This includes information that is too detailed: \"Help me generate a one-day trip in NYC, I live at 153 W 57th St, New York, NY 10019\"; or irrelevant information: \u201cPlease help me proofread the following email to my colleague peter (peter.parker@spider.com)\u201d.\nThe excessive and sensitive disclosure issues pose a challenge to the classic data privacy principle, data minimiza-tion [15, 37, 41]. Although researchers and practitioners have employed data sanitization [14, 35, 38] and private cloud computing techniques [1] to achieve the goal of removing PII at the training stage and avoiding unnecessary data retention at the inference stage, there are several inherent limitations of the techniques because of the lack of involvement of the end-users. First, the privacy and utility needs depend on contexts and vary from person to person, which means the post-hoc data sanitization may not remove all the information the user found sensitive and unnecessary. Second, once the data leaves the device, the users will have no direct control over their information. Since the existing techniques operate in a way that is concealed from users, it leads to difficulty for users in establishing correct mental models of the protection they offer, reducing users' privacy concerns, and increasing users' trust in using LLM-based conversational agents for handling personal and sensitive information. These issues reveal a research gap in designing user-friendly tools that enhance users' ability to navigate privacy risks and minimize unnecessary information disclosure, specifically enabling user-led data minimization.\nPrior research has emphasized the trade-offs among privacy, utility, and convenience as a major challenge to achieving privacy-preserving disclosure behaviors to LLM-based conversational agents [55]. This is especially true when it comes to the excessive disclosure issue. First, one cause of excessive disclosure is that users paste in texts that accidentally"}, {"title": "Rescriber: Smaller-LLM-Powered User-Led Data Minimization for Navigating Privacy Trade-offs in LLM-Based Conversational Agents", "content": "include semantically unrelated information, which the user did not realize or found too cumbersome to remove. Zhang et al. [55] found that users sometimes manually sanitize inputs, but the manual actions fall short due to forgetfulness and tediousness. They also found that users were pessimistic about preserving privacy while reaping the benefits of AI, and users were neither capable of identifying all the opportunities associated with minimizing excessive sensitive disclosures nor willing to invest in efforts and time to do that [55]. Hence, we argue that a tool that supports user-led data minimization should act in a way that maintains response quality and incurs minimal overhead.\nIn this work, we make an initial foray into designing user-led data minimization support for LLM-based conversational agents to tackle the aforementioned challenges. Specifically, we aim to design a browser extension that leverages LLMs to streamline the process of identifying and sanitizing unnecessary disclosures of personal information, as prior research suggests LLMs significantly outperform traditional NLP methods for detecting PII [7]. There are several key challenges in achieving this goal. First, the accuracy of detection and sanitization recommendations is a primary requirement of effective support for user-led data minimization [54]. According to the neural scaling laws [42, 51], using larger models increases the accuracy, while it also imposes higher computational costs and raises the bar of the user's device for deploying the system locally. Furthermore, it is difficult to determine the sweet spots of how to leverage large language models to achieve the ultimate goal of empowering users to staying in control over their data while maintaining the response utility. Lastly, it is unclear how different approaches to data minimization affect users' perceptions and behaviors about privacy. Formally, we aim to investigate the following research questions on core efficacy, adoption intention, and impact on users:\nRQ1 How can we design a tool to support effective data minimization and reduce users' privacy concerns?\nRQ2 How can we design a tool for user-led data minimization that users trust and intend to use?\nRQ3 How does the support for user-led data minimization affect users' privacy awareness and behaviors?\nWe present Rescriber, a browser extension for Chrome, which detects and highlights potential personal information disclosures, and then provides users with options to redact or abstract these details before sending a message. Redaction replaces personal terms with entity placeholders (e.g., \"Alex\" becomes \"[NAME1]", "my colleague\u201d). When the address is highlighted as personal information in \"Help me generate a one-day trip in NYC, I live at 153 W 57th St, New York, NY 10019\", we could abstract the information and the message would turn into \"Help me generate a one-day trip in NYC, I live near Central Park": "this way, the contextual information still exists, but with a level of details that would not expose too much of the user's private info. Likewise, when the email address \"peter.parker@spider.com\" is highlighted and suggested by the tool to be replaced, users could directly redact it with a placeholder like \"[EMAIL1]"}, {"title": "2 RELATED WORK", "content": "2.1 Privacy issues in LLMs and LLM-based Conversational Agents\nLLM-based conversational agents (CAs) present two main types of privacy threats [55]. The first involves traditional security and privacy risks, such as data breaches and the sale of personal information. The process of user interaction with LLM-based CAs are also vulnerable to cyberattacks, data leaks, or ransomware threats [20]. The second type of threat is specific to LLMs and concerns the issue of memorization risks [8, 9, 33, 53]. Research has shown that LLMs can inadvertently retain and reproduce precise details from their training datasets. The memorization issue poses significant privacy and security concerns, especially when the training data includes sensitive or personal information. The cost of exploiting this vulnerability is relatively low. For example, research has found that by prompting the model to continuously output \"poem,"}, {"title": "2.3 User Agency in Privacy Protection", "content": "\"Privacy as control\u201d is an essential conception of privacy that has deeply influenced privacy laws in many regions, such as the U.S. and the EU [50]. In usable privacy research, designing user-friendly privacy control features to increase users' agency in privacy protection has been an important research topic. Sharma et al. [45] explore the concept of user-controlled data minimization in the context of search engines, emphasizing the critical role of end users in managing their own data privacy. It shows that enabling user agency through customizable and transparent data minimization features can significantly enhance privacy protection in search engines. Many studies demonstrate an evolution from simple manual controls to sophisticated, user-centric approaches in privacy protection design, incorporating tangible solutions, perceptible assurance, AI-facilitated method and accessibility considerations. Do et al. [13] introduced a Smart Webcam Cover for tangible, assisted privacy control. Smart speakers implemented wake-up words as a user-initiated mechanism. Do et al. [12] developed a battery-free, wireless microphone powered by intentional user interactions, providing perceptible assurance. Zhang et al. [54] explored AI-facilitated data sanitization for photo sharing among visually impaired users. Notably, many of these approaches primarily offer coarse-grained, binary control over data collection, which potentially limit users' ability to control their privacy preferences.\nOur work investigates the challenges of increasing user agency over their privacy specifically in the context of LLM-based CAs, such as ChatGPT. This domain of applications have caused unique challenges to privacy agency due to the intense tensions among privacy, utility, and convenience [55]. To tackle this challenge, we leverage the power of large language models and present a more refined, user-centered approach to data protection, offering users granular, real-time control over their privacy settings during interactions. Chong et al. [11] developed a system that also aimed at prompt sanitization using web-based LLMs. Their work focused on detection and sanitization techniques, while our work further contributes to the design of a user-facing tool and user studies that provide in-depth insights into the design considerations, as well as measuring the end-to-end performance of users-led data minimization."}, {"title": "3 THE RESCRIBER SYSTEM", "content": "3.1 Overview\nWe designed and implemented a Chrome extension prototype, Rescriber, to assist users in minimizing data when chatting with LLM-based conversational agents like ChatGPT. The tool makes privacy-related data minimization visible to users without interfering with their regular use of GPT. Figure 2 illustrates the system's overall workflow and when it comes into play during user interactions with GPT.\n3.1.1 Design Goals. The design and main functionality of Rescriber are guided by three design goals, as detailed below.\nD1: Perceptible privacy protection. Existing solutions to privacy risks of LLMs [1, 14, 35, 38] face challenges in building trust with users, as their effects are often invisible and users lack assurance about whether they prevent data leakage. To address this, our tool tries to resolve the privacy issue at its source by using a \"what-you-see-is-what-you-get\" model.\nD2: Protecting privacy without compromising utility and convenience. We want users to adopt the tool and develop a habit of using privacy-preserving techniques to protect themselves. At the same time, we aim to maintain the utility of GPT's responses, ensuring that users' messages-after being sanitized with the help from our tool-still lead to high-quality responses. Additionally, the tool should sanitize messages without compromising response effectiveness or adding extra burden, as convenience is a key factor in ChatGPT use and often leads to excessive data disclosure [55].\nD3: Flexible control over information sanitization. Privacy concerns and preferences are nuanced, context-sensitive [34], and vary from person to person [27]. Therefore, our tool needs to offer rich and flexible support in reducing the specificity and amount of information shared. The flexible control plays an important role in increasing users' agency and gaining users' trust in the tool. Additionally, providing granular control also helps users better minimize unnecessary disclosure by striking a balance between privacy and utility.\n3.2 Example Usage Scenario\nWe present a hypothetical example to illustrate a typical use case of Rescriber (also illustrated in Figure 2). Nova, a college student, is participating in a study abroad program, taking classes in South Korea. She is getting acquainted with some local students. On a Friday night, some upperclassmen invite Nova and her friends to a party, where drinking might be part of it. Nova is unsure how to navigate the situation, especially because her friend Jennie cannot consume alcohol. With limited knowledge of how locals typically handle such situations, Nova believes GPT, trained on a wide range of data, could help her draft a message for the situation. Nova types a brief description, informing GPT that both she and Jennie are 20 years old, meaning they can legally drink in Korea, where the legal drinking age is 19. However, Jennie is allergic to alcohol, making it tricky for them as newcomers. Before sending, Rescriber automatically flags personal and sensitive information, such as Jennie's name, their ages, and Jennie's health condition. Initially unaware"}, {"title": "3.3 Detection", "content": "3.3.1 Process. Detection is achieved by prompting an LLM using a pre-defined taxonomy of personal information that could potentially be identifiable or sensitive, as detailed in Section 3.3.2. Results are formatted as a JSON object, listing all the detected entities and their data type categories (e.g., NAME). These categories serve as the basis of the placeholders for future replacements. If multiple entities of the same category exist in a conversation, the system adds a number to each placeholder (e.g., NAME1). The mapping between the entity and placeholder of each conversation is stored locally, with users retaining full control over the stored data, which they can delete anytime. A magnifier icon signals successful detection at the bottom right of the webpage. By clicking it, users can view detected entities highlighted in yellow in a tooltip or review them along with placeholders in a control panel.\n3.3.2 Taxonomy and Prompt Design. The prompt we used for detection was designed based off Bubeck et al. [7]. We made two main enhancements. First, we added the instruction to avoid detecting public information that matches a certain PII entity. For example, celebrity names (e.g., Taylor Swift) usually are not private information and should not be detected. However, celebrity names should be detected when they are used in private contexts (e.g., Sam Altman sends an email to his colleague). Therefore, our instruction defines the criteria as \"information should be related to a real person not in a public context, but okay if not uniquely identifiable\u201d. Another enhancements is that we included more data types about sensitive personal experiences in addition to the conventional PII types in our taxonomy (Table 1). We include health information, financial information, and educational records, as they are three crucial data types regulated by sectoral consumer privacy laws in the U.S. The prompt is documented below.\n[System:] You are an expert in cybersecurity and data privacy. You are now tasked to detect PII from the given text, using the following taxonomy only: {taxonomy omitted}\nFor the given message that a user sends to a chatbot, identify all the personally identifiable information using the above taxonomy only, and the entity_type should be selected from the all-caps categories.\nNote that the information should be related to a real person not in a public context, but okay if not uniquely identifiable.\nResult should be in its minimum possible unit.\nReturn me ONLY a json in the following format: {\"results\": [{\\\u201centity_type", "text": "PART_OF_MESSAGE_YOU_IDENTIFIED_AS_PII}]}\n[User:] {currentMessage}"}, {"title": "3.4 Data Minimization Support Design", "content": "For the detected sensitive entities, Rescriber offers two data minimization methods: replacement and abstraction (D1). Instead of removing all the identifiable information, which could lead to low utility, the two options help substituting detected data with their non-personal or less-personal equivalents. The replacement method enables easy batch actions to replace sensitive information with the corresponding placeholders (D2), which is suitable when the detected information is not semantically essential to the task. The abstraction method, inspired by Dou et al. [14], provides a middle ground for users to rewrite detailed information into a more general version while still maintaining some context. It is suitable when the detected information is relevant to the task but overly detailed. The two options, targeting different data minimization needs, offer users with flexible control on the granularity of information sharing (D3).\n3.4.1 Replacement. At the end of the detection step mentioned in Section 3.3, each detected item is assigned with a unique placeholder label. If the user opts for replacement, the tool substitutes all instances of the selected entities with their corresponding placeholders in the message, with changes directly applied to the input box. One tricky situation is when some detected entity is a substring of another, e.g., 15 (age) and 2015 (year). To avoid replacing the entity with the wrong placeholder, Rescriber follows a descending order of detected entity length, ensuring that longer terms are replaced before shorter ones.\nWrite-back. To increase the response readability and also facilitate copying the answer (e.g., email reply), Rescriber automatically detects the placeholders generated by the tool and writes the original entity back in the response (D2).\n3.4.2 Abstraction. Rescriber designs the abstraction mode to ensure the sanitized version not only maintains high utility but also keeps the surrounding text unchanged, making all the changes more perceptible (D1) and controllable (D3) for users. If the user uses abstraction, based on the current selections, the system would send a request to the LLM, asking it to rewrite the current message, abstracting all selected protected information while leaving the rest intact. The rewritten message then replaces the original text in the input box. The abstraction prompt is documented below.\n[System:] Rewrite the text to abstract the protected information, and don't change other parts, directly return the text in JSON format: {\"text", "REWRITE_TEXT}\n[User": "Text: {currentMessage}\nProtected information: {userSelectedAbstractionEntityList.join(\", \")}"}, {"title": "3.5 Implementation", "content": "Rescriber is a chrome extension that consists of a frontend that displays the results and handles the user interactions, and a backend server that hosts the LLM for detection and abstraction. According to the neural scaling laws [42, 51], using larger models increases the accuracy. This suggests a trade-off between performance and on-device deployment feasibility. To study this trade-off in terms of user experience, data minimization efficacy, and users' trust, we implemented two versions of the tool, Rescriber-Llama3-8B and Rescriber-GPT-40, with the same frontend while different backend designs. Rescriber-Llama3-8B,powered by a smaller model that can run on consumer devices (e.g., Mac mini with Apple M2 chip), is representative of the experience of on-device data minimization support that can be achieved with current technology. Meanwhile, Rescriber-GPT-40, powered by a state-of-the-art commercial model, represents an \"upper-bound\" performance in terms of accuracy and time latency. We detail the implementation of the two versions below.\n3.5.1\nRescriber-Llama3-8B . We serve the Llama3-8b (4-bit quantized version) model using the Ollama framework\u00b9, which is a lightweight, extensible framework for building and running language models on the local machine. Note that we chose to set up the Ollama server on a Google Cloud Platform virtual machine to relax the requirements of our par-ticipants' devices and achieve consistent performance during the studies. According to our performance benchmarking results (Section 3.5.3), this model can be deployed locally on consumer computers. The model's temperature was set to 0 (greedy sampling) to ensure deterministic detection and abstraction results.\n3.5.2\nRescriber-GPT-40 . We use the OpenAI API endpoints to implement this version of the system. The temperature was set to 0. However, GPT-40 model is known to be non-deterministic even when the temperature = 0. This is caused by its use of Sparse MoE. This is an inherent limitation of the model's architecture that turns out to affect users' preferences as we later discovered from the user studies.\nClustering. Despite effective PII detection, we identified issues related to clustering variants of the same entity. For example, in pilot testing, the extension successfully masked \u201c[first name] [last name]\" but failed to replace \"[first name], [last name]\", resulting in a suboptimal user experience. This highlights that partial masking may create a false sense of security or heighten discomfort if users become aware of incomplete data minimization. To address this, we further built in a same-entity clustering component in the Rescriber-GPT-40. The Rescriber-Llama3-8B, due to its smaller size and limited capabilities, skips clustering because it is resource-intensive and time-consuming.\n3.5.3 Performance benchmarking. We conducted a small experiment to benchmark the accuracy and time performance of the two models to establish an objective comparison of the two models' performance. This serves as a reference to users' subjective preferences and data minimization actions enabled by the extension in our user studies. Our evaluation dataset comprises 240 randomly selected samples from Ai4Privacy's world's largest open dataset for privacy [2], which encompasses 229 discussion subjects and use cases. These samples were processed by aligning the 54 PII classes from the dataset with the categories in our taxonomy (Table 1). To ensure accuracy, two authors independently reviewed and verified the privacy information labels and the corresponding text. Their consensus was used as the ground truth. The experiments were conducted on a Mac Mini with an Apple M2 chip, featuring an 8-core CPU, 10-core GPU, 16-core Neural Engine, and 24 GB of unified memory. We evaluated the models based on three key metrics directly related to user experience: Precision, Recall, and Response Time. Precision represents the ratio of correctly identified sensitive entities to the total number of sensitive entities that the model outputs. Recall measures the ratio of correctly identified sensitive entities to the total number of sensitive entities present in the ground truth data. Response Time refers to the time taken by the model to process a request and return a response, reflecting the model's efficiency in handling and delivering results. Table 3 shows that Rescriber-GPT-40 has higher precision and recall than Rescriber-Llama3-8B. It is important to note that Llama3 model's lower instruction-following capability compared to GPT-40 often results in cases where there are minor discrepancies in entity types, but the corresponding text is accurate. We considered them as a correct detection in our evaluation. Table 4 shows that Rescriber-GPT-40 is consistently faster than Rescriber-Llama3-8B, with the latter taking up to 32.2 seconds in extreme cases. However, with a 10 confidence interval of 4.74 to 9.64 seconds, Rescriber-Llama3-8B remains within an acceptable range for users, making it a viable alternative.\""}, {"title": "4 METHODOLOGY", "content": "We conducted twelve semi-structured online interviews with participants from the U.S. The study has been approved by the IRB of our institution. We detail the study methodology below.\n4.1 Participants\nParticipants were recruited through Prolific, an online research recruiting website. The recruitment message for the screening survey did not mention privacy. We employed screening criteria that required participants to have some experience using ChatGPT and privacy concerns related to its use. These criteria ensured that participants had related experiences that could benefit from Rescriber, and could provide real-world prompts for the testing. Of the 275 participants who completed the initial screening survey, 105 expressed privacy concerns and willingness to participate in a 1-hour follow-up remote interview study. Participants were compensated $20 USD. We randomly selected 28 of these participants and distributed a pre-study survey for interview signup and preparation. Ultimately, 12 participants successfully completed the interviews.\nAccording to their self-reported data, all participants were fluent English speakers. The age distribution was as follows: 3 were between 18-24 years old, 6 were between 25-34 years old, 1 was between 35-50 years old, and 2 were over 50 years old. Among those interviewed, 6 used ChatGPT several times a day, 2 used it less than once a week, another 2 used it several times a week, and 2 used it once a day. All twelve participants had concerns about exposing personal information (e.g., demographic information, health details, educational records) to the model, and 11 self-reported to have tried to minimize unnecessary information sharing with ChatGPT.\n4.2 Study Design\nOur study design involves four sessions, consisting of having two conversations with GPT using our extension powered by two different back-end models. The two conversations included one participant's shared example, while the other one used a hypothetical scenario prepared by the research team, addressing common use scenarios inspired by the ShareGPT dataset [40]. Each participant was assigned one of three hypothetical scenarios. The three scenarios include reading and responding to an email, performing data analysis on a dataset, and writing a thank-you letter to a therapist post-treatment.\nThe study followed a counterbalanced design to reduce the order effects. Participants were divided into groups, with half testing the Rescriber-GPT-40 (referred to as Model 1) first and then the Rescriber-Llama3-8B (referred to as Model 2), and the order of using their own example was also alternated. They always started by testing one example (their own example or the hypothetical example) on two models sequentially, and then switched to testing another example on two models in the same order. This design allowed the participant to make direct comparison between the experiences with the two models on the same example.\n4.3 Interview Procedure\nDuring the interviews, participants were provided with an information sheet detailing their rights, ensuring they shared only what they were comfortable with. We also obtained their consent to record the session for note-taking and post-interview data analysis. Once recording commenced, we asked them to introduce the typical ChatGPT use cases, their privacy concerns and data minimization experiences (if any)."}, {"title": "4.4 Qualitative Analysis Method", "content": "We conducted a bottom-up qualitative coding analysis using affinity diagramming [3]. All the interview transcripts were imported into a Figjam, a online collaborative whiteboard. In the first stage, we selected four interviews, and have four researchers review the recordings, and wrote interpretive memos around notable excerpts relevant to our research questions. The researchers meet twice every week to collectively conduct affinity diagramming to develop clusters of the memos. At the end of this stage, the four researchers collectively refined the clustering results and created a name for each cluster to form an initial codebook. In the next stage, the rest of the eight interviews were evenly assigned to the four researchers so that each interview was reviewed by two different researchers, who used the initial codebook to code and suggest changes based on emerging patterns. All four researchers held regular meetings to discuss and refine the codebook together. The final codebook can be found in Appendix B. We did not calculate inter-rater reliability because the goal of this analysis is to identify emergent themes rather than to seek agreement, following the recommendations of McDonald et al. [28]."}, {"title": "4.5 Methodological Limitations", "content": "There are several methodological limitations that need to be considered when interpreting the results. First, although our participants shared many real-world use cases that could benefit from our tool, including legal inquiries about family matters, planning travel or medical arrangements, creating 401(k) distribution plans, and drafting emails for seeking lab opportunities, many participants felt uncomfortable sharing a prompt they used in these cases with the researchers in a recorded session, and chose to share a different prompt which contained less personal information. This could influence their perceptions of the usefulness of the tool. It is noteworthy that our three hypothetical test cases covered several of the use cases participants mentioned, such as proofreading (E1) and providing a dataset and asking GPT to perform data analysis (E2), which helps alleviate this issue. Another limitation is due to the nature of the controlled usability study that we ran, which makes the users interact with the tool under the observation of the researcher. The participants may be affected by the Hawthorne effect and tend to speak in favor of our tools. To address this issue, we created two versions of the system and referred to them as Model 1 and Model 2, so as not to inform users which one was the main evaluation target. We also constantly encouraged them to voice negative comments."}, {"title": "5 QUANTITATIVE RESULTS", "content": "All 12 participants successfully completed every task. We examined key metrics such as the number of attempts made per participant for each example, participant satisfaction, and sanitization efforts (replacements and abstractions) to compare the overall performance of the models. There was one execution error of the counterbalancing settings though: For P6, the interview was intended to start with Rescriber-Llama3-8B followed by Rescriber-GPT-40, but the moderator mistakenly began with Rescriber-GPT-40.\n5.1 Data Minimization Efficacy\nWe allowed the participants to experiment with each example multiple times. For the three hypothetical examples (E1-E3), each participant made 1.1 attempts per example on average. For the user's own example (E4), the average number of attempts was 1.4, showing that users explored different ways to sanitize their prompt using Rescriber a bit more than in the three examples provided by us.\nWe first evaluate the efficacy of Rescriber for helping with data minimization, measured by the amount of reduced disclosure in attempts that still resulted in a satisfactory response (RQ1). The results are summarized in Table 5. Among all attempts, 84% led to a satisfactory response. Among the satisfactory responses, users reduced more unnecessary disclosure in the examples provided by us (E1-replying email: 3.7, E2-data analytics: 30.0, E3-writing a letter: 2.8) than their own example (E4: 1.3). Note that E4 is a collection of unique examples provided by each participant. The less"}, {"title": "5.2 Subjective Preferences", "content": "Table 6 summarizes the analysis results of users' subjective ratings of the two versions of Rescriber after usage. The four questions are related to the reduction of unnecessary personal disclosure, perceived reduction of personal disclosure, fewer privacy concerns, and intention to use the tool, rated using a 5-point Likert scale (higher is better). The median ratings of participants' perceptions of the two models were identical for the reduction of unnecessary disclosure and intention to use and were close for the other questions (with Rescriber-Llama3-8B slightly lower). We further performed the Wilcoxon signed-rank test to compare the subjective ratings between the two models and found no statistically significant difference between the two models for all four questions. This suggests that participants viewed both models as relatively equivalent in terms of minimizing unnecessary disclosure, alleviating privacy concerns, and the desire to continue using the tool."}, {"title": "6 QUALITATIVE RESULTS", "content": "We summarize the qualitative analysis results, including users' privacy concerns with ChatGPT, their natural data minimization strategies and the limitations (RQ1); factors that affect users' trust in Rescriber's ability to enhance privacy (RQ2); factors that affect users' intention to use Rescriber (RQ2); and the data minimization strategies in reaction to the tool, as well as the educational effect and learning curve (RQ3)."}, {"title": "6.1 Privacy Concerns, Yet Why Still Using ChatGPT?", "content": "We start by examining privacy concerns related to ChatGPT, how users address them, and why they continue using it. This reveals factors influencing users' decision on disclosure and sanitization when using ChatGPT, reinforcing the need for user-driven data minimization.\n6.1.1 Concerns with identifiability (P1, P2, P3, P5, P6, P8, P9, P11, P12). A prominent theme was users' concerns with the risk of identifiability when sharing information with ChatGPT. Participants expressed concerns about various data types that could potentially reveal their identities. One of them was personal identifiers, such as names (P1, P2, P3, P6, P8, P9, P12) and contact information (P1, P2, P8, P11). P3 emphasized the importance of names when trying the extension on the hypothetical email reply example (E1), stating, \u201cdemographics are pretty important", "the most important thing to abstract is the names.\"\nConcerns about identifiability extended beyond direct identifiers to include demographic details like gender, raising fears of linkage attacks [49": "and broader profiles emerging from multiple data points. P5 observed, \u201cI think that being 6 foot 3 and weighing 200 pounds are identifiers that could be more general. But when"}]}