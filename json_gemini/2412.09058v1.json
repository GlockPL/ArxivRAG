{"title": "EmbedGenius: Towards Automated Software Development for Generic Embedded IoT Systems", "authors": ["Huanqi Yang", "Mingzhe Li", "Mingda Han", "Zhenjiang Li", "Weitao Xu"], "abstract": "Embedded IoT system development is crucial for enabling\nseamless connectivity and functionality across a wide range\nof applications. However, such a complex process requires\ncross-domain knowledge of hardware and software and hence\noften necessitates direct developer involvement, making it\nlabor-intensive, time-consuming, and error-prone. To ad-\ndress this challenge, this paper introduces EmbedGenius,\nthe first fully automated software development platform\nfor general-purpose embedded IoT systems. The key idea\nis to leverage the reasoning ability of Large Language Mod-\nels (LLMs) and embedded system expertise to automate the\nhardware-in-the-loop development process. The main meth-\nods include a component-aware library resolution method\nfor addressing hardware dependencies, a library knowledge\ngeneration method that injects utility domain knowledge\ninto LLMs, and an auto-programming method that ensures\nsuccessful deployment. We evaluate EmbedGenius's perfor-\nmance across 71 modules and four mainstream embedded\ndevelopment platforms with over 350 IoT tasks. Experimen-\ntal results show that EmbedGenius can generate codes with\nan accuracy of 95.7% and complete tasks with a success\nrate of 86.5%, surpassing human-in-the-loop baselines by\n15.6%-37.7% and 25.5%-53.4%, respectively. We also show\nEmbedGenius 's potential through case studies in environ-\nmental monitoring and remote control systems development.", "sections": [{"title": "1 INTRODUCTION", "content": ""}, {"title": "1.1 Background and Motivation", "content": "Embedded IoT systems support numerous applications, with\nthe market projected to reach $258.6 billion by 2032 [48].\nThese systems play key roles across various industries, in-\ncluding healthcare [46, 55], agriculture [20, 36, 56], and smart\ncities [51, 52]. For example, in a smart city [40, 49], embedded\nsystems control street lighting and traffic signals based on\nsensor data to optimize energy use and traffic flow.\nDeveloping such systems straddles the hardware-software\ninterface, demanding a cross-domain understanding of how\ndevices interact with the physical world [31]. Specifically,\nthis intricate process involves several labor-intensive steps.\nInitially, developers must manually address dependencies by\ninstalling and configuring the essential libraries for hardware\nmodules. Next, they write code that defines how the system\nbehaves, considering the functions and attributes of various\nsensors, actuators, and communication modules. Afterward,\nthe code is compiled, which often reveals configuration or\ndependency errors requiring further refinement. Once the\ncode is successfully compiled, it needs to be uploaded to the\ndevelopment platform for testing and deployment. Currently,\nthe development of embedded systems predominantly relies\non manual processes using traditional Integrated Develop-\nment Environments (IDEs) (e.g., Eclipse, Keil \u00b5Vision, and\nIAR). However, the variety and complexity of programming\ntools lead to high learning and development costs for devel-\nopers [7, 50]. Additionally, the diversity of hardware modules\n(e.g., sensors, displays, and communication modules) further\nincreases the complexity, making manual development even\nmore challenging. In summary, the traditional embedded\nIoT system development process is labor-intensive, time-\nconsuming, and error-prone.\nThis paper introduces a novel automation approach that\nleverages Large Language Models (LLMs) to streamline em-\nbedded IoT system development. Building on the recent suc-\ncess of LLMs (e.g., GPT-4 and Claude-3.5), we aim to harness\ntheir capabilities to simplify the development process by\nautomating processes such as dependency-solving, coding,\nand deployment. Although there are some existing coding\nautomation tools, such as those utilizing LLMs for general\ncoding tasks [12, 42, 69], they only provide partial relief by\nassisting in tasks like code generation and debugging. For\ninstance, CODET [12] aids in generating code with built-\nin tests, while LDB [69] verifies runtime execution step by\nstep to handle complex logic flow and data operations. How-\never, they fall short in driving specific hardware devices (e.g.,\nmicrocontrollers and sensors) due to a lack of hardware-\nspecific knowledge in embedded systems, such as peripheral\ninterface configurations and library dependencies. Moreover,"}, {"title": "1.2 Challenges and Contributions", "content": "We need to tackle several key challenges to achieve the afore-\nmentioned goal.\nChallenge 1: Diversity in Hardware Dependency. Var-\nious hardware components, such as communication modules,\nsensors, and displays, are built on different architectures and\nperform specialized tasks. Each component relies on spe-\ncific library dependencies to function effectively, leading to\nunique challenges in dependency resolution. As noted on\nthe Arduino library website [4], there are over 7,000 libraries\nacross diverse categories, including 1,527 communication\nlibraries, 1,285 device control libraries, 1,435 sensor libraries,\nand so on. Therefore, accurately identifying and selecting\nthe essential libraries for different hardware components\nis a significant challenge. To address this issue, we explore\nthe fundamental principles of library selection, revealing\nthat different libraries exhibit distinct compatibility with\nspecific models and varying support for development board\narchitectures. Additionally, we find that a high version iter-\nation frequency suggests active maintenance and ongoing\nimprovements, reflecting the library's stability and function-\nality. Based on these findings, we propose an automated\ndependency solving method that can efficiently identify the\nmost suitable libraries for specific hardware components.\nChallenge 2: Lack of Library Knowledge. Embedded\nsystems development requires specialized library knowledge,\nwhich standard LLMs are not inherently equipped to handle.\nMoreover, the diversity of libraries across different modules\n(e.g., Adafruit SSD1306 library for OLED displays or the Fas-\ntLED library for LED strips [3, 5]) and the variation in their\nusage further complicate the issue. As shown in Fig. 2, dif-\nferent libraries include numerous source and example files\nthat enhance developer understanding by providing exten-\nsive API definitions and usage examples. This underscores\nthe need for a robust method to enhance LLMs with library\nknowledge. Specifically, different libraries often have distinct\nAPIs, each with varying parameters, return values, and usage\npatterns. Correctly interfacing with these libraries requires a\ndeep understanding of their specific designs. To address this\nchallenge, we propose a knowledge generation method that\nextracts and injects library API and utility knowledge into\nthe LLM's memory, enabling syntactically and contextually\nappropriate programming solutions.\nChallenge 3: Complexity of Embedded System Pro-\ngramming. Programming is a crucial part of embedded sys-\ntems development, involving coding, compiling, and flashing,\nwith each step requiring precise execution [8, 25]. Fig. 3 illus-\ntrates the differences between general-purpose programming\nand embedded system programming. In general-purpose\nprogramming, the workflow typically involves coding, de-\nbugging, and deployment. In contrast, embedded system\nprogramming introduces additional steps such as compiling\nand flashing, which require specialized configurations and\nare particularly error-prone. For example, using an incorrect\nmicrocontroller configuration or selecting an incompatible\nlibrary can result in compilation failures, while coding errors\nmay lead to functional verification failures after flashing. To\ntackle this challenge, we propose an automated programming\nprocess that incorporates two nested reasoning and acting\nfeedback loops: a compile loop and a flash loop. Specifically,\nthe compile loop ensures that the correct configurations\nand libraries are applied before each flashing cycle, while\nthe flash loop verifies that the embedded system functions\naccurately and completely as described in the user tasks.\nBy incorporating the above solutions, we design and imple-\nment EmbedGenius, a comprehensive framework that fully\nautomates the dependency-solving, programming, and de-\nployment processes for embedded system development. This\nadvancement lowers the barrier to entry for embedded sys-\ntem development, pushing it into practical, real-world appli-\ncations. Our extensive evaluation, involving over 70 hard-\nware modules, four development platforms, and over 350 IoT\ntasks, demonstrates that EmbedGenius achieves an average"}, {"title": "2 PRELIMINARIES", "content": ""}, {"title": "2.1 Embedded System Development", "content": "Assume an embedded system $E_s = g(D, M)$, where $D$ repre-\nsents the development platform, and $M$ refers to the con-\nnected modules (e.g., sensors, communication modules, and\ndisplays). The system $E_s$ is the result of combining $D$ and\n$M$, forming the foundation for development. The pipeline of\nembedded system development is shown in Fig. 4.\nCompiler Setting. The first step in the development process\nis configuring the settings within the development environ-\nment to align with the specific characteristics of the target\nsystem $E_s$. This configuration process can be formulated as\n$C_h = f(E_s, P)$, where $C_h$ represents the configured hardware\nsettings, and $P$ refers to the hardware-level configuration\nparameters (e.g., pin mappings, target platforms).\nSolving Dependencies. Following the setting, solving de-\npendencies of $E_s$ becomes crucial to ensure smooth inte-\ngration and functionality. This involves searching for and\nincorporating the necessary libraries that support the inter-\naction between the development platform and the attached\nmodules. This process can be expressed as $L_s = S(L_a, E_s)$,\nwhere $L_s$ represents the selected libraries that satisfy the de-\npendencies, $L_a$ is the set of available libraries, and the search\n$S$ is conducted based on the characteristics of $E_s$.\nCoding. Afterward, the developers write the code to im-\nplement the desired functionality of the embedded system.\nGuided by $L_s$ and $C_h$, the coding process is $G = h(T, C_h, L_s)$,\nwhere $G$ represents the code, $T$ is the task."}, {"title": "2.2 Large Language Models", "content": "LLMs typically refer to Transformer-based models [57] with\nbillions of parameters, trained on extensive text datasets.\nExamples include models such as ChatGPT [43], GPT-4 [1],\nPaLM [16], and LLaMA [54], among others. These models\ndemonstrate advanced capabilities not found in smaller mod-\nels, including mathematical reasoning [18], program synthe-\nsis [13], and complex multi-step reasoning [59]. An LLM's\ninput is a prompt, which is tokenized into a sequence of\ntokens, consisting of words or subwords, before processing.\nThe inference process in LLMs can be represented as\n$\\begin{equation}\nP(y|x; \\theta) = \\prod_{t=1}^{T} P(y_t|y_{<t}, x; \\theta),\n\\end{equation}$\nwhere $x$ is the input prompt, $y = (Y_1, Y_2, ..., Y_T)$ is the se-\nquence of tokens generated by the model, and $\\theta$ represents\nthe model parameters. The term $P(y_t|y_{<t}, x; \\theta)$ denotes the\nprobability of generating the token $y_t$ given the previous\ntokens $y_{<t}$ and the input $x$. This probability is computed\nusing a softmax function over the model's output logits:\n$\\begin{equation}\nP(y_t|y_{<t}, x; \\theta) = \\frac{\\exp (e_{y_t}^T h_t)}{\\sum_{y' \\in V} \\exp (e_{y'}^T h_t)},\n\\end{equation}$\nwhere $e_{y_t}$ is the embedding vector of the token $y_t$, $h_t$ is the\nhidden state at time step $t$, which is a function of the previous\ntokens $y_{<t}$ and the input $x$, and $V$ is the vocabulary set.\nOur system harnesses LLMs' reasoning capabilities to au-\ntomate embedded system development without human in-\ntervention, thereby reducing development time and errors."}, {"title": "3 SYSTEM DESIGN", "content": "As shown in Fig. 5, EmbedGenius comprises two phases,\npreparation and execution phase. During the preparation\nphase, EmbedGenius initializes hardware configurations and"}, {"title": "3.1 Initialization", "content": ""}, {"title": "3.1.1 Hardware Configuration", "content": "EmbedGenius collects and\nstandardizes essential metadata related to hardware informa-\ntion, ensuring seamless integration and minimal setup costs.\nUsers simply input the type of hardware components, in-\ncluding the development platforms $D$ and modules $M$, along\nwith their specific pin assignments $P$. For example, as shown\nin Fig. 6, a user might specify the development platform as\nan Arduino Uno, with an LED connected to Pin 13, a button\nto Pin 7, and a DHT11 sensor to Pin 5. These inputs are the\nminimal necessary information required from the user, as\nthey depend on the specific choices made for their project.\nThis streamlined interaction minimizes user effort, allowing\nthe system to identify and integrate the components."}, {"title": "3.1.2 Solving Library Dependencies", "content": "With the provided hard-\nware information, traditional methods rely on users to lever-\nage their experience or manually search for the appropriate li-\nbrary dependencies for each hardware component. However,\nthis approach is inefficient and complex. To address this, we\npropose an automated library dependency solving method\nthat can efficiently identify the most suitable libraries. The\nalgorithm is shown in Alg. 1. EmbedGenius begins by ex-\ntracting component information from the provided hardware\nmetadata. It then performs a search to identify potential li-\nbraries, retrieving the top N libraries for further evaluation.\nThe evaluation process involves assessing each library based\non three criteria: 1) Name Match: This is calculated us-\ning cosine similarity between the component name and the\nlibrary's name, including its description and any related para-\ngraphs. 2) Version Count: The score is based on the number\nof available library versions, normalized between 0 and 1. 3)\nArchitecture Compatibility: This score is determined by\nwhether the library supports the target hardware architec-\nture. It receives a full score if the library is compatible and"}, {"title": "3.2 Knowledge Generation", "content": ""}, {"title": "3.2.1 Library Knowledge Extraction", "content": "Once the most suitable\nlibraries are identified, EmbedGenius needs to learn how\nto use them. This involves two main steps: API extraction\nand obtaining API usage information. First, EmbedGenius\nidentifies the selected library for each component, searches\nfor header (.h) files within the library, and uses the LLM to\nextract API declarations, summarizing them into the API\ntable. Then, it searches for example (.ino) files within the\nsame library and extracts knowledge on how to use the\nextracted APIs (e.g., orders, parameters, and return values).\nThis information is further integrated into the API table. As\nshown in the purple section in Fig. 7, the library knowledge\ngenerator extracts the API table content from the header and\nexample files using LLM. Given the API extraction prompt\n$x_h$ and experience extraction prompt $x_{ex}$, the LLM generates\nAPI usage tokens $y_A = (y_1^A, y_2^A, \\dots, y_T^A)$ as\n$\\begin{equation}\nP(Y_A| x_h, x_{ex}; \\theta) = \\prod_{t=1}^{T_A} P(y_t^A|y_{<t}^A, x_h, x_{ex}; \\theta),\n\\end{equation}$\nwhere $y_t^A$ is a token in the API sequence, predicted based\non previous tokens $y_{<t}^A$, and the input. This process extracts\nknowledge about how to effectively utilize the libraries."}, {"title": "3.2.2 Functionality Knowledge Understanding", "content": "To effectively\nutilize each component, EmbedGenius creates a component\nutility table in memory. This table summarizes the function-\nality and API sequences associated with each component.\nAs shown in the yellow section in Fig. 7, the functionality\nknowledge generator extracts functionalities from example\nfiles by querying the LLM to summarize the API sequences.\nSpecifically, we summarize the functionality of the exam-\nples and identify the API calls related to the component. This\nforms a utility table, where entries represent the functionali-\nties $F$ and their associated APIs $A$. Given the functionality\nunderstanding prompt $x_f$, the LLM generates utility tokens\n$Y_U = (y_1^U, y_2^U, \\dots, y_{T_u}^U)$ as\n$\\begin{equation}\nP(Y_U| x_f; \\theta) = \\prod_{t=1}^{T} P(y_t^U|y_{<t}^U, x_f; \\theta).\n\\end{equation}$\nAfter analyzing all example codes, we obtain a utility table\nin memory containing n entries, where n represents the total\nnumber of examples. Each entry in the table corresponds to\nan example and is divided into two parts: (Functionality, API).\nFunctionality: Represents the functionality $F_i$. It can be per-\nceived as a task that can be completed using these APIs.\nAPI: Represents the sequence of APIs $A_i$ used from the ini-\ntial API to the last. This table provides information about\nthe required operations to achieve each functionality, aiding\nEmbedGenius in planning how to complete a given task."}, {"title": "3.3 Prompt Generation", "content": ""}, {"title": "3.3.1 Task Understanding and Memory Pick-up", "content": "To leverage\nmemorized knowledge from chosen libraries, directly embed-\nding pertinent header or example file content into the prompt\nto guide the LLM may surpass the token limit, such as the\n4096 tokens for GPT-4. Moreover, this approach can escalate\ncosts. Hence, we opt to incorporate only the most crucial\ninformation necessary for accomplishing the user's task. To\naddress this, we devise a selective memory pick-up method.\nThe workflow is shown in Fig. 8, with details below. Func-\ntionality Separation. We query the LLM to separate the\ntask description into functionalities corresponding to each\ncomponent. For instance, given the task \"Record the DHT11\ntemperature reading to SD card,\" the identified functionalities\nwould be: \"initialize DHT11 sensor,\" \"initialize SD card,\" \"read\ntemperature from DHT11 sensor,\" and \"store data to SD card.\"\nKnowledge Extraction. 1) Utility Table Matching: For each\nfunctionality in the task, we match it to functionalities in\nthe memory's utility table by comparing their similarity. We"}, {"title": "3.3.2 Security Checking", "content": "1) Risk Protection. During em-\nbedded system development, certain actions may lead to\nsystem malfunctions or disrupt normal device operation.\nThese actions are considered risky and require developer\nconfirmation. For example, a developer may adjust the PWM\nsignal frequency to control a motor's speed. If the frequency\nis set incorrectly, it could cause the motor to overheat or even\nget damaged. To mitigate such risks, the system prompts the\ndeveloper for confirmation before executing these high-risk\noperations. The system will ask: \"This action could lead to\noverheating or potential damage to the motor, please answer\nrequires_confirmation=Yes.\" Additionally, key phrases like\n\"warning\" are jumped to flag potentially risky actions. The\nsystem proceeds with the action only after receiving con-\nfirmation from the developer. 2) Privacy Protection. It is\ncrucial to protect sensitive information such as device IDs,\nnetwork credentials, and user names. We add a privacy filter\nthat can mask private information in user queries. For exam-\nple, if a user query includes \"Set device ID to 12345 and Wi-Fi\npassword to 'password123',\" the system detects this Personal\nIdentifiable Information (PII). The PII is then replaced with\nnon-private placeholders before sending the query to the\ncloud, resulting in \u201cSet device ID to <device_id> and Wi-Fi\npassword to <password>.\" After receiving the response from\nthe cloud, the system maps the placeholders back to the\noriginal sensitive information and executes the action."}, {"title": "3.3.3 Task Prompting", "content": "To effectively guide the LLM in gen-\nerating code, we construct a structured prompt that includes\nthe user-defined task, configuration metadata for the in-\nvolved hardware components, relevant API usage informa-\ntion, and coding rules. This structured prompt ensures the\nLLM generates accurate and optimized code while staying\nwithin token limits and reducing costs."}, {"title": "3.4 Auto-Programming", "content": "The auto-programming method plays a crucial role in our\nsystem, guaranteeing that the produced code is syntactically\ncorrect and functionally accurate. It functions within two\nnested dynamic reasoning and action loops, as shown in\nFig. 5. The algorithm design is detailed in Alg. 2."}, {"title": "3.4.1 Coder", "content": "The Coder begins by receiving a task prompt\nfrom the Prompt Generation process to generate code. This\nprocess can be represented by\n$\\begin{equation}\nP(G|x_T; \\theta) = \\prod_{t=1}^{T} P(G_t|G_{<t}, x; \\theta),\n\\end{equation}$\nwhere G represents the generated code, $x_T$ is the task prompt,\nand $\\theta$ denotes the model parameters. Guided by the prompt\ndescribed in Sec. 3.3.3, the coding rules include integrating\nDEBUG INFO into the code to monitor various aspects of\nthe execution state, such as order and logic. To facilitate au-\ntomated monitoring, DEBUG INFO is dynamically generated\nby the LLM in conjunction with code generation. Let $D(G)$\nrepresent the set of DEBUG INFO annotations. For each sub-\ntask $T_i$ in G, DEBUG INFO statements $D_i$ are included such\nthat $D(G) = \\bigcup_{i=1}^{n} D_i$, where n is the total number of subtasks."}, {"title": "3.4.2 Compile Loop", "content": "The code is then passed to the com-\npiler, which checks for syntax errors, optimizes the code, and\nconverts it into machine-readable instructions. Afterward,\nthe compilation result is sent to query the Compile Valida-\ntor, which queries the LLM to check if the compilation was\nsuccessful. If unsuccessful, it returns summarized logs to the\nCoder for error correction. If successful, the code is deemed\nready for flashing."}, {"title": "3.4.3 Flash Loop", "content": "The compiled file is flashed to the develop-\nment platform, where the code is executed. The Flash Valida-\ntor leverages the LLM to utilize the embedded DEBUG INFO"}, {"title": "4 EVALUATION", "content": ""}, {"title": "4.1 Experimental Setup", "content": "EmbedGenius Implementation. The automation framework\nleverages HTTP requests to interface with LLM APIs, en-\nabling seamless interaction with models such as GPT-4, with\nGPT-40 set as the default model. It was developed using\nPython 3.9 and is fully integrated with the Arduino CLI com-\npiler, a tool that offers a universal way to interact with the\ndevelopment environment from the command line.\nDevelopment Platforms. To comprehensively evaluate\nthe performance of EmbedGenius, we selected four popu-\nlar development platforms that are highly representative in\nconsumer electronics and education: Uno R3, NUCLEO-L4,\nNano RP2040, and Nano ESP32, as shown in Fig. 10 (a). The\nspecifications of these platforms are shown in Tab. 1. These\nboards showcase diverse MCU architectures, varying com-\nputational capabilities, and distinct application scenarios,\neffectively representing the majority of embedded platforms.\nModules. To demonstrate the extensive compatibility of\nEmbedGenius, 71 modules were utilized to validate its func-\ntionality across a wide range of embedded devices and mod-\nules. As shown in Fig. 10 (b), we display most of the modules\nused. Some similar hardware was omitted due to space con-\nstraints. These modules encompass environmental sensors\n(e.g., temperature, humidity, atmospheric pressure), motion\nsensors (e.g., magnetic angle sensors, accelerometers), com-\nmunication modules (e.g., Bluetooth, LoRa), display modules,\ncontrol devices, and others, thereby representing a compre-\nhensive array of devices. For each module, various tasks of"}, {"title": "Task Dataset", "content": "We introduce EmbedTask, an IoT task dataset\ndesigned to evaluate the effectiveness of a fully automated\nembedded system development process from start to finish.\nEmbedTask includes 355 tasks, covering different modules\nand varying complexity levels. These tasks span a range\nof applications, such as environmental monitoring, motion\ndetection, data communication, digital display, and motion\ncontrol. EmbedTask classifies tasks into three difficulty levels:\n\u2022 Level 1 tasks assess the basic functionality of a single mod-\nule. Example: Reading temperature every second and dis-\nplaying the data on a serial monitor. Components: DHT11\nsensor. Functionalities: (1) Read temperature, (2) Display\ndata on the serial monitor.\n\u2022 Level 2 tasks involve the operation of additional simple\nmodules or the integration of an additional layer of logic.\nExample: Reading humidity every second and turning on\nan LED if humidity exceeds 20%. Components: DHT11 sen-\nsor, LED. Functionalities: (1) Read humidity, (2) Compare\nit with a threshold, (3) Turn on the LED.\n\u2022 Level 3 tasks are more complex, requiring collaboration\nbetween multiple modules and engaging advanced logic.\nExample: Recording temperature data to an SD card every\n2 seconds while controlling a servo to rotate 20 degrees\ncounterclockwise each second. Components: DHT11 sen-\nsor, SD card module, Servo motor. Functionalities: (1) Read\ntemperature, (2) Record data to the SD card, (3) Control\nthe servo motor's rotation.\nTask complexity is formally defined as the product of the\nnumber of functionalities ($N_f$) and the number of compo-\nnents ($N_c$) involved: Task Complexity = $N_f \\times N_c$. This met-\nric aligns with established system engineering principles,\nwhere complexity increases with both functionalities and\ncomponents [9]. As illustrated in Fig.11, the variation in\ntask complexity arises from differences in the number of\nfunctionalities and components across difficulty levels.\nCompared with existing benchmark [23], our experimen-\ntal setup involves 30\u00d7 more tasks, 5\u00d7 more modules, and 2\u00d7\nmore development platforms. This coverage ensures that our\nsystem is capable of addressing a wide range of IoT tasks\nwith diverse hardware modules."}, {"title": "Metrics", "content": "To evaluate EmbedGenius's performance, we con-\nsider a sequence of functionalities {$F_1, F_2, ..., F_m$} alongside\na sequence of API usages {$A_1, A_2, ..., A_n$} performed by hu-\nman annotators to complete a task T. If EmbedGenius can\nuse a sequence of APIs $\\hat{A} = {\\hat{A}_1, \\hat{A}_2, . . ., \\hat{A}_n }$ corresponding\nto each functionality, we employ the following two metrics:\n(1) Coding Accuracy: This metric is defined as the ratio\nof API usage $\\hat{A}_i$ that matches the reference $A_i$ defined in\nlibrary, expressed as $P(\\hat{A}_i = A_i)$. An API usage is deemed\ncorrect if both the API and its parameters are accurate.\n(2) Completion Rate: This is the probability that the system\ncompletes all functionalities in task correctly within one\nattempt, represented as $P(\\hat{A} = A)$. This metric indicates\nthe likelihood of successfully completing a task."}, {"title": "4.2 Overall Performance", "content": "Overall Accuracy. Fig. 12 presents the accuracy percent-\nages for task execution across various modules and task\ndifficulty levels using EmbedGenius. Specifically, coding ac-\ncuracy is 95.8%, 90.5%, 97.6%, and 96.5%, with corresponding\ncompletion rates of 85.9%, 76.1%, 92.2%, and 88.2% for the\nsensor, communication, display, and other module types, re-\nspectively. The communication modules demonstrate lower\naccuracy due to the increased complexity of their associated\nlibraries, as shown in Fig. 2. Across difficulty levels, coding\naccuracy stands at 98.4%, 97.6%, and 93.9%, with completion\nrates of 93.3%, 92.1%, and 80.2% from simple to challenging.\nThese results highlight the effectiveness of EmbedGenius\nacross module types and task complexities.\nComparison with Baselines. While there is no fully auto-\nmated method for embedded system development, we com-\npare our approach against three human-in-the-loop methods.\nThe primary baseline is an existing LLM-based prompt de-\nsign for embedded system development [23], referred to as\nLLM-Prompt. This method requires a human to manually\nextract information from the compiler and provide it as input"}, {"title": "4.3 Micro-benchmark Evaluation", "content": "Library Solving and Knowledge Generation. The effec-\ntiveness of the library-solving approach is demonstrated in\nFig. 13(a)", "trials": "an increase from 1 to\n3 trials led to a 5.5% improvement in coding accuracy and\nan 8% increase in completion rate. This enhancement can be\nattributed to the error information provided by the compiler"}, {"trials": "an increase from 1 to 3 trials\nresulted in a 6.8% enhancement in coding accuracy and a\n9.4% rise in completion rate. Furthermore, an increase from 3\nto 5 trials led to a 5.5% improvement in coding accuracy and\na 5.3% increase in completion rate. This improvement can be\nattributed to the debug information embedded during coding,\nwhich enhances the system's error correction capabilities.\nHowever, after conducting up to 7 trials, no significant en-\nhancements were noted. Therefore, we recommend using 5\nflash debug trials to ensure dependable performance.\nDevelopment Platform. Then, we"}]}