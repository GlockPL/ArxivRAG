{"title": "A Review of Human-Object Interaction Detection", "authors": ["Yuxiao Wang", "Qiwei Xiong", "Yu Lei", "Weiying Xue", "Qi Liu", "Zhenao Wei"], "abstract": "Human-object interaction (HOI) detection plays a key role in high-level visual understanding, facilitating a deep comprehension of human activities. Specifically, HOI detection aims to locate the humans and objects involved in interactions within images or videos and classify the specific interactions between them. The success of this task is influenced by several key factors, including the accurate localization of human and object instances, as well as the correct classification of object categories and interaction relationships. This paper systematically summa-rizes and discusses the recent work in image-based HOI detection. First, the mainstream datasets involved in HOI relationship detec-tion are introduced. Furthermore, starting with two-stage meth-ods and end-to-end one-stage detection approaches, this paper comprehensively discusses the current developments in image-based HOI detection, analyzing the strengths and weaknesses of these two methods. Additionally, the advancements of zero-shot learning, weakly supervised learning, and the application of large-scale language models in HOI detection are discussed. Finally, the current challenges in HOI detection are outlined, and potential research directions and future trends are explored.", "sections": [{"title": "I. INTRODUCTION", "content": "With the explosive growth of image data, understanding and analyzing the content within these images has become a crucial challenge. Relying solely on human vision for recognition is far from sufficient to meet the needs of modern society. Consequently, human-object interaction (HOI) detection has emerged as a key technology in the field of computer vision. HOI detection aims to accurately locate humans and objects in images or videos and recognize the corresponding interaction categories to better understand human activities. Specifically, HOI detection takes an image or video as input and out-puts a series of triplets (< human, object, interaction >). It is widely used in autonomous driving, action recognition, human-computer interaction, social network analysis, emotion recognition, security monitoring, and video surveillance [1].\nThis paper primarily reviews the research achievements in image-based HOI detection."}, {"title": "II. DATASETS", "content": "In the past years, many excellent HOI detection datasets have emerged. Based on annotation granularity, the levels are instance-level, part-level, and pixel-level. The summary and analysis of datasets across these three annotation levels are listed in Table I. Below are their detailed descriptions:\nV-COCO [57]: V-COCO is a dataset based on COCO [58], containing 10,346 images, with 5,400 used for training and 4,946 for testing. It includes 80 object categories and 29 action categories, 4 of which represent body actions that do not involve any interaction. These interactions cover various actions such as \"eating\", \"reading\u201d, and \u201cwearing\" associated with objects like \"bread\", \"book\", and \"clothes\".\nHICO-DET [6]: In 2018, the University of Michigan in-troduced HICO-DET, a HOI detection dataset with a larger number of images and more complex interaction relationships. The data is sourced from Flickr and comprises a total of 47,776 images, with 38,118 used for training and 9,658 for testing. The dataset includes 80 object categories and 117 action categories, resulting in 600 different interaction types, of which 462 are non-rare categories and 138 are rare categories. The HICO-DET and V-COCO datasets cover a wide range of object and action categories, which are the most commonly used benchmarks for HOI detection.\nHCVRD [59]: The University of Adelaide in Australia constructed a large-scale human-centered visual relationship detection dataset called HCVRD. Compared to previously released datasets, this dataset contains a large number of relationship annotations, with nearly 10,000 categories. This extensive label space more accurately reflects real-world HOIs. Unlike HICO-DET and V-COCO, HCVRD not only focuses on interaction relationships but also includes the relative positional relationships between humans and objects.\nPaStaNet-HOI [59]: Under coarse-grained instance-level annotation supervision, the model is prone to overfitting, which leads to poor generalization capabilities. Shanghai Jiao Tong University constructed the PaStaNet-HOI dataset. This dataset provides approximately 110,000 annotated images, with 77,260 images used as the training set, 11,298 images as the validation set, and 22,156 images as the test set. PaStaNet-HOI discards the \u201cno interaction\u201d category. It consists of 116 interaction relationships and 80 object categories, forming a total of 520 HOI relationship categories.\nPIC [60]: Liu et al. construct the pixel-level HOI database PIC (Person in Context) by performing pixel-level annotations on both the human body and objects. This enables more precise localization of humans and objects, even in cases of occlusion. PIC collects 17,122 human-centered images from the internet, which include a training set of 12,339 images, a validation set of 1,916 images, and a test set of 2,867 images. Additionally, as one of the most finely annotated databases for HOI detection. It includes a comprehensive range of annotation types, such as 141 object categories and 23 interaction relationships between humans and objects."}, {"title": "III. THE ARCHITECTURES OF HOI DETECTION", "content": "Two-stage detection algorithms primarily focus on multi-stream models and graph models. One-stage detection algo-rithms can be divided into bounding box-based, relationship point-based, and query-based models. This section briefly reviews the above work and introduces new techniques such as zero-shot learning, weakly supervised, and large-scale lan-guage models."}, {"title": "A. Two-stage HOI detection architecture", "content": "Two-stage HOI detection is an instance-guided, bottom-up deep learning approach. Currently, two-stage methods are divided into multi-stream models and graph models.\nMulti-stream models. The multi-stream model is an early attempt in the field of HOI detection. Firstly, the object detector generates region proposals about humans and objects. Secondly, an interaction relationship classification network extracts features for the targets and then fuses the classifi-cation results. In 2018, Chao et al. [6] construct a widely influential public dataset known as the HICO-A dataset. In addition, they propose a standard two-stage HOI network HO-RCNN for extracting features of spatial relationships between humans and objects. The HO-RCNN is composed of three key components: the human stream, the object stream, and the HOI stream. Moreover, Gkioxari et al. [2] propose a new human-centered model called InteractNet, which uses human appearance as a cue to predict the location of target objects. It can simultaneously detect humans, objects, and interaction relationships. In work [3], Li et al utilize an interaction-aware network to learn general interaction knowledge from several HOI datasets. The non-interaction suppression strategy is em-ployed before HOI classification inference, thereby improving network performance.\nThe subtle visual differences between various interaction relationships bring challenges to HOI detection. To address these challenges, Wan et al. [8] propose a multi-level rela-tionship detection strategy that utilizes human pose cues to capture interactions. Specifically, they employ a multi-branch network to learn pose-enhanced relationship representations across three semantic levels. By integrating these features, the model generates robust results. Zhong et al. [13] propose a new polysemy decoding network, PD-Net, which further mitigates the issue of verb polysemy through three strategies.\nGraph models. The aforementioned multi-stream mod-els overlook the correlations between different human-object pairs. Additionally, processing each human-object pair indi-vidually increases time costs. To overcome these limitations, Qi et al. [7] first introduce the Graph Parsing Neural Net-work (GPNN). GPNN uses nodes and edges to identify in-stances and interaction relationships, respectively. Later, Zhou et al. [64] propose the Relational Pairwise Neural Network (RPNN), which utilizes object-body part and human-body part relations to analyze pairwise relationships between two graphs.\nPrevious graph-based algorithms treat humans and objects as the same type of node, failing to distinguish information exchanged between different entities. Therefore, Wang et al. [65] propose a heterogeneous graph network, CHGNet, which models humans and objects as distinct types of nodes. Gao et al. [66] utilize abstract spatial semantic representations to describe each human-object pair. They employ a dual relation graph to aggregate contextual information from the scene and capture discriminative cues, effectively addresses the prediction ambiguity issue. Most methods primarily focus on the visual and semantic features of instances. However, they do not leverage the high-level semantic relationships within the image. In order to solve this shortcoming, He et al. [14] embed scene graphs into global contextual cues. Additionally, a message passing module was developed to gather relational information from the neighbors of objects."}, {"title": "B. one-stage HOI detection architecture", "content": "One-stage algorithms have surpassed traditional two-stage models in both speed and accuracy. Unlike two-stage methods, one-stage approaches can directly output HOI triplets without an additional object detection process.\nBounding box-based models. The bounding box-based algorithms directly detect the location and category of targets using a simple structure while simultaneously predicting po-tential interaction relationships. This simplified design signifi-cantly enhances the inference speed of the algorithm. Previous works first detect instances and then predict interaction actions, which results in longer inference times for HOI detection. To address this challenge, Kim et al. [25] propose UnionDet, a method that directly captures interaction regions, eliminating the need for additional inference stages. Compared to tradi-tional two-stage algorithms, UnionDet significantly improves inference speed by 4 to 14 times. This innovation makes HOI detection more efficient and real-time.\nTraditional one-stage methods typically focus on the joint region of interaction, which can introduce unnecessary visual noises. To tackle this issue, Fang et al. [21] propose DIRV, which focuses on the interaction regions of each human-object pair at different scales and extracts the most relevant subtle visual features. Additionally, DIRV develops a voting strategy that leverages the overlapping parts within the interaction region, replacing the traditional non-maximum suppression method.\nRelationship point-based models. Inspired by anchor-free detectors, research based on relationship points opens a new era in one-stage methods. Wang et al. [26] argue that extracting appearance features only is insufficient to handle complex HOI sciences. Therefore, they propose IP-Net, the first algorithm to view HOI detection as a keypoint detection problem. PPDM [19] is the first real-time HOI detection method that redefines the HOI triplet as <human point, interaction point, object point>. As a novel parallel architecture, PPDM signif-icantly reduces computational costs by filtering of interaction points.\nExisting one-stage models lack a reasoning step for dynamic discriminative cues. Zhong et al. further improve PPDM by GGNet [22]. GGNet first determines whether a pixel is an interaction point, then infers action-aware points around the pixel to refine the point's position.\nQuery-based models. Tamura et al. [29] propose a transformer-based feature extractor, where the attention mech-anism and query-based detection play key roles. One-stage HOI detection algorithms based on the transformer architec-ture are gradually emerging and developing. Unlike existing transformer-based models that query at a single level, Dong et al. [28] explicitly merge and sum queries to better model the relationships between parts and the whole, which are not directly captured within the transformer. Kim et al. [25] use HOTR to predict triplets from images directly. This method effectively leverages the inherent semantic relationships within the image, eliminating the post-processing strategies used in previous approaches. Moreover, Liao et al. [38] propose a dual-branch GEN-VLKT, which eliminates the need for post-matching. CLIP [70] is also embedded to initialize the classi-fier, effectively leveraging image and text information. Simi-larly, the category-aware transformer network (CATN) [71] en-hances detector performance by initializing object queries with category-aware semantic information. In work [72], ERNet utilizes multi-scale deformable attention to capture essential HOI features."}, {"title": "C. New techniques", "content": "This section introduces new techniques, including zero-shot learning, weakly supervised, and large-scale language models.\nZero-shot learning. Maraghi et al. [44] is the first to utilize zero-shot learning methods to address the long-tail problem in HOI detection. In this work, a decomposed model is used to separately infer verbs and objects, enabling the detection of new verb-object combinations during the testing phase. Eum et al. [45] propose an HOI detection method based on verb-object relationship reasoning, where semantic and spatial information is embedded into the visual stream. Due to the combinatorial nature of visual relationships, collecting a sufficiently large amount of trainable triplet data is hard. To address this, Peyre et al. [46] develop a model that successfully merges semantic and visual spaces.\nWeakly supervised models. Most existing HOI detection models require fully annotated data for supervised training. However, obtaining complete data labels remains a challenging task. To effectively address this issue, weakly supervised HOI detection typically uses image-level interaction labels for training. For example, Peyre et al. [54] introduce a weakly supervised discriminative clustering model that learns rela-tionships solely from image-level labels. They also propose a new and challenging dataset, UnRel, to accurately evaluate visual relationships. In work [55], Sarullo et al. model the relationships between actions and objects in the form of a graph. Align-Former [48] is equipped with an HOI alignment layer that generates pseudo aligned human-object pairs based on weakly supervised. Furthermore, Baldassarre et al. [47] attempt to leverage a multi instance learning framework to detect instances and then use image-level labels to supervise the interaction classifier.\nLarge-scale language models. With the help of large language models, Unal et al. [53] use only image-level labels to query possible interactions between human and object categories. The graph-based ProposalCLIP [56] addresses the limitations of CLIP cues by predicting the categories of objects without annotations, effectively enhancing perfor-mance. Recently, Wan et al. [49] develop a CLIP-guided HOI representation that integrates prior knowledge at both the image level and the human-object pair level. This dual-layer framework is designed to more effectively utilize image-level information through a shared HOI knowledge base, thereby further enhancing the learning of interaction features."}, {"title": "IV. COMPLEX PROBLEM OF HOI DETECTION", "content": "Table II and Table III present the results of different methods on the HICO-DET and V-COCO datasets, respectively. The advantage of two-stage methods is they can decouple object detection and interaction classification, allowing each stage to focus on optimizing its specific task. However, this approach encounters many obstacles. There are imbalanced positive and negative sample distributions, additional computational, and insufficient information exchange.\nOne-stage HOI detection algorithms significantly improve efficiency and accuracy by directly predicting interactions. However, these models typically use a multi-task learning approach to share features, which may lead to interference between tasks, preventing the model from achieving optimal performance.\nSignificant progress has been made in the field of HOI detection, especially with the emergence of new technologies. These methods can directly predict interactions, avoiding the complexity of the two-stage process. Nevertheless, despite these advancements, several issues remain in the field of HOI detection. Current models may perform poorly when handling multiple interaction types. Additionally, environmental condi-tions and lighting issues can further affect the detection of small objects."}, {"title": "V. CONCLUSION", "content": "HOI detection is a research hotspot in computer vision, with widespread applications in action recognition, pose es-timation, autonomous driving, and other scenarios. This paper discusses the latest advancements in two-stage and one-stage HOI detection tasks, providing an overview of the cutting-edge developments. At the same time, new techniques including zero-shot learning, weakly supervised learning, and large-scale language methods are introduced. Finally, the current challenges faced by HOI detection, from the perspective of technical difficulties, are summarized, and its future develop-ment trends are predicted."}]}