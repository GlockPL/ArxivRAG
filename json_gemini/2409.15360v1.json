{"title": "REWARD-ROBUST RLHF IN LLMS", "authors": ["Yuzi Yan", "Xingzhou Lou", "Jialian Li", "Yiping Zhang", "Jian Xie", "Chao Yu", "Yu Wang", "Dong Yan", "Yuan Shen"], "abstract": "As Large Language Models (LLMs) continue to progress toward more advanced forms of intelligence, Reinforcement Learning from Human Feedback (RLHF) is increasingly seen as a key pathway toward achieving Artificial General Intelligence (AGI). However, the reliance on reward-model-based (RM-based) alignment methods introduces significant challenges due to the inherent instability and imperfections of Reward Models (RMs), which can lead to critical issues such as reward hacking and misalignment with human intentions. In this paper, we introduce a reward-robust RLHF framework aimed at addressing these fundamental challenges, paving the way for more reliable and resilient learning in LLMs. Our approach introduces a novel optimization objective that carefully balances performance and robustness by incorporating Bayesian Reward Model Ensembles (BRME) to model the uncertainty set of reward functions. This allows the framework to integrate both nominal performance and minimum reward signals, ensuring more stable learning even with imperfect reward models. Empirical results demonstrate that our framework consistently outperforms traditional RLHF across diverse benchmarks, showing improved accuracy and long-term stability. We also provide a theoretical analysis, demonstrating that reward-robust RLHF approaches the stability of constant reward settings, which proves to be effective in a stochastic-case analysis. Together, these contributions highlight the framework's potential to enhance both the performance and stability of LLM alignment with RLHF.", "sections": [{"title": "1 INTRODUCTION", "content": "Reinforcement Learning from Human Feedback (RLHF) has emerged as a pivotal methodology for ensuring the alignment of foundational models with human values and preferences . This technique has proven instrumental in enhancing the capabilities of Large Language Models (LLMs) to generate responses that are more helpful, harmless and honest. The standard RLHF framework is delineated into two principal phases: 1) Reward Model (RM) training from Human/Artificial Intelligence Feedback: In this phase, a RM is trained by preference data, typically utilizing Maximum Likelihood Estimation (MLE). 2) RM-based Reinforcement Learning: Subsequently, the model employs the established deep reinforcement learning algorithm, Proximal Policy Optimization (PPO) Schulman et al. (2017), to refine performance based on the reward function determined in the preceding phase. This structured approach ensures that LLMs operate in a manner that is consistent with ethical guidelines and user expectations, thereby enhancing their capability and trustworthiness in practical applications.\nMany researchers believe that the most significant challenges in developing a RM is the inherent biases among annotators-whether human or AI-in the preference data Bai et al. (2022a); Lee et al. (2023). These biases make it exceedingly difficult to create a RM that is fully aligned with the diverse preferences of humanity as a whole. A poor RM may provide incorrect signals for certain data points during the PPO training phase, ultimately compromising the performance of the finetuned model. Here, we outline several issues that arise from an imperfect RM: 1) Reward hacking:"}, {"title": "2 RELATED WORKS", "content": ""}, {"title": "2.1 RM-BASED ALIGNMENT IN LLMS", "content": "The core idea behind RM-based alignment is to use a RM, typically trained on human/AI-annotated data, to guide the optimization of the model's policy Christiano et al. (2017); Ouyang et al. (2022). Although RM-free algorithms such as DPO Rafailov et al. (2024) and IPO Azar et al. (2024) have"}, {"title": "2.2 ROBUST REINFORCEMENT LEARNING", "content": "In the realm of robust RL, robustness is primarily focused on addressing uncertainties related to transition, observation, action, or disturbance Moos et al. (2022). Transition-robust approaches deal with uncertainties in system dynamics by deliberately adjusting state transition probabilities Heger (1994); Nilim & El Ghaoui (2005); Satia & Lave Jr (1973); Givan et al. (2000). Observationrobust methods involve distorting the perceived system state to influence policy decisions Zhang et al. (2020); Gleave et al. (2019). Action-robust designs modify system transitions by introducing disturbances to the agent's actions Tessler et al. (2019). Disturbance-robust strategies account for external forces that introduce uncertainty into system behavior Pinto et al. (2017).\nIn contrast, there has been comparatively less focus on reward robustness. Xu & Mannor (2006) consider an MDP with an uncertain reward function and then propose a weighted sum between a nominal and a robust performance criterion, which directly inspires our work. The trade-off can be directly made on the expected return Xu & Mannor (2006) or by defining a chance constraint optimization problem Delage & Mannor (2010). Other research targets the rectangularity assumption, which assuming that the uncertainty in each state is independent of all other states, identifying it as a primary source of the reward uncertainty Mannor et al. (2012; 2016); Goyal & Grand-Clement (2023). Vadori et al. (2020) proposes a risk-sensitive RL approach to handle the reward uncertainty by applying Doob decomposition on the reward. Wang et al. (2020) develops a robust RL framework where the agents can only observed perturbed rewards.\nIn conclusion, due to the focus of past RL applications, particularly in domains like robotics, research on reward robustness has not received sufficient attention. However, in the context of LLM training, the uncertainty inherent in the reward model significantly impacts final performance. Serious issues arising from these shortcomings, such as reward hacking, are significantly hindering the progress of LLM development Chen et al. (2024); Kwon et al. (2023). Therefore, the development"}, {"title": "3 PRELIMINARIES", "content": "Large Language Model (LLM). An LLM defines a 0-parameterized conditional distribution \u03c0\u03b8(a|x), which takes a prompt x as input and produces a response a. More specifically, the sampling from LLMs is performed in an auto-regressive manner:\n\u03c0\u03b8(a|x) = \u03a0\u03c4 \u03c0\u03bf(a\u03c4|x, a1:t\u22121),\nwhere a\u03c4 is the t-th token in the response a and a1:t\u22121 are tokens in the response before a\u03c4.\nStandard RLHF. Training LLMs typically involves three stages: Pretraining, SFT, and RLHF. In this section, we outline the standard RLHF-PPO paradigm, widely adopted in advanced research Ziegler et al. (2019); Ouyang et al. (2022).\nBeginning with a well-trained SFT model, denoted as \u03c00, we proceed by sampling two responses from \u03c00 for each instance in a given prompt set. Subsequently, we compile a set of comparisons D = {(x, a+, a\u2212)}, where a+ and a\u2212 denote human-preferred and human-dispreferred completions, respectively. The distribution of the preference dataset is assumed to follow the Bradley-Terry model Bradley & Terry (1952), i.e., the probability of response a+ is better than a\u2212 is given by:\npr(a+ > a\u2212|x) = exp(r(x, a+))/ exp(r(x, a+)) + exp(r(x, a\u2212)) = \u03c3(r(x, a+) \u2212 r(x, a\u2212)),\nwhere > represents the preference relation, and \u03c3(x) = 1/1+e\u2212x is the sigmoid function. To train a reward model r, we maximize the log-likelihood of the observed preferences by minimizing the following loss function:\nCRM(r) = \u03a3(x, a+, a\u2212) log pr(a+ > a\u2212|x) = \u2212 \u03a3(x, a+, a\u2212) log \u03c3(r(x, a+) \u2212 r(x, a\u2212)).\nDuring the RL optimization phase, we update the LLM to maximize the return from the learned reward model using the following principle:\nmax\u03b8 Jr\u03c0 (\u03b8) = max\u03b8 Ex\u223cD\u03c0\u03b8(a|x) [r(x, a) \u2212 \u03b2log \u03c0\u03b8(a|x)/\u03c00(a|x)],\nwhere \u03c0\u03b8 is initialized as \u03c00 and \u03b2 controls the deviation from the original model. PPO Schulman et al. (2017) is typically used to solve the problem in practice. Algorithms that optimize the policy using a separate reward model are referred to as RM-based alignment."}, {"title": "4 INHERENT IMPERFECTION OF REWARD MODELS", "content": "In this section, we demonstrate that imperfection is an inherent characteristic of RMs in RLHF/RLAIF pipelines. This imperfection arises from two key factors: 1) Disagreements between annotators, which significantly affect the quality of the preference dataset. 2) The inherent difficulty in achieving an optimal reward model, even with perfectly aligned annotators.\nThe issue of disagreement among human annotators in the RLHF pipeline has been noted in previous research Bai et al. (2022a). More recently, researchers have begun using AI as annotators in what is known as RLAIF, claiming comparable or even superior performance to RLHF Lee et al. (2023); Bai et al. (2022b). However, our evaluation experiments revealed that in the RLHF process, the scoring consistency between human annotators and domain experts was approximately 70%, whereas in the RLAIF process, the consistency between multi-agent AI annotators and domain experts dropped to around 64%. The details can be found in Appendix A."}, {"title": "5 REWARD-ROBUST RLHF", "content": "In this section, we formally present our reward-robust RLHF framework. According to Section 4, the golden reward function r\u2217 is not accessible. Conversely, we can only access a nominal reward function, which is believed to be a good approximation of r\u2217 regardless of uncertainty, and a set of other reward functions Runcertain = {ri|i = 1, 2, ..., n}, which can be referred to an uncertainty set.\nThe root of the problem in the standard RLHF pipeline lies in the excessive reliance on a single nominal reward model and the neglect of the uncertainty set (see Equ. 5). Inspired by the previous work in robust RL Xu & Mannor (2006), we introduce a worst-case analysis to eliminate the"}, {"title": "5.1 BRME: UNCERTAINTY SET MODELING", "content": "In this section, we will show how we model the uncertainty set Runcertain as well as the nominal reward function r\u03c0. We propose Bayesian Reward Model Ensembles (BRME): We train a multi-head Bayesian reward model where the reward is modeled as a Gaussian distribution. Each head i has two outputs: one representing the mean \u00b5i and the other representing the standard deviation \u03c3i. A sample from this distribution is then output as the reward.\nThe diagram for BRME is shown in Figure 2. To facilitate deployment and conserve computing resources, we do not train multiple reward models independently as an ensemble; instead, we use parameter sharing. All reward heads share a common base model, which serves as a feature extractor. During training, each data sample is randomly assigned to a head for training, with reparameterization employed to address the non-differentiability of the sampling process.\nThe training process is divided into two stages. In the first stage, we train a normal one-head reward model following the loss function in (3). In the second stage, we leverage a MSE loss (8) to train the reward model, which is first introduced in Wu et al. (2024). The use of MSE loss to train the ensembled reward model ensures that: 1)"}, {"title": "5.2 EXPERIMENTAL RESULTS", "content": ""}, {"title": "5.2.1 EXPERIMENTAL SETUP", "content": "Models. We use LLaMa3-8B-Instruct Dubey et al. (2024) as the initialization of the actor. In BRME setting, we train a single-head RM and a 5-head BRME starting from LLaMa3-8BInstruct Dubey et al. (2024) as is described in Appendix B. The single-head reward model is used as the initialization of the critic model and the ensembled reward model is used as the reward signal source.\nDatasets. For the training process, we use HH-RLHF Bai et al. (2022a) and UltraFeedBack Cui et al. (2024) to train the BRME. HH-RLHF, UltraFeedBack Cui et al. (2024), along with an internal prompt dataset collected by the PM team, is employed to implement the PPO algorithm. Details of the datasets are deferred to Table 6. For performance evaluation, we select ARC Clark et al. (2018), LAMBADA Paperno et al. (2016), PIQA Bisk et al. (2020), SciQ Welbl et al. (2017), WinoGrande Sakaguchi et al. (2019), TQA Lin et al. (2022), MMLU Hendrycks et al. (2020), GSM8K Cobbe et al. (2021), FDA Arora et al. (2023), EQ-Bench Paech (2023), Arithmetic Brown et al. (2020), and ANLI Nie et al. (2020) as our benchmarks. The evaluation dimensions include robustness, general knowledge, numerical computation, emotion reading, information extraction, reasoning, context understanding, and commonsense. Detailed descriptions of the datasets are shown in Table 7.\nOther details. The reward model training and PPO experiments are conducted on 24 Nvidia H800-SXM-80GB GPUs in 3 nodes using DeepSpeed library and Zero stage 2 Rasley et al. (2020), and HuggingFace Accelerate Gugger et al. (2022). In PPO process, the actor model and the critic model occupy 10 gpus respectively. The initialized SFT model and the reward model occupy 2 gpus respectively. We use AdamW optimizer Loshchilov et al. (2017). The experience batch size in PPO is set to be 128."}, {"title": "5.2.2 MAIN RESULTS", "content": "We incrementally increased the performance trade-off hyperparameter \u03bb from 0 to 1 in intervals of 0.2 and repeated PPO training under each setting for 800 steps. The performance evaluation results at 200 and 800 steps are shown in Figure 3. It is important to note that when \u03bb = 1, the algorithm reduces to standard RLHF with a single nominal reward model.\nIn the short run (200 steps), although there are a few exceptions\u2014such as ANLI-r1 and LAMBADA, where \u03bb outperforms the other settings\u2014in most cases, the trade-off versions with \u03bb = 0.4 and \u03bb = 0.6 show better performance. This suggests that, even early in training, incorporating a balance between performance and robustness offers notable advantages. The ability of the reward-robust RLHF framework to temper the optimization process appears to result in more stable performance gains compared to standard RLHF. Compared with standard RLHF, the accuracy of reward-robust RLHF increases by 0.99% and 1.40% respectively when \u03bb = 0.4 and 0.6.\nOver the long run (800 steps), the advantages of incorporating robustness into the training process become even more pronounced. As shown in the results, nearly all experimental groups with \u03bb \u2260 1 exhibit improved performance by the end of 800 steps, confirming the long-term benefits of reward-robust RLHF. In contrast, standard RLHF with \u03bb = 1 not only fails to capitalize on the full optimization process but, in some cases, even results in negative performance growth. For example, in tasks like ANLI-r1 and GSM8K, we observe a decrease in model capability during the additional 600-step optimization process under standard RLHF. This highlights a key limitation of the purely"}, {"title": "6 DISCUSSION", "content": "In this section, we provide insights into why the proposed reward-robust framework is effective and how it improves upon the previous standard pipeline. Given that the reward signal is inherently imperfect, we empirically demonstrate that over-scoring is more detrimental than under-scoring, which supports our choice of minimum return as a robustness measure. Additionally, we show that in the stochastic case where rewards are given randomly, selecting the minimum reward ensures that the trained model remains at least acceptable. We also do ablation study to compare our method with other integration strategies such as using mean reward in Section 6.3."}, {"title": "6.1 OVER-SCORING VS. UNDER-SCORING", "content": "We have demonstrated that the reward model is inherently imperfect (Section 4), meaning the reward used in the PPO training is either over-scored or under-scored. Through PPO training on minimized reward, maximized reward, and mean reward, we show that over-scoring is significantly more harmful than under-scoring in LLM RLHF. The results of this comparison are illustrated in Figure 4a. In"}, {"title": "6.2 STOCHASTIC CASE ANALYSIS", "content": "With a poorly-trained reward model, the worst-case scenario is that all rewards are given randomly. In this section, we demonstrate that even in such a stochastic case, the reward-robust RLHF framework still yields an acceptable model, and we provide an explanation for why this is the case. First, we have the following lemma,\nLemma 1. If the reward model provides a constant reward for all actions during PPO training, the actor will not be optimized, as the gradient of the PPO objective function with respect to the policy parameters will be zero."}, {"title": "6.3 ABALATION STUDY", "content": "Our framework has two main different parts with the previous RLHF method. One is the Bayesian ensembled reward model. In previous works, the reward model directly output a scalar to be the final reward and the loss function is MLE(3). Another is the integration strategy. We use the trade-off version between the lower-bound reward and a nominal reward, while there are other strategies such as using the mean reward. Here we provide the ablation study results."}, {"title": "7 CONCLUSIONS AND FUTURE WORKS", "content": "In this paper, we proposed a reward-robust RLHF framework to address the problem of reward hacking in LLM alignment. We demonstrated that imperfection is an inherent characteristic of current reward model training, leading to the model exploring incorrect optimization directions. To mitigate this issue, we trained a Bayesian ensemble reward model with multi-head outputs to model the uncertainty set of the reward function. We showed that the head with the minimum variance effectively models the nominal reward function, representing the most confident scoring output. The newly proposed robustness-performance trade-off objective was proven effective, consistently outperforming baselines across most benchmarks. Furthermore, we demonstrated that under-scoring is preferable to over-scoring when dealing with imperfect reward models. Even in the worst-case scenario, where rewards are assigned randomly, the reward-robust RLHF framework still yields an acceptable model.\nSince our method can be easily integrated into existing pipelines, there is potential to further improve performance by incorporating additional reward sources to better model the uncertainty set. Future work will explore the adoption of heterologous reward sources, including reward models trained on diverse datasets and direct scoring from closed-source LLM APIs such as GPT-4, as well as other markers mentioned in Yan et al. (2024a); Liu et al. (2023). The advantage of using heterologous models lies in their diverse base training datasets, which result in more varied reward scores, thereby improving the coverage of the uncertainty set. Preliminary exploration results on heterologous reward fusion are provided in Appendix F."}, {"title": "A ANNOTATOR DISAGREEMENT IN RLHF AND RLAIF", "content": "For human-annotated data, we conducted an agreement test on 209 data points, each containing one prompt and two responses. The prompts were selected from an internal dataset by the PM team, and the responses were generated by Baichuan2-13B Yang et al. (2023). Data categories include general knowledge, logical reasoning, tables, mathematics, etc. We used two distinct annotator groups: one consisted of highly educated internal annotators who had undergone multiple rounds of specialized annotation training, referred to as the Expert Group. The other group was composed of external annotators hired from the general public, referred to as the External Group. For each data point, annotators were tasked with a Good-Same-Bad evaluation: 1) G: response 1 is better than response 2. 2) S: both responses are of the same quality. 3) B: response 1 is worse than response 2. We compared the G/S/B annotations for the same data between the two groups. Across all samples, the consistency rate was 70%, with a 4.5% rate of opposite judgments (one group scored G while the other scored B). When only considering G/B samples, the consistency rate increased to 77%, with an opposite rate of 6.5%.\nWe also established an AI feedback pipeline, primarily using the GPT-4 API as the annotator, referred to as the Al Group. The PM team biasedly sampled 85 examples, focusing on cases where the Expert Group and External Group showed inconsistent labeling. For these data points, the consistency between the Expert Group and the Al Group was 64%, while the consistency between the External Group and the AI Group was 44%. In 9% of the cases, the Expert Group differed from both the External Group and the AI Group.\nThese results indicate that, whether in the RLHF or RLAIF process, annotator disagreement remains a significant challenge currently, which complicates RM training and presents obstacles that must be addressed."}, {"title": "B BAYESIAN REWARD MODEL ENSEMBLES", "content": "In this section, we will provide the training detail, the theoretical explanation, and the empirical performance of the proposed Reward Model Ensembles (BRME)."}, {"title": "B.1 TRAINING PIPELINE", "content": "The training process is divided into two stages. In the first stage, we train a normal one-head reward model following the loss function in (3). In the second stage, we leverage a MSE loss to train the reward model, which is first introduced in Wu et al. (2024). The loss function for a single head i is given by:\nli = 1/2 {ri,t \u2212 \u03c3 (ri[p(a+ > a\u2212) \u2212 1])}2+ 1/2{ri,t\u2212 \u03c3 (ri[p(a+ > a\u2212) \u2212 1])}2,\nwhere p(a+ > a\u2212) is derived from a separate Bradley-Terry model, as defined in (2), and is the output of the normal reward model trained in the first stage. The reward is modeled as a Gaussian distribution, with each head i producing two outputs: one representing the mean and the other"}, {"title": "B.2 DATA PARTITION", "content": "We use different data to train each head. We first shuffled the data in Table 6 and randomly assigned each data point to one of the reward heads, with each head consuming only 20% of the total data. This approach helps distinguish the score distribution for each reward head."}, {"title": "B.3 EXPERIMENTAL RESULTS OF BRME", "content": "We conduct several experiments to directly test the performance of BRME. We train another tradition RM with MLE loss to be the baseline. One of the main reasons that we use ensembling is to expand the reward coverage and obtain an informative uncertainty set. We use a separated preference testset to measure the coverage. The results are shown in Figure 6. We use the rewards of chosen response and rejected response as the horizontal and vertical axes and visualize the distribution results. BRME is measured by mean integration (Figure 6b) and trade-off integration with \u03bb = 0.5 (Figure 6c). It can be seen that BRME rewards are more widely distributed.\nAnother important performance measurement is the reward margin between chosen and rejected responses. A larger reward margin indicates a greater ability of the reward model to differentiate between better and worse responses, which is critical for guiding the optimization process effectively. We provide a reward margin comparison between BRME and the traditional reward model in Figure 8. The results show that the reward margin in BRME is significantly larger than in the traditional reward model, suggesting that BRME has a stronger capability to distinguish between high-quality and low-quality responses. This improved differentiation leads to more"}, {"title": "C ADDITIONAL EXPERIMENTS", "content": ""}, {"title": "C.1 DETAILED EXPERIMENTAL RESULTS IN SECTION 5.2", "content": "In this section, we provide the detailed experimental results in Section 5.2. Detailed data related to Figure 3 is shown in Table 1, Table 2 and Table 3. In a short run, when training step is 200, \u03bb = 0.4 enjoys the best performance, where 9 of 16 benchmarks outperforms the baseline and the standard RLHF (\u03bb = 1.0). However, there are still 2 benchmarks (ANLI-r1 and LAMBADA) where the standard RLHF is the best. It indicates that at the very beginning of the PPO process, choosing the nominal reward function to be the signal may lead to faster improvement.\nAs the training process is prolonged to 800 steps, the benefits of incorporating robustness into the RLHF framework become more evident. The results show that settings with \u03bb \u2260 1.0 generally outperform standard RLHF across a majority of the benchmarks. Notably, \u03bb = 0.2,0.4 and 0.6 continue to exhibit strong performance, particularly on tasks such as ARC-challenge and GSM8K, where the reward-robust RLHF outperforms standard RLHF with a clear margin. Specifically, on GSM8K, \u03bb = 0.6 results in a 4.93% improvement over standard RLHF. These results highlight the long-term advantages of balancing performance and robustness, as the reward-robust setting helps avoid the pitfalls of overfitting to imperfect reward signals, which is more likely to occur under standard RLHF.\nMoreover, in tasks like ANLI-r1, which initially favored standard RLHF at 200 steps, the performance of \u03bb = 0.4 surpasses the nominal reward strategy by the 800-step mark. This indicates that while the nominal reward model may provide faster short-term gains, incorporating robustness into the optimization process enables more consistent long-term improvements. Similarly, on the ARC-challenge, \u03bb = 0.6 outperforms standard RLHF with a 1.35% gain, further confirming the utility of the reward-robust approach for challenging tasks where the reward model may struggle with accuracy in the initial stages.\nThe degradation in performance observed in some tasks, such as ANLI-r1 and GSM8K, underscores the limitations of purely performance-driven RLHF, particularly in cases where the reward signal is noisy or unreliable. Over the extended training period of 800 steps, standard RLHF tends to overfit or follow misleading reward signals, leading to a decline in model performance. This is particularly concerning in tasks like GSM8K, where standard RLHF results in negative performance growth, while reward-robust strategies maintain stability and even improve accuracy.\nOn the other hand, the conservative nature of the reward-robust RLHF framework, mitigates this risk. By accounting for uncertainty in the reward model, the framework effectively narrows the"}, {"title": "C.2 DETAILED EXPERIMENTAL RESULTS IN SECTION 6.1", "content": "In Table 4, we present the comparison results between over-scoring setting and under-scoring setting, where maximum reward and minimum reward are chosen as the reward signal respectively."}, {"title": "C.3 ABLATION STUDY ON REWARD MODEL", "content": "In this section, we present ablation study results on the reward models to highlight the superiority of BRME over the traditional RM. The coverage and margin advantages are detailed in Appendix B.3. Here, we provide a comparison of accuracy and the direct effect on the PPO process.\nWe first compared the accuracy of the single-head reward model trained with MLE loss to the nominal head of the Bayesian ensemble reward model (BRME) on the preference dataset. The results in Table 5 clearly demonstrate that BRME achieves higher accuracy across most benchmarks, indicating a better ability to distinguish between preferred and non-preferred responses. Specifically, BRME outperforms the traditional RM by a notable margin across different categories such as General QA, Writing, Comprehension, and Math. The overall accuracy of BRME reaches 75.6%, compared to 74.5% for the traditional RM, showing that BRME's ability to model uncertainty provides a tangible advantage in distinguishing between correct and incorrect responses.\nIn the General QA category, BRME achieves a 1.2% improvement over the traditional RM, and similar improvements are observed in Writing (1.3%) and Math (1.8%). These results demonstrate the robustness of BRME across various types of tasks, where more accurate reward signals are"}, {"title": "C.4 ABLATION STUDY ON INTEGRATION STRATEGY", "content": "In addition to evaluating the min/max integration strategies, we conducted the same PPO experiment using the mean of all reward signals and compared it to other integration strategies, such as the min, max, and trade-off strategies. The mean strategy represents a baseline approach where all reward signals are averaged, which can smooth out the variability in individual rewards but may also mask valuable distinctions between reward sources. Comparing it with other strategies allows us to assess how different approaches to integrating reward signals impact the stability and performance of the model during training. The results are presented in Figure 9.\nThe performance of the mean strategy consistently falls between the min and max strategies, indicating that while averaging rewards offers a more balanced approach, it does not capture the advantages of more targeted integration strategies. Specifically, in the later stages of training (after 800 steps), the mean strategy shows a tendency toward performance decline, suggesting that the averaging process may dilute the reward signal over time, leading to suboptimal policy learning. This is especially evident in tasks requiring precise optimization, where overly smoothing the reward signal prevents the model from fully leveraging high-quality responses.\nOn most datasets where PPO has a significant effect, the reward-robust RLHF setting with a trade-off parameter of \u03bb = 0.6 outperforms the mean strategy. The trade-off strategy balances nominal"}, {"title": "D DATASET DESCRIPTIONS", "content": "In this section, we provide an overview of the datasets used for training and the benchmarks used for evaluation in our experiments. This includes a detailed description of the datasets utilized in the BRME training process and the PPO training pipeline, as well as the benchmarks employed to evaluate the performance of our models.\nTable 6 outlines the various datasets used for training BRME and the PPO models. These datasets span a wide range of task types, ensuring that the reward model is exposed to diverse examples during training, helping it generalize across different domains. In UltraFeedBack, we selected response pairs where the score is greater than or equal to 2 to form the chosen and rejected response pairs, which were then used to train the BRME. By focusing on high-quality response pairs, we aim to ensure that the reward model is robust and capable of accurately distinguishing between better and worse responses."}, {"title": "E PROOF FOR LEMMA 1", "content": "In this section we first reclaim the Lemma 1, and provide the proof details.\nRevisiting Lemma 1. Let the reward model provide a constant reward r(s, a) = c for all actions a \u2208 A and states s \u2208 S during PPO training. Then, the gradient of the PPO objective function with respect to the policy parameters \u03b8 is zero, implying that the actor cannot be optimized under such a reward model.\nProof. During PPO training, the objective function is given by:\n\u2113ePPO (\u03b8) = Et [min (rt(\u03b8) \u00c2t, clip (rt(\u03b8), 1 \u2212 \u03f5, 1 + \u03f5) \u00c2t)],\nwhere rt(\u03b8) = \u03c0\u03b8(at|st)/\u03c0\u03b8old(at|st) is the probability ratio between the current policy and the old policy, and \u00c2t is the advantage function. The advantage function is defined as:\n\u00c2t = Q(st, at) \u2212 V(st),\nwhere Q(st, at) is the state-action value function and V(st) is the state value function, both output by the critic model.\nIf the reward model provides a constant reward r(st, at) = c for all actions, the state-action value function and the state value function become:\nQ(st, at) = c/1\u2212\u03b3 V(st) = c/1\u2212\u03b3"}, {"title": "F FUTURE WORKS: HETEROLOGOUS REWARD FUSION", "content": "Heterologous Reward Fusion (HRF) aims to enhance the robustness and coverage of the uncertainty set in our reward models by incorporating multiple heterologous reward sources. This method involves combining several different reward models trained on diverse datasets, such as Baichuan233B Yang et al. (2023), Qwen2-72B Yang et al. (2024), and LLaMa3-8B Dubey et al. (2024). The key advantage of using heterologous models lies in the diversity of their training data, which produces more varied reward scores and helps to enrich the uncertainty set.\nBy integrating heterologous rewards, we aim to capture a broader spectrum of reward signals, leveraging the strengths of models trained on different datasets and with varied optimization goals. For example, integrating direct scoring from closed-source LLM APIs like GPT-4 alongside open-source models ensures a wider and more balanced reward distribution, implicitly utilizing data from diverse sources. This heterogeneity allows for a more comprehensive assessment of the model's performance across different scenarios.\nHowever, one of the primary challenges in heterologous reward fusion is that each reward source has different value ranges, making direct comparisons potentially unfair. To address this, we perform empirical reward normalization. We score a separate dataset, HH-RLHF Bai et al. (2022a), using each reward source and compute the mean and variance for each. During PPO training, we transform the rewards from each source by normalizing them based on their respective means and standard deviations. This normalization helps ensure that the reward signals from different models are comparable, allowing for a more fair integration of the rewards.\nTable 8 presents the results of our heterologous reward fusion experiment, showing the reward distribution characteristics (max, min, mean, and standard deviation) for each model, along with their accuracy. Accuracy here is defined as the proportion of instances where the model correctly identifies the chosen response as superior to the rejected one.\nHowever, it is important to note that we did not conduct end-to-end PPO experiments incorporating the full reward model pipeline in this exploration. The primary reason for this limitation is the significant computational resources and time required to carry out such experiments comprehensively. Running full-scale PPO experiments, especially when integrating multiple heterologous reward models, is computationally expensive and requires extended periods of training, particularly when dealing with large models like Baichuan2-33B and Qwen2-72B. Moving forward, our next steps include performing the full end-to-end PPO experiments to evaluate the impact of HRF on the performance of the trained policy."}]}