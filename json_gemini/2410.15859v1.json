{"title": "Mesa-Extrapolation: A Weave Position Encoding Method for Enhanced Extrapolation in LLMs", "authors": ["Xin Ma", "Yang Liu", "Jingjing Liu", "Xiaoxu Ma"], "abstract": "Large language models (LLMs), although having revolutionized many fields, still suffer from the challenging extrapolation problem, where the inference ability of LLMs sharply declines beyond their max training lengths. In this work, we conduct a theoretical analysis to better understand why No Position Encoding (NoPE) fails outside its effective range, as well as examining the power of Position Encoding (PE) in this context. Our findings reveal that with meticulous weave position, PE can indeed be extended beyond effective range. Our theorems establish that LLMs equipped with weave PE can achieve improved extrapolation performance without additional cost. Furthermore, we introduce a novel weave PE method, Mesa-Extrapolation, which utilizes a chunk-based triangular attention matrix and applies Stair PE to manage the final chunk. This method not only retains competitive performance but also offers substantial benefits such as significantly reduced memory demand and faster inference speed. Extensive experiments validate the effectiveness of Mesa-Extrapolation, demonstrating its potential as a scalable solution to enhancing LLMs' applicative reach. Our code is available at https://github.com/soacker/Mesa-Extrapolation.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) with their powerful in-context learning capabilities Brown et al. (2020) offer versatile solutions to a wide-range of intelligent applications. However, one pressing challenge, the extrapolation problem Del\u00e9tang et al. (2022) Zhang et al. (2022), dictates that the inference ability of LLMs sharply declines beyond their max training lengths, imposing a serious limitation on applications with long inputs. An naive solution is to extend the length of training samples. However, the inherent quadratic complexity of calculations presents practical challenges, demanding more resources, longer training time and higher cost.\nPositional encoding (PE) has become a pivotal component of Transformer architecture, to compensate for the overlooking of position information by the attention mechanism. In the realm of extrapolation capability, PE is considered as a key factor influencing the extrapolating ability of LLMs. A few PE approaches, such as the popular ROPE Su et al. (2023) and ALiBi Press et al. (2021), claim to offer improved extrapolation capabilities and have gained widespread usage in industrial applications. Meanwhile, a counter-narrative has emerged. Some works demonstrate that Transformer can achieve better extrapolation capabilities by removing position encoding (NoPE) Kazemnejad et al. (2023), and contend that the mask already plays a significant role in capturing position information."}, {"title": "2 Background", "content": "Since the self-attention mechanism itself does not contain position information, PE components have become an integral part of the Transformer architecture. Cmmon choices for PE are either absolute, where each absolute position (e.g., 1, 2, 3, ...) is directly represented, or relative, where the distance between tokens is used as positional information. Absolute Position Embedding (APE) Vaswani"}, {"title": "3 Model Extrapolation: NoPE vs. Weave PE", "content": "We mainly consider relative PE methods and formally define their self-attention dot product as a function $f_{PE}$, which takes the query $q_t$ located on position $t$, the key $k_i$ located on position $i$, and their relative positions $t - i$ as input parameters, as follows:\n$(q_t, k_i) := f_{PE}(q_t, k_i, t - i)$\nwhere $f_{PE}$ denotes a relative PE method such as ROPE or ALiBi.\nFor ALiBi,\n$\\langle q_t, k_i\\rangle := f_{ALiBi}(q_t, k_i, t - i) = q_t^T k_i - (t - i) \\cdot C^{m+1}$\nwhere $m$ is head index and $C = 2^{-2^{-log_2(\\text{#heads}+3)}}$\nWhereas for NOPE,\n$(q_t, k_i) := q_t^T k_i$\nBased on Equ.1 we formally define weave PE as follows:\n$(q_t, k_i) := f_{\\text{weavePE}} (q_t, k_i, t - i) = f_{PE}(q_t, k_i, W(t - i))$\nwhere $W$ is a weave function which takes the relative position $t - i$ as input parameter.\nFor example, ReRoPE Su (2023b) can be considered as an example of weave PE, with its $W$ function defined as follows:\n$W(t - i) := \\begin{cases}\nt - i, & t-i<N \\\\\nN, & t-i>N\n\\end{cases}$\nwhere $N$ is a constant. ReRoPE's dot-product attention is:\n$\\langle q_t, k_i\\rangle := f_{\\text{ReROPE}}(q_t, k_i, t - i) = f_{\\text{ROPE}}(q_t, k_i, W(t - i)) = q_t^T R^{W(t-i)\\theta} k_i$\nwhere $R$ is a rotation matrix that rotates $W(t \u2013 i)\\theta$ radians. This is based on ROPE's dot-product attention:\n$\\langle q_t, k_i\\rangle := f_{\\text{ROPE}}(q_t, k_i, t - i)) = q_t^T R^{(t-i)\\theta} k_i$"}, {"title": "3.2 Motivation", "content": "Chen et al. (2023) explores the evolution of hidden state values and reveals a noticable phenomenon: as the position increases, the hidden state values will explode. This finding appears consistent with observed failures in extrapolation. Through probe experiments (refer to Appendix F), we investigate"}, {"title": "3.3 Theoretical Analysis", "content": "Based on our observations, thresholds in different dimensions can serve as either upper or lower bounds. For simplicity, we treat thresholds solely as lower bounds and make the following assump-tions:\nAssumption. In LLM, there is a lower bound as threshold $H$ for the hidden state value $o$ in specific dimension and specific layer. Let $M$ be the max window length for LLM. Predefine query $W_Q$, key $W_K$, value $W_V$ and output $W_O$ matrices, and feed-forward sub-layer $W_1$, $W_2$ matrices. When $o > H$, LLM extrapolates successfully. Once $o < H$, LLM extrapolation fails.\nBuilding upon this assumption, theoretical results for NoPE exceeding the effective window length are as follows:\nTheorem 3.1 (NoPE Extrapolation). Let $x = [< bos >,x_1,...,x_T]$ be an input sequence of length $T + 1$ to the model. Then, there exists $W_Q, W_K, W_V, W_O, W_1$, and $W_2$ matrices, such that when $T < M, o_T > H$; and when $T > M, o_T < H$.\nFull proof is given in Appendix E.1. This theorem reveals the internal mechanism of NoPE extrapola-tion as the input length changes. The theoretical results for PE are as follows:\nTheorem 3.2 (PE Extrapolation). Let $x = [< bos >,x_1,...,x_T]$ be an input sequence of length $T+1$ to the model. Consider a simple relative PE schema where dot product between query $q_t$ and key $k_i$ at positions $t$ and $i$ ($t > i$) can be expressed as:$\\langle q_t, k_i\\rangle := q_t^T k_i - (t-i)$. Then, there exists $W_Q, W_K, W_V, W_O, W_1$, and $W_2$ matrices, such that when $T < M, o_T > H$; and when $T > M, o_T < H$.\nFull proof is given in Appendix E.2. Theorems 3.1 and 3.2 state the failure of length extrapolation in NoPE and PE, respectively.\nBuilding on Theorem 3.2, we further investigate the case for carefully orchestrated weave PE. The theoretical result is as follows:\nTheorem 3.3 (Weave PE Extrapolation). Let $N$ be a positive constant. Consider a simple weave PE extrapolation schema: when $t - i < N, W(t - i) = t \u2013 i$; and when $t \u2013 i > N, W(t \u2013 i) = N$. Then, the attention dot product is fixed as below:\n$\\langle q_t, k_i\\rangle :=\\begin{cases}q_t^T k_i - (t-i) ,& t-i<N\\\\q_t^T k_i - N ,& t-i > N\\end{cases}$, where $N < M$. Then, applying $W_Q, W_K, W_V, W_O, W_1$, and $W_2$ matrices from Theorem 3.2, we have when $T > M, o_T > H$.\nFull proof is given in Appendix E.3. This theorem suggests that for existing LLMs relying on PE, simply weaving its relative positional encoding can effectively extend the input window.\nThrough Theorem 3.1 and Theorem 3.2, we formulate pertinent theoretical models for NoPE and PE, respectively, shedding light on the intricate relationship between extrapolation and the effective window length. Building upon these findings, Theorem 3.3 delves deeper into the realm of explicit PE, revealing that a well-designed weave PE scheme can effectively broaden the original effective"}, {"title": "3.4 Validating Extrapolation Using Observed Thresholds", "content": "Further, we design probe experiments (refer to Appendix F.3 for more results) to validate the observed phenomena and our theorems, as shown in Figure 2. From Figure 2, two observations are noted: Firstly, for hidden state values of the same dimension, the first layer undergoes minimal change, while the second layer exhibits a more pronounced transition. Exceeding the threshold implies extrapolation failure. This observation aligns with our theoretical model construction, where the first layer primarily refines positional information, with significant signal changes occurring in the second layer, as demonstrated in Theorem 3.2. Secondly, based on observational thresholds, when the length of the input sequence is around 12k, the values of hidden state corresponding to Dynamic-NTK Liu et al. (2023) surpass the threshold, implying extrapolation failure. Conversely, for ReROPE Su (2023b), extrapolation succeeds. These two predictive outcomes corroborate with subsequent experimental results."}, {"title": "4 Mesa-Extrapolation", "content": "In this section, we begin by introducing a novel weave position encoding method, termed Stair Position Encoding (Stair PE). Following this, we propose a chunk-based triangular attention matrix. Building on these concepts, we introduce the implementation of Mesa-Extrapolation. Lastly, we discuss the theoretical properties of these innovations."}, {"title": "4.1 Stair PE", "content": "Following the concept of weave PE in Equ.2, we define a novel weave PE method, namely Stair PE as follows:\n$\\langle q_t, k_i\\rangle := f_{\\text{StairPE}}(q_t, k_i,t - i) = f_{PE}(q_t, k_i, W(t - i)), \\text{ and } W(t - i) := \\begin{cases}\nt-i,& t-i<N \\\\\nI,& t-i>N\n\\end{cases}$\nwhere $I = N + \\lfloor \\frac{t-i-N}{E}\\rfloor$. $N$ denotes the extrapolated position, and $E$ denotes the extrapolated width. Both $N$ and $E$ are positive constants. Stair PE can be applied to existing relative PEs such as ROPE and ALiBi. For example, for ROPE:\n$\\langle q_t, k_i\\rangle := f_{\\text{StairROPE}}(q_t, k_i, t - i) = f_{\\text{ROPE}}(q_t, k_i, W(t - i)) = q_t^T R^{W(t-i)\\theta} k_i$\nTaken a sequence of length 10 as an example, the right subplot of Figure 1 shows that the relative positions generated by PE (Both RoPE and ALiBi) are linear, while those generated by Stair PE"}, {"title": "4.2 Chunk-based Triangular Attention Matrix", "content": "We design a chunk-based triangular attention matrix as shown in the left subplot of Figure 1. To achieve approximate linear memory consumption and computational speed, we further split the triangular attention matrix into several chunks and concatenate these chunks. We segment the input sequence into several sub-sequences according to DynamicSplit function (defined in Appendix 2), which divides a sequence into sub-sequences of equal length, with the exception of the first and last sequences. The length of each sub-sequence is determined by both the input token length and the max training length. Each of the generated sub-sequences then undergoes a self-attention operation to generate a corresponding chunk. That is, a sub-sequence of length $l$ will generate a corresponding chunk with the size $l \\times l$."}, {"title": "4.3 Implementation", "content": "Mesa-Extrapolation mainly utilizes the chunk-based triangular attention matrix and Stair PE. Notice that regular PE (such as ROPE or ALiBi) is applied to all chunks except for the last chunk, for which Stair PE is applied. For the last chunk, all previous chunks are concatenated, and Stair PE is used to rearrange relative positional encod-ing to achieve extrapolation beyond the effective window length.\nIn summary, the process of Mesa-Extrapolation mainly contains four steps (Algorithm 1): The first three steps correspond to the prefill stage, which is used to calculate all input tokens. The last step corresponds to the decoding stage, which is used to generate next-token one by one.\nFirstly, DynamicSplit function segments the input sequence, and the first segmented sub-sequence is fed into LLM to generate the first attention matrix chunk (line 3 in Algo.1). Secondly, subsequential sequences are iteratively processed while simultaneously feeding the key and value pairs of the first chunk into LLM to generate subsequent chunks (line 6-10 in Algo.1). Thirdly, the last sub-sequence is processed by concatenating the key and value pairs of all previous chunks together and using Stair PE to modify the relative positional encoding. Then, it is fed into the LLM to produce the last chunk (line 11-12 in Algo.1). Finally, Stair PE is applied to process the current token and cached Key and Value pairs to generate next-token one by one (line 13-14 in Algo.1). We establish the effectiveness of our proposed Mesa-Extrapolation in the next section."}, {"title": "4.4 Theoretical Properties", "content": "Theorem 3.2 establishes a theoretical measurement for evaluating the effectiveness of extrapolation. We consistently apply this indicator, along with adjustments in relative positioning using Stair PE, to validate the feasibility of extrapolation. The result is as below:\nCorollary 4.1 (Mesa Extrapolation). Let $N$ be a positive constant. Consider a simple Stair PE extrapolation schema, and the attention dot product is fixed as:\n$\\langle q_t, k_i\\rangle : f_{\\text{stairPE}} (q_t, k_i, t-i) = \\begin{cases}q_t^T k_i - (t-i)& t-i<N\\\\q_t^T k_i-I& t-i\\geq N\\end{cases}$, where $N < M$, $I = N + \\lfloor \\frac{t-i-N}{E}\\rfloor$, and the extrapolated width $E$ is a constant. Then, Apply $W_Q, W_K, W_V, W_O, W_1$, and $W_2$ matrices from Theorem 3.2. Although $T > M$, it still $o_T > H$.\nFull proof is provided in Appendix E.4. We prove that Mesa-Extrapolation can effectively extrapolate outside the max window length."}, {"title": "5 Experiments", "content": "In this section, we validate the performance of Mesa-Extrapolation through experiments measured over multiple metrics. We choose GovReport Huang et al. (2021), Pile Gao et al. (2020), LongBench Bai et al. (2023), and LongEval Krishna et al. (2023) datasets, and also generate a passkey dataset, which has been integrated in the code warehouse. More experimental details and results are referred to Appendix B and C.\nSince our method is completely free plug-in and does not require fine-tuning, we choose methods of this type for comparison, including: model self (Origin), ReRoPE Su (2023b), Leaky-ReRoPE Su (2023b), Dynamic-NTK Liu et al. (2023), LM-Infinite Han et al. (2023), and Streaming-LLM Xiao et al. (2023).\nWe evaluate Mesa-Extrapolation using three prominent LLM families: LLAMA Touvron et al. (2023a) Touvron et al. (2023b) (including LLaMA-3B (Open-LLaMA-3B), LLaMA2-7B-Chat, and Vicuna-13B-V1.3), MPT Team (2023) (including MPT-7B), and PyThia Biderman et al. (2023) (including PyThia-6.9B and PyThia-12B). Notably, LLaMA and PyThia incorporate ROPE Su et al. (2023), whereas MPT employs ALiBi Press et al. (2021) \u2013 two of the most influential PE techniques in recent research. Furthermore, we validated our approach using the Phi-3-mini-128k-instruct model Microsoft (2024) (refer to Appendix C.9).\nWe also conduct ablation experiments (refer to Appendix C.6). We use a 2xA800 80GB NVIDIA GPU server as the experimental environment and adopt the PyTorch framework."}, {"title": "5.1 Evaluation on Passkey Retrieval Tasks", "content": "We assess the accuracy of Mesa-Extrapolation using the generated passkey dataset. This dataset comprises samples of varying lengths, each storing a random password at a random position. The sample length initiates at 1024 and increments by 1024. Simultaneously, 100 samples are randomly generated for each length. The proportion of correct answers found by LLMs is calculated for each input length.\nFig.3 shows the results of 6 LLMs on passkey retrieval task. The LLaMA families can employ various methods, including Origin, ReRoPE, Leaky-ReROPE, Dynamic-NTK, LM-Infinite, Streaming-LLM, and our Mesa-Extrapolation. Note that these methods are model-specific and may not be universally applicable across all model series. For the MPT model, Origin, Streaming-LLM, ReROPE, and Mesa-Extrapolation can be utilized. Similarly, for the PyThia model, Origin, Streaming-LLM, and Mesa-Extrapolation can be applied.\nAnalyzing the LLaMA model series reveals that weave PE-based methods, including ReROPE, Leaky-ReROPE and Mesa-Extrapolation, achieve superior extrapolation capabilities. Additionally, under our existing hardware constraints, Mesa-Extrapolation demonstrates longer extrapolation capabilities."}, {"title": "5.2 Evaluation on Language Modeling", "content": "We further assess the fluency of Mesa-Extrapolation utilizing the perplexity metric. Results evaluated on the Pile dataset are presented in Fig.4. X-axis represents the length of the input token, while the Y-axis corresponds to NLL (Negative Log-Likelihood) values. It can be observed that the NLL value of Origin consistently increases when the maximum training length is exceeded. Other methods maintain low NLL values. LM-Infinite performs marginally better on Vicuna-13B. Dynamic-NTK"}, {"title": "5.3 Evaluation on Summary of Tasks", "content": "We conduct a summary task using the GovReport dataset and employ ROUGE ROUGE (2004) (ROUGE-1/2/L) as evaluation metrics. ROUGE assess overlapping N-grams by comparing the generated text with reference answers.\nFor the GovReport dataset, we segment the range from 3*1024 to 11*1024 based on sample length, with each interval of 1024 units. A test set is created by randomly selecting 8 samples from each interval. We choose LLaMA2-7B-Chat as the evaluated LLM. The experimental results for ROUGE is presented in Tables 1 below:\nIn Table 1, we record the average scores of Rouge-1, Rouge-2, and Rouge-L within each interval. It is evident that once the effective input window is exceeded, the performance of Origin and Streaming-LLM declines rapidly, rendering it useless. For LM-Infinite, the scores exhibit a slight decrease as the length increases. Dynamic-NTK shows slightly better performance within 11k. However, combined with the Fluency experiment on Fig.4, it seems that the effective extrapolation range of Dynamic-NTK cannot exceed 12k, which is consistent with the threshold observed in Fig.2. Weave PE-based methods, including ReRoPE, Leaky-ReRoPE and Mesa-Extrapolation maintain similar generation quality as the length increases. Note our Mesa-Extrapolation shows slight variability in performance within mid-length (8k-11k) in the summary task. We speculate that with a fixed extrapolation width param (E = 50), the 7k-11k range may spread the model's attention more thinly compared to the 4k-6k range. We hypothesize that optimizing the extrapolation width could alleviate or improve performance in the 7k-11k range."}, {"title": "5.4 Latency & Memory Usage", "content": "To compare actual memory consumption and inference speed, we conduct experiments using both the 3B and 7B versions of the LLaMA model. The results are presented in Fig.5. In Fig.5, the X-axis represents the input token length, the left Y-axis denotes memory usage, and the right Y-axis indicates decoding time. It is noteworthy that decoding time is closely related to memory usage, primarily from the computation of the attention matrix.\nObserving Fig.5, both memory usage and decoding time for Origin and Dynamic-NTK exhibit a quadratic trend. Similarly, ReRoPE and Leaky-ReRoPE exhibit the highest memory usage and decoding time, showcasing a quadratic trend, which aligns with our analysis (refer to Appendix C.7). Although LM-Infinite demonstrates a linear trend, its increase is substantial. In contrast, Mesa-Extrapolation method also exhibits a linear trend but significantly outperforms other methods in terms of memory usage and decoding time. Furthermore, as the input length increases, this trend becomes more pronounced."}, {"title": "6 Conclusion", "content": "Our study addresses the critical challenge faced by Large Language Models (LLMs) when confronted with longer input lengths, commonly referred to as the extrapolation problem. Through theoretical exploration, we uncover the underlying mechanisms of this challenge, shedding light on the reason of extrapolation failure for both NoPE and PE. Furthermore, we present theoretical evidence demon-strating the potential for effective extrapolation using Weave PE. Based on Weave PE, we introduce a practical solution called Mesa-Extrapolation, which strategically organizes input tokens into chunks to achieve competitive performance with minimal resource usage. Empirical validation demonstrates its effectiveness. Our work not only advances the understanding of the extrapolation problem but also offers a practical solution, with a complete free plug-in for LLMs.\nLimitations. Mesa-Extrapolation is a plug-and-play method that does not require additional fine-tuning. However, previous work, \"NTK-aware\" ntk (2023) also shows that applying further fine-tuning to plug-in extrapolation is possible. Therefore exploring fine-tuning based on Mesa-Extrapolation can be an interesting next step.\nBroader Impacts We contend that the significance of completely free plug-in extrapolation method lies in two aspects. Firstly, it enables the expansion of the effective window length of already trained LLMs with no additional cost. Secondly, it allows for training LLMs from scratch with short texts and subsequently expanding their effective window length with a free plug-in extrapolation method, which can greatly help the industry improve the extrapolation capabilities of diverse LLMs."}, {"title": "A Related Work", "content": "Extrapolation for PE A distinct line of research is dedicated to refining PE for enhanced extrapola-tion capabilities. Notable contributions include the introduction of Rotary Position Encoding (RoPE) Su et al. (2023), implementing relative PE through absolute position information. Similarly, Press et al. (2021) proposes a novel PE method ALiBi, fusing position information by directly introducing the relative position distance term in dot multiplication. In addition, there are other PE methods such as absolute position embedding (APE) Vaswani et al. (2017) and T5's Relative PE Raffel et al. (2020).\nAnother school of research focuses on enhancing the extrapolation performance of existing LLMs, categorized by whether to include further training. The first subcategory involves further fine-tuning, enlarging the effective window length by training LLMs on longer input texts. Chen et al. (2023) demonstrates that Position Interpolation (PI) method has a superior fine-tune effect, resulting in extended extrapolation capabilities with fewer fine-tuning steps. Mohtashami & Jaggi (2023) utilizes a new landmark token to represent individual input blocks, and enables model to select relevant blocks by further training, allowing for longer extrapolation. Zhang et al. (2024) follows similar idea by training a special token. Bertsch et al. (2023) proposes Unlimiformer, a k-nearest-neighbor (kNN) indexed encoder-decoder Transformer, enhancing efficiency by retrieving top-k keys for each decoder layer. Despite claiming support for decoder-only Transformer, differences in retrieval results across decoder layers may lead to potential failure. Tworkowski et al. (2023) introduces Focused Transformer (FOT), utilizing a contrastive learning-inspired training process to enhance the (key, value) space structure and enable effective context extension. The second subcategory explores methods that require no fine-tuning yet offer improved extrapolation through plug-ins. Su (2023b) introduces Rectified Rotary Position Embeddings (ReRoPE), a method that rectifies extrapolated relative positions based on RoPE within a specified interval to extend the effective window length. In addition, Leaky-ReRoPE offers an alternative by allowing a controlled leakage of position extrapolation within an interval. Both approaches are well-suited for LLaMA model families Touvron et al. (2023b) Touvron et al. (2023a), eliminating the need for fine-tuning. These methods do suffer from some drawbacks, such as twofold increase in memory consumption and longer inference time. We refer to this class of methods that achieve extrapolation by weaving the relative positions of PE without fine-tuning as Weave PE. We also notice that, recent work, InfLLM Xiao et al. (2024), proposes an additional memory units, which lookup token-relevant units for attention computation. In addition, it uses a modified encoding scheme similar to ReROPE to achieve longer extrapolation. It is worth noting that the methods of the weave PE class can be applied seamlessly to these new designs. However, there is still no theory to explain why the methods of weave PE class can work. In a different line of inquiry, ntk (2023) proposes the \"NTK-aware\" method by drawing from the Neural Tangents (NTK) idea, which explores the high-frequency extrapolation and low-frequency interpolation concept for RoPE. Based on \"NTK-aware\", recent works bloc97 (2023), Peng et al. (2023), Roziere et al. (2023), Liu et al. (2023) perform further fine-tuning for optimal results.\nHan et al. (2023) proposes LM-Infinite, which employs a A-shaped mask to prevent surpassing the effective window length by discarding central tokens. However, this design choice inevitably results in a loss of information. Likewise, Streaming-LLM Xiao et al. (2023) adopts a similar strategy to avoid surpassing the effective window length by discarding a portion of input tokens.\nExtrapolation for NoPE There also exists a counter perspective asserting that the position informa-tion of an input sequence can be perceived without utilizing PE. In a comprehensive experimental comparison presented in Kazemnejad et al. (2023), the decoder-only Transformer is shown to exhibit superior extrapolation properties with no position encoding (NoPE). Haviv et al. (2022) conjectures that causal attention enables the Transformer to infer position information without the help of PE, demonstrating its comparable performance with standard Transformer models through probing exper-iments. These new studies pose a key challenge regarding the choice of whether using PE or not in Transformer architecture.\nTheorems for Extrapolation We mainly focus on Transformer architecture. Although the Trans-former architecture is considered as a \"black box\", there are ongoing efforts trying to explain its inner workings. Von Oswald et al. (2023) begins by providing a specific weight construction that elucidates the mechanistic understanding of in-context learning within optimized Transformers. Lindner et al. (2023) introduces interpretability for Transformers through programming, allowing the derivation of a program Transformer architecture tailored for specific tasks. In Han et al. (2023), the"}, {"title": "C.7 Theoritical Speed & Memory", "content": "We assess the computational memory usage and inference time about decoding speed on various methods.\nTable 4 shows the theoretical result about the memory usage. For Origin, the attention matrix calculation requires the current token to compute the attention score with each previous token, resulting in a memory footprint of $O(n^2)$. Dynamic-NTK only alters the angle base, making it different from the Origin but still quadratic. For ReRoPE and Leaky-ReRoPE, as showed in Su (2023b), since the attention matrix needs to be calculated twice, their memory footprint is $2 \\times O(n^2)$. For Mesa-Extrapolation, its strategy involves splitting chunks while avoiding the quadratic term. Accounting for splicing in the last chunk, its total memory usage scales proportional to $O((2 + \\sqrt{2})n)$. LM-Infinite and Streaming-LLM adopt a similar A-shaped mask, that scales proportional to $O((1 + \\sqrt{2})n)$."}, {"title": "FProbe Experiment Visualization", "content": "We hypothesize that when the input length surpasses the effective window length of the model, some dimensions' values in the exceeded positions will experience a jump as the position changes.\nTo investigate this jump phenomenon's correlation with extrapolation failure, we design the following experiment: we adopt a standardized input by repeating the word \"hello\" 8000 times, resulting in 8001 tokens (automatically fill in initial token) after tokenization. For a more accurate explanation, we take the LLaMA2-7B-Chat model and list the sequences converted by the tokenizer, as follows:\ntokens = [1,22172, ..., 22172]\nwhere 1 denotes the initial token $ )$, and 22172 denotes the word \"hello\". It's noted that the initial token $ )$ is filled in automatically.\nF.1 Normal Case\nWe input this token sequence into the model and observe the hidden state. We focus on the hidden states produced by the first 11 layers, specifically selecting the position intervals from 4000 to 5000 and the last 1000 positions. We then concatenated the hidden states from these two intervals. This selection was deliberate, considering that the LLaMA2-7B-Chat model's training length is 4096, implying the effective input window is in proximity to this location.\nFollowing this, we created a matrix graph, yielding the following results Fig.11:\nIn Fig.11, the X-axis denotes the position of the input token, ranging from 0 to 1000, representing tokens at positions 4000 to 5000. The scale of 1000-2000 represents tokens at positions 7001 to 8001. The Y-axis signifies the token dimension, ranging from small to large. The LLaMA2-7B-Chat model has a total of 4096 dimensions, and we display the first 640 dimensions in Y-axis. Red color means greater than 0, and blue color means less than 0. Fig.11 shows some noticeable jump from 0-th to 640-th dimensions, especially at the 1000 scale for the original model.\nF.2 Extrapolation Case\nGiven the effectiveness of the extrapolation method ReRoPE in extending to lengths of up to 8k, we apply it to the LLaMA2-7B-Chat model. Employing the same conditions as those in the Normal Case settings, the results are as follows:\nIn Fig.12, the obvious jumping phenomenon is successfully suppressed. It can be seen that each dimension at different positions still maintains consistent values. This illustrates that by suppressing sudden changes in the values of these dimensions, extrapolation can be made successful even outside the effective window length."}]}