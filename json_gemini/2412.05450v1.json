{"title": "Promoting Cooperation in the Public Goods Game using Artificial Intelligent Agents", "authors": ["Arend Hintze", "Christoph Adami"], "abstract": "The tragedy of the commons illustrates a fundamental social dilemma where individual rational actions lead to collectively undesired outcomes, threatening the sustainability of shared resources. Strategies to escape this dilemma, however, are in short supply. In this study, we explore how artificial intelligence (AI) agents can be leveraged to enhance cooperation in public goods games, moving beyond traditional regulatory approaches to using AI as facilitators of cooperation. We investigate three scenarios: (1) Mandatory Cooperation Policy for AI Agents, where AI agents are institutionally mandated always to cooperate; (2) Player-Controlled Agent Cooperation Policy, where players evolve control over AI agents' likelihood to cooperate; and (3) Agents Mimic Players, where AI agents copy the behavior of players. Using a computational evolutionary model with a population of agents playing public goods games, we find that only when AI agents mimic player behavior does the critical synergy threshold for cooperation decrease, effectively resolving the dilemma. This suggests that we can leverage AI to promote collective well-being in societal dilemmas by designing AI agents to mimic human players.", "sections": [{"title": "1 Introduction", "content": "The tragedy of the commons [1] refers to a fundamental social dilemma where individual rational actions lead to collectively undesired outcomes. In systems involving shared resources or public goods, individuals who cooperate by contributing are often disadvantaged compared to defectors who withhold their contributions. Because the public good is evenly distributed among all participants, defectors achieve higher personal gains than their cooperative counterparts because they reap the benefits without incurring the cost of contribution. This imbalance undermines cooperative behavior and threatens the shared resources' sustainability, posing significant challenges across disciplines such as economics, environmental science, and public policy.\n\nThe quintessential tool to study the emergence, evolution, and persistence of cooperation is Evolutionary Game Theory (EGT) [2-4]. Within EGT, the Public Goods game [5] plays a prominent role: it is a multi-player extension of the well-known Prisoner's Dilemma game [3] that makes it possible to model the tragedy of the commons in terms of a cooperation dilemma. At its core, the public goods game presents par-ticipants with a choice: to cooperate (C) by contributing a cost (or ante, here set to 1 without loss of generality) to the common good, or to defect (D) by withholding that contribution. The total contributions are amplified by a multiplication factor $r$, symbolizing the synergy between cooperating parties. This enhanced public good is then evenly distributed among all participants, regardless of their contributions (see Figure 1). In such a game, defectors always obtain higher rewards than cooperators in the same group, even though they could reap an even higher return if everyone cooper-ated. However, once the synergy factor $r$ becomes high enough ($r > k + 1$, where $k+1$ is the group size), a defecting player fares worse than if they had cooperated instead. To promote cooperation, we ask: \"How can we change the game so that cooperation becomes the rational choice for synergies where $r < k + 1$?\"\n\nAlthough numerous mechanisms have been proposed to promote cooperation in public goods dilemmas \u2013 including the green beard effect [6], reciprocity [7], spatial dynamics [8], group selection [9], costly punishment [5, 10-12], institutional incen-tives [13] and resource redistribution [14] \u2013 these approaches do not account for the transformative impact of AI integration. Recognizing this gap, we pursue a novel exploration into how AI agents can be strategically leveraged to enhance cooperative behavior, an aspect previously unaddressed in the literature.\n\nGovernments are fundamentally responsible for establishing institutions that address the tragedy of the commons [15]. By enacting legislation that incorpo-rates mechanisms like costly punishment and resource redistribution, they create regulations, norms, and incentives that make cooperation more advantageous than defection [5, 13, 16]. For instance, legal penalties for free-riding discourage defection,"}, {"title": "2 Results", "content": "We will test the impact that different policies regulating AI agents that participate in the public goods game have on player behavior, using an agent-based evolutionary computational model. A population of players, each defined by a probability of coop-erating in a public goods game, evolves under selection and mutation over thousands"}, {"title": "2.1 Mandatory Cooperation Policy for AI Agents", "content": "A governing body might attempt to promote cooperation by forcing AI-controlled agents to always cooperate, with the idea of ensuring that at least the agents behave\n\nwell and support the public good, potentially promoting cooperation among human players (see Figure 3 left panel). However, the calculation that bounded the region in which a dilemma is present showed that the region does not depend on the density of cooperators, which makes it unlikely that influencing the density of cooperators will not change the boundaries and, therefore, not alleviate the dilemma. We experimen-tally confirm this prediction by creating an evolutionary scenario that implements the mandatory cooperation policy for AI agents.\n\nWe find the probability of cooperation after evolutionary optimization to be unaf-fected by the infusion of cooperating AI agents (see Figure 4 A and C); even though the frequency of cooperating agents rises (see Figure 4 B). As such, mandating AI agents to always cooperate only increases the cooperation with the population in proportion to the density of AI agents $p_A$, but does not encourage players to cooperate."}, {"title": "2.2 Player-Controlled Agent Cooperation Policy", "content": "In this scenario, the government does not regulate agent behavior in any way. Still, the AI companies set their agent's policies, or whoever owns or deploys an AI agent does so (see Figure 3 middle panel). To model this, each player still has a probability of cooperating ($p_c$) that can adapt over the course of evolution to the conditions of the current scenario. In addition, each player also has at their disposal an auxiliary probability ($p_{AC}$) that is used to determine the probability of an agent cooperating within its neighborhood. This auxiliary probability is also optimized by evolution. As a consequence, depending on the AI agent density, the density of cooperators around the central player is determined by the likelihood of other (non-AI agent) players and its probability reserved for agent cooperation ($p_{AC}$).\n\nTheir payoff, however, is now not only dependent on their own cooperation proba-bility $p_c$ but also on their auxiliary probability for agent cooperation ($p_{AC}$). Evolution is optimizing both probabilities. If it is, for example, beneficial for a player to coop-erate while also surrounding itself with cooperating agents, both probabilities should be maximized ($p_c = 1.0$ and $p_{AC} = 1.0$).\n\nNote that the central player only pays its own ante when cooperating. In contrast, an AI agent would pay for its own cost to cooperate even though the central player controls its probability of cooperating. As an illustration, if an AI-driven car is pro-grammed by a driver to give way to them coming from an on-ramp, the AI agent having to wait does not cost the driver anything.\n\nWhile this scenario can increase the likelihood of cooperators, we again do not expect this to reduce the synergy threshold for the player to engage in cooperation following the previous logic. We indeed find the synergy factor to be unaffected (see Figure 5 A and C). While the controlled agents become cooperators the moment they appear in the neighborhood ($p_A > 0.0$) as this maximizes the payoff of the player controlling them ($P_D(N_C) < P_D(N_C+1)$) (see Figure 5 B), this scenario does not affect the probability of the central player to cooperate, and thus the dilemma is not lifted."}, {"title": "2.3 Agents Mimic Players", "content": "Having analyzed the shortcomings of both mandating cooperation and allowing players to control AI agents, we recognize the need for a fundamentally different policy. These\n\ninitial strategies fall short of removing the dilemma because they do not alter the critical point at which cooperation becomes advantageous for players.\n\nFrom previous experience, we hypothesized that incentivizing players to cooperate requires increasing the payoff associated with cooperative actions. This realization led us to propose an innovative policy wherein AI agents mimic the behavior of the central player. In this paradigm, if the central player chooses to cooperate, the neighboring agents (proportional to the agent density $p_A$) will also cooperate (see Figure 3, right panel). This approach establishes a reciprocal relationship between agents and players, effectively reshaping the incentives and promoting cooperative behavior.\n\nAlthough the public goods game is not an iterative game where agents can estimate the likelihood of players cooperating based on previous actions, we can model such a mechanism by defining the AI agents' cooperation probability to be identical to that of the central player. Thus, depending on the agent density $p_A$, players will encounter groups where agents mirror player behavior. If the central player is a cooperator, the likelihood of having cooperating neighbors increases proportionally to the AI agent density. This effectively enhances the expected payoff for cooperators, as they are more likely to be in groups with other cooperators.\n\nIf $p_A$ is the likelihood that a peripheral player mimics the strategy of the focal player, then if the focal player is a cooperator, the number of defectors is reduced from $N_D$ to\n\n$\u039d\u2081 = (1 \u2013 \u03c1\u0391)ND$. (3)\n\nOn the other hand, if the focal player is a defector, the number of cooperators is reduced by mimicry to\n\n$N'C = (1 - \u03a1\u0391)NC$. (4)\n\nWe can now calculate the cooperation condition PC-PD \u2265 0 using these substitutions. We find\n\n$\\frac{r(k - N + 1)}{k+1} - 1> \\frac{N'C}{k+1},$ (5)\n\nwhich simplifies to\n\n$r\u2265 \\frac{k+1}{Pak+1}$ (6)\n\nSurprisingly, this relation is independent of the number of cooperators and defectors in each neighborhood and only depends on the mimicry probability (here given by the AI agent density $p_A$) and the group size. It reduces to the dilemma $r > k + 1$ in the limit $p_A = 0$, and completely eliminates the dilemma when $p_A = 1$. Figure 6 shows the numerical experiments implementing this scenario and confirms the prediction Eq. (6). Simply put, agents that mimic player behavior reduce the synergy threshold r so that cooperation can evolve at much-reduced synergies, promoting collaboration within the group and thus changing player behavior."}, {"title": "3 Discussion", "content": "We explored the impact of different policy scenarios on promoting cooperation in the public goods game, specifically in the context of integrating AI agents into human social systems. Our findings highlight the nuanced role that AI agents can play in influencing human cooperative behavior, depending on how their actions are regulated or programmed.\n\nWe first examined the Mandatory Cooperation Policy for AI Agents, where AI agents are forced institutionally to always cooperate. Contrary to intuitive expecta-tions, this policy did not enhance cooperation among human players. While the overall frequency of cooperative actions increased due to the agents' enforced cooperation, the critical threshold for human players to find cooperation advantageous remained unchanged. This outcome aligns with theoretical predictions that the critical point in the public goods game is independent of the frequency of cooperators in a player's immediate environment [1, 11]. The inability of this policy to shift the cooperative dynamics (and thus lift the dilemma) suggests that simply injecting cooperative agents into the system is insufficient to influence human behavior in the desired manner.\n\nNext, we investigated the Player-Controlled Agent Cooperation Policy, where human players determine the cooperation probability of AI agents in their vicinity. Despite the potential for players to benefit from surrounding themselves with cooperative agents, the evolutionarily optimized strategies did not lead to increased cooperation among players themselves. Players evolved to maximize their own payoffs by programming agents to cooperate while choosing to defect themselves, exploiting the cooperative agents without reciprocating. This mirrors real-world scenarios where individuals might leverage AI systems for personal gain without contributing to the collective good, highlighting a potential pitfall in allowing unrestricted control over AI behavior."}, {"title": "3.1 Implications for Policy and Governance", "content": "Our findings offer vital insights for policymakers and institutions aiming to foster coop-erative behavior in an era increasingly dominated by AI technologies. By introducing and validating a novel policy where AI agents mimic human behavior, we provide a transformative framework that could redefine strategies for enhancing cooperation in complex social systems. Mandating cooperation from AI agents or allowing users to control agent behavior without oversight may not lead to the desired increase in human cooperation. Instead, policies that encourage AI systems to respond adaptively to human behavior -specifically by mimicking or reciprocating actions - could foster more cooperative interactions.\n\nThe approach outlined here aligns with concepts of responsive regulation and adap-tive governance, where rules and systems are designed to be flexible and responsive to the behaviors of individuals within the system [15, 20]. By integrating AI agents that promote reciprocity, governments and institutions can harness technology to enhance social welfare without imposing rigid mandates that may be ineffective or counterproductive."}, {"title": "3.2 Limitations", "content": "The use of evolutionary game theory (EGT) as the framework for modeling human behavior has important limitations. EGT assumes that strategies evolve over time based on their relative success, akin to natural selection in biological systems. However,\n\nhuman decision-making is influenced by a myriad of factors beyond simple payoff max-imization, including emotions, social norms, and cognitive biases [26]. Consequently, the extent to which EGT can accurately capture the complexities of human coopera-tion is debatable. While EGT has been successfully applied in economics and biology to model strategic interactions [2], its assumptions may oversimplify the nuanced processes underlying human behavior.\n\nMoreover, the agents in our simulations are controlled via parameters that directly influence their likelihood to cooperate or defect. In scenarios like \u201cMandatory Coop-eration\" or \"Player-Controlled Agents,\" we impose external rules or allow players to evolve control over agents, respectively. However, in real-world settings, the mechanisms by which human behavior would determine agent actions are not straight-forward. Autonomous agents, particularly those powered by advanced Als such as large language models, operate on the basis of complex algorithms and vast datasets [27]. Directly coupling player behavior to agent actions, as done in our models, may not accurately reflect how such systems function in practice. However, governmental rul-ings on AI policies can pave the path to integrate AI agents such that they promote cooperation in the ways exemplified here.\n\nTo address the discrepancy between models and reality, future research could explore more sophisticated models where agent behavior emerges from interactions with players rather than being directly controlled. For instance, incorporating rein-forcement learning mechanisms could allow agents to adapt their strategies based on observed player actions over time [28]. This approach would better mirror the itera-tive and dynamic nature of human-agent interactions, capturing feedback loops and mutual adaptation. It also presents an opportunity to determine agent behavior. As mentioned before, reinforcement learning based on the heuristic to do upon others as done upon yourself can implement the mimicking strategy for AI agents, thus promoting cooperation among humans.\n\nAnother methodological limitation lies in the assumption of well-mixed popula-tions and the absence of spatial structure in our simulations. In reality, interactions often occur within networks where the structure can significantly impact the evolu-tion of cooperation [29, 30]. Incorporating network topology into the model could provide a more accurate representation of social dynamics, potentially altering the outcomes observed in our study. However, we remark that spatial proximity in itself tends to promote cooperation, so we do not expect structured populations to negate our findings.\n\nWe note that the \"Agents Mimic Players\" scenario presupposes that agents can perfectly observe and replicate player behavior without error. In practical applications, noise and imperfect information are inevitable [31]. Introducing stochastic elements or uncertainties into the agents' observation and imitation processes could yield more realistic results, shedding light on how robust the cooperative dynamics are to such imperfections. In addition, our model simplifies the cost and benefit structure of coop-eration and defection to a single parameter, the synergy factor r. While this abstraction is useful for theoretical exploration, real-world public goods scenarios involve more complex and often non-linear cost-benefit relationships [15]. Extending the model"}, {"title": "4 Conclusion", "content": "This study examined the potential of artificially intelligent agents to promote coopera-tion in public goods games by testing three different scenarios: Mandatory Cooperation Policy for AI Agents, Player-Controlled Agent Cooperation Policy, and Agents Mimic Players. Our computational simulations revealed that only when agents mimic player behavior does the critical synergy threshold r for the emergence of cooperation decrease, thus lifting the dilemma. This finding suggests that agents designed to reciprocate player actions can foster cooperative behavior among humans.\n\nWhile our model simplifies complex real-world interactions, it provides a theo-retical foundation for how AI agents might be leveraged to enhance cooperation in societal dilemmas. The assumption that agents can perfectly mimic player behavior is an idealization; in practice, implementing such strategies may involve challenges related to imperfect information and stochasticity. Future research should explore more sophisticated models that incorporate learning mechanisms, uncertainties, and network structures to better reflect the intricacies of human-agent interactions.\n\nEthical considerations are paramount when deploying AI agents that can influence human behavior. Ensuring that such agents promote cooperation without infringing on individual autonomy or causing unintended consequences is crucial. As AI continues to integrate into various aspects of daily life, understanding and guiding its impact on social dynamics becomes increasingly important.\n\nIn conclusion, our study illuminates a pivotal role for AI agents as catalysts for cooperation in social dilemmas. By uncovering and validating a novel mechanism wherein agents mimic player behavior, we offer a new paradigm that could profoundly impact how cooperative behavior is fostered in increasingly complex societal interac-tions. By designing agents that reciprocate human actions, we can create environments that encourage cooperative behavior, ultimately contributing to more harmonious and efficient societies."}, {"title": "5 Methods", "content": null}, {"title": "5.1 Computational Evolutionary Model", "content": "We model the public goods game using evolutionary game theory. Each player's behav-ior is characterized by an individual probability to cooperate ($p_c$) in a group of k"}]}