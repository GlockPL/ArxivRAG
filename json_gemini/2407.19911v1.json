{"title": "Efficient Shield Synthesis via State-Space Transformation", "authors": ["Asger Horn Brorholt", "Andreas Holck H\u00f8eg-Petersen", "Kim Guldstrand Larsen", "Christian Schilling"], "abstract": "We consider the problem of synthesizing safety strategies for control systems, also known as shields. Since the state space is infinite, shields are typically computed over a finite-state abstraction, with the most common abstraction being a rectangular grid. However, for many systems, such a grid does not align well with the safety property or the system dynamics. That is why a coarse grid is rarely sufficient, but a fine grid is typically computationally infeasible to obtain. In this paper, we show that appropriate state-space transformations can still allow to use a coarse grid at almost no computational overhead. We demonstrate in three case studies that our transformation-based synthesis outperforms a standard synthesis by several orders of magnitude. In the first two case studies, we use domain knowledge to select a suitable transformation. In the third case study, we instead report on results in engineering a transformation without domain knowledge.", "sections": [{"title": "1 Introduction", "content": "Cyber-physical systems are ubiquitous in the modern world. A key component in these systems is the digital controller. Many of these systems are safety critical, which motivates the use of methods for the automatic construction of controllers. Unfortunately, this problem is intricate for any but the simplest systems [27,14]. Two main methods have emerged. The first method is reinforcement learn- ing (RL) [8], which provides convergence to an optimal solution. However, the solution lacks formal guarantees about safety. The second method is reactive synthesis, which constructs a nondeterministic control strategy that is guaranteed to be safe. However, the solution lacks optimality for secondary objectives.\nDue to their complementary strengths and drawbacks, these two methods have been successfully combined in the framework of shielding [12,5] (cf. Fig. 1). Through reactive synthesis, one first computes a nondeterministic control strat- egy called a shield, which is then integrated in the learning process to prevent unsafe actions. This way, safety is guaranteed and, at the same time, RL can still provide optimality with respect to the secondary objectives."}, {"title": "1.1 Related Work", "content": "Abstraction-based controller synthesis is a popular approach that automatically constructs a controller from a system and a specification [18,29]. The continu- ous dynamics are discretized and abstracted by a symbolic transition system, for which then a controller is found. The most common abstraction is a regular hyperrectangular (or even hypercubic) grid. The success of this approach de- pends on the choice of the grid cells' size. If too small, the state space becomes"}, {"title": "2 Preliminaries", "content": "Intervals. Given bounds $l,u \\in \\mathbb{R}$ with $l < u$, we write $I = [l, u[ \\subseteq \\mathbb{R}$ for the corresponding (half-open) interval with diameter $u - l$."}, {"title": "3 Shielding in Transformed State Spaces", "content": "In this section, we show how a transformation of the state space can be used for grid-based shield synthesis, and demonstrate that it can be instrumental."}, {"title": "3.1 State-Space Transformations", "content": "We recall the principle of state-space transformations. Consider a state space $S \\subset \\mathbb{R}^d$. A transformation to another state space $T \\subset \\mathbb{R}^{d'}$ is any function $f : S \\rightarrow T$.\nFor our application, some transformations are better than others. We call these transformations grid-friendly, where, intuitively, cells in the transformed state space $T$ are better able to separate the controllable from the uncontrollable states, i.e., capture the decision boundaries well. This is for instance the case if there is an invariant property and $f$ maps this property to a single dimension."}, {"title": "3.2 Shield Synthesis in a Transformed State Space", "content": "In the following, we assume to be given a control system $(S, \\mathsf{Act}, \\delta_S)$, a safety property $\\varphi$, another state space $T$, a transformation $f : S \\rightarrow T$, and a grid $G \\subseteq 2^T$. Our goal is to compute the controllable cells similar to Eq. (2). However, since the grid is defined over $T$, we need to adapt the definition. The set of controllable cells is the maximal set of cells $C_\\varphi \\subseteq G$ such that\n$C_\\varphi = [f(\\varphi)]_G \\cap \\{C \\in G \\mid \\exists a \\in \\mathsf{Act}. \\forall C'. C \\twoheadrightarrow C' \\Rightarrow C' \\in C_\\varphi \\}.\n(3)\nThe first change is to map to cells over $T$. Next, it is convenient to define a new control system $(T, \\mathsf{Act}, \\delta_T)$ that imitates the original system in the new state space. The new successor function $\\delta_T : T \\times \\mathsf{Act} \\rightarrow 2^T$ is given indirectly as\n$\\delta_T(f(s), a) = f(\\delta_S(s, a)).\n(4)\nThe second change in Eq. (3) is implicit in the transition relation $C \\twoheadrightarrow C'$ of the labeled transition system $(G, \\mathsf{Act}, \\twoheadrightarrow)$. Recall from Eq. (1) that the transitions are defined in terms of the successor function $\\delta_T$:\n$C \\twoheadrightarrow C' \\Leftrightarrow \\exists t \\in C. \\delta_T(t, a) \\cap C' \\neq \\emptyset$.\nState-based successor computation. To simplify the presentation, for the moment, we only consider a single state $t \\in T$. To effectively compute its successors, we cannot directly use Eq. (4) because it starts from a state $s \\in S$ instead. Hence, we first need to map $t$ back to $S$ using the inverse transformation $f^{-1} : T \\rightarrow 2^S$, defined as $f^{-1}(t) = \\{s \\in S \\mid f(s) = t\\}$. The resulting set is called the preimage.\nNow we are ready to compute $\\delta_T(t, a)$ for any state $t \\in T$ and action $a \\in \\mathsf{Act}$. First, we map $t$ back to its preimage $X = f^{-1}(t)$. Second, we apply the original successor function $\\delta_S$ to obtain $X' = \\delta_S(X, a)$. Finally, we obtain the corresponding transformed states $Y = f(X')$. In summary, we have\n$\\delta_T(t, a) = f(\\delta_S(f^{-1}(t), a)).$\n(5)"}, {"title": "3.3 Shielding and Learning", "content": "We assume the reader is familiar with the principles of reinforcement learning. Here we shortly recall from [6] how to employ $\\sigma_\\varphi$ for safe reinforcement learning. The input is a Markov decision process (MDP) and a reward function, and the output is a controller maximizing the expected cumulative return. The MDP is a model with probabilistic successor function $d_p : S \\times \\mathsf{Act} \\times S \\rightarrow [0, 1]$. An MDP induces a control system $(S, \\mathsf{Act}, \\delta_S)$ with nondeterministic successor func- tion $\\delta_S(s, a) = \\{s' \\in S \\mid d_p(s, a, s') > 0\\}$ as an abstraction where the distribution has been replaced by its support.\nNow consider Fig. 1, which integrates a transformed shield into the learning process. In each iteration, the shield removes all unsafe actions (according to $\\sigma_\\varphi$) from the agent's choice. By construction, when starting in a controllable state, at least one action is available, and all available actions are guaranteed to lead to a controllable state again. Thus, by induction, all possible trajectories are infinite and never visit an unsafe state. Furthermore, filtering unsafe actions typically improves learning convergence because fewer options need to be explored.\nLearning in S and T. Recall from Theorem 1 that we can apply the shield both in the transformed state space and in the original state space by using the transformation function $f$. This allows us to also perform the learning in either state space. We consider the following setup the default: learning in the original state space S under a shield computed in the transformed state space T.\nAn alternative is to directly learn in T. A potential motivation could be that learning, in particular agent representation, may also be easier in T. For instance, the learning method implemented in UPPAAL STRATEGO represents an agent by axis-aligned hyperrectangles [23]. Thus, a grid-friendly transformation may also be beneficial for learning, independent of the shield synthesis. We will investigate the effect in our experiments."}, {"title": "4 Experiments", "content": "In this section, we demonstrate the benefits of state-space transformations for three models.\u00b9 For the first two models, we use domain knowledge to select a suitable transformation. For the third model, we instead derive a transformation experimentally. The implementation builds on our synthesis method [6]."}, {"title": "4.1 Satellite Model", "content": "For the first case study, we extend the harmonic oscillator with two more control actions to also move inward and outward: $\\mathsf{Act} = \\{\\mathsf{ahead}, \\mathsf{out}, \\mathsf{in}\\}$. The box to the side shows the relevant information about the transformation. Compared to Example 3, beside the actions, we modify two parts. First, the transformed state space $T$ is reduced in the radius dimension to $r \\in [0, 2[$ because values outside the disc with radius 2 are not considered safe (see below). Second, the successor function still uses matrix $A$ from Example 1 but with a control period of $t = 0.05$. The successor function thus becomes $\\delta(s, a) = e^{At} \\cdot f(s)$, where $s = \\binom{r \\cos(\\theta)}{r \\sin(\\theta)}$ and $f$ as in Example 3 we have\n$\\binom{r'}{\\theta'} = f(s), e^{At} \\approx \\binom{1.00 \\quad 0.05}{-0.05 \\quad 1.00},\\quad c = \\begin{cases}0.99 & \\text{if } a = \\mathsf{in}\\\\1.01 & \\text{if } a = \\mathsf{out},\\\\1 & \\text{otherwise.}\\end{cases}$\nInstead of one large obstacle, we add several smaller stationary (disc-shaped) obstacles. The shield has two goals: first, the agent must avoid a collision with the obstacles; second, the agent's distance to the center must not exceed 2. Fig. 4 shows the size and position of the obstacles (gray). Overlaid is a trajectory (blue) produced by a random agent that selects actions uniformly. Some states of the trajectory collide with obstacles (red)."}, {"title": "4.2 Bouncing-Ball Model", "content": "For the second case study, we consider the model of a bouncing ball from [6]. Fig. 6 shows an illustration of the system, while Fig. 7(a) shows the hybrid- automaton model. The state space consists of the velocity $v$ and the position $p$ of the ball. When the ball hits the ground, it loses energy subject to a stochastic dampening (dashed transition). The periodic controller is modeled with a clock $x$ with implicit dynamics $\\dot x = 1$ and control period $P = 0.1$. The available actions are $\\mathsf{Act} = \\{\\mathsf{nohit}, \\mathsf{hit}\\}$, where the $\\mathsf{nohit}$ action has no effect and the $\\mathsf{hit}$ action pushes the ball downward subject to its velocity, but only provided it is high enough ($p \\geq 4$).\nThe goal of the shield is to keep the ball bouncing indefinitely, which is modeled as nonreachability of the set of states $p < 0.01 \\land |v| \\leq 1$.\nThe optimization task is to use the $\\mathsf{hit}$ action as rarely as possible, which is modeled by assigning it with a cost and minimizing the total cost.\nDespite its simple nature, this model has quite intricate dynamics, including stochastic and hybrid events that require zero-crossing detection, which makes determining reachability challenging. It was shown in [6] that a sampling-based shield synthesis is much more scalable than an approach based on guaranteed reachability analysis (19 minutes compared to 41 hours). The grid needs to be quite fine-grained to obtain a fixpoint where not every cell is marked unsafe. This corresponds to 520,000 cells, and the corresponding shield is shown in Fig. 7(b).\nNow we use a transformation to make the shield synthesis more efficient. The mechanical energy $E_m$ stored in a moving object is the sum of its potential energy and its kinetic energy, respectively. Formally, $E_m(p, v) = mgp + \\frac{1}{2}mv^2$, where $m = 1$ is the mass and $g = 9.81$ is gravity. Thus, the mechanical energy of a ball in free fall (both with positive or negative velocity) remains invariant. Hence, $E_m$ is a good candidate for a transformation.\nHowever, only knowing $E_m$ is not sufficient to obtain a permissive shield because states with the same value of $E_m$ may be below or above $p = 4$ and hence may or may not be hit. The equation for $E_m$ depends on both $p$ and $v$. In this case, it is sufficient to know only one of them. Here, we choose the transformed state space $T$ with just $E_m$ and $v$. The transformation function is $f(v, p) = (mgp+\\frac{1}{2}mv^2, v)$ and its inverse is $f^{-1}(E_m, v) = ((\\frac{E_m - \\frac{1}{2}mv^2}{mg}), v)^\\mathsf{T}$. We"}, {"title": "4.3 Cart-Pole Model", "content": "For the third case study, we consider a model of an inverted pendulum installed on a cart that can move horizontally. This model is known as the cart-pole model. An illustration is shown in Fig. 8. The dynamics are given by the following"}, {"title": "Approximating the Decision Boundaries", "content": "Fig. 9(a) shows the decision boundaries of a fixpoint computed using 30 \u00d7 30 cells. However, our work of state-space transformations was motivated because computing the shield is gen- erally not feasible in the first place."}, {"title": "4.4 Strategy Reduction", "content": "We provide an overview of the savings due to computing the shield in the trans- formed state space in Table 1. The column labeled Number of cells clearly shows a significant reduction in all cases. We remark that, in order to have a fair com- parison, we have selected the grid sizes from visual inspection to ensure that the plots look sufficiently close. However, it is not the case that one of the shields is more permissive than the other.\nThe strategies above can be represented with a d-dimensional matrix. Matri- ces are inherently limiting representations of shields, especially when the shield should be stored on an embedded device. Empirically, a decision tree with axis- aligned predicates is a much better representation. To demonstrate the further saving potential, we converted the shields to decision trees and additionally ap- plied the reduction technique from [20]. The last column in Table 1 shows the number of nodes in the decision trees. As can be seen, we always achieve another significant reduction by one to two orders of magnitude."}, {"title": "4.5 Shielded Reinforcement Learning", "content": "The only motivation for applying a state-space transformation was to be able to compute a cheaper shield. From the theory, we cannot draw any conclusions"}, {"title": "5 Conclusion", "content": "We have demonstrated that state-space transformations hold great potential for shield synthesis. We believe that they are strictly necessary when applying shield synthesis to many practical systems due to state-space explosion.\nIn the first two case studies, we used domain knowledge to select a suitable transformation. In the third case study, we instead engineered a transformation in two steps. We plan to generalize these steps to a principled method and investigate how well it applies in other cases.\nState-space transformations can be integrated with many orthogonal prior extensions of grid-based synthesis. One successful extension is, instead of pre- computing the full labeled transition system, to compute its transitions on the fly [21]. Another extension is the multilayered abstraction [19,22]. Going one step further, in cases where a single perfect transformation does not exist, we may still be able to find a family of transformations of different strengths."}]}