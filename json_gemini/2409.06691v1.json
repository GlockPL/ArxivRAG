{"title": "Geometric-Averaged Preference Optimization for Soft Preference Labels", "authors": ["Hiroki Furuta", "Kuang-Huei Lee", "Shixiang Shane Gu", "Yutaka Matsuo", "Aleksandra Faust", "Heiga Zen", "Izzeddin Gur"], "abstract": "Many algorithms for aligning LLMs with human preferences assume that human preferences are binary and deterministic. However, it is reasonable to think that they can vary with different individuals, and thus should be distributional to reflect the fine-grained relationship between the responses. In this work, we introduce the distributional soft preference labels and improve Direct Preference Optimization (DPO) with a weighted geometric average of the LLM output likelihood in the loss function. In doing so, the scale of learning loss is adjusted based on the soft labels, and the loss with equally preferred responses would be close to zero. This simple modification can be easily applied to any DPO family and helps the models escape from the over-optimization and objective mismatch prior works suffer from. In our experiments, we simulate the soft preference labels with AI feedback from LLMs and demonstrate that geometric averaging consistently improves performance on standard benchmarks for alignment research. In particular, we observe more preferable responses than binary labels and significant improvements with data where modestly-confident labels are in the majority.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [1, 7, 32] capture a wide range of behaviors and values from training data. However, we would usually prefer these models to focus on useful and safe expressions and abide by social norms. The popular approaches to this are preference optimization [4, 33] through finetuning, such as reinforcement learning from human feedback (RLHF) [11], and offline preference methods [51]. In this process, these models are trained on preference data and labels generated by human raters with a wide variety of priorities, backgrounds, knowledge, and skill sets. Nevertheless, many existing RLHF algorithms and Direct Preference Optimization (DPO) [40] variants assume binary preferences, which ignore the subtle relationship and amplify the bias in the preference labels.\nTo address this issue, we introduce the concept of distributional soft preference labels and improve DPO and its algorithmic families by incorporating a weighted geometric average of LLM output likelihood into the loss function. This approach adjusts the scale of learning loss based on the soft labels and effectively minimizes the loss when presented with equally preferred responses.\nIn the experiments, we simulate the soft preference labels with AI feedback from LLMs [4, 23] and show that soft preference labels and weighted geometric averaging achieve consistent improvement to the baselines on popular benchmarks for the alignment research literature, such as Reddit TL;DR [48], and Anthropic Helpful and Harmless [3], as well as original natural language planning dataset based on Plasma [6]. In particular, our results highlight that the proposed methods significantly improve the performance with the data dominated by modestly-confident labels, while conservative DPO (cDPO) [29], a method leveraging soft labels via linear interpolation of objectives, is stuck to sub-optimal performances there. When the models are trained with rich modestly-confident labels, the responses are preferable to those from the models trained with binary labels biased to high-confidence regions. The performance on preference label classification also reveals that cDPO struggles with objective mismatch between the text generation and preference modeling and the weighted geometric averaging could successfully balance both.\nOur primary contributions are:\n\u2022 We introduce the notion of soft preference labels, which can reflect the distributional preference and the fine-grained relationship between the response pairs (Section 2.1). Soft preference labels contribute to avoiding over-optimization issues (Section 5.3) and aligning the models to more preferable responses than binary labels (Section 5.1).\n\u2022 We propose the weighted geometric averaging of the output likelihood in the loss function. This can be applied to a family of any algorithms derived from DPO (Section 3).\n\u2022 We point out the objective mismatch between text generation and preference modeling. The better preference accuracy from DPO-style objectives does not ensure better alignment, which conservative DPO suffers from and our geometric averaging can resolve (Section 5.2)."}, {"title": "2 Preliminaries", "content": "We denote $x \\in \\mathcal{X}$ as a text prompt from the set of prompts $\\mathcal{X}$, $y \\in \\mathcal{Y}$ as an answer corresponding to the prompts from the set of possible candidates $\\mathcal{Y}$, and $\\pi(y \\mid x)$ as a LLM (i.e. policy). We also write a preference relationship between $y_1$ and $y_2$ as $Y_1 > Y_2$ when $y_1$ is more preferable than $y_2$, and a dataset of the paired preference as $\\mathcal{D} = \\{(x^{(n)}, y_1^{(n)}, y_2^{(n)})\\}_{n=1}^{(N)}$. For the notation, we assume that $y_1 > y_2$ always holds in this paper unless otherwise mentioned.\nIn the RLHF pipeline, we typically go through three phases, such as supervised finetuning (SFT), reward model training, and RL-finetuning [33, 68]. SFT phase conducts a maximum likelihood training of pre-trained LLMs on downstream tasks, which results in an initial model or reference model $\\pi_{ref}$ for the later RL-finetuning. For the reward modeling, the Bradley-Terry model [5] is often assumed as underlying modeling for the oracle human preference such as\n$p^* (Y_1 > Y_2 | x) = \\frac{\\exp(r^* (x, y_1))}{\\exp(r^* (x, y_1)) + \\exp(r^*(x, y_2))} = \\sigma(r^*(x, y_1) - r^*(x, y_2)),$ (1)\nwhere $r^* (x, y)$ is a true reward function and $\\sigma(\\cdot)$ is a sigmoid function. Following this assumption, the parameterized reward function $r_\\psi$ is initialized with a supervisedly-finetuned LLM $\\pi_{ref}$ and trained with negative log-likelihood loss: $\\min_\\psi -\\mathbb{E} [\\log \\sigma(r_\\psi(x,y_1) - r_\\psi(x, y_2))]$. RL-finetuning phase leverages the learned reward to update the LLM $\\pi_\\theta$ by optimizing the following objective [33, 68],\n$\\max_\\theta \\mathbb{E}_{x\\sim D, y\\sim \\pi_\\theta (y \\mid x)} [r_\\psi(x, y)] - \\beta D_{KL}(\\pi_\\theta(y \\mid x) || \\pi_{ref}(y \\mid x)),$ (2)\nwhere $\\beta > 0$ is a coefficient to control the KL-divergence regularization. PPO [44] is often employed to maximize Equation 2. However, because such online RL methods require two independent LLMs for training, computational inefficiency, training instability, and complex pipelines have been significant issues in practice."}, {"title": "2.1 Soft Preference Labels", "content": "While a reward model is often trained with binary preferences, it is a decent assumption that we can obtain the distributional soft feedback via majority voting among the human raters or AI feedback with scoring [23], such as y\u2081 is better than y\u2082 in 70%. Even assuming soft preference labels, we can easily recover the binary preference with a threshold.\nWe assume that the binary preference labels, $\\mathbb{1}(y_1 > Y_2|x) = 1$, are sampled from the Bradley-Terry model preference distribution with the parameter $p^* (Y_1 > Y_2|x)$. We define soft preference labels as estimates of the true preference probability:\n$\\hat{p}_{x,y_1,y_2} := P(Y_1 > Y_2|x) \\approx p^* (Y_1 > Y_2|x).$ (3)\nWe denote $p_{x,y_1, y_2}$ as $\\hat{p} \\in [0.5, 1.0]$ for simplicity in the later sections. For instance, we can estimate this via monte-carlo sampling such as $\\hat{p} = \\frac{1}{M} \\sum_{i=1}^{M} l_i$ where $l_i \\in \\{0,1\\}$ is a sampled binary label,"}, {"title": "2.2 Direct Preference Optimization and Related Methods", "content": "We start with a brief review of DPO and the variants derived from it, such as conservative DPO, IPO, and ROPO. DPO maximizes the estimate of preference probability under the Bradley-Terry model, $p_\\theta(Y_1 > Y_2 | x) = \\sigma(r_\\theta(x,y_1) - r_\\theta(x,y_2))$, by parameterizing reward models with the policy model $\\pi_\\theta$ itself, which comes from the following relationship in the constraint Lagrangian of RLHF objective (Equation 2),\n$r_\\theta(x,y) = \\beta \\log \\frac{\\pi_\\theta(y|x)}{\\pi_{ref}(y|x)} + \\beta \\log Z(x),$ (4)\nwhere $Z(x) = \\sum_y \\pi_{ref} (y | x) \\exp(\\frac{r_\\theta(x, y)}{\\beta})$ is the partition function. Substituting Equation 4 into $\\log p_\\theta (Y_1 > Y_2 | x)$, the following objective is derived:\n$\\mathcal{L}_{DPO}(\\pi_\\theta, \\pi_{ref}) = -\\mathbb{E}_{(x,y_1,y_2)\\sim D} [\\log \\sigma(h_\\theta(x, Y_1, Y_2))]$\n$= -\\mathbb{E}_{(x,y_1,y_2)\\sim D} \\big[\\log \\sigma(\\beta \\log \\frac{\\pi_\\theta(Y_1 | x) \\pi_{ref}(Y_2 | x)}{\\pi_{ref}(Y_1 | x) \\pi_\\theta(Y_2 | x)})\\big],$ (5)\nNote that we define the reward difference function as $h_\\theta(x, Y_1, Y_2) := r_\\theta(x, y_1) - r_\\theta(x,y_2)$.\nConservative Direct Preference Optimization Conservative DPO (cDPO) [29] is the most representative work that could incorporate soft labels. cDPO smooths the objective functions with soft preference labels via linear interpolation, such as\n$\\mathcal{L}_{cDPO}(\\pi_\\theta, \\pi_{ref}) = -\\mathbb{E}_{(x,y_1,y_2,\\hat{p})\\sim D} [\\hat{p} \\log \\sigma(h_\\theta(x, Y_1, Y_2)) + (1 - \\hat{p}) \\log \\sigma (h_\\theta(x, Y_2, Y_1))]$\n$= -\\mathbb{E}_D [\\hat{p}\\log \\sigma(h_\\theta(x, Y_1, Y_2)) + (1 - \\hat{p}) \\log (1 - \\sigma (h_\\theta(x, Y_1, Y_2)))],$ (6)\nwhere the later term is the DPO loss under flipped labels (i.e. $y_2 > y_1$). Moreover, prior works incorporating an extra reward model $r_\\psi$ to DPO objective have also adopted this formulation [8, 20], by replacing $\\hat{p}$ into $\\sigma(r_\\psi(x, y_1) - r_\\psi(x, y_2))$.\nIdentity Preference Optimization Assuming the Bradley-Terry model as an underlying preference modeling causes over-optimization issues in DPO [2, 51]. To mitigate this problem, IPO [2] has been introduced by replacing reward maximization in Equation 2 with preference distribution maximization. The objective of IPO can be written as,\n$\\mathcal{L}_{IPO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{(x,y_1,y_2)\\sim D} \\Big[h_\\theta(x, Y_1, Y_2) - \\frac{1}{2\\beta}\\Big]^2,$ (7)\nwhere $\\beta > 0$ is a regularization hyper-parameter. Similar to cDPO, we can also introduce conservative IPO (cIPO) [2, 26], which results in\n$\\mathcal{L}_{cIPO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{(x,y_1,y_2,\\hat{p})\\sim D} \\Big[h_\\theta(x, Y_1, Y_2) - \\frac{2\\hat{p}-1}{2\\beta}\\Big]^2.$ (8)\nRobust Preference Optimization ROPO [26] designs its objective to resolve the instability under noisy label problems, which is inspired by the unhinged loss [53] and reverse cross-entropy loss [58] in the noise-tolerant supervised learning literature. The objective is a combination of the regularization term and original DPO loss such as,\n$\\mathcal{L}_{ROPO}(\\pi_\\theta, \\pi_{ref}) = \\alpha \\mathbb{E}_{(x,y_1,y_2,\\hat{p})\\sim D} [\\sigma (h_\\theta(x, y_2, Y_1))] - \\gamma \\mathbb{E}_{(x,y_2,y_1)\\sim D} [\\log \\sigma (h_\\theta(x, Y_1, Y_2))]$\n$= \\alpha \\big(1 - \\mathbb{E}_{(x,y_1,y_2,\\hat{p})\\sim D} [\\sigma (h_\\theta(x, Y_1, Y_2))\\big]) + \\gamma \\mathcal{L}_{DPO}(\\pi_\\theta, \\pi_{ref}),$ (9)\nwhere $\\alpha > 0$ and $\\gamma > 0$ are extra hyper-parameters to balance the contribution of each term."}, {"title": "3 Methods", "content": "DPO and its family assume binary preference and they could not reflect the fine-grained relationship between the pair of responses during training. The conservative formulation of DPO may leverage the soft preference labels, but we found that it could not achieve good performance if modestly-confident labels shape the distribution as a majority (see Section 5). In this section, we propose a simple yet effective modification, weighted geometric averaging of LLM output likelihood in the learning loss, which can be applied to a family of algorithms derived from DPO."}, {"title": "3.1 Weighted Geometric Averaging and Practical Algorithms", "content": "Instead of taking the linear interpolations of objectives, we assume that the pairs of winner and loser outputs $(y_w, y_l)$ are sampled from the weighted geometric average of LLM policies $\\pi_\\theta(\\cdot | x)$ such as,\n$\\pi_\\theta (y_w | x) := \\frac{1}{Z_{\\pi,w} (x)} \\pi_\\theta(Y_1 | x)^{\\hat{p}} \\pi_\\theta(Y_2 | x)^{1-\\hat{p}},$\n$\\pi_\\theta (y_l | x) := \\frac{1}{Z_{\\pi,l} (x)} \\pi_\\theta(Y_1 | x)^{1-\\hat{p}}\\pi_\\theta(Y_2 | x)^{\\hat{p}},$ (10)\nwhere $Z_{\\pi,w}(x) := \\sum_{y_j>y_k,\\hat{p}} \\pi_\\theta(Y_j | x)^{\\hat{p}} \\pi_\\theta(Y_k | x)^{1-\\hat{p}}$ and $Z_{\\pi,l}(x) := \\sum_{y_j>y_k,\\hat{p}} \\pi_\\theta(Y_j | x)^{1-\\hat{p}} \\pi_\\theta(Y_k | x)^{\\hat{p}} (y_j > y_k)$. Because these values are hard to obtain precise estimation with sampling, we will set those normalization terms to constant and ignore them in practice, which is a common assumption in deep RL literature [17, 38, 45, 60]. If we have true binary labels (i.e. $\\hat{p} = 1$), Equation 10 reduces to the original formulation under the assumption of $y_1 > y_2$.\nWeighted geometric averaging is one of the design choices for regularization, which pushes the large likelihood down to small when the soft preference is far from 1. We can apply this to any DPO family. We will propose three modified algorithms; Geometric DPO (GDPO), GIPO, and GROPO. Those are derived from the replacement of winner output likelihood $\\pi(y_1 | x) \\rightarrow \\pi(Y_1 | x)^{\\hat{p}}\\pi(Y_2 | x)^{1-\\hat{p}}$ and loser output likelihood $\\pi(y_2 | x) \\rightarrow \\pi(Y_1 | x)^{1-\\hat{p}}\\pi(y_2 | x)^{\\hat{p}}$ for both $\\pi_\\theta$ and $\\pi_{ref}$:\nGeometric Direct Preference Optimization (GDPO)\n$\\mathcal{L}_{GDPO}(\\pi_\\theta, \\pi_{ref}) = -\\mathbb{E}_D \\Big[\\log \\sigma\\Big(\\beta \\log \\frac{\\pi_\\theta(Y_1 | x)^{\\hat{p}}\\pi_\\theta(Y_2 | x)^{1-\\hat{p}}}{\\pi_{ref}(Y_1 | x)^{\\hat{p}}\\pi_{ref}(Y_2 | x)^{1-\\hat{p}}}\\Big)\\Big]$\n$=-\\mathbb{E}_{D} \\bigg[\\log \\sigma \\bigg((\\hat{2p}-1) \\log \\frac{\\pi_{\\theta}(Y_1 | x)}{\\pi_{\\theta}(Y_2 | x)} \\frac{\\pi_{ref}(Y_2 | x)}{\\pi_{ref}(Y_1 | x)}\\bigg)\\bigg],$ (11)\nGeometric Identity Preference Optimization (GIPO)\n$\\mathcal{L}_{GIPO}(\\pi_\\theta, \\pi_{ref}) = \\mathbb{E}_{(x,y_1,y_2,\\hat{p})\\sim D} \\Big[(\\hat{2p} - 1)^2 \\big( h_\\theta(x, Y_1, Y_2) - \\frac{1}{2\\beta}\\big)^2\\Big],$ (12)\nGeometric Robust Preference Optimization (GROPO)\n$\\mathcal{L}_{GROPO} (\\pi_\\theta, \\pi_{ref}) = \\alpha \\bigg(1 - \\mathbb{E}_{D} \\sigma \\Big(\\beta (\\hat{2p} - 1) \\log \\frac{\\pi_{\\theta}(Y_1 | x)}{\\pi_{\\theta}(Y_2 | x)} \\frac{\\pi_{ref}(Y_2 | x)}{\\pi_{ref}(Y_1 | x)}\\Big)\\bigg) + \\gamma \\mathcal{L}_{GDPO}(\\pi_\\theta, \\pi_{ref}),$ (13)"}, {"title": "3.2 Geometric Averaging Can Adjust the Scale of Gradients", "content": "To analyze the role of weighted geometric averaging, we consider the gradient of loss function with respect to model parameters $\\theta$ in a general form, which can be written as:\n$\\nabla_\\theta \\mathcal{L} = -\\beta \\mathbb{E}_{(x,y_1,y_2,\\hat{p})\\sim D} \\omega_\\theta (x, Y_1, Y_2, \\hat{p}) [\\nabla_\\theta \\log \\pi_\\theta(Y_1 | x) - \\nabla_\\theta \\log \\pi_\\theta(Y_2 | x)],$ (14)\n$\\Big(\\beta \\log \\frac{\\pi_\\theta(Y_1 | x) \\pi_{ref}(Y_2 | x)}{\\pi_{ref}(Y_1 | x) \\pi_\\theta(Y_2 | x)},\\quad\\Big)$\nwhere $\\omega_\\theta (x, Y_1, Y_2,\\hat{p})$ is a scaling factor of positive and negative gradients. While defining an estimated preference probability by their own policy LLMs under the Bradly-Terry model as:\n$\\rho_\\theta := \\sigma \\Big(\\beta \\log \\frac{\\pi_\\theta(Y_1 | x) \\pi_{ref}(Y_2 | x)}{\\pi_{ref}(Y_1 | x) \\pi_\\theta(Y_2 | x)}\\Big) := \\sigma \\Big(\\beta (\\hat{2p} - 1) \\log \\frac{\\pi_\\theta(Y_1 | x) \\pi_{ref}(Y_2 | x)}{\\pi_{ref}(Y_1 | x) \\pi_\\theta(Y_2 | x)}\\Big),$ (15)\nwe summarize the scaling factor of each method in Table 1. Comparing $\\nabla_\\theta \\mathcal{L}_{DPO}$ and $\\nabla_\\theta \\mathcal{L}_{cDPO}$, DPO optimizes the model until the estimate preference $\\rho_\\theta$ reaches 1 ($\\omega_\\theta = 1 - \\rho_\\theta$), and CDPO does until $\\rho_\\theta$ matches the soft preference $\\hat{p}$ by assigning a high weight when the estimation is wrong ($\\omega_\\theta = \\hat{p} - \\rho_\\theta$). DPO pushes the distribution to the oracle preferable outputs, and cDPO may work well as a regularization if the label has high confidence (e.g. $\\hat{p} = 0.95$). However, the gradient of cDPO may also cause unnecessary model updates around $\\hat{p} = 0.5$. Intuitively, $\\hat{p} = 0.5$ means either candidate answers (y1, y2) are equally good, but $\\nabla_\\theta \\mathcal{L}_{cDPO}$ forces their likelihoods to be balanced.\nIn contrast, GDPO adjusts the gradient scale based on soft preference by multiplying $(\\hat{2p} - 1)$, which can also ignore the gradient from even candidate pairs. The left of visualizes that weighted geometric averaging can adjust the scale of gradient based on the soft preference labels. If soft preference labels are close to 1 (e.g. $\\hat{p} = 0.95$), the norm of the scaling factor is almost the same, and small soft preference makes the scaling factor small while the norm reaches zero (e.g. $\\hat{p} = 0.55$). This maintains the effect from clear relationship pairs, reduces the effect from equally good outputs, and reflects the detailed preference signals among the responses. In practice, we set a larger value for $\\beta$ in GDPO than in DPO to maintain and amplify the scale of the gradient for acceptable preference pairs, which works as an implicit filtering of soft preference labels. We will explain this in Section 5.1."}, {"title": "3.3 Analysis in 1-D Synthetic Bandit Problem", "content": "To highlight the advantage of geometric averaging and the failure case of linear interpolation as done in cDPO (Equation 6), we consider a 1-D bandit problem with 100 discrete actions and a linear reward function. The right of illustrates the histogram of train data and true reward function, paired preference distribution, and action distributions from the learned policies. The 500,000 training instances are sampled from a bimodal mixture of Gaussian distribution (with the mode in the 20-th and 70-th indices), and we prepare the paired data from those while labeling preferences with the Bradley-Terry model. We train the parameterized reward ry by minimizing $\\mathcal{L}_{DPO}$, $\\mathcal{L}_{CDPO}$, and $\\mathcal{L}_{GDPO}$, and then recover the learned policies analytically as $\\pi_{r\\psi}(y) \\propto \\pi_{data}(y) \\exp(r_\\psi(y))$, where $\\pi_{data}(y)$ is an underlying train data distribution. The results demonstrate that cDPO accurately fits the data distribution, which is because the linear interpolation of the loss function in Equation 6 can be interpreted as a minimization of KL divergence $\\mathbb{E}[D_{KL}(P || p_\\theta)]$. However, this could result in a sub-optimal solution when the train data has a peak in a low-reward region. Because greedy decoding considers the mode of learned distributions, this accurate modeling in cDPO is not aligned with the text generation objectives. On the other hand, DPO and GDPO can assign a probability mass in a high-reward region. GDPO has an advantage against cDPO by resolving such an objective mismatch. Similar trends can be observed in the LLM experiments (Section 5)."}, {"title": "4 Experiments", "content": "In the experiments, we use PaLM 2-XS [1] for the base LLM, as done in prior works [15, 19, 23, 42] (Appendix L uses Gemma-2B/7B as base LLMs). We use the popular RLHF datasets, such as Reddit TL;DR [48, 54] (summarization), and Anthropic Helpful and Harmless [3] (conversation) for the benchmark. To simulate the soft preference labels, we relabel the preference to the datasets by leveraging AI feedback [4, 23] from instruction-tuned PaLM 2-L (Section 4.1). However, because we found that the soft label distributions in popular RLHF datasets only have similar shapes concentrating on high-confidence regions such as $\\hat{p} \\in [0.95, 1.0]$ (Figure 10 in Appendix I), we prepared (1) new competitive paired responses from a winner in the original dataset and from LLMs and (2) the novel preference dataset based on Plasma Plan [6], a dataset of daily-life natural language planning, which simulate more diverse preference label distributions we may face in a practical scenario. For instance, Plasma Plan has a pair of instruction x (e.g. see a movie) and the human-written gold plan y (e.g. Step 1: Choose a movie, Step 2: Buy a ticket, Step 3: Go to the theater). To construct a pair of plans, we generated the plans to all the instructions using PaLM 2-L with few-shot prompting, and then obtained the triplet (x, Ydata, YPaLM). We gathered about 60K response pairs for train split and 861 examples for test split. Following this procedure, we prepared about 93K (Reddit TL;DR), 44K (Anthropic Helpful), and 42K (Harmless) response pairs as train split. To reduce the inference cost, we sample 1000 test prompt-response tuples in Reddit TL;DR while removing the duplicated ones. For other datasets, we have 1639 (Helpful) and 1614 (Harmless) examples in the test split.\nTo prepare the SFT models, we finetune PaLM 2-XS using 50% of winner responses in train split for Reddit TL;DR, Anthropic Helpful, and Harmless, and using the responses from PaLM 2-L for Plasma Plan. We use those SFT models as an initial checkpoint of preference methods and the reference models $\\pi_{ref}$. See Appendix B for further details on training."}, {"title": "4.1 Simulating Soft Preference with AI Feedback", "content": "Following prior works [4, 9, 14, 23], as reliable alternatives to human raters, we simulate the soft preference labeling with AI feedback from LLMs. AI rating is well aligned with humans and is often used as a proxy of human evaluation in the RLHF literature [67]. Through the work, we use PaLM 2-L instruction-tuned on Flan dataset [12] as an AI rater. To obtain the soft preferences, we put the context x, first output y1, second output y2, and the statement such as \u201cThe more preferable output is: \u201d, and then get the log probability (score) of token \u201c(1)\u201d and \u201c(2)\u201d from LLMs. Assuming the Bradley-Terry model, we compute the AI preference as follows:\n$P_{AI}(Y_1 > Y_2 | x) = \\frac{\\exp(score((1)))}{\\exp(score((1))) + \\exp(score((2)))}.$ (16)\nLastly, to reduce the position bias [39, 57] in LLM rating, we take the average of $P_{AI}$ by flipping the ordering of $(y_1, y_2)$ in the prompt. See Appendix F for the prompts of AI rating. For a fair comparison, we prepare the binary labels based on $P_{AI}$ rather than the original labels in the dataset."}, {"title": "4.2 Binary and Percentage Judge for Evaluation", "content": "For the evaluation, we conduct a pairwise comparison between the response from the trained models ($Y_{llm}$) and the reference response from PaLM 2-L, and GPT-4 [32] ($Y_{ref}$). The reference responses from PaLM 2-L and GPT-4 are generated with few-shot prompting (see Appendix G). In addition, we directly compare our methods and corresponding baselines (e.g. GIPO v.s. IPO or cIPO).\nAs evaluation metrics, we employ the winning rate from binary and percentage judge. We first calculate the AI preference between the response from the trained models and evaluation data as explained in Section 4.1. We calculate the average binary and percent winning rate as follows:\n$binary = \\frac{1}{|D|} \\sum_{(x, Y_{ref}, Y_{llm})} \\mathbb{1} [P_{AI} (Y_{llm} - Y_{ref} | x) \\geq .5], percent = \\frac{1}{|D|} \\sum_{(x, Y_{ref}, Y_{llm})} P_{AI} (Y_{llm} - Y_{ref} | x).$ (17)\nNote that $P_{AI}$ is also averaged among the flipped order to alleviate the position bias."}, {"title": "5 Results", "content": "We first compare the alignment performance among the algorithms with binary feedback (DPO, IPO, ROPO), their conservative variants (cDPO, cIPO), and weighted geometric averaging with soft feedback (GDPO, GIPO, GROPO) on six preference datasets (Section 5.1), and evaluate the preference label classification by the learned models (Section 5.2). We also analyze the log-likelihood"}, {"title": "5.1 Weighted Geometric Averaging Improves the Alignment Performance", "content": "Table 2 presents the winning rate on Reddit TL;DR, Anthropic Helpful and Harmless, Plasma Plan, Plasma Plan Skewed, and Stairs. We compare the performance between the baseline algorithms derived from DPO (SFT, DPO, cDPO, IPO, CIPO, ROPO) and the ones applying geometric averaging (GDPO, GIPO, GROPO). Through the experiments, we set the temperature to 0.0 for the inference.\nThe results demonstrate that the methods applying geometric averaging (GDPO, GIPO, GROPO) achieve consistently better or comparable performances against binary preference methods (DPO, IPO, ROPO) or their conservative variants (CDPO, CIPO). The trend is clearer on Plasma Plan, Plasma Plan Skewed, and Stairs, which have richer modestly-confident labels. The improvement of GDPO, GIPO, and GROPO compared to baselines have statistical significance with p < 0.01 on the Wilcoxon signed-rank test, compared to SFT, binary preference methods, and soft preference methods with linear interpolation. Appendix I also provides the results with the original paired response from Reddit TL;DR, Anthropic Helpful, and Harmless, where many soft labels concentrate on p\u2208 [0.95, 1.0]. Table 2 highlights that rich soft labels help align LLMs better than those binary ones. Focusing on DPO variants, CDPO does not work well while GDPO performs the best. We hypothesize that this comes from the objective mismatch between the text generation and preference modeling, which will be verified in Section 5.2. Moreover,  shows the binary winning rates in the direct comparison between corresponding methods, such as GDPO v.s DPO, and GIPO v.s. cIPO, etc, which also reveals that geometric averaging consistently outputs more preferable responses."}, {"title": "Large \u03b2 as Implicit Preference Filtering", "content": "As discussed in Section 3.2, weighted geometric averaging makes the norm of the gradient smaller based on soft preference label p. However, an unnecessarily small gradient could stick to sub-optimal solutions. It would be necessary to maintain and even amplify the scale of the gradient from reliable preference pairs. For the rescaling of the gradient, we set larger \u03b2 because, in geometric averaging, we can regard as using smaller \u03b2\u2019 := \u03b2E[2p \u2013 1] < \u03b2. Such a larger \u03b2 works as an implicit filtering of soft preference labels.  (left) presents the binary winning rate of DPO and GDPO with different \u03b2\u2208 [0.1, 0.5] on Plasma Plan dataset. GDPO has a peak at \u03b2 = 0.3, which is larger than that of DPO (\u03b2 = 0.1), and GDPO can achieve better performance. See Appendix B for further details of hyper-parameters."}, {"title": "5.2 Preference Label Classification", "content": "Since DPO objective (Equation 5) is derived from the assumption under the Bradley-Terry model, we can regard it as training reward models and implicitly estimating preference probability. We here compare DPO, CDPO, and GDPO, estimate the preference probability $\\rho_\\theta$ from Equation 15, make a binary label classification (as done in Equation 17), and then compute the average accuracy between predicted labels and true labels given via AI rating. We use Plasma Plan and prepare three different pairs of outputs between PaLM 2-L and (1) humans, (2) GPT-4, and (3) GPT-3.5.\n (left) shows that all the methods can classify preference labels well when the test split is composed of the responses from PaLM 2-L and humans, which is the same data distribution as the train split. However, DPO sharply decreases the performance for classifying out-of-distribution pairs, such as from GPT-4 and GPT-3.5 (94.0% 61.6%/66.3%). CDPO achieves the best classification accuracy on average, and GDPO mitigates the performance drop in DPO. Despite the best accuracy"}, {"title": "5.3 Weighted Geometric-Averaging Suppresses Over-Optimization", "content": "The analysis of the log-likelihood ratio and the estimated reward gap can characterize the behavior of offline alignment algorithms [50", "In": "we measure the log-likelihood ratio of winner/loser responses and estimated reward gap on Plasma Plan and Anthropic Harmless.\nDPO aggressively pushes down both log ratios and increases the reward gap, since DPO objective forces the model to achieve $r_\\theta(x, Y_w) \u2013 r_\\theta(x,Y_l) \\rightarrow \\infty$, which causes an over-optimization issue. CDPO is more conservative in pushing down the log ratio while leading to worse alignment quality due to objective mismatch. GDPO avoids the issues of such objective mismatch and over-"}]}