{"title": "Expertise Is What We Want", "authors": ["Munir Al-Dajani", "Dr. Keegan Duchicela", "Kiril Kafadarov", "Othman Larakit", "Amina Lazrak", "Wendy McKennon", "Dr. Rebecca Miksad", "Jayodita Sanghvi", "Dr. Alan Ashworth", "Dr. Divneet Mandair", "Dr. Travis Zack", "Dr. Allison Kurian"], "abstract": "Clinical decision-making depends on expert reasoning, which is guided by\nstandardized, evidence-based guidelines. However, translating these guidelines\ninto automated clinical decision support systems risks inaccuracy and importantly,\nloss of nuance. We share an application architecture, the Large Language Expert\n(LLE), that combines the flexibility and power of Large Language Models (LLMs)\nwith the interpretability, explainability, and reliability of Expert Systems. LLMS\nhelp address key challenges of Expert Systems, such as integrating and codifying\nknowledge, and data normalization. Conversely, an Expert System-like approach\nhelps overcome challenges with LLMs, including hallucinations, atomic and\ninexpensive updates, and testability.\nTo highlight the power of the Large Language Expert (LLE) system, we built an\nLLE to assist with the workup of patients newly diagnosed with cancer. Timely\ninitiation of cancer treatment is critical for optimal patient outcomes. However,\nincreasing complexity in diagnostic recommendations has made it difficult for\nprimary care physicians to ensure their patients have completed the necessary\nworkup before their first visit with an oncologist. As with many real-world clinical\ntasks, these workups require the analysis of unstructured health records and the\napplication of nuanced clinical decision logic. In this study, we describe the\ndesign & evaluation of an LLE system built to rapidly identify and suggest the\ncorrect diagnostic workup. The system demonstrated a high degree of clinical-level\naccuracy (>95%) and effectively addressed gaps identified in real-world data from\nbreast and colon cancer patients at a large academic center.", "sections": [{"title": "Background", "content": "Oncology care, like most other domains in healthcare, relies on deep clinical expertise. After\nmedical school, an aspiring oncologist must complete further training in internal medicine, pursue a\n*These authors contributed equally\n*Correspondence: ai@color.com"}, {"title": "Problem Overview", "content": "While extensive investments have been made in building tools to support clinical decision making,\nthese tools have been both challenging to build and impractical for clinicians to use. We break\ndown these challenges into two broad categories. The first revolves around the accurate and efficient\nencoding of clinical guidelines in software in a manner that is consistent with actual clinical practice.\nThe second category of challenges is in the practical implementation of these tools in a clinical\nsetting."}, {"title": "Challenges Representing Clinical Guidelines in Software", "content": "Two common strategies for representing clinical guidelines in software are Machine Learning (ML)\nand rule-based (RB) strategies. ML systems such as LLMs build statistical models from data to\ngenerate probabilistic predictions, enabling them to encode complex relationships. RB systems such\nas Expert Systems rely on explicitly defined rules to create a direct software representation of the\nlogic for a given domain. Each of these strategies on its own runs into substantial challenges when\nused to represent clinical guidelines, these are described in Table 1."}, {"title": "Challenges in Clinical Implementation", "content": "In addition to the challenges of representing clinical guidelines and logic in a software-based system,\na number of clinical implementation challenges need to be addressed in order for a tool to be usable\nand trusted by physicians.\nData quality Medical data is often complex, incomplete, stored in inconsistent formats, and\nrepresented as unstructured text (e.g. clinical notes). Additionally, patient data is frequently distributed\nacross multiple institutions and transferred through PDFs and scanned images without any common\ndata format. This requires clinicians to laboriously pore through documents to extract key information"}, {"title": "Color's Approach", "content": "Color designed a novel architecture, the Large Language Expert (LLE), that applies complex and\ndynamic clinical guidelines to real-world medical data. This architecture is a hybrid LLM & Rule-\nBased system that leverages the strengths of each approach to overcome the limitations of the other.\nThe LLE powers applications that flexibly interface with unstructured data while maintaining a robust,\ndeclarative logic model to drive decisions.\nThe LLE evolves ideas behind Retrieval Augmented Generation (RAG) models in the direction\nof structured expert system formal models. In the LLE architecture, guidelines are organized and\ndeployed as knowledge bases composed of natural language and structured logic that are namespaced\nand versioned. At runtime, the application invokes one or multiple knowledge bases for a given\nworkflow. While this adds rigidity to the knowledge base content and format, it enables robust\ninterrogation of reasoning, consistency, and biases in using these systems in the real world. The\nintegration of this LLE system into an application is shown in Figure 1.\nWith this architecture, it becomes possible to:\n\u2022 check the logic for consistency\n\u2022 build robust test cases\n\u2022 build robust pipelines to transform the primary guideline sources into a knowledge base\nformat\n\u2022 build applications that expose the logic as a human expert would reason about a problem\n\u2022 make deterministic/declarative logic updates"}, {"title": "Knowledge Base Development", "content": "At the core of the LLE model is the translation of clinical guidelines that codify expert-guided\nworkflows. The LLE approach is to translate specific, versioned expert documents into an LLM-\noptimized declarative format. The artifacts generated from this translation step remain human-\nreadable, which enables domain expert review and adjustments. However, the extraction of a formal\nlogic structure makes it possible to automate logic checks, identify gaps and contradictions - and\nmost importantly power robust and reliable workflows. By using structured knowledge bases, we are\nable to generate targeted and streamlined LLM prompts which yield highly accurate results.\nTranslating Logic An obstacle in managing expert system knowledge bases is the difficulty\nconverting a complex, interweaved, and interdependent set of expert rules into executable code. In\nthe LLE, we bypass this complexity by staying as close as possible to the original guidelines and\nusing English as the \u201cexecutable code.\" The advanced reasoning capabilities of OpenAI's o1 model\n[12] enable us to accurately parse the logic embedded within natural language in clinical guidelines"}, {"title": "Real-world application: Identifying pre-treatment workup gaps for patients\nrecently diagnosed with cancer", "content": "After a patient has been diagnosed with cancer and prior to initiation of treatment, they typically\nneed to complete a diagnostic workup (e.g. tests, labs, imaging) to determine their treatment\nplan. For many cancers, professional guidelines exist that help inform what workup is needed.\nUnfortunately, access to guideline compliant workup plans in advance of the first oncology\nvisit is rare and during this visit patients discover that they need to wait several more weeks for\nworkup results [13]. Delays in treatment initiation have been associated with worsened outcomes,\nincluding reduced survival rates. A systematic review and meta-analysis found that even a\nfour-week delay in cancer treatment is associated with increased mortality for seven cancers. Ad-\nditionally, every four-week delay in cancer surgery resulted in a 6-8% increase in the risk of death [14].\nGiven proper tooling, a clinician would be able to identify relevant workup gaps and ensure they are\naddressed as early as possible thereby reducing the time to treatment initiation."}, {"title": "Cancer Copilot Overview", "content": "Color's Cancer Copilot implements the LLE architecture to the problem of workup plan generation. It\nis a two step human-in-the-loop system that in the first step extracts clinical decision factors required\nfor evaluating workups, as identified by the LLE. In the second step the extracted clinical decision\nfactors are used with the logic evaluator to determine which workups are relevant to a patient, and\nwhether they are completed."}, {"title": "Step 1: Clinician review of clinical decision factors", "content": "For each patient, the clinician reviews Copilot's assessment of clinical decision factors. A few\nexamples:\n\u2022 Does patient have positive lymph nodes (cN+)?\n\u2022 Is there a preoperative suspicion of lymph node metastasis (e.g., through imaging or palpable\nnodes on exam)?\n\u2022 Are there abnormalities seen on CT or MRI scan that are considered suspicious but incon-\nclusive for metastases?\nFor each clinical decision factor, this includes a yes/no/unknown answer, along with an explanation\nand list of citations from the patient data explaining the answer. A clinician can adjust answers as\nneeded. This is shown in Figure 6."}, {"title": "Step 2: Clinician review of potential workup gaps", "content": "After reviewing clinical decision factors, the clinician reviews Cancer Copilot's assessment of\nrecommended workups for the patient. Each recommended workup is categorized as being complete\nor incomplete (i.e. \"a gap\") along with an explanation and related information from the knowledge"}, {"title": "Study Design", "content": "Color Health and University of California at San Francisco (UCSF) collaborated on a retrospective\nstudy of patients diagnosed with breast and colon cancer. The goal of the study was to determine if\nthe LLE-based tool (\u201cCancer Copilot\u201d) would enable clinicians to (a) efficiently extract key clinical\nfactors relevant to a newly diagnosed cancer patient and (b) identify which key workup items, as\ninformed by guidelines, were already complete, and which were still required in order to initiate\ntreatment.\nUCSF provided Color with 50 de-identified patient cases for breast cancer and 50 for colon cancer.\nFor each patient, we had two sets of records: diagnosis records, which included all available records"}, {"title": "Study Results", "content": "97.9% of Clinical Decision Factors (Step 1) were not adjusted by clinician\nAcross two runs for 50 breast cancer and 50 colon cancer patients, 12,532 clinical decision factors\nwere extracted by Copilot (8932 for breast, 3600 for colon). The clinician changed 260 outputs, or\n2.1% (172 for breast, 88 for colon). See Table 4 for a granular breakdown.\nIn our post-study analysis, the adjustments made by a clinician spanned three high level categories:\n\u2022 Clinician corrections (1.3%): Valid adjustments made by the clinician. These are the actual\nerrors made by Copilot.\n\u2022 Study artifacts (0.7%): Manual fixes due to content formatting conditions specific to the study\ndata (e.g. de-identified field placeholders in areas that are non-essential for de-identification).\nThese changes would not be needed outside the context of the study."}, {"title": "Time Spent: Non-specialists unfamiliar with patient cases take less than 7.5 minutes to\nfinalize recommendations", "content": "Along with assessing the system's capability to generate guideline-concordant recommendations,\nwe also measured the time spent by a non-specialist physician, unfamiliar with the patient cases, at\neach step of the workflow. For colon cancer, the median time spent by the clinician to approve the\nextracted decision factors (step 1) was 2.3 minutes, followed by a median time of 2.1 minutes to\nfinalize the workup recommendations (step 2). For breast cancer, these times were 5.2 & 2.1 minutes,\nrespectively.\nIt should be noted that these times should be considered directional, given the retrospective study\nconditions. Even directionally, though, these are a considerable improvement over the hours anecdo-\ntally shared by physicians and their teams that are spent on reviewing new patient cases in the context\nof guidelines."}, {"title": "Conclusions", "content": "Our study provides a promising demonstration of the potential for Large Language Experts (LLEs) to\nmeasurably improve the delivery of high-quality, guideline-concordant cancer care. By bringing to-\ngether the strengths of large language models (LLMs), structured knowledge bases, and a streamlined\nexperience, the Cancer Copilot enables clinicians to efficiently review patient records and identify\nworkup gaps, while maintaining a very high level of accuracy.\nAcross the real-world breast & colon cancer cases that were used to test the system, the correction rate\non Copilot's recommendations, measured through changes by the clinician, fell below 5% (2.1% on\nclinical decision factors extracted, 4.5% on workups recommended). Necessitating such infrequent\ninterventions by a clinician indicates the effectiveness of the LLE architecture in faithfully encoding\nand applying complex clinical logic to real-world patient data.\nEven at this initial performance level, the Copilot represents a tool with the potential to meaningfully\nreduce busywork and enhance clinical quality. The <5% correction rate falls short of what would be\nneeded for purely autonomous use. However, the limited human input required to reach clinical-level\naccuracy took less than 7.5 minutes per patient case on average - a substantial improvement over the\nhours often spent reviewing records and guidelines to determine appropriate workup outside of the\nstudy setting."}, {"title": "LLE Architecture", "content": "Compared to pure machine learning approaches, the LLE architecture proved highly effective and\nefficient to implement in practice. The initial knowledge base development required a focused but\nmanageable effort, with the LLM-assisted translation from natural language guidelines to structured\nlogic. The resulting system achieved high baseline performance without the need for extensive model\nfine-tuning or large labeled training datasets.\nMore importantly, when issues did arise, the transparent rule definitions enabled rapid diagnosis and\nresolution. The limited set of failure modes seen in the study, such as challenges with temporal math\nor occasional spurious logical leaps, point to clear opportunities for further system refinement. For\nexample, key calculations like age could be delegated to specialized sub-functions, and the knowledge\nbase logic could undergo additional adversarial testing to identify and tighten overly permissive rules.\nLooking beyond the scope of this initial application, the LLE architecture may offer a powerful\nnew approach to implementing guideline-based care more broadly. Many healthcare domains\ninvolve intricate-but-standardized workflows that have to interface with an inconsistent and largely\nunstructured data reality. Potential use cases could include clinical trial matching, emergency\ndepartment triage, insurance pre-authorizations, quality measures, and the scaling of access to scarce\nexpert knowledge.\nThe ability of the LLE approach to smoothly integrate centrally-curated guidelines with site-specific\ncustomizations makes it appealing as a framework that can balance consistency with flexibility.\nAs comfort and capabilities grow, we envision an ecosystem of knowledge bases published by\nprofessional societies, adapted and validated by individual institutions, and made accessible through\npurpose-built clinical applications.\nUltimately, our results suggest that thoughtful combination of language models with more traditional\nknowledge representational structures can enable a new level of capable, reliable, and user-friendly\nsystems. With further validation and hardening, this approach could help transform the way that\nmany forms of specialized expertise are disseminated and applied to real-world decision-making."}]}