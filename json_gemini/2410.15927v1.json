{"title": "GReFEL: Geometry-Aware Reliable Facial Expression Learning under Bias and Imbalanced Data Distribution", "authors": ["Azmine Toushik Wasi", "Taki Hasan Rafi", "Raima Islam", "Karlo \u0160erbetar", "Dong-Kyu Chae"], "abstract": "Reliable facial expression learning (FEL) involves the effective learning of distinctive facial expression characteristics for more reliable, unbiased and accurate predictions in real-life settings. However, current systems struggle with FEL tasks because of the variance in people's facial expressions due to their unique facial structures, movements, tones, and demographics. Biased and imbalanced datasets compound this challenge, leading to wrong and biased prediction labels. To tackle these, we introduce GReFEL, leveraging Vision Transformers and a facial geometry-aware anchor-based reliability balancing module to combat imbalanced data distributions, bias, and uncertainty in facial expression learning. Integrating local and global data with anchors that learn different facial data points and structural features, our approach adjusts biased and mislabeled emotions caused by intra-class disparity, inter-class similarity, and scale sensitivity, resulting in comprehensive, accurate, and reliable facial expression predictions. Our model outperforms current state-of-the-art methodologies, as demonstrated by extensive experiments on various datasets.", "sections": [{"title": "1. Introduction", "content": "One of the most universal and significant ways that people communicate their emotions and intentions is through the medium of their facial expressions [34]. In recent years, facial expression learning (FEL) has garnered growing interest within the area of computer vision due to the fundamental importance of enabling computers to recognize interactions with humans and their emotional affect states. While FEL is a thriving and prominent research domain in human-computer interaction systems, its applications are also prevalent in healthcare, education, virtual reality, smart robotic systems, etc [29, 30].\nDespite recent strides in facial expression recognition technology, the task remains daunting for several reasons.\nOne major hurdle lies in the diverse and complex nature of human facial expressions (as presented in Figure 1). People's facial structures, movements, tones, and demographics contribute to a wide variance in expressions, making it challenging for current systems to accurately interpret and classify them. For instance, telling the difference between a happy smile and a mischievous smirk can be tricky because their lip movements can look quite similar. Also, people express emotions in different ways some might smile broadly, while others might give a more subtle grin. This variation makes it even harder for computers to accurately pick up on the meaning behind facial expressions. Additionally, consider the challenge of differentiating between a surprised expression and a confused one. Both might involve raised eyebrows and widened eyes, but the context and subtle cues can make a big difference in interpreting the emotion accurately. The complexity of emotions such as anger is often amplified by factors like skin tone and contextual cues, leading to a multitude of potential interpretations that current FEL systems struggle to navigate.\nThese issues, named by intra-class disparity and inter-class similarity, present persistent challenges in facial expression understanding systems [26, 37, 41, 47]. Within-class variations, such as subtle differences in expression intensity or style, pose difficulties in accurately categorizing similar expressions. For instance, a slight change in eyebrow positioning or mouth curvature can drastically alter the perceived emotion, making classification more ambiguous. Conversely, inter-class similarity adds another layer of complexity, as distinct expressions may share common features or gestures, leading to misclassification. Addressing these nuances is crucial for enhancing the reliability and robustness of FEL frameworks, yet current approaches often fall short in effectively mitigating these challenges.\nAnother significant obstacle in FEL stems from biased and imbalanced datasets used for training. These datasets often fail to adequately represent the diversity of facial expressions across different demographics, leading to skewed and inaccurate predictions. For example, datasets may over-"}, {"title": "2. Related Works", "content": "Facial Expression Learning. Facial expression learning involves labeling expressions from facial images, comprising facial detection, feature extraction, and expression recognition phases [34]. Deep learning algorithms, such as self-supervised feature extraction [40], have optimized FEL systems. Recent advancements include multi-branch networks [37], uncertainty estimation [30] and relation-aware local-patch representations [39]. Attention networks based on regions have shown effectiveness for robust FEL[21, 33].\nVision Transformers in FEL. Recent works demonstrate the resilience of Vision Transformers (ViT) against dis-"}, {"title": "3. Approach", "content": "In our approach, we propose a robust feature extraction strategy using ViT and a reliability balancing mechanism to address challenges in FEL. We scale input photos and apply augmentation techniques like rotation and color enhancement for better augmentation. Our pipeline mitigates biases and overfitting by randomly selecting images and expressions during training. Cross-attention ViT is employed for feature extraction, addressing scale sensitivity and intra-class discrepancy. Landmark extraction locates facial landmarks, and a pre-trained image backbone model extracts features. Multiple feature extractors detect low to high-level features, integrated using a cross-attention mechanism for feature vector embedding. Then, primary label distributions are generated using MLPs. Confidence is evaluated using Normalized Entropy. We introduce a reliability balancing method to improve model predictions, addressing limitations in predicting similar classes. Learnable anchors and multi-head self-attention mechanism stabilize label distribution, enhancing reliability. Dropout layers provide additional regularization for robustness against noise and inadequate data. The resulting model, integrating extensive feature extraction and reliability balancing, offers precise and credible predictions even in ambiguous contexts.\nProblem Formulation. Let $x^i$ be the i-th instance variable in the input space X and $y^i \\in Y$ be the label of the i-th instance with $y = {Y_1,Y_2......Y_{Ncls} }$ being the label set. Let $P_r$ be the set of all probability vectors of size n. Furthermore, let $l^i \\in P^{Ncls}$ be the discrete label distribution of i-th instance. Additionally, let $e = p(x; \\theta_p)$ be the embedding output of the Window-Based Cross-Attention ViT (explained in 3.1) network p with parameters $ \\theta_p$ and let $f(e; \\theta_f)$ be the logit output of the MLP classification head network $f_{CH}$ with parameters $ \\theta_f$."}, {"title": "3.1. Feature Extraction", "content": "We use a complex image encoder by integrating a window-based cross-attention mechanism, to capture patterns from input images. We extract features by the image backbone and facial landmark detectors. We use IR50 [35] as image backbone and MobileFaceNet [5] as facial landmark detector, both pre-trained models. For each level, firstly, division of image features $X_{img} \\in R^{N_p\\times D}$ is performed, where $N_p$ represents the number of patches and $D$ denotes the feature dimensions. The number of patches dictates how the image is fragmented into smaller pieces (e.g., 9 patches would result in 9 small pieces in 3 \u00d7 3 formation). These patches are then transformed into many non-overlapping windows, $Z_{img} \\in R^{M\\times D}$, where $Z_{img}$ contains $M$ tokens. We use 28 \u00d7 28 patches for low-level (local) feature extraction, 14 x 14 for mid-level, and 7 \u00d7 7 for high-level (global) feature extraction, as described in Section 4.1.\nAfter $Z_{img} \\in R^{M\\times D}$, down-sampling of the landmark feature $X_{lm} \\in R^{A_c\\times H\\times W}$ takes place, where $A_c$ is the number of channels in the attention network, $H$ and $W$ are the height and width of the image. The down-sampled features are converted into the window size, where the smaller representation of the image is taken and it is represented by $Z_{lm} \\in R^{c\\times h \\times w}$ where $c = D, h \\times w = M$. The features are reshaped in accordance with $z_{img}$'s shape. The cross-attention with I heads in a local window can be formulated as follows at this point:\n$q = Z_{lm}W_q, k = z_{img}W_k, v = Z_{img}W_v$ (1)\n$o^{(i)} = softmax(q^{(i)}k^{(i)T}/\\sqrt{d} + b)v^{(i)}, i = 1, ..., I$ (2)\n$o = [o^{(1)}, ..., o^{(I)}]W_o$ (3)\nwhere $W_q$, $W_k$, $W_v$ and $W_o$ are the matrices used for mapping the landmark-to-image features, and q, k, v denote the query matrix for landmark stream, and key, and value matrices for the image stream, respectively from different windows used in the window-based attention mechanism. [] represents the merge operation where the images patches are combined to identify the correlations between them and lastly, the relative position bias is expressed as $b \\in R^{IXI}$ which aids in predicting the placement between landmarks and image sectors.\nWe use the equations above to calculate the cross-attention for all the windows, named by Overall Cross Attention (OCA), as shown in Figure 3. The transformer encoder for the cross-fusion can be calculated as follows:\n$X'_{img} = OCA(X_{img}) + X_{img}$ (4)\n$X_{img\\_0} = MLP(Norm(X'_{img})) + X'_{img}$ (5)\nwhere $X'_{img}$ is the combined image feature using OCA, $X_{img\\_0}$ the output of the Transformer encoder, and $Norm()$ represents a normalization operation for the full image of all windows combined. Using window information and dimensions ($z_{img}$, M, D, C, H, W, etc.), we extract and combine window based feature information to $X_{Oi}$ (i-th level window-based combined features of each image) from $X_{img\\_0}$ (extracted features of all windows of each image together).\nWe introduce a vision transformer to integrate the obtained features at multiple scales $X_{O1}, ..., X_{Oi}$. Our attention mechanism is able to capture long-range dependencies as it combines information tokens of all scale feature maps:\n$X_o = [X_{O1}, ..., X_{Oi}]$ (6)\n$X_o' = MHSA(X_o) + X_o$ (7)\n$X_{oout} = MLP(Norm(X_o')) + X_o'$ (8)"}, {"title": "3.2. Reliability Balancing", "content": "Majority of Facial Expression Learning datasets are labeled using only one label for each sample. Inspired by [7, 13], we provide an alternative approach, in which, we learn and improve label distributions utilizing a label correction approach. We calculate a label distribution primarily that uses the embedding e directly into the MLP network. Subsequently, the reliability balancing section employs label correction techniques to stabilize the primary distribution. This results in improved predictive performance through more accurate and reliable labeling.\nPrimary Label Distribution. From sample x, using the p network we can generate the corresponding embedding $e = p(x; \\theta_p)$ and using the f-network consisting MLP, we can generate the corresponding primary label distribution:\n$l = softmax(f(e; \\theta_f)).$ (9)\nWe use the information contained in the label distribution with label corrections during training to improve the model performance.\nConfidence Function. To evaluate the credibility of predicted probabilities, a confidence function is designed. Let $C_f : P^{Ncls} \\rightarrow [0, 1]$, be the confidence function. $C_f$ measures the certainty of a prediction made by the classifier using normalized entropy function H(l). The functions are defined as:\n$C_f(l) = 1 - H(l)$ (10)\n$H(l) = - \\frac{1}{Ncls} \\sum_i l_i log(l_i)$ (11)\nFor a distribution where all probabilities are equal, the normalized entropy is 1, indicating maximum uncertainty, and the confidence value is 0. Conversely, if the probability of one class is 1 and all others are 0, the normalized entropy is 0, indicating no uncertainty, and the confidence value is 1."}, {"title": "3.3. Label Correction", "content": "The conundrum of label accuracy, distribution stability, and reliability has been a mainstream problem in FEL. The novel approach we propose to resolve this is a combination of two distinct measures of label correction: anchor label correction (geometric) and attentive correction.\nAnchor Label (Geometric) Correction. We define anchor $a^{ij}$ ($i \\in {1, ..., Ncls}, j \\in {1,2 . . . K}$) to be a point in the embedding space. Let A be a set of all anchors. During training, we use K trainable anchors for each label, with K being a hyperparameter ($k^{th} \\in K$). We assign another label distribution $m^{ij} \\in P^{Nels}$ to anchor $a^{ij}$, where $m^{ij}$ is defined as:\n$m_k^{ij} = \\begin{cases}1, if k = i\\\\0, otherwise\\end{cases} \\\\ \\(k^{th}anchor\\) $ (12)\nIntuitively, here it means anchors $a^{1,1}, a^{1,2} . . . a^{1,K}$ are labeled as belonging to class 1, anchors $a^{2,1}, a^{2,2} . . . a^{2,K}$ are labeled as belonging to class 2 and so on. To correct the final label and stabilize the distribution, we use geometric information about similarity between the embeddings and anchors. The similarity score $s^{ij}(e)$ is a normalized measure of similarity between an embedding e and an anchor $a^{ij} \\in A$. The distance between e and a for each batch and class is:\n$d(e, a) = \\sqrt{\\sum \\frac{(a_i - e_i)^2}{dim_e}}$ (13)\nHere, $dim_e$ is the dimension of embedding e. Distances $|a - e|^2$ are reduced over the last dimension $dim_e$ and element-wise square root is taken for stabilizing values. The similarity score $s^{ij}$ is then obtained by normalizing distances:\n$s^{ij}(e) = \\frac{exp(-d(e, a^{ij})))}{\\sum_{N} \\sum_{K} exp(-d(e, a^{is}))/\\delta}$ (14)\nwhere $\\delta$ is a hyperparameter used in the computation of Softmax to control the steepness of the function. The default value used for $\\delta$ is 1.0. From similarity scores we can calculate the anchor label correction term as follows:\n$t_g(e) = \\sum_{i}^{N} \\sum_{j}^{K} s^{ij}(e) m^{ij} .$ (15)\nAttentive Correction. For multi-head attention [31], Let a query with query embeddings $q \\in R^{d_e}$, key embeddings $k \\in R^{d_k}$, and value embeddings $v \\in R^{d_v}$ is given. With the aid of independently learned projections, they can be modified with h, which is the attention head. These parameters are then supplied to attention pooling. Finally, these outputs are altered and integrated using another linear projection. The process is described as follows:\n$h_i = f(W^{(Q)}q, W^{(K)}k, W^{(V)}v) \\in R^{PV}, W_{out} = W_o [h_1...h_{nheads}]$ (16)"}, {"title": "Final Label correction", "content": "Where $W^{(Q)} \\in \\mathbb{R}^{d_{model}xdQ}, W^{(K)} \\in \\mathbb{R}^{d_{model}xdk},$ $W^{(V)} \\in \\mathbb{R}^{d_{model}xdv}, and W_{0} \\in \\mathbb{R}^{n_{headsdv}xdmodel}$ are trainable parameters [31], f is the attentive pooling, and each $h_i(i = 1,2,..., n_{heads})$ is an attention head. Also, $dq = dx = dv = dmodel/Nheads$ following [31].\nAs we are using self-attention, all inputs (q, k, v denoting query, key and value parameters respectively) are equal to the embedding e [31]. Self-attention is applied to individual visual embeddings, not across the entire batch. e is passed through the multi-head self-attention layer to obtain the attentive correction term $t_a$. $t_a$ is calculated based on the output $W_{out}$ from Eq. (16):\n$t_a = softmax (W_{out}).$ (17)\nMulti-head self attention (MHSA) [31] is designed to focus on the crucial parts relevant to a particular class. Self-attention offers context-aware representations for each sequence element, while multi-head self-attention enhances this by learning various aspects of element relationships, resulting in a more robust understanding [3, 31]. In this work, MHSA can identify important facial areas for each class, thereby improving classification accuracy.\nFinal Label correction. To combine the correction terms, we use weighted sum, with weighting being controlled by the confidence of label corrections:\n$t = \\frac{C_g}{C_g + C_a} t_g + \\frac{C_a}{C_g + C_a} t_a$ (18)\nwhere $c_g = C_f(t_g)$ and $c_a = C_f (t_a)$. $t_a$ is the attentive correction term, achieved from h by normalizing. $C_f()$ stands for the confidence function, calculates confidence of each class predictions.\nFinally, to obtain the final label distribution $L_{final}$, we use a weighted sum of label distribution l and label correction t, as follows:\n$L_{final} = \\frac{C_l}{C_l + C_t} l + \\frac{C_t}{C_l + C_t} t$ (19)\nwhere $c_l = C_f(l)$ and $c_t = C_f(t)$. The label with the maximum value in the final corrected label distribution $L_{final}$ is provided as a corrected label or a final predicted label."}, {"title": "3.4. Loss Function", "content": "The loss function used to train the model consists of three terms such as class distribution loss, anchor loss, and center loss.\nClass Distribution Loss ($L_{cls}$): To make sure each example is classified correctly, we use the negative log-likelihood loss between the corrected label distribution $L_{final}$, and label $y^i$:\n$L_{els} = - \\sum_{i}^{N} \\sum_{j}^{M} y_j^i log L_{finalj}$ (20)\nAnchor Loss ($L_a$): In order to amplify the discriminatory capacity of the model, we want to make margins between anchors large so that we add an additional loss term:\n$L_a = -\\sum \\sum \\sum \\sum |a^{is} \u2013 a^{kl}|^2$. (21)\nWe include the negative term in front because we want to maximize this loss. The loss is also normalized for standard uses.\nCenter Loss ($L_c$): To make anchors good representation of their class, we want to make sure anchors and embeddings of the same class stay close in the embedding space. To ensure that, we add an additional error term:\n$L_c = min |z^i \u2013 a^{v*}k|^2$. (22)\nTotal Loss ($L_{total}$): Our final loss function can be defined as:\n$L_{total} = A_{cls}L_{cls} + \\lambda_a L_a + \\lambda_c L_c$ (23)\nwhere $A_{cls}$, $\u03bb_a$, and $\u03bb_c$ are hyperparameters used to keep the loss functions on the same scale."}, {"title": "4. Experiments", "content": "4.1. Experimental Setup\nDatasets. We use AffectNet [27] (420,299 samples; 8 classes), Aff-Wild2 [11] (1,413,000 samples), RAF-DB [17, 18] (68,718 samples),FERG-DB [1] (55,769 samples), JAFFE [23] (213 samples), and FER+ [2] (35,801 samples) datasets, having 6-8 classes. Among them, AffectNet, Aff-Wild2, FER+, and RAF-DB datasets exhibit class imbalances and are collected in real-world settings.\nData Distribution Adjustments. We use sample augmentation to expand the training set in class-imbalanced cases, aiding feature identification. Common FEL preprocessing steps include resizing, scaling, rotating, flipping, cropping, color augmentation, and normalization. Uneven class distributions can cause bias and over-fitting. To counter this, equally distributing information from all classes improves model accuracy. Refining datasets ensures balanced training data, mitigating biases. During training, $N_{pg}$ images are randomly selected from each video or face group. From these, B images per expression are chosen for training, creating a batch of ($B \u00d7 N_{cls}$ (number of classes)) images per epoch, reducing biases and overfitting.\nBaselines. We utilized the following baselines in our experiments: SCN [34], RAN [33], TransFER (T.FER) [39], DMUE [30], RUL [42], EfficientFace [46], Face2Exp (F2Exp) [41], POSTER [47], EAC [43], Latent-OFER (L. OFER) [14], LA-Net [38], DAN [36], and POSTER++ [26].\nImplementation Details. For each dataset, we exclusively use cropped and aligned images. These images are resized to 256\u00d7256 and then randomly cropped to 224x224 to address overfitting and data imbalance. Heavy augmentation methods are applied during pre-processing as described in Section 4.1. For data refinement, we consider 512 images"}, {"title": "4.2. Comparison with State-of-the-Art Methods", "content": "The table shows the comparison of the accuracy of multiple State-of-the-Art facial expression learning methods. Upon investigation of the results, it is apparent that GREFEL outperforms all other models across all datasets, attaining the highest accuracy scores for each dataset. Specifically, GREFEL earns an accuracy score of 68.02% in AffecteNet, 72.48% on the AffWild2 and 92.47% on RAF-DB dataset (large in-the-wild dataset), which is significantly higher than POSTER++ and the third best TransFER[30] (CVPR'21). Among the compared methods, we think POSTER++ (AffectNet 63.76%, AffWild2 69.18%) is the most suitable baseline of our work. Compared to this baseline, our ReFEL achieves 68.02% on AffectNet 72.4% on AffWild2 (3-5% better accuracy than POSTER++ on these in-the-wild benchmarks). GReFEL also outperforms every other model in the study, with accuracy scores on the FER+, FERG-DB and JAFFE datasets of 93.09%, 98.18% and 96.67%, respectively, outperforming every other model tested. Our novel reliability balancing section reduces all kinds of biases, resulting in exceptional performance in all circumstances.\nConfusion Matrix. shows confusion matrices from the AffectNet and RAF-DB datasets, with and without reliability balancing, and reveals several key insights. In AffectNet, reliability balancing notably enhances true positive rates for most emotions, except for neutral and contempt expressions. Without balancing, the classifier struggles with neutral, contempt, and anger distinctions. RAF-DB's performance sees minor improvements with balancing, showcasing a better overall classification compared to AffectNet. Despite this, neutral, contempt, and anger remain challenging to classify accurately. Both datasets show higher true positive rates for surprise and happy expressions, with intriguing confusions between certain emotion pairs like fear and surprise. It indicates that reliability balancing functions effectively, reducing disparities between classes.\nFeature Extraction and Clustering. In Fig. 5, the t-SNE plot visually illustrates class differences in the embedding space, with each color representing a distinct class using AffWild2 dataset. The Davies-Bouldin score (\u2193) evaluates cluster resemblance, while the Calinski-Harabasz score (\u2191) measures cluster variance. Observations reveal uniformly spaced groups with reliable classifications and noisy areas indicating inter-class similarity and disparity issues."}, {"title": "4.3. Ablation Study", "content": "Here we explore the impact of different reliability balancing and loss function setups. More ablation studies are available in the supplementary materials.\nStudy of Different Model Setups for Reliability Balancing. The table 2 summarizes model setups, their accuracy and the F1 score for the AffWild2 dataset. Integration of the Reliability Balancing (RB) module indicates that the F1 scores significantly increase after using reliability balancing methods. We also observe that the initial ViT-based feature extraction requires 43.6M parameters to achieve an accuracy of 68.15%. However, by incorporating a few additional parameters for reliability balancing, we can significantly enhance the performance, achieving an accuracy of 72.48% in the model. Also, the increment in computational complexity is minimal.\nStudy of Different Loss Setups. Table 3 summarizes different loss setups and their associated accuracy and F1 score using AffWild2 dataset. Combining classification, anchor, and center losses achieves the highest accuracy of 72.48%,"}, {"title": "5. Conclusion", "content": "Our paper introduces GReFEL, a novel FEL approach addressing biased and unbalanced data. GREFEL combines attentive feature extraction with reliability balancing using heavy augmentation and data refinement alongside a Vision Transformer (ViT). Our method effectively handles inter-class similarity, intra-class disparity, and label ambiguity. By incorporating trainable anchor points in embedding space to learn and differentiate between different facial expression landmarks, we stabilize distributions and enhance performance. Experimental analysis across datasets demonstrates GReFEL's superiority over state-of-the-art models, highlighting its potential to advance facial expression learning."}, {"title": "6. Ablation Studies", "content": "6.1. Study of Different Values of \u5165\nThe A values were chosen by our grid search on Aff-Wild2 dataset. Table 4 shows the results. Interestingly, setting all A values to 1.0, which is our default setting, achieves the best performance.\n6.2. Study of Different Loss Functions\nFig. 6 demonstrates the effects of different loss function setups in the training stage of our experiment using AffWild2 [12] dataset. Anchor loss dominance causes the model to drop its performance after some initial good epochs, conveying that the model starts over-fitting on anchors, ignoring true labels. Relying more on similarities than the actual prediction performance, this setup fails to fulfill the criteria. The other setups are quite stable and close. The ideal combination used in the study helps the model train faster and better.\n6.3. Effects of Data Augmentation\nTable 5 shows that without data augmentation, GReFEL still obtains competitive performance and outperforms POSTER++ in challenging Aff-Wild2 dataset.\n6.4. Study of Different Number of Anchors K\nTable 6 demonstrates that optimal recognition accuracy is achieved with 8\u201310 anchors. Accuracy gradually increases until it reaches this range, beyond which it sharply declines. Few anchors fail to model expression similarities effectively, while excessive anchors introduce redundancy and noise, leading to decreased performance.\n6.5. Study of Noise and Label Smoothing\nK for Different Noise vs. Accuracy. Table 7 illustrates that increasing noise levels decrease model accuracy due to data clarity and complexity issues in AffWild2 [12] dataset. However, increasing the value of K improves performance"}, {"title": "7. Explaining Reliability Balancing", "content": "The reliability balancing module plays a crucial role in enhancing the accuracy and reliability of predictions by stabilizing probability distributions in our framework. This strategy increases probability confidence values for appropriate labels while decreasing confidence in incorrect predictions, as Fig. 9 clearly indicates. For instance, Labels 2, 5, and 7 experience a noticeable rise in their maximum confidence values after applying reliability balancing, ensuring more accurate predictions. Conversely, the method reduces the confidence levels of incorrect predictions, as seen in Labels 0, 1, and 3, where the incorrect maximum values decrease to a range of 0.15-0.25. Notably, even in these cases, the correct labels maintain a probability range of 0.2-0.3, enabling the model to make the right predictions. After implementing the corrective measures, the maximum and minimum probabilities across the sample increased to 0.5429 and 0.0059, respectively, resulting in a more stable and balanced distribution. A key observation is that the standard deviation of the corrected predictions (0.0881) was found to be lower than that of the primary predictions (0.1316), providing strong evidence for enhanced stability and balance.\nFurthermore, the reliability balancing strategy proves invaluable in scenarios where the primary model struggles with label ambiguity, intra-class similarity, or disparity issues within the images. As evident from Fig. 9, even when the maximum primary probability exceeds 0.4, the associated labels may be erroneous, rendering the model unreliable. Thus, the reliability balancing method supports the model in both extremely uncertain conditions and extremely confident scenarios where the primary model makes poor conclusions."}]}