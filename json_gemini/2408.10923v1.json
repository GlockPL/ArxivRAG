{"title": "LBC: Language-Based-Classifier for Out-Of-Variable Generalization", "authors": ["Kangjun Noh", "Baekryun Seong", "Hoyoon Byun", "Youngjun Choi", "Sungjin Song", "Kyungwoo Song"], "abstract": "Large Language Models (LLMs) have great success in natural language processing tasks such\nas response generation. However, their use in tabular data has been limited due to their inferior\nperformance compared to traditional machine learning models (TMLs) such as XGBoost. We find\nthat the pre-trained knowledge of LLMs enables them to interpret new variables that appear in\na test without additional training, a capability central to the concept of Out-of-Variable (OOV).\nFrom the findings, we propose a Language-Based-Classifier (LBC), a classifier that maximizes the\nbenefits of LLMs to outperform TMLs on OOV tasks. LBC employs three key methodological\nstrategies: 1) Categorical changes to adjust data to better fit the model's understanding, 2) Advanced\norder and indicator to enhance data representation to the model, and 3) Using verbalizer to\nmap logit scores to classes during inference to generate model predictions. These strategies,\ncombined with the pre-trained knowledge of LBC, emphasize the model's ability to effectively\nhandle OOV tasks. We empirically and theoretically validate the superiority of LBC. LBC\nis the first study to apply an LLM-based model to OOV tasks. The source code is at https:\n//github.com/ASDASDanonymous/Language-Based-Classifier-for00Vtasks.", "sections": [{"title": "Introduction", "content": "LLMs [19, 1, 24, 4] have recently been applied to tabular data. Language-Interfaced-Fine-Tuning\n(LIFT) [5] demonstrated that LLMs achieve reasonable performance on tabular data tasks while\nmaintaining LLM's original structure. However, the pre-trained knowledge of LLMs holds even\nanother potential, their ability to interpret OOV. So, we propose a new model called a Language-\nBased-Classifier (LBC) to solve the OOV tasks.\nOOV tasks are an important problem and are the subject of several ongoing studies [8, 22, 6].\nHowever, studies applying LLM to tabular data do not handle tabular data in an OOV setting. In\nreal-world settings, a variety of constraints often hinder model training, emphasizing the importance\nof OOV tasks. For example, in healthcare, privacy and regulatory barriers prevent data sharing\nbetween hospitals. A model trained on Hospital A's data may encounter new, unseen variables\nwhen applied to Hospital B's data, leading to OOV situations. We argue that LBC has strengths in\nhandling OOV tasks, and our rationale is as follows. Converting tabular data to natural language\nprompts is intuitive, flexible, and easy. This transformation significantly simplifies the handling\nof OOVs, allowing us to seamlessly handle variables that might not have been discovered during\ntraining, overcoming a common limitation of TMLs. Furthermore, LBC leverages the pre-trained\nknowledge built into LLMs. Unlike TMLs, which struggle with data points or scenarios not present\nin the training set, LLMs leverage their inherent knowledge. We verified that LBC use OOVs to\nincrease the probability of the correct answer class based on pre-trained knowledge. These advantages"}, {"title": "Related Works", "content": ""}, {"title": "Tabular Data Analysis with LLMs", "content": "LLMs now extend to analyzing tabular data. LIFT [5] converts tabular data into natural language\nprompts for use in LLM, and performs similarly to traditional models like XGBoost [3]. Models like\nTP-BERTa [25] and TabPFN [10] follow the LM structure but lack the ability to contextualize OOVs.\nOn the other hand, LBC excels at handling OOV tasks and consistently outperforms existing models.\nLBC's performance in tabular data classification has been validated through theoretical analysis and\nstatistical tests."}, {"title": "Out-of-Variable", "content": "Machine learning (ML) models often face the challenge of adapting to new environments with\nadditional, unobserved variables. MomentLearn [7] was proposed to address this by using a predictor\ntrained in a source environment and a additional objective matrix for partial derivatives for OOV\ntasks. However, its application in real-world scenarios is limited. The LBC method overcomes these"}, {"title": "Verbalizer", "content": "Verbalizer is a mechanism for mapping the various output forms from an LLM to specific classes [21,\n12]. Verbalizer contributed to reducing subjective bias in LLM by using a knowledge base to leverage\ndiverse and comprehensive label words. It is also said that the noise of label words in classification\ncan also be improved with a verbalizer. We argue that even in tabular data classification, we need a\nparticular way to map the output of an LLM to the output of a classifier and that we should apply a\nverbalizer to it."}, {"title": "Low-Rank Adaption", "content": "LORA [11] has emerged as an innovation in adapting pre-trained models to specific tasks without\nextensive retraining of the entire model. LoRA introduces an approach to fine-tuning large pre-\ntrained models. Instead of updating the whole parameter set, LoRA modifies a small subset of\nthe model's weights through a low-rank matrix. This method allows pre-trained models to adapt\nefficiently while maintaining their original structure and strengths. We theoretically validate the strong\nclassification performance of LBC fine-tuned with LoRA, backed by the proven generalization ability\nof LoRA [27]."}, {"title": "Preliminary", "content": ""}, {"title": "Basic Dataset Conversion", "content": "This section describes the process of converting tabular data into prompts for input to LBC. Since our\nmodel relies on a frozen pre-trained LLM, converting tabular data into prompts is a crucial step. Let an\ninstance of tabular data with K features be represented as [[V\u2081: x1], [V2: X2],..., [VK:xK], [class: y]],\nwhere Vk is the kth variable name and xk is the kth variable value. We need a method for the LLM to\nclearly distinguish between the variables in this dataset as prompts and the class as the output. This\ninvolves creating a conversion technique that clearly marks the end of the prompt and the beginning of\nthe response while ensuring that the answer isn't overly lengthy. Therefore, we format the conversion\nas: \"prompt: V\u2081 is x1, V2 is x2, ..., VK is K. What is the class? label: y@@@\u201c.\nIn this setup, the 'prompt' is the input to LLM, and the 'label' is the label for the data instance."}, {"title": "The Order of the Variables", "content": "During the tabular data to the prompt conversion process, different prompts are generated depending\non the variable order. One instance of tabular data converts to several different types of prompts\nbased on the order of the variables. The total number of prompts that can be generated by changing\nthe order of the variables is K!. Every prompt is a transformation of a single instance of tabular data,\nbut the order of the variables gives it a different form, which causes LBC to interpret it differently.\nTherefore, the order of the variables is a factor that directly affects LBC's performance."}, {"title": "Fine-tuning LLM", "content": "Feeding the converted prompts into LBC yields a vector of vocabulary sizes, which is logit for each\nword in the vocabulary. We use this logit to fine-tune the LLM. Let Logit be the logit vector for\na single input vector. During fine-tuning, J obtained from the model is used to compute the loss\nagainst the true labels. Let Label be the one-hot encoded vector of the true label for the input. The\nloss is calculated using a loss function J defined as:\nJ(Logit, Label) = CE(Logit, Label)\nwhere CE is cross-entropy with logit loss function. After calculating the loss, the model's parameters\nare updated using an optimizer through gradient descent. The update rule in gradient descent can be\ndescribed as follows:\n$\\\\\\\\theta = \\\\\\\\theta - \\\\\\\\eta \\\\\\\\nabla_{\\\\\\\\theta} J$\nwhere @ is the model's parameters, \u03b7 is the learning rate, and VoJ is the gradient of the loss with\nrespect to the model parameters."}, {"title": "LLM-based Tabular Prediction", "content": "TMLs face significant challenges when processing textual data within feature sets. Text preprocessing\ninevitably leads to semantic information loss. Despite applying specialized techniques such as one-hot\nencoding or text vectorization methods (e.g., TF-IDF, Word2Vec, etc.), TMLs remain vulnerable\nto noise due to its lack of linguistic comprehension. Furthermore, the high dimensionality of text\nembeddings often impedes efficient learning, and attempts to mitigate this through dimensionality\nreduction techniques risk further information loss.\nIn contrast, LLMs offer a promising alternative for handling textual and numerical data in\nmachine learning tasks. LLMs demonstrate superior capability in comprehending semantic content\nand discerning inter-feature relationships, which is beneficial when critical information is presented\ntextually.\nThe previous approaches to LLM-based tabular data classification tasks [5] rely on directly\ncomparing the output text generated by the model with class texts such as 'no' or 'yes'. If the\nprediction is an exact match, it is classified with the corresponding class text. Conversely, if the output\ntext differs, the model's prediction is marked as 'None' and automatically classified as incorrect. To\naddress this limitation, we utilize the logit score to map directly to a specific class rather than using\nthe model's output texts. For this mapping process, we utilize the probability values of the synonyms\nof the logit score's class text."}, {"title": "Methodology", "content": ""}, {"title": "Categorical Change", "content": "We find that LBC has a better interpretation of categorical variables than numerical ones because it\nis an LLM-based model. However, this poses a challenge as many key variables in tabular data are\nnumerical. In particular, when LBC deals with OOVs, if the value of the input is numeric, pre-trained\nknowledge cannot be utilized, unlike categorical type values where the word itself has meaning.\nTherefore, we need a method to convert numerical variables to categorical types so that LBC leverages\nits pre-trained knowledge of important variables or OOVs for easier interpretation, and we find that\nmapping numerical variables to categorical variables using N categories improves performance. The\nN categories are determined based on the principles of N-tiles, similar to quartiles but dividing the\ndataset into N equal parts. The thresholds are the values that divide the dataset into these N parts.\nFor example, we converted values below the first threshold (Q1) to \"Category 1\", between Q1 and\nQ2 to \"Category 2\", and so on, up to values above the last threshold (QN-1), which are converted to\n\"Category N\". A specific example sentence of Categorical Change is shown in figure 2."}, {"title": "The Advanced Order and Indicator", "content": "As shown in 3.2, for a single instance of data, different prompts are generated depending on the order\nof the variables. The same problem occurs in the OOV task, where the number of variables increases\ndue to the addition of OOVs, resulting in more variability in the prompts. This hinders LBC's ability\nto learn the relationships between tokens. Therefore, we find the format that performs best with\noptimal learning and inference among a large number of prompt formats, which can vary depending\non the order of the OOVs and the trained variables that are not OOVs, called In-Variables (IVs). The\nformat of the training and test prompts with both methods is as follows.\nTraining Prompt: IV Indicator + IV part + Question\nTest Prompt: OOV Indicator + OOV part + IV Indicator\n+ IV part + Question\nBy positioning the OOV part at the front of the prompt and matching the variable order of the IV part\nexactly as in training, the IV part in the test prompt has the exact same structure as the IV part in\nthe training prompt. This allows LBC to apply the relationships between variables captured during\ntraining to the test as well. Also, since the indicator is always fixed in the same position, it allows\nLBC to distinguish between the OOV part and the IV part in training and inference. A prompt\nwith both categorical change and advanced order and indicator applied is referred to hereafter as an\nadvanced prompt (AP). An example of an AP can be found in figure 2."}, {"title": "Generalization Ability of LBC: LoRA", "content": "According to Zeng and Lee [27], an arbitrary model fine-tuned with LoRA approximates the target\nmodel. We theoretically prove that, under certain assumptions, LLMs are fine-tuned with LoRA\napproximate arbitrary classifiers. Theorem 1 supports the idea that LBC has a high generalization\nperformance in tabular data classification.\nTheorem 1 Let f(x) represents the ReLU neural network to which LoRA is applied, with no\nactivation function in the last layer, and f(x) represents the target single-layer linear network. Let\ng(x) is the logistic function $(1 + e^{-x})^{-1}$. \u03c3(W); is the i-th greatest singular value of W. Wi\nand W are l-th layer weight matrix of the frozen model and the weight matrix of the target model,\nrespectively. where R\u2081, RE are Rank(W\u2081), Rank(W \u2013 [W\u2081), respectively. L is the number of\nlayers in f."}, {"title": "Verbalizer", "content": "Language generative models were adapted for classification tasks by utilizing verbalizers in the\nloss function. During the training process, using verbalizers encourages the model to generate\nsemantically accurate responses rather than comparing labels precisely at the token level. Since this\napproach does not fit the model to fixed token-level labels, we can expect faster convergence when\ntraining generative language models for classification problems. LBC slightly modifies the structure\nof traditional LLMs in training and inference to use a verbalizer.\nGiven a vector Logit = {lw1, lw2, ..., lwv }, where V is the vocabulary size and lw; is the score\nfor the word w\u2081 in the vocabulary, LBC's score for a single class Ck is calculated as follows:\nScore(Ck) = a1lk + a2 \u03a3 \u0399\u03c9\nWESk\nwhere k is the central word representing class Ck, a1 and 2 are the hyperparameters for the central\nword and synonyms, and Sk is the set of synonyms of central word k. For example, if k = 'Yes', then\nSk = {'yes', 'yeah', 'true'...}. The probability for Ck is computed using a softmax function:\nP(Ck) =$\\frac{exp(Score(Ck))}{\\sum_{k'\\in K}exp(Score(Ck'))}$\nwhere K is the set of central words of all classes. Besides, we modify the existing loss function as\nfollows:\n$J = a\u2081CE(Logit, Lk) + a\u2082 \\sum_{W \\in S_k} CE(Logit, L_w)$"}, {"title": "Experiments", "content": ""}, {"title": "Experiment Settings", "content": "We conducted experiments using reliable datasets that have been frequently used in studies, specifi-\ncally selecting those that have been run multiple times on OpenML [23], Kaggle, or other benchmarks.\nInformation about the eleven datasets is in Table 5. Details on the evaluation methods are in Appendix\nD. As baselines, we selected five models, referred to as TMLs, which are known for their strong\nperformance in tabular data classification. Details of the TMLs are in Appendix C. Additionally, to\nassess the performance improvements brought by LBC's three methodologies, we conducted direct\ncomparisons with LIFT's methodology."}, {"title": "OOV Setting", "content": "To experiment with the performance of LBC on OOV tasks, it is essential to create scenarios where\nvariables that do not exist in training appear in testing. However, we faced a problem because no\nexisting tabular datasets fulfill this requirement. We randomly deleted 50% of the variable columns\nin the original tabular dataset. As a result, variables that are deleted become OOV, not learned by\nthe model during training, and emerge as new variables in the test. This allows for the assessment\nof LBC's ability to interpret OOVs. We compare the performance of TMLs and LBC with the data\ngenerated by this method."}, {"title": "Avoiding Bias", "content": "When fine-tuning LBC, if prompts consistently end with the same token, such as a question mark, the\nmodel may focus more on that token than on the actual variables when predicting class labels. This\nissue is particularly pronounced in datasets with class imbalance. To mitigate this, inserting random\nwords at the end of the sentence helps reduce bias towards specific tokens. An example of the use of\nrandom word is shown in figure 2"}, {"title": "Results", "content": ""}, {"title": "Performance in OOV tasks", "content": "Table 5.3 presents the accuracy, F1, and AUC scores\nof TMLs and LBCs on eight binary classification\ndatasets after conducting 50% OOV conversion. In\nthe average rows for the evaluation metrics, LBC\nconsistently outperforms the five TMLs in binary\nclassification problems. Building on these results, we\nextended our experiments to multiclass classification\ntasks, as shown in Table 5.3. LBCs continue to out-\nperform TMLs, with LBC-Llama3, demonstrating\nstrong performance in multiclass scenarios."}, {"title": "Importance of Advanced Prompt", "content": "In this section, we examine how Ad-\nvanced Prompts, such as \"Consider\nthe order of variables\" or \"Add an in-\ndicator,\" used to generate test prompts,\naffect LBC's probability output.\nTo verify the importance of the\nvariables' order, we experiment with\nrepeatedly generating two types of\nprompts by randomly selecting an in-\nstance from the tabular data: One,\nwhere the order of all variables is ran-\ndomized (LBC-RO), and the other,\nwhere the order of the IVs matches\nto the IV of the training data, and only\nthe order of the OOVs are randomized\n(LBC-AO). We randomly select two\ninstances from the Creditcard dataset\nand generate 100 different prompts for\neach instance with the RO and AO\nmethods, respectively, to compare the probability distributions generated by LBC for the two methods.\nFigure 6 illustrates the performance difference between prompts where the order of variables is\nmatched with the training data and those where it is not. LBC-RO exhibits a large variance in the\nprobability distribution, leading to variations in the model's predictions for a single data instance. In\ncontrast, LBC-AO shows a small variance in the probability distribution, which means that the model\nmakes consistent predictions.\nTo further investigate the benefits\nof matching the order of IVs of test\nprompts with the training prompts, we\ncompose the training and test data us-\ning only the IVs, excluding the OOVs\nselected from the Steel Plate dataset\nused in Table 5.3. Then, for the vari-\nables that make up the test prompt, we\nexperiment with increasing the ratio\nof variables in the same order as the\nvariable order of the training prompt\nto check the scores for the three eval-\nuation metrics. Figure 7 illustrates the\nscores for the three evaluation metrics.\nAs the IVs ratio increases, the perfor-\nmance improves on all three metrics.\nThis shows that LBC performs best\nwhen the test data follows the same\nvariable order as the training data."}, {"title": "LBC- Black-box LLM", "content": "Although it is possible to configure LBC using the latest LLM, most of the latest models are black-box,\nso we conduct in-context learning experiments. The model with the LBC methodology incorporates\nthe categorical change, advanced order, and indicator methodologies. Table 4 compares the Accuracy,\nF1, and AUC scores before and after applying the LBC methodology. We use GPT-3.5 as the\nmodel, and performance improves significantly on all datasets when we add the LBC methodology.\nThis demonstrates that the LBC methodology can also be applied to black-box LLMs to improve"}, {"title": "Conclusion", "content": "In this work, we propose LBC to solve OOV\ntasks. LBCs utilize prompt-based inference,\nwhich allows information about OOVs to be\nadded to prompts in a straightforward way and\nenables understanding of the new information\nthrough pre-trained knowledge. LBC's three\nmethodologies maximize the above advantages\nto achieve high performance on OOV tasks. As\na result, utilizing LLMs' pre-trained knowledge\nis a key strategy for solving the OOV task, and\nwe plan to combine it with various statistical\nmethods. LBC is the first approach to apply\npre-trained LLM to OOV tasks."}, {"title": "Limitations", "content": "Based on our three methodologies, LBC demonstrates superior performance over TML in addressing\nthe OOV generalization problem, leveraging pre-trained knowledge and the contextual understanding\ncapabilities of LLMs. However, several limitations still exist. The first limitation is the potential\npresence of data that requires knowledge not covered in pre-training. When column names are\nunintelligible or involve extremely recent information not included in pre-training, LBC faces\ndifficulties in interpretation. The second limitation is that LBC requires more resources compared\nto TML. In terms of training time and GPU specifications, LBC demands higher costs than TML.\nTherefore, in cases where the information content of OOV is low or when the problem does not\ninvolve OOV, LBC is less suitable compared to TML."}, {"title": "Proof of Lemma 1", "content": "A function f: R \u2192 R is Lipschitz continuous if\n$\\exists K > 0, \\forall x_1, x_2 \\in R, |f(x_1) - f(x_2)| \\le K|x_1 - X_2|$.\nBy substituting f with g, and considering that g is a monotonic function, we can obtain the\nfollowing expression:\n$\\frac{g(x_1) - g(x_2)}{x_1 - x_2} \\le K$.\nBy the mean value theorem,\n$g'(c) = \\frac{g(x_2) - g(x_1)}{x_2-x_1} \\le K$, and\n$0 < g'(c) \\le \\frac{1}{4}$\n$g'(c) = g(c)(1 \u2013 g(c)) ^ 0 < g(c) < 1$\n$K \\ge \\frac{1}{4}$"}, {"title": "Proof of Theorem 1", "content": "Let f(x) represents the ReLU neural network to which LoRA is applied, with no\nactivation function in the last layer, and f(x) represents the target single-layer linear network. Let\ng(x) is the logistic function (1 + e\u00afx)\u22121. \u03c3(W); is the i-th greatest singular value of W. Wi\nand W are l-th layer weight matrix of the frozen model and the weight matrix of the target model,\nrespectively.\n$E ||g(f(x)) - g(\\bar{f}(x))||^2$\n$\\le E||(f(x) - \\bar{f}(x))||^2$\n$\\le \\frac{1}{16}||E(xx^T)||_F o^2 (W - [IW_l])_{min(\\sum_{i=1}^{i=l} R_l, R_E)+1}$,\n(g is 1/4 Lipschitz by Lemma 1)\nwhere R1, RE are Rank(W\u2081), Rank(W \u2013 [W\u2081), respectively. L is the number of layers in f."}, {"title": "Traditional Machine Learning Models", "content": "For Traditional Machine Learning Models, we selected 5 models. For tree-based models, we chose\nDecision Tree and XGBoost. Tree-based models have strong performance in tabular data classification.\nWe also included K-Nearest Neighbor, Logistic Regression, and Support Vector Machine to increase\nthe diversity of the models.\nDecision Tree. A Decision Tree (DT) is a model used for classification and regression tasks. The\nmodel trains on data to make predictions based on simple decision rules. The advantage of decision\ntrees is that they capture non-linear patterns in data, and the results of the model are easy to interpret.\nK-Nearest Neighbor The K-Nearest Neighbor (KNN) algorithm is used in classification and\nregression to make predictions based on the data of the K closest neighbors.\nLogistic Regression Logistic regression (LogReg) is a model often used for classification prob-\nlems. This model is often used when the outcome is binary, and it estimates probabilities to perform\nclassification based on decision boundaries.\nSupport Vector Machine A Support Vector Machine (SVM) is a machine learning model used\nfor classification and regression problems. The model finds the optimal decision boundary to divide a\ngiven set of data into categories, and in this study, we used a Radial Basis Function (RBF) kernel.\nXGBoost XGBoost is a high-performance machine learning model based on the Gradient Boost-\ning algorithm, which is a decision tree-based ensemble learning method that combines multiple tree\nmodels. At each step, XGBoost adds a new model to reduce the error of the previous model and uses\nthe Gradient Boosting technique in the process. XGBoost is a model that is state-of-the-art on many\nbenchmarks.\nAll 5 models were imported and used from scikit-learn. We also used scikit-learn's HalvingGrid-\nSearchCV class to explore the optimal hyperparameters."}, {"title": "Evaluation Methods", "content": "Accuracy measures the proportion of correct predictions and is defined as $Accuracy = \\frac{n_{correct}}{n_{samples}}$\nHere, ncorrect is the number of correct predictions, and nsamples is the total number of samples. F1\nscore, a harmonic mean of Precision and Recall, is calculated as F1 score =$\\frac{2 \\times \\text{Precision} \\times \\text{Recall}}{\\text{Precision}+\\text{Recall}}$,where\nPrecision = $\\frac{TP}{TP+FP}$ and Recall = $\\frac{TP}{TP+FN}$\nAUC score represents the area under the ROC curve, which plots the True Positive Rate (TPR)\nagainst the False Positive Rate (FPR) at various threshold settings."}, {"title": "Selecting Pre-trained LLM", "content": "Our research focuses not merely on prompt tuning using LLMs but on modifying the structure itself\nto construct a model that demonstrates high performance in classification. Specifically, one of our"}]}