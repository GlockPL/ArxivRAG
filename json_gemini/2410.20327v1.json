{"title": "R-LLaVA: Improving Med-VQA Understanding through Visual Region of Interest", "authors": ["Xupeng Chen", "Zhixin Lai", "Kangrui Ruan", "Shichu Chen", "Jiaxiang Liu", "Zuozhu Liu"], "abstract": "Artificial intelligence has made significant strides in medical visual question answering (Med-VQA), yet prevalent studies often interpret images holistically, overlooking the visual regions of interest that may contain crucial information, potentially aligning with a doctor's prior knowledge that can be incorporated with minimal annotations (e.g., bounding boxes). To address this gap, this paper introduces R-LLaVA, designed to enhance biomedical VQA understanding by integrating simple medical annotations as prior knowledge directly into the image space through CLIP. These annotated visual regions of interest are then fed into the LLaVA model during training, aiming to enrich the model's understanding of biomedical queries. Experimental evaluation on four standard Med-VQA datasets demonstrates R-LLaVA's superiority over existing state-of-the-art (SoTA) methods. Additionally, to verify the model's capability in visual comprehension, a novel multiple-choice medical visual understanding dataset is introduced, confirming the positive impact of focusing on visual regions of interest in advancing biomedical VQA understanding.", "sections": [{"title": "1 Introduction", "content": "Medical Visual Question Answering (Med-VQA) has recently garnered significant attention (Chen et al., 2022b; Gong et al., 2021; Ren and Zhou, 2020; Khare et al., 2021). As an emerging area in medical AI, Med-VQA aims to answer medical questions in natural language based on input medical images. A robust Med-VQA system can assist clinicians in interpreting medical images, thereby ensuring accuracy and expediting the diagnostic process. For patients, automated Med-VQA services can greatly meet the demand for personalized healthcare consultations (Liu et al., 2023a).\nNumerous deep learning-based approaches have been explored in the realm of Med-VQA (Tiong et al., 2022; Banerjee et al., 2021; Changpinyo et al., 2022; Liu et al., 2023b; Gai et al., 2024). For instance, Nguyen et al. (2019) utilized Bilinear Attention Networks (BAN) (Kim et al., 2018), enhancing them with Mixed Enhanced Visual Features (MEVF), which integrates pre-trained meta-learning modules and Convolutional Denoising Autoencoders (CDAE) to improve the performance of Med-VQA models. Building on this, Zhan et al. (2020) proposed a conditional reasoning framework to further enhance the inference capabilities of Med-VQA models. However, many of these methods tend to underperform in practical scenarios, primarily due to limitations in extracting and integrating information from a limited number of medical images and text data (Eslami et al., 2021; Song et al., 2022; Wang et al., 2022). To address this, Eslami et al. (2021) introduced the CLIP architecture into the MEVF framework (Nguyen et al., 2019), using CLIP as the visual encoder pre-trained on the multimodal medical dataset ROCO (Pelka et al., 2018), which demonstrated significant per-"}, {"title": "2 Method", "content": "R-LLaVA is based on the premise that visual LLMs should analyze not only the visual content of the images themselves but also the specific regions of interest highlighted by clinicians. In this section, we detail our approach, starting with the reconstruction of medical VQA datasets to incorporate region-based information, simulating how clinicians annotate critical regions of interest. Following this strategy of dataset reconstruction, we explain the process utilized to train R-LLaVA based on these annotations."}, {"title": "2.1 Medical Dataset Reconstruction with Region-of-Interest (RoI) VQA", "content": "In Med-VQA, utilizing datasets incorporating region-based information is helpful for enabling models to accurately focus on and interpret specific regions of medical images. This targeted focus ensures that model responses are grounded in the precise anatomical or pathological features of interest, leading to more accurate and clinically meaningful answers in complex medical scenarios. To evaluate and enhance the region-learning capabilities of Vision-and-Large-Language Models(VLLMs) in medical VQA tasks, we propose a strategy to reconstruct the existing medical VQA datasets by integrating RoI VQA pairs. These pairs are designed to improve and evaluate the model's ability to localize and describe specific regions within medical images.\nAs is shown in Fig. 2, the reconstructed dataset comprises four types of QA pairs: (1) Region localization - where the model is required to predict the bounding box coordinates corresponding to a described region, e.g., \"Please provide the bounding box coordinate of the region this sentence describes: Heart\". (2) Region selection - where the model is required to select the bounding box among four corresponding to a described region, e.g., \"Select the bounding box (bbox) describes spleen. A. Yellow B. Purple C. Green D. Red\". (3) Region description with bounding box coordinates - where the model is asked to provide a description given a bounding box, e.g., \"Please provide a short description for this region: [115, 163, 243, 268]\". (4) Region description with Bounding Box highlighted in input image - where the model is asked to describe the bounding box, e.g., \"Please provide a short description inside the bounding box\".\nThis reconstruction of Med-VQA datasets with RoI annotations aims to push the boundaries of current VLLMs, providing a more rigorous evaluation framework for their region-learning capabilities in medical imaging tasks."}, {"title": "2.2 R-LLaVA Training Stage I: Pretraining", "content": "The first stage of training is to align the biomedical concepts while maintaining efficiency. As shown in Fig. 3(a), the image encoder (e.g., CLIP (Rad-"}, {"title": "2.3 R-LLaVA Training Stage II: Instruction Tuning with Visual RoI Approach", "content": "In Stage II, the model is aimed to learn to follow various types of textual instructions across a wide range of specific medical fields and complete field-specific tasks. The model is also designed to focus on the Region-of-Interest (RoI) (e.g., bounding boxes) to enhance its vision ability for Med-VQA. As illustrated in Fig. 3(b), during stage II, the visual encoder is frozen while the pre-trained weights of the projection layer and the language model are updated. To enhance the model's capacity for instruction-following and task execution in a biomedical conversational context, we further fine-tune the model on biomedical language-image instruction-following data, improving its ability to handle task transfer across established Med-VQA datasets.\nFor certain biomedical applications, it is essential to create highly accurate, dataset-specific models to meet the required performance standards. After completing the pretraining based on LLaVA-Med, we trained on four distinct Med-VQA datasets, each covering a range of dataset sizes and specialized medical topics. Given a biomedical image as input, the model is tasked with answering multiple natural language questions, generating responses in free-form text for both close-set and open-set question types.\nTo strengthen the model's ability to interpret arbitrary visual regions of interest, we utilize CLIP's built-in capacity to encode both the image and supplementary human-annotated visual markers. This provides enhanced guidance by merging potential doctor annotations into the image via alpha blending, drawing focus to regions of interest:\n$X_{merged} = \\alpha \\cdot P_{doc} + (1 - \\alpha) \\cdot X_{img}$\nwhere $\\alpha$ controls the transparency level, $X_{img}$ is the base image, and $P_{doc}$ represents the medical regions of interest. The composite image $X_{merged}$ is then fed into the multimodal language model to guide its attention toward these key areas.\nTo capture both detailed and abstract information, we extract multi-level features from several layers of CLIP. The shallower layer is used for capturing finer details, while deeper layers capture more abstract semantic representations (Ruan et al., 2024). These features are combined, normalized using LayerNorm for stability, and then processed by an MLP layer to integrate the diverse visual cues effectively.\nThis direct integration of visual prompts over regions of interest brings several benefits. It reduces model complexity by eliminating additional processing components and mirrors natural doctor-patient interactions, making it suitable for real-world applications.\nWe employ autoregressive language modeling to train the model, optimizing the likelihood of"}, {"title": "3 Experiment", "content": "In this section, we first provide a detailed description of the utilized dataset (Section 3.1), including the types of questions it encompasses. Next, we present the selected evaluation metrics (Section 3.2). Based on the chosen datasets and metrics, we outline the training strategies and settings employed in our experiments (Section 3.3). We present the main results in Section 3.4, along with a comprehensive ablation study analyzing the impact of our methodological choices (Section 3.5). Additional qualitative examples are provided in the Appendix for further reference."}, {"title": "3.1 Dataset", "content": "For pretraining, we utilize a large-scale medical image-caption dataset from (Li et al., 2024), containing 600K pairs to facilitate foundational alignment with medical concepts. For Fine-tuning, we employ four VQA datasets: VQA-RAD, SLAKE-EN, PathVQA, and VQA-Med. We augment the SLAKE-EN dataset with RoI QA, as detailed in Section 2.1.\nWe define three question types: close-ended, multi-choice, and open-ended. Close-ended questions yield straightforward answers, typically \"Yes\" or \"No\" (e.g., \"Is there any abnormality in the spleen?\"). Multi-choice questions provide a set of predefined answers (e.g., \"Which rectangle contains the object representing Cardiomegaly?\"). Open-ended questions solicit descriptive responses (e.g. \"Please provide a short description for this region\")."}, {"title": "3.2 Metrics", "content": "For close-ended and multi-choice questions, we use accuracy as the evaluation metric, assessing it by directly comparing predictions with ground truth. For open-ended questions, we measure recall, calculating the proportion of ground-truth tokens present in the generated sequences (Chen et al., 2024; Jiang et al., 2024)."}, {"title": "3.3 Training Strategy and Setting", "content": "We use CLIP-ViT-L/14@336px (Radford et al., 2021) as the vision encoder to extract relevant features from input medical images, and Vicuna 1.5 (Chiang et al., 2023) as the language model. A 2-layer MLP serves as the multi-modal projector.\nStage I: Pretraining. The 2-layer multimodal projector is pre-trained on large-scale medical image-caption pairs, developing a foundational understanding of visual data without instruction-following abilities. The training uses a batch size of 256, a learning rate of $1 \\times 10^{-3}$, and a maximum sequence length of 2048 tokens for 1 epoch without weight decay.\nStage II: Instruction Fine-Tuning. With the CLIP encoder fixed, we fine-tune the remaining model"}, {"title": "3.4 Main Results", "content": "Qualitative Comparison The experimental results in Table 1 demonstrate that our proposed model, R-LLaVA(7B), sets a new state-of-the-art on several medical visual question answering (VQA) benchmarks. Specifically, R-LLaVA(7B) achieves the best performance on SLAKE-EN, with an accuracy of 89.47% on open-ended questions and 90.13% on close-ended questions, outperforming all competing models by a significant margin. This highlights the effectiveness of our region of interest approach. Furthermore, R-LLaVA(7B) also delivers superior results on VQA-Med 2019, achieving an accuracy of 80.97% on close-ended questions. For the VQA-RAD and PathVQA datasets, our model consistently achieves superiority on both open-ended and closed-ended tasks, reinforcing its robustness across diverse medical VQA challenges.\nQualitative Comparison From Fig. 4, R-LLaVA demonstrates superior performance over LLaVA-Med across all question types. Specifically, for close-ended questions, such as identifying abnormalities in specific organs (e.g., spleen), R-LLaVA provides more accurate responses. For open-ended questions that require descriptive answers, R-LLaVA accurately describes specific regions with detailed terms closely aligned to medical findings. For example, it correctly identifies a \"Brain Enhancing Tumor,\" demonstrating a deeper understanding of the medical context, whereas LLaVA-Med misclassifies it as \"Brain Edema,\" highlighting a gap in spatial and contextual comprehension. In multi-choice scenarios, R-LLaVA consistently selects the correct bounding box corresponding to medical conditions like cardiomegaly. Its precision in"}, {"title": "3.5 Ablation Study", "content": "In this section, we evaluate the effectiveness of each component in the model training process, covering model selection, two-stage training strategy, and the visual Region-of-Interest (RoI) approach."}, {"title": "3.5.1 Model Selection", "content": "We first conducted an ablation study on model configurations, focusing on whether the initial parameters were loaded from Vicuna (Zheng et al., 2023) or ViP-LLaVA (Cai et al., 2024). Additionally, we compared two fine-tuning strategies: single and all. In the single strategy, the model is trained and evaluated independently on each dataset. In contrast, the all strategy involves training the model on all datasets collectively, enabling it to generalize and better understand medical prompts across different datasets. Furthermore, we evaluated two model sizes: 7B and 13B.\nAs shown in Table 2, the 7B model, initialized with parameters from ViP-LLaVA and fine-tuned on data from all four datasets, achieves the best performance."}, {"title": "3.5.2 Two-stage Training Strategy", "content": "We demonstrate the importance of both training stages: pretraining on the LLaVA-Med dataset and fine-tuning across four VQA datasets.\nFrom Table 3, we observe that without pretraining and fine-tuning (row 1), the model achieves very low accuracy across all four datasets, with open-ended question accuracy below 30% on the first three datasets and only 8.74% on PathVQA. With pretraining only (row 2) or fine-tuning only (row 3), the performance improves. Fine-tuning alone generally yields better results across all datasets compared to pretraining alone; for instance, it achieves an accuracy of 70.27% on SLAKE-EN open-ended questions versus 40.05% for pretraining only and 26.82% without any training. However, pretraining only slightly reduces accuracy on close-ended questions (57.81% for VQA-RAD and 45.22% for SLAKE-EN) compared to the baseline without training (59.19% for VQA-RAD and 50.24% for SLAKE-EN). With both training stages (row 4), our model achieves optimal results, showing double-digit improvements across various question types and datasets compared to other experiments.\nOur findings highlight the importance of performing both pretraining and fine-tuning. Fine-tuning enables the model to acquire task-specific abilities, while pretraining provides essential background knowledge on domain-specific topics, which significantly enhances the performance com-"}, {"title": "3.5.3 Visual Region-of-Interest (RoI) Approach", "content": "We demonstrate the effectiveness of the visual Rol approach through both quantitative results in Table 4 and qualitative results in Fig. 4. In Table 4, \"Bbox in Prompt\" means adding bounding box information in the prompt, and \"alpha\" is the weight of alpha blending.\nVisual Region-of-Interest (RoI) Approach From Table 4, without bounding box information in both prompt and input image (Row 1), we observe an accuracy of 80.39% on open-ended questions, 81.32% on close-ended questions, and 31.06% on multi-choice questions for the SLAKE-EN dataset. The lack of bounding box information in both the prompt and input image results in lower performance across all datasets, particularly in the multi-choice questions, than in the experiments with Visual Rol (Row 3, 4, 5, 6). Adding bounding box information in the prompt while keeping alpha at 0 based on Row 1, Row 2 slightly improves performance across open-ended questions (83.41%) and close-ended questions (81.86%) in SLAKE-EN, with a minor boost in multi-choice questions (25.22%). This indicates that bounding box prompts contribute positively but are insufficient alone.\nThese findings demonstrate that the visual Rol approach, which incorporates bounding boxes in both the prompt and input image, is critical for enhancing model performance on VQA datasets, particularly in the multi-choice categories of SLAKE-EN.\nAlpha Blending Strategy This experiment analyzes the impact of varying bounding box opacity (alpha) on VQA performance across multiple datasets. Higher alpha values represent a more visible bounding box, while lower values signify a subtler presence. Alpha values include fixed lev-"}, {"title": "4 Related Work", "content": "Integrating visual understanding into large language models has significantly advanced multimodal AI capabilities. Early models like ViLBERT (Lu et al., 2019) and VisualBERT (Li et al., 2019) extended the BERT architecture to process both text and images, enabling tasks such as visual ques-tion answering and image captioning. Contrastive learning frameworks like CLIP (Radford et al., 2021) learned joint representations of images and text from large-scale datasets, achieving impressive zero-shot performance on various tasks. Recent developments focus on a more seamless integration of vision and language. Models like Flamingo (Alayrac et al., 2022) and PaLI (Chen et al., 2022a) incorporate visual information into language models using gated cross-attention and scale-up pre-training with multilingual data. Furthermore, the enhanced multimodal features of ChatGPT and Gemini (Team et al., 2023) model represent significant strides toward more integrated and capable AI systems."}, {"title": "4.2 Med-VQA", "content": "Med-VQA is a method where a model answers questions from patients or clinicians based on medical images like CT scans, MRI scans, or pathology images. With the advancement of deep learning, researchers have proposed numerous Med-VQA methods. M2I2 (Li et al., 2022b) is a self-supervised vision-language pretraining method that significantly improves medical VQA performance by learning multimodal representations through masked modeling and contrastive learning. However, its ability to predict free-form answers may be influenced by the complexity of the dataset. BiomedCLIP (Zhang et al., 2024) is a multimodal biomedical foundation model trained on PMC-15M. It outperforms earlier vision-language models like PubMedCLIP (Eslami et al., 2023b) and Med-CLIP, as well as radiology-specific models such as BioViL (Boecking et al., 2022). Bazi (Bazi et al., 2023a) proposes a vision-language model based on a Transformer encoder-decoder architecture. It leverages the CLIP model for semantic embedding and uses a generative decoder to produce answers in an autoregressive manner. Recently, inspired by LLaVA (Liu et al., 2024), LLaVA-Med (Li et al., 2024) fine-tunes LLaVA by self-generated biomedical instruction-following dataset to address challenges in medical image interpretation. However, the explicit handling of region-specific information in complex medical scenarios remains insufficiently explored."}, {"title": "5 Conclusion", "content": "In this paper, we introduce R-LLaVA, an effective method to enhance Med-VQA understanding by leveraging Visual Regions of Interest. This approach integrates simple physician annotations, such as bounding boxes, as prior knowledge, directly infusing these visual cues into the image space via CLIP. During training, these annotated Visual Regions of Interest are fed into the LLaVA model to boost Biomedical VQA Understanding. A specially constructed multiple-choice dataset demonstrates the positive impact of Visual Regions of Interest within R-LLaVA. Experimental results on four Med-VQA datasets show that R-LLaVA outperforms existing SoTA techniques, significantly surpassing recent methods."}, {"title": "Limitation", "content": "Based on our experiments, we demonstrate that even minimal annotations provided by doctors can significantly enhance the accuracy of R-LLaVA. While doctor-specified regions of interest are not required during inference\u2014R-LLaVA is capable of processing both annotated and non-annotated cases\u2014some degree of annotation is necessary during the training phase to achieve optimal performance. Additionally, we conducted a series of ablation studies examining model configurations, training strategies, and the inclusion of visual regions of interest. These studies consistently showed that the proposed method outperforms baseline approaches, particularly in clinical scenarios where visual region annotations are utilized."}, {"title": "Potential Risks and Broader Impacts", "content": "The proposed method, which builds on LLaVA (Liu et al., 2024; Cai et al., 2024), inherits several inherent challenges associated with VLLMs, such as hallucination and biases. Regarding hallucination, as demonstrated in Table 1 and Fig. 4, R-LLaVA makes more effective use of the visual regions of interest, leading to generally superior performance. Nevertheless, it may still produce responses that are not grounded in factual information or the input data. We consider this work a significant step toward enhancing biomedical VQA by utilizing doctor guidance. To address potential biases (Ruan et al., 2023), particularly those that could favor or disfavor specific demographics, we test R-LLaVA on cases where annotations were unavailable due to privacy constraints. Additionally, integrating improved vision or language encoders could further alleviate these biases. Given the high computational cost and energy demand of training LLMs in general, we leverage pre-trained image and language encoders. This approach is especially appropriate given the relatively small size of biomedical datasets, eliminating the need for training from scratch.\nIn particular, R-LLaVA could democratize access to expert-level biomedical information, offering non-specialists\u2014such as primary care physicians, medical trainees, and even patients themselves\u2014an enhanced tool for understanding complex medical data. This capability is crucial in low-resource settings where access to specialized medical expertise is limited, thus reducing healthcare disparities on a global scale. Furthermore, by enabling more accurate and contextually grounded responses, the model could assist clinicians in making more informed, evidence-based decisions, potentially improving patient outcomes and reducing diagnostic errors.\nIn summary, the proposed method not only advances the technical status of biomedical VQA but also holds promise for generating significant social benefits, particularly by improving healthcare accessibility, reducing disparities, and promoting AI development. These contributions align with the growing emphasis on the ethical and socially responsible deployment of AI in sensitive domains such as healthcare."}, {"title": "A Qualitative Examples", "content": ""}]}