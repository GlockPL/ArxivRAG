{"title": "Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges", "authors": ["Jonggi Hong", "Hernisa Kacorri"], "abstract": "Object recognition technologies hold the potential to support blind\nand low-vision people in navigating the world around them. How-\never, the gap between benchmark performances and practical us-\nability remains a significant challenge. This paper presents a study\naimed at understanding blind users' interaction with object recog-\nnition systems for identifying and avoiding errors. Leveraging a\npre-existing object recognition system, URCam, fine-tuned for our\nexperiment, we conducted a user study involving 12 blind and low-\nvision participants. Through in-depth interviews and hands-on er-\nror identification tasks, we gained insights into users' experiences,\nchallenges, and strategies for identifying errors in camera-based\nassistive technologies and object recognition systems. During inter-\nviews, many participants preferred independent error review, while\nexpressing apprehension toward misrecognitions. In the error iden-\ntification task, participants varied viewpoints, backgrounds, and\nobject sizes in their images to avoid and overcome errors. Even after\nrepeating the task, participants identified only half of the errors,\nand the proportion of errors identified did not significantly differ\nfrom their first attempts. Based on these insights, we offer impli-\ncations for designing accessible interfaces tailored to the needs of\nblind and low-vision users in identifying object recognition errors.", "sections": [{"title": "1 INTRODUCTION", "content": "The field of computer vision has made significant strides, achieving\nconsiderable benchmarking rates in object recognition tasks. Yet, de-\nspite these advancements, real-world applications often encounter\nsubstantial discrepancies between expected and observed perfor-\nmance [21, 62]. Factors such as complex tasks, resource limitations\n(e.g., mobile device processing), and inputs that deviate from the\ntraining data (e.g., classifying images with personal items or clut-\ntered backgrounds collected by a user) pose persistent challenges,\nleading to higher-than-anticipated error rates in practical scenar-\nios [9]. Moreover, the vulnerability of object recognition systems\nto adversarial attacks further compounds these challenges [31, 47].\nWhile image classifiers hold potential for supporting the blind com-\nmunity in day to day tasks, they are hindered by their inability to\neffectively convey recognition errors, especially when tactile or\nolfactory verification is impractical (e.g., distant objects or scenes).\nThus, despite advancements, the gap between benchmark perfor-\nmance and real-world usability remains a critical concern for assis-\ntive object recognition systems.\nIn this work, we explore the challenges that blind users face\nwhen handling object recognition errors and the strategies they use\nto overcome them. Specifically, we conduct a user study with 12\nblind and low-vision participants, using a two-pronged approach: a\nsemi-structured remote interview and a hands-on error identifica-\ntion task in participants' homes. In the interview, we aim to answer\nthe following research question: \"What are the experiences of blind\nand low-vision users with error handling in camera-based assistive\ntechnologies?\" Participants describe how often they verify recogni-\ntion results, the frequency with which they encounter errors, the\nimportance they place on these errors, and the challenges they face\nin identifying them. To better contextualize their responses, we\ndiscuss their confidence in photo composition, the frequency of\nuse, and the purposes for each of their camera-based assistive tech-\nnologies. The interview is then followed by the experiment with\nan error identification task, where we aim to answer the following\nresearch questions: \"How do blind and low-vision users identify and\nrespond to object recognition errors, and what are the relationships\nbetween recognition error types, decision-making time, confidence\nlevels, and task repetition?\" Participants interact twice with URCam,\nan object recognition iOS app that we developed for this experi-\nment. We fine-tuned the underlying model to recognize 15 object\nstimuli relevant to our study. The app provides object labels or a\n'Don't know' response when the recognition confidence is low. To\nbetter contextualize the results, we report the accuracy of URCam\nduring the task and manually code the strategies participants use\nfor capturing photos when URCam responds with 'Don't know."}, {"title": "2 RELATED WORK", "content": "Object recognition, encompassing both object detection and classi-\nfication [12, 73], has been the subject of active research for decades,\nrepresenting fundamental and inherently challenging problems\nwithin computer vision. Object detection specifically seeks to as-\ncertain the precise location and dimensions of objects within an\nimage, often represented through bounding boxes [91, 92]. On the\nother hand, image classification aims to determine whether certain\nobjects, belonging to predefined classes, are present within an im-\nage or not [48, 57]. Both object detection and image classification\nfind application in a diverse array of fields, including accessibility.\nJust within the context of technologies for blind and low vision\npeople, the focus of this paper, there are a myriad of publications.\nIn a recent review by Gamage et al., the breakdown highlights the\nvarious assistive tasks where this technology is being utilized, cov-\nering a wide range of contexts from handling object and devices,\norientation and mobility, communication and information, personal\ncare and protection, cultural and sports activities, to personal medi-\ncal treatment [26]. Given the inherently error-prone nature of this\ntechnology, understanding and designing for user interactions with\nprediction errors is critical. Below we synthesize prior literature\nthat discuss this in the context of assistive technologies for the\nblind and more broadly."}, {"title": "2.1 Interactions with Errors in AI Technology in\nthe Context of the Blind Community", "content": "Previous research has consistently demonstrated the significant\nimpact of errors on the experiences of blind users. Table 1 illustrates\nprevious research examples concerning the ramifications of errors\nin Al-infused assistive technology. For instance, safety concerns\nregarding malfunctions in autopilot systems of self-driving vehi-\ncles pose a primary apprehension for blind individuals who are\nencouraged to use such vehicles autonomously [18, 19]. Similar\nconcerns arise in systems where error risks are less critical than\nthose in self-driving vehicles but still consequential. For instance,\nstudies have shown that minor errors in navigation systems can\nlead to frustration and disorientation, even when the destination\nis just a few meters away (e.g., [74]). Prior studies also highlighted\nthe need for blind users to distinguish and handle errors when\nunderstanding images with AI-based image descriptions [30, 51].\nThese findings underscore the importance of user-error interaction\ninterfaces that provide contextual information and predictions from\nmachine learning models to help blind users accurately assess error\ncauses and severity.\nSimilarly, errors significantly impact blind users' experiences\nwith object recognition systems, as blind individuals often rely\nsolely on system outputs due to the challenge of verifying them [64,\n75]. Consequently, understanding the implications of errors in AI-\ninfused assistive technology is critical. Research has highlighted\ninstances where such errors have led to adverse outcomes. For\nexample, blind users tend to overtrust automatically generated\ncaptions on social media images, even when the captions are incor-\nrect and nonsensical [59]. While some errors in blind navigation\nsystems are manageable in familiar environments, they become\nproblematic when they can lead to embarrassing situations with by-\nstanders [1, 55]. Moreover, errors in image recognition systems\nused for controlling household objects can pose safety threats."}, {"title": "2.2 Interactions with Errors in Al-infused\nTechnology in a Broader Context", "content": "While errors are easy to tell in some applications where users can\nunderstand the outcome from the system and ground truth easily\n(e.g., navigating familiar routes with a way-finding system), the\noutcome from the system may not be clearly perceived due to the\ncharacteristics of the task, a poorly designed interface, the com-\nplexity of the information, or poor concentration caused by a high\nworkload [45, 46]. For example, the ground truth may not be avail-\nable immediately when the outcome is provided by the system (e.g.,\nmedical diagnosis, weather prediction). The ground truth may not\nbe straightforward to the user if the system handles data in an unfa-\nmiliar work domain [76]. Therefore, many researchers have worked\non developing user interfaces for Al-infused systems aimed at effec-\ntively managing errors and aiding users in navigating discrepancies\nbetween system outputs and desired outcomes. Noteworthy efforts\ninclude strategies to temper user expectations regarding AI system\nperformance [44, 58], alongside the presentation of user-friendly\ninterfaces tailored to address errors arising from diverse Al-infused\napplications.\nGesture recognition technology has found utility in control-\nling an array of devices, from visual displays [50] and robots [63]\nto wearable devices [66, 83], and even in virtual reality interac-\ntions [22, 33, 38]. Despite considerable advancements in gesture\nrecognition accuracy and usability, input recognition errors persist,\nsignificantly detracting from user experiences [49]. Research en-\ndeavors have thus delved into comprehending the ramifications of\nthese errors, uncovering that user tolerance is frequently shaped\nmore by the context of interaction than solely by system perfor-\nmance. Remarkably, users may tolerate recognition error rates of\nup to 40% before opting for alternative interaction modes over ges-\ntures [43]. Moreover, endeavors to alleviate the detrimental impacts\nof gesture recognition errors have encompassed various strategies,\nsuch as real-time error detection and adaptive model adjustments\nbased on discerning whether the erroneous inputs stem from user\nmistakes or recognition errors [77].\nSimilarly, in the context of speech recognition systems, while\nsignificant progress has been made in minimizing errors under\ncontrolled environments, practical challenges such as speaker vari-\nability and ambient noise persist [29, 40, 65]. These errors manifest\nin various forms, including failure to detect speech, misrecognition,\nor incorrect handling of recognized speech [37, 68]. Studies have\nrevealed that users overlook more than half of speech recognition\nerrors in the absence of visual cues [37]. To address this, researchers\nhave explored techniques for automated error detection in speech\nrecognition outputs, ranging from visually highlighting potentially\nerroneous words to employing neural network-based predictive"}, {"title": "3\nMETHODS", "content": "To gain insight into blind people's challenges and strategies in\nhandling errors in AI-infused applications for object recognition,\nwe carry out a comprehensive two-phase user study. The study first\nencompasses a semi-structured interview that captures participants'\nexperience with camera-based assistive tools. The interview is then\nfollowed by an object recognition task, where participants are asked\nto identify errors when interacting with a mobile application in their\nhomes. We adopt this two-pronged approach from a prior study\nby Hong et al. [37] looking at challenges and strategies adopted by\nblind people when reviewing automatic speech recognition errors.\nOur study was approved by the Institutional Review Board at our\nanonymized institution (IRB number anonymized). Participants were\ncompensated at a 15$/hour rate for a total of $26.21 on average\n($2329, SD = 1.73)."}, {"title": "3.1 Participants", "content": "We recruited 12 blind participants (6 women, 6 men, 0 nonbinary)\nfrom campus email lists and local organizations. As shown in Ta-\nble 2, their age ranged from 32 to 70 (M = 54.3, SD = 15.2). Three\nparticipants reported being totally blind, five having some light\nperception, and four being legally blind. P1 and P2 reported \u201can au-\nditory processing disorder\u201d and difficulty hearing \"very high sounds\u201d,\nrespectively. Yet, all participants indicated that they faced no prob-\nlems in using a screen reader. All mentioned using smartphones\nseveral times a day. All participants were right-handed except for\none, who was left-handed (P4). When asked to report their levels of\nfamiliarity with machine learning, two participants reported being\nsomewhat familiar, eight being slightly familiar, and two being not\nfamiliar at all. We used a 4-point scale for this question, where\nnot familiar at all indicated that participants have never heard of\nmachine learning, slightly familiar that they have heard of it but\ndon't know what it does, somewhat familiar that they have a broad\nunderstanding of what it is and what it does, and extremely famil-\niar that they have extensive knowledge on machine learning. All\nquestions are available in Appendix A."}, {"title": "3.2\nProcedure", "content": "The study is conducted over two days that may be up to 7 days apart.\nOn the first day, participants engage in a semi-structured interview\nand answer questions related to demographics, and technology\nexperience. On the second day, they complete a recognition task\nwith an object recognition application engineered by our team that\naims to serve as a testbed. The app is called URCam. During this\nsession, participants interact with URCam and a set of given object\nstimuli. They attempt to identify any recognition errors that the\napp might have made and express their confidence."}, {"title": "3.2.1 Semi-structured interview.", "content": "The interview lasted 51 minutes\non average (18 \u2013 90m, SD = 21.37). It was completed remotely\nover Zoom and recorded for later analysis. Beyond demographics,\nparticipants responded to questions about:\n\u2022 frequency of using a mobile device, taking photos, reviewing\nphotos, and changing settings of the camera;\n\u2022 purpose of taking photos, subjects included, applications and\ndevices used, and confidence on photo composition;\n\u2022 frequency of use of a camera-based assistive application, its\nusefulness, and device;\n\u2022 frequency of verifying the recognition results of a camera-\nbased assistive application, encountering errors, importance\nof errors, and difficulty of identifying the errors;\n\u2022 strategy of taking photos with an assistive application, de-\ngree of understanding how that application works.\nAs shown in Appendix A, questions assessing frequency are cat-\negorized into two groups. The first includes those answerable with\nan absolute 7-point scale, adopted from Rosen et al. [72] (ranging\nfrom 'never' to 'several times a day'). For example, 'How often do\nyou take photos or record a video?\" The second group includes those\nsuited to a relative 6-point scale (from 'never' to 'always') [20] e.g.,\n'How often do you encounter misrecognitions when you use Seeing\nAI?'"}, {"title": "3.2.2 Error identification task.", "content": "Given a set of object stimuli and an\niPhone 8 device with an object recognition app, participants are\nasked to try to identify the objects using the application. When\ndeployed in real-world environments, object recognition errors\nare typically confounded by blurred images, viewpoints with low\ndiscriminative characteristics, cluttered backgrounds, low saliency,"}, {"title": "3.3 Object Stimuli", "content": "For the error identification task, we utilize a fixed set of 15 objects\nacross all participants (Figure 1). We adopt similar stimuli to those\npreviously employed in a study examining the interaction of blind\nusers with a teachable object recognizer by Kacorri et al. [42]. We\nadopt their methodology, which involved the selection of objects\nto encompass a variety of shapes, sizes, materials, and visual simi-\nlarities. While some products, such as baking soda, chicken broth,\ndiced tomatoes, and diet coke, featured logos or images on their con-\ntainers that differed slightly from those used in the prior study due\nto design updates, the fundamental aspects affecting participants'\ntactile perception, such as shape, material, and weight, remained\nconsistent across all objects."}, {"title": "3.4 URCam: An Object Recognition App", "content": "For the error identification task, we build an object recognition\napp, called URCam, that serves as a testbed; the software used as a\nbasis for experimentation. URCam is fine-tuned using the images\nof objects in Figure 1. The base model of the object recognizer is\nInceptionV3 [81], originally trained on the ImageNet dataset [24].\nThe dataset for fine-tuning comprises photos captured by nine blind\nparticipants in a previous study by Lee et al. [53], where they trained\na teachable object recognizer. Their dataset includes 225 images for\neach object, totaling 3375 images. Although other existing datasets\n(e.g., [14]) provide images collected by blind and low-vision people,\nthey did not include fine-grained labels for the specific objects in\nour study. Therefore, we opted for the dataset collected with blind\nparticipants that included those objects. Fine-tuning involves 500\niterations of gradient descent with a learning rate of 0.01. During\nthe identification task, our study participants interact with URCam\non an Apple iPhone 8. As shown in Figure 2, upon pressing the\nScan item button, the app transmits the image to a server via\nHTTP, where the fine-tuned object recognition model generates\npredictions regarding the image's label, subsequently relaying it\nback to the participant's device via voice and visual display.\nTo differentiate between objects within our training set and those\nthe app hasn't encountered previously, we employ a technique\nthat assesses the model's discriminative capacity by measuring\nthe entropy of its confidence scores [88]. Specifically, we establish\na threshold for both the entropy value and the confidence score\nto determine instances where the model should refrain from pro-\nviding a predicted label and instead output \"Don't know\". If the\nentropy value exceeds 2.0 or the confidence score falls below 0.4,\nthe application synthesizes the phrase \"Don't know\" rather than\npresenting a predicted label. With this precautionary measure, the\nmodel strives to abstain from delivering potentially misleading or\ninaccurate predictions when it lacks sufficient confidence in its\ndiscriminatory abilities."}, {"title": "3.5 Data Analysis", "content": "The responses from the semi-structured interview and tasks are\ncaptured via Zoom. We transcribe these responses to enable a com-\nprehensive analysis of the participants' experience and feedback.\nWe also explore how participants handle application uncertainty\n(i.e., \"Don't know\") and deal with potential misrecognitions during\nthe error identification task."}, {"title": "3.5.1 Semi-Structured Interview.", "content": "We use a thematic coding ap-\nproach to find the major themes in the participants' responses [17].\nTo reduce the subjectivity, two researchers cooperate to code the\nresponses. One of the researchers transcribes the responses. With\nthe transcribed data, the two researchers code the responses in-\ndependently and create initial codebooks. They compare the two\ncodebooks and code data to resolve the disagreements through\nconsensus. After resolving the disagreements (a total of 35 out of\n373 answers), they establish a shared codebook and code the data.\nIn the final codebook, the responses of 17 open questions in the\nsemi-structured interview include a total of 153 codes."}, {"title": "3.5.2 Error Identification Task.", "content": "We manually annotate the images\ncaptured by participants and compare these annotations with the"}, {"title": "4 INSIGHTS FROM THE INTERVIEW", "content": "The central themes explored during the interview encompass blind\npeople's experiences with photography or video recording and their\ninteraction with camera-based assistive applications. Our discus-\nsion delve into various aspects, such as how blind people assess the\nquality of their photographs, the motivations behind their photog-\nraphy, and the methods they employ to discern inaccuracies within\ncamera-based assistive apps."}, {"title": "4.1 Capturing and Reviewing Photos", "content": "By delving into participants' experiences with capturing photos\nor videos, our goal is to uncover the degree of integration of these\ntechnologies into the daily routines of blind people. Furthermore,\nthrough an exploration of the techniques participants employed to\nmanipulate camera settings, we aim to uncover insights into their\napproach for capturing photos that would allow them to achieve\ntheir goal be it sharing them with others or completing visual tasks.\nWe find that all participants consistently engage in photography\nactivities, each capturing photos at least once a month, as depicted\nin Figure 3. This aligns with findings from a previous study indi-\ncating that BLV people actively use cameras for daily tasks [39].\nOne of the primary reasons for using a camera was to share images\nor videos via social media or video calls as shown in prior stud-ies [39, 78]. The majority (8 out of 12) report taking photos or videos\nmore frequently than several times a week. One reason for using a\ncamera was to share photos or engage in video calls. For instance,\nP4 explained, \"Video calls, share photos, I take videos of bands as I\nplay songs. I've got a YouTube channel with several hundred videos\nof shows I've gone to.\" Additionally, using assistive technology was\ncited as another reason, as described by P9: \"Sometimes I'll check to\nsee what SeeingAl will say. Just curious to know, what the app will\nsay about. I've used glasses with an app Aira. [...] if you're in like\nWalgreens, you can connect with Aira and they will tell you what's\non the shelf.\" During their photographic endeavors, participants\ntend to maintain consistent camera settings and environmental\nconditions. The majority (8 out of 12) of participants reported never\naltering their camera settings. Among the participants who did\nmake adjustments (4 out of 12), modifications primarily aimed to\noptimize lighting conditions. Specifically, three participants sought\nout locations with ample natural light, while one experimented with\nflash settings. For instance, P7 sought to evade shadows, stating,\n\u201cI'll strategically reposition them to ensure optimal lighting without\nan excess of shadows or other visual distractions.\u201d Additionally, one\nparticipant (P8) explored varying camera angles to enhance their\nphotographic outcomes. P8 expressed a preference for home pho-\ntography due to the favorable lighting conditions and exploring\ncamera angles, explaining, \u201cwhen I'm home, I feel it gives me the\nmaximum amount of light and I get the best pictures. [...] I might\nmove it around a couple of times so that it'll describe it in the most\ndetailed way.\"\nWe also posed questions regarding how often they review their\nphotos, as this practice may influence the quality of their images.\nIn general, participants did not frequently review their photos. The\nmajority (8 out of 12) reported checking their photos several times\na month or less. Most participants reviewed their photos in-\ndependently without sighted help. Participants who identified"}, {"title": "4.2 Handling Image Recognition Errors", "content": "To gain insights into the experiences and preferences regarding\ncamera-based assistive applications, we conducted a comprehen-\nsive inquiry into the apps they regularly utilize. Participants re-\nported using a total of eight camera-based assistive apps, with\ninquiries aimed at elucidating their experiences with each. Across\n20 participant-app pairs, the predominant choice was Seeing AI,"}, {"title": "5 ERROR IDENTIFICATION RESULTS", "content": "We conducted a comprehensive evaluation of participants' experi-\nence with identifying errors within the context of object recognition.\nOur analysis centered on discerning patterns in participants' error-\nhandling behavior throughout the task. Additionally, we examined\nthe influence of repeated object recognition efforts on error han-\ndling by comparing the two attempts. Furthermore, participants'\nfeedback provided valuable insights into their attitudes toward\nerrors encountered in object recognition."}, {"title": "5.1 Identifying Object Recognition Errors", "content": "Across the 30 trials (15 in the first and 15 in the second attempt), the\naverage accuracy of object recognition stood at 0.76 (SD = 0.10)."}, {"title": "5.2 Identifying Errors a Second Time", "content": "In a real-world scenario, participants tend to interact with a recog-nition application and similar objects over a long period and often\nlearn to anticipate failures. In Section 5.1, we present aggregated\nobservations from both attempts. To understand even at a small\nscale the effect of repeated use of the object recognition application\non handling incorrect recognitions, in this section we compared\nthe two attempts in the error identification task. Overall, we find"}, {"title": "5.3 Subjective Feedback", "content": "While participants missed around half of the errors, they gener-ally perceived identifying errors as not challenging, confirming\nthe finding from a prior study that BLV users have mixed feelings\nwith both confidence and concerns regarding identifying errors in\nprivate object detection [89]. When asked about the difficulty, the\nmajority disagreed (N = 5), with some strongly disagreeing (N = 3).\nFor instance, P8, who has low vision, was able to discern correct\nand incorrect predictions based on their vision and the textures\nof the object. Other participants identified errors by comparing"}, {"title": "6 DISCUSSION", "content": "Our user study, exploratory in nature, shows both promising results\nand future research directions for supporting blind users' interac-\ntions with error-prone AI-infused technologies. In this section we\ndiscuss lessons learned and limitations that may affect the general-izability of our findings."}, {"title": "6.1 Implications", "content": "Enable users to leverage their expertise in reviewing errors\nindependently. The findings from the interview have shed light\non an interesting trend: most participants expressed a preference\nfor evaluating the quality of their photographs without the assis-\ntance of sighted individuals or remote sighted aid services, such as\nBe My Eyes or Aira, when using camera-based assistive technolo-\ngies. This preference seems to stem from a fundamental aspect of\nthe utilization of AI-based systems - namely, the desire to carry\nout visual tasks independently when sighted assistance is unavail-able. It further shows the preference of blind and low-vision users\nto utilize their expertise in assistive technology, such as integrat-ing recognition results from multiple AI apps to identify errors.\nThis personalized approach to using assistive technology was high-lighted in a previous study [34]. This observation underscores a\ncrucial need within the blind community: the ability for individu-als to autonomously assess the quality of their photos, taking into\naccount factors such as framing, background clutter, and blurriness.\nAddressing this challenge will likely require innovative approaches,\nparticularly in the realm of computer vision. Developing techniques\nthat can accurately quantify the quality factors of photographs with-out relying on visual cues accessible only to sighted individuals\nholds great promise in this regard. Such techniques could poten-tially leverage advanced algorithms and machine learning models\nto analyze various aspects of a photograph, from composition to\nsharpness, and provide meaningful feedback to blind and low-vision\nusers. While some initial strides have been made in this area, such\nas image descriptors for blind users to assess photos for training"}, {"title": "6.2 Limitations", "content": "Variability between confined study conditions and real-world\nexperience. A notable limitation of our study lies in the potential\ndisparity between error identification under confined conditions\nand real-world usage scenarios. In the user study setting, partici-pants were confined to a specific study setup in their home envi-ronment with limited variables such as lighting, background, and\nframing. Typically, they would place the study materials on a table\nand sit nearby. In a way, they were restricted in their ability to move\naround freely to find optimal positions for capturing photos, which\ncould influence the quality of the image and subsequently impact er-ror identification. Furthermore, a somewhat staged' indoor setting\nmay not fully replicate the diverse conditions encountered in real-world scenarios, such as varying lighting conditions, backgrounds,\nand the presence of outdoor elements. Participants' level of famil-iarity and experience with the application may also differ between a\none-off and real-world usage contexts. While participants received\nguidance and instructions during the user study, their experience\nin using the application in real-world settings may vary, potentially\naffecting their proficiency in error identification. Last, the number\nand types of objects encountered in real-world scenarios may differ\nfrom the stimuli in the study. Real-world scenarios often involve a\nwider variety of objects and contexts, presenting unique challenges\nfor error identification.\nSingle-session limitation and potential longitudinal vari-ability. An inherent limitation of our study is that the error identi-fication task was conducted within a single session at participants' homes. While this approach allowed us to gather valuable data\nin a naturalistic setting, it may not fully capture the evolution of\nparticipants' error identification abilities over time. Indeed, our ob-servations revealed differences between participants' performance\nin the first and second attempts of the error identification task. This\ndiscrepancy suggests that participants' understanding of the object\nrecognizer's performance, the characteristics of objects, and optimal\nphoto-taking techniques may have improved with repeated expo-sure and experience. Consequently, a longitudinal study spanning\nmultiple sessions could provide deeper insights into how partici-pants' error identification abilities evolve over time.\nTherefore, while our study provides valuable initial insights\ninto error identification in a single-session context, future research\nemploying longitudinal methodologies could offer a more compre-hensive understanding of the development and refinement of error\nidentification experience and expertise that blind users build while\ninteracting with their camera-based assistive technologies."}, {"title": "7 CONCLUSION", "content": "We explored the experiences of blind and low-vision people re-garding photo-taking, usage of camera-based assistive systems, and\nerror identification within these systems. Through semi-structured\ninterviews, we uncovered that participants predominantly utilize"}]}