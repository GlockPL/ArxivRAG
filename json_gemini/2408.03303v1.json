{"title": "Understanding How Blind Users Handle Object Recognition Errors: Strategies and Challenges", "authors": ["Jonggi Hong", "Hernisa Kacorri"], "abstract": "Object recognition technologies hold the potential to support blind and low-vision people in navigating the world around them. However, the gap between benchmark performances and practical usability remains a significant challenge. This paper presents a study aimed at understanding blind users' interaction with object recognition systems for identifying and avoiding errors. Leveraging a pre-existing object recognition system, URCam, fine-tuned for our experiment, we conducted a user study involving 12 blind and low-vision participants. Through in-depth interviews and hands-on error identification tasks, we gained insights into users' experiences, challenges, and strategies for identifying errors in camera-based assistive technologies and object recognition systems. During interviews, many participants preferred independent error review, while expressing apprehension toward misrecognitions. In the error identification task, participants varied viewpoints, backgrounds, and object sizes in their images to avoid and overcome errors. Even after repeating the task, participants identified only half of the errors, and the proportion of errors identified did not significantly differ from their first attempts. Based on these insights, we offer implications for designing accessible interfaces tailored to the needs of blind and low-vision users in identifying object recognition errors.", "sections": [{"title": "1 INTRODUCTION", "content": "The field of computer vision has made significant strides, achieving considerable benchmarking rates in object recognition tasks. Yet, despite these advancements, real-world applications often encounter substantial discrepancies between expected and observed performance [21, 62]. Factors such as complex tasks, resource limitations (e.g., mobile device processing), and inputs that deviate from the training data (e.g., classifying images with personal items or cluttered backgrounds collected by a user) pose persistent challenges, leading to higher-than-anticipated error rates in practical scenarios [9]. Moreover, the vulnerability of object recognition systems to adversarial attacks further compounds these challenges [31, 47]. While image classifiers hold potential for supporting the blind community in day to day tasks, they are hindered by their inability to effectively convey recognition errors, especially when tactile or olfactory verification is impractical (e.g., distant objects or scenes). Thus, despite advancements, the gap between benchmark performance and real-world usability remains a critical concern for assistive object recognition systems.\nIn this work, we explore the challenges that blind users face when handling object recognition errors and the strategies they use to overcome them. Specifically, we conduct a user study with 12 blind and low-vision participants, using a two-pronged approach: a semi-structured remote interview and a hands-on error identification task in participants' homes. In the interview, we aim to answer the following research question: \"What are the experiences of blind and low-vision users with error handling in camera-based assistive technologies?\" Participants describe how often they verify recognition results, the frequency with which they encounter errors, the importance they place on these errors, and the challenges they face in identifying them. To better contextualize their responses, we discuss their confidence in photo composition, the frequency of use, and the purposes for each of their camera-based assistive technologies. The interview is then followed by the experiment with an error identification task, where we aim to answer the following research questions: \"How do blind and low-vision users identify and respond to object recognition errors, and what are the relationships between recognition error types, decision-making time, confidence levels, and task repetition?\" Participants interact twice with URCam, an object recognition iOS app that we developed for this experiment. We fine-tuned the underlying model to recognize 15 object stimuli relevant to our study. The app provides object labels or a 'Don't know' response when the recognition confidence is low. To better contextualize the results, we report the accuracy of URCam during the task and manually code the strategies participants use for capturing photos when URCam responds with 'Don't know."}, {"title": "2 RELATED WORK", "content": "Object recognition, encompassing both object detection and classification [12, 73], has been the subject of active research for decades, representing fundamental and inherently challenging problems within computer vision. Object detection specifically seeks to ascertain the precise location and dimensions of objects within an image, often represented through bounding boxes [91, 92]. On the other hand, image classification aims to determine whether certain objects, belonging to predefined classes, are present within an image or not [48, 57]. Both object detection and image classification find application in a diverse array of fields, including accessibility. Just within the context of technologies for blind and low vision people, the focus of this paper, there are a myriad of publications. In a recent review by Gamage et al., the breakdown highlights the various assistive tasks where this technology is being utilized, covering a wide range of contexts from handling object and devices, orientation and mobility, communication and information, personal care and protection, cultural and sports activities, to personal medical treatment [26]. Given the inherently error-prone nature of this technology, understanding and designing for user interactions with prediction errors is critical. Below we synthesize prior literature that discuss this in the context of assistive technologies for the blind and more broadly."}, {"title": "2.1 Interactions with Errors in AI Technology in the Context of the Blind Community", "content": "Previous research has consistently demonstrated the significant impact of errors on the experiences of blind users. For instance, safety concerns regarding malfunctions in autopilot systems of self-driving vehicles pose a primary apprehension for blind individuals who are encouraged to use such vehicles autonomously [18, 19]. Similar concerns arise in systems where error risks are less critical than those in self-driving vehicles but still consequential. For instance, studies have shown that minor errors in navigation systems can lead to frustration and disorientation, even when the destination is just a few meters away (e.g., [74]). Prior studies also highlighted the need for blind users to distinguish and handle errors when understanding images with AI-based image descriptions [30, 51]. These findings underscore the importance of user-error interaction interfaces that provide contextual information and predictions from machine learning models to help blind users accurately assess error causes and severity.\nSimilarly, errors significantly impact blind users' experiences with object recognition systems, as blind individuals often rely solely on system outputs due to the challenge of verifying them [64, 75]. Consequently, understanding the implications of errors in AI-infused assistive technology is critical. Research has highlighted instances where such errors have led to adverse outcomes. For example, blind users tend to overtrust automatically generated captions on social media images, even when the captions are incorrect and nonsensical [59]. While some errors in blind navigation systems are manageable in familiar environments, they become problematic when they can lead to embarrassing situations with bystanders [1, 55]. Moreover, errors in image recognition systems used for controlling household objects can pose safety threats."}, {"title": "2.2 Interactions with Errors in Al-infused Technology in a Broader Context", "content": "While errors are easy to tell in some applications where users can understand the outcome from the system and ground truth easily (e.g., navigating familiar routes with a way-finding system), the outcome from the system may not be clearly perceived due to the characteristics of the task, a poorly designed interface, the complexity of the information, or poor concentration caused by a high workload [45, 46]. For example, the ground truth may not be available immediately when the outcome is provided by the system (e.g., medical diagnosis, weather prediction). The ground truth may not be straightforward to the user if the system handles data in an unfamiliar work domain [76]. Therefore, many researchers have worked on developing user interfaces for Al-infused systems aimed at effectively managing errors and aiding users in navigating discrepancies between system outputs and desired outcomes. Noteworthy efforts include strategies to temper user expectations regarding AI system performance [44, 58], alongside the presentation of user-friendly interfaces tailored to address errors arising from diverse Al-infused applications.\nGesture recognition technology has found utility in controlling an array of devices, from visual displays [50] and robots [63] to wearable devices [66, 83], and even in virtual reality interactions [22, 33, 38]. Despite considerable advancements in gesture recognition accuracy and usability, input recognition errors persist, significantly detracting from user experiences [49]. Research endeavors have thus delved into comprehending the ramifications of these errors, uncovering that user tolerance is frequently shaped more by the context of interaction than solely by system performance. Remarkably, users may tolerate recognition error rates of up to 40% before opting for alternative interaction modes over gestures [43]. Moreover, endeavors to alleviate the detrimental impacts of gesture recognition errors have encompassed various strategies, such as real-time error detection and adaptive model adjustments based on discerning whether the erroneous inputs stem from user mistakes or recognition errors [77].\nSimilarly, in the context of speech recognition systems, while significant progress has been made in minimizing errors under controlled environments, practical challenges such as speaker variability and ambient noise persist [29, 40, 65]. These errors manifest in various forms, including failure to detect speech, misrecognition, or incorrect handling of recognized speech [37, 68]. Studies have revealed that users overlook more than half of speech recognition errors in the absence of visual cues [37]. To address this, researchers have explored techniques for automated error detection in speech recognition outputs, ranging from visually highlighting potentially erroneous words to employing neural network-based predictive models [16, 25, 27, 28, 82]. However, despite advancements, predicting speech recognition errors remains an ongoing research area, with current methods achieving moderate precision and recall rates.\nBeyond gesture and speech, researchers are also endeavoring to mitigate errors in other AI-infused applications, including robotics [56] and autonomous vehicles [85]. In these domains, where safety and reliability are paramount, error-handling mechanisms play a critical role in ensuring smooth operation and user trust [7, 71]. Strategies such as fault tolerance, redundancy, and fail-safe mechanisms are being explored to minimize the impact of errors and safeguard against catastrophic failures [13, 60, 86]. Moreover, advancements in simulation and testing methodologies enable researchers to systematically evaluate robustness and user experience with errors in real-world deployment scenarios [2, 3, 69]. Overall, the quest for error-resilient AI-infused systems represents a multifaceted and interdisciplinary endeavor, requiring collaboration across domains to achieve the vision of intelligent, trustworthy technology."}, {"title": "3 METHODS", "content": "To gain insight into blind people's challenges and strategies in handling errors in AI-infused applications for object recognition, we carry out a comprehensive two-phase user study. The study first encompasses a semi-structured interview that captures participants' experience with camera-based assistive tools. The interview is then followed by an object recognition task, where participants are asked to identify errors when interacting with a mobile application in their homes. We adopt this two-pronged approach from a prior study by Hong et al. [37] looking at challenges and strategies adopted by blind people when reviewing automatic speech recognition errors. Our study was approved by the Institutional Review Board at our anonymized institution (IRB number anonymized). Participants were compensated at a 15$/hour rate for a total of $26.21 on average ($2329, SD = 1.73)."}, {"title": "3.1 Participants", "content": "We recruited 12 blind participants (6 women, 6 men, 0 nonbinary) from campus email lists and local organizations. As shown in , their age ranged from 32 to 70 (M = 54.3, SD = 15.2). Three participants reported being totally blind, five having some light perception, and four being legally blind. P1 and P2 reported \u201can auditory processing disorder\u201d and difficulty hearing \"very high sounds\u201d, respectively. Yet, all participants indicated that they faced no problems in using a screen reader. All mentioned using smartphones several times a day. All participants were right-handed except for one, who was left-handed (P4). When asked to report their levels of familiarity with machine learning, two participants reported being somewhat familiar, eight being slightly familiar, and two being not familiar at all. We used a 4-point scale for this question, where not familiar at all indicated that participants have never heard of machine learning, slightly familiar that they have heard of it but don't know what it does, somewhat familiar that they have a broad understanding of what it is and what it does, and extremely familiar that they have extensive knowledge on machine learning. All questions are available in Appendix A."}, {"title": "3.2 Procedure", "content": "The study is conducted over two days that may be up to 7 days apart. On the first day, participants engage in a semi-structured interview and answer questions related to demographics, and technology experience. On the second day, they complete a recognition task with an object recognition application engineered by our team that aims to serve as a testbed. The app is called URCam. During this session, participants interact with URCam and a set of given object stimuli. They attempt to identify any recognition errors that the app might have made and express their confidence.\n3.2.1 Semi-structured interview. The interview lasted 51 minutes on average (18 \u2013 90m, SD = 21.37). It was completed remotely over Zoom and recorded for later analysis. Beyond demographics, participants responded to questions about:\n\u2022 frequency of using a mobile device, taking photos, reviewing photos, and changing settings of the camera;\n\u2022 purpose of taking photos, subjects included, applications and devices used, and confidence on photo composition;\n\u2022 frequency of use of a camera-based assistive application, its usefulness, and device;\n\u2022 frequency of verifying the recognition results of a camera-based assistive application, encountering errors, importance of errors, and difficulty of identifying the errors;\n\u2022 strategy of taking photos with an assistive application, degree of understanding how that application works.\nAs shown in Appendix A, questions assessing frequency are categorized into two groups. The first includes those answerable with an absolute 7-point scale, adopted from Rosen et al. [72] (ranging from 'never' to 'several times a day'). For example, 'How often do you take photos or record a video?' The second group includes those suited to a relative 6-point scale (from 'never' to 'always') [20] e.g., 'How often do you encounter misrecognitions when you use Seeing AI?'\n3.2.2 Error identification task. Given a set of object stimuli and an iPhone 8 device with an object recognition app, participants are asked to try to identify the objects using the application. When deployed in real-world environments, object recognition errors are typically confounded by blurred images, viewpoints with low discriminative characteristics, cluttered backgrounds, low saliency, and more importantly partially included or out-of-frame objects of interest [14, 23, 54]. Thus, we do not conduct this session in our lab, but move the study to the homes of blind participants. As in Lee et al. [52], all study materials are delivered at home, and instructions are conveyed via Zoom. Each participant received a laptop where the Zoom call is set up for remote communication. Furthermore, participants are provided with Vuzix Blade smart glasses, featuring an integrated camera and initiated with the Zoom call. The smart-glasses can both enable real-time access to participants' first-person perspectives and allow for recordings of observations for subsequent data analysis. At the beginning of the task, the experimenter presents a list of 15 objects for reference (Figure 1). During each trial, participants randomly select an object, capture its image, and obtain a label from the object recognition app, which is communicated via synthesized speech. Upon hearing \u201cDon't know\u201d from the app, indicating that it failed to recognize any object in the photo, participants proceed to capture additional photos until the app provides a label for an object. Subsequently, participants indicate whether the recognition was accurate and express their confidence level in their judgment of correctness for the recognition. After completing the initial 15 trials with all objects (Attempt 1), participants repeat the process with the objects in a randomized order (Attempt 2), totaling 30 trials. Participants are encouraged to think aloud throughout the task. Upon task completion, participants provide feedback on the difficulty level and the strategies they employ for identifying errors."}, {"title": "3.3 Object Stimuli", "content": "For the error identification task, we utilize a fixed set of 15 objects across all participants. We adopt similar stimuli to those previously employed in a study examining the interaction of blind users with a teachable object recognizer by Kacorri et al. [42]. We adopt their methodology, which involved the selection of objects to encompass a variety of shapes, sizes, materials, and visual similarities. While some products, such as baking soda, chicken broth, diced tomatoes, and diet coke, featured logos or images on their containers that differed slightly from those used in the prior study due to design updates, the fundamental aspects affecting participants' tactile perception, such as shape, material, and weight, remained consistent across all objects."}, {"title": "3.4 URCam: An Object Recognition App", "content": "For the error identification task, we build an object recognition app, called URCam, that serves as a testbed; the software used as a basis for experimentation. URCam is fine-tuned using the images of objects in Figure 1. The base model of the object recognizer is InceptionV3 [81], originally trained on the ImageNet dataset [24]. The dataset for fine-tuning comprises photos captured by nine blind participants in a previous study by Lee et al. [53], where they trained a teachable object recognizer. Their dataset includes 225 images for each object, totaling 3375 images. Although other existing datasets (e.g., [14]) provide images collected by blind and low-vision people, they did not include fine-grained labels for the specific objects in our study. Therefore, we opted for the dataset collected with blind participants that included those objects. Fine-tuning involves 500 iterations of gradient descent with a learning rate of 0.01. During the identification task, our study participants interact with URCam on an Apple iPhone 8. As shown in , upon pressing the Scan item button, the app transmits the image to a server via HTTP, where the fine-tuned object recognition model generates predictions regarding the image's label, subsequently relaying it back to the participant's device via voice and visual display.\nTo differentiate between objects within our training set and those the app hasn't encountered previously, we employ a technique that assesses the model's discriminative capacity by measuring the entropy of its confidence scores [88]. Specifically, we establish a threshold for both the entropy value and the confidence score to determine instances where the model should refrain from providing a predicted label and instead output \"Don't know\". If the entropy value exceeds 2.0 or the confidence score falls below 0.4, the application synthesizes the phrase \"Don't know\" rather than presenting a predicted label. With this precautionary measure, the model strives to abstain from delivering potentially misleading or inaccurate predictions when it lacks sufficient confidence in its discriminatory abilities."}, {"title": "3.5 Data Analysis", "content": "The responses from the semi-structured interview and tasks are captured via Zoom. We transcribe these responses to enable a comprehensive analysis of the participants' experience and feedback. We also explore how participants handle application uncertainty (i.e., \"Don't know\") and deal with potential misrecognitions during the error identification task.\n3.5.1 Semi-Structured Interview. We use a thematic coding approach to find the major themes in the participants' responses [17]. To reduce the subjectivity, two researchers cooperate to code the responses. One of the researchers transcribes the responses. With the transcribed data, the two researchers code the responses independently and create initial codebooks. They compare the two codebooks and code data to resolve the disagreements through consensus. After resolving the disagreements (a total of 35 out of 373 answers), they establish a shared codebook and code the data. In the final codebook, the responses of 17 open questions in the semi-structured interview include a total of 153 codes.\n3.5.2 Error Identification Task. We manually annotate the images captured by participants and compare these annotations with the object recognition results recorded by the app to assess the accuracy of object recognition during the task. We categorize the trials based on how well participants identify any object recognition errors by analyzing their responses captured in the video recordings. During this analysis, if participants cannot tell whether the recognition was correct or incorrect, which happened for a total of 7 trials, we interpret this as them perceiving that the recognition can be incorrect but being very uncertain about it. Specifically, we group the trials into:\nTrue positive: The object recognition is correct and the participant perceive it as correct.\nFalse positive: The object recognition is incorrect, but the participant perceive it as correct.\nTrue negative: The object recognition is incorrect and the participant perceive it as incorrect.\nFalse negative: The object recognition is correct, but the participant perceive it as incorrect.\nWe examine the correlation between participants' confidence levels, and trial completion time, along these 4 groups. Trial completion time is manually measured through video analysis, which involves recording the elapsed time from when the app provided the recognition result to when the participant reports its correctness to the experimenter. Additionally, we investigate any adjustments in participants' strategies for capturing photos when they receive a \"Don't know\" response from the URCam. We categorize the adjustments by looking at variation in background, viewpoint, illumination, and object size; a coding scheme adopted by Hong et al. [36]."}, {"title": "4 INSIGHTS FROM THE INTERVIEW", "content": "The central themes explored during the interview encompass blind people's experiences with photography or video recording and their interaction with camera-based assistive applications. Our discussions delve into various aspects, such as how blind people assess the quality of their photographs, the motivations behind their photography, and the methods they employ to discern inaccuracies within camera-based assistive apps."}, {"title": "4.1 Capturing and Reviewing Photos", "content": "By delving into participants' experiences with capturing photos or videos, our goal is to uncover the degree of integration of these technologies into the daily routines of blind people. Furthermore, through an exploration of the techniques participants employed to manipulate camera settings, we aim to uncover insights into their approach for capturing photos that would allow them to achieve their goal be it sharing them with others or completing visual tasks. We find that all participants consistently engage in photography activities, each capturing photos at least once a month, as depicted in . This aligns with findings from a previous study indicating that BLV people actively use cameras for daily tasks [39].\nOne of the primary reasons for using a camera was to share images or videos via social media or video calls as shown in prior studies [39, 78]. The majority (8 out of 12) report taking photos or videos more frequently than several times a week. One reason for using a camera was to share photos or engage in video calls. For instance, P4 explained, \"Video calls, share photos, I take videos of bands as I play songs. I've got a YouTube channel with several hundred videos of shows I've gone to.\" Additionally, using assistive technology was cited as another reason, as described by P9: \"Sometimes I'll check to see what SeeingAl will say. Just curious to know, what the app will say about. I've used glasses with an app Aira. [...] if you're in like Walgreens, you can connect with Aira and they will tell you what's on the shelf.\" During their photographic endeavors, participants tend to maintain consistent camera settings and environmental conditions. The majority (8 out of 12) of participants reported never altering their camera settings. Among the participants who did make adjustments (4 out of 12), modifications primarily aimed to optimize lighting conditions. Specifically, three participants sought out locations with ample natural light, while one experimented with flash settings. For instance, P7 sought to evade shadows, stating, \u201cI'll strategically reposition them to ensure optimal lighting without an excess of shadows or other visual distractions.\u201d Additionally, one participant (P8) explored varying camera angles to enhance their photographic outcomes. P8 expressed a preference for home photography due to the favorable lighting conditions and exploring camera angles, explaining, \u201cwhen I'm home, I feel it gives me the maximum amount of light and I get the best pictures. [...] I might move it around a couple of times so that it'll describe it in the most detailed way.\"\nWe also posed questions regarding how often they review their photos, as this practice may influence the quality of their images. In general, participants did not frequently review their photos. The majority (8 out of 12) reported checking their photos several times a month or less. Most participants reviewed their photos independently without sighted help. Participants who identified as legally blind (N = 4) predominantly relied on their own visual assessment. They utilized automatically generated image descriptions from assistive tools like Seeing AI and iOS's built-in image captioning function (N = 5). For instance, P12 would judge the quality of their photo based on text recognition results, stating, \"What's relevant are the OCR results I get from it. Especially if there is a garbled section that doesn't fall into a normal OCR error pattern, then I know the photo's not good.\" A possible reason for independent reviewing behavior could be concerns about privacy issues when sharing their photos with sighted people [8, 80, 87, 90]. Few (3 out of 12) participants sought assistance from sighted individuals in their vicinity and only one (P1) utilized remote assistance through apps like Aira [6] and BeMyEyes [15].\nTo provide context for understanding the motivations behind participants' photography, we asked questions about the subjects they captured in their photos. Participants cited documents for text recognition (N = 10), people (N = 9), objects (N = 8), food (N = 6), landscapes (N = 5), and miscellaneous items such as a scene and a bill (N = 4). Similarly, the most prevalent purposes for taking photos or recording videos were for text recognition (N = 10), video calls (N = 8), and object recognition (N = 5). These responses diverge somewhat from the findings of a previous study conducted by Jayant et al. [39] in 2011, which suggested that blind individuals primarily took photos to capture friends or family for leisure, while their most sought-after camera function was text recognition. This result indicates the increasing prevalence of computer vision-based assistive applications among blind and low-vision people. However, many participants still found image framing challenging (N = 9), a difficulty highlighted in prior studies [4, 39, 53]. For instance, P1 and P5 expressed concerns such as, \"Making sure the information I'm trying to capture is in the frame of the camera,\" and \"I don't know how far away from the object to hold the phone\", respectively. Participants also identified other difficulties such as maintaining focus on the object (N = 2), stabilizing the camera (N = 2), adjusting lighting conditions (N = 2), and orienting objects correctly (N = 2)."}, {"title": "4.2 Handling Image Recognition Errors", "content": "To gain insights into the experiences and preferences regarding camera-based assistive applications, we conducted a comprehensive inquiry into the apps they regularly utilize. Participants reported using a total of eight camera-based assistive apps, with inquiries aimed at elucidating their experiences with each. Across 20 participant-app pairs, the predominant choice was Seeing AI,"}, {"title": "5 ERROR IDENTIFICATION RESULTS", "content": "We conducted a comprehensive evaluation of participants' experience with identifying errors within the context of object recognition. Our analysis centered on discerning patterns in participants' error-handling behavior throughout the task. Additionally, we examined the influence of repeated object recognition efforts on error handling by comparing the two attempts. Furthermore, participants' feedback provided valuable insights into their attitudes toward errors encountered in object recognition."}, {"title": "5.1 Identifying Object Recognition Errors", "content": "Across the 30 trials (15 in the first and 15 in the second attempt), the average accuracy of object recognition stood at 0.76 (SD = 0.10). Participants encountered 7.33 incorrect recognitions on average (SD = 2.99), experiencing more false positives (M = 3.67, SD = 2.46) than false negatives (M = 0.83, SD = 1.03). When looking at whether participants could distinguish between correct and incorrect recognitions, we find that on average, participants successfully identified 21.83 (SD = 2.82) correct (true positives) and 3.17 (SD = 2.44) incorrect (true negatives) recognition results. However, participants identified errors at a proportion of 0.49 on average (SD = 0.32), indicating that they could detect less than half of the errors. This outcome is consistent with the over-reliance on image recognition results observed in a previous study by MacLeod et al. [59]. The low rate of error identification can be attributed to the challenge of distinguishing objects within the same category that share similar shapes, textures, and weights (e.g., coca-cola and diet coke) when limited visual information is available.\nWhen looking at participants' strategies for recovery from \"Don't know\" predictions, we find that they cluster around varying object size, background, and viewpoint (shown in . This is exciting as none of the participants reported having machine learning expertise. Yet, these patterns underscore participants' awareness of the potential impact of object's size, viewpoint and background on the performance of the object recognition model, drawing from parallels to how humans recognize objects independent of size, viewpoint, location, and illumination [67]. On average, the object recognition app provided a \"Don't know\" response in 8.2 trials (SD = 4.17) out of 30 trials, totaling 216 cases; a \"Don't know\" response would often be followed by subsequent a \"Don't know\u201d responses with an average of 2.01 (SD = 0.89) occurrences. As detailed in , when participants encountered \u201cDon't know,\u201d the most prevalent (116 cases) approach to circumvent it was rotating the object to display its other side, thereby varying the viewpoint in the image, a strategy also prevalent among sighted non-experts in prior work [36]. The second most common approach (29 cases) also involved adjusting the viewpoint, with participants moving the camera instead of the object. Additionally, participants occasionally (23 cases) altered the background of the image by relocating the object to different positions.\nWe examine participants certainty around the error identification task by looking at their responses for each trial where they indicate their confidence in their judgment of the model prediction. Overall, participants expressed varying levels of certainty, reporting being \"very certain,\" \"certain,\u201d \u201cuncertain,\" and \"very uncertain\" across 17.67 (SD = 5.71), 7.83 (SD = 5.77), 2.25 (SD = 2.83), and 1.75 (SD = 2.01) trials, respectively. As shown in , we find that participants reported being either certain or very certain in 90% of true positive trials; trials where the object recognition is correct and the participant perceive it as such. This seems promising. Yet, they also reported being either certain or very certain in 84% of false positive trials; trials where the object recognition is incorrect but the participant perceive it as such. In contrast, participants reported being either certain or very certain in 76% of true negative trials, and 40% of false negative trials. This trend underscores a tendency for heightened certainty when participants perceived recognition outcomes as correct. Overall, these findings indicate a prevalent inclination among participants to place trust in the predictions from the object recognizer.\nThrough analysis of trial completion time, we observe that participants tend to make quicker decisions regarding the correctness of predictions when they were very certain (M = 3.91s, SD = 2.70) compared to when they were just certain (M = 8.48s, SD = 5.28), uncertain (M = 7.87s, SD = 2.71), or very uncertain (M = 7.69s, SD = 8.30). A small correlation was observed between the level of certainty and the trial completion time, as indicated by the Pearson Correlation Coefficient (r = 0.27)."}, {"title": "5.2 Identifying Errors a Second Time", "content": "In a real-world scenario, participants tend to interact with a recognition application and similar objects over a long period and often learn to anticipate failures. In Section 5.1, we present aggregated observations from both attempts. To understand even at a small scale the effect of repeated use of the object recognition application on handling incorrect recognitions, in this section we compared the two attempts in the error identification task. Overall, we find that the proportion of errors identified by the participants was not significantly\u00b9 different across the two with it being at 0.51 on average (SD = 0.40) for the first and 0.46 (SD = 0.36) for the second attempt. Regarding the level of certainty in the correctness of the recognitions, in the second attempt, participants were certain or very certain for a smaller proportion of trials across all four categories compared to the first attempt, as shown in . One of the reasons for this difference was inconsistent recognition results with the same object across the first and second attempts, supported by P9's response: \"the second time around, they gave me different information. So then I became uncertain about trusting what it was telling me.\"\nFurthermore, in the second attempt, the trial completion time significantly decreased to 4.22 seconds (SD = 2.33), compared to the first, where we recorded a longer duration of 6.75 seconds (SD = 3.15). This discrepancy suggests a meaningful variation between the attempts. Possible explanations for this observed difference could be attributed to participants' increased familiarity with the task procedure in the second attempt, as well as quicker decision-making based on prior experience with the task in the first attempt."}, {"title": "5.3 Subjective Feedback", "content": "While participants missed around half of the errors, they generally perceived identifying errors as not challenging, confirming the finding from a prior study that BLV users have mixed feelings with both confidence and concerns regarding identifying errors in private object detection [89]. When asked about the difficulty, the majority disagreed (N = 5), with some strongly disagreeing (N = 3). For instance, P8, who has low vision, was able to discern correct and incorrect predictions based on their vision and the textures of the object. Other participants identified errors by comparing predictions across multiple trials. For example, P10 explained, \"I didn't recognize a mistake until the second similar object appeared. So like the two cans of the Lacroix apricot and Lacroix mango, one of them was incorrect because it was telling me apricot both times.\" Errors were sometimes evident to participants because predicted and true objects had distinct textures, shapes, or weights, as noted by P12: \"[...] for example, the diced tomatoes versus the chicken broth, chicken broth is more liquid. It was easy to identify that it was wrong.\" On the other hand, three participants strongly agreed that identifying errors was challenging. Among them, two mentioned that the recognition results were inconsistent with an object, making it difficult to determine their correctness. P9 said \"Two things that seem similar, but the first time they said they were the same, and then the next time putting them back, they said something different on one of them. So now I'm not sure. So I strongly agree, it was difficult for me to tell us it was in error.\" Another participant mentioned that it was challenging to remember all objects explained at the beginning of the study, which complicated the decision-making regarding the correctness of recognition results."}, {"title": "6 DISCUSSION", "content": "Our user study, exploratory in nature, shows both promising results and future research directions for supporting blind users' interactions with error-prone AI-infused technologies. In this section we discuss lessons learned and limitations that may affect the generalizability of our findings."}, {"title": "6.1 Implications", "content": "Enable users to leverage their expertise in reviewing errors independently. The findings from the interview have shed light on an interesting trend: most participants expressed a preference for evaluating the quality of their photographs without the assistance of sighted individuals or remote sighted aid services, such as Be My Eyes or Aira, when using camera-based assistive technologies. This preference seems to stem from a fundamental aspect of the utilization of AI-based systems - namely, the desire to carry out visual tasks independently when sighted assistance is unavailable. It further shows the preference of blind and low-vision users to utilize their expertise in assistive technology, such as integrating recognition results from multiple AI apps to identify errors. This personalized"}]}