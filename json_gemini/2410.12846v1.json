{"title": "Accurate and Regret-aware Numerical Problem Solver for Tabular Question Answering", "authors": ["Yuxiang Wang", "Jianzhong Qi", "Junhao Gan"], "abstract": "Question answering on free-form tables (a.k.a. TableQA) is\na challenging task because of the flexible structure and the\ncomplex schema of tables. Recent studies use Large Lan-\nguage Models (LLMs) for this task, exploiting their capa-\nbility in understanding the questions and tabular data which\nare typically given in natural language and contains many\ntextual fields, respectively. While this approach has shown\npromising results, it overlooks the challenges brought by\nnumerical values which are common in tabular data, while\nLLMs are known to struggle with such values. We aim to\naddress this issue and answer numerical questions. We pro-\npose a model named TabLaP that uses LLMs as a planner\nrather than an answer generator, exploiting LLMs' capabil-\nity in multi-step reasoning while leaving the actual numeri-\ncal calculations to a Python interpreter for accurate calcula-\ntion. Recognizing the inaccurate nature of LLMs, we further\nmake a first attempt to quantify the trustworthiness of the an-\nswers produced by TabLaP, such that users can use TabLaP in\na regret-aware manner. Experimental results on two bench-\nmark datasets show that TabLaP is substantially more accu-\nrate than the state-of-the-art models, improving the answer\naccuracy by 5.7% and 5.8% on the two datasets, respectively.", "sections": [{"title": "Introduction", "content": "Tables are a commonly used data format to organize and\npresent information. Table Question Answering (TableQA),\nwhich aims to answer questions based on data in tables,\narises as an important problem to automatically extract and\nanalyze meaningful information from free-form tables for\nnon-experts (Ye et al. 2023a). In a typical TableQA task,\nquestions are given in natural language. The input tables\nare in free-form and may not have a well-defined schema,\nthat is, they are not necessarily relational tables, e.g., web\ntables (Cafarella et al. 2008). In other words, these tables\nmay have mixed data types in columns and non-predefined\nlogical forms (Jin et al. 2022), such that structural query\nlanguage-based solutions (Pasupat and Liang 2015; Zhong,\nXiong, and Socher 2017; Pourreza and Rafiei 2023; Gao\net al. 2024) are not always applicable. See Figure la for an\nexample of the TableQA task.\nRecent studies (Ye et al. 2023b; Cheng et al. 2023; Patnaik\net al. 2024; Liu, Wang, and Chen 2024) use Large Language\nModels (LLMs), exploiting their semantic understanding ca-\npability to analyze textual questions as well as tabular data\nwhere many fields are often in text. Unfortunately, these\nstudies have overlooked a notorious issue of the LLMs\ntheir limited capability in handling numerical data (Frieder\net al. 2024). As Figure 1b shows, prompting an LLM (GPT-\n3.5 Turbo) directly with a table and a question for numerical\ncalculation could lead to an inaccurate answer.\nTo address this issue, we study how to strengthen the ca-\npability of LLMs to handle numerical questions. A crucial\nobservation behind the design of our proposed solution is\nthat while LLMs struggle with carrying out numerical calcu-\nlations, they are capable of producing plans to execute such\ncalculations. To see this, as shown in Figure 1c, using the\nsame LLM but prompting it to generate a calculation plan\nexpressed in Python, the Python code generated can actually\nbe executed and yield the correct answer.\nBased on this observation, we propose a TableQA model\nwith an LLM as a planner, TabLaP. We call the planner LLM\nNumSolver and design a prompt to guide it to generate a\nplan that decomposes a complex numerical question into se-\nquential steps (in Python script) based on Chain-Of-Thought\n(Wei et al. 2022). The generated script is then executed by a\nPython interpreter to produce an answer to the question.\nMoreover, to largely retain the strong capability of LLMs\nto process non-numerical questions, TabLaP takes a dual-\nbranch structure, where NumSolver forms a branch and a\nState-Of-The-Art (SOTA) TableQA model forms the other.\nAs each of the two branches produces an answer, to inte-\ngrate the answers from them, we exploit a third LLM (named\nAnsSelecter) an open-source one (LlaMa 3-8B-Instruct)\nwhich allows for fine-tuning \u2013 to take the question and an-\nswers from both model branches (including the reasoning\ntexts) as input and selects a branch to trust. The answer from\nthe selected branch is then returned as the final answer.\nRecognizing the inaccurate nature of the answers gener-\nated by LLMs, we further quantify the trustworthiness of the\nanswers generated by TabLaP. We propose a module named\nTwEvaluator based on yet another LLM and Multi-Arm\nBandit (MAB) (Vermorel and Mohri 2005) that together\ntracks the correctness of the answers from both branches of\nTabLaP over time and yields a trustworthiness flag of the fi-\nnal answer based on its source branch's accuracy. This trust-\nworthiness flag enables users to consume the answers in a\nregret-aware manner.\nOverall, this paper makes the following contributions:"}, {"title": "Related Work", "content": "TableQA has attracted much attention in recent years. Ex-\nisting research is mainly driven by studies to design models\nthat can understand questions in natural language and tab-\nular data. These studies can be categorized into semantic\nparsing-based, pre-trained language model (PLM)-based,\nand large language model (LLM)-based.\nSemantic parsing-based methods. Semantic-parsing-\nbased methods transform natural language questions into a\nlogical form (e.g., SQL) which machines can easily under-\nstand and execute. There are two sub-categories of methods:\n(i) weakly-supervised (Pasupat and Liang 2015; Yu et al.\n2018a; Neelakantan et al. 2017), and (ii) fully-supervised,\nsuch as NL-to-SQL (Zhong, Xiong, and Socher 2017; Pour-\nreza and Rafiei 2023; Gao et al. 2024). In weakly super-\nvised methods, the semantic parser generates the logical\nform based on an input question, a table, and the answer.\nThere is no pre-defined ground-truth logical form. Fully su-\npervised methods, on the other hand, further take a ground-\ntruth logical form as their input. Both sub-categories focus\non analyzing the questions. They are less effective on tables\nwith complex structures and data types (Hong et al. 2024).\nPLM-based methods. Language model-based methods,\nincluding PLM-based ones, focus on guiding the models to\nunderstand tabular data. There are two directions: (i) Tai-\nloring Transfomer (Vaswani et al. 2017) model structure\nfor better tabular data understanding. For example, TAPAS\n(Herzig et al. 2020) masks cells in the tables and extends\nthe BERT (Devlin et al. 2019) structure by adding column,\nrow, and rank embeddings. TUTA (Wang et al. 2021) fur-\nther masks columns or table segments and designs a spe-\ncial attention mechanism with a tree structure to capture\nthe hierarchical relationships and dependencies for tabular\ndata. TaBERT (Yin et al. 2020) extends BERT with verti-"}, {"title": "Methods", "content": "Given a table $T$ and a question $Q$ regarding the data in $T$,\nour goal is to design a model which produces an accurate\nanswer for $Q$. Here, $Q = (q_1, q_2, ..., q_{|Q|})$ is given in nat-\nural language, where $q_i \\in Q$ ($i \\in [1, |Q|]$) is a token (e.g.,\na word), $T$ is also represented as a sequence of tokens in\nnatural language, where the cells are separated by special\ncharacters such as '' while the rows are separated by new-\nline characters. The effectiveness of a model is measured by\nthe token-based comparison between the answer generated\nby the model and the ground truth.\nIn this paper, we are particular interested in multi-step nu-\nmerical questions which require applications of two or more\nbasic arithmetic and relational operators {+, -, *, /, >, < }.\nThese operators are common in our experimental datasets,\nwhile the techniques proposed can be extended to support\nmore relational operators such as > and < straightforwardly.\nIt is known that existing LLMs are less effective on handling\nthis type of questions.\nModel Overview\nThere are three stages in the question answering process of\nour TabLaP: (i) answer generation, (ii) answer selection,\nand (iii) answer trustworthiness evaluation."}, {"title": "Answer Generation Stage", "content": "In this stage, we use two models: NumSolver and an SOTA\nmodel (Mix-SC), as two separate branches to answer the\nquestion. The details of Mix-SC is explained in the Related\nWork section.\nIn NumSolver, we force a backbone LLM to answer an\ninput question $Q$ step by step and generate the reasoning\nprocess with the intermediate results. Besides, we also ask\nthe LLM to write down the Python Scripts, which can be\nused to answer the question, and the answer directly ob-\ntained through the table reasoning. By executing the Python\nscripts with a Python Interpreter, question answers are ob-\ntained which are found more accurate, especially for those\nmulti-step numerical questions, than those obtained with the\ndirect reasoning methods. When the Python-execution an-\nswers are different from the answers generated by the LLM\ndirectly and they contain numerical values, priority is given"}, {"title": "Answer Selection Stage", "content": "In this stage, AnsSelecter utilizes the answers generated by\nthe two branches in the answer generation stage to decide the\nbest solution for the given problem. To achieve this, AnsS-\nelecter first analyzes the reasoning steps of the two models,\nand then makes a decision based on the question, generated\nanswers and their corresponding reasoning process."}, {"title": "Answer Trustworthiness Evaluation", "content": "In the trustworthiness evaluation stage, unlike the existing\nTableQA models, TwEvaluator returns a trustworthiness flag\nwhich indicates whether TabLaP is able to provide a reliable\nanswer to the given question. TwEvaluator determines the\nreliability of the answer by analyzing the logical rationality\nand the accuracy of the models. When the returned flag sug-\ngests that the answer provided by TabLaP is reliable, then\nusers can use the answer given by the AnsSelecter with cer-\ntain confidence. Conversely, when the returned label sug-\ngests that TabLaP lacks ability to solve the problem, users\nwill be informed that the answer to the question might be\nwrong.\nTo enhance the reliability of TwEvaluator, we incorpo-\nrate two methods: the Expanding Window method and the\nMulti-arm Bandits with Upper Confidence Bound (UCB)\n(Slivkins 2019). The Expanding Window method involves\ninitially establishing a set of cases to calculate the accuracy\nof TwEvaluator's results and then using this accuracy to de-\ntermine the probability of accepting TwEvaluator's output.\nAs new cases are encountered, the probability is updated ac-\ncordingly with respect to the accumulating data over time.\nOn the other hand, the Multi-arm Bandits with UCB method\nbalances the exploration and exploitation by selecting ac-\ntions (in this context, decisions regarding TwEvaluator's out-\nputs) that maximize the expected reward with consideration\non the uncertainty of the outcomes. The UCB component\nspecifically allows the model to prioritize actions that are\nless certain but potentially more rewarding, thus improving\nthe overall performance and the robustness of TwEvaluator\nin providing accurate and reliable answers."}, {"title": "Fine-tuning for AnsSelecter and TwEvaluator", "content": "We adopt Low-Rank Adaptation (LoRA) (Hu et al. 2022),\nto fine-tune our AnsSelecter and TwEvaluator with Llama3-\n8B-Instruct as a backbone model. LoRA is a parameter-\nefficient fine-tuning method for low-rank decomposition of\nthe gradient update matrices in transformers. The effective-\nness of LoRA is proven by (Aghajanyan, Gupta, and Zettle-\nmoyer 2021) that pre-trained language models can still learn\nefficiently despite a random projection to a smaller sub-\nspace. Thus, the forward pass for both AnsSelecter and\nTwEvaluator can be represented as:\n$h = W_0X_{(T,Q,A)} + \\Delta W_{x(T,Q,A)}$\ngradient descent update\n$= W_0X_{(T,Q,A)} + AB_{X(T,Q,A)}$\nLORA update\n(1)\nwhere $W_0$ is the initial parameter weights for the transform-\ners, and $X_{(T,Q,A)}$ is the pair of table structure, question, and\nmodel answer sets, including answers and their correspond-\ning reasoning processes as input data. $\\Delta W$ is the gradient\ndescent update for parameters which is decomposed by low-\nrank matrices A and B, where $\\Delta W \\in R^{mxd}, A \\in R^{mxk},$\n$B\\in R^{k\\times d}$. Matrices A and B have much fewer parameters\ncompared with matrix W, because $k \\ll min(m, d)$."}, {"title": "Experiments and Results", "content": "This section presents experimental results to verify the ef-\nfectiveness of TabLaP. We aim to answer the following ques-\ntions: (Q1) How does TabLaP compare with SOTA models\nand latest LLMs, in terms of accuracy to process TableQA\ntasks? (Q2) How effective is TabLaP in tracking the trust-\nworthiness of its answers? (Q3) How effective is TabLaP\nin handling numerical problems? (Q4) How much does the\nmodules contribute to the overall accuracy of TabLaP?\nExperimental Setup\nDatasets. We use a public benchmark dataset named\nWikiTableQuestions (denoted as WTQ) (Pasupat and\nLiang 2015) and a dataset named FTQ which we adapted\nfrom the FeTaQA dataset (Nan et al. 2022), to showcase the\napplicability of TabLaP across datasets.\nFeTaQA is also a TableQA dataset. Its answers contain\ndescriptive contents that may not be directly relevant to the\nquestions. We remove such noisy information from the an-\nswers and only retain the entities that answer the questions,\nto form the FTQ dataset. Appendix A.1 shows an example"}, {"title": "Prompt for NumSolver", "content": "For NumSolver, we ask GPT-3.5\nTurbo to generate the reasoning process, Python script, and"}, {"title": "Prompt for AnsSelecter", "content": "For AnsSelecter, we prompt a\nfined-tuned Llama3-8B-Instruct with the reasoning process\nobtained from the answer generation stage, the answers, and\nthe table information. The prompt template is shown in Fig-\nure 7. AnsSelecter returns a label of either [A] or [B], in-\ndicating that the answer from the SOTA TableQA branch or\nour NumSolver is preferred, respectively."}, {"title": "TwEvaluator", "content": "We provide additional details about TwE-\nvaluator in this section, including the prompt template used\nby the LLM of TwEvaluator, design details of the module,\nand additional experimental results for the module.\nPrompt. The LLM (i.e., a fine-tuned Llama3-8B-Instruct)\nof TwEvaluator uses a similar input to that of AnsSelecter,\nas shown in Figure 8. It returns an answer of either [True] or [False], indicating whether TabLaP has answered the\ninput question correctly.\nDesign details of TwEvaluator. Empirically, we observe\nthat using yet another LLM to evaluate the outcome of Ans-\nSelecter (which is also an LLM) is highly accurate when\nit predicts [True] but is less accurate when it predicts"}, {"title": "Fine-tuning for AnsSelecter and TwEvaluator", "content": "Since Llama3-8B-Instruct is used as a backbone model,\nthe fine-tuning of AnsSelecter and TwEvaluator is es-\nsentially a fully supervised classification task which,\nspecifically, can be considered as an utterance prediction\nof the label words. The aim of the models is to maximize\n$P_{0\\theta}(x_1^{(i)},x_2^{(i)},...,x_T^{(i)}|x_1^{(i)},x_2^{(i)},...,x_{T-1}^{(i)})$ which means given previous\ntokens ($x_1^{(i)},x_2^{(i)},...,x_{T-1}^{(i)}$) to maximize the conditional\nprobability of $x_t|x_1^{(i)},x_2^{(i)},...,x_{T-1}^{(i)}$, where $\\theta$ is the model parameter.\nTherefore, the loss functions for this fine-tuning task are:\n$L_{cls} = - \\sum_i y_{m,i} log(\\hat{y}_{m,i})$\n(2)\n$L_{verif} = - \\sum_i y_{n,i} log(\\hat{y}_{n,i})$\n(3)\n$L_{total} = - log(\\hat{y}_{m,y_m}) - \\alpha log(\\hat{y}_{n,y_n})$\n$= L_{cls} + \\alpha L_{verif}$\n(4)\nwhere $y_{m,i}$ and $Y_{n,i}$ are the ground-truth labels for the mth\nand nth tokens at position i in one-hot encodings. $\\hat{y}_{m,i}$ and\n$\\hat{y}_{n,i}$ are the model predicted probabilities for the mth and\nnth tokens at position i. T and V are the dictionary lengths\nof the AnsSelecter and TwEvaluator, respectively. $\\alpha$ is a co-\nefficient used to control the influence of TwEvaluator on the\nresult, and its default value is 1."}, {"title": "A.2 Numerical Question Filtering", "content": "We collect frequently used\nkeywords for the numerical questions, including \"how\nmany\", \"number\u201d, \u201cthe most\u201d, \u201cdifference\u201d, \u201ccount\", \"high-\nest\u201d, \u201caverage\u201d, \u201cat least\u201d, \u201crank\u201d, \u201clowest\u201d, \u201cpercentage\",\n\u201csum\u201d, \u201ccompare\u201d, and \u201cfrequency\u201d to extract potential nu-\nmerical questions."}, {"title": "The Multi-armed Bandit method.", "content": "The MAB method aims\nto balance exploration and exploitation by selecting actions\nthat maximize expected rewards while considering uncer-\ntainty. We use an MAB of two arms representing either to\naccept or to reject a [FALSE] prediction of the TwEval-\nuator LLM. We aim to select the arm that maximizes the\nTwAccuracy of TabLaP, as guided by the equation below:\n$\\hat{\\mu}_i(t) = \\frac{\\sum_{n=1}^{t-1}r_i(n) \\cdot \\mathbb{I}(a(n) = i)}{N_i(t-1)},$\n(7)\nwhere $i(t)$ is the estimated mean reward of arm i at time\n(i.e., test instance) t; $r_i(n)$ is the reward received when arm\ni is selected at time n; $\\mathbb{I}(a(n) = i)$ is the indicator func-\ntion, which equals to 1 if arm i is selected at time n, and 0\notherwise; and $N_i(t)$ is the number of times arm i has been\nselected up to time t. As we aim to maximize the TwAccu-\nracy of TwEvaluator, we set the reward $r_i(n)$ as 1 if choosing\narm i is consistent with the ground-truth, and -1 otherwise.\nTo balance exploitation (i.e., to follow the arm with a\nlarger estimated mean reward $\\hat{u}_i(t)$) and exploration (i.e.,\nto try the other arm and accumulate more accurate mean re-\nward estimates for the arm), we use the Upper Confidence\nBound (UCB) algorithm:\n$UCB_i(t) = \\hat{u}_i(t) + c \\cdot \\sqrt{\\frac{In t}{N_i(t)}}$,\n(8)\nwhere $UCB_i(t)$ is the UCB for arm i at time t; c is the ex-\nploration parameter that controls the balance between explo-\nration and exploitation (we empirically set $c = \\sqrt{2}$); and In\nis the natural logarithm function. For arm selection, we use:\n$a(t) = arg max (UCB_i(t))$,\n(9)\nwhere a(t) is the arm to be selected at time t, when TwEval-\nuator LLM predicts [FALSE] for test instance t. The MAB\nmethod accepts or rejects the prediction following a(t)."}]}