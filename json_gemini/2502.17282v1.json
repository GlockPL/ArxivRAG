{"title": "Capability Instruction Tuning: A New Paradigm for Dynamic LLM Routing", "authors": ["Yi-Kai Zhang", "De-Chuan Zhan", "Han-Jia Ye"], "abstract": "Large Language Models (LLMs) have demonstrated human-like instruction-following abilities, particularly those exceeding 100 billion parameters. The combined capability of some smaller, resource-friendly LLMs can address most of the instructions that larger LLMs excel at. In this work, we explore how to route the best-performing LLM for each instruction to achieve better overall performance. We develop a new paradigm, constructing capability instructions with model capability representation, user instruction, and performance inquiry prompts to assess the performance. To learn from capability instructions, we introduce a new end-to-end framework called Model Selection with Aptitude Test (MODEL-SAT), which generates positive and negative samples based on what different models perform well or struggle with. MODEL-SAT uses a model capability encoder that extends its model representation to a lightweight LLM. Our experiments show that MODEL-SAT understands the performance dimensions of candidate models and provides the probabilities of their capability to handle various instructions. Additionally, during deployment, a new model can quickly infer its aptitude test results across 50 tasks, each with 20 shots. MODEL-SAT performs state-of-the-art model routing without candidate inference and in real-world new model-released scenarios. The code is available at https://github.com/Now-Join-Us/CIT-LLM-Routing.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) (OpenAI 2022; Du et al. 2022; Touvron et al. 2023a; Chiang et al. 2023; Jiang et al. 2023) rapidly evolve, demonstrating near-human general capabilities, especially in understanding, reasoning, and creative tasks related to instruction-response scenarios. Recent advancements have even enabled these LLMs to be trained in multilingual (Yang et al. 2024; Dubey et al. 2024), multidomain (Zhou et al. 2024; Yang et al. 2024), and multimodal (Chen et al. 2015, 2023; Reid et al. 2024) environments, allowing them to tackle complex instructions such as \"What is the relationship between Fourier series and Hilbert space?\" or to interpret images by identifying, \u201cWhat are the basis vectors of the Hilbert space?\"\nThe rise of LLMs and their extensions has incredibly energized community applications. However, achieving more comprehensive capabilities often requires LLMs of a larger scale. According to the Open LLM Leaderboard (Aidar Myrzakhan 2024), 60% of the top 50 LLMs have around 70 billion (B) parameters or more, with only three LLMs under 10B. Additionally, some closed-source LLMs consistently dominate performance rankings over extended periods. Consequently, optimizing LLM applications often hinges on substantial computational resources or costly token purchases. A natural idea arises: Can we utilize multiple smaller LLMs, which are more resource-friendly and have below one-tenth of the parameters of their larger counterparts, to achieve performance comparable to gigantic LLMs while maintaining low inference costs?\nIn the experiments, we find that the combined capability of some smaller-scale LLMs, despite their lower overall performance, can address most of the instructions that larger LLMs excel at. As shown in Figure 1, on the Massive Multitask Language Understanding (MMLU) (Hendrycks et al. 2020) benchmark, the Phi-1 LLM with 1.3B performs nearly 50% worse than GPT-40. However, it exhibits similar effectiveness to GPT-40 in the high school mathematics category. Moreover, we create an early-access LLM zoo that includes Phi-1 (Gunasekar et al. 2023) and four 7B LLMs, which were released a year earlier than GPT-40 and exhibit an approximately 30% performance gap compared to GPT-40. However, the combined accurate responses from this zoo cover 90% of which GPT-40 handles correctly and address nearly 80% with which GPT-40 struggles. By strategically assigning instructions to the suitable LLM in the zoo, there is potential to exceed GPT-40's performance by 15%. From this phenomenon, the model routing for each instruction enhances performance with seamless LLM transitions and minimal inference costs, all without user awareness.\nThe key to the proposed instruction-level model routing is to efficiently identify the optimal model from a vast pool of options, without prior access to the potential candidates' inference outputs (Tan et al. 2023; Xiao et al. 2023) or the target task's ground truth (You et al. 2022; P\u00e1ndy et al. 2022). In this paper, we introduce MODEL-SAT: Model Selection with Aptitude Test. Our approach leverages 50 core 20-shot tasks, where the model test result represents model capability. By learning the generalization relationships between the capability representations of the candidate models and the instructions to be assigned, we can select the most suitable model across various repositories and target instructions.\nDriven by the model capability representation, the MODEL-SAT framework establishes a novel paradigm, denoted as capability instruction tuning. Capability instructions consist of a capability representation, a user instruction, and a prompt to probe whether the model can perform that instruction. Using extensive historical performance data, capability instruction tuning learns an implicit relationship between core capability representations and unseen instructions. Moreover, it delves deeper into understanding the mapping between the capabilities' performance and the instructions' semantic distribution. This intuition comes from the observation that individuals who perform well in the mathematical sections of the college admission SAT in the United States often pursue careers that involve logical reasoning. Capability instruction tuning aims to equip the model with a lightweight standardized guide to assess its effectiveness in handling future instructions.\nSpecifically, we combine model capability representation with positive and negative training instructions regarding current model performance, yielding statements like, \u201cThe model achieves accuracy 85% on the task of 'Mathematics, Geometry, ...'. Instruction: Predict whether the model can handle ...\". To align the performance distribution inherent in model representation to the instruction semantic, we are the first to incorporate a capability encoder and extend the input of a lightweight LLM to include capability representation. The end-to-end MODEL-SAT functions as a model router that outputs the probabilities indicating which models will likely excel at specific instructions.\""}, {"title": "Preliminary", "content": "We begin by discussing the key elements and the pipeline of model selection, followed by the evolution of related works."}, {"title": "Instruction, Output, and Answer", "content": "Consider a test instruction dataset $\\mathcal{D}_{test} = \\{(x_i, a_i)\\}_{i=1}^N$ with N labeled samples. The $x_i$ and $a_i$ represent the instruction and its corresponding answer, respectively. Given an LLM or its extension, represented as f, the output generated for instruction $x_i$ is denoted as $o_i$, i.e., $f(x_i) = o_i$. There are no restrictions on the language, domain, or modality of $x_i$; In this paper, we focus on decoder-only text generation models, which means that $a_i$ is typically presented in text form. For the model f to excel at instruction $x_i$, it is equivalent to obtaining a high score on the evaluation $eval(o_i, a_i)$."}, {"title": "Pipeline of Model Routing", "content": "Consider a candidate model zoo composed of many trained LLMs, $\\mathcal{M} = \\{f_m\\}_{m=1}^M$. Model routing involves selecting a model from the zoo for each instruction $x_i$ in the test dataset $\\mathcal{D}_{test}$. Specifically, the sequence of selected models is formalized as $\\mathbf{f} = (f_1, f_2, ..., f_N)$, where $f_i \\in \\mathcal{M}$. We define the optimal model f for instruction $x_i$ as the model that maximizes the score: $eval(f(x_i), a_i)$. The objective of the instruction-level model routing is:\n$\\mathbf{f} = \\arg \\min_{f_m \\in \\mathcal{M}} \\sum_{i=1}^N l(f_m(x_i), a_i)$ (1)\nwhere $l(\\cdot)$ represents the loss function associated with the metrics between $o_i^m = f_m(x_i)$ and the ground truth $a_i$. The model routing bottleneck arises from the number of instructions on which no model in the zoo performs well."}, {"title": "Revisit from Requirement, Target, and Key Inputs", "content": "Routing target of parameter initialization or models with zero-shot capabilities: Early model router (Tran, Nguyen, and Hassner 2019; Nguyen et al. 2020; Tan, Li, and Huang 2021; Ding et al. 2022) efforts primarily focus on identifying a good training initialization that facilitates fine-tuning downstream tasks to achieve optimal performance. In this context, candidate models likely required additional training to adapt to the target task. Recently, guided by scaling laws, foundational models like LLMs have experienced remarkable advancements in their zero-shot capabilities (Touvron et al. 2023b; Wei et al. 2022; Team et al. 2023). Extended models have demonstrated considerable potential in multilingual, multi-domain, and multimodal applications. For instance, Llama 3.1 (Dubey et al. 2024) serves as a multilingual agent, Qwen2-Math (Yang et al. 2024) tackles several Olympiad-level problems, and GPT-40 (OpenAI 2023) processes information from multiple sources.\nRouting requirements with target instruction annotation, backpropagation delay, or candidate output: Some works (Bao et al. 2019; Li et al. 2021; You et al. 2021; Deshpande et al. 2021; P\u00e1ndy et al. 2022) design the proxy metric of transferability, which approximates the lower bound of fine-tuned performance. These works often rely on certain source clues, labeled instructions, or backpropagation steps to assess the transferability from the source pre-trained model to the target dataset. Additionally, some re-ranking-based works (Tan et al. 2023; Xiao et al. 2023; Zhang et al. 2024a) train an extra model to learn the contrastive relationships between the instruction and the candidate inference outputs $\\{o_i^m\\}_{m=1}^M$, routing the optimal one linked to model $f_m$. However, obtaining all inferences may introduce significant delays when the number of models M in the repository becomes excessively large (Shnitzer et al. 2023; Lu et al. 2023; Hu et al. 2024). Our MODEL-SAT aims to route models without annotation or inference requirements, considering candidates as black boxes. A central feature is constructing model representations for each model and learning the adjusted relationship between it and the target instructions.\nKey input \u2013 model representation for model routing: When routing a model for instruction, the router requires the key representation that captures the model's characteristics. We followed the concept of learnware (Zhou 2016), leveraging a small amount of model-proficient data to construct shared specifications (Zhou and Tan 2024; Tan et al. 2024). Other relevant methods leverage forward behavior or results on target as model representation, which inevitably introduces inference delays. Recently, some approaches (Lu et al. 2024; Srivatsa, Maurya, and Kochmar 2024; Ding et al. 2024; Feng, Shen, and You 2024) have started to utilize learnable parameters as model representations. For instance, some introduce a surrogate scorer as the corresponding model representation, learning the mapping from the task to the accuracy of candidate model outputs. Model Spider (Zhang et al. 2023b) takes this concept by encoding the model representation into a learnable vector, which acts as the input token for a Transformer-based router. However, learnable representation face challenges when new models are introduced, as they require extensive historical performance for costly pre-training of the router. Our solution uses text-only descriptions of capabilities. New models can create representations by inferring 50 quick tasks, each with 20 shots."}, {"title": "MODEL-SAT: Model Routing with Aptitude Test", "content": "In this section, we start by building the model representation and progress to the details of MODEL-SAT, training data, and optimization process. Finally, we outline an efficient deployment framework for model routing."}, {"title": "Capability Instructions", "content": "The capability instruction mainly comprises the capability representation of the candidate model $f_m$, user instruction $x_i$, and performance inquiry prompt. Specifically, the model's capability representation is formed from 50 distinct tasks across various categories from the MMLU dataset, with each task being 20-shot. We provide a concise description of five keywords for each task. Next, we evaluate the candidate models across these 50 tasks and describe the results in natural language, i.e., model representation. Furthermore, the advantage of representing in natural language is that it helps to include extra expert knowledge, such as mentioning which languages a model supports. The easy-to-obtain representations serve as an aptitude test for the models, indicating their potential capabilities across various dimensions.\nTo assess how well the candidate model can follow a single or a set of instructions $x_i$, we introduce the training instructions that were executed correctly versus those incorrectly. These will be paired with the performance inquiry prompt p to form the capability instruction, denoted as $z_i$, which drives the router to predict adaptation scores. As illustrated in Figure 3, it combines the capability representation $c_m$ for candidate m, the instruction $x_i$, and a inquiry prompt p.\nCore task sampling: We sample instructions of core tasks with the highest distinguishability, avoiding those where most models perform correctly or incorrectly. In the model zoo of training, samples for which half of the models make mistakes while the other half are correct carry greater weight."}, {"title": "Architecture", "content": "Motivation: Although LLMs demonstrate strong instruction-following abilities, a gap exists between performance and the semantic distribution in capability instructions, particularly in understanding combinations of performance dimensions. For example, if a candidate model achieves 80% in mathematics and 95% in legal principles, the model may possess legal reasoning skills. To address this, we propose extending a capability encoder E5-Large ~0.5B (4) before a Phi-3-Mini 3.8B LLM (4) to align the candidate performance with the instructions. The architecture is illustrated in Figure 4.\nStructure: The capability instruction comprises the capability representation $c_m$ for model m, the instruction $x_i$, and the query prompt p. We first align the model representation, mapped by the capability encoder, into an embedded feature of LLM inputs. This is achieved using a single-layer MLP, which acts as a connector to adapt the dimensions. Consequently, we derive the aligned model capability vector:\n$e_{c_m} = W \\cdot \\Phi(c_m)$, (2)\nwhere $e_{c_m}$ is combined with the input embeddings of $x_i$ and p to form the capability instruction $z_i$, i.e.,\n$z_i = [e_{c_m}, e_{x_i}, e_p] = [e_1, e_2, ..., e_s]$, (3)\nwhere s denotes the length of the concatenated capability instruction sequence. Our alignment module operates at a natural language level, allowing for a streamlined design. In the following Section, we also explore alternative approaches, including training without the alignment module."}, {"title": "Tuning Recipe", "content": "Forward Process of the Prediction Score: As shown in Figure 3, the query prompt p in the capability instruction includes keywords related to positive terms that the model excels at. For example, \"Yes\" serves as the key response in the prompt \"predict whether the model can handle test instruction by indicating 'Yes' or 'No'.\u201d In this context, the model routing prediction score is:\n$\\Pr(\\text{`Yes'} | z_i) = \\prod_{t=1}^S \\Pi(e_t | [e_1, ..., e_{t-1}])$, (4)\nwhere we omit the input embedding layer for the LLM $\\Phi$.\nPositive and Negative Instructions for Training: We apply Homogeneous In-Batch Negative Sampling (Karpukhin et al. 2020; Zhang et al. 2023a) for each capability representation $c_m$ with its well-performed and poorly-performed instructions to enhance the discriminative during training. Typically, a k-shot training batch $Z = \\{z_i\\}_{i=0}^k$ contains 1 positive instruction and k - 1 negative ones.\nLoss Design: We denote the position of the positive instruction in the training batch Z as $Y_{pos}$, and the remaining ones are k - 1 negative instructions. Our objective is to enhance the prediction score for the positive ones as the candidate performs better on this instruction. We employ the cross-entropy loss to optimize this in one batch Z:\n$\\mathcal{L}_{CE} = \\mathbb{E}_{Z \\in \\mathcal{D}_{test}} [-\\log \\Pr(\\hat{h}_{Y, \\text{`Yes'}}(Z) = y_{pos} | Z)]$,(5)\nwhere $\\hat{h}_{Y, \\text{`Yes'}}(Z) = \\arg \\max_{z_i \\in Z} \\Pr(\\text{`Yes'} | z_i)$ is the LLM $\\Phi$ to identify which instruction $z_i \\in Z$ can be done well (positive) and which cannot (negative).\nLearning Strategy: The model representation is derived from the capability distribution on MMLU. Similarly, we develop both in-domain and out-of-domain learning environments for MODEL-SAT. In the first stage, we collect in-domain positive and negative training instructions, primarily sourced from the same category as the MMLU dataset. We only fine-tune the connector between the capability encoder $\\Phi$ and the LLM $\\Phi$, establishing an initial capability-to-instruction mapping. In the second stage, we fine-tune all model parameters. We apply a larger learning rate on the encoder and connector to enhance capability alignment with instruction semantics.\nData Refinement: We further address noise in whether the candidate model can accurately perform the instructions, influencing whether a capability instruction is a positive or negative training sample. For those difficult instructions that only a few models handle correctly, we implement a circle test by rotating the sequence of options to prevent lucky guesses. Furthermore, we prioritize higher-ranked candidates in the training data by sampling with increased weight."}, {"title": "Efficient Deployment", "content": "MODEL-SAT provides the routing prediction for the candidate model applied to the target instruction. These scores are generated by the same model, rendering them comparable. In this paper, we propose an open-source and comprehensive model routing toolkit, MODEL-SAT. This toolkit offers a viable solution for dynamic model routing within communities such as HuggingFace, harnessing the repository to boost performance on target tasks.\nMODEL-SAT exhibits remarkable generalization capabilities for unseen data, which can be directly concatenated into the capability instruction. Similarly, the incremental extension to new models proves highly efficient, requiring only inference on 50 core tasks for the model representation. As later addressed in the experiments, MODEL-SAT exhibits zero-shot model routing abilities, facilitating the streamlined development of capability instructions in broader contexts."}, {"title": "Experiments", "content": "This section begins by detailing the construction of training and test instructions in capability instructions tuning. It then presents different zoo setups for testing and concludes with an analysis of results and ablation studies."}, {"title": "Training and Test Instructions", "content": "As mentioned earlier, the capability instruction consists of model representation $c_m$, instructions $x_i$ to assign, and performance inquiry prompts p.\nCandidate Model Representations $c_m$ for candidate m: We introduced 66 open-source LLMs of varying scales. This includes 60 models under 10B, 15 ones between 10B and 20B, and 5 ones around 60B. We sample 50 categories from the MMLU dataset, with 20 distinguishing instructions from each. Different candidate models share core tasks to ensure stability in capability demonstration.\nInstructions $x_i$ Pending to Assign: We consider more than 20 datasets that include areas such as language, analysis, understanding, and reasoning in general evaluations, as well as specialized fields like mathematics and medicine. For each dataset, we sample sets of positive and negative instructions where the model performed well or poorly, with sampling on stronger models assigned greater weight. Each dataset contains about 100 instructions on average.\nPerformance Inquiry Prompts p: We explore different approaches for the probability of model routing. For capability instructions, we design the performance inquiry prompt, such as \"predict whether the model can handle test instruction by indicating 'Yes' or 'No'.\u201d In this context, a response of 'Yes' signifies that the model is well-performed to the instruction. We also experiment with integrating a regression linear layer onto the next token embedding.\nThe capability instruction for the test $z_i$ similarly consists of the model representation $c_m$, the target instruction $x_i$ to be assigned, and the performance inquiry prompt p. To ensure test stability, we conduct a perturbation evaluation on model representation. Specifically, we randomly alter the ranking of the aptitude test results in capability representation twice and then calculate the average routing scores $\\Pr(\\text{`Yes'} | z_i)$. The response on this instruction $x_i$ is provided by the candidate model with the highest routing score."}, {"title": "Benchmarks of LLM Routing", "content": "In this section, we outline benchmarks with various LLMs and their extension zoos, featuring detailed settings.\nSmaller-Scale LLM do Better: As demonstrated in the Table 1, the smaller-scale zoo contains InternLM2.5 (7.7B), Meta-Llama-3-Instruct (8.0B), Qwen2-Instruct (7.6B), GLM-4 (9.4B), and Phi-3-Small-128K (7.4B). The smallermixed zoo includes the smaller-scale zoo and Phi-1 (1.3B), BLOOMZ (3B), and Zephyr-Alpha (7.2B). These LLMs have fewer than 10B parameters and low deployment costs. In Figure 1, we show that the union of correct responses can cover a set of instructions that only larger-scale ones can manage.\nGeneral LLM Zoo Settings. 1) Middle-Mixed and Larger-Mixed LLM Zoo: The middle-mixed zoo includes the smaller-scale zoo and Phi-3-Medium-128K (14B), and Yi-1.5-Chat (34B). The larger-mixed zoo consists of the middle-mixed ones, Meta-Llama-3-Instruct (70B), Qwen2-Instruct (72B), and Mixtral-8x22B-Instruct-v0.1 (140B). The mixed zoo can validate the routing method across different capabilities. 2) High-Performance LLM Zoo: We select from larger-scale LLMs to boost performance further. The model zoo contains only three models above with over 70B parameters. 3) Multimodal LLM Zoo: To verify the generality of capability instruction tuning, we construct a multimodal LLM zoo that includes MiniCPM-Llama3-V 2.5, Phi-3-Vision-128k-Instruct, and InternLM-XComposer2-VL-7B.\nInstructions for Model Routing Evaluation: The test capability instructions differ from the training ones of model routers and consist of seven evaluation datasets. Datasets including MMLU (Hendrycks et al. 2021) (5-shot) and WinoGrande (Sakaguchi et al. 2020) (5-shot) cover a broad range and are involved in the training part as the in-domain"}, {"title": "Toward Comprehensive and Effective Routing", "content": "Performance Analysis in Various Model Zoos. Table 1 demonstrates that MODEL-SAT performs impressively across five comprehensive LLM Zoos. (a) Smaller-Scale: routing of LLMs under 10B achieve performance comparable to the ~70B LLMs. MODEL-SAT's average score of 75.28% closely matches Meta-Llama-3-Instruct-70B's 76.90%, and outperforms it on the ARC-C and TruthfulQA benchmarks. Furthermore, MODEL-SAT selects the optimal model for each instruction, surpassing the best-performing models in the Smaller-Scale Zoo. (b) Smaller-Mixed: We add three earlier released, weaker, and smaller LLMs. MODEL-SAT maintained stable performance, with a slight decrease of about 1% compared to row (a), while performance on ARC-C, BoolQ, and MNLI benchmarks remained nearly identical. (c) Middle-Mixed: Row (c) includes two medium-scale models (10B to 70B), resulting in improved performance for MODEL-SAT compared to row (a). Its average performance now closely matches that of the 70B model. (d) Larger-Mixed: Incorporating three 70B models in row (d) showed that MODEL-SAT remains robust despite significant performance variances in the LLM zoo, with improvements of nearly 5% on MMLU, BoolQ, and so on. (e) High-Performance: Row (e) features a routing of only three 70B models, revealing that the capabilities of gigantic LLMs are further unleashed, achieving a state-of-the-art score of 85.64% on MMLU and 73.63% on the ARC-Challenge.\nComparative Analysis of Routing Delay. In Table 1, the selected model parameter-scale is denoted as xB. The overhead associated with the re-ranking method is related to the M candidates of xB. Figure 2 illustrates that the reranking requires obtaining all inference results from the zoo first, while MODEL-SAT processes model representations all at once and utilizes them throughout its lifetime. As the scale of models in the zoo grows, MODEL-SAT's routing cost remains unaffected by inference ones, ensuring efficient model routing.\nComparison Ranking-based Methods: We evaluate reranking methods such as Cappy, BGE-Large, and GTE-Large. Although they have access to the outputs of each candidate, re-ranking often struggles to find optimal results, potentially because it primarily focuses on retrieving different semantics rather than optimizing performance across similar outputs.\nDetailed Comparative Analysis: Furthermore, we explore various learning strategies within the capability instruction tuning framework. Using features extracted from ROBERTa, we train MLP (classification-based), k Nearest Neighbors (clustering-based), and Random Forest (tree-based). We observed that, except for tree models that are suitable for handling capability representation as similar tabular data, other learning strategies fail to capture performance distribution mappings. Additionally, we analyze MODEL-SAT without the capability encoder. Since the model representation is expressed at the natural language level, Phi-3 in Table 2 can also learn some LLM-Instruction mappings, but its performance remains inferior to that of capability-encoder-based ones.\nAblation Studies: Explore Generalization in Multimodal Scenarios. Most multimodal LLMS (MLLMs) are derived from input-extending LLMs. Multimodal MODEL-SAT is built on the Wings (Zhang et al. 2024b) training architecture, integrating model representation embeddings with visual ones. It maintains strong performance in multimodal scenarios, achieving optimal average performance across MMMU-VAL, AI2D-TEST, and CCBench datasets."}, {"title": "Conclusion", "content": "This paper proposes a novel model routing paradigm called capability instruction tuning with instruction-level model routing. It constructs a capability instruction comprising capabilities, instructions, and inquiry prompts to select the most suitable one. We present MODEL-SAT, featuring a capability encoder and lightweight LLM. It selects models without inference overhead of candidates and quickly adapts to new models. Model-SAT performs optimally across five proposed LLM routing settings and its multimodal extension."}]}