{"title": "Provably Efficient Exploration in Reward Machines with Low Regret", "authors": ["Hippolyte Bourel", "Anders Jonsson", "Odalric-Ambrym Maillard", "Chenxiao Ma", "Mohammad Sadegh Talebi"], "abstract": "We study reinforcement learning (RL) for decision processes with non-Markovian reward, in which high-level knowledge of the task in the form of reward machines is available to the learner. We consider probabilistic reward machines with initially unknown dynamics, and investigate RL under the average-reward criterion, where the learning performance is assessed through the notion of regret. Our main algorithmic contribution is a model-based RL algorithm for decision processes involving probabilistic reward machines that is capable of exploiting the structure induced by such machines. We further derive high-probability and non-asymptotic bounds on its regret and demonstrate the gain in terms of regret over existing algorithms that could be applied, but obliviously to the structure. We also present a regret lower bound for the studied setting. To the best of our knowledge, the proposed algorithm constitutes the first attempt to tailor and analyze regret specifically for RL with probabilistic reward machines.", "sections": [{"title": "1 Introduction", "content": "Most state-of-the-art reinforcement learning (RL) algorithms assume that the underlying decision process has Markovian reward and dynamics, i.e. that future observations depend only on the current state-action of the system. In this case, the Markov Decision Process (MDP) is a suitable mathematical model to represent the task to be solved (Puterman, 2014). However, there are many application scenarios with non-Markovian reward and/or dynamics (Bacchus et al., 1996; Brafman and De Giacomo, 2019; Littman et al., 2017) that are more appropriately modeled as Non-Markovian Decision Processes (NMDPs).\nNMDPS capture environments in which optimal action depends on events that occurred in the past, implying that the learning agent has to remember parts of the history. For example, a robot may receive a reward for delivering an item only if the item was requested previously, and a self-driving car is more likely to skid and lose control if it rained previously. Consider a mobile robot that has to track an object which is no longer in the robot's field of view. By remembering where the object was last seen, the robot has a better chance of discovering the object again. An even more precise estimation is given by the sequence of last observations (which also capture direction of movement). This can be formalized by defining high-level events that correspond to past observations.\nIn general, the future observations of an NMDP can depend on an infinite history or trace, preventing efficient learning. Consequently, recent research has focused on tractable sub-classes of NMDPs. A tractable and recently introduced sub-class is Regular Decision Processes (RDPs) (Brafman and De Giacomo, 2019, 2024), where the reward function and next state distribution is determined by conditions over the history that fall within the class of the regular languages. Another popular formalism enjoying tractability is the Reward Machine (RM) (Toro Icarte et al., 2018, 2022), which is a Deterministic Finite-State Automaton (DFA) providing a compact representation of history that compresses the entire sequence of past events into"}, {"title": "1.1 Contributions", "content": "We make the following contributions. We formalize regret minimization in MDPRMs with probabilistic machines under the average-reward criterion, and establish a first, to the best of our knowledge, regret lower bound for MDPRMs. We introduce a provably efficient algorithm called UCRL-PRM, which implements the principle of optimism in the face of uncertainty through a model-based approach, whose design is inspired by the celebrated UCRL2 algorithm (Jaksch et al., 2010), which guarantees a near-optimal regret bounds in the class of communicating MDPs without any prior knowledge on the MDP. UCRL-PRM uses high-probability and time-uniform confidence sets for unknown parameters of the underlying MDPRM and performs policy optimization over the correspondingly defined set of plausible MDPRMs. However, UCRL-PRM is carefully tailored to leverage the structure in MDPRMs, which is a key departure from UCRL2-style algorithms for MDPs. Specifically, we derive two variants of UCRL-PRM that mainly differ in the choice of confidence sets used: UCRL-PRM-L1, which uses L\u2081-type confidence sets, and UCRL-PRM-B relying on Bernstein concentration. As a result, they attain different regret bounds.\nMore precisely, we show that UCRL-PRM-L1 achieves a high-probability regret growing as $O(D_\\times \\sqrt{(Q^2E + O^2A)T})$ in an MDPRM M after T steps of interaction, with O and A being the respective size of observation and action spaces, Q denoting the number of states of the RM, and E denoting the maximum number of relevant labels at any RM state. Finally, D\u00d7 denotes the diameter of the cross-product MDP associated to M. In the case of UCRL-PRM-B, we derive a regret bound informally growing as $(\\tilde{O}(D_\\times \\sqrt{OAK + QEK'})T)$, where K and K' are the respective support size of the next-state of observations and RM states. These bounds improve over the regret bounds of baselines that scale as $\\tilde{O}(D_\\times QO\\sqrt{AT})$ and $\\tilde{O}(D_\\times \\sqrt{QOAK''T})$ (for some K'' > max{K, K'}).\nIn addition, we establish refined regret bounds for UCRL-PRM in the case of deterministic machines. Specifically, we show that in an MDPRM M with deterministic RM, UCRL-PRM-L1 (resp. UCRL-PRM-B) achieves a regret growing as $\\tilde{O}(\\sqrt{c_MOAT})$ (resp. $\\tilde{O}(\\sqrt{C'MOAT})$), where c\u043c and c'm are problem- dependent quantities defined in terms of a novel notion of connectivity in MDPRMs, which we call the RM-restricted diameter. This notion is a problem-dependent refinement of the diameter DX of the cross- product MDP associated to M. The RM-restricted diameter of M reflects the connectivity in M jointly determined by the dynamics and the sparsity structure of the RM, and we believe it could be of interest in other settings of reward machines. The RM-restricted diameter is always smaller than DX, and in some MDPRM instances, it is proportional to DX/Q.\nAlthough the design and analysis of UCRL-PRM build on UCRL2 and its variants, we stress that there are some non-trivial components. First, directly using the policy optimization procedure of UCRL2-style algorithms would require solving a bilinear program, which is NP-hard in general. To circumvent this issue, we perform policy optimization over a surrogate set of candidate MDPRMs, which entails solving a linear"}, {"title": "1.2 Related Work", "content": "In the case of Markovian rewards and dynamics, there is a rich literature on average-reward RL, presenting several algorithms with theoretical regret guarantees. While there is an abundance of work on the tabular case (i.e., without structural assumptions), there is a well-growing line of work on structured RL. For the former category we mention (Burnetas and Katehakis, 1997; Jaksch et al., 2010; Fruit et al., 2018a; Talebi and Maillard, 2018; Wei et al., 2020; Bourel et al., 2020; Zhang and Ji, 2019; Pesquerel and Maillard, 2022; Saber et al., 2023)), whereas some the latter one include (Wei et al., 2021; Ok et al., 2018; Talebi et al., 2021; Lakshmanan et al., 2015). In the absence of structure assumptions, as established by Jaksch et al. (2010), no algorithm can have a regret lower than $\\Omega(\\sqrt{ DSAT})$ in a communicating MDP with S states, A actions, diameter D, and after T steps of interactions. The best available regret bounds for communicating MDPs, achievable by computationally implementable algorithms, grow as $O(\\sqrt{DSAKT \\log(T)})$ (Fruit et al., 2020) or as $O(D\\sqrt{KSAT \\log(T)})$ (Fruit et al., 2018a), where K denotes the maximal number of next-states under any state-action pair in the MDP. Recently, Boone and Zhang (2024) present an algorithm achieving a regret of $O(DSAT \\log(T))$, albeit with an additive term scaling as $5/2T9/20 making the bound less interesting.\nThe progress in the domain of non-Markov RL has been substantially slower than the Markovian counterpart due to challenges posed by history dependence. A generic NMDP, with rewards and transition function arbitrarily depending on the history, is not PAC learnable. Nevertheless, there is already a broad literature in various sub-classes of NDMPs that admit some form of tractability, making learning a possibility. A line of such work tackle the problem of state-representation, where the agent must select a representation of the environment (i.e., a mapping from histories to a discrete state-space) from an input set (Lattimore et al., 2013; Maillard et al., 2013; Sunehag and Hutter, 2015). Although these algorithms could be applied to RMs, they do not exploit the particular structure of the underlying RMs, and hence the resulting theoretical bounds may grow prohibitively large. As a result, state representation learning algorithms render impractical for learning RMs.\nRDPs (Brafman and De Giacomo, 2024) constitute another tractable class of NMDPs, which can be modeled via some unobservable DFA. More precisely, the automaton state of an RDP may be viewed as a hidden information state (Subramanian et al., 2022), and as shown by Brafman and De Giacomo (2024), any RDP is also a Partially-Observable Markov Decision Process (POMDP) (Kaelbling et al., 1998), whose hidden dynamics evolve according to its finite-state automaton. As a result, RMs may fall into the class of RDPs; however, they are simpler to learn because of full observability. RL in RDPs is a recent under-taking and remains mostly unexplored. The S3M algorithm of Abadi and Brafman (2020) integrates RL with the logical formulas of RDPs, but does not admit polynomial sample complexity in the PAC setting. Ronca and De Giacomo (2021) present the first online RL algorithm for RDPs whose PAC sample complexity grows polynomially in terms of the underlying parameters, though the sample complexity bound is prohibitively large in the relevant parameter. Recently, Cipollone et al. (2024) introduced RegORL, a provably efficient algorithm for offline RL in RDPs with near-optimal sample complexity. Nevertheless, none of these work could be used to control exploration in RMs with provable regret guarantees.\nResearch on reward machines is relatively recent, but has grown quickly in popularity and already attracted many researchers to the field. Initial research focused on proving convergence guarantees for RL algorithms specifically devised for RMs (Toro Icarte et al., 2018, 2022)."}, {"title": "1.3 Organization", "content": "The remaining of the paper is organized as follows. In Section 2, we introduce the notion of MDPRM under the average-reward criterion and formulate the corresponding regret minimization problem. We present two variants of the UCRL-PRM algorithm in Section 3, and report high-probability and finite-time bounds on their regret in Section 4. A regret lower bound for MDPRMs with deterministic machines is derived in Section 5. Finally, Section 6 concludes the paper and provides some some future research directions. Proofs as well as some algorithmic details are presented in the appendix."}, {"title": "2 Problem Formulation", "content": ""}, {"title": "2.1 MDPRMs: Average-Reward Markov Decision Processes with Reward Machines", "content": "We begin with introducing some necessary background on labeled Markov decision processes and reward machines. We introduce notations that will be used throughout. Given a set A, AA denotes the simplex of probability distributions over A. A* denotes (possibly empty) sequences of elements from A, and A+ denotes non-empty sequences. IIA denotes the indicator function of event A."}, {"title": "2.1.1 Labeled Markov Decision Processes", "content": "A labeled average-reward MDP is a tuple M = (O, A, P, R, AP, L), where O is a finite set of (observation) states with cardinality O, A is a finite set of actions available at each state with cardinality A, P : O \u00d7 A \u2192 Ao is the transition function such that P(o'lo, a) denotes the probability of transiting to state o' \u2208 O, when executing action a \u2208 A in state o \u2208 O. R : (O\u00d7A)+ \u2192 \u2206[0,1] denotes a history-dependent reward function such that for every history h \u2208 (O\u00d7A)* \u00d7 O and action a \u2208 A, R(h, a) defines a reward distribution. Further, AP denotes a set of atomic propositions and L : O\u00d7A\u21922AP denotes a labeling function assigning a subset of AP to each (o, a). These labels describe high-level events associated to the various (o, a) pairs that can be detected from the environment, and they prove instrumental in defining the history-dependent reward function R.\nThe notion of M above coincides with the conventional notion of average-reward MDPs except that (i) it assumes a non-Markovian reward function and (ii) it is equipped with a labeling mechanism (defined via L and AP). The interaction between the agent and the environment M proceeds as follows. At each time step t\u2208 N, the agent is in state ot \u2208 O and chooses an action at \u2208 A based on ht := (01, A1, ..., Ot\u22121, at-1, Ot). Upon executing at in ot, M generates a next-state ot+1 ~ P(\u00b7|ot, at) and assigns a label ot = L(ot, at). Then, the agent receives a reward rt ~ R(ht, at). Then, the state transits to Ot+1 and a new decision step begins. As in MDPs, after T steps of interactions, the agent's cumulative reward is $\\sum_{t=1}^T t$"}, {"title": "2.1.2 (Probabilistic) Reward Machines", "content": "We restrict attention to a class of non-Markovian reward functions that are encoded by RMs (Toro Icarte et al., 2022; Dohmen et al., 2022), whose definitions are inspired by conventional DFAs. In this work, we consider probabilistic RMs (Dohmen et al., 2022). A probabilistic RM is a tuple R = (Q, 2AP, \u315c, v), where Q is a finite set of states and 2AP is an input alphabet. \u315c : Q\u00d7 2AP \u2192 \u2206o denotes a transition function such that \u315c(q'q, \u03c3) denotes the probability that R transits to q' \u2208 Q when an input o is received in state q, with the convention that \u315c(q, 0) =q. Finally, v : Q\u00d72AP \u2192 \u2206[0,1] denotes the output function of R, which returns a distribution over [0, 1] for any (q, 0). Let Eq be the set of relevant labels at q \u2208 Q and Eq be its cardinality. Further, define E := maxq Eq. Note the labeling function is not necessarily one-to-one; i.e., there might exist two or more observation-action pairs generating the same label.\nIn the case of deterministic transition function \u315c : Q \u00d7 2AP \u2192 Q, R coincides with the conventional notion of RM considered in Toro Icarte et al. (2022). In this paper, we use RM to refer to both deterministic and probabilistic machines. In words, the RM R converts a (sequentially received) sequence of labels to a sequence of Markovian reward distributions such that the output reward function at time t is v(qt, \u03c3\u03c4), and it thus only depends on the current state qt and current label \u03c3\u03c4. Conditioned on (qt, \u03c3\u03c4), the reward distribution at time t is independent of earlier labels and RM states (q1,01,\u06f0\u06f0\u06f0, qt-1,0-1). Thus, RMs provide a compact representation for a class of non-Markovian rewards that can depend on the entire history."}, {"title": "2.1.3 Average-Reward MDPs with RMs", "content": "Restricting the generic history-dependent reward function R to RMs leads to decision processes that are often termed MDPRMs. Formally, an average-reward MDPRM is a tuple M = (O, A, P, R, AP, L), where O, A, P, AP, and L are defined as in (labeled) average-reward MDPs, and where R is an RM, which generates reward functions. The agent's interaction with an MDPRM M proceeds as follows. At each time t\u2208 N, the agent observes both ot\u2208 O and qt \u2208 Q, and chooses an action at \u2208 A based on ot and qt as well as (potentially) her past decisions and observations. The environment reveals an event ot = L(ot,at). The RM R, being in state qt, receives \u03c3\u03c4 and outputs a reward distribution \u03bd(qt, \u03c3\u03c4) \u2208 \u2206[0,1]. Then, the agent receives a reward rt ~ v(qt, \u03c3\u03c4) (at the end of the current time step). Then, the environment and RM states transit to their next states Ot+1 ~ P(\u00b7|ot, at) and qt+1 ~ \u315c(\u00b7|qt, \u03c3\u03c4), and a new step begins. This is summarized in Figure 1."}, {"title": "2.2 Regret Minimization in MDPRMS", "content": "We are now ready to formalize RL in MDPRMs in the regret minimization setting, which is the main focus of this paper. As in tabular RL, it involves an agent who is seeking to maximize its cumulative reward, and its performance is measured in terms of regret with respect to an oracle algorithm who knows and always applies a gain-optimal policy. To formally define regret, we introduce some necessary concepts. A stationary deterministic policy in an MDPRM M is a mapping \u3160 : S \u2192 A prescribing an action \u03c0(q, o) \u2208 A for all (q, o) \u2208 S. Let II be the set of all such policies in M. The long-term average-reward (or gain) of policy"}, {"title": "3 Learning Algorithms for MDPRMs", "content": "In this section, we present algorithms for learning in MDPRMs that fall into the category of model-based algorithms. In general, the equivalence between MDPRM M and its associated cross-product MDP MX implies that one could apply any off-the-shelf algorithm to MX, as it perfectly adheres to the Markovian property. This implies that provably efficient algorithms designed for average-reward MDPs such as UCRL2 (Jaksch et al., 2010) and its variants (e.g., (Fruit et al., 2020, 2018b; Bourel et al., 2020)) could be directly applied to MX while guaranteeing sublinear regret performance. However, this may lead to inefficient exploration, and thus large regret, since these generic algorithms are oblivious to the special structure of MX. Nevertheless, characterization of MX can indeed be used as a proxy to develop learning algorithms for MDPRM.\nTo simplify exposition, we assume that the reward distributions v of the RM are known. This assumption can be easily relaxed at the expense of a slightly increased regret. We discuss in Appendix C how to tailor the algorithms to the case of unknown rewards."}, {"title": "3.1 Confidence Sets", "content": "We begin with introducing empirical estimates and confidence sets used by the algorithms. We first present confidence sets for observation dynamics P and RM dynamics \u315c, and then show how they yield confidence sets for the transition and reward functions of the cross-product MDP MX."}, {"title": "3.1.1 Confidence Sets for Observation Dynamics P", "content": "Formally, under a given algorithm and for any o, o' \u2208 O and a \u2208 A, let Nt(o, a, o') denote the number of times a visit to (o, a) was followed by a visit to o', up to time t: $N_t(o, a, o') := \\sum_{i=1}^t \\mathbb{I}\\{(o_i,a_i,o'_{i+1})=(o,a,o')\\}$. Further, Nt(0, a) := max{1, \u2211\u03bf, Nt(o, a, o')}. Using the observations collected up to t > 1, we define the empirical estimate $P_t(o'|o, a) = \\frac{N_t(o,a,o')}{N_t(o,a)}$ for P(o'lo, a). We consider two confidence sets for P. The"}, {"title": "3.1.2 Confidence Sets for RM State Dynamics \u315c", "content": "Formally, under a given algorithm and for any q, q' \u2208 Q and \u03c3 \u2208 Eq, let N\u2081(q, o, q') denote the number of times a visit to (q, o) was followed by a visit to q', up to time t: N\u2081(q, o, q') := \u2211i=1\u2161{(9i,&i,qi+1)=(9,0,qi)}. Further, Nt(q, \u03c3) := max{1,\u03a3, Nt(q, \u03c3, q')}. Using the observations collected up to t > 1, we define the empirical estimate $T_t(q'|q, 0) = \\frac{N_t(q,\u03c3,q')}{N_t(q,\u03c3)}$ for (q'\\q, \u03c3).\nSimilarly to the case of P, we consider two confidence sets for \u315c. The first one is built using time-uniform Weissman's concentration inequality:"}, {"title": "3.1.3 Set of Plausible Models", "content": "Either choice of confidence sets introduced above, (C1, D\u00b9) or (C2, D2), yields a set of MDPRMs that are plausible with the collected data up to any time step. More formally, consider a time step t\u2265 1 and a confidence parameter \u03b4 \u2208 (0, 1). Given a set D, let RD be the set of RMs defined using transition functions collected in D, i.e., Rp := {R' = (Q,2AP, \u03c4', \u03bd) : \u03c4' \u2208 D}. Equipped with this, we build the set of MDPRMS"}, {"title": "3.2 From Confidence Sets to Algorithms: UCRL-PRM-L1 and UCRL-PRM-B", "content": "Equipped with the confidence sets presented above, we are ready to present an algorithm, called UCRL-PRM, for learning in MDPRMs. We consider two variants of UCRL-PRM depending on which confidence set is used: The variant using (C\u00b9, D\u00b9), called UCRL-PRM-L1, can be seen as an extension of UCRL2 (Jaksch"}, {"title": "4 Theoretical Regret Guarantees", "content": "In this section, we present finite-time regret bounds for the two variants of UCRL-PRM that hold with high probability. We present regret bounds for both probabilistic and deterministic RMs. First, we introduce a notion of diameter that renders relevant for RMs."}, {"title": "4.1 RM-Restricted Diameter", "content": "As in tabular MDPs, regret performance of an RL algorithm in average-reward MDPRMs would depend on some measure of connectivity. Specifically, for MDPRMs with communicating cross-product MDPs, diameter notions render most relevant. We distinguish between two notions of diameter for MDPRMs. The first one, denoted by DX, is defined as the diameter of the cross-product MDP associated to the considered MDPRM, coinciding with the classical definition of diameter, which we recall below for completeness:\nDefinition 1 (Jaksch et al. (2010)) The diameter DX of an MDP MX is defined as\n$D_X := \\max_{s\\neq s'} \\min_{\\pi} E[T^{\\pi}(s, s')]$,\nwhere T\u03c0 (s, s') is the number of steps it takes to reach s' \u2208 S starting from s \u2208 S and following policy \u3160: \u03a3 \u2192 \u0391.\nThe second one is a novel notion, which we shall call RM-restricted diameter, and is tailored to the structure of MDPRMs. To formalize it, let us introduce for s = (q, o) \u2208 S,\n$B_s := \\cup_{a} \\{q' \\in Q : \\tau(q'|q, L(o, a)) > 0\\}$.\nIntuitively, for a given s = (q, 0), Bs Q collects all possible next-states of the RM that can be reached from q via the detectable labels in o. In the worst-case, one has B = Q for some state s. However, many high-level tasks in practice often admit RMs with sparse structures, where B may be a small subset of Q (cardinality-wise). Equipped with this, we define the RM-restricted diameter for state s = (q, o) \u2208 S:\nDefinition 2 (RM-Restricted Diameter) Consider state s=(q, o) \u2208S. For $s_1, s_2 \\in B_s \\times O$ with $s_1 \\neq s_2$, let T\u03c0 (s1, s2) denote the number of steps it takes to reach s2 starting from s\u2081 and following policy \u03c0: S \u2192 A. The RM-restricted diameter of MDPRM M for state s is defined as\n$D_s := \\max_{S_1,S_2 \\in B_s \\times O} \\min_{\\pi} E[T^{\\pi}(s_1, s_2)]$.\nIt is evident that Ds <D\u00d7 for all s\u2208 S, in view of Bs \u2286 Q. Further, if Bs = Q for some state s, then the RM-restricted diameter for s coincides with DX. Since B, might be a proper (and possibly small) subset of Q, Ds is a problem-dependent refinement of DX. It is worth noting that a small Bs does not necessarily imply that Ds \u226aD\u00d7 as Ds is determined by both Bs and the transition function Px of MX. Interestingly, however, there exist cases where Ds \u2264 D\u00d7 /Q, as we illustrate next.\nConsider the MDPRM shown in Figure 3, where there are two observation states 00 and 01, with identical transition probabilities parameterized by \u03b4 \u2208 (0, 1). In 00, there is one action, but no event. In 01, there are two actions: ao resulting in detecting \u03c3\u03b1, and a\u2081 that leads to \u03c3\u03b2. The underlying (deterministic)"}, {"title": "4.2 Regret Bounds", "content": ""}, {"title": "4.2.1 Probabilistic Reward Machines", "content": "First, we present regret bounds for UCRL-PRM in the case of MDPRMs with probabilistic RMs.\nThe following theorem provides a regret bound for UCRL-PRM-L1, which is constructed using (C1, D1):\nTheorem 1 Under UCRL-PRM-L1, with probability higher than 1 \u2013 3\u03b4 and uniformly over all T > 2,\n$R(T) \\leq D^* \\sqrt{OAT (O + \\log(T/\\delta))} + D_X \\sum_{i \\in E_t} \\sqrt{ET(Q + \\log(T/\\delta))} + D^*(OA + QE) \\log(T)$"}, {"title": "4.2.2 Deterministic Reward Machines", "content": "Now we restrict attention to the special class of deterministic RMs and present improved regret bounds. In the case of deterministic RMs, it is evident that there will be no need to maintain a confidence set for \u315c. Further, it will no longer be necessary to use a surrogate set of models. Hence, the variants of UCRL-PRM reduce to their respective form of UCRL-RM, which were initially presented in our previous work (Bourel et al., 2023). We will refer to this special case of UCRL-PRM as UCRL-RM, to comply with the terminology used in (Bourel et al., 2023).\nTheorem 3 Given an MDPRM M, let $c_M = \\sum_{q \\in Q} \\max_{0 \\in O} D^2_{q,0}$. Uniformly over all T > 2, with probability higher than 1 \u2013 3\u03b4, the regret under UCRL-RM-L1 in M satisfies\n$R(T) \\leq \\sqrt{c_MAT (O + \\log(T/\\delta))} + D^* \\sqrt{T\\log(T/\\delta)}.$\nTheorem 4 Given an MDPRM M, let $c'_M = \\sum_{O \\in O,\\sigma \\in A} K_{o,a} \\max_{q \\in Q} D^2_{q,0}$. Uniformly over all T > 2, with probability higher than 1 \u2013 3\u03b4, the regret under UCRL-RM-B in M satisfies\n$R(T) \\leq \\sqrt{c'_MT \\log(\\log(T)/\\delta)} + + D^* \\sqrt{T \\log(\\log(T)/\\delta)}.$\nHere, cm and c'M are problem-dependent quantities that reflect the contribution of various states to the regret, weighted by their associated RM-restricted diameter. In the worst-case, $c_m < OD_X^2$ and $c'_m = D_X^2 \\sum_{o,a} K_{o,a}$. But in view of the example earlier, there are problem instances in which $c_m \\leq OD_X^2/Q^2$ and $c'_m \\leq D_X^2/Q^2 \\sum_{o,a} K_{o,a}$. Therefore, these quantities could adapt to the sparsity structure of the underlying MDPRM. In contrast, the reported regret bounds for the case of probabilistic RMs would scale with the diameter DX of the cross-product MDP, which is structure-oblivious and (potentially much) larger. It is an interesting direction to derive similar regret bounds for probabilistic RMs."}, {"title": "4.3 Discussion", "content": "Any algorithm available for tabular RL could be directly applied to MX, obliviously to the structure induced by the RM. In particular, UCRL2 (Jaksch et al., 2010) attains a regret of scaling as D\u00d7OQ\u221aAT log T, although its regret with improved confidence sets used here would grow as D\u00d7 \u221aAOQT(OQ + log T).\nFurther, UCRL2-B achieves a regret of $O(D_\\times \\sqrt{T \\log(\\log(T))} \\sum_{o,a} K^{\\vartheta}_{o,a}))$.\nIt would render natural to compare UCRL-PRM-L1 with UCRL2, and UCRL-PRM-B with UCRL2-B, in terms of regret dependency on problem parameters O, Q, and A. Under UCRL-PRM-L1, the regret order is \u221a(OA + \u03a3\u03b1 Eq)T log(T) for large time horizon T (relative to O and Q), whereas it is or \u221a(O2A + Q\u03a3, E\u2084)T for moderate T. In contrast, for UCRL2 it scales as OQ \u221aAT log(T). It is clear that a dependency on QO is improved to one of the form Q + O, especially when Eq \u226a Q and A \u00ab O, which reasonably hold in practical situations. A similar remark holds when comparing to the improved regret of UCRL2. In the case of UCRL-PRM-B compared with UCRL2-B, a dependency on the (cumulative) support size of P\u00d7 is traded with Ko,a + Kq,o, that is, the support sizes of P and \u315c. The quantities Ko,a and Kq, are more capable of representing the sparsity of MDPRM than the support size of P\u00d7.\nIn the case of deterministic RMs, UCRL-RM-L1 improves over UCRL2 by a multiplicative factor of Q. However, in some specific instances, we have Ds \u2264 D\u00d7 /Q for all s, so that c\u043c ~ O(D\u00d7/Q)\u00b2 for such M. On such MDPRMs, for moderate T, we obtain an improvement in the regret bound by a multiplicative factor of at least Q, but in some examples this can be as large as Q2. For large horizons (relative to O), the respective gains over UCRL2 are \u221aQ and Q3/2.\nIn view of D\u300f < D*, c'm \u2264 D\u00d72 \u03a3\u03bf,\u03b1 Ko,a. Hence, the regret of UCRL-RM-B, in the worst case grows as DX \u03a3\u03bf,\u03b1 Ko,aT. However, in some specific instances, we have D\u300f \u2264 D\u00d7 /Q for all s, thus yielding $C'_M \\leq (D_\\times/Q)^2 \\sum_{o,\\alpha} K_{o,\\alpha}$. On such MDPRMs, its regret is of order $\\sum_{o,\\alpha} K_{o,\\alpha}T$. In summary, UCRL-RM-B improves UCRL2-B in regret by a factor of, at least, \u221aQ. Moreover, in instances where Ds \u2264 DX/Q, the improvement could be as large as a factor of Q3/2."}, {"title": "5 Regret Lower Bound", "content": "In this section, we present a regret lower bound for the class of MDPRMs under the communicating assumption on the associated cross-product MDP. For communicating tabular MDPs with S states, A actions, and diameter D, a regret lower bound of (\u221aDSAT) is presented by Jaksch et al. (2010), which relies on a carefully constructed family of worst-case MDPs. However, this does not translate to a lower bound of (\u221aD\u00d7QOAT) for the cross-product M\u00d7 associated to a given MDPRM M. This is due to the fact that the transition function of the aforementioned worst-case MDPs does not satisfy (1). In other words, there exist no MDPRMs for which those worst-case MDPs become their associated cross-product MDPs. In the following theorem, we present a regret lower bound that holds for any MDPRM M with a communicating cross-product MX.\nTheorem 5 For any O \u2265 3, A >\u2265 2, Q \u2265 2, and $D_\\times > Q(6 + 2\\log_A(O))$, T > D\u00d7OA and E \u2265 4, there exists a family of MDPRMs with deterministic RMs comprising O observations states, A actions, Q RM states, and diameter D\u00d7 of the associated M\u00d7, in which the regret of any algorithm A satisfies\n$\\mathbb{E}[R(A, T)"}]}