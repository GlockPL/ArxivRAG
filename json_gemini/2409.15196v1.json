{"title": "HOTVCOM: Generating Buzzworthy Comments for Videos", "authors": ["Yuyan Chen", "Yiwen Qian", "Songzhou Yan", "Jiyuan Jia", "Zhixu Li", "Yanghua Xiao", "Xiaobo Li", "Ming Yang", "Qingpei Guo"], "abstract": "In the era of social media video platforms, popular \"hot-comments\" play a crucial role in attracting user impressions of short-form videos, making them vital for marketing and branding purpose. However, existing research predominantly focuses on generating descriptive comments or \"danmaku\" in English, offering immediate reactions to specific video moments. Addressing this gap, our study introduces HOTVCOM, the largest Chinese video hot-comment dataset, comprising 94k diverse videos and 137 million comments. We also present the ComHeat framework, which synergistically integrates visual, auditory, and textual data to generate influential hot-comments on the Chinese video dataset. Empirical evaluations highlight the effectiveness of our framework, demonstrating its excellence on both the newly constructed and existing datasets.", "sections": [{"title": "1 Introduction", "content": "With the increasing prevalence of video content on digital platforms, there is an evident significance of video comments in amplifying video reach (Ren et al., 2024; Chen et al., 2024d). Specifically, \u201chot-comments\" have the potential to attract considerable user interaction, substantially increasing a video's user impressions, which is essential for product marketing and branding. A typical hot-comment often meets specific standards: receiving a larger number of likes and replies, being highly pertinent to the video content, and including the elements that resonate with viewers.\nHowever, the prevailing literature, such as the work by Ma et al. (2019) and Wang et al. (2020),\nlargely concentrates on generating descriptive comments or \"danmaku\", which are closely tied to certain video moments, offering instantaneous reactions. While these comments do engage viewers to some extent, their potential to highlight the entire short-video content or foster deep user interactions is somewhat limited. When aiming to elevate a video's visibility, these real-time reactions might not be as influential as those hot-comments. Additionally, most research, including that by Sun et al. (2023b), predominantly focuses on English comments, leaving a gap in the domain of Chinese hot-comment generation. Recently, large language models (LLMs) have played a significant role across various fields, making it feasible to use them for generating hot comments (Xiong et al., 2024; Chen et al., 2023d, 2024a, 2023e,b, 2024b). Due to the lack of large-scale training datasets, we first construct a comprehensive Chinese video hot-comment generation dataset, HOTVCOM, which includes video titles, descriptions, captions, audio speeches, keyframes, and engagement records. Compared to the English dataset from Sun et al. (2023b), our dataset HOTVCOM is more comprehensive, encompassing 93k videos with 137 million comments, making it the largest of its kind.\nIn addition, for the convenience of analysis and targeted generation of comments, videos in HOTVCOM are further categorized into different themes. Such a categorization helps language models to understand the context, therefore generating hot-comments that resonate with specific themes accordingly.\nA key challenge in video hot-comment generation lies on assessing whether the generated comments are truly impressive to users to boost the interaction. The existing work primarily utilizes ROUGE (Ma et al., 2019; Wang et al., 2020; Chen et al., 2024f) or the number of likes (Sun et al., 2023b) as metrics. However, these metrics might not always reflect the genuine engagement of a comment (Chen et al., 2023c, 2024c). In this paper, we propose a novel comprehensive evaluation metric including the informativeness, relevance, creativity besides user engagement (i.e. likes and replies) of a comment, which thus reveals a well-rounded understanding of a comment's \u2018hotness'. With the guidance of our evaluation metrics, we then propose a novel video hot-comment generation framework named ComHeat. We implement the Supervised Fine-Tuning technique to generate preliminary comments and then enhance them with reinforcement learning. By incorporating knowledge-enhanced Tree-of-Thought method, the comments are further refined to improve the chance of their popularity.\nIn summary, our contributions are:\n\u2022 We work on a novel task namely video hot-comment generation. To achieve this, we construct the largest Chinese video comment dataset including 93k videos with 137 million comments, named HOTVCOM.\n\u2022 We introduce a novel comprehensive evaluation metric for video hot-comments generation, including informativeness, relevance, creativity, and user engagement, bridging the gap in qualitative comment analysis.\n\u2022 We propose the ComHeat framework, which incorporates visual, auditory, and textual aspects, for generating engaging comments for Chinese short videos using reinforcement learning and Tree-of-Thought.\n\u2022 Empirical results showcase that our ComHeat framework outperforms existing baselines on the newly-constructed dataset and also excels"}, {"title": "2 Datasets", "content": "In this section, we construct a large-scale Chinese short video comment dataset named HOT VCOM including 94k short videos from Douyin with 137 million comments, where the entire process is shown in Fig. 2. We also conduct an extensive exploratory data analysis as shown in Fig. 5.\nWe initially collect Douyin videos from various themes in reverse chronological order up till 100k. Next, we conduct Optical Character Recognition (OCR) with PaddleOCR library \u00b9 and Automatic Speech Recognition (ASR) with Xunfei open platform 2 on the videos to obtain their video captions and audio speech, respectively, and capture key frames of videos with the K-means clustering algorithm. After that, we also adopt PaddleOCR to extract the video's title, creation time, publishers' profile and engagement information, which includes the number of likes, comments, shares, and favorites, as well as each comment with its content, commenter's profile and engagement information, which includes the number of likes and replies. Furthermore, from the tags marked with \"#\" in titles and themes provided by the Douyin platform, we categorize the videos with GPT-4 3 into 20 themes, including pets, food, etc. We also provide the descriptions for the video content and keyframes, encompassing scenes, objects, primary actions, atmosphere, and emotions with the help of video-ChatGPT (Li et al., 2023) and miniGPT4 (Zhu et al., 2023), respectively. More details are shown in Appendix A.\nTo maintain comment quality, we filter out comments with emojis, ASCII characters below 127, and those with less than 1 character or more than 50 characters. We also remove the comments with profanity, political content, negative tones, or promotional intent. The short videos with 0 or 1 comment are also discarded to avoid long-tail bias. In the end, we have 94k videos with 137 million comments, including video captions, audio texts, keyframes, engagement information, etc. The statistics of HOTVCOM are shown in Table 1. On average, these videos are around 96.44 seconds long, indicating a preference for 1-2 minute content. The titles and descriptions average 43.62 and 419.22"}, {"title": "3 Evaluation", "content": "We develop comprehensive evaluation metrics for hot-comments from four main aspects: informativeness, relevance, creativity, and user engagement.\nThe informativeness score $I$ quantifies the utility of the information conveyed by a comment from lengths penalty and vocabulary diversity. Lengths penalty, denoted as $L_p$, quantify the lengths appropriateness of a comment. Vocabulary diversity, denoted as $V_d$, is calculated as the ratio of total bigrams to unique ones of this comment. The calculation process of informativeness score is as follows:\n$I_p = \\begin{cases}\nL/L_{min} & \\text{if } L < L_{min} \\\\\nL/L_{max} & \\text{if } L_{min} < L \\leq L_{max} \\\\\n1 - \\alpha \\times (L - L_{max}) & \\text{if } L > L_{max},\n\\end{cases}$   (1)\n$V_d = \\frac{T_n}{U_n}$,\n$I = w_1 \\times L_p + w_2 \\times V_d$,  (2)\nwhere $L$ denotes the actual length of a given comment, $L_{min}$ and $L_{max}$ specify the optimal length boundaries of the comment length, which are set at 1 and 50, respectively, meaning that the comment length ranges from a minimum of 1 to a maximum of 50 characters. The constant $\\alpha$, between 0 and 1, adjusts penalties for a comment that go beyond the optimal length. $T_n$ and $U_n$ signifying total and unique bigrams of a comment, respectively. $w_1^I$ and $w_2^I$ are trainable weights.\nThe relevance score $R$ quantifies the alignment of a comment to the video content through two primary dimensions: keyword and context matching degree. The keyword matching degree, denoted by $D_k$, is the proportion of words in a comment resonating with the keywords of video captions that are extracted with ChatGPT. The context matching degree, denoted by $D_c$, is derived from cosine similarity between video captions and a corresponding comment. The calculation process of relevance score is as follows:\n$D_k = \\frac{N_x}{N_k}$, (3)\n$D_c = \\frac{Com \\cdot Vid}{||Com||^2 \\times ||Vid||^2}$, (4)\n$R = w_1^R \\times D_k + w_2^R \\times D_c$,\nwhere $N_x$ denotes the number of words or phrases in a comment that matches the keywords in video captions, and $N_k$ is the total number of keywords extracted from the video captions with ChatGPT."}, {"title": "4 Methods", "content": "We propose a Chinese video hot-comments generation framework named ComHeat as shown in Fig. 3. Initial comments are first generated by the LLMs through supervised fine-tuning. Then we train a reward model based on the comprehensive score of comments and adopt reinforcement learning to refine the popularity of the generated comments. Finally, we utilize knowledge-enhanced Tree-of-Thought method for further optimization to generate hotter comments.\n4.1 Visual Feature Extraction\nThe aim of this step is to bridge the semantic gap between videos and comments. We first leverage embeddings from key video frames inspired by the previous work (Alayrac et al., 2022; Tao et al., 2024). Next, recognizing the significance of event progression in videos, we preserve the keyframe sequence via positional embeddings. This ensures the sequence's essence and progression are encapsulated for subsequent processing. Mathematically:\n$F_k = LVM(k), S = \\sum_{k \\in K} F_k \\oplus P_k$, (10)\nWhere $F_k$ represents features from the k-th keyframe, extracted by LVM, $P_k$ represents the positional embedding for the k-th keyframe, S represents the serialized sequence of keyframe embeddings, and K represents a video's keyframe set.\n4.2 Supervised Fine-Tuning\nWe leverage an LLM, such as baichuan2-13B (Yang et al., 2023a), for Supervised Fine-Tuning (SFT) with the aim of generating hot-comments that resonate with the ground truths. The input encompasses both textual information (titles, video captions, audio speeches, video descriptions)"}, {"title": "5 Experiments", "content": "In this section, we conduct extensive experiments to evaluate the performance of our proposed ComHeat framework in comparison to other baselines on generating hot-comments for HOTVCOM.\n5.1 Experimental Setups\nOur experiments are conducted on four Nvidia A100 GPUs, each with 80GB of memory, using PyTorch 4 in Python 5. For enhanced training efficiency, we utilize DeepSpeed. We set the maximum sequence length for both input and output sequences to maximum 1024 tokens. The training process is set to 10 epochs. We list detailed training hyperparameters in Table 10 in the Appendix.\n5.2 Datasets, Baselines and Metrics\nWe utilize four datasets and eleven baselines for comparison with details shown in Appendix D. All results are reported on the corresponding test sets or 20% subset split from the original dataset. For public datasets, including VideoIC, Livebot, and MovieLC, the tests are conducted directly on these public datasets without training. For our self-collected datasets, which include HOTVCOM and the TikTok dataset, other baseline models have undergone training on our datasets."}, {"title": "6 Related Work", "content": "6.1 Video Comment Generation\nRecent research predominantly emphasizes live or synchronized video comments like danmaku. Systems like GraspSnooker by Sun et al. (2019) and the rap-style generator by Jumneanbun et al. (2020) exemplify this trend. Major datasets like VideoIC by Wang et al. (2020) and innovations like the open-domain approach by Marrese-Taylor et al. (2022) have been introduced. While Chen et al. (2023a) targets long videos, Ma et al. (2019) merges visual and textual contexts. Our focus diverges towards generating alluring comments for full videos. Despite Sun et al. (2023b)'s contributions with the ViCo-20k dataset, they lack comprehensive engagement metrics. Notably, datasets like LiveBot (Ma et al., 2019) are live-comment centric, and Sun et al. (2023b)'s dataset, being English, might be less fitting for Chinese scenarios.\n6.2 Video Caption Generation\nResearch on video caption generation has seen diverse approaches, which is widely used in various scenarios (Li et al., 2024b,c,a). Qi et al. (2023) launched GOAL, emphasizing Knowledge grounded Video Captioning (KGVC). Techniques like attention-based learning have been explored by Ji et al. (2022), while Song et al. (2022) introduce the Contextual Attention Network (CANet) for context-rich learning. Meanwhile, Yan et al. (2022) and Babavalian and Kiani (2023) offer unique frameworks for improved caption relevance and diversity. Yang et al. (2023b) propose a weighted semantic model, VMSG. Among multimodal language model advancements (Chen and Xiao, 2024), our work generate richer video descriptions, aiding in effective comment generation."}, {"title": "7 Conclusions and Future Work", "content": "In conclusion, this study underscores the crucial role of \"hot-comments\" in enhancing video visibility, setting them apart from the prevalent \u201cdanmaku\u201d comments. Through the introduction of the extensive Chinese video hot-comment dataset HOTVCOM and the innovative ComHeat framework, we offer a novel approach to generate relevant and engaging video comments. In the future, we plan to ensure the ethics and fairness of the ComHeat framework from equal access and fair algorithms. We will provide low-cost or free tools and services, enabling smaller brands to enhance their video visibility. We also intend to include even more categories, especially those that are more niche. Moreover, we will explore cross-lingual perspectives for video comment generation.\nLimitations\nWhile our research offers promising advancements in video hot-comment generation, it also presents certain limitations. First, our approach predominantly caters to Chinese short videos, which may constrain its applicability in diverse linguistic and cultural contexts. Second, the reliance on the ComHeat framework assumes that visual, auditory, and textual data are always present and of high quality, which might not always be the case in real-world scenarios. Furthermore, the optimization techniques employed, though effective, may not capture the full depth of human creativity. Lastly, while the dataset we introduce is comprehensive, it is inherently subject to the biases and characteristics of its source, potentially affecting the generalizability of our findings.\nEthic Statement\nThroughout the course of our research, we have maintained unwavering commitment to the highest ethical standards. Our foremost priorities include ensuring transparency, fairness, and the utmost respect for all participants involved in this study. We have taken extensive measures to safeguard user identities and protect privacy through a meticulous anonymization process applied to all data within our dataset. Our overarching objective is to enrich the user experience and interactions on video platforms while simultaneously upholding the principles of individual rights and human dignity. In a world where the influence of AI and technology continues to expand, we remain acutely aware of the profound impact these innovations can have on society as a whole. It is essential to acknowledge that in the realm of AI-driven comment generation, there exists the potential for harmful comments to emerge. Thus, we remain vigilant and resolute in our commitment to responsible research practices, with a strong emphasis on ethical considerations and societal well-being."}]}