{"title": "DIAGen: Semantically Diverse Image Augmentation with Generative Models for Few-Shot Learning", "authors": ["Tobias Lingenberg", "Markus Reuter", "Gopika Sudhakaran", "Dominik Gojny", "Stefan Roth", "Simone Schaub-Meyer"], "abstract": "Simple data augmentation techniques, such as rotations and flips, are widely used to enhance the generalization power of computer vision models. However, these techniques often fail to modify high-level semantic attributes of a class. To address this limitation, researchers have explored generative augmentation methods like the recently proposed DA-Fusion. Despite some progress, the variations are still largely limited to textural changes, thus falling short on aspects like varied viewpoints, environment, weather conditions, or even class-level semantic attributes (e.g., variations in a dog's breed). To overcome this challenge, we propose DIAGen, building upon DA-Fusion. First, we apply Gaussian noise to the embeddings of an object learned with Textual Inversion to diversify generations using a pre-trained diffusion model's knowledge. Second, we exploit the general knowledge of a text-to-text generative model to guide the image generation of the diffusion model with varied class-specific prompts. Finally, We introduce a weighting mechanism to mitigate the impact of poorly generated samples. Experimental results across various datasets show that DIAGen not only enhances semantic diversity but also improves the performance of subsequent classifiers. The advantages of DIAGen over standard augmentations and the DA-Fusion baseline are particularly pronounced with out-of-distribution samples.", "sections": [{"title": "1 Introduction", "content": "A common problem in the field of computer vision is the insufficient amount of real-world training data [23]. Collecting and annotating data at scale can"}, {"title": "2 Related Work", "content": "Synthetic Data for Few-Shot Learning. Numerous works have explored synthetic image generation using GANs [5,15,41,52] and diffusion models [13, 14,28,34]. In few-shot learning, the small number of labelled images presents a challenge due to the inherently scarce class sampling. Collecting more real-world data is resource intensive [21], but synthetic data can utilize the knowledge of pre-trained generative model. While GANs have already been used for few-shot learning [3], diffusion models offer better results [10] due to their stability, high image quality, and flexibility during image generation [6].\nRecently, Trabucco et al. [46] attempted to generalize their pipeline based on a diffusion model, DA-Fusion, to unseen concepts by integrating Textual Inversion [8]. This method uses three to five real images to learn new visual concepts, creating pseudo word vectors for a text-to-image model. Textual Inversion is ideal for few-shot learning, enhancing the text encoder's vocabulary with new concepts. DA-Fusion then uses these embeddings to generate synthetic images with Stable Diffusion [33]. The denoising process of the diffusion model is conditioned on the text prompt and guided by a real training image [22]. DIAGen builds upon the work of Trabucco et al. [46] to increase the semantic diversity of the synthetic images further.\nDiversity in Datasets. The quality of machine learning models heavily relies on the diversity of their training data. A lack of diversity can lead to biases and poor performance [17], particularly in few-shot scenarios [16]. Creating synthetic data comes with a challenging trade-off: balancing fidelity for accurate representation and diversity for increased coverage [26]. While previous methods have improved synthetic data quality, they only address coverage implicitly. That said, there have been attempts to explicitly focus on the aspect of diversity. Wang et al. [48] demonstrated promising results by using a diversity measurement-based meta-learner. He et al. [10] utilized a text-to-image diffusion model and inserted"}, {"title": "3 Methodology", "content": "The input to our model DIAGen is a small dataset, R = {Rn | 1,..., N} of N real images, containing only a few images per class. The output of the pipeline (see Fig. 2) is an expanded labelled dataset that includes both the real images as well as M corresponding synthetic images Sn,m for each real image Rn. The goal is to augment the small given dataset in a semantically diverse way to enable the training of a downstream application with better generalization.\nBefore detailing our method to increase semantic diversity, we first lay out, how we define diversity. We focus on improving the intra-class diversity that"}, {"title": "3.1 Embedding Noise", "content": "The diffusion model that DIAGen builds upon is conditioned on a text prompt [33] containing the learned pseudo word vector for a specific class. The word vector of a class, e.g., car to give a concrete example, is learned with Textual Inversion [8] and the resulting embedding vector S is inserted into the prompt \"a photo of a S*\". Following Mikolov et al. [24], who observed that directions in embedding spaces represent semantic meaning, e.g., king \u2212 man + woman = queen, and that vectors that are very close to each other also have very similar meaning, we propose adding noise on top of the learned class concept vectors (see Fig. 2, contribution a). We hypothesize that varying S\u2217 of an object yields images of similar object types, since their representations are likely to be close together in the embedding space. This may result in scenarios where the representation of oldtimer is next to the embedding vector of our learned representation of car."}, {"title": "3.2 LLM Prompting", "content": "To achieve more explicit control over image generation beyond simply adding noise to the class embedding, we utilise a large language model (LLM) to provide textual guidance for the diffusion model (see Fig. 2, contribution b). Specifically, we employ GPT-4 [1], known for its robustness and extensive knowledge acquired from internet-scale data. We also tried the smaller model Llama2 (7B) [45] and observed a similar performance.\nDue to the different functioning and training data of language and image models, the covered knowledge also differs. This is beneficial in scenarios where the diffusion model has rarely seen a concept and hence has no contextual knowledge of the concept. An LLM such as GPT-4 can provide additional meaningful context so that the resulting synthetic images exhibit high semantic diversity.\nWe instructed GPT-4 to dynamically generate a certain number of prompts in the following style:\na photo of a (adjective) Su (location and/or weather preposition) (weather) (location) (time of day with preposition)\nAs mentioned earlier, Su denotes the learned embedding vector of a class after adding noise, which can be treated as a new pseudo-word. Every place-holder enclosed in brackets is optional and may be completed by GPT-4 to generate prompts of varying lengths and complexity. For instance, the final prompt of class: dog could be \"a photo of a (fluffy) Su\", for class: plane \u201ca photo of a Su (flying above a city at night)", "class": "spoon \"a photo of an (antique) Su (on a wooden table)"}, {"title": "3.3 Weighting Mechanism", "content": "Similar to DA-Fusion [46], the extent to which the generated images can deviate from the guiding image is controlled by a strength hyperparameter to. This parameter, ranging from 0 to 1, relates to the time step of inserting the guiding image during the diffusion model's denoising process. When to \u2192 0, the gener- ated images closely resemble the guiding image. While increasing to enhances the image diversity, this increased freedom also leads to a higher probability of generating synthetic images that do not match the intended class label, which can result in either distorted class representations or entirely unrelated concepts."}, {"title": "4 Experiments", "content": "4.1 Datasets\nTo evaluate the effectiveness of our method, four datasets were utilized. A consistent set of hyperparameters was used across all datasets to maintain the model's off-the-shelf property and to allow for direct comparison.\nFirst, the FOCUS dataset [17] was chosen, which contains 21K images of 10 different classes in common and uncommon settings, altering the time of day, weather condition, and location. This broad distribution makes FOCUS a well-suited dataset for our experiments, enabling the evaluation of DIAGen's ability to reproduce the distribution of the data only knowing very few images per class.\nSecond, we test our model on the MS COCO dataset [19]. This dataset comprises common objects in context, which implies that objects occur in different settings and have a variety of appearances. This dataset allows for a direct comparison to the baseline DA-Fusion, as it was also used in Trabucco et al. [46]."}, {"title": "4.2 Experimental Setup", "content": "We compare DIAGen's results against two baselines: DA-Fusion was chosen as the first baseline since DIAGen is built upon this model. We use the original experimental setup of Trabucco et al. [46]. Secondly, we compare DIAGen to standard augmentations, given their widespread use for data augmentation tasks. For this, we used a combination of rotations, flips, scale adjustments, and crops. More details on the experimental setup including all values for the hyperparameters can be found in the supplemental material.\nThe model's effectiveness was evaluated on a downstream classifier, comparing its behaviour on four datasets: FOCUS [17], MS COCO [19], Custom COCO, and Uncommon Settings. The downstream classifier accuracy serves as the primary metric for our studies, following the work of Ravuri and Oriol [30].\nTo ensure relevance for few-shot learning, we trained on small, varying dataset sizes containing 2, 4, and 8 examples per class. Furthermore, to increase the reproducibility and reliability of our findings, we used 3 different seeds to alter the selection of the images in the training split and calculated the mean."}, {"title": "4.3 Classification Accuracy", "content": "Fig. 3 shows the downstream classifier accuracy for DIAGen, the baseline DA-Fusion, and standard augmentations. We plot the accuracy over different few-shot dataset sizes, limiting the size to 2, 4, and 8 examples for each class.\nWe observe a consistent improvement in validation and test accuracy, by as much as +5% points across the four datasets when compared to DA-Fusion. The gain of DIAGen against standard augmentations is even more evident, reaching"}, {"title": "4.4 Ablation Study", "content": "We now analyze the contributions of the three components in our DIAGen pipeline: embedding noise, LLM prompts, and weighting mechanism. We conduct an ablation study by running each module independently to assess their individual impact. Fig. 4 illustrates the accuracy gains attributed to each component relative to the DA-Fusion baseline.\nEmbedding noise leads to major improvements when only 2 examples per class are used for training. Although the positive effect of adding noise on its own decreases with more examples per class, its combination with the other components yields significant benefits. We attribute the synergy of the combined method to the ability of the embedding noise and LLM to increase diversity at the expense of class fidelity, a trade-off that the weighting mechanism mitigates by"}, {"title": "4.5 Diversity Analysis", "content": "While it is important to evaluate the results of the downstream application, we also consider the overall quality and especially the semantic diversity of the synthetic dataset by exploring alternative metrics. If we visually compare the two datasets generated by DA-Fusion and DIAGen, we clearly observe a higher level of diversity with our method (see Fig. 6). At first glance, the DA-Fusion images"}, {"title": "5 Conclusion", "content": "We introduced DIAGen, an off-the-shelf image augmentation technique designed to increase semantic diversity in datasets with few labeled examples per class. DIAGen expands the DA-Fusion framework [46] by incorporating three key components: The first two modules of DIAGen focus on increasing diversity in the augmentation process by (i) introducing noise to the class representations in the embedding space, and (ii) enriching text prompts with semantically meaningful content, leveraging the capabilities of an LLM. The last module is designed to complement these strategies to keep a high class fidelity by (iii) using a weighting mechanism to reduce the influence of suboptimal generated images using a classifier. These components help balance fidelity and diversity in synthesized images. The resulting model improves classification accuracy across various datasets and enhances recall as a diversity metric. It is particularly effective in enabling downstream models to generalize to uncommon scenarios and edge cases, making it valuable for augmenting data in few-shot settings."}]}