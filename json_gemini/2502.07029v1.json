{"title": "Leveraging Allophony in Self-Supervised Speech Models for Atypical Pronunciation Assessment", "authors": ["Kwanghee Choi", "Eunjung Yeo", "Kalvin Chang", "Shinji Watanabe", "David Mortensen"], "abstract": "Allophony refers to the variation in the phonetic realization of a phoneme based on its phonetic environment. Modeling allophones is crucial for atypical pronunciation assessment, which involves distinguishing atypical from typical pronunciations. However, recent phoneme classifier-based approaches often simplify this by treating various realizations as a single phoneme, bypassing the complexity of modeling allophonic variation. Motivated by the acoustic modeling capabilities of frozen self-supervised speech model (S3M) features, we propose MixGoP, a novel approach that leverages Gaussian mixture models to model phoneme distributions with multiple subclusters. Our experiments show that MixGoP achieves state-of-the-art performance across four out of five datasets, including dysarthric and non-native speech. Our analysis further suggests that S3M features capture allophonic variation more effectively than MFCCs and Mel spectrograms, highlighting the benefits of integrating MixGoP with S3M features.", "sections": [{"title": "1 Introduction", "content": "A phoneme can be phonetically realized differently depending on its environment, a phenomenon known as allophony in phonology (Twaddell, 1952; Ladefoged, 1965; Collins et al., 2019). For instance, the English phoneme /t/ exhibits various allophonic realizations: [th] (aspirated stop) in tap, [t] (unaspirated stop) in stop, [r] (flap) in butter, and [?] (glottal stop) in kitten. Accurately capturing these variations is crucial, as it reflects the full spectrum of phonetic realizations within a phoneme. It is particularly important for atypical pronunciation assessment (Twaddell, 1952; Jokisch et al., 2009; Vidal et al., 2019), as it has to distinguish atypical (out-of-distribution; OOD) from atypical (in-distribution) pronunciations (Yeo et al., 2023a)."}, {"title": "2 Method", "content": "In this section, we briefly review the conventional approach to pronunciation assessment, Goodness of Pronunciation (GoP) (Witt and Young, 2000). We highlight its limitations: (i) modeling a phoneme as a single cluster, and (ii) assuming atypical speech are in-distribution with respect to typical speech. We then introduce our method, MixGoP, which addresses these limitations by (i) modeling allophonic variation through a mixture distribution and (ii) relaxing in-distribution assumptions by removing the softmax function."}, {"title": "2.1 What is Goodness of Pronunciation?", "content": "GoP is a phoneme-level pronunciation score2 that measures how much the acoustic output of atypical (dysarthric or nonnative) speech deviates from that of typical speech (healthy or native). GoP is measured by how likely a speech segment (s) is to be the intended phoneme (p). Given the phoneme classifier \\(P_{\\theta}(p|s)\\) with trainable parameters \\(\\theta\\), GoP is measured as the log phoneme posterior,3\n\n\n\n\\(GoP_p(s) = log P_{\\theta}(p|s).\\)"}, {"title": "2.2 Limitations of GoP", "content": "Conventional phoneme classifiers used in GoP assume a single cluster for each phoneme. This is because logits \\(f_{\\theta}(s)\\) are often modeled with a speech encoder Enc and a subsequent fully-connected (FC) layer with weights \\(W \\in \\mathbb{R}^{|V| \\times F}\\) (Xu et al., 2021; Yeo et al., 2023a):\n\n\n\\(f_{\\theta}(s) = W \\cdot Enc(s)\\)\n\nwhere \\(Enc(s) \\in \\mathbb{R}^{F}\\), \\(|V|\\) denoting the vocabulary size (total number of phonemes), and F the output dimension of the encoder. If we consider a frozen encoder, the trainable parameter \\(\\theta = \\{W\\}\\). Here, the weights W can be understood as a codebook, containing a F-dim centroid for each phoneme. It requires a unimodal (single peak) clustering of hidden features Enc(s) for each phoneme. This limits the ability to capture allophonic variation, as allophones are represented as distinct acoustic subclusters within each phoneme.\nAnother limitation comes from the assumption that observed speech segments are in-distribution with respect to the training data. This comes from the phoneme classifier \\(P_{\\theta}\\) formulation,\n\n\n\\(P_{\\theta}(p|s) = softmax(f_{\\theta}(s))[p].\\)\n\nWith the phoneme classifier relying on the softmax function, which models a categorical distribution, s is expected to be within phoneme distribution found in typical speech. However, this assumption is less suitable for atypical speech, which often exhibits substantial acoustic differences from typical speech (Yeo et al., 2023a; Korzekwa et al., 2021)."}, {"title": "2.3 MixGoP: Modeling multiple subclusters within a single phoneme", "content": "To address the two limitations presented in Section 2.2, we introduce MixGoP, a mixture distribution-based GoP.\nFirst, to overcome the unimodal assumption, MixGoP replaces phoneme classifier \\(P_{\\theta}(p|s)\\) in eq. (1) with a Gaussian mixture model (GMM).\nGMM is a weighted sum of Gaussian distributions that can directly model the phoneme likelihood \\(P_{\\theta}(s|p)\\) (distribution of speech segment s for each individual phoneme p). Accordingly, we formulate the phoneme likelihood as follows:\n\n\n\\(P_\\theta(s|p) = \\sum_{c=1}^{C} \\pi_c \\mathcal{N}(Enc(s)|\\mu_c, \\Sigma_c)\\)\n\nwhere \\(\\mathcal{N}\\) denotes the multivariate Gaussian distribution, \\(\\mu_c \\in \\mathbb{R}^{F}\\) and \\(\\Sigma_c \\in \\mathbb{R}^{F \\times F}\\) is the mean vector (centroid) and covariance matrix, and \\(\\pi_c \\in [0, 1]\\) is the mixing coefficient. Here, the trainable parameter \\(\\theta = \\{\\mu_c, \\Sigma_c, \\pi_c\\}_{c\\in[C], p\\in V}\\). Then, we can newly define our MixGoP score as:\n\n\n\n\\(MixGoP_p(s) = log P_\\theta(s|p).\\)\n\nOur MixGoP score differs from the original GoP score in eq. (1) by replacing the phoneme posterior \\(P_{\\theta}(p|s)\\) with the phoneme likelihood \\(P_{\\theta}(s|p)\\). By doing so, we are also removing the influence of phoneme prior \\(P(p)\\), which is known to be effective in practice (Yeo et al., 2023a).\nSecond, MixGoP removes the softmax function of eq. (3) by directly using the log-likelihood in eqs. (4) and (5). It relaxes the assumption of phonemes in atypical speech being in-distribution. The quadratic term inside each Gaussian:\n\n\n\\(-\\frac{1}{2} (Enc(s) - \\mu_p)^T (\\Sigma_p)^{-1} (Enc(s) - \\mu_p)\\)\n\ndirectly relates to the Mahalanobis distance, which is commonly used for OOD detection (Lee et al., 2018). By avoiding the softmax, MixGoP is likely to be more robust in handling OOD speech.\nIn summary, we train a total of \\(|V|\\) GMMs (one for each phoneme) where each GMM is composed of C subclusters, e.g., \\(C = 32\\). C is kept constant across all phonemes, as it is known that sufficiently large number of Gaussian mixtures can approximate any probability density (Nguyen et al., 2020). Experiments on the influence of C on downstream performance can be found in Appendix C.2. We use the k-means algorithm to determine the initial cluster centers and the expectation-maximization (EM) algorithm to optimize the parameters of the Gaussian mixtures, using scikit-learn 1.4.1 (Pedregosa et al., 2011). By considering allophony in modeling, MixGoP is expected to better reflect the distribution of each phoneme."}, {"title": "3 Experiments", "content": "We use five datasets: three dysarthric speech datasets (UASpeech (Kim et al., 2008), TORGO (Rudzicz et al., 2012), and SSNCE (TA et al., 2016)) and two non-native speech datasets (speechocean762 (Zhang et al., 2021) and L2-ARCTIC (Zhao et al., 2018)). In this paper, we use healthy or native speech as the training sets, and dysarthric and non-native speech as the test sets, in line with the OOD literature (Hendrycks and Gimpel, 2017). Refer to Appendix A for more details."}, {"title": "3.2 Feature extraction", "content": "For our experiments, we compare various speech feature extractors Enc(s) (eqs. (2) and (4)).\nTraditional acoustic features. We use the Mel-Frequency Cepstral Coefficients (MFCCs) and Mel spectrograms as baselines, using the default hyperparameters of librosa (McFee et al., 2015).\nTDNN-F features. We compare with a factorized time-delay neural network (TDNN-F) model (Povey et al., 2018) for the speechocean762 dataset, as TDNN-F features have been often used as baselines (Zhang et al., 2021; Gong et al., 2022; Chao et al., 2022; Do et al., 2023).\nS3M features. We employ two frozen S3Ms: XLS-R-300M (Babu et al., 2022) and WavLM-Large (Chen et al., 2022). XLS-R (shorthand for XLS-R-300M), trained cross-lingually, has demonstrated strong performance in ASR for low-resource languages (Babu et al., 2022) and dysarthric speech assessment (Yeo et al., 2023a). We also employ WavLM (shorthand for WavLM-Large), a state-of-the-art model for various tasks, including phoneme recognition (Feng et al., 2023; Yang et al., 2021).\nAs different layers of S3Ms are known to encode different information (Pasad et al., 2021, 2023), we use features from each layer. Specifically, we extract convolutional features (denoted as layer index 0) and all consecutive Transformer features (denoted as layer indices 1 through 24).\nFeature segmentation. We segment the features according to the start and end timestamps of each phoneme. Refer to the detailed time-alignment process in Appendix A. Then, we apply center pooling to extract one feature per segment."}, {"title": "3.3 Baselines", "content": "We verify the effectiveness of our MixGoP by comparing it against various baselines (Yeo et al., 2023a; Sun et al., 2022; Shahin et al., 2024; Sch\u00f6lkopf et al., 2001). We evaluate on all the speech features listed in Section 3.2 across all the methods for fair comparison. These baselines are categorized into two groups: (i) phoneme classifier-based and (ii) OOD detector-based approaches.\nPhoneme classifier-based approaches encompass conventional GoP formulations, which assume a unimodal distribution and in-distribution of phonemes, as discussed in Section 2.2. We employ four popular GoP formulations: GMM-GoP (Witt and Young, 2000), NN-GoP (Hu et al., 2015b), DNN-GOP (Hu et al., 2015b), and MaxLogit-GoP (Yeo et al., 2023a). Note that all formulations use the same underlying phoneme classifier \\(P_{\\theta}(p|s)\\). They only differ by how to calculate the GoP scores. Refer to Yeo et al. (2023a) for more details.\nOOD detector-based approaches calculate GoP by measuring how likely an input is to be an outlier. In other words, they can quantify the level of atypicalness. Our MixGoP is one of these approaches, as MixGoP models the likelihood \\(P_{\\theta}(s|p)\\) with typical speech (eq. (4)) and identifies outliers (atypical speech) based on their likelihood (eq. (5)). We additionally test three baselines: k-nearest neighbors (kNN) (Sun et al., 2022), one-class support vector machine (oSVM) (Sch\u00f6lkopf et al., 2001), and phoneme-specific oSVM (p-oSVM) (Shahin and Ahmed, 2019). While kNN has been utilized for OOD detection (Sun et al., 2022), it has not previously been applied to dysarthric or non-native speech. Conversely, OSVM and p-oSVM have been applied to the evaluation of both disordered and non-native speech (Shahin and Ahmed, 2019; Shahin et al., 2024)."}, {"title": "3.4 Training details", "content": "Phoneme classifier-based. The phoneme classifier in eq. (1) is trained on features from Section 3.2 with a single learnable FC layer (eq. (2)) with the default settings of Adam optimizer (Kingma and Ba, 2015) for a maximum of 500 iterations.\nOOD detector-based. For KNN, we construct a KNN model for each phoneme using the features from Section 3.2. Then, we use the maximum Euclidean distance between the test data feature and the nearest 10% training data feature as the GoP score, following Sun et al. (2022). For oSVM, all phonemes is modeled with a single oSVM model (Shahin et al., 2024), while p-oSVM modeled each phoneme as a separate oSVM model. All OSVM models are trained with features using the default hyperparameters of scikit-learn 1.4.1 (Pedregosa et al., 2011). Radial basis function was used for both oSVM and p-oSVM. We use the distance from the hyperplane as the GoP score.\nMixGoP. In our MixGoP framework, we apply random subsampling of 512 features per phoneme. Empirical analysis indicates that subsampling does not necessarily degrade performance (See Section 5.2). The number of subclusters for each phoneme-wise Gaussian mixture is set to 32. A detailed investigation into the effect of the number of subclusters on GoP performance is discussed in Appendix C.2."}, {"title": "3.5 Evaluation", "content": "As described in Section 3.2, we segment the spoken utterance x phoneme-wise: \\(x = \\{(p_1, s_1), (p_2, s_2), \\dots, (p_N, s_N)\\}\\), where \\(p_i\\) is the phoneme label, \\(s_i\\) is the observed speech segment, and N is the total number of phonemes within the utterance. Following Yeo et al. (2023a), we define the pronunciation score of an utterance x as:\n\n\n\n\\(Pronunciation(x) = \\frac{1}{N} \\sum_{i=1}^{N} GoP_{p_i}(s_i),\\)\n\nwhere the definition of the GoP is different per each method. That is, the GoP scores are averaged across the utterance.\nSimilar to Yeo et al. (2023a), we evaluate performance using the Kendall-tau correlation coefficient between the utterance-level pronunciation scores and the ground truth dysfluency/disfluency scores provided by the dataset. While Yeo et al. (2023a) used additional training data, our setting only uses the aformentioned datasets.\nUnlike other datasets, L2-ARCTIC contains only phoneme-wise mispronunciation detection labels"}, {"title": "4 Allophony of S3M features", "content": "Section 3 empirically demonstrates that leveraging S3M features with MixGoP helps enhance downstream performance compared to other features, such as MFCCs and Mel spectrograms. This section aims to further verify the suitability of S3Ms for representing individual phonemes with allophonic variations. First, we examine S3M features at the phoneme-level, using the dimensionality reduction technique in Section 4.1. Next, we design a metric to quantify the ability of capturing allophonic variations, comparing S3M features to MFCCs and Mel spectrogram in Section 4.2. For our analyses, we used the healthy speech recordings from the TORGO dataset (Rudzicz et al., 2012), which includes gold-standard phonemic transcriptions and alignments."}, {"title": "4.1 Motivating Observation", "content": "S3Ms are trained to reconstruct masked signals using surrounding information. Hence, we hypothesize that this will allow the S3Ms to capture local acoustic characteristics, including allophones from various phonetic environments. To verify such phenomena, we observed the final layer features of WavLM for each phoneme, which have generally shown the best performance across datasets (See Figure 5). Specifically, we use UMAP dimensionality reduction (McInnes et al., 2018) with the cosine distance metric to visualize the features, similar to Choi and Yeo (2022). We also extract the four utterances closest to each of the ten centroids to observe the phonetic environments of each cluster.\nFigure 2 demonstrates one example, with the distribution of /\u028c/ (/AH/ in ARPABET) and its environments of the healthy subset of TORGO. We observed multiple clusters for each phoneme, each with phonetically similar environments, which motivates the metric for quantifying allophony ability."}, {"title": "4.2 Quantifying Allophony", "content": "Previous studies have found that S3M features model the phoneme distributions with multiple clusters (Wells et al., 2022; Martin et al., 2023). However, there has been limited analysis on directly quantifying the relationship between the S3M feature subclusters and allophony. To this end, we design a setting that measures the mutual information between the S3M feature subcluster indices and the surrounding phonetic environment of each phoneme, which is an indicator of allophones.\nFirst, to obtain the subclusters within MFCCs, Mel spectrograms, and S3M features, we apply the k-means algorithm with \\(k = 32\\) clusters to the features of each phoneme \\(V \\in V\\). Then, each utterance has the designated k-means cluster index I. For a dataset with a total of \\(|V|\\) phonemes, we train \\(|V|\\) different k-means models.\nNote that our MixGoP uses k-means clusters as the initializer. Also, we observed few to no EM optimization steps due to high dimensionality (Wang et al., 2015). As a result, the initialized cluster centroids will likely be similar to the final centroids \\(\\mu\\) in eq. (4) for calculating phoneme likelihood \\(P_{\\theta}(s|p)\\).\nWe then compare the utterance-wise cluster indices with their allophony. Since the TORGO dataset does not provide phonetic transcriptions, we utilize the surrounding phonetic environment, which is closely linked to allophonic variation. For simplicity, we define the environment E as the natural class of the preceding and following phonemes, similar to phoneme environment clustering (Sagayama, 1989). We use the height, backness, and roundness for the vowels and the place and manner of the consonants for the natural class. For example, each /i/ and /k/ is represented as close-front-unrounded and velar-plosive,"}, {"title": "5 Analysis", "content": "It is crucial to examine whether capturing phonetic environments (or allophones) actually improves downstream performance. To assess this, we compare the amount of phonetic environment information inside S3Ms and the actual downstream performance on the pronunciation assessment. In Figure 4, we observe that the downstream performance positively correlates until around NMI < 0.72, where the downstream performance saturates even if the amount of phonetic environment information increases. We suspect this behavior is due to S3Ms capturing more surrounding information, which may not be useful for the pronunciation assessment task. Our hypothesis aligns with the previous empirical observation of Pasad et al. (2023) and Choi et al. (2024b) that S3Ms have non-negligible word-level modeling abilities, which requires a larger temporal receptive field. Moreover, the layerwise trends of Figure 3, i.e., WavLM persistently increasing and XLS-R peaking in the middle, are also similar to previous empirical observations on word-level layerwise information (Pasad et al., 2023)."}, {"title": "5.2 Sample efficiency of MixGoP", "content": "We randomly subsampled training set samples to check the influence of training data size. To train the GMM for each phoneme, we can either use all the occurrences in the dataset or limit the maximum number of samples. For optimal performance, we searched for the maximum number of samples between 64, 128, 256, 512, or using the full dataset. For example, if we set the maximum as 64, and /a/ and /i/ each have a total of 100 and 50 samples, to train the GMM of /a/, we randomly subsample 64 samples. On the other hand, since there are only 50 samples for /i/, all 50 samples are used."}, {"title": "6 Related works", "content": "Witt and Young (2000) first introduced GoP to estimate the log posterior probability of a phoneme using a Hidden Markov Model (HMM). Later improvements replaced HMMs with deep neural networks (Hu et al., 2015a,b; Li et al., 2016) and S3Ms (Xu et al., 2021; Yeo et al., 2023a; Cao et al., 2024). GoP has also been enhanced by considering additional factors, such as HMM transition probabilities (Sudhakara et al., 2019; Shi et al., 2020) and phoneme duration (Shi et al., 2020).\nSection 2.2 emphasizes the usefulness of framing the pronunciation assessment of atypical speech as the OOD detection task. Yeo et al. (2023a) also addressed this by not using softmax for phoneme classifiers improved performance. However, the use of softmax during training introduced in-distribution bias. Cheng et al. (2020) modeled input probability with latent representations, but their method still depended on assessment scores to train the prediction model. Our approach improves by directly modeling phoneme likelihood instead of relying on phoneme classifiers. Furthermore, our approach explicitly accounts for allophonic variation within phonemes."}, {"title": "6.2 S3M Feature Analysis", "content": "Previous literature on the phonetics and phonology of S3Ms often compared downstream task performance of different layers (Martin et al., 2023; Pasad et al., 2021, 2023, 2024; Choi et al., 2024a). Linear probes (Martin et al., 2023; Choi et al., 2024a) or canonical correlation analysis (Pasad et al., 2021, 2023, 2024) are often used to measure the amount of information. Our work is complementary as previous works focus on the existence of the information, whereas we further investigate on how the information is structured within the S3M features.\nAlso, discrete speech units from k-means clustering of S3M features have been used as the tokenizer for speech (Chang et al., 2024). Its underlying assumption comes from the feature structure being useful, i.e., similar-sounding segments are close to each other. Choi et al. (2024b) showed that phonetically similar words are close to each other. Also, Baevski et al. (2020); Hsu et al. (2021); Liu et al. (2023) demonstrated that phonemes and S3M cluster indices strongly correlate with each other. Sicherman and Adi (2023); Abdullah et al. (2023) showed that natural classes are also well-clustered. Finally, Wells et al. (2022) showed that the dynamic nature of a single phoneme articulation is captured by a stream of cluster indices. Extending previous works, we focus on the multimodal nature of phonemes and demonstrate that allophones are construct subclusters within the single phoneme."}, {"title": "7 Conclusion", "content": "We demonstrated that improved modeling of allophony can enhance performance in OOD detection for the assessment of atypical speech, and that leveraging S3M features can further improve this performance. Specifically, our novel approach, MixGoP, addresses the limitations of uni-modality and in-distribution assumptions by employing Gaussian mixtures, which effectively model allophones and eliminate the need for softmax probabilities. Additionally, we show that utilizing S3M features further enhances OOD detection performance. Our results also confirm that S3M features capture allophonic variation more effectively than traditional features, validating the extension of our approach to include S3Ms. We evaluated eight methods across five dysarthric and nonnative speech datasets, with MixGoP achieving state-of-the-art performance on four of the datasets. Our work provides a deeper understanding of how S3M representations can be hierarchically structured, from allophones to phonemes. Further, it sheds new light on the acoustic modeling perspective of speech, expanding the existing k-means-based speech discretization. It shows the possibility of using atypical speech as a benchmark to measure the quality of S3M features, especially regarding OOD robustness."}, {"title": "Limitations", "content": "First, a key limitation is the restricted generalizability of our findings across languages. Although we aim for our work to benefit a wide range of atypical speakers, including both dysarthric and non-native speakers, our research primarily focuses on English (four English datasets and one Tamil dataset). This limitation stems from the availability of publicly accessible datasets, but we recognize the need for broader cross-linguistic research in future work to ensure that our findings are applicable across diverse languages.\nAdditionally, we employed different methods for forced alignment across datasets, as outlined in Appendix A. Time alignments were either provided by the dataset or automatically generated using the Montreal Forced Aligner (McAuliffe et al., 2017). However, we did not verify the quality of these alignments in our study. This introduces the possibility that variations in alignment quality could have impacted the GoP scores, potentially affecting the overall results. While we do not primarily focus on comparing performance across datasets, future work could benefit from verifying alignment quality to ensure more reliable GoP scores, and cross-dataset comparisons.\nWe also acknowledge that our allophony analysis was primarily based on the TORGO dataset, which provided time alignments that were manually annotated by linguists. Extending this analysis to other datasets with similarly verified time alignments would further support the generalizability of our findings.\nFinally, the method used to calculate utterance-level (Equation (7)) pronunciation scores can be improved. In our current approach, we simply averaged phoneme probabilities across each utterance; however, it is well-known that certain phonemes have a greater impact on overall pronunciation scores. While our initial analysis, as presented in Appendix C.3, provides a preliminary exploration of this issue, further investigation is needed to identify more robust approaches. Expanding upon this analysis could lead to improved techniques that more accurately evaluate atypical speech."}, {"title": "Ethics Statement", "content": "The risk of atypical pronunciation assessment research primarily pertains to data handling and the potential for unintended consequences in use.\nFirstly, while we used publicly available datasets that have undergone prior ethical review, it is important to recognize that these datasets still contain sensitive information, particularly speakers' voices. Since no additional anonymization processes were applied in this study, we strongly recommend that any replication of this work prioritize the protection of participants' rights and privacy to the greatest extent possible.\nSecondly, concerns arise regarding the potential usage of atypical speech assessment scores. These assessments may unintentionally reinforce negative stereotypes or stigmas associated with speech disorders or non-native accents. If the results are interpreted as evaluations of an individual's language ability or intelligence, they could further marginalize dysarthric or non-native speakers. Also, there is a risk in placing too much emphasis on 'correctness' in phoneme-level pronunciation assessment. Focusing heavily on accurate phoneme production prescribes a rigid, normative standard of speech, potentially penalizing linguistic diversity and variation. For both dysarthric and non-native speakers, such an emphasis might overshadow more functional measures of communication success, which may be more meaningful in real-world contexts. Despite these concerns, which warrant careful consideration, we want to emphasize that our work is intended to have a significant positive impact from an ethical perspective.\nFinally, we note that ChatGPT was employed for grammatical refinement and to improve the clarity of English usage in the manuscript. We also state that every sentence generated by ChatGPT was reviewed by the authors."}]}