{"title": "B-cosification: Transforming Deep Neural Networks to be Inherently Interpretable", "authors": ["Shreyash Arya", "Sukrut Rao", "Moritz B\u00f6hle", "Bernt Schiele"], "abstract": "B-cos Networks have been shown to be effective for obtaining highly human interpretable explanations of model decisions by architecturally enforcing stronger alignment between inputs and weight. B-cos variants of convolutional networks (CNNs) and vision transformers (ViTs), which primarily replace linear layers with B-cos transformations, perform competitively to their respective standard variants while also yielding explanations that are faithful by design. However, it has so far been necessary to train these models from scratch, which is increasingly infeasible in the era of large, pre-trained foundation models. In this work, inspired by the architectural similarities in standard DNNs and B-cos networks, we propose 'B-cosification', a novel approach to transform existing pre-trained models to become inherently interpretable. We perform a thorough study of design choices to perform this conversion, both for convolutional neural networks and vision transformers. We find that B-cosification can yield models that are on par with B-cos models trained from scratch in terms of interpretability, while often outperforming them in terms of classification performance at a fraction of the training cost. Subsequently, we apply B-cosification to a pretrained CLIP model, and show that, even with limited data and compute cost, we obtain a B-cosified version that is highly interpretable and competitive on zero shot performance across a variety of datasets. We release our code and pre-trained model weights at https://github.com/shrebox/B-cosification.", "sections": [{"title": "1 Introduction", "content": "Despite their strong performance on a variety of tasks, understanding decisions of deep neural networks (DNNs) remains challenging. Explanation methods, such as feature attributions [45, 47, 52, 5], have been proposed in an attempt to explain such decisions post-hoc, but have often found to be unfaithful to the model being explained [2, 3, 39, 58].\nInherently interpretable Deep Neural Network (DNN) models have recently gained popularity. In contrast to the common approach of explaining existing DNNs in a post-hoc fashion, these models typically feature certain architectural constraints that allow for extracting human-interpretable, model-faithful simplifications of the models' computations by design; examples of this include prototype-based [13, 16, 31], dynamic linear [8, 9], or concept-bottleneck models [27, 56, 32, 41]. However, given those architectural changes, this comes at a price: specifically, the models need to be trained from scratch, which-especially in the case of large foundational models, which are increasingly popular-can cost millions of dollars.\nTo mitigate this, in this work, we explore a novel approach of fine-tuning DNNs for inherent interpretability and propose to 'B-cosify' existing DNNs. Specifically, we investigate whether pre-trained DNNs can simply be efficiently fine-tuned to obtain a similar degree of interpretability"}, {"title": "2 Related Work", "content": "Explanation Methods. Post-hoc attributions [45, 42, 52, 47, 5, 55] have popularly been used to understand the decisions of trained DNNs, but have often been shown to be unfaithful to the model being explained [2, 3, 58, 39]. Inherently interpretable models [13, 27, 9], in contast, incorporate architectural changes to the model and can yield explanations that are interpretable and faithful to the model by design. However, such models need to be trained from scratch, which imposes a significant additional cost. In this work, we explore fine-tuning for interpretability, and propose a method to transform existing black-box DNNs to inherently interpretable B-cos DNNs, bringing together the best of both worlds.\nAttribution Priors [37, 36, 26, 49, 4] have often been used to train or fine-tune models to have explanations with desirable properties, such as inducing smoother explanations [26], consistent explanations [36, 37], or to guide models to be 'right for the right reasons' [43, 19, 40, 33]. Similar to such work, we fine-tune black-box DNNs for interpretability, but in contrast, we only make architectural modifications to transform the DNNs to B-cos DNNs, and do not use any additional constraints on the explanations themselves while training.\nCLIP Interpretability and Localization. Post-hoc attribution methods [45, 35, 12, 6] have also been used to explain VLMs such as CLIP [38], however, as with supervised DNNs, their faithfulness to the model is not guaranteed and the explanations are often coarse-grained and not very human interpretable. While inherently interpretable architectures could address this, the high costs of training such large models from scratch makes their use unappealing. In this work, we bridge the gap by instead fine-tuning from pre-trained black-box CLIP models to inherently interpretable B-cosified CLIP variants, and find that the B-cosification process is effective in yielding performant and interpretable models. A separate line of work [7] involves improving localizability of VLMs, and is orthogonal to our work since our goal is to obtain explanations that are faithful to the model.\nLearning mappings between model features. Recent work [29, 32] has explored using simple linear transforms to map features between models, and in particular also mapping features from arbitrary models to CLIP's representation space. In the context of our work, such methods can be used to map a supervised B-cos feature extractor to CLIP using a linear transform, to obtain an inherently interpretable DNN that can mimic CLIP. In our evaluation, we compare with such an approach, and find that our approach of architecturally transforming the full model and fine-tuning for interpretability yields improved zero shot performance."}, {"title": "3 From conventional to B-cos models", "content": "In the following, we describe the process of fine-tuning standard black box DNNs into inherently interpretable B-cos DNNs. In Sec. 3.1, we first introduce the B-cos models and enumerate the key ways in which they differ from standard models. In Sec. 3.2, we then perform a detailed study on strategies to bridge each of these differences for effective B-cosification."}, {"title": "3.1 B-cos Models: Background", "content": "Many common DNNs consist of a series of blocks of linear layers followed by non-linear ReLU activations [30], and are thus piece-wise linear functions\u00b9: i.e., for every input x, they effectively compute a linear transformation of that input: y(x) = W(x)x + b(x), cf. [51]. In [8, 10], models of this kind have been called 'dynamic linear', a naming convention that we adopt in this paper.\nInterestingly, for piece-wise linear models, W(x) is given by the models' gradient with respect to x [51]-except for the input-dependent bias b(x), the gradient thus constitutes an exact summary of the models' computations. This linear mapping W(x) is unfortunately typically not easily interpretable, and many techniques have been proposed to derive qualitatively more convincing explanations [45, 55]. These, however, have been shown to often not faithfully reflect the underlying model [1, 39].\nFurther, if the models employ bias terms, W(x) does not yield a complete explanation [51], i.e. y(x) \u2260 W(x)x. Integrating bias terms as proposed by [51] yields a set of importance attribution maps, summarizing which requires carefully selecting a post-processing function with inherent tradeoffs. Even when not using bias terms [22], however, the resulting matrices W(x) are often not easily human interpretable, and the resulting models can suffer from significant drops in performance.\nTo address this, [9, 10] propose to architecturally modify the DNNs to introduce additional alignment pressure during model optimisation. For this, they replace the ubiquitously used linear transformation by the B-cos transformation, which dynamically scales the output of the linear transformations:\nB-cos transformation: $f_{b-cos}(x; w) = (\\cos(x, w)|^{B-1} \\frac{x}{\\|x\\|} w) \\frac{x}{\\|x\\|} = w^T (x)x$,\nwith B a hyperparameter, cos the cosine similarity between x and the weights w, and $w=\\frac{w}{\\|w\\|}$.\nLike piece-wise linear models, B-cos models are dynamic linear and thus accurately summarised by a single linear transformation W(x) s.t. y(x) = W(x)x; as B-cos models do not employ bias terms, this model summary is complete. Crucially, it has been shown that with B>1, the matrix W(x) aligns with task-relevant input patterns, making it more easily human interpretable (e.g. Fig. 1, right).\nImportantly, as the B-cos transformation can serve as a drop-in replacement for linear transformations at every layer of a DNN, it is possible [9, 10] to leverage existing DNN architectures and the resulting B-cos models obtain similar classification accuracies as their conventional counterparts (Tab. 4, cols. 'pretrained' and 'B-cos').\nExtending this, we investigate if it is possible to leverage existing DNN weights\u2014i.e., our goal is to fine-tune existing models to be similarly interpretable as B-cos models, whilst not requiring to train them from scratch. However, despite the architectural similarities between B-cos and conventional models, there are multiple key differences that make transforming pre-trained models into B-cos models non-trivial: e.g., apart from replacing linear transformations with the B-cos transformation and not employing biases, B-cos models are trained on image representations with 6 color channels as [r, g, b, 1-r, 1-g, 1-b] to be able to visualise the model-inherent linear summaries W(x) in color, whereas conventional models use 3 channels (see also Tab. 1). In the next section, we show how to overcome these differences and convert existing models into functionally equivalent B-cos models."}, {"title": "3.2 B-cosification of Deep Neural Networks", "content": "We analysed the differences between B-cos models and their conventional counterparts in detail and compiled the results in Tab. 1. In this section, we discuss one by one how to bridge these differences. In particular, we show that a conventional model can be framed as a functionally equivalent B-cos model as in [10] with B=1, which additionally employs bias terms. Only upon modifying these two aspects, i.e. biases and B, does the model need to be fine-tuned to adapt the weights to those changes."}, {"title": "3.2.1 Functionally Equivalent B-cos Models", "content": "Input Encoding and Normalisation. As mentioned in Sec. 3.1, B-cos models use input represen-tations with six color channels [r, g, b, 1-r, 1-g, 1-b] to be able to visualise the explanations in color, cf. [10]. However, most conventional DNNs (e.g. models from Torchvision [53], CLIP [38]) are applied to 3-channel inputs in which images are encoded via [r, g, b]. As a result, visualising the dynamic matrices W(x) of piece-wise linear models (cf. Sec. 3.1) in color would not seem possible.\nHowever, we note that in combination with the commonly used input normalisation, we can convert the first linear transformation in conventional models (e.g., a convolutional layer) into an equivalent transformation that accepts 6-channel inputs. Specifically, for input normalisation, the channel-wise means \u03bc\u03b5 are subtracted from the individual channels, followed by a division by the standard devia-tions \u03c3s, yielding s' =(s \u2013 \u03bcs)/\u03c3s for s\u2208 {r, g, b}. Conversely, mean-normalising the 3 additional color channels yields \u2013s'. Leveraging this, we use the models' weights learnt for 3-channel inputs, wj = [wj,r, wj,g, wj,b] for every feature j, to construct an equivalent 6-channel transformation:\n$w' = \\begin{bmatrix}\\frac{w_{j,r}}{2} & \\frac{w_{j,g}}{2} & \\frac{w_{j,b}}{2} & \\frac{w_{j,r}}{2} & \\frac{w_{j,g}}{2} & \\frac{w_{j,b}}{2}\\end{bmatrix}$,\nNote that applying w' to the mean-normalised, 6-channel inputs yields the same results as applying w; to the original mean-normalised inputs that the pre-trained models have seen during training.\nActivation Functions. Owing to the non-linearity inherent to the B-cos transform, explicit activation functions are not necessary in between B-cos layers. However, the authors of [9, 10] showed that the model-inherent explanations are compatible with MaxOut [20]. Note that the very commonly used ReLU non-linearity applied to v\u00b9x for any weight vector v, is just a special case of MaxOut:\n$MaxOut(x; v, 0) = max(v^Tx, 0^Tx) = ReLU(v^Tx)$.\nAs the pre-trained models' weights have been optimised for the ReLU non-linearity and given its compatibility with the B-cos explanations, we leave them untouched in the B-cosification process.\nWeight normalization. B-cos transformations employ unit norm weights, see also Eq. (1), which the authors motivated by the fact that the only way any given neuron can achieve its maximal output is by increasing the weight-input alignment, which in turns leads to the improvements of the explanations. However, conventional models have been trained with unconstrained weights and using unit norm weights would thus lead to unpredictable model behaviour. Interestingly, we note that the weight normalisation in the latest version of the B-cos models can actually not impact the explanation quality, as the authors of [10] re-introduce normalisation layers into the B-cos models. To better understand this, let us consider the compound function of a batch normalisation layer and a B-cos layer:\n$f(x) = BatchNorm \\cdot B-cos(x)$\nwith $BatchNorm(y) = \u03b1 \\times \\frac{y - mean(y)}{\\sqrt{var(y)}} + \u03b2$,"}, {"title": "3.2.2 Fine-tuning for Interpretability", "content": "The changes introduced in the preceding section have not functionally changed the pre-trained models, but rather allow us to interpret the existing models as a special case of B-cos models. Now we introduce the necessary changes to increase the interpretability of the dynamic matrices W(x). As these functionally change the models, they need to be fine-tuned to recover their original performance.\nIn particular, the remaining differences between conventional and B-cos models are (1) the value of B, and (2) the use of biases, Tab. 1. We will now discuss how we bridge these differences individually.\nAblation Setup. We evaluate various fine-tuning strategies using a ResNet-18 [21] model supervised on ImageNet [15] from Torchvision [53] for B-cosification, and compare with a B-cos ResNet-18 from [10]. We optimize using AdamW [25] with cosine scheduling and train for 90 epochs, and evaluate both classification accuracy as well as interpretability using the GridPG metric [8].\n(1) Increasing B. As shown in [9], using B>1 is critical to obtain easily interpretable explanations. To increase B for the pre-trained models, we investigate three strategies: (1) immediately setting B to a higher value and then fine-tuning, (2) linearly interpolating from B = 1 to B = 2 throughout fine-tuning, and (3) setting B as a learnable parameter. (2) has the advantage of changing the model in small steps, making it more likely that it maintains performance while fine-tuning, but requires using the full number of epochs to reach the target value of B. (1) on the other hand is likely to adversely affect the utility of the weights, but offers the opportunity to stop fine-tuning early if performance and interpretability metrics are sufficiently high. (3) offers the most flexibility, but also adds a new set of parameters that need to be optimized. We show the results of this evaluation in Tab. 2. Interestingly, we find that using (1), i.e. setting B = 2 and then fine-tuning, yields performance that is on par with learnable B parameters, whilst being significantly simpler to implement. To easily test the generality of the B-cosification scheme, we therefore opt for this approach in Sec. 4.1.\n(2) Decreasing biases. As discussed in Sec. 3.1, dynamic linear models with bias terms are not exactly summarised by the matrix W(x), cf. [51]. To obtain the same level of faithfulness of the explanations as B-cos models (in particular w.r.t. explanation completeness, cf. [51, 52]), we need to remove the biases from the model. To do so, we investigate two approaches: (1) removing all biases first and then fine-tuning, and (2) fine-tuning while decaying biases using weight decay. Similar to the setup with B, (2) has the advantage of avoiding drastic changes to the model, but requires potentially fine-tuning for longer. Further, the weight given to the bias decay in the loss constitutes a tradeoff between maintaining classification performance and pushing the biases to be close to zero. We report the results of this evaluation in Tab. 3. Similarly to the experiments for B, we find that immediately setting the biases to zero constitutes a simple yet performant approach to achieve both good localisation and accuracy. To assess the generality of the B-cosification scheme across a wide range of models, we thus choose this the simpler approach of setting biases to zero in Sec. 4.1."}, {"title": "4 B-cosification Results", "content": "In the following, we evaluate the effectiveness of the B-cosification strategy we developed in Sec. 3. In Sec. 4.1, we first apply B-cosification to supervised models across various architectures, and evaluate for classification performance and interpretability. In Sec. 4.2, we B-cosify CLIP [38], a large foundational vision-language model, and show that despite fine-tuning at a fraction of the training cost, the B-cosified CLIP shows strong zero shot generalization whilst being highly interpretable."}, {"title": "4.1 Supervised Classification Models", "content": "Tab. 4 reports the classification accuracy of the B-cosified models across architectures, and compares them with their conventional counterparts from Torchvision and B-cos models trained from scratch. We find that across architectures (col. 1), B-cosified models perform competitively with conventional DNNs (cols. 2-4) and interestingly, in contrast to the findings reported by [10], often outperform them, i.e. for five out of twelve architectures. Notably, we find (col. 5) that our B-cosified models significantly outperform B-cos models trained from"}, {"title": "4.2 B-cosifying CLIP Towards Inherently Interpretable Foundation Models", "content": "In this section, we evaluate our B-cosification paradigm on CLIP [38], a powerful pre-trained vision-language model, and evaluate its interpretability and zero shot performance.\nSetup. We B-cosify a CLIP [38] with a ResNet-50 [21] backbone using the procedure described in Sec. 3.2. We use the recently proposed SigLIP loss [57] between the image embeddings of the pre-trained CLIP and the B-cosified CLIP's and train the models on either the ImageNet [15] or the CC3M datasets [46]. For evaluation, we rely on the CLIP Benchmark [14] and report zeroshot and linear probing results for accuracy. To assess the models' interpretability, we explain the similarity between the image embeddings and the text embedding of the pre-trained CLIP model via the dynamic linear summaries, see Sec. 3.1 or GradCAM, and report the EPG scores [55, 40] on the VOC dataset [18]. For full details, see Appendix C.2.\nIn Fig. 5, we report the zeroshot and linear probing accuracies of the two B-cosified CLIP models (trained on ImageNet or CC3M) and compare it to the original CLIP (Standard) and the recently proposed Text2Concept technique [29]; for the latter, we train a linear layer on top of a frozen, pre-trained B-cos ResNet-50 from [10] to mimic the embeddings of CLIP [29]. We find that the B-cosified models significantly outperform the Text2Concept approach and achieve accuracies that are more similar to the original CLIP's zeroshot and linear probing accuracies.\nEvaluating Model Interpretability. We evaluate the B-cosified CLIP's ability to localise classes in the VOC dataset in two ways. On the one hand, we directly explain the similarity of the models' embedding to the text embedding of a given prompt such as \"A photo of a cow.\". On the other hand, we note that the final attention pooling layer in the CLIP model only computes a weighted sum of the last layer's value vectors. Therefore, we additionally evaluate whether we can also explain the similarity between the text embeddings and these value vectors to improve the localisation ability.\nIn this context, we notice that explaining the average similarity to the text embedding yields highly distributed attribution maps, see Fig. 6b, col. 2. On the other hand, explaining only the most similar embedding localises very well, see Fig. 6b, col. 5. To better understand this phenomenon, we additionally interpolate between these two approaches and compute weighted means $ \\sum_i w_i v_i $ of those value vectors vi, in which the weights are determined by the cosine similarity between the value vectors vi and the text embedding t, i.e. with weights $w_i = cos(t, v_i)$ for various p.\nWe find that this not only significantly improves the explanations qualitatively, see Figs. 2 and 6b, but also quantitatively: in Fig. 6a we report results for explaining the final image embedding (B-cosified CLIP), the dynamic linear summary for the CLIP ResNet-50 (CLIP W(x)x), its GradCAM explanations (CLIP GradCAM), and the weighted mean of the value vectors, which we call"}, {"title": "5 Discussion", "content": "The B-cosification approach presented in this work addresses a common issue with developing inher-ently interpretable models: achieving model interpretability without compromising on performance or incurring high training costs. By leveraging pre-trained models, B-cosification opens a new path towards developing interpretable yet performant models, which can be of particular interest in the context of foundation models such as CLIP [38], which might otherwise be prohibitively expensive to train on a limited budget. Our results suggest that B-cosification not only maintains but, in several cases, even enhances model accuracy, whilst yielding significant improvements on interpretability metrics, providing a viable and resource-efficient alternative to training B-cos models from scratch.\nSpecifically, we find B-cosified models to much faster reach the same levels of interpretability and accuracy than their counterparts trained from scratch, with training speedups of up to 9x in some models. The approach appears to be general, being applicable for both CNNs and ViT models. We hope that this increase in efficiency will make interpretable models much more accessible in settings with constrained computational resources and could thus facilitate their adoption. In particular, when applying our proposed B-cosification scheme to a foundational model\u2014CLIP\u2014we find that the B-cosified CLIP model is able to maintain competitive zero-shot performance while at the same time providing interpretable and model-faithful explanations.\nDespite these advancements, certain aspects remain open for further exploration. Specifically, while some models quickly recover original performance after B-cosification, others exhibit slower convergence rates, suggesting potential for optimisations in the fine-tuning process. Additionally, for the larger B-cosified ViT models, while yielding results that are on par with those trained from scratch, the B-cosification process did not succeed in achieving speed-ups, indicating that the interplay between model architecture and the proposed B-cosification might require further exploration.\nIn summary, our results establish B-cosification as an effective method for enhancing interpretability in pre-trained models with low computational cost. The method consistently enables high interpretability without compromising performance, even achieving substantial training speedups in many cases."}, {"title": "A Additional Qualitative Results", "content": "In Fig. A1, we provide additional qualitative examples to illustrate the interpretability gains achieved by B-cosifying a CLIP model. Specifically, we show explanations generated by the original CLIP model using GradCAM [45] (row 2) for a diverse set of input images (row 1), for which explanations are generally coarse and lack clear localization. In contrast, the third row displays explanations produced by B-cosified CLIP, which yields finer-grained, more visually interpretable explanations.\nIn Fig. A2, we show further comparisons on specific object classes with using different cosine powers p (cos, cos-7, cos-19, and cos-inf) to qualitatively demonstrate the effect of increasing the exponent p in gathering the value vectors, see also Sec. 4.2. Higher cosine thresholds result in increasingly focused and interpretable representations, capturing fine details that are often absent in the original CLIP explanations.\nIn Fig. A3, we show additional qualitative examples for prompting the B-cosified CLIP model with different prompts for the same image, thus highlighting the class-specificity of the explanations as well as the potential that inherently interpretable CLIP models might yield. Specifically, B-cosified CLIP models allow to explain the similarity of a given image with a free-form textual prompt, which shows that the zero-shot performance of CLIP with respect to classification also transfers well to the corresponding explanations."}, {"title": "B Additional Quantitative Results", "content": "In this section, we provide a series of additional quantitative results on the performance and in-terpretability of B-cosified models. These tables cover various ablation studies, comparisons with standard and B-cos models, and performance across different configurations. Specifically, in Tab. B1, we extend our evaluation to compare models that are trained for the same effective number of epochs\u00b3. Further, in Tab. B2, we evaluate the impact of using normalized weights in the B-cos layers. In Tab. B3, we show additional results for B-cosified ResNet-50 models that are initialised from different pre-trained checkpoints. Specifically, we find that apart from initialising the ResNet-50 from CLIP weights, we observe consistent improvements through B-cosification, with stronger pre-training paradigms that are based on the ImageNet dataset (V2, DINO [11]) leading to larger improvements. Finally, we report full ablation results on the impact of the individual changes that we perform on the pre-trained models Tab. B4, B-cosifying models with different strategies for setting the parameter B in the B-cos transformation Tab. B5, decaying the bias term Tab. B6, as well as full zero-shot and linear probing results for the CLIP benchmark, see Tabs. B7 and B8."}, {"title": "C Implementation Details", "content": "We implement our code in Pytorch [34] for all the experiments and use Captum [28] for visualisations."}, {"title": "C.1 Standard Models", "content": "We B-cosify models from Torchvision [53] supervised on ImageNet [15]. We use a diverse set architectures, including both CNNs (ResNet-18 [21], ResNet-50 [21], and DenseNet-121 [23]), and ViTs [17] with (ViTc-Ti, ViT-S, ViT-B, ViT-L) and without (ViT-Ti, ViT-S, ViT-B, ViT-L) convolutional stems. For ResNet-50, we use both the weights originally released by Torchvision and the updated V2 weights, which constitute models trained for longer and with more augmentations [54]."}, {"title": "C.1.2 Datasets", "content": "We use ImageNet [15] to fine-tune all the B-cosified standard models and evaluate them on ImageNet's validation set. For training, we use train transforms - crop size of 224, horizontal flip with 0.5 probability, random resized crop of 224 with bilinear interpolation, Add Inverse transform [9] and modified mean-std normalisation (to accommodate for 6 channel input from the AddInverse). For evaluation, instead of a random resized crop, we do a center crop with a crop size of 224."}, {"title": "C.1.3 Optimization", "content": "For each architecture, we use the B-cosification stategy derived in Sec. 3, and fine-tune for 90 epochs using the AdamW optimizer [25] and cosine scheduling for the learning rate learning rate of 10-4 for the convolutional models (since the standard pre-trained models end with a learning rate of 10-4 at the 90th epoch, from which we want to fine-tune further). For ViTs, as the learning rate decays to a very small value, we tested with different learning rates (10-3, 10-4, 10-5) and found 10-3 worked best for all the models. Also, we only use a linear learning rate warmup of 10,000 steps with a decay of 0.01 for the base and large ViT models."}, {"title": "C.1.4 Experiments", "content": "Increasing B: We tested three different setups for increasing B. 1) Discrete B setting to 1, 1.25, 1.5, 1.75, 2, 2.5, 3, 5, 7; 2) Linear increase of B in n epochs from B=1 to B=2. We used n = 5, 10, 20, 45, 90; 3) Learning B parameter to increase to B=2 using weight decay with coeffi-cients 0.2, 0.5 and 0.9. See Tab. 2 for results.\nRemoving biases: We test two setups for removing the biases from the network: 1) Removing all the bias parameters; 2) Decay the bias parameter using the weight decay with coefficients 0.5 and 0.9. See Tab. 3 for results.\nImpact of pre-trained weights: To check the impact of pre-trained weights on fine-tuning, we fine-tuned weights from CLIP [38] ResNet-50 [21], DINO ResNet-50 [11], and Torchvision [53] ResNet-50 weights v1 and v2 (long trained recipe) [54]."}, {"title": "C.1.5 Evaluation", "content": "As in Sec. 3.2, we evaluate both for classification accuracy and for interpretability using the GridPG [8] metric. We compare both accuracy and interpretability of the B-cosified models with B-cos models trained from scratch from [10]. For interpretability, we also compare with several post-hoc attribution methods as baselines, namely Guided Backprop [50], Gradient [48], DeepLIFT [47], IxG [47], IntGrad [52], and GradCAM [45]. Qualitatively, we visualize the colored B-cos explanations and the attribution maps [10]."}, {"title": "C.2 CLIP Models", "content": "We use a CLIP [38] ResNet-50 [21] model for B-cosification."}, {"title": "C.2.1 Datasets", "content": "We use ImageNet [15] and CC3M [46] to fine-tune all the B-cosified CLIP models and test them on multiple datasets from CLIP benchmark [14]. For training, we use the same transform setup as the standard B-cosified models. We use train transforms - crop size of 224, horizontal flip with 0.5 probability, random resized crop of 224 with bilinear interpolation, and Add Inverse transform [9] and modified mean-std normalisation (to accommodate for 6 channel input from the AddInverse) as discussed in the paper. For evaluation, instead of a random resized crop, we do a center crop with a crop size of 224."}, {"title": "C.2.2 Evaluation", "content": "We use the CLIP benchmark [14] for zeroshot and linear probing experiments with the default parameters provided in the official benchmarking code. For text-based localisations, we use the text-based templates from the CLIP for the ImageNet dataset and use them to encode the text features. As text encoder, we use the CLIP ResNet-50 text encoder. The cosine scores between the B-cosified CLIP's image encoding and the pre-trained text encoder are used to do B-cos style localisations and calculate the GridPG scores. We use the unpooled features technique at inference to increase the localisation focus."}, {"title": "C.2.3 Optimization", "content": "We use the Adam optimizer [25] and fine-tuned models till 90 epochs, while the CC3M models are fine-tuned for 30 epochs. The size of CC3M is approximately three times that of ImageNet, so the trained models are comparable. Keeping consistent with the Standard B-cosification recipe, we train with a learning rate of 1e-4 using cosine scheduling. SigLIP contrastive loss [57] is used to train the models."}]}