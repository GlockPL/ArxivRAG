{"title": "Nearest Neighbor Normalization Improves Multimodal Retrieval", "authors": ["Neil Chowdhury", "Franklin Wang", "Sumedh Shenoy", "Douwe Kiela", "Sarah Schwettmann", "Tristan Thrush"], "abstract": "Multimodal models leverage large-scale pre-training to achieve strong but still imperfect performance on tasks such as image captioning, visual question answering, and cross-modal retrieval. In this paper, we present a simple and efficient method for correcting errors in trained contrastive image-text retrieval models with no additional training, called Nearest Neighbor Normalization (NNN). We show an improvement on retrieval metrics in both text retrieval and image retrieval for all of the contrastive models that we tested (CLIP, BLIP, ALBEF, SigLIP, BEIT) and for both of the datasets that we used (MS-COCO and Flickr30k). NNN requires a reference database, but does not require any training on this database, and can even increase the retrieval accuracy of a model after finetuning.", "sections": [{"title": "Introduction", "content": "Contrastive image and text models are a fundamental building block of large-scale text-to-image or image-to-text retrieval systems. These models utilize contrastive loss functions to learn joint text and image embeddings, aligning embeddings for matching text and image pairs while separating embeddings for non-matching pairs. However, contrastive embeddings optimize pretraining objectives such as InfoNCE rather than downstream retrieval accuracy, so learned embeddings can be suboptimal for retrieval. Many methods for improving contrastive models on downstream retrieval tasks require additional training to adapt models across domains or aggregate information from an external database , and others are specialized for individual error categories, such as gender bias."}, {"title": "Nearest Neighbor Normalization", "content": "Retrieval models compute a match score s(q, r) between a query q and database retrieval candidate r, and return the highest-scoring candidates. In the case of contrastive multimodal models such as CLIP, this score is typically the cosine similarity between image and text embeddings.  To correct for bias towards hubs in image-text retrieval, we propose NNN, an approach that estimates bias for each retrieval candidate using a database of reference queries, D. The bias is then applied as an additive correction to the original match score, then used for retrieval. Specifically, given a contrastive retrieval score s(q, r) = q.r, we define the bias b(r) for a retrieval candidate r as a constant multiple (\u03b1) of the mean of s(q1, r), s(q2,r),...,s(qk,r), where {q1,..., qk} = Dtopk(r) are the k queries from the reference query dataset that have the highest similarity score s(qi,r) with r. Namely, if we define the operator argmaxk to denote the k arguments for the which a function attains its k maximum values, then we have Dtopk(r) = arg maxq\u2208D s(q, r), and our bias is computed as:\n\n$b(r) = \\frac{1}{k} \\alpha \\sum_{q_j \\in D_{top k}(r)} s(q_j, r)$.\n\nNNN uses the nearest k query embeddings to differentiate similar objects, capturing fine-grained distinctions between retrieval candidates.\nEach retrieval candidate has a constant bias score, so these scores can be computed offline and cached. The debiased retrieval score can then be computed by subtracting the estimated bias from the original score:\n\n$s_p(q, r) = s(q, r) \u2013 b(r)$.\n\nWhen using vector retrieval to compute match scores, bias scores are computed in sublinear time and add a constant factor to retrieval runtime; see Section 3.1 for further discussion."}, {"title": "Experiments", "content": "We evaluate NNN on both text-to-image and image-to-text retrieval using a variety of contrastive multimodal models (CLIP, BLIP, ALBEF, SigLIP, BEIT) on well-established retrieval datasets Flickr30k and COCO. We also report the accuracy of DBNorm, the top-performing baseline, using DBNorm's DualIS scoring function. Additional DN , QBNorm, and DualDIS baselines are discussed in Appendix D."}, {"title": "Retrieval performance", "content": "Accuracy. To evaluate the impact of NNN on retrieval performance, we hold out a random subset of the training set with the same size as the test set, and optimize \u03b1 and k via a hyperparameter search (Appendix B1). We use the same approach to optimize the DBNorm hyperparameters (but we note that optimizing these parameters takes 100x the compute). Then, we evaluate both methods on the test set: for image retrieval, we use training captions as the reference database, and for text retrieval, we use training images. We performed experiments with both in-distribution queries (e.g. normalizing COCO retrieval using COCO reference queries) and out-of-distribution queries (e.g. normalizing Flickr using COCO). NNN still shows consistent gains over the original model when scores are normalized with out-of-distribution queries. We also ran ablation studies on the size of the reference query database using various subsets of Flickr and COCO and find minimal performance decrease (see Appendix E).\nEfficiency. Since NNN only requires the k-nearest reference queries per retrieval candidate, unlike QBNorm and DBNorm, it does not require an exhaustive search over the |RETRIEVAL DATASET|\u00d7|REFERENCE DATASET| matrix of similarity scores. We can use an inverted file index from Faiss to efficiently compute the per-retrieval candidate bias scores. Then, to use bias scores in retrieval with a vector index, we modify retrieval embedding r to r' = (r, b), where b is the associated bias with r, and modify query embedding q to q' = (q, \u22121). Thus, the new inner product between r' and q' is r'\u00b7q' = r.q-b, which is equivalent to Equation 2."}, {"title": "Correcting image and caption bias", "content": "To provide intuition on how NNN impacts hubness, we analyzed hub images that match with many queries, despite having only a few correct ground-truth captions. In Figure 2, we show that for CLIP on COCO image retrieval, NNN significantly reduces imbalance in this distribution and decreases the effect of hubs comparably to finetuning directly on the reference query dataset. distribution shifts for additional image and text retrieval settings (Appendix G) show a similar trend."}, {"title": "Reducing gender bias in image retrieval", "content": "In addition to broad retrieval experiments, we also measure the effect of NNN on unwanted correlations between specific input attributes and retrieval scores. We examine gender bias, where most corrective methods show a tradeoff between bias and retrieval accuracy: stronger debiasing is accompanied by a performance drop. NNN reduces gender bias while improving retrieval accuracy. We evaluate NNN on CLIP for a subset of the VisoGender benchmark, which contains images of people and objects corresponding to 23 occupations (5 images perceived male and 5 female per occupation), and associated gender-neutral captions of the form \u201cThe occupation and their object.\u201d Retrieval returns the closest n images for a caption (e.g. the supervisor and their computer). Applying NNN to this setting requires a choice of reference captions, as VisoGender does not include a training distribution. Further work could also explore generation of task-specific reference sets."}, {"title": "Conclusion", "content": "We introduce Nearest Neighbor Normalization for contrastive multimodal retrieval. By precomputing bias correction scores using only the k-nearest neighbors, NNN is substantially more efficient while slightly improving accuracy over previous test-time inference methods. We also show that NNN can be used flexibly with arbitrary reference datasets and performs well at reducing gender bias."}, {"title": "Limitations", "content": "NNN can be applied to contrastive multimodal models to achieve significant and consistent retrieval score improvements. We have not shown that the same holds for models with a dedicated cross-attention between image and text embeddings, and show evidence that it might not be effective in Appendix F. Furthermore, although NNN is fast for contrastive models due to the efficiency of vector retrieval, it is much slower for crossmodal models, as computing each image-text matching score requires a forward pass."}, {"title": "Ethical considerations", "content": "Contrastive models can be used in consumer-facing retrieval and search systems by major tech companies, and so failures can have a wide impact. Extensive bias has been documented in such models. Although our paper primarily evaluates the generic case of improving multimodal retrieval scores, we have also shown that NNN works to debias targeted attributes, such as gender. Still, our method should not be seen as a replacement for human oversight and careful training dataset curation."}, {"title": "Appendix", "content": ""}, {"title": "Baselines", "content": ""}, {"title": "DBNorm", "content": "The main DBNorm scoring function, DualIS, is described as follows: given a query q, retrieval candidate ri, reference query database Q, and reference retrieval candidate database R, the normalized score \u015d(q, ri) is computed using the following expressions (where s(q, r) denotes the dot product score between the embeddings):\n\n$\\hat{s}(q, r_i) = S_R * S_Q$\n\n$S_R = \\frac{exp(\\beta_1 s(q, r_i))}{\\Sigma_{r\\in R} exp(\\beta_1 s(q, r_i))}$\n\n$S_Q = \\frac{exp(\\beta_2 s(q, r_i))}{\\Sigma_{q\\in Q} exp(\\beta_2 s(q, r_i))}$\n\nDualDIS is a variant of DualIS that uses the original s(q, ri) score instead of \u015drq,ri or sqrq,ri for a given query q if the closest retrieval candidate to q is not in a precomputed \u201cactivation set\u201d that contains all likely hubs. See Wang et al. (2023) for details on how the activation sets are computed. In our experiments, we find that DualDIS and DualIS are very similar in performance (Table A6, A7).\nIn our experiments, we use the training images as the reference retrieval candidate database for image retrieval and the training captions for text retrieval. Note that NNN has the advantage of requiring a reference query database only, and does not use a reference retrieval candidate database. Moreover, NNN has a constant runtime with respect to the reference database size for calculating each individual normalized score while DBNorm has a linear runtime since the summation in the denominator requires all reference embeddings."}, {"title": "QBNorm", "content": "QBNorm is equivalent to DBNorm when \u03b2\u2081 is set to 0. Since our hyperparameter sweep of DBNorm includes \u03b2\u2081 = 0, we do not explicitly include QBNorm as a baseline in our results."}, {"title": "Distribution Normalization (DN)", "content": "DN computes a first-order approximation of the DualIS normalization score by normalizing the query and retrieval embeddings to have zero mean based on reference datasets. While it also has constant time performance for each query, we find that it has far lower accuracy gains than NNN."}, {"title": "Results for all methods", "content": "A full comparison of DN, DualIS, DualDIS, and NNN is shown in Table A6 and A7."}, {"title": "Hyperparameter selection", "content": ""}, {"title": "NNN", "content": "We compute the hyperparameters used for retrieval in Section 3 on a per-model, evaluation dataset, and reference query dataset basis. To do so, we perform a hyperparameter sweep on\n\n$\u03b1 \u2208 {0.25, 0.375, 0.5, . . ., 1.5}$\nand\n\n$k \u2208 {1, 2, 4, . . .,512}$.\n\nWe evaluate hyperparameters with image retrieval performed on a randomly selected split of the training set from the evaluation dataset. For Flickr30k, we take a split of 1,000 images and their 5,000 corresponding captions, and for COCO, we take a split of 5,000 images and their 25,000 corresponding captions. When selecting hyperparameters, we optimize for R@1 accuracy, and find that this generally does not come with significant degredation in R@5 or R@10 performance. We present the hyperparameters we use for text-to-image retrieval in Table Al and for image-to-text retrieval in Table A2.\nWe find four main trends in hyperparameter selection: (1) for out-of-distribution reference query databases, smaller \u03b1 (0.25 to 0.5) and k (8 to 16)"}, {"title": "DBNorm", "content": "To tune the hyperparameters \u03b2\u2081 and \u03b22, we first performed a grid sweep in logspace on\n\n$log \u03b2_1, log \u03b2_2 \u2208 {log 0.001, ..., log 400}$\n\nwith a resolution of 20 values. We found that the best performing \u03b2\u2081 and \u03b22 occupied a tight range, so we performed a denser sweep on\n\n$log \u03b2_1 \u2208 {log 0.001,..., log 15}$"}, {"title": "Runtime", "content": "A quantitative comparison of NNN runtimes using an exhaustive search (\"Base\" column) on GPU and using a Faiss index for computing bias scores is shown in Table A5. All of our experiments can be run using a single NVIDIA V100 GPU."}, {"title": "Full retrieval results", "content": "We present the full results of NNN applied to both text-to-image and image-to-text retrieval for the Flickr30k and COCO datasets, including R@1, 5, and 10 with associated 95% confidence intervals in tables A8, A9, A10, A11. nnn provides a consistent improvement in performance, even at higher recall values, but provides the greatest improvement to R@1. Confidence intervals are computed with bootstrapping."}, {"title": "Ablation Study", "content": "In some scenarios, it is possible that one may not have access to a very large reference query dataset."}, {"title": "Crossmodal attention", "content": "We find that NNN consistently increases retrieval accuracy in contrastive models, but does not significantly improve cross-attention models: for the image-text matching version of BLIP on COCO, Image Recall@1 improves from 66.16% to 66.24%."}, {"title": "Image and caption bias (extended results)", "content": "In Figure A2, we show more examples of reducing hubness using NNN for both text retrieval and image retrieval. The effect is more observable in image retrieval as there are 5 times more captions than images."}]}