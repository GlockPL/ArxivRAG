{"title": "The Unreasonable Effectiveness of Gaussian Score Approximation for Diffusion Models and its Applications", "authors": ["Binxu Wang", "John J. Vastola"], "abstract": "Diffusion models have achieved remarkable results in multiple domains of generative modeling. By learning the gradient of smoothed data distributions, they can iteratively generate samples from complex distributions, e.g., of natural images. The learned score function enables their generalization capabilities, but how the learned score relates to the score of the underlying data manifold remains largely unclear. Here, we aim to elucidate this relationship by comparing the learned scores of neural-network-based models to the scores of two kinds of analytically tractable distributions: Gaussians and Gaussian mixtures. The simplicity of the Gaussian model makes it particularly attractive from a theoretical point of view, and we show that it admits a closed-form solution and predicts many qualitative aspects of sample generation dynamics. We claim that the learned neural score is dominated by its linear (Gaussian) approximation for moderate to high noise scales, and supply both theoretical and empirical arguments to support this claim. Moreover, the Gaussian approximation empirically works for a larger range of noise scales than naive theory suggests it should, and is preferentially learned by networks early in training. At smaller noise scales, we observe that learned scores are better described by a coarse-grained (Gaussian mixture) approximation of training data than by the score of the training distribution, a finding consistent with generalization. Our findings enable us to precisely predict the initial phase of trained models' sampling trajectories through their Gaussian approximations. We show that this allows one to leverage the Gaussian analytical solution to skip the first 15-30% of sampling steps while maintaining high sample quality (with a near state-of-the-art FID score of 1.93 on CIFAR-10 unconditional generation). This forms the foundation of a novel hybrid sampling method, termed analytical teleportation, which can seamlessly integrate with and accelerate existing samplers, including DPM-Solver-v3 and UniPC. Our findings strengthen the field's theoretical understanding of how diffusion models work and suggest ways to improve the design and training of diffusion models.", "sections": [{"title": "1 Introduction", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Song & Ermon, 2019; Ho et al., 2020; Song et al., 2021) have revolutionized the field of generative modeling by achieving remarkable performance across diverse domains, including image (Rombach et al., 2022), audio (Kong et al., 2021; Chen et al., 2021; Popov et al., 2021), and video (Harvey et al., 2022; Ho et al., 2022; Blattmann et al., 2023) generation. Despite these successes, why diffusion models perform as well as they do is poorly understood. Two major open questions are as follows. First, given that samples are generated via a dynamic process from noise, when do different sample features emerge, and what controls which features appear in the final sample? Second, given the empirical fact that diffusion models often generalize beyond their training data, what distribution do they learn instead?\nCentral to understanding these models is characterizing the score function, the dynamic vector field learned by a neural network during training. By modeling the gradient of the smoothed data distribution, the score function guides the iterative sample generation process. Curiously, it has been observed to deviate from its theoretically expected behavior: it may not be the gradient of a mixture of data points, and it may not even be a gradient field at all (Wenliang & Moran, 2023). Such deviations raise fundamental questions about the nature of what networks learn and how they generalize training data.\nHow does the learned score function compare to that of the underlying data manifold, especially given that neural networks only have access to a finite set of training examples in practice? Critically, a network that learns the exact score function of the training set-i.e., a mixture of delta functions centered on training data-can only reproduce training examples, and hence cannot generalize. This observation suggests that neural networks optimize for a balance between learning the score function of the training set and capturing aspects of the underlying data manifold. However, the precise characteristics of the learned score function, and how the development of this balance proceeds over the course of training, remain largely unexplored.\nOne simple possibility is that diffusion models generalize in part by learning the score function associated with certain summary statistics of the traning set, like its mean and covariance matrix. If this were true, one would expect the score function to be that of a Gaussian model, i.e., linear in its state features. While it is decidedly not true that learned neural scores are Gaussian, we find that this is much closer to being true than one might expect. To show this, we first mathematically characterize the properties of the Gaussian model. Next, we compare the Gaussian model's score function to real score functions, and find that the Gaussian model well-approximates the behavior of neural network models for moderate to high noise levels (Fig. 1A). Finally, we show how this insight can be applied to accelerate sampling from diffusion models (Fig. 1B)."}, {"title": "1.1 Main contributions", "content": "Our main contributions are as follows:\n\u2022 We thoroughly analyze the Gaussian score model, including characterizing its sampling trajectories by exactly solving the associated probability flow ODE (PF-ODE). (Sec. 3.1-3.2)\n\u2022 We show how the Gaussian model recapitulates nontrivial features of 'real' sample generation, including the low dimensionality of sampling trajectories, the time course of feature emergence, and the effect of perturbations during sampling. (Sec. 3.3)\n\u2022 We theoretically (Sec. 4.1) and empirically (Sec. 4.2-4.3) support our claim that, in the high-noise regime, the score field of real diffusion models is dominated by its Gaussian/linear approximation.\n\u2022 We find that learned neural scores align more closely with those of low-rank Gaussian mixtures than the (delta mixture) score of the training distribution. (Sec. 4.4)\n\u2022 We characterize the learning dynamics of neural score functions, and in particular find that Gaussian/linear structure is preferentially learned early in training. (Sec. 5)\n\u2022 We apply our findings by using the Gaussian model solution to accelerate sampling. (Sec. 6)"}, {"title": "2 Mathematical formulation", "content": "In this section, we review the mathematics of diffusion models and define the idealized score models of interest. To streamline notation and match popular implementations of diffusion models, we use the \"EDM\" framework of Karras et al. (2022). This choice implies no loss of generality, as a simple reparametrization allows one to map to other formalisms (Song et al., 2021; Ho et al., 2020). See Appendix D.2 for more details on this point."}, {"title": "2.1 Basics of score-based modeling", "content": "Let \\(p_{\\text{data}}(x)\\) be a data distribution in \\(\\mathbb{R}^D\\). The core idea of diffusion models is to corrupt this distribution with (usually Gaussian) noise, and to learn to undo this corruption; this way, one can sample from the generally complex distribution \\(p(x)\\) by first sampling from a Gaussian, and then iteratively removing noise. The noise-corrupted distribution is defined as\n\n\\[p(x; \\sigma) := \\int_{\\mathbb{R}^D} dx' p(x | x', \\sigma) p_{\\text{data}}(x') = \\int_{\\mathbb{R}^D} dx' \\mathcal{N}(x; x', \\sigma^2 I) p_{\\text{data}}(x') \\quad (1)\\]\n\nwhere \\(\\sigma \\geq 0\\) is the noise scale. There are many ways to remove noise, but a popular method introduced by Song et al. (2021) is to utilize the PF-ODE, which in the Karras et al. (2022) formulation has the form\n\n\\[dx = - \\sigma \\frac{d \\sigma}{dt} s(x, \\sigma) dt \\quad (2)\\]\n\nwhere \\(t\\) denotes time and \\(\\sigma_t\\) denotes the noise scale at time \\(t\\) (with \\(\\frac{d \\sigma}{dt}\\) its derivative). The key ingredient of this process is the score function \\(s(x, \\sigma) := \\nabla_x \\log p(x; \\sigma)\\), i.e., the gradient of the noise-corrupted data distribution. To generate a sample, one samples \\(x_T \\sim \\mathcal{N}(x; \\sigma_T^2 I)\\) with a large \\(\\sigma_T = \\sigma_{\\text{max}}\\), and then integrates Eq. 2 backward in time until \\(\\sigma_{t_{\\text{min}}} = \\sigma_{\\text{min}} \\approx 0\\). Since we are interested in understanding these dynamics in detail, we must carefully study the dynamic vector field \\(s(x, \\sigma)\\).\nThere are various ways to learn a parameterized score approximator \\(\\hat{s}_\\theta(x, \\sigma)\\) (Yang et al., 2023), including score matching (Hyv\u00e4rinen, 2005) and sliced score matching (Song et al., 2020b), but the current most popular and performant approach is denoising score matching (Raphan & Simoncelli, 2006; Vincent, 2011; Song & Ermon, 2019). The corresponding objective, which is minimized by the score function of the training set, is\n\n\\[\\mathcal{L}_\\theta = E_{y \\sim p_{\\text{data}}} E_{\\eta \\sim \\mathcal{N}(0, \\sigma^2 I)} [ ||D_\\theta(y + \\eta, \\sigma) - y||^2 ] \\quad (3)\\]"}, {"title": "2.2 Idealized score models of interest", "content": "In this paper, we will try to understand the score functions learned by real diffusion models by comparing them to the score functions of certain idealized distributions. We consider three such idealized score models: the Gaussian model, which assumes only the overall mean and covariance of the data are learned; the score of the delta mixture distribution, which is the Exact score of the training dataset, assuming training data are precisely memorized and that no generalization occurs; and the Gaussian mixture model (GMM), which lies somewhere between the previous two models. Below, we write down some basic but important properties of each of these idealized score models.\nGaussian model. Suppose the target distribution is a Gaussian distribution with mean \\(\\mu \\in \\mathbb{R}^D\\) and covariance \\(\\Sigma \\in \\mathbb{R}^{D \\times D}\\). Since the effective dimensionality of data manifolds is often much lower than the dimensionality of the ambient space (Turk & Pentland, 1991; Hinton & Salakhutdinov, 2006; Camastra & Staiano, 2016) (consider, e.g., the dimensionality of image manifolds versus the dimensionality of pixel space),"}, {"title": "3 Exact solution and interpretation of the Gaussian score model", "content": "In this section, we present the exact solution to the Gaussian model. The utility of this model is that it is simple enough that it can be solved exactly, and hence one can precisely quantify various aspects of sample generation dynamics. First, we briefly describe the solution method (Sec. 3.1-3.2), and then we discuss the qualitative insights we can obtain from it (Sec. 3.3). In Sec. 4-5, we will show how this admittedly simple model relates to real diffusion models."}, {"title": "3.1 Solution method: Exploiting a decomposition of the covariance matrix", "content": "Since the covariance \\(\\Sigma\\) is symmetric and positive semidefinite, it has a compact singular value decomposition \\(\\Sigma = U \\Lambda U^T\\), where \\(U = [u_1, ..., u_r]\\) is a \\(D \\times r\\) semi-orthogonal matrix and \\(\\Lambda \\in \\mathbb{R}^{r \\times r}\\) is diagonal with \\(\\Lambda_{k} := \\Lambda_{kk} > 0\\) for all \\(k\\). The columns of \\(U\\), \\(u_k\\), are the principal component (PC) axes along which the Gaussian mode varies, and their span comprises the 'data manifold' of the Gaussian model.\nThis decomposition is useful since it allows us to write the score and denoiser of the Gaussian model in a more explicit form. Using the Woodbury identity (Woodbury, 1950),\n\n\\[(\\sigma^2 I + U \\Lambda U^T)^{-1} = I - U (\\sigma^2 I + \\Lambda^{-1})^{-1} U^T = [I - U \\text{diag}[\\frac{\\Lambda_k}{\\Lambda_k + \\sigma^2}] U^T] \\quad (11)\\]\n\nFor convenience, define the diagonal matrix \\(\\tilde{\\Lambda} := \\text{diag}[\\frac{\\Lambda_k}{\\Lambda_k + \\sigma^2}]\\). We can now write\n\n\\[s(x, \\sigma) = \\frac{1}{\\sigma^2} (I - U \\tilde{\\Lambda} U^T) (\\mu - x) \\quad D(x, \\sigma) = \\mu + U \\tilde{\\Lambda} U^T (x - \\mu) . \\quad (12)\\]\n\nSlightly more explicitly, the optimal denoiser can be written\n\n\\[D(x, \\sigma) = \\mu + \\sum_{k=1}^r \\frac{\\Lambda_k}{\\Lambda_k + \\sigma^2} [u_k^T (x - \\mu)] u_k. \\quad (13)\\]"}, {"title": "3.2 Closed-form solution of PF-ODE for Gaussian model", "content": "Using the rewritten score (Eq. 12), the PF-ODE (Eq. 2) takes a particularly simple form:\n\n\\[\\dot{x}_t = \\frac{\\sigma}{\\sigma^2} (I - U \\tilde{\\Lambda} U^T) (x - \\mu). \\quad (14)\\]\n\nThe above ODE is linear, and its dynamics along each principal axis \\(u_k\\) are independent. Solving it in the usual way (see Appendix D.1), we find\n\n\\[\\begin{aligned} x_t &= \\mu + x_t^+ + \\sum_{k=1}^r \\psi(t, \\lambda_k) c_k(T) u_k \\\\ x_t^+ &:= (I - U U^T)(x_T - \\mu) \\\\ \\psi(t, \\lambda) &:= \\frac{\\sigma_t}{\\sigma_T} \\sqrt{\\frac{\\sigma_T^2 + \\lambda}{\\sigma_t^2 + \\lambda}} \\\\ c_k(T) &:= u_k^T(x_T - \\mu) . \\end{aligned} \\quad (15)\\]\n\nThe solution has three components: (i) the distribution mean, (ii) an off-manifold component, and (iii) an on-manifold component. The distribution mean term does not change throughout sample generation. The off-manifold component shrinks to zero as \\(t \\rightarrow 0\\). The on-manifold component, which is determined by the manifold-projected difference between \\(x\\) and \\(\\mu\\), evolves independently according to \\(\\psi(t, \\lambda)\\) along each PC direction."}, {"title": "3.3 Interpreting Gaussian generation dynamics", "content": "The closed-form solution of the Gaussian model provides the exact relationship between the covariance matrix and sample generation dynamics. Since the dynamics are separable along each PC, we only need to study the functions \\(\\xi\\) and \\(\\psi\\) to describe how the variance of the data distribution along each PC influences dynamics along that direction. Here, we highlight interesting consequences of the Gaussian model's exact solution related to four aspects of sample generation: 1) the determination of the final sample, 2) the geometry of trajectories, 3) the feature emergence order, and 4) the effect of perturbations."}, {"title": "4 The far-field Gaussian structure of real diffusion models", "content": "Most distributions interesting enough to train generative models on are not Gaussian, so how is the exact solution of the Gaussian model related to real diffusion models trained on complex datasets? Our central claim is that, at moderate to high noise scales, the score function of an arbitrary bounded point cloud is nearly indistinguishable from that of a Gaussian with matching mean and covariance. We call such structure \u201cfar-field\u201d; we borrow the term from physics, where it describes a region far enough from the source of (e.g., electromagnetic) waves that the size and shape of the source can be neglected."}, {"title": "4.1 Theoretical basis for far-field Gaussian score structure", "content": "Consider a point cloud \\({y_i}_{i=1}^N \\subset \\mathbb{R}^D\\). When the noise level is high and/or the query point is far from the point cloud's support, we claim that the score of this distribution is nearly indistinguishable from that of a Gaussian with the same mean and covariance. Here, we provide a theoretical argument for this claim.\nLet \\(\\mu\\) and \\(\\Sigma\\) denote the mean and covariance of the point cloud. Recall from Sec. 2.2 that the score function of the (noise-corrupted) point cloud is\n\n\\[s(x; \\sigma) = \\sum_{i=1}^N w_i(x) \\frac{\\mu - x}{\\sigma^2} = \\frac{\\mu - x}{\\sigma^2} + \\sum_{i=1}^N w_i(x) \\frac{(y_i - \\mu)}{\\sigma^2} \\\\ w_i(x) = \\frac{\\text{exp}(-\\frac{1}{2\\sigma^2} ||x - y_i||^2)}{\\sum_{j=1}^N \\text{exp}(-\\frac{1}{2\\sigma^2} ||x - y_j||^2)} = \\frac{\\text{exp}(-\\frac{1}{2\\sigma^2} ||\\mu - y_i||^2 + (x - \\mu)^T (y_i - \\mu))}{\\sum_{j=1}^N \\text{exp}(-\\frac{1}{2\\sigma^2} ||\\mu - y_j||^2 + (x - \\mu)^T (y_j - \\mu))} \\quad (24)\\]\n\nNote that we have rearranged the score to be written in terms of distances to the point cloud mean \\(\\mu\\). In the far-field regime, one expects this distance to account for most of the distance to each data point.\nNext, note that \\((x - \\mu)^T(\\mu - y_i)\\) is on the order of \\(\\sigma \\sqrt{\\text{tr}\\Sigma}\\) and \\(||\\mu - y_i||^2\\) is on the order of \\(\\text{tr}\\Sigma\\) (see Appendix D.8 for a detailed derivation). Hence, when \\(\\sigma \\gg \\sqrt{\\text{tr}\\Sigma}\\) - i.e., when the noise scale is substantially larger than the radius of the point cloud along each direction-the cross term will dominate. When this term dominates, to leading order we have\n\n\\[w_i(x) \\approx \\frac{1 + \\frac{1}{\\sigma^2} (x - \\mu)^T (y_i - \\mu)}{\\sum_j [1 + \\frac{1}{\\sigma^2} (x - \\mu)^T (y_j - \\mu)]} = \\frac{1 + \\frac{1}{\\sigma^2} (x - \\mu)^T (y_i - \\mu)}{N} + O(\\frac{1}{\\sigma^4}) \\quad (25)\\]\n\nSubstituting this result back into the point cloud score (Eq. 24), we find that\n\n\\[s(x, \\sigma) \\approx \\frac{\\mu - x}{\\sigma^2} + \\frac{1}{\\sigma^2} \\sum_{i=1}^N \\frac{1}{N} (y_i - \\mu) + \\frac{1}{\\sigma^4} \\sum_{i=1}^N \\frac{1}{N} (y_i - \\mu) (x - \\mu)^T (y_i - \\mu) = \\frac{\\mu - x}{\\sigma^2} + \\frac{1}{\\sigma^4} \\Sigma (\\mu - x) . \\quad (26)\\]\n\nFinally, we observe that the score function of the Gaussian model (see Sec. 2.2) is identical to leading order:\n\n\\[(\\frac{1}{\\sigma^2}I + \\frac{1}{\\sigma^4} \\Sigma)^{-1} (\\mu - x) \\approx \\frac{\\mu - x}{\\sigma^2} + \\frac{1}{\\sigma^6} \\text{tr}\\Sigma \\frac{\\mu - x}{\\sigma^2} = O(\\frac{1}{\\sigma^6}). \\quad (27)\\]\n\nThis proves the claim. As a parenthetical comment, we note that our expansion strategy is similar in spirit to the multipole expansion used in (for example) electrodynamics (Jackson, 2012); in both cases, the key idea is to approximate a vector field by matching certain low-order statistics.\nIs the Gaussian approximation really only accurate when \\(\\sigma \\gg \\sqrt{\\text{tr}\\Sigma}\\)? In the next subsection, we empirically show that it works quite well even for somewhat lower noise scales and in particular when the noise scale is smaller than the top eigenvalue of the covariance matrix (Fig. 6). The surprisingly broad range of noise scales for which the Gaussian approximation works well is what led us to note its \u2018unreasonable effectiveness'.\nAt least theoretically, this need not be true for arbitrary data distributions. Special data distribution features, like low-rank structure and smoothness, appear to help the Gaussian approximation work for smaller noise scales than the above argument suggests. See Appendix D.9 for examples and more discussion of this point."}, {"title": "4.2 Learned score vectors are empirically well-approximated by the Gaussian model", "content": "Motivated in part by the theoretical argument from the previous subsection, we empirically validated the claim that the score function of pre-trained diffusion models is well-described by the Gaussian model in the far-field/high-noise regime. To do this, we examined the score functions of three pre-trained models (of CIFAR-10, FFHQ64, and AFHQv2-64) from Karras et al. (2022). For each dataset, we computed the scores of several tractable approximations of the data:\n\u2022 Isotropic Gaussian score. (Iso) This score simplifies the Gaussian model by removing covariance information, and has \\(s(x, \\sigma) := (\\mu - x)/\\sigma^2\\).\n\u2022 Gaussian score. (see Eq. 5) The Gaussian model with the mean and covariance of the dataset.\n\u2022 Exact point cloud score. (Exact delta, see Eq. 10) The score of the training dataset.\n\u2022 Per-class Gaussian mixture score. (GMM; see Eq. 7) For datasets with class labels, (e.g. CIFAR-10), we computed the score of a GMM with one Gaussian mode corresponding to each class, equipped with the mean and covariance of that class. See Appendix A.2 for more details.\nFor various noise scales \\(\\sigma\\), we evaluated the neural and analytical scores at random points sampled from \\(\\mathcal{N}(0, \\sigma^2 I)\\), and quantified their difference using the fraction of unexplained variance, defined via\n\n\\[\\text{fraction of unexplained variance} := \\frac{||s_{\\text{EDM}}(x, \\sigma) - s_{\\text{analy}}(x, \\sigma)||^2}{||s_{\\text{EDM}}(x, \\sigma)||^2} \\quad (28)\\]\n\nWe characterized the average deviation as a function of noise scale for each type of idealized score (Fig. 6).\nGaussian score predicts the learned score at high noise. For all three datasets, at most noise levels (\\(\\sigma > 1.08\\)), the Gaussian score explains almost all variance (\\(> 99\\%\\)) of the neural score. As expected, it explains more variance than the isotropic model, which does not incorporate covariance information. Further, for CIFAR-10, at all levels, the 10-class Gaussian mixture score predicts the neural score slightly better than the Gaussian score, which shows that adding more modes indeed helps capture the details of the score field. Surprisingly, at most noise levels (\\(\\sigma > 1.08\\)) the 10-mode model improved the fraction of explained variance by less than \\(2.5 \\times 10^{-3}\\), showing that even a single Gaussian may be sufficient for explaining the learned neural score. To our surprise and against the intuition suggested by our earlier theoretical argument (see the red dashed line in Fig. 6), the Gaussian model well-approximates the neural score even when \\(\\sigma < \\sqrt{\\lambda_{\\text{max}}}\\).\nNeural score deviates from 'exact delta' score at low noise. Although the exact point cloud score slightly outperforms the Gaussian model in the high-noise regime, it deviates substantially from the learned score in the low-noise regime (Fig. 6), and in fact performs worse than the Gaussian and Gaussian mixture models for all three datasets. This suggests that, especially in the low noise regime, the models learned something substantially different from the 'exact' score, and more similar to the Gaussian or Gaussian mixture scores. As previously argued, deviations from the 'exact' score model can be viewed as a signature of generalization (Kadkhodaie et al., 2023; Yi et al., 2023). In Sec. 4.4, we study the effect of adding more modes to a Gaussian mixture approximation more comprehensively."}, {"title": "4.3 Gaussian model empirically captures early sample generation dynamics", "content": "If the Gaussian model produces score vectors similar to those learned by neural networks, then their sampling trajectories may (at least initially) be similar too. On the other hand, it is also possible that initially small differences may accumulate over the course of sample generation, and hence that real sampling trajectories differ substantially from those predicted by the Gaussian model. To test this, we compared the solutions of PF-ODE based on the idealized score models to the sampling trajectories of neural diffusion models with deterministic samplers. We used the exact solution for the Gaussian model (Eq. 15), Heun's 2nd-order method to integrate the neural scores, and an off-the-shelf RK4 integrator to integrate the mixture scores."}, {"title": "4.4 Beyond Gaussian: Low-rank Gaussian mixture scores as a model of learned neural scores", "content": "Our previous analyses demonstrated that learned score functions closely match the Gaussian model at high noise levels, but not at low noise levels. To study the behavior of the learned score function at lower noise-or, equivalently closer to the data manifold, we need to go beyond the single Gaussian. As a next step, we studied the score of a Gaussian mixture model whose covariance was allowed to be low-rank. We asked 1) to what extent does adding Gaussian components help explain the learned neural score? and 2) what is the minimum rank required for good score approximation?\nWe systematically compared the structure of neural scores to the scores of GMMs fit to the same training data (Fig. 9). We varied both the number of modes and the ranks of the associated covariance matrices; for the details of the fitting procedure, see Appendix A.5. In brief, we performed k-means clustering on the training set and utilized the empirical mean and covariance of each cluster to define Gaussian components. The fraction of samples within each cluster was used to define the weights of components. To obtain low-rank covariance matrices, we computed the eigendecomposition of the empirical covariances and retained the top r directions.\nNeural score is best explained by Gaussian mixture with moderate number of modes. First, note that both the Gaussian model and 'exact' score models are special cases of the GMM; the former has"}, {"title": "5 Learning of far-field Gaussian score structure", "content": "In the preceding sections (Sec. 4.2, 4.3, 4.4), our analyses focused on the structure of the score of trained diffusion models, but did not touch on the question of how that structure emerges during training. Given that overparameterized function approximators like neural networks are thought to learn 'simpler' structure first, it stands to reason that Gaussian/linear score structure may be learned relatively early. Is this true?\nTo test this idea, we trained neural network score approximators on different datasets using the training procedure described by Karras et al. (2022). Throughout training, we sampled query points xt from the noised distribution, and compared the neural score to the three idealized score models from Sec. 2.2.\nAs we have observed, at higher noise scales (\\(\\sigma \\geq 10\\)), all score approximators are similar to the neural score after convergence. During training, the neural score function steadily converges to these approximators as well (Fig. 12, Fig. 25). Intriguingly, at lower noise scales, the network displays non-monotonic learning dynamics for simpler scores. Specifically, the neural score initially aligns with simpler score models (the Gaussian model and a GMM with few modes) before starting to deviate from them. This pattern was consistently observed across various training sets (refer to Fig. 25 for CIFAR-10, AFHQ, FFHQ). On the other hand, the neural score approached more complex score approximators (the delta mixture) monotonically. This effect suggests an initial tendency to fit the score of simpler distributions.\nThis tendency manifested slightly differently on the MNIST dataset (Fig. 26). Here, the score network approaches different GMM score approximators in order of increasing complexity. Namely, the deviation between the neural score and simpler Gaussian scores decreased and reached a floor, before moving on to more complex models, such as 2-mode and 5-mode Gaussian mixtures. Although the non-monotonic effect"}, {"title": "6 Application: Accelerating sampling via teleportation", "content": "Above, we have shown that the Gaussian model's exact solution provides a surprisingly good approximation to the early sampling trajectory. We can exploit this fact to accelerate diffusion by 'analytical teleportation' (Fig. 14 A). By this, we mean replacing a certain number of initial PF-ODE integration steps with a single evaluation of the Gaussian analytical solution at some intermediate time t' (or equivalently noise scale \\(\\sigma_{\\text{skip}}\\)). In this way, one can nontrivially reduce the number of neural function evaluations (NFEs) required to generate a sample. In principle, this speedup can be combined with any deterministic or stochastic sampler.\nExperiment 1. First, we showcase its effectiveness using the optimized second-order Heun sampler from Karras et al. (2022), which yields near state-of-the-art image quality and efficiency. See Alg. 1 and Appendix A.7, B.2 for method details and additional experiments with DDIM (Song et al., 2020a).\nWe evaluated our proposed hybrid sampler on unconditional diffusion models trained on CIFAR-10, FFHQ-64, and AFHQv2-64. We sampled 50,000 images using both the Heun sampler and our hybrid sampler with various numbers of skipped steps (or equivalently, different times t'), and evaluated the Frechet Inception Distance (FID) in each case. We found that we can consistently save 15-30% of NFEs at the cost of a less"}, {"title": "7 Related work", "content": "Diffusion models and Gaussian mixtures. There has been increasing interest in characterizing diffusion models associated with tractable target distributions, and specifically in Gaussian and Gaussian mixture models. Shah et al. (2023) and Gatmiry et al. (2024) focused on learning to generate samples from Gaussian mixtures using diffusion models, and Shah et al. (2023) found a connection between gradient-based score matching and the expectation-maximization (EM) algorithm and derived convergence guarantees. Concurrently, Pierret & Galerne (2024) derived the exact solution to the reverse SDE and PF-ODE for a Gaussian model, which allowed them to compare Wasserstein errors for any sampling scheme. But to our knowledge, no existing work has provided the in-depth comparisons between idealized score models and neural scores that we present here."}, {"title": "8 Discussion", "content": "In summary, even for real diffusion models trained on natural images, in the high-noise regime the neural score is well-approximated by a Gaussian model; in the low-noise regime, Gaussian mixture models approximate neural scores better than the score of the training distribution. We mathematically characterized the sampling trajectories of the Gaussian model, and found that it recapitulates various aspects of real sampling trajectories. Finally, we leveraged these insights to accelerate the initial phase of sampling from diffusion models. Below, we mention additional implications of our results for the training and design of diffusion models:\nNoise schedule. As the early time evolution of the sample distribution is well-predicted by the Gaussian model, we in principle do not need to sample high noise levels to train a denoiser \\(D(x, \\sigma)\\). Instead, sampling can directly begin (or 'warm start') from the non-isotropic Gaussian distribution \\(\\mathcal{N}(\\alpha_t \\mu, \\alpha_t^2 \\Sigma + \\sigma_t^2 I)\\).\nModel design. Since it is empirically true that neural scores are dominated by Gaussian/linear structure at high noise levels, we can directly build this structure into the neural network to assist learning. For example, we can add a linear by-pass pathway based on the covariance of training distribution, and let the neural network only learn the nonlinear residual not accounted for by the Gaussian model term.\nTraining distribution. As the neural networks first need to learn the data covariance structure by score matching, we may be able to assist learning by reshaping the training distribution. One hypothesis is that if we pre-condition the target distribution by whitening its spectrum, then the neural score may converge faster. If true, this may explain the higher efficiency of latent diffusion models (Rombach et al., 2022): with KL regularization, the autoencoder not only compresses the state space, but also 'whitens' the image distribution by morphing it to be closer to a Gaussian distribution."}, {"title": "9 Limitations and future work", "content": "Higher-resolution and conditional diffusion models. Our experiments focused on lower-resolution image generative models. Generalizing our results to higher-resolution models, or popular text-to-image conditional diffusion models (Rombach et al., 2022), involves overcoming difficulties related to covariance estimation. For high-dimensional models, estimating covariances is substantially harder given a limited"}, {"title": "A Detailed Methods", "content": ""}, {"title": "A.1 Image datasets and Pre-trained Models", "content": ""}, {"title": "A.2 Idealized Score Approximations", "content": "In the paper, we compared several analytical approximations of the score. We listed their formula below.\nIsotropic score, only depends on the mean of data, isotropically pointing towards \\(\\mu\\).\n\n\\[ \\frac{\\mu - x}{\\sigma^2} \\quad"}]}