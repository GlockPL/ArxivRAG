{"title": "Life, uh, Finds a Way: Systematic Neural Search", "authors": ["Alex Baranski", "Jun Tani"], "abstract": "We tackle the challenge of rapidly adapting an agent's behavior to solve spatiotemporally continuous problems in novel settings. Animals exhibit extraordinary abilities to adapt to new contexts, a capacity unmatched by artificial systems. Instead of focusing on generalization through deep reinforcement learning, we propose viewing behavior as the physical manifestation of a search procedure, where robust problem-solving emerges from an exhaustive search across all possible behaviors. Surprisingly, this can be done efficiently using online modification of a cognitive graph that guides action, challenging the predominant view that exhaustive search in continuous spaces is impractical. We describe an algorithm that implicitly enumerates behaviors by regulating the tight feedback loop between execution of behaviors and mutation of the graph, and provide a neural implementation based on Hebbian learning and a novel high-dimensional harmonic representation inspired by entorhinal cortex. By framing behavior as search, we provide a mathematically simple and biologically plausible model for real-time behavioral adaptation, successfully solving a variety of continuous state-space navigation problems. This framework not only offers a flexible neural substrate for other applications but also presents a powerful paradigm for understanding adaptive behavior. Our results suggest potential advancements in developmental learning and unsupervised skill acquisition, paving the way for autonomous robots to master complex skills in data-sparse environments demanding flexibility.", "sections": [{"title": "1 Introduction", "content": "For a system to be truly adaptive, it must always be able to find a way of accomplishing any chosen physically achievable goal in a given context, given sufficient time. Doing this means being able to in-principle generate any behavior, i.e. try anything. We argue that it is possible to use cognitive graphs to accomplish this. Many works inspired by cognitive graphs focus on learning good state-space decompositions and using goal-directed planning to improve generalization [1, 2]. These are indeed virtues of cognitive graphs, but we go farther and show that it is possible to actually enumerate the set of all behaviors using a sequence of cognitive graphs. Enumeration makes it possible to search over all possible behaviors, enabling hyper-robust adaptation.\nA popular viewpoint in machine learning research is that artificial general intelligence can be achieved with systems that can generalize from their training data, enabling them to succeed at tasks they've never encountered before [3]. This is theoretically difficult [4], and success requires tremendous amounts of training data and compute. This contrasts with the rapid adaptability of biological organisms. One possible explanation is that organisms do not only rely on generalization, but also excel at ad-hoc solution finding, taking advantage of search. Search is a staple of symbolic approaches to artificial intelligence [5-7], and is often used to augment deep networks [8, 9]. Search dynamics are found in genes [10], slime molds [11], and primate brain activity [12], suggesting it is a foundation of behavior, rather than emergent.\nExhaustive search may seem impossible for continuous trajectories, but mammalian brains contain the very mechanisms that could support it, cognitive graphs [13] and maps [14]. These are implemented by a variety of spatially-sensitive cells such as band [15], grid [16], and place [17] cells in the hippocampus and entorhinal cortex, where they support spatial reasoning. Non-spatial analogs to these cells have been found in the same regions [18, 19] and across cortical areas [20, 21], suggesting a general-purpose algorithm.\nWe accomplish search over all possible behaviors by identifying behaviors with paths over a cognitive graph that segments state-space, a segraph. By identifying behaviors with trajectories that are sampled from paths over a segraph and using some simple relationships within the set of all segraphs, we show that it is possible in theory to enumerate all behaviors, even if those behaviors are arbitrarily long, complex, and spatiotemporally continuous. Segraphs come with the usual benefits of cognitive graphs, namely state-space decomposition and goal-oriented planning. We believe that this \u201cbehavior-as-search\" framing makes behavioral adaptation trivial. It may seem like a brute-force approach, but the framework automatically favors simpler behaviors and accommodates experience-biased search.\nWe describe a neural implementation inspired by known cell populations, including band cells, for which we introduce a novel model based on Fourier theory called Harmonic Relational Keys (HaRKs) that enables a variety of high-dimensional spectral computations. Segraphs are implemented using HaRKs and Hebbian learning [22], and we introduce a highly efficient neural pathfinding algorithm P. Implicit enumeration of behaviors occurs finitely within a single segraph, and infinitely over the sequence of segraphs, guided by experience collected while traversing paths. The system operates in continuous space-time without environment resets, as real organisms do. We introduce the Adaptive Realtime Metasearch over Segraphs (ARMS) algorithm to guide this search, as a proof-of-principle for the concept. Neural implementation of segraphs and the ARMS algorithm results in the systematic neural search for behaviors, demonstrating the biological feasibility of our framework."}, {"title": "2 Theory of Behavioral Enumeration", "content": "We start by operationalizing a \u201cbehavior\" as a finite trajectory through a continuous state-space, so formally we need a method of enumerating continuous trajectories, requiring that we not repeat the same trajectory twice. For a failed trajectory (red line, Fig. 1A) this is a good idea anyway. Which trajectory should be tried next? A minor variation (blue) or a major deviation (green)? A-priori we know neither the scope (overall region of state-space) nor the precision (resolution of control) of a successful behavior. There are infinitely many minor and major variations, so we could spend an infinite amount of time in one category and entirely miss a solution in the other. Until we find a satisficing [23] behavior, we need to keep incrementally expanding the scope and the precision of available behaviors.\nThis is accomplished using segraphs (cartoon schematic Fig. 1B). Vertices of the segraph are associated with subsets (segments) of state-space. We identify classes of behaviors with paths over a segraph. Under suitable constraints the set of paths over a segraph is finite and enumerable (Fig. 1C). Individual behaviors are identified with trajectories, which are sampled from a path by probabilistically sampling waypoints from the fields of the vertices along the path (Fig. 1D). The state-space coverage of the segraph and size of the vertex-fields controls the scope and precision of behaviors. A single segraph with finite scope and precision cannot enumerate all behaviors, to do this we must also enumerate segraphs.\nWe incrementally expand both the scope and precision of segraphs using two \u201cmutator\u201d functions, \"extension\u201d and \u201crefinement\u201d. Extension adds new fields that increase the coverage of the segraph around a particular vertex (increases scope), and refinement replaces an existing field with smaller fields (increases precision) . In each case we are always expanding the set of potential trajectories, in which sense we say that the \"child\" segraph resulting from a mutation \"contains\" its parent. A given segraph has many potential children which all contain it, but which it does not contain and which do not contain each other. This produces an infinite set of segraphs which is partially ordered [24] (Fig. 1E).\nUsing only extension and refinement it is impossible to hit every segraph in the set. However, based on the example in Fig. 1D where iv contains both ii and iii, we can see that each segraph has a backward \u201ccone\" containing all potential ancestors (Fig. 1F). As this containment relation directly involves sets of trajectories, we conjecture that so long as a lineage (sequence) of segraphs is balanced (extending and refining somewhat uniformly across state-space), then we can guarantee that any segraph in the set will eventually be contained by a segraph in the sequence. This means that for any given behavior (trajectory), there will eventually be a segraph in the sequence that can generate that behavior, allowing an exhaustive search over behaviors. While exhaustive search guarantees success, efficiency is also key. Our implementation focuses on searching more promising paths first, and segraph evolution tends to generate \u201cgood-enough\" behaviors before \u201cexcellent\u201d ones, creating a controllable time/quality trade-off that is in-line with how biological organisms learn. Also, more complex behaviors automatically take longer to find, as would be expected. The shared structure between paths induced by the segraph allows for efficient pruning of the search space. This, along with the physical necessity to only try paths beginning at the agent's current state, significantly narrows the search space."}, {"title": "3 Systematic Neural Search", "content": "We imagine an agent A that wants to be able to reach any point in an abstract state-space S = $\\mathbb{R}^d$ from any other point in S. It is endowed with a continuous sensorimotor controller K, which cannot fully navigate S on it's own. We equip A with a segraph G* that decomposes S into chunks that K can more easily navigate between, reaching distant goals via paths over G*. We describe the underlying neural implementation of this graph, and the ARMS algorithm that adapts it.\nNeural Substrate: Hebbian learning between two vectors, denoted $a \\triangleright b$, produces a matrix mapping a to b. Sums of such matrices act like dictionaries (Methods 6.1). By representing graph vertices with vectors, edges can be represented by Hebbian-learned matrices, and the entire graph is represented by the sum of its constituent edge-matrices (Methods 6.2). Compared to dense-vector graph representations such as [25], choosing vertex-vectors to be one-hot enables wave-propagation across the graph; we take advantage of this to implement the P\u2092 pathfinding algorithm, which finds a shortest-path of length k in O(klog(k)) matrix multiplications, the path can begin being followed after only O(k) operations (Methods 6.3).\nTo ground the graph, each location p \u2208 $\\mathbb{R}^d$ in state-space is represented by a unique high-dimensional complex-valued vector $X_p$ \u2208 $\\mathbb{C}^N$, N \u226b d, called a Harmonic Relational Key (HaRK). The HaRK at position p + \u03b4 is given by $X_{p+\\delta} = \\text{HaRK}(X_p, \\delta) = X_pe^{2\\pi i \\Gamma \\delta}$, where \u0413 is a matrix of N-oriented frequencies sampled from a distribution $p_y(\\gamma)$, $y \u2208 \\mathbb{R}^d$. Modulated Hebbian learning can bind \"value\" vectors to HaRKs, associating information with points in state-space. Fourier theory allows modulation to exactly control the \u201cresolution\u201d of information in $\\mathbb{R}^d$, with retrieved items having a Gaussian activation field (Methods 6.4).\nSegraphs: A segraph G* is composed of vertices V* and undirected edges E*. Each vertex v has a hyperball subset of S called its field F\u1d65(v), with a hypervolume M\u1d65(v) (Fig. 2A). The one-hot vectors of vertices are given a specific location by binding them to a HaRK key with a specific resolution. Binarizing the resulting Gaussian response produces the actual field, the size of which is controlled by the resolution, set on storage (Methods 6.4, Fig. 5E). This construction establishes the vertex : place cell, vertex-field : place-field, HaRK : entorhinal cortex analogy between our system and the brain. The edge e = (v\u2081, v\u2082) implicitly represents information about the set F\u2091(e) = F\u1d65(v\u2081) \u00d7 F\u1d65(v\u2082) of possible start and target points inside F\u1d65(v\u2081) and F\u1d65(v\u2082), and records the number of failed and successful traversals over itself. The ratio of successful to total traversals $A(e)$ is an estimate of the edge's reliability.\nEach edge has an activity potential $\u03bb = pM_e$ (colored lines in Fig. 2A) which is compared to a threshold $\u03bb*$. If \u03bb(e) < \u03bb*, then e is inhibited. Inhibiting an edges prevents it from being used during pathfinding, allowing entire classes of paths to be instantaneously turned off, or if an edge is disinhibited, turned back on (Fig. 2B and C). Edges can also be manually (dis)inhibited, independent of their A-value. Disinhibiting low A edges encourages exploration, and inhibiting low A edges encourages the use of more reliable paths.\nG* can be modified in two major ways. (1) A vertex v can be extended, causing new vertices to be added around the perimeter of its field, expanding the region of S covered by G* (Fig. 2D). By default, fields of vertices added by extension are slightly larger than that of the original field, to encourage exploration (see Methods 6.5 for details). (2) An edge e can be refined, taking its largest vertex and replacing it with several vertices with smaller fields covering the area of the original field, which increases the local resolution of G* (Fig. 2E). The agent A traverses across an edge to an adjacent vertex by sampling a target-point from the field F\u1d65, then passing the displacement to that point and sensory data to it's controller K. A succeeds if it reaches the target, and fails if it hits an obstacle (Fig. 2F).\nAdaptive Realtime Metasearch over Segraphs: Fig. 3 shows a basic conceptual overview of the algorithm as a flowchart. The agent starts out at a point inside of the field of a start vertex v\u2092(the segraph G* may have an arbitrary initial configuration, perhaps with only a single vertex). [a] The agent calculates an epistemic score $\u03a5(e)$ for each edge e, equal to an edge's size divided by its total number of traversals (see Methods 6.10.3). Then, the agent selects the edge with the highest \u03a5 and sets it as it's goal-edge, e\u209c, analogous to various count-based exploration methods [26]. Also, all manual inhibition is cleared, and [b] the agent uses the pathfinding algorithm P\u2092 to find a path e\u209a is manually disinhibited, p beginning at v\u2092 and ending (by passing through) the edge e\u209c. [c] If such a path is found, [d] the last vertex on p is designated the goal-vertex, v\u2096. [e] Then \u0394\u00af is adapted to regulate the number of pathfinding failures (we will return to this).\nNext [f] the agent takes the next vertex from the path and sets it as its sub-goal vertex v\u208a, the edge between A's current vertex v\u2092 and the subgoal vertex is designated the target edge e\u208a, which A will try to traverse next. A uniformly samples a target point from F\u1d65(v\u208a) and passes the displacement to its controller K. Then [g] A tries to physically traverse the edge, ultimately ending up at a new point s'. [h] If A reached the target point, [i] the traversal is marked as a success, and the agent updates its current vertex to v\u2092 \u2190 v\u208a, after which [j] the vertex the agent just reached is extended. Then [k] the agent checks if it has reached the goal vertex v\u2096. If it has, the agent selects a new goal (back to [a]), otherwise, the agent continues along the path [f].\nIf at [h] the agent doesn't reach the target point, then [l] the traversal is marked as a failure, and the agent recalculates what vertex it is in. [m] A manually inhibits the failed edge (inhibited independently of its actual \u5165-value), so that it can't immediately be used again in a path. If the edge is stressed (has accumulated too many failures, see Methods 6.10.2), the edge is refined, in the hopes that some of the child-edges are more reliable than the original edge. Then, \u0394+ is adapted to reflect the difference between activity potential of the edge and \u5165*, and \u5165* is increased to discourage further failures on other edges.\nThen [q] the graph is threshholded by \u5165*, producing a new pathing graph so that [b] A can look for a new path. Repeated failures may result in \u5165* being so high that a path cannot be found between v\u2092 and the goal edge e\u209c, resulting in a pathing failure ([c] branches to [r]), which increments \u03ba (the number of consecutive pathing failures) and decreases \u5165* by \u03ba\u0394\u00af (decreasing the odds of another pathing failure), which then goes back to [q]. Recall that if pathfinding succeeds [e], \u043a is reset back to 0, and \u0394\u00af is adapted to reflect the difference between A (initial threshold value) and the current threshold value, to regulate the number of consecutive pathing failures. This also clears all manual inhibition.\nThe ARMS algorithm has several means of self-regulation. Overall the algorithm has three interlocking loops, shown in Fig. 3 as thick colored arrows: the \u201csuccess\u201d, \u201cfailure\u201d, and \u201cpathing failure\" loops. Edge-traversal collects information that helps the agent discriminate between edges, so long-term both [i] and [l] from the success and failure loops up-regulate the success loop. However, the success loop causes the addition of new edges via extension [j] and reliability-agnostic selection of goal-edges [a], which causes the success loop to up-regulate the failure loop. The failure loop down-regulates itself by (1) inhibiting an edge after failure to prevent immediate repeat-failure, (2) potentially refining an edge to create better options for pathfing in the long-term, and (3) raising \u5165* to lower the chance of other low-reliability edges being used. This however also up-regulates the \u201cpathing-failure loop\u201d, as over-inhibition can prevent a path from being found. To counteract this, this loop down-regulates itself by lowering \u5165*, allowing the system to return to either the success or failure loops. See Methods 6.6 for details of \u5165* adaptation.\""}, {"title": "4 Results", "content": "Since our stated goal is being able to go between any two points in a state-space, we introduce a measure of the agent's segraph to do just that, which we call the reliability $R(G*)$ of the segraph. It is a distance-weighted average of the reliability of the graph for navigating between pairs of points sampled from the state-space, allowing the agent to re-path if necessary to reflect the operation of the ARMS algorithm (see Methods 6.9). $R(G*) = 0$ means complete unreliability, $R(G*) = 1$ means total reliability. For the sake of conceptual clarity, we evaluate the ARMS algorithm in randomly generated 2D mazes (see Methods 6.8).\nARMS is effective and robust to prior knowledge: The ARMS algorithm can optionally shape the way that new vertices are added during extension (see Methods 6.5) using Perceptual Knowledge of Navigability Affordance (PKNA). By default, the agent is Naive (top of Fig. 4A), meaning it assumes all points in space are traversable. An astute agent (middle of Fig. 4A) has an oracle that can distinguish traversable from un-traversable areas and places new vertices accordingly. A misled agent (bottom of Fig. 4A) uses the same oracle but inverts its judgements. Fig. 4B shows the $R(G*)$ for Naive, Astute, and Misled agents over time in a variety of mazes with different properties, such as rectilinear mazes and irregular polygonal mazes. Under different hyperparameters (not shown), the Astute agent learns considerably more quickly, but under more optimal hyperparameters (shown), the Astute agent merely has a higher long-term performance, achieving an $R(G*)$ of almost 1. The naive agent learns exactly as quickly and has only a slightly worse long-term $R(G*)$, with the Misled agent being again only slightly worse. We show the final $R(G*)$ as (Q1, median, Q3) in Table 1.\nARMS is partially robust to perturbations: We investigated the ability of the ARMS algorithm to recover from a mid-exploration \u201cmaze-swap\u201d. Fig. 4C shows the $R(G*)$ over time, the abrupt drop at 100k time-steps is when the swap occurs. The naive and misled agents recover quite quickly, while the astute agent's recovery is much slower.\nARMS is scale-invariant: One question that concerns us is the sensitivity of the ARMS algorithm to the scale of the environment, especially for the Naive system. If the ARMS algorithm creates too many tiny fields, it can waste it's time on irrelevant details of the environment, while if the ARMS algorithm only creates large fields, it may never actually master the maze. The ARMS algorithm starts with an initial state with a predefined field-size, which is a hyper-parameter. The size of this initial field will control to some extent the size of subsequently added fields. How sensitive is the operation of the ARMS algorithm to this initial choice? We tested our system with a range of initial vertex-field sizes, Fig. 4D shows that performance was unaffected.\nARMS can self-supervised an extension-network: The \"Astute\u201d agent using an oracle is unrealistic, so we ask the question, can the ARMS algorithm support an \u201couter-loop\" of lifelong learning, whereby the activity of the ARMS algorithm collects data that is used to off-line train a neural network replacement for the oracle? We collected data from a single long (300k sim. steps) in a large (7x7) maze, trained a neural estimator on this data, then tested it on the 6x5(1), 5x5(2), Triangle, and Pentagon mazes.\nARMS works by selective refinement: Finally, we ask why in theory the ARMS algorithm should work. In Methods 6.7 we derive that for an edge with measure M and reliability p, the expected increase in local traversability for refining the edge is $\u0394Q \u00d7 M_0p_0(1 - p_0)$. We conjecture that the ARMS algorithm works by preferentially refining edges with a high \u0394Q. Though not explicitly designed to do this, Fig. 4F shows that this does in fact happen, ignoring the many edges have \u0394Q = 0 (not shown in plot).\""}, {"title": "5 Discussion", "content": "We have introduced the outlines of a theory of adaptive behavior which ties the ability of a system to robustly solve a problem to the potential of that system to generate any possible behavior, and explained how enumeration of behaviors with sequential modification of a cognitive graph can accomplish this. We introduced a simple algorithm for guiding the evolution of the graph, and tested it in maze environments. Our results demonstrate the basic validity of the approach. The neural substrate we introduced also shows that the system can be instantiated biologically. While only preliminary, this work provides the outlines of a system that can meaningfully try anything.\nOur proposal is not perfect: the theory of behavioral enumeration could be further formalized, the ARMS algorithm only works well in static state-spaces, and many design choices were ad-hoc or based on intuition. The system is only tested in 2D environments, and we do not provide noise-tolerant sensory-based localization, and we provide no account of map consolidation nor proper skill learning. However, learned vertex-fields could probably enable this system to operate in high-dimensional dynamic environments, and HaRKs on their own provide a variety of potential sensory-localization mechanisms. Many design-choices were made for simplicity's sake, and expanding those choices should lead to new capabilities.\nElements of our proposal overlap substantially with other works. Segraphs appear to be some kind of hybrid system such as [27], however they are fully neural, embedding symbolic dynamics inside neural ones. Decomposition of the environment for planning is a feature of many systems, but the shared inspiration from the hippocampus creates an overlap of concern with successor representations [28] and architectures such as the Tolman-Eichenbaum machine [1], but we instead focus on real-time adaptation of the graph rather than value-propagation or learning a generalized state-space decomposition, though we think segraphs can probably support both. To the best of our knowledge, our elevation of search from a tool to a principle of behavioral adaptation is novel, though Ross Ashby's concept of ultrastability [29] may be a precursor.\nIn conclusion, this work describes how adaptation and problem-solving can be, surprisingly, achieved by exhaustive search across the set of all spatiotemporally continuous behaviors. The works presented here is just one possible instance of this highly general \u201cbehavior-as-search\" paradigm. We invite other researchers to elaborate on our findings and develop more sophisticated models with more powerful capabilities. Future directions could model habituation of behaviors as \u201ccaching\u201d search results, extend the ARMS algorithm to other types of state-spaces, and develop more sophisticated uses of HaRKs. Having the capacity to try anything, progress in this direction might lead to radically autonomous robotic systems, a better understanding of developmental learning, and strong artificial intelligence."}, {"title": "6 Methods", "content": "We cover some technical details that important for understanding the operation of the ARMS algorithm, as well as its neural implementation."}, {"title": "6.1 Hebbian Learning", "content": "We implement Hebbian learning between two vectors a and b using a \u201cbind\" operator $a \\triangleright b := \\frac{ba^*}{||a||^2}$, which is just a scaled outer product of a and b having the property that $(a \\triangleright b)c = \\frac{b}{||a||^2} \\langle a, c \\rangle$, where $\\langle a, c \\rangle$ is the cosine similarity between a and c. We can interpret a sum of such \"bind\" matrices as a dictionary, as $[\\sum_{i=1}^k a_i \\triangleright b_i] c = \\sum_{i=1}^k [a_i \\triangleright b_i] c = \\sum_{i=1}^k \\frac{b_i}{||a_i||^2} \\langle a_i, c \\rangle$. If all of the \u201ckey\u201d vectors $a_i$ are orthogonal to each other (have zero cosine-similarity), and $c = a_j$, then $[\\sum_{i=1}^k a_i \\triangleright b_i] a_j = b_j$, meaning it's possible to selectively retrieve associations. Our approach is comparable to [25] and [30]. See Appendix A.1 for details."}, {"title": "6.2 Graph Working Memory", "content": "Let G = (V, E) be an undirected graph with vertices V and edges E. Each vertex $v_i \u2208 V$ is assigned a unique M-dimensional one-hot vector $v_i$. If an edge $e \u2208 E$ is an unordered tuple of vertices with one-hot vectors $v_i$ and $v_j$, then the edge e is represented by a matrix $E = [v_i \\triangleright v_j] + [v_j \\triangleright v_i]$. The graph G is represented by the matrix $G = \\sum_{e \u2208 E} E$, so that for any vertex $v_i$, $Gv_i$ will be the sum of one-hot vectors for the neighbors of $v_i$ (Fig. 5A). Being represented by one-hot vectors, they are easy to individually identify.\nWhile the actual operation of the ARMS algorithm is implemented non-neurally, it is important to understand how certain operations and structures can be made neural. In particular, stacks, queues, and linked-lists are important for representing paths over the graph. To illustrate, we give an implementation of a stack. First, let each node of the stack be a graph-vertex $v_i$, which has a single link to the next node of the stack, $v_{i+1}$. The link is just a directed edge represented by the matrix $v_i \\triangleright v_{i+1}$, stored together as a sum in the matrix L. Then, nodes $v_i$ are linked to their contents $y_i$ by a matrix C that is a sum of $v_i \\triangleright y_i$ matrices."}, {"title": "6.3 P Pathfinding Algorithm", "content": "We take advantage of the one-hot-vector graph-representation (Methods 6.2) to implement the P\u2092 pathfinding algorithm. By propagating across the graph a forward \u201cwave-front\u201d from a set of starting vertices and a backward \u201cwave-front\u201d from some target vertices (compare to [31]), we can find the set of \u201cmid-point\u201d vertices where the waves overlap in O(k) matrix multiplications, where k is the length of the shortest path between the start and target vertices. By treating the mid-point vertices as a new set of target vertices and ignoring the cost of matrix multiplication, P\u2092 recursively solves the pathfinding problem in time O(k log(k)), and moreover, finds the first vertex on the path in time O(k), meaning the agent can begin following the path before the whole path has been found. A toy-example is shown in Fig. 5B. Algorithm details in Methods 6.3.\nThe pathfinding algorithm P\u2092 proceeds by recursive application of a simple midway-point-finding process. A combination of passive \u5165*-thresholded inhibition and manual inhibition will yield a plain undirected graph G = (V, E) from the original segraph G*, this plain graph G is used for pathfinding. G always has the same vertices as G*, but in general has fewer edges. These edges can be represented via Hebbian learning by a matrix G. The P\u2092 algorithm takes as input a set of starting vertices $V_s \u2282 V$ and a set of goal vertices $V_g \u2282 V$, which have corresponding multi-hot vector representations $m^s_i$ and $m^g_i$.\nThe basic idea is to propagate out across the graph a \"wave-front\u201d from these two sets and wait until the wave-fronts overlap. We can summarize this with the following functions:"}, {"title": "6.4 Harmonic Relational Keys", "content": "Having introduced our method of constructing cognitive graphs, we must now explain how to construct cognitive maps representing $\\mathbb{R}^d$ state-spaces. We take inspiration from band cells, and represent the state of each band cell as a unit-complex number $x_j \u2208 \\mathbb{C}$, with an oriented frequency $\u03b3_j \u2208 \\mathbb{R}^d$ (Fig. 5C). The population of band cells is a vector $x \u2208 \\mathbb{C}^N$ (N large, Fig. 5D), and each $\u03b3_j$ is sampled from a distribution $p_\u03b3(\u03b3)$ over $\\mathbb{R}^d$. If we associate the key $x_p$ with a point $p \u2208 \\mathbb{R}^d$, then the key corresponding to the point $p + \u03b4$ is given by $X_{p+\\delta} = f_{HaRK} (X_p, \u03b4) = X_pe^{2\\pi i \\Gamma \\delta}$, where \u2299 is an element-wise product, and F is the matrix of oriented frequencies $\u03b3_j$.\n$f_{HaRK}$ can assign a unique $\\mathbb{C}^N$ key to each point in $\\mathbb{R}^d$. We can use element-wise modulation to also control the resolution of information stored in $\\mathbb{R}^d$. The bind matrix between an input-modulated key-vector $x \u2299 \u03bc$ and the value-vector y is $[(x \u2299 \u03bc) \\triangleright y]$. We then query this matrix with a d-offset key $x_\u03b4 = f_{HaRK}(\u00e6, \u03b4)$ modulated by \u03b7, yielding $\\tilde{y}(\u03b4) = [(x \u2299 \u03bc) \\triangleright Y](x_\u03b4 \u2299 \u03b7)$, which can be decomposed as $\\tilde{y}(\u03b4) = c(\u03b4)y$, where $c(d) = \\mathcal{F}^{-1}[g(\u03b3)p(\u03b3)]$, with $\\mathcal{F}^{-1}$ being the inverse Fourier transform and g(\u03b3) a function mapping oriented frequencies \u03b3 to modulation weights: in other words, g(\u03b3) determines \u03bc and \u03b7. For this initial work we choose c(d) to be an isometric Gaussian $c_\u03c3(\u03b4)$ with width \u03c3 (Fig. 5E), though this is not strictly necessary. Manipulating \u03c3 can be used to control the resolution of information on storage, on retrieval, or both to achieve spatial band-pass filtering.\nOur approach to harmonic representations can be thought of as generalization and simplification of the model presented in [32]. See Appendix B for details."}, {"title": "6.5 Extension and PKNA", "content": "Extension adds new vertices around an existing vertex so as to increase the coverage of G*. During extension, the agent randomly samples points $p_j$ around the exterior of the field of the vertex v. If the agent can estimate the reachability $r_j$ of each of these points using Perceptual Knowledge of Navigability Affordances (PKNA), then it can store that information using HaRKs in a matrix $R = \\sum_j X_{p_j} \\triangleright r_j$, along with a normalization matrix $C = \\sum_j X_{p_j} \\triangleright 1$. If the agent is Naive, then $r_j = 1$ regardless of the actual reachability (we could also call the Naive agent \"optimistic\").\nThen, the agent tries to add new fields. Suppose the size of the field $F_v(v)$ is \u03c3, then the agent starts by trying to add new fields with a size of $\u03c3' = \\sqrt{2}\u03c3$. It does this by checking the value $r_j = R \\frac{\\mathcal{C}(x_{p_j} \u03b7 \u03c3_1)}{C(x_{p_j}\u03b7 \u03c3_1)}$, where $\u03b7$ is the local \"average\" of reachability, with \"local\" defined by the scale of \u03c3'. All $p_j$ are evaluated and ordered from highest to lowest $r_j$, and the points with the highest $r_j$ (above a certain threshold) are added as new vertex with fields of size $\u03c3'$ (new vertices fields that are too redundant with older vertices are not added).\nAfter doing this check for $\u03c3' = \\sqrt{2}\u03c3$, the agent does this check for $\u03c3' = \u03c3$ and $\u03c3' = 2\u03c3$. For the last check, it doesn't matter if $r_j$ is above the pre-defined threshold, the space surrounding $F_v(v)$ is filled with new vertices of field-size 2\u03c3. The \"Astute\" agent uses a direct collision detection calculation to determine $r_j$, while the \u201cNeural\u201d agent uses a neural network to predict $r_j$ from sensory data."}, {"title": "6.6 ARMS A* Regulation", "content": "Internally, information about inhibition dynamics is stored in a tuple $\u039b = (\u03bb, \u03bb*, \u0394+, \u0394\u00af, \u03ba)$, with three main functions that modify \u039b, $f^{\u03bb*}_\u2191$, $f^{\u03bb*}_\u2193$, and $f^{\u03ba}_\u2193$. Referring back to the ARMS flowchart in Fig. 3, there are three points where \u039b is updated, those being steps [p", "e": "where $f^{\u03bb*}_\u2193$ is called after a path has been found), and [r", "U([0,2": "n&\u03bb* \u2190 \u03bb* + n \u22c5 \u0394 \\\\\n&\\text{if } \u03bb* < 0 \\text{ then} \\\\\n&\u03bb* \u2190 0 \\\\\n&\\text{return } \u03bb*\n\\end{align*}\n\\begin{align*}\n&\\text{function } f^{\u03bb*}_\u2191(\u039b, e) \\\\\n&(\u03bb, \u03bb*, \u0394+, \u0394\u00af, \u03ba) \u2190 \u039b \\\\\n&\u0394\u03bb* \u2190 \u03bb(e) \u2212 \u03bb* \\\\\n&\u0394+ \u2190 (1 \u2212 \u03b2\u2191) \u22c5 \u0394+ + \u03b2\u2191 \u22c5 max(0, \u0394\u03bb*) \\\\\n&\u03bb* \u2190 d_{\u03bb*}(\u03bb*, \u0394+) \\\\\n&\u039b \u2190 (\u03bb, \u03bb*, \u0394+, \u0394\u00af, \u03ba) \\\\\n&\\"}]}