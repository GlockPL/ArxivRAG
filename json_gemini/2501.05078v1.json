{"title": "Analyzing Memorization in Large Language Models through the Lens of Model Attribution", "authors": ["Tarun Ram Menta", "Susmit Agrawal", "Chirag Agarwal"], "abstract": "Large Language Models (LLMs) are prevalent in modern applications but often memorize training data, leading to privacy breaches and copyright issues. Existing research has mainly focused on post-hoc analyses such as extracting memorized content or developing memorization metrics-without exploring the underlying architectural factors that contribute to memorization. In this work, we investigate memorization from an architectural lens by analyzing how attention modules at different layers impact its memorization and generalization performance. Using attribution techniques, we systematically intervene in the LLM's architecture by bypassing attention modules at specific blocks while keeping other components like layer normalization and MLP transformations intact. We provide theorems analyzing our intervention mechanism from a mathematical view, bounding the difference in layer outputs with and without our attributions. Our theoretical and empirical analyses reveal that attention modules in deeper transformer blocks are primarily responsible for memorization, whereas earlier blocks are crucial for the model's generalization and reasoning capabilities. We validate our findings through comprehensive experiments on different LLM families (Pythia and GPT-Neo) and five benchmark datasets. Our insights offer a practical approach to mitigate memorization in LLMs while preserving their performance, contributing to safer and more ethical deployment in real-world applications.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have become ubiquitous in modern applications, powering everything from conversational agents to advanced data analysis tools. However, these models often memorize parts of their training data, causing serious concerns such as privacy and copyright infringements, hindering its employment in high-stakes applications. Prior works approach memorization from a post-hoc perspective, where they predominantly focus on methods to extract memorized samples from trained LLMs or propose metrics to quantify memorization. While some works also explore machine un-learning techniques to remove specific memorized content after training, they do not delve into the fundamental model mechanisms that contribute to memorization. There has been little to no exploration of understanding memorization from a fundamental architectural viewpoint. In particular, systematically analyzing which layers and modules within transformer architectures contribute to memorization. We argue that identifying these components is essential for developing strategies to mitigate memorization without compromising the model's generalization capabilities. In this work, our goal is to understand the trade-off between reasoning and memorization within LLMs by leveraging model attribution techniques. In particular, we perform systematic architectural interventions by bypassing (or \"short-circuiting\") the attention modules at targeted blocks of the transformer while preserving other modules such as layer normalization and MLP operations, allowing us to isolate the impact of specific attention modules on both memorization and generalization. We support our experimental findings with theoretical analysis of the impact of bypassing attention modules at different blocks. Our theorems introduce bounds on the difference in model outputs with standard attention and short-circuited attention. Our theoretical insights and empirical results indicate that short-circuiting attention modules in earlier blocks leads to significant differences in the output vectors compared to the original model and these differences disrupt the model's internal representations, directly hindering its performance and affecting generalization capabilities. In contrast, bypassing attention modules in deeper blocks results in smaller differences in the output vectors, where these changes are sufficient to prevent the model from generating memorized content verbatim and do not significantly impair its ability to generalize from the data. Our empirical analysis across two different model families-Pythia and GPT-Neo-and four benchmark datasets align with the theoretical findings and demonstrate that intervening in the attention modules of deeper layers mitigates memorization and preserves their generalization capabilities. Our contributions can be summarized as follows:\n\u2022 We introduce a method to bypass attention modules in transformers while maintaining other transformations like layer normalization and MLP operations. This targeted intervention allows for precise analysis of different components within the model.\n\u2022 Our theoretical framework explains how differences in output representations caused by bypassing attention modules at various depths impact memorization and generalization and provide evidence that memorization in LLMs primarily stems from the attention modules in deeper transformer layers.\n\u2022 We offer a practical approach to enhance the ethical deployment of LLMs by demonstrating that it is possible to mitigate memorization while preserving generalization capabilities by bypassing attention modules in deeper blocks."}, {"title": "2 Related Work", "content": "Memorization and Extraction of Training Data. Works on image classifiers argue that a certain degree of memorization is inevitable during generalization. This phenomenon is even more pronounced in LLMs with huge parameter counts. LLMs have been shown to memorize and reproduce verbatim samples from their training corpus. Various works have studied this phenomenon, from the angle of model scale, and data deduplication . Recent work has studied memorization in LLMs with a deeper emphasis on semantics and other factors contributing to memorization. However, very few works explore the architectural aspect of memorization in large transformers. shows that while memorization cannot be localized to any part of the transformer, predictable gradient patterns are observed. In our work, we show via an attention-bypassing technique that certain blocks of the transformer can be identified as pertinent for memorization, while not boosting the generalization capabilities of the model.\nInterpretability and Pruning in LLMs. Many attempts have been made to study the inner workings of transformers , attribute certain behaviors to individual components , and understand the mechanisms of reasoning and knowledge storage in LLMs. Pruning of LLMs has also emerged as a key area of interest, with multiple works showing that LLMs can be pruned in both width and depth without significant performance loss. To our knowledge, our work is the first that attempts to extend such an investigation to memorization in LLMs. We attempt to attribute memorization to individual model components and study if generalization and memorization can be disentangled."}, {"title": "3 Methodology", "content": "Our goal is to study the phenomenon of memorization in LLMs from the perspective of model attribution. Previous literature has shown the efficacy of pruning both width and depth of transformers while maintaining downstream performance. This motivates us to study the effect of similar interventions on the memorization characteristics of LLMs. In particular, we wish to isolate the contribution of individual model components to memorization. The attention mechanism is fundamental to transformer models, enabling them to understand context and manage long-range dependencies. By dynamically weighing the significance of different words relative to each other, transformers capture nuanced meanings and relationships within data. For these reasons, we posit that the major contribution of memorization in LLMs stems from attention, and focus our analysis around it. We develop a mechanism to \"short-circuit\" the contribution of the attention mechanism in various blocks of a trained model during, and use it to conduct our experiments."}, {"title": "3.1 Preliminaries", "content": "Here, we formally define memorization and provide a primer to attention mechanism.\nDefinition of Memorization. Memorization can be defined in various ways within language modeling. Memorization does play a crucial role in language models since the training objective focuses on maximizing the overall likelihood of the training dataset. For example, memorization of well known facts is a desirable characteristic of LLMs. On the other hand, LLMs have been shown to spuriously memorize personal or private data like names, phone numbers, and email addresses; IRC conversations; code; and 128-bit UUIDs. Given a language model $f_\\theta$, trained on a set of documents D, we follow the definition of extractable memorization set out in previous works . A string having a prefix p of tokenized length $l_p$ and a suffix s of tokenized length $l_s$ such that the concatenated sequence $(p||s) \\in D$ is a substring of some document in the training corpus is said to be extractable if the suffix can be recovered by prompting the LLM with the prefix under greedy sampling (GS), i.e., $GS(f_\\theta,p) = s$. In this work, we only explore greedy sampling. Extension to other more directed extraction attacks is left as future work.\nPrimer on Attention Mechanism. In this section, we setup the notation for the remainder of this work, and provide a brief refresher on the architecture and inner working of a transformer model. We denote a decoder-only transformer LLM as $f_\\theta$. $f_\\theta$ consists of L transformer layers/blocks in a sequential manner, where the computation of the $l^{th}$ block is as follows\nX' = X + MultiHeadSelfAttention(X)  (1)\nX\" = X' + FFN(X') (2)\nThe FFN layer is a simple two-layer MLP with a non-linearity. The main focus of our work is on the multi-head self-attention mechanism. The computation of a single self-attention operation is as follows:\nQ = XWQ; K = XWK; V=XWv\nAttention(Q, K, V) = softmax$\\left(\\frac{Q K^{T}}{\\sqrt{d_{k}}}\\right) V$"}, {"title": "3.2 Attention Short-Circuiting", "content": "We now describe our approach for isolating the contribution of various attention blocks. The key component of the attention mechanism is the attention weights - computed as an inner product between query and key vectors, i.e.,\nAttentionWeight = $Q K^{T}$\nThis enables the model to effectively model both short and long-range dependencies and identify patterns in the input. Pruning methods explore removing the last K layers of the LLM, or sparsifying model weights, while maintaining the original output distribution. However, we wish to disable intermediate attention modules, and inspect its effect on memorization and general capabilities of the model. While a naive approach would be to simply replace the multi-head self-attention operation in Eq. 1 with zero, this has an undesirable effect of changing the distribution of X' and eventually leading to model collapse, i.e., model generating gibberish text. Instead, we systematically \u201cshort-circuit\" certain attention layers of the model by replacing this attention weight with the identity matrix in all heads of the layer i.e.,\nSHORTCIRCUITATTENTION(Q, K, V) = I \u00b7 V\nwhere I is the identity matrix, effectively removing the contribution of that attention layer while not destroying the distribution of X'. We perform this 'short-circuit' operation to all attention heads of a layer when applying to any block in the model."}, {"title": "3.3 Theoretical Analysis", "content": "In this section, we derive the impact of bypassing the attention module of a given block/layer on subsequent layers in the transformer model. We use the terms block and layer interchangeably. Here, we are specifically interested in the representation of the last token of a given sequence, as the next token is predicted based on its value. First, we derive the effect of short-circuiting attention at a single layer and study the difference in the output representations. We then use the resulting expression to derive the effect of short-circuiting the attention block at a given layer L vs. a later layer L + 1. Our results indicate that the difference in output representations is amplified as more layers are stacked on top of the short-circuited block, resulting in heavy degradation in generation abilities.\nNotations. Let $V^{L-1}$ denote the output representation at layer L - 1 of the last token in a given sequence X = {$X_1,X_2,...,X_n$}, where n is the length of the input sequence and $V^{L-1}$ = {$v^{L-1}_1,v^{L-1}_2,...,v^{L-1}_n$} serves as an input to layer L. Moving forward, we denote v' = v as the representation of the last token for a given layer l for mathematical brevity. The output of layer L with standard attention is represented as $v^L$, while $v^L_{IA}$ refers to the output of layer L with identity attention. The difference at layer L is denoted by $D^L$ = $v^L$ \u2013 $v^L_{IA}$. Similarly, $v^{L+1}$ represents the output of layer L + 1 with standard attention. Now, the outputs at layer L + 1 with attention replaced at layer L and L + 1 are given by $v^{L+1}_{IA,L}$ and $v^{L+1}_{IA,L+1}$, respectively. The output difference at layer L + 1 when attention is replaced at layer L is denoted by $D^{L+1}_{IA,L}$ = $v^{L+1}$ - $v^{L+1}_{IA,L}$, and when attention is replaced at layer L + 1, it is represented by $D^{L+1}_{IA,L+1}$ = $v^{L+1}$ - $v^{L+1}_{IA,L+1}$. The attention weights at layer L are denoted by $\\alpha_l$, satisfying $\\sum_i \\alpha^l_i= 1$, while $W^L$ is the weight matrix of a linear approximation of the feed-forward network (FFN) at layer L, and $\\epsilon^L$ represents the approximation error at layer L. Such linear approximations can be achieved using methods such as Neural Tangent Kernels or piecewise approximations.\nTheorem 1 (Bounding the Difference Between Identity Attention and Standard Attention for a Transformer with a single block). The normed difference between the output vectors obtained by using standard attention and short-circuited attention for a single transformer block is upper bounded by:\n$||v_{IA} - v|| \\leq (1 + ||W||)M(1 - \\alpha_i) + ||\\epsilon_{IA} - \\epsilon||$,\nwhere M = $max_{i \\neq 1} ||x_n - x_i||$, W is the weight matrix of the FFN layer, and e is the error in the linear approximation of the activation function.\nProof Sketch. The proof relies on the insight that the value vectors undergo simple MLP-based transformations when passing through the transformer layers, with the attention module replacing each value vector with a convex combination of all input value vectors. Our intervention replaces the convex combination of the last token of the sequence with the identity function. Note that this is a special case of an arbitrary convex combination, where all weight is put on the coefficient of the final token. The proof then follows through when expanding the expressions for the output value vectors with and without short-circuiting the attention, and adding the residual components. The proof has been provided in Appendix A.1. The theorem gives an upper bound of the difference between the outputs of a standard transformer block and a transformer block with short-circuited attention.\nNext, we use the above bound to derive how this difference propagates if another transformer block is added on top of the short-circuited block and compare it with short-circuiting the attention of the later block alone.\nTheorem 2 (Impact of Replacing Attention in any two consecutive Transformer Layers). For any two adjacent layers L and L + 1, the difference in the output representations can be bounded as:\nWhen replacing attention at layer L:\n$||D^{L+1}_{IA,L}|| \\leq (1 + ||W^{L+1}||)(1 + \\alpha^{L+1}_i)||D^{L}||$, where $||D^{L}|| \\leq (1+||W^{L}||)M^{L}(1-\\alpha^{L}_i)+||\\epsilon^{L}_{IA} - \\epsilon^{L}||$.\nWhen replacing attention at layer L + 1:\n$||D^{L+1}_{IA,L+1}|| \\leq (1 + ||W^{L+1}||)M^{L+1}(1 - \\alpha^{L+1}_i) + ||\\epsilon^{L+1}_{IA} - \\epsilon^{L+1}||$\nProof Sketch. This proof uses the result from Theorem 1 to compute the expression of the output of layer L with and without short-circuiting the attention module. Next, we derive the output of layer L + 1 based on the output of the first and compute the difference in output vectors when replacing the attention in the earlier layer vs. the later layer. The bounds indicate that replacing attention at an earlier layer L introduces differences that propagate and potentially amplify through subsequent layers due to the operations of the FFNs and residual connections. The amplification is influenced by the operator norms of the weight matrices and the attention weights. The theoretical bounds suggest that replacing attention at a lower layer (L) can lead to larger differences at layer L + 1, while replacing attention at layer L + 1 introduces more localized differences."}, {"title": "4 Experiments", "content": "Using our model attribution method described in Sec. 3, we now study the effect of 'short-circuiting' the attention mechanism in various blocks of an LLM on its memorization of the training dataset."}, {"title": "4.1 Experimental Setup", "content": "Models. We consider six LLMs of varying scales across two architectures \u2013 GPTNeo{1.3B, 2.7B} and Pythia{1.4B, 2.8B, 6.9B, 12B}, trained on the publicly available Pile dataset , allowing us to find samples from the training corpus that show extractable memorization.\nMemorized Dataset. For each model family, we collect 15k samples from the training dataset which are highly memorized (more than 90% samples show extractable memorization) by all model scales. These samples were made available by in their works on investigating memorization in LLMs. The memorized samples collected for GPTNeo models have a prefix length $l_p$ of 150 tokens and a suffix length $l_s$ of 50 tokens. The memorized samples collected for Pythia models have a prefix length $l_p$ of 32 tokens and a suffix length of $l_s$ of 32 tokens.\nEvaluation Benchmarks. Across our model attribution experiments, we also aim to quantify the general capabilities of the model alongside the memorization characteristics. To this end, we employ five different benchmarks \u2013 ARC, HellaSwag , LAMBADA , PIQA , and Wikitext \u2013 which test the model's performance on a variety of capabilities. In particular, we include benchmarks that test the reasoning and language understanding of the LLM. Refer to Appendix A.2 for a detailed description of each benchmark.\nMemorization Metrics. The focus of this work is to study the effect of various model components on the memorization characteristics of the LLM. While exact memorization, as defined in the previous section, is the strongest indicator of memorization, we also employ two additional metrics of memorization, for a total of three:\n1) Exact Match: This is simply the definition of extractable memorization, realized as a metric, i.e.,\nEM($f_\\theta$, p, s) = 1(GS($f_\\theta$,p) == s)) (3)\n2) Token Accuracy: Samples may not be verbatim memorized, but still show a large overlap with the exact training example. To account for this, we calculate the number of matching tokens between the generated sequence, and the suffix, i.e.,\nTA($f_\\theta$, p, s) = $\\frac{1}{l_s}$$\\sum_{i=1}^{l_s}$$\\sum(GS($f_\\theta$, p) [i] == s[i])) (4)\nWe cut off the generation process after $l_s$ tokens and ignore any further tokens.\n3) Completion Entropy: is the total entropy of the logit distribution for the each token of the suffix conditioned on the prefix for memorized and non-memorized samples. Let the concatenated sequence (p||s) be represented by a sequence of tokens ($x_1,..., x_{l_p+l_s}$). The completion entropy (CE) is defined as follows\nCE($f_\\theta$, p, s)=-$\\sum_{i=l_p}^{l_p+l_s-1}$$\\sum_{j=1}^{|V|}$P\\theta (x_{i+1}/x_{1:i}) log p\\theta (x_{i+1}|x_{1:i}), (5)\nwhere p is the prediction probability for the jth token in the vocabulary from the LLM $f_\\theta$ and |V| is the vocabulary size."}, {"title": "4.2 Results", "content": "Next, we present the insights from our experiments."}, {"title": "4.2.1 Memorization vs. Performance", "content": "We aim to understand the trade-off between the memorization and downstream performance of LLMs. In particular, we strive to answer: \u201cCan we isolate model components that correspond to general performance and memorization, or are the two deeply intertwined?\u201d To answer this question, we produce L edited variants of the given LLM: {$f^{1}_\\theta$, $f^{2}_\\theta$,..., $f^{L}_\\theta$} by short-circuiting the attention mechanism in each of the L blocks of the model. Next, we quantify the memorization performance of each model variant using the memorization metrics defined in Sec. 3 and their performance on standard benchmark. Following , we evaluate all memorization metrics using greedy sampling and leave the exploration of other targeted extraction techniques as future work. We present our findings in Fig. 1 for the GPTNeo-1.3B model across all benchmarks. Interestingly, we observe that short-circuiting attention in many blocks (particularly, the last quartile blocks, i.e., layers 18-24 for GPTNeo-1.3B) leads to a significant drop in memorization, with negligible drop in model performance. In some cases, the short-circuited variants even outperform the benchmark performance of the original model (see LAMBADA results in Fig. A7c). Further, short-circuiting the attention mechanism in the earlier layers of the LLMs leads to almost no memorization and leads to a significant drop of benchmark performance, with accuracy no better than random chance, i.e., the LLM collapses. We observe that applying attention short-circuiting in the first block (pure white circle in Fig. 1) sometimes spuriously leads to high performance, however the model is unable to generate any coherent sequences in this state, as we discuss in Sec. 4.2.3. Please refer to Appendix A.3 for additional results on other LLMs and benchmarks."}, {"title": "4.2.2 Short-Circuiting across Model Scale", "content": "Our findings in Sec. 4.2.1 show that the attention in later blocks of LLMs contributes strongly to memorization and has an insignificant effect on the downstream performance. This insight is crucial within the context of memorization in LLMs as it implies that the attention mechanism in later layers of a transformer can be skipped during inference, with the desirable result of lower memorization and retaining downstream performance. Next, we investigate whether this insight generalizes to LLMs of different scale (increasing parameter size), and our findings in Fig. 3 show a consistent trend across four different scales of the Pythia models, wherein short-circuiting attention in later blocks of the LLM results in reduced memorization. We note that larger models are more resistant to this phenomenon, with the gap in memorization between edited and original models reducing as model scale increases. We hypothesize that applying attention short-circuiting to groups of blocks in parallel may result in a larger drop in memorization scores, though we leave this for future exploration."}, {"title": "4.2.3 Qualitative Analysis", "content": "Here, we analyze the qualitative effect of short-circuiting attention on its generation performance. In Fig. 4, we show the result of short circuiting the attention mechanism in four different blocks of GPTNeo-1.3B, finding that short-circuiting attention in the earlier blocks leads to a breakdown of coherent generations. We hypothesize that the earlier layers of the LLM are responsible for general semantics and grammar and any change in them results in model collapse. The same effect is not observed when short-circuiting later blocks of the LLM, wherein the model still generated coherent and plausible completions to the given prompt. This behavior is also reflected in the Wikitext perplexity scores obtained by short-circuiting each of the different attention blocks in the model, shown in Fig. 5, where the short-circuit operation when applied to later attention blocks, does not degrade the perplexity scores significantly."}, {"title": "4.2.4 Reasoning vs. Non-reasoning Tasks", "content": "In the above sections, we evaluate short-circuited models across different scales on a comprehensive set of benchmarks, where results clearly show that short-circuiting applied to the later blocks of an LLM yields a huge decrease in all memorization metrics, without sacrificing model performance significantly. Here, we further investigate this phenomenon by dividing the benchmark tasks into reasoning and language understanding tasks. We measure the average drop in performance across benchmarks in each type of task after short-circuiting the attention mechanism in each layer.\nResults in Fig. 6 show a clear pattern, where the average relative decrease in score on reasoning tasks (PIQA, ARC-Easy) is much lesser than in the case of language understanding tasks (Hellaswag, LAMBADA). Additionally, a large set of transformer blocks show almost zero drop in relative performance on reasoning tasks after attention short-circuiting, suggesting that these blocks contribute almost exclusively to memorization, while not improving the reasoning capabilities of the model. We present consistent results across all six models in Appendix A.3."}, {"title": "5 Conclusion", "content": "Our study explores how attention modules at different depths contribute to memorization in LLMs. By systematically bypassing attention modules at targeted blocks while preserving other components, we isolate their impact on memorization and generalization. Our theoretical analysis showed that bypassing attention in earlier layers disrupts output representations, harming performance and generalization, while interventions in deeper layers prevent memorized content generation without impairing generalization abilities. Empirical experiments with Pythia and GPT-Neo models aligned with these predictions, indicating that deeper attention blocks play a critical role in memorization phenomena. This work enhances understanding of how transformer components influence memorization and offers a practical approach to address ethical concerns like privacy violations. By reducing memorization without degrading model performance, we contribute to developing more ethical and reliable LLMs suitable for real-world applications. Our targeted intervention strategy opens avenues for future research into architectural modifications that balance memorization and generalization, advancing responsible artificial intelligence technologies."}, {"title": "6 Limitations and Future Directions", "content": "While our approach shows promising results, several limitations warrant further investigation:\n1. Effects of attribution on multiple attention blocks: Our work provides the first step towards analyzing memorization from an architectural perspective. We study memorization by isolating and short-circuiting individual attention modules. The current work does not include studying the effects of short-circuiting multiple attention modules simultaneously,\nAdditionally, the work currently studies the effects of only attention modules, not the other transformer components such as layer normalizations or MLP layers. The study of these components is also another interesting direction for future work.\n3. Study on targeted extraction: While our work shows that attention short-circuiting is successful in reducing verbatim memorization, we only study this under the conditions of greedy sampling. An extension of this work to include more targeted extraction attacks is an important future exploration.\n4. Study of models trained on closed datasets: Finally, the models and datasets studied in this work are on the smaller side compared to commercial models like GPT-4. However, such large-scale models cannot be directly evaluated for memorization as the data used to train such models is not publicly available. However, our results indicate that applying the proposed interventions can nevertheless prevent them from generating memorized content, without compromising on their generation capabilities."}, {"title": "7 Ethical Considerations", "content": "Our method studies the impact of attention modules on memorization and provides a mechanism to overcome generation of memorized sequences. All data used in our studies come from the public domain and do not contain private information. All models are also publicly available. Hence, our work does not have any ethical concerns that need to be explicitly addressed."}, {"title": "A Appendix", "content": "Contents\nA.1 Proofs\nA.2 Description of Benchmarks\nA.3 Additional Results\nA.3.1 Memorization vs Downstream Performance on Additional Models\nA.3.2 Applying Short-Circuiting to Multiple Layers\nA.3.3 Short-Circuiting across Model Scale on Additional Benchmarks\nA.3.4 Short-Circuiting on Reasoning vs. Non-Reasoning tasks on Additional Models"}, {"title": "A.1 Proofs", "content": "Theorem 1 (Bounding the Difference Between Identity Attention and Standard Attention for a Transformer with a single block). The normed difference between the output vectors obtained by using standard attention and short-circuited attention for a single transformer block is upper bounded by:\n$||v_{IA} - v|| \\leq (1 + ||W||)M(1 - \\alpha_i) + ||\\epsilon_{IA} - \\epsilon||$,\nwhere M = $max_{i \\neq 1} ||x_n - x_i||$, W is the weight matrix of the FFN layer, and e is the error in the linear approximation of the activation function.\nProof. We begin by expressing the difference D between the output value vectors after the application of identity attention and standard attention:\nD = $v_{IA}$ - v\nSubstituting the expressions for $v_{IA}$ and v:\nD = $[2x_n + 2 FFN(x_n) + \\epsilon_{IA}] - [z_n + FFN(z_n) + \\epsilon]$\nwhere $z_n$ is the output of the standard attention module corresponding to the input $x_n$, after the residual addition.\nSimplifying, we get:\nD = $[2x_n - z_n] + [2 FFN(x_n) - FFN(z_n)] + (\\epsilon_{IA} - \\epsilon)$\nThe first term is:\n$2x_n - z_n = 2x_n - (\\sum_i \\alpha_i x_i + x_n) = x_n - \\sum_i \\alpha_i x_i$\nThis can be rewritten as:\n$2x_n - z_n = (1 - \\alpha_n) x_n - \\sum_{i \\neq n} \\alpha_i x_i$\nSince $\\alpha_n + \\sum_i \\alpha_i = 1$, we have:\n$2x_n - z_n = \\sum_{i \\neq n} \\alpha_i (x_n - x_i)$\nNow consider the second term:\n2 FFN($x_n$) - FFN($z_n$) = 2 FFN($x_n$ ) - FFN $(\\sum_i \\alpha_i x_i + x_n)$"}, {"title": null, "content": "Using the linearity of the FFN:\n2 FFN($x_n$) - FFN($z_n$) = FFN($x_n$) - $\\sum_i \\alpha_i$FFN($x_i$)\nThus, the total difference D becomes:\nD = $\\sum_{i \\neq n} \\alpha_i (x_n - x_i + FFN(x_n) - FFN(x_i)) + (\\epsilon_{IA} - \\epsilon)$\nSince FFN($x_n$) \u2013 FFN($x_i$) = FFN($x_n$ - $x_i$) due to the linearity of FFN, we can rewrite D as:\nD = $\\sum_{i \\neq n} \\alpha_i (\\delta x_i + FFN(\\delta x_i)) + (\\epsilon_{IA} - \\epsilon)$\nwhere $\\delta x_i$ = $x_n$ - $x_i$.\nNow, using the triangle inequality:\n$||D|| \\leq \\sum_{i \\neq n} \\alpha_i (||\\delta x_i|| + ||FFN(\\delta x_i)||) + ||\\epsilon_{IA} - \\epsilon||$\nSince ||FFN($\\delta x_i$)|| = ||W$\\delta x_i$|| ||W||||$\\delta x_i$||, we have:\n$||D|| \\leq \\sum_{i \\neq n} \\alpha_i (||\\delta x_i|| + ||W||||\\delta x_i||) + ||\\epsilon_{IA} - \\epsilon||$\nFactoring ||$\\delta x_i$||:\n$||D|| \\leq (1 + ||W||) \\sum_{i \\neq n} \\alpha_i||\\delta x_i|| + ||\\epsilon_{IA} - \\epsilon||$\nLet M = $max_{i \\neq n} ||x_n - x_i||$. Then:\n$||\\delta x_i|| \\leq M$\nThus, we can bound D as:\n$||D|| \\leq (1 + ||W||)M \\sum_{i \\neq n} \\alpha_i + ||\\epsilon_{IA} - \\epsilon||$\nSince $\\sum_{i \\neq n} \\alpha_i$ = 1 - $\\alpha_n$, we obtain the final bound:\n$||D|| \\leq (1 + ||W||)M(1 - \\alpha_n) + ||\\epsilon_{IA} - \\epsilon||$\nTheorem 2 (Impact of Replacing Attention in any two consecutive Transformer Layers). For any two adjacent layers L and L + 1, the difference in the output representations can be bounded as:\nWhen replacing attention at layer L:\n$||D^{L+1}_{IA,L}|| \\leq (1 + ||W^{L+1}||)(1 + \\alpha^{L+1}_i)||D^{L}||$, where $||D^{L}|| \\leq (1 + ||W^{L}||)M^{L}(1 - \\alpha^{L}_i) + ||\\epsilon^{L}_{IA} - \\epsilon^{L}||$.\nWhen replacing attention at layer L + 1:\n$||D^{L+1}_{IA,L+1}|| \\leq (1 + ||W^{L+1}||)M^{L+1}(1 - \\alpha^{L+1}_i) + ||\\epsilon^{L+1}_{IA} - \\epsilon^{L+1}||$\nProof. Case 1: Replacing Attention at Layer L\nStandard Attention Output at Layer L:\n$v^{L} = z^{L} + FFN^{L}(z^{L}) + \\epsilon^{L}$,\nwhere\n$z^{L} = \\sum_i \\alpha^{L}_i v^{L-1}_i$."}, {"title": null, "content": "Identity Attention Output at Layer L:\n$v^{L"}, {"L": "n$D^{L"}, "v^{L}_{IA}$ \u2013 $v^{L}$ = $\\delta z^{L} + FFN^{L}(\\delta z^{L}) + (\\epsilon^{L}_{IA} - \\epsilon^{L})$,\nwhere\n$\\delta z^{L} = z^{L} - \\bar{z}^{L}$ = $\\sum_i \\alpha^{L} (v^{L-1}_i - \\bar{v}^{L-1}_i)$.\nBounding $|| D^{L} ||$:\nUsing the triangle inequality and linearity of the FFN approximation:\n$||D^{L}|| \\leq (1 + ||W^{L}||)|| \\delta z^{L}|| + ||\\epsilon^{L}_{IA} - \\epsilon^{L}||$.\nSince\n$||\\delta z^{L}|| \\leq M^{L} (1 - \\alpha^{L}_n)$,\nwhere $M^{L} = max_{i \\neq n} ||v^{L-1} - \\bar{v}^{L-1}||$, we have:\n$||D^{L}|| \\leq (1 + ||W^{L}||) M^{L} (1 - \\alpha^{L}_n) + ||\\epsilon^{L}_{IA} - \\epsilon^{L}||$.\nPropagating the Difference to Layer L + 1:\nThe modified input to layer L + 1 is:\n$v^{L}_{IA}$ = $v^{L} + D^{L}$.\nStandard Attention Output at Layer L + 1:\n$v^{L+1} = z^{L+1} + FFN^{L+1}(z^{L+1}) + \\epsilon^{L+1}$,\nwhere\n$z^{L+1} = \\sum_i \\alpha^{L+1}_i \\bar{v}^{L} i$.\nModified Attention Output at Layer"]}