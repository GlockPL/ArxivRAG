{"title": "Supervised Contrastive Learning from Weakly-Labeled Audio Segments for Musical Version Matching", "authors": ["Joan Serr\u00e0", "R. Oguz Araz", "Dmitry Bogdanov", "Yuki Mitsufuji"], "abstract": "Detecting musical versions (different renditions of the same piece) is a challenging task with important applications. Because of the ground truth nature, existing approaches match musical versions at the track level (e.g., whole song). However, most applications require to match them at the segment level (e.g., 20 s chunks). In addition, existing approaches resort to classification and triplet losses, disregarding more recent losses that could bring meaningful improvements. In this paper, we propose a method to learn from weakly annotated segments, together with a contrastive loss variant that outperforms well-studied alternatives. The former is based on pairwise segment distance reductions, while the latter modifies an existing loss following decoupling, hyper-parameter, and geometric considerations. With these two elements, we do not only achieve state-of-the-art results in the standard track-level evaluation, but we also obtain a breakthrough performance in a segment-level evaluation. We believe that, due to the generality of the challenges addressed here, the proposed methods may find utility in domains beyond audio or musical version matching.", "sections": [{"title": "1. Introduction", "content": "When two audio tracks contain different renditions of the same musical piece, they are considered musical versions\u00b9. Musical versions are inherent in human culture and predate recorded music and notation, as ancient music was transmitted solely through playing and listening (Ball, 2010), which naturally led to variations in tunes, rhythms, structures, etc. Learning representations of musical versions is a challenging task due to the degree and amount of variations that can be present between versions, which go beyond typical augmentations used by the machine learning community. Two musical versions may feature different instrumentation or timbre, together with tonality and chord modifications, altered melodies, substantial changes to rhythm and tempo, an alternate temporal development or structure, and many more (Yesiler et al., 2021). Yet, musical versions retain their essence, to the point that we can generally agree whether two of them correspond to the same piece or not\u00b2. Therefore, learnt version representations need to encapsulate multiple characteristics shared between versions that, at the same time, can discriminate them from other pieces.\nMusical version matching has several relevant applications (Serr\u00e0, 2011; Yesiler et al., 2021), including specific applications to plagiarism and near-duplicate detection\u00b3. Beyond business impact (Page, 2023) and cultural/artistic appreciation, some applications have become even more relevant today, given the sustained rise and improvement of music generative models (e.g., Copet et al., 2023; Evans et al., 2024; Liu et al., 2024). Indeed, the recent efforts on assessing music data replication, memorization, and attribution in such models exploit some form of music similarity (Barnett et al., 2024; Bralios et al., 2024) or, for improved results, musical version matching (Batlle-Roca et al., 2024).\nA fundamental limitation of version matching approaches is that they operate at the full-track level, learning and extracting individual representations from relatively long recordings (for instance, a few-minute song). This is due to ground truth version annotations being only available per track. However, the segments of interest, for both classical and modern applications, are much shorter than the track length (for instance, around 10-20 s). This mismatch between the learning and inference stages, as we will see, causes a dramatic performance degradation (Sec. 5). Another challenge is that, in contrast to standard supervised learning tasks, musical version data sets contain only a few items per class. For instance, up to 56% of a recent realistic large-scale"}, {"title": "2. Background", "content": "After a history of rule-, feature-, and model-based approaches (Serr\u00e0, 2011; Yesiler et al., 2021), musical version matching is currently tackled as a supervised learning problem, focusing on full-track pairwise matching. However, two versions do not necessarily need to match for their entire duration, and actually several applications rely on few-second partial matches. Only a couple of approaches base their learning or retrieval stages on segments or partial matches, respectively. ByteCover3 (Du et al., 2023) pioneered learning from segments with their \u201cmaxmean\u201d operator. However, such operator still does not allow for partial matches, as it forces all segments of a track to match some segment from another track. CoverHunter (Liu et al., 2023) is able to detect partial matches of around 45 s. However, the learning strategy to do so is based on a two-stage brute-force approach. First, it trains a coarse detector model on 15-second segments using classification, focal, and center losses. Then, it resorts to this first-stage model and a rule-based approach to (weakly) label 45-second segments, which are finally used to train the second-stage model with the same losses. In both stages, CoverHunter treats segments as full tracks. To our knowledge, we are the first to consider an entirely segment-based approach for both learning and retrieval stages.\nThe literature on musical version matching has traditionally considered a number of classification (Sun et al., 2014) and triplet (Schroff et al., 2015) loss variants, and their combination. However, given the same ground truth, another approach to learning version representations would be to consider a supervised contrastive loss like N-pairs (Sohn, 2016) or SupCon (Khosla et al., 2021). In addition, a number of well-established losses for self-supervised learning like InfoNCE/NT-Xent (Van den Oord et al., 2018; Chen et al., 2020), alignment and uniformity (Wang & Isola, 2020), or SigLIP (Zhai et al., 2023) could also be adapted. An analysis of the relations between many of such losses is"}, {"title": "3. Contrastive Learning from Weakly-Labeled Audio Segments", "content": "We now detail our approach to perform contrastive learning from weakly-labeled audio segments (CLEWS). The first part deals with track-level labels and their allocation to segment distances (we base our development on distances, but it can be easily reformulated using similarities). The second part details the contrastive loss function we use. The third part explains our architecture and training procedure."}, {"title": "3.1. Segment Distance Reduction", "content": "Framework Given the k-th waveform segment of the i-th music track, $x_i^k$, we compute latent representations $z_i^k = F(x_i^k)$, where $F$ represents a neural network that pools the time-varying information of the segment into a single vector (architecture details can be found in Sec. 3.3 and Appendix A). Then, for every possible pair of segments k and l of every possible pair of tracks i and j, we compute their distance $d_{ij}^{kl}$ and obtain the distance matrix $\\overrightarrow{D}$. At this point, if there are n query tracks and m candidate tracks with u and v segments\u2074, respectively, we have $D \\in \\mathbb{R}^{nuxmv}$. However, since labels are only provided at the track level, our binary ground truth assignments (1 for version/positive and 0 for non-version/negative) are $A \\in \\mathbb{Z}^{nxm}$. Therefore, we need some strategy to (weakly) allocate nx m labels to nu \u00d7 mv segment distances.\nA na\u00efve strategy to do such allocation would be to propagate all positive/negative track assignments to all segment comparisons in the sub-rectangle Dij defined by a pair of tracks i and j. This is the approach implicitly followed by CoverHunter (Liu et al., 2023). However, besides its poor performance (Sec. 5), this strategy incurs a fundamental error, in the sense that it is teaching the model that all positive segments are 'similar' to all other positive segments, which in the case of musical versions is false (two segments could reproduce two different motives of the same song; even though it is the same song, the segments are usually not the same, unless it is an extremely repetitive song). Instead of trying to allocate n \u00d7 m positive/negative distances to nu \u00d7 mv segment distances, we take the opposite view"}, {"title": "Reduction Functions", "content": "The na\u00efve strategy outlined above would correspond to a mean reduction over the entire sub-rectangle determined by Dij:\n$d_{ij} = R_{mean} (\\overrightarrow{D_{ij}}) = \\frac{1}{A_{ij}} \\Sigma_{1<k<u \\atop 1<l<v} d_{ij}^{jkl}$\nInstead, if we just consider the r best matches across segments, we have\n$d_{ij} = R_{bestr} (\\overrightarrow{D_{ij}}) = \\frac{1}{r} \\Sigma_{1<t<r} top_r(\\overrightarrow{D_{ij}})_t$\nfor r < uv, where $top_r(D)$ is a function that returns the lowest r distances in D (Fig. 1, bottom left). Another possibility is to consider just the single best correspondence in the entire sub-rectangle (Fig. 1, bottom right), yielding\n$d_{ij} = R_{min} (\\overrightarrow{D_{ij}}) = \\underset{1<k<u \\atop 1<l<v}{min} d_{ij}^{jkl}$"}, {"title": "Best-pair Reduction", "content": "Motivated by the issues of consecutive and global segment matching above, we decide to design an additional reduction strategy that explicitly deals with both. We term it bpwr-r, for 'best pair without replacement' with a threshold r. In a nutshell, Rbpwr-r operates by sorting all distances in the Dij sub-rectangle in increasing order, then taking the first one (say the one involving segments k and l), removing all distances computed by using either one or the other segment (either k or l), and iterating r times. It then takes the average among those r best pairwise distances. More formally, we can express it as\n$d_{ij} = R_{bpwr-r} (\\overrightarrow{D_{ij}}) = \\frac{1}{r} \\Sigma_{1<q<r} R_{min} (D^{(q)}_{ij})$\nfor r < min(u, v), with the recursion\n$D^{(q)}_{ij} = \\begin{cases} Dij & \\text{for q = 1,} \\\nmask_{min}(D^{(q-1)}_{ij}) & \\text{for q > 1,} \\end{cases}$\nwhere maskmin(D) is a function that masks the row and the column corresponding to the minimum element in D, such that those elements are not eligible by the Rmin of Eq. 1 in iterations q > 1. A schema of Rbpwr-3 is illustrated in Fig. 1 (top right; recursion is depicted by progressively darker colors). Notice that masking rows and columns avoids the issue of consecutive segment matching, and that a threshold r < min(u, v) avoids the issue of full-track matching, which only happens when r = min(u, v)."}, {"title": "Positives and Negatives Reduction", "content": "We note that the previous distance reduction strategies have some conceptual parallelism with the negative/positive mining strategies used with triplet losses (Schroff et al., 2015) or in some contrastive approaches (cf. Kalantidis et al., 2020). In our case, for instance, Rmin could correspond to a hard mining strategy, while Rbest-r or Rbpwr-r could be regarded as semi-hard mining of segment pairs. Thus, inspired by those strategies, we decide to study if applying different reduction strategies for positives and negatives has some effect in our setup. To obtain a track-based pairwise distance matrix that combines different reductions for positive and negative pairs, we calculate\n$\\overrightarrow{D} = A \\odot R^+(\\overrightarrow{D}) + (1 - A) \\odot R^-(\\overrightarrow{D}),$\nwhere $\\odot$ denotes element-wise multiplication and 1 is the all-ones matrix (recall that the elements in A are 1 for positives and 0 otherwise). The reductions R+ and R- can be chosen among the ones presented above."}, {"title": "3.2. Contrastive Loss", "content": "Motivation After computing pairwise track-level distances $\\overrightarrow{D}$, we need a contrastive loss that can exploit them and that, ideally, can outperform the existing losses in the considered task. For that, one can consider any supervised contrastive loss function that operates on distances, or adapt an existing self-supervised loss to the supervised framework (Sec. 2). In our case, we opt for the latter and choose the A&U loss of Wang & Isola (2020) due to its appealing properties and intuitive derivation. One of the practical properties we value is that, by using expectations, we have a similar behavior for different batch sizes (Wang & Isola 2020; see also Koromilas et al. 2024). In our analysis, we will use the concept of \u201cpotential\u201d as introduced by Wang & Isola (2020), but nonetheless will depart from the concept of uniformity in the hypersphere.\nChanges to Alignment and Uniformity The A&U loss was designed for self-supervised contrastive learning, and thus expects one positive for each item in the batch (obtained through some augmentation), and negatives correspond to all other elements in the batch (uniformly sampled from all available data). To adapt A&U to supervised contrastive learning with multiple positives per anchor, we need to carefully define both positive and negative sets. In particular, we want to preserve the decoupling of the alignment and uniformity terms as, apart from respecting the original idea of A&U, it typically yields improved performance (Yeh et al., 2022). Therefore, from all pairwise assignments A in the batch, we need to gather positive $A^+$ and negative $A^-$ assignment sets such that $A^+ \\cap A^- = \\emptyset$. This also implies discarding comparisons of one track against itself (that is, the diagonals of D and A, and potentially other spurious cells corresponding to sampling the same track more than"}, {"title": "Role of the Hyper-parameters", "content": "With the decoupling, hyper-parameter, and geometric considerations above, we formulate the CLEWS loss as\n$\\mathcal{L} = \\frac{1}{\\vert A^+ \\vert} \\Sigma_{(i,j) \\in A^+} d_{ij}^2 + log(\\frac{1}{\\vert A^- \\vert} \\Sigma_{(i,j) \\in A^-} e^{-\\gamma d_{ij}^2} + \\varepsilon ) \\bigg)$,\nwhere $d_{ij}$ are distances after reduction (Eq. 2) and $\\gamma, \\varepsilon > 0$ are hyper-parameters. We use dimension-normalized Euclidean distances (root mean squared differences), as this does not affect our geometric considerations (it just adds a constant) and facilitates maintaining the same hyper-parameters when changing the dimensionality of z. The $\\varepsilon$ hyper-parameter is initially introduced for numerical stability. However, we note that it also has a soft thresholding or smoothing effect for the potential between negative pairs.\nWe now briefly and intuitively study the role of $\\gamma$ and $\\varepsilon$ in $\\mathcal{L}$ (a full analysis is beyond the scope of the present paper). To do so, we"}, {"title": "3.3. Architecture and Training", "content": "We now overview CLEWS' network architecture $F$ and its training procedure (further details are available in Appendix A and in our code). To obtain segment embedding vectors z on which to compute distances, we start from the full-track audio waveform and uniformly randomly cut a 2.5 min block x from it. We further cut x into 8 non-overlapping 20-second segments $x^k$ (we repeat-pad the last segment). We then compute its constant-Q spectrogram, downsample it in time by a factor of 5, and normalize it between 0 and 1, all following similar procedures as common version matching approaches (Yesiler et al., 2021). After that, we pass it to a learnable frontend, formed by two 2D strided convolutions, batch normalization, and a ReLU activation. Next, we employ a pre-activation ResNet50 backbone (He et al., 2016) with ReZero (Bachlechner et al., 2021) and instance-batch normalization (Pan et al., 2018). We pool the remaining spectro-temporal information with"}, {"title": "4. Evaluation Methodology", "content": "Data - We train and evaluate all models on the publicly-available data sets DiscogsVI-YT (DVI; Araz et al., 2024a) and SHS100k-v2 (SHS; Yu et al., 2020), using the predefined partitions. SHS is a well-established reference data set. However, since it is based on YouTube links, it is almost impossible to gather it entirely nowadays (we managed to gather 82% of it). In addition, one could consider it slightly biased, as the version group sizes are unrealistically large (cf. Doras & Peeters, 2020; Araz et al., 2024a). Instead, the recently proposed DVI data set is 5 times larger and better represents the real-world distribution of version group sizes. For both data sets, we use 16kHz mono audio and cap the maximum length to the first 10 min.\nBaselines To compare the performance of the proposed approach with the state of the art, we consider several baselines: CQTNet (Yu et al., 2020), MOVE (Yesiler et al., 2020a), LyraC-Net (Hu et al., 2022), CoverHunter (Liu et al., 2023), DVINet+ (Araz et al., 2024b), Bytecover2 (Du et al., 2022), ByteCover3 (Du et al., 2023), and ByteCover3.5 (Du et al., 2024). For CoverHunter, we just consider the first \"coarse\" stage, as that is the part dealing with segments. CoverHunter, CQTNet, and DVINet have convenient source code available, and thus we can produce results by using it in our own pipeline (this way we can compare those baselines with our model rigorously under the same setting). Due to GPU memory restrictions, we train with randomly-sampled audio blocks of 2.5 min. For the other baselines, we can only"}, {"title": "Evaluation", "content": "During testing, to compute candidate embeddings, we treat all models as if they were segment-based and extract overlapping blocks or segments using the same length as in training and a hop size of 5 s (this yielded a marginal improvement for full-track baselines trained on 2.5 min blocks). With these candidate embeddings, we perform both track- and segment-level evaluations. The former is equivalent to the usual evaluation setup in musical version matching, while the latter focuses on the retrieval of best-matching segments. For the track-level evaluation, we use the same segment length (the training segment length) and hop size for both queries and candidates. Then, to measure the performance of the system working at the full-track level, we use Rmeanmin to compute the final query-candidate distance. For the segment-level evaluation, we keep the same segment configuration as in the track-level case, but we vary the query segment length 7. This way we assess a model's performance on different query lengths found in real-world scenarios. Then, to measure the performance of the system working at the segment level, we use Rmin to compute a best-match query-candidate distance. With this best-match approach, we simulate the performance of an equivalent segment-based retrieval system using all raw segments as candidates. As evaluation measures, we compute the usual mean average precision (MAP) plus an enhanced version of the normalized average rank (NAR). MAP focuses on the precision in the top candidates while NAR focuses more on the overall recall, which we think is a better option for musical version matching, especially for segment-based applications."}, {"title": "5. Results", "content": "Comparison with the State of the Art - First of all, we focus on the track-level evaluation and compare with the state of the art. We observe that CLEWS outperforms all considered approaches, many of them by a large margin (Table 2). CLEWS obtains a NAR of 2.70 on DVI-Test and a MAP of 0.876 on SHS-Test, setting a new state-of-the-art"}, {"title": "Segment-based Version Matching", "content": "We now focus on the segment-level evaluation and study the performance as a function of the query segment length 7. We observe that CLEWS again outperforms all considered models both in DVI-Test (Fig. 2) and SHS-Test , and for both NAR and MAP measures. Importantly, CLEWS maintains a high performance for all considered lengths (Fig. 2). The only exception is with t = 5 where, according to our listening experience, it is sometimes difficult even for a human to establish if two audio segments are versions or not. According to our segment-level evaluation, ByteCover3\u2020 features some noticeable performance degradation with large T, perhaps due to the global match approach. CQTNet and DVINet+, both based on the same plain convolutional architecture, show an early performance decline for T < 60."}, {"title": "Ablations and Hyper-parameters", "content": "Finally, we focus our attention on possible variations to the default CLEWS. We start by studying the effect of positive R+ and negative R-segment distance reductions . For R+, if we keep R=Rmin, we observe that, depending on the evaluation measure, we have two options that are better than the default:"}, {"title": "6. Conclusion", "content": "In this paper, we tackle the task of segment-based musical version matching, and propose both a strategy to deal with weakly-labeled segments and a contrastive loss that outperforms well-studied alternatives. Through a series of extensive experiments, we show that our approach not only achieves state-of-the-art results in two different datasets and two different metrics, but also that it significantly outperforms existing approaches in a best-match, segment-level evaluation. We also study the effect of different reduction strategies, compare against existing losses, and analyze the effect of the hyper-parameters in our ablation studies. As weakly labeled segment information is ubiquitous in many research areas, and since the concepts exploited here are general to a wide range of contrastive learning tasks, we believe our methods could serve as inspiration or find usefulness in domains beyond audio and musical version matching."}]}