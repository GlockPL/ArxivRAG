{"title": "ContextIQ: A Multimodal Expert-Based Video Retrieval System for Contextual Advertising", "authors": ["Ashutosh Chaubey", "Anoubhav Agarwaal", "Aayush Agarwal", "Sartaki Sinha Roy", "Susmita Ghose"], "abstract": "Contextual advertising serves ads that are aligned to the content that the user is viewing. The rapid growth of video content on social platforms and streaming services, along with privacy concerns, has increased the need for contextual advertising. Placing the right ad in the right context creates a seamless and pleasant ad viewing experience, resulting in higher audience engagement and, ultimately, better ad monetization. From a technology standpoint, effective contextual advertising requires a video retrieval system capable of understanding complex video content at a very granular level. Current text-to-video retrieval models based on joint multimodal training demand large datasets and computational resources, limiting their practicality and lacking the key functionalities required for ad ecosystem integration. We introduce ContextIQ, a multimodal expert-based video retrieval system designed specifically for contextual advertising. ContextIQ utilizes modality-specific experts-video, audio, transcript (captions), and metadata such as objects, actions, emotion, etc. to create semantically rich video representations. We show that our system, without joint training, achieves better or comparable results to state-of-the-art models and commercial solutions on multiple text-to-video retrieval benchmarks. Our ablation studies highlight the benefits of leveraging multiple modalities for enhanced video retrieval accuracy instead of using a vision-language model alone. Furthermore, we show how video retrieval systems such as ContextIQ can be used for contextual advertising in an ad ecosystem while also addressing concerns related to brand safety and filtering inappropriate content.", "sections": [{"title": "1. Introduction", "content": "Contextual advertising involves placing ads based on the content a consumer is viewing, improving user experience and leading to higher engagement and ad monetization [74]. This method mitigates the need for personal data in advertising, which raises notable legal and ethical concerns [20, 59]. Contextual advertising is already widely integrated into ads served on websites powered by platforms like Google AdSense [27]. Recent AI advancements have taken this a step further by enabling a deeper semantic understanding of multimedia content [34], allowing for more precise ad placements beyond traditional targeting [41]. Building on these developments, this study expands the use of contextual advertising to video content, aiming to improve ad targeting on social platforms such as YouTube and streaming services like Free Ad-Supported Streaming Television (FAST) and Video on Demand(VoD).\nModern video streaming platforms, with their vast content libraries, require advanced methods for analyzing video content to retrieve the most suitable content for ad placements for contextual advertising. We propose that this task of identifying the most relevant video can be achieved with text prompts that align with an advertisement campaign and hence can be formulated as the text-to-video (T2V) retrieval problem in multimodal learning [45, 46, 51,78]. Therefore, improvements in T2V retrieval models can enhance contextual advertising by better-aligning ads with the semantic context of video content.\nWith the exponential growth of video content, T2V retrieval has become more essential, driving progress in search algorithms [11,21,48] and video representation techniques such as masked reconstruction [31, 62, 66], multimodal contrastive learning [18, 72], and next-token prediction from video data [43,60].\nA recent trend in T2V retrieval is large-scale pretraining to learn a joint multimodal representation of a video [17, 18, 69]. While these joint models have demonstrated strong performance on public benchmarks, their reliance on massive multimodal datasets and substantial computational resources limits their practical use. In contrast, there are expert-based approaches that use specialized models to extract features from individual video modalities such as visuals, motion, speech, audio, OCR, etc. [45, 50]. Though this method is well-established in T2V retrieval, its prominence has declined with the rise of joint multimodal models.\nHowever, expert models present unique benefits for contextual advertising, which we explore in this work.\nThis paper introduces ContextIQ, a multimodal expert-based video retrieval system for contextual advertising. ContextIQ leverages experts across multiple modalities-video, audio, captions (transcripts), and metadata such as objects, actions, emotions, etc. to create semantically rich video representations (ref. Sec. 3 & 4). Its modular design makes it highly flexible to various brand targeting needs. For instance, for a beauty brand, we could integrate advanced object detection to spot niche beauty accessories. Moreover, the proposed system can selectively use only a relevant expert model, focusing on the key metadata to enable efficient targeting, making it well-suited for a fast ad-serving system. Furthermore, our system enhances interpretability by allowing individual experts to be analyzed for error tracing and model improvements, which is vital for explaining business outcomes in ad targeting. Beyond conventional video retrieval, we show how ContextIQ can be integrated seamlessly into the ad ecosystem, processing long-form streaming content and implementing brand safety filters to ensure ads are placed within contextually appropriate and safe content (ref. Sec. 6).\nWe evaluate the effectiveness of our ContextIQ system across several video retrieval benchmarks, such as MSR-VTT [73], Condensed Movies [9], and a newly curated dataset (Val-1) of high-production content that we make public. We compare it to state-of-the-art joint multimodal models [54, 68, 77] and commercial solutions like Google's multimodal embedding API [28] and Twelvelabs Marengo [63]. The results show that our expert-based approach is better or comparable to these solutions (ref. Sec. 5.1).\nWe also address key evaluation challenges, particularly the differences between video domains in existing public datasets such as shorter, amateur-produced content-and the complex, high-production content typical of contextual advertising by validating the proposed technique on the curated dataset (Val-1) of movie contents and evaluating different approaches by manual validators. To further concretize our design choice of having multiple experts, we perform ablation studies on the impact of incorporating multiple modalities on an internal dataset (Val-2), demonstrating how our system effectively aggregates modality-specific experts. By leveraging the complementarity of different modalities, ContextIQ achieves improvements in both performance and coverage (ref Sec. 5.2). We release our datasets, retrieval queries and annotations publicly at https://github.com/WACV2025Submission/ContextIQ. In summary, our contributions are as follows,\n(i) We present ContextIQ, a multimodal expert-based video retrieval system specifically designed for contextual advertising. With capabilities for long-form content processing, modularity for real-time ad serving, and robust brand safety measures, it integrates seamlessly into the advertising ecosystem, effectively bridging the gap between video retrieval research and contextual video advertising.\n(ii) On existing benchmarks, we show that combining modality-specific experts can yield better or comparable results with state-of-the-art joint multimodal models without the need for joint training, large-scale multimodal datasets, or significant computational resources.\n(iii) We release a validation dataset (Val-1) of high-production content for text-to-video retrieval for contextual advertising and show the superior performance of the proposed approach on that dataset.\n(iv) We perform ablation studies to show that additional modalities via expert-based models increase the coverage and accuracy of text-to-video retrieval for contextual advertising by showing results on an internal dataset of video contents."}, {"title": "2. Related Works", "content": "AI in advertising. Liu et al. [44] employed text-based topic modeling to tag content, enabling buyers to make more informed ad bids. Wen et al. [61] utilized decision trees to identify key attributes that enhance ad persuasiveness, facilitating the selection of suitable ads and insertion points within videos. Bushi et al. [12] analyzed sentiment to prevent negative ad placements, while Vedula et al. [65] predicted ad effectiveness by training separate neural network models on video, audio, and text modalities.\nText-to-Video Retrieval. Earlier approaches relied on convolutional neural networks (CNNs) and recurrent neural networks (RNNs) [30, 51] to extract spatio-temporal features from video frames. However, the advent of Transformer-based architectures [24, 26, 57] has led to a paradigm shift, with Transformers outperforming traditional CNNs and RNNs in many public benchmark datasets.\nMultimodal Pre-training and Joint Learning. The impact of large-scale pre-training, especially using models like CLIP [54] that align images with text, has been groundbreaking in the video retrieval domain [23, 38, 46, 47, 75]. For instance, CLIP4Clip [46] builds upon CLIP to enable end-to-end video-text retrieval via video-sentence contrastive learning. Similarly, CLAP [71] aligns audio with text using contrastive learning. Incorporating additional modalities contrastively can significantly enhance the model's performance and robustness [18, 69]. Language-Bind contrastively binds language across multiple modalities within a common embedding space [77]. VALOR utilized separate encoders for video, audio, and text to develop a joint visual-audio-text representation [17]. We show"}, {"title": "3. Approach", "content": "For a video dataset $V = \\{v_1, ..., v_n\\}$, our goal is to develop a text-to-video retrieval system. Conceptually, we aim to learn a similarity function $S(v_i, t)$ that assigns higher scores to videos that are more relevant to the given text query $t$.\nWe extract embeddings from multiple modalities, including raw video, audio, captions (speech transcripts), and combined visual (objects, places, actions, emotion) and textual (NER, emotion, profanity, hate speech) metadata, as shown in the Metadata Extraction Module in Fig. 1. Since the expert encoders for each modality are not jointly trained, their embeddings reside in separate semantic spaces. These embeddings are then stored in a multimodal embeddings database. We propose a retrieval approach (Fig. 2) that compares, merges, and re-ranks these modality-specific embeddings to deliver the most contextually relevant videos."}, {"title": "3.1. Multimodal Embedding Generation", "content": ""}, {"title": "3.1.1 Video", "content": "As illustrated in Fig. 1, we consider each video $v_i$ to be composed of $M$ frames \uba53, which are encoded using an image-encoder $f_{\\theta_v}$ from a vision-text model [42]. This encoding captures intrinsic video features learned by the vision-text model.\nWe found that splitting the video into fixed non-overlapping time segments, each containing $T$ frames, enhances both retrieval accuracy and localization. For each temporal segment $C = \\{v_z | z \\in \\{kT+1, ..., (k+1)T\\}\\}$, segment-level embeddings are obtained by applying an aggregation function $A$ to the frame-level features,\n$e_k^v = A(\\{f_{\\theta_v}(v_z) | z \\in \\{kT + 1, ..., (k + 1)T\\}\\})$\nThe final set of video-level embeddings for the video $v_i$ that we store is,\n$F^v_i = \\{e_1^v,...,e_{M/T}^v\\}$\nNote that for a single video, we store $(M/T)$ embeddings in the multimodal database corresponding to intrinsic video features. This ensures that for long-form video content, the features have high temporal resolution and do not average out, resulting in better retrieval."}, {"title": "3.1.2 Audio", "content": "To capture the non-verbal audio elements in a video, such as sound effects, music, and ambient noise, we utilize the"}, {"title": "3.1.3 Caption", "content": "To incorporate textual and speech information from a video, we encode the caption (transcript) $c_i$ for a given video using a text encoder $h_{\\theta_3}$, resulting in the caption feature embedding of the video:\n$F^c_i = h_{\\theta_3}(c_i)$"}, {"title": "3.1.4 Metadata", "content": "Foundation vision models [42,54] excel at learning features for vision-specific tasks with minimal fine-tuning. To enhance the performance of our text-to-video retrieval system, we incorporate models specifically trained for vision and textual tasks, effectively augmenting foundational models (ref. Sec. 5.2).\nAfter inference with the task-specific models, we store the extracted information into a metadata sentence $m_i$. This sentence captures objects, places, actions, emotions, named entities and flags any profanity, hate speech, or offensive language. This metadata is vital for contextual advertising. For example, object detection can allow fashion brands to target content featuring clothing or accessories, while place detection can benefit outdoor gear brands by identifying natural landscapes. Video action recognition can enable fitness brands to reach workout-related content, and named entity recognition might help travel agencies focus on videos mentioning tourist destinations. $m_i$ provides a unique and flexible method for integrating outputs from the different expert models. $m_i$ follows the template, as illustrated in Fig. 1.\nFinally, we encode $m_i$ by the same text encoder model $h_{\\theta_3}$ from Sec. 3.1.3, to obtain the metadata feature embedding for the video:\n$F^m_i = h_{\\theta_3}(m_i)$\nAdditionally, the emotion, profanity, and hate speech metadata are used as a filtering mechanism during the search process to ensure the retrieval of brand-safe videos. This capability extends beyond the conventional scope of video-text retrieval research, with its application further elaborated in Sec. 6 on contextual advertising."}, {"title": "3.2. Multimodal Search", "content": "Our system employs multimodal embeddings from audio, video, text and metadata to enable efficient and precise content retrieval. As shown in the Fig. 2, a text query $t$ is first encoded using the text encoders of the vision-text model ($f_{\\theta_v}$), audio-text model ($g_{\\theta_2}$) and text-text model ($h_{\\theta_3}$) resulting in the embeddings $F_v$, $F_a$ and $F_c$ respectively.\nThe text embeddings are then compared against their corresponding embedding databases using cosine similarity. Specifically, $F_a$ is compared with audio embeddings $\\{F^a_i\\}$, producing similarity scores $\\{S^a(v_i, t)\\}$; $F_v$ is compared with visual embeddings $\\{F^v_i\\}$, yielding similarity scores $\\{S^v(v_i, t)\\}$; and $F_c$ is compared with metadata embeddings $\\{F^m_i\\}$ and caption embeddings $\\{F^c_i\\}$, generating similarity scores $\\{S^m(v_i,t)\\}$ and $\\{S^c(v_i, t)\\}$, respectively. Note that $\\{S^v (v_i,t)\\}$ is the max-similarity score of t with the segments $(\\{C^v\\}_1,..., C^v(M/T))$ of video $v_i$.\nThese similarity scores $\\{(S^c,S^m,S^a,S^v)(v_i,t)\\}$ are subsequently aggregated into a combined score $S(v_i, t)$ using an aggregation module $A_s$ (ref. Fig. 2) which involves normalization, thresholding, and weighted merging,"}, {"title": "\u2022 Normalization.", "content": "Scores are standardized within their respective modality space and weighted to produce normalized score:\n$N_k (v_i, t) = \\lambda_k \\frac{S_k (v_i, t) - \\mu_k}{\\sigma_k}$\nwhere $\\mu_k = \\frac{1}{n} \\sum_{i=1}^n S_k(v_i, t)$ is the mean, $\\sigma_k = \\sqrt{\\sum_{i=1}^n (S_k(v_i, t) - \\mu_k)^2}$ is the standard deviation and $\\lambda_k$ is the weight of modality k (metadata, caption, video or audio)."}, {"title": "\u2022 Thresholding.", "content": "Scores are thresholded to obtain dictionaries for each modality $\\aleph^k = \\{(i : N_k (v_i, t)) | S_k (v_i,t) > \\alpha_k\\}$ where $\\alpha_k$ is the modality specific threshold."}, {"title": "\u2022 Merging.", "content": "The thresholded dictionaries for each modality are merged using a max-aggregation approach, with the final scores S(vi, t) determined by the maximum value for each key.\nThe thresholds $\\alpha_k$ and weights $\\lambda_k$ can be tuned for optimal performance on a representative set of queries. The resulting video contents are finally filtered through the brand safety mechanism (ref. Sec. 6)."}, {"title": "4. Implementation Details", "content": ""}, {"title": "4.1. Multimodal encoders", "content": "We use PyTorch [8] for all our model implementations, and we use four NVIDIA RTX 4090 GPUs to run all our experiments. The text encoder $h_{\\theta_3}$ is a pre-trained MPNet model [58] fine-tuned on a set of 1 billion text-text pairs as described by Reimers et al [55]. The vision-text model $f_{\\theta_v}$ is a pre-trained BLIP2 Qformer [42], and we use the implementation provided by LAVIS [40]. We split the video into equal segments of 15 seconds each and sample one out of ten frames for embedding generation (ref. Sec. 3.1.1).\nThe audio encoder $g_{\\theta_2}$ is the CLAP [71] model as implemented in [2]. Since CLAP is trained on 5-second audio segments, we divide the audio into 5-second chunks ($a_i$) for inference. The aggregation functions $A_a$ and $A_v$ used during audio and video embedding generation (Sec. 3.1) are temporal mean pooling functions, shown to be effective over other temporal aggregation techniques [10,46]."}, {"title": "4.2. Metadata Extraction", "content": "We use the YOLOv5 model [64], trained on the Objects365 dataset [56], to detect objects within videos with an IOU threshold of 0.45 and a confidence threshold of 0.35. An object is considered present in the video only if it appears in at least 20% of the frames, to filter false positives.\nFor place detection, we finetune a ResNet50 model [32] on the Places365 dataset [76]. Only frames where object detection for the Person class covers less than 10% of the area are used to ensure a clear background. Top predictions from these frames with softmax scores above 0.3 are considered, and the most frequent place prediction is tagged as the video's location. Since video segments are short, we assume a single location per segment. For both place and object detection, predictions are sampled from every 10th frame.\nWe use a fine-tuned video masked autoencoder model (VideoMAE2) [67] on the Kinetics 710 dataset [14, 15, 35] for video action recognition. The Kinetics dataset comprises shorter, simpler YouTube clips featuring single actions, whereas our video retrieval algorithm is applied to videos with longer, more complex scenes involving multiple actions. To bridge this gap, we reduced the Kinetics 710 classes to 185 by eliminating less relevant or overly specific classes for advertising, merging correlated classes, and discarding those with low Kinetics validation accuracy. We also refined the majority voting method by incorporating prediction probability scores, improving the handling of multiple actions in a single clip. More implementation details are present in the supplementary material, and we share the list of reduced classes on our github repository - https://github.com/WACV2025Submission/ContextIQ.\nFor named entity recognition, we utilize the text-based ROBERTa model, en_core_web_trf from Spacy [33] to extract the named entities from the captions of each video scene. For profanity detection, we used the alt-profanity-check [1] python package on the video transcript (caption). Additionally, we use a predefined list of words given in [3] to further filter out profane videos.\nFor hate speech detection, we use a weighted ensemble of two models. First, we use a pre-trained LLAMA 3 8B Instruct model [7] with a temperature of 0.6, leveraging advanced prompting strategies such as JSON-parseable responses and chain-of-thought reasoning to flag content as hateful. Secondly, we use a pre-trained BERT classifier [36] trained on the HateXplain dataset [49] to categorize content into three classes: hate speech, offensive, and normal. For text emotion recognition, first, we use a pre-trained Emoberta-Large model [37] from huggingface [6] which is trained on the MELD [53] and IEMOCAP [13] datasets. Furthermore, we also leverage the computed video and audio embeddings from Sec. 3.1 for tagging emotions by associating text concepts with different emotions. For example, we tagged the emotion joy with text queries like people smiling and people dancing, and assigned the emotion joy to all videos retrieved through the video (vision) modality using these queries. More implementation details of emotion, profanity, and hate speech detection are present in the supplementary material."}, {"title": "4.3. Validation datasets and Metrics", "content": "Validation datasets. As mentioned before, we show the efficacy of the proposed approach in comparison to state-"}, {"title": "5. Results", "content": ""}, {"title": "5.1. Zero-shot text-to-video retrieval", "content": "We evaluate the performance of ContextIQ for text-to-video retrieval using the validation datasets mentioned in Sec. 4.3. Tab. 1 shows the performance of the proposed approach on the MSR-VTT dataset [73] compared to a state-of-the-art jointly trained multimodal model (LanguageBind [77]) and a popular industry solution for video retrieval (Google's Vertex API [28]). For LanguageBind we use the huggingface implementation - LanguageBind_Video_FT [4]. Although ContextIQ is not jointly trained on multiple modalities, it performs slightly better than Google's Vertex. We hypothesize that LanguageBind's superior performance on MSR-VTT can be attributed to its joint training on the large-scale 10M VIDAL dataset, which includes a diverse range of videos similar to the general content found in MSR-VTT, rather than being focused on entertainment-specific content. Additionally, LanguageBind incorporates modalities such as depth, which are absent in ContextIQ.\nAs shown in Tab. 2, for the Condensed Movies dataset [9], we compare the results of our proposed technique with TwelveLabs, utilizing their Marengo API [63], a jointly trained multimodal model for text-to-video retrieval. For all the approaches listed in Tab. 2, we first retrieve videos for each of the text queries generated by ChatGPT (ref. Sec. 4.3) and then ask three manual validators to validate the top 5 results using the validation tool built using Streamlit [5] as shown in Fig. 3. We use a voting-based system among the annotators to compile the results of our validation. We can see that ContextIQ performs better than LanguageBind on all the metrics, while it performs comparable to Twelve-Labs. These findings further show that ContextIQ, a mixture of expert-based models can perform comparable to or even better than the jointly trained multimodal models.\nTab. 3 shows the performance of the proposed approach as compared to different approaches on our curated dataset (Val-1 as described in Sec. 4.3). We use the large variant of CLIP [54] for our comparison. Note that One-Peace [68] and LanguageBind [77] are multimodal models that are trained jointly, where embeddings from all modalities reside in the same space. ContextIQ performs significantly better than the baselines, especially when we check the precision at higher k values. It is also important to highlight that CLIP, which is just a vision-language model, performs comparably to the multimodal approaches LanguageBind and One-Peace, highlighting that for most of the queries, only visual understanding results in good retrieval results."}, {"title": "5.2. Ablation: Impact of additional experts and modalities", "content": "We saw in the previous paragraph that only a vision-language model achieves comparable results on the task of video retrieval for advertising on our advertising-focused curated dataset (Val-1). In this section, we perform an ablation study to see the efficacy of different modalities when combined with the vision-text model encoder.\nSince we want to simulate the effect of using our system in a large database of multimedia content, we utilize an internal dataset (Val-2) comprising over 2,000 long-form videos (movies, TV, and OTT contents) processed through our ContextIQ system (ref. Sec. 6) to generate over 100,000 scenes (videos), averaging 30 seconds in duration. We curate retrieval query sets for each additional modality, focusing on queries that highlight their individual strengths and are relevant to ad targeting. Details about the dataset are included in our github repository.\nWe compute the audio, video, caption and metadata embeddings for the entire dataset and store them separately. We then compare the performance of vision-only (video) embeddings with vision combined with different modalities as shown in Tab. 4. Since we do not have labeled ground truth for this internal dataset as well, we employ a similar manual validation technique which was used for validating Condensed Movies (ref. Tab. 2). For each query, we search and retrieve the top 30 videos from the dataset. Vision-only results are based on similarity scores between the text query and vision embeddings, while vision + additional modality results combine scores from both modalities using the aggregation module (ref. Sec. 3.2). The top 30 videos from both methods are then annotated for correctness by 3 annotators.\nTab. 4 shows that adding an additional modality consistently improves precision across all K values compared to using a vision-only model. The precision gap between vision and vision+modality widens as K increases. $A_{avg}$ represents the average difference in precision between vision and vision+additional modality:\n$A_{avg} = \\frac{1}{K}\\sum PV,K - PV+X,K$\nWhere, K is the set of K values (5,10,...,30} and $P_{V,K}$ is the precision for vision-only variant at K, and $P_{V+X,K}$ is the precision for vision+additional modality.\nWe observe that the average precision delta is highest for audio, followed by caption and then metadata. This is due to the fact that the vision modality doesn't capture raw audio or captions (transcripts) but is better at representing metadata elements like objects, actions, places, etc.\nTab. 5 further shows that without our aggregation module, the scenes retrieved by each modality independently for the same query differ significantly from those retrieved by the vision-only variant, resulting in very low overlap. This confirms that each modality captures distinct aspects of the video. The overlap with vision follows the order: audio < caption < metadata, which aligns with expectations. These findings show that utilizing the complementarity of various modalities improves both performance and coverage."}, {"title": "6. Contextual Advertising", "content": "Contextual Advertising targets ads based on the content a user is viewing, improving the experience, boosting engagement, and increasing conversion rates, all without using personal information, making it privacy-friendly. Fig. 4 shows how ContextIQ is integrated into the Connected TV advertising ecosystem that enables advertisers to perform contextual advertising.\nProcessing Long-Form content. Long-form content, such as movies and shows, is processed by ContextIQ by breaking them into shorter videos using scene detection. We use PySceneDetect [16] for scene detection with default parameters. Each scene is subsequently processed through ContextIQ's multimodal embedding generation module (ref. Sec. 3.1) to generate the reference multimodal scene embeddings.\nIntegration into Ad Serving system. Depending on the brand campaign and the advertisements to be served, an advertiser defines a set of relevant text queries. For example, a pet food brand might have queries such as dogs, cats, pet food, etc. Using these brand-specific text queries and the multimodal embeddings, ContextIQ's multimodal search (ref. Sec. 3.2) identifies scenes where creatives can be contextually served. Additionally, these scenes can be passed through brand safety filters to ensure that brands don't get associated with sensitive/profane scenes. The selected scenes are stored in the scene-to-context lookup DB. When a viewer is watching the TV, the ad gateway looks up the scene to context lookup DB to find out the relevant context for showing advertisements; the ad gateway serves the brand's ad as shown in Fig. 4. ContextIQ can easily be extended to retrieve relevant scenes for showing an ad based on image, video or even audios. For example, an advertiser might directly use their brand advertisement as a query for video retrieval and get the relevant search results (more details in the Supplementary material).\nBrand Safety. Ensuring brand safety is crucial in video retrieval systems for contextual advertising. Advertisers are increasingly vigilant about the environments in which their brands appear, as association with inappropriate content, such as offensive language, hate speech, negative emotions, adult material, or references to crime and terrorism, can cause significant reputational damage and erode consumer trust. To address these concerns, our ContextIQ system integrates a safety mechanism comprising two key filters: (i) Emotion Recognition filter, which evaluates the emotional quality of content, ensuring alignment with the brand's messaging. (ii) Hate Speech and Profanity Detection filter, which blocks content containing hate speech, explicit language, or other inappropriate communication, preventing ads from appearing alongside harmful content. Since we already extract emotional and profanity information during metadata extraction (ref. Sec. 3.1.4 & 4.2), we use the same information during brand safety filtering with no additional compute.\nModularity. ContextIQ, with its diverse set of expert models, offers flexibility by allowing the use of a specific subset of models tailored to particular use cases. For example, the textual modality can be used for real-time content ingestion to serve ads during live news segments, filtering out violent content that many brands prefer to avoid. Additionally, each expert model can be fine-tuned to meet specific brand requirements. For instance, place and object detection models can be fine-tuned accordingly to support a casino brand looking to detect both a casino location and a roulette wheel within the content."}, {"title": "7. Conclusion", "content": "This paper introduces ContextIQ, an end-to-end video retrieval system designed for contextual advertising. By leveraging multimodal experts across video, audio, captions, and metadata, ContextIQ effectively aggregates these diverse modalities to create semantically rich video representations. We demonstrate strong performance on multiple video retrieval benchmarks, achieving results better or comparable to jointly trained multimodal models without requiring extensive multimodal datasets and computational resources. Our ablation study shows the advantage of incorporating multiple modalities over a vision-only baseline. We further examine how ContextIQ extends beyond the conventional video retrieval task by integrating seamlessly into the ad ecosystem, processing streamed long-form content, offering modularity for efficient real-time ad serving, and implementing brand safety filters to ensure ads are placed within contextually appropriate and safe content."}, {"title": "A. Alternative Modality Queries to ContextIQ", "content": "The flexibility of our system allows us to effortlessly perform queries across different modalities, including video, audio, and image, ensuring any-to-any search capabilities.\nImage Query: The process begins by encoding the input query image using the vision encoder of the vision-text model $f_{\\theta_v}$, resulting in an image embedding. This embedding is then compared against the vision embeddings of all available content $\\{F^v_i : i = 1,2, ..., N\\}$ using cosine similarity. The system retrieves and ranks content based on these similarity scores.\nVideo Query: For video queries, the system first extracts frame-level embeddings from the sampled frames of the query video using the vision encoder of the vision-text model $f_{\\theta_v}$. These frame-level embeddings are then aggregated using the previously defined aggregation function $A_v$ to generate a single video embedding. This video embedding is compared directly with the vision embedding database $\\{F^v_i : i = 1,2,..., N\\}$ using cosine similarity. The content is then ranked according to similarity, with the most relevant videos appearing at the top of the results.\nAudio Query: The process for audio queries begins by segmenting the query audio into a fixed number of segments. Each segment is encoded using the audio encoder of the audio-text model $g_{\\theta_2}$. These segment-level encodings are then aggregated using the previously defined aggregation function $A_a$ to form a single audio embedding. This aggregated embedding is compared against the audio embeddings database $\\{F^a_i : i = 1, 2, ..., N\\}$ using cosine similarity. The results are then ranked based on these similarity scores."}, {"title": "B. Video Action Recognition", "content": ""}, {"title": "B.1. Simplifying Kinetics 710 classes", "content": "Reducing Kinetics 710 [14", "35": "classes to minimize inter-class confusion can be done by either discarding irrelevant classes or combining similar ones. A hierarchical approach to combining Kinetics classes was explored in [39", "https": "github.com/WACV2025Submission/ContextIQ. The signals used were:\n1. Relevance to contextual advertiser: Some classes", "stretching arm\" or \"shuffling feet\" may be too mundane, while others, like \"playing oboe\" or \"clam digging,\" are too niche for a broad audience targeting. Using GPT-4, we identified and marked about 50% of classes as irrelevant for audience targeting (highlighted in red in the attached sheet). Examples of discarded classes include \"Playing oboe\" (niche instrument with limited audience), \"Pole vault\" (niche sport), and \"Stretching leg\" (too general for segmentation).\nGPT4 Prompt": "nI have a list of 710 actions. Create a downloadable sheet with three columns", "actions": "n2. Groupings and correlated classes: Many classes in the Kinetics 710 set have high overlap", "approaches": "one extends the existing Kinetics 400 [35", "Groupings": "The K400 set [35"}]}