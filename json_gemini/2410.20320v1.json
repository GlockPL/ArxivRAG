{"title": "Few-shot Open Relation Extraction with Gaussian Prototype and Adaptive Margin", "authors": ["Tianlin Guo", "Lingling Zhang", "Jiaxin Wang", "Yunkuo Lei", "Yifei Li", "Haofen Wang", "Jun Liu"], "abstract": "Few-shot relation extraction with none-of-the-above (FsRE with NOTA) aims at predicting labels in few-shot scenarios with unknown classes. FsRE with NOTA is more challenging than the conventional few-shot relation extraction task, since the boundaries of unknown classes are complex and difficult to learn. Meta-learning based methods, especially prototype-based methods, are the mainstream solutions to this task. They obtain the classification boundary by learning the sample distribution of each class. However, their performance is limited because few-shot overfitting and NOTA boundary confusion lead to misclassification between known and unknown classes. To this end, we propose a novel framework based on Gaussian prototype and adaptive margin named GPAM for FsRE with NOTA, which includes three modules, semi-factual representation, GMM-prototype metric learning and decision boundary learning. The first two modules obtain better representations to solve the few-shot problem through debiased information enhancement and Gaussian space distance measurement. The third module learns more accurate classification boundaries and prototypes through adaptive margin and negative sampling. In the training procedure of GPAM, we use contrastive learning loss to comprehensively consider the effects of range and margin on the classification of known and unknown classes to ensure the model's stability and robustness. Sufficient experiments and ablations on the FewRel dataset show that GPAM surpasses previous prototype methods and achieves state-of-the-art performance.", "sections": [{"title": "1. Introduction", "content": "Relation extraction (RE) is an important task in the field of Natural Language Processing (NLP), which aims to identify and extract semantic relationships between entities from text based on the given list of relations. For the example in Fig.1(a), the model should be trained with a large number of examples from the given categories and extract the relation capital of when given the entity pair (Beijing, China) in the sentence Beijing is the capital of China. RE task has been extensively studied in previous work [1, 2] and existing models can already achieve good classification performance. But for RE task, there is the few-shot issue in the real world, which means that large-scale labeled datasets are difficult to obtain. In addition, sentences that express relations not included in the given set should also be taken into consideration in practical applications, and these unknown classes are called \"none of the above\" (NOTA) [3] as the example shown in Fig.1(b). One of its characteristics is that the proportion of unknown classes in each task is much larger than that of known classes. For example, for the Few-shot Relation Extraction (FsRE) with NOTA task in Fig.1(b), the number of known classes is 2, namely capital of and member of, and all the remaining classes in the relation set are unknown classes. When given the entity pair (Maya Airport, Brazzaville) in the sentence The airline's hub is Maya Airport in Brazzaville, the model should determine that the relation is neither capital of nor member of in the relation list, and output the result NOTA. To the best of our knowledge, the performance of existing models in dealing with this problem is limited. We summarize that there are two critical challenges in solving this task:"}, {"title": "1) Few-shot Overfitting.", "content": "Training a classifier for each class can easily lead to overfitting in the case of limited sample supervision. To overcome this issue, many meta-learning approaches, especially prototype-based learning [4] ones, have been proposed, which extract the features of each sample and obtain the prototype anchor of each class by averaging or other techniques, and classify the samples based on their distances to these prototypes. However, the traditional prototype-based method has two major flaws. Firstly, directly and simply extracting sample features is incomplete and biased. Relevant studies [5, 6] have demonstrated that head and tail entity information and context information can cause biases in RE model training, which means that over-reliance on entity or context information can cause the model to obtain non-existent relationships when encountering similar entities or contexts. Taking the entity biases as an"}, {"title": "2) NOTA Boundary Confusion.", "content": "There is a tendency for boundary confusion in the FsRE with NOTA scenario, where instances of the NOTA class may be misclassified as one of the known classes. Only a few conventional models specifically address the NOTA problem, which simply treat NOTA as a single class during training. For example, Liu et al. [8] leverage triplet paraphrase to pre-train low-shot relation extraction ability and matches queries and relation labels including NOTA. However, since the semantics of NOTA classes differ in various scenarios, learning their features as a whole fails to capture the correct distribution. This limitation can lead to faults, as the model may incorrectly assign NOTA instances to the closest known class prototypes, rather than recognizing them as belonging to a separate, unknown category. Meanwhile, the few-shot issue will be exacerbated in the presence of the NOTA class, as the model must distinguish between known classes and the unknown class with even fewer data points, increasing the risk of misclassification.\nTo solve this difficult subject, we propose the framework GPAM, a prototypical learning method using Gaussian Prototype and Adaptive Margin. As shown in Fig.2, our GPAM is mainly composed of three key modules, the semi-facutal representation, the GMM-prototype metric learning and the decision boundary learning module. The first module is to extract better feature representations based on debiased views. The main view and three debiased views are used to deal with the biases caused by the entity and context information shown in Fig.2(a). This can reduce the impact of bias information on model training and obtain more accurate prototype representations through augmentation. The second module is to measure the distance between samples and"}, {"title": "4. Methodology", "content": "The workflow of our model GPAM is shown in Fig.2. Our model consists of three core modules: 1) Semi-Factual Representation, three debiased views are included as semi-factual data derived from the main view to augment the few-shot datasets; 2) GMM-Prototype Metric Learning, the four views' features are fitted to a Gaussian mixture model and obtaining the distance metric; 3) Decision Boundary Learning, prototype range and adaptive margin are used to accurately distinguish various categories and get the decision boundary. We will further introduce these modules as follows."}, {"title": "4.1. Semi-Factual Representation", "content": "In previous studies [4], prototypes were learned solely from original sentence information, with model performance constrained by biases from entities and contexts. We follow the Semi-Factual Representation (SFR) strategy proposed in our previous work [23], which learns representations through multiple debiased views to mitigate this limitation. As shown in Fig.2(a), the details are as follows.\nMain View. This view marks the head and tail entity in the raw sentence $s = (s_1, s_2, ..., s_L)$ with tokens <h>, </h> and <t\u3009, \u3008/t), respectively. We denote the sentence after applying this view as $s^m = (s^m_1, s^m_2, ..., s^m_L)$.\nHead and Tail Debiased Views. These two views replace the head entity $h_i$ and the tail entity $t_i$ with their attribute features. For example, the head entity \"Tennen Mountains\" is replaced by its attribute \"[Mountain]\". The sentences in head debiased"}, {"title": "4.2. GMM-Prototype Metric Learning", "content": "The conventional prototype method only calculates the prototype anchor and radius, that is, the prototype is only regarded as a sphere with a certain radius in high-dimensional space [7]. It only considers the mean of samples, but ignores the variance information that accurately reflects the distribution. This is prone to inaccurate description, especially for samples near the decision boundary. To alleviate this problem, we assume that the relation r follows a mixed Gaussian distribution aggregated from the features of four views and propose a Gaussian Mixture Module (GMM)-based strategy detailed as follows.\nFirst, prompt templates are constructed for four views and inserted into the input embedding sequence as prefix tokens to do prompt-tuning for K shots in a meta-task,\n$\\[input^j\\] = \\[prompt^j\\]\\[z_1^j\\]\\[z_2^j\\]... \\[z_K^j\\]$  (1)\nwhere $prompt^(j = m, h, t, c)$ represents the prompt templates corresponding to various views. Different from traditional high-dimensional space distance estimation strategies, Mahalanobis distance is used instead of Eucilidean distance to better adapt the characteristics of Gaussian distribution, utilizing both the mean and variance information. Next, the mean vector $\u00b5$ and diagonal variance matrix diag(v) of the Gaussian space are calculated corresponding to the relation type r and the following formulas are used to get the u, v values of the main view and three debiased views in turn:\n$\\mu, v^j = Transformer(\\[prompt^j\\] \\[z_1^j\\]\\[z_2^j\\]... \\[z_K^j\\]; \\theta)$   (2)\nwhere $\\theta$ is a learnable parameter which is the same for all views. The four views reflect the features of relation r from different aspects, therefore, the overall feature of r can be"}, {"title": "4.3. Decision Boundary Learning", "content": "Since in few-shot scenarios, the distribution of positive samples for a certain category has a key influence on the relation, we introduce the prototype range indicator $R_c$ to show the range of each category with positive samples. However, using only a single indicator $R_c$ to judge the category may cause the problem of sample misjudgment near the decision boundary between known and NOTA classes. In order to alleviate this problem, the adaptive margin of NOTA, $M_c$, is introduced. $M_c$ is another indicator that affects the boundary between positive and negative examples, reflecting the tolerance of the learned prototype to negative examples. We utilize the distances from the positive instances to the relation anchor $r_c$ to obtain $R_c$ and the distances from the negative instances to $r_c$ to obtain $M_c$ as follows:\n$R_c = h_{t_1} (\\{d(x_i, r_c)\\}_{i=1}^{K}; x_i \\in S)$  (6)\n$M_c = h_{t_2} (\\{d(x_i, r_c) - R_c\\}_{i=1}^{(N-1)K}; x_i^{-} \\in S)$  (7)\nwhere $h(.)$ is the quantile function which follows the principle that most positive instances should be within the prototype range $R_c$ and most negative instances should be outside $R_c + M_c$, $\\tau_1$ is a learnable parameter that controls the boundary range, $\\tau_2$ is a learnable parameter that controls the tolerance for negative instances, and $x_i^+$ and $x_i^-$ represent positive and negative instances respectively.\nFor the query set $Q = Q^k \\cup Q^u$, we utilize Eq.(4) to do the same thing and obtain the distances between instances and each prototype anchor. To determine whether a instance x belongs to relation $r_c$, the classification rules are as follows:\n$\\begin{cases}r_c, & \\text{when } d(x, r_c) \\leq R_c, \\\\  \\text{when } d(x, r_c) > R_c + M_c, & \\text{the label of x is } r_c.\\end{cases}$\nBesides, if instance x meets the criteria of multiple classes, the instance with the smallest GMM distance $r_c$ will be selected, and if instance x does not belong to any of the relations in the known set, it will belong to the NOTA class. In this way we can accurately predict the labels of the instances in the query set whether they belong to a known class or NOTA class.\nIn the training procedure, we find that one of the important reasons for the poor performance of prototype learning methods when facing the NOTA problem is that the boundary of NOTA is complex and difficult to accurately describe with a small number of samples. Therefore, we refer to the method of Song [22] to expand some negative instances outside the $R_c + M_c$ region in the background regions by a certain proportion, and these examples are called pseudo negative samples (PNS), which don't belong to any of the classes and serve as negative samples of all classes. According to the research of Wang [26] et al., since negative instances close to the boundary will have a greater impact on the classification between known and unknown classes, a higher generation probability is assigned to these negative instances. For each meta-task, several points are sampled outside the range of $R_c + M_c$ in the feature vector space as pseudo negative example sets. Then, the probability of selecting a sample in the pseudo-negative example set is calculated by the ratio of its distance from the margin"}, {"title": "5.1. Settings", "content": "Datasets. We perform experiments on the public relation extraction dataset: FewRel [3]. FewRel is a benchmark dataset designed for evaluating models in relation classification tasks. It features 100 diverse relation types with annotated examples sourced from Wikipedia, and contains 700 instances for each relation. We use meta-learning methods to randomly extract samples from the FewRel dataset according to different task settings to form meta-task datasets without redundant instances. Different from conventional few-shot learning task, we add NOTA instances to the meta-task datasets according to a certain NOTA rate.\nComparing Methods. We compare our model GPAM with the following outstanding baselines. These methods can be categorized into three groups. 1) FsRE models with NOTA. Proto-BERT [4]: the original prototype network algorithm; BERT-PAIR [3]: an approach to measure the similarity of sentence pairs; MCMN [8]: an approach using triplet paraphrase and meta-learning paradigm to do low-shot RE. 2) Those without NOTA. Proto-HATT [27]: a hybrid attention-based prototypical network; MLMAN [28]: a multi-level matching and aggregation prototypical network; REGRAB [29]: a Bayesian meta-learning approach to learn the posterior distribution of the prototype and solve the uncertainty of the prototype vector; CTEG [30]: a model to decouple high co-occurrence relations; HCRP [31]: an approach to introduce relation label information and distinguish task difficulty; SimpleFSRE [16]: a direct addition approach that fuses the embedding of relation description to the prototype representation; SaCon [32]: a framework using diverse viewpoints through instance-label pairs to capture intrinsic textual semantics. 3) Large language models. GPT-40: an outstanding closed-source large language model developed by OpenAI; GLM-4: an open-source large language model developed by the Tsinghua Zhipu AI team.\nTraining Details. For GMM-prototype metric learning module, the length of prompt template is set to 100; For decision boundary learning module, the initial values of $\\lambda$, $\\tau_1$ and $\\tau_2$ are set to 0.001, 0.1 and 0.2 respectively, and the ratio of pseudo negative sampling is set to 0.2; In the loss function, the positive impact parameter $\\alpha$ is set to 1 and the negative impact parameter $\\beta$ is set to 3 to ensure that negative instances have a greater influence in the contrastive learning process. For the training process,"}, {"title": "5.2. Performance Comparison", "content": "Results on FsRE with NOTA task. Results on FewRel dataset with NOTA are shown in Table 1 and total, known and NOTA are evaluated individually. The observations are as follows:\n\u2022 Compared with previous methods, our GPAM clearly achieves state-of-the-art performance on all settings. The total accuracy of our GPAM exceeds the previous best conventional model MCMN, improving by 5.11%, 4.15%, 8.19%, and 7.13% on four tasks respectively. Benefit from the three designed modules, GPAM achieved good results on the FsRE with NOTA task.\n\u2022 GPAM's performance for NOTA class is particularly outstanding under the NOTA rate 0.5 setting. Compared with the previous best performing method MCMN, our GPAM improves the accuracy of NOTA class extraction by 8.85% ~ 10.30% at NOTA rate 0.5. The reason is that GMM-based distance metric and adaptive margin are introduced, GPAM has achieved significant advancements in the classification of both known and unknown classes.\n\u2022 As the number of shots K increases, the performance of GPAM becomes higher and more stable. When the number of shots increases from 1 to 5, the accuracy for the NOTA class increases from 84.25 to 93.25 at NOTA rate 0.15, and from 90.75 to 96.10 at NOTA rate 0.5. We can obtain better prototype of categories and more precise decision boundaries with more instances because the performance improvement is reasonable as the shot increases.\nMoreover, to compare the performance changes as the NOTA rate improves, we follow the evaluation methods of BERT-PAIR [3], and all models are trained and tested under four different NOTA rates: 0%, 15%, 30%, 50%. The experimental results are shown in Fig.3. GPAM outperforms the compared methods across all NOTA rate settings and maintains better stability. As the NOTA rate increases, the performance"}, {"title": "5.3. Ablation Study on Semi-Factual Representation", "content": "In order to verify the role of each view and study the impact of each debiased view on the main view, we perform ablation studies on three debiased views. Results are shown in Table 3. We can make the observations as follows:\n\u2022 The debiased information of all views has a benefit for all task settings. It can be seen that removing any view will cause performance to degrade to varying degrees."}, {"title": "5.4. Ablation Study on GMM-Prototype Metric Learning", "content": "To validate the effectiveness of the key strategies in the GMM-prototype metric learning module and to quantify their respective impacts, we construct four variants of GPAM as follows and evaluate their performance on the FewRel dataset shown in Table 3. The analysis of the results is as follows.\nAnalysis on Guassian Distance: the variant model that uses Euclidean distance instead of Mahalanobis distance as the metric in Eq.(5), that is, treats the prototype as a sphere. Mahalanobis distance shows a more significant improvement in the 5-shot scenario, with increases of 7.70% and 6.38%, respectively. This indicates that Mahalanobis distance based on Gaussian distribution is more advantageous than Euclidean distance for prototype construction in scenarios with multiple samples.\nAnalysis on Multi-Prompt: the variant model that utilizes the same prompt template instead of multi-prompt. We modify Eq.(2) and set the prompt templates prompt of the four views to be the same. Multi-prompt strategy has a more significant effect when the NOTA rate is higher, with improvements of 8.80% and 7.17% respectively.\nAnalysis on Mixed Weights: the variant model that uses the averaging stategy to aggregate four views instead of mixed Gaussian weights. We remove the weight computation formula Eq.(3), set the weights of the four views to be the same, and then use Eq.(4) to obtain the aggregated prototype distance. Mixed weights strategy"}, {"title": "5.5. Ablation Study on Decision Boundary Learning", "content": "As shown in Table 3, we also conduct an ablation study on decision boundary learning module, analyzing the impact of the presence or absence of key strategies as follows.\nAnalysis on Class Margin $M_c$. In order to study the impact of margin presence and dynamics on performance, we construct two variants of GPAM: 1) The variant model that utilizes only range indicator $R_c$ without margin $M_c$ to classify unknown classes. We achieve this by setting $M_c$ to zero and remove this item in the loss function Eq.(9). It can be seen that in the case of a relatively simple task 5-way-1-shot 0.15, the increase of the margin strategy is small, but in the other three complex tasks, the presence or absence of margin has a great impact, with an increase of more than 6%. This indicates that measuring the boundary solely with the prototype range is inaccurate, and the margin is crucial. 2) The variant model that utilizes fixed margin instead of adaptive margin which changes with negative instances of the prototype. We achieve this by changing $M_c$ in the margin computation formula Eq.(7) and the loss function Eq.(9) to a fixed value. Taking 5-way-1-shot 0.15 task as an example, using a fixed margin reduces the performance by 5.87%, which is even more significant than simply removing the margin. One possible reason is that when there are fewer shots, using the same $M_c$ for different categories will lead to inaccurate boundary ranges.\nAnalysis on Pseudo Negative Sampling (PNS). We construct a variant model with no pseudo negative samples in the train dataset. It can be seen that the PNS strategy shows a modest improvement of only 1.12% for the 5-way-1-shot task with the NOTA rate of 0.15, while it achieved over 3% improvement for the other three tasks. We analyze the reason for the results and infer that for the 5-way-1-shot 0.15 task, adding pseudo negative examples expands the boundary range of prototype. As a result, more NOTA classes are misclassified as known classes, even though the overall performance is improved. In order to obtain the optimal negative sampling rate, we conduct experiments and plot the performance of different negative sampling rates as shown in Figure 4. The performance is optimal in most cases when the negative sampling rate is 0.2, and the effect drops significantly when the value is 0.4 or higher."}, {"title": "5.6. Case Study", "content": "In order to intuitively demonstrate the effect of GPAM, we conduct a case study and artificially constructs a difficult and confusing 5-way-1-shot 0.5 meta tasks. For test requirements, we choose support and query set instances from five certain known classes, and select NOTA instances for query set from the remaining classes. And we choose BERT-PAIR [3] as a baseline model to compare it with the original GPAM and its three variant models: 1) GPAM#1 is the variant model that removes the semi-factual representation module; 2) GPAM#2 is the variant model that uses Euclidean distance instead of Mahalanobis distance as the metric; 3) GPAM#3 is the variant model that removes the margin strategy and only uses range for boundary division. We show some of the instances and the corresponding extraction results in a batch during the validation process in Tabel 4. Next, we analyze the results of the case study in detail."}, {"title": "6. Conclusion", "content": "In this paper, we propose a model based on Gaussian prototype and adaptive margin named GPAM to solve few-shot relation extraction with NOTA task. The three core modules of GPAM are the semi-factual representation, the GMM-prototype learning and the decision boundary learning module. Besides, in decision boundary learning, we design a pseudo negative sampling strategy for NOTA scenarios to enhance the classification performance of the model. Experimental results on the FewRel dataset demonstrate that the performance of GPAM is better than that of the comparison methods, and ablation experiments convey the effectiveness of the designed modules and optimization strategies. In the future, we hope to study the ability of LLMs on FsRE with NOTA task by combining semi-factual representation and adaptive margin."}]}