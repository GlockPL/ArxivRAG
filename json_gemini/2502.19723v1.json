{"title": "CNsum:Automatic Summarization for Chinese News Text", "authors": ["Yu Zhao", "Songping Huang", "Dongsheng Zhou", "Zhaoyun Ding", "Fei Wang", "Aixin Nian"], "abstract": "Obtaining valuable information from massive data efficiently has become our research goal in the era of Big Data. Text summarization technology has been continuously developed to meet this demand. Recent work has also shown that transformer-based pre-trained language models have achieved great success on various tasks in Natural Language Processing (NLP). Aiming at the problem of Chinese news text summary generation and the application of Transformer structure on Chinese, this paper proposes a Chinese news text summarization model (CN-sum) based on Transformer structure, and tests it on Chinese datasets such as THUCNews. The results of the conducted experiments show that CNsum achieves better ROUGE score than the baseline models, which verifies the outperformance of the model.", "sections": [{"title": "1 Introduction", "content": "In recent years, the rapid development of information technology has led to an ex-ponential increase in the amount of data, and more and more data appear on the Internet in various forms. But not all data can directly provide the information we want to obtain, which requires us to invest a lot of time and energy to understand massive text data. How to quickly filter and obtain information from massive texts has become an urgent problem to be solved. The task of the text summa-rization is to condense long documents into short summaries while preserving the important information and meaning of the documents[1,2].Generating news headlines is a typical application field of text summarization techniques.Simple news classification has been difficult to meet the needs of news reading because of misleading headlines. The application of automatic text summarization tech-nology has provided solutions, which could greatly improve the efficiency of news reading and selecting valuable information from the massive news."}, {"title": "2 Related Work", "content": "At present, the research on automatic text summarization technology in academia and industry has made great progress, and good results have also been achieved in practical applications. The research on automatic text summarization tech-nology begins with the method based on word frequency statistics, trying to find the key words and central sentences in the text through statistical technology, and then combine them to achieve text summarization [3]. H.P.Luhn[4] calcu-lated the relative importance through the statistical information obtained from word frequency and distribution, becoming the first automatic text summariza-tion system.\nAutomatic text summarization technology mainly includes two types: ex-tractive text summarization and adstractive text summarization[5]. The former mainly extracts keywords and sentences in text data through some scoring cri-teria to generate summaries. The abstractive summarization method is closer to the understanding of text summarization and it mainly generates a new abstract based on the learning of the source text. At present, most abstractive summa-rization models are based on the Seq2seq framework, which was first proposed by Sutskever et al[6]. It is mainly composed of an encoder and a decoder and it performed well in abstractive summarization tasks.\nThe application of deep learning methods, especially Transformers[7], has greatly improved the performance of automatic text summarization. And re-lated researchs and comparisons proved that the use of neural networks had better performances[5]. However, almost all pre-trained language models cur-rently are based on English, and these pre-trained language models' perfor-mances in Chinese are not as effective as their in English[8].Moreover, research based on Chinese text summaries has developed slowly in recent years. Based on this and inspired by GPT2-chinese[9], this paper studies the performance of the Transformer-based pre-trained language model in generating Chinese news headlines."}, {"title": "3 Model", "content": "We proposed CNsum in this paper and it mainly includes two stages: the first stage is to realize the encoder output based on Bert's preprocessing method of Chinese news text, that is, to encode the Chinese text and convert the Chinese news text into processable sequences; the second-stage decoder is composed of GPT-2 parts. With the help of its powerful text generation ability, it can generate Chinese news headlines. Fig. 1 shows the construction of CNsum."}, {"title": "3.1 BERT-based encoder", "content": "Language model (LM) is a basic concept in natural language processing. The language model task is also the core problem in the field of NLP. After using the language model to process the text data related to the natural language, a language representation that can be processed by a computer can be obtained, which is convenient for the processing of the text data. The calculation of prob-ability of the sequence occurrence is shown in Eq.(1).\n$p(S) = p(w_1, w_2, w_3, ..., w_n) = \\prod_{i=1}^{n} p(w_i/w_{(i-n+1)},..., w_{(i-1)})$ (1)\nThe BERT pre-training language model uses a bidirectional Transformer as an encoder for feature extraction, and based on the attention mechanism, it can better extract text information features. The two-way language representation of the system provides high-quality textual data information for downstream tasks. The Transformer coding unit mainly includes two parts, namely the self-attention mechanism and the feed-forward neural network. The self-attention mechanism can pay attention to the internal correlation of data or features, and is less dependent on external information, which is an effective capture method.\n$Attention(Q, K, V) = softmax((QK^T)/\\sqrt{d_k}))V$ (2)\nAs Eq.(2)shows, the input part of the self-attention mechanism includes Query vector (Q), Key vector (K) and Value vector (V). The Query vector and Key vector of text data are multiplied to obtain QKT, and then\u221adk is used to ensure that the obtained results meet the specifications, then input the results to the softmax layer for normalization to obtain the probability of the text data, then obtain the word vector representation in the text data. So, we use BERT's [10]"}, {"title": "3.2 GPT-2-based decoder", "content": "The GPT-2 model has outperformance on text generation. Based on the pow-erful text generation capability of GPT-2, the model proposed in this paper is composed of the decoder part of the multi-layer one-way Transformer, so the model only considers the influence of the words on the left side of the position of the word to be predicted on the predicted word when processing text data, and calculates self-attention based on these. The researchers [7] found that in Trans-former, the Query vector (Q), the Key vector (K) and the Value vector (V) were first linearly transformed, and then input to the scaled dot product Attention. After multiple calculations, the results can be stitched together to achieve better results. The effect, that is, the realization of multi-head self-attention allows the model to learn relevant information in different dimensions. So similar to BERT, it is multi-heads self-attention that the model need to calculate when processing text information to Self-attention mechanism.\n$head_i = Attention(QW_i^Q, KW_i^K, VW_i^V)$ (3)\n$MultiHead(Q, K, V) = Concat(head_1,...,head_h)W^O$ (4)\nAs Eq.(3) and Eq.(4) show, $head_i$ represents self-attention i, $W \\in R^{(d_{model} \\times d_k)}$, $W^K \\in R^{(d_{model} \\times d_k)}$, $W^V \\in R^{(d_{model} \\times d_v)}$, $W^O \\in R^{(h d_v \\times d_{model})}$.The model need to calculate 8 self-attentions, which means the number of heads is 8. For each self-attention calculation, $d_k = d_v = d_{model}/h = 64$.\n$FFN(x) = max(0, xW_1,+b_1)W_2 + b_2$ (5)\nUsing the Mask can mask the information of all the data to the right of the cur-rent calculation position when calculating the self-attention, which can enhance the model's attention to the information of the current position. Using the Layer Norm layer can stabilize the distribution of data features, thereby accelerating the convergence of model training. In the Feed Forward Neural Network layer, the RELU activation function is used. And we use Cross-Entropy Loss, It calculates the loss between news headlines and generated headlines to train the model.So We constructed the decoder part of CNsum by referencing the architecture of GPT-2[11]. And in view of resource consumption, we only use a 6-layer network based on the existing model."}, {"title": "4 Experiments", "content": "We use the Adam optimizer [26] and set the learning rate to be le-8, with a batch size of 8 and 10 epochs. Hugging Face's Transformers library [27]was used in all our experiments. In this paper, based on comparing with the running tests of the baseline models, the results are shown in Table 1. In order to verify the reliability of the model, we tested it on several different Chinese corpus, and the results are shown in Table 2. Fig 2 shows the summaries automatically generated by different models based on the same news article. It can be concluded that CNsum pays more attention to the generalization of the news content and the complete expression of the key information of the source text.It can be concluded that the model proposed in this paper has a good effect in Chinese news headline generation task."}, {"title": "4.1 Datasets", "content": "NLPCC2017: Provided by the 2017 CCF International Conference on Natu-ral Language Processing and Chinese Computing (NLPCC2017) [12],including 49500 news texts in the training set and 500 news texts in the test set. Articles obtained from news websites such as Sina and summaries of experts.It is used for model training and testing in the experiment.\nSogouCS: Compiled by Sogou Lab[13] and came from Sohu News. In this ex-periment, 500 samples are randomly selected to be used to test the performance of the model.\nLCSTS:It was organized by Hu et al[14]. and created the dataset based on news abstracts published by news media on Weibo.It has the characteristics of short length and more noise. 300 samples are randomly selected as the test dataset.\nTHUCNews:Tsinghua News (THUCNews) is compiled by the NLP Lab of Tsinghua University [15]. It is filtered and generated according to the historical data of the Sina News. It is used for model training and testing in the experiment."}, {"title": "4.2 Evaluation Metrics", "content": "ROUGE: ROUGE[16] is a summary evaluation tool based on recall statistics. It can reflect the coverage of the abstract to the news text.\n$ROUGE_N = \\frac{\\sum_{S \\in RS} \\sum_{g_n \\in S} Count_{m}(g_n)}{\\sum_{S \\in RS} \\sum_{g_n \\in S} Count (g_n)}$ (6)\nAs Eq.(6) shows, $RS$ represents the artificial standard abstract, that is, the reference abstract,$Count_m(g_n)$ represents the maximum number of the same n-grams that appear between the generated abstract and the reference abstract, and $Count(g_n)$ represents the number of n-grams in standard summary.\n$ROUGE - L = F_{LCS} = \\frac{(1+B^2) R_{LCS}P_{LCS}}{R_{LCS} + B^2P_{LCS}}$ (7)\nAs Eq.(7) shows, $R_{LCS}, P_{LCS}$ are respectively Recall and Precision.\nBLEU: Proposed by researchers at IBM in 2002[17], BLEU can calculate the similarity between the summaries computed by the model and the reference sum-maries.\nBERTScore: Tianyi Zhang et al[18]. proposed BERTScore, an evaluation met-ric for calculating the similarity. Compared with ROUGE and BLEU, this indi-cator is closer to human understanding of similarity. BERTScore uses contextual embedings to compute token similsrity."}, {"title": "4.3 Baseline Models", "content": "The experiments in this paper select 7 models as baseline models:\nLexPageRank:LexPageRank system defines sentence centrality based on graph-based prestige[19].It Applies the PageRank algorithm to the textual sen-tence relational representation and the extractive summarization.\nMEAD:MEAD system is a extractive summarization baseline system. Text summaries are selected by scoring the importance of sentences by considering their centroids, positions, common sequences, and keywords[20].\nSuBmodular:SuBmodular system treat the text summarization problem as maximizing a submodular function under a budget constraint [21].\nUniAttention:UniAttention system is the basic sequence-to-sequence model. The model takes into account the attention mechanism in the process of text summarization [22].\nNLPONE:NLPONE proposes to add an new attention mechanism on out-put sequence and uses the subword method. And it gets a significant improvement [23].\nPGN:PGN: Proposed in ACL2017, which used pointer networks and attention-based Seq2Seq models to get improvements [24].\nTKF:TFK system is a multi-attention sequence-to-sequence model that pays attention to topic keyword information [25]."}, {"title": "4.4 Results", "content": "We use the Adam optimizer [26] and set the learning rate to be le-8, with a batch size of 8 and 10 epochs. Hugging Face's Transformers library [27]was used in all our experiments. In this paper, based on comparing with the running tests of the baseline models, the results are shown in Table 1. In order to verify the reliability of the model, we tested it on several different Chinese corpus, and the results are shown in Table 2. Fig 2 shows the summaries automatically generated by different models based on the same news article. It can be concluded that CNsum pays more attention to the generalization of the news content and the complete expression of the key information of the source text.It can be concluded that the model proposed in this paper has a good effect in Chinese news headline generation task."}, {"title": "5 Conclusion", "content": "In this work, we proposed CNsum, a Seq2Seq model for abstractive text summa-rization on Chinese News texts. Experiments show that in the task of generating Chinese news headlines, CNsum generates a summary that is closer to the stan-dard summary and contains key information of the source text. According to the indicators ROUGE, BLEU and BERTScore of model effect evaluation, the model has achieved good results. The results show that the model's BERTScore scores are always greater than 0.5, which verifies the model's outperformance on headlines generations. After training on the news corpus, the model has achieved better performance than the baseline models in the task of generating news head-lines, which has certain application values. Tests on similar Chinese news corpus shows that the model has good generalization ability. The experimental results also demonstrate the potential for applying the structure of the pre-trained lan-guage model, validated on English text, to other languages. Compared with the benchmark model, it is improved by several percentage points. We will continue to pay attention to the development of Chinese text summarization techniques to further improve the accuracy and objectivity of Chinese text summarization."}]}