{"title": "Exploring Vision Language Models for Multimodal and Multilingual Stance Detection", "authors": ["Jake Vasilakes", "Carolina Scarton", "Zhixue Zhao"], "abstract": "Social media's global reach amplifies the spread of information, highlighting the need for robust Natural Language Processing tasks, like stance detection, across languages and modalities. Prior research predominantly focuses on text-only inputs, leaving multimodal scenarios, such as those involving both images and text, relatively underexplored. Meanwhile, the prevalence of multimodal posts has increased significantly in recent years. Although state-of-the-art Vision-Language Models (VLMs) show promise, their performance on multimodal and multilingual stance detection tasks remains largely unexamined. This paper evaluates state-of-the-art VLMs on a newly extended dataset covering seven languages and multimodal inputs, investigating their use of visual cues, language-specific performance, and cross-modality interactions. Our results show that VLMs generally rely more on text than images for stance detection and this trend persists across languages. Additionally, VLMs rely significantly more on text contained within the images than other visual content. Regarding multilinguality, the models studied tend to generate consistent predictions across languages whether they are explicitly multilingual or not, although there are outliers that are incongruous with macro F1, language support, and model size.", "sections": [{"title": "Introduction", "content": "The ubiquity of social media has led to an increased reliance on these platforms for news and science communication, often surpassing traditional, slower-to-publish sources such as print news media and peer-reviewed scientific journals (Matsa and Eva 2015; G\u00fcrer, Hubbard, and Bohon 2023). A side-effect of this is an increase in the speed of information spread, including rumors and fake news (Ceylan, Anderson, and Wood 2023). Understanding the spread of information on social media requires knowing the viewpoints users take with respect to claims, topics, or entities. This is the purview of stance detection, a key task within Natural Language Processing (NLP), which aims to automatically classify an author's viewpoint with respect to a specific target.\nWhile stance detection has been studied extensively on English text, efforts to extend it to modalities beyond text and languages beyond English are relatively sparse (cf. K\u00fc\u00e7\u00fck and Can (2020) sections 9.1 and 9.2). Additionally, the studies that do go beyond tend to investigate only one or the other, and in the case of other languages tend to focus on one or a few languages at a time. One potential reason is that obtaining high quality data requires significant time and resources. Still, extending stance detection systems to other modalities and languages is crucial, as rumors on social media often spread across languages (Singh, Bontcheva, and Scarton 2024), and posts often make use of images or videos to reinforce their message. For example, a user might comment on a news segment, use an image as supporting evidence for a scientific claim, or post a meme that expresses their view of a political figure.\nRecent development of Vision Language Models (VLMs), available within easy-to-use tools such as Chat-GPT2 and Huggingface\u00b3, provides powerful and accessible means to perform stance detection using both text and images. Furthermore, the Large Language Models (LLMs) that form the backbone of VLMs are often capable of processing and analyzing text in a variety of languages. However, to our knowledge, there has been no study investigating the ability of VLMs to perform stance detection using both text and image modalities, nor has there been any examination of their performance on this task across languages.\nGiven this research gap, this paper investigates the performance of state-of-the-art VLMs for stance detection at the intersection of modalities beyond text and languages beyond English. Our specific contributions are threefold.\n\u2022 Image Use in Multimodality: an analysis of the extent to which VLMs can effectively use information from images when performing stance detection.\n\u2022 Multilinguality: an investigation into the performance of VLMs on stance detection across languages.\n\u2022 Joint Multimodality and Multilinguality: an exploration of the interaction between text and images across languages in VLMs for stance detection.\nOur experiments make use of a recently developed multimodal stance detection dataset in English, which we extend to cover six additional languages (Liang et al. 2024)."}, {"title": "Background", "content": "We begin with an overview of the stance detection task, highlighting previous efforts on multimodal and multilingual approaches, including a discussion of currently available datasets. We then provide a review of the general architecture of VLMs, as well as previous research that attempts to use them for stance detection."}, {"title": "Stance Detection", "content": "Stance detection is the task of automatically determining the viewpoint, position, or sentiment of an author regarding a target. This target could be a topic (e.g., a news event or scientific theory), an entity (e.g., a political figure), or even the viewpoint of another author (e.g., another tweet expressing a viewpoint about a political figure) (K\u00fc\u00e7\u00fck and Can 2020). Generally, stance detection is formulated as a paired text classification problem where the input is a stance text and a target text (e.g., a tweet and the name of a political figure) and the output is one of Favour, Against, or Neutral.\nStance detection is fundamental to wrangling the spread of rumors online. Interest in this task has therefore grown alongside the increased popularity of social media, which has both lowered the barrier to content publishing and increased the speed at which content can be disseminated (AL-Dayel and Magdy 2021). Early efforts focused on supervised classification with SVMs (Elfardy and Diab 2016; Mohammad, Sobhani, and Kiritchenko 2017), naive Bayes (Walker et al. 2012), or neural networks (Siddiqua, Chy, and Aono 2019; Li and Caragea 2019), which led to models based on pretrained transformers (Fajcik, Smrz, and Burget 2019; Kawintiranon and Singh 2021; Khandelwal 2021). The recent development of LLMs has introduced the possibility of 0-shot stance detection, although research so far has evaluated mostly GPT models (Lan et al. 2024; Liyanage, Gokani, and Mago 2023; Suppa et al. 2024). The exception is Cruickshank and Ng (2023) who performed evaluations with both 0-shot and fine-tuned LLMs, finding that fine-tuning does not necessarily increase performance.\nResearch on multimodal and multilingual stance detection is more sparse. There are only a few studies using images alongside text (Hu et al. 2023; Wang et al. 2024a; Niu et al. 2024), including Liang et al. (2024) which introduced the dataset used in this work. Stance detection on languages besides English has generally focused on one or a few languages at a time. These include Zotova et al. (2020) (Catalan and Spanish), Vamvas and Sennrich (2020) (German, French, Italian), Alhindi et al. (2021) (Arabic), and Zheng et al. (2022) (Hindi, Arabic). To our knowledge the only exception is the COFe dataset, which contains comments from a debate platform covering 26 languages (Barriere, Jacquet, and Hemamou 2022)."}, {"title": "Vision Language Models", "content": "Vision Language Models (VLMs) are an extension of LLMS to images and videos. Generally, they are comprised of a vision model and an LLM. The vision model encodes the image input, which is then projected using a neural network into a latent space consistent with the text embeddings of the LLM. These \"image tokens\" are then concatenated with the text tokens and input to the LLM which generates text output. An in-depth review of VLMs can be found in Yin et al. (2024)."}, {"title": "Evaluation Methodology", "content": "This section describes the dataset used in our experiments, how we chose which models to evaluate, and an overview of each of our three experiments."}, {"title": "Dataset", "content": "Despite the importance of treating text and images together when performing stance detection on social media data, there is a lack of datasets that include both modalities, not to mention both modalities covering languages other than English. We therefore use the dataset introduced by Liang et al. (2024), which contains tweet-image pairs in English collected from X covering 5 news topics."}, {"title": "Preprocessing", "content": "In line with X's developer agreement\u2074, the dataset released by Liang et al. (2024) contains only post IDs. We therefore obtain the tweets and their paired images via the X developer APIS. All tweets are normalized and anonymized by replacing all URLs with the string HTTPURL and all user mentions with the string @USER. Following Liang et al. (2024) and for fair comparison with their results, we use examples with videos and GIFs by extracting the first frame as an image."}, {"title": "Translating the dataset", "content": "The dataset as released contains English-language tweets only. In order to study the performance of VLMs across languages, we produce machine translations of this dataset into 6 additional languages: German (de), Spanish (es), French (fr), Hindi (hi), Portuguese (pt), and Chinese (zh). These languages were chosen to represent a range of language families (Germanic, Romance, other Indo-European, and Sino-Tibetan) and scripts (Latin, Devanagari, Simplified Chinese). A fair comparison across languages is paramount, so we opted to translate the dataset to obtain parallel texts -as opposed to sourcing disparate datasets in other languages- in order to ensure model predictions are comparable across languages. We obtained translations using the Google Translate API. While it would be ideal to obtain human translations of the dataset to eliminate errors introduced by machine translation and remain faithful to the natural idiomatic variability that occurs across languages, this was not possible because human translation is a cumbersome and expensive task, that cannot be easily reproduced for multiple languages. Still, Google Translate has been shown in previous evaluations to perform well on these languages, and we maintain that the benefits of having parallel texts outweigh the potential pitfalls of using machine translation (Aiken et al. 2019; Taira et al. 2021). The result is 7 datasets: the original English dataset plus its translations into the 6 languages."}, {"title": "Models", "content": "We chose 4 state-of-the-art open-source VLMs to evaluate on the above datasets, detailed in Table 2. We chose these particular models as they have similar overall model size and they provide a diverse sample of language model (LM) and vision model (VM) components. Additionally, the LM components of each model are either explicitly multilingual (e.g. Llama 3.1) or have demonstrated multilingual capabilities despite being advertised as English only (e.g., Gemma2). We provide a summary of each model's multilingual capabilities later in Table 5."}, {"title": "Experiment 1: Multimodality", "content": "According to Liang et al. (2024), 46% of images contained in the dataset convey stance information. It is thus crucial to determine whether the VLMs are actually leveraging information contained in the images to predict the stance. Therefore, the goal of this set of experiments is to determine the extent to which each model uses its vision component. We conduct two experiments. The first examines the overall contribution of the text and image modalities. The second delves deeper into which portions of the images are most useful for prediction. These evaluations are conducted on the original English dataset only, since this is free from any noise introduced by the machine translations and is the primary language for each model's LM component."}, {"title": "Contribution of text and images", "content": "First, it is important to establish the general contribution of the text and image modalities to the overall performance of the VLM. We therefore compute the performance in terms of macro F1 of each model in three scenarios:\n\u2022 Tweet & Image: We input the instruction prompt containing the tweet text as well as the image. This is the default evaluation scenario.\n\u2022 Tweet only: We input the instruction prompt containing the tweet text, but replace the image with Gaussian noise. This way the model is still forced to use the vision component but obtains no useful information from the image.\n\u2022 Image only: We input the instruction prompt and the image, but leave the tweet text empty. I.e., the {tweet} variable in the instruction prompt is an empty string.\nWe then compare the Tweet/Image Only macro F1 scores to that of the Tweet & Image scenario. Statistical significance between predictions in each scenario is computed using McNemar's test, which determines whether the errors made by two systems on the same data points are equal, with a significant result indicating different error distributions (McNemar 1947). The results of this experiment will indicate the extent to which the text and image components on their own contribute to each model's overall performance."}, {"title": "The role of text in images", "content": "According to the analysis performed by the dataset authors, 24% of the images contain text that is useful for making a correct stance prediction (cf. Table 2 of Liang et al. (2024)). Applying the GATE OCR tool to the images, we further found that 67% contain text of any sort. The VLMs we evaluate have demonstrated OCR capabilities (cf. Wang et al. (2024b) appendix A), suggesting that they should be able to obtain useful information from text contained in the images. We therefore investigate whether the models' VM components are able to leverage this in-image text when making a prediction. To test this we introduce three additional experiment scenarios, which ablate certain portions of the images.\n\u2022 Text Blackout: Using the bounding boxes detected by the GATE OCR tool, we cover all text in each image with a black box. In other words, we remove the text cues from the images while retaining all other visual information.\n\u2022 Content Blackout: This scenario is the inverse of Text Blackout. Instead of covering up all text in the image, we cover up all but the text. The goal of this experiment is to isolate the text from all other visual information.\n\u2022 Image Text: We extract the plain text from the images using the OCR and craft a new prompt that provides the models with both the tweet and the image text, shown in Figure 1. Like Content Blackout, this isolates the text cues from other visual cues but also bypasses the visual encoder entirely. In this case, the VM is only ever given Gaussian noise.\nIn each scenario, we evaluate the models in the Image Only and Tweet & Image scenarios, using the blacked out image or the plain text extracted from the image. Comparing the results from each to the original, unmodified Image Only and Tweet & Image results will provide insight into how the visual component of each model is using the content of the images. Additionally, comparing the Image Text experiments to the Content Blackout experiments specifically will tell how how effectively the visual components are leveraging text contained in the image. Finally, we further explore the effect of in-image text on model predictions by estimating a logistic regression model to predict whether a VLM's prediction changes from correct to incorrect after blacking out portions of the image. The resulting regression coefficients will show whether, on average across the dataset, removing a portion of the image is correlated with a model changing a correct prediction to an incorrect one, which would suggest that content is helpful for making correct predictions."}, {"title": "Experiment 2: Multilinguality", "content": "As discussed in the introduction, information online often spreads across geographic regions and languages. Given that the LM components of many VLMs demonstrate multilingual capabilities, it is important to determine whether prediction performance is consistent across languages given input that is semantically identical. Thus, in this set of experiments we compare the performance of each model across languages, using the translated datasets to control for semantic content. We first conduct a brief evaluation regarding the extent to which each VLM supports each language. Then, we perform a comparative model evaluation that investigates the following two ways in which predictions may differ.\nPerformance across languages\nAs in Experiment 1, we compare the macro F1 score of each model across the dataset languages in the Tweet Only, Image Only, and Tweet & Image scenarios, and perform a McNemar's test between predictions in each language and the English results to determine whether there is a statistically significant difference in the prediction errors.\u2078\nAgreement between languages\nA model may have similar performance in two languages as measured by F1 score but high disagreement at an instance level. For example, two evaluations may have similar numbers of a certain class predicted (in)correctly, but the precise examples that they predict (in)correctly don't overlap. In essence, we are interested in measuring the extent to which a model agrees with itself when the tweet language changes. We measure this by computing the Cohen's kappa between the predictions in each language for each model. Because the inputs in two languages are semantically identical and their label distributions are the same, we would expect that a model \u201cfluent\u201d in each language would produce exactly the same labels for each language and produce random labels for any language it does not understand."}, {"title": "Experiment 3: The Intersection of Multimodality and Multilinguality", "content": "Experiments 1 and 2 investigate modality and language separately. Here, we investigate how the impact of input language on each VLM's reliance on the text and image modalities. To do this we conduct the following experiments.\n\u2022 Because the image input is language-independent, the amount of information it is able to provide to the model is the same for each language (this is confirmed later in Figure 4). We therefore investigate the contribution of the vision modality on top of the tweet by measuring the difference in F1 between the Tweet & Image and Tweet Only scenarios for each language. A larger positive difference suggests a greater reliance on the image input.\n\u2022 To investigate the effect of in-image text vs. other image content, we estimate a logistic regression model for inputs in each language besides English as we did in Experiment 1. We then compare the resulting regression coefficients to those for English. A difference in coefficients suggests the model is more or less reliant on that type of image content for the target language."}, {"title": "Results", "content": "We here present and discuss the results of each set of experiments in turn. We provide a high-level summary of our findings as well as key takeaways later in the Discussion section."}, {"title": "Experiment 1: Multimodality", "content": "Figure 3 shows the performance of each VLM in the Tweet Only, Image Only, and Tweet & Image scenarios on the English data. Three of the four models (InternVL2, Qwen2, and Ovis) perform significantly better in the Tweet Only and Tweet & Image scenarios than the Image Only scenario. This is intuitive, as the analysis in Liang et al. (2024) found that only 46% of images contain information relevant to the stance label. The exception is Llama-Vision, which performs the worst in the Tweet Only scenario and the worst overall among the models, despite having the greatest number of parameters at 11B. Llama-Vision does, however, have the largest vision model of those evaluated, so it may be that it is more reliant on the image modality. It is also interesting to note that for InternVL2, inclusion of the image modality hurts performance over the Tweet Only scenario, where for all other models it either improves or does not significantly change the results. InternVL2 does have the smallest vision model at 300M parameters and performs worst overall in the Image Only scenario, so it may be that its vision model is simply not powerful enough to encode useful information from the images.\nA more in-depth investigation of each model's use of each modality is given in Table 3, which shows results for the image ablation experiments. To better understand the role of text and other content in the images, we split the evaluation according to whether the image does or does not contain text according to the output of the OCR tool.\nWe first discuss results in the Image Only scenario. We note that removing any visual information (i.e., Text or Content Blackout) reduces performance. However, on those images that contain text (T columns), blacking out the text portion of the image has a greater negative effect on the F1 score than blacking out other content. For example, Qwen2's F1 drops by 0.18 when blacking out the text, but only 0.05 when blacking out other image content. This suggests that when text is present in the image, it is helpful for making a correct prediction. We also notice that using the plain image text only performs worse than Content Blackout for all models, suggesting that the visual organization of the text in the image is important, since this is the only aspect that is lost between the Image Text and Content Blackout experiments.\nTrends are less clear in the Tweet & Image scenario. Only a few results are significant and those that are occasionally go against intuition. For example, InternVL2 performs better after blacking out the image text and for Qwen2 and Ovis blacking out Text or Content does not significantly change model performance. This may suggest that these models are more reliant on the text modality when it is available. Nevertheless, these models do obtain some information from the image modality, as indicated by a significant performance decrease when using only the plain text extracted from the images (i.e., the Image Text rows). Like in the Image Only scenario, Image Text underperforms Content Blackout across models on examples that contain in-image text, suggesting that the visual organization of the text is important.\nWe can gain more insight into the role of text in the images by controlling for the proportion of the images that contain text, as indicated by the areas of the bounding boxes identified by the OCR tool. To determine the effect of text coverage on performance, we estimate a logistic regression model with text coverage as the independent variable to predict whether the VLM's predictions changed from correct to incorrect after blacking out the text or other content in the images."}, {"title": "Experiment 2: Multilinguality", "content": "We first attempt to establish the extent to which each VLM supports each language beyond what each is reported to officially support in the documentation. Towards this, we performed an evaluation of each model's performance on each language for 5 tasks: question answering, target language to English translation, English to target language translation, image description, and image question answering. We manually evaluated the output of each model on each task according to whether the output was fluent, and in the target language. The results are given in Table 5, and additional details on these experiments are given in the appendix. We see that all models support all of our target languages to some extent, with the only exception being InternVL2 which did not demonstrate any understanding of Hindi.\nThe F1 scores of each model across languages in each evaluation scenario are given in Figure 4. Bars marked with stars indicate results that are significantly different from the corresponding English results according to a McNemar's test. We can determine the consistency of each model across languages by examining the F1 discrepancy compared to English as well as the number of languages whose predictions differ significantly from English. Overall, we see that Ovis is the most consistent across languages as its F1 scores are all similar and only two languages (Hindi and Chinese) have predictions that differ significantly from English. While Qwen2 has similar F1 scores across languages, the McNemar's tests revealed that its prediction errors for five out of six languages differ significantly from English. InternVL2 and Llama-Vision also differ significantly for five languages and their F1 scores are less consistent. Again we see that Llama-Vision performs poorly despite being the largest model evaluated: in addition to being the worst performing model overall it is also one of the least consistent across languages.\nWe further investigate the consistency of each model's predictions across languages in Figure 5, which shows the Cohen's kappa between predictions in each pair of languages. First, we note that agreement seems to correlate with performance, with better performing models achieving higher kappa scores. Again we see that Ovis is the most consistent, with kappa scores \u2265 0.7 between all languages, and that Llama-Vision is the least consistent with all scores < 0.3. Hindi is the least consistent across all models, although this is somewhat expected as it is only officially supported by Llama-Vision (cf. Table 5). Additionally, there is notable disagreement between Chinese and all other languages on Qwen2. This is particularly interesting because Qwen2 was developed with a specific focus on Chinese and English, and officially supports 6 of the 7 languages investigated here (Wang et al. 2024b)."}, {"title": "Experiment 3: Joint Multimodality and Multilinguality", "content": "As a first step towards understanding interactions between modality an language across models, we reexamine the results for each language between Text Only, Image Only, and Text & Image scenarios given in Figure 4. Because the Image Only scenario is independent of the dataset language (i.e., the results are identical across languages), we can view the difference between the Tweet & Image and Tweet Only F1 on each language as a proxy for the amount of information contributed by the vision component.\nThe results of this comparison are shown in Figure 6. Examining the bars for each language, we see that all models besides InternVL2 show a positive reliance on the vision modality (with Qwen2 on Chinese being a notable exception). For InternVL2 we see that the vision modality generally harms performance, although the harm is greatest for English. For Qwen2 and Ovis there is in general a greater positive effect of the vision modality for languages other than English, while the opposite is true for Llama-Vision.\nThe dark red bar in Figure 6 is a measure of variability, computed as the absolute difference between the maximum and minimum values of the other bars for a given model. It can thus be viewed as a measure of consistency in the effect of the vision modality across languages. From these red bars, we see that Ovis is the most consistent overall in its use of the vision modality, followed by Llama-Vision and Qwen2. While the variability is greatest for InternVL2, we note that because it expresses a fairly consistent decrease in F1 from Tweet Only to Tweet & Image, it is difficult to conclude the extent to which it relies on the vision modality for each language.\nTo investigate the role of text in the images for languages other than English, we proceed as in Experiment 1, estimating logistic regression models for the other languages and using the coefficients to compute the probability of each model changing a correct prediction to an incorrect one after removing in-image text or other content. We report the differences in probabilities vs. the English results (cf. Table 4) for each model-language pair in Figure 7. These results only cover the Tweet & Image scenario since we are specifically interested in how the tweet language affects the models' use of the image modality.\nFigure 7 can be interpreted as follows. A significant positive difference indicates that the model is more likely to flip a correct prediction for the given language than it is for English, while a significant negative difference indicates this is less likely. A positive difference further suggests that the model is more reliant on what was blacked out on the target language vs. English, and vice versa for a negative difference.\nOverall, the differences are small, in the range \u00b10.015 for all models and languages, which suggests that the models are generally consistent regarding their reliance on the in-image text and other content across languages. It is notable that InternVL2 and Llama-Vision tend to have more extreme differences than Ovis and Qwen2, for example on Spanish and Hindi after blacking out the text and on Hindi after blacking out content. Of particular interest is Llama-Vision on Hindi in the Text Blackout scenario. According to the figure we see that Llama-Vision is much less likely to change a correct to an incorrect prediction in Hindi than in English, as there is a significant difference in probability of -0.017. The F1 scores support this: on English Llama-Vision drops 0.065 F1 (0.441 \u2192 0.376) after blacking out text but only drops 0.038 on Hindi (0.349 \u2192 0.311), suggesting that Llama-Vision relies more on the in-image text when the tweet is in Hindi than when it is in English.\nFigure 7 also provides additional context to the consistency results from Experiment 2. Ovis, as the most consistent model across languages in terms of F1 score, is also highly consistent with English across languages after blacking out the text, while it generally relies more on the non-text content of the images in languages other than English. On the other hand, Qwen2 is much more consistent when blacking out content and more reliant on the text in the images on languages besides English."}, {"title": "Discussion", "content": "In summary, we identified the following takeaways:\n1. The VLMs are more reliant on the text modality than the image modality for this task, with inclusion of the image modality providing only small performance increases over text alone (cf. Figure 3). The exception is Llama-Vision, which is generally more reliant on the vision modality than text.\n2. When provided with images, VLMs rely more on the text contained in the images than the other content, as suggested by the performance drops in Table 3 and differences in probabilities in Table 4. This is despite text occupying a minority portion of most images. Additionally, our experiments using plain text extracted from the images suggest that the visual organization of the text in the images is important, rather than only the text itself.\n3. The VLMs produce generally consistent predictions across languages, although with some idiosyncrasies. These include performance drops on Hindi across models (cf. Figure 4) and Qwen2's inconsistent predictions on Chinese (cf. Figure 5). Despite officially supporting English only, we found Ovis to be the most consistent across the languages tested with its predictions differing significantly from English for only 2 languages. It is also the best performing model overall, obtaining a macro F1 score of 0.614 in the Text & Image scenario.\n4. Each model exhibits a relatively consistent reliance on the text and vision modalities across languages, evidenced by similar per-language trends in F1s across the Text Only, Image Only, and Text & Image scenarios in Figure 4. When it comes to reliance on the in-image text and other content, Ovis is again highly consistent, yet we see a slight increased reliance on non-text content on languages other than English.\nThe results emphasize that the largest model is not necessarily the best performing (Llama-Vision), and just because a VLM claims to support multiple languages, does not necessarily mean that its predictions will be consistent across those languages (Llama-Vision and Qwen2-VL, especially on Chinese). Further, the high consistency exhibited by Ovis 1.6 suggests that its LLM, Gemma2, is in practice multilingual, despite claiming to support English only."}, {"title": "Limitations and Future Work", "content": "Regarding our overall evaluation methodology, we used a fixed random seed for all experiments in order to ensure reproducibility and because we did not have the computational resources to run each experiment over multiple seeds. However, we noticed anecdotally that the labels predicted by the models were somewhat variable depending on the random seed, so in the future it would be worthwhile to rerun all experiments with different seeds to determine whether this has any effect on the results. In addition, we only used one dataset since it was the only stance detection dataset with paired images and text available at the time our research began. Future work should extend this analysis to new multimodal stance datasets such as MultiClimate (Wang et al. 2024a) (released after our research had begun) and may require the creation of novel datasets in this area.\nModality Limitations\nA number of examples in the dataset used here contain videos or GIFs. We followed the dataset authors by using the first frame of these examples for our evaluation, but this certainly resulted in a loss of information. Future work ought to leverage some VLMs' (e.g., Qwen2's) ability to process different types of vision inputs in order to realize the dataset's full potential.\nThis study investigated VLMs' reliance on modalities by analysing their overall prediction distributions. Future explorations might obtain more instance-level insights by using attention distributions or feature attribution methods to 1) quantify the effect of the text and vision modalities on single examples and 2) investigate interactions (e.g., in terms of attention weights) between text and images.\nLanguage Limitations\nAs previously mentioned, using Google Translate to obtain translations of the dataset is not ideal and may have introduced errors into the dataset that we could not account for in our evaluations. Additionally, we avoided translating the dataset into low-resource languages as the quality of Google Translate on these can be poor (Aiken et al. 2019). Future work on evaluating the multilingual capabilities of these models ought to obtain human translations and ideally introduce lower-resource languages as well as additional language families/scripts. VLMs are expected to perform worse on these languages which may highlight inconsistencies in predictions that were not noticeable here. Finally, it was too resource-intensive for us to perform a full evaluation of each VLM's language support -covering a large variety of tasks and examples- so our evaluation in Table 5 was cursory."}, {"title": "Conclusion", "content": "This study explored the performance of four VLMs - InternVL2, Qwen2-VL, Ovis 1.6, and Llama-Vision- on a stance detection task in terms of their reliance on the text and image modalities as well as performance variability on languages other than English. We found that VLMs are highly reliant on text for this task, both from the text modality and text contained in the image modality. This latter finding is surprising because of those images that contain any text at all, the text occupies on average only 28% of their total area. Additionally, the models' predictions and reliance on the image modality are relatively consistent across the languages studied. Ovis, which officially only supports English, was notably the most consistent among the VLMs evaluated, and Llama-Vision, which officially supports all but one language studied, was found to be one of the least consistent."}]}