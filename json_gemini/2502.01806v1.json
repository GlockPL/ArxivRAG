{"title": "Toward Neurosymbolic Program Comprehension", "authors": ["Alejandro Velasco", "Aya Garryyeva", "David N. Palacio", "Antonio Mastropaolo", "Denys Poshyvanyk"], "abstract": "Recent advancements in Large Language Models (LLMs) have paved the way for Large Code Models (LCMs), enabling automation in complex software engineering tasks, such as code generation, software testing, and program comprehension, among others. Tools like GitHub Copilot and ChatGPT have shown substantial benefits in supporting developers across various practices. However, the ambition to scale these models to trillion-parameter sizes, exemplified by GPT-4, poses significant challenges that limit the usage of Artificial Intelligence (AI)-based systems powered by large Deep Learning (DL) models. These include rising computational demands for training and deployment and issues related to trustworthiness, bias, and interpretability. Such factors can make managing these models impractical for many organizations, while their \"black-box\" nature undermines key aspects, including transparency and accountability. In this paper, we question the prevailing assumption that increasing model parameters is always the optimal path forward, provided there is sufficient new data to learn additional patterns. In particular, we advocate for a Neurosymbolic research direction that combines the strengths of existing DL techniques (e.g., LLMs) with traditional symbolic methods-renowned for their reliability, speed, and determinism. To this end, we outline the core features and present preliminary results for our envisioned approach, aimed at establishing the first Neurosymbolic Program Comprehension (NsPC) framework to aid in identifying defective code components.", "sections": [{"title": "I. INTRODUCTION", "content": "There is no doubt that the recent rise of Large Code Models (LCMs) has revolutionized the automation of Software Engi-neering (SE) activities. To understand why, when, and how this transformation occurred, we must narrow down our analysis to two key aspects that have contributed significantly to this revolution: (i) the availability of large, text-rich datasets, which provide the foundational knowledge required for training these models, and (ii) the increasing scale of deep learning (DL) architectures, with models now boasting trillions of parameters (e.g., GPT-4 [1]). These two elements together have not only expanded the ability of models to embed and generalize vast amounts of programming knowledge but also facilitated their capacity to capture peculiar elements within the code, including intricate patterns, structures, and relationships. This duality (i.e., large corpus and models) laid down the groundwork for achieving new levels of automation in SE, that were once thought to be beyond reach.\nIn this regard, tools, functioning as \u201cartificial collaborators\", such as GitHub Copilot [2] and ChatGPT [3], have been effective in assisting and supporting developers in multiple phases of the software development lifecycle [4]\u2013[6] as well as enhancing their understanding of code [7], [8].\nWhile recent literature has presented the various and mul-tifaceted possibilities of AI methods for software engineering activities [9], the \"no free lunch\" theorem reminds us that these benefits come at a cost. In particular, as models continue to grow in complexity and scale, the computational demands for training and maintaining them have become a significant burden [10]. Also, concerns about bias, trustworthiness, and interpretability in large DL models such as LCMs, highlight a significant roadblock, preventing further advancement.\nIn this paper, we challenge the prevailing belief that scaling up models indefinitely is the path forward for every domain where AI-driven methods are deployed, including SE. To this end, Villalobos et al. [11] recently challenged the assumption that Large Language Models (LLMs) can continue to learn effectively from existing data. They noted that society is approaching a point where the amount of relevant information available for LLMs to learn from will be nearly exhausted, an event projected to occur between 2026 and 2032. In other words, we are nearing a critical threshold where the size of these models-counted in terms of parameters-could outstrip the volume of meaningful data available for processing.\nGiven this state of affairs, we ask: \"What if we take a step back now to move two steps forward later?\" In other words, we have hit the limits of improvement through sheer model scaling, making it necessary to reconsider the dominant paradigm that has fueled innovations in the past decade.\nWith this in mind, our overarching goal is to develop a new framework that harnesses the probabilistic capa-bilities of LLMs while seamlessly integrating traditional symbolic rules. This combination enhances interpretability, ensures deterministic reasoning, and overcomes the inherent limitations of purely probabilistic approaches\u2013that as seen-are increasingly plateauing.\nAs a first step towards this endeavor, we focus on vul-nerability detection, a critical task in software security that heavily depends on program comprehension. Understanding how code is structured and behaves is essential for identifying security weaknesses, as detecting vulnerabilities requires the ability to analyze and reason about code effectively. This understanding also plays a key role in related tasks such as debugging, refactoring, and secure software maintenance. To support these efforts, we propose the Neurosymbolic Program Comprehension (NsPC) paradigm, which combines LCMS with symbolic reasoning to equip developers with more pow-erful tools for identifying and addressing insecure code."}, {"title": "II. BACKGROUND", "content": "Shapley Additive exPlanations (SHAP) [13] is a technique for estimating each feature's contribution to the output $y$ of a deep learning model $f(x)$. Rooted in cooperative game theory, SHAP is based on Shapley values, introduced by Lloyd Shapley as a method for fairly distributing payouts among participants in cooperative games [14]. SHAP values correspond to the Shapley values of a conditional expectation function derived from the model, capturing feature interactions and dependencies to provide robust explanations.\nIn practice, SHAP isolates the impact of individual features ($w_i \\in x$) on the model's output while accounting for the influence of other features ($x \\setminus w_i$). It calculates the average difference in predictions when a feature is included versus excluded, offering insights into how features influence the model's decisions. SHAP is applicable across various models, including tree-based [15] and neural network models [16], en-abling researchers to identify key predictors and analyze model behavior. Its flexibility and strong theoretical foundation make SHAP invaluable for post-hoc interpretability [17], particularly in applications requiring both accuracy and interpretability, such as medical diagnostics [18], financial risk assessment [19] and software engineering tasks, as explored in this study."}, {"title": "III. METHODOLOGY", "content": "In this section, we present the Neurosymbolic Program Comprehension (NsPC) framework, which leverages SHAP values (refer to Sec. II) to interpret and guide model predic-tions. We first describe our approach to identifying patterns in SHAP values for input features. Next, we explain how these patterns are transformed into symbolic rules to improve model performance, particularly in scenarios with low prediction confidence."}, {"title": "A. Pattern Identification", "content": "Drawing inspiration from probing classifier techniques widely used in NLP [20] and SE [21], [22], our framework leverages supervised machine learning techniques to identify patterns in the SHAP values computed for specific predictions in classification tasks. Probing techniques work by examin-ing the latent representations of a model to determine the extent to which specific types of information are encoded. Specifically, a supervised model (e.g., classifier) is trained to predict properties of interest from the neural network's hidden representations [23]. In the context of our framework, we propose training classifiers to predict target classes from SHAP value distributions enabling the formulation of symbolic rules, as illustrated in Fig. 1.\nFirst, given a set of inputs $X$ that the LCM predicts as belonging to a specific class $y \\in Y$ (e.g., Secure/Insecure), we compute SHAP values ($\\Phi$) for each input $x \\in X$. The SHAP values are calculated relative to the expected predicted class: $y = E[f(X)]$. Inspired by syntax decomposition [24]\u2013[26], we apply an alignment function $\\phi(w_i) : w_i \\rightarrow \\mu\\in M$ to tag tokens $w_i \\in x$ with meaningful AST types $M$, defined by the programming language grammar. This process produces a SHAP tensor for each target class: $(i, w_i, \\Phi_i, \\mu_i)$, where $i$ is the position, $w_i$ is the token, $\\Phi_i$ is the SHAP value, and $\\mu_i$ is the associated AST type. The entire process is depicted in region \u2460 of Fig. 1.\nAfter computing the SHAP tensors for each target class in $Y$, we merge them and group the $\\Phi$ values by the AST tag associated with their corresponding tokens. We define position ranges as $[a,b]$, $0 \\leq a \\leq b \\leq max |x| : x \\in X$. For each range, we train a supervised model (e.g., logistic regression, decision tree, random forest) to identify curves that best capture the relationship between $\\Phi$ values and feature positions. Curves with an accuracy exceeding 60% and a well-defined decision boundary for the target class (i.e., intersection with the x-axis) provide evidence of patterns in specific AST type positions where SHAP values influence the model's decisions. The computed curves allow us to identify regions and position ranges where a feature's $\\Phi$ value (i.e., SHAP value corresponding to a specific AST node) consistently influences the overall prediction of the expected outputs either positively or negatively."}, {"title": "IV. CASE STUDY", "content": "To demonstrate the practical application of NsPC, we con-ducted a case study to identify SHAP value patterns in the context of insecure code detection (i.e., binary classification task) using Java code snippets. This study aimed to address the following research question:"}, {"title": "RQ1 [Symbolic rules from SHAP] To what extent SHAP values enable the definition of symbolic rules?", "content": "Selected LCM. For our analysis, we selected CodeBERT [27], fine-tuned for detecting insecure code snippets as BERT-like architectures are widely adopted in SE for clas-sification tasks [28]\u2013[31]. Specifically, we focus on a binary classification task, where the presence of insecure code in a code snippet is treated as the \"positive\" class prediction, while the absence of insecure code is the negative. The selected model, trained on the Devign [32] (CodeXGLUE-Defect Detection [33]), features a vocabulary size of 50, 265 and comprises 12 hidden layers with attention heads. The model was deployed on an Ubuntu 20.04 system with an AMD EPYC 7532 32-Core CPU, an NVIDIA A100 GPU with 40GB VRAM, and 1TB of RAM.\nEvaluation Dataset. For evaluation, we used the valida-tion split of the CodeXGLUE dataset for Defect Detection."}, {"title": "A. Results & Discussion", "content": "Table I summarizes the results of the trained logistic re-gression models for each position range and identified AST type. The trained logistic regression model surpassed the 60% accuracy threshold and exhibited a clear decision boundary for the two possible outcomes only for the AST types punc-tuation, operator, literal, type, and primitive. For instance, as illustrated in Fig. 2, if a snippet contains a literal token within positions [0 \u2013 43], there is a high probability that the snippet will be classified as insecure. Similarly, if a snippet contains an operator within positions [251 \u2013 280], there is a high probability that the snippet will be classified as secure.\nThese patterns reflect meaningful correlations arising from the underlying dataset and programming conventions. For instance, literal tokens often appear early in code snippets due to the prevalence of hardcoded values, initialization blocks, or function arguments, which are common in insecure pat-terns. Conversely, operator tokens in later positions typically belong to logical constructs or functional operations, often associated with structured and secure code. We capitalize on these patterns to present compelling evidence supporting the instantiation of the NsPC framework to identify symbolic rules for the selected LCM (i.e., fine-tuned CodeBERT).\nHowever, as the pattern identification relies on SHAP values computed specifically for this model, the evidence obtained is not sufficient to generalize these rules to other models, tasks, or datasets, highlighting the need for further research. Nevertheless, this study represents a foundational step toward introducing the first NsPC in the literature aimed at supporting program comprehension tasks, with a particular focus on vulnerability detection."}, {"title": "RQ1 [Neurosymbolic Component]: Using the proposed NsPC framework, we identified meaningful insecure-prone patterns within specific position ranges, which facilitated the definition of symbolic rules for detecting secure and insecure code snippets. The patterns reveal that tokens from certain AST types in particular positions have a significant impact on the model's predictions.", "content": null}, {"title": "V. RELATED WORK", "content": "In this section, we present an overview of studies relevant to this paper, including (i) the interpretability of models for SE and; (ii) applications of neurosymbolic AI for SE.\nInterpretability of Models for SE: Chen et al. [34] introduce CAT-probing, a method to quantitatively interpret how pre-trained models (CodePTMs) for programming lan-guages capture the structural properties of code. They highlight that the middle layers in models may significantly influence transfer of general structural knowledge, while later layers refine task-specific knowledge. Anand et al. [35] approach interpretability of code LLMs (cLLMs) via attention analy-sis and show that attention maps of cLLMs fail to encode syntactic-identifier relations. Bui et al. [36] aim to enhance the interpretability of attention-based models for code by adapting code perturbations to evaluate the meaningful code elements. Other research works proposed interpretability techniques by applying the principles of information storage [37], AST-probe [38], and syntactic structures combined with prediction confidence [25].\nNeurosymbolic AI in SE: Princis et al. [39] integrate sym-bolic reasoning techniques into LLMs to improve SQL query generation. This hybrid system leverages symbolic checks for query validation and repair during the generation process. To achieve this, the system employs partial query evaluation and early elimination of invalid queries, significantly improving runtime and accuracy. The study does not explore the inter-pretability of this hybrid system.\nArakelyan et al. [40] combine neural and symbolic meth-ods to improve the multi-step reasoning and compositional querying abilities of semantic code search (SCS) systems. The approach utilizes rule-based parsing of the natural language queries to identify matches between the parsed query com-ponents and code snippets. The rules, however, are manually created by the authors and might not generalize well for other natural and programming languages.\nJana et al. [41] present CoTran, an LLM-based neurosym-bolic system for translating code between programming lan-guages. The proposed system leverages a symbolic execution feedback to ensure functional equivalence of translated code. The code translation is available between Java and Python languages. Integration of the symbolic component improves the system's ability to maintain the original code's logic and leads to more robust and reliable translations.\nThere are also works on the applications of neurosymbolic AI techniques in program synthesis [42] [43], representation learning [44], error correction [45], semantic code repair [46], and bug fixing [47]."}, {"title": "VI. FUTURE PLANS", "content": "In this paper, we presented our framework designed to enhance the capabilities of LCMs through the definition of a deterministic layer built upon symbolic rules. By leveraging interpretability techniques such as SHAP, our approach iden-tifies patterns in model predictions, which can be formalized into symbolic rules. We believe that interpretability techniques not only provide valuable insights into model behavior but also serve as a foundation for defining rules that improve both the transparency and performance of LCMs, particularly in tasks requiring high reliability and explainability. In other words, we are challenging the canonical paradigm that has dominated software engineering automation over the past decade, where the predictive capabilities of machine learning methods, particularly deep neural networks, have streamlined various SE-related practices\nAs next steps, we aim to address two fundamental key areas to further refine and expand our framework. First, we seek to establish a more rigorous mathematical foundation for our framework to formalize its theoretical underpinnings and improve its reliability and generalizability in diverse appli-cations that extend beyond classification tasks. Second, we aim to incorporate human validation of the derived symbolic rules to ensure their correctness, interpretability, and practical relevance, thereby bridging the gap between automated rule generation and real-world applicability."}]}