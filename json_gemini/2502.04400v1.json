{"title": "Adaptive Prototype Knowledge Transfer for Federated Learning with Mixed Modalities and Heterogeneous Tasks", "authors": ["Keke Gai", "Mohan Wang", "Jing Yu", "Dongjue Wang", "Qi Wu"], "abstract": "Multimodal Federated Learning (MFL) enables multiple clients to collaboratively train models on multimodal data while ensuring clients' privacy. However, modality and task heterogeneity hinder clients from learning a unified representation, weakening local model generalization, especially in MFL with mixed modalities where only some clients have multimodal data. In this work, we propose an Adaptive prototype-based Multimodal Federated Learning (AproMFL) framework for mixed modalities and heterogeneous tasks to address the aforementioned issues. Our AproMFL transfers knowledge through adaptively-constructed prototypes without a prior public dataset. Clients adaptively select prototype construction methods in line with tasks; server converts client prototypes into unified multimodal prototypes and aggregates them to form global prototypes, avoid clients keeping unified labels. We divide the model into various modules and only aggregate mapping modules to reduce communication and computation overhead. To address aggregation issues in heterogeneity, we develop a client relationship graph-based scheme to dynamically adjust aggregation weights. Extensive experiments on representative datasets evidence effectiveness of AproMFL.", "sections": [{"title": "1 Introduction", "content": "Multimodal Federated Learning (MFL) [Feng et al., 2023; Chen et al., 2024; Li et al., 2024] has attracted increasing attention in recent years due to its technical merits in using multimodal data to collaboratively train models, which facilitates the extension of Federated Learning (FL) applications [Huang et al., 2023]. Due to the advancement of hardware and network-related technologies, participants can collect data in multiple modalities, so that the traditional unimodal FL architecture no longer meets demands of collaborative model training for multimdoal clients [Wang et al., 2025]. Thus, attempts of MFL essentially aims at addressing the limitations caused by the assumption that each client has unimodal data and modalities across all clients are identical.\nHowever, MFL still faces challenges deriving from mixed modallities and task heterogeneity. Currently, varied sensing devices may cause modality heterogeneity, even though most previous studies [Zong et al., 2021; Yan et al., 2024; Qi and Li, 2024; Li et al., 2023a] rely on a common assumption that all clients are modality-homogeneous (refer to Figure 1(a)). Two common scenarios of heterogeneous modalities are partial sample modality missing [Bao et al., 2024; Xiong et al., 2023] and mixed modalities [Peng et al., 2024; Peng et al., 2024]. Figure 1(b) exhibits a typical situation of partial sample modality missing, in which each client possesses a certain amount of aligned multimodal data, guiding the alignment of locally incomplete modality samples. Figure 1(c) exhibits a typical case of mixed modality, which shows multimodal data are aligned only in multimodal clients while unimodal clients lack access to aligned multimodal data. Aligning modality data across clients in MFL with mixed modalities requires effective modality knowledge transfer, otherwise models lacking certain modalities may become biased towards the existing local modality. In addition, MFL with partial sample modality missing typically assumes identical tasks across clients, while MFL with mixed modalities involves clients with different modalities handling different tasks, leading to difficult alignment of different samples and model drift [Yu et al., 2023].\nExisting methods basically can be grouped into three types, including the public dataset-based [Yu et al., 2023; Poudel et al., 2024], prototype-based [Le et al., 2024], and block-based MFL [Chen and Zhang, 2022]. The drawback of public dataset-based MFL is that the performance is highly dependent on the quality of public dataset, since public datasets are used as prior knowledge or medium for knowledge transfer, enabling local knowledge sharing between multimodal and unimodal clients. Prototype-based MFL uses prototypes to represent local modality information, but this method is dependent on an unpractical assumption that all clients' labels are unified. Block-based MFL tackles mixed modalities and task heterogeneity by dividing each model into modules to enable knowledge sharing through module aggregation. The challenge is that it involves all model components and causes a higher-level computational and communication overhead.\nTo address challenges above, we propose an Adaptive prototype-based MFL (AproMFL) framework for addressing issues of mixed modalities and heterogeneous tasks. Our framework uses prototypes that are adaptively constructed on local dataset to represent local modality information without the need for prior public dataset. Local model training is standardized by the aggregated global prototype and mapping module, so that alignment between client local modal representation and global modal representation is achieved. In addition, the method of the client prototype construction in AproMFL is adaptively determined by local tasks; thus, it avoids uniform labels and addresses the issue caused from unpractical assumptions in prior work. Differ from block-based methods, our scheme divides the model into separate modules, such that only mapping modules are aggregated rather than all modules during the aggregation. Both computation and communication costs are reduced from this exploration. Moreover, to mitigate degradation caused from model averaging in task-heterogeneous, we develop a client relationship graph-based adaptive scheme for model aggregations. To reduce errors between local representations and global knowledge, our framework enables clients to use global multimodal prototype transfer loss and global model knowledge transfer loss for training local mapping modules, thereby strengthening local model generalization.\nThe main contributions are summarized as follows: (1) We propose a novel MFL framework, AproMFL, which can handle complex heterogeneous scenarios, that is, it allows clients with heterogeneous modalities and tasks to participate in the FL training process independently of public datasets. AproMFL guides the alignment of local modality knowledge and global modality knowledge through global multimodal prototype knowledge transfer loss and global model knowledge transfer loss. To the best of our knowledge, this is the first work to achieve MFL with mixed modalities and heterogeneous tasks through prototypes. (2) We propose a cross-modal prototype aggregation scheme for matching demands of complex heterogeneous MFL, which allows the server to aggregate prototypes generated by different modalities and tasks. (3) We carried out experiments by implementing classification tasks and multimodal retrieval tasks on three baselines. The results indicated that AproMFL achieved superior precision and recall performance with training a model with much less training parameters, comparing to other methods."}, {"title": "2 Related Work", "content": "Data-Heterogeneous Federated Learning. FL is a distributed machine learning framework where clients train locally on private data, and the server aggregates client models to update a global model. Throughout this process, data remains local, effectively preserving client privacy. Among FL algorithms, FedAvg [McMahan et al., 2017] is one of the most representative. Some prior work has tried to address low performance due to data heterogeneity, where client data is non-independent and identically distributed (Non-IID) [Li et al., 2023b]. FedProx [Li et al., 2020] and MOON [Li et al., 2021] introduce learning objectives to adjust local model training, while methods like FedAvgM [Hsu et al., 2019], FedNova [Wang et al., 2020b], and FedMA [Wang et al., 2020a] mitigate heterogeneity's impact on model performance through aggregation. Other strategies to address data heterogeneity include meta-learning [Fallah et al., 2020], hypernetworks [Shamsian et al., 2021], multi-task learning [Lu et al., 2024], and knowledge distillation [Zhang et al., 2022]. However, existing methods generally assume that clients are unimodal. In MFL, clients may exhibit heterogeneity in modality, task, and statistics. Due to these differences in modality and task, existing FL methods for handling data heterogeneity cannot be directly applied to MFL.\nMultimodal Federated Learning. MFL extends unimodal FL by enabling multimodal clients to participate in training. Zong et al. [Zong et al., 2021] introduced a framework for federated cross-modal retrieval, allowing multiple clients to collaboratively train a cross-modal retrieval model in a structure similar to FedAvg. Li et al. [Li et al., 2023a] proposed an unsupervised cross-modal hashing approach to enhance client privacy using prototype representations of local multimodal data. While these studies provide foundations for MFL, they assume all clients possess identical multimodal capabilities. Modality heterogeneity is a critical challenge in MFL, where clients differ in modality types. Current modality heterogeneity mainly falls into two types: MFL with partial modality missing in some samples [Xiong et al., 2023; Bao et al., 2024] and MFL with mixed modalities [Peng et al., 2024; Le et al., 2024]. Existing research primarily focuses on cases with partial modality missing in some samples, while the study of mixed modalities has not been fully explored. To address the challenge of aligning modality knowledge in mixed modalities, Yu et al. [Yu et al., 2023] proposed an MFL framework that distills knowledge from clients with different modality types into a unified global model via knowledge exchange through a public dataset. Similarly, Huy et al. [Le et al., 2024] developed a multimodal joint cross-prototype learning method that enables classification training with missing client modalities. However, these approaches rely on strong assumptions, such as the availability of a public dataset or sufficient labeled data for each client."}, {"title": "3 Method", "content": "Problem Formulation\nAssume that there are $M_M$ multimodal clients, $M_I$ image clients, $M_T$ text clients, and one server $S$. Without loss of generality, we assume that multimodal clients have no labels, while unimodal clients possess labels. Each multimodal client ($i \\in [M_M]$) holds $N_M$ image-text pairs $\\{(x_i^I, x_i^T)\\}_{j=1}^{N_M}$, denoted by $D_i^M$. An image client $c_i$ ($i \\in [M_I]$) possesses $N_I$ images $\\{(x_i^I, y_i^I)\\}_{j=1}^{N_I}$, denoted by $D_i^I$. A text client $c_i$ ($i \\in [M_T]$) holds $N_T$ texts $\\{(x_i^T, y_i^T)\\}_{j=1}^{N_T}$. $y_i^I$ and $y_i^T$ represent labels for images and texts, respectively. We divide the model of each client $\\Omega_i$ into an encoder $E_*$, $* \\in \\{I, T\\}$ and a mapping module $f_*$, $* \\in \\{I, T\\}$. Multi-modal clients possess an image encoder, a text encoder, and mapping modules, while unimodal clients only have a unimodality encoder and a mapping module. The encoder extracts features from images/texts; the mapping module $f_*$ with parameters $\\theta_*$ maps features (extracted from different modalities) into a unified space. Clients participating in the classification task possess a classification module $g$ with parameters $\\omega$ to obtain the final prediction output. The objective function of AproMFL is expressed by Equation (1), that targets at minimizing the average loss of each client.\n$\\min \\{\\mathcal{R}(\\{\\Omega_i\\}_{i=1}^M) = \\frac{1}{M} \\sum_{i=1}^M \\mathcal{L}_i(\\Omega_i)\\},$ (1)\nwhere $\\mathcal{L}_i$ denotes the loss for $c_i$, $M = M_M + M_I + M_T$.\nAproMFL (refer to Figure 2) mainly consists of three components, including adaptive local prototype construction, server-side adaptive aggregation, and modality knowledge transfer. By implementing our framework, clients select prototype construction methods in terms of tasks, which enables an adaptive training for heterogeneous modalitities and tasks.\nAdaptive Local Prototype Construction\nThis component aims at facilitating knowledge enhancement between unimodal and multi-modal clients by adopting prototypes as the medium of information transfers, which made up shortcomings of existing methods in handling heterogeneous modalities and tasks among various clients. To address multiple cases of labels, e.g., variety in requiring labels or sharing labels, we adopt a label-guided local prototype construction for labeled tasks and a clustering-based local prototype construction for unlabeled tasks, such that clients with different modalities are allowed to select the construction scheme in terms of local tasks for generating varied prototypes.\nLabel-guided Local Prototype Construction. We take an image client $c_i$ as an example to explain the label-guided construction, as the training process for text clients is similar. Local image data are mapped into a unified space to obtain image embedding $e_i^I$, and the process is conceptualized by $e_i^I = f_i^I(E_i^I(x_{ij}^I), \\theta_i^I)$. Let the number of classes for client (c) is $C_i^I$. Equation (2) defines the prototype of the k-th class.\n$p_{ik}^I = \\frac{1}{|D_{ik}^I|} \\sum_{x_{ikj}^I \\in D_{ik}^I} e_{ikj}^I,$ (2)\nwhere $D_{ik}^I$ denotes the subset of samples corresponding to the k-th class in the dataset $D_i^I$. Thus, it is evident that the prototype is related to the embeddings output by the mapping module. To obtain prototypes with better representations of local data, we use the task loss $\\mathcal{L}_{task}$, global prototype knowledge transfer loss $\\mathcal{L}_{GPT}$ and global model knowledge transfer loss $\\mathcal{L}_{GMT}$ to guide learning of the client model. We utilize the task loss to guide the model in learning task-related features, since the task loss generally is attached to the current client's task, i.e., a cross-entropy loss for classification tasks and a contrastive loss for multimodal retrieval tasks. $\\mathcal{L}_{GPT}$ and $\\mathcal{L}_{GMT}$ are used to align local and global knowledge (see the modality knowledge transfer section). After multiple training rounds, the client computes local prototypes by Equation"}, {"title": "(2) and sends outputs with the mapping module that extracts local representations to the server.", "content": "Clustering-based Local Prototype Construction. We obtain local image-text prototypes by clustering to ensure modality alignment information. Unlike the label-guided construction, mutlimodal clients have two models, including a private clustering model and a task model. Specifically, the clustering model generates prototype pairs to represent local modality information rather than participating in server aggregation, while the task model is designed for local multimodal retrieval tasks and aligning with global knowledge during training. Thus, the client trains a private multimodal clustering model to obtain a personalized local prototype, guided by the private model, global prototype, and global model.\nTo obtain paired multimodal prototypes, we propose a multimodal clustering model, by which client obtains image-text embeddings pairs $(e_{ij}^I, e_{ij}^T)$ when inputting samples of multiple modalities into the mapping module and fusing modalities' embeddings, denoted by $e_{ij}^M$, where $e_{ij}^M = (e_{ij}^I + e_{ij}^T)/2$. Unlike other existing methods that typically use cluster centroids as local prototypes, we use a K-means to cluster the fused embeddings and obtain pseudolabels, in that the effectiveness of server aggregation is limited as the local prototype from modality fusion in multimodal clients differs from those in unimodal clients. We construct the set $S_{ik}^M$ for $k \\in [K]$ to retrain image-text pair information in prototypes, containing all image-text embedding pairs with the same pseudolabe; we compute the mean of image embeddings and the mean of text embeddings within each set, making the mean pair set $\\{(p_{ik}^I, p_{ik}^T)\\}_{k=1}^K$ the local prototype set for multimodal clients.\nTo enhance clustering effectiveness, i.e., retrieving better image-text embeddings, we involve a group of losses in the clustering model training, such as the task, intra-modal contrastive, and inter-modal contrastive loss. Specifically, the task loss is similar to per discussed. The intra-modal contrastive loss is used to make those embeddings with the same pseudolabel in the same modality (positive samples) becoming closer; otherwise, samples with different pseudolabels are treated as negative samples. The intra-modal contrastive loss for the i-th sample is defined by Equation (3).\n$\\mathcal{L}_{intra}^i = - \\frac{1}{|S_{ik}^M|} \\sum_{j \\in S_{ik}^M} \\log \\frac{\\exp(S(e_{ij}^{*I}, e_{ik}^{*I})/\\tau)}{\\sum_{t=1}^{N_M} \\exp(S(e_{ij}^{*I}, e_{it}^{*I})/\\tau)},$ (3)\nwhere $* \\in \\{I, T\\}$. A better clustering can be achieved as minimizing the intra-modal contrastive loss sharpens the boundary between positive and negative samples. In addition, the inter-modal contrastive loss is defined by Equation (4).\n$\\mathcal{L}_{inter}^i = - \\frac{1}{|S_{ik}^M|} \\sum_{j \\in S_{ik}^M} \\log \\frac{\\exp(S(e_{ij}^I, e_{ij}^T)/\\tau)}{\\sum_{t=1}^{N_M} \\exp(S(e_{ij}^I, e_{it}^T)/\\tau)},$ (4)\nThe inter-modal contrastive loss aligns image-text embeddings with the same pseudo-label. The overall loss function $\\mathcal{L}_M$ is defined by Equation (5) during the clustering process.\n$\\mathcal{L}_M = \\mathcal{L}_{task} + \\sum_{i=1}^{N_M}(\\mathcal{L}_{intra}^i + \\mathcal{L}_{inter}^i).$ (5)"}, {"title": "After training the clustering model, the multimodal client computes local image-text prototypes by using the final pseudolabels and sends them to the server for aggregation.", "content": "To learn embeddings with global knowledge across different modalities, we train the local task model by using $\\mathcal{L}_{task}$, $\\mathcal{L}_{GPT}$, $\\mathcal{L}_{GMT}$ and local mapping module regularization loss $\\mathcal{L}_{LMR}$. The objective of the $\\mathcal{L}_{LMR}$ is to minimize the difference between the mapping module of the task model and that of the private clustering model, expressed by Equation (6).\n$\\mathcal{L}_{LMR} = \\lambda \\sum_{* \\in \\{I, T\\}} ||\\theta_* - \\theta_*^{\\prime}||_3,$ (6)\nwhere $\\theta_*^{\\prime}$ represents the private mapping module obtained from clustering model training. $\\lambda$ is a parameter used to balance the relationship between personalization and global knowledge. $\\mathcal{L}_{LMR}$ not only accelerates the task model training but also facilitates knowledge distillation from the existing private mapping module into the task model.\nServer-side Adaptive Aggregation\nThis component addresses limitations deriving from the implementation of averaging aggregation in heterogeneous modalities and tasks. Two key aggregations are involved.\nAdaptive Heterogeneous Prototype Aggregation. We take an image client $c_i$ as a case to explain this aggregation process. The aggregation includes semantic completion and multimodal clustering. For semantic completion, the server first computes the similarity between image prototype $c_{iI}$ and image prototypes of the multimodal clients. The top-k most multimodal image prototypes are selected, denoting corresponding text prototypes as $P_i = \\{p_{ik}^T\\}_{k=1}^K$. Thus, we convert similarities into weight values, by applying a positive relationship between similarity and weight. Then, new image-text prototype pairs are formed from the obtained prototypes paired with the image prototype of $c_{iI}$, i.e., multiplying each element in $P_i$ by its corresponding weight. The same operations also apply for text clients prototypes. Server aggregates local prototypes by a multimodal clustering scheme once obtaining the prototypes. The process is similar to the construction of local prototypes by multimodal clients. Eventually, the server obtains K image-text prototype pairs.\nClient Relationship Graph-based Model Aggregation. To address issue of varied feature spaces of clients' local models, we propose this aggregation scheme that aims at mitigating performance degradation by only aggregating each client's mapping modules. Take the image client $c_{iI}$ as an example. The similarity between client's image mapping module and those of other clients is computed. These similarities are normalized into weights by summing to 1, and eventually each image mapping module is weighted accordingly, making an aggregated image mapping module for the client. Text clients follow the same aggregation process, while multimodal clients separately aggregate image and text mapping modules using their corresponding similarities.\nModality Knowledge Transfer\nTo align global modality knowledge with local modality knowledge, we utilize global prototype pairs and the global model to guide local model training. We adopt a global"}, {"title": "model knowledge transfer loss $\\mathcal{L}_{GPT}$ and a global prototype knowledge transfer loss $\\mathcal{L}_{GKT}$ during the client's local training process. For $\\mathcal{L}_{GPT}$, we denote the global prototype as $P = \\{(p_i^I, p_i^T)\\}_{i=1}^K$, where K represents the number of image-text prototype pairs in global prototype. Taking the image client as an example, for the j-th image sample, the client calculates the assignment probability of the image embedding $e_{ij}^I$ to the i-th global image prototype. The assignment probability is defined by Equation (7).", "content": "$q_{j,i}^I = \\frac{\\exp(S(e_{ij}^I, p_i^I))}{\\sum_{t=1}^K \\exp(-S(e_{ij}^I, p_t^I))},$ (7)\nwhere $S(\\cdot, \\cdot)$ denotes the cosine similarity. For K global image prototypes, we ultimately obtain K assignment probabilities, denoted as $Q_j^I = (q_{j,1}^I, ..., q_{j,c}^I)$. Similarly, we obtain the assignment probabilities for the local image embedding $e_{ij}^I$ to the K global text prototypes, denoted as $Q_j^T = (q_{j,1}^T, ..., q_{j,c}^T)$. Since the global image-text prototypes are paired, we assume that the assignment probabilities of the local image embedding to the paired image and text prototypes should be closely aligned, refer to Equation (8).\n$\\mathcal{L}_{GPT} = D_{JS}(Q_j^I||Q_j^T)$ (8)\n$= \\frac{1}{2} D_{KL}(Q_j^I||(\\frac{Q_j^I+Q_j^T}{2})) + \\frac{1}{2} D_{KL}(Q_j^T||(\\frac{Q_j^I+Q_j^T}{2})),$ (9)\nwhere $D_{JS}$ denotes the Jensen-Shannon (JS) divergence, $D_{KL}$ represents the Kullback-Leibler (KL) divergence.\nTo further reduce the deviation between local model and global model, we adopt a loss $\\mathcal{L}_{GMT}$. After receiving a global model, client uses it as a teacher model for knowledge distillation, such that the embeddings $(Emb_{ij}^I)$ output by the local mapping module are made to align with those $(Emb_{ij}^I)$ output by the global mapping module. To prevent a poorly performing global model from affecting local model training, we adopt a factor $\\upsilon$ ($\\upsilon = \\mathcal{L}_{task}/\\mathcal{L}_{task}^{\\prime}$). When a task loss of current local model ($\\mathcal{L}_{task}$) is smaller than that of the global model ($\\mathcal{L}_{task}^{\\prime}$), the factor reduces the amount of knowledge transferred from the global model to the local model, and vice versa. The loss $\\mathcal{L}_{GMT}$ is expressed by Equation (9).\n$\\mathcal{L}_{GMT} = \\upsilon D_{KL}(Emb_{ij}^I||Emb_{ij}^{I\\prime}).$ (10)\nComputations of $\\mathcal{L}_{GPT}$ and $\\mathcal{L}_{GMT}$ for image and multimodal clients are similar. The difference is that $Q_j^T$ represents"}, {"title": ""}]}