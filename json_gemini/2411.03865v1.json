{"title": "AdaSociety: An Adaptive Environment with Social Structures for Multi-Agent Decision-Making", "authors": ["Yizhe Huang", "Xingbo Wang", "Hao Liu", "Fanqi Kong", "Aoyang Qin", "Min Tang", "Xiaoxi Wang", "Song-Chun Zhu", "Mingjie Bi", "Siyuan Qi", "Xue Feng"], "abstract": "Traditional interactive environments limit agents' intelligence growth with fixed tasks. Recently, single-agent environments address this by generating new tasks based on agent actions, enhancing task diversity. We consider the decision-making problem in multi-agent settings, where tasks are further influenced by social connections, affecting rewards and information access. However, existing multi-agent environments lack a combination of adaptive physical surroundings and social connections, hindering the learning of intelligent behaviors. To address this, we introduce AdaSociety, a customizable multi-agent environment featuring expanding state and action spaces, alongside explicit and alterable social structures. As agents progress, the environment adaptively generates new tasks with social structures for agents to undertake. In AdaSociety, we develop three mini-games showcasing distinct social structures and tasks. Initial results demonstrate that specific social structures can promote both individual and collective benefits, though current reinforcement learning and LLM-based algorithms show limited effectiveness in leveraging social structures to enhance performance. Overall, AdaSociety serves as a valuable research platform for exploring intelligence in diverse physical and social settings.", "sections": [{"title": "1 Introduction", "content": "Classic learning environments [55, 41, 9, 42, 34] have agents trained in small and stationary worlds, which hinders the improvement of agents' intelligence. The learning process stagnates once the environments can no longer provide novel data for agents' explorations. Additionally, agents trained on a fixed task set may suffer from a loss of generalization ability [13]. Single-agent environments [18, 25, 61] set out to solve this problem by constructing adaptive environments that continuously generate new tasks based on agent actions, providing a multitudinous task set.\nIn multi-agent settings, however, the task set is determined by not only physical surroundings but also social connections among agents. Social connections dramatically impact agents' decision-making by shaping their reward structures and information access [20], and different social structures endow the environments with radically different research problems. For example, centralized scenarios focus on issues like credit assignment and consensus establishment [21, 44], while decentralized settings require agents to address opponent modeling issues and non-stationarity [3, 21, 29, 33]."}, {"title": "2 Environment", "content": "The key components of AdaSociety (Fig. 1) include the physical component, composed of resources, events, and agents' inventories, and the social component describing connections between agents and organizations. Agents can observe and act to modify both physical and social states."}, {"title": "2.1 Basic Components", "content": null}, {"title": "2.1.1 Physical Component", "content": "Resource and Event Resources are natural or synthetic. Natural resources scatter randomly on the map. Some natural resources are visible to everyone while others can only be seen when an agent has specific resources in its inventory. For example, only the agent possessing a hammer can observe coal. When agents with specific resources in their inventories stand on an event grid and take the 'synthesize' action, one unit of new resource is synthesized. Synthetic resources will be automatically placed into agents' inventories. These resources and events can be systematically described as a synthesis tree (see Fig. 4). Importantly, agents are unaware of this synthesis tree. They gradually learn the tree through interaction with the environment. Resources, event grids, and agents are initialized in random locations on the map for every episode. While there are existing 3D benchmark environments focusing on perception challenges, our research centers on the domain of multi-agent decision-making. To this end, the map is intentionally crafted in a 2D symbolic format.\nAgent's Inventory Every agent has an inventory with maximal capacities of every resource, implying skill diversity. For example, an agent with a 0 capacity for hammers cannot possess hammers and observe coal. Agents can collect resources from the map into their inventories and dump resources on the map. Agents' rewards are attached to the resources in their inventories, while they exhibit heterogeneity in resource preferences. Specifically, for agent i, the reward of resource p is Ri(p) = mhi(p)\u00b7\u00ba, where me is the amount of resource p in i's inventory, h\u2081(p) \u2208 R represents i's preference for p, \u00ba is the objective reward of a unit of p (see details in Sec. A.1)."}, {"title": "2.1.2 Social Component", "content": "The social component explicitly exhibits the connections between agents or organizations. These connections drastically influence multi-agent decision-making by affecting agents' accessible information and reward structures. Centralization and its complete opposite, decentralization, can be seen as two typical connection structures, presenting very different decision-making problems. AdaSociety supports adaptive connections, with corresponding interactions being modeled as general-sum games. AdaSociety considers not only the connections between agents but also the subordinate connections between agents and organizations established autonomously by agents. This makes hierarchical connections possible. Agents take social actions to change social states, like connecting or disconnecting with someone. Fig. 1 illustrates evolving connection structures, from fully independent agents to sparsely connected agents with several non-overlapping small groups, and finally to a unified large group. On the other hand, as a customized environment, AdaSociety also supports users to predefine and/or fix social connections for their specific research problems. The semantics of connections are diverse, which can be reward sharing, information sharing, or division of labor between involved agents. AdaSociety supports that agents negotiate their connection semantics (Sec. 4).\nTo maintain consistency with the physical component, we refer to these connections between agents and organizations as social states, which are expressed as a multi-layer directed graph (Sec. 3). Social states explicitly and quantitatively express relations between agents or organizations. For example, the cooperation level of two agents can be measured by the frequency of connections between them. Moreover, the combination of social states with successive tasks in AdaSociety supports the establishment of stable and long-term relations and the study of social intelligence, like coalition formation and the emergence of hierarchy."}, {"title": "2.1.3 Observation and Action", "content": "Observation Each agent navigates with a partially observable window, reaching o grids in the four cardinal directions of its current position. Agents can get their own inventory states of collected resources, but not those of co-players. The social states of all the agents are accessible to everyone.\nAction Action space consists of social actions and physical actions. Social actions aim to build and break connections with others, including other agents or organizations. Connections are directional. If agent i connects to agent j, but not vice versa, i shares its information or reward with j, but gets nothing from j. Physical actions include movement, picking and dumping specific resources, synthesizing resources on corresponding event grids, and communicating with someone. Newly synthesized resources enrich picking and dumping actions and the action space."}, {"title": "2.2 Evaluation Metrics", "content": "AdaSociety provides diverse metrics to evaluate the performances of agents and organizations including Individual reward, Fairness score, Completion rate, and Average degree and Maximum degree of the social network. Definitions and details of the metrics are discussed in Sec. A.5."}, {"title": "2.3 Environment Characteristics", "content": "There are various characteristics of AdaSociety that make it novel (see Tab. 1). AdaSociety is a multi-agent decision-making environment, which provides both mini-games for specific research problems and a customizable platform to researchers (see details in Sec. A.4). Agents dynamically connect with other agents or organizations and autonomously communicate to negotiate the semantics of connections, making the emergence of hierarchical social structure and diverse social intelligence possible. With these dynamic and non-deterministic connections, friends may become foes, and vice versa. Thus, the interactions between agents can be modeled as general-sum games, where cooperation coexists with competition. Agents navigate this playground with a partially observable window centered on their current position. The state and action spaces of AdaSociety dynamically expand, adapting to agents' (physical and social) behavior. That generates massive and diverse tasks, supporting an evaluation of agents' abilities in multiple aspects. AdaSociety is friendly to LLM- and tensor-based agents. We evaluate state-of-the-art RL methods and LLMs in Sec. 5. In addition, we want to stress that the mutual adaptation between agents and AdaSociety, which generates a variety of successive tasks and multiple possible victory paths. Achieving success in AdaSociety requires a balance between the exploration of physical components and the alteration of social connections (see Fig. 5). Agents continually learn policies to efficiently explore and achieve goals in AdaSociety. Meanwhile, agents' (physical and social) behavior will affect the dynamics of AdaSociety. Synthesizing new resources will gradually expand AdaSociety's physical state space and the corresponding physical action space, transition function, and reward function. Updated social states will reshape agents' observation and reward structures. Thus, tasks and task sequences are influenced by agents' behavior and social states, not sampled according to some predefined distribution of tasks. That is to say, AdaSociety adapts its tasks and task sequences to agents. Mutual adaptation provides exceptionally massive and diverse complex tasks. The stochasticity and non-stability of AdaSociety produce various environment dynamics. Agents need to keep learning to adapt to changing situations."}, {"title": "2.4 Research Challenges", "content": "As an adaptive multi-agent environment, AdaSociety provides a comprehensive platform that presents plenty of research challenges. The adaptive and dynamic characteristics of the physical and social components bring challenges mainly lying in the intricate and unpredictable interactions between agents. Through multi-dimensional exploration, agents learn the ability of dynamic environmental adaptation and engage in communication-enabled interactions. Meanwhile, agents may develop"}, {"title": "3 Formulation", "content": "We now provide a comprehensive definition and analysis of the Growing-MG with a social structure, which are general enough to encompass all the research challenges mentioned above. Three concrete scenarios will be instantiated in next section.\nThe predominant model in multi-agent sequential decision-making is the Markov Game (MG)[37]. However, a significant limitation of MG is the assumption of constant state and action spaces and unchanged Markovian transitions or rewards, ensuring convergence to some classical solutions such as global optimality or Nash equilibrium[4, 57]. To address dynamic state and action spaces, we introduce two new structures, Monotonic-MG-bundle and Growing-MG as below. A Growing-MG yields a multi-agent non-stationary decision-making framework. At time step t, with state st and action at, the Monotonic-MG-bundle produces St+1, At+1, Tt+1, Rt+1 = \u03b2(st, at), forming one new MG instance. This framework differs from time-varying games[10, 64, 5], which only model payoff matrix dependent on past actions. On the other hand, both the transition probability and reward function in Growing-MG will evolve triggered with some certain transitions. For simplicity, we denote all possible transition and reward functions on arbitrary state and action space S, A, as T(S, A) = {T|T : S \u00d7 A \u2192 S} and R(S, A) = {R|R : S \u00d7 A \u2192 R} and the largest possible spaces supported by the environment as universal state space Sw and action space Aw."}, {"title": "Definition 1.", "content": "A base-MG is a tuple MG\u2081 = (I, Sb, Ab, Tb, Rb, \u03c1, \u03b3), where I = {1, ..., I} is a set of agents; S\u2081 = {St,..., St} and A\u2081 = {A},..., A{} is the state space and action space of all agents; To : Sb \u00d7 A\u266d \u00d7 S\u266d \u2192 [0, 1] and R\u266d : S\u266d \u00d7 A\u266d \u2192 R\u00b9 is the transition and reward function; p: St\u2192 [0, 1] is the initial state distribution and y is the temporal discount factor."}, {"title": "Definition 2.", "content": "A Monotonic-MG-bundle upon a base-MG MG\u266d within the universal state and action space Sw = {S,...,S}, \u0391w = {\u0391., \u0391} is a map \u03b2: St \u00d7 At \u2192 {St+1, At+1, Tt+1, Rt+1 SSS+1} where St+1 Aw, Tt+1 \u0395 T(St+1, At+1), Rt+1 \u2208 R(St+1, At+1)."}, {"title": "Definition 3.", "content": "A Growing-MG upon a base-MG MG\u266d within the universal state and action space Sw, Aw is a tuple MG9 = (MG\u0463, \u03b2)."}, {"title": "4 Mini-games", "content": "To provide a comprehensive benchmark and illustrate the characteristics of AdaSociety, we propose a set of mini-games (Fig. 2). The three mini-games are arranged in ascending order of the complexity of decision-making. Social structure, prescribing agents' partners and connection semantics, evaluates agents' ability to adapt to the changeable social structure. Contract predefines connection semantics, where agents need to select partners while learning coordination with various co-players."}, {"title": "In Negotiation,", "content": "agents independently select partners, determine the reward distribution plan with their partners, and behave under the negotiated relationship. All of the three mini-games share the same physical component (Sec. 5.1), which contains a part of the synthesis tree. The following text provides a detailed description of the social components of Social structure, Contract, Negotiation. To show the full complexity of our physical components, another mini-game Exploration, which contains all built-in resources and events, is introduced in Sec. C.2."}, {"title": "Social Structure.", "content": "The explicit represen-tation of social structure allows dynamic changes as agents interact with the environment. Pre-defined rules for structure change could be designed to compel agents to alter their social relationships while interacting with the environment. We implement structure change at certain steps: when step t reaches T1, T2, ..., the social structures are modified to G1, G2, ..., respectively. Different categories of social structures are stated in Sec. C.1. This forces agents to learn policies to adapt to the changing social environment."}, {"title": "Contract.", "content": "The environment is divided into two stages: the contract formation stage for determining social connections and the physical interaction stage to interact with the physical component and co-players with determined social connections. The contract formation stage lasts for cN time steps, where c is a positive integer and N is the number of agents, while the physical interaction stage has a duration of T. Therefore, the total duration of each episode is cN + T. Before the contract formation stage (0 < t < cN), an order (i1, i2, ..., iv) is randomly sampled. At time t, agent ik, where k =t mod N, takes social action, selecting a group node vg \u2208 Vg to connect. An agent can connect with only one group node. Agents within the same group are considered to have formed a contract to share rewards. In the physical interaction stage (t > cN), all agents act synchronously within the physical component, and the rewards received are equally divided among the agents within the same group."}, {"title": "Negotiation.", "content": "The game has a negotiation stage followed by a physical stage. In negotiation, agents seek cooperation by selecting an opponent and sending him a request. After mutual requests, agents bargain by exchanging proposals until agreement or breakup. In the bargaining session, agents i and j take turns to perform one of the three actions: (i) PROPOSE a new scheme (wi, wj) s.t. wi + wj = 1, where wi and wj represent the partition of rewards obtained by i and j respectively in the physical stage. (ii) ACCEPT the proposal from one's opponent and form a new group (coalition). (iii) DECLINE the proposal and end this session without any commitment. Once a new group is formed, the cooperative relationship between i and j represented by edge Eij with a payoff distribution (wi, wj) is established. Later, when i or j seeks to negotiate with others, it represents the group {i, j}. For example, if i and an out-group agent k reach a new distribution plan (wnew, whew), then k is regarded as joining {i, j} to form a new group {i, j, k} with an updated distribution (w. wnew, w wnew, wnew)."}, {"title": "5 Experiments", "content": null}, {"title": "5.1 Environment Setup", "content": "We have designed two physical task settings, featuring different levels of difficulty, for Social Structure, Contract, and Negotiation. The parameters of these tasks are provided in Sec. C.3.\nIn the Easy task, the environment involves a single event HammerCraft. Within this task, agents are categorized into two types based on their inventory capacity and value preference: carpenters and miners. Carpenters have the ability to gather wood and stone, which they can then use to produce hammers through the HammerCraft event. However, their inventory is limited to holding only one hammer at a time. On the other hand, miners are unable to collect stone, making them incapable of producing hammers. However, miners possess the advantage of being able to store a considerable number of hammers in their inventory. Additionally, hammers held by miners are assigned a higher value compared to those held by carpenters."}, {"title": "5.2 Baseline Methods", "content": "We use several deep reinforcement learning algorithms as baselines. Proximal Policy Optimization (PPO) [49] strikes a balance between sample efficiency and policy stability by constraining policy updates using a trust region approach and a clipped surrogate objective. RecurrentPPO(RecPPO) uses PPO for training and add LSTM [28] to maintain memories in the network. Rainbow [27] is a value-based method that incorporates several key enhancements into the Deep Q-learning framework. MAPPO is the multi-agent version of PPO. It learns a critic that takes the global state and other agents' actions as inputs during training. We employ a convolutional neural network for encoding grid information and a graph convolutional network [32] for encoding social state in all RL methods. The open-source library RLLib [36] is used for RL training.\nAdditionally, we design a curriculum learning (CL) algorithm. It starts with shared rewards to enhance cooperation strategies, then gradually increases social state randomness for learning under different social structures, and finally allows agents to perform social actions to establish their own social state. RecPPO is used for RL training at each stage. We also present a Large Language Model + controller (LLM-C) framework based on GPT-4 [1], which converts environmental information into prompts to query an LLM for high-level plans and then calls a rule-based controller to execute actions based on the generated plan. LLM has been shown to be effective in some single-agent environments, such as MineCraft [59, 56, 58, 67, 60]. The details of the last two algorithms are given in Appendix D."}, {"title": "5.3 Results", "content": null}, {"title": "5.3.1 Social Structure", "content": "In the Social Structure mini-game, various static and dynamic social structures are tested to evaluate baseline algorithms. Detailed results are presented in Appendix E. Here, we discuss the result of one Dynamic scenario, where the social structure starts with Inequality, then switches to Independent (Ind.) group at step 30, and alters to Overlapping (Ovlp.) group at step 60.\nFig. 3a presents the reward accumulation as agents take actions with three static-structure scenarios and one dynamic-structure scenario, respectively. The results verify the influence of dynamic change in social structure on agent performance since the Dynamic curve resembles the Inequality scenario initially but then it drops in later steps and approaches Ovlp. group scenario.\nFig. 3b and Fig. 3c illustrate the performance of various learning methods. Some traditional methods, such as PPO, RecPPO, and MAPPO, exhibit similar performance, with MAPPO performing worse due"}, {"title": "5.3.2 Contract", "content": "As depicted in Tab. 2, Contract presents a challenge for popular RL methods, as they are stuck in a local equilibrium of completing limited HammerCraft on both tasks (see Fig. 7b), while CL demonstrates notable performance on the Easy tasks and surpasses general RL methods on the Hard tasks. The first curriculum in CL equips the agent with the ability to learn effective policies in the physical realm, and the second curriculum empowers the agent to make informed judgments about different social structures while considering rational physical policies. Ultimately, this knowledge aids CL in selecting an appropriate contract. However, it appears that CL may forget the strategies acquired during the first curriculum, as the reward at the end of the second stage has dropped significantly compared to the end of the first stage (see Tab. 12 for details). This might hamper the performance of CL on the Hard task.\nSharing rewards has been recognized as an effective method for agent groups to acquire cooperative strategies, thereby supporting the feasibility of CL's approach. Fig. 7c and Fig. 7d also illustrates that. In the case of the Easy task, CL eventually establishes a stable group of three individuals who actively share rewards and form a cooperative alliance. However, it is important to note that the size of the group does not directly correlate with high returns. Rainbow, for instance, frequently forms large groups in both tasks but fails to achieve substantial returns. This outcome primarily stems from inherent limitations in the algorithm's learning capabilities."}, {"title": "5.3.3 Negotiation", "content": "Traditional RL methods struggle to enable carpenters and miners to learn to cooperate through negotiation, dumping some tools to increase the benefit of teammates with larger capacities on the physical stage as shown in Tab. 2. This challenge arises from the complexity of coupling the negotiation and physical stages. Once negotiation fails, dumping tools in the subsequent physical stage would substantially reduce the agents' rewards. Meanwhile, the complex negotiation process exacerbates the convergence problem in multi-agent settings, and agents have the incentive to claim a larger share for themselves to exploit the co-players in bargaining, posing challenges to reaching a consensus agreement. Consequently, in both easy and hard tasks, the average and maximum degrees are low, with most agents opting to complete tasks independently, leading to low completion rates in HammerCraft and even a complete failure in TorchCraft (Fig. 8). In the easy task, miners' rewards heavily rely on carpenters' cooperation, which severely compromises fairness. In contrast, by first learning the optimal strategies in physical environments under different social structures, CL can identify structures with higher cooperation degrees as more beneficial, facilitating consensus during negotiation learning and achieving higher group rewards, fairness, and successful TorchCraft.\nAdditionally, we show the Carpenters/Miners (abbreviated as C/M) split ratio when the negotiation stage is done, which is computed by $\\sum_{i \\in {Carpenters}} W_i / \\sum_{j \\in {Miners}} W_j$. All results exceed 1, aligning with the intuition that miners are disadvantaged in negotiations as they cannot independently produce the more rewarding hammers."}, {"title": "5.3.4 LLM-C in AdaSociety", "content": "LLM-C runs three times for each task. Tab. 3 presents the quantitative results across various metrics. Benefiting from the embedded commonsense reasoning and social intelligence of LLMS, LLM-C exhibits outstanding performance in all three mini-games, achieving average rewards nearly surpassing all RL-based methods. After being informed of the game rules and the capability differences between carpenters and miners, LLM-C can accurately recognize the importance of cooperation and swiftly form alliances with other players through negotiation or contract. During the physical stage, manu-ally coded controllers complement LLM's deficiencies in path planning and position judgment, precisely and efficiently realizing the high-level planning generated by the LLM based on the current social structure and physical environment. However, due to common issues with LLMS such as hallucinations, context length limitations, and randomness in outputs, LLM-C does not achieve Oracle performance, and it underperforms compared to CL in Contract-Easy, further validating the effectiveness of our proposed CL approach."}, {"title": "6 Related Works", "content": "Environments. Several craft-based environments like Malmo [31], Crafter [25], Minedojo [18] and Conan [61] create dynamic state and action spaces that expand with the agent's exploration, which, however, mainly focuses on single-agent setting. Environments including MAgent [65], XLand [54], and Miniworld [12] provide a set of different and transferrable tasks that build from basic elements, and they are open for customization. Melting Pot [2] contains a set of over 50 MARL learning substrates with limited customizability. Interactive games including AI Economist [66], Overcooked [11], MPE [42], Neural MMO [53], and SMAC [48] place agents in diverse systems allowing them to compete or cooperate. Other examples like Diplomacy [6] focus on communication between agents. None of these environments contain both dynamic social connections and adaptive tasks like AdaSociety.\nUnsupervised Environment Design (UED). In the paradigm of UED [16, 40, 30], the environment learns a policy \u0393: \u03a0 \u2192 \u0394(\u0398), which is a function from agent policy II to the environment's parameters \u04e8. Such a policy will automatically produce a distribution over solvable environments and further support the continued learning of the agent's policy. AdaSociety does not implement UED to produce diverse tasks. Unlike UED, AdaSociety has no goals or objectives, like most ecological systems, and produces multiple tasks through adaptive social structures and expanding physical surroundings.\nStructured multi-agent systems. In multi-agent systems, various connections may be formed between agents, and these connections may form certain structures. [17], [51] and [45] focus on finding communication topology for multi-agent coordination. Some research models the locality of interaction and learns a joint value via coordination graphs [24, 8, 35]. Networked MARL [63, 46, 47, 62, 50] learns localized policies on environments where agent interactions are contingent upon their connections within a static graph. We focus on dynamic agent connections which shape agents' rewards and observations, and these connections are modeled as a multi-layer graph."}, {"title": "7 Conclusion", "content": "We introduce AdaSociety, a multi-agent environment featuring expanding physical surroundings and adaptive social connections. The environment is capable of generating multiple tasks in adaptation to agents' behavior. AdaSociety is friendly to tensor-based and LLM-based methods. AdaSociety provides interfaces supporting superb customization and also offers a set of mini-games with diverse social connections. We test several RL and LLM-based algorithms in mini-games. Preliminary results indicate that AdaSociety maintains a rational complexity level for current decision-making methods."}, {"title": "F Broader Impact", "content": "To contribute to the development of multi-agent decision-making algorithms, we propose AdaSociety, a customizable environment with massive and diverse tasks generated by expanding state and action spaces and adaptive social structures. Due to the complexity of tasks and the heterogeneity of agents' capacities and preferences, agents need to team up and even cooperatively establish hierarchical social structures to achieve goals. However, agents may also learn some strategies that are harmful to their co-players, as is common in multi-agent research. We have made significant efforts to mitigate such behaviors through thoughtful design within the environment. Given the heterogeneity among agents and adaptive social structures, harmful behaviors tend to be short-sighted and inferior when it comes to maximizing long-term benefits, with stable cooperation emerging as the optimal strategy.\nThe multiple evaluation metrics introduced in AdaSociety, like fairness, also empower researchers to identify and exclude extreme or exploitative agents and facilitate the learning of cooperative behaviors.\nNevertheless, some harmful behaviors may still arise during training. We ask researchers utilizing our platform to meticulously observe agents' behaviors to ensure they align with human values and preferences. Should any misalignment or misrepresentation happen, we encourage contributions to the source code (including but not limited to new evaluation metrics, environmental dynamics or incentive mechanisms) to enhance the platform."}, {"title": "A Environment Elements", "content": "In this section, we elaborate the environment elements predefined in AdaSociety including resources, events, and their dependency."}, {"title": "A.1 Resources", "content": "There are 15 kinds of resources in AdaSociety, which can be divided into Natural Resources and Synthesized Resources based on whether they can be produced through events. Some of the nat-ural resources can only be discovered and gathered by agents with certain resources (denoted by Requirements) in their inventories. The details of resources are listed in Tab. 4."}, {"title": "A.2 Events", "content": "There are 9 built-in events in AdaSociety as listed in Tab. 5. Each event takes 2 to 3 kinds of resources as input and outputs 1 kind of product. Events can only be observed and executed by agents whose inventories meet the event requirements."}, {"title": "A.3 Synthesis tree", "content": "An illustration of the synthetic tree is shown in Fig. 4, which is used by all the mini-games offered by this paper. In Fig. 4, natural and synthetic resources are depicted within a green circle and blue octagon icons respectively. The solid red arrow line attached by a square event icon links low-level resources to high-level products. The eye icons indicate that some resources can help their owner discover new resources or events."}, {"title": "A.4 Customization", "content": "AdaSociety is a versatile multi-agent environment platform that supports extensive customization of various elements, features, and hyper-parameters. Researchers can easily create tailored environments for different objectives without needing to delve into the underlying code."}, {"title": "A.5 Evaluation Metrics", "content": "Individual reward is calculated as:\nR = \\sum_{p \\in P} R_i(p),\nrepresenting agent i's subjective reward of all types of resources \u03c1."}, {"title": "Fairness score", "content": "is computed based on Gini index [19] to assesses the group-wise fairness:\nF=1-\\frac{\\sum_{i=1}^{N} \\sum_{j=1}^{N}|R_i - R_j|}{2N^2 \\sum_{i=1}^{N} R_i},\nwhere N is the number of agents. Intuitively, the greater the value of F a group gets, the fairer it is.\nIndividual reward is one of the most common metrics for decision-making problems. It measures agents' decision-making abilities in maximizing self-interest. However, relying solely on individual rewards can be risky. In general-sum games, agents focus on maximizing their own rewards may engage in shortsighted and exploitative behaviors that harm their own long-term rewards and the collective benefit. For example, in Prisoner's Dilemma, self-interested agents always fall into the inefficient Nash equilibrium of defection, which minimizes one's own reward and the collective benefit. To tackle this issue, we introduce the fairness score calculated using the Gini index, which evaluates fairness within a group. In real societies, fairness is a crucial component of social justice, significantly influencing the stability of social structures and the maintenance of long-term cooperation. This metric serves as a reference for selecting agents and algorithms that balance efficiency and fairness, rather than merely pursuing individual gains."}, {"title": "Completion rate", "content": "pertains to the ratio of successful executions of an event to its maximum potential executions. It is computed separately for each event. The completion rate is introduced to measure agents' exploration within the synthesis tree. It is calculated as the ratio of actual executions to the optimal executions of the oracle policy (computation of the oracle policy can be found in Supplementary). The higher the dimension of the completion rate, the deeper the exploration. Exploration is crucial in RL. The introduction of completion rate will guide decision-making algorithms to avoid local optima, actively explore the environment, and find the optimal policy effectively."}, {"title": "Average degree", "content": "of node type \u0393\u2208 {agent, group} is calculated as:\nDr = \\frac{1}{|Nr|} \\sum_{n \\in Nr} D_n,\nwhere Nr is the set of I nodes and Dn is the degree of node n."}, {"title": "Maximum degree", "content": "reflects the maximum degree of a certain type of node, defined as:\nD^{max} = \\max_{n \\in Nr} D_n.\nIn asymmetric cases (where not all edges are bidirectional), the maximum degree and the average degree mentioned above are calculated separately for in-degrees and out-degrees. Social structure is the distinctive feature of AdaSociety. Degree-based metrics, including average degree and maximum degree, are proposed to describe and measure the topology of social structure, which significantly influences agents' policies and performances by shaping their information streams and reward functions. Agents' degree distribution is generally correlated with their rewards. For example, an agent with a high degree can obtain more information or participate in more reward distribution, thereby gaining higher returns. Combining degree-based metrics with other metrics, like individual reward and fairness, we can recognize the effective social structure for scenarios, guiding the learning of algorithms."}, {"title": "A.6 Supplementary figures for environment description and formulation", "content": null}, {"title": "A.6.1 Mutual adaption between agents and AdaSociety", "content": "Based on complex network theory, we say AdaSociety is an adaptive environment. In complex network theory, a network is called an adaptive network, if there is a feedback loop between the attributes or behavior of nodes and the topology of the network [22, 43, 7]. In AdaSociety, agents build or break connections with others and impact social structure. Conversely, social structure influences agents' observations and reward structures and further influences their attributes and behavior. Thus, following the definition of adaptive networks, the social structure of AdaSociety is"}, {"title": "A.6.2 A multi-layer directed graph expression for social structure", "content": "As shown in Fig. 6, AdaSociety expresses social states as a multi-layer directed graph. Each layer shows a level of social organization. AdaSociety supports the description of social structures with arbitrary levels, depending on the research problems and the required granularity of social structures. The bottom Oth-level consists of individual agents, who are the fundamental units of decision-making. Nodes in each layer represent entities/agents in the corresponding level. Any agent on the kth-level (k\u2265 1) is composed of its connected agents on the (k-1)th-level. Its decision-making relies on group norms, like voting, consensus decision-making and delegation. A kth-level agent will affect its (k-1)th-level neighbors' reward functions and observations, thereby influencing their decision-making and enabling their division of labour and cooperation. One agent on the (k-1)th-level may be simultaneously subordinate to any number of agents on the kth-level. For example, an individual employee is the 0th-level agent, a project team composed of several employees is the 1st-level agent, a company consisting of many teams is the 2nd-level agent, and a business group composed of many companies is the 3rd-level agent.\nAdaSociety supports the emergence of high-level social organizations. Edges inside one layer represent cooperative connections, which share information or rewards between involved entities. Edges across layers represent subordinate connections, with low-level entities complying with the policy implemented by the high-level entities. Modeling social states as a multi-layer graph will facilitate the application of existing graph theory knowledge to our research."}, {"title": "B Research Challenges", "content": "Exploration Agents start with a few resources and events within a simple environment initially. As the agents explore the synthesis tree, their behaviors trigger the mechanisms to depict changes in the physical environment. During this process, more resources and events"}]}