{"title": "AlphaAgent: LLM-Driven Alpha Mining with Regularized Exploration to Counteract Alpha Decay", "authors": ["Ziyi Tang", "Zechuan Chen", "Jiarui Yang", "Jiayao Mai", "Yongsen Zheng", "Keze Wang", "Jinrui Chen", "Liang Lin"], "abstract": "Alpha mining, a critical component in quantitative investment, focuses on discovering predictive signals for future asset returns in increasingly complex financial markets. However, the pervasive issue of alpha decay-where factors lose their predictive power over time-poses a significant challenge for alpha mining. Traditional methods like genetic programming face rapid alpha decay from overfitting and complexity, while approaches driven by Large Language Models (LLMs), despite their promise, often rely too heavily on existing knowledge, creating homogeneous factors that worsen crowding and accelerate decay. To address this challenge, we propose AlphaAgent, an autonomous framework that effectively integrates LLM agents with ad hoc regularizations for mining decay-resistant alpha factors. AlphaAgent employs three key mechanisms: (i) originality enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alphas, (ii) hypothesis-factor alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (iii) complexity control via AST-based structural constraints, preventing over-engineered constructions that are prone to overfitting. These mechanisms collectively guide the alpha generation process to balance originality, financial rationale, and adaptability to evolving market conditions, mitigating the risk of alpha decay. Extensive evaluations show that AlphaAgent outperforms traditional and LLM-based methods in mitigating alpha decay across bull and bear markets, consistently delivering significant alpha in Chinese CSI 500 and U.S. S&P 500 markets over the past four years. Notably, AlphaAgent showcases remarkable resistance to alpha decay, elevating the potential for yielding powerful factors.", "sections": [{"title": "1 Introduction", "content": "Factor investing is a key strategy in modern quantitative finance, focusing on the systematic identification and exploitation of alpha factors (a.k.a., alphas) - quantifiable characteristics that can predict asset returns. However, alpha decay, or the decline of factor returns over time, arises from two main challenges. First, overfitting through excessive data mining (\"p-hacking\") leads to the emergence of spurious factors that appear significant in backtests but decay rapidly in real-world applications [10]. Second, factor crowding occurs when too many investors adopt similar strategies, which can accelerate alpha decay and trigger sudden reversals during market stress [2, 5, 9]. This was evident in early 2024 with the size factor's underperformance in China's A-share market [6, 34], highlighting the risks of concentrated positioning in popular factors. Thus, it is rather crucial to counteract alpha decay during alpha mining.\nTraditional methods for alpha mining mainly build on genetic programming (GP) [7, 18, 21, 22, 37, 40] and reinforcement learning (RL). However, they struggle to effectively address the alpha decay challenge. Traditional GP and RL approaches tend to over-emphasize the optimization of historical performance metrics while neglecting the underlying financial and economic rationale. Without sufficient consideration of financial soundness and economic intuition, these methods often produce alpha factors that show strong historical performance but experience rapid alpha decay"}, {"title": "2 Related Work", "content": "Alpha factors, or mathematical expressions designed to predict future asset returns, have been a central focus in quantitative finance since Fama and French's pioneering work on their three-factor model [10]. Traditional methods for alpha mining primarily rely on genetic programming (GP) for exploring the vast search space of factor formulation [18, 21, 22, 40], yet they struggle to generate factors resistant to alpha decay. AlphaEvolve [7] enriches traditional GP by incorporating parameter learning and matrix operations while maintaining GP's explicit formula structure. [24] strengthens GP with sparsity constraints during mutation, which guides the search toward alpha factors with lower complexity. Another branch of traditional alpha factor mining relies on Reinforcement learning (RL) to optimize factor formulation through policy learning [19, 25, 37]. AlphaGen [37] mines formulaic alphas with deep reinforcement learning, using combination model performance as a reward signal to guide exploration within the alpha factor search space. Shi et al. [25] propose an RL-based framework that simultaneously discovers and combines multiple alpha factors with fixed weights to form a unified signal. Building upon PPO [23], an RL-based approach [39] is implemented that discards the critic network and introduces a reward-shaping mechanism aiming to generate more profitable and stable alphas for quantitative investment. However, these traditional paradigms face significant challenges in addressing alpha"}, {"title": "3 AlphaAgent", "content": "To address these issues, we propose a novel paradigm that effectively constrains LLM-based factor generation to mitigate alpha decay, addressing key limitations of conventional approaches. At its core, AlphaAgent introduces three critical regularization mechanisms to guide LLM-based factor generation: (1) complexity control through symbolic expression trees and parameter counting, (2) hypothesis alignment via LLM-evaluated semantic consistency between market hypotheses and generated factors, and (3) novelty enforcement through a similarity measure based on abstract syntax trees (ASTs) against existing alpha libraries (e.g., Alpha101).\nAlphaAgent formalizes factor construction through an operator library and abstract syntax trees (ASTs), implementing a pairwise subtree isomorphism detection mechanism to quantify factor originality while using LLMs to verify financial intuition alignment through consistency scoring between hypotheses, descriptions, and expressions. These constraints guide LLMs to explore novel market inefficiencies while maintaining theoretical soundness, alleviating the alpha decay challenge. Based on these constraints, AlphaAgent builds an autonomous workflow encompassing hypothesis proposal, factor construction, factor development, backtesting, and feedback mechanisms. The framework begins with the idea agent for the hypothesis proposal, with the first market insight provided by domain experts. Grounded in these hypotheses, the factor agent constructs parsimonious and original factors to explore unexploited market inefficiencies, applying regularization mechanisms to balance complexity, novelty, and hypothesis alignment. The eval agent then rigorously validates factors' executability and numerical stability while backtesting assesses their predictive effectiveness on historical data. The feedback mechanism evaluates performance metrics and theoretical soundness, guiding iterative refinement. This closed-loop process progressively derives a family of factors that capture emerging rather than overcrowded market inefficiencies, promoting alpha mining by balancing theoretical soundness with factor originality."}, {"title": "3.1 Problem Formulation", "content": "The alpha mining task considers a set of stocks $S = \\{1, . . . , S_N\\}$, a time window $T = \\{t_1, . . . , t_\\dagger\\}$, and a feature matrix $X \u2208 \\mathbb{R}^{N\u00d7T\u00d7D}$, where D denotes the dimension of the raw features. The objective of alpha mining is to learn an alpha factor (or alpha) f that maps a slice of input features $X_t$ to a predictive signal $r_{t+1}$, namely the subsequent return. Formally, an alpha can be written as $f(X_t) \u2192 r_{t+1}$, where $r_{t+1}$ is the return on the day t + 1. The alpha factor mining problem can be formulated as an optimization task:\n$f^* = arg max_{f\u2208F} L(f(X),y) \u2013 \\frac{1}{\\lambda}R(f)$,  (1)\nwhere F denotes the space of all possible factor expressions, y represents the ground-truth future returns (e.g., next-day returns), L measures predictive effectiveness (such as the information ratio or other performance metrics), R is a regularization term encouraging simplicity or novelty of the factor expression, and \\frac{1}{\\lambda} is a balancing parameter that trades off between performance and complexity.\nDistinct from conventional pure data-driven methods, we propose to leverage Large Language Models (LLMs) as intelligent agents for alpha factor generation. Traditional methods often struggle to incorporate domain expertise effectively or tend to generate factors that lack economic intuition. LLMs, with their strong natural language understanding and reasoning capabilities [27, 31, 41], offer a promising solution by being able to comprehend and operationalize human market insights. However, LLMs are inherently intractable due to their stochastic nature and limited ability to extract key information from lengthy contexts, which could lead to inconsistent or irrelevant factor generation. To address these limitations, we introduce two critical aspects in the regularization term to ensure both the practical relevance and long-term effectiveness of the generated factors. Specifically, we introduce market hypotheses"}, {"title": "3.2 Factor Generation Modeling", "content": "To operationalize the objective function defined in Eq. 2, a factor implementation mechanism is required that ensures both robustness and alignment with domain hypotheses. However, the inconsistent quality of LLM-generated outputs poses significant challenges in code-based factor construction. Approaches that generate code-based factors may frequently encounter operational barriers such as data format incompatibilities, inconsistencies across package versions, and difficulties in maintaining semantic coherence in extended code implementations. These challenges create a fundamental tension between code executability and semantic consistency, requiring LLMs to constantly balance these competing objectives in factor generation."}, {"title": "3.2.1 Factor Parsing with Abstract Syntax Trees", "content": "To address these limitations, we introduce an operator library O that abstracts and standardizes various mathematical and financial operations (e.g., rolling minima/maxima, moving averages, conditional checks). This abstraction layer significantly streamlines the factor construction process by providing LLMs with a consistent and well-defined set of operations, thereby simplifying the semantic alignment between operator compositions and market hypotheses. The Operator Library serves as an intermediate representation that bridges the gap between high-level market insights and low-level implementation details, enabling more robust and maintainable factor generation.\nWe define a parsing procedure:\n$G: (H,X) \u2192 F$, (3)\nwhere H represents the space of market hypotheses, with each hypothesis often described in a semi-structured form. X denotes the space of raw features. The output space F"}, {"title": "3.2.2 Interpretability and Complexity Control", "content": "Although objective (1) focuses on maximizing predictive quality subject to domain alignment, it is equally crucial to control the complexity of any candidate expression. We incorporate a regularization term Rg(f) that penalizes overly complex syntax trees or large numbers of free hyperparameters. This ensures that the final solution not only adheres to the economic rationale encoded in h but also remains interpretable and robust. For instance, we may define\n$R_g(f,h) = \\alpha_1 \u00b7 SL(f) + \\alpha_2\u00b7 PC(f) + \\alpha_3 \u00b7 ER(f, h)$, (4)\nwhere SL(f) measures symbolic length, PC(f) counts free parameters (e.g., window lengths), and ER(f, h) captures both the factor's novelty compared to existing alpha factors and its alignment with the given market hypothesis h. By tuning the weighting coefficients $\\{\\alpha_1, \\alpha_2, \\alpha_3\\}$, we can obtain parsimonious yet powerful factor specifications.\nTo quantitatively assess the originality of proposed alpha factors and detect potential duplicates, we introduce a pair-wise factor similarity metric based on AST matching. For any given factor $f_i$, we first parse its expression into an AST representation T($f_i$). To compute the similarity between two ASTs $f_i$ and $f_j$, we identify their largest common subtree by recursively comparing their AST structures T($f_i$) and T($f_j$). The similarity metric s is calculated as:\n$s(f_i, f_j) = max_{t_i \\in T(f_i),t_j \\in T(f_j)} \\{|t_i|: t_i = t_j\\}$, (5)\nwhere $t_i$ and $t_j$ are subtrees of T ($f_i$) and T($f_j$) respectively, |$t_i$| denotes the size of the subtree (number of nodes), and $t_i$ = $t_j$ indicates structural isomorphism between subtrees. With this similarity metric, a newly proposed factor can be compared with existing alphas that have been widely validated (see Fig. 2), such as Alpha101 [14]. Formally, we compute its maximum similarity score against an existing alpha zoo Z = {\u03a61, \u03a62, ..., \u03a6N }, providing a quantitative measure of the factor's originality, written as:\n$S(f) = max_{\\Phi \\in Z} s(f, \\Phi)$, (6)\nTo ensure the semantic consistency between market hypotheses and generated factors, we employ LLMs to evaluate two critical alignments: (1) whether the factor description aligns with the market hypothesis as a valid implementation, and (2) whether the factor expression accurately reflects its description. For a given hypothesis h, factor description d, and factor expression f, we formulate a consistency scoring function:\n$C(h, d, f) = c_1(h, d) + c_2(d, f)$, (7)\nwhere $c_1(h, d) \u2208 [0, 1]$ evaluates whether the factor description d represents a valid implementation of hypothesis h, $c_2(d, f) \u2208 [0, 1]$ measures the consistency between the factor description and its mathematical expression, and a is a weighting parameter. For example, if a factor claims to capture market liquidity dynamics in its description but its expression contains no liquidity-related components (such as trading volume, bid-ask spread, or market depth), it would receive a low $c_2$ score, indicating potential misalignment between the claimed economic intuition and actual implementation. This scoring mechanism helps filter out factors that either deviate"}, {"title": "3.3 Autonomous Multi-Agent Framework", "content": "As illustrated in Figure 1, AlphaAgent implements a recurrent framework for alpha factor mining through three specialized agents"}, {"title": "4 Experiments", "content": "AlphaAgent employs GPT-3.5-turbo as the foundational LLM to support agents. For RD-Agent [4], following its authors, GPT-4-turbo is used. Four fundamental alphas, including intra-day return, daily return, 20-day relative volume, and normalized daily range, serve as the base alphas and will be concatenated with newly proposed alphas to train a LightGBM model [15]. Before being fed into LightGBM, both features and returns undergo cross-sectional Z-score normalization to ensure comparability across stocks. The LightGBM model, with a maximum depth of 4 layers, is responsible for forecasting the next-day returns. In backtesting, a top-k dropout strategy is employed to select the 50 top-ranked stocks based on the predicted returns and exclude the 5 lowest-ranked stocks. All backtesting results account for transaction fees. For CSI 500, the transaction fees are set at 0.0005 for buying and 0.0015 for selling. For S&P 500, only selling fees are applied, with a rate of 0.0005."}, {"title": "4.1 Experiment Settings", "content": "4.1.1 Metrics. This study focuses on evaluating how AlphaAgent counteracts the alpha decay challenge against established baseline approaches using a comprehensive set of financial metrics. The Information Coefficient (IC) and Rank Information Coefficient (RankIC) measure forecasting precision through the correlation between predicted scores and actual returns, with IC using raw values and RankIC using ranked values. Risk-adjusted performance indicators include the Information Coefficient Information Ratio (ICIR), which evaluates the consistency of IC performance by comparing its mean to standard deviation, and the Information Ratio (IR), which measures risk-adjusted excess returns relative to a benchmark. For absolute performance assessment, we use the Annualized Return (AR) to quantify the yearly investment outcome normalized from cumulative returns, and the Maximum Drawdown (MDD) to capture the largest peak-to-trough decline in portfolio value. With these multi-faceted evaluation metrics, we can assess both the factor's long-term effectiveness and its resistance to alpha decay, as persistent IC/RankIC values and stable ICIR indicate sustained predictive power, while AR and MDD metrics reveal the practical impact of any deterioration in the factor's effectiveness over time.\n4.1.2 Backtest Settings. The backtesting experiments were conducted using the Qlib framework [36], on CSI 500 of the Chinese A-share market and S&P 500 of the U.S. stock market, spanning 2021 to 2024. Raw data employed to construct alpha factors include only OHLCV (i.e., $open, $high, $low, $close, $volume). CSI 500 data is collected from Baostock [3], while S&P 500 data is from Yahoo Finance [1]. See Table 1 for precise dataset splits."}, {"title": "4.2 Overall Performance", "content": "In Table 2, we show a comparison of the overall performance of the different methods for four years, from January 1, 2021, to December 31, 2024, in the CSI 500 (China) and S&P 500 (U.S.) stock markets. Regarding AlphaForge, RD-Agent, Deepseek-R1, OpenAI-01, and AlphaAgent, we apply their respective output alphas directly in this comparative analysis. For reasoning models Deepseek-R1 and OpenAI-01, we streamline the task prompt of AlphaAgent to facilitate alpha mining and evolution across five iterative rounds, with each round providing corresponding backtesting results using the identical configuration of AlphaAgent. For RD-Agent and AlphaAgent, we conduct 20 independent trials, with each trial comprising five evolutionary rounds. In each trial, we insert the optimal factors into their respective alpha zoos, yielding the optimal combination as the final result. The performance of each method is evaluated by five key metrics: IC, ICIR, AR, IR, and MDD. Bold numbers indicate"}, {"title": "4.3 Alpha Decay Analysis", "content": "Figure 4 compares the yearly performance between Alpha158 [36], GP [18], a technical indicator RSI, and AlphaAgent's alphas over 5 years on CSI 500. Alpha158, GP, and RSI all exhibit substantial declines in predictive power, with their ICs dropping from 0.022-0.036 to near zero and RankICs decreasing from 0.020-0.042 to around zero, highlighting the widespread challenge of alpha decay in the Chinese stock market. In contrast, AlphaAgent's alphas demonstrate remarkable stability, maintaining predictive effectiveness with IC values consistently around 0.02 and RankIC values around 0.025 throughout the period. This contrast highlights the superior sustainability of AlphaAgent compared to traditional factors, which exhibit stronger signs of alpha decay.\nThis analysis reveals fundamental differences between how traditional alphas and our approach face modern financial market challenges. GP's rapid performance deterioration suggests potential overfitting to historical patterns, making it particularly vulnerable to changing market conditions. Meanwhile, the significant performance degradation of Alpha158 and RSI Indicator exemplifies the \"alpha decay\" phenomenon described in Sec. 1, to a great extent,"}, {"title": "4.4 Alpha Mining Efficiency Analysis", "content": "This subsection conducts an analysis that focuses on the quality of generated alpha factors and the computational efficiency of the generation process. Figure 5 illustrates the evolution of IC values across five rounds for AlphaAgent and RD-Agent on CSI 500's test split. First, the results show that RD-Agent exhibits relatively stable and smaller variance (shown by the consistent width of its shaded region), likely due to its lack of exploration incentives for LLM-based agents, indicating more homogeneous factor candidates. Such candidates lead to factor crowding and accelerated alpha decay as similar signals become widely exploited in the market. On the contrary, AlphaAgent consistently maintains higher average IC values compared to RD-Agent throughout all rounds, demonstrating the superior predictive power of its generated factors gained from complexity and hypothesis-alignment constraints against overfitting and financial rationality of the generated factors. A notable observation is an increasing variance (shown by the expanding shaded region) of AlphaAgent's IC values as the rounds progress, suggesting its factor diversity and potentially a wider exploration space brought by its originality penalties, which could lead to a higher probability of discovering effective factors. AlphaAgent's originality penalties drive broad exploration of the factor space, while these complementary mechanisms ultimately contribute to consistently higher average IC values."}, {"title": "4.5 Ablation Study", "content": "In Figure 6, we ablate AlphaAgent's core components, i.e., factor modeling constraints and symbolic assembly, across three key metrics. This evaluation is based on 100 rounds of evolution, evenly split between the CSI 500 and S&P 500 markets. The hit ratio measures the proportion of generated alphas achieving exceptional"}, {"title": "4.6 Implications", "content": "The results shown in Sec. 4.3 and Sec. 4.4 underscore a critical insight into modern quantitative finance: the imperative for alpha mining methods to possess continuous exploration capability beyond fitting historical patterns and relying on established theories. This is particularly evident in today's highly efficient markets where"}, {"title": "5 Conclusion", "content": "This paper introduces AlphaAgent, a novel LLM-driven framework that effectively counteracts the critical challenge of alpha decay with three key regularization mechanisms: originality enforcement, complexity control, and hypothesis alignment. By incorporating these mechanisms into an autonomous framework, AlphaAgent produces decay-resistant and performant alpha factors while maintaining theoretical soundness, achieving substantial excess returns with remarkable consistency and decay resistance through various market conditions. The framework also suggests a promising direction for the next-generation alpha mining framework that swiftly adapts to market evolution while maintaining alpha sustainability."}]}