{"title": "LLM-Empowered State Representation for Reinforcement Learning", "authors": ["Boyuan Wang", "Yun Qu", "Yuhang Jiang", "Jianzhun Shao", "Chang Liu", "Wenming Yang", "Xiangyang Ji"], "abstract": "Conventional state representations in reinforcement learning often omit critical task-related details, presenting a significant challenge for value networks in establishing accurate mappings from states to task rewards. Traditional methods typically depend on extensive sample learning to enrich state representations with task-specific information, which leads to low sample efficiency and high time costs. Recently, surging knowledgeable large language models (LLM) have provided promising substitutes for prior injection with minimal human intervention. Motivated by this, we propose LLM-Empowered State Representation (LESR), a novel approach that utilizes LLM to autonomously generate task-related state representation codes which help to enhance the continuity of network mappings and facilitate efficient training. Experimental results demonstrate LESR exhibits high sample efficiency and outperforms state-of-the-art baselines by an average of 29% in accumulated reward in Mujoco tasks and 30% in success rates in Gym-Robotics tasks.", "sections": [{"title": "1. Introduction", "content": "Traditional reinforcement learning (RL) algorithms (Tesauro et al., 1995; Watkins & Dayan, 1992; Rummery & Niranjan, 1994) generally require a large number of samples to converge (Maei et al., 2009; Ohnishi et al., 2019). Compounding this challenge, in most cases, the intricate nature of RL tasks significantly hampers sample efficiency. (Dulac-Arnold et al., 2019). For handling complex tasks and enhancing generalization, neural networks are utilized to approximate value functions (Mnih et al., 2015; Schulman et al., 2015b). However, value networks often lack smoothness even when they are trained to converge, leading to instability and low sample efficiency throughout the training process (Asadi et al., 2018). Considering that in deep RL, state vectors serve as the primary input to value networks, sub-optimal state representations can result in limited generalization capabilities and non-smoothness of value network mappings (Schuck et al., 2018; Merckling et al., 2022).\nIn most RL environments (Todorov et al., 2012; Brockman et al., 2016; de Lazcano et al., 2023), source state representations typically embody environmental information but lack specific task-related details. However, the learning of value networks depends on accurately capturing task dynamics through rewards (Yang et al., 2020; Yoo et al., 2022). The absence of task-related representations may impede the establishment of network mappings from states to rewards, affecting network continuity. Previous researches leverage diverse transitions, intensifying the issue of time-consuming data collection and training (Merckling et al., 2020; Sodhani et al., 2021; Merckling et al., 2022). Consequently, a question arises regarding the existence of a more efficient method for identifying task-related state representations.\nIndeed, incorporating task-specific information into the state representation can be achieved through introducing expert knowledge (Niv, 2019; Ota et al., 2020). Recently, significant strides have been accomplished in the field of Large Language Models (LLM) (Touvron et al., 2023; OpenAI, 2023). The exceptional performance of LLM in various domains (Shen et al., 2023; Miao et al., 2023; Liang et al., 2023; Madaan et al., 2023; Stremmel et al., 2023), particularly in sequential decision-making tasks, showcases their extensive knowledge and sufficient reasoning abilities (Wang et al., 2023a;b; Feng et al., 2023; Wu et al., 2023; Du et al., 2023; Shukla et al., 2023; Sun et al., 2023; Zhang & Lu, 2023). This motivates the idea that the prior knowledge embedded in LLM can be exploited to enhance the generation of task-related state representations.\nTo preliminarily validate LLM's capacity in enhancing state representations, an illustrative toy example is provided. In Figure 1, results demonstrated that the state representation generated by LLM can help to enhance the continuity of value networks and expedite the convergence of policy learning. To substantiate this, we employ the Lipschitz constant (Jones et al., 1993) for smoothness assessment. The LLM-generated state representations enhance the Lipschitz continuity of value networks, accounting for the improvement of sample efficiency and performance.\nIn this paper, we propose a novel method named LLM-Empowered State Representation (LESR). We utilize LLM's coding proficiency and interpretive capacity for physical mechanisms to generate task-related state representation function codes. LLM is then employed to formulate an intrinsic reward function based on these generated state representations. A feedback mechanism is devised to iteratively refine both the state representation and intrinsic reward functions. In the proposed algorithm, LLM consultation takes place only at the beginning of each iteration. Throughout both training and testing stages, LLM is entirely omitted, ensuring significant time savings and flexibility.\nIn summary, our main contributions are:\n\u2022 We propose a novel method employing LLM to generate task-related state representations accompanied by intrinsic reward functions for RL. These functions are demonstrated to exhibit robustness when transferred to various underlying RL algorithms.\n\u2022 We have theoretically demonstrated that enhancing Lipschitz continuity improves the convergence of the value networks and empirically validated more task-related state representations can enhance Lipschitz continuity.\n\u2022 LESR is a general framework that accommodates both continuous and discontinuous reward scenarios. Experimental results demonstrate that LESR significantly surpass state-of-the-art baselines by an average improvement of 29% in Mujoco tasks and 30% in Gym-Robotics tasks. We have also experimentally validated LESR's adaptability to novel tasks."}, {"title": "2. Related Work", "content": "Incorporating LLM within RL Architecture Since the advent of LLM, researchers have endeavored to harness the extensive common-sense knowledge and efficient reasoning abilities inherent in LLMs within the context of RL environments. Challenges have arisen due to the misalignment between the high-level language outputs of LLMs and the low-level executable actions within the environment. To address this, Qiu et al. (2023); Agashe et al. (2023); Yuan et al. (2023); Wang et al. (2023a;b); Feng et al. (2023); Wu et al. (2023) have sought to employ environments where observation and action spaces can be readily translated into natural language (Carta et al., 2023; Puig et al., 2018). Alternative approaches employ language models as the policy network with fine-tuning (Zhang & Lu, 2023; Li et al., 2022; Shi et al., 2023; Yan et al., 2023; Carta et al., 2023). Meanwhile, other endeavors focus on leveraging LLMs as high-level planners, generating sub-goals for RL agents (Sun et al., 2023; Shukla et al., 2023; Zhang et al., 2023b). Nevertheless, these works encounter a common challenge: the tight coupling of LLMs with RL agents, leading to frequent communication between the two even during the testing stage, a process that proves time-consuming and inefficient.\nState Representation Derived from LLM Some researchers use LLM for state representations that contain more information in partially observable scenarios. Da et al. (2023) employs LLMs to provide extra details about road conditions in traffic control areas. Similarly, Shek et al. (2023) focuses on robot locomotion in unstructured environments, using LLM as translators to extract environmental properties and generate contextual embeddings for training. Chen et al. (2023), tracks key objects and attributes in open-world household environments via LLM, expanding and updating object attributes based on historical trajectory information. These methods mainly addressed the issue of missing state information in partially observable scenarios."}, {"title": "3. Method", "content": "We consider a Markov decision process (Puterman, 1990) defined by a tuple $(S, A, R, P, p_0, \\gamma)$, where $S$ denotes the source state space and $A$ denotes the action space. Given a specific task, $R$ is the source extrinsic reward function of the environment. $P(s'|s, a)$ denotes the dynamic transition function, $p_0$ is the initial state distribution, and $\\gamma$ is the discount factor. The primary objective is to learn a RL policy $\\pi(a|s)$ that maximizes the cumulative rewards expectation, which is defined as value function\n$Q^{\\pi}(s_t, a_t) = {\\mathbb E}_{\\pi} [\\sum_{t'=t}^\\infty \\gamma^{t'-t} r(s_{t'}, a_{t'}) | s_t, a_t]$\nIn order to assess the impact of LESR on network continuity, we introduce the Lipschitz constant (Jones et al., 1993):\nDefinition 3.1. Denote the data space as $X \\subset {\\mathbb R}^d$ and the label space as $Y \\subset {\\mathbb R}$. Consider a dataset $X_0 \\subset X$, and the label $Y_0 = \\{y_i | y_i = u(x_i), \\text{ where } x_i \\in X_0\\} \\subset Y$. Here, $x_i$ represents a sequence of i.i.d. random variables on $X$ sampled from the probability distribution $p$, and $u : X_0 \\subset X \\rightarrow Y$ is the Lipschitz constant of a mapping given by\n$\\text{Lip}(u; X_0) = \\underset{x_1, x_2 \\in X_0}{\\text{sup}} \\frac{||u(x_1) - u(x_2)||_2}{||x_1 - x_2||_2}$         (1)\nWhen $X_0$ is all of $X$, we write $\\text{Lip}(u; X) = \\text{Lip}(u)$. A lower Lipschitz constant indicates a smoother mapping $u$.\n3.2. LLM-Empowered State Representation\nIn many RL settings (Todorov et al., 2012; Brockman et al., 2016; de Lazcano et al., 2023), source state representations usually contain general environmental information, while often lacking specific details related to current tasks which is critical to the training of the value networks (Yang et al., 2020; Yoo et al., 2022). The absence of task-related representations may hinder network mappings from states to rewards, impacting the continuity of networks. Recognizing this limitation, the identification and incorporation of additional task-related state representations emerge as a pivotal strategy. This strategic augmentation can expedite the establishment of network mappings, subsequently boosting the smoothness of networks and augmenting training efficiency.\nDue to the extensive knowledge and priors embedded in LLM, utilizing it for generating task-related state representations can be promising. In this context, we present a direct and efficient method named LLM-Empowered State Representation (LESR). The whole framework is depicted in Figure 2. Our methodology hinges on leveraging LLM to facilitate the generation of more task-specific state representations. Herein, we denote LLM as $M$ and a descriptor translating symbolic information into natural language as $d$. Consequently, the input to LLM $M$ is represented as $d(\\mathcal S)$, which constitutes four parts: (1) Task Description: information about the RL environment and the specific task. (2) State Details: information pertaining to each dimension of the source state. (3) Role Instruction: assignments that require LLM to generate task-related state representation and intrinsic reward codes. (4) Feedback: historical information from previous iterations. For a full comprehensive descriptions of our prompts, refer to Appendix C.\nOur primary objective is to harness LLM $M$ to formulate a python function $F : \\mathcal S \\rightarrow \\mathcal S'$, where $\\mathcal S'$ denotes the LLM-empowered state representation space. $F$ is sampled from $M(d(\\mathcal S))$, and $d(\\mathcal S)$ explicitly embeds the task information into LLM. The state representation function $F$ utilizes source state dimensions for calculations, generating task-specific state dimensions. At each timestep $t$, when the agent get the current state $s_t$ from the environment, the corresponding state representation $s'_t = F(s_t)$ will be concatenated to the source state as the input for the policy $\\pi(a_t|s_t, s'_t)$ and value $Q(s_t, s'_t, a_t)$.\nOnce the state representation function $F$ is obtained, LLM $M$ is subsequently required to provide an intrinsic reward function $G : \\mathcal S' \\rightarrow {\\mathbb R}$ in python code format based on $s^\\# = (s_t, s'_t) \\in \\mathcal S^\\#$, where $\\mathcal S^\\# = \\mathcal S \\times \\mathcal S'$ is the joint state space. More precisely, we stipulate in the prompt that LLM is obliged to incorporate the LLM-empowered state representations $s'_t$ to calculate the intrinsic rewards, and it also retains the option to incorporate the source state $s_t$ for a better intrinsic reward design. We formulate the joint optimization objective as:\n$\\underset{\\pi}{\\text{max}} \\underset{F, G}{max} {\\mathbb E}_{F, G, \\pi} [\\sum_{t=0}^\\infty \\gamma^t (r + w \\cdot r^i)| s_t^\\# = G(s_t, F(s_t))]$,          (2)\nwhere $F, G \\sim M(d(\\mathcal S))$ and $w$ is the weight of the intrinsic reward.\n3.3. Lipschitz Constant for Feedback\nIn practice, to enhance the robustness of state representations, we iteratively query LLM multiple times, incorporating previous training results as feedback. During each training iteration, we sample K state representation and intrinsic reward function codes $F_k, G_k, k = 1,..., K$ from LLM $M$. Subsequently, we concurrently execute K training processes over $N_{small}$ training timesteps to evaluate the performance of each function $F_k, G_k$. Here, $N_{small}$ is intentionally set to be smaller than the total timesteps $N$ employed in the final evaluation stage.\n\u2022 Continuous Scenarios For scenarios with continuous extrinsic reward, we maintain a Lipschitz constant array $C_k \\in {\\mathbb R}^{|S|}$ for each of the K training instances. Each element of $C_k$ signifies the Lipschitz constant of a mapping $u_i, i = 1,..., |S|$, which maps each dimension of $\\mathcal S^\\#$ to the extrinsic rewards. Note: $u_i$ is introduced to signify the Lipschitz constant computed independently for each state dimension concerning the extrinsic reward. This assessment is crucial for guiding the LLM in identifying and eliminating undesired dimensions within the state representation. Given a trajectory $T = \\{s_t, r_t\\}_{t=1}^H$ of length H, the current Lipschitz constant array is calculated as follows:\n$C_k^H = \\underset{t=1}{\\overset{H}{\\sum}} Lip(u_i; T_i) ; \\qquad (3)$\nwhere $T_i = \\{s_t[i], r_t\\}_{t=1}^H$, $s_t[i]$ denotes the i-th dimension of the joint state representations $\\mathcal S^\\#$, and $C_k^f$ denotes the Lipschitz constant array of the current trajectory. we soft-update $C_k$ over trajectories:\n$C_k = \\tau C_k + (1 - \\tau) C_k^f, \\qquad (4)$\nwhere $\\tau \\in [0, 1]$ is the soft-update weight. At the end of each training iteration, the Lipschitz constant array $C_k$ and policy performance $v_k$ are provided to LLM for CoT (Wei et al., 2022) suggestions, which, along with the training results, serve as feedback for subsequent iterations. The feedback information helps LLM to generate task-related state representations that exhibit a lower Lipschitz constant with extrinsic rewards, as elaborated in Section 3.4 where we discuss the theoretical advantages of a lower Lipschitz constant for network convergence. In the subsequent iterations, LLM leverages all historical feedback to iteratively refine and generate improved state representation and intrinsic reward function codes $\\{F_k\\}_{k=1}^K, \\{G_k\\}_{k=1}^K$. The whole algorithm is summarized in Algorithm 1.\n\u2022 Discontinuous Scenarios Dealing with scenarios with discontinuous extrinsic reward using conventional RL baselines is notably challenging and constitutes a specialized research area (Vecerik et al., 2017; Trott et al., 2019; Liu et al., 2023). Despite this challenge, LESR remains effective in such scenarios. We provide two ways of estimating"}, {"title": "3.3. Lipschitz Constant for Feedback", "content": "In practice, to enhance the robustness of state representations, we iteratively query LLM multiple times, incorporating previous training results as feedback. During each training iteration, we sample K state representation and intrinsic reward function codes $F_k, G_k, k = 1,..., K$ from LLM $M$. Subsequently, we concurrently execute K training processes over $N_{small}$ training timesteps to evaluate the performance of each function $F_k, G_k$. Here, $N_{small}$ is intentionally set to be smaller than the total timesteps $N$ employed in the final evaluation stage.\n\u2022 Continuous Scenarios For scenarios with continuous extrinsic reward, we maintain a Lipschitz constant array $C_k \\in {\\mathbb R}^{|S|}$ for each of the K training instances. Each element of $C_k$ signifies the Lipschitz constant of a mapping $u_i, i = 1,..., |S|$, which maps each dimension of $\\mathcal S^\\#$ to the extrinsic rewards. Note: $u_i$ is introduced to signify the Lipschitz constant computed independently for each state dimension concerning the extrinsic reward. This assessment is crucial for guiding the LLM in identifying and eliminating undesired dimensions within the state representation. Given a trajectory $T = \\{s_t, r_t\\}_{t=1}^H$ of length H, the current Lipschitz constant array is calculated as follows:\n$C_k^H = \\underset{t=1}{\\overset{H}{\\sum}} Lip(u_i; T_i) ; \\qquad (3)$\nwhere $T_i = \\{s_t[i], r_t\\}_{t=1}^H$, $s_t[i]$ denotes the i-th dimension of the joint state representations $\\mathcal S^\\#$, and $C_k^f$ denotes the Lipschitz constant array of the current trajectory. we soft-update $C_k$ over trajectories:\n$C_k = \\tau C_k + (1 - \\tau) C_k^f, \\qquad (4)$\nwhere $\\tau \\in [0, 1]$ is the soft-update weight. At the end of each training iteration, the Lipschitz constant array $C_k$ and policy performance $v_k$ are provided to LLM for CoT (Wei et al., 2022) suggestions, which, along with the training results, serve as feedback for subsequent iterations. The feedback information helps LLM to generate task-related state representations that exhibit a lower Lipschitz constant with extrinsic rewards, as elaborated in Section 3.4 where we discuss the theoretical advantages of a lower Lipschitz constant for network convergence. In the subsequent iterations, LLM leverages all historical feedback to iteratively refine and generate improved state representation and intrinsic reward function codes $\\{F_k\\}_{k=1}^K, \\{G_k\\}_{k=1}^K$. The whole algorithm is summarized in Algorithm 1.\n\u2022 Discontinuous Scenarios Dealing with scenarios with discontinuous extrinsic reward using conventional RL baselines is notably challenging and constitutes a specialized research area (Vecerik et al., 2017; Trott et al., 2019; Liu et al., 2023). Despite this challenge, LESR remains effective in such scenarios. We provide two ways of estimating"}, {"title": "4. Theoretical Analysis", "content": "Definition A.1. We firstly define the identity projection and closet point projection map $Id : X \\rightarrow X, \\sigma_{X_k} : X \\rightarrow X_k = \\{x_1,...,x_n\\}$ that satisfies\n$\\forall x \\in X, Id(x) = x$\n$\\forall x \\in X, ||x - \\sigma_{X_k}x||_2 = \\text{min} \\{||X - x_i||_2\\} $           (8)\nNow we investigate the convergence under the mapping $f$ in Theorem 3.4:\nTheorem A.2 (Convergence). If $u_1 \\in U_1$ is any minimizer of $L(u, X_1)$, where $U_1$ denotes the solution set of $L(u, X_1)$ in Definition 3.2, then for any $t > 0$, there exists $C \\geq 1$ and $0 < c < 1$ satisfies the following with probability at least $1-Ct^{-1}N^{-(ct-1)}$:\n$E_{x\\sim p_f} ||u'_1 - u_1||_2 \\leq C \\left[Lip(u'_1) + Lip(u_1)\\right] \\left( \\frac{t log N}{N} \\right)^{1/d}$                   (9)\nProof of Theorem A.2. Since $u_1 \\in U_1$ is a minimizer of $L(u, X_1)$, we must have $u'_1 (x_i) = u_1(x'_i)$ for all $1 < i < N$. Then for any $x \\in X$ we have\n$||u_1(x) - u'_1(x)||_2 = ||u_1(x) - u'_1(\\sigma_{X_1}(x)) + u'_1(\\sigma_{X_1}(x)) -  u_1(\\sigma_{X_1}(x)) +u_1(\\sigma_{X_1}(x)) - u'_1(x)||_2$    \\\\\n$\\leq ||u_1(x) - u'_1(\\sigma_{X_1}(x))||_2 + ||u'_1(\\sigma_{X_1}(x)) -   u_1(\\sigma_{X_1}(x))||_2 + ||u_1(\\sigma_{X_1}(x)) - u'_1(x)||_2$\n$=0$\n$\\leq ||u_1(x) - u'_1(\\sigma_{X_1}(x))||_2 + || u_1(\\sigma_{X_1}(x)) - u'_1(x)||_2$\n$\\leq \\left[Lip(u'_1) + Lip(u_1)\\right] ||x - \\sigma_{X_1}(x)||_2$                   (10)\nNow we provide a bound between the identity projection and closet point projection:\nLemma A.3 (Lemma 2.9. in (Oberman & Calder, 2018)). For any $t > 0$, the following holds with probability at least $1-Ct^{-1}N^{-(ct-1)}$.\n$E_{x\\sim p_f} ||x - \\sigma_{X_1}(x)||_2 \\leq C \\left( \\frac{t log N}{N} \\right)^{1/d}$                   (11)\nThe proof is completed by combining Lemma A.3 into Eq 10.\nNext, we can prove that the generalization loss converges based on Theorem A.2:\nTheorem A.4. Assume that for some $q > 1$ the loss $l$ in Definition 3.2 satisfies $l(y_i, y_k) \\leq C||y_i - y_k||_2$ for all $y_i, y_k \\in Y$. Then under Theorem A.2, the following bound of the loss $L[u'_1, X]$ in Definition 3.2 holds with probability at least $1-Ct^{-1}N^{-(ct-1)}$.\n$L[u'_1, X] \\leq C\\left[ Lip(u'_1) + Lip(u_1) \\right] \\left( \\frac{t log N}{N} \\right)^{q/d}$                   (12)\nProof of Theorem A.4. We can bound the loss as:\n$L[U'_1, X] = \\int_{x \\in X} p_f(x)l(u_1(x), u'_1(x)) dx \\leq CE_{x\\sim p_f} ||u'_1 - u_1||^q$                 (13)\nThe proof is completed by combining Theorem A.2 into Eq 13.\nNow we turn to the proof of Theorem 3.4, we start with the following lemma:\nLemma A.5. Under Assumption 3.3, $p$ and $p_f$ denote the source probability distribution on $X$ and probability distribution on $f(X)$:\n$E_{x\\sim p_f} ||x - \\sigma_{X_1}(x)||_2 = E_{x\\sim p}||x - \\sigma_{X_0}(x)||_2$                 (14)"}, {"title": "B. Why Lipschitz constant is crucial for RL", "content": "In this section we main focus on the relationship between the Lipschitz constant of the reward function and the continuity of the value function in RL. Firstly we make some assumptions.\nDefinition B.1. In reinforcement learning, given a policy $\\pi$, $\\gamma$ denotes the discounted factor, $r$ ($r: s \\rightarrow \\mathbb R$) denotes the reward function, $H$ denotes the length of trajectory, the definition of the value function $V(s)$ is:\n$V^{\\pi} (s) = {\\mathbb E} [\\sum_{t=0}^{H} \\gamma^t r(s_t) | s_0 = s, a_t \\sim \\pi(s_t)]$.             (20)"}]}