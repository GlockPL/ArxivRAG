{"title": "LLM-Empowered State Representation for Reinforcement Learning", "authors": ["Boyuan Wang", "Yun Qu", "Yuhang Jiang", "Jianzhun Shao", "Chang Liu", "Wenming Yang", "Xiangyang Ji"], "abstract": "Conventional state representations in reinforcement learning often omit critical task-related details, presenting a significant challenge for value networks in establishing accurate mappings from states to task rewards. Traditional methods typically depend on extensive sample learning to enrich state representations with task-specific information, which leads to low sample efficiency and high time costs. Recently, surging knowledgeable large language models (LLM) have provided promising substitutes for prior injection with minimal human intervention. Motivated by this, we propose LLM-Empowered State Representation (LESR), a novel approach that utilizes LLM to autonomously generate task-related state representation codes which help to enhance the continuity of network mappings and facilitate efficient training. Experimental results demonstrate LESR exhibits high sample efficiency and outperforms state-of-the-art baselines by an average of 29% in accumulated reward in Mujoco tasks and 30% in success rates in Gym-Robotics tasks.", "sections": [{"title": "1. Introduction", "content": "Traditional reinforcement learning (RL) algorithms (Tesauro et al., 1995; Watkins & Dayan, 1992; Rummery & Niranjan, 1994) generally require a large number of samples to converge (Maei et al., 2009; Ohnishi et al., 2019). Compounding this challenge, in most cases, the intricate nature of RL tasks significantly hampers sample efficiency. (Dulac-Arnold et al., 2019). For handling complex tasks and enhancing generalization, neural networks are utilized to approximate value functions (Mnih et al., 2015; Schulman et al., 2015b). However, value networks often lack smoothness even when they are trained to converge, leading to instability and low sample efficiency throughout the training process (Asadi et al., 2018). Considering that in deep RL, state vectors serve as the primary input to value networks, sub-optimal state representations can result in limited generalization capabilities and non-smoothness of value network mappings (Schuck et al., 2018; Merckling et al., 2022).\nIn most RL environments (Todorov et al., 2012; Brockman et al., 2016; de Lazcano et al., 2023), source state representations typically embody environmental information but lack specific task-related details. However, the learning of value networks depends on accurately capturing task dynamics through rewards (Yang et al., 2020; Yoo et al., 2022). The absence of task-related representations may impede the establishment of network mappings from states to rewards, affecting network continuity. Previous researches leverage diverse transitions, intensifying the issue of time-consuming data collection and training (Merckling et al., 2020; Sodhani et al., 2021; Merckling et al., 2022). Consequently, a question arises regarding the existence of a more efficient method for identifying task-related state representations.\nIndeed, incorporating task-specific information into the state representation can be achieved through introducing expert knowledge (Niv, 2019; Ota et al., 2020). Recently, significant strides have been accomplished in the field of Large Language Models (LLM) (Touvron et al., 2023; OpenAI, 2023). The exceptional performance of LLM in various domains (Shen et al., 2023; Miao et al., 2023; Liang et al., 2023; Madaan et al., 2023; Stremmel et al., 2023), particularly in sequential decision-making tasks, showcases their extensive knowledge and sufficient reasoning abilities (Wang et al., 2023a;b; Feng et al., 2023; Wu et al., 2023; Du et al., 2023; Shukla et al., 2023; Sun et al., 2023; Zhang & Lu, 2023). This motivates the idea that the prior knowledge embedded in LLM can be exploited to enhance the generation of task-related state representations.\nTo preliminarily validate LLM's capacity in enhancing state representations, an illustrative toy example is provided. In Figure 1, results demonstrated that the state representation generated by LLM can help to enhance the continuity of value networks and expedite the convergence of policy learning. To substantiate this, we employ the Lipschitz constant (Jones et al., 1993) for smoothness assessment. The LLM-generated state representations enhance the Lipschitz continuity of value networks, accounting for the improvement of sample efficiency and performance.\nIn this paper, we propose a novel method named LLM-Empowered State Representation (LESR). We utilize LLM's coding proficiency and interpretive capacity for physical mechanisms to generate task-related state representation function codes. LLM is then employed to formulate an intrinsic reward function based on these generated state representations. A feedback mechanism is devised to iteratively refine both the state representation and intrinsic reward functions. In the proposed algorithm, LLM consultation takes place only at the beginning of each iteration. Throughout both training and testing stages, LLM is entirely omitted, ensuring significant time savings and flexibility.\nIn summary, our main contributions are:\n\u2022 We propose a novel method employing LLM to generate task-related state representations accompanied by intrinsic reward functions for RL. These functions are demonstrated to exhibit robustness when transferred to various underlying RL algorithms.\n\u2022 We have theoretically demonstrated that enhancing Lipschitz continuity improves the convergence of the value networks and empirically validated more task-related state representations can enhance Lipschitz continuity.\n\u2022 LESR is a general framework that accommodates both continuous and discontinuous reward scenarios. Experimental results demonstrate that LESR significantly surpass state-of-the-art baselines by an average improvement of 29% in Mujoco tasks and 30% in Gym-Robotics tasks. We have also experimentally validated LESR's adaptability to novel tasks."}, {"title": "2. Related Work", "content": "Incorporating LLM within RL Architecture Since the advent of LLM, researchers have endeavored to harness the extensive common-sense knowledge and efficient reasoning abilities inherent in LLMs within the context of RL environments. Challenges have arisen due to the misalignment between the high-level language outputs of LLMs and the low-level executable actions within the environment. To address this, Qiu et al. (2023); Agashe et al. (2023); Yuan et al. (2023); Wang et al. (2023a;b); Feng et al. (2023); Wu et al. (2023) have sought to employ environments where observation and action spaces can be readily translated into natural language (Carta et al., 2023; Puig et al., 2018). Alternative approaches employ language models as the policy network with fine-tuning (Zhang & Lu, 2023; Li et al., 2022; Shi et al., 2023; Yan et al., 2023; Carta et al., 2023). Meanwhile, other endeavors focus on leveraging LLMs as high-level planners, generating sub-goals for RL agents (Sun et al., 2023; Shukla et al., 2023; Zhang et al., 2023b). Nevertheless, these works encounter a common challenge: the tight coupling of LLMs with RL agents, leading to frequent communication between the two even during the testing stage, a process that proves time-consuming and inefficient.\nState Representation Derived from LLM Some researchers use LLM for state representations that contain more information in partially observable scenarios. Da et al. (2023) employs LLMs to provide extra details about road conditions in traffic control areas. Similarly, Shek et al. (2023) focuses on robot locomotion in unstructured environments, using LLM as translators to extract environmental properties and generate contextual embeddings for training. Chen et al. (2023), tracks key objects and attributes in open-world household environments via LLM, expanding and updating object attributes based on historical trajectory information. These methods mainly addressed the issue of missing state information in partially observable scenarios."}, {"title": "3. Method", "content": "3.1. Problem Statement\nWe consider a Markov decision process (Puterman, 1990) defined by a tuple $(S, A, R, P, p_0, \\gamma)$, where $S$ denotes the source state space and $A$ denotes the action space. Given a specific task, $R$ is the source extrinsic reward function of the environment. $P(s'|s, a)$ denotes the dynamic transition function, $p_0$ is the initial state distribution, and $\\gamma$ is the discount factor. The primary objective is to learn a RL policy $\\pi(a|s)$ that maximizes the cumulative rewards expectation, which is defined as value function\n$Q^{\\pi}(s_t, a_t) = \\mathbb{E}_{\\pi} [\\sum_{t'=t}^\\infty \\gamma^{t'-t} r_{t'} | s_t, a_t]$\nIn order to assess the impact of LESR on network continuity, we introduce the Lipschitz constant (Jones et al., 1993):\nDefinition 3.1. Denote the data space as $X \\subset \\mathbb{R}^d$ and the label space as $Y \\subset \\mathbb{R}$. Consider a dataset $X_0 \\subset X$, and the label $Y_0 = \\{y_i | y_i = u(x_i), \\text{where } x_i \\in X_0\\} \\subset Y$. Here, $x_i$ represents a sequence of i.i.d. random variables on $X$ sampled from the probability distribution $p$, and $u : X_0 \\subset X \\rightarrow Y$ is the Lipschitz constant of a mapping given by\n$\\text{Lip}(u; X_0) = \\underset{x_1, x_2 \\in X_0}{\\text{sup}} \\frac{||u(x_1) - u(x_2)||_2}{||x_1 - x_2||_2}$     (1)\nWhen $X_0$ is all of $X$, we write $\\text{Lip}(u; X) = \\text{Lip}(u)$. A lower Lipschitz constant indicates a smoother mapping $u$.\n3.2. LLM-Empowered State Representation\nIn many RL settings (Todorov et al., 2012; Brockman et al., 2016; de Lazcano et al., 2023), source state representations usually contain general environmental information, while often lacking specific details related to current tasks which is critical to the training of the value networks (Yang et al., 2020; Yoo et al., 2022). The absence of task-related representations may hinder network mappings from states to rewards, impacting the continuity of networks. Recognizing this limitation, the identification and incorporation of additional task-related state representations emerge as a pivotal strategy. This strategic augmentation can expedite the establishment of network mappings, subsequently boosting the smoothness of networks and augmenting training efficiency.\nDue to the extensive knowledge and priors embedded in LLM, utilizing it for generating task-related state representations can be promising. In this context, we present a direct and efficient method named LLM-Empowered State Representation (LESR). The whole framework is depicted in Figure 2. Our methodology hinges on leveraging LLM to facilitate the generation of more task-specific state representations. Herein, we denote LLM as $M$ and a descriptor translating symbolic information into natural language as $d$. Consequently, the input to LLM $M$ is represented as $d(S)$, which constitutes four parts: (1) Task Description: information about the RL environment and the specific task. (2) State Details: information pertaining to each dimension of the source state. (3) Role Instruction: assignments that require LLM to generate task-related state representation and intrinsic reward codes. (4) Feedback: historical information from previous iterations. For a full comprehensive descriptions of our prompts, refer to Appendix C.\nOur primary objective is to harness LLM $M$ to formulate a python function $F : S \\rightarrow S'$, where $S'$ denotes the LLM-empowered state representation space. $F$ is sampled from $M(d(S))$, and $d(S)$ explicitly embeds the task information into LLM. The state representation function $F$ utilizes source state dimensions for calculations, generating task-specific state dimensions. At each timestep $t$, when the agent get the current state $s_t$ from the environment, the corresponding state representation $s'_t = F(s_t)$ will be concatenated to the source state as the input for the policy $\\pi(a_t|s_t, s'_t)$ and value $Q(s_t, s'_t, a_t)$.\nOnce the state representation function $F$ is obtained, LLM $M$ is subsequently required to provide an intrinsic reward function $G: S \\rightarrow \\mathbb{R}$ in python code format based on $s^q = (s_t, s'_t) \\in S'$, where $S' = S \\times S'$ is the joint state space. More precisely, we stipulate in the prompt that LLM is obliged to incorporate the LLM-empowered state representations $s'_t$ to calculate the intrinsic rewards, and it also retains the option to incorporate the source state $s_t$ for a better intrinsic reward design. We formulate the joint optimization objective as:\n$\\underset{F, G}{\\text{max }} \\underset{\\pi}{\\text{max }} \\mathbb{E}_{F,G, \\pi} [\\sum_{t=0}^\\infty \\gamma^t (r_t + w\\cdot r_t^i) |r_t^i = G(s_t, F(s_t))]$       (2)\nwhere $F, G \\sim M(d(S))$ and $w$ is the weight of the intrinsic reward."}, {"title": "3.3. Lipschitz Constant for Feedback", "content": "In practice, to enhance the robustness of state representations, we iteratively query LLM multiple times, incorporating previous training results as feedback. During each training iteration, we sample $K$ state representation and intrinsic reward function codes $F_k, G_k, k = 1,..., K$ from LLM $M$. Subsequently, we concurrently execute $K$ training processes over $N_{small}$ training timesteps to evaluate the performance of each function $F_k, G_k$. Here, $N_{small}$ is intentionally set to be smaller than the total timesteps $N$ employed in the final evaluation stage.\n\u2022 Continuous Scenarios For scenarios with continuous extrinsic reward, we maintain a Lipschitz constant array $C_k \\in \\mathbb{R}^{|S'|}$ for each of the $K$ training instances. Each element of $C_k$ signifies the Lipschitz constant of a mapping $u_i, i = 1,..., |S'|$, which maps each dimension of $S'$ to the extrinsic rewards. Note: $u_i$ is introduced to signify the Lipschitz constant computed independently for each state dimension concerning the extrinsic reward. This assessment is crucial for guiding the LLM in identifying and eliminating undesired dimensions within the state representation. Given a trajectory $T = \\{s_t, r_t\\}_{t=1}^H$ of length $H$, the current Lipschitz constant array is calculated as follows:\n$C_k^H =  \\underset{i=1}{\\vert S \\vert} \\{ Lip(u_i; T_i) \\} \\text{;}$        (3)\nwhere $T_i = \\{s_t[i], r_t\\}_{t=1}^H, s_t[i]$ denotes the i-th dimension of the joint state representations $s'$, and $C_k^f$ denotes the Lipschitz constant array of the current trajectory. we soft-update $C_k$ over trajectories:\n$C_k = \\tau C_k + (1 - \\tau) C_k^f$       (4)\nwhere $\\tau \\in [0, 1]$ is the soft-update weight. At the end of each training iteration, the Lipschitz constant array $C_k$ and policy performance $v_k$ are provided to LLM for CoT (Wei et al., 2022) suggestions, which, along with the training results, serve as feedback for subsequent iterations. The feedback information helps LLM to generate task-related state representations that exhibit a lower Lipschitz constant with extrinsic rewards, as elaborated in Section 3.4 where we discuss the theoretical advantages of a lower Lipschitz constant for network convergence. In the subsequent iterations, LLM leverages all historical feedback to iteratively refine and generate improved state representation and intrinsic reward function codes $\\{F_k\\}_{k=1}^K$, $\\{G_k\\}_{k=1}^K$. The whole algorithm is summarized in Algorithm 1.\n\u2022 Discontinuous Scenarios Dealing with scenarios with discontinuous extrinsic reward using conventional RL baselines is notably challenging and constitutes a specialized research area (Vecerik et al., 2017; Trott et al., 2019; Liu et al., 2023). Despite this challenge, LESR remains effective in such scenarios. We provide two ways of estimating Lipschitz constant as feedback in discontinuous extrinsic reward settings.\nLESR with Discounted Return In Equation 3, $u_i$ initially maps each dimension of $S'$ to dense extrinsic rewards. In sparse reward settings, we substitute these extrinsic rewards with the discounted episode return $\\sum_t \\gamma^t r_t$. Thus, $u_i$ now maps each dimension of $S'$ to the discounted episode returns, consistent with the algorithmic framework and theoretical scope proposed in LESR.\nLESR with Spectral Norm Since in Theorems B.4 and B.8 show that reducing the Lipschitz constant of the reward function lowers the upper bound of $\\text{Lip}(V; S)$ and improves the convergence of value functions. Therefore, we can use the spectral norm to estimate $\\text{Lip}(V; S)$ (Anil et al., 2019; Fazlyab et al., 2019) as feedback to LLM. By calculating the spectral norm of the $N$ weight matrices $W_1,..., W_N$ of the value functions, the Lipschitz constant of the value function is bounded by $\\Pi_{i=1}^N ||W_i||_2$, which is then presented to the LLM as feedback."}, {"title": "3.4. Theoretical Analysis", "content": "In this section, we present analysis of the theoretical implications of the Lipschitz constant on convergence in neural networks, inspired by Oberman & Calder (2018). Consider the dataset and labels $X_0, Y_0$ defined in Definition 3.1 with $N = |X_0|$ elements. The true mapping from $X$ to $Y$ is denoted as $u : X \\rightarrow Y$. Let $f : X \\rightarrow X$ be a function transforming a source $x \\in X$ into a more task-related $f(x)$.\nDefinition 3.2. Denote $u(\\psi; y)$ as a neural network mapping parameterized by $\\psi$. Consider the empirical loss for $X_0, Y_0$, where $l : Y \\times Y \\rightarrow \\mathbb{R}$ is a loss function satisfying (i) $l \\geq 0$, (ii) $l(y_1, y_2) = 0$ if and only if $y_1 = y_2$:\n$\\underset{u: X_0 \\rightarrow Y}{\\text{min }} L(u, X_0) = \\frac{1}{N} \\sum_{i=1}^N l(u(x_i; \\psi), Y_i)$.        (5)\nAssumption 3.3. The mapping $f : X \\rightarrow X$ only swaps the order of $x$ in the dataset $X_0$, which means $X_1 = \\{f(x_i)|f(x_i) \\in X_0, i = 1, ..., N\\}$ and $X_1 = X_0$. While for $x \\in X_2 = \\{x|x \\in X, x \\neq X_0\\}, f(x) = x$. Under $f$, the true mapping from $f(X)$ to $Y$ is denoted as $u_1 : f(X) \\rightarrow Y$. It can be derived that $u_1 = u \\circ f^{-1}$. We suppose under $f$ a lower Lipschitz constant is achieved:\n$\\text{Lip}(u_1) < \\text{Lip}(u)$.       (6)\nTheorem 3.4. Under Assumption 3.3, Given $X_0, Y_0$ and $X_1, Y_1 = \\{Y_i | Y_i = u_1(x_i), X_i \\in X_1\\}, u_0 \\in U_0$ is any minimizer of $L(u, X_0)$ and $u_1 \\in U_1$ is any minimizer of $L(u, X_1)$, where $U_0$ and $U_1$ denote the solution set of $L(u, X_0)$ and $L(u, X_1)$ in Definition 3.2 relatively, then on the same condition when $\\text{Lip}(u_0) = \\text{Lip}(u_1)$:\n$\\underset{u_1 \\in U_1}{\\text{sup }} \\mathbb{E}_{x \\sim p_f} ||u_1 - u_1||_2 \\leq \\underset{u_0 \\in U_0}{\\text{sup }} \\mathbb{E}_{x \\sim p} ||u - u_0||_2$,       (7)\n$p$ and $p_f$ denote the source probability distribution on $X$ and probability distribution on $f(X)$, relatively. In Theorem 3.4, it is demonstrated that the mapping $f$ exhibiting a lower Lipschitz constant can attain superior convergence. This observation underscores the significance of identifying task-related state representations characterized by lower Lipschitz constants with respect to the associated rewards. Such analysis to some extent sheds light on why smoother network mappings exhibit improved convergence performance. Proofs of Theorem 3.4 can be referred to Appendix A.\nWe delve deeper into the significance of the Lipschitz constant of the reward concerning state representations in RL. We introduce two additional theorems, namely Theorem B.4 and Theorem B.8, establishing a strong correlation between $\\text{Lip}(r; S)$ and $\\text{Lip}(V; S)$ and, consequently, the convergence of RL's value functions. Theorem B.4 indicates that reducing the Lipschitz constant of the reward function lowers the upper bound of $\\text{Lip}(V; S)$. Theorem B.8 illustrates how decreasing $\\text{Lip}(r; S)$ can enhance the convergence of RL algorithms' value functions. These theorems collectively emphasize our focus on minimizing the Lipschitz constant of the reward function to improve RL algorithms' convergence. Detailed proofs are available in Appendix B."}, {"title": "F. Future Work", "content": "Methodology framework viability Our methodology framework in LESR remains viable for image tasks, where Vision-Language Models (VLMs)(Radford et al., 2021; Kim et al., 2021b) can be employed to extract semantic features from images, followed by further processing under the LESR framework. We anticipate utilizing VLMs for future research.\nGeneral applicability beyond symbolic environments While our primary focus lies on symbolic environments, our method extends beyond them. LESR serves as a general approach for leveraging large models to generate Empowered State Representations, offering potential applicability across various environments.\nOffline reinforcement learning LESR is also feasible for offline reinforcement learning scenarios(Qu et al., 2024; Shao et al., 2024; Mao et al., 2023; 2024). The LESR framework is versatile and not limited to online RL. In the future, we aim to explore various applications and possibilities.\nWithin LESR, we utilize LLMs to generate Empowered State Representations, showcasing their effectiveness in enhancing the Lipschitz continuity of value networks in reinforcement learning. Our experimental results, along with supplementary"}]}