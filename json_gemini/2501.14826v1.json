{"title": "Multi-Modality Transformer for E-Commerce: Inferring User Purchase Intention to Bridge the Query-Product Gap", "authors": ["Srivatsa Mallapragada", "Ying Xie", "Varsha Rani Chawan", "Zeyad Hailat", "Yuanbo Wang"], "abstract": "E-commerce click-stream data and product catalogs offer critical user behavior insights and product knowledge. This paper propose a multi-modal transformer termed as PINCER, that leverages the above data sources to transform initial user queries into pseudo-product representations. By tapping into these external data sources, our model can infer users' potential purchase intent from their limited queries and capture query relevant product features. We demonstrate our model's superior performance over state-of-the-art alternatives on e-commerce online retrieval in both controlled and real-world experiments. Our ablation studies confirm that the proposed transformer architecture and integrated learning strategies enable the mining of key data sources to infer purchase intent, extract product features, and enhance the transformation pipeline from queries to more accurate pseudo-product representations.", "sections": [{"title": "Introduction", "content": "E-commerce platforms generate vast amounts of click-stream data capturing users' shopping journeys. This data encompasses users' product searches, page clicks, cart additions, and purchases. When a user searches for a product, they typically click on various retrieval results before adding desired items to their cart. Analyzing these online shopping patterns provides insight into purchase intent connecting queries to product clicks and cart adds. Additionally, aggregating data across users reveal diversity in product choice and purchasing behavior. While different users may search the same query, their subsequent clicks and purchases may widely vary."}, {"title": "Motivation 1: Purchase intention", "content": "Click-stream data reveals that users with the same query may have different purchase intentions. Fig. 1a shows three users searching for a \"leather sofa cover,\" purchase different products. Current retrieval systems employ various techniques based on click-stream data to enhance query understanding and capture purchase intentions [3, 7, 12, 14, 50]. However, these methods heavily depend on user queries and do not consider the purchased products associated with the query, creating a gap between the query and the purchased product. To bridge this gap, we utilize untapped information from query and Add To Cart (ATC) product pairs in click-stream data to transform the initial query into a pseudo product representation. This bridge can be defined as a purchase intention that is an amalgamation of information from query-ATC product pairs. To handle patterns of similar purchase intentions among users, we developed a reward-based competitive learning system that employs vector quantization to quantize crowd wisdom from query-ATC product pairs, inspired by Likas's work [29]. After training, purchase intention embeddings provide supplementary information about the history of similar query-product pairs to the retrieval system, helping generate a new pseudo product embedding closer to the user's choice of potentially relevant products."}, {"title": "Motivation 2: Granular Product Features", "content": "Fig. 1b illustrates a scenario where users exhibit similar purchase intentions, querying \u201ct-shirt with red color\", but they purchase different products. The product title purchased by the red, green, and yellow users contain \"red\" and \"t-shirt\" from the query, but the granular product text and image features contribute to users' preferences. A retrieval system should match the query at a finer granularity level to diversify the relevant products, aligning query words or sub-words with product words, sub-words, image patches, or kernels. Product features enable a retrieval system to focus on specific user preferences beyond their purchase intentions. This advocates for a system capable of processing both text and image product features from the catalog to meet users' needs. We consider this as a second motivation for our contributions and leverage multi-modal information from the product catalog, along with purchase intentions, to transform sparse queries into robust pseudo product representations for diverse, intent-aligned search."}, {"title": "Proposed Multi-modal Transformer Framework", "content": "The examples in figures la and 1b highlight the limitations of current retrieval systems [39, 9, 14] in capturing complex user purchase intents and retrieving diverse, intent-aligned products. To address this, we propose Purchase Intention-based Neural Causal E-commerce Retrieval (PINCER), a novel multi-modal transformer framework that transforms sparse queries into robust pseudo product representations by integrating extracted purchase intent vectors into the query transformation pipeline. PINCER is the first framework to connect queries and products through derived intent. A sequential two-stage training process estimates purchase intention and granular product features, from which"}, {"title": "Related Work", "content": "PINCER is a novel multi-modal transformer framework, designed to transform a query text input into a pseudo product embedding and retrieve most relevant products from the product catalog. It accommodates relevance ranking at it's core, but distinct from existing e-commerce product search systems [40, 1, 17, 35, 47, 28, 49]. PINCER advances beyond relevance ranking by extracting purchase intents from the query transaction history, and generate new pseudo product embeddings for product retrieval."}, {"title": "Purchase intention estimation", "content": "PINCER defines purchase intention as a user's potential intention to add a specific product to the shopping cart, associating a query with the ATC product. Although PINCER shares similarities with personalized search systems [25, 11, 15, 32, 1, 16, 43, 47, 5, 19, 50, 12], it is not one. Personalized search systems incorporate user profile information, queries, and product selections for user-specific search, while PINCER improves core product search with a novel data extraction and pseudo product embedding generation pipeline. Personalized search can filter and rank products tailored to users but suffers from cold start problems without prior user history [4] and only benefits high-entropy queries [13]. In contrast, PINCER does not have cold start issues and can be used on any query from any user. PINCER's purchase intention vectors represent crowd wisdom linking queries to products, not individual user preferences. Unlike personalized search systems utilizing user information from click-stream data, intention vectors are estimated from query-product pairs. It involves incremental latent space learning, manifesting as shared vector representations useful as centroids for similar pairs. Although existing literature lacks a direct implementation of these vectors, we designed a competitive learning training strategy with vector quantization. While SwAV [8] and MoCo [18] show self-supervised online clustering methods that may suffice our needs, they fall short for intent estimation due to disjoint query and product data sources. Critically, they do not enforce pairing queries and products to the same cluster center during training, which is essential for modeling intent via purchase history. Some retrieval studies [48, 46, 22] demonstrate query-product space quantization but share similar limitations by not aligning query-product pairs to the same intent vector. Inspired by Likas [29], we view query-product pairs alignment as a Bernoulli trial and use reward-based competitive learning with vector quantization to enforce each pair to select the same purchase intention vector and quantize the latent space. This quantization enables PINCER to model purchase intents as vectors capturing associations between queries and products."}, {"title": "Granular product features in retrieval", "content": "PINCER, during training, aligns both text and image granular product features with the query features. It later stores individual text and image features from all the catalog products in vector databases. PINCER, during training and inference, retrieve these query aligned product features for pseudo product generation. Existing approaches like Pseudo Relevance Feedback (PRF) [42, 44] use product information to expand the query context and perform a secondary retrieval. This uses a two stage retrieval approach that is relied on overall product information to improve query embeddings. Unlike PRF using individual product embeddings, PINCER aims to utilize the individual product features aligned with query tokens to generate a new pseudo product embedding. This requires the storage of granular features for a real-time retrieval during the query transformation process. This makes PINCER unique to store and retrieve product features when required."}, {"title": "Multi-modal transformer for retrieval", "content": "PINCER, as a multi-modal framework, transforms a textual query into a pseudo product embedding aligned with product embeddings from the catalog. Stage 1 training semantically maximizes the similarity between query and product embeddings in the shared latent space for further transformation, employing contrastive learning strategies that have demonstrated success in e-commerce product and personalized search models [17, 21, 50, 34, 20, 38, 12]. Research in text-to-image (t2i) retrieval, such as ViLBERT [33], UNITER [10], ALIGN [26], ViLT [24], and BLIP [27], highlights the learning of feature extraction and fusion of modalities to derive comprehensive representations. PINCER employ pre-trained encoders for query and product encoding in stage 1. This methodology uses a two-tower architecture [45, 31, 41] to accommodate query text alignment with product text and image embeddings using the contrastive learning strategy. Stage 2 training combines purchase intention, product features, and query within the latent space to generate a composite pseudo product embedding, leveraging causal attention in a transformer decoder. The decoder adapts preference modeling as a training strategy, utilizing soft positive and negative rewards. Amanda et al. [2] showed a use case of preference modeling pre-training, utilizing a binary reward system to enhance sample efficiency in Large Language Models (LLMs). This encouraged us to choose a positive target product and sample negative targets from a neighborhood of products around the target product purchase intention vector for training. This makes the decoder generate embeddings closer to the target product within the neighborhood of similar products."}, {"title": "Methodology", "content": "Let $D = (X,Y)$ denote the observable query-product pairs from the click-stream data, where $X = {X_1,X_2,X_3, ..., X_N }$ represents the set of user queries and $Y = {Y_1, Y_2, Y_3, ..., y_N }$ represents the set of corresponding products added to the cart. Each query $x_n$, governed by the vocabulary of the query text encoder, may vary in length, comprising (sub)words. Similarly, the product set $Y$ follows suit, defined by the vocabulary of the product text encoder. In the case of image modality products, each $y_n$ consists of a sequence of fixed-length image patches, as per the constraints of the image encoder. Here, $N = |D|$ denotes the size of the training data, and $(x_n, Y_n) \\in R^d$, residing in a shared latent embedding space of dimensionality $d$. We select $d = 128|256$ to optimize computational efficiency during retrieval. Importantly, it's worth noting that $D$ encompass repetitions of $x_n, Y_n$, but no $(x_n, Y_n)$ pairs. A set of users' purchase intentions are denoted as $S = {s_1, s_2, s_3, \\cdot s_K }$ with $s \\in S$ reflecting a distinct intention marginalized over queries and purchased products. A set of product text and image features derived from catalog products are represented as $F = {f_1, f_2, f_3, ... f_J }$ with $f \\in F$, and $f_j = (f_{jt}, f_{jr})$ being an ordered set of text and image representations."}, {"title": "Model Architecture", "content": "PINCER is a unified model that improves recall over precision to allot more relevant and diverse products in a ranked list of relevant products. PINCER maximize respective probabilities $P(y_n|x_n)$, $P(x_n|y_n)$, $P(s_k|x_n)$, $P(s_k|y_n)$, and $P(f_j|x_n)$ in stage 1 training using query $x_n$ and product $y_n$ pairs. $P(y_n|x_n)$, and $P(x_n|y_n)$ aligns the query and product embeddings [36], whereas $P(f_j|x_n)$ aligns the product features with the query token embeddings. The reward-based competitive training of $s_k$ maximizes $P(s_k|x_n)$ and $P(s_k|y_n)$ by reducing the distance between $x_n$ and $s_k$, and $y_n$ and $s_k$. This joint training enhances query-product alignment for retrieval and intent estimation. Stage 2"}, {"title": "Stage 1: Purchase intention estimation and relevance ranking", "content": "The concept of purchase intention is central to understanding the methodology of the PINCER algorithm. In e-commerce, users typically have a purchase intention in mind when they search a product. This intention is often reflected in their search query and thereby their choice of the products added to their cart. By capturing and modeling these purchase intentions, PINCER aims to bridge the gap between user queries and relevant products, improving the overall retrieval process. The purchase intention vectors in PINCER serve as a representation of the latent space where queries and products are associated based on historical user behavior. These vectors act as a crowd purchase intentions for aligning diverse yet similar purchase intentions of users. PINCER understand user preferences from the crowd purchase intention vectors and combines the information with incoming query to retrieve products that closely match users intentions.\nPINCER, depicted in Fig 2a), incorporates encoders and a pseudo product decoder trained across stages. The query embeddings, derived from the encoders, are projected into both product text and image spaces. This process aims to optimize the selection of the most relevant product feature for a given user query. The resultant query embedding, formed by concatenating these projections, aligns with the product embedding concatenated from product text and image embeddings. These semantic training strategies employ a contrastive loss from (1), utilizing parameters such as batch size (B) and temperature (T), akin to the approach in CLIP [36]. This contrastive loss proves instrumental in semantically ranking embeddings for dense retrieval purposes [50, 21, 34]. The contrastive loss between query and product text encoders is represented as $L_{qpt}$, image encoders is $L_{qpi}$, and concatenated vectors is $L_{qp}$ from (2).\n$L_{cont} = \\sum_{n=1}^{N} -log\\frac{exp(x_n y_n/T)}{\\sum_{b=1}^{B} exp(x_n y_n/T)}$,  (1)\nThe estimation of purchase intention begins by initializing a uniformly distributed fixed set of vectors that match the dimensions of concatenated query and product vectors. Inspired by competitive learning [29], queries and products select the nearest intent vector by Euclidean distance. Assuming user's intention is the latent link between a query and ATC product, a reward system (3) positively rewards the query-product pair to choose the same purchase intention and pushes the intent vector towards the query-product pair. Mismatched choices get negative rewards, separating query-product-intent vectors. The remaining probability $r_{p@n,s_k@}$ from (5) serves as a learning rate, balancing loss updates between converging and diverging vectors. Since the choice of closest intent vector for query or product is a binary operation, the selection probability (6) converts query/product-intent distances into Bernoulli probabilities. This lowers the remaining probability as distances decrease, controlling the loss for tuning query, product, and the purchase intention vectors.\n$L_{stage1} = \\lambda(L_{qp}) + (1 - \\lambda)(L_{qpt} + L_{qpi}) + RCL$ (2)"}, {"title": "Inference:", "content": "During inference (Fig. 2), PINCER utilizes the query encoder to project input text into the shared latent space. The resulting query embedding selects the nearest purchase intent vector and product features from the database. The decoder takes the intent, features, and bias vectors as input, with query vector cross-attending over the input vectors, to output the final pseudo product embedding. PINCER leverages the query encoder and the decoder to generate pseudo product embeddings from input text and retrieve relevant products in real-time. All product embeddings are pre-generated from the catalog and stored externally. The generated pseudo-product embeddings are matched against pre-stored product embeddings using cosine similarity to retrieve the top-k matches. This similarity-based ranking enables efficient retrieval of relevant products in real-time. For optimization, we employ vector clustering on product vectors using the purchase intent vectors as cluster centers. This reduces the retrieval time by pre-selecting clusters via the purchase intention vector that aligns with incoming query."}, {"title": "Experiments", "content": "This study tests PINCER model on real world experiments with real-world data and controlled experiments to test the model's capability to learn the synthetic purchase intention modeled data."}, {"title": "Real-world experiments:", "content": "We evaluate PINCER using real-world e-commerce click-stream data from a large company, containing 1.38M training and 173K validation/testing query-product pairs across 212K products in 4 home decor categories. The data includes user queries and their corresponding add-to-cart products, capturing real purchase intents through pairs of searches and items added to carts. This tests PINCER's ability to deduce purchase intents from crowd patterns and leverage them to improve product search over state-of-the-art (SOTA) baselines. Experiments on these query-product pairs with direct user actions validate whether PINCER can effectively extract and apply purchase intents to advance retrieval. Overall, these real e-commerce interactions provide an authentic test-bed for evaluating our approach's capabilities in a production environment."}, {"title": "Controlled experiments:", "content": "We conduct controlled experiments using synthetic datasets with queries generated for product catalogs from Amazon Cross-Market [6] (124K products, 44 categories) and FashionGen [37] (67K products, 48 categories) via ChatGPT 3.5 (LLM). The purpose is to embed synthetic purchase intentions to mimic click-stream transaction logs. Considering the Fig. 3, products were grouped by taxonomy and titles clustered to extract top words for query creation. ChatGPT 3.5 produced 5 customer-like queries per group. Products were ranked via text based semantic retrieval and filtered by image brightness/gradient as a Purchase Intention (PI) infusion process to isolate simulated cart adds. The color brightness and product patterns may reflect user choices as purchase intention and specificity in granular features. 1 5 random products per query were selected to simulate a search with one user-choice from 5 pages with 20 products per page. Product-specific queries were also generated, resulting in 238K training, 29K validation/testing pairs for FashionGen, and 844K training, 105K validation/testing pairs for Amazon. The synthetic benchmarks enable evaluating the model with tailored search behaviors. These datasets are available for free download at OSF."}, {"title": "Implementation Details", "content": "PINCER framework can accommodate light weight pre-trained encoders to LLMs that generate semantic embeddings. However, we decided to show the efficiency of the framework with light weight encoders competing with SOTA multi-modal retrieval systems. We employ distilBERT for text encoding and ResNet-50 for image encoding, both pre-trained. These models are projected to a 128-dimensional space for efficiency. Each projector consists of a feedforward layer with GELU activation, post-activation layer norm, and 10% dropout. AdamW optimizer with weight decay 1e -4 is used, adjusting learning rate on plateau. Training spans 15 epochs for each stage, with epochs varying based on dataset size. Models are trained on a Quadro RTX 6000 GPU (24GB RAM) for synthetic datasets and an A100 40GB GPU for real-world data with 3 hours of training per epoch."}, {"title": "Evaluation", "content": "We assess our e-commerce retrieval framework by matching text queries with text and image products, using evaluation metrics such as precision and recall at various top values (10, 20, 50, and 100). We compare PINCER's recall using $SumR = (Recall@10+ Recall@20+ Recall@50+ Recall@100)$ with some baselines because it is a holistic measure of cumulative recall across top-k retrieval values, which aligns with similar retrieval benchmarks in e-commerce. This metric ensures a comprehensive view of the model's performance across various levels of user interaction. The chosen baselines-CLIP, FashionCLIP, and RetroMAE [30] are well-established models in large-scale e-commerce retrieval and have demonstrated strong performance in this domain. CLIP provides a general multi-modal benchmark, FashionCLIP is tailored to fashion retrieval tasks, and RetroMAE optimizes retrieval-oriented embeddings for e-commerce with a dual encoder approach for the downstream task. To ensure comparability, all baselines were fine-tuned on task-specific datasets under consistent evaluation settings. In our assessment, we use fully trained PINCER outcomes to exhibit its advancements over domain-specific, text, and multi-modality models. Other e-commerce models mentioned in the literature are neither openly available nor trainable in a reasonable amount of time with our GPU resources."}, {"title": "Quantitative Results", "content": "Table 1 shows PINCER's superior precision and recall across all metrics, with significant improve- ments over other methods. PINCER achieved a 10.81% boost in overall recall on real-world data compared to the closest baseline. Crucially, it had markedly higher recall at the top 10 and 20 products, which is critical for e-commerce as users are more likely to purchase from the first page of results. PINCER's substantial early recall improvements demonstrate its ability to reliably surface relevant products within immediate view, enhancing user experience and driving business metrics like engagement and conversion. The results prove PINCER's real-world value in dramatically improving top-ranked retrieval and customer experience."}, {"title": "Qualitative Results", "content": "Fig. 4 provides close-up views of the t-SNE plots for CLIP, RetroMAE, and PINCER, revealing their distinct query-product pair distributions. CLIP (Fig. 4a) exhibits distinct pockets of query-product pairs, demonstrating a strict one-to-one relationship that increases precision but reduces the ability to retrieve relevant products, decreasing recall. RetroMAE (Fig. 4b) groups products in distinct neighborhoods, with queries positioned around them, contributing to good retrieval performance (Tables 1 & 2). However, some products belonging to different neighborhoods are well separated, but the queries relevant to those products overlap with other neighborhoods, reducing recall. In contrast, PINCER (Fig. 4c) transforms queries into pseudo products by locating relevant purchase intention vectors, reducing the need for query rephrasing and improving the retrieval of relevant products. PINCER strategically distributes query and product embeddings around the purchase intention vectors, which act as grouping centers, clustering all pairs around their nearest vectors. This approach increases the model's ability to retrieve relevant products and improves recall while efficiently diversifying the retrieval process. The purchase intention vectors also highlight their potential use for real-time retrieval without requiring additional vector indexing libraries.\nThe figures 5, and 6 compare the retrieval results from RetroMAE, CLIP, and PINCER for various test queries, each associated with five purchased products. A green box around a retrieved product image indicates a match with a purchased product. For the query \u201cbest sellers incense sticks\" (Fig. 5), RetroMAE and PINCER retrieve incense stick products that are possibly best-sellers, matching the purchased products, while CLIP only retrieves one relevant product. RetroMAE focuses more on text matching of \u201cbest sellers\" brand, whereas PINCER demonstrates its ability to capture the underlying purchase intention of most sold incense sticks including the brand. This results in more relevant and diverse product retrievals compared to the other models.\nFig. 6 presents the retrieval results for the query \u201cbelt satin pullover\". Among the three models, only PINCER successfully retrieved a matching product within the top five results. Although all the models retrieved pullover products, PINCER's results specifically included a product that was part of the purchased products list. This can be attributed to the preference modeling of purchased products, where the pseudo product embeddings are drawn closer to the user's potentially purchasable products. This phenomenon makes PINCER superior to other models in terms of achieving high precision & recall in product retrieval tasks."}, {"title": "Ablation Study", "content": "The results presented in Table 3 underscore the performance gains achieved by PINCER with and without the integration of Purchase Intention (PI) and Product Features (PF). This ablation study uses one of the synthetic datasets to showcase the model functionality because the prior experimental results prove the validity of the datasets. In Table 3, Stage 1 uses PI and PF individually to show their individual contribution to the model performance. The ablation study conducted affirms the pivotal role played by purchase intention and product features, as their inclusion significantly enhances the training outcomes in Stage 2. Without PI and PF, PINCER's two-stage training process would essentially resemble a two-tower CLIP architecture. It is important to note that PI and PF are integral components of PINCER that influence the stability of Stage 2 training. The stage 2 cannot be evaluated in isolation with only one of the two components present. Nonetheless, the pseudo product embeddings generated by PINCER demonstrate the criticality of utilizing purchase intention and product features, as it outperforms existing multi-modal and text retrieval models."}, {"title": "Real-time retrieval Performance", "content": "Fig. 7 illustrates the comparison between full-scale retrieval and Purchase Intention (PI) clustered products from the PINCER algorithm, encompassing 21023 queries and 67K unique products. Post- training, we retain all product embeddings and apply nearest neighbor algorithm for each cluster using PI vectors as cluster centroids. The use of cluster centroid indices from PI embeddings enables the matching of incoming queries with relevant cluster groups. While Fig. 7a highlights a slight degradation in PINCER's performance at 100 queries and beyond with clustered retrieval, Fig. 7b demonstrates its advantage in retrieval time. The observed drop in Recall@100 can be attributed to the specificity constraints of product clustering within the purchase intent vectors. As queries retrieve from a narrower cluster subset, fewer highly relevant matches may be found in the larger recall range, thereby impacting the overall Recall@100 score. Clustered retrieval for top@100 on 1000 queries achieves a 15ms latency compared to the 30ms real-time retrieval requirement. This comparison between full-scale and clustered retrieval showcases PINCER's potential for deployment in real-time retrieval platforms within a scalable environment."}, {"title": "Limitations", "content": "PINCER's reliance on contrastive learning makes its performance scale with batch size and GPU power. This study is limited to very few open-source models that are trainable with in the available GPU power."}, {"title": "Conclusion", "content": "In conclusion, this work introduces PINCER, a novel multi-modal transformer framework that bridges the gap between queries and purchased products in e-commerce retrieval by modeling potential user purchase intention. Through a two-stage training process, PINCER estimates purchase intention via reward-based competitive learning, stores and retrieves granular product features, optimizes relevance ranking, and generates pseudo product embeddings close to the target. Experimental results highlight PINCER, an efficient framework that outperform existing retrieval models by capturing user purchase nuances. PINCER advances e-commerce retrieval techniques in a systematic process to improve the retrieval recall. Future directions involve enabling PINCER to accept multi-modal query input, including user profile information from click-stream data with purchase intention for personalized search and product recommendations."}, {"title": "Algorithm:", "content": "The query transformation contains pre-trained language model encoder, latent embedding projectors, purchase intention vectors, and decoder. The product encoding uses pre-trained language and vision model encoders and latent embedding projects. The framework employ the pre-trained models to leverage the world knowledge of text and images to generate semantically relevant embeddings in a shared latent space.\nThe framework efficiently train the query and the product modules in stage 1 to align the purchase intention vectors with the query and product embeddings, and query-product granular features. Stage 2 training uses the same stage 1 data to generate the pseudo-product embedding. The purchase intentions are a fixed number of vectors that are shared between the queries and products, and instigate a shared learning.\nA competitive learning strategy trains purchase intentions, rewarding encoders when query and product choose the same intent vector.\nThe training stages in Fig. 2 use the following algorithms for training."}, {"title": "PINCER Stage 1 Training", "content": "Input: Query-product training pairs $(x_n, y_n) \\in T$ (batches) $\\subset D$(training data), learning rate a,\ntemperature T, batch size B, number of purchase intention vectors |S|\nInitialize: Pre-trained query encoder $E_q$, product text $E_{pt}$ and image encoder $E_{pi}$, query text\n$P_{qt}$ and image $P_{qi}$ projectors, product text $P_{pt}$ and image $P_{pi}$ projectors, purchase\nintention vectors S\nOutput: Trained PINCER $E_q$, $E_{pt}$, $E_{pi}$, $P_{qt}$, $P_{qi}$, $P_{pt}$, $P_{pi}$, and S"}, {"title": "PINCER Stage 2 Training", "content": "Input: Query-product training pairs $(x_n, y_n) \\in T$, learning rate a, temperature T, batch size B,\nstage 1 trained encoders ($E_q$, $E_{pt}$, $E_{pi}$), projectors ($P_{qt}$, $P_{qi}$, $P_{pt}$, $P_{pi}$), product features\nF, and purchase intention vectors S\nInitialize: Randomly initialize learnable vector $L_v$, transformer decoder D\nOutput: Trained PINCER decoder D, and learned vector $L_v"}]}