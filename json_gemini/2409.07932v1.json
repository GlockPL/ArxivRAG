{"title": "Reinforcement Learning Discovers Efficient Decentralized Graph Path Search Strategies", "authors": ["Alexei Pisacane", "Victor-Alexandru Darvariu", "Mirco Musolesi"], "abstract": "Graph path search is a classic computer science problem that has been recently\napproached with Reinforcement Learning (RL) due to its potential to outperform\nprior methods. Existing RL techniques typically assume a global view of the\nnetwork, which is not suitable for large-scale, dynamic, and privacy-sensitive\nsettings. An area of particular interest is search in social networks due to its\nnumerous applications. Inspired by seminal work in experimental sociology,\nwhich showed that decentralized yet efficient search is possible in social networks,\nwe frame the problem as a collaborative task between multiple agents equipped\nwith a limited local view of the network. We propose a multi-agent approach\nfor graph path search that successfully leverages both homophily and structural\nheterogeneity. Our experiments, carried out over synthetic and real-world social\nnetworks, demonstrate that our model significantly outperforms learned and\nheuristic baselines. Furthermore, our results show that meaningful embeddings\nfor graph navigation can be constructed using reward-driven learning.", "sections": [{"title": "1 Introduction", "content": "Graph path search is a fundamental task in Computer Science, pivotal in various domains such as\nknowledge bases [1], robotics [2], and social networks [3]. Given a start node and end node, the\ngoal is to find a path from a source to a destination in the graph that connects them and optimizes\ndesiderata such as minimizing path length. We refer to search strategies that achieve this as efficient.\nThe problem is generally framed from a centralized perspective with a global view of the network,\nwhich is impractical or infeasible for several applications. In peer-to-peer networks [4], where\nprivacy is a primary concern, a centralized agent poses significant risks [5-7]. Large graphs may also\ninduce scalability bottlenecks as the storage requirements of a centralized directory strain memory\nlimitations [8]. Moreover, in dynamic networks, maintaining a consistent global view of the topology\nmay be impossible [9]. Graph path search is of particular interest in social networks given the inherent\ncommercial applications and potential for new insights from a social sciences perspective [10, 11].\nIn this paper, we will study the problem of decentralized path graph search using local information.\nWe will consider social networks and we will discuss how the proposed method can be directly applied\nto any networks for which topological and node attribute information is available. Indeed, prior\nexperiments in human social networks, such as Stanley Milgram's renowned \u201csmall world\" experiment\n[12]\u00b2 reveals the existence of short paths in social networks that are discoverable solely through local\ngraph topology and high-level node attributes, e.g., characteristics of the individuals, such as their"}, {"title": "2 Related Work", "content": "Search is a common operation in network applications. Various classic algorithms [26] ensure path\ndiscovery between two nodes under specific conditions. They require maintaining global knowledge\nof the graph structure, which, as we have argued, is impractical in certain cases due to considerations\nof privacy, scalability, and dynamicity. We therefore focus our attention on graph path search using\nonly local information.\nAs previously discussed, our inspiration for studying this problem is Milgram's \"small world\"\nexperiment [12]. The findings, later validated on a larger scale [27], support this hypothesis in social\nnetworks, which are characterized by short mean path lengths. Subsequent research [28] highlighted\nthe discovery of effective routing strategies, emphasising the concept of homophily [15], which states\nthat individuals seek connections to others that are similar to themselves.\nIn addition to homophily, many networks are characterized by a power-law degree distribution [29]\nand exhibit heterogeneity in node degree. In such networks, a few \u201chub\u201d nodes with numerous\nconnections coexist with many nodes having a relatively small degree. Highly connected nodes\ntherefore offer potential shortcuts in search trajectories by bridging sparsely connected communities.\nFor effective search, finding the bridging node between two communities is often required. Relying\nsolely on homophily or node degree may be ineffective, as the bridging node might lack a large\ndegree or significant attribute similarity with the target node. In networks with large clusters, an agent\nmay spend considerable time navigating the current cluster before reaching the desired community.\nIdentifying weak ties [17] between communities is challenging using only node attributes or degrees;\ntherefore, an effective search for weak ties requires awareness of candidate nodes' neighborhoods.\nA useful lens for viewing this problem is through a \"hidden metric space\" [30] of node features.\nAssuming node features are representative of their position within this space, the probability of"}, {"title": "2.2 Reinforcement Learning for Graph Routing and Search", "content": "Reinforcement Learning methods have been applied for a variety of graph optimization problems in\nrecent years as a mechanism for discovering, by trial-and-error, algorithms that can outperform classic\nheuristics and metaheuristics [33]. Their appeal stems from the flexibility of the RL framework to\naddress a wide variety of problems, requiring merely that they can be formulated as MDPs. The most\nrelevant works in this area treat routing and search problems over graphs.\nOne of the first instances of applying RL to such scenarios is the adaptive Q-routing algorithm [34].\nThe authors formulated the MDP with states as a tuple of the current node and destination node. They\nused a tabular Q-learning algorithm [35] where agents at each node on a graph maintain a lookup\ntable of routing time estimates for neighbor - destination pairs. This method allows for adaptation\nin networks with changing topology using dynamic updates and does not require a central routing\ncontroller. Several extensions and variations on this idea have been explored [36, 37], but all suffer\nfrom the main pitfall of tabular RL methods: poor scalability.\nInterest in learning to route has been reignited recently by several works that employ function\napproximation for better scalability. Notably, Graph Neural Networks (GNNs) [38] have emerged\nas a suitable learning representation for allowing RL policies to generalize to similar yet unseen\nconditions and graph topologies. In this line of work, Valadarsky et al. [39] considered learning\nrouting policies on a dataset of historical traces of pairwise demands and applying them in new traffic\nconditions. The MDP is framed as learning a set of edge weights from which the routing strategy is\ndetermined. More recent work by Almasan et al. [40] leveraged a GNN representation trained using a\npolicy gradient algorithm. They frame actions as the choice of a middle-point for a given flow given\nstart and target nodes, with previous action choices becoming part of the state.\nAnother important line of work studies how to perform search on graphs. In contrast to routing, for\nsearch tasks there is no notion of a link load associated with traversing a particular node or edge in\nthe graph. Notable contributions in this direction include work by P\u00e1ndy et al. [21], where RL agents\nare tasked with learning a heuristic function for augmentation of A* search.\nThe problem of knowledge graph completion may also be viewed as graph traversal in instances with\nheterogeneous edge types [41] and with a target node that is not specified a priori. Das et al. [22]\nproposed an MDP formulation of this task, in which an agent chooses the next relationship to traverse\ngiven the current node. A proportion of the true relationships in the knowledge graph is masked and\nused to provide the reward signal for training the agent via REINFORCE. The M-Walk method [23]\nbuilds further in this direction by leveraging the determinism of the transition dynamics. Therefore,\ntraining with trajectories from a Monte Carlo Tree Search (MCTS) policy [42] can overcome the\nreward sparsity associated with the random exploration of model-free methods.\nLastly, we note that, while the works reviewed in this section share features of our MDP and model\ndesign, none are directly applicable to the problem formulation. Chiefly, we consider a decentralized\ngraph path search scenario in which each agent has only partial visibility of the network."}, {"title": "3 Methods", "content": "In this section, we first introduce our decentralized mathematical formulation of the graph search\nproblem. Next, we describe the proposed multi-agent reinforcement learning algorithm, which\nleverages learnable graph embeddings."}, {"title": "3.1 MDP Formulation", "content": ""}, {"title": "3.1.1 State & Action Space", "content": "We frame the search problem as a Decentralized Partially Observable Markov Decision Process\n(Dec-POMDP) [43] taking place over an attributed, undirected, and unweighted graph structure\n$\\mathcal{G} = (\\mathcal{V}, \\mathcal{E})$ with n nodes and m edges. An agent is placed on each node $u_i \\in \\mathcal{V}$ in the graph, while\nthe edges $\\mathcal{E}$ indicate direct bidirectional communication links between agents. An attribute vector\n$x_{u_i} \\in \\mathbb{R}^d$ is associated with each agent. The aim is to find a path starting from an initial node $u_{src}$ to\na designated target node $u_{tgt}$.\nAt each timestep t of an MDP episode, a node $u_i$ receives a message $m \\in \\mathbb{R}^d$ specifying the attributes\n$x_{u_{tgt}}$ of the target node (but not its identity). It chooses as an action one of its neighbours, denoted\n$\\mathcal{N}(u_i)$, to pass the message on to. All other agents take no action at this step. We denote the presence\nor absence of a message at a given node $u_i$ at time t with the binary indicator $M_t^{(i)}$.\nTo ensure MDP stationarity, the state of the receiver node at each step is conditioned on the current\nepisode's target node with variable $U_{tgt}$. We emphasize that this is a technical detail required to define\nthe MDP, and the agent does not have visibility of the location of the target node beyond the attributes\ngiven in the message. The states and actions are therefore defined as:"}, {"title": "3.1.2 Observation Space", "content": "In accordance with our motivations, we provide agents with only local observations of the graph\ntopology. Concretely, we equip each agent $u_i$ with observations of 1-hop ego subgraphs $G_{u_i}$\ncentered on its neighboring nodes, including visibility of pairwise edges between 1-hop neighbors.\nSymmetrically, the transmitted message comprises an ego graph centered on the target node. More\nformally, the observation provided to agent i at time t is defined as:"}, {"title": "3.1.3 Transitions and Rewards", "content": "If an action is permitted, the message moves deterministically to the selected node, updating states\naccordingly:\nThe episode ends when the message reaches the target node, yielding a collective reward of +1 for the\nagents and terminating the episode. Additionally, to prevent agents from becoming stuck in action\ncycles, we introduce a truncation criterion: after $T_{max}$ interaction steps with the environment, the\nepisode ends without rewarding the agents. More formally:"}, {"title": "3.2 Model Description", "content": "In our design, we employ the common multi-agent Reinforcement Learning paradigm of Centralized\nTraining with Decentralized Execution (CTDE) [44]. We consider a fully collaborative setting in\nwhich the agents are all rewarded if messages are successfully delivered to the target node. The\ncollaborative objective is formulated such that each agent selecting the optimal next action results in\nan optimal trajectory through the graph. Therefore, the optimal trajectory can be constructed in a\ndecentralized manner.\nAs it is common in the CDTE paradigm, we utilize parameter-sharing across agent networks. In the\ntraining scheme, a centralized agent receives localized observations from individual agents at each\nstep, and is tasked with selecting optimal actions in the search path. The optimal decision is first\nlearnt, and then replicated and distributed to individual nodes at execution time.\nAt each training step, a central agent is given incomplete observations and receives sparse and\ndelayed rewards from the environment. Given these specifications, we propose the use of a variant of\nthe Advantage Actor-Critic (A2C) algorithm [45] to promote adequate exploration with acceptable\nsample efficiency. The A2C value network is also learned in a centralized fashion to guide the training\nof the policy network. Learning a stochastic policy (rather than a deterministic one) is important\nfor the problem under consideration given that a short path to the target may not be available via\na particular neighbor despite a high level of attribute similarity. Lastly, we incorporate entropy\nregularization to ensure the policy maintains a high degree of randomness while still aiming to\nmaximize the expected discounted return."}, {"title": "3.2.1 Policy Design", "content": "We formulate the choice of neighbor to which the message should be transmitted based on values\noutput by an MLP-parameterized policy network $f_\\pi$. The policy network is applied for each neighbor\n$u_j \\in \\mathcal{N}(u_i)$ of the node $u_i$ that is currently in possession of the message at time t, and the SoftMax\nfunction is used to derive a probability distribution. Concretely, the policy is defined as:"}, {"title": "3.2.2 GARDEN", "content": "Recall the \"hidden metric\" hypothesis discussed in Section 2.1, which posits that a viable policy can\nbe motivated by moving through the graph to reduce node distance, provided a good approximation\nof the underlying metric is obtained. Instead of prescribing that the raw node attributes should be\nused to approximate this metric, we propose that relevant node features, which capture the potentially\ncomplex interplay between attributes and topologies, can be learned. To do so, we suggest replacing\nraw node attributes with learned embeddings $x_{GAT}^{(i)}$ obtained from a Graph Attention Network (GAT)\n[25]. These embeddings are computed via a message-passing procedure starting from the raw node\nattributes and the graph topology.\nThe method, which we refer to as Graph Attention-guided Routing for DEcentralized Networks\n(GARDEN), is shown using pseudocode in Algorithm 1. The parameters of $f_{rep}$ are trained implicitly\nas we take gradient descent steps over the combined episodic loss $\\Sigma_{t} L_{\\pi}(t) + L_{v}(t)$. The node\nembeddings are recalculated at the start of each episode. The notation $[[\\cdot]]$ denotes the partial stopping\nof gradients, and $H(p)$ denotes the entropy of a discrete distribution, given by $\\sum_{i} p_i \\log(p_i)$."}, {"title": "4 Experimental Setup", "content": ""}, {"title": "4.1 Datasets", "content": "To assess the performance of decentralized graph strategies on real-world data,\nwe consider several ego graphs from the Facebook social network [46] present in the SNAP [47]\nrepository. These graphs depict individuals and their Facebook friendships. Each node is equipped\nwith binary, anonymized attributes collected through surveys. Due to computational budget con-\nstraints, we select the largest connected components of 5 graphs such that they have between 100 and\n600 nodes. High-level descriptive statistics for these graphs are presented in Table 4 in the Appendix.\nWe additionally consider synthetically generated graphs that are both attributed\nand display homophily. This allows for the creation of a diverse range of graphs with varying degrees\nof sparsity, enabling evaluations under different synthetic conditions. We follow the generative\ngraph construction procedure proposed by Kaiser and Hilgetag [48], which samples node attributes\nuniformly from a unit box $[0, 1]^d$ and creates edges stochastically according to the rule $p((u, u') \\in\n\\mathcal{E}) = \\max(1, \\beta e^{-\\alpha ||x_u - x_{u'}||^2})$, where $\\alpha$ and $\\beta$ are scaling coefficients."}, {"title": "4.2 Baselines", "content": "The learned baselines we use are the MLPA2C and MLPA2CWD techniques as introduced in\nSection 3.2.1. Furthermore, we consider a suite of heuristic baselines that utilize homophily and\ngraph structure for graph path search. The simplest baseline, GreedyWalker, selects the next node\ngreedily based on the smallest Euclidean attribute distance: $\\pi(u'|u) = \\text{argmin}_{u'\\in\\mathcal{N}(u)}||x_{u'} - x_{u_{tgt}}||^2$.\nGiven that deterministic policies may result in action loops, we generalize this to a stochas-\ntic agent (DistanceWalker) that acts via a SoftMax policy over attribute distances with a tuned\ntemperature parameter:"}, {"title": "4.3 Model Evaluation & Selection", "content": "For evaluating models, we consider the following metrics:\n1. Mean Oracle Ratio $R_{oracle}$: the ratio between episode length and the shortest path length averaged\nover all source-destination pairs;\n2. Truncation Rate $R_{trunc}$: % of episodes exceeding the truncation length $T_{max}$;\n3. Win Rate $R_{win}$: % of episodes where a given agent obtains the relative shortest path length, with\nties broken randomly.\nTo mitigate potential memorization of routes during training, especially when nodes are uniquely\nidentifiable based on attributes, we partition the node set $\\mathcal{V}$ into three disjoint groups: $\\mathcal{V}_{train}, \\mathcal{V}_{val}$,\nand $\\mathcal{V}_{test}$ at ratios of 80%/10%/10%. The source node $u_{src}$ is sampled uniformly at random from $\\mathcal{V}$,\nwhile the target node $u_{tgt}$ depends on whether training, validation, or testing is performed. For training,\nwe always sample a \"fresh\u201d source-target pair, while for validation and evaluation the source-target\npairs are serialized and stored (such that the performance evaluated over them is consistent). The\nMean Oracle Ratio $R_{oracle}$ is used as the primary metric for model validation and evaluation."}, {"title": "4.4 Sensitivity Analysis of Graph Density Parameter", "content": "Given a constant graph size, reduced graph density diminishes available paths to a target [49]. This\nintensifies exploration challenges and heightens the risk of truncated episodes, yielding sparser\nreward signals in training. Motivated by this rationale, we assess GARDEN across a set of\ngenerated graphs with diverse sparsity levels. We randomly generate 10 graph topologies for\n$\\beta \\in \\{0.01, 0.05, 0.1, 0.2, 0.3, 0.4, 0.5, 0.75, 1.0\\}$ with number of nodes n = 200 and $\\alpha$ = 30. We\ntrain GARDEN separately for each value of $\\beta$ and gauge its performance against the baselines."}, {"title": "4.5 Ablation of Node Representation", "content": "We assess our GNN-based model against alternative designs through an ablation study on synthetic\ngraphs. Using five random seeds and fixed graph parameters (n = 200, $\\alpha$ = 30, $\\beta$ = 5), we conduct\nexperiments on our three model designs: the MLPA2C model using only the raw node attributes x, the\nMLPA2C variant incorporating both node attributes and degrees $x_{WD}$, and the proposed GNN-based\nGARDEN method, which employs learned graph embeddings $x_{GAT}$."}, {"title": "5 Experimental Results", "content": ""}, {"title": "5.1 Facebook Graphs", "content": "As shown in Table 1, we find that GARDEN significantly outperforms baselines across all the\nreal-world datasets and metrics we have tested on. Given the variety of attribute dimensions and\ndensities, as displayed in Table 4 in the Appendix, we may argue that in graphs with high amounts of\nlatent structure, our model is robust to these factors.\nIn Figure 3 in the Appendix, we visualize the value function learned by GARDEN on these social\nnetwork ego graphs. This highlights that the values obtained by GARDEN serve as a reliable proxy\nfor graph distance, assigning highest values to nodes in the target's cluster or clusters with strong\nconnectivity to the target's community. Furthermore, it demonstrates the interpretability of the\nproposed technique for graph path search."}, {"title": "5.2 Sensitivity Analysis of Graph Density and Temperature Parameters", "content": ""}, {"title": "6 Conclusions and Future Work", "content": "In this paper, we have considered the problem of decentralized search in graphs, which is motivated\nby privacy, scalability, and dynamicity requirements of many network modeling scenarios. Despite\nthe lack of a central view of the network, the homophily and community structure observed in"}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Implementation Details", "content": "In a future version, we will release code that enables reproducibility of the results presented in\nthis work. This will include instructions on how to set up the dependencies, download the publicly\navailable data, and run the methods.\nWe train our models using the Adam optimizer [50] for 200, 000 episodes, evaluating performance\nevery 100 episodes on the serialized validation set. Early stopping is applied based on the validation\nloss $R_{oracle}$. Unless otherwise stated, we train and evaluate models over 10 random seeds, reporting\nconfidence intervals where appropriate. Table 3 presents the hyperparameter configuration shared\nacross the three model designs. We fix $\\gamma$ = 0.99 and the maximum episode length $T_{max}$ = 100. Lastly,\nwhen providing node input features to the GAT, we append a binary indicator variable that signals\nthat a particular node u is the center of the ego graph to the raw attributes defined as $[x_w || I[w = u]]$.\nThis is used to distinguish the node from which the message must be sent."}, {"title": "A.2 Additional Tables and Figures", "content": "In Figure 3, we plot GARDEN's learned value function across the social network ego graphs.\nBrighter colors indicate a higher estimated value function relative to the target node, which is\nindicated with a black arrow and chosen randomly from the respective test sets. For comparison, we\nalso plot the implicit preferability score $-\\||x_u - x_{u_{tgt}}||^2/\\tau$ generated by the best-performing baseline,\nDistanceWalker, for the same source-target pairs. DistanceWalker struggles with Euclidean pairwise\nattribute distance due to high dimensionality and sparsity of node attributes. Conversely, values\nobtained by GARDEN serve as a reliable proxy for graph distance, assigning highest values to nodes\nin the target's cluster or clusters with strong connectivity to the target's community."}]}