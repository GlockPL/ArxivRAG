{"title": "Cross-lingual Back-Parsing: Utterance Synthesis from Meaning Representation for Zero-Resource Semantic Parsing", "authors": ["Deokhyung Kang", "Seonjeong Hwang", "Yunsu Kim", "Gary Geunbae Lee"], "abstract": "Recent efforts have aimed to utilize multilingual pretrained language models (mPLMs) to extend semantic parsing (SP) across multiple languages without requiring extensive annotations. However, achieving zero-shot cross-lingual transfer for SP remains challenging, leading to a performance gap between source and target languages. In this study, we propose Cross-lingual Back-Parsing (CBP), a novel data augmentation methodology designed to enhance cross-lingual transfer for SP. Leveraging the representation geometry of the mPLMs, CBP synthesizes target language utterances from source meaning representations. Our methodology effectively performs cross-lingual data augmentation in challenging zero-resource settings, by utilizing only labeled data in the source language and monolingual corpora. Extensive experiments on two cross-lingual SP benchmarks (Mschema2QA and Xspider) demonstrate that CBP brings substantial gains in the target language. Further analysis of the synthesized utterances shows that our method successfully generates target language utterances with high slot value alignment rates while preserving semantic integrity.", "sections": [{"title": "Introduction", "content": "Semantic Parsing (SP) is the task of converting natural language utterances into meaning representations such as SQL or Python code. With numerous English parsing datasets available, recent studies have enabled applications ranging from natural language interfaces for databases to code generation (Le et al., 2022; Li et al., 2023). Despite SP's practicality, extending it beyond English is challenging. Manually annotating examples for other languages is very costly, and relying on machine translation is often impractical due to the complex slot alignment step after translation (Nicosia et al., 2021).\nRecent studies focus on leveraging multilingual pretrained language models (mPLMs) (Devlin et al., 2019; Xue et al., 2021) to extend SP across multiple languages without costly annotations (Sherborne and Lapata, 2022; Held et al., 2023a). After being pretrained on large-scale non-parallel multilingual corpora, mPLMs demonstrate strong zero-shot cross-lingual transferability: Once these models are fine-tuned with labeled data from the source language, they show remarkable performance in target languages without using any labeled data from the target language. Nonetheless, zero-shot cross-lingual transfer for SP is still challenging for state-of-the-art multilingual models, resulting in a notable performance gap between the source and target languages (Ruder et al., 2021).\nTo this end, we propose Cross-lingual Back-Parsing (CBP), a novel data augmentation methodology for enhancing zero-shot cross-lingual transfer for SP. CBP is designed to be widely applicable by synthesizing target utterances from source meaning representations under zero-resource settings where resources such as translators, annotated examples, and parallel corpora in target languages are unavailable. As shown in Figure 1, CBP comprises two components: an utterance generator synthesizing utterances in the target languages and a filtering mechanism discarding low-quality utterances.\nTo synthesize target utterances in the zero-resource setting, the utterance generator leverages a multilingual pretrained sequence-to-sequence (seq2seq) model such as mT5 (Xue et al., 2021) with modular language-specific adapters (Houlsby et al., 2019; Pfeiffer et al., 2020) inserted into the decoder. To enable the model to generate output text different from the input, we design a novel source-switched denoising objective for training the language adapters, leveraging findings (Yang et al., 2021) that the language identity component can be extracted from contextualized representations of mPLMs. Using unlabeled target language sentences, we train the adapters to denoise input sentences from encoded representations with their language identity switched to the source language. This allows the adapters to control the output language of the utterance generator during inference.\nWe then synthesize target utterances from the source meaning representations using the utterance generator equipped with the target language adapters. This process effectively performs data synthesis to create new target language utterances, serving as data augmentation. Finally, we filter these synthesized utterances to discard low-quality ones using a filtering mechanism inspired by round-trip consistency (Alberti et al., 2019), thereby enhancing the quality of the augmented dataset.\nWe assess the efficacy and robustness of CBP on two challenging cross-lingual SP benchmarks, Mschema2QA (Zhang et al., 2023) and Xspider (Shi et al., 2022), encompassing a total of 11 languages. In the Mschema2QA benchmark, CBP notably improves the average exact match by 3.2 points. Utilizing solely monolingual corpora for data augmentation, CBP surpasses all baselines that rely on translator-based data augmentation. For the Xspider benchmark, CBP exceeds the state-of-the-art, improving the exact match for Chinese from 52.7 to 54.0. Extensive analyses substantiate the effectiveness of our methodology. Further investigations into synthesized utterances indicate that CBP successfully generates utterances in the target languages high slot value alignment rates while moderately preserving semantic integrity, despite the absence of parallel corpora."}, {"title": "Related Work", "content": "Zero-shot cross-lingual semantic parsing Zero-shot cross-lingual SP aims to transfer parsing capabilities from a high-resource language (e.g., English) to low-resource languages without requiring any training data in the low-resource languages. To enhance cross-lingual transfer, several studies introduce auxiliary objectives during training to improve the alignment of semantic spaces between languages (Sherborne and Lapata, 2022; Held et al., 2023b). Our method, however, aligns with a different line of research: data augmentation. Xia and Monti (2021) utilize machine translation to convert English datasets into target languages, followed by word aligners to match corresponding elements, whereas Nicosia et al. (2021) directly generate aligned datasets using a fine-tuned model. Although not in the zero-shot setting, some works prompt large language models (LLMs) to generate synthetic data in the target language, using a few examples in the target language (Rosenbaum et al., 2022; Awasthi et al., 2023). In contrast, our research addresses data augmentation in a relatively unexplored zero-resource setting, where no target language data, translators, or parallel corpora are available. Our approach leverages multilingual pretrained language models and monolingual corpora in the target language for augmentation, ensuring effective cross-lingual transfer without such resources.\nMultilingual language models Research on the representation geometry of multilingual pretrained language models (mPLMs) has revealed that the encoder representations of these models possess a shared multilingual representation space while still encoding language-specific information. A study by Libovick\u1ef3 et al. (2019) shows that subtracting the language mean from representations enhances cross-lingual transfer by inducing language-agnostic representations. Additionally, Chang et al. (2022) demonstrates that projecting representations onto language-specific subspaces can facilitate token predictions in specific languages. Leveraging these findings, Yang et al. (2021) enhances cross-lingual retrieval performance by removing language information from multilingual representations, while Deb et al. (2023) improves cross-lingual question answering by projecting source representations onto target language subspaces during fine-tuning. The study most closely related to ours is by Wu et al. (2022), which enhances"}, {"title": "Methodology", "content": "Overview In this study, we synthesize target language utterances $u_{tgt}$ from source language meaning representation $mr_{src}$ to enhance the performance of SP models that convert $u_{tgt}$ into $mr_{tgt}$. CBP consists of two components: (1) an utterance generator (Section 3.1) that synthesizes utterances in the target languages from source mr; (2) a filtering mechanism (Section 3.2) that discards low-quality synthesized utterances. To train the models in each step, we utilize SP datasets in the source language and monolingual corpora in both the source and target languages.\nThe utterance generator, which utilizes a seq2seq Transformer (Vaswani et al., 2017) as its backbone, is trained to generate $u_{src}$ from the input $mr_{src}$, and subsequently generates $u_{tgt}$ from $mr_{src}$ during inference. To achieve this, the model must be capable of generating utterances in different languages from the same meaning representations. Therefore, we introduce a language identity switch operation and a language-specific adapter to control the language of the generated utterances.\nThe language identity switch operation alters the encoder output representation of the generator to reflect the source language identity, ensuring that the generator's decoder always receives the encoder representation with the source language identity regardless of the input language. We then train the utterance generator to produce output sequences in the target language using the modified encoder representation, while integrating language-specific adapters (Houlsby et al., 2019) into the Transformer decoder. This training enables the adapter to prompt the generator to produce utterances in different languages while maintaining the same meaning from a given representation.\nThen, we remove low-quality data from the synthesized utterances using the filtering mechanism. By re-parsing the generated utterances, we measure round-trip consistency (Alberti et al., 2019) to determine whether it accurately maps back to the input meaning representation used during generation. This data filtration process improves the quality of the synthesized data."}, {"title": "Utterance generator", "content": "Architecture We construct the utterance generator using a multilingual pretrained seq2seq model, such as mT5 (Xue et al., 2021), as its backbone. To control the output languages, we integrate a language-specific adapter into each decoder block of the generator, positioning it immediately after the feed-forward layers. These adapters are lightweight bottleneck feed-forward layers that enable the generator to adapt to specific languages by learning modular representations (Pfeiffer et al., 2020; Parovi\u0107 et al., 2022).\nTraining language adapters As illustrated in Figure 2a, we initially train the language adapters using monolingual corpora for each language, respectively. Each language adapter is updated through a denoising task, where the utterance generator reconstructs randomly masked sentences into their original forms. During this training process, the model learns solely from data where the input and output sequences share the same language. However, during the data synthesis step, the model is required to generate an output sequence in the target language ($u_{tgt}$) when provided with an input sequence in the source language ($mr_{src}$). When we train the adapter with a conventional denoising objective (Lewis et al., 2020; \u00dcst\u00fcn et al., 2021), this mismatch leads to failure in synthesizing utterances in target languages (Figure 4). To mitigate this language mismatch in the zero-resource setting without parallel corpora, we propose a novel source-switched denoising objective to train the adapters, leveraging the representation geometry of mPLMs.\nPrevious studies (Libovick\u1ef3 et al., 2020; Yang et al., 2021) have shown that the representation of mPLMs can be decomposed into language-specific and language-neutral components, which respectively capture language identity and semantic information. Inspired by this property, we switch the language identity of input sequences to the source language during the denoising task to prevent the model from determining the output language based on the input language. Following Libovick\u1ef3 et al. (2020), we estimate the language-specific component for language l as the language mean vector $\\mu_l$. We compute $\\mu_l$ as the mean of 1M contextualized token representations obtained from the encoder of the utterance generator, using a set of sentences from the monolingual corpora $C_l$.\nDuring the training of the language adapter $A_l$, a masked sentence $g(s_l)$ in language l is fed into the encoder Enc of the utterance generator and encoded into a representation. We then modify the language-specific component of the encoded representation to the source language using the language identity switch operation $\\Phi$. Formally, the operation is defined as:\n$\\Phi(Enc(g(s_l))) = Enc(g(s_l)) - \\mu_l + \\mu_{src}$\nwhere $\\mu_{src}$ is a language mean vector of the source language. This operation maintains the semantic equivalence of the representation while changing its identity to the source language (Figure 3).\nThe language-specific adapter learns to map input sentences from the source language to sentences in each target language while preserving the meaning, using the source-switched denoising objective. Initially, sentences are distorted using a noise function g, which replaces consecutive spans of the input sentence with a mask token. The decoder then reconstructs the original sentence based on the encoder representation with the language identity switched to the source language. For each language l, language adapter $A_l$ is separately trained to minimize $L_{A_l}$:\n$L_{A_l} = \\sum_{s_l \\in C_l} -logP(s_l | \\Phi(Enc(g(s_l))); A_l)$\nwhere $s_l$ is a sentence belonging to monolingual corpora $C_l$ of language l. All utterance generator parameters are frozen during the training except those of the adapter.\nWhile we focus on training adapters in this work, these source-switched denoising training strategies can potentially be applied to other modular methods such as LoRA (Hu et al., 2022). We chose to focus on adapters for two main reasons: (1) they generally show better performance compared to other modular methods given the same size of trainable parameters (He et al., 2022), and (2) the literature background on their usage for cross-lingual transfer (Pfeiffer et al., 2020, 2023)."}, {"title": "Fine-tuning Utterance generator", "content": "After training language adapters for each language, we finetune the utterance generator to synthesize $u_{src}$ from $mr_{src}$ using labeled data in the source language, as shown in Figure 2b. This process involves integrating the source language adapter into the decoder and selectively freezing other layers of the utterance generator and the adapter to prevent catastrophic forgetting."}, {"title": "Synthesizing target utterances", "content": "After finetuning the utterance generator, we synthesize $u_{tgt}$ from $mr_{src}$. For each $mr_{src}$ in the labeled data, we generate $u_{tgt}$ across various target languages by incorporating the corresponding language-specific adapter $A_{tgt}$ into the decoder."}, {"title": "Filtering mechanism", "content": "To filter out low-quality synthesized utterances, we propose a filtering mechanism inspired by roundtrip consistency (Alberti et al., 2019). We fine-tune the same backbone model for the utterance generator for the SP task using only labeled data from the source language. For each target language utterance $u_{tgt}$ initially generated from $mr_{src}$, the trained SP model predicts its corresponding meaning representation $mr_{pred}$. We use the set $(u_{tgt}, mr_{src})$ where $mr_{src}$ exactly matches $mr_{pred}$ to ensure that the synthesized $u_{tgt}$ preserves the meaning of the $mr_{src}$."}, {"title": "Experimental Settings", "content": "To evaluate whether our methodology generalizes across different languages and meaning representations, we assess our methods on two cross-lingual SP datasets: Mschema2QA (Zhang et al., 2023) and Xspider (Shi et al., 2022). Examples of each dataset are presented in Table 1.\nMschema2QA is a question-answering dataset over schema.org web data that pairs user utterances with meaning representations in the ThingTalk Query Language. The dataset contains 8,932 training and 971 test examples, each available in 11 languages. Using English as the source language, we evaluate our model on the test split across 10 target languages.\nXspider is a cross-domain text-to-SQL dataset that pairs user utterances with SQL queries. We train our model on the English Spider dataset(Yu et al., 2018) consisting of 7,000 training examples and evaluate on the Chinese (Min et al., 2019) and Vietnamese (Nguyen et al., 2020) dev split. We did not assess Farsi and Hindi as they are not publicly available.\nMonolingual corpora We create unlabeled monolingual corpora $C_l$ for each language l by extracting 1 million sentences from the November 20, 2023, Wikipedia dump in the respective language. We extract the raw article texts from the dump using WikiExtractor (Attardi, 2015) and split them into sentences using BlingFire (Microsoft, 2020)."}, {"title": "Implementation details", "content": "We use the multilingual pretrained seq2seq model mT5-large (Xue et al., 2021) as the backbone for our SP model and utterance generator. The synthesized datasets for Mschema2QA and Xspider contain 49.4k and 8.2k examples, respectively. We train the model in a single stage using these synthesized datasets along with the labeled data in the source language ($D_{src}$), which is English. Employing AdamW (Loshchilov and Hutter, 2017) optimizer, we train the SP model for 50 epochs on both datasets, with a batch size of 32 and a learning rate of 3e-5. Appendix A.1 has further details."}, {"title": "Baselines", "content": "As the datasets have been proposed recently, few prior results are available in the literature. Therefore, we developed several strong baselines that do not use labeled datasets in target languages. All baselines, except those using LLM, utilize mT5-large as the backbone model.\nTranslation-Based Baselines For Translate-Test, we use Google Translate (Wu et al., 2016) to convert the target language test set into English and then input it into the model trained only with $D_{src}$. In Translate-Train, $D_{src}$ is translated into all target languages using machine translation (MT), and a model is trained on this data. For TAP-Train, we translate utterances from $D_{src}$ into all target languages with MT. Then, we use representative neural word aligners - awesome-align (Dou and Neubig, 2021) - to align utterances with values from meaning representations, constructing a dataset to train a multilingual parser. In TAP-Train + source, we supplement the dataset from TAP-Train with $D_{src}$ to train the model.\nIn-Context Learning with Multilingual LLMS We use gpt-3.5-turbo for in-context learning. The prompt is constructed by appending English examples and an utterance from the evaluation dataset, with eight examples for Mschema2QA and one for Xspider to meet input limits. For Xspider, we additionally compare against the state-of-the-art method that uses LLM, DE-R2+Translation-P (Shi et al., 2022).\nZero-Resource Baselines For Zero-shot, we train a model using the English-labeled dataset $D_{src}$ only. In word translation, inspired by Zhou et al. (2021), we create an augmented dataset by replacing words in English utterances from $D_{src}$ with their counterparts in the target language, using bilingual dictionaries from MUSE (Conneau et al., 2017). To preserve alignment between the meaning representation and the utterance, we only replace words that are not part of the values. Models are trained using both $D_{src}$ and the word-replaced dataset across target languages. For reconstruction, inspired by Maurya et al. (2021), we train an SP model with an auxiliary task of reconstructing input from noisy data using unlabeled corpora across target languages. This reconstruction objective aims to enrich the cross-lingual latent representation space across languages."}, {"title": "Evaluation metrics", "content": "We measure Exact Match (EM) accuracy for the Mschema2QA and XSpider datasets. Additionally, we report Test-suite (TS) accuracy for the XSpider dataset following Zhong et al. (2020). Each score is averaged over three runs with different random seeds."}, {"title": "Results and Analysis", "content": "In Tables 2 and 3, we compare the performance of CBP against competitive baselines on the Mschema2QA and Xspider benchmarks. CBP improves the average EM score on Mschema2QA by 3.2%, with significant improvements of 8.8% in Turkish and 5.0% in German, compared to the zero-shot method without data augmentation. Similarly, on Xspider, our method enhances Chinese performance by 4.7% in EM and 3.8% in TS. The filtering mechanism proves essential for our method, as evidenced by the significant drop in performance in its ablation (w/o filtering). Remarkably, despite operating under the zero-resource setting, our method outperforms all baseline models on the Mschema2QA dataset and even surpasses DE-R2+Translation-P, the state-of-the-art in the literature on the Xspider dataset. These results highlight the effectiveness and practicality of CBP in cross-lingual SP.\nAdditionally, we find that gpt-3.5-turbo exhibits different performance trends on the two datasets. On Mschema2QA, gpt-3.5-turbo performs poorly, indicating that in-context learning with English examples alone is insufficient to learn the dataset's domain-specific grammar. This highlights the practicality of zero-shot cross-lingual transfer through fine-tuning. Conversely, on Xspider, where the model has pre-trained knowledge about text-to-SQL (Liu et al., 2023), gpt-3.5-turbo shows strong performance, surpassing ours in TS. However, our backbone model, mT5-large (1.2B parameters), is notably more parameter-efficient and cost-effective than gpt-3.5-turbo.\nSlot value alignment One key challenge in cross-lingual data augmentation for SP is aligning slot values between the utterance and the meaning representation. Compared to translation-based baselines, we measure the slot value alignment rate of augmented data synthesized by CBP. The alignment"}, {"title": "Quality of synthesized utterances", "content": "We evaluate the translation quality between the synthesized utterance from CBP and the English utterance paired with the meaning representation for the synthesized utterance. We assess the quality only for the synthesized utterances that were identified as being in the target language by the language detection API. We employ GEMBA-stars (Kocmi and Federmann, 2023), a state-of-the-art GPT-based metric that assesses translation quality on a one-to-five-star scale through zero-shot prompting. Figure 5 shows the star distribution for synthesized utterances across all languages on Mschema2QA. We find that the majority of utterances fall within the two to four-star range, indicating similar meaning to some degree. This suggests that our method not only adjusts the synthesized utterances' language but also preserves their meaning to some extent."}, {"title": "Conclusion", "content": "We present Cross-lingual Back-Parsing (CBP), a novel data augmentation methodology aimed at enhancing zero-shot cross-lingual transfer for semantic parsing. Leveraging the representation geometry of multilingual pretrained language models, our method enables data augmentation in zero-resource settings. Our experiments on two cross-lingual semantic parsing benchmarks demonstrate that CBP significantly improves performance, underscoring its effectiveness and practical applicability. While we focus on semantic parsing, we believe that CBP has the potential to be applied to other cross-lingual generation tasks in zero-resource settings. Future work will investigate the application of our method to tasks such as cross-lingual text style transfer (Krishna et al., 2022)."}, {"title": "Limitations", "content": "Our proposed methodology, CBP, synthesizes target language utterances from source meaning representations by leveraging the representation geometry of mPLMs. Although we have demonstrated that CBP can effectively synthesize target utterances while preserving semantics, our experiments were conducted using only one mPLM (mT5-large). Validating our methodology with mPLMs of different parameter sizes and pretraining objectives would further demonstrate its generalizability.\nAdditionally, while we demonstrated that our approach is beneficial even when the available monolingual corpora are small in size (Figure 6; applicable to actual low-resource language settings), we couldn't experiment on actual low-resource languages due to the limited natural language coverage of current semantic parsing datasets (Zhang et al., 2023). Evaluating our methodology on actual low-resource languages could further verify its effectiveness. Finally, our methodology is less effective in synthesizing data when the zero-shot task performance is low. This indicates that our approach may not be effective for mPLMs with lower inherent performance, such as small-sized models. Future work could focus on improving our methodology to enhance performance even in these challenging scenarios."}, {"title": "Utterance generator", "content": "Architecture We construct the utterance generator using a multilingual pretrained seq2seq model, such as mT5 (Xue et al., 2021), as its backbone. To control the output languages, we integrate a language-specific adapter into each decoder block of the generator, positioning it immediately after the feed-forward layers. These adapters are lightweight bottleneck feed-forward layers that enable the generator to adapt to specific languages by learning modular representations (Pfeiffer et al., 2020; Parovi\u0107 et al., 2022)."}]}