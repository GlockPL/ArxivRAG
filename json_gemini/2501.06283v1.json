{"title": "Dafny as Verification-Aware Intermediate Language for Code Generation", "authors": ["Yue Chen Li", "Stefan Zetzsche", "Siva Somayyajula"], "abstract": "Using large language models (LLMs) to generate source code from natural language prompts is a popular and promising idea with a wide range of applications. One of its limitations is that the generated code can be faulty at times, often in a subtle way, despite being presented to the user as correct. In this paper, we explore ways in which formal methods can assist with increasing the quality of code generated by an LLM. Instead of emitting code in a target language directly, we propose that the user guides the LLM to first generate an opaque intermediate representation, in the verification-aware language Dafny, that can be automatically validated for correctness against agreed on specifications. The correct Dafny program is then compiled to the target language and returned to the user. All user-system interactions throughout the procedure occur via natural language; Dafny code is never exposed. We describe our current prototype and report on its performance on the HumanEval Python code generation benchmarks.", "sections": [{"title": "1 Introduction", "content": "Prompting large language models (LLMs) to generate code from natural language specifications has garnered significant attention from both academia and industry [2]. However, such code can either contain bugs or be counterintuitive when compared to the initial specification, despite being confidently presented as correct.\nFor example, a user might prompt an LLM to generate an efficient Python implementation of the Fibfib sequence (Figure 2), which is similar to the classic Fibonacci sequence. It differs in that each term is equal to the sum of the previous three terms instead of the previous two. In Figure 3 we present Python code that was generated by a state-of-the-art model in response. Despite seeming correct, it contains a subtle error that causes the code to return the wrong value for n > 2. To allow the LLM to recover from its mistake, we provided it with the observation that the test cases in the initial prompt were violated by its synthesised code. In certain instances, this was enough to obtain a correct implementation with its next response. Occasionally, however, it introduced a new bug and, worse, modified the provided\nIn this paper, we propose to utilise Dafny as an intermediate technology within a code-generating chatbot prototype on the way to higher quality mainstream-language code. Dafny is particularly well-suited for this purpose, because:\n1. It allows for a clear distinction between the specification of a program-derived from a natural language prompt using an LLM-and its implementation, which can again be dialogically derived using an LLM with verification feedback in the loop.\n2. The multiple backends supported by the Dafny compiler enable code generation in a variety of mainstream languages, which is ultimately presented to the user, while internally only requiring verification once.\nFor example, the Fibfib sequence described in (Figure 2) can be formalised in Dafny as the recursive function:\nThe synthesis problem faced by an LLM then can be specified as the one of finding an imperative implementation and proof hints to fill into the method below:\nIn the next section, we detail the architecture of the chatbot prototype and explain, in particular, how the use of Dafny itself can be and is opaque to the user. In the following sections, we present some initial benchmarks and the immediate next steps."}, {"title": "1.1 Related work", "content": "Dafny-guided code generation lies between traditional program synthesis and proof synthesis for math olympiad problems that systems like Google DeepMind's AlphaProof target for. Work exploring the role of prompt engineering and search-based approaches for the synthesis of Dafny code include that of Misu et. al. [4] and Branfonbrener et. al. [1]. Here, we utilise few-shot prompting to improve the quality of generated code. Other work leverages Dafny to catch incorrect code, e.g., by automatically synthesizing specifications to check for program correctness [3] [5]. Lastly, we utilize the idea of consistency between an implementation and its specifications as introduced by Clover [6]."}, {"title": "2 Current Prototype", "content": "In this section, we provide an overview of a prototype chatbot that is based on Claude Sonnet 3.5 but has been modified to leverage Dafny as an opaque intermediate language for the generation of correct Python code."}, {"title": "2.1 Higher quality code generation with Dafny", "content": "Figure 1 depicts the high-level structure of the prototype. We start on the left, with the natural language input of the user. The LLM formalises the prompt in Dafny and returns to the user a natural language translation of the Dafny program that captures the code synthesis task the LLM understands it is supposed to solve.\nThe user is able to give their feedback to the formalisation and the natural language conversation loops until both sides agree on a description of the task. This provides a transparent notion of correctness that we can validate a solution against.\nThe LLM then generates an attempt at a solution and a proof of its correctness. The Dafny verifier either accepts the candidate solution or rejects it and allows the LLM to refine it or start from scratch again. The process then continues in a loop until a solution appears. This step is completely automated.\nOnce the loop has converged, Dafny generates additional unit tests for the solution and then the prototype uses its compiler to translate the solution to the target language. In our case the target is Python, but it could also be Java, etc. The compiler output then may be further optimised and verified for equivalence.\nFinally, on the right, the user is presented a specification of the problem in the target language, an implementation, and unit tests. The correctness of the solution is implicit as it was present in Dafny and has been preserved with high assurance by the translation."}, {"title": "2.2 A demo run", "content": "To illustrate the interactive nature of the pipeline, we prompt the chatbot with a variation of Figure 2 that contains a typo: in the inductive step, we incorrectly write n-4 instead of n-3.\nAfter a few seconds, the modified chatbot responds with a natural language summary of the underlying Dafny specification of the problem it's been served. Dafny itself is never mentioned in the conversation. There is a slight delay that comes from a loop in the background that ensures only code that is run-time safe and verifiable is returned. At the end of the summary, the LLM points out a few key differences between the initial prompt and the Dafny specification, such as that the input must in fact be non-negative. Most interestingly, it presents a fourth base case fibfib(3) == 1 that it says has been implicit in the initial prompt. During the opaque synthesis of the problem specification in Dafny, the verifier seems to have realised that with the typo the recursive definition of fibfib is not well-defined. To solve the issue, the LLM then seems to have synthesised a missing base case, with which the program verified! While the new specification is well-defined, it does not represent our initial intent. Therefore, we respond to the summary by pointing out the typo in our initial prompt.\nThe LLM comes back with a new specification of the problem which now looks correct, so we accept it and continue with target language code generation. After a few moments, we are presented a Python programs that contains an imperative method fibfib, a recursive function fibfibFunc, and a Main method with user provided test cases as well as additional tests that check the equivalence between fibfib and fibfibFunc at runtime (Figure 4). Under the hood, the LLM has generated an intermediate Dafny program and a proof that it satisfies the agreed on specification, before translating it to Python. The returned Python program now looks good, so we skip on prompting further modifications. A transcript of the full conversation is given in Figure 9."}, {"title": "2.3 Comparison of native and compiled solutions", "content": "Figure 3 shows the Python code that was generated directly by an LLM without utilising Dafny in any way. It incorrectly returns fib_values[0] instead of fib_values[2].\nFigure 4 shows the Python program that was generated by our prototype with Dafny in the loop. The program consists of the following components:"}, {"title": "2.4 Overview of Dafny code generation strategies", "content": "Prompts provide rules for output generation. To help the LLM with generating a Dafny specification from natural language, we provide it with a system prompt (Figure 5). The prompt states the LLM's role on a high-level and defines its tasks. It specifies a format for the output and some constraints that enable the easy extraction of the generated code for further processing. To avoid running into common classes of errors, the prompt also instructs the LLM to use certain Dafny syntax, e.g. to use a custom Option datatype to deal with Python functions that could return None. Finally, we instruct it to translate user provided test cases into expect statements that can be checked at runtime.\nFew-shot prompts show code organization. Further guidance to the LLM is given through few-shot prompting (Figure 6), via multiple input and output pairs that exemplify the high-level instructions of the system prompt. The examples of output Dafny programs share a common structure: they contain a bodyless method that is prefixed with an explanatory docstring and annotated with the {: testEntry} attribute, as well as a Main method that provides a fixture for user-provided test cases.\nIterative improvement with Dafny feedback. The chatbot iteratively converges (automatically, i.e., without user intervention) on a verified Dafny implementation by feeding verifier feedback (errors and warnings) along with a generation task instance. In the process, the LLM not only generates the Dafny implementation code, but also the inline invariants and assertions required to achieve verification.\nPost-processing of generated Dafny code. After a Dafny implementation that correctly implements an agreed-on specification is generated, we must still post-process it to eventually get a target language implementation. Before we run the Dafny compiler on the generated code, we utilise Dafny's experimental generate-tests command to generate tests in addition to the ones that were provided by the user. The generated tests provide complete coverage of the method that was annotated with {: testEntry} (Figure 6). Dafny offers a choice between different notions of coverage: Block, Path, and InlinedBlock. We used block coverage after inlining (InlinedBlock), a call-graph sensitive version of block coverage. Whereas user provided tests tend to compare the output of a method to a concrete value (fibfib(1) == 0), generated tests will check the validity of an ensures clause at runtime. Often this results in the generated test comparing the output of a method indirectly in the same way as user-defined tests (fibfib(1) == fibfib_func(1)). To prevent Dafny at times generating excessively long inputs, we set the length-limit option to 512."}, {"title": "2.5 Overview of prototype interaction", "content": "Specification is refined via natural language. Initially, the user conversationally guides the LLM to an agreed Dafny specification of the code generation task (ref. the initial segment of Figure 9) by first specifying the input/output behavior of the program and some example input/output pairs in a format similar to the few-shot prompts. Below, we explain how this is achieved without actually revealing any Dafny code to the user.\nConsistency checks help validate translations. One of the main challenges with using Dafny as a purely opaque intermediate language for high-assurance code generation in a target language like Python is that the user and the LLM need to come to an agreement on a Dafny specification through natural language only. That is, we are aiming to prevent the leakage of Dafny specification and implementation code at all times during the chatbot interaction. To allow a discussion, the chatbot refers to natural language representation of internal Dafny code. To ensure that the natural language translation is, as much as possible, an unambiguous description of the Dafny internals, we employ a consistency check as described in [6]. In essence, a natural language description is consistent with a Dafny program, if it allows an LLM to reconstruct from it a second Dafny program that is equivalent to the first. We currently use an LLM to decide whether two Dafny programs are equivalent (rather than verifying their equivalence symbolically).\nPreserve opacity by filtering system outputs. To guide the chatbot towards falling back on the Dafny ecosystem if prompted with a Python code generation task, we provide it with a system prompt that outlines a number of steps to follow in such a case. In particular, we utilise Claude's function calling capability to point the chatbot at the right moment to externally implemented functionality that either synthesizes a Dafny specification or synthesizes an implementation that is correct with respect to a specification. To prevent intermediate Dafny representations from being surfaced to the user, we mark such calls with a specific tag and selectively hide its output from the user."}, {"title": "3 Benchmarking", "content": "We've evaluated our early prototype on HumanEval, which is a state-of-the-art benchmark set for Python code generation. On a set of 164 tasks, the prototype converged to a candidate solution in 144 cases, of which 127 passed all test cases. Overall, this leaves us with a pass rate of 77%, which is lower than the 86% for native Python code generation with Claude Sonnet 3.5. However, in a more realistic scenario one falls back on native Python code generation if the prototype does not converge. This would result in a pass rate of ~88%, a slight improvement over 86%.\nThe results are promising because there is almost no training data available for Dafny: at the time of writing, there are over 15 million repositories on GitHub that contain Python code, but only 409 repositories with Dafny code. It's worth noting that after moving from Claude 3.0 Sonnet to the newer model Claude 3.5 Sonnet, we saw a dramatic increase in the syntactic correctness of Dafny code generated, which previously accounted for a large class of errors. There are plenty of opportunities to improve these results, for example by synthesizing Dafny code for training and fine-tuning."}, {"title": "3.1 Most failed tasks have explainable causes", "content": "Most of the 37 cases in which the prototype failed to return a Python program that passes all tests have explainable causes. For around half of the cases, the prototype failed to converge towards a candidate solution, primarily due to a failure to produce verifiable code. If the prototype successfully converged towards a candidate solution, but which then did not pass all tests, the failure was only half of the time due to an algorithmically wrong implementation. Often it instead was due to either too ambiguous a prompt or failing interoperability with native Python code."}, {"title": "4 Future Work", "content": "In this section, we briefly discuss immediate points of improvement for the current prototype."}, {"title": "4.1 Improving the readability of compiled code", "content": "An issue that we particularly struggled with is that the Python code produced by Dafny's compiler tends to not be entirely idiomatic and requires a Dafny runtime. A possible solution could be to write a more idiomatic Python compiler for Dafny, something we did not attempt at. Another possible solution one can imagine is that the code is post-processed to increase its readability, using heuristics or even an LLM. To keep the correctness guarantees, such a transformation would require an additional check for semantic equivalence. In our chatbot implementation, we post-processed the code with an LLM, but did not verify for semantic equivalence. In Figure 8 we present an example of such a transformation. For benchmarking, we used the raw output of Dafny's backend, up to some targeted interventions that allowed us to interact with native Python code."}, {"title": "4.2 Resolving interoperability issues", "content": "Another inconvenient consequence of depending on the Dafny runtime is that the generated Python code is not always interoperable with native Python code.\nConsider for example the following simple Python program that returns a list with mixed types:\nIn Dafny, a list is only allowed to contain elements of one type. A program as the one above thus needs to be modelled with the help of a custom datatype:\nWhile this Dafny program successfully compiles to Python, the backend will generate Python classes for it that do not directly compare to the native Python equivalent (Figure 7). We often ran into this issue during benchmarking, since the test suite was set up in a way that it expects candidate solutions to operate over native Python types. We were able to circumvent some of the issues with the help of custom written wrappers and embeddings, but this solution clearly won't scale. We believe that this particular issue is one of the main hurdles of the overall approach presented here."}]}