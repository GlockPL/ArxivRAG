{"title": "Watch Your Steps: Observable and Modular Chains of Thought", "authors": ["Cassandra A. Cohen", "William W. Cohen"], "abstract": "We propose a variant of chain of thought (CoT) prompting called Program Trace Prompting that makes explanations more observable while preserving the power, generality and flexibility of CoT. In our approach, few-shot CoT demonstrations are wrapped in a formal syntax based on Python, and each prompt: identifies and names steps; defines the input/output behavior of steps; and replaces CoT explanations of in-context examples with chains of these formalized steps on the same examples. Program Trace Prompting is applicable to many tasks, achieving strong results on the 23 diverse tasks in the BIG-Bench Hard benchmark. More importantly, by instrumenting explanations in this way, we enable new types of analysis. In particular, we identify \"non-local errors\" (which correspond to incorrectly learning the reasoning method illustrated in the demonstrations) as an unaddressed issue in CoT learning, and we present methods for verifying the \"modularity\" of steps in a CoT explanation.", "sections": [{"title": "Introduction", "content": "While chain of thought (CoT) prompting is powerful, standard CoT outputs can be \"unfaithful\" (Jacovi and Goldberg, 2020): i.e., CoT can lead to incorrect (but superficially plausible) explanations for biased outputs (Turpin et al., 2024), and CoT explanations \"may not align with ... sequential causal reasoning\" (Bao et al., 2024).\nAlthough unfaithful explanations do not affect CoT's utility as a means of improving performance of prompted models, they do reduce the potential of CoT for other purposes, e.g., justifying a response to an end user. Despite numerous proposals (Lanham et al., 2023; Bentham et al., 2024; Parcalabescu and Frank, 2024), unfaithfulness remains difficult to detect and measure. The potential unfaithfulness of CoT explanations presents a jarring contrast with symbolical proofs, in which modular, verifiable reasoning steps are combined in well-understood ways.\nWe believe that a significant obstacle to progress on the faithfulness of CoT prompts is their syntactic diversity: because a CoT explanation can take nearly any form, they are difficult to analyze in any general way. The goal of this paper is to make CoT explanations easier to analyze while preserving the power, generality and flexibility of CoT. To do this, we propose a new prompting variant of CoT prompting in which few-shot CoT demonstrations are wrapped in a semi-formal syntax which (1) identifies and names steps;(2) defines the input/output behavior of steps; and (3) replaces CoT explanations of in-context examples with chains of these formalized steps on the same examples.\nWe use Python syntax to describe steps, and call our method Program Trace Prompting (PTP). As shown in Figure 1, CoT demonstrations are replaced with documentation for a Python program, together with traces of that program's behavior on some sample inputs. Each Python subroutine corresponds to a different kind of \u201cstep\u201d, and in addition to traces, the prompt includes stubs (function-level comments and type signatures) for the subroutines called in the traces. An LLM is then prompted with this information as context, and asked to produce a trace for a novel input. An answer to the question is finally extracted from the LLM-generated trace.\nNote that in Program Trace Prompting, no code is presented to the LLM. Similarly, there is no \"tool use\"; no generation of code or pseudo-code that is executed by Python or some other engine; and no pipeline of LLM calls as in agent frameworks (e.g., LangChain (Topsakal and Akinci, 2023) or DsPy (Khattab et al., 2023)). Instead, for a new test input, the LLM simply generates a new trace, similar to the ones given in the"}, {"title": "Methods", "content": "We used the Big-Bench hard (Suzgun et al., 2022) tasks to evaluate our approach (see Table 1). This is a well-studied and diverse set of 23 tasks which are known to be challenging for LLMs. All the tasks have 3-shot CoT prompts, and all the tasks also well-suited to evaluation, having answers that can be easily tested for correctness (most of the tasks are multiple-choice.) About half the tasks are considered \"NLP\" tasks, and are broadly similar to the simplified example of Figure 1: a high-level reasoning strategy is followed, which requires calling some low-level routines, and the low-level routines require some kind of AI. The other half are considered \u201calgorithmic\u201d tasks, which can be solved with simple extraction rules and an algorithm. For instance, one of the algorithmic tasks is evaluating a Boolean expression; another is reasoning with logical constraints on the ordering of a small number of objects."}, {"title": "Constructing prompts", "content": "In all our experiments, we constructed program trace prompts that closely followed the existing CoT prompt for the task (as suggested by Figure 1.) The existing CoT prompts were also used as a performance baseline. Modifications to CoT prompts were made only (1) to encourage the model to closely follow the in-context demonstrations, and in particular to produce output in an easily-parsed format and (2) rarely, to fix errors in the CoT prompt. See Appendix A.1 for details.\nWe used a semi-automatic scheme to produce our prompts. For each task, we manually write a Python program that implements the algorithm suggested by the CoT prompt; this is easy because the program need work only for the few in-context examples appearing in the CoT prompt. For instance, the implementation of sport_for just retrieves the required values from a small dictionary containing entries for the specific entities and phrases in the CoT prompt. Following the parlance of unit testing, we call these programs mocks. Functions can be nested (recursively if necessary) in a mock. When a mock is executed it automatically prints a trace; see Appendix A.2 for more information. Our traces primarily record function calls, although they can also include printed output-notably, as in Figure 1, the trace contains printed output for the \"final answer\" to a question.\nOur placement of the traces (in the function-level documentation of the top-level function) follows a widely used convention for documenting behavior by giving input/output examples.\nMore details are in Appendix A.2."}, {"title": "Mock development", "content": "Our process is semi-automatic because the mocks are written by hand. For many tasks, the mock is a"}, {"title": "Special prompting strategies", "content": "One interesting aspect of Program Trace Prompting is that the same prompt can be used to either predict traces for either a full CoT inference process, or any individual step, by just replacing the code fragment at the end of the prompt used in stage (3) of the Program Trace Prompting process of Figure 1. For example, to predict a trace for the step sport_for(\u2019scored a touchdown'), one could replace the text\nin the stage (3) prompt with\nAn ideal generation for the prompt above might be something like\nThe final output of the step is then simply the last line of the generation."}, {"title": "Improving to single-step prompting", "content": "The result of single-step prompting is more variable than the result of PTP prompting on full examples. One common failure mode for single-step prompts is that the LLM returns a trace that begins with the requested step, but then continues, often \"solving\" steps that might plausibly follow the requested step. We call this overgeneration.\nOvergeneration can be reduced by two methods. One is to add single-step traces to the stubs for the subroutine being probed. Empirically, we find that performance for single-step prompting is improved even if the only traces added are ones that are part of the top-level traces. A second method is to use more robust ways of extracting output from single-step generations, e.g., parsing the trace to identify steps, and then using, as the single-step result, the return value from the first step with correct subroutine name. More details are given in Section 4.1 and Appendix A.4."}, {"title": "Completing a partial trace", "content": "A similar prompting trick can be used to complete a trace given a prefix of the trace: we just replace the question in the stage (3) prompt with one that requests the LM to complete the trace, rather than generate it. See Appendix A.3 for details."}, {"title": "Language models", "content": "Program Trace Prompting can be used, in principal, with any LLM. After preliminary studies, we used Anthropic's Sonnet 3 model (2024-06-20) as the basis for developing our prompts, as it obtains a good balance between cost, reliability, and performance. Unless we state otherwise, this is the LLM model used in the experiments below."}, {"title": "Results: Accuracy and Types of Errors", "content": ""}, {"title": "How well do PT prompts work?", "content": "Our goal is to instrument CoT prompts so that they can be more easily analyzed but we would like to do that without impacting the performance. Hence, we first look at the overall accuracy of PTP prompts compared to the CoT prompts on which they are based.\nThe results of Table 2 strongly support our first claim, that PTP is applicable to a broad range of"}, {"title": "Are generated traces syntactically well-formed?", "content": "The main goal of Program Trace Prompting is to produce structured explanations, to enable deeper analysis of CoT performance. To verify that the explanations do indeed have the intended structure, we wrote tools to automatically evaluate generated traces for the following properties.\nCall correctness. Traces were parsed to see if they either (a) contain \"hallucinated\" calls to functions not listed in the prompt or (b) are a well-formed function-call trace (i.e., every \"Calling f(a, b)...\" line can be paired with a corresponding \"...f returns c\" line, following nesting rules.)\nSyntactic correctness. For well-formed traces, the arguments and return value for every function call were checked for syntactic correctness according to Python's rules (i.e., whether the LLM outputs could be evaluated to construct a Python object).\nType correctness. Syntactically correct arguments and return values were checked to check if they were type-correct, according to the type hints given in the stubs for each step.\nThe analyses of this paper only require call correctness, but interestingly, Program Trace Prompting generally produces traces that are syntax- and type-correct as well. This analysis supports our second claim, that the traces generated by PT prompts can almost always be automatically parsed into legal sequences of the predefined steps associated with the prompt. This allows us to further analyze the traces and characterize their errors."}, {"title": "What kinds of errors are made?", "content": "Since the PTP prompt specifies the semantics of individual steps, is possible to annotate where errors occur. In general, errors in PTP reasoning can"}, {"title": "Why do non-local errors occur?", "content": "To our knowledge, the existence of non-local errors in CoT reasoning has not been discussed previously in the literature. There are several possible reasons for this. First, non-local errors are relatively rare: only about 2.6% of the traces had non-local errors. Second, non-local errors occurred mainly in a small number of tasks. More than 80% of the non-local errors were in three of the tasks (Dyck Languages, Geometric Shapes, or Formal Fallacies), so more than 85% of the tasks contained very very few non-local errors. Third, non-local errors are hard to detect, even by careful manual annotators. Despite this, non-local errors are not unimportant: 22% of the incorrect traces were annotated as having non-local errors.\nOur conjecture is that non-local errors are correlated with the difficulty of learning to sequence the steps: in other words, non-local errors arise when the algorithm suggested by the CoT trace is complex, and hence difficult to learn.\nTo test this, we analyzed the traces produced for each task on the test examples. For every legal trace, we simplify the trace by removing the arguments and return values, leading to a abstract trace. For example, the traces of Figure 1 all have the same abstract trace: analyze_input() sport_for()"}, {"title": "Are individual steps meaningful?", "content": "We now turn to the question of how reliably individual steps are learned. The results of Section 3.2 show that PTP traces syntactically look like sequences of well-defined steps, each of which has known inputs and outputs. However, past work on evaluating the \"faithfulness' of CoT explanations (Turpin et al., 2024; Bao et al., 2024) shows that appearances of this sort can be deceiving. To determine if the abstraction of a \"step\" is meaningful, also we evaluated how well LLMs can execute individual steps, using the prompting scheme of Section 2.3.1."}, {"title": "Individual steps can be evaluated in isolation", "content": "Evaluating performance on a single step is difficult, since we do not have gold labels for each step's output. Fortunately, several individual steps in the BBH tasks are relatively easy to encode with rules-so easy, in fact, that in writing the mocks, these steps were implemented by general Python routines, rather than demonstration-specific dictionaries. The local correctness of these steps can be readily evaluated by using the corresponding Python function from the mock as an oracle. In the BBH suite, there were 16 different steps from five tasks with oracles.\nWe evaluated these oracle-testable steps on the inputs that were actually used in the course of solving the dev set examples (a total of 480 step executions). The overall performance is more than 90%, so the steps are \u201cmeaningful\u201d\u2014the model \u201cunderstands\" then well enough to execute them in a \"step by step\" manner, where each step is performed by an independent LLM call. Here we use the full method described in Section 2.3.1, including the two methods used to address overgeneration, adding micro-traces, and parsing traces to"}, {"title": "Why measuring performance of a single step is hard", "content": "We next turn to more general ways of measuring single-step performance. Unlike the steps in Table 7, in many cases, no such oracle exists, because the output contains natural-language text, or because the utility of the step depends on downstream problem-solving in some complex way.\nIn general, single-step prompting need not give the same result as executing a step in the context of a larger problem-solving task. Informally we say that a step is non-modular if it behaves differently with a single-step prompt than in the context of a complete trace.\nA simple experiment shows that steps can be highly non-modular. We modified the consistent_sports step in the sports_understanding task by removing the first input argument. Recall that this step compares two sports descriptions, which clearly cannot be done without seeing the first one. One might expect this prompt to be less accurate, but evaluated it in the usual way, accuracy is statistically identical to the standard version.8 The problem, of course, is that the LLM is not restricted to generate step outputs from the step inputs in generating a trace, the LLM can attend to any previous text. In short, LLM trace-generation is robust enough to generate traces that get the right answer even after this change.\nWe next explore forcing the LLM to consider only the designated inputs to this step, by modifying the trace: we extract the consistent_sports step, re-execute it with a single-step prompt, and then finally regenerate the remainder of the trace"}, {"title": "Defining and measuring modularity", "content": "We write a program trace $t^1$ as a series of steps\n$f_1(x_1, y_1) ... f_i(x_m, y_n) a$\nwhere each $f_i$ is a Python function/step name, $x$ a tuple of inputs, $y$ an output (also possibly a tuple), and $a$ the final answer produced for the initial CoT input $f_1, x$. For brevity we will drop the superscripts when possible.\nIf we model the LLM's generations as a joint distribution over variables $F_1, X_1, Y_1, ..., F_n, X_n, Y_n, A$, each $Y_i$ depends on the entire preceding sequence, which we write:\n$Y_i \\sim P(Y_i|F_{<i}, X_{<i}, Y_{<i})$(1)\nwhere $X_{<i}$ is shorthand for $X_1, ..., X_i$, and so on. Equation 1 holds for any LLM; a stronger condition is that the $f_i$'s act like actual Python functions, where $y_i$ depends only on $f_i$ and $x_i$:\n$Y_i \\sim P(Y_i|F_i = f_i, X_i = x_i)$\nIn other words, each $Y_i$ is conditionally independent of everything else given $X_i$ and $F_i$, i.e.\n$P(Y_i|F_i, X_i) = P(Y_i|F_{<i}, X_{<i}, Y_{<i})$(2)\nWe define step $i$ to be modular if and only iff Equation 2 holds. If all steps are modular, then generation is arguably a \"step by step\" process, in the precise sense that the i-th output $y_i$ depends only on (1) the choice of what step $f_i$ to use and (2) the selection of the inputs $x_i$ for that step.\nTo measure modularity, we force a particular step to be modular, by replacing its output with the result of a single-step prompt. After making this change, we then extend the resulting trace prefix using the method of Section 2.3.2. Precisely, if $i$ is the position of the designated step in the trace $t$\n$t = f_1(x_1, y_1) ... f_i(X_i, Y_i) ... f_n(x_n, Y_n)a$\nthen we replace $y_i$ with a new value $\\tilde{y}_i$ produced with a single step prompt\n$\\tilde{Y}_i \\sim P(Y_i|F_i = f_i, X_i = x_i)$\nand ask the LLM to predict the rest of the trace, which will potentially change every subsequent x, f, and y, yielding the new trace\n$t' = f_1(x_1, y_1) ... f_i(x_i, \\tilde{Y}_i) ... \\tilde{f}_n(\\tilde{I}_n, \\tilde{y}_n)\\tilde{a}$\nLet $P(T|F_1 = f_1, X_1 = x_1)$ be the distribution of traces produced from input $f_1, x_1$, and let $P(T'|F_1 = f_1, X_1 = x_1)$ be the distribution of traces after this intervention. Assume that the step of completing a a trace prefix does not change the distribution.Below we call this assumption the stable incremental generation assumption. If stable incremental generation holds, and step i is modular, then $P(T|F_1, X_1) = P(T'|F_1, X_1)$.\nSimilarly, suppose S is any summary statistic derived from a trace, i.e. $s \\sim P(S|T)$. For example, S could be the length of the trace or the correctness of the trace. (When S is just a deterministic function of T we also write this as $s = g_s(T)$.) If we assume stable incremental generation, then if step i is modular this also implies\n$P(\\tilde{S}|F_1, X_1) = P(S|F_1, X_1)$\nThis means we can detect failures in modularity if we can detect changes in the distribution of the distributions of the statistics S and S'.\nWhile prior work (Lanham et al., 2023) used accuracy as a summary statistic, we considered additional ones, motivated by Table 8, designed to detect changed behavior in a forced-modular trace. In addition to $S_{corr}$, trace correctness, we consider $S_{numSteps}$, the number of steps in the trace; and $S_{abTrace}$, the abstract trace of Section 3.4.\nSpecifically, in the experiments below, we take a sample of m inputs $X_1 = {x_1^1,...,x_1^m}$ and compute corresponding samples of the summary"}, {"title": "Which tasks are non-modular?", "content": "Let's briefly review the preceding subsections. In Section 4.1.1 we showed that we can prompt the model to accurately execute individual steps in isolation, using the method of Section 2.3.1.\nIn Section 4.1.3, we defined and motivated modularity: steps are modular if they depend only on their defined inputs, and not other aspects of the trace in which they are embedded.\nIn Section 4.1.3, we proposed a way to measure modularity of a step by using forced-modularity interventions. A subtlety is that this intervention is based on two changes: (a) replacing the step output with a forced-modular version, and then (b) re-generating the rest of the trace. Hence we can only measure the impact of distributional changes due to (a) if we are confident that (b) does not also change the distribution, as assumption we call stable incremental generation.\nIn the top of Table 9, we evaluate the result of the forced-modularity intervention on some of the oracle-checkable steps from Table 7 on the dev data. The columns labeled p are the p-values from our permutation test, so a small value indicates that the step changed the distribution of the statistic. The column p\u0302 is the p-value after a multiple-test correction.10 Corrected p-values below 0.1 are italicised and corrected p-values values below 0.05 are boldfaced: these correspond to steps where some statistic is changed by the forced-modularity intervention.\nFor steps where corrected p-values are low, we also tested the assumption of stable incremental generation with another intervention, where only (b) occurs. We call this intervention split-and-generate: we truncate the trace at step i and then re-generate it. We see that the assumption fails for three of the steps, so we cannot conclude that\nthese step is non-modular; hence the final column is labeled \"unclear\" for them. There is no evidence against the stable incremental generation assumption \u201csolve negations\" step, but weak evidence (p = 0.083) against it for \"rewite expressions\u201d step, so we mark the boolean expressions step (with p = 0.014 and stable incremental generation) as non-modular and the other (with p = 0.083 and stable incremental generation) as \"likely\" to be non-modular.\nWe note that the nonmodular steps are all cases where the single-step prompts underperformed in Table 7. We also note that correctness is the least useful summary statistic on this data.\nThe situation is quite different for the algorithmically x=simpler NLP tasks. In Table 10 we summarize the results of repeating this experiment on 25 steps from 12 NLP tasks. (We choose steps which were prone to local errors as being relatively more interesting.) Only 3 of these steps show a significant distributional shift with the forced-modularity intervention, and one of these may be explained by a failure of the stable incremental generation assumption.\nWe conclude that non-modularity is rare in the NLP tasks. We note again that the tradition statistic for interventions of accuracy is not sensitive enough to detect any significant changes.\""}, {"title": "Discussion", "content": "These results show that on non-algorithmic tasks, non-modularity is rare. In fact, of the 12 NLP tasks we consider, only one contained any steps that appear to be non-modular.\nThis observation, of course, depends in part on the specific mocks that we used in these experiments. One disadvantage of this test is that, while one can identify non-modularity (when a null hypothesis is refuted), you cannot ensure modularity-the null might still be refuted if you examine more data. However, it does seem clear that at least most step executions of a step are modular."}, {"title": "Related Work", "content": ""}, {"title": "Understanding CoT and Measuring Faithfulness", "content": "There has been a great deal of work on CoT prompting and reasoning in LLMs, as surveyed here (Huang and Chang, 2022)."}, {"title": "Calling Python from LLMs", "content": "One method that has been proposed to improve faithfulness, and more generally to enhance reasoning abilities, is to combine LLMs with symbolic methods-often methods based on Python. For example, in Program of Thoughts (Chen et al., 2022) and Program Aided LMs (Gao et al., 2023) and elsewhere (Lyu et al., 2023), an LLM generates a Python program for each task instance, which is then executed to produce an answer. An alternative to this architecture is tool use (Paranjape et al., 2023), where an LLM is augmented in-terleave text generation and execution of symbolic tools. A third architectural alternative is to generate Python-like \u201cpseudocode\u201d (Li et al., 2023; Weir et al., 2024; Chae et al., 2024)-code which has a symbolic, Python-like syntax, but includes routines that cannot be executed directly, but must be emulated by an LLM, or Python code which uses LLMs as a tool (Zhou et al., 2024).\nOf the approaches above, the \"pseudocode\" architecture is closest to Program Trace Prompting. To our knowledge, however, prior research in this direction have sought to produce a single program that works for all instances of a task. In contrast, our approach does not produce a program, or even require that such a program exists, since a trace is generated for each task instance.\nA second difference is that unlike the prior work discussed above, Program Trace Prompting does not use a hybrid LLM/symbolic architecture, but is a pure prompting method, thus providing an improvement in simplicity. We also quantify and measure faithfulness in a novel way."}, {"title": "Calling LLMs from Python", "content": "Another widely-used methodology for developing applications with LLMs is to manually decompose a task into substeps that can be performed by mak-"}, {"title": "CoT Prompt generation", "content": "CoT prompting is powerful, but it is expensive to collect examples, and difficult to optimize CoT prompts, although these problems can been ameliorated by synthetic generation of CoT prompts (Shao et al., 2023) and prompt optimization (Wang et al., 2023). Prior work has also explored semi-automatic CoT prompt generation by making use of post-hoc extractive rationales of classification decisions (Krishna et al., 2024).\nProgram Trace Prompting proposes new ways to semi-automatically produce CoT prompts; however unlike prior work, these prompt-generation approaches are fairly specific to the generation of PT prompts."}, {"title": "Learning programs from traces", "content": "Learning from program traces has been studied for many decades in machine learning (e.g., (Smith, 1984)), and has gone by various names including programming by example and learning by demonstration (Cypher and Halbert, 1993; Lieberman, 2001; Lau et al., 2003). To our knowledge, there has been little work on this topic in the LLM era, although algorithmically produced programs traces have been used to analyze the noise-sensitivity of CoT reasoning (Havrilla and Iyer, 2024). Our work differs from (Havrilla and Iyer, 2024) in that we propose program traces as a way of constructing controllable chain-of-thought prompts for end tasks, rather than a tool for generation of analytic data."}, {"title": "Conclusions", "content": "We proposed Program Trace Prompting, a variant of CoT prompting in which few-shot CoT demonstrations are wrapped in a semi-formal syntax which (1) identifies and names steps; (2) defines the input/output behavior of steps; and (3) replaces CoT explanations of in-context examples with chains of these formalized steps on the same example. We show that this approach is broadly applicable, in that comparable accuracies can be obtained by many models on many tasks. It also offers some advantages over prior prompting schemes.\nFirst, Program Trace Prompting traces are easier to analyze automatically than arbitrary CoT explanations, since they follow the syntax used for steps-even maintaining the type-correctness of step inputs and outputs, in most cases. This means we can extract inputs and outputs of individual steps, count the number of inference steps, and perform other types of analysis easily. For example, we can more easily localize errors in an incorrect explanation. We can also identify errors which cannot be localized to steps (e.g., copying errors where the input to one step should be output of another, but is not). Above we showed that trace entropy correlates with the presence of non-local errors: simply put, more complicated reasoning strategies are harder to learn from CoT prompts than simpler ones.\nSecond, Program Trace Prompting allows prompts to elicit results of executing a single step of a CoT process in isolation, outside of a larger reasoning task. This enables a new class of interventions, where we force a step inside a trace to be modular by executing it in isolation, as if it were performed in by an agentic LLM program. By doing this and measuring statistical perturbations in the result, we can detect if the step actually depends only on its declared inputs, or if it depends on other previous steps in unexpected ways. We show that this sort of non-modular behavior can occur, but it is rare in our collection of tasks."}]}