{"title": "PROVABLY ROBUST WATERMARKS FOR OPEN-SOURCE LANGUAGE MODELS", "authors": ["Miranda Christ", "Sam Gunn", "Tal Malkin", "Mariana Raykova"], "abstract": "The recent explosion of high-quality language models has necessitated new methods for identifying AI-generated text. Watermarking is a leading solution and could prove to be an essential tool in the age of generative AI. Existing approaches embed watermarks at inference and crucially rely on the large language model (LLM) specification and parameters being secret, which makes them inapplicable to the open-source setting. In this work, we introduce the first watermarking scheme for open-source LLMs. Our scheme works by modifying the parameters of the model, but the watermark can be detected from just the outputs of the model. Perhaps surprisingly, we prove that our watermarks are unremovable under certain assumptions about the adversary's knowledge. To demonstrate the behavior of our construction under concrete parameter instantiations, we present experimental results with OPT-6.7B and OPT-1.3B. We demonstrate robustness to both token substitution and perturbation of the model parameters. We find that the stronger of these attacks, the model-perturbation attack, requires deteriorating the quality score to 0 out of 100 in order to bring the detection rate down to 50%.", "sections": [{"title": "1 INTRODUCTION", "content": "As generative AI becomes increasingly capable and available, reliable solutions for identifying AI-generated text grow more and more imperative. Without strong identification methods, we face risks such as model collapse (Shumailov et al., 2024), mass disinformation campaigns (Ryan-Mosley, 2023), and detection false positives leading to false plagiarism accusations (Ghaffary, 2023). Watermarking is a prominent and promising approach to detection. Recent works (Kirchenbauer et al. (2023); Aaronson (2022); Zhao et al. (2024); Christ et al. (2024); Kuditipudi et al. (2024); Fairoze et al. (2023); Christ & Gunn (2024)) construct \"sampler-based watermarks,\" for the setting where the watermark is embedded at generation time and users have only query access to the model. Such watermarks modify the algorithm used by the LLM to sample from the probability distribution over the next token, leaving the underlying neural network untouched. For example, Zhao et al. (2024) shifts this distribution to prefer sampling tokens on a fixed \u201cgreen list.\u201d These existing approaches are ill-suited for scenarios where an attacker has access to the code for the watermarked model and can simply rewrite the sampling algorithm to sample from the unmodified probability distribution - yielding the original, unwatermarked model, with no loss in quality.\nSuch scenarios are gaining importance as open-source models become more widely available and higher quality (e.g., LLaMA Touvron et al. (2023), Mistral Jiang et al. (2023), OPT Zhang et al. (2022)). However, watermarks for open-source models have been severely understudied. In this work we initiate a formal study of such watermarks, in the setting where the model parameters and associated code are publicly available, and the watermark is embedded by altering the weights of the neural network. We consider the standard notion of autoregressive language models used in existing watermarking works (see Appendix A.2 for a precise definition). Constructing watermarks"}, {"title": "2 TECHNICAL OVERVIEW", "content": "Formalizing unremovability (Section 4.1). Recall that the goal of the adversary is to take a watermarked model and produce a high-quality unwatermarked model. One cannot hope for a watermark to be unremovable by an adversary with sufficient knowledge to train its own unwatermarked model. Therefore, we must consider only adversaries with limited knowledge about the distribution of high-quality text. We formalize this notion as follows: we model ideal-quality language as being described by an original neural network $\\mathcal{M}^*$. That is, the ideal-quality language is the distribution of text that would be produced by an LLM using $\\mathcal{M}^*$ to compute the probability distribution over each token. This neural network $\\mathcal{M}^*$ is then watermarked, and the resulting $\\mathcal{M}'$ is given to the adversary. The adversary's uncertainty is captured by its posterior distribution over $\\mathcal{M}^*$, given $\\mathcal{M}'$.\nWe assume that the adversary's posterior distribution over $\\mathcal{M}^*$ is normally distributed in a particular representation space of models, and centered at $\\mathcal{M}'$. Under this assumption, we prove that there is a steep trade-off between the magnitude of changes the adversary must make to remove the watermark, and the magnitude of changes required to add the watermark. That is, text produced by the adversary with access to $\\mathcal{M}'$ is either watermarked or low-quality.\nOur scheme (Section 5). Our scheme is quite natural: We modify the bias of each neuron in the last layer of the model by adding a perturbation from $\\mathcal{N}(0, \\epsilon^2)$, as described in Algorithms 1 and 2. The watermarking detection key is the vector of these perturbations. Given the weights of a model, we can detect the watermark by checking the correlation between the biases of its last layer and the watermark perturbations. More precisely, we compute the inner product between the watermark perturbations, and the difference between the biases of the given model and the original model (Algorithm 3).\nFurthermore, we observe that it is possible to approximate this inner product given only text produced by the model. This text detector (Algorithm 4) computes a score that is the sum of the watermark perturbations corresponding to the distinct tokens observed in the text. We show that in expectation, this score is approximately the inner product from the detector from weights (Algorithm 3), where the approximation error is lower for higher-entropy text. We prove that in sufficiently high-entropy text, our text detector succeeds with overwhelming probability.\nProving unremovability (Theorem 3). We show unremovability in two steps: (1) Any high-quality model produced by altering the watermarked model must have a high inner product score (Theorem 1), and (2) If a language model has a high inner product score, its responses are watermarked (Theorem 3)."}, {"title": "3 RELATED WORK", "content": "We describe the most relevant related works here, but refer readers to Tang et al. (2023); Piet et al. (2023) for more comprehensive surveys.\nExisting watermarks change the sampling function of the LLM, and are therefore easily removable given code for this sampling function. Aaronson (2022); Kirchenbauer et al. (2023); Zhao et al. (2024); Christ et al. (2024); Fairoze et al. (2023) sample each token by partitioning the token set into red and green lists (that depends on the previously-output tokens), then increasing the probability of the tokens in the green list. To detect the watermark, one computes the fraction of green tokens in the given text and compares it to a threshold. Kuditipudi et al. (2024); Christ & Gunn (2024) operate slightly differently-for each response, they choose a random seed. In Kuditipudi et al. (2024) there are polynomially many possible seeds for a given watermarking key, and in Christ & Gunn (2024) there are exponentially many. They then choose the $i$th token of the response to be correlated with the $i$th value of the random seed. To detect the watermark, one essentially computes the correlation between the response and the seed. While not unremovable, some of these watermarks satisfy a weaker robustness property: Given a watermarked response, it is difficult to make a bounded number of edits to yield an unwatermarked text."}, {"title": "4 PRELIMINARIES AND DEFINITIONS", "content": "Let $\\mathbb{N} := \\{1, 2, . . . \\}$ denote the set of positive integers. We will write $[q] := \\{1, ..., q\\}$. For a set $X$, we define $X^* := \\{(x_1,...,x_k)|x_1,...,x_k \\in X \\wedge k \\in \\mathbb{Z}_{>0}\\}$ to be the set of all strings with alphabet $X$. For a binary string $s \\in X^*$, we let $s_i$ denote the $i$th symbol of $s$ and $\\text{len}(s)$ denote the length of $s$. For a string $s \\in X^*$ and positive integers $a \\le b < \\text{len}(s)$, let $s[a : b]$ denote the substring $(s_a,..., s_b)$. We use $\\log(x)$ to denote the logarithm base 2 of $x$, and $\\ln(x)$ to denote the natural logarithm of $x$.\nFor a finite set $X$, we will use the notation $x \\leftarrow X$ to denote a uniformly random sample $x$ from $X$. If $X$ is a set of $n$-dimensional column vectors, we will write $X^m$ to refer to the set of $n \\times m$ matrices whose columns take values in $X$. Unless otherwise specified, vectors are assumed to be column vectors. For a vector $x \\in \\mathbb{R}^n$, we let $||x|| = ||x||_2 = \\sqrt{\\sum_{i=1}^{n} x_i^2}$.\nLet $\\text{Ber}(p)$ be the Bernoulli distribution on $\\{0,1\\}$ with expectation $p$. Let $\\text{Ber}(n,p)$ be the distribution on $n$-bit strings where each bit is an i.i.d sample from $\\text{Ber}(p)$. For a distribution $D$, we let $\\text{Supp}(D)$ denote its support.\nLet $\\lambda$ denote the security parameter. A function $f$ of $\\lambda$ is negligible if $f(x) = O(\\frac{1}{\\text{poly}(x)})$ for every polynomial $\\text{poly}(.)$. We write $f(x) \\le \\text{negl}(x)$ to mean that $f$ is negligible. We say a probability is overwhelming in $\\lambda$ if it is equal to $1 - f$ for some negligible function $f$. We let $\\approx$ denote computational indistinguishability and $=$ denote statistical indistinguishability.\nWe provide a formal description of a language model in Appendix A.2."}, {"title": "4.1 WATERMARKS", "content": "We present general definitions for watermarking content from some content distribution $\\mathcal{C}$ of real vectors; that is, $\\text{Supp}(\\mathcal{C}) \\subseteq \\mathbb{R}^n$. We will later use this real-content watermark framework to watermark the weights of a neural network.\nDefinition 1 (Watermark). A watermark is a tuple of polynomial-time algorithms $\\mathcal{W} = (\\text{Setup}, \\text{Watermark}, \\text{Detect})$ such that:\n$\\bullet$ $\\text{Setup}(1^\\lambda) \\rightarrow sk$ outputs a secret key, with respect to a security parameter $\\lambda$.\n$\\bullet$ $\\text{Watermark}_{sk}(x) \\rightarrow x'$ is a randomized algorithm that takes as input some content $x$ and outputs some watermarked content $x'$.\n$\\bullet$ $\\text{Detect}_{sk}(x') \\rightarrow \\{\\text{true}, \\text{false}\\}$ is an algorithm that takes as input some content $x'$ and outputs true or false."}, {"title": "5 WATERMARKING SCHEME", "content": "We first show a general watermarking scheme for vectors in $\\mathbb{R}^n$. We then show how to apply this paradigm to neural networks, by watermarking the biases of the output layer. Our watermark detector is strongest when given explicit access to the weights of the watermarked model (Section 5.1). Furthermore, for a language model, where these biases directly affect the distribution of tokens in its outputs, our watermark is even detectable in text produced by a watermarked model (Section 5.2)."}, {"title": "5.1 DETECTING THE WATERMARK FROM THE WEIGHTS OF THE MODEL", "content": "We consider an arbitrary vector $w^*$ in $\\mathbb{R}^n$ to which Gaussian noise is added, to obtain $w_{\\text{wat}}$. Observe that the inner product between $(w_{\\text{wat}} - w^*)$ and the vector of the Gaussian perturbations $\\Delta$ is large. Naturally, our detector $\\text{WeightDetect}$ (Algorithm 3) computes this inner product. We show that any adversary whose posterior distribution (after seeing $w_{\\text{wat}}$) over $w^*$ is Gaussian cannot remove the watermark without adding significant perturbations of its own. In particular, if $w^*$ has dimension $n$, the adversary must add a vector of Euclidean norm $\\Omega(\\frac{n}{\\sqrt{\\log n}})$ to succeed at removing the watermark. In contrast, embedding the watermark involves adding a perturbation of Euclidean norm only $O(\\epsilon n)$: Watermark removal requires a change that is larger than watermark embedding by a factor of nearly $\\sqrt{n}$. So far, this approach is general to watermarking any real vector. Although our work focuses on applying it to LLMs, it is likely that this paradigm can be applied to other scenarios, which we leave for future work.\nTo apply our watermark to LLMs, we will later take $w^*$ to be the biases in the last layer of a neural network, making $n$ the size of the token alphabet."}, {"title": "5.2 DETECTING THE WATERMARK FROM TEXT", "content": "In this section, we will show that the inner product from $\\text{WeightDetect}$ can be approximated given text. Our detector from text, $\\text{TextDetect}$ (Algorithm 4), simply computes the sum of the perturbations of the biases of tokens in the given text. We observe that a positive perturbation increases the probability of outputting the given token, and the magnitude of the perturbation corresponds to the magnitude of the increase. Similarly, a negative perturbation decreases a token's likelihood. Therefore, we expect the frequency of a token in a watermarked response to have an observable correlation with the sign and magnitude of the perturbation added to its bias.\nWe prove in Theorem 3 that this intuition is indeed correct. In particular, this sum of perturbations of observed tokens approximates a value that is 0 in expectation for natural text, and grows linearly with the text length for watermarked text. This is true even when the watermarked model is modified by an adversary attempting to remove the watermark. The accuracy of the detector's approximation depends on the entropy of the text, and the quality of the model produced by the adversary. That is, low-entropy responses and low-quality adversarial models will have lower watermark detectability.\nWe first introduce some notation and recall the structure of a language model. Let $z$ be the biases of the model produced by an adversary modifying the watermarked model $w_{\\text{wat}}$. Let $p_t^i$ and $q_t^i$ be the probabilities that the models $z$ and $w^*$, respectively, assign to token $t \\in [n]$ at step $i$. Let $l$ be the"}, {"title": "6 EXPERIMENTAL EVALUATION", "content": "In this section we present the results of an experimental evaluation of our watermarking construction which demonstrate its behavior under concrete parameter settings. Our experiments aim to show feasibility rather than optimality, as we open the door to further open-source watermarking work with experimental findings consistent with our theoretical results.\nExperimental Setup. We watermark two LLMs: the OPT model with 1.3B and 6.7B parameters. We generate responses of three types: story, essay, and code; we include these prompts and additional prompt parameters in Appendix A.4. In all of our plots, we compute the fraction of responses detected among those with at least 20 distinct tokens, for a range of $\\epsilon$ parameter values. Recall that $\\epsilon$ is the standard deviation of the Gaussian perturbations added to the biases. Therefore, the detectability of the watermark increases with epsilon.\nDetectability. In Figure 2, we plot the fraction of watermarked responses that are detected at 1% and 5% false positive rates. These responses are output by a model watermarked using our scheme, without further modification. The curves for \"Inner Product Detector\" show detectability under our detector $\\text{TextDetect}$ from Algorithm 4. Observe that for $\\epsilon \\geq 0.5$, we have a true positive rate of at least 80%. Furthermore, this plot supports our result (Theorem 3) that the true positive rate increases with $\\epsilon$. We emphasize that although our detectability is not as strong as some inference-time watermarks, ours is the only open-source watermark with a provable unremovability guarantee.\nWe also plot detectability for an alternate detector (\"Count Detector\"). This alternate detector is analogous to that of Kirchenbauer et al. (2023); Zhao et al. (2024) in that it computes the fraction of tokens whose perturbations were positive (e.g., green list tokens). Observe that this detector has a significantly lower true positive rate than of $\\text{TextDetect}$ for our scheme. This shows that the open-source setting requires new techniques for reliable detection, and that existing inference-time detectors do not carry over even when implementable in the weights of the model. This supports our strategy of considering the magnitudes of the perturbations during detection, in our Inner Product Detector."}]}