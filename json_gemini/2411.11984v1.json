{"title": "UNDERSTANDING CHAIN-OF-THOUGHT IN LLMS\nTHROUGH INFORMATION THEORY", "authors": ["Jean-Fran\u00e7ois Ton", "Muhammad Faaiz Taufiq", "Yang Liu"], "abstract": "Large Language Models (LLMs) have shown impressive performance in complex reasoning\ntasks through Chain-of-Thought (CoT) reasoning, allowing models to break down problems\ninto manageable sub-tasks. However, existing CoT evaluation techniques either require an-\nnotated CoT data or fall short in accurately assessing intermediate reasoning steps, leading\nto high rates of false positives. In this paper, we formalize CoT reasoning in LLMs through\nan information-theoretic lens. Specifically, our framework quantifies the 'information gain' at\neach reasoning step, enabling the identification of failure modes in LLMs without the need for\nexpensive annotated datasets. We demonstrate the efficacy of our approach through extensive\nexperiments on toy and GSM-8K data, where it significantly outperforms existing outcome-\nbased methods by providing more accurate insights into model performance on individual tasks.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have demonstrated remarkable capabilities across a wide range of tasks, from\ncomplex reasoning to code generation [Chowdhery et al., 2024, OpenAI et al., 2024, Bubeck et al., 2023, Anil\net al., 2023]. Many of these advances can be attributed to Chain-of-Thought (CoT) reasoning [Wei et al., 2024,\nNye et al., 2021, Li et al., 2024], which involves breaking down complex problems into a series of intermediate\nsteps, mirroring human-like reasoning processes. The success of CoT reasoning, particularly in domains such as\nmathematics, logic, and multi-step decision-making, has led researchers and developers to incorporate CoT-like\nfeatures directly into model training, i.e. the FLAN family of models [Chung et al., 2022, Wei et al., 2022].\nThis paper introduces a new formal framework for analyzing CoT in LLMs. We provide a rigorous method\ngrounded in information theory, to evaluate the quality of each step in a model's reasoning process, thus offering\ninsights beyond simple accuracy metrics to identify areas for improvement.\nPrevious work in this area has proposed \"Process Supervision\" [Lightman et al., 2023], which requires expensive,\nhuman-annotated step-by-step data. While effective, this approach is often impractical due to the high cost and\neffort of creating large-scale annotated datasets. In turn, alternative methods have recently been proposed, such\nas outcome reward modelling [Havrilla et al., 2024] or the Math-Shepherd [Wang et al., 2024b]. Both these\napproaches avoid reliance on annotated step-wise CoT data by instead modelling the correctness of each step\nbased on the correctness of final outputs. However, as we demonstrate in this paper, these methods can be\nunsound for detecting incorrect reasoning steps and can thus lead to a high false-positive rate in certain scenarios.\nTo address these shortcomings, we employ an information-theoretic approach, grounded in the following key\ninsight: Each correct step in a reasoning process should provide valuable and relevant information that aids in\npredicting the final correct outcome. Building on this insight, we develop a framework to quantify the \"infor-\nmation gain\" after each sub-task in the reasoning process, without the need for step-by-step annotations. This\nenables us to detect sub-tasks that fail to contribute meaningful information toward the correct solution, signalling\npotential errors or irrelevant steps in the model's reasoning. In addition, we also introduce a practical algorithm\nto assess LLM performance across various sub-tasks within a Chain-of-Thought (CoT) reasoning process.\nThe key contributions of this paper are as follows:\n1.  We develop a framework for sequential applications of sub-tasks, e.g. Chain-of-Thought and provide a\n    rigorous language to describe and detect detect failure modes in LLMs.\n2.  Based on this framework, we propose a practical algorithm to assess the task-wise performance of mod-\nels. This yields more granular information about a model's CoT performance without requiring anno-\ntated data for intermediate reasoning steps."}, {"title": "2 Proposed Framework: Setup and Notation", "content": "Before diving into our framework, we first provide a high-level overview and notation on how LLM generation\nwill be treated throughout this paper. This will allow us to set the foundation for describing our information-\ntheoretic framework. In particular, following the approach in Gonz\u00e1lez and Nori [2023], we view LLMs as\nabstract execution machines with a natural language interface. From this perspective, prompts are designed to\nsolve specific problems (e.g., mathematical or logical problems), and the LLM processes the information in the\nprompt to generate an output.\nWe now define the notation for a typical prompt as a combination of two components:\n1.  An initial state, represented by a random variable $X_0 \\in \\mathcal{X}$, denotes information provided in the prompt\n    that the LLM must operate on to obtain the queried information.\n2.  A task $\\lambda \\in \\Upsilon$ (e.g., addition followed by multiplication) which encapsulates how the LLM should\n    process information in $X_0$.\nGiven the prompt, defined as a tuple $(X_0, \\lambda)$, the state $X_1$ represents the result of applying task $\\lambda$ to the initial\nstate $X_0$. Formally, we denote this using the update mapping $\\Lambda : \\mathcal{X} \\times \\Upsilon \\rightarrow \\mathcal{X}$ which outputs the updated\nstate $X_1$ by applying the task $\\lambda$ on $X_0$, i.e. $X_1 = \\Lambda(X_0, \\lambda)$. This updated state is then used to obtain the final\noutput, denoted by $Y \\in \\mathcal{X}$, by extracting only the information in $X_1$ which is relevant to the queried final answer.\nThis notation defines a prompt that instructs a model to process information drawn from some initial distribution\n$p(X_0)$ (e.g., mathematical problems).\nLet us use the following simple example to illustrate the notation:\nPrompt: \"James has 3 apples and Abbey has 9. How many apples do the two have in total?\"\nHere, using the above notation, the initial state $x_0$ denotes the information \"James has 3 apples; Abbey has 9\napples\", and $\\lambda$ denotes the addition task. Next, $x_1 = \\Lambda(x_0, \\lambda)$ represents the updated information after correctly\nperforming the addition operation, i.e. $x_1$ = \"James has 3 apples; Abbey has 9 apples; The two have 12 apples in\ntotal\". The final output, $y$, is then obtained by simply extracting the total number of apples from $x_1$, i.e. \"The two\nhave 12 apples in total\"\u00b9. With this basic notation established, we now consider compositions of tasks, enabling\nus to formalize the Chain of Thought (CoT) process in LLMs."}, {"title": "2.1 Compositionality", "content": "Many mathematical or logical problems require a sequential application of operations. Our notation is also\namenable to such problems as it accommodates the composition of tasks. Consider a problem which requires two\nsuccessive steps to arrive at the correct output:\nPrompt: \"Solve for $z = 2 \\times (x + y)$ where $x = 12$ and $y = 13$\".\n(1)\nIn this example, first, we apply the addition operation to find the value of $x + y$, and next, we apply the multipli-\ncation operation to find the value of $z$. Using our notation this can be expressed as $\\Lambda(x_0, \\lambda_1 \\circ \\lambda_2)$, where $\\lambda_1, \\lambda_2$\ndenote the addition and multiplication tasks respectively. The following property allows us to concretely define\nthe application of compositional task $\\lambda_1 \\circ \\lambda_2$:\nDefinition 2.1. We say that an update rule $\\Lambda : \\mathcal{X} \\times \\Upsilon \\rightarrow \\mathcal{X}$ is compositionally consistent if:\n$\\Lambda(x_0, \\lambda_1 \\circ \\lambda_2) = \\Lambda(\\Lambda(x_0, \\lambda_1), \\lambda_2) \\quad \\text{ for all } x_0 \\in \\mathcal{X} \\text{ and } \\lambda_1, \\lambda_2 \\in \\Upsilon$.\nHere, $\\stackrel{d}{=}$ denotes equality in distribution and is sufficient in many cases. For example, where a query may have\nmultiple correct responses, an almost sure equality may be too restrictive.\nGoing back to the prompt in (1), Figure 1 shows that the model first computes $x + y$, and next multiplies the\nresult by 2. Here, we refer to $X_1, X_2$ as intermediate states and $Y$ is the correct final output. More generally,\nif a problem statement requires sequential application of $T$ sub-tasks, $\\lambda = \\lambda_1 \\circ \\dots \\circ \\lambda_T$, then the Chain-of-\nThought (CoT) reasoning is divided up into $T$ steps, where the output of the $t$'th step is recursively defined as"}, {"title": "2.2 Primitive tasks", "content": "In this subsection, we introduce the notion of primitive tasks which form the basic building blocks of any task.\nIntuitively, our formulation is reminiscent of ideas from linear algebra, where basis vectors form the basic building\nblocks of a vector space. In our case, any task $\\lambda \\in \\Upsilon$ can be expressed as a sequence of primitive tasks. This\ndecomposition will allow us to establish which tasks the model could have learned from the training data. For\nexample, if a specific primitive task is not available in the LLM training data, it would be impossible for the model\nto execute any instructions which involve this primitive task correctly. With this in mind, we now introduce this\nconcept formally:\nDefinition 2.2 (Primitive tasks). We say that a set of tasks $\\Gamma \\subseteq \\Upsilon$ is primitive if, for any task $\\lambda \\in \\Upsilon$, there exists\na unique subset $\\{\\lambda_i\\}_{i=1}^{k} \\subseteq \\Gamma$ such that $\\lambda = \\lambda_1 \\circ \\dots \\circ \\lambda_k$.\nNote that the decomposition is not unique but the set of components is. In some cases, there may exist distinct\npermutations of primitive tasks which compose to yield the same task as is common in many associative op-\nerations. As an example, in the context of mathematical problem-solving, the basic arithmetic operation could\nbe considered primitive. The composition of these primitive tasks allows us to construct extremely complex\noperations. Just like in linear algebra, we define the span of these tasks as the set obtained by their sequential\napplications.\nDefinition 2.3 (Span of tasks). Let $\\Phi \\subseteq \\Upsilon$ be a set of tasks, then:\n$\\text{Span}(\\Phi) = \\{\\lambda_1 \\circ \\dots \\circ \\lambda_k : \\lambda_i \\in \\Phi \\text{ for } 1 \\leq i \\leq k, k \\in \\mathbb{Z}_{>0}\\}.$\nThe set $\\text{Span}(\\Phi)$ comprises all the tasks that can be applied by composing sub-tasks in the set $\\Phi$. This means that\nany compositionally consistent update rule $\\Lambda$ which is well-defined on the set of tasks $\\Gamma$ will also be well-defined\non $\\text{Span}(\\Phi)$. However, this $\\Lambda$ may still be ill-defined for any task not in this span. This limitation is captured by\nthe concept of unidentifiability, which plays a central role in determining the boundaries of what a model can and\ncannot infer."}, {"title": "2.3 Unidentifiability", "content": "The unidentifiability of tasks forms a key part of our framework. It directly addresses the fundamental challenge\nthat models, such as LLMs, face when dealing with unseen tasks. If a task $\\lambda$ lies outside of $\\text{Span}(\\Phi)$, the span\nof tasks the model has been trained on, then the model cannot be expected to infer or apply it correctly. In other\nwords, the model's capacity is constrained by the identifiability of tasks within the training set. This notion and\nformalization of unidentifiability allows us to highlight a critical limitation in the generalization of models: tasks\nnot encountered during training cannot be reliably executed, as they remain beyond the model's learned task span.\nMore formally:\nDefinition 2.4 (Unidentifiability). Let $\\Phi \\subseteq \\Upsilon$ be any set of tasks, then a tasks $\\lambda$ is said to be unidentifiable in $\\Phi$\niff, $\\lambda \\notin \\text{Span}(\\Phi)$.\nRemark In practice, the concept of unidentifiability may depend on the initial state $X_0$. For instance, an LLM\nmight accurately perform addition for 2-digit numbers but fail with 10-digit numbers [Razeghi et al., 2022].\nOur framework can be extended to account for such cases by explicitly incorporating the distribution of initial\nstates into the notion of identifiability. For example, addition could be considered unidentifiable when the initial\nstate distribution is $p(X_0 \\mid X_0 \\text{ includes 10-digit numbers})$. However, for simplicity, we keep this distributional\ndependence implicit in the definition provided earlier.\nWith this general framework in place, we can now turn this theoretical foundation into a practical algorithm for\ndetecting unidentifiable sub-tasks. Specifically, we explore how the notion of unidentifiability can be combined\nwith information-theoretic approaches to detect failure points in LLMs."}, {"title": "3 Operationalising our framework", "content": "This section aims to operationalise the above framework to make inferences regarding the unidentifiability of\nintermediate sub-tasks in a model's CoT reasoning process. This would subsequently allow us to detect any\nsub-task at which a model's CoT reasoning process starts to diverge from the ground truth, thereby providing\ninsights into how the model can be improved. For example, suppose we are in a setting where the \u201caddition\u201d\noperation is unidentifiable, then we could further improve the model's mathematical reasoning by fine-tuning it\non the addition operation."}, {"title": "3.1 An information-theoretic perspective", "content": "To make the concept of unidentifiability practical in the context of CoT generations, we begin by introducing the\nfundamental assumption. The core assumption in our approach is that each correctly executed CoT reasoning step\nshould contribute meaningful and relevant information that aids in predicting the correct final output, denoted as\n$Y$. If we encounter a step after which the amount of information regarding $Y$ stops increasing, then we can take\nthis as an indication of an incorrectly executed task. We concretise this assumption using using our notation from\nthe previous section:\nAssumption 3.1 (Bayesian network). Let $\\lambda \\neq \\lambda'$ be two operations with primitive decompositions:\n$\\lambda = \\lambda_1 \\circ \\dots \\lambda_{k-1} \\circ \\lambda_k \\circ \\lambda_{k+1} \\dots \\circ \\lambda_{T} \\text{ and } \\lambda' = \\lambda_1 \\circ \\dots \\lambda_{k-1} \\circ \\lambda_k' \\circ \\lambda_{k+1}' \\dots \\circ \\lambda'_{T'}$,\nwhere $\\lambda_k$ is unidentifiable in $\\{\\lambda_1,...,\\lambda_T\\}$. Then, the intermediate states corresponding to the tasks $\\lambda, \\lambda'$ have\nthe following Bayesian network:\nIntuition The Bayesian network in Figure 2 implies that for any two reasoning paths which diverge at step $k$, the\nfuture states $X_i$ and $X_i'$ for any $i, j > k$ satisfy the conditional independence $X_i \\amalg X_j' \\mid X_{k-1}$. Consequently,\nonce we apply $\\lambda'$, the subsequent states along the new reasoning path (in red) add no information regarding the\nsubsequent states or the output of the original path (in green). Hence the figure represents the fact that, for any\ngiven input, the output of $\\lambda_k$ (top fork) contains no information regarding the output of any other primitive task\n$\\lambda'$ (bottom fork).\nNow that we have formalised our key information-theoretic assumption on the ground-truth CoT process, we turn\ntowards the model behaviour on unidentifiable tasks in the following section."}, {"title": "3.2 Task execution in LLMs", "content": "To operationalise our framework, we formally distinguish between the model i.e. LLM's task execution and the\nground truth process which arises from following the instructions correctly. To this end, we explicitly define how\nan LLM interprets a specified task $\\lambda$ using the update rule, $\\Lambda_M(X_0, \\lambda)$, which is in general distinct from the\nground truth update rule $\\Lambda(X_0, \\lambda)$.\nHere, one option would be to consider the idealised setting where the model learns to perfectly follow some of the\nprimitive tasks available in the training data. However, this may be considered too restrictive since in reality most\nLLMs do not always follow a \"learned\u201d task perfectly. Instead, we consider a much weaker assumption that the\nmodel cannot correctly execute a task which is unidentifiable in the training data. To this end, suppose $\\Gamma_M \\subseteq \\Gamma$\ndenotes the primitive tasks available in the LLM training data. Concretely, we make the following assumption on\nLLM's task execution.\nAssumption 3.2 (Task execution in LLMs). $\\Lambda_M$ is compositionally consistent and for any $(x_0, \\lambda) \\in \\mathcal{X} \\times \\Upsilon$,\nthere exists some $\\hat{\\lambda} \\in \\text{Span}(\\Gamma_M)$ such that $\\Lambda_M(x_0, \\lambda) \\subseteq \\Lambda(x_0, \\lambda)$.\nIntuition Assumption 3.2 means that for any task which we would like the LLM to apply, the LLM ends up\nexecuting some task in $\\text{Span}(\\Gamma_M)$ which the model has been trained on. In other words, the model's execution\nis restricted only to the tasks which could be inferred from the training data (i.e. in $\\text{Span}(\\Gamma_M)$). Moreover, this\nassumption also allows us to encapsulate cases where the model does not follow the correct instructions or does\nnot decompose a given task correctly."}, {"title": "3.3 Testing for unidentifiability using information gain", "content": "Having established all the essential components of our framework, we can now provide a concrete description\nof how to practically identify unidentifiable sub-tasks using information theory. As is common in the literature\n[Wang et al., 2024b, Havrilla et al., 2024], we assume access to a dataset consisting of prompts and their corre-\nsponding final answers, obtained by correctly applying the task $\\lambda$. This dataset is denoted as $\\mathcal{D}_\\lambda := \\{(x, y)\\}_{i=1}^{N}$.\nAdditionally, recall that $X_j^M$ and $X_{j-1}^M$ represent the model's chain of thought (CoT) reasoning at steps $j$ and\n$j - 1$, respectively. Consequently, each element in the conditional independence statement in Equation (2) can\nbe derived from the data and/or the model.\nTo this end, we consider the mutual information between $Y$ and $X_j^M$ conditional on $X_{j-1}^M$, denoted by $I(Y; X_j^M \\mid\nX_{j-1}^M)$. This conditional mutual information term intuitively represents the additional information contributed\nby the $j$'th step of CoT, that is relevant for predicting the ground truth final output $Y$. Therefore, we refer to\n$I(Y; X_j^M \\mid X_{j-1}^M)$ as the information gain at step $j$.\nIt follows from Theorem 3.3 that if an LLM encounters a sub-task at step $i$ which is unidentifiable in its training\ndata, no subsequent step should contribute any additional information relevant for predicting $Y$ (i.e. the informa-\ntion gain should remain 0 after step $i$). If, on the other hand, we observe that $I(Y; X_j^M \\mid X_{j-1}^M) > 0$ for some\n$j \\geq i$, then under Assumptions 3.1 and 3.2, the task $i$ is not unidentifiable. To estimate the information gain in\npractice, we use the following result:\nProposition 3.4. Let $I(X; Y \\mid Z)$ denote the mutual information between random variables $X$ and $Y$ condi-\ntional on $Z$. Then,\n$\\mathbb{E}[\\log p(Y \\mid X_j^M)] - \\mathbb{E}[\\log p(Y \\mid X_{j-1}^M)] = I(Y; X_j^M \\mid X_{j-1}^M) \\geq 0$.\n(3)\nTo estimate the information gain in (3) using Proposition 3.4, we train a separate LLM, which we refer to as the\nsupervisor model $g_{\\text{sup}}$. This model takes as input the model's CoT reasoning up to any given intermediate step $t$,\n$X_t^M$, and is fine-tuned to directly predict the ground truth final output $Y$. In this way $g_{\\text{sup}}(X_t^M)$ approximates the\nconditional distribution $p(Y \\mid X_t^M)$. Then, the quantity $\\mathbb{E}[\\log p(Y \\mid X_t^M)]$ can be estimated using the negative\ncross-entropy loss for predicting $Y$, i.e.\n$\\mathbb{E}[\\log p(Y \\mid X_t^M)] \\approx \\mathbb{E}[\\log \\hat{p}(Y \\mid X_t^M)] = -\\mathbb{E}[l_{CE}(Y, g_{\\text{sup}}(X_t^M))]$,\nwhere $l_{CE}$ denotes the cross-entropy loss. From this, it follows that\n$\\mathbb{E}[\\log p(Y \\mid X_j^M)] - \\mathbb{E}[\\log p(Y \\mid X_{j-1}^M)] \\approx \\mathbb{E}[l_{CE}(Y, g_{\\text{sup}}(X_{j-1}^M))] - \\mathbb{E}[l_{CE}(Y, g_{\\text{sup}}(X_j^M))].$\n(4)\nSummary: The information gain (IG) between steps $j$ and $j - 1$ reflects how much relevant information step\n$j$ contributes towards predicting $Y$. If task $\\lambda_j$ is executed correctly, this gain is positive, as indicated by a\ndecrease in the cross-entropy loss. Conversely, if step $j$ does not provide additional information, the loss remains\nunchanged. This can be interpreted as the conditional mutual information between $X_j^M$ and $Y$, conditioned on"}, {"title": "4 Related works", "content": "Evaluation of CoT reasoning Several recent works propose methodologies for evaluating CoT reasoning [Wei\net al., 2024, Havrilla et al., 2024, Li et al., 2023, Joshi et al., 2023, Nguyen et al., 2024, Wang et al., 2024a, Yu\net al., 2024, Xie et al., 2024]. For example, Li et al. [2023] verifies individual steps in a model's CoT reasoning by\ngenerating multiple LLM responses per prompt and comparing correct responses with incorrect ones. Similarly,\nWang et al. [2024b,c] use a fine-tuned LLM to decode multiple reasoning paths from each step and check the\ncorrectness of these reasoning paths. However, as we show in our experiments, approaches which simply rely\non the correctness of the final output are not sound in general and can lead to false positives. Moreover, these\nsolutions may not be plausible for problems of high difficulty where correct LLM responses might be scarce.\nFormalising CoT framework The formalisation of LLM reasoning remains an active area of research. Most\nnotably Gonz\u00e1lez and Nori [2023] introduces a formal framework for LLMs and is a key source of inspiration\nbehind our formalism. Additionally, Feng et al. [2023] theoretically examines the expressivity of LLMs with CoT\nin solving mathematical and decision-making problems, focusing on the transformer architecture's implications\non accuracy. Besides this, Xu et al. [2024] provides a formal definition of hallucinations, but does not consider\nCoT reasoning specifically.\nReward modelling One notable line of work known as outcome-based reward models (ORM) [Cobbe et al.,\n2021, Havrilla et al., 2024, Lightman et al., 2023] predicts the probability of reaching the correct final answer\ngiven a model's intermediate CoT steps. While ORMs do not require demonstrations of correct intermediate\nsteps, we show in Section 5 that this approach is not sound for detecting errors in a model's CoT reasoning.\nAnother related method is step-wise ORM (SORM) Havrilla et al. [2024] which estimates the probability of an\n'optimal' model reaching a correct answer, given the CoT reasoning of our model of interest. However, unlike\nour approach, SORM requires training a model which is larger and more capable than our base model.\nProcess-based reward modelling (PRMs) [Lightman et al., 2023, Uesato et al., 2022] is an alternative approach\nwhich directly predicts the correctness of intermediate CoT reasoning steps. Likewise, various other approaches\nrely on annotated CoT datasets for benchmarking [Jacovi et al., 2024, Yu et al., 2024, Amini et al., 2019, Liu\net al., 2020, Xi et al., 2024, Nguyen et al., 2024, Xie et al., 2024, McLeish et al., 2024]. While these benchmarks\nand methodologies can be valuable for improving LLM reasoning, collecting annotated data can be very costly\nand is not readily scalable to other tasks. Unlike these methods, our approach computes the information gain at\neach step, providing a richer measure of LLM performance without requiring any human-annotated CoT data."}, {"title": "5 Experiments", "content": "In this section, we empirically demonstrate the practical utility of our framework. In addition to our proposed\nmethod dubbed information gain (denoted by IG), we consider two common baselines that can be used to detect\nthe errors in a model's CoT reasoning and assume access to only the model's CoT generations $X_0^M, X_1^M, ..., X_T^M$\nas well as the correct final answers denoted as $Y$.\nOutcome Reward Model (ORM) [Havrilla et al., 2024] This involves training a classifier, denoted as $f_{ORM}$,\nwhich takes as input model generations up to any step $t$ in its CoT reasoning, $X_t^M$, and predicts the probability\nof the model's final answer being correct, i.e.\n$f_{ORM}(X_t^M) \\approx P(Y^M = Y \\mid X_t^M)$.\n(6)\nHere, if we observe that this probability of correctness drops significantly after step $t$, i.e. if $f_{ORM}(X_t^M) \\gg\nf_{FORM}(X_{t+1}^M)$, this indicates that the model does not apply the task $\\lambda_{t+1}$ correctly."}, {"title": "5.1 Toy data experiments", "content": "First, we consider a toy setting where we have full control over the model behaviour on different tasks. Our\nprompts comprise of an integer vector $Z_0 \\in \\mathbb{Z}^5$ sampled randomly from a given distribution. The task $\\lambda$ com-\nprises 5-steps $\\lambda = \\lambda_1 \\circ \\dots \\circ \\lambda_5$, where each sub-task $i$ denotes an operation which transforms a given integer\nvector $Z_{i-1} \\in \\mathbb{Z}^5$ into another $Z_i \\in \\mathbb{Z}^5$. Finally, in this setup, the correct final answer $Y$ is the value of $Z_5$.\nAdditional details on the data generating mechanism as well as the sub-tasks are provided in Appendix B.1.\nGenerating the dataset To investigate partial unidentifiability for a given task $\\lambda_i$ we modify the obtained\ndataset by introducing 'noise' at step $i$. In other words, the task $i$ is applied incorrectly on a subset of the\ndata, whereas all other tasks are always applied correctly. This represents a model which sometimes fails at\nstep $i$ and we use 'LLM$'i$' to denote this model in this experiment. We repeat this procedure for all tasks $i$ for\n$i \\in \\{1,...,5\\}$ which yields 5 LLMs $\\{LLM_1, ..., LLM_5\\}$.\nTo also investigate the robustness of the methods, we introduce a special case in LLM3. Here, task $\\lambda_3$ is applied\nincorrectly if and only if the output after task 2 (i.e., after $\\lambda_2$) lies in some set $S$. This choice has been made\ndeliberately to highlight a pitfall of the existing baselines (as we will explain below) and is in contrast to the rest\nof LLMs where any errors occur at random. In other words, the correctness of task $\\lambda_3$ is dependent on the output\nof $\\lambda_2$. For more details, see Appendix B.1.2."}, {"title": "5.1.1 Results", "content": "Figure 3 shows how the different baselines quantify the correctness of the different tasks for the 5 different\nLLMs under consideration. This figure only considers samples where the final answer of the LLM was incorrect,\ni.e. $Y^M \\neq Y$. For our method (IG), Figure 3a shows the information gain across the different steps for each\nLLM. Likewise, Figure 3b presents the results for ORM and shows how the average probability of correctness\nin (6) changes across the different steps, whereas, for Math-Shepherd, Figure 3c shows the proportion of correct\ncompletions starting after each step (7). Here, any significant drop in the plotted values indicate an incorrect\napplication of a task.\nInformation gain accurately quantifies step-wise correctness We observe that for each LLM the information\ngain remains positive until we encounter an incorrect reasoning step, at which point it drops to negative values."}, {"title": "5.2 Arithmetic operations on LLama-3-8B", "content": "Following our toy experiments, we now evaluate our framework in a more realistic setting using the Llama-3-8B\nmodel [Dubey et al., 2024]. We focus on a simple arithmetic task that involves both multiplication and addition\ntasks. The goal is to assess the model's performance on individual operations as well as their combination.\nExperimental setup We sample two integers $x$ and $y$ uniformly from the range [1, 100000). The prompt given\nto the model is structured as follows:\nPrompt: \u201cx = {x}, y = {y}, Please calculate the following: 1. 3x, 2. 2y, 3. 3x + 2y\u201d\nModel accuracy We observe that the model's accuracy varies across the three steps:\nNotably, the majority of failures occur in the third step, which\ninvolves addition of the previously computed values. We ana-\nlyzed the distribution of (x, y) values where the model obtains\nthe correct final output. Interestingly, as Figure 4 illustrates, we\nobserved that most errors occur when exactly one of the vari-\nables (x, y) is large and the other is small. This suggests that\nthe model's correctness is highly dependent on the (x, y) values\nin the prompt, resulting in baselines struggling to identify the er-\nroneous step in the model's CoT reasoning (as we show below)."}, {"title": "5.2.1 Results", "content": "Our Method We trained the supervisor model by fine-tuning\na Llama-3-8b model using Low Rank Adaptation (LoRA) [Hu\net al., 2021]. Table 2 shows that there is a significant drop in\ninformation gain at step 3 relative to steps 1 and 2, demonstrating\nthat our information-theoretic method is able to correctly identify\nthat the failure mainly occurs at step 3.\nOutcome Reward Model (ORM) In contrast, for ORM the\nmean probability of correctness included in Table 2 remains un-\nchanged at each step. This could be explained by Figure 4 which\nsuggests that ORM classifier can predict the correctness of the final output using only the values of x and y avail-\nable in the prompt. Crucially, the classifier's confidence remains unchanged even as the model's intermediate"}, {"title": "5.3 Experiments on the Controlled GSM-8K Dataset", "content": "To evaluate our method on a complex dataset, we conducted experiments on GSM-8K [Cobbe et al., 2021", "2024": "to generate answers for GSM-8K questions where the \"multipli-\ncation\" operation is always done incorrectly, while all other operations are correct. Next, we filtered the dataset\nto ensure that \"multiplication\u201d, \u201csubtraction\u201d, and \u201caddition\u201d never appeared together within the same Chain of\nThought (CoT) solution. In particular, we ensured in our setting that, all incorrect final answers included both\n\"multiplication\" and \"subtraction\", whereas correct final answers did not involve either operation. This introduces\na spurious correlation between \"subtraction\" and wrong answers.\nIn this setup, we mainly focused on evaluating ORM and our proposed method, as Math-Shepherd (with the same\ncompleter) fails trivially under these conditions. Specifically, \u201cmultiplication\u201d is inherently unidentifiable, since\nany CoT containing \u201cmultiplication\u201d negates the"}]}