{"title": "Review of Explainable Graph-Based Recommender Systems", "authors": ["THANET MARKCHOM", "HUIZHI LIANG", "JAMES FERRYMAN"], "abstract": "Explainability of recommender systems has become essential to ensure users' trust and satisfaction. Various types of explainable recommender systems have been proposed including explainable graph-based recommender systems. This review paper discusses state-of-the-art approaches of these systems and categorizes them based on three aspects: learning methods, explaining methods, and explanation types. It also explores the commonly used datasets, explainability evaluation methods, and future directions of this research area. Compared with the existing review papers, this paper focuses on explainability based on graphs and covers the topics required for developing novel explainable graph-based recommender systems.", "sections": [{"title": "1 INTRODUCTION", "content": "Due to the large exponential growth of information online, users are usually bombarded with an excessive number of choices. To cope with this information overload issue, recommender systems have become an essential tool to suggest pieces of information (items) that potentially match users' personal interests. During the early era of recommender systems, most were developed as black-box systems, meaning the internal mechanisms or decision-making processes were not transparent or easily interpretable [3]. However, recent research has shed some light on potential issues that might be caused by using recommender systems without comprehension of how they generate outputs [6]. These issues include biases in making decisions and ethical violations which can cause serious consequences [65]. To address such issues, recent regulations, such as the General Data Protection Regulation (GDPR) of the European Union and other countries [31], have been established. This emphasizes the importance of developing recommender systems that not only perform well in terms of accuracy but also provide explainability of recommendations [115].\nRecognizing this necessity, numerous studies in recommender systems have focused on developing explainable recommender systems that are transparent and understandable. These systems aim to generate accurate recommendations while offering explanations for their recommendations or encouraging the predictability of explainable recommendations over non-explainable ones. These systems allow users to better understand the decision-making process, leading to increased trust and confidence in the systems. Also, by providing clear explanations for their recommendations, any biases that may be present in the data or algorithms can be identified and addressed accordingly. This ensures that the recommendations are fair, unbiased, and ethically sound.\nTo achieve both accuracy and explainability in recommendations, various resources have been utilized, including graphs. A graph is a collection of nodes (or vertices) and relations (or edges) connecting pairs of nodes. Each edge represents a relationship or connection between two nodes. In recommendation scenarios, nodes can represent entities such as users, items, or attributes, while relations signify the connections between them such as user-item interactions, social connections between users, and item attributes. In recent years, graphs have been widely leveraged to develop explainable graph-based recommender systems [27]. These systems leverage graph structures to make personalized recommendations while also providing explainability of their recommendations via connectivity information within graphs. Unlike traditional recommender systems that predominantly rely on user-item interactions, explainable graph-based recommender systems leverage a variety of interconnected entities and relationships via multi-hop relations [93, 96, 103, 117]. A multi-hop relation represents a high-order connection between two nodes that are not adjacent. For example, in Figure 1, there is a multi-hop relation between \"User 1\" and \"T-shirt D\" through a path \"User1\" - \"T-shirt C\" - \"Category: T-shirt\" - \"T-shirt D\u201d. Based on this path, it is possible to recommend \"T-shirt D\u201d to \u201cUser 1\u201d since they are linked through a high-order connection. Also, this multi-hop relation that connects them can serve as an explanation. This explanation can be interpreted as \"T-shirt D\" is recommended to \"User 1\" because it is in the same category as one of \"User 1\"'s previously bought items.\nExplainable graph-based recommender systems represent a pivotal advancement in the recommendation research domain. They have garnered significant interest and have undergone continuous development [22, 33]. When developing an explainable graph-based recommender system, there are three principle aspects that should be considered: (1) a learning method, (2) an explaining method, and (3) an explanation type.\n\u2022 A learning method pertains to the model architecture used to extract connectivity information within a graph for recommendation learning. In graph-based recommender systems, there are three approaches: embedding- based, path-based, and hybrid approaches [33]. Understanding these methods aids in the selection of the suitable architecture for developing a graph-based recommender system, taking into account the characteristics of available data and the methods used for extracting multi-hop relations.\n\u2022 An explaining method refers to the mechanism of the explainability component within an explainable model. It can be categorized as either model-specific, designed to provide insights into the decision-making process of a particular model, or model-agnostic, aiming to offer broadly applicable explanations across different models [42]."}, {"title": "Review of Explainable Graph-Based Recommender Systems", "content": "Comprehending these mechanisms is crucial for incorporating explainability into graph-based recommender systems.\n\u2022 An explanation type refers to the form of explainability provided by an explainable graph-based recommender system. Given the diverse information within a graph, diverse formats of explanations can be offered, for instance, node-level explanations (e.g., predictive/significant nodes) or path-level explanations (e.g., selected paths connecting users and recommended items). When developing an explainable graph-based recommender system, the choice of explanation type should be carefully considered to align with the expectations of stakeholders. Some scenarios may necessitate node-level explanations, focusing on detailed insights of individual nodes, or higher-level path explanations that emphasize interconnected relationships and sequences of nodes influencing specific recommendations.\nGaining insight into various learning methods, explaining methods, and explanation types can be highly beneficial for developing an explainable graph-based recommender system. It enables developers or researchers to make more informed decisions regarding the model architecture, explainability component, and the format of explanations generated by the model. Therefore, to facilitate advances in explainable graph-based recommender systems, this review paper summarizes state-of-the-art explainable graph-based recommender systems and categorizes them based on these three aspects. Furthermore, this paper provides a summary of benchmark datasets and explainability evaluation techniques necessary for examining and validating the performances of explainable graph-based recommender systems. This will allow new researchers or anyone interested in explainable graph-based recommender systems to conveniently grasp the whole picture and identify the gaps in this research area.\nContributions and Differences. The contributions of our review paper and the differences compared to other previously published review papers are summarized as follows:\n\u2022 This review focuses on explainable graph-based recommender systems unlike previous review papers that focus on graph-based recommend systems [27, 33, 102], explainable recommender systems encompassing various types, not exclusively graph-based ones [18, 115], explainable graph-based artificial intelligence [88] or explainable artificial intelligence [32, 65], which are broader topics compared to ours.\n\u2022 In this review, explainable graph-based recommender systems are categorized based on three aspects, i.e., learning method, explaining method, and explanation type.\n\u2022 In [22], the focus is on explainable graph-based recommender systems using deep learning. In contrast, our paper covers both deep learning-based and machine learning-based systems. While [22] categorizes explainable graph-based recommender systems based on how graphs, graph embeddings, or path embeddings are learned and merged with deep learning methods, our paper focuses on categorizing these systems across various aspects. Additionally, our survey paper discusses datasets and evaluation methods used in state-of-the-art explainable graph-based recommender systems.\n\u2022 In [15], the authors focus on visual explanations in recommender systems, exploring various types of explainable recommender systems broadly. In contrast, our paper reviews explainable graph-based recommender systems, covering aspects from their structure to the visualization of obtained explanations. It provides a more in-depth focus on this specific type of recommender system.\nThe rest of the paper is organized as follows: Section 2 summarizes definitions, notations, and technical terms typically used in explainable graph-based recommender systems to provide background knowledge for the subsequent"}, {"title": "4", "content": "sections. Additionally, in this section, the categorization of explainable graph-based recommender systems is explained. Section 3 discusses the variety of learning methods in explainable graph-based recommender systems and presents state-of-the-art systems categorized by their respective learning approaches. Section 4 explores explainable graph-based recommender systems from the aspect of their explaining methods and provides a summary of the state-of-the-art using different explaining methods. Section 5 delves into different explanation types available in explainable graph-based recommender systems and presents state-of-the-art systems, categorized based on these various types. Section 6 presents commonly used datasets for explainable graph-based recommender systems. Section 7 discusses methods for evaluating explainability in graph-based recommender systems. Finally, Section 8 provides conclusions and outlines future directions for explainable graph-based recommender systems.\n2 PRELIMINARIES\nBefore discussing state-of-the-art explainable graph-based recommender systems, this section provides definitions, notations, and technical terms commonly used in these recommender systems. Then, it explains the categorizations of explainable graph-based recommender systems in this paper, laying the groundwork for the subsequent sections.\n2.1 Definitions, Notations, and Technical Terms\nDEFINITION 1. (Graph) In the context of recommendation, a graph (also known as a knowledge graph or an information network) is defined as a directed graph $G = (N, R)$, where $N$ is a set of nodes (or entities), and $R$ is a set of relations (or edges). Each node and relation is associated with its type mapping function: $ \\phi : N \\rightarrow N$ and $ \\varphi : R \\rightarrow R$, respectively, where $N$ represents the set of node types, and $R$ represents the set of relation types. In a graph, a triplet $(h, r, t)$ represents a relationship between the head node $h \\in N$ and the tail node $t \\in N$ connected by the relation $r \\in R$.\nDEFINITION 2. (Path) A path is a sequence of nodes where each adjacent pair is connected by a relation. Given a graph $G = (N, R)$, a path $(n_0, n_1, ..., n_{k-1}, n_k)$ exists if their exist triplets $(n_0, r_0, n_1), (n_1, r_1, n_2), ..., (n_{k-2}, r_{k-2}, n_{k-1}), (n_{k-1}, r_{k-1}, n_k)$ where $n_i \\in N$ for $i = 0, 1, ..., k$ and $r_i \\in R$ for $i = 0, 1, ..., k - 1$ in the graph. In other words, a path is a way to traverse a graph from one node to another by following the relations. The length of a path is determined by the number of relations it contains.\nDEFINITION 3. (Single-hop relation) Given a graph $G = (N, R)$, a single-hop relation refers to a direct relation between two nodes. Mathematically, a single-hop relation between nodes $n_0 \\in N$ and $n_1 \\in N$ exists if there exists a relation $r \\in R$ that connects them, i.e., there exists a triplet $(n_0, r, n_1)$ in the graph.\nDEFINITION 4. (Multi-hop relation) Given a graph $G = (N, R)$, a multi-hop relation refers to an indirect relation between two nodes through a path. Mathematically, a multi-hop relation between nodes $n_0 \\in N$ and $n_k \\in N$ exists, if there exists a path $(n_0, n_1, ..., n_{k-1}, n_k)$ such that $n_i \\in N$ for $i = 1, 2, ..., k - 1$ in the graph.\nDEFINITION 5. (Random walk) A random walk on a graph $G = (N,R)$ can be denoted as a sequence of nodes $n_0, n_1, ..., n_k$ where $n_i$ is the node visited at step $i$ for $i = 0, 1, ..., k$. It is obtained through a stochastic process defined by the sequence of random variables $X_0, X_1, ..., X_k$, where each $X_i$ represents the node visited at step $i$. The walk starts at an initial node $n_0 \\in N$, and at each step $i$, the walker moves to a neighboring node $n_{i+1}$ of the current node $n_i$ with equal probability. The random walk continues indefinitely or until it forms a sequence of visited nodes of a certain length."}, {"title": "5", "content": "DEFINITION 6. (Meta-path) [83] Given a graph $G = (N, R)$, a meta-path $\\pi$, defined as\n$N_1 \\xrightarrow{R_{N_1,N_2}} N_2 \\xrightarrow{} ... \\xrightarrow{} N_{l+1}$\n(abbreviated as $N_1 N_2  N_{l+1}$), describes a composite relation $R_{N_1,N_2} \\circ   R_{N_1,N_{l+1}}$ between $N_1$ and $N_{l+1}$ where $ \\circ $ denotes the composition operator on relations. A meta-path signifies a structural relationship among various types of nodes and relations in a graph. It serves as a tool to extract high-order connectivity information under a specific assumption within a graph. The length of meta-path $ \\pi $ is denoted by $|\\pi|$.\nDEFINITION 7. (Meta-path based random walk) Given a graph $G = (N, R)$ and a meta-path $ \\pi = N_1 N_2 N_{l+1}$, a meta-path-based random walk is a sequence of nodes $n_0, n_1, ..., n_{l+1}$ where $n_i$ is the node visited at step $i$ for $i = 0, 1, ..., k$ such that $n_i$ has the type $N_i$ in $\\pi$. It is obtained via a stochastic process on a graph, where the sequence of nodes visited is influenced by a predefined meta-path, capturing specific semantic relationships between nodes. The random walk continues until it reaches the last node type in the meta-path or continues indefinitely. In the case that it continues indefinitely, once it reaches the last node type in the meta-path, it goes back to the first node type and repeats following the meta-path again.\nDEFINITION 8. (Node embedding method) A node embedding method refers to the process of finding a mapping function $ \\phi : N \\rightarrow R^{|V| \\times d}$, where $d << |V|$. This function maps a node into a latent space and generates a latent representation (embedding) of this node, capturing graph topological information. In a graph, node embeddings for all nodes can be obtained and utilized as input or features for learning recommendations.\nDEFINITION 9. (Explainability and interpretability) Explainable artificial intelligence (AI), including recom- mender systems, can provide explanations or allow a human to understand the logic behind its decision-making process. In the research area of explainable AI, two terms are usually mentioned, i.e., \u201cexplainability\u201d and \u201cinterpretability\u201d. According to the previous work [6, 32], the term explainability refers to the ability to provide an explanation of why a decision is made. Meanwhile, the term interpretability refers to the characteristic of an AI system in which its internal mechanism is comprehensible for a human. In the case of interpretable models, developers or users may take advantage of their interpretability to extract some explanations. Thus, whether an Al system has explainability or interpretability, it is possible to obtain an explanation from it.\nFor instance, linear/logistic regression models are interpretable models because their coefficients directly indicate the magnitude and direction of influence that each corresponding input feature has on the output [11]. The relationship between input variables and the model's predictions makes these models interpretable and enables them to provide explanations for the predictions. On the other hand, neural networks are not interpretable due to their black-box internal mechanisms [32]. However, explanations can be derived from these networks by using techniques such as LIME [75], Anchors [76], SHAP [58], Global Sensitivity Analysis [20], ASTRID [37] or Concept Activation Vectors (CAVs) [44]. In other words, these methods allow neural networks to be explainable, providing them with the ability to offer explanations.\n2.2 Categorization of Explainable Graph-Based Recommender Systems\nExplainable graph-based recommender systems can be categorized by many aspects. In this survey, we consider three following aspects:"}, {"title": "6", "content": "(1) Types of learning method (embedding-based, path-based, or hybrid): Graph-based recommender systems can be categorized by their learning methods into three categories, i.e., embedding-based, path-based, and hybrid approaches. In an embedding-based approach, node embeddings are utilized to capture either local neighborhood structures or global graph structures, without explicit consideration of the actual paths within the graphs. A path-based approach, on the other hand, explicitly extracts paths from a graph and incorporates these paths as information for learning recommendations. Lastly, a hybrid approach combines both path-based and embedding-based approaches to leverage the strengths of both methodologies.\n(2) Types of explaining method (model-specific or model-agnostic): Considering explaining methods, explain- able graph-based recommender systems can be classified into two groups: model-specific and model-agnostic. Model-specific explainable graph-based recommender systems are characterized by explainability components that are tailor-made for their own structures and functionalities. In contrast, model-agnostic explainable graph- based recommender systems encompass post-hoc explainability components capable of application not only to themselves but also to other diverse recommender systems.\n(3) Types of explanations (node-level, path-level, meta-path-level or implicit): Explainable graph-based rec- ommender systems can further be categorized based on the formats of their explanations. These explanations can be in various forms, including node-level explanations (e.g., predictive or significant nodes, counterfactual nodes, or node neighborhoods), path-level explanations (e.g., selected paths connecting users and recommended items), and meta-path-level explanations (e.g., predictive or significant meta-paths). Additionally, there exists an implicit level, where explicit explanations are not generated alongside recommendations. Instead, recommendations with higher explainability are emphasized over those with lower explainability in this category."}, {"title": "3 CATEGORIZATION BY LEARNING METHODS: EMBEDDING-BASED, PATH-BASED, AND HYBRID APPROACHES", "content": "As discussed in Section 2, explainable graph-based recommender systems can be categorized based on various aspects. This section focuses on the categorization by learning methods: embedding-based, path-based, and hybrid approaches. In this section, the concepts of each approach and the models within each approach are discussed, along with the advantages and disadvantages of each approach.\n3.1 Embedding-Based Approach\nAn embedding-based approach uses graph embedding methods [12] to generate node embeddings and incorporate them into recommendation frameworks. These embeddings reflect the structural information of users, items, and other entities in a graph. They provide connectivity information for graph-based recommendation learning frameworks. This allows explanations to be generated by reasoning over the embeddings. Several methods can be applied to generate node embeddings, such as autoencoders, translation-based models, reinforcement learning, and GNNs. The following paragraphs discuss the use of each method within embedding-based models for explainable graph-based recommendations."}, {"title": "8", "content": "3.1.1 Autoencoder An autoencoder is a type of neural network used for learning efficient representations of data by encoding the input into a lower-dimensional space and then reconstructing it. The original purpose of autoencoders was to achieve data compression and dimensionality reduction, allowing the model to capture essential features while discarding less critical information. Despite its original purpose, an autoencoder can also be used to build explainable graph-based recommender systems. Bellini et al. [8] proposed an explainable recommender system based on Semantics- Aware Autoencoders (SemAuto) [7]. Originally, SemAuto was developed to incorporate structural information in a graph into an autoencoder. Each neuron in SemAuto represents a node in the adopted graph, and each link between neurons represents the edge connecting the nodes represented by these neurons. The weights on all neurons are learned and then extracted to build user profile representations. Since neurons correspond to nodes in the graph, the importance of each node toward a specific user can be discovered and used as an explanation.\n3.1.2 Translation-based models Some embeddings-based models use translation-based methods such as TransE [10] and TransR [53] to generate node embeddings. Given a triplet $(h, r, t)$ in a graph where $h$ denotes a head node, $r$ denotes a relation, and $t$ denotes a tail node. These methods are based on the assumption that the information from $h$ combined with the information from $r$ should equal the information from $t$, i.e., $h + r = t$. These embeddings have been proven to be highly flexible for many downstream tasks including recommendation [92]. The following are examples of state-of-the-art explainable graph-based recommender systems using translation-based models:\n\u2022 Ai et al. [4] modeled the distribution of node and relation embeddings by optimizing the generative probability of observed relation triplets similarly to TransE. To explain each recommendation, a soft matching algorithm was proposed to generate explanations for the recommended items. Given a pair of user and his/her recommended item, a breadth-first search was initially applied to find potential explanation paths. Then, for each of these paths, the probability based on the learned distribution of node and relation embeddings was computed. The path with the highest probability was applied to a predefined template in natural languages to generate a final explanation for the user.\n\u2022 Zhu et al. [120] proposed neural logic reasoning for explainable recommendation (LOGER) leveraging a neural logic model to guide the path-reasoning process for generating explanations. A set of logical rules based on user-item interactions was first mined from a graph. Then, for each user, a set of personalized rule importance scores was generated by using Markov Logic Networks [73], an interpretable probabilistic logic reasoning method. Subsequently, these personalized scores and the learned node embeddings from TransE were jointly considered to train an LSTM-based path reasoning network. Starting from a given user, this network predicted a sequence of nodes and relations until it reached the recommended item forming an explanation path.\n3.1.3 Reinforcement learning models Finding an explanation/reasoning path in a graph can be formulated as a deterministic Markov Decision Process (MDP) over a graph. Thus, some embedding-based models use a reinforcement learning (RL) method to navigate reasoning paths connecting a user and his/her recommended item. These models typically build a policy network to find such a path attempting to achieve the highest cumulative reward at the end of navigation. Starting from a given user, an agent in an RL-based model navigates from one node to the next adjacent node until it reaches an item of interest. This leaves the navigated path to be used as an explanation for why this item is recommended to this user. Here are examples of explainable graph-based recommender systems using RL models:\n\u2022 Xian et al. [103] proposed Policy-Guided Path Reasoning (PGPR), using RL to navigate a graph to find rec- ommended items. Generally, navigating through nodes and relations in a graph requires a high amount of"}, {"title": "9", "content": "computational resources. Therefore, in their work, a user-conditional action pruning strategy was proposed to decrease the size of the action spaces. Moreover, since the agent guided by the policy network is likely to repeatedly search the same path with the largest cumulative rewards, the recommendations typically lack diversity. To increase the diversity of both recommended items and explanation paths, a beam search-based algorithm guided by the policy network was used to sample diverse paths. These candidate paths are ranked according to their rewards from the reward function.\n\u2022 Tai et al. [86] proposed a user-centric path reasoning network (UCPR) that considers both local and user-centric views. The local view is based on a sequence of actions taken by the agent encoded by using LSTM. The user- centric view is corresponding with the potential user's demand information in the path reasoning process. This demand information was modeled by the user-centric view reasoning network. Assuming that user's demand depends on a group of nodes and relations, not just a single node or relation, a user demand portfolio can be represented by a set of triplets (h, r, t) where h is a node connected with a given user at the n-th hop. These triplets were assigned with different weights according to the user's demand. At each step, these weights were updated accordingly depending on the fulfilled demand in the previous step. The most relevant triplet was also identified during updating. The selected relevant triplets from every step then finally formed a reasoning path that reflects the highlighted triplets in the user demand portfolio.\n\u2022 Tao et al. [87] proposed a multi-modal knowledge-aware RL network called MKRLN to incorporate visual information in RL. This model utilizes both structural information from a graph and visual information from item images to generate state representations for the policy network. Based on these representations, the policy network learns to lead this user to his/her recommended item. Moreover, to reduce the size of action spaces, an attention mechanism was used to weight the importance of each neighbor. These attention weights were used to guide the agent to reduce the number of possible action spaces.\n\u2022 The previous work typically ignored the interpretability of the generated explanation paths discovered by their path reasoning/finding processes. Zhao et al. [117] proposed an adversarial actor-critic (ADAC) model for the demonstration-guided path finding to ensure the interpretability of the generated explanation paths. The idea is to supervise the path-finding process by using demonstrations. Due to the fact that there were no ground- truth paths for being used as demonstrations, they proposed a meta-heuristic-based demonstration extractor to generate a set of demonstrations without the ground truth. Three types of demonstrations were considered: (1) shortest path, (2) meta-path based random walk, and (3) path of interest (random-walk paths in which most of the nodes match with users' interests). To involve the demonstrations in the path-finding process, Generative Adversarial Imitation Learning [38] was adopted. Two discriminators, i.e., path and meta-path discriminators, were used to estimate how likely a discovered path follows the demonstrations. In this way, ADAC can learn to generate paths that are similar to the demonstrations to increase the interpretability of the explanations.\n\u2022 Wang et al. [96] proposed ReMR, an RL framework that leverages both instance-level and ontology-level information in graphs to model users' profiles. In this framework, a multi-level reasoning path extraction method was proposed to construct reasoning paths by using both high-level (ontology-level) and low-level (instance-level) concepts. First, an ontology-view graph was constructed by using the Microsoft Concept Graph and added to an original instance-view graph built from item meta-data. This combined graph was then used to perform recommendation along with path reasoning for providing explanations. Compared to existing recommender systems that only consider an instance-view graph, ReMR can better capture users' interests and provide more"}, {"title": "10", "content": "diverse explanations with high-level concepts. However, adding an ontology-view graph expands the search space of reinforcement learning. This may result in higher computational time required for learning path reasoning.\n\u2022 Zhao et al. [119] proposed a novel Time-aware Path reasoning for Recommendation method (TPRec), which leverages graphs augmented with temporal information. To augment a graph with time-aware interactions, temporal statistical and temporal structural features based on user-item relation timestamps were extracted. These features were projected into a temporal feature space and then were clustered by using Gaussian Mixture Model. Then, the user-item relations in the original graph were replaced by temporal relations based on their corresponding clusters in a temporal feature space. Based on the augmented graph, TransE was adopted to generate node embeddings. Then, the RL-based model was adopted to learn path reasoning for recommendation.\nGenerally, using RL techniques to find reasoning paths for recommendations is a straightforward way to provide explanations. However, the major drawback of this approach is the scalability issue due to the size of the action spaces. Using such techniques requires numerous computational resources especially when there are a large number of nodes and relations or the out-degrees of nodes in the graph are large. How to effectively and efficiently find reasoning paths in a graph is still a challenging research problem for developing RL-based models.\n3.1.4 GNN-based models Another line of embedding-based models uses Graph Neural Networks (GNNs) to recur- sively propagate multi-hop information [102]. These GNN-based models update each node embedding based on the embeddings of its neighbors and recursively perform such embedding propagation to capture high-order connectivity. They have shown that the neighborhood aggregation schemes are highly effective for capturing high-order relations in graphs [94]. To provide explanations, these systems heavily rely on attention mechanisms to identify nodes/relations with significant contributions toward user profiling and recommendation-making. Below are examples of state-of-the-art explainable graph-based recommender systems using GNNs:\n\u2022 Wang et al. [93] proposed a model called Knowledge Graph Attention Network (KGAT) which is an end-to- end model based on a Graph Convolution Network (GCN) and a Graph Attention Network [91]. First, TransR was adopted to generate initial node embeddings. Then, each node embedding was refined by recursively propagating the embeddings of its neighbors. During the propagation, to identify each neighborhood's importance, a knowledge-aware attention mechanism was proposed. Given a user and his/her recommended item, the attention weights of cascaded propagation form explanation paths connecting them. Despite the effectiveness, this model suffers from high computational time especially when the number of nodes and relations is large.\n\u2022 Yang et al. [110] proposed an explainable model using a hierarchical attention graph convolutional network called HAGERec. This model uses a bi-directional information propagation method to learn user and item repre- sentations. For each user/item node, HAGERec attentively aggregates information from its local neighborhood by using a hierarchical attention mechanism. Based on this attention mechanism, significant neighbors can be selected and used to explain the recommendation.\n\u2022 The aforementioned models rely only on original nodes and relations in graphs to model users' profiles. To effectively profile these users, some GNN-based models use higher concepts beyond nodes and relations in graphs to capture users' interests. Wang et al. [94] proposed a Knowledge Graph-based Intent Network (KGIN) to model users' underlying intents based on user-item interactions. These intents are normally represented by latent factors which are not easily interpretable. To improve interpretability, KGIN models these intents by using relations in a graph. Specifically, each intent is formed by attentively combining relation embeddings in a graph. The relation with the highest attention weight can be severed as an explanation of the user's intents. Also, a"}, {"title": "11", "content": "new aggregation method was proposed to effectively propagate information of high-order relations. Intuitively, each node in a graph has different semantics and meanings depending on its relational contexts. However, these contexts are usually ignored in most neighborhood-based aggregation methods in normal GNNs. Unlike these methods, their proposed method takes relational contexts into consideration when performing propagation. This allowed a GNN to infuse user intents into learning user and item representations.\n\u2022 In [55], an improved aggregation method for GCNs was introduced. Specifically, the traditional GCN model was modified to factorize user and item embeddings based on multiple concepts. To do so, the aggregation at the last layer was modified to discriminate information from the lower layers based on concepts. Specifically, all embeddings from the lower layer were categorized into different concepts and then propagated separately. To categorize the embeddings into different concepts, the factor affiliation degrees were estimated from the learnable parameters of each factor. This allowed them to identify significant concepts for learning user and item embeddings in the modified GCN model and use them as explanations.\n\u2022 Shimizu et al. [78] proposed KGAT+, an improved framework of KGAT that tackles the problem of scalability. The key idea is to compress one-to-many relations by using a latent class model. First, a set of target relations for compression was defined. If relation r in the triplet (h, r, t) is a target relation, then a latent class between h and t is assumed. In this way, the relation between \"h and t\" is decomposed into \"h and the latent class\" and \"t and the latent class\" with specific probabilities. Such probabilities are considered when computing the attention weights and loss for learning recommendations. According to their experiments, KGAT+ required less computational cost and produced recommendations with higher interpretability compared to KGAT.\n\u2022 Yu et al. [113] proposed Causality-guided Graph Learning for Session-based Recommendation (CGSR) that is capable of blocking shortcut paths (direct links between two items that disregard sequential dependencies and intermediate interactions) on the session graph while retaining crucial causal relations for learning users' preferences. In CGSR, two main components were introduced: distillation and aggregation. In the distillation component, the back-door adjustment of causality [19, 71, 112] was adopted to block shortcut paths in the session graph, containing sequential interactions of users within a given session. With this component, a distilled session graph containing causal relations among items was obtained. Subsequently, the second component was used to aggregate high-order information from different relation types on the distilled session graph by using GNNs. This aggregation process generated user and item representations for learning recommendations based on the causal relationships within this graph.\n\u2022 Taking into account causality and correlation relationships between items, Wu et al. [101] proposed a method called Causality and Correlation Graph Modeling for Effective and Explainable Session-based Recommendation (CGSR). In their method, three types of graphs were constructed, i.e., cause, effect, and correlation graphs. The cause graph and the effect graph were constructed based on causal relations, particularly cause relations and effect relations between items, respectively. The correlation graph was constructed by considering the first-order relationship in the session graph and various types of second-order relationships derived from the session graph. After obtaining these graphs, an end-to-end GNN-based model with an attention mechanism was applied to these graphs to generate three kinds of item embeddings and attentively aggregate them to obtain final session representations for predicting recommendations.\nTypically, the degree of nodes in a graph normally follows a long tail distribution. Numerous cold users and cold items with few interactions normally appear in graphs. To effectively capture the structural information of these cold"}, {"title": "12", "content": "users/items, more GNN layers are required compared to those active users/items with many interactions. Despite the fact that stacking multiple GNN layers expands the reception field of nodes and allows high-order connectivity to be considered, this can introduce noise during the propagation [46", "56": ".", "102": ".", "30": "and restarting strategy [48"}]}