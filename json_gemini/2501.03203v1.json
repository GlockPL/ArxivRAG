{"title": "Detecting AI-Generated Text in Educational Content: Leveraging Machine Learning\nand Explainable AI for Academic Integrity", "authors": ["Ayat A. Najjar", "Huthaifa I. Ashqar", "Omar A. Darwish", "Eman Hammad"], "abstract": "This study seeks to enhance academic integrity by providing tools to detect AI-generated content in student\nwork using advanced technologies. The findings promote transparency and accountability, helping\neducators maintain ethical standards and supporting the responsible integration of AI in education. A key\ncontribution of this work is the generation of the CyberHumanAI dataset, which has 1000 observations,\n500 of which are written by humans and the other 500 produced by ChatGPT. We evaluate various machine\nlearning (ML) and deep learning (DL) algorithms on the CyberHumanAI dataset comparing human-written\nand AI-generated content from Large Language Models (LLMs) (i.e., ChatGPT). Results demonstrate that\ntraditional ML algorithms, specifically XGBoost and Random Forest, achieve high performance (83% and\n81% accuracies respectively). Results also show that classifying shorter content seems to be more\nchallenging than classifying longer content. Further, using Explainable Artificial Intelligence (XAI) we\nidentify discriminative features influencing the ML model's predictions, where human-written content tends\nto use a practical language (e.g., use and allow). Meanwhile AI-generated text is characterized by more\nabstract and formal terms (e.g., realm and employ). Finally, a comparative analysis with GPTZero show\nthat our narrowly focused, simple, and fine-tuned model can outperform generalized systems like GPTZero.\nThe proposed model achieved approximately 77.5% accuracy compared to GPTZero's 48.5% accuracy\nwhen tasked to classify Pure AI, Pure Human, and mixed class. GPTZero showed a tendency to classify\nchallenging and small-content cases as either mixed or unrecognized while our proposed model showed a\nmore balanced performance across the three classes.", "sections": [{"title": "1. Introduction", "content": "Our communication practices are quickly changing due to the emergence of generative AI models. It is\nwidely used in various disciplines, including healthcare, academic research, the arts, and content\nproduction. Large Language Models (LLMs) has demonstrated performance in comprehending user\ninquiries and producing text that resembles human speech. LLMs attracted wide attention for researchers,\npolicymakers, and educators. Although LLMs is claimed to have the ability to transform society, there are\nsome potential risks as well. The advent of innovative AI-based chatbots, emphasizes the need to questions\nthe originality of the ideas, languages, and solutions (i.e., whether a sentence was generated by an AI or by\na human). Investigating the originality of a written idea has significant effects in several sectors including\ndigital forensics and information security. Defensive measures are necessary to prevent increasingly\ncomplex attacks that exploit textual content as a potent weapon due to the dynamic nature of cybersecurity,\nsuch as the transmission of false information and disinformation or social engineering attempts [1].\nSpecifically, in the field of information security, where the ability to recognize AI-generated material is\nvital, a detrimental application of AI is required. Additionally, it can spread false information and fake\nnews throughout online platforms [2]. Moreover, LLMs may also provide inaccurate answers and\ninformation since they were trained on outdated data, or they may suffer from hallucination [1], [3].\nAs the use of generative AI tools like ChatGPT becomes widespread in education [4], [5], [6], ensuring that\nstudent work is genuinely human-authored is a growing concern for educators. By developing a robust\nmodel to detect AI-generated text, this study provides a valuable tool for educators, promoting fairness and"}, {"title": "2. Related Work", "content": "The growing use of generative AI models in the arts, academics, healthcare, and content creation is a fast-\nchanging communication method [7], [8], [9]. The difference between text produced by humans and by AI\nmust be highlighted to identify and address the potential influence in these fields. These two studies [10],\n[11] worked on the difference between Al and human text generated. The first study [10] presented a\nparadigm for recognizing AI-generated material, especially in academic and scientific writing. A model is\ntrained using predetermined datasets, and it is then deployed on a cloud-based service. The suggested\nframework, which made use of artificial neural networks, obtained an accuracy of 89.95% compared to\ntools like OpenAI Text Classifier (42.08%) and ZeroGPT (87.5%). The other study [11] examined a\nplausible situation in which text is converted from human-written to AI-generated texts using neural\nlanguage models. It showed that annotators have difficulty with this activity but can become better with\nrewards. To encourage further research in human text recognition and evaluation, the study analyzed several\naspects influencing human detection performance, including model size and prompt genre. It also\nintroduces the RoFT dataset with 21,000 human annotations and mistake classifications [11].\nAcademic institutions raised concerns about plagiarism because more students are using LLMs for their\nassignments and term papers, which could affect their writing abilities. Many studies concentrated on\nrecognizing LLM-generated content across different areas such as [12], [13], [14], [15], [16], [17], [18],\n[19]. This study [12] examined the distinctions between medical writings produced by ChatGPT and those"}, {"title": "3. Methodology", "content": "Generally, data preparation and feature selection processes from the generated dataset play important roles\nin simplifying the overall subsequent tasks, like the classification task, and therefore leading to improved"}, {"title": "3.1 Dataset", "content": "We generated ChatGPT/Human cybersecurity paragraph dataset, which has about 1000 observations and\nwas compiled in September 2023. It has 500 paragraphs written by humans and another 500 produced by\nChatGPT, all of which are on cybersecurity and share the same title. This dataset to acts as a fundamental\nstep for creating machine-learning models capable of differentiating ChatGPT-generated cybersecurity\ndocuments. The human-written cybersecurity paragraphs were extracted from Wikipedia API using Python\nand through the keyword computer security. This unique generated dataset offers a great tool for researchers\nand practitioners who want to investigate and address cybersecurity document categorization problems\nusing ML methods.\nA preliminary check was done to find and remove empty observations. We prepared the dataset by stop\nwords removal, lemmatization, punctuation removal, and tokenization of the text [20] putting the text data\ninto a clean, structured format suited for classification and model creation. The word cloud for the two\nclasses human and ChatGPT is displayed in Figure (2) (a) and Figure (2) (b), respectively. Table (1) displays\nthe word frequency for the two classes as counts and percentages. Results compares the frequency of words\nused by humans and ChatGPT, which highlight the differences in their vocabulary when discussing topics\nrelated to security and computing. The word \"security\" is the most frequent for both, with humans using it\n420 times (1.71%) and ChatGPT using it 411 times (1.52%). However, differences emerge with other terms:\nhumans tend to use \"use\" (312 counts, 1.27%) more frequently, while ChatGPT emphasizes \"system\" (261\ncounts, 0.97%) and \"computer\" (233 counts, 0.86%) more than humans. Notably, \"information\" is used\nmore often by humans (206 counts, 0.84%) compared to ChatGPT (166 counts, 0.61%). Humans and\nChatGPT show similar trends with some variation in the emphasis of technical terms."}, {"title": "3.2 Classification Algorithms", "content": "The process of training models to generate predictions and categorize the cybersecurity documents written\nby ChatGPT was done using a variety of algorithms. This method enables us to compare between different\nalgorithms that can recognize patterns in data and take actions based on those patterns. We used a variety\nof ML techniques, including RF, Support Vector Machines (SVM), J48, and XGBoost. Each carefully\ncrafted to quickly explore and categorize cybersecurity content to detect plagiarism [22] .The following is\na brief description of the used algorithms.\nRF is a robust ensemble learning method and it is well-known for its performance in both classification and\nregression applications. With the help of a group of decision trees, it performs well at managing complex\ndatasets and reducing overfitting. Random Forest is a highly favored option in numerous fields due to its\nadaptability and resilience, yielding precise and dependable forecasts as well as valuable insights via feature\nimportance analysis [23], [24], [25].\nSVM is a strong supervised learning algorithm that may be used for regression and classification tasks.\nSVM is known to perform well if data is divided into different classes and maximizing the margin between\nthem by finding the best hyperplanes. Known for their adaptability and efficiency in high-dimensional\nareas, SVMs are an important ML tool for producing precise and dependable predictions [26], [27], [28].\nJ48 is known for its ease of use and interpretability in ML as a common decision tree classifier. J48\nrecursively divides data according to attribute values, generating a tree structure for effective decision-\nmaking, and is based on the C4.5 algorithm. J48 is a highly valuable tool in data mining and classification\njobs due to its reputation for handling both numerical and categorical data. It offers clear and actionable\ninsights for well-informed decision support [29], [30].\nXGBoost is known as Extreme Gradient Boosting, which is an ML technique that is notable for its great\nefficiency and scalability. The XGBoost algorithm performs well in predictive modeling applications.\nXGBoost is a preferred option in many industries, including finance and healthcare, due to its capacity to\nmanage complicated relationships in data, regularization techniques, and parallel processing.\nAcknowledged for its swiftness and efficiency, XGBoost has established itself as a mainstay in both\npractical and competitive ML scenarios [31], [32].\nThe study also explored the rapidly changing neural network landscape, utilizing the power of\nConvolutional Neural Networks (CNN) and deep Neural Networks (DNN) with a focus on cybersecurity"}, {"title": "3.3 Explainable Artificial Intelligence (XAI)", "content": "In recent years, artificial intelligence has advanced significantly, sparking interest in previously\nunderstudied fields. The focus has shifted from solely focusing on model performance as Al advances to\nrequiring experts to look at algorithmic decision-making processes and the logic behind AI models' output.\nAs modern ML algorithms, especially deep learning, using black box techniques become more powerful\nand complex. It becomes more difficult to understand how they behave and why specific outcomes were\nachieved, or mistakes were made. XAI systems can be used to understand models' behaviors, which allow\nusers to develop the proper level of trust and reliance [40], [41].\nIn this study, we use Local Interpretable Model-agnostic Explanations (LIME) to provide a way to\nunderstand how ML model make decisions. Because LIME is based on a model-agnostic premise, which\nwas developed by Ribeiro et al. in 2016 [42], it can offer visible and interpretable insights into the\npredictions of different black-box models. LIME generates locally faithful approximations through\nperturbed samples around individual instances, enabling users to understand the reasoning behind\nindividual predictions. Its interpretability-enhancing capabilities and adaptability have led to LIME's\nwidespread adoption in various domains, where it is a valuable resource for researchers and practitioners\nseeking transparency in the decision-making process of complex ML algorithms."}, {"title": "4. Experimental Results", "content": "This section assesses the performance of different ML algorithms using an 11th generation Intel(R) Core\n(TM) i5-1135G7 @ 2.40GHz processor, 16.0 GB of RAM, and a 64-bit operating system. We started our\nexperiment by investigating whether the use of full articles or paragraphs as the main unit of comparison\nproduces different results. The comparison is shown in Table (2) and Table (3), which highlights the"}, {"title": "4.1 Machine Learning and Deep Learning Results", "content": "As the results from using paragraphs instead of articles showed a more challenging problem, this section\nand the later ones will focus on these results. This section discusses the results by investigating the\nconfusion matrix for four different ML methods (i.e., RF, SVM, XGBoost, and J48) and two DL algorithms\n(i.e., DNN and CNN) as shown in Figure (4). The confusion matrices in Figure (4) provide insights into the\nperformance of various algorithms in differentiating between human-written and ChatGPT-generated\ncontent. XGBoost, shown in Figure (4) (c), demonstrated relatively the highest performance. XGBoost was\nable to classify 42.42% of ChatGPT-generated content and 40.91% of human content, with minimal\nmisclassification of 11.11% and 5.56%, respectively. For RF in Figure (4) (a), it follows closely with about\nsimilar results, correctly identifying 40.91% of ChatGPT-generated and 40.40% of human-written content.\nNonetheless, SVM and J48 in Figure (4) (b) and (d) showed slightly higher misclassification rates for\nhuman-generated content, with SVM incorrectly labeling about 11.11% and J48 misclassifying about"}, {"title": "4.2 Explainable AI (XAI) Results", "content": "As XGBoost algorithm achieved the highest performance in detecting AI-generated cybersecurity text, we\nemployed LIME as an XAI to deeply explain the classification results of XGBoost. By providing insights\ninto the reasons impacting the model's conclusions in the field of cybersecurity, this technique improves\ntransparency and reliability. Figure (6) shows the top ten important features (i.e., words) for the human and\nChatGPT classes in the XGBoost model, as generated by LIME. It clarifies the precise words that have a\nmajor influence on the model's predictions in each class. Figure (5) shows the interpretability of LIME on\nthe local level. This helps to clarify the decision-making process of the black-box model by providing real\ninsights into the critical characteristics driving classification results for various text categories. For the\nhuman class, terms like \"allow,\" \"use,\" \"virus,\" and \"people\" are considered highly discriminative, which\nindicates that humans tend to use more practical, action-oriented language related to security (e.g., viruses,\nprevention, and business terms). However, the ChatGPT class is dominated by more of an abstract and more\nformal words such as \"realm,\" \"employ,\" \"serve,\" and \"establish,\" which reflects a more structured,\ngeneralized tone common in AI-generated content."}, {"title": "4.3 Comparison with GPTZero", "content": "In this section, we compared our model's accuracy to a widely used software developed by the industry.\nThe main goal is to benchmark our highest-performed model (i.e., XGBoost) against GPTZero [43], [44].\nDoing so, we are not only verifying our model and its potential to be further scaled and transferred but also\nproviding insights about the performance of GPTZero.\nGPTZero, which was introduced in 2023 to address worries about AI-driven academic plagiarism, has\nreceived praise for its work but has also drawn criticism for producing false positives, particularly in\nsituations where academic integrity is at risk [43], [44]. The program uses burstiness and perplexity metrics\nto identify passages that are created by bots [45]. Burstiness examines phrase patterns for differences,\nwhereas Perplexity measures text randomization and odd construction based on language model prevalence.\nHuman text has greater diversity than content generated by AI. In previous studies comparing GPTZero\nand ChatGPT's efficacy in assessing fake queries and medical articles [46], GPTZero was utilized. The\nstudy found that GPTZero had low false-positive and high false-negative rates. Another study of more than\na million tweets and academic papers looked at opinions regarding ChatGPT's capacity for plagiarism [47].\nIn this study, we will investigate GPTZero for cybersecurity texts.\nFrom the generated dataset of this study, we created new observations for this task. We divided 600\nobservations into three classes by creating combinations of text generated by ChatGPT and humans, as\nshown in Table (4). The first class includes only AI-generated text, labeled as Pure AI class. The second\nclass includes a mix of human- and AI-generated texts of different ratios. The third class includes only\nhuman-written text. This split reflects the reality as ChatGPT was documented to be used in the two different\nforms; mixed with human-written, which sometimes referred to as paraphrased, and pure ChatGPT-\ngenerated text. We used 400 observations as training dataset for our model and 200 observations as a testing\ndataset.\nThe differences between GPTZero and our proposed model can be explained by their design goals and\ntraining data. GPTZero seems to be likely designed to be more cautious and conservative. It tends to classify\nuncertain cases as either mixed or unrecognized rather than taking the risk of misclassifying them as Pure\nAI or Pure Human. This results in high precision for the mixed cases but a lower performance for the other\nclasses. The other reason is for this disparity is that GPTZero had trouble identifying text that had less than\n250 characters [48]. Nonetheless, our proposed model shows a more balanced performance, with fewer\nmisclassifying cases as mixed. This indicates that our proposed model was trained on a more specific\ndataset, which made it more fine-tuned to better capture the discriminative features between Al-generated\nand human-written content. This suggests that using a narrow AI system fine-tuned with a suitable dataset\nin a specific task can beat a more generalized Al systems."}, {"title": "4.5 Advancing Knowledge on the Pedagogical Use of Digital Technology", "content": "The results of this study have significant implications in advancing the pedagogical use of digital\ntechnology, particularly in maintaining academic integrity and improving learning environments [49], [50].\nAs Al-generated content becomes more prevalent in educational settings, the ability to accurately\ndistinguish between human-written and AI-generated text is critical for ensuring fairness, transparency, and\nthe authenticity of student work [51], [52]. The study demonstrates that traditional machine learning\nmodels, such as XGBoost and Random Forest, can effectively classify AI-generated text with high accuracy,\nwhich can be applied to educational contexts where verifying the originality of student submissions is vital.\nIn an academic environment where students increasingly have access to powerful generative AI tools like\nChatGPT, this research highlights how automated systems can assist educators in identifying instances\nwhere Al is used excessively. By incorporating XAI techniques such as LIME, the study also provides\ntransparency, allowing educators to understand why certain content is flagged as AI-generated. This\nenhances trust in the technology and helps educators make informed decisions, which fosteres a balanced\napproach to integrating AI in education while maintaining ethical standards. The findings have implications\nfor how educators design assessments and encourage original thought. With reliable AI detection tools,\ninstructors can confidently promote digital tools in the classroom for learning purposes while ensuring that"}, {"title": "5. Conclusion", "content": "This study seeks to advance the pedagogical use of digital technology by providing tools to detect AI-\ngenerated content in educational settings, which promots academic integrity and fairness. By leveraging\nmachine learning models including traditional ML, DL, and XAI techniques, the study helps educators\nidentify AI use in student work, ensuring transparency and accountability. These findings support the ethical\nintegration of AI in education, which helps maintain academic standards while fostering digital literacy and\ncritical thinking in learning environments. This study proposes a model that distinguish between human-\nwritten and Al-generated text, which has become a critical challenge, particularly in fields like\ncybersecurity. This study highlights the importance and practical applications of this distinction, not only\nwithin cybersecurity field but also in academic writing and business operations. We tested various ML and\nDL algorithms on a generated dataset that contains cybersecurity articles written by humans and AI-\ngenerated articles with the same topic by LLMs (specifically, ChatGPT). We demonstrated the high\nperformance of traditional ML algorithms, specifically XGBoost and RF, to accurately classify AI-\ngenerated content with an accuracy of 83% and 81% respectively and with minimal misclassification rates.\nWe also showed in this experiment that classifying relatively smaller content (e.g., paragraphs) is more\nchallenging than classifying larger ones (e.g., articles).\nWe then used LIME, as an XAI model, to elucidate the discriminative features that influence the XGBoost\nmodel's predictions. Results offered insights into the characteristics that differentiate human-written content\nfrom Al-generated text on the dataset level and on the instance level. It showed that humans tend to use\nmore practical and action-oriented language related to security (e.g., virus, allow, and use) while LLMs use\nmore of an abstract and formal words such as \"realm,\" \"employ,\" \"serve,\" and \"establish,\".\nThe main reveal of the comparative analysis between GPTZero and our proposed model showed that a\nnarrowly focused and fine-tuned AI system can outperform more generalized AI systems like GPTZero in\nspecific tasks. This provides evidence of the effectiveness of tailoring AI models to specific datasets and\ntasks, where precision and performance can be significantly improved with a more targeted approach.\nGPTZero model showed an accuracy of 48.5% with about 16% of the cases that were not recognized, while\nour proposed model achieved about 77.5% accuracy. GPTZero had tendency to classify uncertain cases as\neither mixed or unrecognized rather than taking the risk of misclassifying them as Pure AI or Pure Human.\nHowever, our proposed model showed a more balanced performance across the three classes, namely, Pure\nAI, Pure Human, and mixed."}]}