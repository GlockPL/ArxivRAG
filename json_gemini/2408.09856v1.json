{"title": "TeamLoRA: Boosting Low-Rank Adaptation with Expert Collaboration and Competition", "authors": ["Tianwei Lin", "Jiang Liu", "Wenqiao Zhang", "Zhaocheng Li", "Yang Dai", "Haoyuan Li", "Zhelun Yu", "Wanggui He", "Juncheng Li", "Hao Jiang", "Siliang Tang", "Yueting Zhuang"], "abstract": "While Parameter-Efficient Fine-Tuning (PEFT) methods like LORA have effectively addressed GPU memory constraints during fine-tuning, their performance often falls short, especially in multidimensional task scenarios. To address this issue, one straightforward solution is to introduce task-specific LORA modules as domain experts, leveraging the modeling of multiple experts' capabilities and thus enhancing the general capability of multi-task learning. Despite promising, these additional components often add complexity to the training and inference process, contravening the efficient characterization of PEFT designed for. Considering this, we introduce an innovative PEFT method, TeamLoRA, consisting of a collaboration and competition module for experts, and thus achieving the right balance of effectiveness and efficiency: (i) For collaboration, a novel knowledge-sharing and -organizing mechanism is devised to appropriately reduce the scale of matrix operations, thereby boosting the training and inference speed. (ii) For competition, we propose leveraging a game-theoretic interaction mechanism for experts, encouraging experts to transfer their domain-specific knowledge while facing diverse downstream tasks, and thus enhancing the performance. By doing so, TeamLoRA elegantly connects the experts as a \"Team\" with internal collaboration and competition, enabling a faster and more accurate PEFT paradigm for multi-task learning. To validate the superiority of TeamLoRA, we curate a comprehensive multi-task evaluation (CME) benchmark to thoroughly assess the capability of multi-task learning. Experiments conducted on our CME and other benchmarks indicate the effectiveness and efficiency of TeamLoRA. Our project is available at https://github.com/Lin-Tianwei/TeamLoRA.", "sections": [{"title": "1 Introduction", "content": "Instruction fine-tuning of Large Language Models (LLMs) (Achiam et al. 2023a; Reid et al. 2024; Cai et al. 2024; Yang et al. 2024) and Multimodal Large Language Models (MLLMs) (Radford et al. 2021; Li et al. 2022; Huang et al. 2023; Achiam et al. 2023b; Zhang et al. 2024) has achieved impressive proficiency in Natural Language Processing (NLP) and multi-modal understanding by effectively adapting task-agnostic foundations to task-specific domains. However, this approach requires substantial memory and computational resources for full fine-tuning (FFT), i.e., fine-tuning models with more than one billion parameters, which hinders its applicability.\nTherefore, Parameter-Efficient Fine-Tuning (PEFT) techniques have emerged with the aim of reducing the cost by fine-tuning a small subset of parameters, offering a streamlined approach for domain adaptation. Among these methods, Low-Rank Adaptation (LoRA) (Hu et al. 2022), a popular PEFT approach, fine-tunes models by adapting lightweight auxiliary modules $A_W = AB$ on top of pre-trained weights $W_0$, where $A$ and $B$ are low-rank matrices. LORA offers performance comparable to full fine-tuning when focusing on the one-dimensional domain or task with less computational effort. Nonetheless, qualitative research highlights LoRA's limitations in handling multidimensional task scenarios, mainly due to the catastrophic forgetting and interference (Kalajdzievski 2024) between tasks in the training stage.\nOne straightforward solution is to adaptively integrate the knowledge diversity of multiple LoRA experts to handle different task characteristics, a method known as multi-LoRA architecture (MoELORA). Specifically, this method involves adding multiple LoRA modules as experts within the Transformer sub-layers (Gao et al. 2024), and selectively activating weights based on input through a gating mechanism (Router), thereby enhancing performance of multi-task learning. Currently, multi-LoRA architecture (Dou et al."}, {"title": "2 Related Work", "content": "Mixture-of-Experts. MoE integrates the outputs of multiple sub-models (experts) using a token-based routing mechanism (Jacobs et al. 1991). Shazeer et al. (Shazeer et al. 2017; Fedus, Zoph, and Shazeer 2022) introduced a sparsely-gated top-k mechanism where the router activates a subset of experts for each input token, significantly reducing resource consumption during both training and inference. To balance expert loads, GShard (Lepikhin et al. 2020) and OpenMoE (Xue et al. 2024) introduced importance and load losses to ensure fair load distribution among experts, reducing issues such as tail dropping and early routing learning. Additionally, the router's z-loss has been used to enhance training stability (Zoph et al. 2022), and it addresses the expert balancing issue in multi-task models by maximizing mutual information between tasks and experts (Chen et al. 2023). Beyond token selection gating, Expert-Choice Gating allows experts to actively select the top-k tokens they will process, evenly distributing the load and avoiding the need for auxiliary losses (Zhou et al. 2022). Recently, MoE has further explored potential in terms of the number of experts (He 2024) and multimodal fusion (Lin et al. 2024), becoming a focus of research.\nParameter-Efficient Fine-Tuning. PEFT (He et al. 2021) reduces the dependency of fine-tuning Large Language Models (LLMs) on computational costs by introducing additional modules to replace updates to the large-scale pre-trained weights. Adapters (Houlsby et al. 2019) introduce"}, {"title": "3 Methods", "content": "This section demonstrates the details of TeamLoRA. Figure 2 illustrates the architecture of TeamLoRA."}, {"title": "3.1 Problem Formulation", "content": "In a multi-task learning cenarios, Parameter-Efficient Fine-Tuning (PEFT) adapts to various application through a lightweight auxiliary module that is shared among tasks. This multi-task PEFT approach allows the model to remain compact while addressing multiple task requirements. Specifically, PEFT organizes shared auxiliary modules $C_{aux}$ to a pre-trained layer $C_{pre}$ for various types of tasks. The input sequence $x=[x_1, x_2, ..., x_N]$ is processed by the pre-trained layer and auxiliary module as follows:\n$C_{mix}(x; \\Theta_{pre}, \\theta_{aux}) = C_{pre}(x; \\Theta_{pre}) \\bigoplus C_{aux} (x; \\theta_{aux}),$ (1)\nwhere $\\Theta_{pre}$ and $\\theta_{aux}$ denote the parameters of the pre-trained layer and the auxiliary module, respectively. $\\bigoplus$ represents combination strategies based on the method being used, which can be addition, multiplication, or concatenation. During training, only the parameters of the auxiliary module are updated. This parameter update strategy maintains knowledge stability and reduces computational overhead:\n$\\Theta_{pre} \\leftarrow \\Theta_{pre}, \\theta_{aux} \\leftarrow \\theta_{aux} - \\eta \\nabla_{\\theta_{aux}} L(y, Y_{gt}),$ (2)\nwhere $\\eta$ represents the learning rate and target optimization function $L$ assesses the deviation between the predicted output $y$ and the ground truth $y_{gt}$."}, {"title": "3.2 Preliminaries", "content": "Low-Rank Adaptation. LoRA (Hu et al. 2022) captures downstream data features by introducing a pair of low-rank matrices as auxiliary modules for the pre-trained weights. The core idea of LoRA is to decompose the auxiliary weight matrix $A_W \\in R^{d_{in} \\times d_{out}}$ of the linear layer into two matrices, $A \\in R^{d_{in} \\times r}$ and $B \\in R^{r \\times d_{out}}$ with $r\\ll min\\{d_{in}, d_{out}\\}$, reducing the number of learnable parameters. Assuming the origin input to pre-trained weights is $x \\in R^{N \\times d_{in}}$ and the output $h \\in R^{N \\times d_{out}}$ with LoRA can be represented as:\n$h = xW + x\\Delta W = xW_0+ xAB,$ (3)\nwhere matrix $A$ is initialized with a random Gaussian distribution and matrix $B$ as a zero matrix to ensure that LoRA does not affect the original output at the start of training. Typically, $\\Delta W$ is scaled by $ \\alpha/r$, using a scaling factor $\\alpha$ to adjust the impact of the LoRA module.\nMixture of Experts. MoE (Fedus, Zoph, and Shazeer 2022) greatly expands the model scale while activating only a small number of parameters. In large models (LMs), MoE"}, {"title": "3.3 TeamLoRA", "content": "TeamLoRA facilitates efficient collaboration and effective competition among experts, optimizing the mechanisms for knowledge sharing and transfer to boost performance:\n$C_{mix} (x; W_0, \\Theta_{col}, \\Theta_{cop}) = x W_0 + C_{aux}(x; \\Theta_{col}, \\Theta_{cop}),$ (6)\nwhere $\\Theta_{col}$ represents parameters of efficient collaboration module $M_{col}$ and $\\Theta_{cop}$ represents parameters of effective competition module $M_{cop}$.\nEfficient Collaboration among Experts. We first analyze MoELORA, which adopts an adaptive collaboration approach, dynamically combining LoRA expert knowledge $\\{E_i\\}_{i=1}^{k}$ through a router mechanism. The combined knowledge is added as a bypass to the pretrained weights. Specifically, MoELORA constructs multiple identical expert pairs $\\{A_i, B_i\\}_{i=1}^{k}$ to perform multi-task learning and the mechanism of MoELORA is illustrated as follows:\n$C_{aux} (x; \\Theta_R, \\{A_i, B_i\\}_{i=1}^{k}) = \\sum_{i=1}^{k} w_i E_i(x; A_i, B_i),$ (7)\nwhere $E_i(x; A_i, B_i) = xA_iB_i$, and $w$ represents the normalized output of the router adaptively learned from tasks.\nIn fact, regarding MoELORA, we have two key observations: (i) Based on the stacking of multiple LoRA experts, MOELORA introduces an additional approximately 2*k matrix operations, significantly impairing the GPU's parallel processing capabilities. For example, in our CMT benchmark, k values of 4 or greater are nearly impossible to train (when k equals 2, 4, and 8, MoELORA introduced additional training times of 19%, 62%, and 138%, respectively, compared to LoRA). (ii) The independence among experts leads to learning redundant knowledge, evidenced by achieving 98.5% performance on the CMT benchmark as Table 1, when only the most advantageous experts (Top-1) are retained, which dilutes the collective expressive power of the expert ensemble. These scenarios prevent MoELORA from effectively balancing between efficiency and performance.\nConsidering the structural hierarchy between A and B, TeamLoRA designs a collaboration module aimed at facilitating hierarchical collaboration between them. The general module (matrix A) captures homogeneous features across tasks, responsible for learning and transmitting domain-agnostic general knowledge; the expert modules (matrix B) considered as domain-specific plugins capture and promote corresponding knowledge transfer in specialized domains. TeamLoRA defines matrix $A \\in R^{d_{in} \\times r_A}$ and k matrices $B_i \\in R^{r_B \\times d_{out}}$, where $r_A = k r_B$. The input x is processed through matrix A to compute an intermediate state $z = xA$, where $z \\in R^{N \\times r_A}$. Then z is evenly split into k segments along its last dimension, a process we refer to as \u201csplit\u201d:\n$z_i = split(z)_i = z_{(i-1)r_B+1:ir_B}.$ (8)\nSubsequently, each segment $z_i$ undergoes a linear transformation through its corresponding matrix $B_i$. The final partial output $h_i \\in R^{N \\times d_{out}}$ as below:\n$h_i = M_{col}(x; A, B_i) = split(xA)_iB_i.$ (9)\nAssuming expert weights is $w$, the final output of the collaboration module can be represented as $h = \\sum_{i=1}^{k} w_ih_i$. Such an operation is considered a knowledge organization and forward transfer by the \"Team\".\nUnlike the fully symmetric structure of MoELORA, the efficient collaborative module allows general modules and expert modules to adaptively organize team knowledge to cope with multi-task scenarios. The general module captures domain-independent common knowledge and maintains generalization performance in complex scenarios. Subsequently, the expert modules provide specialized knowledge supplementation and organization based on \u201cplug-in\u201d action, effectively capturing and integrating task-specific details, thereby improving the efficiency of knowledge transfer. Additionally, this collaborative module significantly reduces computational costs by decreasing matrix operations, requiring only 87%, 70%, and 63% of the training time of MoELORA with the same number of experts when k is 2, 4, and 8 respectively, achieving the efficiency objective.\nEffective Competition among Experts. Common routing mechanisms have key flaws such as inefficiency in allocation and knowledge silos (Zuo et al. 2021), which contradict the design philosophy. To address this, we introduc a shapley-based mechanism (Shapley et al. 1953) that actively shapes expert competition based on adaptive interactions. This approach prevents centralized decision-making and promotes the effective transfer of expertise to specific downstream tasks. By dynamically adjusting input distribution and expert responsibilities, the competition module ensures more effective and equitable knowledge transfer across tasks."}, {"title": "4 Experiments", "content": "4.1 Benchmark and Setting\nBenchmark. All PEFT methods used the 2.5M training set from 22 datasets effectively organized by CME(refer to Appendix A) and were comprehensively evaluated on tasks across 11 different tasks: OpenAI-Summarize-TLDR (Stiennon et al. 2020), IMDB (Maas et al. 2011), ANLI (Nie et al. 2020), QQP (Wang, Hamza, and Florian 2017),\nWe first introduce the concept of fuzzy Shapley values to offer a perspective on how routers assess the marginal contributions of experts. Unlike the traditional binary participation (participation or absence), fuzzy Shapley values permit participation degrees to range from 0 to 1. The following equation represents the marginal contribution of experts:\n$\\phi_i(x; \\omega_i) = \\int_{S} (v_i(x, \\omega_i, s) - v_i(x, 0, s)) ds,$ (10)\nwhere $\\phi_i(x; \\omega_i)$ represents the marginal contribution of expert i with participation degree $\\omega_i$, and s denotes the space of possible participation degrees for the remaining experts, satisfying $\\sum_{j} s_j = 1 - \\omega_i$ and $j \\neq i$. $v_i(x, \\omega_i, s)$ represents the total payoff from the combined participation $\\{\\omega_i\\} + s$. From the perspective of shapley values, the mechanism of the router can be understood as assessing the average marginal contributions of each expert across all possible combinations of experts. This provides a theoretical basis for the allocation of activation weights and highlights the importance of considering synergistic effects among experts. Although calculating shapley values is an NP-hard problem in practical applications, we can use an MLP as an approximation module for fuzzy Shapley values, estimating the marginal contributions of each expert:\n$\\phi_i(x; \\theta_S) = \\text{Softmax}(S(x; \\theta_S))_i,$ (11)\nwhere $\\phi_i$ represents the fuzzy Shapley value of the i-th expert and S represents Shapley value calculator.\nTo fully capture the competitive dynamics among experts, we introduce an interaction matrix that evaluates and adjusts their interactions. This matrix captures the mutual influences among experts and adjusts their participation based on Shapley interactions. Specifically, the interaction matrix M is designed to adaptively adjust each expert's participation based on their competitive relationships, as detailed below:\n$\\omega_i = M_{cop}(x; \\theta_S, M) = \\sum_{j=1}^{k} M_{ij}\\phi_j(x; \\theta_S),$ (12)\nwhere $\\omega_i$ represents the adjusted optimal degree of participation, and $M_{ij}$ denotes the element in the interaction matrix reflecting the influence of expert j on expert i. The interaction matrix M is initialized with a uniform distribution, with all diagonal elements set to 1 for baseline self-influence. M is a learnable matrix that adapts during the training process to fully account for synergistic effects among experts and adequately captures the competitive relationships.\nUltimately, the output of TeamLoRA is represented as:\n$h = xW_0 + M_{col}(x; A, \\{B_i\\}) \\odot M_{cop}(x; \\theta_S, M),$ (13)\nwhere $\\odot$ represents the element-wise product."}, {"title": "4.2 Overall Performance", "content": "We evaluated the performance of TeamLoRA in a multi-task learning scenarios using the CME benchmark, compared to other PEFT methods as shown in Table 2. Our observations are summarized as follows: (i) TeamLoRA (Rank=32) showed the best or second-best performance across multiple tasks, with an average score of 60.29, significantly higher than other PEFT methods. Particularly, it achieved the best performance on MMLU, demonstrating TeamLoRA's strong capability in handling multi-domain tasks. (ii) Despite a training time of 28 hours for TeamLoRA (Rank=16), slightly longer than baseline methods like LoRA, Prompt-Tuning, and IA3, it achieved competitive average scores of 59.95 with half the parameter count, highlighting its efficient parameter utilization. (iii) Compared to other multi-LoRA architectures, TeamLoRA not only showed significant performance improvements but also reduced training costs significantly, with approximately 70% of MoELORA and 85% of HydraLoRA. This demonstrates TeamLoRA's effective balance between efficiency and effectiveness."}, {"title": "4.3 Quantitative Analysis", "content": "Analysis of Parameter Scales. Table 3 explore Team-LoRA's performance in multi-task learning across different parameter scales. Experiments demonstrate that TeamLoRA performs exceptionally well across various parameter configurations, indicating that TeamLoRA consistently exhibits superior performance compared to MoELORA. Notably, with an increase in parameter size, LoRA encounters catastrophic forgetting, as evidenced by a sharp decline in scores for TQA (close book QA). In contrast, both MoELORA and TeamLoRA alleviate this knowledge collapse, reflecting the stability of their adaptive mechanisms.\nAblation Analysis. We conducted an exploration for collaboration and competition modules. As shown in Table 4, both individual modules and their combinations enhance the model's expressive and adaptive capabilities in multi-task scenarios. The collaboration module, utilizing a \"Team\" architecture based on knowledge sharing, effectively promotes the integration and transfer of knowledge among experts, thereby enabling \u201cplug-in\u201d based knowledge organization. The competition module considers the interactions between experts, adjusting the model's preferences for transferring specific knowledge to downstream tasks in response to multi-task performance. The above evidence thoroughly demonstrates the positive significance of the modules."}, {"title": "4.4 In-Depth Analysis", "content": "Stability Analysis. In the stability analysis of TeamLoRA, we examined its performance across different configurations of expert module quantities (see Figure 3(a)). The results indicate that performance improves progressively as the number of expert modules increases from 1 to 4, thanks to the hierarchical knowledge structure and effective \u201cplug-in\u201d knowledge sharing and organization. However, when the number of modules reaches 8, there is a slight decrease in performance, likely due to the added complexity of knowledge transfer with excessive layers. Figure 3(b) illustrates TeamLoRA's adaptability to varying data scales, demonstrating its ability to maintain efficient domain knowledge transfer across data scales ranging from 10% to 100%, highlighting its potential for multi-task scenarios.\nComputational Costs and Loss Convergence. Figure 4 illustrates the advantages of TeamLoRA over MoELORA in terms of training and inference times. Specifically, TeamLoRA reduces training time by 30% and increases inference speed by 40%, as shown in Figure 4(a). Additionally, the loss convergence curve in Figure 4(b) demonstrates that TeamLoRA achieves lower loss values more quickly, highlighting its optimization in training efficiency.\nExpert Load Analysis. We observed the expert paths of MoELORA across four tasks. The features exhibited overconfidence(see Figure 5(a.1)) in the model's forward path. In contrast, TeamLoRA, which incorporates a competitive module, effectively learns task-specific models by assigning different expert modules as plug-ins for knowledge combinations(see Figure 5(a.2)). Furthermore, we conducted balanced load testing on 57 tasks in MMLU, as shown in Figure 5(b.1)(MoELORA) and Figure 5(b.2)(TeamLoRA). TeamLoRA demonstrated better load balancing compared to MoELORA, ensuring greater model stability.\nPerformance Comparison of Different Base Models. To explore the performance of TeamLoRA on other models, we replaced the base model with the more powerful Llama-3 8B and conducted a comprehensive comparison of the CME benchmark. Table 5 shows the results of this experiment, where TeamLoRA consistently demonstrated the best performance. This indicates that TeamLoRA maintains its advantages in multi-task learning across different base models.\nPerformance Analysis of MLLM. We further expanded the applicability of TeamLoRA by extending the model from single-modal to multimodal. We fine-tuned the LLaVA-1.5 7B model and evaluated it on nine benchmark tests, including MME (Fu et al. 2023), MMB/MMB-CN (Liu et al. 2023), SEED (Li et al. 2023a), POPE (Li et al. 2023b), SQA-I (Lu et al. 2022), VQA-T (Singh et al. 2019), MM-Vet (Yu et al. 2023), and VizWiz (Gurari et al. 2018). As seen, TeamLoRA achieved the best performance on the majority of benchmarks(see Table 6), indicating that TeamLoRA demonstrates strong generalizability in multimodal scenarios. Experimental details are provided in Appendix B."}, {"title": "5 Conclusion", "content": "TeamLoRA introduces an innovative PEFT approach by integrating collaborative and competitive modules, which significantly improves the efficiency and effectiveness of multi-task learning. In the proposed CME benchmark tests, TeamLoRA not only achieves faster response speed but also outperforms existing multi-LoRA architectures in performance."}]}