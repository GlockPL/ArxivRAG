{"title": "Temporal Preference Optimization for Long-Form Video Understanding", "authors": ["Rui Li", "Xiaohan Wang", "Yuhui Zhang", "Zeyu Wang", "Serena Yeung-Levy"], "abstract": "Despite significant advancements in video large multimodal models (video-LMMs), achieving effective temporal grounding in long-form videos remains a challenge for existing models. To address this limitation, we propose Temporal Preference Optimization (TPO), a novel post-training framework designed to enhance the temporal grounding capabilities of video-LMMs through preference learning. TPO adopts a self-training approach that enables models to differentiate between well-grounded and less accurate temporal responses by leveraging curated preference datasets at two granularities: localized temporal grounding, which focuses on specific video segments, and comprehensive temporal grounding, which captures extended temporal dependencies across entire video sequences. By optimizing on these preference datasets, TPO significantly enhances temporal understanding while reducing reliance on manually annotated data. Extensive experiments on three long-form video understanding benchmarks-LongVideoBench, MLVU, and Video-MME-demonstrate the effectiveness of TPO across two state-of-the-art video-LMMs. Notably, LLaVA-Video-TPO establishes itself as the leading 7B model on the Video-MME benchmark, underscoring the potential of TPO as a scalable and efficient solution for advancing temporal reasoning in long-form video understanding.", "sections": [{"title": "1. Introduction", "content": "Recent advancements in video large multimodal models (video-LMMs) [2, 45, 51] have marked a significant step forward for generalizable video understanding. While image-based LMMs (image-LMMs) [5, 20, 39] primarily focus on spatial reasoning, video-LMMs face the additional complexity of modeling temporal dependencies\u2014a critical aspect for capturing the dynamic nature of video content.\nCurrent video-LMM approaches often follow a two-stage training paradigm: pre-training on large-scale multimodal datasets, followed by supervised fine-tuning (SFT) [6] on curated video-text instruction tuning datasets [7, 65]. This process typically involves dense captioning of video frames using cutting-edge large image-LMMs like GPT-40 [2]. These captions are then transformed into question-answer pairs by leveraging large language models (LLMs). The hope is that this two-stage training enables the model to align video content with LLM capabilities via the text interaction, ultimately learning instruction-following behavior for both general and temporal understanding tasks.\nHowever, these methods for temporal modeling have significant challenges. First, they depend on large-scale, meticulously curated instruction-tuning datasets, which rely heavily on costly advanced models for dataset generation. Second, these approaches lack explicit optimization for temporal grounding capabilities, as the training signals primarily originate from weakly aligned video content and instruction-tuning pairs. As a result, these methods often fail to capture nuanced temporal relationships, particularly in long-form videos, leading to suboptimal performance on tasks requiring fine-grained or long-context temporal grounding. Zhang et al. [64] takes a step forward by utilizing Direct Preference Optimization (DPO) [44] to align the outputs of video-LMMs with those of advanced LLMs. However, this approach still evaluates preferences based solely on textual inputs, overlooking the complex temporal information inherent in the video modality. These limitations underscore the urgent need for more effective and efficient strategies to enhance temporal understanding, particularly for long-form video comprehension.\nIn this work, we introduce Temporal Preference Optimization (TPO), a self-training framework designed to enhance the temporal grounding capabilities of video-LMMs. Under the preference learning framework, TPO utilizes the contrast of the preferred and dis-preferred responses to encourage the model to prioritize preferred and temporally grounded responses. To achieve this, TPO injects two different granularities of temporal preference into the prefer-"}, {"title": "2. Preliminaries", "content": "Video large multimodal model (video-LMM) [32, 34, 51, 65] usually consists of a large language model (LLM) \\( \\pi_{\\theta} \\), a visual encoder \\( f_V \\) and a multimodal projector \\( f_P \\). Video-LMM accepts the input of a video V and a sequence of input text x. The input video is first tokenized into the visual token \\( X_V \\) by video-LMM's visual encoder, which is then projected into the textual embedding space by the multimodal projector \\( v = f_P(f_V(V)) \\). Such video-LMM models the probability of a response y:\n\\[\nP(y|x, V) = \\Pi_{i=1}^{n} \\pi_{\\theta}(y_i|y_{<i}, x, V) \\tag{1}\n\\]\nIn our experiments, we implement TPO based on two widely-used video-LMMs, LongVA-7B [63] and LLaVA-Video-7B [66], leveraging their publicly available open-source checkpoints. LongVA-7B is specifically designed for long-form video comprehension by adapting methodologies from long-text processing, providing a robust foundation for our work. LLaVA-Video-7B is one of the open-source state-of-the-art video-LMMs."}, {"title": "3. Temporal Preference Optimization", "content": "Preference learning [41, 49, 70] focuses on modeling human preferences to align model behavior with user expectations. In LLMs and image-LMMs, this involves training models to generate responses favored by users. This is typically achieved by collecting human feedback [41, 70] on pairs of model-generated outputs and learning a function that predicts which output is preferred. Formally, given an input x and two outputs y+ (preferred) and y\u2212 (dis-preferred), the model aims to satisfy:\n\\[\n\\pi_{\\theta}(y^+|x) > \\pi_{\\theta}(y^-|x) \\tag{2}\n\\]\nwhere \\( \\pi_{\\theta}(y|x) \\) is the model's probability of generating output y given input x with parameters \u03b8.\nDirect Preference Optimization (DPO) [44] is a methodology that directly integrates human preference data into the optimization of model parameters. Compared to Proximal Policy Optimization (PPO) [41, 49, 70], another popular preference learning implementation, DPO eliminates the need for explicit reward models or complex reinforcement learning algorithms. By leveraging human preference data as a guiding signal during optimization, DPO enhances the model's ability to generate outputs that are better aligned with human values and expectations.\nUnlike approaches that align image-LMM's outputs with human preferences, our work focuses on aligning model outputs with intrinsic temporal preferences. Specifically, given a video input, the model is expected to produce responses that are better aligned with temporally grounded video content. To address this challenge, as illustrated in Figure 1, we introduce Temporal Preference Optimization (TPO), a framework designed to encourage the model to prioritize outputs that are more temporally grounded. To minimize reliance on extensive human annotations or advanced teacher models, we develop a scalable pipeline for generating temporal preference data by bootstrapping the model's existing capabilities (Sec. 3.1).\nUsing this pipeline, we automatically generate paired datasets consisting of temporally preferred and dis-preferred examples. These pairs are then utilized for preference optimization (Sec. 3.5), enabling the model to learn to differentiate and favor temporally grounded outputs while preserving the capabilities of the original pre-trained model. Using this self-training approach, TPO effectively enhances temporal reasoning while maintaining scalability and practicality."}, {"title": "3.1. Temporal Preference Data Curation Overview", "content": "Formally, a temporal preference dataset D is constructed as a collection of tuples (V, q, r+,r\u2212), where V denotes a video, q represents a query, r+ is the preferred temporally grounded response, and r\u2212 is the dis-preferred response. In this context, r+ is designed to be more aligned with the temporal context of the video-query pair (V, q) than r\u2212.\nWhat constitutes good temporal grounding for video-LMMs? Effective temporal grounding in videos can be intuitively understood through two key scenarios: 1) Localized Temporal Grounding (Sec. 3.2): Given a question and a long video, the answer often pertains primarily to a small segment of the video. A video-LMM with strong temporal grounding capabilities should be able to localize and focus on this short duration, generating an accurate response without being distracted by irrelevant parts of the video. 2) Comprehensive Temporal Grounding (Sec. 3.3): When a question involves multiple keyframes across a video, the video-LMM is expected to identify and ground all relevant keyframes, ensuring no crucial information is overlooked.\nMotivated by these two scenarios, we propose a multi-granularity temporal preference data generation pipeline that systematically captures these aspects of temporal grounding. This pipeline is designed to create temporal preference data that emphasizes both localized and comprehensive temporal grounding, enabling video-LMMs to excel across varying levels of temporal complexity."}, {"title": "3.2. Localized Temporal Preference Data", "content": "Query Generation The queries are specifically crafted to focus on a subsegment of the video, ensuring they cannot be accurately answered using only general knowledge of the entire video. To generate such queries, we first sample a subsegment from the full video. Next, we sparsely sample N frames from the selected subsegment and use an image-based vision-language model (CogVLM2 [20]) to generate captions for each frame. These captions serve as the basis for creating targeted questions. To ensure diversity and relevance, we design multiple question types and feed a structured question-generation prompt, along with the generated captions, into a LLM (GPT-4o-mini). This process produces a set of candidate questions specifically tailored to the sampled subsegment, facilitating the creation of localized temporal preference data.\nPreferred Response Generation The preferred response in the curated dataset is expected to be of high quality and closely grounded in the corresponding temporal content. Current video-LMMs exhibit strong performance in understanding short video clips, making them well-suited to generate reliable responses when the video clip is both concise and directly relevant to the given query. To generate the preferred response, we provide the model with the same video segment used to curate the query, along with the query itself. By ensuring that the video clip is both brief and highly relevant to the query, we create conditions that maximize"}, {"title": "3.3. Comprehensive Temporal Preference Data", "content": "Query Generation Similar to the process used for localized temporal query generation, we employ a pre-trained image-LMM to generate captions. However, in this case, N frames are sparsely sampled from the entire video to ensure broader coverage of the video content. These captions are then input into an LLM, which generates queries designed to capture comprehensive temporal relationships across multiple frames.\nPreferred Response Generation To generate the preferred response for the query in comprehensive TPO, we input the same sampled N frames used during query generation into the video-LMM, ensuring an unaltered and exhaustive source of visual information. N is set to a maximum of 32, striking a balance between information richness and generation precision. This complete visual input allows the video-LMM to effectively identify salient keyframes and perform reasoning across the entire temporal sequence of the video, producing comprehensive and accurate responses.\nDis-Preferred Response Generation For the comprehensive task, we divide the video into subsegments and uniformly drop specific subsegments to create a modified version of the video with incomplete information. This truncated video, along with the comprehensive query, is then fed into the video-LMM to generate dis-preferred responses. Since the model lacks access to the full information relevant"}, {"title": "3.4. Preference Data Post Filtering", "content": "After generating the queries and preference data pairs, some incorrect pairs may exist, such as cases where the dis-preferred response is equal to or better than the preferred response, or where the preferred response is incorrect in relation to the given query. To address this, we implement a post-filtering pipeline to enhance data quality and reduce noise. Using the LLM (GPT-4o-mini), we provide the captions along with their corresponding queries and preference data pairs, instructing the model to filter out pairs that meet these conditions. This post-filtering step allows us to eliminate corner cases that could introduce noise into the preference dataset, resulting in a more refined and higher-quality dataset that better supports the optimization process."}, {"title": "3.5. Training Objective", "content": "The generated preference dataset is subsequently leveraged to optimize the video-LMM's temporal preferences using Direct Preference Optimization (DPO) [44], chosen for its flexibility and stability in handling preference-based learning tasks.\nGiven the preference dataset D (V,q,r+,r\u2212) and the video-LMM \u03c0\u03b8, the DPO loss function is defined as:\n\\[\nL_{DPO}(\\pi_{\\theta}; \\pi_{ref}) = -E_{(v,q,r^+,r^-)\\sim D} \\left[ log \\sigma (\\beta (log \\frac{\\pi_{\\theta}(r^+|V, q)}{\\pi_{ref}(r^+|V, q)} - log \\frac{\\pi_{\\theta}(r^-|V, q)}{\\pi_{ref}(r^-|V, q)})) \\right] \\tag{3}\n\\]\nwhere \u03c3 is the sigmoid function. This objective drives the model to assign higher probabilities to preferred outputs, aligning its behavior more closely with human judgments, while preventing the model from deviating too much from its pretrained distribution.\nTo better align the model with the preferred responses, we incorporate a supervised fine-tuning objective into the DPO training framework. This combined objective is controlled by the hyperparameter \u03b1, following [8, 10, 13].\n\\[\nL_{SFT}(\\pi_{\\theta}) = -E_{(V,q,r^+,r^-)\\sim D} log \\pi_{\\theta}(r^+|V,q) \\tag{4}\n\\]\n\\[\nL(\\pi_{\\theta}; \\pi_{ref}) = L_{DPO} + \\alpha L_{SFT} \\tag{5}\n\\]"}, {"title": "4. Experiments", "content": "4.1. Experimental Settings\nEvaluation Benchmarks We evaluate TPO and baselines on three widely recognized benchmarks in multimodal video understanding, with particular attention to long-form video comprehension."}, {"title": "6. Conclusion", "content": "We introduced Temporal Preference Optimization (TPO), a scalable post-training framework that enhances temporal grounding in video-LMMs. By leveraging preference learning at both localized and comprehensive levels, TPO effectively captures the intricate temporal dependencies required for long-form video understanding. Extensive experiments across three challenging benchmarks\u2014LongVideoBench, MLVU, and Video-MME\u2014demonstrated TPO's robust improvements, achieving state-of-the-art performance. By integrating multi-granularity temporal preferences, TPO addresses diverse challenges in long-form video understanding, offering a robust and efficient solution for advancing temporal reasoning in multimodal tasks. One future direction is scaling the preference data to improve coverage and diversity, thereby potentially enhancing TPO's generalizability. Additionally, while this work focuses on LongVA-7B and LLaVA-Video-7B as representative Video-LMMs, applying TPO to a broader range and larger scale of Video-LMMs would provide insights into its adaptability and performance across different architectures."}, {"title": "A. Reproducibility Statement", "content": "To ensure reproducibility, we have released the full scripts and source code of the TPO pipeline, accompanied by the curated preference dataset, which includes videos, associated queries, and corresponding preference responses, as well as the trained model weights. This release will include detailed implementations of all steps involved in the preference dataset curation and the preference optimization process. By providing these resources, we aim to facilitate the replication of our results and support further advancements in this area of research."}, {"title": "B. Appendix overview", "content": "This document provides more details of our approach and additional experimental results, organized as follows:\n\u2022 \u00a7 C More Implementation Details of TPO.\n\u2022 \u00a7 D More Details of the Preference Dataset Curation.\n\u2022 \u00a7 E More Examples in the Preference Dataset.\n\u2022 \u00a7 F Additional Ablation Study on Data Mixture.\n\u2022 \u00a7 G More Qualitative Examples."}, {"title": "C. Implementation Details", "content": "We conduct Temporal Preference Optimization (TPO) on LongVA [63] and LLaVA-Video [66], two state-of-the-art video-LMMs. The two TPO models are trained using 8 Nvidia A100 80GB GPUs, with a batch size of 1. For preference optimization, we set the KL-divergence weight (\u03b2) to 0.3 and the supervised fine-tuning (SFT) loss weight (a) to 0.5 for LongVA-TPO and we set the KL-divergence weight (8) to 0.2 and the supervised fine-tuning (SFT) loss weight (a) to 1 for LLaVA-Video-TPO. We employ full fine-tuning for both the multimodal projector and the language model while keeping the visual encoder frozen, using a learning rate of 4 \u00d7 10-6 for LongVA-TPO and 3 \u00d7 10-7 for LLaVA-Video-TPO. The training is performed on a curated dataset of 10k samples for one epoch for LongVA-TPO and 10k samples for one epoch for LLaVA-Video-TPO. A cosine learning rate scheduler with a warm-up ratio of 0.1 is utilized [38]. The entire TPO fine-tuning process takes approximately 4 hours on both two models.\nFor evaluation, we adopt the protocol outlined by LongVA [63] and LLaVA-Video [66], leveraging the official lmms-eval repository [62] to assess our model's performance on three benchmarks. For LongVA-TPO, we set the parameter max-frames_num = 128 across all three benchmarks. For LLaVA-Video-TPO, we set the parameter max_frames_num = 96 for the Video-MME benchmark"}, {"title": "D. Preference Dataset Curation", "content": "We manually curated a set of 200 keywords assisted with GPT-40-mini [2], which were utilized to retrieve 8,000 videos from the internet, forming a diverse and comprehensive dataset. Using this dataset, we further developed 10,000 queries paired with their corresponding preference responses, covering a broad range of tasks. The detailed prompts for preference dataset curation are provided in Figure 7 and Figure 8. For LLaVA-Video, we sampled a subset of 10k QA pairs from the LLaVA-Video-178k dataset for comprehensive TPO.\nThe distribution of video lengths in our collected dataset is presented in Figure 5. The distribution of tasks is illustrated in Figure 6, encompassing Temporal Reasoning (8.7%), Action Reasoning (12.4%), Causal Reasoning (11.1%), Information Extraction (18.0%), Descriptive Questions (12.8%), Summarization (7.5%), Object Reasoning (14.9%), and Spatial Reasoning (13.5%)."}, {"title": "E. Preference Dataset Examples", "content": "We provide three additional examples of preference datasets, as illustrated in Figure 9. For instance, in Example (a), the task involves an OCR-based query aimed at retrieving the quote located beneath a mural. The dis-preferred response incorrectly identifies the relevant frame, failing to locate the quote below the mural and instead referencing another frame containing the phrase \"Forward, Warrior.\" In contrast, the preferred response accurately identifies the corresponding frame based on the question. This is achieved by leveraging the highly relevant sub-video segment provided to the video-LMM, enabling the correct extraction of both the quote and its attribution.\nFor Example (b), the task involves summarizing information by identifying the four levels depicted in a pyramid diagram. The dis-preferred response, based on irrelevant video clips, provides incorrect names and an incorrect order for the four levels. In contrast, the preferred response accurately identifies both the correct names and the proper order of the four levels, demonstrating a better understanding of the context and alignment with the video content.\nFor Example (c), the task involves a high-level descriptive query requiring a summary of the exercise routine depicted in the video. The dis-preferred response, relying only on down-sampled frames, omits significant key information and provides an incomplete summary. In contrast,"}, {"title": "F. Additional Ablation Study", "content": "Different Data Mix Ratio We assessed the performance of our TPO model under various data mixing ratios for localized TPO and comprehensive TPO data based on the LongVA [63] model, keeping the total data size constant at 10k. The evaluated ratios included 10:0, 8:2, 5:5, 2:8, and 0:10. The experimental results, summarized in Table 5, clearly demonstrate that the model achieves optimal performance on general video understanding tasks when an equal proportion of localized TPO and comprehensive TPO data is used. This balanced data distribution effectively integrates fine-grained local temporal information with broader high-level temporal context, leading to superior overall model performance."}, {"title": "G. Qualitative Analysis Examples", "content": "We provide three additional qualitative analysis examples from the Video-MME dataset [15], as illustrated in Figure 10. Example (a) involves an information extraction and optical character recognition (OCR) task, where the question asks for the total number of measurements involved in chip manufacturing. The original LongVA model failed to accurately locate the relevant frame containing the necessary information, resulting in an incorrect response. In contrast, our LongVA-TPO model, enhanced through temporal preference optimization, successfully identified the pertinent frame within the lengthy input and provided the correct answer to the question.\nExample (b) involves a high-level video understanding and information extraction task, where the question asks for the main topic introduced in the video. The original LongVA model failed to capture the overarching theme, instead responding with an unrelated term, \u201cCriminal Trial,\u201d mentioned elsewhere in the video. In contrast, our LongVA-TPO model effectively identified the video's central theme and accurately provided the correct topic introduced in the"}]}