{"title": "RANDLORA: FULL-RANK PARAMETER-EFFICIENT FINE-TUNING OF LARGE MODELS", "authors": ["Paul Albert", "Frederic Z. Zhang", "Hemanth Saratchandran", "Cristian Rodriguez-Opazo", "Anton van den Hengel", "Ehsan Abbasnejad"], "abstract": "Low-Rank Adaptation (LoRA) and its variants have shown impressive results in reducing the number of trainable parameters and memory requirements of large transformer networks while maintaining fine-tuning performance. However, the low-rank nature of the weight update inherently limits the representation power of fine-tuned models, potentially compromising performance on complex tasks. This raises a critical question: when a performance gap between LoRA and standard fine-tuning is observed, is it due to the reduced number of trainable parameters or the rank deficiency? This paper aims to answer this question by introducing RandLoRA, a parameter-efficient method that performs full-rank updates using a learned linear combinations of low-rank, non-trainable random matrices. Our method limits the number of trainable parameters by restricting optimization to diagonal scaling matrices applied to the fixed random matrices. This allows us to effectively overcome the low-rank limitations while maintaining parameter and memory efficiency during training. Through extensive experimentation across vision, language, and vision-language benchmarks, we systematically evaluate the limitations of LoRA and existing random basis methods. Our findings reveal that full-rank updates are beneficial across vision and language tasks individually, and even more so for vision-language tasks, where RandLoRA significantly reduces\u2014 and sometimes eliminates\u2014the performance gap between standard fine-tuning and LoRA, demonstrating its efficacy.", "sections": [{"title": "1 INTRODUCTION", "content": "Large pre-trained models that leverage broad data have demonstrated significantly improved gen-eralization capabilities and remarkable versatility across diverse tasks. However, the resultant high parameter count also leads to a significant increase in the computational resources required to fine-tune such models on downstream tasks. To tackle this issue, parameter-efficient fine-tuning (PEFT) approaches such as low-rank adaptation (LoRA) (Hu et al., 2022), draw inspiration from the low intrinsic dimensionality of pre-trained models (Li et al., 2018; Aghajanyan et al., 2021) and char-acterize the weight updates as the product of two low-rank matrices, substantially reducing the number of trainable parameters and memory requirements during training. This formulation leads to an adaptable number of trainable parameters, as one modifies the rank of the matrices, providing great flexibility under various resource constraints.\nIn spite of the strong performance of LoRAs in parameter-efficient settings, our investigation un-covers an accuracy plateau, wherein an increase of rank and thus learnable parameters fail to bridge the accuracy gap with standard fine-tuning. These undesirable scaling properties (Kopiczko et al., 2024) raise questions about the inherent limitations imposed by the low-rank structure, particularly when tackling complex tasks that benefit from larger parameter counts. This issue would ideally be addressed by introducing full-rank updates while maintaining the parameter-efficiency. To this end, we propose RandLoRA, a PEFT method that leverages a set of linearly-independent random bases in the form of non-trainable low-rank matrices. By solely learning scaling coefficients for the linear combination of the random low-rank bases, our method achieves full-rank updates, while maintain-"}, {"title": "2 RELATED WORK", "content": ""}, {"title": "2.1 Low RANK ADAPTATION OF LARGE MODELS", "content": "Low Rank Adaptation (LoRA) of large language models has revolutionized the fine-tuning paradigm, enabling memory-constrained adaptation to specialist tasks and democratizing access to larger models. Initially introduced by (Hu et al., 2022), LoRA leverages the observation that weight updates during fine-tuning can converge to suitable performances without necessitating full rank updates. By factorizing weight updates into the product of two low rank matrices, LoRA achieves a memory-efficient solution for adapting large models. Moreover, once the low rank matrices are"}, {"title": "2.2 PARAMETER-EFFICIENT FINE-TUNING (PEFT) USING RANDOM BASES", "content": "Recent research has focused on further reducing the trainable parameter count of LoRA, a crucial aspect for low-shot applications where minimizing trainable parameters can prevent overfitting and enhance generalization. A promising direction involves utilizing random bases combinations, where randomly generated matrices are combined using a limited number of trainable parameters to esti-mate a weight update.\nPRANC (Nooralinejad et al., 2023) pioneered the random base strategy by learning a weighted averaged of random matrices through back-propagation. PRANC's solution averages multiple full size weight matrices for each layer, leading to high memory consumption. To address this, the authors generate random bases on the fly during forward and backward passes using a fixed seed random number generator, reducing memory usage to that of the largest trained layer in the network at the cost of training latency.\nBuilding upon PRANC, NOLA (Koohpayegani et al., 2024) introduces an improved algorithm where random bases are estimated as the product of two low-rank random matrices, each weighed using a learnable scalar and summed before matrix multiplication. This approach effectively ap-proximates a rank 1 LoRA with significantly fewer trainable parameters and largely reduces memory consumption during training over PRANC.\nConcurrently, VeRA (Kopiczko et al., 2024) proposed an alternative strategy utilizing a single high-rank random matrix (typically 256 or 1024), instead of summing multiple rank 1 matrices as in NOLA. VeRA also employs a scaling strategy of random bases distinct from NoLA, detailed in section 4, which relates to our approach. Both NOLA and VeRA achieve comparable performance to LoRA in few-shot fine-tuning scenarios while training substantially fewer parameters."}, {"title": "2.3 ALTERNATIVE STRATEGIES FOR PARAMETER-EFFICIENT FINE-TUNING", "content": "We report here on alternatives to weight tuning for parameter-efficient adaptation, specifically fo-cusing on prompt tuning. Context Optimization (CoOP) (Radford et al., 2021) introduced learnable context vectors for CLIP class names, later generalized to instance-specific prompts in Conditional COOP (CoCoOP) (Zhou et al., 2022). Recent prompt tuning methods, like DePT (Zhang et al., 2024b) and PromptSRC (Khattak et al., 2023b), emphasize knowledge preservation by isolating shared subspaces or regularizing prompts. While parameter-efficient, prompt tuning can struggle with generalization beyond few-shot settings (Han et al., 2024) and may be less effective than LoRA as data increases (Zanella & Ben Ayed, 2024). We therefore consider prompt tuning orthogonal to weight-tuning for the scope of this paper and exclude it from direct RandLoRA comparisons except for early results found in Appendix B.3."}, {"title": "3 MOTIVATIONS", "content": "Our literature review reveals that research on improving LoRA is focused on reducing the number of trainable parameters further, either through adaptable ranks or by using fixed or shared low rank"}, {"title": "4 RANDLORA\u2014PARAMETER-EFFICIENT FINE-TUNING WITH FULL RANK", "content": ""}, {"title": "4.1 WEIGHT UPDATES AS A SUM OF LOW-RANK MATRICES", "content": "Let \\(W_o \\in \\mathbb{R}^{D \\times d}\\) be a weight matrix of a large pre-trained model. Fine-tuning aims to find an appropriate \\(\\Delta W \\in \\mathbb{R}^{D \\times d}\\), such that the fine-tuned weights \\(W_o + \\Delta W\\) lead to an adapted model, tailored to a specific downstream task. Without loss of generality, let us assume \\(d < D\\). The motivation behind RandLoRA stems from the singular value decomposition (SVD) of \\(\\Delta W\\), i.e., \\(\\Delta W = U \\Sigma V^T\\), where \\(U \\in \\mathbb{R}^{D \\times d}\\), \\(\\Sigma \\in \\mathbb{R}^{d \\times d}\\), \\(V \\in \\mathbb{R}^{d \\times d}\\). This decomposition can be written as the sum of the product of rank-one matrices, as follows\n\\begin{equation}\n\\Delta W = \\sum_{i=1}^{d} \\sigma_i u_i v_i^T,\n\\end{equation}\nwhere \\(u_i\\) and \\(v_i\\) denote the columns of \\(U\\) and \\(V\\), respectively. We suggest that in this context, low-rank updates such as LoRAs can be characterized as an approximation of the few largest singular values while the rest of the information in \\(\\Delta W\\) being discarded. To better illustrate this point, let us denote the rank of LoRA by \\(r\\) and for brevity of exposition, assume \\(d\\) is divisible by \\(r\\). We rewrite equation 1 as a sum of the product of rank-\\(r\\) matrices, as follows\n\\begin{equation}\n\\Delta W = \\sum_{j=1}^{n} U_j \\Sigma_j V_j^T,\n\\end{equation}\nwhere \\(U_jV_j = \\sum_{i = r(j-1)+1}^{rj} u_i v_i\\) and where \\(n = d/r\\). This formulation reveals how LoRA mod-els the approximates the first low-rank partition \\(U_1 \\Sigma_1 V_1^T\\), and implicitly assumes \\(\\sum_{j=2}^{n} U_j \\Sigma_j V_j \\approx 0\\). We however argue that the remaining \\(n-1\\) terms can play a crucial role when capturing more complex task-specific variations that require larger deviations from the pre-trained weight \\(W_o\\)."}, {"title": "4.2 PARAMETER-EFFICIENT APPROXIMATION OF LOW-RANK MATRICES", "content": "Approximating more terms in the decomposition of \\(\\Delta W\\) using LoRA's formulation quickly be-comes parameter inefficient, culminating to \\(Dd + d^2\\) parameters for a full rank \\(d\\) in place of the orig-inal \\(Dd\\) parameters of \\(\\Delta W\\). To perform full-rank updates while maintaining parameter-efficiency, we propose instead to approximate each term of \\(\\Delta W\\) in equation 2 using low-rank random bases where only scaling coefficients are learned,\n\\begin{equation}\n\\Delta W = \\sum_{j=1}^{n} B_j \\Lambda_j A \\Gamma_j\n\\end{equation}"}, {"title": "4.3 CONVERGENCE ANALYSIS", "content": "In this section, we present a theorem showing that weight updates using RandLoRA is an accurate approximation of general matrices under certain theoretical conditions.\nTheorem 4.1. Let \\(W\\) be a fixed \\(D \\times d\\) matrix, with \\(D > d\\) and \\(rank(W) = d\\). Fix \\(1 \\leq n \\leq d\\), such that \\(d = nr\\). The matrix \\(W\\) can be factorized using SVD as\n\\begin{equation}\nW = \\sum_{j} U_j \\Sigma_j V_j^T,\n\\end{equation}\nwhere \\(U_j \\in \\mathbb{R}^{D \\times r}, V_j \\in \\mathbb{R}^{r\\times d}\\) are partitions of the left and right singular vectors, and \\(\\Sigma_j \\in \\mathbb{R}^{r \\times r}\\) contains \\(r\\) singular values. For each \\(1 \\leq j \\leq n\\), let \\(B_j\\) denote a random \\(D \\times r\\) matrix whose entries are drawn i.i.d from either a Gaussian or uniform distribution, \\(\\Lambda_j\\) denotes an \\(r \\times d\\) matrix whose entries are drawn similarly, \\(A_j\\) is a diagonal \\(r \\times r\\) matrix and \\(\\Gamma_j\\) is a diagonal \\(d \\times d\\) matrix drawn similarly. Assume\n\nfor each \\(1 \\leq j \\leq n\\) for some \\(0 < \\epsilon\\). Then we have that with probability 1 that each \\(B_j \\Lambda_j A_j \\Gamma_j\\) has full rank and\n\n\nFor details on the proof of theorem 4.1 please refer to appendix D.1.\nTheorem 4.1 is premised on \\(B_j \\Lambda_j A_j \\Gamma_j\\) being a good approximation for the \\(r\\)-truncated singular value of \\(\\Delta W\\), which is shown to be true empirically in VeRA (Kopiczko et al., 2024) for example. We show in this case that \\(\\Delta W\\) can be accurately approximated as \\(\\sum_{j=1}^{n} B_j \\Lambda_j A_j \\Gamma_j\\), motivating RandLoRA's formulation. In contrast, since the best approximation a rank-\\(r\\) LoRA can achieve is the \\(r\\)-truncated SVD of \\(W\\), then by Eckart-Young-Mirsky theorem, the Frobenius norm of the difference between \\(W\\) and low-rank adaptation BA is lower bounded as follows\n\\begin{equation}\n\\|\\|W - BA\\|\\|_F > \\|\\| W - \\sum_{i=1}^r u_i \\sigma_i v_i^T \\|\\|_F = \\sum_{i=r+1}^d \\sigma_i^2.\n\\end{equation}\nWe conclude that while LoRA's rank \\(r\\) approximation is limited by the sum of the last \\(d-r\\) squared singular values of \\(W\\), RandLoRA does not present this low bound and is only limited by how close (\\(\\epsilon\\)) can \\(B_j \\Lambda_j A_j \\Gamma_j\\) approximate length-\\(r\\) segments of the SVD of \\(W\\)."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 EXPERIMENTAL SETTINGS", "content": "We conduct a comprehensive comparison with three state-of-the-art approaches: LoRA (Hu et al., 2022), NOLA (Koohpayegani et al., 2024), and VeRA (Kopiczko et al., 2024). We perform a hyper-parameter search to identify optimal settings for LoRA, NoLA, VeRA, and RandLoRA to ensure a fair comparison. More details about the experimental settings can be found in appendix C. Additional experiments on the General Language Understanding Evaluation (GLUE) (Wang et al., 2019) and End-to-end (E2E) Novikova et al. (2017) natural language generation benchmarks as well as further comparison with prompt-tuning algorithms are available in appendix B."}, {"title": "5.2 VISION: DINOV2 AND CLIP'S VISION BACKBONE", "content": "We evaluate fine-tuning vision backbones for image classification using pre-trained ViT-B/14 Di-noV2 (Oquab et al., 2023) and ViT-B/32, ViT-L/14 CLIP (Radford et al., 2021) vision only back-bones. We fine-tune on 21 datasets (Appendix C.1, Table 7) and evaluate {1, 2, 4, 16}-shot learning and performance with 50% and 100% training data.\nWe compare RandLoRA to LoRA rank 32 where RandLoRA's rank is adjusted to match LoRA's parameters, and include VeRA and NoLA as random base alternatives. We fine-tune the vision backbones and learn linear classifiers for DinoV2, or use frozen CLIP language embeddings for classification. Results are displayed in Figure 2 where we also report VRAM usage, detailed results are available in Appendix E.2.\nWe find that LoRA exhibits a smaller accuracy gap with standard fine-tuning (FT) on DinoV2 than CLIP. With equal parameters, RandLoRA improves over LoRA, bridging the FT gap in both cases. We believe that LoRA's success on the DinoV2 backbone is partly explained by its training objective (see Section 6.1). RandLoRA demonstrates LoRA's rank limitation for CLIP architectures and the benefit of full-rank updates in matching FT performance. VeRA and NoLA are efficient in few-shot settings but become limited with more data."}, {"title": "5.3 VISION-LANGUAGE: CLIP", "content": "We extend in this section our experimental setting to fine-tuning CLIP-like transformer architec-tures on classification datasets where contrary to section 5.2 both the language and vision encoders of CLIP are trained. We add ImageNet (Krizhevsky et al., 2012) to the dataset pool to scale up to 22 classification datasets. To assess the effectiveness of RandLoRA compared to LoRA on models of varying sizes, we consider three variants of pre-trained CLIPs from the open-clip repository (Cherti et al., 2023): ViT-B/32 (151M parameters), ViT-L/14 (428M parameters) and ViT-H/14 (1B pa-"}, {"title": "5.4 \u0421\u043e\u043cMONSENSE REASONING", "content": "We evaluate RandLoRA for fine-tuning LLMs on eight commonsense reasoning tasks (see Ap-pendix C.4). We fine-tune Qwen2 (0.5B), Phi3 (3B), and Llama3 (8B) models and assess data effi-ciency by training on both a 170,000-sample full dataset and a 15,000-sample subset, following Hu et al. (2023).\nTable 1 compares RandLoRA to LoRA, VeRA, and NoLA. We test two LoRA ranks: rank-16 (\"Ef-ficient\") and rank-32 (\"Performant\"). We then scale RandLoRA the same or lower amount of pa-rameters to ensure a fair comparison. Detailed results are found in Appendix 15\nRandLoRA performs competitively with, and sometimes surpasses, LoRA. Phi3's strong zero-shot abilities enable VeRA and NoLA to achieve strong results despite fewer parameters. Conversely, Qwen2 and Llama3 require more adaptation, challenging VeRA and NoLA to match LoRA's perfor-mance. The 15k-sample regime can lead to overfitting when scaling trainable parameters for LORA and RandLoRA, decreasing performance even with dropout regularization. When training on the full 170k samples, RandLoRA consistently outperforms LoRA. Results comparing with DoRA (Liu et al., 2024) for LLama3 only are available in Table 6 in the appendix where RandLoRA outper-forms both DoRA and LoRA for larger parameter budgets, while DoRA and LoRA are competitive at \"Efficient\" budgets. We conclude RandLoRA is a compelling alternative to LoRA and DORA for LLM fine-tuning, especially with larger datasets and parameter budgets."}, {"title": "6 DISCUSSION", "content": ""}, {"title": "6.1 SIMILARITIES WITH FINE-TUNING: ACTIVATIONS", "content": "We evaluate activation similarity to assess LoRA and RandLoRA's ability to mimic fine-tuned model activations. Using the Centered Kernel Alignment (CKA) (Kornblith et al., 2019) metric, we mea-sure the similarity between activations of LoRA, RandLoRA, and a fully fine-tuned model. This protocol assesses how well each method captures dataset-specific activation patterns. Figure 4a shows CKA scores for self-attention and MLP layers in CLIP and DinoV2 vision backbones, av-eraged over 5 datasets where RandLoRA imrpoves over LoRA. For CLIP, LORA's CKA decreases in deeper layers, losing alignment with fine-tuned activations. RandLoRA, with equal parameters, matches LoRA's early layer alignment but improves upon it in deeper layers. This CKA drop for LORA in deeper layers is absent in DinoV2, explaining LoRA's near-identical accuracy to fine-tuning on DinoV2. This difference likely arises from training objectives: DinoV2's visual objective creates classification-ready features needing minimal weight adjustments, thus low-rank LoRA suf-"}, {"title": "6.2 SIMILARITIES WITH FINE-TUNING: LOSS LANDSCAPE", "content": "We analyze loss landscape connectivity for models fine-tuned with standard fine-tuning, LoRA, and RandLoRA. We visualize a 2D loss landscape plane by positioning LoRA, RandLoRA, and fine-tuning models at (0,0), (1,0), and (0.5,1) respectively. For each point \\((x, y)\\) on this plane, we interpolate model weights by solving for coefficients \\(\\alpha_i\\) (where \\(\\sum_{i=1}^{3} \\alpha_i = 1\\)) and evaluate the interpolated model's loss on a 5% training subset. Figure 4b shows that for CLIP, RandLoRA reaches a deeper loss minima than LoRA, often with a low-loss path to the fine-tuning optimum, and despite training the same parameter count. For DinoV2, all optima reside in a shared low-loss basin, with LoRA already close to fine-tuning, reflecting LoRA's strong performance on this task. These visualizations reinforce LoRA's low rank it particularly limiting for complex tasks, and demonstrate RandLoRA's ability to achieve deeper minima than LoRA with equal parameters due to full-rank updates. Appendix A provides 3D visualizations for additional datasets."}, {"title": "6.3 FURTHER STUDIES ON FULL VS LOW RANK FINE-TUNING OF CLIP", "content": "We investigate whether RandLoRA's CLIP performance advantage over LoRA stems from better SVD approximation or its full-rank capability. We ablate RandLoRA with two rank-controlled variants. RandLoRA-a restricts the update rank to \\(r\\) by averaging bases before multiplication:\n\\(\\Delta W = (\\sum_{i=1}^{N} B_j \\Lambda_j A) (\\sum_{j=1}^{N} A \\Gamma_j)\\). RandLoRA-b uses half-rank updates by setting \\(N = rank(\\Delta W)/r/2\\) and adjusting base rank to maintain parameter count parity with RandLoRA-\\(r\\). All variants train the same parameters, only update rank varies. Table 2 presents accuracy on 100% of 22 datasets for CLIP ViT-B/32. Results show that higher update rank correlates with better performance, given equal parameter counts. This supports the importance of large rank updates, particularly for CLIP fine-tuning."}, {"title": "6.4 SPARSE RANDOM MATRICES", "content": "We propose to investigate using sparse random matrices for improved memory and computational efficiency, drawing inspiration from random projection literature and the Johnson-Lindenstrauss lemma (Lindenstrauss & Johnson, 1984). We adopt the sparse construction from Bingham & Man-nila (2001) and Li et al. (2006), where matrix elements are \\(\\{-1, 0, 1\\}\\) with probabilities \\(\\{\\frac{1}{s}, 1-\\frac{2}{s}, \\frac{1}{s}\\}\\) (\\(s \\in [2, \\sqrt{D}]\\) for \\(W \\in \\mathbb{R}^{D \\times d}\\)), followed by normalization. Appendix C.6 discusses why this formu-lation preserves full rank. Table 3 shows experimental results using these sparse bases in RandLoRA. We explore sparsity ratios \\(s \\in \\{2, 6, \\sqrt{D}, 100, 200\\}\\), achieving sparsity levels from 66 to 99%. Con-sistent with Li et al. (2006), the recommended sparsity levels (\\(\\sqrt{D}\\)) yield performance comparable to dense matrices, theoretically reducing memory and compute. However, higher sparsity can de-"}, {"title": "6.5 SUMMARY OF DIFFERENCES WITH RELATED RANDOM BASES ALGORITHMS", "content": "Prior work like VeRA (Kopiczko et al., 2024) and NoLA (Koohpayegani et al., 2024) utilizes random bases for parameter-efficient fine-tuning. However, unlike VeRA and NoLA which approximate a low-rank LORA update, RandLoRA aims to approximate the full-rank weight update. It could be argued that VeRA approximates only the first block in a decomposition of W, whereas RandLoRA approximates all blocks. Thus, while VeRA and NoLA improve parameter-efficiency while main-taining low-rank updates, RandLoRA addresses cases requiring full-rank updates. Furthermore, Equation equation 4 evidences the flexibility in RandLoRA's parameter count, ranging from VeRA's parameter efficiency (\\(r = rank(W)\\)) to full fine-tuning parameters (\\(r = 1\\)) while maintaining full-rank."}, {"title": "6.6 LIMITATIONS", "content": "Despite RandLoRA's effectiveness, we identify three key limitations for future research.\nFirst, RandLoRA introduces computational overhead in weight update calculations, increasing train-ing time for larger models (Appendix C.6.1). We however evidence room for improvement using ternary sparse bases in Section 6.4. Future work should explore matmul-free matrix combinations using these ternary sparse bases. Efficient implementations could replace costly matrix products with simple aggregations, eliminating floating-point arithmetic (Li et al., 2006), and accelerating RandLoRA training time pending the development of optimized CUDA kernels (Zhu et al., 2024).\nSecond, exploring non-random, optimal bases \\(B_i\\) and \\(\\Lambda\\) could improve convergence and efficiency by further reducing \\(\\epsilon\\) in equation equation 6. Discovering such bases, potentially through experi-ments or decomposition of pre-trained weights (Ba\u0142azy et al., 2024; Meng et al., 2024), is a promis-ing research direction to enhance RandLoRA.\nThird, hybrid approaches combining LoRA and RandLoRA warrant investigation. LoRA could estimate the dominant SVD components of \\(W\\), while RandLoRA captures the remaining spectral information efficiently. Despite challenges in harmonizing training objectives, a starting point would use RandLoRA to refine a LoRA when convergence is insufficient. Addressing these limitations will further improve RandLoRA's potential for efficient full-rank fine-tuning."}, {"title": "7 CONCLUSION", "content": "This paper introduces RandLoRA, a method achieving parameter efficiency and low memory cost while enabling full rank model updates. Our findings underscore the critical importance of full-rank updates when fine-tuning pre-trained architectures and we observe that our approach surpasses LoRA's performance for an equal parameter count, highlighting the value of full-rank updates in large model fine-tuning. Through extensive experiments across diverse tasks we demonstrated the efficacy of our method. While RandLoRA incurs additional computational overhead due to random basis multiplications, memory consumption remains contained and we provide venues for reducing this compute in practice. As a results, RandLoRA offers a viable alternative to LoRA for fine-tuning large pre-trained models on consumer-grade hardware. Our results have significant implications for efficient and effective model adaptation, prompting for future research in scalable and versatile full-rank fine-tuning techniques."}]}