{"title": "Global Semantic-Guided Sub-image Feature Weight Allocation in High-Resolution Large Vision-Language Models", "authors": ["Yuxuan Liang", "Xu Li", "Xiaolei Chen", "Haotian Chen", "Yi Zheng", "Chenghang Lai", "Bin Li", "Xiangyang Xue"], "abstract": "As the demand for high-resolution image processing in Large Vision-Language Models (LVLMs) grows, sub-image partitioning has become a popular approach for mitigating visual information loss associated with fixed-resolution processing. However, existing partitioning methods uniformly process sub-images, resulting in suboptimal image understanding. In this work, we reveal that the sub-images with higher semantic relevance to the entire image encapsulate richer visual information for preserving the model's visual understanding ability. Therefore, we propose the Global Semantic-guided Weight Allocator (GSWA) module, which dynamically allocates weights to sub-images based on their relative information density, emulating human visual attention mechanisms. This approach enables the model to focus on more informative regions, overcoming the limitations of uniform treatment. We integrate GSWA into the InternVL2-2B framework to create SleighVL, a lightweight yet high-performing model. Extensive experiments demonstrate that SleighVL outperforms models with comparable parameters and remains competitive with larger models. Our work provides a promising direction for more efficient and contextually aware high-resolution image processing in LVLMs, advancing multimodal system development.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Large Vision-Language Models (LVLMs) have made significant strides, advancing multimodal un- derstanding across a range of vision-language tasks. The development of various LVLMs, such as LLaVA [1], InternVL [2], and ShareGPT4V [3], has substantially broadened the scope and potential of cross-modal applications involving both vision and language. With the increasing demand for more detailed and accurate visual comprehension, high-resolution image processing has become increasingly critical for LVLMs, as such images contain more fine-grained information [4]. However, traditional LVLM architectures often face limitations in their ability to effectively process high-resolution images, primarily due to the constraints of their vision encoders, which are typically trained to handle lower-resolution inputs [5], [4]. To address the limitations in processing high-resolution images, several strategies have been proposed. Common ap-"}, {"title": "II. RELATED WORK", "content": "Recent advancements in large language models (LLMs) have significantly improved natural language understand- ing and generation, facilitating breakthroughs in vision- language integration. This progress has led to the develop- ment of LVLMs, such as LLaVA [1], InstructBLIP [12] and ShareGPT4V [3]. These LVLMs typically consist of three key components: a vision encoder, a vision-language projector, and an LLM. The vision encoder, often based on Vision Transformer (ViT) [13], [14], extracts visual features from the input images. The vision-language projector aligns the extracted visual features with the input embedding space of the LLM [15]. The LLM then generates responses by jointly processing the aligned visual features and textual instructions [16]. However, initial LVLMs struggle with high-resolution im- ages, as their resolution capacity is typically limited by the vision encoders. Vision encoders are optimized for lower resolutions, making it difficult to capture fine-grained details"}, {"title": "III. PRELIMINARY OBSERVATIONS", "content": "One inherent characteristic of human visual cognition is the ability to selectively process visual signals, prioritizing regions with higher information density [6]. This mechanism allows humans to focus on key areas that are critical for understanding the overall context. However, current sub-image partitioning- based LVLMs lack such a mechanism. Instead, they treat all sub-images equally, which limits their ability to achieve effective visual perception in complex scenarios. To address this limitation, we propose and verify two key hypotheses: 1) Sub-images that are more semantically aligned with the full image typically encapsulate richer visual information; 2) Sub-images with higher information density are more critical for supporting the model's visual understanding capabilities. We adopt InternVL2-2B [11] as the baseline model to demonstrate the hypotheses. This model comprises an InternViT-300M [2] (a CLIP-style vision encoder) for image encoding, an MLP for vision-language alignment, and an InternLM2-1.8B [18] for multimodal understanding and text generation. For any high-resolution image input, the model dynamically partitions the image into a set of sub-images, each with a resolution that meets the image encoder's requirement (448px)."}, {"title": "IV. METHODS", "content": "Based on the two key insights outlined in Section III, we posit that LVLMs should differentiate the treatment of sub- images based on their relative information density. To this end, we propose a Global Semantic-Guided Weight Allocator (GSWA) module, which enables the model to autonomously perceive the information densities of different sub-images by quantitatively allocating corresponding weights, thereby addressing the limitation of uniform treatment of sub-images. As the proposed module can be seamlessly integrated into any LVLM that uses the sub-image partitioning strategy, we employ the widely-adopted InternVL2 [11] framework for demonstration in the following. As shown in Fig. 4(a), the complete model architecture consists of five key components: a dynamic cropping module, a vision encoder, a GSWA module, a vision-language projector, and a large language model. We provide a comprehensive overview of each module as follows. In our method, the pre- processing pipeline for high-resolution images follows the configuration established by InternVL2 [11]. To maintain the natural aspect ratio during processing, predefined aspect ratios are determined based on the specified maximum number of sub-images. During the matching process, the aspect ratio of the input image is calculated and compared to the absolute differences with the predefined ratios. We employ Intern-ViT [2] as the vision encoder, which accepts a fixed input image resolution of 448 \u00d7 448. The input image $I \\in R^{H\\times W\\times C}$, is passed through the vision encoder for feature extraction. After the input image is cropped into N sub-images along with one thumbnail representing the global image, they are then passed through multiple transformer layers to ultimately obtain the corresponding visual embeddings: $F = VisionEncoder(Crop(I)) \\in R^{(N+1)\\times(M+1)\\times D}$, (1) where M +1 and D denote the number and dimension of the extracted visual embeddings for each sub-image, respectively. It is important to note that each set of visual embeddings comprises M patch features accompanied by a < cls > token, which contains the global semantic information of the"}, {"title": "V. EXPERIMENTS", "content": "We employ InternVL2-2B [11] as the baseline framework to demonstrate the effectiveness of our method. The final model includes an InternLM2-1.8B [18] LLM backbone, a two-layer MLP projector, an InternViT- 300M [2] vision encoder, and a GSWA module. The GSWA module consists of four stacked transformer blocks with a hidden dimension of 1024, and each multi-head self-attention layer contains four attention heads. The additional multi-head self-attention layer follows the same configuration. We set the maximum number of sub-images for the dynamic cropping algorithm to 8 and the drop path rate of the vision encoder to 0.1. AdamW [26] was used as the optimizer with a learning rate of 4e-9, following a cosine decay schedule with a warmup ratio of 0.03 and a weight decay of 0.01."}, {"title": "VI. LIMITATION AND FUTURE WORKS", "content": "While this work presents an effective approach to enhance the sub-image partitioning method for high-resolution image processing in LVLMs, several limitations must be addressed. The introduction of sub-image partitioning to LVLMs inher- ently results in the addition of extra visual tokens, which in turn leads to increased inference and training cost. Further- more, the GSWA module also introduces additional computa- tional overhead as an additional module, further contributing to the prolongation of inference and training time. Another limitation of our approach is that the GSWA module assigns weights to sub-images solely based on the provided high- resolution image itself, without considering the accompanying natural language input provided to the LVLM. This could result in sub-optimal weight distribution, as the relevance of certain sub-images may not fully align with the semantic context conveyed by the language input, potentially affecting the overall performance and accuracy of the model in corner cases. In future work, we aim to address the aforementioned limitations from two key aspects: (i) by integrating a text- guided weight allocator into the existing GSWA module, we seek to incorporate the provided natural language input when assigning weights to sub-images, thereby improving the alignment between the visual content and the semantic context conveyed by the language input; (ii) by introducing a token compression method to mitigate the increase in visual tokens resulting from sub-image partitioning, we plan to reduce the number of visual tokens, particularly from sub-images with lower weights, in order to ensure faster inference times while maintaining the model's performance."}, {"title": "VII. CONCLUSION", "content": "This paper aims to emulate the bottom-up visual atten- tion mechanism observed in human vision to enhance the existing sub-image partitioning method for high-resolution image processing in LVLMs. We propose the GSWA module, which utilizes the self-attention mechanism to assign weights to sub-images derived from partitioning. We first conduct a comprehensive analysis of how sub-images from partitioning carry varying levels of information density. Building on this analysis, we introduce the GSWA module, which leverages semantic information from the global image to guide the allocation of weights to sub-images. This approach ensures the adaptive assignment of distinct weight values to sub- images based on their varying information densities, thereby avoiding the uniform processing of sub-images. By integrating the GSWA module into the InternVL2 framework, we develop SleighVL, which demonstrates exceptional performance across multiple task categories. Despite certain limitations, including increased inference time, additional computational overhead, and the underutiliza- tion of textual input, our module introduces a novel approach that assigns weights to sub-images based on the mimicry of human visual behavior. This mechanism enables the model to focus more effectively on regions of the image with higher information density, therefore offering a fresh perspective for research in high-resolution image processing within LVLMs."}]}