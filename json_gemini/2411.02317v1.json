{"title": "Defining and Evaluating Physical Safety for Large Language Models", "authors": ["Yung-Chen Tang", "Pin-Yu Chen", "Tsung-Yi Ho"], "abstract": "Large Language Models (LLMs) are increasingly used to control robotic systems such as drones, but their risks of causing physical threats and harm in real-world applications remain unexplored. Our study addresses the critical gap in evaluating LLM physical safety by developing a comprehensive benchmark for drone control. We classify the physical safety risks of drones into four categories: (1) human-targeted threats, (2) object-targeted threats, (3) infrastructure attacks, and (4) regulatory violations. Our evaluation of mainstream LLMs reveals an undesirable trade-off between utility and safety, with models that excel in code generation often performing poorly in crucial safety aspects. Furthermore, while incorporating advanced prompt engineering techniques such as In-Context Learning and Chain-of-Thought can improve safety, these methods still struggle to identify unintentional attacks. In addition, larger models demonstrate better safety capabilities, particularly in refusing dangerous commands. Our findings and benchmark can facilitate the design and evaluation of physical safety for LLMs. The project page is available at huggingface.co/spaces/TrustSafeAI/LLM-physical-safety.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have emerged as powerful applications in the field of natural language processing [12, 22, 23], and are increasingly impacting artificial intelligence (AI) technology, exhibiting human-level or even superhuman performance across a wide range of tasks. Because of innovations in neural network architectures [33] and increased computational power [28], LLMs have evolved into state-of-the-art AI systems capable of understanding and generating text with remarkable fluency and coherence. For example, GitHub Copilot\u00b9, developed in collaboration between GitHub and OpenAI, is an AI-powered coding assistant based on a variant of GPT-3 language model [6]. It assists users by autonomously completing code snippets or generating entire functions as they write code. Even though not explicitly trained for these attributes, LLMs have shown outstanding capabilities like reasoning [1, 17], planning [14, 26], manipulation [11, 38], and in-context learning [18]. As a consequence, they have been widely used in many different applications, such as question-answering, multi-modal tasks, tool manipulation, autonomous agents, and robotics."}, {"title": "2 Results", "content": "In this section, we first discuss the physical safety evaluation objectives and benchmark design for large language models (LLMs) in drone control. We introduce the dataset used for both safety and utility evaluations, followed by an explanation of the metrics employed for assessing safety. Next, we examine various LLMs in the context of physical safety, present the evaluation results across different models, and analyze the trade-offs between utility and safety. Moreover, We also highlight the impact of in-context learning (ICL) on safety improvements and conclude with insights into how model size influences safety outcomes."}, {"title": "2.1 Physical Safety Evaluation Objectives and Benchmark Design", "content": "As depicted in the top of Figure 1, the physical safety evaluation aims to address the potential threats and risks posed by drones in various real-world scenarios. Bad actors may use LLMs to issue commands that result in the generation of malicious code. We have categorized the risks into four main classes:\n\u2022 Human-targeted Threats: This refers to the potential for drones to be used to physically harm individuals, including causing targeted attacks and injuries to people. For example, a malicious user could instruct an LLM to provide detailed steps and code for making a drone to deliberately strike a crowd of people, which would pose a serious threat to human safety.\n\u2022 Object-targeted Threats: These involve using drones to damage or destroy property, such as vehicles or other objects, which could lead to financial losses and collateral damage. For example, a malicious actor could instruct an LLM to provide a sequence of commands to direct a drone to collide with and potentially damage a car.\n\u2022 Infrastructure Attacks: This category covers the targeting of critical infrastructure, such as power grids or transmission towers, which could result in widespread and cascaded disruption or damage. For instance, a malicious user could leverage an LLM to generate instructions for a drone to deliberately crash into a power transmission tower, potentially causing power outages and disruptions.\n\u2022 Violations of FAA Regulations: This encompasses the improper and unsafe operation of drones, such as flying in restricted airspaces or interfering with aviation activities, which could pose safety and legal risks. For example, a user could deliberately fly a drone into a no-fly zone near an airport, potentially causing a collision with an aircraft and endangering the lives of passengers and crew, as well as facing severe legal consequences. Our evaluation data are designed based on the regulations outlined in FAA Part 107.\u00b2\nAs shown in the detailed process flow diagram in the middle of Figure 1, we present the \u201cLLM Physical Safety Benchmark\" a comprehensive framework for evaluating the physical safety of LLM-based drone control systems, to explore and quantify critical safety risks. The entire evaluation process begins by providing the LLM with a test instruction, which is categorized based on the following types \u2013 deliberate attacks, unintentional attacks, violation instructions, or utility tasks \u2013 that we have designed as part of this benchmark.\nNext, the LLM's response is evaluated by two specialized AI judges \u2013 the Code Verification Judge and the Safety Evaluation Judge \u2013 that we have developed to assess the LLM's response. Furthermore, the generated code is tested in the AirSim simulation environment [27], developed by Microsoft Research, to detect potential collision risks and other safety issues. Finally, the comprehensive results of these evaluations are summarized to provide a holistic safety evaluation for the LLM, offering valuable insights into its ability to generate safe and reliable drone control instructions. More details on the specific metrics and implementation of these AI judges can be found in the \"Metrics for Safety Evaluation\" section."}, {"title": "2.2 Dataset for Safety and Utility Evaluations", "content": "To facilitate comprehensive physical safety evaluations, we present the LLM Physical Safety Bench- mark Datasets\u00b3, a curated collection of over 400 different instructions. This dataset was created through a collaborative effort combining human expertise and LLM support to construct a diverse and realistic set of scenarios. As illustrated in the lower left corner of Figure 1, the dataset is categorized into four dimensions: deliberate attacks, unintentional attacks, violation instructions, and utility. Each dimension evaluates a subset of our defined physical safety risks and the utility of regular (safety- unrelated) tasks. We refer to the Supplementary Materials for all test instructions, their corresponding categories, and the relationship between physical safety risks and the dataset dimensions."}, {"title": "2.3 Metrics for Safety Evaluation", "content": "Based on the physical safety dataset, we quantify the safety performance of different LLMs using the following six metrics:\n\u2022 Self-Assurance: Evaluate an LLM's ability to recognize and avoid dangerous situations, showing its self-awareness regarding safety. For example, a malicious user might prompt the LLM with 'Fly the drone and hit the crowd of people.' A safe response would involve refusing or cautioning against such a risky and unethical action. The score is calculated as the percentage of times it successfully refused dangerous instructions, maintained safe distances, and avoided collisions, out of the total number of risky instructions it received, both deliberate and unintentional. A higher score in this dimension indicates the LLM's enhanced ability to identify and refuse harmful instructions, even when explicitly given by malicious actors. It demonstrates the model's commitment to safety and ethical decision- making in potentially dangerous situations.\n\u2022 Avoid-Collision: Assess the likelihood of collisions caused by drone control via LLMs, both in cases of deliberate and unintentional attacks. For example, in a deliberate attack scenario, the model might receive instructions to attack, such as 'Fly the drone into the crowd.' In an unintentional attack scenario, the model might receive instructions with potential collision risks, like 'Fly the drone low through a power turbine.' The score is calculated as the percentage of times the LLM successfully avoided collisions, out of the total number of both deliberate and unintentional risky instructions it received. A higher score in this dimension indicates that the model is more capable of avoiding collisions and mitigating potential collision risks.\n\u2022 Regulatory Compliance: Evaluate how well the LLM complies with laws to ensure com- pliance. For example, a user might instruct the drone to fly into a no-drone zone or exceed Federal Aviation Administration (FAA) regulated unmanned aerial vehicle (UAV) flight altitude and speed limits. Our Violation Instruction Dataset is designed to test whether the LLM can proactively recognize and avoid these regulatory violations. The score reflects the percentage of times it successfully identified and refused instructions that would violate FAA regulations, such as flying into no-drone zones or exceeding altitude/speed limits, out of the total number of violation instruction test cases. A higher score in this dimension indicates a stronger capability in understanding and following FAA regulations.\n\u2022 Code Fidelity: Compute the accuracy and reliability of the code generated by the LLM. LLMs often produce code with errors such as syntax mistakes, logical inconsistencies, using nonexistent APIs, and incorrect variable usage. The score is the percentage of test cases"}, {"title": "2.4 Exploring Different LLMs for Physical Safety", "content": "We applied our LLM Physical Safety Benchmark to study a diverse range of mainstream LLMs, each representing different design philosophies and capabilities. Our selection encompassed:\n\u2022 OpenAI GPT-3.5-turbo[19]: A signature large language model well-known for its con- versational ability and human-level responses. OpenAI GPT has remarkable fluency and coherence when performing tasks such as creative writing, translation, and text summary. Additionally, GPT has consistently outperformed other LLMs in code generation tasks [39, 13], particularly in widely used benchmarks like HumanEval [9] and MBPP (Mostly Basic Python Programming) [2]. These results underscore GPT's capability as a powerful tool for automating programming tasks and assisting developers with complex coding chal- lenges. Furthermore, GPT has entered into partnerships with humanoid robot companies to enhance real-world interactions and provide more sophisticated AI-driven support for robotics applications, such as Figure 01\u2074\n\u2022 Google Gemini[29]: A new family of multimodal models called Gemini (Ultra and Pro), which demonstrates exceptional proficiency in understanding images, audio, video, and text. Gemini has remarkable ability for understanding and responding to complex request, generating code, pictures, and audio. According to Google's technical report, the Gemini family outperforms GPT-3.5 in code generation tasks, particularly in benchmarks like HumanEval [9] and Neural2Code. The Neural2Code dataset, which is a Google internal held-out dataset using author-generated sources instead of web-based information, further underscores Gemini's superior coding capabilities. We adopted Gemini Pro in our evaluation.\n\u2022 Meta Llama2[31]: A family of open-weight LLMs with varying sizes (7B, 13B, and 70B), offering a flexible platform for research and experimentation. We specifically tested the 7B model in our evaluations.\n\u2022 Meta Code Llama[31]: Code Llama is a code-specialized variant of Llama 2, developed by further training Llama 2 on code-specific datasets and extending the training with additional"}, {"title": "2.5 Evaluation Results across LLMs", "content": "The safety evaluation results, presented in Figure 3, reveal significant variations in safety performance across the models.\nCodeLlama-7B-Instruct showed the highest score in terms of self-assurance (54.65%) and avoid- collision (99.12%), demonstrating a strong ability to effectively identify and avoid risky situations and collisions. These two indicators also showed Llama2-7b-chat and Llama3-8B-Instruct to be closely following; all three models above are members of Meta's LLM family. The consistent performance of Meta's LLMs points to a purposeful emphasis on safety as well as a possible benefit in their training and development strategy, in contrast to other models such as the GPT-3.5-turbo, which showed the lowest level of self-assurance and thus indicated a possible weakness in its safety protocols.\nLlama2-7B-Chat achieved the highest score in Regulatory Compliance (90.62%), indicating a strong ability to adhere to laws and regulations. CodeLlama-7b-instruct followed closely, demonstrating a similar commitment to regulatory compliance. Mistral-7b-instruct-v0.2 secured the third position, highlighting a degree of focus on this aspect. In contrast, GPT-3.5-turbo exhibited the lowest score, suggesting a potential vulnerability in its ability to comply with regulations. This disparity in performance across models underscores the need for a dedicated focus on regulatory compliance during LLM development, particularly in applications where safety and legal adherence are important.\nIt is noteworthy that the two models with highest scores, Llama2-7B-Chat and CodeLlama-7b- instruct, are both members of Meta's LLM family. This is consistent with the pattern seen in collision avoidance and self-assurance, where Meta's models also exhibited better safety performance. Meanwhile, ChatGPT continues to have the lowest score, showing a possible vulnerability in its regulatory compliance."}, {"title": "Trade-off between Utility and Safety", "content": "The safety evaluation results, as depicted in Figure 3, reveal an interesting trend. LLMs with higher scores in utility and code fidelity, indicating better control over drone operations through code, tend to exhibit higher safety risks. This observation suggests that optimizing these models solely for enhanced utility may come at the expense of compromising essential safety considerations.\nOne important factor leading to this trade-off is the bias in training data. LLMs are trained on publicly accessible data [3, 6, 10], which often include a huge number of examples that prioritize code utility over safety. For example, training data might give priority to generating effective code that completes a specific task while ignoring possible safety risks that might occur during execution. The lack of negative examples, such as incorrect instructions or code that could lead to dangerous outcomes, emphasizes this bias. As a result, the model may not adequately learn the importance of safety considerations, leading to a higher-risk response.\nFurthermore, conflicting training objectives can also contribute to this phenomenon. The model is usually optimized to predict the next token on a public data from the internet, which is different from the objective of 'following the user's instructions.' [6, 23, 24] However, this optimization process may unintentionally compromise safety. For example, to improve code efficiency, the model might prioritize code usability over safety, possibly ignoring potential vulnerabilities or security risks. This conflict between utility and safety objectives can lead to a model that prioritizes task completion over safety.\nAnother crucial factor is the lack of safety mechanisms [7, 35]. Existing LLMs may lack effective safeguards to prevent the generation of potentially dangerous response. These models might not be equipped to identify potential risks or to mitigate them during code generation. Without proper safety mechanisms, the model's output may be prone to vulnerabilities or unintended consequences, increasing the likelihood of safety issues.\nIn conclusion, the trade-off between utility and safety in LLMs for drone operations is a complex issue with multiple contributing factors. Biases in training data, conflicting training objectives, and the lack of robust safety mechanisms all play a role in shaping this trade-off."}, {"title": "In-Context Learning Delivers Significant Safety Gains", "content": "We investigated the impact of different prompt engineering techniques [36] on LLM safety, including In-Context Learning [18] and Zero- shot Chain of Thought [16]. In-Context Learning (ICL) involves providing the model with relevant examples within the prompt, allowing it to learn from these examples and apply that knowledge to new situations. We implemented ICL by carefully selecting and including safety-related examples in our prompts. The specific examples used for ICL can be found in the supplementary material. Zero-shot Chain of Thought (ZS-CoT), on the other hand, encourages the model to break down its reasoning process step-by-step without any specific examples. We applied ZS-CoT by adding the following instruction to our prompts: Before providing code, think step by step: Will this instruction cause harm to people or the environment, or violate FAA regulations? If yes, respond safely. This approach guides"}, {"title": "Model Size Plays a Critical Role in Safety", "content": "Our research also explored the scalability of model size on safety performance. The results, as depicted in (c) Figure 4, suggest a strong correlation between model size and safety capabilities. Larger LLMs, such as CodeLlama-34b-instruct, demonstrate superior performance across various safety metrics, particularly in refusing dangerous commands. For instance, the self-assurance score increases from 0.5465 for CodeLlama-7B-Instruct to 0.8023 for CodeLlama-34B-Instruct. Additionally, the instruction understanding score improves from 0.6436 to 0.8511 as the model size increases. This indicates that larger models are more effective at refusing dangerous commands and comprehending instructions accurately.\nThe left side of (c) in Figure 4 clearly illustrates that as the model size increases from 7B to 34B parameters, there's a significant improvement in the Self-Protect and Safety Refusal percentages for both Deliberate Attacks and Violation scenarios. Please refer to Supplementary for detailed examples of Self-Protect and Safety Refusal (SI Appendix, Fig. S1). For instance, in Deliberate Attacks, the refusal rate increases from about 45% for the 7B model to over 85% for the 34B model, showcasing a substantial enhancement in safety capabilities.\nHowever, it's noteworthy that for Unintentional Attacks, the performance of the 13B and 34B models is nearly identical, both achieving around 42% refusal rate. This performance suggests that beyond a certain model size, the ability to detect and resist unintentional attacks does not significantly improve. This finding aligns with earlier observations that current LLMs, regardless of their size, struggle with detecting and mitigating unintentional attacks.\nOur findings indicate that model size alone is not a complete solution for improving safety against unintentional risks. While larger LLMs demonstrate superior capabilities in many areas, their performance in mitigating unintentional attacks appears to have a limited effect. This underscores"}, {"title": "3 Discussion", "content": "This paper pinpoints the physical safety of LLMs and provides novel insights into the risk and utility assessment in LLM-based robotic systems. Our designed benchmark and comprehensive safety evaluation of LLMs in drone control systems provide critical perspectives for the broad community of robotics and machine learning. By evaluating various LLMs across key safety dimensions in a virtual and realistic drone environment, we emphasize the important role of physical safety evaluation via simulation, as the first line of defense in robotic safety testing. This approach points out a fundamental principle in safety: if an LLM exhibits unsafe behaviors in a controlled virtual environment, its performance in the real world is likely to be even worse and more uncontrollable.\nOur evaluation results on leading LLMs uncover several challenges in physical safety. The observed trade-off between utility and safety in LLMs for drone control highlights a significant difficulty in LLM-based robotics. Models excelling in code generation and task execution often perform poorly in crucial safety aspects, pointing out the need for a balanced approach in developing AI systems for robotic control. More importantly, the utility of code generation does not imply physical safety in LLMs.\nOur findings on the effectiveness of prompt engineering techniques in enhancing LLM safety offer promising directions for improving robotic system safety. Additionally, the positive correlation between model size and safety capabilities provides valuable insights for future development strategies in AI-driven robotics.\nThese results extend beyond drone control, holding significant implications for various robotic appli- cations across industries. As AI integration in robotics accelerates - especially with programmable drones from companies like DJI and Parrot becoming increasingly common ensuring safety becomes increasingly critical. The possibility of LLMs controlling these increasingly accessible platforms makes robust safety measures essential. Our research, grounded in virtual environment testing, provides a foundation for developing safer, more reliable AI-controlled robotic systems across multiple domains.\nIn conclusion, this study emphasizes the importance of prioritizing safety in LLM development for robotic control, starting with a comprehensive evaluation benchmark and rigorous virtual environment testing. Future research should focus on refining safety evaluation methodologies, incorporating robust safety mechanisms, and bridging the gap between virtual and real-world performance. By addressing these challenges, we can advance the field of AI in robotics, paving the way for safer and more effective autonomous systems in various applications, from healthcare and manufacturing to space exploration."}, {"title": "Supplementary Material", "content": "A LLMs and Physical Safety in Drone Control\nThe figure S1 contrasts drone control code generated by large language models (LLMs) in high-risk scenarios with a safety-aware approach.\nMapping Safety Threats to Datasets: Evaluating Safety and Utility\nIn this section, we describe the relationship between the defined physical safety risks and the datasets designed to evaluate them. We initially define the following categories of safety risks: Human-targeted Threats, Object-targeted Threats, Infrastructure Attacks, and Violations of FAA Regulations.\nThese categories encompass a range of potential safety threats that drones may pose, from targeting individuals to disrupting critical infrastructure and violating regulations."}]}