{"title": "GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction", "authors": ["Yanxu Mao", "Peipei Liu", "Tiehan Cui"], "abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities from unstructured document text. Compared to sentence-level relation extraction, it requires more complex semantic understanding from a broader text context. Currently, some studies are utilizing logical rules within evidence sentences to enhance the performance of DocRE. However, in the data without provided evidence sentences, researchers often obtain a list of evidence sentences for the entire document through evidence retrieval (ER). Therefore, DocRE suffers from two challenges: firstly, the relevance between evidence and entity pairs is weak; secondly, there is insufficient extraction of complex cross-relations between long-distance multi-entities. To overcome these challenges, we propose GEGA, a novel model for DocRE. The model leverages graph neural networks to construct multiple weight matrices, guiding attention allocation to evidence sentences. It also employs multi-scale representation aggregation to enhance ER. Subsequently, we integrate the most efficient evidence information to implement both fully supervised and weakly supervised training processes for the model. We evaluate the GEGA model on three widely used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The experimental results indicate that our model has achieved comprehensive improvements compared to the existing SOTA model.", "sections": [{"title": "1 Introduction", "content": "Relation extraction (RE) is a crucial technology used to automatically identify and classify semantic relations between entities in natural language texts. Existing relation extraction tasks can be divided into two types: sentence-level relation extraction and document-level relation extraction (DocRE) (Peng et al., 2017; Verga et al., 2018). In sentence-level relation extraction datasets, each data entry"}, {"title": "2 Preliminary", "content": "2.1 Task formulation for DocRE and DocER\nLet's assume we have a document D containing n entities e = {C1,C2,...,en}. Each entity ei in the document has a corresponding position pi, and there may exist relations between entities. Our objective is to extract a set of relations R = {r1,2,...,rm} from document D, where each relation ri can be represented by a triple (ei, ej, rij), with rij being the relation label between entities ei and ej. Therefore, the task of DocRE can be formulated as follows: R = {(ei, ej, rij) | ei, ej \u2208 E,i \u2260 j}, rij is the relation label predicted by the relation classifier based on the contextual information of entity pairs (ei, ej).\nIn addition, Document-level Evidence Retrieval (DocER) aims to retrieve a list of evidence sentences evi[0,1,...,n] from document D to enhance relation extraction. Nowadays, researchers have extended relation triples (ei, ej, rij) by adding evidence sentence lists, resulting in relation quadruplets (ei, ej, rij, evi[0,1,...,n]). Relations between entity pairs can be predicted solely using the sentences from the evidence lists, without relying on the entire document."}, {"title": "3 Related Work", "content": "Our work is built upon a substantial body of recent work on document-level RE and ER.\nDOC-Relation Extraction (RE). Previous studies can be divided into three major categories (Zhou et al., 2021):"}, {"title": "4 Methodology", "content": "This section elucidates the main framework of the proposed method, illustrated in Figure 2, the model can be segmented into tripartite: Input Encoder Layer, GEGA Module and Classification Layer."}, {"title": "4.1 Input Encoder Layer", "content": "This work adheres to the methodology employed in prior research to incorporate specialized markers [CLS] and [SEP] at the begin and end of a designated document doc=[sentN] N=1 for the purpose of outlining the document's boundaries, where sent=[tn]n=1. Subsequently, input the document into the pre-trained BERT model. When the input length exceeds 512, the document is divided into two overlapping segments3. The first segment has a length of 512 and the second segment comprises the difference between the total input length and 512. Ultimately, the contextual embedding repre-"}, {"title": "4.2 GEGA Module", "content": "The GEGA module comprehends four parts: the Attention Concentration Layer, the Multi-GraphConv Layer, the Transformer-enc Layer, and the Collaborative Prediction Layer."}, {"title": "4.2.1 Attention Concentration Layer", "content": "We employ Attention Concentration Layer to transform the initial dependency tree into a fully connected weighted graph based on the dependency relations within the sentence. This approach can be construed as a soft pruning strategy (Xu et al., 2015) juxtaposed with the conventional hard pruning strategy (Guo et al., 2019). By assigning weights to the sequence data, as opposed to outright deletion, a greater amount of contextual information can be preserved, thereby fostering enhancements in module efficacy. Subsequently, we utilize the multi-head attention mechanism, wherein the input vector is mapped to several heads using a linear transformation layer to produce an adjacency matrix with varied weight distributions, denoted as $A^{(headi)} =Attention (QW_Q,(KW_K)^T,VW_V)$. We employ parallel computing to expedite the computational process.\n$A^{(headi)} = softmax (\\frac{QW_Q(KW_K)^T}{\\sqrt{d}})$\nwhere Q, K \u2208 RN\u00d7dmodel, N is the length of the sequence, dmodel is the dimensionality of the input feature, and $W_Q, W_K \\in \\mathbb{R}^{d_{model} \\times d_k}$ is the weight parameter associated with the linear transformation."}, {"title": "4.2.2 Multi-GraphConv Layer", "content": "The Graph Convolutional Networks (GCNs (Kipf and Welling, 2016)) is a deep learning framework tailored for the processing of graph-structured data. It is a semi-supervised learning method based on graph structure that aggregates and propagates"}, {"title": "4.2.3 Transformer-enc Layer", "content": "The Transformer-enc layer is composed of multiple encoder layers stacked together. These encoder layers bear resemblance to the encoder layers delineated in the transformer model introduced by Vaswani et al. (2017). However, distinctively, our approach involves solely utilizing the output generated by the final three layers of the Encoder for the purpose of averaging. Each encoder layer incorporates self-attention mechanism and Feedforward Neural Network (FFN). This module facilitates the derivation of hidden representations of entities and an attention distribution matrix that are used as input for subsequent layers. The calculations can be outlined as follows:\n$self-Att(H_Q, H_K, H_V) = softmax(\\frac{H_Q (H_K)^T}{\\sqrt{d_k}})H_V$\n$L_a = LayerNorm(H + self-Att(H))$\n$(H, \\widetilde{A}) = L_a + FFN(L_a)$\nWhere $H_Q, H_K, H_V$ is the query, key, and value representations obtained from the linear transformation of H, dk is the dimension of the attention head. LayerNorm is the layer normalization operation."}, {"title": "4.2.4 Collaborative Prediction Layer", "content": "The local context extraction methodology, as described by Zhou et al. (2021), is employed to ascertain the importance of individual tokens in relation to the entity pair (Es, Eo), which is interpreted as the sentence-level importance. Erecting on this base, document-level importance was deduced by apportioning diverse attention weights in accordance to the contribution of each sentence within the document to the prediction of entity relations, and by establishing a fixed threshold. Sentences that exceed this threshold are selected as evidence sentences. The sentence-level importance qi (Es,Eo) and document-level importance pj (Es, Eo) can be computed as follows:\n$q_i (E_s,E_o) = \\sum_{t \\in E_s}^l \\sum_{i \\in E_o} A_{E_s}A_{E_o}$\n$p_j(E_s,E_o) = \\sum_{l=1}^H q_i (E_s, E_o)$\nWe apportion more attention to evidence sentences and less to non-evidence sentences through evidence supervision to further coordinate the prediction results of document-level relation extraction. As depicted in Figure 4, we train a teacher model on the Human-Annotated Data (which encompasses relation labels and evidence sentences) of DocRED (Step 1). We utilize the trained teacher model to predict the entity relations and evidence"}, {"title": "4.3 Classification Layer", "content": "We begin with the computation of a weighted average of entity importance at the sentence-level qi(Es, Eo), and subsequently cascading it with the previous entity representation. Following this, we apply the tanh activation function to normalize the input to range between (-1, +1), resulting in the contextual representation of the two associated entities. The computation is elaborated as:\n$C_{E_s} = tanh (W_{E_s} [h_{e_s}; H^T q_i(E_s,E_o)] + b_{e_s})$\n$C_{E_O} = tanh (W_{E_O} [h_{e_o}; H^T q_i (E_s,E_o)] + b_{e_o})$,\nwhere WES, WEo \u2208 Rd\u00d72d, bes, beo \u2208 Rd.\nFinally, apply the grouped bilinear classifier proposed by Zheng et al. (2019) to calculate the relation category scores. $Score(E_s,E_o) = C_{E_s}^T W_{R_n} C_{E_o} + b_{R_n}$, The possibility of the entity pair (Es, Eo) possessing a relation Rn is computed thusly: $P (R_n | E_s, E_o) = Sigmoid (Score(E_s,E_o))$."}, {"title": "5 Experiments", "content": "5.1 Dataset and Evaluation\nDocRED4 (Yao et al., 2019) is a benchmark dataset for document-level relation extraction tasks, released by Tsinghua University. DocRED comprises numerous documents from Wikipedia and Wikidata, each annotated with entities, relations between entities, and evidence sentences that support relation triples. It serves as the predominant benchmark for DocRE model training and evaluation.\nRe-DocRED5 (Tan et al., 2022b) and Revisit-DocRED (Huang et al., 2022) are modified datasets of DocRED. They supplement a large number of relation triples to solve the problems of incomplete annotations, coreferential errors, and inconsistent logic in docred. Annotation quality has high accuracy and consistency, which provides a more reliable benchmark for DocRE-model training and evaluation.\nWe assess GEGA using an Nvidia Tesla V100 16GB GPU and evaluate it with F1, Ign-F1, and Evi-F1 metrics. Ign-F1 represents the calculated F1 score attained by excluding relational facts present in both the training and development/testing datasets. Evi-F1 serves as a significant measure for assessing the performance of ER and constitutes a new benchmark for assessing the quality of relation extraction models."}, {"title": "5.2 Single and Fusion", "content": "In the task of RE, the most ideal scenario is that the evidence sentence set of the dataset already contains all contextual information necessary to predict entity relations, thereby enabling accurate relation prediction results based solely on the evidence sentence set. However, manually labeled data and distant supervision data often fall short in this regard. Therefore, it is necessary to extract contextual information from the entire document to predict entity relations. We divide the above problem into two evaluation methods: (1) Single: extract entity relations from the entire document and obtain the corresponding prediction scores; (2) Fusion: predict entity relations based on a collection of evidence sentences and combine the prediction results with those from the Single method. This is similar to the Fusion of Evidence approach in Xie et al. (2022)."}, {"title": "5.3 Compared Methods", "content": "To ensure a fair comparison of the performance of DocRE baselines, we compare our model with three state-of-the-art methods, all using BERT-base as the pre-trained language model (PLM), which are: (1) Sequence-based methods: CNN (Yao et al., 2019), LSTM (Yao et al., 2019), BiLSTM (Yao et al., 2019). (2) Graph-based methods: GAIN (Zeng et al., 2020), MRN (Li et al., 2021), DocuNet (Zhang et al., 2021), GTN (Zhang et al., 2023a), SD-DocRE (Zhang et al., 2023b), AA (Lu et al., 2023). (3) Transformer-based methods: AT-LOP (Zhou et al., 2021), EIDER (Xie et al., 2022), SAIS (Xiao et al., 2022), PRISM (Choi et al., 2023), DREEAM (Ma et al., 2023). On this foundation, we categorize the above methods into two major classes: without Distant Supervision and with Distant Supervision."}, {"title": "6 Results and Analyses", "content": "We test the trained student and teacher models in both the Single and Fusion stages, and report the results on DocRED, Re-DocRED and Revisit-DocRED, where the results on Revisit-DocRED are moved to appendix."}, {"title": "6.1 Results on DocRED", "content": "Table 1 indicates that GEGA achieves superior Ign-F1 and F1 metrics compared to the established DocRE Baselines on both the development set and the test set. The single stage of the student model has achieved performance levels comparable to the leading DREEAM (Ma et al., 2023). Notably, the fusion stage of the student model achieved the highest recorded scores, surpassing DREEAM by 0.34% (Ign F1) and 0.55% (F1) on the development set, as well as by 0.17% (Ign F1) and 0.44% (F1) on the test set.\nGEGA also performs well in the test of the new benchmark Evi-F1, surpassing the previous most advanced DREEAM by 0.41% (Evi-F1) and 0.46% (Evi-F1) on the development set and test set respectively. In the table, it is evident that the Transformer-based and Graph-based models outperform the Sequence-based ones, validating the rationality behind integrating infographics with the Transformer."}, {"title": "6.2 Results on Re-DocRED", "content": "Table 2 presents the feedback outcomes of GEGA on the development and test sets of RE-DocRED, demonstrating that our GEGA achieves state-of-the-art results compared to other methods utilizing BERT-base as a pre-trained language model. Notably, GEGA has outperformed all other methods in the table during the fusion stage without Distant Supervision (Teacher). GEGA secured the highest Ign F1 and F1 scores in the fusion stage with Distant Supervision (Student), with improvements of 2.08% (Ign F1) and 2.52% (F1) respectively on the development set over the second-place GTN-BERT (Zhang et al., 2023a), and by 1.58% (Ign F1) and 1.97% (F1) respectively on the test set."}, {"title": "6.3 Effect Analysis of GCNs and ER", "content": "Based on the test scores on DocRED and Re-DocRED, we observe that graph-based models such as DocuNET (Zhang et al., 2021) and GTN-BERT (Zhang et al., 2023a) have achieved superior performance in the field. Graphs have an advantage in conveying document-level contextual information, so we used grid search to select a 2-layer GNNs to guide multiple attention maps. Additionally, DREEAM (Ma et al., 2023) is the first method to enhance relation extraction performance purely through evidence-guided attention, it has already achieved excellent scores on the DocRED set. By integrating GCNs with evidence retrieval, we further improved its scores by 0.41% (Evi-F1) and 0.46% (Evi-F1) on the dev and test sets, respectively. Therefore, we conclude that GCNs and ER significantly enhance performance in the relation extraction task."}, {"title": "6.4 Ablation Studies", "content": "We conducted ablation experiments were conducted on the development set to analyze the GNNS layer and Training phase of GEGA. The single phase of GEGA (Student) was utilized as the test"}, {"title": "7 Conclusion", "content": "We propose GEGA, the first model to employ GCNs and ER jointly guided attention to enhance DocRE. We validate the superiority of our model on three widely used datasets: DocRED, Re-DocRED and Revisit-DocRED. GEGA is trained using parallel computing in both fully supervised and semi-supervised settings, without incurring additional overhead, making it convenient for use in the era of Large Language Models (LLMs). In the future, we aim to leverage the scalability of GEGA and apply it to a broader range of scenarios, including entity recognition, event extraction, and more."}, {"title": "8 Limitations", "content": "The model GEGA is subject to two limitations. Firstly, when utilizing Multi-GraphConv Layers to induce multiple fully connected attention distribution matrices, there is a possibility of generating one matrix that differs significantly from others in terms of weight distribution. This could lead to significant deviations in prediction results. We hypothesize that guiding the construction of multiple fully connected attention matrices using evidence information may reduce the occurrence of such undesirable situations, a conjecture that will be verified in future work. Secondly, it is acknowledged that the relations between most entity pairs can be predicted based on the local context of the entities. However, our model utilizes evidence sentences retrieved from the entire document corpus, which are strongly correlated with the entity pairs of interest, rather than evidence sentences obtained specifically for individual relation triples. This approach may result in the model carrying more global contextual information while reducing the utilization of local context information."}, {"title": "9 Ethics Statement", "content": "Our proposed GEGA demonstrates outstanding scalability and applicability, serving as an excellent solution for both DocRE and DocER tasks. This method is evaluated solely on publicly available datasets, ensuring no compromise on individual privacy. Furthermore, we provide the source code implementation of GEGA to enable researchers to reproduce its performance authentically, fostering academic exchange in the field of DocRE."}, {"title": "A Loss", "content": "To accommodate the requirements of the RE and ER tasks within both the teacher model and the"}, {"title": "A.1 RE loss:", "content": "For the RE tasks of the two models previously described, we implement the Adaptive Thresholding Loss (ATL) as proposed by ATLOP. During the training phase, we use a threshold class (TH) to learn a threshold such that the logits of the positive class RP exceed it, and the logits of the negative class RN fall below it.\n$\\mathcal{L}_{ATL} = - \\sum_{r \\in \\mathbb{R}P} \\sum_{E_s, E_o} log \\frac{exp(Score_{(E_s, E_o)}))}{\\sum_{r' \\in \\mathbb{R}P \\cup \\{TH\\}} exp (Score_{r' (E_s, E_o)}))}$"}, {"title": "A.2 ER loss:", "content": "The tasks targeted by the teacher model and the student model differ in detail. The teacher model is trained on Human-Annotated Data, which includes reliable manually annotated evidence sentences, while the student model is trained on Distantly-Supervised Data that incorporates evidence sentences identified through the teacher model's ER. Considering these distinctions, there is a requirement for specialized loss computation methods. Consequently, we propose both document-level and sentence-level loss calculations."}, {"title": "Document-level Loss:", "content": "By integrating the document-level importance distribution $p_j^{D}(E_s, E_o)$ with the original manually annotated evidence sentences $z^{(E_s,E_o)}$ from the dataset, we induce a localized context representation that contributes significantly to RE. We use the Kullback-Leibler Divergence (KL divergence), a method for measuring the difference between two probability distributions within the same event space. In this paper, it is used to assess the degree of divergence between $p_j^{(E_s,E_o)}$ and $z^{(E_s,E_o)}$:\n$\\mathcal{L}_{ER}^{doc} = KL(z^{(E_s,E_o)}||p_j^{(E_s,E_o)})$\n$\\mathcal{L}_{ER}^{doc} = \\sum_{sent=1}^{L} z^{(E_s,E_o)} log\\frac{z^{(E_s,E_o)}}{p_j (E_s,E_o)}$"}, {"title": "Sentence-level Loss:", "content": "We use the teacher model trained on Human-Annotated Data to perform ER testing on Distantly-Supervised Data, predicting its sentence-level evidence distribution $\\widetilde{q_i^{(E_s,E_o)}}$. Thereafter, utilize Kullback-Leibler (KL) divergence to compute the difference in sentence-level probability distributions between the teacher model and the student model.\n$\\mathcal{L}_{ER}^{t\\=1} = KL(\\widetilde{q_i^{(E_s,E_o)}}\\ ||\\hat{q_i^{(E_s,E_o)}})$\n$\\mathcal{L}_{ER}^{t\\=1} = \\sum_{i} \\widetilde{q_i^{(E_s,E_o)}} log \\frac{\\widetilde{q_i^{(E_s,E_o)}}}{\\hat{q_i^{(E_s,E_o)}}}$\nFinally, we apply the prevalent weighted summation technique for document-level and sentence-level losses to equilibrate the losses of RE and ER, where \\\\ serving as a hyperparameter.\n$\\mathcal{L} = (1 - \\lambda) \\mathcal{L}_{CRE} + \\lambda \\mathcal{L}_{ER}$"}, {"title": "B Hyperparameter", "content": "We ran tests on the above three datasets using different random seeds five times and reported the average test accuracy. In the manuscript, a comprehensive Table 4 delineating the hyperparameters and configuration has been furnished, encapsulating pivotal settings utilized across the experimental trials. These hyperparameters, meticulously curated and fine-tuned, exert control over multifarious facets of the model's dynamics throughout the training and evaluation phases."}, {"title": "C Results on Revisit-DocRED", "content": "Table 5 shows the test results of GEGA (Student) on Revisit-DocRED. According to the test method of Huang et al. (2022), we trained on the DocRED"}, {"title": "D Supplementary Model Analysis", "content": "Here, we conducted experimental analyses on the number of GNNs layers and the number of heads in the Multi-GraphConv Layer of GEGA. We selected the number of GNNs layers from {1, 2, 3, 4, 5} and the number of heads from {1, 2, 3, 4}. The experimental results are shown in Figure 6. Ultimately, the analysis concluded that the performance is optimal when layers=2 and heads=2."}, {"title": "E Case Study", "content": "To further demonstrate the superior performance of GEGA, we extracted a case from the DocRED dataset and used it to compare the performance of GEGA with two other advanced methods (SAIS and DREEAM)."}, {"title": "E.1 RE", "content": "From Figure 7, we observe five types of relations among five entities. SAIS correctly identified four types of relations but overlooked the \\\"country of citizenship\\\" relation between \\\"Robert F./Mark R.\\\" and \\\"United States\\\" Additionally, it incorrectly identified a \\\"country of citizenship\\\" relation between \\\"Robert F./Mark R.\\\" and \\\"American\\\" as well as a \\\"located in the administrative territorial entity\\\" relation with \\\"Terry McAuliffe\\\" While DREEAM correctly identified all the existing relations, it excessively identified a \\\"country\\\" relation between \\\"American\\\" and \\\"United States\\\" In contrast, GEGA perfectly extracted the correct relational network in this case."}, {"title": "E.2 ER", "content": "During evidence retrieval, both GEGA and DREEAM labeled the evidence source for the relation between United States and Virginia as [S1, S2], missing [SO]. Additionally, DREEAM incorrectly labeled the evidence information for the relation between Terry McAuliffe and United States as [SO, S6]. SAIS incorrectly labeled the evidence information for the relation between Virginia and Terry McAuliffe as [SO, S1, S2, S6]. We believe that GEGA's superior performance is also attributed to its better ER performance."}]}