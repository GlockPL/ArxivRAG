{"title": "GEGA: Graph Convolutional Networks and Evidence Retrieval Guided Attention for Enhanced Document-level Relation Extraction", "authors": ["Yanxu Mao", "Peipei Liu", "Tiehan Cui"], "abstract": "Document-level relation extraction (DocRE) aims to extract relations between entities from unstructured document text. Compared to sentence-level relation extraction, it requires more complex semantic understanding from a broader text context. Currently, some studies are utilizing logical rules within evidence sentences to enhance the performance of DocRE. However, in the data without provided evidence sentences, researchers often obtain a list of evidence sentences for the entire document through evidence retrieval (ER). Therefore, DocRE suffers from two challenges: firstly, the relevance between evidence and entity pairs is weak; secondly, there is insufficient extraction of complex cross-relations between long-distance multi-entities. To overcome these challenges, we propose GEGA, a novel model for DocRE. The model leverages graph neural networks to construct multiple weight matrices, guiding attention allocation to evidence sentences. It also employs multi-scale representation aggregation to enhance ER. Subsequently, we integrate the most efficient evidence information to implement both fully supervised and weakly supervised training processes for the model. We evaluate the GEGA model on three widely used benchmark datasets: DocRED, Re-DocRED, and Revisit-DocRED. The experimental results indicate that our model has achieved comprehensive improvements compared to the existing SOTA model.", "sections": [{"title": "Introduction", "content": "Relation extraction (RE) is a crucial technology used to automatically identify and classify semantic relations between entities in natural language texts. Existing relation extraction tasks can be divided into two types: sentence-level relation extraction and document-level relation extraction (DocRE) (Peng et al., 2017; Verga et al., 2018). In sentence-level relation extraction datasets, each data entry contains only one sentence, and there is a single entity pair within the sentence for which the relation needs to be predicted. In contrast, DocRE datasets contain multiple sentences per data entry, corresponding to multiple entity pairs whose relations need to be predicted. As depicted in Figure 1, each entity pair may appear multiple times within the document and may have different relation types (Yao et al., 2019), necessitating the analysis of a larger contextual scope to determine the relation for each entity pair.\nFurthermore, assuming there are n entities in a data entry, predicting the relation for an entity pair involves pairing the anchor entity with the remaining n-1 entities one by one. This approach results in significant unnecessary memory overhead, as most pairs of entities do not have any relation between them.\nExisting methods can be mainly divided into three categories (Zhou et al., 2021): sequence-based, graph-based, and transformer-based. Sequence-based models commonly employ pre-trained language models to produce word embeddings and character embeddings, transforming sequences of words or characters within texts into vector representations for processing (Ye et al., 2020; Tang et al., 2020). Models based on dependency graphs utilize dependency information to construct document-level graph (Zeng et al., 2021; Li et al., 2021; Zhang et al., 2023b,a), which are then processed through graph neural networks for inference. Transformer-based models utilize the self-attention mechanism to refine the representation of each word by assessing its contextual relations with all other words in the text (Xiao et al., 2022; Ma et al., 2023).\nThe aforementioned three types of relation extraction methods suffer from two limitations. First, relation extraction between entity pairs generally requires only a set of sentences as supporting evidence, without the need to focus on redundant irrelevant information. However, these methods utilize all the information in the long text for relation extraction. Therefore, Ma et al. (2023) proposed an evidence retrieval-based relation extraction method. However, this method retrieves a list of evidence information for the entire document, resulting in poor relevance between this information and the relational entity pairs. Second, in DocRE, multiple entities are discretely distributed across different sentences or even paragraphs. Extracting their relations requires fully learning and understanding the semantics of the long text at the document-level. Existing methods rely on dependency parsing to construct multidimensional graph structures for semantic understanding and relation reasoning.However, they perform poorly when faced with complex intersecting relations due to the large number of entity pairs involved.\nTo address the aforementioned two issues and the insufficient annotation of evidence sentences in the dataset (Yao et al., 2019), we propose a novel model for DocRE: GEGA. This model is trained under both fully supervised and weakly supervised settings. First, we utilize a complex model (Teacher) trained with full supervision to infer over distant supervision data, extracting evidence sentences and assigning token weights. Then, we use this weight information as supervisory signals to guide the training of a simplified model (Student). Finally, we fine-tune the student model to adapt it to specific tasks and datasets, thereby improving performance. In summary, this article has two contributions:\n(1) We propose a novel DocRE model, GEGA\u00b9 (Graph Convolutional Networks and Evidence Retrieval Guided Attention). This model combines graph structures and Transformers to retrieve evidence sentences highly relevant to the relational entity pairs from the document, guiding the attention to assign higher weights to this evidence information, thereby enhancing the performance of relation extraction.\n(2) Experiments conducted on the three public datasets DocRED, Re-DocRED and Revisit-DocRED show that GEGA can achieve the new SOTA2 results on document-level relation extraction compared to existing methods under the same experimental settings."}, {"title": "Preliminary", "content": "2.1 Task formulation for DocRE and DocER\nLet's assume we have a document D containing n entities e = {C1,C2,...,en}. Each entity er in the document has a corresponding position pi, and there may exist relations between entities. Our objective is to extract a set of relations R = {r1,2,...,rm} from document D, where each relation ri can be represented by a triple (ei, ej, rij), with rij being the relation label between entities ei and ej. Therefore, the task of DocRE can be formulated as follows: R = {(ei, ej, rij) | ei, ej \u2208 E,i \u2260 j}, rij is the relation label predicted by the relation classifier based on the contextual information of entity pairs (ei, ej).\nIn addition, Document-level Evidence Retrieval (DocER) aims to retrieve a list of evidence sentences evi[0,1,...,n] from document D to enhance relation extraction. Nowadays, researchers have extended relation triples (ei, ej, rij) by adding evidence sentence lists, resulting in relation quadruplets (ei, ej, rij, evi[0,1,...,n]). Relations between entity pairs can be predicted solely using the sentences from the evidence lists, without relying on the entire document."}, {"title": "Related Work", "content": "Our work is built upon a substantial body of recent work on document-level RE and ER.\nDOC-Relation Extraction (RE). Previous studies can be divided into three major categories (Zhou et al., 2021):\nSequence-based methods. Zeng et al. (2014); Cai et al. (2016); Tang et al. (2020); Yao et al. (2019), and Sorokin and Gurevych (2017) use methods such as Conditional Random Fields (CRF) or Recurrent Neural Networks (RNN, (Cho et al., 2014)) to accurately identify and label entities in text and predict the relations between these entities by learning the contextual sequential semantic information of each word. For example, Tang et al. (2020) use different neural network architectures to perform sequence encoding of the entire document to learn the semantic representation of entity pairs and extract the relations between them. Yao et al. (2019) employ BiLSTM to simultaneously consider the forward and backward information in the text sequence, learning and understanding the different semantic paths from one entity to another, thus inferring the relations between entity pairs.\nGraph-based methods. Veli\u010dkovi\u0107 et al. (2018); Christopoulou et al. (2019); Sahu et al. (2019) model entities and relations in documents as graph structures, then use Graph Neural Networks (GNNs) to learn the relations between entities. This approach effectively leverages structural information and global context among entities. Additionally, researchers have proposed hard pruning and soft pruning strategies for dependency tree structures to optimize the model's speed and storage efficiency. Zhang et al. (2018) and Mandya et al. (2020) use hard pruning strategies to retain words near the shortest path between two entities, maximizing the removal of irrelevant content while integrating relevant information. Guo et al. (2019) proposed a soft pruning method that directly takes the full dependency tree as input and automatically learns how to selectively focus on relevant substructures that are useful for the relation extraction task. Subsequently, Li et al. (2021); Nan et al. (2020) proposed refined strategies to enhance cross-sentence relation reasoning by automatically inducing latent document-level graphs. This strategy allowing the model to incrementally aggregate relevant information for both local and global reasoning.\nTransformer-based methods. This approach does not use any graph structures but instead adapts to the document-level relation extraction task by fine-tuning pre-trained models (Wang et al., 2019). Ye et al. (2020) introduced a copy-based training objective into the basic pre-trained language model, enabling the model to better capture coreference information. Tang et al. (2020) employed a hierarchical aggregation method to obtain reasoning information at different granularities at the document-level. Zhou et al. (2021) addressed the multi-label and multi-entity issues in relation extraction datasets through adaptive thresholds and local context pooling.\nDOC-Evidence Retrieval (ER). Currently, a few studies have investigated the importance of evidence information in document-level relation extraction tasks. Yao et al. (2019) directly incorporated evidence sentence instances supporting entity relations into a new dataset. However, in the absence of evidence sentences, evidence information needs to be generated through an Evidence Retrieval (ER) task. Huang et al. (2021) employed heuristic rules to select informative sets of paths from the entire document to discover evidence sentences and further optimized relation extraction by combining BiLSTM. Ma et al. (2023) integrated evidence information into a Transformer-based DocRE system by directly guiding attention, without introducing any additional trainable parameters for the ER task. Compared to our work, they did not incorporate graph neural networks for end-to-end learning to derive the advantages of attention weights. In contrast, GEGA provides a more reliable allocation of weights for evidence information by constructing a fully connected graph and its corresponding fully connected matrix to learn structured information."}, {"title": "Methodology", "content": "This section elucidates the main framework of the proposed method, illustrated in Figure 2, the model can be segmented into tripartite: Input Encoder Layer, GEGA Module and Classification Layer.\n4.1 Input Encoder Layer\nThis work adheres to the methodology employed in prior research to incorporate specialized markers [CLS] and [SEP] at the begin and end of a designated document $doc=[sent_1,...,sent_N]_{N=1}$ for the purpose of outlining the document's boundaries, where $sent=[t_n]_{n=1}^{L}$. Subsequently, input the document into the pre-trained BERT model. When the input length exceeds 512, the document is divided into two overlapping segments\u00b3. The first segment has a length of 512 and the second segment comprises the difference between the total input length and 512. Ultimately, the contextual embedding representation H and attention matrix A of the token are derived:\n$(H, A) = [(h_1, a_1), (h_2, a_2), ..., (h_{i \\times L}, a_{i \\times L})] = BERT([doc])$ (1)\nwhere l is the length of the sentence (i.e., the number of tokens), and L is the length of the input document (i.e., the number of sentences).\nFor each entity, the efficacy of the max pooling function is pronounced when the inter-entity relations are explicitly articulated. Nevertheless, in the context of this study, the relations among entities remain ambiguous. It is understood that an entity may be referenced by one or several mentions, and a mention might uniquely identify an entity or fail to ascertain a definite corresponding entity. This necessitates the calculation of an entity's embedding based on the embeddings of each associated mention. Following methodology of Jia et al. (2019), a soft version of the max function LogSumExp (LSE), is utilized to compute the embeddings of entities while concurrently capturing the attention distribution:\n$C_{emb} = LSE (x_1,...,x_n) = log \\sum_{i}^{M_e}exp (h_i)$ (2)\n$C_{att} = \\frac{exp(h_i)}{\\sum_{i=1}^{Me} exp(h_i)}$ (3)\nwhere e is an entity comprising multiple mentions, while a is the distribution of attention for mention i within entity e. $M_e$ is the number of mentions for entity e.\n4.2 GEGA Module\nThe GEGA module comprehends four parts: the Attention Concentration Layer, the Multi-GraphConv Layer, the Transformer-enc Layer, and the Collaborative Prediction Layer."}, {"title": "Attention Concentration Layer", "content": "We employ Attention Concentration Layer to transform the initial dependency tree into a fully connected weighted graph based on the dependency relations within the sentence. This approach can be construed as a soft pruning strategy (Xu et al., 2015) juxtaposed with the conventional hard pruning strategy (Guo et al., 2019). By assigning weights to the sequence data, as opposed to outright deletion, a greater amount of contextual information can be preserved, thereby fostering enhancements in module efficacy. Subsequently, we utilize the multi-head attention mechanism, wherein the input vector is mapped to several heads using a linear transformation layer to produce an adjacency matrix with varied weight distributions, denoted as $A^{(headi)} =Attention(QW^Q, KW^K)$. We employ parallel computing to expedite the computational process.\n$A^{(headi)} = softmax (\\frac{QW^x(KW^K)^T}{\\sqrt{d}})$ (4)\nwhere Q, K \u2208 RN\u00d7dmodel, N is the length of the sequence, dmodel is the dimensionality of the input feature, and $W^Q, W^K \u2208 R^{dmodelxd_k}$ is the weight parameter associated with the linear transformation."}, {"title": "Multi-GraphConv Layer", "content": "The Graph Convolutional Networks (GCNs (Kipf and Welling, 2016)) is a deep learning framework tailored for the processing of graph-structured data. It is a semi-supervised learning method based on graph structure that aggregates and propagates node features in the graph structure, thus deriving node representations. The Multi-GraphConv (M-G) Layer is a densely connected graph structure data processing module that is constructed based on GCNs. Illustrated in Figure 3, the n-th sublayer's output within the Multi-GraphConv (M-G) Layer serves as the subsequent l - n sublayers' input, with the N-th layer receiving an aggregation of all output features from the initial n - 1 layers.\nInitially, the linear transformation of the adjacency matrix $A^{(head_i)}$ on the input features is computed for each head i. Subsequently, the impact of neighboring nodes' features on the present node is determined for each layer l, and the current node's features are consolidated with the previous layer's output:\n$H^{(i,l)} = ReLU ((A^{(head i)} x) H^{(i,l)} W^{(i,l)}) + H^{(i,l-1)}$ (5)\nwhere, L is the quantity of layers in the graph convolution layer, $W^{(i,l)}$ is the weight parameter of the i-th head in the l-th layer.\nThe feature representation resulting from the output of each head is combined to form the final output of this layer:\n$H^{(MultiHead)} = Concat (H^{(1,L)},..., H^{(i,L)}) W^O$ (6)\nwhere $W^O \u2208 R^{h\\cdot dxdmodel}$ is the weight parameter of the linear transformation applied to the ultimate output."}, {"title": "Transformer-enc Layer", "content": "The Transformer-enc layer is composed of multiple encoder layers stacked together. These encoder layers bear resemblance to the encoder layers delineated in the transformer model introduced by Vaswani et al. (2017). However, distinctively, our approach involves solely utilizing the output generated by the final three layers of the Encoder for the purpose of averaging. Each encoder layer incorporates self-attention mechanism and Feedforward Neural Network (FFN). This module facilitates the derivation of hidden representations of entities and an attention distribution matrix that are used as input for subsequent layers. The calculations can be outlined as follows:\n$self-Att(HQ, \u0397\u03ba, Hv) = softmax (\\frac{H_Q H_K^T}{\\sqrt{dk}}) Hv$ (7)\n$La = LayerNorm(H + self-Att(H))$ (8)\n$(H, \u00c3) = La + FFN(La)$ (9)\nWhere HQ, \u0397\u03ba, Hy is the query, key, and value representations obtained from the linear transformation of H, dk is the dimension of the attention head. LayerNorm is the layer normalization operation."}, {"title": "Collaborative Prediction Layer", "content": "The local context extraction methodology, as described by Zhou et al. (2021), is employed to ascertain the importance of individual tokens in relation to the entity pair (Es, Eo), which is interpreted as the sentence-level importance. Erecting on this base, document-level importance was deduced by apportioning diverse attention weights in accordance to the contribution of each sentence within the document to the prediction of entity relations, and by establishing a fixed threshold. Sentences that exceed this threshold are selected as evidence sentences. The sentence-level importance $q_i (E_s,E_o)$ and document-level importance $p_j (E_s, E_o)$ can be computed as follows:\n$q_i (E_s,E_o) = \\sum_{t \\in E_s} \\sum_{t \\in E_o} A_{E_s, t} A_{E_o, t}$ (10)\n$p_j(E_s,E_o) = \\sum_{l=1}^{L}q_l (E_s, E_o)$ (11)\nWe apportion more attention to evidence sentences and less to non-evidence sentences through evidence supervision to further coordinate the prediction results of document-level relation extraction. As depicted in Figure 4, we train a teacher model on the Human-Annotated Data (which encompasses relation labels and evidence sentences) of DocRED (Step 1). We utilize the trained teacher model to predict the entity relations and evidence sentence distribution in Distantly-Supervised Data (which encompasses relation labels but lacks evidence sentences) (Step 2). Subsequently, we train a student model on the Distantly-Supervised Data that incorporates evidence sentences (Step 3), and retrain the student model using Human-Annotated Data (Step 4). Additionally, we define a row vector $z(E_s,E_o)$ consisting only of 0s and 1s, generated based on Human-Annotated Data. This vector indicates whether each sentence is an evidence sentence for the relation triples: 1 if it is, and 0 if it is not.\n$z(E_s,E_o) = \\frac{\\sum_{sent=1}^{L} z_{sent=1}^{L} (E_s,E_o) /l}{\\sum_{sent=1}^{L}z (E_s,E_o)}$ (12)\nwhere 1 is the row vector composed of all 1, L is the total count of sentences contained within the document."}, {"title": "Classification Layer", "content": "We begin with the computation of a weighted average of entity importance at the sentence-level $q_i (E_s,E_o)$, and subsequently cascading it with the previous entity representation. Following this, we apply the tanh activation function to normalize the input to range between (-1, +1), resulting in the contextual representation of the two associated entities. The computation is elaborated as:\n$C_{ES} = tanh (W_{Es} [h_{es}; H^T q_i(E_s,E_o)] + b_{es})$\n$C_{EO} = tanh (W_{Eo} [h_{eo}; H^T q_i(E_s,E_o)] + b_{eo}),$ (13)\nwhere $W_{Es}, W_{Eo} \u2208 R^{d\u00d72d}$, $b_{es}, b_{eo} \u2208 R^d$.\nFinally, apply the grouped bilinear classifier proposed by Zheng et al. (2019) to calculate the relation category scores. $Score(E_s,E_o) = C_{ES}^T W^{R_n} C_{EO} + b^{R_n}$, The possibility of the entity pair (Es, Eo) possessing a relation Rn is computed thusly: $P (R_n | E_s, E_o) = Sigmoid (Score(E_s,E_o)).$"}, {"title": "Experiments", "content": "5.1 Dataset and Evaluation\nDocRED\u2074 (Yao et al., 2019) is a benchmark dataset for document-level relation extraction tasks, released by Tsinghua University. DocRED comprises numerous documents from Wikipedia and Wikidata, each annotated with entities, relations between entities, and evidence sentences that support relation triples. It serves as the predominant benchmark for DocRE model training and evaluation.\nRe-DocRED\u2075 (Tan et al., 2022b) and Revisit-DocRED (Huang et al., 2022) are modified datasets of DocRED. They supplement a large number of relation triples to solve the problems of incomplete annotations, coreferential errors, and inconsistent logic in docred. Annotation quality has high accuracy and consistency, which provides a more reliable benchmark for DocRE-model training and evaluation.\nWe assess GEGA using an Nvidia Tesla V100 16GB GPU and evaluate it with F1, Ign-F1, and Evi-F1 metrics. Ign-F1 represents the calculated F1 score attained by excluding relational facts present in both the training and development/testing datasets. Evi-F1 serves as a significant measure for assessing the performance of ER and constitutes a new benchmark for assessing the quality of relation extraction models.\n5.2 Single and Fusion\nIn the task of RE, the most ideal scenario is that the evidence sentence set of the dataset already contains all contextual information necessary to predict entity relations, thereby enabling accurate relation prediction results based solely on the evidence sentence set. However, manually labeled data and distant supervision data often fall short in this regard. Therefore, it is necessary to extract contextual information from the entire document to predict entity relations. We divide the above problem into two evaluation methods: (1) Single: extract entity relations from the entire document and obtain the corresponding prediction scores; (2) Fusion: predict entity relations based on a collection of evidence sentences and combine the prediction results with those from the Single method. This is similar to the Fusion of Evidence approach in Xie et al. (2022).\n5.3 Compared Methods\nTo ensure a fair comparison of the performance of DocRE baselines, we compare our model with three state-of-the-art methods, all using BERT-base as the pre-trained language model (PLM), which are: (1) Sequence-based methods: CNN (Yao et al., 2019), LSTM (Yao et al., 2019), BiLSTM (Yao et al., 2019). (2) Graph-based methods: GAIN (Zeng et al., 2020), MRN (Li et al., 2021), DocuNet (Zhang et al., 2021), GTN (Zhang et al., 2023a), SD-DocRE (Zhang et al., 2023b), AA (Lu et al., 2023). (3) Transformer-based methods: ATLOP (Zhou et al., 2021), EIDER (Xie et al., 2022), SAIS (Xiao et al., 2022), PRISM (Choi et al., 2023), DREEAM (Ma et al., 2023). On this foundation, we categorize the above methods into two major classes: without Distant Supervision and with Distant Supervision."}, {"title": "Results and Analyses", "content": "We test the trained student and teacher models in both the Single and Fusion stages, and report the results on DocRED, Re-DocRED and Revisit-DocRED, where the results on Revisit-DocRED are moved to appendix.\n6.1 Results on DocRED\nTable 1 indicates that GEGA achieves superior Ign-F1 and F1 metrics compared to the established DocRE Baselines on both the development set and the test set. The single stage of the student model has achieved performance levels comparable to the leading DREEAM (Ma et al., 2023). Notably, the fusion stage of the student model achieved the highest recorded scores, surpassing DREEAM by 0.34% (Ign F1) and 0.55% (F1) on the development set, as well as by 0.17% (Ign F1) and 0.44% (F1) on the test set.\nGEGA also performs well in the test of the new benchmark Evi-F1, surpassing the previous most advanced DREEAM by 0.41% (Evi-F1) and 0.46% (Evi-F1) on the development set and test set respectively. In the table, it is evident that the Transformer-based and Graph-based models outperform the Sequence-based ones, validating the rationality behind integrating infographics with the Transformer.\n6.2 Results on Re-DocRED\nTable 2 presents the feedback outcomes of GEGA on the development and test sets of RE-DocRED, demonstrating that our GEGA achieves state-of-the-art results compared to other methods utilizing BERT-base as a pre-trained language model. Notably, GEGA has outperformed all other methods in the table during the fusion stage without Distant Supervision (Teacher). GEGA secured the highest Ign F1 and F1 scores in the fusion stage with Distant Supervision (Student), with improvements of 2.08% (Ign F1) and 2.52% (F1) respectively on the development set over the second-place GTN-BERT (Zhang et al., 2023a), and by 1.58% (Ign F1) and 1.97% (F1) respectively on the test set.\n6.3 Effect Analysis of GCNs and ER\nBased on the test scores on DocRED and Re-DocRED, we observe that graph-based models such as DocuNET (Zhang et al., 2021) and GTN-BERT (Zhang et al., 2023a) have achieved superior performance in the field. Graphs have an advantage in conveying document-level contextual information, so we used grid search to select a 2-layer GNNs to guide multiple attention maps. Additionally, DREEAM (Ma et al., 2023) is the first method to enhance relation extraction performance purely through evidence-guided attention, it has already achieved excellent scores on the DocRED set. By integrating GCNs with evidence retrieval, we further improved its scores by 0.41% (Evi-F1) and 0.46% (Evi-F1) on the dev and test sets, respectively. Therefore, we conclude that GCNs and ER significantly enhance performance in the relation extraction task.\n6.4 Ablation Studies\nWe conducted ablation experiments were conducted on the development set to analyze the GNNS layer and Training phase of GEGA. The single phase of GEGA (Student) was utilized as the test benchmark. The results of the score post-ablation of each part are presented in Table 3. Initially, we removed the Attention Concentration Layer, which resulted in a minor decline in performance. Subsequently, upon removing the Multi-GraphConv Layer, a significant performance deterioration was observed, implying the importance of constructing multiple attention distribution graphs for relation extraction. Upon removal of the Transformer-enc Layer, we noted a relatively minor decline in performance. We speculate that this may be related to using Transformer based BERT as PLM.\nAdditionally, during the training process, we performed ablation on the self-training, fine-tuning, and Distant Supervision-training stages, in order to further analyze their impact. The results indicate that when the ER self-training phase is omitted, performance declines, whereas the absence of the fine-tuning stage did not lead to a noticeable decline in performance. Further more, omitting the Distant Supervision-training stage caused severe performance degradation. These findings highlight the effectiveness of our ER method in enhancing relation extraction."}, {"title": "Conclusion", "content": "We propose GEGA, the first model to employ GCNs and ER jointly guided attention to enhance DocRE. We validate the superiority of our model on three widely used datasets: DocRED, Re-DocRED and Revisit-DocRED. GEGA is trained using parallel computing in both fully supervised and semi-supervised settings, without incurring additional overhead, making it convenient for use in the era of Large Language Models (LLMs). In the future, we aim to leverage the scalability of GEGA and apply it to a broader range of scenarios, including entity recognition, event extraction, and more."}, {"title": "Limitations", "content": "The model GEGA is subject to two limitations. Firstly, when utilizing Multi-GraphConv Layers to induce multiple fully connected attention distribution matrices, there is a possibility of generating one matrix that differs significantly from others in terms of weight distribution. This could lead to significant deviations in prediction results. We hypothesize that guiding the construction of multiple fully connected attention matrices using evidence information may reduce the occurrence of such undesirable situations, a conjecture that will be verified in future work. Secondly, it is acknowledged that the relations between most entity pairs can be predicted based on the local context of the entities. However, our model utilizes evidence sentences retrieved from the entire document corpus, which are strongly correlated with the entity pairs of interest, rather than evidence sentences obtained specifically for individual relation triples. This approach may result in the model carrying more global contextual information while reducing the utilization of local context information."}, {"title": "Ethics Statement", "content": "Our proposed GEGA demonstrates outstanding scalability and applicability, serving as an excellent solution for both DocRE and DocER tasks. This method is evaluated solely on publicly available datasets, ensuring no compromise on individual privacy. Furthermore, we provide the source code implementation of GEGA to enable researchers to reproduce its performance authentically, fostering academic exchange in the field of DocRE."}, {"title": "Case Study", "content": "To further demonstrate the superior performance of GEGA, we extracted a case from the DocRED dataset and used it to compare the performance of GEGA with two other advanced methods (SAIS and DREEAM).\nE.1 RE\nFrom Figure 7, we observe five types of relations among five entities. SAIS correctly identified four types of relations but overlooked the \"country of citizenship\" relation between \"Robert F./Mark R.\" and \"United States\" Additionally, it incorrectly identified a \"country of citizenship\" relation between \"Robert F./Mark R.\" and \"American\" as well as a \"located in the administrative territorial entity\" relation with \"Terry McAuliffe\" While DREEAM correctly identified all the existing relations, it excessively identified a \"country\" relation between \"American\" and \"United States\" In contrast, GEGA perfectly extracted the correct relational network in this case.\nE.2 ER\nDuring evidence retrieval, both GEGA and DREEAM labeled the evidence source for the relation between United States and Virginia as [S1, S2], missing [SO]. Additionally, DREEAM incorrectly labeled the evidence information for the relation between Terry McAuliffe and United States as [SO, S6]. SAIS incorrectly labeled the evidence information for the relation between Virginia and Terry McAuliffe as [SO, S1, S2, S6]. We believe that GEGA's superior performance is also attributed to its better ER performance."}]}