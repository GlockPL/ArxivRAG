{"title": "Competitive Programming with Large Reasoning Models", "authors": ["OpenAI"], "abstract": "We show that reinforcement learning applied to large language models (LLMs) significantly boosts\nperformance on complex coding and reasoning tasks. Additionally, we compare two general-purpose\nreasoning models OpenAI ol and an early checkpoint of 03 with a domain-specific system, ol-\nioi, which uses hand-engineered inference strategies designed for competing in the 2024 International\nOlympiad in Informatics (IOI). We competed live at IOI 2024 with ol-ioi and, using hand-crafted\ntest-time strategies, placed in the 49th percentile. Under relaxed competition constraints, ol-ioi\nachieved a gold medal. However, when evaluating later models such as 03, we find that 03 achieves\ngold without hand-crafted domain-specific strategies or relaxed constraints. Our findings show that\nalthough specialized pipelines such as ol-ioi yield solid improvements, the scaled-up, general-purpose\n03 model surpasses those results without relying on hand-crafted inference heuristics. Notably, 03\nachieves a gold medal at the 2024 IOI and obtains a CODEFORCES rating on par with elite human\ncompetitors. Overall, these results indicate that scaling general-purpose reinforcement learning,\nrather than relying on domain-specific techniques, offers a robust path toward state-of-the-art AI in\nreasoning domains, such as competitive programming.", "sections": [{"title": "1 Introduction", "content": "Competitive programming is widely recognized as a challenging benchmark for evaluating reasoning and\ncoding proficiency [2]. Solving complex algorithmic problems demands advanced computational thinking\nand problem solving skills. Moreover, these problems are also objectively gradable, making it an ideal\ntestbed to assess the reasoning capabilities of AI systems.\nRecent work on program synthesis with large language models [1] has demonstrated that even rela-\ntively general models, ranging from 244M to 137B parameters, can generate short Python scripts from\nnatural language instructions. Importantly, performance improves log-linearly with model size, and fine-\ntuning significantly boosts accuracy. Concurrently, Codex [2], an early code-focused LLM, excelled at\nPython program generation and powered GitHub Copilot. Further progress came from AlphaCode [7],\nwhich tackled competitive programming tasks using large-scale code generation and heuristics at in-\nference, and the subsequent AlphaCode2 [6], whose improvements nearly doubled AlphaCode's solved\nproblems and placed it in the 85th percentile on the CODEFORCES platform. Both AlphaCode systems\nused large-scale sampling of up to a million candidate solutions per problem before selecting their top\n10 submissions with a hand-engineered test-time strategy.\nSince then, significant progress has been made in harnessing reinforcement learning to improve LLMS'\nreasoning skills. This has led to the emergence of large reasoning models (LRMs): language models\ntrained via reinforcement learning to \"reason\" and \"think through\" extended chains of thought. In\nparticular, OpenAI's o1 [4, 12] and its soon-to-be-released successor 03 [13] use chain-of-thought reasoning\nto tackle intricate tasks such as mathematics and coding. Work by DeepSeek-R1 [3] and Kimi k1.5 [15]\nindependently illustrates how learning chain-of-thought boosts performance on both mathematical and\nprogramming challenges.\nAn open question is how domain-specific, hand-engineered inference strategies compare to learned\napproaches that models generate and execute on their own. We have three systems available that can\nshed light on this question: 01, ol-ioi, and early checkpoints of 03. OpenAI ol was the first large rea-\nsoning model and used general purpose methods to improve programming performance. Building on\nthis foundation, ol-ioi was a fine-tuned system tailored to compete in the 2024 International Olympiad\nin Informatics (IOI) and used test-time strategies similar to those used in the AlphaCode system. This\nspecialization led to strong performance improvements on both the 2024 IOI and competitive program-\nming platforms such as CODEFORCES. Subsequent advances led to the development of 03, which has"}, {"title": "2 OpenAI 01", "content": "We start with OpenAI ol, a large language model trained with reinforcement learning to tackle complex\nreasoning tasks. By generating an extended internal chain of thought before answering [16], o1 resembles\na human who methodically works through a challenging problem step by step. Reinforcement learning\nrefines this chain-of-thought process, helping the model identify and correct errors, break down complex\ntasks into manageable parts, and explore alternate solution paths when an approach fails. These in-\ncontext reasoning capabilities substantially boost ol's overall performance on a wide range of tasks.\nAdditionally, OpenAI ol is trained to use external tools [14], especially for writing and executing\ncode in a secure environment.\u00b9 This capability lets ol verify whether its generated code compiles, passes\nprovided test cases, and meets other correctness checks. By testing and refining its outputs, ol iteratively\nimproves its solutions over the course of a single sample."}, {"title": "2.1 CodeForces Benchmark", "content": "CODEFORCES is a programming competition website that hosts live contests. It is internationally com-\npetitive and frequented by some of the best competitive programmers in the world.\nTo assess our models' competitive programming abilities, we simulated CODEFORCES contests under\nconditions that closely mirrored real competitions. This included using the full test suite for each problem\nand enforcing appropriate time and memory constraints for solutions.\nOur evaluation focused on Division 1 contests from 2024 and December 2023, ensuring all test contests\noccurred after the data cut-off for both pretraining and RL. Additionally, we conducted a contamination\ncheck as a sanity measure, leveraging the OpenAI embedding API to verify that test problems had not\nbeen seen during training."}, {"title": "3 OpenAI 01-ioi", "content": "During our development and evaluation of OpenAI ol, we found that increasing both the amount of\nreinforcement learning (RL) compute and test-time inference compute consistently improved model per-\nformance."}, {"title": "3.1 Coding RL Fine-tuning", "content": "Our first step extended the reinforcement learning phase of OpenAI ol, focusing on coding tasks. By\ndedicating additional training compute to programming problems, we bolstered the model's ability to\nplan, implement, and debug more involved solutions. Concretely:\n1. We resumed RL training from the OpenAI 01 checkpoint.\n2. We specifically emphasized challenging programming problems, helping the model improve C++\ngeneration and runtime checks.\n3. We guided the model to produce outputs in the IOI submission format.\nThis added focus on coding allowed ol-ioi to write and execute C++ programs during inference. The\nmodel improved its reasoning by iteratively running and refining solutions, thereby strengthening both\nits coding and problem-solving skills."}, {"title": "3.2 01-ioi Test-time Strategy", "content": "At a high level, we divided each IOI problem into its constituent subtasks, sampled 10,000 solutions from\nol-ioi for each subtask, and then employed a clustering- and reranking-based approach to decide which\nsolutions from this set to submit."}, {"title": "Problem formulation", "content": "For ol-ioi we chose to attempt to solve the individual subtasks of each problem\nseparately, as the scoring for IOI is done on a subtask-by-subtask basis and gives each competitor the\nmaximum score over all of their attempts on each subtask. To do this, we divided each IOI problem into\nits composite subtasks (using the divisions laid out in the scoring guide for each problem). This was\ndone simply by creating one version of the document for each subtask with the information about the\nother subtasks removed."}, {"title": "Clustering", "content": "We clustered the generated solutions based on their outputs on model-generated test in-\nputs. For each subtask, we first prompted the model to write random test input generators in C++ given\nthe problem specification and subtask. We used these generators to generate 256 random test inputs.\nTo ensure the validity of these test inputs, we then prompted the model to write test input validators\nin C++ that check, given a test input, whether it satisfies the subtask constraints. Finally, we accepted\neach test input that passes at least 75% of the validators. For each subtask, we generated 256 of these\nrandom test case inputs, and then clustered based on their outputs for these test cases. Any programs\nthat matched each other's outputs on all test inputs would be placed in the same cluster."}, {"title": "Reranking", "content": "We then implemented the reranking core of our test-time compute strategy. We scored\neach solution based on:\n\u2022 The quality of the solution according to a learned scoring function.\n\u2022 Errors on model-generated test inputs.\n\u2022 Failing the provided public test cases.\nEach cluster was given a score defined as the average score of the samples it contained minus a penalty\nfor each time a sample submission was attempted from that cluster. The weights of all of these penalties\nwere tuned by random search on solutions to previous years' IOI problems, by directly simulating the\nsubmission process."}, {"title": "Submission", "content": "We then submitted up to 50 (the maximum number allowed for human competitors) of\nthese solutions in a round-robin fashion over subtasks, starting from the hardest. We selected the top-\nranked solution in the top-ranked cluster for each given subtask. When a subtask was solved (meaning\nthat the maximum score was attained), we ceased sampling on that subtask. When submitting solutions\nto any subtask that was a strict superset of a solved subtask, we would filter out any solutions that did\nnot match the outputs on test inputs of the solved constituent subtasks, allowing us to rapidly narrow\ndown candidate solutions on harder subtasks by rejecting those that would almost certainly have failed\neasier subtasks."}, {"title": "3.3 CodeForces Benchmark", "content": "Once again, we simulated CODEFORCES contests to evaluate ol-ioi's coding abilities, closely mirroring\ncontest conditions with the complete test suite for each problem and appropriate time and memory\nrestrictions for solutions."}, {"title": "3.4 IOI 2024 Live Competition", "content": "The ol-ioi system participated in the 2024 International Olympiad in Informatics (IOI) under the same\nconditions as human contestants. It had ten hours to solve six challenging algorithmic problems and was\nallowed up to 50 submissions per problem. We show the results in Figure 4.\nDuring the competition, our system generated 10,000 candidate solutions for each problem, and\nselected 50 submissions using our test-time selection strategy. This strategy prioritized submissions\nbased on their performance on the IOI public test cases, model-generated test cases, and a learned\nscoring function. The model scored 213 points, placing it in the 49th percentile of the competition."}, {"title": "4 OpenAI 03", "content": "Building on the insights gained from ol and ol-ioi, we explore the limits of reinforcement learning (RL)\ntraining alone, without relying on human-engineered test-time strategies. While ol-ioi achieved strong"}, {"title": "4.1 CodeForces Benchmark", "content": "We evaluate an early checkpoint of the 03 model on our CODEFORCES benchmark set, where each prompt\nincludes the problem description, constraints, and any available sample test cases."}, {"title": "4.2 IOI 2024 Benchmark", "content": "Although we competed in IOI 2024 using ol-ioi, we retrospectively evaluated a checkpoint of 03 on the\nsame six IOI 2024 problems to compare performance under identical conditions. As with ol-ioi, we\nstrictly adhered to the official IOI rules, which permit a maximum of 50 submissions per problem.\nThe 03 results on the IOI 2024 were produced by a later version of 03 than the CODEFORCES results,\nand included additional fresher training data. IOI 2024 occurred after the training cut-off for this model,\nand we additionally confirmed with search that the IOI test problems are not contaminated with the\ntraining set."}, {"title": "Sampling Approach", "content": "Unlike o1-ioi, which sampled solutions separately for each subtask, we adopted\na different approach when evaluating 03: sampling from a single prompt containing the original problem"}, {"title": "5 Software Engineering Evaluations", "content": "We have demonstrated how reasoning significantly enhances LLM performance in competitive program-\nming, where solving complex algorithmic challenges requires deep logical thinking. However, we also\nsought to evaluate the impact of reasoning on real-world coding tasks. To this end, we tested our models\non two datasets: the HackerRank Astra\u00b2 dataset and SWE-bench verified\u00b3 [5, 11]."}, {"title": "5.1 HackerRank Astra", "content": "The HackerRank Astra dataset is composed of 65 project-oriented coding challenges, each crafted to\nsimulate real-world software development tasks. These challenges cover a range of frameworks, including\nReact.js, Django, and Node.js, allowing for hands-on experience in building features and applications.\nWhat sets this dataset apart is its focus on assessing problem-solving skills in complex, multi-file, long-\ncontext scenarios that mirror actual development environments. Unlike typical competitive programming\ndatasets, HackerRank Astra does not provide public test cases, which prevents us from relying on hand-\ncrafted test-time tactics. Evaluating performance with this dataset reveals whether reasoning abilities\nenhance success in algorithmic problem solving alone, or extend to more practical, industry-related\ncoding tasks."}, {"title": "5.2 SWE-Bench Verified", "content": "SWE-bench Verified is OpenAI's preparedness team's human-validated subset of SWE-bench that more\nreliably evaluates AI models' ability to solve real-world software issues. This validated set of 500 tasks\nfixes certain issues with SWE-bench such as incorrect grading of correct solutions, under-specified prob-\nlem statements, and overly specific unit tests. This helps ensure the benchmark accurately grades model\ncapabilities.\nTo illustrate performance on this software task, we display the results presented in the ol system\ncard [4] as well as results from an early 03 checkpoint [13]. Because ol-preview was not trained to\nuse code execution or file editing tools, the best-performing open-source scaffold at the time of initial\nimplementation, Agentless was used. Unlike for IOI, no specialized test-time strategies was used for\nSWE-Bench verified. All models are given 5 tries to generate a candidate patch. If the model fails after"}, {"title": "6 Conclusion", "content": "Through the o-series large reasoning models, we demonstrate that chain-of-thought reasoning is a pow-\nerful strategy for improving performance in coding tasks, from competitive programming benchmarks\nsuch as CODEFORCES and IOI to complex software engineering challenges like SWE-bench and Astra.\nOur findings highlight that increasing reinforcement learning training compute, coupled with enhanced\ntest-time compute, consistently boosts model performance to nearly match the best humans in the world.\nGiven these results, we believe o-series large reasoning models will unlock many new use cases for AI in\nscience, coding, math, and many other fields."}, {"title": "A Authorship, credit attribution, and acknowledgments", "content": "Data Preparation: Borys Minaiev, Ignasi Clavera, Lorenz Kuhn, Nat McAleese, Oleg M\u00fcrk, Szymon\nSidor\nIOI Model Training: Ahmed El-Kishky, Mostafa Rohaninejad\nSampling Infrastructure: Andre Saraiva, Hunter Lightman, Vineet Kosaraju, Wenda Zhou\nTest-time Strategy: Alexander Wei, Daniel Selsam, David Dohan, Francis Song, Ignasi Clavera, Max\nSchwarzer, Rhythm Garg, Rui Shu\nEvaluation: Andre Saraiva, Ignasi Clavera, Lorenz Kuhn, Nat McAleese\nLeadership: Jakub Pachocki, Jerry Tworek, Lukasz Kaiser, Mark Chen\n03 Model Development 03 contributors [13]."}, {"title": "B Additional CodeForces Details", "content": "In order to compare our models to human competitive programmers, we simulate contests. This section\nprovides details of how the simulation is performed, how the overall score and ratings are calculated, as\nwell as the per-contest results."}, {"title": "B.1 Data", "content": "For our test set we use \"Division 1\" contests from late 2023 and 2024, all of which occurred after the\n03 training set data cut-off. As a redundant additional check, we used embedding search to confirm\nthat the test problems have not been seen by the model during training. We excluded one contest that\ncontained an interactive problem for which grading was inconvenient, but otherwise included all post-\ncut-off Division 1 problems to which we had access at the time. During training we used a validation\nset of primarily Division 2 problems; when that set indicated that performance was very strong we built\nand evaluated the Division 1 set presented here."}, {"title": "B.2 Grading", "content": "We run the complete set of tests for each problem, and have confirmed that our test environment closely\nmatches the official CODEFORCES grading service, including by manually submitting solutions for the\nhardest problems to the official CODEFORCES graders.\nFollowing AlphaCode [6] we allow the model to make 10 independent submissions against the full\ntest set and mark a problem as solved if any one of those 10 passes. This is close to but not strictly the\nsame as the human affordance, as human participants see only the results of the pre-tests during the\ncompetition. However in Division 1 contests the pre-tests are typically \"strong\" (highly correlated with\nfull tests), and in our results the number of failures before a passing submission is typically small (see\n1). We did not have access to labels for which test cases were pre-tests."}, {"title": "B.3 Thinking Time", "content": "Competitors receive a higher score for submitting their solutions faster. Because models can think in\nparallel and simultaneously attempt all problems, they have an innate advantage over humans. We\nelected to reduce this advantage in our primary results by estimating o3's score for each solved problem\nas the median of the scores of the human participants that solved that problem in the contest with the\nsame number of failed attempts.\nWe could instead use the model's real thinking time to compute ratings. 03 uses a learned scoring\nfunction for test-time ranking in addition to a chain of thought. This process is perfectly parallel and true\nmodel submission times therefore depend on the number of available GPU during the contest. On a very\nlarge cluster the time taken to pick the top-ranked solutions is (very slightly more than) the maximum\nover the thinking times for each candidate submission. Using this maximum parallelism assumption and\nthe sequential 03 sampling speed would result in a higher estimated rating than presented here. We note\nthat because sequential test-time compute has grown rapidly since the early language models, it was not\nguaranteed that models would solve problems quickly compared to humans, but in practice 03 does."}, {"title": "B.4 Estimated Rating", "content": "The CODEFORCES rating system is described by the creator in three blog posts [8, 9, 10]. Ratings are\nsimilar to the Elo system and satisfy the property that if competitor A has rating $R_A$ and competitor B\nhas rating $R_B$ then the probability that A ranks better than B any final contest standings is estimated\nas\n$\\frac{1}{1 + 10^{\\frac{R_B-R_A}{400}}}$\nTo find the model rating we first calculate the rank of the model in each of the test contest from the\ntotal contest score (described above) and then directly maximize the likelihood of the observed rankings\nand human ratings with respect to the model rating using the equation above. We average to ensure\nthat contests with more participants are not over-weighted.\nWe validated that this recovers known human ratings based on their contest performance and also\ngives similar values to linearly predicting participant rating from their average solve rate."}, {"title": "B.5 Percentile performance", "content": "Codeforces maintains a global leaderboard of active participants, and an estimated rating can be used\nto compare to that group. We can also directly compare the solve rate of 03 in our test contests to the\nother participants in those contests."}, {"title": "B.6 Per Problem Breakdown", "content": ""}, {"title": "C IOI Submissions", "content": "This section presents the solutions generated by ol-ioi during the 2024 International Olympiad in Infor-\nmatics."}, {"title": "C.1 Nile", "content": "All 100 possible points for Nile were scored in a single submission."}, {"title": "C.2 Message", "content": "This solution achieved a score of 79.64 out of 100, with full marks awarded for subtask 1 and partial\ncredit received for subtask 2."}, {"title": "C.3 Tree", "content": "A total of 30 points were scored on Tree across two separate submissions."}, {"title": "C.3.1 Submission 1", "content": "The first submission achieved a score of 17 out of 100, with points earned from subtasks 1 and 4."}, {"title": "C.3.2 Submission 2", "content": "Submission 2 achieved 13 of 100 points on subtask 2."}, {"title": "C.4 Hieroglyphs", "content": "A total of 44 points was scored on Hieroglyphs across two separate submissions."}, {"title": "C.4.1 Submission 1", "content": "In the first submission, a score of 34 out of 100 points was achieved, distributed across subtasks 1, 2,\nand 4."}, {"title": "C.4.2 Submission 2", "content": "In the second submission, the model scored 10 points on subtask 3."}, {"title": "C.5 Mosaic", "content": "A total of 42 points were scored on Mosaic across two separate submissions."}, {"title": "C.5.1 Submission 1", "content": "The first submission achieved a score of 22 out of 100, with points distributed across subtasks 1, 2, and\n4."}, {"title": "C.5.2 Submission 2", "content": "The model scored 20 points in the second submission on subtasks 1, 3, and 5."}, {"title": "C.6 Sphinx", "content": "A total of 71.5 points were scored on Sphinx across two separate submissions."}, {"title": "C.6.1 Submission 1", "content": "The first submission achieved a score of 50 out of 100, with 50% partial credit earned on all subtasks."}]}