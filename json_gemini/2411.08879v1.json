{"title": "4D Gaussian Splatting in the Wild with Uncertainty-Aware Regularization", "authors": ["Mijeong Kim", "Jongwoo Lim", "Bohyung Han"], "abstract": "Novel view synthesis of dynamic scenes is becoming important in various applications, including augmented and virtual reality. We propose a novel 4D Gaussian Splatting (4DGS) algorithm for dynamic scenes from casually recorded monocular videos. To overcome the overfitting problem of existing work for these real-world videos, we introduce an uncertainty-aware regularization that identifies uncertain regions with few observations and selectively imposes additional priors based on diffusion models and depth smoothness on such regions. This approach improves both the performance of novel view synthesis and the quality of training image reconstruction. We also identify the initialization problem of 4DGS in fast-moving dynamic regions, where the Structure from Motion (SfM) algorithm fails to provide reliable 3D landmarks. To initialize Gaussian primitives in such regions, we present a dynamic region densification method using the estimated depth maps and scene flow. Our experiments show that the proposed method improves the performance of 4DGS reconstruction from a video captured by a handheld monocular camera and also exhibits promising results in few-shot static scene reconstruction.", "sections": [{"title": "Introduction", "content": "Dynamic novel View Synthesis (DVS) aims to reconstruct dynamic scenes from captured videos and generate photorealistic frames for an arbitrary new combination of a viewpoint and a time step. This task has emerged as a vital research area in the 3D vision community with rapid advancements in augmented reality and virtual reality. Early DVS research primarily relied on neural radiance fields [29, 69, 10, 13, 11, 38, 39, 41, 5, 12, 50]. In contrast, more recent methods [61, 18, 31] extend 3D Gaussian Splatting [23] to account for the additional time dimension in dynamic scenes, and these techniques are referred to as 4D Gaussian Splatting.\nDespite the recent success of 4D Gaussian Splatting models [61, 18, 31, 68], their applicability remains largely limited to controlled and purpose-built environments. Most existing models are developed and tested with multi-view video setups [29, 41]. While there are several methods tackling monocular video settings, these setups are still controlled and fall short of in-the-wild scenarios. For instance, [38, 69] maintain multi-view characteristics, where the camera captures a broad arc around a slow-moving object. Also, HyperNeRF [39] relies on unrealistic train-test splits, with both sets sampled from the same video trajectory, which renders the task closer to video interpolation than genuine novel view synthesis. In this paper, we focus for the first time on more natural, real-world monocular videos [14], where a single handheld camera moves around fast-moving objects.\nIn casually recorded monocular videos, which often lack sufficient multi-view information, 4D Gaussian Splatting algorithms tend to overfit the training frames in real-world scenarios. To address overfitting, recent regularization techniques [26, 7, 58, 67, 25, 36, 20] can be applied to provide"}, {"title": "Related Work", "content": "In recent years, significant advances have been made in novel view synthesis [34, 6, 15, 60, 35, 23]. Initially focused on static scenes, novel view synthesis has shifted towards dynamic scenes through the integration of motion modeling, now referred to as Dynamic novel View Synthesis (DVS). Early approaches [13, 10, 38, 5, 12, 50, 11] are largely driven by Neural Radiance Fields (NeRF). Some studies [13, 10] capture dynamics implicitly through temporal inputs or latent codes, and other approaches [38, 5, 12, 50, 11] focus on training the canonical NeRF and its deformation fields.\nThe introduction of 3D Gaussian Splatting (3DGS) [23] has marked a paradigm shift in novel view synthesis, leading to the development of 4D Gaussian Splatting (4DGS) [68, 18, 61, 31] for DVS. These 4DGS methods deform canonical 3D Gaussian primitives over time using additional deformation networks, which can be based on MLPs, learnable control points, Hexplane [5, 61], or polynomial functions. While these models excel at reconstructing dynamic scenes in controlled environments [38, 69, 29, 41, 39], they face significant challenges when applied to casually recorded monocular videos, posing substantial hurdles for real-world applications."}, {"title": "2.2 Regularization Techniques in Sparse Reconstruction", "content": "Casually recorded monocular videos typically provide limited multi-view information, as they are typically captured with a single handheld camera that only exhibits gentle motion. Consequently, reconstructing dynamic scenes from these videos is often regarded as a form of sparse reconstruction due to the lack of multi-view data.\nIn the context of sparse reconstruction, various regularization techniques have been proposed to mitigate overfitting on limited training images [36, 25, 20, 48, 22, 4, 49, 53, 24, 63, 43, 8, 59, 55, 16, 65, 62, 2, 17, 16]. These approaches generally involve rendering images or depth maps for unseen views to provide additional priors. For instance, some methods leverage depth priors based on estimated depths for novel views [43, 8, 59, 55, 16, 72], while others incorporate color smoothness constraints to enhance these views [25, 36]. Building on the success of diffusion models [44], recent approach [62] starts to incorporate diffusion priors to produce more realistic images of novel views. While these methods effectively enhance novel view synthesis performance at test time, they inherently compromise the quality of training image reconstructions. Since both the reconstruction accuracy and the novel view synthesis quality are equality important in our target task, the trade-off caused by the na\u00efve application of the regularization techniques is not desirable."}, {"title": "2.3 Uncertainty Quantifications in Novel View Synthesis", "content": "Uncertainty estimation in novel view synthesis has primarily been explored with Neural Radiance Fields (NeRF) [29]. Pioneering approaches [37, 51, 52] re-parameterize MLP networks in NeRF using Bayesian models to compute the uncertainty of network predictions. Inspired by InfoNeRF [25], which considers entropy along rays for few-shot NeRFs, some studies [70, 66, 27] quantify uncertainty using the entropy of density along rays from a novel view. Additionally, Density-aware NeRF Ensembles [56] measures uncertainty by examining the variance in RGB images produced by an ensemble of models.\nIn contrast, uncertainty quantification in Gaussian Splatting [23] remains largely underexplored, with only a few works addressing this issue. Savant et al. [46] incorporate variational inference into the rendering pipeline, but this approach increases learnable parameters. Similarly, FisherRF [21] quantifies the uncertainty of Gaussian primitives by aggregating the diagonal of the Hessian matrix of the log-likelihood function; however, it is not straightforward to obtain a scalar value of uncertainty from the Hessian matrix for each Gaussian primitive. Our approach, on the other hand, directly quantifies the observed information of each Gaussian primitive by aggregating their contributions to the reconstruction of training images."}, {"title": "3 Preliminary: 4D Gaussian Splatting", "content": "This section briefly overviews 3D Gaussian Splatting (3DGS) [23] and explains the deformation modeling in 4D Gaussian Splatting (4DGS) [68, 18, 61, 31] for dynamic scenes."}, {"title": "3.1 3D Gaussian Splatting", "content": "Gaussian primitive 3D Gaussian Splatting has demonstrated real-time, state-of-the-art rendering quality on static scenes. It uses an explicit 3D scene representation consisting of a set of 3D Gaussian ellipsoids, denoted by \u0393 = {\u03b3\u2081, ..., \u03b3\u03ba}. Each Gaussian primitive, \u03b3k, is based on an unnormalized 3D Gaussian kernel, \\(G_k(x)\\), parameterized by \u03bck and \u03a3k as follows:\n\n\n\nwhere \u03bck \u2208 \\(R^3\\) is the center position, \u03a3k \u2208 \\(R^{3\u00d73}\\) is an anisotropic covariance matrix, and x \u2208 \\(R^3\\) is an arbitrary location in 3D space. The covariance matrix \u03a3k is valid only when positive semi-definite, which is challenging to enforce during optimization. To ensure this condition, we learn \u03a3k by decomposing it into two learnable components, a rotation matrix Rk and a scaling matrix Sk as follows:\n\n\n\nIn addition to the standard Gaussian parameters such as \u03bc\u03ba, Rk, and Sk, the Gaussian primitive requires additional learnable parameters for its opacity, \u03b1k \u2208 [0, 1], and feature, fk \u2208 \\(R^d\\), which is typically represented by RGB colors or spherical harmonic (SH) coefficients. Thus, each Gaussian primitive is represented as \u03b3k := (\u03bc\u03ba, Rk, Sk, \u03b1k, fk).\nDifferentiable rasterization Before rendering with the Gaussian primitives I on an image space, each 3D Gaussian kernel, \\(G_k(x; \u03bc\u03ba, \u03a3\u03ba)\\), is projected onto a 2D image space and forms a 2D Gaussian kernel, \\(G(r; \u03bc, \u03a3)\\), where \u3160 : \\(R^3\\) \u2192 \\(R^2\\) denotes a projection from the world coordinate to an image space. In the projected Gaussian representation, r \u2208 \\(R^2\\) indicates a pixel location in an image, and the 2D mean \u03bc \u2208 \\(R^2\\) and covariance \u03a3\u2208 \\(R^2\\) are given by\n\n\n\nwhere J denotes the Jacobian of the affine approximation of the projective transformation, and W is the world-to-camera transform matrix. When rendering the primitives in I to a target camera, they are sorted by their depths with respect to the camera center. The color of a pixel r is then obtained by a-blending, which is given by\n\n\n\nwhere \\(w_k(r)\\) represents a relative contribution of each Gaussian primitive to pixel r and c(fk, r) is the color of a pixel r measured along the view direction. If a feature vector fk is based on spherical harmonics coefficients, the color is decoded from fk using the view direction associated with pixel r; otherwise, the feature vector fk can be identical to the RGB color of the primitive. For more details, please refer to the original Gaussian Splatting paper [23]. Note that, following the a-blending procedure in 3DGS [23], \\(w_k(r)\\) is given by\n\n\n\nwhere \u03b1\u03baG (r; \u03bc, \u03a3) is the opacity of the kth projected primitive at the junction with a ray corresponding to pixel \\(r\\) and \\(\u03a0_{j=1}^{k-1} (1 - \u03b1_jG(r; \u03bc, \u03a3))\\) is the transmittance at the primitive \u03b3k on pixel r, which measure how much light penetrates the preceding primitives along the ray."}, {"title": "3.2 Deformation Modeling in 4D Gaussian Splatting", "content": "To represent 4D scenes using Gaussian Splatting, recent algorithms [68, 18, 61, 31] deform the 3D Gaussian primitives from their canonical states to a target state over time. The transformed position \u03bct, rotation Rt, and scale St at time t are given by\n\n\n\nwhere the deformation functions \u03c6\u03bc(\u00b7), \u03c6r(\u00b7), and $5(\u00b7) can be various forms, including MLPs, learnable control points [18], Hexplane [5, 61], or polynomial functions. A deformed 3D Gaussian primitive at time t is represented as \u03b3k(t) := (\u03bc, R, S, \u03b1k, fk). The projection onto a 2D space follows the same procedure as the static 3D Gaussian Splatting, presented in Equation (4). Our approach adopts the Hexplane structure for deformation, similar to [61]."}, {"title": "4 Uncertanty-Aware 4D Gaussian Splatting", "content": "We now discuss the proposed uncertainty-aware regularization technique designed for the balance between reconstruction quality on training images and generalization to unseen views."}, {"title": "4.1 Uncertainty-Aware Regularization", "content": "We first estimate how informative each Gaussian primitive is for reconstruction, based on its visibility from all pixels in training images and its opacity, which is given by\n\n\n\nwhere r is a pixel in a training image I \u2208 T, and \\(w_k(r)\\) is the contribution of each Gaussian primitive Yk to pixel r during a-blending, as described in Equation (5).\nThe parameters of an informative Gaussian primitive are typically estimated accurately with high confidence. Conversely, a Gaussian primitive that is not properly supported by training images struggles with low accuracy and high uncertainty of its parameter estimation. Based on these observations, the uncertainty of each Gaussian primitive, Uk, is defined as\n\n\n\nwhere the sigmoid function is used to bound and normalize Ck and {co, C1} control the inflection point shift and the slope of the sigmoid function, respectively. Given an arbitrary unseen viewpoint, a 2D uncertainty map U is constructed using a-blending as follows:\n\n\n\nwhere r is a pixel in the uncertainty map U. We employ the estimated uncertainty map for the adaptive regularization to unseen views. Specifically, we adopt a diffusion prior as well as a depth smoothness prior and the details of these two priors are discussed next."}, {"title": "Uncertainty-aware diffusion-based regularization", "content": "To render natural-looking images for novel views and times, we incorporate Stable Diffusion [44] into our pipeline. We begin by generating text prompts from the training frames using the vision-language model BLIP [28]. These prompts guide fine-tuning of the diffusion model via DreamBooth [45] with the training images, which aligns the model's understanding to the specific content in the training images as discussed in the image-to-3D reconstruction algorithm [42]. Using this fine-tuned model, we produce a refined image, IDDIM from the rendered image \u00ce for novel views or times. Specifically, we first encode \u00ce into the latent space using the latent diffusion encoder Enc, then perturb it into a noisy latent representation as follows:\n\n\n\nwhere at is a scalar value that controls the noise level and t is a diffusion time step. Similar to SDEdit [32], we generate IDDIM by performing the DDIM sampling [54] over k = \\([50\u00b7\u300d\\) steps and running the diffusion decoder Dec as follows:"}, {"title": "Uncertainty-aware depth smoothing regularization", "content": "We introduce an additional regularization term to encourage smooth depth predictions in uncertain regions. To this end, we first generate a depth map D for unseen views using an a-blending method as follows:\n\n\n\nwhere r\u2208 \\(R^2\\) is a pixel coordinate in the depth map \u00d4n, and dk denotes the depth at the center of the kth Gaussian with respect to the camera center. We employ the total variation to regularize the estimated depth map \u00d4n, which promotes smooth depth transitions between neighboring pixels. However, the uniform total variation loss over all pixels produces blur artifacts on accurately predicted regions, resulting in large reconstruction error. To address this drawback, we propose an uncertainty-aware total variation loss, LUA-TV, which applies stronger smoothing to high-uncertainty regions for noise reduction in the depth map, while leaving low-uncertainty areas unregularized to preserve details. Our uncertainty-aware total variation loss is given by\n\n\n\nwhere Dij is the estimated depth at pixel (i, j) and ur and u respectively denote the sums of the average uncertainties of vertically and horizontally adjacent pixels, in other words, \\(U_r = \\frac{\\sum_{i,j} U_{i,j} + U_{i+1,j}}{2}\\) and \\(U_c = \\frac{\\sum_{i,j} U_{i,j} + U_{i;j+1}}{2}\\)."}, {"title": "4.2 Dynamic Region Densification", "content": "Existing 4D Gaussian Splattings initialize Gaussian primitives using point clouds obtained from Structure from Motion (SfM) [47]. However, since SfM assumes static scenes, it is fundamentally unable to reconstruct dynamic regions, particularly those involving rapid motion, as shown in Figure 2c. This failure occurs because the algorithm treats dynamic regions as noise, leaving these areas without initialized primitives. Such incomplete initialization disrupts the training process, causing primitives in static regions to be repeatedly cloned and split in an attempt to fill the dynamic areas. This leads to an excessive number of primitives and, in some case, out-of-memory issues.\nTo address this limitation, we propose a dynamic region densification that initializes additional Gaussian primitives \u0393' = {\u03b3\u2081, ..., \u03b3\u03ba } in dynamic regions. To this end, we first identify dynamic pixels in the training images using scene flows [71] and randomly select a subset of the pixels. For each selected pixel r, the corresponding Gaussian primitive e is initialized with position \u03bc\u03b5 and"}, {"title": "4.3 Data-Driven Losses", "content": "Since the dynamic scene reconstruction from casually recorded monocular video is a highly ill-posed problem, we apply the additional data-driven losses based on depth and flow maps. The depth-driven loss is defined by the difference between the depth maps D and D, which are respectively obtained via the a-blending shown in Equation (13) and the algorithm proposed in [71], as shown in the following equation:\n\n\nThe flow-driven loss is analogously defined. Given two frames II and II', the flow of a pixel r is estimated using a-blending, which is expressed by\n\n\n\nwhere FI\u2192I' represents the deformation of the kth Gaussian primitive from frame II to frame I' in the projected image space, where (\u03c0,t) and (\u03c0', t') denote the projection function and timestamp associated with frames I and I', respectively. Similar to Ldepth, the data-driven loss for flow is defined by\n\n\n\nThe final data-driven loss, Ldata, is defined as the sum of the two data-driven loss terms, as shown in the following equation:"}, {"title": "4.4 Total Loss", "content": "To train our uncertainty-award 4D Gaussian splatting models, the total loss function is given by\n\n\n\nwhere Lrecon is the standard reconstruction loss based on training images, Lgrid is a loss term associated with the Hexplane-based deformation adopted in our baseline [61], and {\\(\u03bb_{grid}\\), \\(\u03bb_{data}\\), \\(\u03bb_{UA-diff}\\), \\(\u03bb_{UA-TV}\\)} are balancing hyperparameters for individual loss terms."}, {"title": "5 Experiments", "content": "This section compares the proposed method, referred to as UA-4DGS, with existing 4D Gaussian splatting algorithms including D-3DGS [68], Zhan et al. [31], and 4DGS [61]. Our method is implemented based on the official code of 4DGS [61] and tested on a single RTX A5000 GPU."}, {"title": "5.1 Settings", "content": "Our primary goal is to reconstruct dynamic scenes from casually recorded monocular videos, for which we use DyCheck [14] as our main dataset. This dataset consists of monocular videos captured with a single handheld camera, featuring scenes with fast motion to provide a challenging and realistic scenario for dynamic scene reconstruction. The DyCheck dataset includes 14 videos; however, only the half of scenes\u2014apple, block, paper-windmill, teddy, space-out, spin, and wheel-are suitable for evaluation due to the availability of held-out views.\nTo evaluate novel view rendering quality, we use three metrics: peak signal- to-noise ratio (PSNR), structural similarity (SSIM), and a perceptual metric called learned perceptual image patch similarity (LPIPS). Additionally, since our target dataset provides a co-visibility mask, we also compute masked versions of these metrics, mPSNR, mSSIM, and mLPIPS, focusing on co-visible regions. For masked evaluations, we employ the JAX implementation provided by DyCheck [14]."}, {"title": "5.2 Experimental Results", "content": "Table 1 presents the quantitative comparison of our algorithm against existing methods based on 4D Gaussian Splatting [68, 61, 31] and MLPs [39] on the DyCheck dataset [14]. Our approach, UA-4DGS, surpasses the performance of all other 4D Gaussian Splatting algorithms across all metrics. Figure 3 shows qualitative results on the space-out, paper-windmill, teddy, and spin scenes, where UA-4DGS synthesizes more realistic images, clearly outperforming existing 4D Gaussian Splatting algorithms.\nAlthough Gaussian Splatting generally outperforms MLP-based approaches on multi-view or less challenging datasets, our experiments show that they fall behind MLP-based methods in our target setting based on casually recorded videos with a monocular handheld camera as presented in Table 1. This is probably because the methods based on Gaussian Splatting focus more on local optimization with respect to individual Gaussian primitives and, consequently, are prone to overfitting to training images in in-the-wild monocular scenarios."}, {"title": "5.3 Analysis", "content": "To demonstrate the generality of our method in static scene reconstruction, we incorporate the proposed uncertainty-aware regularization to FSGS [72], a few- shot Gaussian Splatting algorithm for static scenes, and refer to this version of our model as UA-FSGS. We test both FSGS and UA-3DGS on the LLFF dataset [33] using three training images with five different runs. Table 2 presents quantitative comparisons where UA-3DGS outperforms existing methods, including both the original results and our reproduced ones of FSGS."}, {"title": "Conclusions", "content": "We proposed a novel training framework for 4D Gaussian Splatting, targeting dynamic scenes captured from casually recorded monocular cameras. Our uncertainty-aware regularizations, which incorporate diffusion and depth-smoothness priors, effectively improve novel view synthesis performance while preserving reconstruction quality on training images. Additionally, we addressed the initialization challenges of Gaussian primitives in fast-moving scenes by introducing dynamic region densification. Our method demonstrated performance gains over baseline approaches, both in dynamic scene reconstructions and few-shot static scene reconstructions. We conducted a detailed analysis through extensive experiments, and we believe this work initiates research on an important, emerging problem in 4D Gaussian Splatting, offering valuable insights to the field."}, {"title": "Limitations and future work", "content": "Novel view synthesis performance on casually recorded monocular videos still lags behind that on multi-view or simpler datasets, highlighting potential areas for improvement in future research. Currently, our regularization techniques rely on image-level regularization using 2D uncertainty maps; future work could enhance this by incorporating regularization in the Gaussian primitive level [64, 19] to directly leverage each Gaussian primitive's uncertainty. Additionally, our dynamic region densification does not consider temporal consistency for primitive initialization, but this issue may be addressed by integrating long-term tracking algorithms [9]."}, {"title": "A.2 Implementation Details", "content": "Our method is implemented based on the publicly available official code 1 of 4D Gaussian Splatting (4DGS) [61], using PyTorch [40]. Following our baseline, we utilize the Adam optimizer and set the resolution of the Hexplane grid to (64, 64, 64, 150). For grid smoothness in the Hexplane, we follow the default value of 4DGS. We use the DyCheck dataset 2 as our primary dataset, containing casually captured monocular videos. For diffusion finetuning, we manually select a single prompt from training frames that best represents the overall content of the video. We train our model for 40,000 iterations, where uncertainty-aware regularization is applied starting from iteration 20,000, as the refined images from diffusion model and uncertainty maps become more reliable at this stage.\nThe coefficients (co, C\u2081) for the sigmoid function are set as 0.25 and 20/L, respectively, where L is the number of training images. We set the balance weights for data, AUA-diff, and AUA-TV as 0.5, 0.2, and 0.01, respectively. To measure the quality of generated images, we compute mPSNR, mSSIM, and mLPIPS, leveraging visibility masks provided by the DyCheck.\nFor experiments on the LLFF dataset, our method is implemented based on the publicly available official code 3 of FSGS [72]. We set the balance weights for \u5165UA-diff, and \u5165UA-Tv as 0.1 and 0.001, respectively, for optimal performance, applying the same hyperparameters across all scenes. All experiments are conducted in the Vessl environment [1]."}]}