{"title": "Stacked Universal Successor Feature Approximators for Safety in Reinforcement Learning", "authors": ["Ian Cannon", "Thomas Gresavage", "Ian Leong", "Washington Garcia", "Joseph Saurine", "Jared Culbertson"], "abstract": "Real-world problems often involve complex objective structures that resist distillation into reinforcement learning environments with a single objective. Operation costs must be balanced with multi-dimensional task performance and end-states' effects on future availability, all while ensuring safety for other agents in the environment and the reinforcement learning agent itself. System redundancy through secondary backup controllers has proven to be an effective method to ensure safety in real-world applications where the risk of violating constraints is extremely high. In this work, we investigate the utility of a stacked, continuous-control variation of universal successor feature approximation (USFA) adapted for soft actor-critic (SAC) and coupled with a suite of secondary safety controllers, which we call stacked USFA for safety (SUSFAS). Our method improves performance on secondary objectives compared to SAC baselines using an intervening secondary controller such as a runtime assurance (RTA) controller.", "sections": [{"title": "Introduction", "content": "A recent interest in the study of reinforcement learning (RL) is the ability of learning agents to solve increasingly complex environments yet simultaneously optimize behavior across multiple salient, non-trivial objectives (Yuan et al., 2023; Liu & Ahmad, 2023). To model these types of problems, recent work has proposed the regime of sequential decision-making tasks with multiple objectives (Barreto et al., 2017), formulated as a linear combination of scalar rewards that can drive a reward signal for continuous iterative-learning algorithms, e.g., soft actor-critic (SAC) (Haarnoja et al., 2018). Unfortunately, previous work has not studied the interaction between the aforementioned multi-task formulation and the contextual safety risk of mission-critical problems. In environments that model such problems, there is a consistent threat of intervention (or promise of guidance) from a collection of on-the-loop secondary controllers, which are often unaware of the iterative-learning agent's action policy or value function (Feng et al., 2023). As a guiding example, we leverage the satellite inspection task originally investigated by van Wijk et al. (2023) where an inspecting satellite agent (called a \"deputy\") is tasked with inspecting a passive target agent (called the \"chief\"). In this problem, human operators expect high performance of an agent in completing an inspection (i.e., inspect"}, {"title": "Background", "content": "We base our formulation on a standard Markov decision process (MDP) M(S, A, p, R, \u03b3) with state space S, action space A, transition probabilities p(s' | s, a), a reward function R: S \u00d7 A \u00d7 S \u2192 R, and discount factor \u03b3\u2208 [0, 1] (Puterman, 1994). Based on this standard definition, we first describe successor features (SFs), universal value function approximation (UVFA), and their combination as it relates to our technique described later in Section 3. Afterwards, prior art and relevant techniques involving RL with secondary controllers are discussed."}, {"title": "Demystifying SFs, UVFA, and their combination", "content": "In order to describe USFA, we begin with the following observation: if the reward function decomposes into a linear combination such that_R = $w, where (st, at, St+1) \u2208 Rd, then by varying w, we obtain a family of reward functions Rw. Under this observation, the composite rewards are known as successor features (SFs) (Dayan, 1993). Subsequently, this definition yields a family of MDPs similarly parameterized by w. That is, for a given SF : S \u00d7 A \u00d7 S \u2192 Rd, a weighting w \u2208 Rd induces an MDP M(S, A, p, \u03b3) following Barreto et al. (2017) as\n\n$M_w(S, A, p,\\gamma) = \\{M(S, A, p, R_w, \\gamma) | R_w = \\phi^T w\\},\\tag{1}\n\nwhere Rw and are shorthand for Rw(st, at, St+1) and (st, at, St+1), respectively."}, {"title": "Stacked Universal Successor Feature Approximation for Safety", "content": "In this section, we describe our stacked universal successor features for safety (SUSFAS) method. To reformulate USFA to continuous control, the Q-network is replaced by a network trained to predict the successor representation instead of the scalar Q-value. The task-weight sampling step used by (Borsa et al., 2018) is left unchanged. The sampling of alternative task weightings is itself optional, but it is to be expected that generalization suffers by virtue of only learning about the target task w. Therefore we chose to keep this feature of the original USFA paper. The burden of determining the next action a' according to argmax, maxi (s, b, zi) is similar in the continuous case, with the only difference that bi ~ \u03c0(s, zi) is an action sampled from the task-conditioned policy function approximator instead of using a lookup table.\nThere are some notable differences where we diverge from the inspirational work described in Section 2. Our algorithm uses an array of d-many UVFA-inspired successor feature approximators (SFAs), each trained to predict a single successor feature, decomposed according to to Equation (1). Each SFA has a separate encoder for observations, actions, and task weights. Hence we call this the stacked USFA (SUSFA) architecture. We use GPI and the policy sampling methodology of (Borsa et al., 2018) to train the critic and select actions. Furthermore, our actor does not use separate encoders but makes predictions on a vector of observations concatenated with the task weights. This is in contrast to a collapsed USFA (CUSFA) architecture, under which we consider Borsa et al. (2018) to fall as it uses the same observation and weight encoders for every SF."}, {"title": "Experiments & Results", "content": "In our experiments, we focus on two environments selected for their requirement to solve a challenging secondary objective while also solving a relatively simple primary objective easy to learn, difficult to master. We primarily leverage the environment proposed by van Wijk et al. (2023), Inspection3D, which models a translational 3-dimensional satellite inspection problem. In this case, the agent is subject to a challenging secondary objective of reducing fuel usage while solving a primary objective of inspecting some stationary (from a relative reference frame) satellite and avoiding a crash. The"}, {"title": "Hyperparameters and Architectures", "content": "In order to investigate the structural advantages of our modified successor and network structure, we compare agents structured and trained across different combinations of architectures, agent types, SF weight ranges, and safety contexts. These experimental variables are summarized as follows:\nAgent types. Based on existing literature, we distinguish between two primary agent types. The first, so-called specialists, have been trained to specialize on a fixed set of reward weights w. In practice, we lock the reward weights for specialists to 1.0. Reward weights for specialists are not normalized to sum to 1. As a complement to specialists, we train generalists, which may have their"}, {"title": "Experimental Details", "content": "We present our findings relating to SUSFAS by first stating a motivating research question (RQ), following up with experimental details, and then concluding with the research answer (RA)."}, {"title": "Investigation of expert stacking", "content": "RQ1: Expert stacking (ES) forces agents to learn each successor feature SF independently, where gradients from one SF do not influence other SF networks. How does expert stacking compare to a more traditional structure with a collapsed network to predict SFs?\nThe work of previous groups Liu & Ahmad (2023) and Barreto et al. (2018) to bring SF representations into the continuous domain has employed the construction of an expert repertoire known as \"expert stacking\" (Barreto et al., 2017; Schaul et al., 2015). In this case, a layer or group of layers is specialized to a single task (on the input side of UVFA) or single policy (on the output side of SFs & GPI). The USFA framework avoids the need to build an expert network for each task by combining the experts into flat layers of the final network. However one could leave these \"expert layers\" separated, i.e. stacked.\nWe explore the difference between the pure \u201ccollapsed\u201d USFA architecture (with a necessary action encoder for dimensional compliance) to the fully \u201cstacked\u201d architecture. The expectation is that the"}, {"title": "Ablation of fuel usage in mission-critical environment", "content": "RQ2: RTA presents an intervening secondary controller that activates when some safety-critical condition is met. While our method allows agents to learn successor features independently per reward component, is the presence of RTA required to see improvement in our method?\nGiven the ability for stacked variants to outperform collapsed variants, we investigate the ability of our SUSFAS algorithm to encode the functionality of an RTA controller. In this scenario, there are three cases: RTA off (e.g., SAC or SUSFA), RTA on with a penalty for activating RTA (SAC-S or SUSFAS), and RTA on without a reward penalty for activating RTA (SAC-S w/o R (Reward), SUSFAS w/o R). With the RTA off scenario as our control, we compare the behavior of the algorithms to respond to RTA. By incorporating a penalty for activating RTA into the reward signal, the agent must learn to avoid risky regions of behavior space thus ensuring redundancy of safety systems."}, {"title": "Investigation of safe agent controllability", "content": "RQ3: Reward weights, w, present an interesting new dimension by which to train agents. What effect does altering the w ranges have on the performance of agents?\nDuring training, generalist agents are exposed to task weights within a predefined range in their observation space. In order to investigate the interpolation ability of our method, RQ3 studies the effect of altering the range of w during training in Inspection3D. Until now, we have shown results for weights in [0, 1] (seen in SAC-S Gen[0, 1] and SUSFAS Gen[0, 1]), which represents the full range of tasks containing w combinations from 0 to 1. To these prior experiments, we add two generalists with incrementally narrowing ranges of w during training, [0.2, 0.8] (reflected in SAC-S Gen[0.2, 0.8] and SUSFAS Gen[0.2, 0.8]) and [0.4, 0.6] (reflected in SAC-S Gen[0.4, 0.6] and SUSFAS Gen[0.4,0.6])"}, {"title": "Conclusion", "content": "Successor features (SFs) provide the inspiration behind a wide array of multi-task research. Our research refocuses SF prediction on problem spaces where safety plays a critical role. To that end, this work shows the importance of our expert stacking architecture and how it interacts favorably with an RTA controller. We show that RTA is required to see meaningful improvement in our SUSFAS method over SAC-S baselines. We also study the effect of training progressively specialized agents to demonstrate the advantages of generalized training."}, {"title": "Future Work and Limitations", "content": "We did not explore a variety of structural changes, such as using a single shared encoder for the actions, weights, and observations across all SF predictors, nor did we explore the effect of using split encoders for the actor. A natural complement to successor features is predecessor features (van Hasselt et al., 2021), and it would be interesting to combine the two representations.\nIt would be interesting to investigate disentangling a goal from its representation in weight space, as was done in Ma et al. (2020) and similar to Mozifian et al. (2021). Additionally, the nature of the underlying task space is not well understood. Is it possible that the potential collinearity of the dimensions of w affects learning? Perhaps some form of regularization or enforcing of orthonormality in the weight feature space could positively impact the behavior and generalization of the agent."}, {"title": "Broader Impact Statement", "content": "While our work recognizes the need for safe operation of control systems when using reinforcement learning for real-world problems, much more understanding is needed and care must be taken to ensure appropriate deployment of these methods as part of any safety-critical system. We hope that our work can help practitioners make more informed decisions on safe operations for RL deployment."}, {"title": "Appendix", "content": null}, {"title": "Environments Details", "content": "In Inspection3D, each observation consists of position 6, velocity 6, sun angle \u03b8s and inspected points data so that s = {\u03c6, \u03c6, \u03b8\u03c2, Pi, Pc}, where Pi is the cumulative number of inspected points and Pe is a unit vector pointing toward the largest cluster of remaining uninspected points. The actions available in the environment are simply forces applied in each of the three principal axes. Each episode terminates after either a fixed number of maximum timesteps or when a defined minimum threshold \u03c4\u2208 [0,100] of points inspected is reached. Our reward function is slightly modified from van Wijk et al. (2023) and consists of five components linearly weighted: R = woRo + wcRc + wTRT + wvRv + WARA where each is component consists of the following.\n1. Observed points Ro(st, at, St+1) = 0.01 ([Pi(t+1) \u2013 Pi(t)] + X[7,100] (Pi)), based on the number of new points observed by the agent at timestep t along with a termination re-ward if the minimum number of points required have been inspected.\n2. Crash reward (penalty) Rc = -1, if agent exits inspection area or crashes into target, otherwise Rc = 0.\n3. Timestep (penalty) RT(st, at, St+1) = -0.0001, a constant penalty encouraging the agent to solve the task quickly.\n4. Fuel usage (penalty) R\u2206 = - \u03a3\u03c4 \u0394\u1fe6\u03c4, a cumulative penalty for the total amount of fuel used during the episode.\n5. RTA (penalty) Ra = -0.01na, where na is number of timesteps where the RTA was active.\nThe agent models a \"deputy\" spacecraft, which is orbiting and attempting to inspect a passive \"chief\" satellite modeled as a spherical point cloud containing 100 points. Translational motion is governed by the Clohessy-Wiltshire linearized dynamics in Hill's frame (i.e., centered on the chief satellite), and the rotational position (attitude) of the spacecraft is adjusted by the simulator so the orientation of the observation sensors are always directed towards the chief. Points are considered inspected when they fall within a defined perception cone and are currently illuminated by the Sun. The environment includes a suite of RTA filters that intercept and minimally modify potentially unsafe actions (e.g., actions that would cause a crash or violate dynamic speed constraints) based on Active Set Invariance Filtering (ASIF) (Gurriet et al., 2018)."}, {"title": "Hyperparameters", "content": "Table 5 lists hyperparameters used in the SAC, CUSFAS, and SUSFA algorithms. Notably, CUSFAS and SUSFA algorithms use hyperparameters most commonly used in the SAC algorithm, the only difference between the two being the number of stacked Successor Feature Approximator (SFA) networks used for the critic."}]}