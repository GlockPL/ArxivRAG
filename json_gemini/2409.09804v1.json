{"title": "Abnormal Event Detection In Videos Using Deep Embedding", "authors": ["Darshan Venkatrayappa"], "abstract": "Abnormal event detection or anomaly detection in surveillance videos is currently a challenge because of the diversity of possible events. Due to the lack of anomalous events at training time, anomaly detection requires the design of learning methods without supervision. In this work we propose an unsupervised approach for video anomaly detection with the aim to jointly optimize the objectives of the deep neural network and the anomaly detection task using a hybrid architecture. Initially, a convolutional autoencoder is pre-trained in an unsupervised manner with a fusion of depth, motion and appearance features. In the second step, we utilize the encoder part of the pre-trained autoencoder and extract the embeddings of the fused input. Now, we jointly train/ fine tune the encoder to map the embeddings to a hypercenter. Thus, embeddings of normal data fall near the hypercenter, whereas embeddings of anomalous data fall far away from the hypercenter.", "sections": [{"title": "1 INTRODUCTION", "content": "To ensure the safety and security of public spaces,\nthere is a need for swift and precise detection of ab-\nnormal events, including incidents such as alterca-\ntions or urgent situations like fires. Achieving this\ngoal involves strategically placing surveillance cam-\neras in various locations such as airports, malls, and\npublic streets, resulting in a significant increase in the\nvolume of video data. However, manually detecting\nthese events, or anomalies, is an extremely meticu-\nlous task that often demands more manpower than is\nreadily available. This challenge is exacerbated by\nthe low probability of these abnormal events occur-\nring, making manual detection a laborious and time-\nconsuming endeavor. Consequently, there is an urgent\nrequirement for automated systems capable of identi-\nfying rare or unusual incidents and activities within\nsurveillance videos.\nDefining anomalies can pose a significant chal-\nlenge, primarily because their determination relies\nheavily on the specific context in which they oc-\ncur. For example, a person crossing the road might\nbe considered anomalous if it happens outside of a\ndesignated crosswalk. Furthermore, the precise def-\ninition of what constitutes an anomaly can often be\nvague and subject to interpretation. Different individ-\nuals may have varying opinions on whether activities\nlike walking around on a subway platform should be\ncategorized as normal or flagged as anomalous, po-\ntentially due to suspicions or different perspectives (Chong and Tay, 2017). These complexities pose\nsignificant obstacles when attempting to construct a\nsupervised learning model for distinguishing anoma-\nlies from regular occurrences, mainly because abnor-\nmal events represent only a small fraction of the total\ndataset.\nTo address this challenge, a part of the computer\nvision community approaches the anomaly detection\nproblem as an outlier detection task. They construct a\nnormality model based on training data representing\nnormal events and label deviations from this model as\nanomalous. In our work, we adopt a similar approach\nby mapping the embeddings of our hybrid architec-\nture to a hypercenter. Consequently, embeddings of\nnormal data cluster closely around the hypercenter,\nwhile those of anomalous data are positioned far away\nfrom it. This enables us to effectively identify anoma-\nlies within the data."}, {"title": "2 RELATED WORK", "content": "In the realm of computer vision, various approaches\nhave been proposed to tackle the issue of abnor-\nmal event detection. The choice of a specific ap-\nproach often hinges on several factors, including the\nnature of the input data (whether it's sequential or\nnon-sequential) (Chalapathy and Chawla, 2019), the\ntype of labels available (supervised, unsupervised, or\nsemi-supervised), and the desired output format (bi-\nnary values or normality scores). Broadly speaking,\nwe can categorize these approaches into two primary\ngroups: those designed for sequential data, such as\nvideos, and those tailored for non-sequential data, like\nimages. When dealing with sequential data, such as\nvideos, the focus typically shifts towards techniques\nrooted in CNNs (Convolutional Neural Networks),\nRNNS (Recurrent Neural Networks), or LSTM net-\nworks (Long Short-Term Memory Networks). On the\nother hand, approaches geared toward non-sequential\ndata, like images, tend to favor the utilization of\nCNNs or AEs (Auto-encoders).\nAnomaly detection can be broadly categorized\nalong two axes: supervised approaches and unsuper-\nvised or semi-supervised approaches. In supervised\nmethods, a dataset is meticulously labeled to distin-\nguish normal from abnormal instances, effectively\ntransforming the problem into a conventional classi-\nfication task. While supervised approaches (G\u00f6rnitz\net al., 2013) are generally more effective than unsu-\npervised ones, they demand a substantial number of\nannotations, which, in our specific context, are sel-\ndom obtainable for abnormal events. Conversely, un-\nsupervised approaches do not depend on pre-existing\nlabels and instead rely on the inherent characteris-\ntics of the dataset to pinpoint anomalous instances.\nThese techniques typically operate on the premise that\nabnormal data points are sparsely distributed within\nthe dataset, prompting the search for examples that\nexhibit substantial deviations from the overall data\ndistribution. Unsupervised methods frequently em-\nploy dimensionality reduction techniques like PCA,\nauto-encoders (Zhou and Paffenroth, 2017), or gener-\native models to achieve this goal. The output of the\nanomaly detector can be either a binary value or a\n\"normality\" score. This can be done either globally\non the signal studied, or locally to indicate the po-\nsition of the anomaly. Generally binary approaches\nare based on thresholding a score and we consider\nhere only approaches returning a score. Most meth-\nods calculate distances between the data to be tested\nand a central point of a \"normality\" sphere (Ruff et al.,\n2018); any point sufficiently far from this center is\nconsidered abnormal.\nAccording to (Kiran et al., 2018) abnormal event\ndetection in videos can be categorized into three main\napproaches. Firstly, there are reconstruction-based\nmethods that focus on reducing data dimensionality,\noften through techniques like PCA (Kudo et al., 2013;\nWang et al., 2019) or auto-encoders (Chalapathy and\nChawla, 2019; Sabokrou et al., 2016; Hasan et al.,\n2016; Akhriev and Marecek, 2019). These meth-\nods assume that anomalies are in-compressible and\nthus cannot be effectively reconstructed from low-\ndimensional projections. These methods demonstrate\npromising results when the anomaly ratio is fairly low.\nAlthough the reconstruction of anomalous samples,\nbased on a reconstruction scheme optimized for nor-\nmal data, tends to generate a higher error, a significant\namount of anomalous samples could mislead the au-\ntoencoders to learn the correlations in the anomalous\ndata instead (Li et al., 2021).\nSecondly, prediction-based methods take a differ-\nent approach by employing auto-regressive models or\ngenerative models to predict successive video frames\nbased on prior frames. Anomalies are identified when\nthese predictions deviate significantly from the actual\nframes. (Zhao et al., 2017) thus uses a 3D autoen-\ncoder for anomaly detection. Its decoder is composed\nof two parts, one allowing the reconstruction of the\ninput sequence and the other predicting the follow-\ning sequence. Liu et al. (Liu et al., 2018) train a\nframe prediction network by incorporating different\ntechniques including gradient loss, optical flow, and\nadversarial training. Authors of (Hu et al., 2016)\nuses Slow Feature Analysis (SFA) to detect anoma-\nlies. However, it's noteworthy that many prediction-\nbased techniques may not fully exploit the temporal\ncontext and high-level semantic information of video\nanomalies (Zhang et al., 2023). Furthermore, these\nsequential predictions can be computationally inten-\nsive, and the learned representations may not be opti-\nmized for anomaly detection, as their primary objec-\ntive revolves around sequential prediction rather than\nanomaly identification.\nFinally, generative-based methods utilize mod-\nels like VAES and GANs to understand the distribu-\ntion of \"normal\" examples, aiding in anomaly detec-\ntion. Authors of (Fan et al., 2020) have used VAEs\nfor video anomaly detection. Although these varia-\ntional approaches are able to generate various plausi-\nble outcomes, the predictions are blurrier and of lower\nquality compared to state-of-theart GAN-based mod-\nels. Authors of (Ravanbakhsh et al., 2017) proposed\nto learn the generator as a reconstructor of normal\nevents, and hence if it cannot properly reconstruct a\nchunk of the input frames, that chunk is considered\nan anomaly. However, adversarial training is unsta-\nble. Without an explicit latent variable interpretation,\nGANs are prone to mode collapse as generator fails\nto cover the space of possible predictions by getting\nstuck into a single mode.\nThe drawback of these above methods is that they\ndo not detect anomalies directly. They instead lever-\nage proxy tasks for anomaly detection, e.g., recon-\nstructing input frames or predicting future frames, to\nextract general feature representations rather than nor-\nmal patterns. To address these issues, we exploit the\none-class classification objective to map normal data"}, {"title": "3 METHODOLOGY", "content": "Overview of our method is shown in Fig. 1. The pro-\nposed hybrid architecture is split in to 3 parts.\n\u2022 Latent Feature Extraction.\n\u2022 Feature Fusion.\n\u2022 One class classification"}, {"title": "3.1 Latent Feature Extraction", "content": "In our work we make use of 3 different input modali-\nties :\n\u2022 Depth maps: HR-Depth (Lyu et al., 2020).\n\u2022 Optical flow: RAFT (Teed and Deng, 2020).\n\u2022 Appearance features: CAE(Convolution Auto en-\ncoder).\nThe architectures of these modalities are based on\nthe encoder/decoder principle. We will use the out-\nputs of the encoders to have a latent representation\nof each modality which will then be fused to detect\nanomalies. We use pre-trained models on modality-\nspecific bases. More information about these modal-\nities can be found in (Lyu et al., 2020; Teed and\nDeng, 2020). Instead of Appearance features from the\nCAE, Latent features of Semantic maps from Mask-\nRCNN can also be used. In order to ensure that\nthe modality extractor networks continue to fulfil this\nrole, we decided to freeze their weights for learning.\nAs the anomaly bases do not have any labelling on the\nmodalities used, updating the modalities in an end-to-\nend network is made more difficult. A future exten-\nsion of the work would be to use unsupervised cost\nfunctions to perform this task."}, {"title": "3.2 Feature Fusion", "content": "The optimal fusion of different modalities is a criti-\ncal consideration, with the literature on multi-modal\nfusion offering numerous approaches but no consen-\nsus on the best fusion level. Instead, the ideal fu-\nsion position seems to be task-dependent, making it\na parameter to be optimized like any other. Following\nthe methodology proposed by the authors of (Vielzeuf\net al., 2018), we implement feature fusion. As illus-\nstrated in Figure 2, the fusion block integrates features\nfrom various modalities through multiple branches.\nEach modality includes a standard neural network\nhandling the modality. In addition, there is a central\nnetwork that merges the modalities. At each layer of\nthis central branch we combine via a weighted sum\nthe values of the previous layer and the values of the\nlayers of the same depth of the networks handling the\nmodality. These fusion weights are learned like the\nother parameters of the architecture. Thus the input\nto the fusion layer corresponds to the following equa-\ntion:\n$h_{ci+1} = a_{c}h_{c_{i}} + \\sum_{k=1}^{m} a_{Mk}h_{M_{k}}$\nWhere m is the number of modalities, a a learnable\nscalar, $h_i$ the hidden representation of each modality\nof depth i and $h_{ci}$ the central hidden representation.\nFor each branch of the fusion block, we propose to\nuse the convolutional neural network.\nThe encoder outputs of each modality are of dif-\nferent sizes so in order to process them in fusion block\nwe need to align their dimensions. To do this, we use\ninterpolation to obtain maps of the same size."}, {"title": "3.3 One Class Classification", "content": "The final stage of our approach is to use a one-class\nlearning algorithm to detect anomalies. The goal of\none-class learning is to build a classifier that identifies\nobjects of the same nature as those presented during\ntraining. For this type of approach, the training set is\nonly composed of the objects of interest/ normal data.\nUnlike traditional classification approaches, there is\nno attempt to distinguish between two or more classes\nwith objects in each class. The focus here is on the\nclass of interest and the aim is to delimit its bound-\naries. Historically, this type of classification has been\nstudied on classifiers such as SVMs by seeking to\nidentify the smallest hypersphere encompassing the\ntraining data. The authors of (Xu et al., 2015) use\nthis approach for anomaly detection by using auto-\nencoder. More recently, one-class learning has been\nextended to deep neural network (Ruff et al., 2018).\nIn this article, we propose to use the One-Class Deep\nSVDD approach of (Ruff et al., 2018).\nThe final stage is split in to two parts 1) Pre-\ntraining and 2) Fine tuning. In the pretraining stage\nwe train a convolution autoencoder with the fused fea-\nture maps as the input. In the finetuning stage we use\nthe encoder part of the pretrained CAE to extract the\nfeatures and map the features to the center of the hy-\npersphere such that the distance between the center of\nthe hypersphere and the features are minimized. We\nuse the following regularised loss function:\n$loss_m(W, c) = \\frac{1}{n}\\sum_{i=1}^{n}||f(x_i, W) - c||^2 + \\frac{\\lambda}{2} \\sum_{l=1}^{L} ||W_l||^2$ ,\nwhere W are the weights of the finetuned archi-\ntecture, $x_i$ the n examples of the mini-batch. o is the\ninference of the network on the example $x_i$, for the\nweights W. $\\lambda > 0$ is the weight of the regularisation\nand c the hypercentre of the reference distribution.\nWe thus seek the same hypercentre c surrounding the\nreference data set. In the case of learning the param-\neters of the fusion block, we use a cost function per\nnetwork branch. Consequently, the learning process\nconsists in solving the following problem:\n$\\underset{W,c_f,c_1,...,c_m}{arg \\; min} \\; loss_f(W, c_f) + \\sum_{k=1}^{m}loss_{M_k}(W, c_k)$,\nwhere $loss_f$ is the loss function of the fusion\nbranch and $loss_{M_k}$ the functions associated with each\nmodality. During the inferrence, we will use only the\ncentral branch of the fusion. The output is therefore\n$||f(x_i, W) - c_f ||^2$, where $o_f$ is the output only of the\ncentral branch."}, {"title": "4 EXPERIMENTS", "content": "We validate our approach over several benchmark\ndatasets portraying complex anomalous events in var-\nious scenarios involving multiple scenes captured\nfrom different angles. All datasets comprise 'nor-\nmal' video frames for training and a combination\nof anomalous and non-anomalous frames for testing.\nTheir features are summarised in Table 1.\nThe CHUK Avenue dataset contains 16 normal\nvideos for training and 21 videos for testing, for a total\nof 30,652 frames. Test videos include anomalies like\nthe throwing of objects, walking in the wrong direc-\ntion, running, and loitering. The UCSD anomaly de-\ntection dataset contains surveillance videos of pedes-"}, {"title": "4.2 Results & Discussions", "content": "We evaluate our method on two benchmark datasets.\n1) The UCSD Ped2 dataset (Li et al., 2014) which\ncontains 16 training and 12 test videos with 12 irreg-\nular events, including riding a bike and driving a ve-\nhicle. 2) The CUHK Avenue dataset (Lu et al., 2013)\nconsists of 16 training and 21 test videos with 47 ab-\nnormal events such as running and throwing stuff.\nBy construction the CAE used for pretraining is\nsymmetric. Both encoder and decoder is made of\n4 convolution layers with bias set to zero followed\nby Relu as the activation function. Each convolu-\ntion layer is followed by a BatchNorm2d layer. The\nweights are initialized using Kaiming initialization.\nThe filter are of size 3 x 3 with padding and stride set\nto 1. We pretrain the auto-encoder for 100 epochs and\nfine tune anomaly detector(encoder) for 75 epochs.\nWe use Adam optimizer to optimize the parameters\nof both the CAE and the encoder. The learning rate\nand weight decay hyper-parameter are set to 1 \u00d7 10\u20132\nand 0.1 respectively. Initially, we conducted exper-\niments without pretraining, which led to poor out-\ncomes. However, incorporating both pretraining and\nfine-tuning significantly improved the results.\nThe quantitative results of our approach are tab-\nulated in Table.2. We evaluate the algorithm using\nthe Area under the curve metric. We use the sklearn\nmetric ROC AUC score function to evaluate our al-\ngorithm. We find the AUC for individual videos and\nthe average AUC for all the videos. We compare our\nmethod with other unsupervised methods such as MP-\nPCA (Kim and Grauman, 2009), MPPC+SFA (Ma-\nhadevan et al., 2010), ConvLSTM-AE and (Luo et al.,\n2017). From Table.2 it can be seen that the perfor-\nmance of our method is almost similar to or better\nthan most of the methods.\nThe qualitative results of our approach are shown\nin Figure.3 and Figure.4 respectively. In both the fig-\nures, the blue curve represents the ground truth data.\nThe anomalous frame in ground truth is indicated by\nthe value of 1 whereas the normal frames are indi-\ncated by 0 value. In Figure.3 it can be seen that the\ngreen curve starts ascending as soon as the van enters\nthe cameras field of view and keeps fluctuating till the\nend of the sequence but never drops back to the zero\nvalue. Similarly, In Figure.4 we can see that the green\ncurve remains zero till a person starts behaving ran-\ndomly by repeatedly throwing his bag up in the air."}, {"title": "5 CONCLUSIONS & FUTURE WORK", "content": "In our work we have proposed an unsupervised ap-\nproach to detect anomalies in videos using a fusion\nof motion, depth and appearance features. Experi-\nmental evaluations on standard benchmarks demon-\nstrate the our model performs similar to other unsu-\npervised methods. We believe that the performance\nof our method can be further improved by incorporat-\ning other modalities like pose maps and audio. In our\ncurrent work we just train the parameters for the fu-\nsion block. In the future we would like to simultane-\nously train and update the parameters of the different\nmodalities along with the fusion block."}]}