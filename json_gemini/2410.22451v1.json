{"title": "Addressing Issues with Working Memory\nin Video Object Segmentation", "authors": ["Clayton Bromley", "Alexander Moore", "Amar Saini", "Douglas Poland", "Carmen Carrano"], "abstract": "Contemporary state-of-the-art video object segmentation (VOS) models compare incoming unan-\nnotated images to a history of image-mask relations via affinity or cross-attention to predict object\nmasks. We refer to the internal memory state of the initial image-mask pair and past image-masks\nas a working memory buffer. While the current state of the art models perform very well on clean\nvideo data, their reliance on a working memory of previous frames leaves room for error. Affinity-\nbased algorithms include the inductive bias that there is temporal continuity between consecutive\nframes. To account for inconsistent camera views of the desired object, working memory models\nneed an algorithmic modification that regulates the memory updates and avoid writing irrelevant\nframes into working memory. A simple algorithmic change is proposed that can be applied to any\nexisting working memory-based VOS model to improve performance on inconsistent views, such\nas sudden camera cuts, frame interjections, and extreme context changes. The resulting model\nperformances show significant improvement on video data with these frame interjections over the\nsame model without the algorithmic addition. Our contribution is a simple decision function that\ndetermines whether working memory should be updated based on the detection of sudden, extreme\nchanges and the assumption that the object is no longer in frame. By implementing algorithmic\nchanges, such as this, we can increase the real-world applicability of current VOS models.", "sections": [{"title": "1 Introduction", "content": "VOS is an ongoing challenge in the world of video processing and understanding. As models continue\nto improve performance on clean data, steps must be taken to increase robustness towards challenges\nfound in real-world video data that clean benchmarks fail to capture. One such augmentation that\ncan be expected in both streaming and recorded video data is a camera cut: any instance in which\nthe context changes so significantly from one frame to the next that the segmented object is reliably\nabsent. This can occur by way of sudden camera movement, such that the new frame is entirely differ-\nent from the previous, or the splicing of an entirely different video. VOS datasets and benchmarks are\nconsistently high-quality and continuous video streams, while in the real world, cuts and scene changes\nintroduce an additional layer of complexity to which the inductive biases for current VOS models fail\nto generalize.\nCurrent and previous state-of-the-art VOS models have significant performance drops when these\ncamera cuts are present in the data. When irrelevant frames are present in video data, the following\nprocess occurs:\n1. The desired object is segmented prior to a camera cut.\n2. A camera cut or discontinuity occurs, and the model suddenly loses track of the object. Object\nattention disappears and current frame-to-frame smooth motion is interrupted.\n3. The model may latch onto a different object in the post-camera cut frame, resulting in false\npositive mask predictions. This is because models undergo training on VOS standard data and\nbenchmarks, where objects are always continuous through their spacetime positions.\n4. The subsequent irrelevant frames are written into the model's object memory.\n5. The object memory deteriorates over time as new irrelevant memories populate the buffer.\n6. If the object reappears, the re-identification is ineffective because of the memory deterioration\nduring the irrelevant frames.\nWe propose a simple algorithmic addition to improve performance on data with camera cuts. By\nimplementing a binary classifier via L2 distance on image embeddings from frame to frame, it is\npossible to determine when camera cuts occur. Then, false positive predictions can be prevented, and\nirrelevant frames can avoid being written to memory. This switch maintains object attention despite\nlong camera cuts and substantially improves re-identification during long intermission periods from\nrelevant objects."}, {"title": "Contributions", "content": "In this paper we improve former and current state-of-the-art VOS models on multiple benchmarks\ninvolving temporal inconsistencies using a model-agnostic approach. This includes the following steps:\n1. We demonstrate that frame interjections are detectable without fine-tuning image encoders for\nprevious three state-of-the-art VOS models.\n2. We use the image embedding stream and sequence knowledge to compare frame-to-frame em-\nbeddings by proposing a new distance function on the image embedding space utilizing sequence\nfor standard error.\n3. We engineer features which lead to highly-discriminatable interjection periods.\n4. We propose a simple classifier to determine interjections and improve regulation of the working\nmemory buffer on the three previous state-of-the-art VOS models."}, {"title": "2 Related Works", "content": "VOS models generally employ one of two different methods for mask prediction. Until recently, state-\nof-the-art models were affinity-based, including XMem[2] which first applied the concept of an external\nworking memory buffer to the VOS task. Segment-Anything-Model 2[6] (SAM 2) introduced an ar-\nchitecture which depends only on memory embeddings rather than previous frames, and provides\nimprovement over other architectures. It is still, however, limited by the working memory assumption\nthat frames are continuous and relevant."}, {"title": "2.1 Affinity-Based VOS", "content": "Some VOS models rely on the affinity, or soft-max similarity, between a frame's mask/image embed-\ndings, and the keys stored in memory from previous frames. For example, XMem[2], a VOS model\nreleased in 2022, implements affinity alongside a three-tiered memory system. Upon receiving an initial\nmask and frame, a key encoder and a value encoder are used to create an embedding representation of\nthe frame and the image-mask relationship respectively. These embeddings are written to the working\nmemory layer as a function of the affinity to previous frame embeddings. However, with each progres-\nsive memory layer, the memory is encoded after a constant amount of time (default 5 frames). As\nsuch, the model is unable to account for irrelevant frames. The affinity matrix between the memory\nand the mask/frame embeddings is then used to generate the new predicted segmentation mask.\nCutie[1] is an improved affinity-based VOS model that was built from XMem and utilizes the same\npixel memory structure. First, Cutie encodes segmented frames into a high-resolution pixel memory\nand a high-level object memory to be stored for future use. The same working memory buffer structure\nis used in both XMem and Cutie, and thus they are both susceptible to the same data imperfections.\nAn initial pixel readout is retrieved from the pixel memory using encoded query features and enriched\nwith object-level semantics by augmenting with information from the object memory. The enriched\noutput is finally passed to the decoder for generating the final output mask.\nWhile these affinity-based models consider the frame-to-frame similarity in mask prediction, there are\nno checkpoints in place to directly account for sudden changes, such as camera cuts. We propose a\nfix that applies the affinity between the embeddings of the current frame and the previous frame to\ncreate a binary \"classifier to identify camera cuts and avoid writing irrelevant frame embeddings into\nmemory."}, {"title": "2.2 Affinity-Free VOS", "content": "Unlike its predecessors, Segment-Anything-Model 2[6] (SAM 2) is not dependant on affinity, and it\nfundamentally changes the way VOS is explored as a problem. Rather than using frame-to-frame\naffinity for mask prediction, SAM 2 utilizes attention between the frame embeddings and the memory,\nto which we refer as \"affinity-free modeling\". This model can be seen as a generalization of the original\nsegment-anything-model[4], the state-of-the-art image segmentation model, such that images can be\nexplored as one frame videos. Masks, bounding boxes, and points can all be used as inputs at any\npoint through a video, and the prompt encodings are using to decode the mask prediction. Finally,\nthe predicted mask is encoded into memory for the subsequent mask prediction.\nUnlike XMem and Cutie, SAM 2 adds an additional head to the mask decoder to predict whether the\nobject is visible in the frame via a multilevel perceptron (MLP). The occlusion head determines the\nprobability that the object is not present in the frame and writes the memory accordingly. This new\naddition does an excellent job at accounting for objects that are slowly and temporarily obscured. As\nshown in Table 1, however, sudden and extreme cuts or context shifts still can cause SAM 2 to lose\ntrack of the segmented object."}, {"title": "2.3 Datasets", "content": "Previous datasets have been created to explore the segmentation of partially or fully obscured objects.\nDAVIS[5] is a dataset of multi-object videos which have been segmented for ground truth masks. It\nis a highly popular benchmark on which models can compare performance. While this data contains\nlimited object obscurations, it provides a good baseline for VOS. The DAVIS dataset will be used to\ngenerate the data used in this paper.\nMOSE[3] is a popular video dataset similar to DAVIS that emphasizes object obscuration. In many\nsamples, obscured objects will become partially or fully obscured for a section of the video to test\nVOS model robustness. State-of-the-art models, particularly Cutie and SAM 2, have emphasized\nthis dataset when developing their architectures and have great performance on these obscurations.\nHowever, MOSE fails to provide fully-annotated long-term scalable obscurations and sudden camera\ncutting. For this reason, we use DAVIS to create a dataset of videos with the necessary annotated\nlong-term interjections."}, {"title": "3 Experimental Design", "content": "An interjection dataset is created from DAVIS to benchmark the various models explored in this paper.\nAn object is selected from a video of interest, and a select number of frames from a separate, unrelated\nvideo are interjected in the middle of the video of interest, as shown in Figure 2. An ideal VOS model\nis expected to do the following during the three stages of the interjection video:\n1. Prefix: Take the initial mask and write the object into memory\n2. Interjection: Identify the absence of the object and make no false positive predictions\n3. Suffix: Re-identify the object and continue segmenting at the same accuracy as during the prefix.\nFive datasets are collected and benchmarked over all explored VOS models. First, the clean DAVIS\nvideo dataset is benchmarked to ensure that any modifications designed to improve performance on\ncamera cuts does not diminish performance on reliable, clean video. Then, four separate datasets\nare created using the structure in Figure 2. In all cases, the prefix and suffix are both 12 frames to\nprevent biased results from having additional time to embed the object of interest into memory. The\ninterjection lengths are 4, 16, 128, and 512 frames respectively, allowing the models to be benchmarked\non both short-term and long-term obscurations."}, {"title": "4 Method", "content": "Working-memory-buffer utilizing models have no decision function on updating the working memory.\nRather, memory is updated every 5 steps by default. Our algorithmic modification introduces a learn-\nable step in which the model uses image features to determine whether the memory should be updated\nto move away from strong assumptions made for VOS benchmarks on clean data and toward more\nchallenging real-world data. The general method employed to account for this data is shown in Figure\n1. The result is a simple binary classifier that identifies whether or not each consecutive frame is part\nof the same video or if a context change has occurred. Each frame is encoded using the encoder built\ninto the model, which is trained around the segmentation task, and the resulting image embeddings\nare compared to the image embeddings from previous frames. If the features are highly dissimilar\nin segmentation-embedding space, it means the shape of the objects present in the scene are highly\ndifferent, and indicator of a context shift\nThe L2 distance between the current embeddings and the element-wise z-score of the previous frame\nembeddings is used as the similarity function as shown in Equation 1 where $f_i$ represents the encoded\npixel values of the current frame in stream, $f_m$ represents the element-wise mean of the equivalent\nencodings for the frames in memory, and $\u03c3_m$ represents the element-wise variance of the memory\nencodings from the previous frames.\nRegularized Distance = L2 $\\frac{f_i- f_m}{\u03c3_m}$   (1)\nThe memory frame embeddings come from a stored list of image embeddings that are known to be\nfrom the original video. While we know the true placement of interjection frames during training, this\nplacement is unknown during validation. For a context window of size w, the previous w frame image\nembeddings are used to find the element-wise mean ($f_m$) and element-wise variance ($\u03c3_m$) such that $f_i$,\n$f_m$, and $\u03c3_m$ all maintain the same dimensionality. A context window is necessary to avoid overfitting\nto frames with small variances over long video lengths. For example, without a context window, CCTV\nvideo footage located in an inactive area might overfit to no variance between frames, then classify an\ninterjection whenever anything changes due to the large resulting Z-score.\nThe resulting L2 distance values can be compared to a threshold, frame by frame, to complete the\nclassification without the need for any trained parameters. If the frame is below the threshold, then\nit can be classified as a part of the same video, and the model's mask predictor can operate as usual.\nThe image embeddings can be written to working memory and stored for future frame embeddings\ncomparisons. If the frame is above the threshold, it is assumed to be an interjection, the current\nposition is not added to memory buffer, and the mask predicts no false positives. The challenge is\nin finding a threshold function that identifies interjection frames 100% of the time without any false\npositive classifications. Several functions are explored to accomplish this."}, {"title": "4.1 Zeroth-Order L2 Comparison", "content": "The simplest threshold possible is a set value that acts as the classification decision boundary. Any\nL2 values above the threshold value is an interjection while any below is from the same video. This\nfunction, while easy to implement, is unreliable for all context window lengths. In cases where the\ninterjected video is conceptually similar to the original video, the L2 value might be small despite\nbeing an interjection. For example, if a video of a animal is interjected with a video of a different\nanimal, the embedding difference may be below the threshold.\nSimilarly, if a video contains quick movements or has a low frame rate, the embedding differences\nmight be too large, resulting in a false positive classification. Because the lowest possible interjection\nL2 value is below the highest possible non-interjection L2 value, zeroth-order comparisons cannot be\nused for accurate classifications."}, {"title": "4.2 First-Order L2 Comparison", "content": "The first-order threshold function takes into account comparisons between the current L2 distance and\nthe previous window L2 distance. Both the difference between the values and the ratio between the\nvalues are explored alongside a zeroth-order comparison to create a piecewise threshold function with\nsignificantly higher accuracy.\nWhile nearly every sample can be correctly classified using these techniques, a few samples in particular\ncause difficulties. Two of these samples are shown in the second plot of Figure 3. The red sample in\nthe second plot has a relatively small jump from frame 11 to frame 12. In this particular sample, the\ninterjection frames are contextually similar to the prefix frames so functions that rely on L2 difference\nand ratio may cause the model to not recognize the interjection. The orange sample has the opposite\nissue. The jump between the prefix, interjection, and suffix are relatively large and therefore easy to\nidentify. Between frames 19 and 20, however, there is another large jump caused by quick camera\nmovements. First-order threshold functions tend to incorrectly identify this jump as an interjection.\nIt is impossible to create a perfect classifier using this technique without including many piecewise\ncomponents, however this results in overfitting and cannot be generalized.\nAnother issue with this technique is the number of frames required to make a decision. Each L2\ndistance requires w+1 frames to compute. Thus, comparing L2 distance values between windows\nrequires a minimum of w+2 frames. If an interjection occurs within the first w+2 frames of a video,\nit will be impossible for this technique to identify it. Higher-order classifiers will continue to magnify\nthis problem so a different approach is necessary."}, {"title": "4.3 Maximum Distance Ratio", "content": "The model can rely on the fact that frames in the same video are likely to be far more similar to each\nother than to an interjected frame. Regardless of the actual L2 distance values between the frames,\nit is a safe assumption that the distance to the interjected frame is larger than any of the distances\nbetween any of the frames within the same video. This can be used to avoid false positive interjection\nclassifications. In cases where the camera quickly pans, the distance to the subsequent frame will be\nlarge despite being from the same video. Yet, the distance will still be smaller than an interjection in\nthe same sample.\nThis method maintains a variable for the maximum window distance seen up to that point. For each\nnew window explored, the maximum distance ratio (MDR) is the window L2 distance divided by the\nmaximum distance seen. Thus, if a frame is furthest from the previous frames, the MDR will be greater\nthan 1 and vice versa. Large MDR values are indicative of interjection frames. Once an interjection\nis identified, subsequent interjection frames will compute MDR values in comparison to the initial\ninterjection. As a result, the remaining MDR values will be around 1. For this reason, MDR cannot\nbe the only indicator of interjection, but when used in conjuction with the other methods, accuracy\ncan be maximized."}, {"title": "5 Implementation", "content": "This injection classification technique was applied to Cutie, XMem, and SAM 2 VOS models. Both\nlong-term distance (w=5) and short-term distance (w=1) are explored. To prevent the loss of frames\nbefore a full long-term window, intermediate windows of size [1, 2, w-1] are used on the first w-1\nframes instead."}, {"title": "5.1 Cutie", "content": "One of the Cutie interjection datasets is represented in Figure 4. The tree used to make the interjection\nclassification is shown in Figure 5. The long-term first-order ratio of the current window divided by"}, {"title": "5.2 XMem", "content": "Because XMem is an older model than Cutie, the frame encoder is of worse quality. Thus, additional\nsteps are required to achieve high accuracy in interjection classification. Mainly, the zeroth and first\norder threshold function are explored for both the short term frame window (w=1) and the long term\nframe window (w=5). The same MDRT function is used, and as with the Cutie decision tree, MDR is\ncomputed using the long-term window."}, {"title": "5.3 SAM 2", "content": "While the SAM 2 algorithm is affinity free, it still relies on working memory and is thus susceptible to\nbeing warped by irrelevant frames. Because of the more complex encoding algorithm used by SAM 2,\nthe same thresholding methods cannot be used by themselves without overfitting to the training data.\nAlternatively, a variety of functions were used to effectively separate the available interjection and\nnon-interjection frames. Equation 3 shows the best thresholding function identified, where STo and\nST\u2081 represent the zeroth-order and first-order short-term thresholding functions respectively. Above\nthis value, all points are correctly identified as interjections.\n$S T_0 * S T_1 > 287$   (3)\nSimple functions such as $ST_0 > 170$, $ST_1 > 1$, and MDR > 0.97 can also be used to identify inter-\njections with perfect accuracy. The remaining points, however, cannot be perfectly separated using\nthe currently explored methods. Equation 4 can be used to classify the general trend of interjection\nframes with 97.6% accuracy, but additional research is required to find a perfect SAM 2 interjection\nclassifier.\n$S T_1> e^{-0.15(S T_0 - 170)} + 1.03$ (4)\nLong-term frame similarity (w=5) is not a useful classifier for SAM 2, as the previous frame is a better\nindicator of interjection in both zeroth and first order L2 functions."}, {"title": "6 Results", "content": "Figure 7 shows a visualization of the mask predication for a 512-frame interjected video applied to\nCutie and Cutie+. Vanilla Cutie does a good job of segmentation in the prefix, as shown by the\nhighlighted person. During the interjection period, however, the model latches onto the duck in the\nvideo, demonstrating false positive predictions and the tarnishing of memory. In the suffix, the model\nincludes parts of the other person's fist in the mask prediction, showing working memory's imperfect\nre-identification capabilities. Cutie+, on the other hand, does a much better job at segmenting the\nobject. No false positive predictions are made during the interjection period, and the re-identification\nis much cleaner without any additional pixels."}, {"title": "7 Conclusion", "content": "A simple algorithmic change is proposed that can be applied to existing VOS models to improve\nperformance on sudden camera cuts, frame interjections, and extreme context changes. Working\nmemory is inherently susceptible to deterioration over time in the presence of irrelevant frames due to\nresulting false positive predictions and the writing of these frames into memory. By identifying these\nirrelevant frames before making predictions, their image embeddings can be skipped, and the working\nmemory can be conserved. Through a series of proposed thresholding functions, very high classification\naccuracy removes the potential trade off of poor performance on clean video. Future work aims to\ncontinue developing patches for working memory issues in video segmentation."}]}