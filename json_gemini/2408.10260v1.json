{"title": "Optical Music Recognition in Manuscripts from the Ricordi Archive", "authors": ["FEDERICO SIMONETTA", "RISHAV MONDAL", "LUCA ANDREA LUDOVICO", "STAVROS NTALAMPIRAS"], "abstract": "The Ricordi archive, a prestigious collection of significant musical manuscripts from renowned opera composers such as Donizetti, Verdi and Puccini, has been digitized. This process has allowed us to automatically extract samples that represent various musical elements depicted on the manuscripts, including notes, staves, clefs, erasures, and composer's annotations, among others. To distinguish between digitization noise and actual music elements, a subset of these images was meticulously grouped and labeled by multiple individuals into several classes. After assessing the consistency of the annotations, we trained multiple neural network-based classifiers to differentiate between the identified music elements. The primary objective of this study was to evaluate the reliability of these classifiers, with the ultimate goal of using them for the automatic categorization of the remaining unannotated data set.", "sections": [{"title": "1 INTRODUCTION", "content": "The Ricordi Archive, or Archivio Storico Ricordi in Italian, is a vast repository of historical documents amassed by the Italian publisher, Ricordi. The archive is renowned for its digitized manuscripts of distinguished opera composers such as Donizetti, Verdi, and Puccini. These manuscripts, digitized and systematically cataloged in a database maintained by the author's institution, are a significant asset for musicological and historical research.\nThis study aims to annotate the entire database with pertinent musical symbols, thereby improving the accessibility and discoverability of these priceless manuscripts. To achieve this, we developed a suitable Optical Music Recognition (OMR) methodology.\nOMR is a subfield of computer science dedicated to converting music notation from a visual format, such as scanned images or printed music sheets, into a digital form that can be manipulated by software. It serves as a bridge between physical music representations and their digital equivalents, with the aim of automating the interpretation of music symbols for various applications, including content retrieval, digital music libraries, and musicological research [3, 14, 15].\nThe evolution of OMR, particularly for printed music, is largely attributed to advancements in image processing and machine learning. Although early efforts relied heavily on rule-based systems [6], contemporary strategies use modern neural architectures. These architectures excel in feature extraction and help to accurately identify and classify musical symbols across various datasets and notation styles [2, 8, 11].\nHandwritten Music Recognition (HMR) focuses on the recognition of handwritten scores, adding an extra layer of complexity due to the unique styles and nuances inherent in individual handwriting. Recent methodologies suggest various strategies, including data augmentation and transfer learning, to enhance system performance and address the challenges specific to HMR [3, 18].\nAutomating the analysis of handwritten music presents significant obstacles due to the intricacy and variability of human handwriting; here, progress has been driven by the implementation of advanced machine learning models, specifically Convolutional Recurrent Neural Networks. Such models effectively capture both the spatial characteristics of the image and the sequential nature of music notation, which are crucial for the development of successful OMR systems for handwritten scores [1].\nThe development of large-scale datasets has been fundamental to the advancement of OMR technology. A key resource in OMR research is the MUSCIMA++ dataset [9]. This dataset, comprising 140 pages of handwritten music scores, is meticulously annotated with over 91000 symbols across 107 classes. It facilitates a wide range of tasks,"}, {"title": "2 BACKGROUND", "content": "The Ricordi Archive originated alongside the publishing house Casa Ricordi, established in 1808. Regarded as a paramount private musical repository, it safeguards the original handwritten scores of 23 out of Verdi's 28 operas, all operas by Giacomo Puccini (except La Rondine), and numerous works by composers such as Bellini, Rossini, Donizetti, as well as contemporary composers like Nono, Donatoni, Sciarrino, and Bussotti.\nThe archive's exceptional significance lies in the diversity of its materials, offering an articulated view of Italian culture, industry, and society. This archive preserves an extensive collection of visual materials associated with numerous premieres worldwide and locally, encompassing set and costume designs, photography compilations, correspondence, and business records. These resources empower researchers to reconstruct the inception of significant operas and the evolution of the musical publishing industry during the 19th and early 20th Centuries. Furthermore, the visual collection covers various artistic domains such as painting, stage design, and decorative arts, offering insights into costume history, jewelry design, stage properties, and the broader publishing landscape. It also sheds light on the relationship between publishers and artists across different fields and provides glimpses into the theatrical realm. Scholars can trace the"}, {"title": "3 DATASET", "content": "The original core of the digitization campaign of Ricordi Archive consisted of about 3000 digitized images, mainly handwritten scores by Donizetti, Puccini, Verdi, and Respighi.\n3.1 Preprocessing\nThe creation of the dataset necessitated preliminary processing to identify pertinent objects and reduce the annotation effort in its initial phase. This process entailed the following steps:\n\u2022 Staff Line Removal - A neural autoencoder-based algorithm [7] was employed to identify staff lines. Given the distinct clarity of the staff lines in the 19th-century documents from the archive, this method proved highly effective. The staff lines, being printed rather than handwritten, were easily distinguishable from the musical symbols, thereby enhancing the reliability of this step;\n\u2022 Blob Detection - The Difference of Gaussians (DoG) method, as implemented by the scikit-image Python module [12], was utilized to identify the musical symbols in the images. We used \u03c3\u2208 [10, 50] and a threshold of 0.1. Although this step does not guarantee the detection of all relevant objects in the images, it was tuned to be particularly sensitive to the ink regions. As a result, a large number of false positives were included to minimize the occurrence of false negatives, i.e., relevant objects not included in the dataset;\n\u2022 Rescale and Save - The grayscale images of the detected blobs were stored after rescaling their intensity values to [0, 255].\nThe construction of our inter-referencing database, which utilizes JSON files, began with the collection of blob images. In total, 473,238 blobs were extracted. These images were then systematically stored to facilitate easy access and reference.\nThe initial step in this process involved generating a grayscale image for each image in the original Ricordi Archive by removing the staff lines. Each of these grayscale images was then associated with a JSON file, which contained a reference to the original image, the path to the grayscale image without staff lines, and a list of JSON files associated with the blobs detected in the image.\n3.2 Annotation\nThe annotation phase was facilitated by 15 local high-school students with music reading skills, who were divided into seven groups of two or three. We developed a custom interface that enabled the students to assign labels to each blob image.\nFor reference, each detected blob was highlighted with a bounding box within a larger excerpt of the original image. Additionally, an HTML link to the original image was provided for further examination if necessary. A screenshot of the annotation interface is depicted in Figure 2.\nWe identified 16 classes of objects that could be recognized as blobs. These included page border, erasure, smudge, printed and handwritten text, rest, single or multiple notes, single or multiple chords, alterations, clefs, ornaments, multiple categories (with and without music signs), and an \"other\" category (with and without music signs) for objects that did not fit into any of the other categories.\nTo assess the accuracy of the annotations, a sample of 500 blobs was randomly selected for cyclic annotation by all annotators. This process involved the selection of a control blob with a 20% probability at each annotation cycle. The control blobs were initially used during the annotation process to gamify the labeling work by providing the annotators with simple scores that reflect the quality of their work. The score was calculated based on the average of two factors: the Spearman correlation coefficient of the annotator's labels, which represents intra-agreement, and the Spearman correlation coefficient between a) the average of the annotator's labels for each control blob and b) the average of the annotations already stored in the database, provided by other annotators, representing inter-agreement."}, {"title": "4 EXPERIMENTS", "content": "To evaluate the efficacy of statistical models in recognizing musical symbols, we fine-tuned three renowned deep learning classifiers: ResNet, DenseNet, and GoogleNet. We also employed an advanced AutoML method [5] to compare neural networks with conventional machine learning techniques.\nConsidering the significant imbalance depicted in Figure 3, we initially subsampled the classes with the highest cardinalities to achieve perfectly balanced training and validation sets. This subsampling involved randomly selecting n samples from the largest categories, where n corresponds to the number of samples in the smallest category. While this"}, {"title": "5 RESULTS", "content": "For the binary classification task, all three deep learning models achieved a balanced accuracy of 85% and an f1-score of 74%, as detailed in Table 1. In contrast, the constant predictor yielded a balanced accuracy of 50% and an f1-score of 46%, underperforming the random guessing.\nBy considering varying confidence levels, we observed a consistent monotonic trend for both accuracy and the proportion of retained test data. This indicates that higher accuracies can be attained by predicting fewer samples, as illustrated in Fig. 6. For example, with GoogleNet, a balanced accuracy of 95% and an f1-score of 88% can be achieved by retaining only samples with a confidence exceeding 50%, which constitutes 64% of the test set.\nIn this study, we employed the entropy of the neural output as a foundation for the confidence score, serving as a comprehensive measure of both epistemic and aleatoric uncertainty. Mathematically, given the network outputs $y_i, i \\in [1, N]$ for classifying $N$ classes, the entropy is calculated as:\n$H = \\sum_{i=1}^{N} SoftMax(y_i) \\times log_y (min(1, SoftMax(y_i) + \\epsilon))$,\nHere, $min()$ and $\\epsilon$ are incorporated to circumvent numerical instability, and $SoftMax$ is conventionally defined as:\n$SoftMax(y_i) = \\frac{e^{y_i}}{\\sum_{j=1}^{N} e^{y_j}}$\nFor N = 2, H aligns with the classical Shannon entropy computed using bits as information units. Generally, it always lies within [0, 1], allowing the computation of a confidence score as 1 \u2013 H.\nWe conducted all experiments for both the binary classification task, which involves distinguishing between \"musically relevant\" and \"musically irrelevant\" blobs, and the multi-class classification task, which involves differentiating among the 14 classes of objects."}, {"title": "6 CONCLUSIONS", "content": "This study presents a comprehensive methodology for OMR applied to historical and handwritten music scores, with a particular focus on the Ricordi Archive. This prestigious archive, housing significant musical manuscripts from eminent opera composers, has been digitized and meticulously annotated to generate a novel dataset of musical symbols. This dataset, along with the models and source code employed in our experiments, is publicly available, thereby contributing to the wider research community.\nWe have addressed several OMR challenges by training and evaluating multiple neural classifiers to differentiate between these symbols. Three renowned deep learning classifiers, namely ResNet, DenseNet, and GoogleNet, were fine-tuned, and a robust AutoML approach was utilized as a baseline. The deep learning models demonstrated promising results, achieving a balanced accuracy of 85% in the binary classification task. By leveraging the confidence of the models, even higher accuracies were attained.\nThe primary contribution of this work lies in the creation of a unique dataset of musical symbols derived from real-world annotated manuscripts, which can be utilized to train and evaluate OMR models. Additionally, our work outlines a comprehensive methodology for preprocessing, annotating, and classifying musical symbols, which can be replicated and expanded upon in future research.\nFuture work will involve using the trained models to annotate additional data, discarding irrelevant sub-images and focusing on images where the model exhibits low confidence. This strategy will enable automatic pixel-wise classification of all pages, followed by a focus on image regions with lower confidence. The ability to identify musical objects will facilitate the provision of more specific labels for such objects. This approach will significantly simplify the annotation of the full corpus, providing the research community with an updated version of the dataset."}]}