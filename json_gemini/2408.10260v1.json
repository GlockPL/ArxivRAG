{"title": "Optical Music Recognition in Manuscripts from the Ricordi Archive", "authors": ["FEDERICO SIMONETTA", "RISHAV MONDAL", "LUCA ANDREA LUDOVICO", "STAVROS NTALAMPIRAS"], "abstract": "N/A", "sections": [{"title": "1 INTRODUCTION", "content": "The Ricordi Archive, or Archivio Storico Ricordi in Italian, is a vast repository of historical documents amassed by the Italian publisher, Ricordi. The archive is renowned for its digitized manuscripts of distinguished opera composers such as Donizetti, Verdi, and Puccini. These manuscripts, digitized and systematically cataloged in a database maintained by the author's institution, are a significant asset for musicological and historical research.\nThis study aims to annotate the entire database with pertinent musical symbols, thereby improving the accessibility and discoverability of these priceless manuscripts. To achieve this, we developed a suitable Optical Music Recognition (OMR) methodology.\nOMR is a subfield of computer science dedicated to converting music notation from a visual format, such as scanned images or printed music sheets, into a digital form that can be manipulated by software. It serves as a bridge between physical music representations and their digital equivalents, with the aim of automating the interpretation of music symbols for various applications, including content retrieval, digital music libraries, and musicological research [3, 14, 15].\nThe evolution of OMR, particularly for printed music, is largely attributed to advancements in image processing and machine learning. Although early efforts relied heavily on rule-based systems [6], contemporary strategies use modern neural architectures. These architectures excel in feature extraction and help to accurately identify and classify musical symbols across various datasets and notation styles [2, 8, 11].\nHandwritten Music Recognition (HMR) focuses on the recognition of handwritten scores, adding an extra layer of complexity due to the unique styles and nuances inherent in individual handwriting. Recent methodologies suggest various strategies, including data augmentation and transfer learning, to enhance system performance and address the challenges specific to HMR [3, 18].\nAutomating the analysis of handwritten music presents significant obstacles due to the intricacy and variability of human handwriting; here, progress has been driven by the implementation of advanced machine learning models, specifically Convolutional Recurrent Neural Networks. Such models effectively capture both the spatial characteristics of the image and the sequential nature of music notation, which are crucial for the development of successful OMR systems for handwritten scores [1].\nThe development of large-scale datasets has been fundamental to the advancement of OMR technology. A key resource in OMR research is the MUSCIMA++ dataset [9]. This dataset, comprising 140 pages of handwritten music scores, is meticulously annotated with over 91000 symbols across 107 classes. It facilitates a wide range of tasks"}, {"title": "2 BACKGROUND", "content": "The Ricordi Archive originated alongside the publishing house Casa Ricordi, established in 1808. Regarded as a paramount private musical repository, it safeguards the original handwritten scores of 23 out of Verdi's 28 operas, all operas by Giacomo Puccini (except La Rondine), and numerous works by composers such as Bellini, Rossini, Donizetti, as well as contemporary composers like Nono, Donatoni, Sciarrino, and Bussotti.\nThe archive's exceptional significance lies in the diversity of its materials, offering an articulated view of Italian culture, industry, and society. This archive preserves an extensive collection of visual materials associated with numerous premieres worldwide and locally, encompassing set and costume designs, photography compilations, correspondence, and business records. These resources empower researchers to reconstruct the inception of significant operas and the evolution of the musical publishing industry during the 19th and early 20th Centuries. Furthermore, the visual collection covers various artistic domains such as painting, stage design, and decorative arts, offering insights into costume history, jewelry design, stage properties, and the broader publishing landscape. It also sheds light on the relationship between publishers and artists across different fields and provides glimpses into the theatrical realm. Scholars can trace the"}, {"title": "3 DATASET", "content": "The original core of the digitization campaign of Ricordi Archive consisted of about 3000 digitized images, mainly handwritten scores by Donizetti, Puccini, Verdi, and Respighi."}, {"title": "3.1 Preprocessing", "content": "The creation of the dataset necessitated preliminary processing to identify pertinent objects and reduce the annotation effort in its initial phase. This process entailed the following steps:\n\u2022 Staff Line Removal - A neural autoencoder-based algorithm [7] was employed to identify staff lines. Given the distinct clarity of the staff lines in the 19th-century documents from the archive, this method proved highly effective. The staff lines, being printed rather than handwritten, were easily distinguishable from the musical symbols, thereby enhancing the reliability of this step;\n\u2022 Blob Detection - The Difference of Gaussians (DoG) method, as implemented by the scikit-image Python module [12], was utilized to identify the musical symbols in the images. We used \u03c3\u2208 [10, 50] and a threshold of 0.1. Although this step does not guarantee the detection of all relevant objects in the images, it was tuned to be particularly sensitive to the ink regions. As a result, a large number of false positives were included to minimize the occurrence of false negatives, i.e., relevant objects not included in the dataset;\n\u2022 Rescale and Save - The grayscale images of the detected blobs were stored after rescaling their intensity values to [0, 255].\nThe construction of our inter-referencing database, which utilizes JSON files, began with the collection of blob images. In total, 473,238 blobs were extracted. These images were then systematically stored to facilitate easy access and reference.\nThe initial step in this process involved generating a grayscale image for each image in the original Ricordi Archive by removing the staff lines. Each of these grayscale images was then associated with a JSON file, which contained a reference to the original image, the path to the grayscale image without staff lines, and a list of JSON files associated with the blobs detected in the image."}, {"title": "3.2 Annotation", "content": "The annotation phase was facilitated by 15 local high-school students with music reading skills, who were divided into seven groups of two or three. We developed a custom interface that enabled the students to assign labels to each blob image.\nFor reference, each detected blob was highlighted with a bounding box within a larger excerpt of the original image. Additionally, an HTML link to the original image was provided for further examination if necessary. A screenshot of the annotation interface is depicted in Figure 2.\nWe identified 16 classes of objects that could be recognized as blobs. These included page border, erasure, smudge, printed and handwritten text, rest, single or multiple notes, single or multiple chords, alterations, clefs, ornaments, multiple categories (with and without music signs), and an \"other\" category (with and without music signs) for objects that did not fit into any of the other categories.\nTo assess the accuracy of the annotations, a sample of 500 blobs was randomly selected for cyclic annotation by all annotators. This process involved the selection of a control blob with a 20% probability at each annotation cycle. The control blobs were initially used during the annotation process to gamify the labeling work by providing the annotators with simple scores that reflect the quality of their work. The score was calculated based on the average of two factors: the Spearman correlation coefficient of the annotator's labels, which represents intra-agreement, and the Spearman correlation coefficient between a) the average of the annotator's labels for each control blob and b) the average of the annotations already stored in the database, provided by other annotators, representing inter-agreement."}, {"title": "4 EXPERIMENTS", "content": "To evaluate the efficacy of statistical models in recognizing musical symbols, we fine-tuned three renowned deep learning classifiers: ResNet, DenseNet, and GoogleNet. We also employed an advanced AutoML method [5] to compare neural networks with conventional machine learning techniques.\nConsidering the significant imbalance depicted in Figure 3, we initially subsampled the classes with the highest cardinalities to achieve perfectly balanced training and validation sets. This subsampling involved randomly selecting n samples from the largest categories, where n corresponds to the number of samples in the smallest category. While this method yields balanced training and validation sets, the test sets remain highly imbalanced. Therefore, it is crucial to implement appropriate validation measures that account for class imbalance during testing, as shown in Tables 1 and 2.\nIn all instances, we enhanced the training set by applying random rotations of up to \u00b110 degrees, random flips with probability of 0.5, brightness, contrast, and saturation jitters with factor 0.25. To improve the quality of the images, we also implemented Gaussian blur denoising with kernel of 3 and sigma 1.5 and contrast correction to 1.5 of the original contrast. To ensure compatibility with the ImageNet pre-trained weights, we renormalized the image channels and resized all images to 256 \u00d7 256 pixels.\nFor the deep-learning classifiers, we utilized the pre-trained weights available in the torchvision library, obtained from the ImageNet 1K dataset [4, 13]. We re-trained all models using standard cross-entropy loss and the 1 cycle learning rate policy [16] with Stochastic Gradient Descent, setting the maximum learning rate at 0.01. The models were trained for 500 epochs with early stopping based on validation loss, exhibiting a patience of 20 epochs. This resulted in approximately 40 epochs of actual training, with a maximum of 70 epochs for GoogleNet in binary classification. We used a batch size of 64 and allocated 68%, 17%, and 15% of the dataset for training, validation, and testing, respectively.\nWe placed particular emphasis on uncertainty analysis by examining the activations in the networks' final layer. Ideally, a confidence close to 1 suggests that the input sample is located in a region of the feature space familiar to the network, while a confidence near 0 indicates unfamiliarity. Consequently, we can disregard low-confidence predictions to minimize the risk of incorrect classifications. In Bayesian statistics, uncertainties are categorized as epistemic and aleatoric [10]. Epistemic uncertainty pertains to the model parameters, indicating that the model parameters have learned the under-represented region of the data. Aleatoric uncertainty, on the other hand, relates to the data itself, suggesting that the data is inherently noisy."}, {"title": "5 RESULTS", "content": "For the binary classification task, all three deep learning models achieved a balanced accuracy of 85% and an f1-score of 74%, as detailed in Table 1. In contrast, the constant predictor yielded a balanced accuracy of 50% and an f1-score of 46%, underperforming the random guessing.\nBy considering varying confidence levels, we observed a consistent monotonic trend for both accuracy and the proportion of retained test data. This indicates that higher accuracies can be attained by predicting fewer samples, as illustrated in Fig. 6. For example, with GoogleNet, a balanced accuracy of 95% and an f1-score of 88% can be achieved by retaining only samples with a confidence exceeding 50%, which constitutes 64% of the test set.\nIn the multi-class classification task, the deep learning models achieved a balanced accuracy of 43% and an f1-score of 38%. For comparison, random guessing and constant predictors were used as baselines, yielding a balanced accuracy of 9% and an f1-score of 6%.\nUpon considering confidence levels, we observed a convex accuracy curve with a peak between 25% and 50% for the three models. Specifically, GoogleNet and ResNet can achieve a balanced accuracy and f1-score of 50% and 51% respectively, by retaining 68% of the data, as depicted in Fig. 7. For confidence levels larger than 0.5, only printed text, keys, and page border can be identified satisfactorily, with f1-scores near to 1. However, a little number of samples are misclassified, leading the a fall in balanced accuracy, which is computed as the arithmetic mean of the per-class recall."}, {"title": "6 CONCLUSIONS", "content": "This study presents a comprehensive methodology for OMR applied to historical and handwritten music scores, with a particular focus on the Ricordi Archive. This prestigious archive, housing significant musical manuscripts from eminent opera composers, has been digitized and meticulously annotated to generate a novel dataset of musical symbols. This dataset, along with the models and source code employed in our experiments, is publicly available, thereby contributing to the wider research community.\nWe have addressed several OMR challenges by training and evaluating multiple neural classifiers to differentiate between these symbols. Three renowned deep learning classifiers, namely ResNet, DenseNet, and GoogleNet, were fine-tuned, and a robust AutoML approach was utilized as a baseline. The deep learning models demonstrated promising results, achieving a balanced accuracy of 85% in the binary classification task. By leveraging the confidence of the models, even higher accuracies were attained.\nThe primary contribution of this work lies in the creation of a unique dataset of musical symbols derived from real-world annotated manuscripts, which can be utilized to train and evaluate OMR models. Additionally, our work outlines a comprehensive methodology for preprocessing, annotating, and classifying musical symbols, which can be replicated and expanded upon in future research.\nFuture work will involve using the trained models to annotate additional data, discarding irrelevant sub-images and focusing on images where the model exhibits low confidence. This strategy will enable automatic pixel-wise classification of all pages, followed by a focus on image regions with lower confidence. The ability to identify musical objects will facilitate the provision of more specific labels for such objects. This approach will significantly simplify the annotation of the full corpus, providing the research community with an updated version of the dataset."}, {"title": "In this study, we employed the entropy of the neural output as a foundation for the confidence score, serving as a comprehensive measure of both epistemic and aleatoric uncertainty.", "content": "Mathematically, given the network outputs Yi, i \u2208 [1, N] for classifying N classes, the entropy is calculated as:\nH = \u2211 SoftMax(yi) \u00d7 logy (min(1, SoftMax(yi) + \u0454)),\ni=1\nHere, min() and e are incorporated to circumvent numerical instability, and SoftMax is conventionally defined as:\nSoftMax(yi) = eYi / \u2211j=1 \u03a3N eyj\nFor N = 2, H aligns with the classical Shannon entropy computed using bits as information units. Generally, it always lies within [0, 1], allowing the computation of a confidence score as 1 \u2013 \u041d.\nWe conducted all experiments for both the binary classification task, which involves distinguishing between \"musically relevant\" and \"musically irrelevant\" blobs, and the multi-class classification task, which involves differentiating among the 14 classes of objects."}]}