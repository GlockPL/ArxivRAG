{"title": "PateGail: A Privacy-Preserving Mobility Trajectory Generator with Imitation Learning", "authors": ["Huandong Wang", "Changzheng Gao", "Yuchen Wu", "Depeng Jin", "Lina Yao", "Yong Li"], "abstract": "Generating human mobility trajectories is of great importance to solve the lack of large-scale trajectory data in numerous applications, which is caused by privacy concerns. However, existing mobility trajectory generation methods still require real-world human trajectories centrally collected as the training data, where there exists an inescapable risk of privacy leakage. To overcome this limitation, in this paper, we propose PateGail, a privacy-preserving imitation learning model to generate mobility trajectories, which utilizes the powerful generative adversary imitation learning model to simulate the decision-making process of humans. Further, in order to protect user privacy, we train this model collectively based on decentralized mobility data stored in user devices, where personal discriminators are trained locally to distinguish and reward the real and generated human trajectories. In the training process, only the generated trajectories and their rewards obtained based on personal discriminators are shared between the server and devices, whose privacy is further preserved by our proposed perturbation mechanisms with theoretical proof to satisfy differential privacy. Further, to better model the human decision-making process, we propose a novel aggregation mechanism of the rewards obtained from personal discriminators. We theoretically prove that under the reward obtained based on the aggregation mechanism, our proposed model maximizes the lower bound of the discounted total rewards of users. Extensive experiments show that the trajectories generated by our model are able to resemble real-world trajectories in terms of five key statistical metrics, outperforming state-of-the-art algorithms by over 48.03%. Furthermore, we demonstrate that the synthetic trajectories are able to efficiently support practical applications, including mobility prediction and location recommendation.", "sections": [{"title": "Introduction", "content": "Human mobility trajectory data is instrumental for a large number of applications. For example, for the mobile Internet service providers, based on mobility trajectories, the movement and communication process of mobile users can be simulated to implement a reliable and efficient performance evaluation of the mobile networks [16]. For the government, mobility trajectories can characterize the travel demand of the population and the traffic condition of the city, and thus provide important guidance to the transportation system planning [8]. However, the utilization of real-world mobility trajectories leads to a growing privacy concern, since sensitive information of users can be leaked from their trajectories, e.g., which places they have visited and who they have met. Thus, it is hard to obtain a large-scale human mobility trajectory dataset to support numerous downstream applications. Under these circumstances, simulating human mobility behavior to produce realistic and high-quality mobility trajectory data becomes an important task for downstream applications, and has drawn much attention from both academia and industry.\nNumerous existing approaches have been proposed to generate mobility trajectories by utilizing powerful deep learning techniques including variational autoencoder (VAE) [18], and generative adversarial network (GAN) [9, 27, 22, 23], etc. However, as shown in Figure 1(a), these methods still require a number of real-world human trajectories centrally collected as the training data, where there exists the risk of privacy leakage. The rising paradigm of federated learning has provided a promising solution to this problem, which is a distributed machine learning framework with the goal of training machine learning models based on data distributed across multiple devices and protecting users' privacy at the same time. Federated learning has shown success in a number of practical applications, including personalized recommendation [5], keyboard prediction [14], etc.\nThus, we seek to train the mobility trajectory generator in the manner of federated learning. As shown in Figure 1(b), in the federated mobility trajectory generation system, each user device keeps the private mobility trajectory data belonging to its owner (user). Only aggregated intermediate results proceeded by privacy protection mechanisms are shared between devices, while this system does not take any piece of the user trajectory data away from the device. In this way, we can train the mobility trajectory generator without privacy leakage.\nHowever, training an efficient trajectory generator based on federated learning is not an easy task with the following challenges. First, mobility trajectories are with high dimensions and complicated interactions with both spatial venues and timestamps. How to develop a trajectory generator that accurately models human mobility behavior is the first challenge. Second, users' privacy is still possible to be leaked from the transmitted intermediate results in the training process, while most existing solutions do not provide privacy-preserving guarantees of the training process [10, 27]. How to provide the privacy-preserving guarantees of the training process is the second challenge.\nTo overcome these challenges, in this paper, we propose PateGail, a privacy-preserving imitation learning based mobility trajectory generator. Specifically, this model utilizes the powerful technique of generative adversary imitation learning (GAIL) to extract the hidden human movement decision process correlated with both spatial venues and timestamps, and thus is able to produce plausible mobility trajectories with preserved utility. Further, in order to provide privacy-preserving guarantees, we locally train a separate personal discriminator on each user device to distinguish and reward the generated and real-world decision-making sequences of human mobility, and then only share the generated trajectories and the rewards of the generated trajectories obtained based on personal discriminators between the devices and the server in the training process. Further, we propose a perturbation mechanism to prevent privacy leakage from the rewards of personal discriminators, which is theoretically proven to satisfy the differential privacy criterion. Finally, we propose a novel aggregation mechanism based on the mean and variance of reward obtained from different personal discriminators, which is able to model the dynamics of reward function across users. Furthermore, we theoretically prove that under the reward obtained based on our proposed aggregation mechanism, our proposed model maximizes the lower bound of the discounted total rewards of users. Our contributions can be summarized as follows:\n\u2022 We propose a powerful mobile trajectory generator based on GAIL and federated learning, which is able to extract the hidden human decision process to generate plausible mobile trajectories and preserve user privacy with differential privacy guarantees at the same time.\n\u2022 We propose a novel reward aggregation mechanism of reward obtained from personal discriminators of different users, which is able to model the dynamics of reward function across users. Furthermore, we theoretically prove that under our proposed reward aggregation mechanism, the obtained model maximizes the lower bound of the discounted total rewards of users.\n\u2022 Extensive experiments show that the synthetic trajectories of our proposed model are able to preserve the statistical properties of the original dataset, and are able to efficiently support downstream applications by augmenting their training data. We release the code of our proposed algorithm as well as the datasets to better reproduce the experimental results\u00b9."}, {"title": "Mathematical Model and System Overview", "content": "Mathematical Model\nFor the sake of convenience, we summarize the notations used in this paper in Table A1 of the Appendix. Specifically, we consider the scenario where there are multiple users with their own mobile devices. Each device has recorded the historical mobility trajectory of the corresponding user. We define the set of users as U. Further, for each user u \u2208 U, we define the mobility trajectory of u as a sequence of spatio-temporal points, i.e., \\(T_u = \\{(t_1,l_1), (t_2, l_2), ..., (t_v,l_n)\\}\\), where \\(l_i\\) is the identifier of the visited location and \\(t_i\\) is the corresponding timestamp. Then, the human mobility generation problem can be defined as follows.\nDefinition 1 (Privacy-Preserving Federated Mobility Trajectory Generation Problem) Given a set of user U and their historical mobility trajectory \\(\\{T_u\\}_{u \\in U}\\), the goal of this problem is to train a mobility trajectory generator distributedly, which is able to generate mobility trajectory with the preserved utility. In addition, the privacy information involved in the trajectory of each user should be preserved.\nSpecifically, the preserved utility indicates that the generated trajectories should statistically resemble the real-world trajectories. Furthermore, they should be able to effectively support the downstream applications relying on trajectory data. On the other hand, the preserved privacy indicates that any pieces of the users' private trajectories should neither be taken away from their own devices, nor be inferred from the uploaded intermediate results of the user devices.\nSystem Overview\nWe propose a privacy-preserving federated imitation learning system to solve this problem, of which the framework is shown in Figure 2."}, {"title": "Method", "content": "In order to model the substantial human decision-making process to simulate the human mobility behavior, we utilize the powerful technique of the generative adversary imitation learning (GAIL) under the model of Markov decision processes (MDPs). Specifically, the MDP model is defined by a 4-tuple < S, A, P, R >, where S is the state space, A is the action space, \\(P : S \\times A \\times S \\rightarrow R^+\\) represents the state transition probability, and \\(R: S \\times A \\rightarrow R\\) represents the reward function. Specifically, we define the state of users as the set of their historical spatio-temporal points, i.e., \\(S_t = \\{(t_r,l_i)\\}_{r < t}\\), and the action space is defined based on the widely-adopted exploration and preferential return (EPR) model [20, 34], which includes four actions composed of stay, home return, preferential return, and explore. Then, the state transition probability, which defines the probability distribution of the next state given the current state and action, is also defined based on the EPR model (see Appendix for details). Then, each user is regarded as an \"agent\" who dynamically determines the action to be executed based on its current state through its policy function, and the goal of the agent is to maximize the discounted total rewards \\(E_t[\\gamma^t R(s_t, a_t)]\\), where \\(\\gamma \\leq 1\\) is the discount factor. In the imitation learning problem, the reward function R as well as the policy function are unknown and thus need to be learned from the real-world data. Thus, in the following part of this section, we first introduce the utilized policy function and reward function. Then, we introduce how to train our proposed system based on GAIL. Finally, we analyze the system in terms of its theoretical privacy-preserving performance.\nPolicy Function\nThe policy function defines the decision strategy taken by users, which takes the current state \\(s_t\\) as the input and then outputs the action \\(a_t\\) to be executed. We follow the common settings adopted in most imitation learning problems, and consider the stochastic policy rather than the deterministic policy. In this case, the policy function actually gives the probabilistic distribution of executing arbitrary action \\(a_t \\in A\\). Specifically, we utilize a self-attention transformer [36] parameterized by \\(\\theta\\) to model the policy function, which is denoted by \\(\\pi_\\theta(a_t|s_t)\\).\nCombining the policy function \\(\\pi(a_t|s_t)\\) and the state transition probability function \\(P(s_{t+1}|s_t, a_t)\\), each agent is able to dynamically determine which action to be executed and then change the current state from \\(s_t\\) to \\(s_{t+1}\\) via interaction with \\(P(s_{t+1}|s_t, a_t)\\). By repeating this process, the agent is able to sample synthetic trajectories corresponding to the policy net \\(\\pi_\\theta\\). Thus, the policy function acts as the trajectory generator in our system.\nIn our system, we only utilize a global policy network, which is trained on the server. Further, in the training process of our system, the policy network only interacts with the private user trajectories through the reward function. By designing a privacy-preserving reward function, we are able to obtain a policy network without privacy leakage. Then, the server is able to send the parameter of the global policy network to the devices, and thus the devices also have the ability to generate synthetic human trajectories. Specifically, the policy function is optimized to imitate real-world human trajectories, of which the degree is measured by the reward function introduced in the following section.\nReward Function\nThe reward function measures to what degree arbitrary given trajectories imitate real trajectories. Specifically, it takes a state-action pair \\((s_t, a_t)\\) as the input, and outputs a real number, where higher values indicate that the state-action pair better imitates real-world human decisions.\nIn the standard GAIL, a discriminator network is utilized to model the reward function, which is trained based on the positive samples of real-world human state-action pairs and the negative samples of synthetic state-action pairs. However, in order to protect user privacy, the private trajectory data stored on mobile devices cannot be gathered together to train the discriminator. Thus, in our system, we replace it with a number of separate personal discriminators of users and a private aggregation mechanism, of which the idea is inspired by the techniques of Private Aggregation of Teacher Ensembles (PATE) [29, 21]. Specifically, each user device trains its personal discriminator based on its private trajectory data, and the final utilized reward function comes from the private aggregation of their ensemble. Note that the personal discriminators play a similar role with the teacher models in the standard PATE and PATE-GAN model [29, 21]. However, different from them, there is no student discriminator trained in our designed system. Instead, the aggregated reward is utilized to directly update the quality function or the advantage function in reinforcement learning algorithms such as A2C [25] and PPO [31]."}, {"title": null, "content": "In the following part of this section, we first introduce the personal discriminator. Then, we introduce how to implement a private aggregation to obtain the reward function based on ensemble learning. Finally, we propose a reward dynamics compensation mechanism to eliminate the potential noise introduced by ensemble learning and model the dynamics of reward function across users.\nPersonal Discriminators: The discriminator takes the state-action pair as input and then outputs its plausibility. The personal discriminator plays a similar role, but it can only access the trajectory data belonging to its corresponding user and the synthetic trajectories generated based on the global policy network. Specifically, we denote the personal discriminator belonging to user u as \\(D_{\\phi_u}\\), which is parameterized by \\(\\phi_u\\). Then, \\(D_{\\phi_u}\\) is optimized based on the following loss function:\n\\(C(\\Phi_u) = -E_{\\pi_u^{te}}[logD_{\\phi_u}(s, a)] - E_{\\pi}[log(1- D_{\\phi_u}(s, a))]\\),                                                                                                          (1)\nwhere \\(E_{\\pi}\\) represents the expectation with respect to the trajectories sampled based on the policy \\(\\pi\\). Specifically, for an arbitrary function \\(f :S\\times A \\rightarrow R\\), we have \\(E_{\\pi}[f(s,a)] = E[\\Sigma_{i=1}^N f(s_i, a_i)]\\), where \\(a_i \\sim \\pi(\\cdot|s_i)\\) and \\(s_{i+1} \\sim P(\\cdot|s_i, a_i)\\). In addition, \\(E_{\\pi_u^{te}}\\) represents the expectation in terms of the state-action pairs obtained from the real-world trajectory of user u.\nPrivate Aggregation Mechanism: The trajectory data on each user device is insufficient and largely influenced by the user personality, which prevents the discriminator from capturing the principle plausibility of state-action pairs. Thus, it is necessary to incorporate the plausibility estimated by all personal discriminators.\nFormally, for arbitrary state-action pair \\((s, a)\\), each mobile device u estimates its plausibility based on the local personal discriminator \\(D_{\\phi_u} (s, a)\\), which is then uploaded to the server. The server computes the average value of the obtained personal rewards and then adds a perturbation to it, of which the process can be expressed as follows:\n\\(R(s,a) = \\frac{1}{|U|} \\sum_{u \\in U} D_{\\phi_u} (s, a) + Laplace(0, \\lambda)\\). (2)\nNote that this process from uploading \\(D_{\\phi_u} (s, a)\\) to calculating (2) can be protected by homomorphic encryption techniques. Specifically, the server is in charge of generating public keys and sending them to all devices. Each device u then encrypts \\(D_{\\phi_u}(s, a)\\) and sends it to another third-party server, which is in charge of implementing the computation of (2) and sending the results to the server in charge of training the trajectory generator.\nThe third-party server can only influence the performance of the trained trajectory generator and no user privacy can be leaked from it. To order to further prevent malicious third-party servers, we can randomly select a client to be the third-party server in each communication round. It is also possible for the malicious third-party server to incorrectly calculate (2), which can be solved by introducing verification information to the uploaded message \\(D_{\\phi_u} (s, a)\\) of the clients.\nAnother difference of the aggregation mechanism (2) from standard PATE is that we compute the average value of the uploaded outputs of the personal discriminators, while the aggregation results of the standard PATE are discrete. The reason is that utilizing a discrete aggregation mechanism may lead to a sparse reward function, which reduces the performance of our proposed system. Due to this difference, the required perturbation scale \\(\\lambda\\) to achieve differential privacy is also different.\nReward Dynamics Compensation Mechanism: The above ensemble learning based method will introduce extra noise to the obtained reward function. Specifically, users' mobility behavior has intrinsic stochasticity. Furthermore, there also exist personal differences between the reward functions of different users. Since each personal discriminator can only observe the trajectory of a single user, it is more affected by stochasticity and personal differences, which might reduce the performance.\nIn order to eliminate the influence introduced by the distributed training method, we propose a reward dynamics compensation mechanism. Specifically, it models the reward dynamics actively based on the variance of the obtained rewards from the personal discriminators, which is further incorporated into the reward function to derive the lower bound of obtained reward. The process can be formally expressed by the following equations:\n\\[\\begin{aligned} &\\xi(s,a) = \\sqrt{var(D_{\\phi_u} (s, a))} + Laplace(0, \\lambda_c),  \\\\ &R(s, a) = R(s, a) - \\beta\\xi(s, a),  \\end{aligned}\\] (3)\nwhere var(X) is the variance of the stochastic variable X, and \\(\\beta\\) is a hyper-parameter to adjust the influence of the reward dynamics compensation mechanism. Intuitively, R can be regarded as the lower bound of the personal rewards of users, which is formally described in the following theorem.\nTheorem 1 Denote the discounted total reward based on the policy function \\(\\pi\\) and reward function R as \\(J(\\pi, R) = \\Sigma_i \\gamma^i R(s_i, a_i)\\), where \\(a_i \\sim \\pi(\\cdot|s_i)\\) and \\(s_{i+1} \\sim P(\\cdot|s_i, a_i)\\). Let \\(R_u\\) denote the personal reward function of user u, i.e., \\(R_u = D_{\\phi_u}\\). Then, for a randomly selected user u and policy function \\(\\pi\\), we have \\(Pr\\Big(J(\\pi, R_u) \\ge J(\\pi, R)\\Big) \\ge 1 - \\frac{1}{\\beta^2}.\nThis theorem tells us that under arbitrary policy function \\(\\pi\\), the discounted total reward of user u obtained based on the reward function R is the lower bound of that obtained based on R with probability \\(1 - \\frac{1}{\\beta^2}\\). By setting a sufficiently large \\(\\beta\\), we can obtain a probability very close to one. Thus, by utilizing the reward function (3) to replace the original reward function (2), our proposed model is able to maximize the lower bound discounted total reward of the majority of users, which helps to eliminate the potential noise introduced by the ensemble learning and model the dynamics of reward function across users.\nModel Training\nThe training process of our proposed model is summarized as follows. Firstly, a batch of synthetic trajectories is sampled to train the personal discriminators belonging to different"}, {"title": "Privacy Analysis", "content": "Before we analyze our proposed system in terms of its theoretical privacy-preserving performance, we first provide preliminaries about differential privacy, which is the most widely-used privacy preserving criterion.\nDefinition 2 ((\u03b5, \u03b4)-differential privacy) A randomized mechanism M : D \u2192 O satisfies (\u20ac, \u03b4)-differential privacy if and only if, for arbitrary adjacent datasets D\u2081 and D\u2082, and subset \u039f \u2208 O, we have \\(Pr(M(D_1) \\in O) \\leq e^\\epsilon Pr(M(D_2) \\in O) + \\delta\\).\nThen, based on the above definition, we will further examine the theoretical privacy-preserving performance of our system by investigating how to set the value of the parameters of the adding noise in (2) and (3) to achieve (\u20ac, \u03b4)-differential privacy. Overall, our obtained results can be summarized in the following theorems.\nTheorem 2 By setting \\(\\lambda = \\frac{3|U|}{\\epsilon}\\), the random mechanism (2) satisfy (\u20ac, 0)-differential private.\nTheorem 2 provides how to achieve (\u20ac, \u03b4)-differential privacy by using the reward function based on the private aggregation mechanism. If further utilizing the reward dynamics compensation mechanism, we can achieve (\u20ac, \u03b4)-differential privacy based on the following theorem."}, {"title": "Related Work", "content": "Human Trajectory Generators. Human trajectory generation models have been investigated for decades. Early approaches mainly utilize classical probabilistic methods or rule-based methods to model and generate human trajectories [20, 19, 40, 3, 44, 30]. However, these methods are derived from strong assumptions of human mobility, and whether these assumptions hold true in reality is questionable. In addition, these assumptions also limit a small number of parameters describing the human mobility process, leading to their weakness in terms of modeling the complicated relationship of high-dimensional mobility trajectories. In recent years, more deep learning based human trajectory generators have been proposed by utilizing variational autoencoder (VAE) [18], generative adversarial network (GAN) [9, 27, 22, 23, 1], imitation learning [28, 43, 38, 42, 32, 24]. However, none of them considers the critical privacy problem, indicating that there exist privacy leakage risks of the trajectory data utilized to train these models. Different from them, in this paper, we address the privacy-preserving issue, and propose a federated mobility generator by utilizing the techniques of imitation learning.\nImitation Learning. The goal of imitation learning is to learn the policy function, which gives the action to be executed based on the current state [2, 4, 47, 46, 47, 17]. The most successful imitation learning method is generative adversarial imitation learning (GAIL), which utilizes the non-linear neural network to model the reward function"}, {"title": "Conclusion", "content": "In this paper, we propose a privacy-preserving federated mobility trajectory generator based on imitation learning techniques, which is able to generate plausible synthetic mobility trajectories with the preserved utility to be utilized in downstream applications and preserve users' privacy at the same time. Extensive experiments validate the effectiveness of our proposed model. Specifically, the generated trajectories based on our proposed algorithm are able to preserve the statistical properties of the original dataset in terms of a number of key statistical metrics. Furthermore, the synthetic trajectories are able to efficiently support practical applications, including mobility prediction and location recommendation, demonstrating its effectiveness."}, {"title": "Proofs of the Theorems", "content": "Theorem 1 Denote the discounted total reward based on the policy function \\(\\pi\\) and reward function R as \\(J(\\pi, R) = \\Sigma_i \\gamma^i R(s_i, a_i)\\), where \\(a_i \\sim \\pi(\\cdot|s_i)\\) and \\(s_{i+1} \\sim P(\\cdot|s_i, a_i)\\). Let \\(R_u\\) denote the personal reward function of user u, i.e., \\(R_u = D_{\\phi_u}\\). Then, for a randomly selected user u and policy function \\(\\pi\\), we have \\(Pr\\Big(J(\\pi, R_u) \\ge J(\\pi, R)\\Big) \\ge 1 - \\frac{1}{\\beta^2}.\nProof: First, based on Chebyshev's inequality, we have:\n\\[\\begin{aligned} & Pr\\Big( \\Big| J(\\pi, R_u) - \\frac{1}{|U|} \\sum_{u \\in U} J(\\pi, R_u) \\Big| \\ge \\beta \\sum_i \\gamma^i \\xi(s_i, a_i) \\Big) \\\\ & \\leq Pr\\Big( \\Big| J(\\pi, R_u) - \\frac{1}{|U|} \\sum_{u \\in U} J(\\pi, R_u) \\Big|^2 \\ge \\Big( \\beta \\sum_i \\gamma^i \\xi(s_i, a_i) \\Big)^2 \\Big) \\\\ & \\leq \\frac{var(J(\\pi, R_u))}{\\Big( \\beta \\sum_i \\gamma^i \\xi(s_i, a_i) \\Big)^2} \\end{aligned}\\]                                                                                                                                       (5)\nThen, in terms of J(\u03c0, Ru), since the policy function \u03c0 has been given, conditioned on \u03c0 the sampled station-action pair is independent with the reward function. Thus, we have \\(R_u(s_i, a_i)\\) is independent with \\(R_u(s_j, a_j)\\) for i \u2260 j. Then, we have:\n\\[\\begin{aligned} & var(J(\\pi, R_u)) \\\\ &= \\frac{1}{|U|^2} \\sum_{u \\in U} var \\Big( \\sum_i \\gamma^i (R_u(s_i, a_i) - \\overline{R_u}(s_i, a_i)) \\Big) \\\\ &= \\frac{1}{|U|^2} \\sum_{u \\in U} \\sum_i \\gamma^{2i} var \\Big(R_u(s_i, a_i) - \\overline{R_u}(s_i, a_i) \\Big) \\\\ &= \\sum_i \\gamma^{2i} \\xi^2(s_i, a_i) \\end{aligned}\\] (6)\nSince \\(\\xi(s_i, a_i) \\geq 0\\) for all state-action pair \\((s_i, a_i)\\), we have \\(\\sum_i \\gamma^{2i} \\xi^2(s_i, a_i) \\leq \\Big(\\sum_i \\gamma^{i}\\xi(s_i, a_i) \\Big)^2\\). Then, based on (5), we have:\n\\(Pr\\Big( \\Big| J(\\pi, R_u) - \\frac{1}{|U|} \\sum_{u \\in U} J(\\pi, R_u) \\Big| \\ge \\beta \\sum_i \\gamma^i \\xi(s_i, a_i) \\Big) \\leq \\frac{1}{\\beta^2}\\)                                                                                            (7)\nAt the same time, we have\n\\(J(\\pi, R) = \\frac{1}{|U|} \\sum_{u \\in U} J(\\pi, R_u) - \\beta \\sum_i \\gamma^i \\xi(s_i, a_i)\\), (8)\nwhich proves our theorem.\nTheorem 2 By setting \\(\\lambda = \\frac{3|U|}{\\epsilon}\\), the query based on (2) satisfy (\u20ac, 0)-differential private.\nProof: In our scenarios, the complete dataset is separately stored in U devices. Further, for an arbitrary query for the reward of the state-action pair \\((s, a)\\), we define the personal discriminator and the aggregated reward function (2) conditioned on the complete dataset Das \\(D_{\\phi_u}(s, a|D)\\) and \\(R(s, a|D)\\), respectively. Without loss of generality, for two arbitrary adjacent datasets D\u2081 and D\u2082, we assume they only differ in the trajectory data of the user \\(\\mu_0\\). Thus, we have \\(D_{\\phi_u}(s,a|D_1) = D_{\\phi_u}(s,a|D_2), \\forall u \\neq \\mu_0\\). Then, we have the following equations:\n\\[\\begin{aligned} & \\frac{p(R(s, a|D_1) = r)}{p(R(s, a|D_2) = r)} \\\\ &= \\frac{exp\\Big(- \\frac{|\\frac{1}{|U|} \\sum_{u \\in U} D_{\\phi_u} (s, a|D_1) - r|}{\\lambda}\\Big)}{exp\\Big(- \\frac{|\\frac{1}{|U|} \\sum_{u \\in U} D_{\\phi_u} (s, a|D_2) - r|}{\\lambda}\\Big)} \\\\ &= \\frac{exp\\Big( - \\frac{1}{\\lambda} \\Big| \\frac{1}{|U|} \\sum_{u \\in U} D_{\\phi_u} (s, a|D_1) - r\\Big| \\Big)}{exp\\Big( - \\frac{1}{\\lambda} \\Big| \\frac{1}{|U|} \\sum_{u \\in U} D_{\\phi_u} (s, a|D_2) - r\\Big| \\Big)} \\\\ & \\leq exp \\Big( \\frac{1}{\\lambda |U|} \\Big| \\sum_{u \\in U} D_{\\phi_u} (s, a|D_1) - \\sum_{u \\in U} D_{\\phi_u} (s, a|D_2) \\Big| \\Big) \\\\ &= exp \\Big( \\frac{1}{\\lambda |U|} \\Big|D_{\\phi_{\\mu_0}} (s, a|D_1) - D_{\\phi_{\\mu_0}} (s, a|D_2) \\Big| \\Big) \\\\ & \\leq exp \\Big( \\frac{3}{\\lambda |U|} \\Big) \\\\ &= exp(\\epsilon). \\end{aligned}\\] (9)\nThen, for arbitrary set O, we have :\n\\[\\begin{aligned} & Pr(R(s, a|D_1) \\in O) = \\int_{r \\in O} p(R(s, a|D_1) = r)dr \\\\ &\\leq \\int_{r \\in O} exp(\\epsilon) p(R(s, a|D_2) = r)dr \\\\ &= exp(\\epsilon) \\int_{r \\in O} p(R(s, a|D_2) = r) dr \\\\ &= exp(\\epsilon) Pr(R(s, a|D_2) \\in O), \\end{aligned}\\](10)\nwhich proves our theorem.\nBefore we prove Theorem 3, we first provide some preliminaries in terms of lemmas that have been proven in existing studies.\nLemma 1 (Composition Theorem for Differential Privacy) For arbitrary randomized mechanism M\u2081 satisfying \\(( \\epsilon_1, \\delta_1)\\)-differential private and M2 satisfying \\(( \\epsilon_2, \\delta_2)\\)-DP, their combination M = (M1, M2) satisfying \\(( \\epsilon_1 + \\epsilon_2, \\delta_1 + \\delta_2)\\)-differential private.\nThe proof of Lemma 1 can be found in [6]. Based on this lemma, we can prove Theorem 3 as follows.\nTheorem 3 By setting \\(\\lambda = \\frac{3|U|}{\\epsilon}\\) and \\(\\lambda_c = \\frac{3\\kappa}{\\epsilon(\\kappa-1)|U|}\\) for all \\(\\kappa > 1\\), the query based on (3) satisfy (\u20ac, 0)-differential private.\nProof: Based on Theorem 2, we can obtain that by setting \\(\\lambda = \\frac{3|U|}{\\epsilon}\\), the randomized mechanism R(s,a) satisfies \\((\\epsilon/\\kappa, 0)\\)-differential private. Next, we further analyze the privacy bound of the randomized mechanism \\(\\xi(s, a|D)\\). Without loss of generality, we analyze the privacy bound of \\(\\xi^2(s, a|D)\\). Similarly, for two arbitrary adjacent datasets D1"}]}