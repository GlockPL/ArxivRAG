{"title": "PateGail: A Privacy-Preserving Mobility Trajectory Generator with Imitation Learning", "authors": ["Huandong Wang", "Changzheng Gao", "Yuchen Wu", "Depeng Jin", "Lina Yao", "Yong Li"], "abstract": "Generating human mobility trajectories is of great importance to solve the lack of large-scale trajectory data in numerous applications, which is caused by privacy concerns. However, existing mobility trajectory generation methods still require real-world human trajectories centrally collected as the training data, where there exists an inescapable risk of privacy leakage. To overcome this limitation, in this paper, we propose PateGail, a privacy-preserving imitation learning model to generate mobility trajectories, which utilizes the powerful generative adversary imitation learning model to simulate the decision-making process of humans. Further, in order to protect user privacy, we train this model collectively based on decentralized mobility data stored in user devices, where personal discriminators are trained locally to distinguish and reward the real and generated human trajectories. In the training process, only the generated trajectories and their rewards obtained based on personal discriminators are shared between the server and devices, whose privacy is further preserved by our proposed perturbation mechanisms with theoretical proof to satisfy differential privacy. Further, to better model the human decision-making process, we propose a novel aggregation mechanism of the rewards obtained from personal discriminators. We theoretically prove that under the reward obtained based on the aggregation mechanism, our proposed model maximizes the lower bound of the discounted total rewards of users. Extensive experiments show that the trajectories generated by our model are able to resemble real-world trajectories in terms of five key statistical metrics, outperforming state-of-the-art algorithms by over 48.03%. Furthermore, we demonstrate that the synthetic trajectories are able to efficiently support practical applications, including mobility prediction and location recommendation.", "sections": [{"title": "Introduction", "content": "Human mobility trajectory data is instrumental for a large number of applications. For example, for the mobile Internet service providers, based on mobility trajectories, the movement and communication process of mobile users can be simulated to implement a reliable and efficient performance evaluation of the mobile networks [16]. For the government, mobility trajectories can characterize the travel demand of the population and the traffic condition of the city, and thus provide important guidance to the transportation system planning [8]. However, the utilization of real-world mobility trajectories leads to a growing privacy concern,"}, {"title": "Mathematical Model and System Overview", "content": "For the sake of convenience, we summarize the notations used in this paper in Table A1 of the Appendix. Specifically, we consider the scenario where there are multiple users with their own mobile devices. Each device has recorded the historical mobility trajectory of the corresponding user. We define the set of users as U. Further, for each user $u \\in U$, we define the mobility trajectory of u as a sequence of spatio-temporal points, i.e., $T_u = \\{(t_1,l_1), (t_2, l_2), ..., (t_v,l_n)\\}$, where $l_i$ is the identifier of the visited location and $t_i$ is the corresponding timestamp. Then, the human mobility generation problem can be defined as follows.\nDefinition 1 (Privacy-Preserving Federated Mobility Trajectory Generation Problem) Given a set of user U and their historical mobility trajectory ${\\{T_u\\}}_{u\\in U}$, the goal of this problem is to train a mobility trajectory generator distributedly, which is able to generate mobility trajectory with the preserved utility. In addition, the privacy information involved in the trajectory of each user should be preserved.\nSpecifically, the preserved utility indicates that the generated trajectories should statistically resemble the real-world trajectories. Furthermore, they should be able to effectively support the downstream applications relying on trajectory data. On the other hand, the preserved privacy indicates that any pieces of the users' private trajectories should neither be taken away from their own devices, nor be inferred from the uploaded intermediate results of the user devices."}, {"title": "System Overview", "content": "We propose a privacy-preserving federated imitation learning system to solve this problem, of which the framework is shown in Figure 2."}, {"title": "Method", "content": "In order to model the substantial human decision-making process to simulate the human mobility behavior, we utilize the powerful technique of the generative adversary imitation learning (GAIL) under the model of Markov decision processes (MDPs). Specifically, the MDP model is defined by a 4-tuple < S, A, P, R >, where S is the state space, A is the action space, $P : S\\times A\\times S \\rightarrow R^+$ represents the state transition probability, and R: S\\times A\\rightarrow R represents the reward function. Specifically, we define the state of users as the set of their historical spatio-temporal points, i.e., $S_t = \\{(t_r,l_r)\\}_{< t}$, and the action space is defined based on the widely-adopted exploration and preferential return (EPR) model [20, 34], which includes four actions composed of stay, home return, preferential return, and explore. Then, the state transition probability, which defines the probability distribution of the next state given the current state and action, is also defined based on the EPR model (see Appendix for details). Then, each user is regarded as an \"agent\" who dynamically determines the action to be executed based on its current state through its policy function, and the goal of the agent is to maximize the discounted total rewards $E_t[\\gamma^tR(s_t, a_t)]$, where $\\gamma \\leq 1$ is the discount factor. In the imitation learning problem, the reward function R as well as the policy function are unknown and thus need to be learned from the real-world data. Thus, in the following part of this section, we first introduce the utilized policy function and reward function. Then, we introduce how to train our proposed system based on GAIL. Finally, we analyze the system in terms of its theoretical privacy-preserving performance."}, {"title": "Policy Function", "content": "The policy function defines the decision strategy taken by users, which takes the current state $s_t$ as the input and then outputs the action $a_t$ to be executed. We follow the common settings adopted in most imitation learning problems, and consider the stochastic policy rather than the deterministic policy. In this case, the policy function actually gives the probabilistic distribution of executing arbitrary action $a_t \\in A$. Specifically, we utilize a self-attention transformer [36] parameterized by $\\theta$ to model the policy function, which is denoted by $\\pi_{\\theta}(a_t | s_t)$.\nCombining the policy function $\\pi(a_t|s_t)$ and the state transition probability function $P(s_{t+1}|s_t, a_t)$, each agent is able to dynamically determine which action to be executed and then change the current state from $s_t$ to $s_{t+1}$ via interaction with $P(s_{t+1}|s_t, a_t)$. By repeating this process, the agent is able to sample synthetic trajectories corresponding to the policy net $\\pi_{\\theta}$. Thus, the policy function acts as the trajectory generator in our system.\nIn our system, we only utilize a global policy network, which is trained on the server. Further, in the training process of our system, the policy network only interacts with the private user trajectories through the reward function. By designing a privacy-preserving reward function, we are able to obtain a policy network without privacy leakage. Then, the server is able to send the parameter of the global policy network to the devices, and thus the devices also have the ability to generate synthetic human trajectories. Specifically, the policy function is optimized to imitate real-world human trajectories, of which the degree is measured by the reward function introduced in the following section."}, {"title": "Reward Function", "content": "The reward function measures to what degree arbitrary given trajectories imitate real trajectories. Specifically, it takes a state-action pair $(s_t, a_t)$ as the input, and outputs a real number, where higher values indicate that the state-action pair better imitates real-world human decisions.\nIn the standard GAIL, a discriminator network is utilized to model the reward function, which is trained based on the positive samples of real-world human state-action pairs and the negative samples of synthetic state-action pairs. However, in order to protect user privacy, the private trajectory data stored on mobile devices cannot be gathered together to train the discriminator. Thus, in our system, we replace it with a number of separate personal discriminators of users and a private aggregation mechanism, of which the idea is inspired by the techniques of Private Aggregation of Teacher Ensembles (PATE) [29, 21]. Specifically, each user device trains its personal discriminator based on its private trajectory data, and the final utilized reward function comes from the private aggregation of their ensemble. Note that the personal discriminators play a similar role with the teacher models in the standard PATE and PATE-GAN model [29, 21]. However, different from them, there is no student discriminator trained in our designed system. Instead, the aggregated reward is utilized to directly update the quality function or the advantage function in reinforcement learning algorithms such as A2C [25] and PPO [31]."}, {"title": "Personal Discriminators", "content": "The discriminator takes the state-action pair as input and then outputs its plausibility. The personal discriminator plays a similar role, but it can only access the trajectory data belonging to its corresponding user and the synthetic trajectories generated based on the global policy network. Specifically, we denote the personal discriminator belonging to user u as $D_{\\phi_u}$, which is parameterized by $\\phi_u$. Then, $D_{\\phi_u}$ is optimized based on the following loss function:\n$C(\\Phi_u) = \u2013\\mathbb{E}_{\\pi_{T_u}}[logD_{\\phi_u}(s, a)] \u2013 \\mathbb{E}_{\\pi}[log(1\u2212 D_{\\phi_u}(s, a))]$, (1)\nwhere $\\mathbb{E}_{\\pi}$ represents the expectation with respect to the trajectories sampled based on the policy $\\pi$. Specifically, for an arbitrary function $f :S\\times A \\rightarrow R$, we have $\\mathbb{E}_{\\pi}[f(s,a)] = \\mathbb{E}[\\sum_{i=1}^N f(s_i, a_i)]$, where $a_i \\sim \\pi(\\cdot|s_i)$ and $s_{i+1} \\sim P(\\cdot|s_i, a_i)$. In addition, $\\mathbb{E}_{\\pi_{T_u}}$ represents the expectation in terms of the state-action pairs obtained from the real-world trajectory of user u."}, {"title": "Private Aggregation Mechanism", "content": "The trajectory data on each user device is insufficient and largely influenced by the user personality, which prevents the discriminator from capturing the principle plausibility of state-action pairs. Thus, it is necessary to incorporate the plausibility estimated by all personal discriminators.\nFormally, for arbitrary state-action pair $(s, a)$, each mobile device u estimates its plausibility based on the local personal discriminator $D_u (s, a)$, which is then uploaded to the server. The server computes the average value of the obtained personal rewards and then adds a perturbation to it, of which the process can be expressed as follows:\n$R(s,a) = \\frac{1}{|U|} \\sum_{u\\in U} D_u(s, a) + Laplace(0, \\lambda)$. (2)\nNote that this process from uploading $D_u (s, a)$ to calculating (2) can be protected by homomorphic encryption techniques. Specifically, the server is in charge of generating public keys and sending them to all devices. Each device u then encrypts $D_{\\phi_u}(s, a)$ and sends it to another third-party server, which is in charge of implementing the computation of (2) and sending the results to the server in charge of training the trajectory generator.\nThe third-party server can only influence the performance of the trained trajectory generator and no user privacy can be leaked from it. To order to further prevent malicious third-party servers, we can randomly select a client to be the third-party server in each communication round. It is also possible for the malicious third-party server to incorrectly calculate (2), which can be solved by introducing verification information to the uploaded message $D_u (s, a)$ of the clients.\nAnother difference of the aggregation mechanism (2) from standard PATE is that we compute the average value of"}, {"title": "Reward Dynamics Compensation Mechanism", "content": "The above ensemble learning based method will introduce extra noise to the obtained reward function. Specifically, users' mobility behavior has intrinsic stochasticity. Furthermore, there also exist personal differences between the reward functions of different users. Since each personal discriminator can only observe the trajectory of a single user, it is more affected by stochasticity and personal differences, which might reduce the performance.\nIn order to eliminate the influence introduced by the distributed training method, we propose a reward dynamics compensation mechanism. Specifically, it models the reward dynamics actively based on the variance of the obtained rewards from the personal discriminators, which is further incorporated into the reward function to derive the lower bound of obtained reward. The process can be formally expressed by the following equations:\n$\\begin{cases}\\xi(s,a) = \\sqrt{ var(D_u (s, a))} + Laplace(0, \\lambda_c), \\\\R(s, a) = R(s, a) \u2013 \\beta\\xi(s, a), \\end{cases}$ (3)\nwhere $var(X)$ is the variance of the stochastic variable X, and $\\beta$ is a hyper-parameter to adjust the influence of the reward dynamics compensation mechanism. Intuitively, $\\tilde{R}$ can be regarded as the lower bound of the personal rewards of users, which is formally described in the following theorem.\nTheorem 1 Denote the discounted total reward based on the policy function $\\pi$ and reward function R as $J(\\pi, R) = \\sum_i \\gamma^iR(s_i, a_i)$, where $a_i \\sim \\pi(\\cdot|s_i)$ and $s_{i+1} \\sim P(\\cdot|s_i, a_i)$. Let $R_u$ denote the personal reward function of user u, i.e., $R_u = D_u$. Then, for a randomly selected user u and policy function $\\pi$, we have $\\Pr(J(\\pi, R_u) \\ge J(\\pi, \\tilde{R})) \\ge 1 - \\frac{1}{\\beta^2}$."}, {"title": "Model Training", "content": "The training process of our proposed model is summarized as follows. Firstly, a batch of synthetic trajectories is sampled to train the personal discriminators belonging to different users."}, {"title": "Privacy Analysis", "content": "Before we analyze our proposed system in terms of its theoretical privacy-preserving performance, we first provide preliminaries about differential privacy, which is the most widely-used privacy preserving criterion.\nDefinition 2 (($\\epsilon$, $\\delta$)-differential privacy) A randomized mechanism $\\mathcal{M} : D \\rightarrow O$ satisfies ($\\epsilon$, $\\delta$)-differential privacy if and only if, for arbitrary adjacent datasets $D_1$ and $D_2$, and subset $\\mathcal{O} \\subseteq O$, we have $\\Pr(\\mathcal{M}(D_1) \\in \\mathcal{O}) \\le e^{\\epsilon} \\Pr(\\mathcal{M}(D_2) \\in \\mathcal{O}) + \\delta$.\nThen, based on the above definition, we will further examine the theoretical privacy-preserving performance of our system by investigating how to set the value of the parameters of the adding noise in (2) and (3) to achieve ($\\epsilon$, $\\delta$)-differential privacy. Overall, our obtained results can be summarized in the following theorems.\nTheorem 2 By setting $\\lambda = \\frac{3\\kappa}{|U|}$, the random mechanism (2) satisfy ($\\epsilon$, 0)-differential private."}, {"title": "Datasets", "content": "We utilize two trajectory datasets to evaluate the performance of our proposed algorithm, which includes a publicly available dataset from previous work and a large-scale dataset obtained from an Internet service provider (ISP).\nISP Dataset. This dataset is provided by an Internet service provider (ISP), which records over 100,000 mobile users' access logs to different cellular base stations covering the duration of one week. Users' locations are obtained based on their accessed cellular base stations, while the timestamps of the access logs are also recorded, together composing the spatio-temporal mobility trajectories of the users.\nGeoLife Dataset. This dataset is collected by [45], which contains the mobility trajectories of 178 users, of which the duration is from April 2007 to October 2011. Users' locations are obtained from the GPS logs of their mobile phones, with each record containing the latitude, longitude, and timestamp."}, {"title": "Experimental Settings", "content": "Compared Algorithms. In order to have a reliable evaluation of our proposed algorithm, we select the following state-of-the-art trajectory generation algorithms to be compared with: (1) IO-HMM [40] modifies the hidden Markov model to incorporate external context information, where the home and work locations of users are used as context information to generate synthetic trajectories. (2) TimeGeo [20] is a rule-based probabilistic model based on the classical exploration and preferential return (EPR) model. (4) GAN [13] utilizes the GAN model to directly generate trajectories,"}, {"title": "Statistical Evaluation Results", "content": "We evaluate the performance of different algorithms in terms of statistical metrics. For fairness, in this group of experiments, no perturbation is added to our proposed algorithm. As the results shown in Table ??, our model beats the baselines in most situations. Compared to the best baseline, our method can obtain a significant performance gap in most metrics. In addition, we can observe that the JSD of most metrics of all algorithms on the GeoLife data is larger than those on the ISP dataset, indicating worse performance. The reason is that the GeoLife dataset is sparser and with a smaller scale than the ISP dataset, leading to the difficulty of capturing the complex temporal and spatial features by only relying on the limited number of trajectories."}, {"title": "Statistical Distribution Visualization", "content": "In Figure 3, we compare the distribution of trajectory data generated from our proposed model and MoveSim on the ISP dataset in terms of the selected statistics metrics. We can observe that the distributions of trajectory data generated by our model are closer to the distribution of real data compared with MoveSim. We also present the corresponding experimental results of the GeoLife dataset in the Appendix."}, {"title": "Practical Demonstrations", "content": "Due to privacy concerns and the collection cost, the available real trajectories are usually limited, which has become the bottleneck of performance of the machine learning models of the downstream applications. In this group of experiments, we examine whether the synthetic trajectory data can help to solve the problem of data scarcity in terms of realistic downstream applications, which include mobility prediction and location recommendation."}, {"title": "Mobility Prediction", "content": "In this experiment, we examine whether the synthetic trajectories can help to train a better mobility prediction model. Specifically, the real trajectories combined with synthetic trajectories are used to train an LSTM mobility prediction model [7], which is then validated on another group of real trajectories as the test set. We consider three different scenarios which only use real-world trajectory data, use real-world trajectory data and synthetic data generated by MoveSim, and use real-world trajectory data and synthetic data generated by our proposed algorithms as the training set, respectively. We select MoveSim since it is the best baseline in the previous analysis. As we can observe from Figure 4, the mobility prediction performance has a significant improvement when adding the generated trajectories, and the improvement of adding trajectories generated by our proposed algorithm is significantly"}, {"title": "Privacy Risk Analysis", "content": "To evaluate the privacy-preserving performance of our proposed algorithms, we further conduct two experiments in terms of actual privacy attacks. The first attack is the membership inference attack (MIA) [33, 26]. Specifically, we consider the white-box inference attack. Given a set of trajectories $T_A$ as the target of attacks, for each trajectory $T_u \\in T_A$, the adversary calculates the reward of each state-action pair in $T_u$, and then uses them as the feature of a Random Forest classifier to infer whether $T_u$ is included in the training dataset of the obtained trajectory generation model. The same number of real-world trajectories used for training and not used for training the trajectory generation model are sampled as the positive samples and negative samples, respectively. Then, a five-fold cross validation is implemented to evaluate the privacy risk in terms of MIA. The"}, {"title": "Related Work", "content": "Human Trajectory Generators. Human trajectory generation models have been investigated for decades. Early approaches mainly utilize classical probabilistic methods or rule-based methods to model and generate human trajectories [20, 19, 40, 3, 44, 30]. However, these methods are derived from strong assumptions of human mobility, and whether these assumptions hold true in reality is questionable. In addition, these assumptions also limit a small number of parameters describing the human mobility process, leading to their weakness in terms of modeling the complicated relationship of high-dimensional mobility trajectories. In recent years, more deep learning based human trajectory generators have been proposed by utilizing variational autoencoder (VAE) [18], generative adversarial network (GAN) [9, 27, 22, 23, 1], imitation learning [28, 43, 38, 42, 32, 24]. However, none of them considers the critical privacy problem, indicating that there exist privacy leakage risks of the trajectory data utilized to train these models. Different from them, in this paper, we address the privacy-preserving issue, and propose a federated mobility generator by utilizing the techniques of imitation learning.\nImitation Learning. The goal of imitation learning is to learn the policy function, which gives the action to be executed based on the current state [2, 4, 47, 46, 47, 17]. The most successful imitation learning method is generative adversarial imitation learning (GAIL), which utilizes the non-linear neural network to model the reward function"}, {"title": "Proofs of the Theorems", "content": "Theorem 1 Denote the discounted total reward based on the policy function \u03c0 and reward function R as $J(\\pi, R) = \\sum_i \\gamma^iR(s_i, a_i)$, where $a_i \\sim \\pi(\\cdot|s_i)$ and $s_{i+1} \\sim P(\\cdot|s_i, a_i)$. Let Ru denote the personal reward function of user u, i.e., Ru = Du. Then, for a randomly selected user u and policy function \u03c0, we have $\\Pr(J(\\pi, R_u) \\ge J(\\pi, \\tilde{R})) \\ge 1 - \\frac{1}{\\beta^2}$.\nProof: First, based on Chebyshev's inequality, we have:\n$\\Pr(\\vert J(\\pi, R_u) - \\frac{1}{|U|} \\sum_{u \\in U} J(\\pi, R_u)\\vert \\ge \\beta \\sum_i \\gamma^i \\xi(s_i, a_i)) \\le \\frac{var(J(\\pi, R_u))}{(\\beta \\sum_i \\gamma^i \\xi(s_i, a_i))^2}$ (5)\nThen, in terms of J(\u03c0, Ru), since the policy function \u03c0 has been given, conditioned on \u03c0 the sampled station-action pair is independent with the reward function. Thus, we have $R_u(s_i, a_i)$ is independent with $R_u(s_j, a_j)$ for i \u2260 j. Then, we have:\n$\\begin{aligned} var(J(\\pi, R_u)) &= \\mathbb{E} [(\\frac{1}{|U|}\\sum_i \\gamma^i (R_u(s_i, a_i) - \\mathbb{E} R_u(s_i, a_i)))^2] \\\\&= \\frac{1}{|U|^2} \\sum_{u \\in U} \\sum_{v \\in U} \\sum_i \\sum_j \\gamma^i \\gamma^j (R_u(s_i, a_i) - \\mathbb{E} R_u(s_i, a_i))(R_v(s_j, a_j) - \\mathbb{E} R_v(s_j, a_j)) \\\\&= \\frac{1}{|U|^2} \\sum_i \\gamma^{2i} \\xi^2(s_i, a_i) \\end{aligned}$ (6)\nSince \u03be(si, ai) \u2265 0 for all state-action pair (si, ai), we have $\\sum_i \\gamma^{2i} \\xi^2(s_i, a_i) \\le (\\sum_i \\gamma^i \\xi(s_i, a_i))^2$. Then, based on (5), we have:\n$\\Pr(\\vert J(\\pi, R_u) - \\frac{1}{|U|} \\sum_{u \\in U} J(\\pi, R_u)\\vert \\ge \\beta \\sum_i \\gamma^i \\xi(s_i, a_i)) \\le \\frac{1}{\\beta^2}$ (7)\nAt the same time, we have\n$J(\\pi, \\tilde{R}) = \\frac{1}{|U|} \\sum_{u \\in U} J(\\pi, R_u) - \\beta \\sum_i \\gamma^i \\xi(s_i, a_i)$, (8)\nwhich proves our theorem."}, {"title": "Action Space and State Transition Probability", "content": "The action space A in our system includes four actions, i.e., stay, home return, preferential return, and explore. Specifically, stay indicates that the user will not move to other locations in this time slot. Home return indicates that the user will go home in the next time slot. Preferential return indicates that the user will go to a previously visited location other than his/her home, while explore indicates that the user will go to a new location.\nAs for the state transition probability, the next states corresponding to the actions of stay and home return are deterministic. As for the next state corresponding to the action of preferential return, the newly visited location will be chosen based on the user's historical visitation frequency to this location. As for the next state corresponding to the action of preferential return, each location l will be visited with the probability in proportion to $rank(l)^{-b}$, where rank(l) is the rank of the location l based on its distance from the user's current location [20]."}]}