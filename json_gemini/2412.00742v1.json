{"title": "Revisiting Self-Supervised Heterogeneous Graph Learning from Spectral Clustering Perspective", "authors": ["Yujie Mo", "Zhihe Lu", "Runpeng Yu", "Xiaofeng Zhu", "Xinchao Wang"], "abstract": "Self-supervised heterogeneous graph learning (SHGL) has shown promising potential in diverse scenarios. However, while existing SHGL methods share a similar essential with clustering approaches, they encounter two significant limitations: (i) noise in graph structures is often introduced during the message-passing process to weaken node representations, and (ii) cluster-level information may be inadequately captured and leveraged, diminishing the performance in downstream tasks. In this paper, we address these limitations by theoretically revisiting SHGL from the spectral clustering perspective and introducing a novel framework enhanced by rank and dual consistency constraints. Specifically, our framework incorporates a rank-constrained spectral clustering method that refines the affinity matrix to exclude noise effectively. Additionally, we integrate node-level and cluster-level consistency constraints that concurrently capture invariant and clustering information to facilitate learning in downstream tasks. We theoretically demonstrate that the learned representations are divided into distinct partitions based on the number of classes and exhibit enhanced generalization ability across tasks. Experimental results affirm the superiority of our method, showcasing remarkable improvements in several downstream tasks compared to existing methods.", "sections": [{"title": "1 Introduction", "content": "Self-supervised heterogeneous graph learning (SHGL) aims to effectively process diverse types of nodes and edges in the heterogeneous graph, producing low-dimensional representations without the need for human annotations [72, 71, 25]. Thanks to its remarkable capabilities, SHGL has attracted significant interest and has been utilized in a broad array of applications, including recommendation systems [44, 12], social network analysis [45, 9], and molecule design [68, 59].\nExisting SHGL methods can be broadly classified into two groups, i.e., meta-path-based methods and adaptive-graph-based methods. Meta-path-based methods typically utilize pre-defined meta-paths to explore relationships among nodes that may share the same label in the heterogeneous graph [18, 74]. However, building meta-paths requires extensive prior knowledge and incurs additional computation costs [69]. To address these drawbacks, adaptive-graph-based methods dynamically assign significant weights to node pairs likely to share the same label, using the adaptive graph structure rather than traditional meta-paths [30]. Both groups of SHGL methods facilitate message-passing among nodes within the same class, either through meta-path-based graphs or adaptive graph structures. As a result, this process minimizes intra-class differences and promotes a clustered pattern in the learned representations, aligning these methods closely with conventional clustering techniques."}, {"title": "2 Method", "content": "Notations. Let G = (V,E,X,T, R) represent a heterogeneous graph, where V and & indicate set of nodes and set of edges, respectively. X = {x}=1 denotes the matrix of node features, where n indicates the number of nodes. Moreover, Tand R indicate set of node types and set of edge types, respectively. Given the heterogeneous graph G, most existing SHGL methods utilize meta-paths or adaptive graph structures to explore connections among nodes within the same class, thus exhibiting the characteristics of clustering and obtaining discriminative representations. To gain a deeper insight of previous SHGL methods, we first propose to revisit them from a clustering perspective as follows."}, {"title": "2.1 Revisiting Previous SHGL Methods from Spectral Clustering", "content": "As mentioned above, previous SHGL methods tend to conduct clustering implicitly, relying on meta-path-based graphs or adaptive graph structures. For example, given an academic heterogeneous graph with several node types (i.e., paper, author, and subject), for the meta-path-based methods, if two papers belong to the same class, there may exist a meta-path \u201cpaper-subject-paper\" to connect them (i.e., two papers are grouped into the same subject). Similarly, for the adaptive-graph-based methods, when two papers belong to the same class, the adaptive graph structures likely assign large weights to connect them. Therefore, representations of nodes within the same class will be close to each other after the message-passing process, thus implicitly presenting a clustered pattern.\nBased on the above observation, actually, we can further theoretically understand previous SHGL methods from the clustering perspective. To do this, we first give the following definition.\nDefinition 2.1. (Spectral Clustering) Given the Laplacian matrix L, the optimization problem of the spectral clustering can be described as follows:\n$\\min_H \\text{Tr} (H^T LH) \\text{ s.t., } H^T H = I,$\\\nwhere L = D \u2013 W, W \u2208 Rn\u00d7n is a data similarity matrix, D is a diagonal matrix whose entries are column sums of W, H \u2208 Rn\u00d7d is a representations matrix, Tr(\u00b7) indicates the matrix trace, and I indicates the identity matrix.\nAccording to Definition 2.1, for both meta-path-based methods [31, 57, 58] and adaptive-graph-based SHGL methods [30], we then have Theorem 2.2, whose proof can be found in Appendix C.1.\nTheorem 2.2. Assume the learned representations H are orthogonal, optimizing previous meta-path-based and adaptive-graph-based SHGL methods is equivalent to performing spectral clustering with additional regularization, i.e.,\n$\\min_H \\mathcal{L}_{SHGL} \\simeq \\min_H \\text{Tr}(H^T \\tilde{L} H) + R(H) \\text{ s.t., } H^T H = I,$\\\nwhere R(\u00b7) indicates the regularization term, $\\tilde{L}$ indicates the Laplacian matrix of the meta-path-based graph or the adaptive graph structure."}, {"title": "2.2 Rank-Constrained Spectral Clustering", "content": "Based on the connections between previous SHGL methods and the spectral clustering as well as the graph-cut algorithm, we have the observations as follows. First, according to Theorem 2.2, previous SHGL methods conduct spectral clustering based on the Laplacian matrix of meta-path-based graph or adaptive graph structure, which may not guarantee optimality and could potentially contain noisy connections, thus affecting the spectral clustering. Second, according to Theorem 2.3, previous SHGL methods conduct the graph-cut to divide the learned representations into d partitions, which are generally not equal to the number of classes c. As a result, optimizing previous SHGL methods becomes a hard or even error problem, and the learned representations can not be clustered well. Therefore, it is intuitive to mitigate noise in the adaptive graph structure as well as divide the learned representations into exactly e partitions to improve existing SHGL methods.\nSpecifically, in this paper, we propose to learn an adaptive affinity matrix with the rank constraint to mitigate noisy connections as much as possible. To do this, we first employ the Multi-Layer Perceptron (MLP) as the encoder g$ \\in \\mathbb{R}^{f\\times d_1}$ to obtain the semantic representations H by:\n$\\mathbf{H} = \\sigma(g_{\\phi}(\\mathbf{X})),$\nwhere f and d\u2081 are the dimensions of node features and representations, respectively, and o is the activation function. After that, we propose to learn an adaptive affinity matrix S \u2208 Rn\u00d7n based on the semantic representations. Intuitively, in a uncorrelated representations subspace, a smaller distance ||hi - h;||3 between semantic representations should be assigned a larger probability sij. Therefore, it is a natural approach to learn the affinity matrix S by:\n$\\min_S \\sum_{i=1}^n(\\sum_{j=1}^n(||h_i - h_j||^2s_{ij} + \\alpha s_{ij})) \\text{ s.t.,} \\forall i, \\sum_{j=1}^n s_{ij} = 1, 0 \\leq s_{ij} \\leq 1,$\nwhere a is a non-negative parameter. In Eq. (5), the first term encourages the affinity matrix to assign large weights to node pairs with small distances. Moreover, the second term avoids the trivial solution that only the nearest node can be the neighbor of vi with probability 1. However, similar to previous SHGL methods, usually the affinity matrix learned by Eq. (5) can not reach the ideal case (i.e., having no noisy connections among different classes and containing exactly c connected components). As a result, the noisy connections in the affinity matrix may induce a negative interference during the message-passing process. To solve this issue, we first introduce the following lemma in [32].\nLemma 2.4. The multiplicity c of the eigenvalue 0 of the Laplacian matrix Ls is equal to the number of connected components in the affinity matrix S.\nLemma 2.4 indicates that if the rank of Ls equals to n \u2013 c, then the affinity matrix S contains exactly c connected components to achieve the ideal scenario, where Ls = D \u2013 S+S\", and D is the degree matrix of S+S. Based on Lemma 2.4, we can solve the above issue by adding the rank constraint on the affinity matrix, i.e., enforcing the smallest c eigenvalues of Ls to be 0:\n$\\text{rank}(L_s) = n - c \\Rightarrow \\min \\sum_{i=1}^c \\tau_i (L_s),$\n\""}, {"title": "2.3 Dual Consistency Constraints", "content": "The message-passing among nodes within the same class reduces intra-class differences and enhances node representations Z. Meanwhile, the message-passing among nodes from different types also contribute to obtaining task-related contents and benefits downstream tasks [69]. To do this, we propose to aggregate the information of nodes from different types in the heterogeneous graph with a heterogeneous encoder fe \u2208 Rf\u00d7d1.\nSpecifically, for the node vi, we concatenate the information of itself and its relevant one-hop neighbors (i.e., nodes of other types) based on edge types in R, and then derive the heterogeneous representations Z by:\n$\\tilde{\\mathbf{Z}}_i = \\sigma(\\frac{1}{|R|}\\mathbf{f}_e(\\mathbf{x}_i) || \\sum_{r \\in R} \\sum_{v_j \\in N_{i,r}} \\mathbf{f}_e(\\mathbf{x}_j)),$\nwhere o is the activation function, |R| indicates the number of edge types, Nir indicates the set of one-hop neighbors of node vi based on the edge typer \u2208 R, fo(\u00b7) indicates the linear transformation, and || indicates the concatenation operation. Therefore, the heterogeneous representations Z aggregate the information of nodes from different types to introduce more task-related contents.\nGiven node representations Z and heterogeneous representations \u017d, most previous SHGL methods utilize the node-level consistency constraint (e.g., Info-NCE loss [36]) to capture the invariant information between them and enhance the effectiveness [57, 30]. In addition, according to Theorem 2.2, previous SHGL methods actually perform spectral clustering to learn node representations. However, previous SHGL methods fail to utilize the cluster-level information outputted by the spectral clustering, thus weakening the downstream task performance. To solve this issue, we design dual consistency constraints to capture the invariant information as well as the clustering information between Z and Z.\nSpecifically, we first employ a projection head qy \u2208 Rd1\u00d7d2 to map both Z and \u017d into the same latent space, i.e., Q = qy(Z) and Q = qy(Z), where d2 is the projected dimension. Then we follow previous works [57, 58] to design a node-level consistency constraint to capture the invariant information between Q and Q, i.e.,\n$\\mathcal{L}_{nc} = ||\\mathbf{Q} - \\tilde{\\mathbf{Q}}||_F^2 + \\eta \\log \\sum_{i} \\sum_{j=1}^{n} e^{\\mathcal{C}_{ij}},$"}, {"title": "3 Experiments", "content": "In this section, we conduct experiments on both heterogeneous and homogeneous graph datasets to evaluate the proposed method in terms of different downstream tasks (i.e., node classification and node clustering), compared to both heterogeneous and homogeneous graph methods. Detailed settings are shown in Appendix D, and additional results are shown in Appendix E."}, {"title": "3.1 Experimental Setup", "content": "3.1.1 Datasets\nThe used datasets include four heterogeneous graph datasets and two homogeneous graph datasets. Heterogeneous graph datasets include three academic datasets (i.e., ACM [56], DBLP [56], and Aminer [11]), and one business dataset (i.e., Yelp [27]). Homogeneous graph datasets include two sale datasets (i.e., Photo and Computers [43])."}, {"title": "3.1.2 Comparison Methods", "content": "The comparison methods include eleven heterogeneous graph methods and twelve homogeneous graph methods. The former includes two semi-supervised methods (i.e., HAN [56] and HGT [13]), one traditional unsupervised method (i.e., Mp2vec [4]), and eight self-supervised methods (i.e., DMGI [38], DMGIattn [38], HDMI [18], HeCo [57], HGCML [58], \u0421\u0420\u0406\u041c [31], HGMAE [48], and HERO [30]). The latter includes two semi-supervised methods (i.e., GCN [20] and GAT [50]), one traditional unsupervised method (i.e., DeepWalk [41]), and nine self-supervised methods, (i.e., DGI [51], GMI [40], MVGRL [8], GRACE [75], GCA [76], G-BT [2], COSTA [70], DSSL [61], and LRD [63]).\nFor a fair comparison, we follow [4, 56, 27, 28] to select meta-paths for previous meta-path-based SHGL methods. Moreover, we follow [29] to implement homogeneous graph methods on heterogeneous graph datasets by separately learning the representations of each meta-path-based graph and further concatenating them for downstream tasks. In addition, we replace the heterogeneous encoder fe with GCN to implement the proposed method on homogeneous graph datasets because there is only one node type in the homogeneous graph. Moreover, we follow previous works [76] to generate two different views for the homogeneous graph by removing edges and masking features. The code of the proposed method is released at https://github.com/YujieMo/SCHOOL."}, {"title": "3.2 Results Analysis", "content": "3.2.1 Effectiveness on Heterogeneous and Homogeneous Graph\nWe first evaluate the effectiveness of the proposed method on the heterogeneous graph datasets and report the results of node classification and node clustering in Table 1 and Appendix E, respectively. Obviously, the proposed method obtains better performance on both node classification and node clustering tasks than comparison methods.\nSpecifically, first, for the node classification task, the proposed method consistently outperforms the comparison methods by large margins. For example, the proposed method on average, improves by 1.1%, compared to the best SHGL method (i.e., HERO), on four heterogeneous graph datasets. The reason can be attributed to the fact that the proposed method adaptively learns a rank-constrained affinity matrix to mitigate noisy connections among different classes, thus reducing intra-class differences. Second, for the node clustering task, the proposed method also obtains promising improvements. For example, the proposed method on average, improves by 3.1%, compared to the best SHGL method (i.e., HGMAE), on four heterogeneous graph datasets. This demonstrates the superiority of the proposed method, which simulates the spectral clustering with the spectral loss and conducts the cluster-level consistency constraint to further utilize the clustering information. As a result, the effectiveness of the proposed method is verified on different downstream tasks."}, {"title": "4 Conclusion", "content": "In this paper, we revisited previous SHGL methods from the perspective of spectral clustering and then introduced a novel framework to alleviate existing issues. Specifically, we first proved that optimizing"}, {"title": "A Related Work", "content": "This section briefly reviews topics related to this work, including self-supervised heterogeneous graph learning in Section A.1, and spectral clustering in Section A.2."}, {"title": "A.1 Self-Supervised Heterogeneous Graph Learning", "content": "In recent years, self-supervised heterogeneous graph learning (SHGL) has emerged as a helpful technique to deal with the heterogeneous graph that consists of different types of entities without needing labeled data [56, 60, 57, 55, 39, 23, 67]. As a result, SHGL captures meaningful representations of nodes and edges, enabling better performance in downstream tasks like node classification and node clustering. Due to its powerful capability, SHGL has been applied to various real applications, such as social network analysis [22, 62, 10], and recommendation systems [44, 17, 15].\nExisting SHGL methods can be broadly classified into two groups, i.e., meta-path-based methods and adaptive-graph-based methods. In meta-path-based methods, several graphs are usually constructed based on different pre-defined meta-paths to examine diverse relationships among nodes that share similar labels [18, 74]. For example, STENCIL [74] and HDMI [18] construct meth-path-based graphs and then conduct node-level consistency constraints (e.g., contrastive loss) between node representations in different graphs. In addition, HGCML [58] and CPIM [31] propose to maximize the mutual information between node representations from different meta-path-based graphs. However, pre-defined meta-paths in these methods generally require expert knowledge and prohibitive computation costs [69]. Therefore, adaptive-graph-based methods are proposed to learn the adaptive graph structures to capture the relationships among nodes that possess the same label, instead of using meta-paths. For example, recently, HERO [30] made the first attempt to learn an adaptive self-expressive matrix to capture the homophily in the heterogeneous graph, thus avoiding meta-paths.\nAlthough existing SHGL methods (especially the adaptive-graph-based methods) have achieved impressive performance in several tasks, the learned graph structure cannot be guaranteed optimal. As a result, the learned graph structure may contain noisy connections from different classes to affect the message-passing process and weaken the discriminative information in node representations."}, {"title": "A.2 Spectral Clustering", "content": "Spectral clustering partitions data points into clusters based on a similarity matrix derived from the data [53, 47, 65]. Owing to its proficiency in identifying clusters with complex shapes and handling non-linearly separable data, spectral clustering is widely used in many scenarios [73, 24, 26, 64].\nThe spectral clustering methods can be broadly classified into two groups, i.e., traditional spectral clustering and deep spectral clustering. Traditional spectral clustering methods aim to group data points that are similar to each other while being dissimilar to points in other clusters by eigendecom-position [34, 35]. For example, CAN [34] proposes to learn the data similarity matrix and clustering structure simultaneously with the eigendecomposition. SWCAN [35] further assigns weights for different features to learn the similarity graph and partition samples into clusters simultaneously. Despite its effectiveness, traditional spectral clustering generally requires expensive computation costs, especially for large datasets. To alleviate this issue, deep spectral clustering methods have been proposed in recent years. For example, DSC [66] employs an encoder and two decoders to train the network, thus obtaining discriminative representations for clustering and implementing the cluster assignment via the neural network. DSCL [21] introduces a novel metric learning framework that leverages spectral clustering principles, thus reducing complexity to linear levels. Spectral-Net [42] proposes to learn a mapping function via the orthogonalization network to address the out-of-sample-extension and scalability problems.\nThe above methods conduct spectral clustering explicitly. Surprisingly, recent research shows that some popular self-supervised methods also implicitly conduct spectral clustering [7, 46]. For example, [7] demonstrates contrastive learning performs spectral clustering on the population augmentation graph by replacing the standard InfoNCE [36] with its proposed spectral contrastive loss. [46] demonstrates that contrastive learning with the standard InfoNCE loss is equivalent to spectral clustering on the similarity graph. Although these methods make efforts to connect previous self-supervised methods with spectral clustering, they cannot be easily transferred to SHGL. First, these methods are almost based on the augmentation graph, which assumes that different augmentations"}, {"title": "B Algorithm and Complexity Analysis", "content": "This section provides the pseudo-code of the proposed method in Section B.1, and the complexity analysis of our method in Section B.2."}, {"title": "B.1 Algorithm", "content": "Algorithm 1 The pseudo-code of the proposed method.\nInput: Heterogeneous graph G = (V, E, X, T, R), non-negative parameters \u03b2, \u03b3, \u03b7, \u03bc and \u03b4;\nOutput: Encoders go, fo;\n1: Initialize parameters;\n2: while not converge do\n3: Obtain semantic representations H with encoder g;\n4: Obtain the closed-form solution of the affinity matrix S by Eq. (10);\n5: Obtain the orthogonal cluster assignment matrix Y by Eq. (11) and Eq. (12);\n6: Conduct the spectral loss based on Y and S by Eq. (13);\n7: Obtain node representations Z by Z = SH;\n8: Obtain heterogeneous representations \u017d with encoder fe;\n9: Project node and heterogeneous representations into a latent space to obtain Q and Q;\n10: Conduct the node-level consistency constraint between Q and Q by Eq. (16);\n11: Obtain cluster representations Q by Eq. (17);\n12: Conduct the cluster-level consistency constraint between Q and Q by Eq. (18);\n13: Compute the objective function I by Eq. (19);\n14: Back-propagate I to update model weights;\n15: end while"}, {"title": "B.2 Complexity Analysis", "content": "Based on the Algorithm 1 above, we then analyze the time complexity of the proposed method. Recalling Eq. (10) in the main text:\n$\\mathbf{s_{ij} = (\\frac{1}{2\\alpha} (-\\text{di}+1)_+,}$,\nwhere dij = ||hi - hj ||2 + B ||fi \u2013 f; ||2, where H \u2208 Rn\u00d7d and F \u2208 Rn\u00d7c are semantic representa-tions and eigenvector matrix, d and c indicate number of dimensions and classes, and n indicates the number of nodes. To reduce the computation costs, the proposed method proposes to only calculate sij between node vi and its k nearest neighbors. Therefore, the time complexity of Eq. (21) is O(nk). Moreover, the proposed method proposes to replace the eigendecomposition with a projection head and orthogonalization layer to further reduce the time complexity. Specifically, the time complexity of the orthogonal process for H and Y with the QR decomposition is O(nd\u00b2) and O(nc\u00b2), respectively. The time complexity of the inversion process in Eq. (12) for H and F is O(d\u00b3) and O(c\u00b3). Moreover, the time complexity of the spectral loss is O(nkc) and the time complexity of Z = SH is O(nkd). In addition, the time complexity of node-level and cluster-level consistency constraints are O(nd\u00b2) and O(n), respectively. Therefore, the overall complexity of the proposed method is O(nd\u00b2 + nc\u00b2 + nkd + nkc + d\u00b3 + c\u00b3) in each epoch, where d\u00b2, c\u00b2 < n, thus is scaled linearly with the sample size."}, {"title": "C Proofs of Theorems", "content": "This section provides definition, detailed proofs of Theorems, and derivation process in Section 2, including the proofs of Theorem 2.2 in Section C.1, the proofs of Theorem 2.3 in Section C.2, the"}, {"title": "C.1 Proof of Theorem 2.2", "content": "Theorem C.1. (Restating Theorem 2.2 in the main text). Assume the learned representations H are orthogonal, optimizing previous meta-path-based and adaptive-graph-based SHGL methods is equivalent to performing spectral clustering with additional regularization, i.e.,\n$\\min_{H} \\mathcal{L}_{SHGL} \\simeq \\min_{H} \\text{Tr}(H\\tilde{L}H) + R(H)  \\text{ s.t., } H^TH = I,$\nwhere R() indicates the regularization term, \\tilde{L} indicates the Laplacian matrix of the meta-path-based graph or the adaptive graph structure.\nProof. First, we prove the connection between previous meta-path-based SHGL methods and spectral clustering. To do this, take a heterogeneous graph with two meta-paths as an example, we let G = {G(1) UG(2)} indicates the union of all meta-path-based graph views. Moreover, we denote the representations of previous methods before the message-passing as H (generally obtained by linear mapping from original node features). In addition, we denote the node representations of different graph views after the message-passing as Z(r), respectively, where r = 1,2, i.e.,\nz) = hi + {hj, Uj \u2208N(vi)(r)},$\nwhere N(vi)(r) indicates the one-hop neighbors of node vi in the r-th meta-path-based graph.\nBased on the node representations Z(r) of each graph, previous meta-path-based SHGL methods generally propose to extract the invariant information among node representations from different meta-path-based graphs. Here, we take the Mean Squared Error (MSE) loss as a simple example to extract the invariance and then conduct an analysis of previous meta-path-based SHGL methods. Therefore, the objective function of previous meta-path-based SHGL methods can be formulated as:\nmin ||z(1) \u2013 z(2) ||2.$\nBased on Eq. (23), we can rewrite Eq. (24) as:\nmin ||z(1) - z2) 2\n= min \u2211 ||hi + {hj, vj \u2208 N(vi)(1) } \u2013 hi \u2013 {hk, Vk \u2208 N(vi)(2)}||2\n= min \u2211 ||hi - {hk, Uk \u2208 N(vi)(2)} + {hj, vj \u2208 N(vi)(1) } \u2013 hi||2\n= min \u2211 ||hi - {hk, Uk \u2208 N(vz)(2) }||2 + ||{hj, vj \u2208 N(vz) (1) } \u2013 hi||2\n+2\u2211((hi - {hk, Uk \u2208 N (vi)(2) }) \u00b7 ({hj, vj \u2208 N (vi) (1) } \u2013 hi))\n= min \u2211\u2211 Gik ||hi \u2013 hi||2 + \u2211\u2211 Gi,j||hi \u2013 hj||2\n+2\u2211((hi - {hk, Uk \u2208 N(vi)(2)}) \u00b7 ({hj, vj \u2208 N(vi)(1) } \u2013 hi))\n= min \u2211\u2211 Gi,t||hi \u2013 hi||2 + 2\u2211(({hi - hk, Uk \u2208 N(vi)(2))\n+2\n\u00b7 ({hj, vj \u2208 N(vi)(1) } \u2013 hi))."}, {"title": "D Experimental Settings", "content": "This section provides detailed experimental settings in Section Experiments, including the description of all datasets in Section D.1, summarization of all comparison methods in Section D.2, evaluation protocol in Section D.3, model architectures and settings in Section D.4, and computing resource details in Section D.5."}, {"title": "D.1 Datasets", "content": "We use four public heterogeneous graph datasets and two public homogeneous graph datasets from various domains. Heterogeneous graph datasets include three academic datasets (i.e., ACM [56], DBLP [56], and Aminer [11]), and one business dataset (i.e., Yelp [71]). Homogeneous graph datasets include two sale datasets (i.e., Photo and Computers [43]). Table 3 summarizes the data statistics. We list the details of the datasets as follows."}, {"title": "D.2 Comparison Methods", "content": "The comparison methods include eleven heterogeneous graph methods and twelve homogeneous graph methods. Heterogeneous graph methods include Mp2vec [4], HAN [56], HGT [13], DMGI [38], DMGIattn [38], HDMI [18], HeCo [57], HGCML [58], CPIM [31], HGMAE [48], and HERO [30]. Homogeneous graph methods include GCN [20], GAT [50], DeepWalk [41], DGI [51], GMI [40], MVGRL [8], GRACE [75], GCA [76], G-BT [2], COSTA [70], DSSL [61], and LRD [63].\nThe characteristics of all methods are listed in Table 4, where \"Hetero\" and \"Homo\" indicate the methods designed for the heterogeneous graph and homogeneous graph, respectively. \u201cSemi-sup\", and \"Self-sup/unsup\" indicate that the method conducts semi-supervised learning, and self-supervised/unsupervised learning, respectively. \u201cMeta-path\u201d indicates that the method requires pre-defined meta-paths during the training process. \"Adaptive\" indicates that the method learns an adaptive graph structure instead of traditional meta-paths."}]}