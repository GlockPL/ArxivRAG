{"title": "Latent Space Score-based Diffusion Model for\nProbabilistic Multivariate Time Series Imputation", "authors": ["Guojun Liang", "Najmeh Abiri", "Atiye Sadat Hashemi", "Jens Lundstr\u00f6m", "Stefan Byttner", "Prayag Tiwari"], "abstract": "Abstract-Accurate imputation is essential for the reliability\nand success of downstream tasks. Recently, diffusion models have\nattracted great attention in this field. However, these models ne-\nglect the latent distribution in a lower-dimensional space derived\nfrom the observed data, which limits the generative capacity\nof the diffusion model. Additionally, dealing with the original\nmissing data without labels becomes particularly problematic.\nTo address these issues, we propose the Latent Space Score-\nBased Diffusion Model (LSSDM) for probabilistic multivariate\ntime series imputation. Observed values are projected onto\nlow-dimensional latent space and coarse values of the missing\ndata are reconstructed without knowing their ground truth\nvalues by this unsupervised learning approach. Finally, the\nreconstructed values are fed into a conditional diffusion model\nto obtain the precise imputed values of the time series. In\nthis way, LSSDM not only possesses the power to identify the\nlatent distribution but also seamlessly integrates the diffusion\nmodel to obtain the high-fidelity imputed values and assess the\nuncertainty of the dataset. Experimental results demonstrate\nthat LSSDM achieves superior imputation performance while\nalso providing a better explanation and uncertainty analysis\nof the imputation mechanism. The website of the code is\nhttps://github.com/gorgen2020/LSSDM_imputation.", "sections": [{"title": "I. INTRODUCTION", "content": "Multivariate time series imputation (MTSI) is a crucial\napproach for addressing missing data, and scholars have devel-\noped a range of MTSI models from diverse perspectives and\napplications [9] [16], including healthcare, transportation, etc.\nLeveraging the strengths of recurrent neural network (RNN)\narchitectures, such as GRU and LSTM, Bi-directional GRU\nare employed in Brits [20] for MTSI tasks. Some researchers\nstate that the time series owns the latent graph structure [3]\nand utilizes a Graph Neural Network (GNN) to implement\nthe imputation by its neighbor's features [15]. Grin [2] is\ndeveloped to deal with the graph structure by GNN with\nbidirectional GRU. HSPGNN [4] incorporates the physics law\ninto the GNN structure and tries to obtain the missing values\nby the physics law. With the development of the transformer\nstructure [1], attention is widely used in some imputation\nmodels [11]. SAITS [21] employs a self-supervised training\nscheme with two diagonal-masked self-attention blocks and\na weighted-combination block to impute missing data. Im-\nputation methods can be broadly categorized into two types:\ndiscriminative and generative approaches. Most discriminative\nmodels belong to supervised learning, which needs access to\nthe ground truth values for training. However, the require-\nment can not be satisfied in most real scenarios since most\ndatasets suffer from the original missing phenomenon. They\nover-rely on the ground truth of the missing values and are\neasily affected by the noise, thereby not accounting for the\nuncertainty in the imputed values. Generative Deep Learning\nMethods, based on their ability to yield varied imputations\nthat reflect the inherent uncertainty in the imputation [13],\n[14], are research hotspots in MTSI. GP-VAE [19] employs\nthe convolutional neural network (CNN) and VAE to find\nout the latent distribution by maximizing the Evidence Lower\nBound (ELBO) on the marginal likelihood. Even though it\nis an efficient, interpretable latent space, it has smoother and\npotentially less detailed outputs. The diffusion model [8] has\nattracted a lot of attention due to its high-quality output\nand better theoretical strength and robustness. CSDI [23]\nimputes the missing values based on the conditional diffusion\nprobability model, which treats different nodes as multiple\nfeatures of the time series and uses a Transformer to capture\nfeature dependencies. PriSTI [12] imputes missing values with\nthe help of the extracted conditional feature by a conditional\ndiffusion framework to calculate temporal and spatial global\ncorrelations, but this method is hard to apply to the health\ncare datasets such as electronic health records (EHRs) since\nthere are no apparent spatial global correlation and geographic\ndependency of this irregular healthcare datasets. Nonetheless,\nthese diffusion models do not consider the latent distribution\nof the dataset, which limits the further generative capability"}, {"title": "II. METHODOLOGY", "content": "A. Preliminary\n0\nAs for the MTSI problem, we denote $X \\in \\mathbb{R}^{N \\times D}$ as the input\nfeature matrix, where N is the number of sensors and D is the\nlength of the time series. For the imputation task, $M \\in \\mathbb{R}^{N \\times D}$\nis used to indicate the missing mask, where $M_{ij} = 0$ means\nthat the j-th sensor at time step i-th is missing, otherwise,\n$M_{ij} = 1$. For Facilitating subsequent discussions, $X^{o} = X \\odot M$ ( $\\odot$ is the Hadamard product) is adopted as the conditional\nobserved features while $X^{ta} = X \\odot (1 - M)$ as the missing\ntarget. Moreover, A represents the adjacent matrix if the time\nseries own the graph structure, i.e., the traffic domain.\nB. Neural network architecture\nta\nThe Framework of LSSDM is shown in Fig 1. Firstly,\nthe conditional observed values $X^{o}$ together with the coarse\nvalues of the linear interpolation $X^{ta}$ are projected onto\nthe stochastic variable of low dimension latent space $Z_{0}$\nby the graph convolutional neural network (GCN) structure.\nSecondly, the reconstructed imputed values $X^{ta}$, are calculated\nby the transformer structure in the decoder stage. Then, $X^{ta}$\nare fed to the forward diffusion process by adding the noise.\nFinally, we adopt DiffWave [25] to learn the noise in the\ndenoising process, which is conditional on the observed values\n$X^{o}$. Finally, the accurate imputed values $X^{ta}$ can be obtained\nby sampling from the standard Gaussian noise through the\nlearned denoising neural network.\nTo determine the precise imputation value and latent dis-\ntribution in the projected space, we apply Bayes' rule to\ncompute the probability distribution of the dataset, $log p(X_{o})$\nas follows:\n$log p(X_{o}) = log p(X^{o}, X^{ta}) = log p(X^{ta}|X^{o}) + log p(X^{o})$\n$=\\log \\int p(X^{ta}, X^{o}|Z_{o})p(Z_{o})dZ_{o} - \\log \\frac{p(X^{ta}|X^{o}, X^{ta})}{p(X^{ta}|X^{o}, X^{ta})}$\n$\\approx \\log \\int p(X^{ta}, X^{o}|Z_{o})p(Z_{o})dZ_{o} + \\log p(X^{ta}|X^{o}, X^{ta}).$ (1)\nThe target function can thus be decomposed into two terms.\nFor the first term, considering the graph structure, we can\nmaximize the ELBO using the Variational Graph Autoencoder\n(VGAE) algorithm. The VGAE is trained to accurately re-\nconstruct missing data from the latent representation without\naccess to the ground truth of the missing values, as it is an\nunsupervised generative model. To mitigate the sparsity of\nthe input features, during the training stage, we use linear\ninterpolation to estimate coarse values $X^{ta}$ for all missing\nvalues, including both the original and simulated missing\nvalues. The deduction is shown as follows:\n$\\log \\int P(X^{ta}, X^{o}|Z_{0})p(Z_{0})dZ_{0} \\geq E_{q_{\\phi}(Z_{0}|X^{o},X^{ta},A)}\n[\\log \\frac{p_{\\psi}(X^{ta}, X^{o}|Z_{0})p(Z_{0})}{q_{\\phi}(Z_{0}|X^{o}, X^{ta}, A)}]$\n$= E_{q_{\\phi}(Z_{0}|X^{o},X^{ta},A)}[\\log p_{\\psi}(X^{ta}, X^{o}|Z_{0})] - KL[q_{\\phi}(Z_{0}|X^{o}, X^{ta}, A)||p(Z_{0})],$(2)\nwhere $\\phi$ and $\\psi$ are the learnable parameters. we assume\nthe posterior distribution obey the Gaussian distribution\n$q_{\\phi}(Z_{0}|X^{o}, X^{ta}, A) = N(\\mu_{\\phi}, \\Sigma_{\\phi})$. Then, the posterior dis-\ntribution can be estimated by GCN model [18]:\n$H^{(l+1)} = GCN(A, H^{(l)}) = \\sigma(L H^{(l)} W^{(l)})$,(3)\nwhere the Laplacian matrix can be formulated as $L =\nD^{-\\frac{1}{2}}(I-A)D^{-\\frac{1}{2}}$, where I is the identity matrix, which means\nadding self-connections, D is the degree matrix. where $H^{(l)} \\in\n\\mathbb{R}^{N \\times F}$ is the input of l layer, while $H^{(l+1)} \\in \\mathbb{R}^{N \\times E}$ is output\nof l layer with E embedding. In addition, $w^{(l)} \\in \\mathbb{R}^{F \\times E}$ is a\nlearnable parameter, and $\\sigma$ represents the nonlinear activation\nfunction. In this study, we adopt the 2-layer GCN as the\nencoder framework. The mean and variance share the first-\nlayer parameters but are independent in the second layer. Thus,\nthe output of the latent variable can be expressed as:\n$\\mu_{\\phi} = GCN(X, A) = Sigmoid(L(ReLU (LXW^{(1)}))W^{(2)})$,\n$\\log \\Sigma_{\\phi} = GCN(X, A) = Sigmoid(L(ReLU (LXW^{(1)})))$. (4)\nThe latent space distribution is obtained through the GCN.\nDuring the reconstruction stage, we utilize a combination of\na Transformer and 1D CNN to reconstruct the observed data,\nrepresented as $f_{\\psi}$ as follows:\n$X^{o} = f_{\\psi}(Z_{0}) = CNN(CNN(transformer(Z_{0})))$. (5)"}, {"title": "III. EXPERIMENTS AND RESULTS", "content": "To testify the performance of our model, PhysioNet 2012\n[5], AQI-36 [22], and PeMS-BAY [24] datasets are used.\nAQI-36 is a benchmark for the time series imputation from\nthe Urban Computing project in China. PM2.5 pollutant is\ncollected hourly from May 2014 to April 2015 with various\nmissing patterns and with 24.6% originally missing rate. Four\nmonths (March, June, September, and December) are used\nas the test set without overlapping with the training dataset.\nPhysioNet 2012 (P12) is a healthcare dataset from the ICU\nthat is 48 hours long with 4000 patients and 35 variables.\nPeMS-BAY is a famous traffic flow time series dataset, which\nwas collected with 325 road sensors in the San Francisco Bay\nArea for nearly six months and re-sampled into 5 minutes\nby previous work [24]. For the P12 and PeMS-BAY datasets,\n70% of the total time series is used for training, while 10% for\nvalidation and 20% for evaluation. For AQI-36, we used the\nsame mask as [2]. As for the PeMS-BAY dataset, we adopt\nblock missing and points missing masks as [2] while P12 only\npoint missing as [23].\nA. Experimental setting\nThe epoch and batch size are 200 and 16, respectively.\nResidual connection is applied, and the learning rate is 0.0001.\nThe attention heads of the transformer adopt 8. For the\ndiffusion stage, we adopt the same hyperparameters setting as\nCSDI [23]. As for the P12 dataset, we use the Identity matrix\nas the Laplacian matrix since there is no pre-defined graph\nin this dataset. Notably, to highly reproduce real application\nscenarios, the simulated missing values are not allowed to\ntake part in the training stage and are only used as the final\nevaluation, which is the same training and evaluation protocol\nas [2], [17] and different from [12], [23] since they allow\nit. Also, we only consider the out-of-sample scenarios, which\nmeans the training and evaluation series are disjoint sequences.\nB. Result\nThe results of different datasets with different missing\npatterns are shown in Tab. I. The result indicates our model\nachieves the best imputation performance in the different miss-\ning situations under different datasets of different domains,\nwhich demonstrates the generalization and performance of our\nmodel. Interestingly, reconstructing the missing values from\nthe projective low-dimension latent space can greatly improve\nthe generative capability of the diffusion model, especially in\ndealing with the originally missing data effectively by this\nunsupervised learning framework without the demand of the\nground truth values. To investigate the performance of the\nC. Uncertainty and latent state analysis\nUnlike traditional deterministic imputation, which provides\na single predicted value, probabilistic forecasts provide a\ndistribution or range of possible outcomes along with their\nassociated probabilities. CRPS [10] measures how well the\npredicted cumulative distribution function (CDF) matches the\nobserved outcome. In this study, we adopt the same measure\nas CSDI [23]. We generate 100 samples to approximate the\nprobability distribution as in the previous section. The result is\nshown in Tab. I. LSSDM outperforms the probabilistic baseline\nmodels, which better quantify the uncertainty accurately.\nTo explore the generative mechanism, 50% simulated miss-\ning and no simulated missing input on the P12 dataset into\nthe trained VAE model and compared with the latent space\ndistribution of LSSDM. The missing effect is tested, and\nwe average all dimensions of $\\mu_{\\phi}$ and $\\Sigma_{\\phi}$ to plot the final\nlatent Gaussian distributions, which are shown in Fig 3. It\ndemonstrates that the missing data can affect the latent space\nlargely and cause the deviation of the latent space, which can\nnot be neglected by the downstream task. However, LSSDM\nmatches the no-simulated missing latent distribution better\nand provides insight into the latent distribution of the dataset,\nwhich is quite beneficial for downstream tasks."}, {"title": "IV. CONCLUSION", "content": "In this study, we propose LSSDM for MSTI. LSSDM\nlearns the latent space of the dataset by GCN, and the coarse\nreconstructed missing values are obtained by Transformer and\nCNN. Then, the accurate imputation values can be achieved\nby inputting the reconstructed values into the diffusion model.\nBy leveraging this innovative framework, we can obtain high-\nfidelity imputation with a better explanation of the imputation\nmechanism. Also, this model can utilize the unsupervised\nVGAE model to deal with the originally missing data without\nthe label, which demonstrates more feasibility and practical-\nity in a real application. Experimental results on real-world\ndatasets indicate that LSSDM achieves superior imputation\nperformance and uncertainty analysis."}]}