{"title": "Towards Foundation-model-based Multiagent System\nto Accelerate AI for Social Impact", "authors": ["Yunfan Zhao", "Niclas Boehmer", "Aparna Taneja", "Milind Tambe"], "abstract": "AI for social impact (AI4SI) offers significant po-\ntential for addressing complex societal challenges\nin areas such as public health, agriculture, educa-\ntion, conservation, and public safety. However, ex-\nisting AI4SI research is often labor-intensive and\nresource-demanding, limiting its accessibility and\nscalability; the standard approach is to design a\n(base-level) system tailored to a specific AI4SI\nproblem. We propose the development of a novel\nmeta-level multi-agent system designed to accel-\nerate the development of such base-level systems,\nthereby reducing the computational cost and the\nburden on social impact domain experts and AI re-\nsearchers. Leveraging advancements in foundation\nmodels and large language models, our proposed\napproach focuses on resource allocation problems\nproviding help across the full AI4SI pipeline from\nproblem formulation over solution design to impact\nevaluation. We highlight the ethical considerations\nand challenges inherent in deploying such systems\nand emphasize the importance of a human-in-the-\nloop approach to ensure the responsible and effec-\ntive application of AI systems.", "sections": [{"title": "1 Introduction", "content": "Artificial intelligence (AI) for social impact (AI4SI), which\nfocuses on leveraging AI to address societal issues, has\ngained traction in both academia and industry Bondi et al.\n[2021]; Perrault et al. [2020]; Cowls et al. [2021]; Ji et al.\n[2024c]; Foffano et al. [2023]. With advancements in AI and\nmulti-agent systems, there is an opportunity to apply these\ntechnologies to complex problems in areas like public safety,\nwildlife conservation, and public health Kwok and others\n[2019]; Wang and Preininger [2019]; Floridi et al. [2018];\nJi et al. [2024b]. Previously, AI4SI research has been very\nlabor-intensive, as it is oftentimes necessary to develop cus-\ntomized approaches going beyond conventional methods to\naddress the challenges characteristic to these domains such as\nlow resources and noisy or scarce data. This limits the overall\nimpact of AI4SI research, as every individual effort requires\nnon-trivial time, expertise, and financial investment. We en-\nvision the formation of a new approach to AI4SI which is less\nlabor-intensive, customizable by non-experts, and can thus be\nmade more widely available. We believe promising progress\ncan be made on this vision by employing recent methodolog-\nical advancements in computer science research, tapping into\na substantial currently unleveraged potential.\nIn this paper, we will use resource allocation problems,\nwhich often arise in AI4SI domains Kruk et al. [2018]; Ayer\net al. [2019]; Perrault et al. [2020]; Wang and Preininger\n[2019]; Baltussen and Niessen [2006]; Nishtala et al. [2021];\nDeo et al. [2013] as our running example, yet our general\nideas also apply more broadly. Some examples of previ-\nously studied resource allocation problems in AI4SI include\nstrategically scheduling patrols in protected conservation ar-\neas Gatmiry et al. [2021]; Golovin et al. [2011]; Gordon et\nal. [2024]; Xu et al. [2016] and distributing scarce health-\ncare resources to optimize people's health outcomes Shaikh\n[2020]; Mizan and Taghipour [2022]; Verma et al.; Zhao et\nal. [2024a].\nWe envision a meta-level multi-agent system that helps\nus accelerate the development of base-level systems, which\ntackle specific AI4SI problems. The meta-level system\nwould help non-profits and AI researchers in social impact\ndomains leverage AI without having to invest significant\namounts of labor and resources to build a tailored base-\nlevel system from scratch. Our envisioned system lever-\nages foundation models, which are typically developed by\npretraining on available datasets, and can be used on differ-\nent downstream tasks or new challenges Bommasani et al.\n[2021]; Kenton and Toutanova [2019]; Zhao et al. [2024b];\nViswanathan et al. [2023]. Our proposed system involves:\n(i) using LLM based meta-level agents to communicate with\ndecision makers in human language to understand the prob-\nlem from the perspective of decision-makers; (ii) employing\nmeta-level agents and foundation models for AI4SI problems\nto design base-level systems for AI4SI problems; and (iii)\nusing meta-level agents for field-testing solutions to validate\ntheir impact.\nImportantly, instead of taking over the AI4SI pipeline,\nthe meta-level system will accelerate the process, im-\nprove generalization, and enable thorough evaluation,\nwith human-in-the-loop required in each part of the sys-\ntem. We will discuss and highlight major challenges and our\nvision for each phase of this system, emphasizing the multi-\nagent aspect. We also discuss ethics and fairness aspects."}, {"title": "2 Preliminaries", "content": "We formally define the key concepts of meta-level and base-\nlevel systems, which we will refer to throughout this work.\nDefinition 1 (Meta-level and base-level systems). A base-\nlevel AI4SI system is the actual deployed system that will\nsolve the problem on the ground. A meta-level system helps\nus accelerate the development of a base-level system or ac-\ncelerate its modification as needed for a new objective.\nNotably, a meta-level system does not actually solve an\nAI4SI problem. A meta-level system may involve several\nmeta-level agents, each responsible for different tasks in the\ndevelopment of the base-level system, which may interact\nwith each other. In the context of the use cases we are fo-\ncusing on in this paper, the base system itself will be a multi-\nagent system or one that models multi-agent interactions such\nas a social network. The base-level agents present in the base-\nlevel system are defined as follows:\nDefinition 2 (Base-level agents). In the base-level system, the\nbase-level agents model or serve as abstract representations\nof individuals or entities in the real-world.\nWhereas the idea of a meta-level architecture has been pro-\nposed in agent architectures, there the idea is to directly as-\nsist agents in their immediate problem solving Corkill and\nLesser [1983]; Rosenbloom et al. [1988]; Genesereth and\nSmith [1983]; Wang et al. [2020c]. In our work, the meta-\nlevel refers to deciding which agents and how many to select,\nhow to evaluate their performance, and other such tasks. An-\nother difference is that at least as conceived now, our meta-\nagents are focused on assisting humans in building base-level\nagents.\nAnother key concept we will frequently refer to is\nfoundation-model-based agents (FM agents) used in the\nmeta-level system.\nDefinition 3 (Foundation-model-based agents (FM-agents)).\nIn the meta-level system, we use meta-level agents that em-\nploy foundation models including LLMs. We will refer to\nthese agents as FM-agents\nAn AI4SI pipeline typically involves three phases Perrault\net al. [2020]; Bondi et al. [2021]:"}, {"title": "3 Formulating the Problem", "content": "In this phase, we discuss how to formulate an AI4SI prob-\nlem. Existing works require researchers and human experts in\nAI to talk to collaborating non-profit organizations to under-\nstand who makes the decisions within a problem and gather\nkey information on the agents such as demographics Wang\nand Preininger [2019]; Wang et al. [2024d]. This process is"}, {"title": "4 Designing Solution Methods", "content": "In this phase, we elaborate on how to design a solution\nmethod for AI4SI problems. Existing approaches often re-\nquire manually designing solution methods tailored to each\napplication scenario Xu et al. [2022]; Aqajari et al. [2024];\nMao et al. [2022b,a]; Wang et al. [2024e]; Xiong et al.\n[2024b]. This approach fails to easily adapt to new appli-\ncation scenarios or knowledge and data from previous ap-\nplication scenarios, motivating the development of founda-\ntion models to accelerate solution approaches for problems in\nAI4SI. Besides adaptation ability, other aspects that are of\nparticular importance when designing solution methods\nin AI4SI problems include ethics, fairness, and collabo-\nration among base-level agents. We will motivate each of\nthese aspects and propose our visions to address them.\nAdaptation is crucial in AI for social impact domains due\nto the dynamic and evolving nature of these environments.\nSocial issues are often complex and multifaceted, with prior-\nities evolving overtime Blumenstock [2018]; De-Arteaga et\nal. [2018]; Behari et al. [2024]. AI systems should adapt to\nnew information and circumstances, accommodating rapidly\nchanging needs and circumstances. Al systems in social im-\npact domains may serve diverse populations with varying\nneeds, backgrounds, and experiences, highlighting the need\nfor AI systems with strong adaptation and generalization abil-\nities that can tailor their responses to specific subpopulations,\nensuring inclusive decision making.\nA foundation model for resource allocation could accel-\nerate developing solutions for different application scenarios\nwithout incurring repeated development costs, making them\nmore affordable and accessible to low-resource communities\nBommasani et al. [2021]; Zhao et al. [2024b]. For example,\na foundation model designed to analyze medical data can be\nadapted to different diseases, health conditions, or popula-\ntions, improving health outcomes on a larger scale Li et al.\n[2023a]. Based on advances in foundation models and adap-\ntation, we propose the following vision:"}, {"title": "4.1 Ethics and Fairness", "content": "In high-stakes resource allocation scenarios like healthcare,\nauthorities frequently prioritize certain groups based on sensi-\ntive attributes, aiming to address the needs of those most dis-\nadvantaged Amon [2020]; Verma et al.. For example, govern-\nments may mandate non-discrimination based on sensitive at-\ntributes, while non-profits may prioritize low-income groups.\nGiven the importance of fairness in base-level system design\nand the solution method's tangible impact on people's lives,\nwe propose the following idea:\nVision 4. Ensure that the FM-agent recommends the design\nof a base-level system that does not discriminate against any\nsubpopulation or result in unfavorable outcomes for under-\nprivileged groups.\nWhen accelerating the design of base-level systems, the\nFM-agent should ensure fairness guarantees or fairness\nchecks are in place. This can be done by explicitly incor-\nporating fairness in designing base-level systems for social\nimpact applications Zehlike et al. [2017]; Verma et al.. How-\never, this added complexity is difficult for FM-agents to han-\ndle, due to the fact that AI may not easily understand demo-\ngraphic information available in text or abstract fairness con-\ncepts potentially based on demographics. Blindly applying\nfairness constraints or solely optimizing a fairness objective\nmay greatly compromise the overall effectiveness of the so-\nlution method. Thus, fairness concerns necessitate innovative\nstrategies to ensure that FM-agents design base-level systems\nthat balance fairness and utility."}, {"title": "5 Testing and Deployment", "content": "In this phase, we explain how to thoroughly evaluate and\ndeploy AI models for social impact domains. We use FM-\nagents to improve model testing and facilitate real-world de-\nployment.\nDeploying an Al model in real-world social impact do-\nmains without sufficient simulation studies may result in poor\ndecision-making on crucial public resources. Thus, thor-\noughly testing and evaluating AI algorithms or trained mod-\ndels is an important aspect of accelerating AI for social impact.\nBased on above, we propose the following research idea:\nVision 5. Employ FM-agents, based on LLMs, to simulate\nagents' behaviors. Here we wish to build a powerful simu-\nlator that serves as a good proxy of real-world deployment\nenvironment and can effectively evaluate the performance of\ntrained models.\nLLM based simulators have recently received great inter-\nest, and there is demonstrated success in using LLMs to sim-\nulate human behaviors in fields including education, health-\ncare, and social sciences Cheng et al. [2023]; Santurkar et al.\n[2023]; Argyle et al. [2023]; Xu et al. [2024a]; Markel et al.\n[2023]. To build such a LLM simulator / evaluator for AI4SI\nproblems, we should represent observations and possible de-\ncisions in a way that the LLM can understand, and poten-\ntially use textual descriptions combined with structured data,\nto help the LLM simulator generate contextually appropri-\nate (e.g. suitable for the domain) individual behaviors. Fur-\nthermore, textual descriptions on individual's characteristics,\nsuch as demographic information (age, gender, geographical\nlocation, etc) may help LLMs to better understand how indi-\nviduals' condition would evolve over time. For a particular\nAI4SI problem, we may need to finetune LLMs on historical\ndata collected to better simulate the individual's trajectories.\nHaving discussed evaluation before real-world deploy-\nment, we now move on to challenges in the deployment. Dur-\ning the deployment of AI models, there can be shifts in the\nuser base or shifts in people's behaviors Elmachtoub et al.\n[2023a]. Different from adaptation ability taken into account\nduring model development and training, distribution shifts in\ntesting can be unexpected and the need to handle these shifts\ncan be urgent. This brings the next research idea:\nVision 6. Have an FM-agent that could (i) involve human-in-\nthe-loop and implement real-time monitoring to track model\nperformance and detect shifts in user behavior or data dis-\ntribution; (ii) use feedback from either human or AI to adjust\nthe model (e.g. enhance model fairness when there are unex-\npected distribution shifts).\nSpecifically, we may use a feedback loop to gather data on\nmodel predictions and user interactions, allowing for prompt\ndetection of shifts in behavior Behari et al. [2024]; Xiong\net al. [2024a]. Once substantial shifts in user behaviors are\ndetected, we could then involve human experts, potentially\nfrom partnering non-profit organizations, to review and pro-\nvide feedback on model predictions and decisions. This feed-\nback can then be used to guide model adjustments and im-\nprove its response to distribution changes. We may retrain\nor finetune the model using newly collected data, potentially\nweighting recent data more to better align with current trends\nand user behavior Bommasani et al. [2021]; Choromanski et\nal. [2023]; Peng and Cao [2024]; Kong et al. [2024]; Zhao et\nal. [a]."}]}