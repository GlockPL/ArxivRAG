{"title": "Natural Language Outlines for Code: Literate Programming in the LLM Era", "authors": ["Kensen Shi", "Deniz Alt\u0131nb\u00fcken", "Saswat Anand", "Mihai Christodorescu", "Katja Gr\u00fcnwedel", "Alexa Koenings", "Sai Naidu", "Anurag Pathak", "Marc Rasi", "Fredde Ribeiro", "Brandon Ruffin", "Siddhant Sanyam", "Maxim Tabachnyk", "Sara Toth", "Roy Tu", "Tobias Welp", "Pengcheng Yin", "Manzil Zaheer", "Satish Chandra", "Charles Sutton"], "abstract": "We propose using natural language outlines as a novel modality and interaction surface for providing AI assistance to developers throughout the software development process. An NL outline for a code function comprises multiple statements written in concise prose, which partition the code and summarize its main ideas in the style of literate programming. Crucially, we find that modern LLMs can generate accurate and high-quality NL outlines in practice. Moreover, NL outlines enable a bidirectional sync between code and NL, allowing changes in one to be automatically reflected in the other. We discuss many use cases for NL outlines: they can accelerate understanding and navigation of code and diffs, simplify code maintenance, augment code search, steer code generation, and more. We then propose and compare multiple LLM prompting techniques for generating outlines and ask professional developers to judge outline quality. Finally, we present two case studies applying NL outlines toward code review and the difficult task of malware detection.", "sections": [{"title": "I. INTRODUCTION", "content": "Outlining a complex document is a standard way to improve its organization and accessibility. A table of contents presents organizational notes separately from the document's content, but these notes may also be interleaved with the content as chapter or section headings. Modern examples of outlines include the \u201cContents\u201d sidebar on Wikipedia pages listing the section headings, and for programming, the \u201cOutline view\" in VS Code and the \u201cSymbols pane\u201d on GitHub list the classes and functions in a code file. These outlines also assist navigation by jumping to the corresponding part of the file when clicked. Advances in large language models (LLMs) such as Gemini [1], [2] and GPT-4 [3] raise new opportunities to automatically generate outlines of software artifacts in reimagined ways. After all, large software projects are incredibly complex, with one study finding that developers spend 70% of their time on program comprehension [4].\nSummarization is another way to accelerate understanding of a document. Scientific papers have an abstract, and business documents may have an executive summary, to help readers quickly understand the key points without reading the entire text. Even if one does read the entire text, reading the summary beforehand may still improve understanding [5], [6]. Recent works apply machine learning and LLMs toward automatic text summarization [7] and code summarization [8]-[14], even considering many styles of summaries for different purposes and audiences [15]-[18]. For example, upon straightforwardly asking Gemini or ChatGPT to summarize a piece of code, one usually receives long paragraphs explaining each aspect of the code in detail, often totaling more text than the code itself. Such explanations are useful for learning but are not optimized for helping an experienced developer work more quickly. Instead, we desire a concise summary that efficiently conveys the key ideas, with clear alignment between the summary and the code.\nAnother common way to improve the clarity of code is to incorporate natural language (NL). Writing docstrings, comments, helpful error messages, and descriptive identifier names are well-known aspects of good coding style [19]. In computational notebooks or other forms of literate programming [20], code and explanatory NL are interleaved in a file that is both executable and self-documenting. NL is generally easier for humans to read than code, so well-written NL can help developers more efficiently understand the associated code.\nUsing these themes of outlining, summarization, and literate programming, we propose using LLMs to generate a new form of code explanation: natural language outlines (NL outlines). An NL outline consists of natural language statements that partition the code into logical sections, while summarizing each section of code with concise efficient wording. The outline may be shown interleaved with the code or displayed separately, depending on the application. NL outlines are a particularly appealing modality for AI developer assistance due to their broad applicability: in addition to helping developers understand and navigate code, Fig. 1 illustrates a vision of how NL outlines can be used throughout the software development process for code generation and maintenance, code search, and code review.\nContributions: We introduce NL outlines (Section II), describe their extensive use cases (Section III), and propose methods for generating them with LLMs (Section IV). We perform a developer study using real code (Section V), finding that 60% of outlines were rated by professionals as excellent overall (90% were acceptable or better). In two case studies we apply NL outlines toward mobile app security and code review (Section VI), finding for example that 26 of 30 professional reverse engineers said that outlines are very or extremely helpful"}, {"title": "II. NATURAL LANGUAGE OUTLINES", "content": "A natural language outline (NL outline) for a function or snippet of code consists of one or more statements written in concise prose, which are aligned with the code and partition it into semantically-coherent sections, such that each outline statement summarizes the section of code it aligns to.\nAs an example, Fig. 2a shows a real Python function from an author's prior project. Even though the code is self-contained and only uses common libraries, it still takes a considerable amount of time for a developer to parse the code and decipher the steps taken to achieve its goals. A docstring might help somewhat, but docstrings generally discuss the function's contract instead of its implementation strategy which may be more pertinent when editing or building upon the code.\nTo gain a high-level understanding of the implementation, one may instead read the NL outline in Fig. 2b which mirrors the code's ideas and structure, while being more concise (using about half as many characters) and easier to read (English versus code). The compact nature of the outline would also reduce scrolling when browsing through the file. If one wishes to see more details or reference the code \"source of truth,\" the outline may also be displayed as comments or annotations interleaved with the code in the style of literate programming [20], as shown in Fig. 2c. In this format, the outline provides visual structure and allows the reader to seamlessly switch between reading NL or code syntax as"}, {"title": "III. USE CASES FOR NL OUTLINES", "content": "The broad applicability of NL outlines is a key factor in their appeal as a surface for human-AI interaction. Here we provide an overview of many potential use cases as illustrated in Fig. 1: code understanding, code maintenance, and overall developer experience. A full evaluation of each use case is beyond the scope of this paper instead, Section V evaluates the common first step of generating NL outlines to begin with, and Sec- tion VI deeply explores two concrete applications in practice.\nCode understanding and navigation. Fig. 3 shows a mockup of how NL outlines could be used in an IDE. NL outlines can be shown in the list of symbols to aid whole-file understanding while being clickable for precise navigation. Outlines can also be shown in the main editor interleaved with the code, assisting with rapid code understanding and offering intuitive code folding [25]. The user experience can be enhanced through customization of outline styling, and IDE shortcuts can show or hide outlines, generate new outlines, convert between outlines and comments, and jump to adjacent outline statements. NL outlines can be similarly displayed in other developer tools for code browsing, searching, and so on.\nCode maintenance. Once the user is accustomed to using NL outlines interleaved with code as in Fig. 3, we can apply"}, {"title": "IV. OUTLINE GENERATION", "content": "We can generate NL outlines by few-shot prompting an LLM, but different prompting techniques have different benefits.\nInterleaved Generation. The most straightforward way to generate NL outlines is to provide an LLM with the code to be outlined and prompt the LLM to repeat the code with outline comments added but without other code changes. This is more effective with prompt instructions describing the purpose of NL outlines and their desirable qualities, and with few- shot examples demonstrating the task and what good NL outlines look like. We call this approach Interleaved Generation because the model predicts an outline interleaved with the code. However, a notable downside of this approach is the possibility that the model changes the code against our wishes. If the model's prediction significantly deviates from the original code, the outline might not describe the original code anymore. Some minor deviations (e.g., changes to blank lines and existing comments are common) can be ignored if the model gets back on track, but in extreme cases the outline must be discarded.\nThis issue can be remedied with the Constrained Generation approach, using constrained decoding [34], [35] to alter token probabilities in a way that prevents changing the code. Specifically, we construct a regex-like constraint that repeats the code exactly, except with optional comment lines between code lines. Appendix D discusses heuristics that help ensure good placement of outline statements and implementation difficulties.\nLine Number Infilling. A drawback shared by Interleaved Generation and Constrained Generation is that they spend output tokens repeating the code, leading to unnecessary cost and latency. We design a solution called Line Number Infilling: we prepend each line of the original code with its line number, and we prompt the model to output a sequence of outline statements, each containing the line number where the outline statement should be added, and the text of the statement itself. For example, the original code \"def sq(x):\\n return x**2\" is given to the model as \u201c1|def sq(x):\\n2| return x**2\u201d, and the model may respond with \u201c2 | Squares the input.\u201d (using multiple lines for multiple outline statements). This prediction task is similar to fill-in-the-middle (FiM) in language modeling [37]\u2013[39], where the LLM must predict text (like outline statements) to insert at specially-marked locations (like the line numbers), although in our case the LLM must also choose which locations to insert at. Since this response format does not involve code tokens at all, we resolve the prior problems of changing the original code or spending tokens to repeat the code. The prompt is also shorter because the few-shot examples do not repeat the code either. In practice,"}, {"title": "V. EXPERIMENTS", "content": "We investigated (a) the rate of formatting issues in LLM predictions and (b) the quality of generated outlines in several dimensions, depending on the LLM and generation technique used. First, we asked 6 professional software engineers and researchers to curate a dataset of 30 Python functions from 21 real projects, emphasizing variety in libraries used and function kinds (standalone functions, methods, constructors, unit tests, main functions, etc.). Each function had 10 to 90 lines of code (median 46.5 LOC). To mimic realistic usage, we removed existing outline-like comments in 7 functions but kept other comments (in 10 functions) including TODOs, developer notes, and comments to disable warnings.\nWe tried 5 LLMs: Gemini 1.0 Pro and Ultra [1], Gemini 1.5 Flash and Pro [2], and DeepSeek-Coder-Instruct 33B [49], all with greedy decoding. We generated outlines using Interleaved Generation and Line Number Infilling, for a total of 5\u00d72 = 10 outlines per function. We used a fixed set of 8 hand-crafted few-shot examples so that each LLM receives the same prompt for a given function and generation technique.\nBecause each LLM response is just text, we must program- matically parse the text to extract the predicted outline. During parsing, we may encounter errors in the prediction's formatting (e.g., changing the code in Interleaved Generation) or other"}, {"title": "VI. CASE STUDIES", "content": "We present deep dives into two concrete applications.\nA. Android Security\nWe consider the problem of assessing the security and privacy of Android applications (or apps). These assessments are performed routinely by app stores such as Google Play and Samsung Galaxy Store, where apps submitted by developers across the world are analyzed for compliance with policies meant to protect app users, e.g., one policy states: Apps that are deceptive, malicious, or intended to abuse or misuse any network, device, or personal data are strictly prohibited.\nDetermining whether an app is deceptive or malicious involves understanding the code and relating its functionality to the app's description, UI, and user expectations. There are millions of apps in stores and tens of thousands of new apps added per month, so security assessments must be efficient.\nTo detect malicious apps, reverse engineers (REs) use static and dynamic program analyses to flag code as potentially problematic and then confirm violations through manual review. The initial challenge is in the volume of code: many apps have millions of lines of code, thousands of classes, and complex event-driven execution paths. Furthermore, an app's source code is usually not available, so REs inspect decompiled code which is difficult to understand for several reasons. Decompiled app code is unnatural due to the compilation process performing optimizations (e.g., inlining code and changing code structure) and removing code artifacts (e.g., comments, docstrings, types, variable names, and names of inlined API constants). App code may be further transformed by minification to reduce app size [50] and obfuscation to protect intellectual property [51]. Nevertheless, REs must quickly understand the code to find policy violations a promising application for NL outlines.\nApplying NL outlines. We collected a dataset of 80 de- compiled functions from real Android apps. Half were manually curated \"suspicious\u201d functions that REs had previously flagged as using malware techniques, and we focused on including a variety of techniques and avoiding similar code. The other half were \"benign\" functions randomly sampled from 8 popular apps assumed to be malware-free, all from different companies and genres. All dataset functions have between 24 and 99 lines\nB. Code Review\nThis application is motivated by a common pain-point in code review: the difficulty in reviewing large or complex change lists (CLs). With increasing CL complexity, it becomes increasingly difficult to maintain a complete mental model of all of the changes and how they interact. This software development anti- pattern is so substantial that code reviewers may even send large CLs back to the authors to be split into smaller CLs that are individually more manageable. Reviewers actually spend much less time per line reviewing large CLs, raising potential"}, {"title": "VII. DISCUSSION", "content": "We discuss practical considerations that arise when integrat- ing NL outlines at scale, ways to improve outline generation, using outlines within LLM pipelines, and limitations.\nVerifying outlines. If integrated into tooling with feedback mechanisms, NL outlines can become a resource shared across developers. For example, thumbs up/down feedback can be collected, and with sufficient thumbs down, an outline may be hidden or regenerated. Enough thumbs up can cause an outline to become \u201cverified\" and displayed in a special way (e.g., with a checkmark) signaling that this outline has approval from other humans. Trusted developers such as code authors, code reviewers, and project leads may be given outsized power to influence outlines, e.g., unilaterally marking an outline as verified. Trusted developers may also be allowed to edit outlines to correct mistakes (with the edits shown to everyone else), increasing the quality of outlines and users' confidence in them. However, verifying NL outlines takes time and effort, which must be considered alongside the quality and confidence benefits of having verified outlines. We emphasize a philosophy"}, {"title": "VIII. RELATED WORK", "content": "Code summarization has attracted much recent attention [8]- [18]. Appendix B presents a taxonomy contrasting the many forms of code explanation. NL outlines are one such form, but instead of a standard paragraph, an outline is written as a list of statements that can be aligned to and interleaved with the code. Moreover, most methods of code summarization have not been applied to the many use cases of NL outlines (Section III).\nSome related works do consider individual use cases, e.g., Panthaplackel et al. [26] aim to update comments given code changes using a custom bidirectional GRU, but NL outlines accomplish this through LLM prompting and generalize to the reverse direction of updating code given outline changes. Liu et al. [29] present a technique for code generation from natural language where the generated code is summarized to assist the user in writing future prompts, but they do not consider applications beyond code generation. Code folding [25] can provide a quick overview of code, and NL outlines extend this idea by providing intuitive folding locations and NL summaries of the folded code. A key contribution of NL outlines is their general applicability toward all of these use cases and more.\nExcellent surveys [55]-[58] discuss LLMs applied broadly to software engineering. Many works explore integrating LLMs into IDEs, e.g., to provide coding suggestions [59]\u2013[61], chat assistance [62], [63], educational information [22], [64], and novel coding experiences [65], [66]. NL outlines offer a different approach to incorporating AI into developer tools."}, {"title": "IX. CONCLUSION", "content": "Natural language outlines are a new form of code explana- tion that abstract and summarize the main steps in a function for rapid understanding. Our experiments show that LLMs can generate accurate and helpful NL outlines in practice. We describe the huge potential to use NL outlines to assist software developers in numerous ways, and we call for further research in this space in order to realize these transformational ideas."}, {"title": "APPENDIX A AUTHOR CONTRIBUTIONS", "content": "This work was led by Kensen Shi and advised by Satish Chandra and Charles Sutton. There were three main work- streams with the following contributors (listed in the same order as in the paper's author list):\nCore NL outlines: Kensen Shi, Deniz Alt\u0131nb\u00fcken, Marc Rasi, Brandon Ruffin, Pengcheng Yin, Manzil Zaheer, Satish Chandra, Charles Sutton\nAndroid security application: Kensen Shi, Saswat Anand, Mihai Christodorescu, Roy Tu\nCode review application: Kensen Shi, Katja Gr\u00fcnwedel, Alexa Koenigs, Sai Naidu, Anurag Pathak, Fredde Ribeiro, Siddhant Sanyam, Maxim Tabachnyk, Sara Toth, Tobias Welp"}, {"title": "APPENDIX B \u03a4\u0391\u03a7\u039f\u039d\u039f\u039cY OF CODE EXPLANATION", "content": "NL outlines are a novel way of explaining code with different benefits and drawbacks compared to other types of code explanation like docstrings and tutorials. To clarify the distinctions, we can classify types of code explanations by:\n1) Topic: It may primarily explain the interface of some code, its implementation, the design decisions it required, details of APIs and concepts involved, etc.\n2) Audience: The target readers could be novice program- mers, experienced developers, maintainers of the code, users of an API, project managers, etc.\n3) Location: Explanations can occur within code like names and inline comments, in a separate document like a design document or bug report, within developer tools like IDEs or code search interfaces, in Web sites like Javadocs, ephemerally in conversation with an LLM chatbot, etc.\n4) Length: Effective explanations can be as short as a single symbol like a well chosen method name, or much longer like a sentence, paragraph, or even a book.\nNL outlines occupy a particular space within this taxonomy. In terms of topic, NL outlines focus on the implementation strategy: the high-level steps taken to achieve the function's goal. The audience comprises experienced developers who wish to write, maintain, or build upon the code. NL outlines would primarily be shown within code and in developer tools. Finally, an outline is a list of sentence-length statements which in total are much shorter than the code they describe.\nThis helps distinguish NL outlines from other forms of code explanation. For example, docstrings focus on the contract of a function and its usage instead of the implementation [21], and are more often read by API users than code maintainers. Pseudocode is more literal than NL outlines (e.g., explicitly providing every variable update instead of abstracting out the broad concepts) and usually appears in textbooks [67] and rarely in professional software projects. Inline comments vary widely in topic, e.g., explaining developer intent, warning about subtle pitfalls, and TODO reminders about future work [23], [24], while NL outlines have a particular focus on the high- level ideas in the function's implementation and are written in a way that still makes sense when presented without code. LLM explanations also vary widely depending on the user's prompt [22], but they are often ephemeral and long explanations going into great detail to help novices and developers unfamiliar with the concepts involved, unlike the concise format of NL outlines which persist in the code or developer tooling. Design documents are a longer form of code explanation, aiming to describe a software architecture including constraints on the design and alternatives that were considered [68], ultimately presenting a different set of information for a different audience (e.g., project managers) compared to NL outlines."}, {"title": "APPENDIX C Finish Changes WITH MULTIPLE CHANGES", "content": "Section III describes using NL outlines for code maintenance, in particular suggesting a Finish Changes feature which allows the LLM to finish any changes the user has made to the code or outline. Fig. 4 illustrates two example usages of the Finish Changes feature, each involving one conceptual edit. But actually, Gemini 1.5 Flash is powerful enough to handle multiple changes at once: Fig. 8 shows a successful implementation of three simultaneous changes.\nIn our experience, the Finish Changes feature with Gemini 1.5 Flash can suggest surprisingly good edits, but it is not always perfect. The likelihood of slight mistakes tends to increase with more changes requested simultaneously, or if one change requires propagating many different kinds of edits to different locations. We observed that the most common mistake is an incomplete propagation of changes that lacks some necessary downstream edit. But, this also means that most of the suggested changes are on the right track.\nGemini 1.5 Flash occasionally suggests an \u201cimprovement\" that is not strictly needed to finish the user's changes, such as adding more detail to a docstring which was not made stale in the first place. The actual suggestions are usually very reasonable, but this behavior may be distracting to users.\nWe chose to explore the Finish Changes feature with Gemini 1.5 Flash due to its low latency-we receive predicted diffs in about 5 seconds, allowing developers to iterate on code quickly."}, {"title": "APPENDIX D CONSTRAINED GENERATION", "content": "Recall from Section IV that Interleaved Generation has the drawback of potentially changing the code against our wishes, resulting in predicted outlines that no longer apply to the original code. Our Constrained Generation approach resolves this issue by using constrained decoding [34], [35]: we construct a regex-like constraint that forces the model to repeat the code exactly, except with optional comment lines between code lines. If the original code parses, we can also require an outline statement before the first line of the function body (after the function signature and docstring) and prevent outline statements from being added above a blank line or in the middle of a multi-line statement or comment block. These extra heuristics help ensure that outline statements are placed appropriately. We found that constraints with these heuristics"}, {"title": "APPENDIX E EXAMPLE PROMPTS FOR GENERATING NL OUTLINES", "content": "We provide example LLM prompts illustrating the Inter- leaved Generation and Line Number Infilling techniques from Section IV which were used in the experiments in Section V. In the actual experiments, we used few-shot prompting with 8 hand-crafted examples. The prompts below each contain a different subset of those examples for brevity. Long lines are wrapped where indicated with the symbol. Some LLMS accept system instructions and conversations with multiple turns between the user and LLM. When applicable, we structure the prompt in that way according to the SYSTEM INSTRUCTIONS:, USER:, and ASSISTANT: delimiters. For other LLMs, we send the prompt as one block of text including those delimiters.\nBelow is an Interleaved Generation prompt with 2 few- shot examples, where {{{code_to_be_outlined}}} is a place- holder for the code for which we want to generate an outline:\nBelow is a similar prompt for the Line Number Infill- ing technique using 3 different few-shot examples, where {{{code_with_line_numbers}}} is a placeholder for the code we want to generate an outline for, except with line numbers prepended in the same way as in the few-shot examples:"}, {"title": "APPENDIX F ERRORS WHEN PARSING OUTLINE PREDICTIONS", "content": "Section V and Table I mention errors detected while parsing an LLM's outline predictions. Here we provide more details about the specific errors and their frequency.\nHere is a list of all errors we checked for when parsing an outline prediction using the Interleaved Generation technique, including the severity (major or minor), the frequency in the experiment (number of LLM predictions where the error occurred, out of 150 predictions considering the 5 models and 30 functions), a description of the error's meaning, and what we do when encountering the error:\nHere is an analogous list of errors we checked for when using the Line Number Infilling technique:"}, {"title": "APPENDIX G QUALITY SURVEY QUESTIONS", "content": "Section V and Fig. 5 refer to a survey we used to gather human opinions about NL outline quality. In total, we obtained 300 outline predictions: 30 functions in the dataset \u00d7 5 LLMs \u00d7 2 generation techniques. For each function, we asked the contributor of that function to inspect the function's 10 predicted outlines, which were presented in shuffled order without any labeling of which LLM or technique produced that outline. For each outline, the function contributor answered five survey questions with multiple-choice answers as follows:"}, {"title": "APPENDIX \u0397 EXAMPLE NL OUTLINES", "content": "Fig. 9, Fig. 10, Fig. 11, Fig. 12, and Fig. 14 showcase a variety of high-quality NL outlines for different functions, using various LLMs and generation techniques. We also show examples with commentary about broader patterns: Gemini 1.0 Pro often gives too much detail (Fig. 13), Gemini 1.5 Pro sometimes gives too little detail (Fig. 15), and Line Number Infilling is the more difficult technique leading to certain common issues with weaker LLMs (Fig. 16). In all examples, we redacted names and edited whitespace to avoid awkward line breaks."}, {"title": "APPENDIX I DRAWBACKS TO STAR COMMENTS", "content": "Star comments, proposed in Section VII, have important downsides in churn, flexibility, and distraction.\nChurn: Source code is expensive to store and edit considering the costs of version-controlled storage forever, the human cost of code review, and extra strain on continuous testing infrastructure. Putting all iterations of NL outlines (which change alongside the code or through LLM upgrades or experimentation) into source code is thus surprisingly expensive. Alternatively, storing NL outlines as non-code metadata in a central database or in extra files adjacent to the source code (but without version control, mandatory code review, or continuous testing) would significantly reduce churn.\nFlexibility: Star comments reduce the flexibility to update outlines without causing more churn. Even if the code is not changed, it may still be desirable to update or deprecate outlines due to prompting improvements, LLM upgrades, or human- provided feedback, and to perform A/B experiments providing users with different outlines. These would be straightforward with outlines stored as non-code metadata, but with star comments, updates are expensive due to churn and A/B experiments are impossible.\nDistraction: Star comments in the code are by default visible, like any other code. This may be distracting to users who suddenly see outlines in developer tooling that does not yet support enough outline-related features to make them worthwhile. Developers need the ability to hide outlines at times, so this functionality must be built into various tooling at the outset. Alternatively, outlines as metadata would be hidden by default and can be gradually shown to users depending on their enthusiasm (e.g., beta testers) and the tooling readiness.\nIn our view, star comments may be used to explore using NL outlines in practice with a limited scope of users and files (e.g., a small team or small company), but outlines should be stored separately from code in the long term at large companies."}, {"title": "APPENDIX J GENERALIZING OUTLINES TO FILES AND PROJECTS", "content": "This paper focuses on NL outlines for individual functions because code flow within a function is naturally structured: code flows from top down, excluding control flow jumps. This implies that adjacent code lines are likely to be semantically related, forming clusters of related lines. Hence, an NL outline is designed to partition the lines into those logical clusters.\nThis is no longer true at the level of a class or file, where code flow is less predictable without enforcement of an organized ordering of functions or classes within a file. If functions were well-organized, then it would be straightforward to partition them into semantic groups with outline statements, but it is less clear what should be done if the functions are not organized. Should the LLM suggest a reordering? That could help keep files more organized but could also confuse developers unaware"}]}