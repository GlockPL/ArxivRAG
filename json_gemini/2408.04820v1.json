{"title": "Natural Language Outlines for Code: Literate Programming in the LLM Era", "authors": ["Kensen Shi", "Deniz Alt\u0131nb\u00fcken", "Saswat Anand", "Mihai Christodorescu", "Katja Gr\u00fcnwedel", "Alexa Koenings", "Sai Naidu", "Anurag Pathak", "Marc Rasi", "Fredde Ribeiro", "Brandon Ruffin", "Siddhant Sanyam", "Maxim Tabachnyk", "Sara Toth", "Roy Tu", "Tobias Welp", "Pengcheng Yin", "Manzil Zaheer", "Satish Chandra", "Charles Sutton"], "abstract": "We propose using natural language outlines as a novel modality and interaction surface for providing AI assistance to developers throughout the software development process. An NL outline for a code function comprises multiple statements written in concise prose, which partition the code and summarize its main ideas in the style of literate programming. Crucially, we find that modern LLMs can generate accurate and high-quality NL outlines in practice. Moreover, NL outlines enable a bidirectional sync between code and NL, allowing changes in one to be automatically reflected in the other. We discuss many use cases for NL outlines: they can accelerate understanding and navigation of code and diffs, simplify code maintenance, augment code search, steer code generation, and more. We then propose and compare multiple LLM prompting techniques for generating outlines and ask professional developers to judge outline quality. Finally, we present two case studies applying NL outlines toward code review and the difficult task of malware detection.", "sections": [{"title": "I. INTRODUCTION", "content": "Outlining a complex document is a standard way to improve its organization and accessibility. A table of contents presents organizational notes separately from the document's content, but these notes may also be interleaved with the content as chapter or section headings. Modern examples of outlines include the \u201cContents\u201d sidebar on Wikipedia pages listing the section headings, and for programming, the \u201cOutline view\" in VS Code and the \u201cSymbols pane\u201d on GitHub list the classes and functions in a code file. These outlines also assist navigation by jumping to the corresponding part of the file when clicked. Advances in large language models (LLMs) such as Gemini [1], [2] and GPT-4 [3] raise new opportunities to automatically generate outlines of software artifacts in reimagined ways. After all, large software projects are incredibly complex, with one study finding that developers spend 70% of their time on program comprehension [4].\nSummarization is another way to accelerate understanding of a document. Scientific papers have an abstract, and business documents may have an executive summary, to help readers quickly understand the key points without reading the entire text. Even if one does read the entire text, reading the summary beforehand may still improve understanding [5], [6]. Recent works apply machine learning and LLMs toward automatic text summarization [7] and code summarization [8]\u2013[14], even considering many styles of summaries for different purposes and audiences [15]\u2013[18]. For example, upon straightforwardly asking Gemini or ChatGPT to summarize a piece of code, one usually receives long paragraphs explaining each aspect of the code in detail, often totaling more text than the code itself. Such explanations are useful for learning but are not optimized for helping an experienced developer work more quickly. Instead, we desire a concise summary that efficiently conveys the key ideas, with clear alignment between the summary and the code.\nAnother common way to improve the clarity of code is to incorporate natural language (NL). Writing docstrings, comments, helpful error messages, and descriptive identifier names are well-known aspects of good coding style [19]. In computational notebooks or other forms of literate programming [20], code and explanatory NL are interleaved in a file that is both executable and self-documenting. NL is generally easier for humans to read than code, so well-written NL can help developers more efficiently understand the associated code.\nUsing these themes of outlining, summarization, and literate programming, we propose using LLMs to generate a new form of code explanation: natural language outlines (NL outlines). An NL outline consists of natural language statements that partition the code into logical sections, while summarizing each section of code with concise efficient wording. The outline may be shown interleaved with the code or displayed separately, depending on the application. NL outlines are a particularly appealing modality for AI developer assistance due to their broad applicability: in addition to helping developers understand and navigate code, Fig. 1 illustrates a vision of how NL outlines can be used throughout the software development process for code generation and maintenance, code search, and code review.\nContributions: We introduce NL outlines (Section II), describe their extensive use cases (Section III), and propose methods for generating them with LLMs (Section IV). We perform a developer study using real code (Section V), finding that 60% of outlines were rated by professionals as excellent overall (90% were acceptable or better). In two case studies we apply NL outlines toward mobile app security and code review (Section VI), finding for example that 26 of 30 professional reverse engineers said that outlines are very or extremely helpful"}, {"title": "II. NATURAL LANGUAGE OUTLINES", "content": "A natural language outline (NL outline) for a function or snippet of code consists of one or more statements written in concise prose, which are aligned with the code and partition it into semantically-coherent sections, such that each outline statement summarizes the section of code it aligns to.\nAs an example, Fig. 2a shows a real Python function from an author's prior project. Even though the code is self-contained and only uses common libraries, it still takes a considerable amount of time for a developer to parse the code and decipher the steps taken to achieve its goals. A docstring might help somewhat, but docstrings generally discuss the function's contract instead of its implementation strategy which may be more pertinent when editing or building upon the code.\nTo gain a high-level understanding of the implementation, one may instead read the NL outline in Fig. 2b which mirrors the code's ideas and structure, while being more concise (using about half as many characters) and easier to read (English versus code). The compact nature of the outline would also reduce scrolling when browsing through the file. If one wishes to see more details or reference the code \"source of truth,\" the outline may also be displayed as comments or annotations interleaved with the code in the style of literate programming [20], as shown in Fig. 2c. In this format, the outline provides visual structure and allows the reader to seamlessly switch between reading NL or code syntax as needed. Some developers already write these kinds of comments manually,\u00b9 adding to the familiarity of NL outlines as a concept. But with recent advancements in large language models (LLMs), there is little need for developers to write NL outlines manually, since modern LLMs can generate high-quality NL outlines even for proprietary code outside their training data (Section V). Thus, we propose that NL outlines can be used as a means to provide AI assistance to developers, with many use cases illustrated in Fig. 1 and further discussed in Section III.\nNL outlines are designed to improve developer efficiency for authors, collaborators, maintainers, and users of code.\nWe provide a taxonomy in Appendix B to classify different forms of code explanation by topic, audience, location, and length. For example, NL outlines concisely summarize a function's implementation, are used primarily by experienced developers, and can be shown with or without code. In contrast, docstrings focus on a function's contract primarily for API users [21], existing LLM explanations vary widely but are often long and designed for novices [22], and inline comments are only shown with code and contain more targeted detail such as design rationales, subtle pitfalls, and remaining tasks [23], [24].\nNL outlines are not intended to do any of the following: document all details about the code (providing too much information would reduce efficiency), educate developers about concepts or APIs (developers can look up tutorials or docu-\nThere is disagreement about whether developers should manually write outline-like comments, since they might become stale and are time-consuming to maintain (but Section III has a solution). Even so, some developers still prefer to write outline-like comments for important code.mentation elsewhere as needed), provide complex reasoning (some comments are too important or complex to relegate to AI, including documentation of tricky implementation or design details), or replace code entirely (code remains the source of truth considering the ambiguity of NL)."}, {"title": "III. USE CASES FOR NL OUTLINES", "content": "The broad applicability of NL outlines is a key factor in their appeal as a surface for human-AI interaction. Here we provide an overview of many potential use cases as illustrated in Fig. 1: code understanding, code maintenance, and overall developer experience. A full evaluation of each use case is beyond the scope of this paper instead, Section V evaluates the common first step of generating NL outlines to begin with, and Sec-tion VI deeply explores two concrete applications in practice.\nCode understanding and navigation. Fig. 3 shows a mockup of how NL outlines could be used in an IDE. NL outlines can be shown in the list of symbols to aid whole-file understanding while being clickable for precise navigation. Outlines can also be shown in the main editor interleaved with the code, assisting with rapid code understanding and offering intuitive code folding [25]. The user experience can be enhanced through customization of outline styling, and IDE shortcuts can show or hide outlines, generate new outlines, convert between outlines and comments, and jump to adjacent outline statements. NL outlines can be similarly displayed in other developer tools for code browsing, searching, and so on.\nCode maintenance. Once the user is accustomed to using NL outlines interleaved with code as in Fig. 3, we can apply NL outlines beyond static (unchanging) code and toward code maintenance, e.g., editing, refactoring, and extending code. For example, the IDE can detect when the user is editing a function. When there is a lull in typing, the IDE may automatically update the outline to reflect the current code. (This can be done by providing an LLM with the old code, old outline, and current code, and prompting the LLM to predict an updated outline.) Some users may prefer that outlines are updated automatically with minimal friction, while others might explicitly request an outline update through a keyboard shortcut and review a diff of the predicted changes. Either way, this use case resolves a common issue of comments becoming stale as the code evolves [26], [27], lifting the burden of tedious updates (and constantly checking if any comments need updating) from the user and delegating it to the LLM. Hence, NL outlines can serve as automatic living documentation of the code.\nWe can also perform updates in the reverse direction. If NL outlines are themselves editable by the user, then the user may specify the high-level idea of a code change simply by editing the outline, letting the LLM predict the corresponding code changes. This concept is quite flexible: edit an outline statement to convey a minor logic change in that portion of the code, add new outline statements to guide a multi-line suggestion of new code, reorder outline statements (potentially outside the function's boundaries) for a refactoring that handles dataflow dependencies better than mere copy/paste, and delete outline statements for a code cleanup that also removes resulting dead code elsewhere. This would allow developers to program primarily through natural language, as if developing high-level pseudocode that is automatically synced to real code (but we still encourage careful review of the LLM's predicted code).\nWe can actually generalize the two previous ideas (where the user makes a complete edit to the code or outline, and the LLM only updates the other to match). Instead, we can let the user start any change(s) (to the code or outline) and have the LLM finish the change(s) (modifying the code and outline as needed). Many code changes start with one key idea and then require propagating changes to code and documentation elsewhere. With this feature, the user only needs to specify the main idea of each desired change, and the LLM can perform the more tedious work of implementing and propagating the change. This may increase developer efficiency by allowing them to think at a higher level of abstraction and delegating the lesser details (with careful review). The user can edit the outline for broad ideas or edit the code to specify more exact changes, seamlessly switching between code and NL as needed, as in the literate programming paradigm but with a novel bidirectional connection between NL and code powered by modern LLMs.\nWe built this into a prototype Finish Changes feature in an IDE by providing the old code, old outline, current code, and current outline to an LLM which then predicts the new code and new outline. Fig. 4 shows screenshots with real predictions from Gemini 1.5 Flash. In Fig. 4a, editing a variable assignment causes updates to its description in the outline and its type throughout the code. In Fig. 4b, replacing one word in the outline causes a multi-line implementation change and a docstring update. Chain-of-thought (CoT) prompting [28] is quite effective for this feature, having the LLM list the user's changes, reason about further edits to finish those changes, and lastly predict the new code with new outline interleaved.\nCode generation. The standard approach to function generation is to take a specification (e.g., the function signature and docstring) and predict the entire body, but it can be difficult to understand incorrect or unexpected predictions. The user might even discover that the prediction is so wrong that they delete it and start over, having wasted their time completely.\nInstead, as previously mentioned, users can write new outline statements to steer code generation using a Finish Changes feature. But when writing an entire new function, we can further assist users by having the LLM propose an NL outline (displayed without code) for the user to approve before proceeding an easier task than writing an outline from scratch or reviewing predicted code directly [29]. The LLM may also predict multiple outlines with different approaches or interpretations for the user to brainstorm with.2 The user may edit an outline to correct mistakes instead of debugging the code later. By having the LLM generate code aligned with a user-approved outline, we increase the likelihood that the code satisfies the user, and it is also easier for the user to interpret the code because they already expect the outlined approach.\nCode review. During code review, a key part of software development [30], [31], NL outlines can help a reviewer understand code written by someone else. For example, if NL outlines are automatically updated to reflect code changes, then the outline changes form an NL summary of the code changes, i.e., reviewers may find it helpful to inspect a diff of the old versus new outlines in addition to the code diff.\nCode search. Code search systems in practice mainly use structured forms of search [32], [33], but users sometimes do not know the needed keyword or regex. NL outlines may enable or enhance more intuitive text-based search: after generating and indexing NL outlines, a search query containing NL and/or regexes could be matched to outlines and/or code. Users could then search for concepts, e.g., to find usage examples or where a particular action occurs within a complex application. NL outlines can also help developers understand the context around search results, whether for NL queries, traditional regex queries, or even searches for symbol references or function call sites.\n2One can increase diversity through CoT prompting, e.g., listing different approaches or interpretations and generating one outline for each."}, {"title": "IV. OUTLINE GENERATION", "content": "We can generate NL outlines by few-shot prompting an LLM, but different prompting techniques have different benefits.\nInterleaved Generation. The most straightforward way to generate NL outlines is to provide an LLM with the code to be outlined and prompt the LLM to repeat the code with outline comments added but without other code changes. This is more effective with prompt instructions describing the purpose of NL outlines and their desirable qualities, and with few-shot examples demonstrating the task and what good NL outlines look like. We call this approach Interleaved Generation because the model predicts an outline interleaved with the code. However, a notable downside of this approach is the possibility that the model changes the code against our wishes. If the model's prediction significantly deviates from the original code, the outline might not describe the original code anymore. Some minor deviations (e.g., changes to blank lines and existing comments are common) can be ignored if the model gets back on track, but in extreme cases the outline must be discarded.\nThis issue can be remedied with the Constrained Generation approach, using constrained decoding [34], [35] to alter token probabilities in a way that prevents changing the code. Specifically, we construct a regex-like constraint that repeats the code exactly, except with optional comment lines between code lines. Appendix D discusses heuristics that help ensure good placement of outline statements and implementation difficulties.\nLine Number Infilling. A drawback shared by Interleaved Generation and Constrained Generation is that they spend output tokens repeating the code, leading to unnecessary cost and latency. We design a solution called Line Number Infilling: we prepend each line of the original code with its line number, and we prompt the model to output a sequence of outline statements, each containing the line number where the outline statement should be added, and the text of the statement itself. For example, the original code \"def sq(x):\n return x**2\" is given to the model as \u201c1|def sq(x):\n2| return x**2\u201d, and the model may respond with \u201c2 | Squares the input.\u201d (using multiple lines for multiple outline statements). This prediction task is similar to fill-in-the-middle (FiM) in language modeling [37]\u2013[39], where the LLM must predict text (like outline statements) to insert at specially-marked locations (like the line numbers), although in our case the LLM must also choose which locations to insert at. Since this response format does not involve code tokens at all, we resolve the prior problems of changing the original code or spending tokens to repeat the code. The prompt is also shorter because the few-shot examples do not repeat the code either. In practice, we observe that Line Number Infilling provides a 4\u00d7 to 5\u00d7 average speedup compared to Interleaved Generation. However, Line Number Infilling has a downside that the prediction format is less natural for LLMs, i.e., \"code with comments\" is more similar to the training data than \u201cline numbers and summaries.\" Hence, this approach sometimes causes LLMs (especially weaker ones) to generate outlines with formatting mistakes, misplaced statements, or generally worse quality. Section VII has suggestions for further research to improve outline generation, e.g., retrieval-augmented generation [40]\u2013[43], few-shot selection [44], [45], and finetuning [46]\u2013[48].\n3 Some approaches claim to decode \"forced\" tokens quickly (like prompt tokens), but this is itself tricky due to tokenization complications [36]."}, {"title": "V. EXPERIMENTS", "content": "We investigated (a) the rate of formatting issues in LLM predictions and (b) the quality of generated outlines in several dimensions, depending on the LLM and generation technique used. First, we asked 6 professional software engineers and researchers to curate a dataset of 30 Python functions from 21 real projects,\u2074 emphasizing variety in libraries used and function kinds (standalone functions, methods, constructors, unit tests, main functions, etc.). Each function had 10 to 90 lines of code (median 46.5 LOC). To mimic realistic usage, we removed existing outline-like comments in 7 functions but kept other comments (in 10 functions) including TODOs, developer notes, and comments to disable warnings.\nWe tried 5 LLMs: Gemini 1.0 Pro and Ultra [1], Gemini 1.5 Flash and Pro [2], and DeepSeek-Coder-Instruct 33B [49], all with greedy decoding. We generated outlines using Interleaved Generation and Line Number Infilling,\u2077 for a total of 5\u00d72 = 10 outlines per function. We used a fixed set of 8 hand-crafted few-shot examples so that each LLM receives the same prompt for a given function and generation technique.\u2078\nBecause each LLM response is just text, we must programmatically parse the text to extract the predicted outline. During parsing, we may encounter errors in the prediction's formatting (e.g., changing the code in Interleaved Generation) or other problems with the outline. We classify these parsing errors into major errors that affect outline quality and minor errors that we can recover from without impacting outline quality. Table I provides the rate of formatting issues in terms of functions out of 30 for which the prediction had no parsing errors, only minor errors, or a major error. We observe that the Gemini 1.5 models consistently produce well-formatted predictions (with zero major errors) while the other LLMs make more mistakes. Table I also shows the average number of statements in the predicted outlines. Too many statements often means poor abstraction of ideas, but too few may indicate a lack of detail.\nWe performed surveys about the quality of generated outlines, presenting each function's 10 outlines in shuffled order to the person who contributed that function to the dataset. That person is very qualified to rate the outlines because they deeply understand the function and its purpose. For each outline, we asked about overall quality, helpfulness, factual correctness, amount of detail, and style/fluency; 10 the results are in Fig. 5.\nEven though the vast majority of dataset functions refer to custom dependencies (e.g., helper functions) which are not in the prompt or training data, we find that LLMs are nevertheless very capable of generating NL outlines that are high-quality, helpful, and correct. For example, using Gemini 1.5 Pro and Interleaved Generation, 60% of outlines were rated as having excellent overall quality (90% were acceptable or better), 63% were very helpful, and 80% were completely correct. Some outlines had minor issues (e.g., 17% were mostly correct), but poor performance was quite rare: 3 of 30 outlines were bad quality, 2 were not helpful, and only 1 was incorrect. The Line Number Infilling technique led to slightly worse predictions (but it is several times faster, see Section IV). Gemini 1.5 Flash was the next best LLM and the others were noticeably worse.\nAppendix F lists the specific errors we checked for.\n4Five of the dataset contributors are also authors of this paper. There was a bias toward research projects simply due to the contributors' background.\n5The LLMs in our experiments were not trained on the removed comments.\n6The LLMs were chosen to avoid sending proprietary code to external APIs.\n7We omit Constrained Generation since it is nearly the same as Interleaved Generation for strong models and difficult to set up for older or weaker models.\n8 Appendix E contains example prompts.\n9 Appendix G lists the survey questions. Appendix H contains several examples of generated outlines and their survey responses, with commentary."}, {"title": "VI. CASE STUDIES", "content": "We present deep dives into two concrete applications.\nAndroid Security\nWe consider the problem of assessing the security and privacy of Android applications (or apps). These assessments are performed routinely by app stores such as Google Play and Samsung Galaxy Store, where apps submitted by developers across the world are analyzed for compliance with policies\u00b9\u00b9 meant to protect app users, e.g., one policy states: Apps that are deceptive, malicious, or intended to abuse or misuse any network, device, or personal data are strictly prohibited.\nDetermining whether an app is deceptive or malicious involves understanding the code and relating its functionality to the app's description, UI, and user expectations. There are millions of apps in stores and tens of thousands of new apps added per month, so security assessments must be efficient.\nTo detect malicious apps, reverse engineers (REs) use static and dynamic program analyses to flag code as potentially problematic and then confirm violations through manual review. The initial challenge is in the volume of code: many apps have millions of lines of code, thousands of classes, and complex event-driven execution paths. Furthermore, an app's source code is usually not available, so REs inspect decompiled code which is difficult to understand for several reasons. Decompiled app code is unnatural due to the compilation process performing optimizations (e.g., inlining code and changing code structure) and removing code artifacts (e.g., comments, docstrings, types, variable names, and names of inlined API constants). App code may be further transformed by minification to reduce app size [50] and obfuscation to protect intellectual property [51]. Nevertheless, REs must quickly understand the code to find policy violations a promising application for NL outlines.\nApplying NL outlines. We collected a dataset of 80 de-compiled functions from real Android apps. Half were manually curated \"suspicious\u201d functions that REs had previously flagged as using malware techniques, and we focused on including a variety of techniques and avoiding similar code. The other half were \"benign\" functions randomly sampled from 8 popular apps assumed to be malware-free, all from different companies and genres. All dataset functions have between 24 and 99 lines of code, and we tried to make the two halves of the dataset have similar distributions in code length and style (minified or obfuscated) as much as reasonably possible to reduce bias.\nWe then used Gemini 1.0 Ultra to generate 3 kinds of predictions per function, designed for different benefits. (1) A suspicion score is an integer from 0 (not suspicious at all) to 3 (very suspicious) which is extremely quick to read, allows more nuance than a simple binary prediction,\u00b9\u00b2 and enables programmatic processing due to its simplicity. (2) A paragraph-form summary of about 1-3 sentences describes what the function does and why it is suspicious or not, in a format that is a baseline for LLM-based code explanation and is familiar to users. (3) Lastly, we generate an NL outline, shown with inline comments, which focuses only on the suspicious parts and which is empty by design if the LLM predicts that the function is not suspicious at all. The purpose is to point directly to the problematic areas of code for faster understanding and localization of the malware implementation, without non-security-related distractions (REs usually do not care about the behavior of benign functions).\nWe obtain all three predictions in a single LLM query with 7 few-shot examples. The LLM first predicts the summary to help it reason about the other predictions, similar to CoT prompting. The suspicion score is predicted next. The outline is predicted last to make it consistent with the score, i.e., it is consistently empty if and only if the score is 0. Fig. 6 shows actual LLM predictions for a decompiled function of our creation.\nEvaluation. We aimed to evaluate the accuracy and perceived helpfulness of each kind of prediction. We first consider the accuracy of predicted suspicion scores. Of the 40 suspicious functions, four had a predicted suspicion score of 0, which we treat as false negatives, and of the 40 benign functions only two were false positives with nonzero suspicion score (both scores of 1, the smallest nonzero amount of suspicion). Further inspection of these cases led to interesting observations.\nThe two false positives were actually reasonable predictions that correctly describe privacy-impacting behavior in fair terms without raising more alarm than necessary.\u00b9\u00b3 In our survey of REs (described below), for both functions, the three REs assessing the function unanimously agreed that the model's predicted suspicion score (of 1) was accurate, and the median of the RE's suspicion scores was also 1, so these cases should not be considered false positives after all. Among the four false negatives, two cases had all three REs agreeing that the predictions were accurate. For example, an RE originally flagged one of those functions as suspicious because it called ActivityManager.getRunningAppProcesses which can be a privacy concern. However, the LLM used the surrounding code context to correctly deduce that it only obtains the name of the current process and nothing else, which is not actually problematic. This leaves two actual false negatives remaining,\u00b9\u2074 for a false negative rate of 5% and false positive rate of 0% after adjustments from manual review.\nTo judge the accuracy of the other kinds of predictions, we surveyed 30 professional REs whose primary role is to review Android apps. We split the REs into 3 groups of 10 by their amount of experience. For each function, the LLM's predictions were rated by one RE from each group, so that each RE rated predictions for 4 suspicious and 4 benign functions.\u00b9\u2075 Prior to seeing the LLM predictions for a function, REs also provided their own suspicion score. Afterward, each RE completed a final survey about the helpfulness of each kind of LLM prediction.\nIn the per-function surveys, 95.4% said the LLM-predicted suspicion score was accurate, 82.1% said the summary was completely correct (versus 17.9% mostly correct and 0% incorrect), and excluding empty outlines for functions predicted as benign, 83.8% said the outline was completely correct (16.2% mostly correct, 0% incorrect). About 77% of both summaries and outlines were rated as right amount of detail (about 23% needs more detail, less than 1% too much detail).\nWe also measured correlations between suspicion scores provided by the LLM and each group of REs. Compared with the scores from the most-experienced REs, the LLM's scores have a Pearson correlation coefficient $r = 0.85$, higher than that of the other two groups of REs ($r = 0.80, r = 0.78$). Alternatively, compared with the average of RE's scores, the LLM's scores have $r = 0.96$ which is higher than that of any group of REs (0.92 \u2264 $r$ \u2264 0.93). Either way, the LLM performs better (making more consistently good predictions) than most REs on this task and dataset. With the previous survey results, we conclude that the LLM predictions are very accurate.\nNext, we analyzed the helpfulness of each style of prediction in Fig. 7. A majority of REs found every kind of LLM prediction very or extremely helpful, but some patterns stand out. First, we expected summaries to be more helpful for suspicious functions than benign functions, because a function's behavior is less important if it is not security-impacting. The survey responses (especially from the most experienced REs) confirm our expectation. This also validates our design decision to have NL outlines only mention security-related aspects of functions predicted to be suspicious. Another particularly interesting pattern is that the most experienced REs found the NL outlines to be much more helpful (70% extremely helpful) than the other REs did. This may reflect a difference in RE responsibilities: the most experienced REs are often final decision-makers who need to fully understand a suspicious implementation, and while suspicion scores and summaries can help prioritize focus, only outlines help in understanding the implementation strategy.\nREs indicated that summaries and outlines were helpful in all the ways we asked about, as summed up by one RE's feedback\u2014\u201cAs proposed, the LLM help will change the game for us RE's: in speed, accuracy, and confidence.\u201d Summaries helped in terms of speed (as indicated by 100% of REs), understanding (83%), and confidence (80%), although for the most experienced group of REs, summaries were not as helpful in understanding (60%) and confidence (50%). On the other hand, outlines helped all groups of REs relatively equally, in speed (93%), understanding (90%), and confidence (80%). Finally, 97% of REs said they would read both summaries and outlines if provided (i.e., they are complementary), but one RE in the most experienced group would read only outlines and provided the following feedback:", "review": "the difficulty in reviewing large or complex change lists (CLs). With increasing CL complexity, it becomes increasingly difficult to maintain a complete mental model of all of the changes and how they interact. This software development anti-pattern is so substantial that code reviewers may even send large CLs back to the authors to be split into smaller CLs that are individually more manageable. Reviewers actually spend much less time per line reviewing large CLs, raising potential concerns about fatigue and review quality. Although tools have been developed to assist in moving code changes between CLs, actually splitting a CL is time-consuming, and in some cases it might not be possible if the changes are particularly entangled. So, the burden often falls on the reviewer to mentally disentangle messy CLs during code review.\nWe address this with a prototype feature called Virtual CL Split, which is a layer in an existing code review interface that allows a reviewer to view individual code changes grouped into logical topics. Each topic is described with a natural language phrase that helps the reviewer interpret the changes. The topics form a partition of the original CL, but this splitting is done virtually, i.e., the CL itself is unchanged and there is no requirement that any individual topic can become a standalone CL. Upon selecting a topic in the UI, all changes in that topic have their code diffs expanded and other changes are collapsed (or shown with muted diff highlighting when the reviewer wants to see more file context), so that the reviewer can focus on the selected topic to simplify their mental stack. Virtual Split ensures that every code change appears in some topic, so that the reviewer is", "done": "fter examining all topics.\nFor example", "topics": "first the new function with its tests", "52": ".", "found the split helpful, especially since [she's": "less familiar with the code here.\" Others provided similar feedback: Virtual Split is particularly helpful for reviewing unfamiliar code. Thus, Virtual Split could be especially useful for migrations to new technologies, reviews on legacy code, and assisting new team members and other reviewers unfamiliar with the codebase (e.g., for security/privacy reviews). We also observed the pattern that, if the CL is complex enough and the split was high-quality (i.e., changes were correctly assigned to accurately-described topics), then the split was almost always considered useful during review. This indicates that the feature has well-designed UX and presents a tangible"}]}