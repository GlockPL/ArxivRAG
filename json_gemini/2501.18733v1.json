{"title": "Integrating LMM Planners and 3D Skill Policies for Generalizable Manipulation", "authors": ["Yuelei Lit", "Ge Yan", "Annabella Macaluso", "Mazeyu Ji", "Xueyan Zou", "Xiaolong Wang"], "abstract": "The recent advancements in visual reasoning capabilities of large multimodal models (LMMs) and the semantic enrichment of 3D feature fields have expanded the horizons of robotic capabilities. These developments hold significant potential for bridging the gap between high-level reasoning from LMMs and low-level control policies utilizing 3D feature fields. In this work, we introduce LMM-3DP a framework that can integrate LMM planners and 3D skill Policies. Our approach consists of three key perspectives: high-level planning, low-level control, and effective integration. For high-level planning, LMM-3DP supports dynamic scene understanding for environment disturbances, a critic agent with self-feedback, history policy memorization, and reattempts after failures. For low-level control, LMM-3DP utilizes a semantic-aware 3D feature field for accurate manipulation. In aligning high-level and low-level control for robot actions, language embeddings representing the high-level policy are jointly attended with the 3D feature field in the 3D transformer for seamless integration. We extensively evaluate our approach across multiple skills and long-horizon tasks in a real-world kitchen environment. Our results show a significant 1.45x success rate increase in low-level control and an approximate 1.5x improvement in high-level planning accuracy compared to LLM-based baselines. Demo videos and an overview of LMM-3DP are available at https://lmm-3dp-release.github.io.", "sections": [{"title": "I. INTRODUCTION", "content": "Building generally capable robots that can perform a wide range of long-horizon tasks in the real world is a long-standing problem. Recent advancements in robotics have been driven by large language models (LLMs) that have shown remarkable capabilities in understanding the real world and common sense reasoning. Some studies leverage LLMs to decompose an abstract task into a sequence of high-level language instructions for planning [1], [2], [3], [4], [5], [6], [7], [8], [9], [10]. Despite the significant advancements they have facilitated in various real-world tasks, the current integration of LLMs into robotics presents several major drawbacks. First, LLMs can only process natural language with no visual understanding, making it difficult to comprehend and adapt to dynamic real-world scenarios requiring rich visual infor-mation. Additionally, LLM-based planners usually depended on human language feedback to perform long-horizon planning consistently [10], [5], [6], which significantly constrains autonomy. However, large multimodal models (LMMs), with multi-sensory inputs, have emerged as a powerful tool to equip robots with strong visual understanding and generalization capabilities across various environments. This allows the robot to adjust language plans according to the environment change.\nIn this paper, we focus on leveraging LMMs to generate language plans based on environment feedback and keep self-improvement in a closed-loop manner.\nExisting LLM-based planners typically rely on a predefined set of primitive skills for low-level control [1], [2], [11], [12], [3], [13], which is the main bottleneck of large-scale applications to open-world environments. Therefore, the ability to acquire robust low-level skills capable of adapting to the novel environment in a data-efficient manner presents a significant challenge for most LLM-based frameworks. Some recent studies use LLMs to directly output low-level control [14], [15]. However, they are only effective in relatively simple manipulation tasks that do not involve rapid high-dimensional control. Due to insufficient 3D understanding, LLMs often fail in complex environments that require comprehending the 3D structure of the scene efficiently. In addition, recent works leverage vision-language models (VLMs) for visual grounding by predicting bounding boxes or keypoints of target objects [16], [1]. Despite promising results, they rely on off-the-shelf VLMs which may not be fully optimized for specific, complex tasks in dynamic environments.\nTo address these challenges, we introduce LMM-3DP, an LMM-empowered framework that integrates LMM for self-improved high-level planning and an efficient 3D policy for low-level control. Our framework is designed to satisfy two key requirements: 1) it ensures our LMM agent achieves high autonomy during continuous deployment by decomposing a long-horizon task into high-level plans, calling low-level policy for execution, receiving the environment feedback, and updating language plans accordingly. 2) it allows the low-level policy to learn various skills efficiently with only a few human demonstrations and improve continually.\nFor high-level planning, we introduce three key modules to build an autonomous agent capable of planning a sequence of language instructions: 1) Interactive planning with visual feedback. Incorporating visual feedback within the loop is crucial for enabling an agent to rapidly adapt to dynamic scene changes. In this work, we adopt GPT-4V [17], [18] as an LMM planner to receive environmental feedback and monitor the ongoing events during execution. 2) Self-improvement with memory and critic. We introduce a critic agent to analyze the plan generated by the LMM planner. It outputs the critique of the planner's decisions and informs whether the plan needs to be updated. In addition, LMM-3DP stores history critique into a memory module and summarizes learned experience for the planner. This approach significantly improves planning accuracy and consistency, especially in challenging long-horizon tasks. 3) Life-long learning with a skill library. Open-ended"}, {"title": "II. RELATED WORK", "content": "LLMs as Task Planners. Recent advancements in large language models (LLMs) have greatly influenced robotics in various applications. Notable methods typically include using LLMs to generate high-level plans [1], [2], [3], [4], [20]. For example, SayCan [1] underscores the extraordinary com-monsense reasoning ability of LLMs by generating feasible language plans and adopting an affordance function to weigh the skill's likelihood for execution. Some approaches also leverage LLMs to produce programming code or symbolic API as plan [12], [21], [8], [22], [23], [24], [25]. However, these methods only take natural language instructions as input and lack the ability to perceive the world with multimodal sensory observations. Therefore, they cannot adjust the language plans based on environmental feedback, which strongly limits their performance in dynamic real-world environments. Due to the emergency of LMMs, some studies [26], [27], [28] leverage GPT-4V [17] for planning with visual input. However, they only use GPT-4V as a fixed planner without critic and self-improvement while we allow the agent to continue exploring and improving in open-world environments.\nLow-Level Robot Primitives. Despite the significant progress in high-level planning, previous LLM-based language planners [1], [2], [11], [12], [3] hold a strong assumption that there exist reliable low-level skills for high-level plan-ners to retrieve, which are usually manually pre-defined set of skills. Some studies [14], [12], [29], [30] use LLMs to output direct low-level control in text, which is impractical to apply to complex real-world tasks requiring high-dimensional"}, {"title": "III. METHOD", "content": "In this work, we aim to develop a robust planning frame-work to generate high-level language plans, along with a generalizable skill-level control policy to follow language plans and execute actions. In this section, we first discuss the design of our self-improved high-level planner, then introduce our language-conditioned skill-level policy (see Figure 2 for the whole pipeline).\nA. LMM for High-Level Planning.\nPlanning with Visual Feedback. In the real world, the optimal plan to execute a task may not be the one initially devised. For instance, you might plan to put vegetables in your favorite blue bowl for dinner, but upon discovering that the blue bowl is unavailable, you use a red bowl instead. Similarly, in robotic planning, the robot must be able to update its plan according to the current situation, which necessitates visual feedback during task execution. We integrate GPT-4V as a planner within the robot's execution loop, enabling it to update the plan after each skill is executed. This design enhances the robot's ability to adapt to dynamic scenes (e.g. when there are environmental disturbances) and reattempt a previous skill if the low-level control fails to execute.\nCritic Agent. To ensure that the plan generated by the planner is correct and reliable, we introduce an additional critic agent to proactively identify flaws in the generated plan with continuous self-improvement. The critic agent, which only takes visual observation and proposed plan as input (without human instruction), checks whether the next step is feasible in the current situation. If the critic finds that executing the next step will result in an undesirable outcome, its reasoning is input back to the planner, which then proposes a new plan. For instance, the planner's output can be easily skewed by human instructions. This issue persists even with popular prompting techniques [49]. If the human instruction is to close all the drawers, but some drawers are already closed in the scene, the planner might still generate a plan that involves closing all the drawers. However, the critic can accurately determine that the robot should not close a drawer that is already closed, thereby correcting the planner's mistake.\nLifelong Learning. We aim for the planner to improve over time and avoid repeating mistakes by learning from past experience, similar to human learning. However, fine-tuning the planner is computationally expensive. Instead, we employ human critiques of the GPT-4V's output plan and reasoning and then summarize these critiques for in-context learning. These summaries are stored as memory for the planner to reference in the future. Additionally, the planner can propose new skills to the skill library when necessary, then the low-level policy will be updated accordingly with these new skills. For example, in a cooking task, without the click skill, the robot cannot turn on the stove. The planner would identify the click skill as essential for future learning. This approach enables our framework to handle more complex tasks as the skill library expands.\nB. Skill learning with 3D Semantic Representation\nGiven the language instructions generated by the planner, we train a language-conditioned 3D policy to learn the re-quired low-level skills from human demonstration data. Instead of predicting every continuous action, we extract keyframe"}, {"title": "actions and convert the skill learning into a keyframe-based action prediction problem. This approach simplifies contin-uous control and is more sample-efficient for learning a generalizable policy capable of handling novel objects and environments.", "content": "Vision and language encoder. To tackle complex real-world environments with various objects and scene structures, we learn a unified 3D and semantic representation by adopting a two-branch architecture: i) Pre-trained with internet-scale data, the vision foundation model has achieved great success in understanding complex scenes with strong zero-shot gener-alization ability. To leverage these powerful vision foundation models in robotics, we apply a foundation model (e.g., DINO [48]) to extract 2D image features with rich semantics. We then obtain a 3D point feature by back-projecting the 2D feature maps to 3D space. ii) Despite rich semantics from the vision foundation model, it still lacks accurate geometric understand-ing. Therefore, we adopt a separate branch of a point-based model (e.g., PointNext [19]) to learn a geometry point feature for better capturing local 3D structures. Subsequently, both semantic and geometry point features are fused by an MLP layer. To incorporate language understanding into our policy, we use a pre-trained language encoder from CLIP [50] to obtain a language embedding.\nKeyframe action prediction. Given the fused 3D point feature, language embedding, and robot proprioception, we adopt a 3D transformer architecture to predict the 6-DOF pose of the next best keyframe. Instead of predicting continuous action, we simplify the model prediction into translation @trans \u2208 R\u00b3, rotation arot \u2208 0,1(360/5)3, gripper openness Aopen \u2208 [0, 1], and collision avoidance acollision \u2208 [0, 1]. Specifically, we approximate the continuous 3D field via sampling a fixed set of query points in the gripper's workspace. We do this because, unlike voxel-based methods that discretize the output space and are memory inefficient, the sampling-based approach provides a continuous output space and saves memory during training. We also define a learnable token to attend to the local structures more efficiently. Both the query points and the learnable token are passed through multiple cross-attention layers with the visual and language features, to obtain the token feature ft and query point feature fq. We then assign a score for each query point by computing the inner product of ft and fq. The next best waypoint P\u2081 is chosen by applying an argmax operation to the score. Inspired by [51], we subsequently resample a reduced set of query points around"}, {"title": "Pi and refine the selection of waypoints among these query points based on previous predictions.", "content": "Lbc = \\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{document}\n\\begin{equation}\n\\begin{aligned}\n&\\lambda_{\\text{trans}} \\mathbb{E}_{Q_{\\text{trans}}, Y_{\\text{trans}}} \\left[LS\\left(V_{\\text{trans}}, Y_{\\text{trans}}, \\alpha\\right)\\right] \\\\\n&-\\lambda_{\\text{rot}} \\mathbb{E}_{Y_{\\text{rot}}} \\left[\\log V_{\\text{rot}}\\right]-\\lambda_{\\text{open}} \\mathbb{E}_{Y_{\\text{open}}} \\left[\\log V_{\\text{open}}\\right] \\\\\n&-\\lambda_{\\text{collide}} \\mathbb{E}_{Y_{\\text{collide}}} \\left[\\log V_{\\text{collide}}\\right]\n\\end{aligned}\n\\end{equation}\n\\end{document}\n\n(1)\n\\documentclass{article}\n\\usepackage{amsmath}\n\n\\begin{document}\n\\begin{equation}\nV_{i} = \\text{softmax}(Q_{i}) \\ \\text{for} \\ Q_{i} \\in [Q_{\\text{trans}}, Q_{\\text{rot}}, Q_{\\text{open}}, Q_{\\text{collide}}] \\ Y_{i} \\in [Y_{\\text{trans}}, Y_{\\text{rot}}, Y_{\\text{open}}, Y_{\\text{collide}}]\n\\end{equation}\n\\end{document}\n\n(2)\nwhere Vi softmax(Qi) for Qi \u0404 [Qtrans, Qrot, Qopen, Qcollide]. Yi\u2208 [Ytrans, Yrot, Yopen, Ycollide] is the ground truth one-hot encoding. For translation, we calculate cross-entropy loss between a predicted point index Qtrans and the ground truth Ytrans. We apply a label smoothing function LS to translation loss to prevent overfitting and mitigate label noise in real-world demonstrations.a is the smoothing parameter."}, {"title": "IV. EXPERIMENTS", "content": "Experiment Setup & Implementation Details. We set up a real-world kitchen environment for our experiments, which is more complicated and has more visual features compared to a simple tabletop setting. Our robot is a 7-DoF Franka Emika Panda robot with a 1-DoF deformable gripper. For visual input, we use two Intel RealSense D435 cameras: one provides a front view, and the other is mounted on the gripper. To collect data for our imitation learning-based low-level policy, we use an HTC VIVE controller and base station to track the 6-DOF poses of human hand movement. Then we use SteamVR to map the controller movement to the end effector of the Franka robot. In low-level policy training, we use 100 human demonstrations for one kitchen setting and 200 demonstrations for two kitchen settings (10 demonstrations for each task). We employ the Adam optimizer with a learning rate of 3 \u00d7 10-4. The training is conducted on one NVIDIA GeForce RTX 3090 with a batch size of 16. We apply color dropout and translation augmentation techniques to improve the model's performance.\nA. Main Results\nTo perform well on long-horizon tasks, we need to ensure the following: 1) a generalizable low-level policy capable of performing various skills, 2) an adaptable low-level policy that can compose these skills together, and 3) a situation-aware high-level planner with strong reasoning abilities. We systematically evaluate our framework on each of the three criteria individually, then integrate all these components and test our framework's performance on long-horizon tasks (See Figure 1 for qualitative results). If not otherwise stated, each reported accuracy rate is obtained with 10 trials.\nLow Level Skills. We train and evaluate our pipeline's per-formance across five distinct skills: grasping, placing, turning, opening, and closing. Each skill is tested with various objects and task scenarios (Pick is tested 5 times for each of 5 objects, place 5 times for each of 4 locations, and other skills 10 times total). To show the generalization ability of our low-level policy, we report the individual skill accuracy with and without distractors, where the distractors include 1 - 2 extra toys placed in the kitchen to make it more cluttered (see Table I)."}]}