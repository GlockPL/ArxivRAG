{"title": "Improving Multimodal LLM Reasoning via Mitigating Visual Hallucination", "authors": ["Haojie Zheng", "Tianyang Xu", "Hanchi Sun", "Shu Pu", "Ruoxi Chen", "Lichao Sun"], "abstract": "Multimodal large language models (MLLMs) have advanced the integration of visual and linguistic modalities, establishing themselves as the dominant paradigm for visual-language tasks. Current approaches like chain of thought (CoT) reasoning have augmented the cognitive capabilities of large language models (LLMs), yet their adaptation to MLLMs is hindered by heightened risks of hallucination in cross-modality comprehension. In this paper, we find that the thinking while looking paradigm in current multimodal CoT approaches\u2014where reasoning chains are generated alongside visual input-fails to mitigate hallucinations caused by misleading images. To address these limitations, we propose the Visual Inference Chain (VIC) framework, a novel approach that constructs reasoning chains using textual context alone before introducing visual input, effectively reducing cross-modal biases and enhancing multimodal reasoning accuracy. Comprehensive evaluations demonstrate that VIC significantly improves zero-shot performance across various vision-related tasks, mitigating hallucinations while refining the reasoning capabilities of MLLMs. Our anonymized code repository can be found at https://github.com/Terry-Xu-666/visual_inference_chain.", "sections": [{"title": "1. Introduction", "content": "Large Language Models (LLMs), such as GPT-4 [1] and Llama [34], have driven remarkable advancements in world comprehension [11] and reasoning capability [12]. Prompting techniques like CoT [9, 28, 38] have been developed to enhance LLMs' ability to handle complex tasks through human-like step-by-step reasoning. Meanwhile, MLLMs have rapidly advanced in recent years [1, 2, 20, 21], extending LLMs' capabilities into the multimodal realm by integrating visual backbones to align visual and language representations. MLLMs have demonstrated exceptional performance in a range of vision-related tasks, including visual question answering [4], object recognition [7, 42], and video comprehension [22], highlighting the impressive evolution of AI-driven visual-language understanding. The success of CoT prompting in unimodal contexts suggests a promising extension to multimodal scenarios. Due to the integration of pretrained vision models [27, 32] and language models [44] in MLLMs, various types of hallucinations have been observed [5, 16, 18], including nonexistent object generation [33], visual misinterpretations [17], and cross-modality biases [35].\nOffering advantages in tackling complex multimodal tasks, current prompting approaches [46, 47] predominantly adhere to the thinking while looking paradigm, where reasoning occurs simultaneously with the integration of visual elements. However, this paradigm encounters substantial challenges due to the prevalence of hallucinations, which undermine both the reliability of the reasoning process and the accuracy of the responses. As illustrated in Figure 1, the MLLM often falls into stereotypes when processing a question-image pair, where it recalls similar prior contexts and overlooks subtle variations, leading to erroneous responses. Even with CoT prompting, the model tends to rely on memory-based stereotypes rather than engaging in accurate reasoning, which leads to incorrect responses.\nTo overcome hallucinations from visual inputs and harness the reasoning capabilities of LLMs, we propose the Visual Inference Chain (VIC), which introduces a reasoning process that occurs prior to engaging with visual elements, following the thinking before looking paradigm. This approach mirrors human cognition [30, 31], where reasoning often precedes perception. For example, when adults hear a question before seeing the image, they generate a preliminary plan based on their accumulated experience. The question activates relevant memories and contextual knowledge, allowing them to deduce a forward-looking reasoning strategy ahead of engaging in visual stereotypes. This underscores that the direct impact of visual elements on the reasoning process is relatively limited. Analogous to human cognition, MLLMs can adopt the thinking before looking paradigm. By leveraging this paradigm, VIC taps into the accumulated forward-looking reasoning capabilities of powerful LLMs, enabling MLLMs to anticipate and recognize patterns more efficiently by dynamically adjusting reasoning steps. Additionally, the VIC framework employs a systematic multi-step detachment strategy to minimize compounding hallucinations from both the question and the image, further enhancing the MLLMs' reasoning capabilities for complex tasks.\nIn this work, we demonstrate that our thinking before looking strategy outperforms the conventional thinking while looking approach in multimodal reasoning tasks. Our method demonstrates improved performance in empirical studies on GPT-series [26] and Gemini-series [6] models across various visual question benchmarks. VIC achieves notable improvements on hallucination-specific benchmarks such as MMVP [33], HallusionBench [10], and POPE [16], as well as on general multimodal benchmarks including MME [8], MathVista [23], and SEED-Bench [14]. For instance, the Gemini 1.5 Pro sees a substantial improvement of 31.74% on the MMVP benchmark, in a meanwhile the GPT-40 mini model shows an increase of 16.59%. Across all benchmarks, GPT-series models average a 8.02% refinement and Gemini-series models show an average improvement of 7.19%, underscoring both the effectiveness and robustness of our method."}, {"title": "2. Related Works", "content": "CoT reasoning with LLMs. Chain-of-thought (CoT) reasoning [38] has significantly enhanced large language models (LLMs) performance by guiding them to break down complex tasks into intermediate reasoning steps. Subsequent work introduced self-consistency [37] where multiple reasoning paths are generated, and the most consistent answer is chosen. Recent advances focus on optimizing CoT with example selection and enhanced reasoning frameworks [45], exploring selecting diverse examples to guide CoT reasoning. Step-Aware Verifier framework enhances CoT by incorporating a verification step at each intermediate reasoning stage, improving reasoning reliability [15]. Meanwhile, Tree of Thought (ToT) [40] extends CoT by exploring multiple reasoning paths in a tree structure, ensuring thorough consideration of potential solutions. Other refinements allow models to self-reflect on and adjust their reasoning steps [39].\nMultimodal CoT reasoning. Considering the natural gap between text and vision data, Multimodal Large Language Models (MLLMs) are less capable of utilizing the reasoning ability of CoT. MMCOT [46] adds a rationale generation block before answer inference. DDCoT [47] is proposed to enhance multimodal reasoning by assigning specific tasks to language and visual models. Meanwhile, Visual CoT [29] incorporates the notion of segmentation, and performs well for object-detection tasks but lacks versatility. Image-of-Thought [48] further explores the MLLMs CoT prompting by introducing specific actions like segmentation, zoom-in, and color-space conversions in the CoT chain, extending the applicable scenarios with the sacrifice of flexibility. Compositional CoT [24] extracts the compositional information of images as a scene graph (SG) and then utilize it during the CoT process. Other works like CoCoT [43] and KAM-CoT [25], consider multiple input image scenarios and incorporate knowledge graphs (KG) of multimodal information. However, many such techniques rely on fixed templates for extracting predetermined information, resulting in a lack of flexibility and the utilization of reasoning capabilities. The straightforward thinking while looking approach struggles to resolve problematic cross-modal interactions, often resulting in hallucinations.\nHallucination in MLLMs. Hallucination remains a significant challenge in MLLMs, where models generate information that is factually incorrect or irrelevant to the given inputs [3]. In multimodal reasoning, hallucinations can occur when models produce textual outputs not grounded in the visual data, leading to fabricated details in tasks such as image captioning or visual question answering (VQA). For hallucination evaluation, FaithScore [13] extracts fine-grained atomic facts from the generated answer and then conducts consistency verifications. POPE, a polling-based query method [16] is proposed for a better evaluation of object hallucination. Recently, HallusionBench [10] further explored the hallucination evaluation, by editing the original input image and forming different text-image pairs to diagnose failure types from language hallucination and visual illusion. Several hallucination-mitigating methods are also proposed. To conduct instruction tuning that mitigate hallucination, LRV [19] serves as the first large and diverse visual instruction tuning dataset and VIGC [36] aims for data generation. As for training-free method, Woodpecker [41] uses fixed steps to correct hallucination in MLLMs. Our method, however, generates applicable reasoning trajectories without visual elements for a given question, which is more flexible and accurate than fixed analyzing steps, and could be further utilized across different VQA tasks."}, {"title": "3. Method", "content": "In this section, we begin by introducing the preliminary architecture of MLLMs and the process of thinking while looking paradigm in Section 3.1 and Section 3.2 respectively. In Section 3.3, we present the foundation of thinking before looking paradigm along with a detailed explanation of how forward-looking reasoning can enhance the reasoning process more effectively than thinking while looking paradigm. Additionally, we explore two key components in the framework in Section 3.4 and 3.5, respectively. The detailed implementation of the prompting mechanism discussed in this section is provided in Appendix D, and the overview of our method is illustrated in Figure 2."}, {"title": "3.1. Multimodal LLM", "content": "MLLMs are designed to handle data from various modalities, such as text, images, and audio, enabling the integration and generation of multimodal information. In the task of Visual Question Answering (VQA), the MLLM is provided with an image input I and a textual input Q. These inputs are then encoded into a shared representation space using a pre-trained visual encoding model, and a fixed text tokenization process for the textual data. The encoded representations from both visual and textual data are subsequently processed by large language model. We characterize the whole MLLM as f'(*), which has been trained on unified representation data from both modalities. During inference, the model generates a response A given I and Q, which can be formalized as:\n$A = f'(I, Q).$\nFor the sake of simplicity, we ignore the tokenizer and pre-trained visual encoding model since they primarily serve as preprocessing components. This formulation allows MLLMs to focus on leveraging the combined information from multiple data types, enabling them to effectively process and respond to complex, multimodal queries with greater efficiency."}, {"title": "3.2. Thinking while looking", "content": "A simple VQA inference process is illustrated in Equation 1. Moreover, the thinking while looking paradigm involves step-by-step reasoning trajectory while simultaneously processing the visual input. This approach generates a reasoning strategy chain {sn}k=1, accompanied by corresponding rationales, denoted as {rn}k=1, where the value of k varies dynamically depending on both the specific model and the given question-image pair. In parallel with generating these reasoning steps and rationales, the model also produces the final answer A.\n$({S_n, r_n}^k_{n=1},A) = f'(I, Q, P_{cot}).$\nIn this formula, {sn,rn}k=1 represents the sequence of reasoning steps and corresponding rationales. Pcot denotes the prompt used for CoT prompting, which can be either a zero-shot prompt such as \"Let's think step by step\" or a few-shot prompt designed in an in-context learning manner."}, {"title": "3.3. VIC generation", "content": "The limitations of the thinking while looking paradigm inspire the development of the thinking before looking paradigm. This new approach promotes forward-looking reasoning, enhancing the reasoning process and improving the quality of rationality. Separating visual and textual inputs allows for more structured thinking and clearer cognitive steps. This approach decouples the question from the image, reducing bias in the reasoning steps sequence and enhancing overall reasoning performance.\nThe language models have internalized extensive reasoning knowledge from large-scale pre-trained data, allowing them to gain beneficial insights into image reasoning analysis. Due to their highly developed pattern recognition abilities, these models can accumulate sufficient background knowledge and generate reasonable reasoning steps for visual elements. Thus, LLMs f(*) can automatically generate forward-looking reasoning steps as an average over a broader context of similar situations, triggered by the input (Q, Pvic), rather than focusing solely on a specific input pair (Q, I). This process discretizes the reasoning steps from the specific input pair, reducing hallucinations while maintaining the validity of instructions for image analysis, as these instructions align with the aggregate of the most relevant contextual knowledge. Consequently, the visual inference process can be expressed as follows.\n${S_n}^k_{n=1} = f(Q, P_{vic})$\nBy employing this specific prompt, we generate a visual inference chain, denoted as {sn}k\u22121n=1, which serves as the reasoning process derived exclusively from the given question. This process can be divided into two main components. The initial k \u2212 1 steps, {sn}k\u22121n=1, correspond to the sequence of instructions related to both recognition and reasoning. And the final step, sk, of the visual inference chain introduces a format instruction based on the specific question. This step is critical in further enhancing the framework's ability to follow complex instructions effectively.\nThe advantage of the thinking before looking phase is that it eliminates the immediate need for visual information. As a result, the model f(*) can function as either a large language model or a multimodal language model operating in a blind mode. This concept of thinking before looking allows us to leverage the superior reasoning capabilities of"}, {"title": "3.4. VIC rationale extraction", "content": "Hallucinations commonly arise from deep entanglement between image and textual inputs. In this step, we decouple the original question and requiring the MLLM to recognize and follow the visual inference chain step by step, mitigating the effects of textual bias. Moreover, the visual inference chain provides a more precise trajectory compared to the original question, necessitating detailed extraction of relevant information and thus reducing the risk of hallucinations by visual inference chain.\nUnlike the previous step, which can use any modality model f, this step specifically employs the MLLM f'. Leveraging the multitask-following capability of closed-form models, we generate the VIC rationales in a single step to produce the entire set of rationales {rn}k\u22121n=1, denoted collectively as R. We further explore the differences between single-step rationale extraction and multi-step rationale extraction in our ablation experiments.\n$R = f'(I, P_{extract}, {S_n}^{k-1}_{n=1})$\nIn this formula, I represents the image corresponding to the specific question Q. The sequence {sn}k\u22121n=1 refers to the first k \u2212 1 steps of the process, excluding the format instructions. Pextract is a prompt designed to integrate the image with the visual inference chain, facilitating effective information extraction."}, {"title": "3.5. Answer Inference", "content": "In the final step, we provide the same inputs to the MLLM f' as used during the VIC rationale generation phase. These inputs include the original question-image pair Q, I, the VIC rationale results R, the format instruction sk, and the reflection prompt Preflect.\n$A = f'(I, Q, R, P_{reflect}, S_k)$\nThis process incorporates the VIC rationale results as additional information to support the model's response. Furthermore, we introduce a reflection mechanism at this stage, encouraging the model to reconsider the question, image, and VIC rationale results, rather than treating the rationale as the definitive answer. The format instruction sk ensures the response adheres to the desired format by guiding the model's analysis of the user's question or query, thereby improving instruction-following performance."}, {"title": "4. Experiment", "content": ""}, {"title": "4.1. Experiment Setup", "content": "Datasets Our framework is evaluated on six benchmark datasets. To evaluate the generality and versatility of the VIC framework, we tested it across two key benchmark categories: (1) Hallucination detection benchmark including HallusionBench [10], MMVP [33], and POPE [16], all of which are designed to analyze hallucinations that are prone to occur in different forms. (2) General multimodal capability benchmark, MME [8] and SEED-Bench [14], which cover general and comprehensive forms of visual question answering. MathVista [23] that focuses on math element recognition and reasoning problems, evaluates the compound capability to solve visual math challenges. We discuss the details and implementations for each benchmark in Appendix A.\nBaseline In our experiments, we compare the proposed VIC methodology with two primary baselines, as outlined in Table 1. The first baseline involves applying the model directly to each benchmark without incorporating any specialized techniques, enabling us to evaluate the added benefit of our method to the pretrained model's performance. The second baseline is zero-shot CoT prompt engineering. In this approach, we append the reasoning prompt such as \"Let's think step by step\" to the input, following the question. Since the model's output in this case includes multiple reasoning steps, we then pass it through an answer extractor to derive the final answer, in a manner similar to the final stage of the VIC method. This comparison highlights the differences and advantages of our thinking before looking strategy. Additionally, we introduce human performance and random choices as further baselines, providing a basic performance reference point for each benchmark.\nImplementation We conducted our experiments primarily on four popular closed-source MLLMs: Gemini 1.5 Flash, Gemini 1.5 Pro, GPT-4o, and GPT-4o mini. The reason for choosing these models mainly involved two aspects. (i) Our approach is grounded in the belief that the thinking before looking capability should not be exclusive to large models, driving us to experiment on both large and small models. (ii) Our method also leverages the long instructions following ability, which is a key strength of closed-source models. The experiments were conducted in three steps. Firstly, we utilized the VIC-prompt, where the input question text was fed into the models to generate Visual Inference Chain. These generated instructions were then paired with the original image to extract VIC rationals. Finally, we derive the answer to the question based on the rationale we extracted. Additionally, we also ran several complementary experiments on open-source models, such as Qwen-VL. For"}, {"title": "4.2. Results", "content": "The results of the Visual-Inference-Chain on six different benchmarks using two open-source models are listed in Table 1. We primarily use average accuracy as the evaluation metric to quantify the models' performance, except for the MME benchmark, where we retain the original composite evaluation score that combines accuracy and a refined accuracy metric (Accuracy+). This approach allows for a consistent comparison with previous records on MME. For the MMVP benchmark, we use pair accuracy to measure the model's ability to overcome the hallucination caused by CLIP.\nAcross the comprehensive benchmarks MME and SEED-Bench, the models show an average performance improvement of 9.13%, demonstrating that our method VIC consistently enhances performance across all tasks. These results highlight the efficacy of VIC in boosting multimodal reasoning capabilities. For the Mathvista benchmark, although the average gain is slightly reduced to 3.15%, this is largely due to the nature of this benchmark, which lacks detailed explanations of questions and is heavily reliant on visual information. For example, questions like \"Is RO greater than 0?\" require direct visual interpretation, making it difficult to generate an effective visual inference chain without explicit vision inputs. Despite these challenges, the improvement underscores the versatility of VIC, even in"}, {"title": "4.3. Ablation Experiment", "content": "By adopting a blind input approach, where the model operates independently of the image, we can leverage various models for VIC generation. In our experiments, we employed a range of models, including pure language models such as GPT-4 Turbo and Qwen-Max, alongside multimodal models like GPT-4o mini and Gemini 1.5 Flash, as visual inference chain generators. Additionally, we conducted ablation experiments to investigate the differences between single-step and multi-step VIC rationale extraction, the impact of reflective prompting during the answer generation phase, and the performance of open-source models. For more detailed results, please refer to Appendix C.\nDifferent VIC generator. As shown in Figure 5, different visual inference chain (VIC) generators significantly affect VIC performance. Leveraging the \"thinking before looking\" paradigm, we can select either a pure language model or a multimodal model operating in a blind mode. Thus, we tested GPT-4 Turbo, Qwen-Max, GPT-4o mini, and Gemini 1.5 Flash as VIC generators on two benchmarks, HallusionBench and SEED-Bench. On SEED-Bench, the maximum performance variation is 3.95% for Gemini 1.5 Flash and 5.98% for GPT-4o mini. A key finding is the minimal performance difference between chains generated by pure language models and those by multimodal models. For instance, Qwen-Max's visual inference chain performs best for Gemini 1.5 Flash on HallusionBench, while GPT-4o mini's chain achieves the highest score for Gemini 1.5 Flash on SEED-Bench. This suggests that both pure language and multimodal models offer comparable reasoning capabilities for image-related tasks, even though multimodal models are more specialized in image comprehension. Overall, there is no clear pattern in which chain consistently performs best, as effectiveness varies based on factors such as the compatibility of the visual inference chain with the MLLM and the VIC generator's ability to address specific question types. For example, the GPT-4o mini visual chain excels on general questions in SEED-Bench but performs the weakest on HallusionBench.\nOne step VIC rationale extraction. The VIC generation process uses two main methods: a single-step approach and a multi-step approach. In the single-step method, the entire inference chain is processed as a unified input, while the multi-step approach completes tasks incrementally, integrating intermediate results at each step. Evaluations on the HallusionBench and SEED-Bench benchmarks show distinct advantages for each approach. Overall, the single-step method yields more stable improvements, ensuring greater consistency across tasks. This approach also reduces latency and computational cost due to its single-input nature. In contrast, the multi-step approach requires re-inputting images and prompts at each step, which increases response times and resource demands. Although the multi-step method has some advantages on benchmarks like Hal-"}, {"title": "5. Discussion", "content": "Although our thinking before looking framework demonstrates remarkable performance across diverse vision-text tasks, we view it as a crucial enhancement rather than the ultimate solution for MLLMs reasoning. It provides a fresh perspective in this domain and holds potential for integration with the thinking while looking paradigm to achieve superior outcomes. In comprehensive evaluations of these two approaches, we found that the performance of thinking before looking lagged behind thinking while looking in certain categories. We suggest that future advancements should prioritize merging these paradigms through a selection mechanism or a process of mutual reflection. Such a combination would allow the paradigms to complement each other effectively, addressing their individual limitations and paving the way for more sophisticated reasoning strategies."}, {"title": "6. Conclusion", "content": "In this paper, we introduce the Visual Inference Chain (VIC) framework, a novel method that mitigates reasoning biases in MLLMs by decoupling visual and textual inputs, advancing the thinking before looking paradigm. By systematically separating visual elements, VIC framework enhances reasoning robustness, significantly reduces hallucinations, and improves performance across diverse vision-language tasks. Our experiments demonstrate that VIC consistently boosts zero-shot performance and provides detailed insights into the impact of different VIC generators, underscoring the effectiveness of reflective prompting. Overall, VIC framework enhances accuracy and strengthens the reliability of multimodal reasoning effectively."}]}