{"title": "BinaryAlign: Word Alignment as Binary Sequence Labeling", "authors": ["Gaetan Lopez Latouche", "Marc-Andr\u00e9 Carbonneau", "Ben Swanson"], "abstract": "Real world deployments of word alignment are almost certain to cover both high and low resource languages. However, the state-of-the-art for this task recommends a different model class depending on the availability of gold alignment training data for a particular language pair. We propose BinaryAlign, a novel word alignment technique based on binary sequence labeling that outperforms existing approaches in both scenarios, offering a unifying approach to the task. Additionally, we vary the specific choice of multilingual foundation model, perform stratified error analysis over alignment error type, and explore the performance of BinaryAlign on non-English language pairs. We make our source code publicly available.", "sections": [{"title": "1 Introduction", "content": "Word alignment refers to the task of uncovering word correspondences between translated text pairs. The automatic prediction of word alignments dates back to the earliest work in machine translation with the IBM models (Brown et al., 1993) where they were used as hidden variables that permit the use of direct token to token translation probabilities. While state of the art machine translation techniques have largely abandoned the use of word alignment as an explicit task (Li, 2022) other use cases for alignments have emerged including lexical constraint incorporation (Chen et al., 2021b), analysing and evaluating translation models (Bau et al., 2018; Neubig et al., 2019), and cross-lingual language pre-training (Chi et al., 2021b).\nIn many real-world applications word alignment must be performed across several languages, often including languages with manually annotated word alignment data and others lacking such annotations. We refer to those languages as high and low-resource languages respectively. While word alignment for high-resource languages can be learned in a few-shot or fully supervised setting depending on the amount of data, for low-resource languages zero-shot learning strategies must be employed due to data scarcity.\nState-of-the-art supervised techniques formalize the task of word alignment as a collection of SQUAD-style span prediction problems (Nagata et al., 2020; Wu et al., 2023) while in zero-shot settings the best performing methods induce word alignment from the contextualized word embeddings of mulitingual pre-trained language models (mPLMs) (Jalili Sabet et al., 2020; Dou and Neubig, 2021; Wang et al., 2022). From a practical perspective, this discrepancy in the preferred method adds complexity to the deployment of word alignment models in real-world applications where both high and low-resource languages must be supported.\nWe observe a deeper issue that both span prediction and contextualized word embeddings are sub-optimal as each induces a bias in word alignment models that limits their accuracy. Span prediction methods cannot robustly deal with discontinuous word alignments without relying on complex post-processing and hyper-parameter tuning. Contextualized word embeddings method cannot deal effectively with untranslated words and one-to-"}, {"title": "2 Related Work", "content": "Recently, mPLM based approaches have significantly outperformed bilingual statistical methods (Och and Ney, 2003; Dyer et al., 2013; \u00d6stling and Tiedemann, 2016) and bilingual neural methods (Garg et al., 2019; Zenkel et al., 2020; Chen et al., 2020, 2021a; Zhang and van Genabith, 2021).\nAmong those approaches, we distinguish methods that achieve good performance without relying on manually annotated word alignment datasets (Jalili Sabet et al., 2020; Dou and Neubig, 2021; Wang et al., 2022) from supervised methods that leverage existing word alignment datasets to train high performing word aligners (Nagata et al., 2020; Wu et al., 2023).\nThe first type of method relies on the approach of SimAlign (Jalili Sabet et al., 2020) which proposes to induce word alignment from the contextualized word embeddings of mPLMs pre-trained on non-parallel data. AwesomeAlign (Dou and Neubig, 2021) builds on top of this approach and proposes to fine-tune mPLMs on parallel text with different objectives to improve the quality of the contextualized word embeddings. More recently, AccAlign (Wang et al., 2022) showed that models trained to learn language-agnostic sentence-level embeddings also learn strong language-agnostic word-level embeddings and set the state of the art in the zero-shot setting. Also, Wang et al. (2022) show that fine-tuning on existing word alignment datasets improves performance of AccAlign on language pairs unseen during word alignment fine-tuning.\nOur method is different from this body of work because our training and inference objective differ. We formalize word alignment as a binary sequence labeling task while we can see those methods as framing word alignment as a token retrieval task.\nIn terms of approaches trained and evaluated in a supervised setting SpanAlign (Nagata et al., 2020) formalizes word alignment as a collection of"}, {"title": "3 Method", "content": "Given a sentence X with n words and a translation into another language Y with m words, the task of word alignment is to produce an n by m adjacency matrix for the bipartite graph with the words of X on one side and the words of Y on the other (refer to Figure 1 for an illustration). As our model will employ commonly used subword tokenization preprocessing we assume access to an invertible tokenizer, often implemented in practice with a list of subword units, a greedy algorithm for subword chunking, and leading symbols to denote word continuation in the vocab file (Sennrich et al., 2016; Wu et al., 2016).\nWe present BinaryAlign, a novel word alignment approach using a binary sequence labeling model, shown in Figure 2. The inputs to this model are a subword tokenized source sentence $X = x_1, x_2, ..., x_{|x|}$, a subword tokenized target sentence $Y = y_1, y_2, \u2026, y_{|Y|}$, and a reference word $w_x = x[i : j]$ which is a subspan of X. We model the distribution of a binary alignment vector A of size |Y| in which each entry $a_k$ indicates if the word in Y that contains $y_k$ is aligned to $w_x$.\nWe first preprocess X by surrounding $w_x$ with unique separator tokens and then cross-encode the source and target sentences with an mPLM. For each token $y_k$ in the target sentence we pass its final encoded representation through a linear layer to produce a single logit $z_k$. We model $a_k$ with a logistic function using $z_k$ as its parameter:\n$p(\\alpha_{\\kappa} = 1|\\omega_{\\chi}) = \\frac{1}{1+e^{-z_k}}$\n(1)\nA supervised signal for token level alignments $a_k$ is easily divined from word alignment data and so this form is sufficient to estimate the parameters of the model. However, our true inference-time goal of word to word alignment requires the use of additional heuristics. To motivate these heuristics we formalize W as the inverse of the surjective mapping between the token indices in Y and its corresponding words; for any subspan $w_y$ of Y, $W(w_y)$ returns the token indices in Y that compose $w_y$.\nGiven an aggregation function agg, we define the probability of the event $a'$ that there exists an alignment between word $w_x$ in X and word $w_y$ in Y as\n$p(\\alpha') = agg\\;p(a_k = 1|w_x)$    (2)\n$\\forall_{k \\in W(w_y)}$\nPreliminary experiments suggest that the maximum aggregation strategy yields slightly superior performance compared to the mean and minimum aggregation strategies; hence, we adopt it for all subsequent experiments.\nWord alignment in its general form is a symmetric problem in that we would expect the same answer if the source and target were swapped. However, like most leading word alignment methods, our method is asymmetric; the source and target sentences are handled differently. This deficiency is empirically detrimental to performance with the common remedy being to perform alignment in both directions and then to merge the two predictions in some manner.\nWe use the following symmetrization technique: letting $p_{X\u2192Y}$ denote the use of X as the source and Y as the target sentence, we average $p_{X \u2192Y}(a')$ and $p_{Y\u2192X}(a')$ and apply a threshold decision rule to make our final inference prediction. While outside the scope of this study, we note that various other options exist and have been explored in previous work such as bidirectional average (Nagata et al., 2020) or intersection, union and grow-diag-final: the default symmetrization heuristics supported in Moses (Koehn et al., 2007)."}, {"title": "4 Experiments", "content": "We use seven datasets of manually annotated word alignment data for our main experiments: French-English (fr-en), Chinese-English (zh-en), Romanian-English (ro-en), Japanese-English (ja-en), German-English (de-en), Swedish-English (sv-en) and ALIGN6. The ja-en data comes from the KFTT word alignment data (Neubig, 2011), while the ro-en and fr-en data are taken from Mihalcea and Pedersen (2003) and the de-en data is provided by Vilar et al. (2006). Also, the zh-en data is"}, {"title": "4.2 Experimental setup", "content": "Unseen alignment experiments: In our unseen alignment experiments, models are not fine-tuned on manual word alignment data of the tested language pair. This replicates a common real-world situation in which alignment data set is not available for a language pair and models must leverage knowledge gleaned from other language pairs and pre-training. This setting is usually referred as zero-shot cross-lingual transfer (Conneau et al., 2020; Chi et al., 2021a). We follow Wang et al. (2022) and fine-tune our model on ALIGN6 and use sv-en as our validation set. We evaluate our method following the experimental protocol of previous work"}, {"title": "4.3 Baseline methods", "content": "Unseen alignment experiments: In this setting, we compare BinaryAlign to the three main bodies"}, {"title": "4.5 Word Alignment Evaluation Metric", "content": "Following previous works (Wang et al., 2022; Dou and Neubig, 2021), we evaluate performance using Alignment Error Rate (AER). Given a set of sure alignments (S), possible alignments (P) and predicted alignments (H) we can calculate AER as follows:\n$AER(S, P, H) = 1 - \\frac{|H \\cap S| + |H \\cap P|}{|H| + |S|}$\nFollowing the protocol in Wu et al. (2023) we use only sure alignments for training but we evaluate on both sure and possible alignments when the distinction is available."}, {"title": "4.6 Results and Discussion", "content": "We performed several experiments to validate BinaryAlign. First we compare our method to other state-of-the-art methods in three different levels of supervision: full available supervision, few-shot (32 samples), and unseen languages (zero-shot cross-lingual transfer). We also evaluate on non-English language pairs. Next, we evaluate the impact of choices for mPLM foundation and symmetrization. Finally, we discuss how our problem formulation compares to span prediction and contextualized word embedding based approaches in different situations."}, {"title": "4.6.1 Comparison to State-of-the-Art", "content": "Unseen alignments: As a first experiment, we apply all methods to new language pairs without performing word alignment fine-tuning on the tested language pair as explained in 4.2. Table 1 reports the AER of all methods. BinaryAlign is the new state-of-the-art on all language pairs. In particular, it outperforms AccAlign by 3.0 points of AER on average. Since AccAlign and BinaryAlign share the same pre-training dataset (ALIGN6), this indicates that our word alignment problem formulation performs better than inducing word alignment from contextualized word embeddings. This is also true when comparing with SpanAlign pre-trained on the same data (ALIGN6). This indicates that our formalization of word alignment promotes learning more language-agnostic signals from word alignment datasets when compared to existing methods."}, {"title": "Full and few shot supervision:", "content": "We compare BinaryAlign to the other baseline methods after fine-tuning on alignment data with few samples and the whole training data set. Table 2 shows the results for both supervision levels.\nOur method achieves new state-of-the-art on all tested language pairs and with both levels of supervision. On average it outperforms WSPAlign, the previous state-of-the-art, by 2.1 points of AER with full supervision and 3.1 with few-shot supervision. Even without pre-training on ALIGN6 our method outperforms all methods. Given that WSPAlign was pre-trained on 2 millions samples, it indicates that BinaryAlign promotes sample efficiency.\nFinally, we highlight that by using only 32 samples for few-shot supervision BinaryAlign outperforms SpanAlign regardless of pre-training. This reinforces the performance improvement of formalizing word alignment as a binary token classification objective over span prediction."}, {"title": "Impact of pre-training on other languages:", "content": "Table 2 reports the results of our method with and without pre-training on ALIGN6. We conclude that pre-training improves AER with few-shot as well as full supervision. However, we observe a smaller improvement with full supervision which suggests that the benefit of pre-training on other languages is inversely correlated with the amount of in-domain word alignment data. While pre-training encourages sample efficiency, we did not find any indication that it could hinder performance."}, {"title": "Non-English language pairs:", "content": "Because usually mPLMs perform better in English it is important to investigate how our method performs on non-English language pairs. Table 3 reports results on"}, {"title": "4.6.2 Design Choices", "content": "mPLM Architecture: Our proposed reformulation of the word alignment problem does not depend on a particular mPLM architecture. In this experiment,"}, {"title": "4.6.3 Post-analysis of errors", "content": "In this section we analyze how the proposed problem formulation of BinaryAlign improves accuracy in complex word alignment situations. We inspect results in three situations: 1) words that are untranslated, also referred as null words (Jalili Sabet et al., 2020) (2) words that are aligned to multiple words (3) words that are aligned to multiple non contiguous words. For each situation, we report the percentage of correctly aligned words in Table 6. Details on how we computed our metric can be found in A.3. Results indicate that our method handles these situations better than both competing methods. This is especially true when aligning multiple non contiguous words which was the main motivation for our reformulation. The prevalence of these situations in a given language pair modulates the performance gain of our method over the others."}, {"title": "5 Conclusion", "content": "We presented BinaryAlign, a novel word alignment training and inference procedure. In particular, we proposed to reformulate the word alignment problem as a binary token classification task. We showed that because of this reformulation BinaryAlign outperforms existing methods regardless of the degree of supervision. In addition we showed that it overcomes the inherent limitations of previous methods relying on span prediction and softmax. As a result, we made the word alignment task easier to tackle by using a single model for both high and low-resource languages.\nIn the future we plan to explore the use of larger decoder-only or encoder-decoder models such as mT5 (Xue et al., 2021) to see how much alignment performance will increase. We also plan on investigating knowledge distillation techniques to improve the inference time of our method."}, {"title": "6 Limitations", "content": "The inference cost is the main limitation of our method. When using symmetrization, it has to perform a forward pass for each word of both sentences, which can be slow with long sequences."}, {"title": "A Appendix", "content": "A.1\nExperimental Environment\nFor all our experiments we use one NVIDIA\nQuadro RTX 6000. Fine-tuning on ALIGN6 took 4\nhours and 30 minutes while our fully supervised experiments took on average 20 minutes per dataset.\nA.2 Dataset statistics\nTable 8 shows the number of samples that our training, validation and test set contains for all level of supervision. All dataset are the same as in Wang et al. (2022). The de-en, ro-en, fr-en and ja-en train-test splits are the same as the one used in Wu et al. (2023). We could not get the zh-en data used in Wu et al. (2023) because the dataset is not publicly available.\nA.3 Metric details\nA.3.1 F1 score\nGiven a set of sure alignments (S), possible alignments (P) and predicted alignments (H), we can compute the Recall, Precision and F1 score as follows:\n$Recall(H, S)$\n$\\frac{|H \\cap S|}{|S|}$\n$Precision(H, P) = \\frac{|H \\cap P|}{|H|}$\n$F1(H, S, P) = \\frac{2* Precision * Recall}{Precision + Recall}$\nWhen S == P, we have:\nAER(H, S, P) = 1 \u2212 F\u2081(H, S, P)\nA.3.2 Post-analysis of errors\nUntranslated words: We report the number of untranslated words correctly aligned by the models over the total number of untranslated words. We consider a word to be correctly aligned if the model has not aligned it to any words in the corresponding translated sentence.\nOne-to-multiple contiguous and non contiguous words: In this case, we report the number of contiguous/non contiguous words correctly aligned by the model over the total number of contiguous/non contiguous words. We consider a word to be correctly aligned if the model has aligned it to the exact same set of ground truth aligned words."}]}