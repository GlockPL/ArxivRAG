{"title": "Cayley Graph Propagation", "authors": ["JJ Wilson", "Maya Bechler-Speicher", "Petar Veli\u010dkovi\u0107"], "abstract": "In spite of the plethora of success stories with graph neural networks (GNNs) on modelling graph-structured data, they are notoriously vulnerable to over-squashing, whereby tasks necessitate the mixing of information between distance pairs of nodes. To address this problem, prior work suggests rewiring the graph structure to improve information flow. Alternatively, a significant body of research has dedicated itself to discovering and precomputing bottleneck-free graph structures to ameliorate over-squashing. One well regarded family of bottleneck-free graphs within the mathematical community are expander graphs, with prior work-Expander Graph Propagation (EGP)\u2014proposing the use of a well-known expander graph family\u2014the Cayley graphs of the SL(2, Zn) special linear group-as a computational template for GNNs. However, in EGP the computational graphs used are truncated to align with a given input graph. In this work, we show that truncation is detrimental to the coveted expansion properties. Instead, we propose CGP, a method to propagate information over a complete Cayley graph structure, thereby ensuring it is bottleneck-free to better alleviate over-squashing. Our empirical evidence across several real-world datasets not only shows that CGP recovers significant improvements as compared to EGP, but it is also akin to or outperforms computationally complex graph rewiring techniques.", "sections": [{"title": "1 Background", "content": "Graph preliminaries. Given an undirected graph denoted as G = (V, E), where V and E denote its nodes and edges respectively. The topology of the graph is encoded in the adjacency matrix A \u2208 R|n|\u00d7|n|, where the number of nodes n = |V|. Let D = D(G) denote the diagonal matrix of degrees as given by Dvv = dr. The normalised graph Laplacian L = L(G) is defined by L = D-1/2(D \u2013 A)D-1/2. From the normalised Laplacian L the eigenvalues 0 = \u03bb\u03bf \u2264 \u03bb1 < ... <"}, {"title": null, "content": "\u03bb\u03b7-2 \u2264 \u03bb\u03b7\u22121. Importantly, from this derivation the spectral gap of graph G is X1 \u2013 10 = 11; the Cheeger inequality then defines that a larger spectral gap of graph G is an indicator of good spectral expansion properties. Accordingly, a graph with desirable expansion properties (or a larger spectral gap) defines that it has strong connectivity, or alternatively it is globally lacking bottlenecks [13]."}, {"title": null, "content": "Expander graphs. An expander graph is categorised by its unique properties of being both sparse and highly connected with the number of edges scaling linearly with the number of nodes (|E| = O(|V|)). One such expansion property that an expander graph satisfies is derived from the aforementioned Cheeger inequality. As a result, in essence, expander graphs do not have any bottlenecks [10]; in Section 3 we further define and explore this link."}, {"title": null, "content": "Due to the definition of an expander graph, there are consequently several known construction approaches [17, 18]. We will focus on the deterministic algebraic approach as introduced in EGP [13]. A family of expander graphs have been precomputed leveraging the well-known theoretical re- sults of special linear groups, SL(2, Zn), for which a family of corresponding Cayley graphs, Cay (SL(2, Zn); Sn), can be derived. Here, Sn ([13], Definition 8) denotes a particular generating set for SL(2, Zn). For appropriate choices of Sn, the corresponding Cayley graphs are guaranteed to have expansion properties. Moreover, from Figure 1 we see that the constructed graph are 4-regular, (|E| = 2|V|). Importantly, although Cayley graphs are scalable, achieving a specific number of nodes is not always feasible; for instance, the node count of Cay (SL(2, Zn); Sn) is given as per:\n\\V (Cay (SL(2, Zn); Sn)] = n\u00b3 \u03a0 (1-2)."}, {"title": null, "content": "Over-squashing. The over-squashing problem was first identified by Alon and Yahav [6], whereby the information in a MPNN is aggregated from too many neighbours, meaning as a consequence they are squashed into fixed-size vectors. This can result in a loss of information [19]. This phenomenon was then formalised [8, 11, 20], showing that the Jacobian of the node features is affected by topological properties of the graph, such as curvature and effective resistance. Furthermore, Di Giovanni et al. [7] analysed how over-squashing impacts the expressive power of GNNs. In the following section, we will address the literature and how current approaches aim to mitigate over-squashing."}, {"title": null, "content": "Over-smoothing. Independent from over-squashing, but another well-known problem impacting the expressivity of GNNs is over-smoothing [21, 22]. This phenomenon occurs in GNNs when the number of layers increases [23, 24], such that node features become increasingly similar [25]. However, the over-smoothing problem is correlated with over-squashing due to a common approach to the latter being graph rewiring; too many additional edges lead to over-smoothing [9]. There are varying approaches to measure over-smoothing for a graph with one such notable metric being the Dirichlet energy [9, 12, 26]."}, {"title": "2 Existing approaches to mitigate over-squashing", "content": "In this part, we explore the current landscape of several novel techniques that try to alleviate the over-squashing phenomenon [6]. In essence, the main principle behind many of these techniques is to decouple the input graph G from the computational one, such that it has structurally fewer bottlenecks. Alon and Yahav [6] simply proposed a rewiring technique that does not require the analysis of the input graph by making the last layer of the GNN fully adjacent (FA), allowing all nodes to interact with each other. The effectiveness of such an approach can be shown by Graph Transformers [27, 28], where every layer is fully-connected. However, such an approach is limited by even modest graph sizes due to it imposing O(|V|2) edges. An alternative approach is a master node [5]; here a new node is introduced, which is connected to all of the nodes within the graph. This approach is effective as it reduces the graph's diameter to 2 by only adding one new node with O(|V|) edges. However, the master node itself becomes the bottleneck. Notably, both of the aforementioned approaches are independent in relation to the input graph topology, therefore satisfying (iv) of no dedicated preprocessing. Conversely, an approach towards this phenomenon is graph rewiring techniques [8, 9, 12], in which the input graph's connectivity is altered in accordance with an optimisation goal."}, {"title": "3 Benefits of Cayley graphs", "content": "In this section, we further explore the benefits of using Cayley graphs and why they have been used as a conduit in a number of over-squashing approaches [10, 13, 29]. In particular, we explore the implications of the expansion properties in regards to the truncated Cayley graphs used by Deac et al. [13]. In contrast to this, we examine the benefits of using the complete and regular Cayley graph structure through the lens of overfitting as per the recent work of Bechler-Speicher et al. [32]."}, {"title": "3.1 Cayley graphs expansion properties", "content": "The structure of a Cayley graph makes it an ideal conduit for the propagation of information due to its sparsity and being highly connected. In particular, the family of Cayley graphs derived from Cay(SL(2, Zn); Sn) are in the magnitude of O(|V|3), and are well-known to have a high spectral gap. As from Section 1, a higher spectral gap is a desirable expansion property, meaning that it is highly connected. Correspondingly, we conjecture that truncating a Cayley graph to match an input graph as done by Deac et al. [13] will have an adverse impact on the desired expansion properties.\nIn Figure 2 we empirically measure this impact using the Cheeger constant, as well as in respect to the diameter. In Appendix (Section D) we extend this analysis through the perspective of effective resistance, which provides a different approach to quantify over-squashing."}, {"title": null, "content": "In the pursuit of this, the recent work of Sterner et al. [33] proposes to heuristically align the Cayley graph, such that it better aligns the corresponding edges. However, this distils to a NP-hard problem, so they opt to implement a greedy strategy to align both graphs. This method imposes a costly preprocessing time which does not comply with our desiderata (i-iv). Consequently, we do not pursue this approach further, but this variant could present an interesting avenue for future work."}, {"title": "3.2 Cayley Graphs as Regular Graphs", "content": "In this subsection, we highlight additional benefits of the Cayley graph."}, {"title": null, "content": "Robustness to Graph Overfitting. It was observed in [32] that graph neural networks tend to overfit the given graph structure, even in cases where it does no provide useful information for the predictive task. Nonetheless, it was shown that regular graphs exhibit robustness to this overfitting. As Cayley graphs are regular graphs, they exhibit this robustness."}, {"title": null, "content": "We repeat the experiments from [32] to ensure the robustness of Cayley graphs to graph overfitting. The task is a binary classification task where the label is independent of the graph, and is computed only over the features. We used the Sum task that was presented in [32]: the label is generated using a teacher GNN that simply sums the node features and applies a linear readout to produce a scalar."}, {"title": null, "content": "We used four different datasets from this baseline by sampling graph-structures from different graph distributions. The set of node feature vectors remains the same across all the datasets, and thus, the datasets differ only in their graph structure. The graph distributions we used are: Cayley graphs over 24 nodes, star-graph (Star) where the only connections are between one specific node and all other nodes, and the preferential attachment model (BA) [37], where the graph is built by incrementally adding new nodes and connecting them to existing nodes with probability proportional to the degrees of the existing nodes. We used the data as is with empty graphs (Empty) as a baseline to compare to."}, {"title": null, "content": "On each dataset, we varied the training set size and evaluated test errors on 5 runs with random seeds. More details on the experiment can be found in the Appendix (Section F)."}, {"title": null, "content": "The results are shown in Figure 3. The GNN trained on the Cayley graphs performs similarly to when trained on empty graphs. Nonetheless, when trained on other distributions, the performance decreases and does not recover even with 4000 training samples. This demonstrate the robustness of Cayley graphs to graph overfitting."}, {"title": null, "content": "Extrapolation. The ability and failures of GNNs to extrapolate to graphs of sizes larger then the one presented during training was examined in Yehudai et al. [38]. It was shown that size generalisation is dependent on local structures around each node, called d-patterns. In particular, if increasing the graph size does not change the distribution of these d-patterns, then extrapolation to larger graph sizes is guaranteed. In the context of Cayley graphs, this implies the following lemma."}, {"title": null, "content": "Lemma 3.1. Let D\u011c be a distribution over Cay(SL(k, Zn); Sn) and Dx be a distribution over node features. Assume a node classification task with training set of infinite size sampled from DG and Dx. Denote the learned model by f. Assume that test examples are sampled from D',, a distribution over Cay (SL(k, Zn'); Sn') where n \u2260 n' and Dx. Then f will have zero test error."}, {"title": null, "content": "The proof is provided in the Appendix (Section G) and it utilises the property that increasing the modulus n, increases the Cayley graph while preserving the regularity degree."}, {"title": "4 Cayley Graph Propagation", "content": "In the previous sections, we provided theoretical motivations for our proposed method of utilising the complete Cayley graph structure. The setup for CGP closely aligns with that of EGP in most aspects. We continue to consider the input to a GNN as a node feature matrix X \u2208 R|V|\u00d7d and an adjacency matrix A \u2208 R|V|\u00d7|V|, which can be fed in an edge-list manner."}, {"title": null, "content": "Additionally, the construction of the Cayley graph Cay(SL(2, Zn); Sn) is still done by choosing the smallest n such that |V (Cay (SL(2, Zn); Sn))| \u2265 |V|. However, we no longer truncate the Cayley graph such that a subgraph A1: V,1: V\nCay (n)\nis extracted \u2013 instead, we opt for a different approach of retaining all of the nodes of the Cayley graph, and its corresponding adjacency matrix ACay(n)."}, {"title": null, "content": "This construction requires us to add new nodes into the graph; hence, we need to modify the feature matrix into an extended version, XCay(n) \u2208 R|V(Cay(n))|\u00d7d. To construct this, we featurise the first |V| nodes using the data from X, and treat any additional nodes as virtual nodes, initialised in some pre-defined way. Specifically:\nX\nCay(n) = X\n1: V\nCay(n)\n|V|+1:|V(Cay(n)| ~ Init Virt"}, {"title": null, "content": "where Init Virt is any sampling procedure for initialising d-dimensional feature vectors; for example, we may choose to sample random features from N(0, 1), or in our case initialise them to zeros."}, {"title": null, "content": "Because EGP makes advantage of both the input graph (specified by A) and the generated Cayley graph (specified by ACay(n)), we can also appropriately extend the original adjacency matrix, A, to incorporate the new nodes. Since the input graph layers are intended to preserve the input graph topology as much as possible, one approach is to construct such a matrix \u00c3 \u2208R|V(Cay(n)|\u00d7|V(Cay(n)|\nby adding self-edges to the virtual nodes only:\nA\n1:|V|,1:|V|\n= A\n\u00c3\n|V|+1:|V(Cay(n)|,|V|+1:|V(Cay(n)|\n= I\nA1:|V|,|V|+1:|V(Cay(n)|\n= \u00c3\n|V|+1:|V(Cay(n)|,1:|V|\n= 0"}, {"title": null, "content": "CGP now proceeds in the same manner as EGP: alternating GNN layers, such that every odd layer operates over the input graph-to preserve the topological information therein\u2014and every even layer operates over the generated Cayley graph-to support bottleneck-free global communication. For a two-layer CGP model, this can be depicted as:\nH = GNN(GNN(X\nCay(n), \u00c3; 01), A Cay(n); 02)"}, {"title": null, "content": "where 01 and 02 are the parameters of the first and second GNN layer, respectively. This implemen- tation works with any choice of base GNN; here we may choose to take advantage of the graph isomorphism network ([39], GIN):\nhu = ( (1+e) + \u03a3v\u2208Nu\n)"}, {"title": null, "content": "where Zu \u2208 Rd are the features of node u, e is a learnable scalar, and \u222e is a MLP. Our experimentation in Section 5 shows that CGP is MPNN agnostic."}, {"title": null, "content": "The final node embeddings H \u2208 R|V(Cay(n)|\u00d7k may then be used for downstream node, graph or graph-level tasks. To avoid direct influence of virtual nodes in these predictions, we use only the embeddings corresponding to the original graph's nodes, that is, H1:|V|, in downstream tasks."}, {"title": null, "content": "The CGP model upholds the requirements of the four criteria (i-iv) set by Deac et al. [13]\u2014arguably, in a more theoretically grounded way than EGP; Figure 1 and 2 provides empirical evidence of this. Specifically, the lower diameter of the graph used in CGP enhances its ability to eliminate over- squashing and bottlenecks, which is further supported by having a higher spectral gap. Furthermore, the CGP model may be able to make up for one of the limitations of the Cayley graph construction: the inability to find the best way to align it to a given input graph, mitigating the potential for stochastic effects in the process. The additional virtual nodes act as \u201cbridges\" between poorly connected communities in the Cayley graph, ameliorating any poorly-connected regions caused by misalignment."}, {"title": "5 Experimentation", "content": "In this section, we empirically validate the efficacy of using the complete Cayley graph structure on a range of graph classification tasks. In particular, we compare the CGP model against approaches that do not incur a computational complexity of having to examine the input graph's structure: master node [5], fully adjacent layer (FA) [6] and EGP [13]. In addition to these comparisons, we further examine the scalability and performance of CGP against state-of-the-art graph rewiring techniques [8, 9, 11, 40, 41], leveraging the TUDataset [42] and LRGB [43]. Due to space limitations, we defer the results of the LRGB to Appendix (Section B), as well as analysing over-smoothing by measuring the Dirichlet energy of a graph to Appendix (Section E)."}, {"title": null, "content": "Experimental setting. We evaluate on the Open Graph Benchmark (OGB) [44] and TUDataset [42]. In our experiments, we prioritise a fair comparison with the only modification to the model being the use of the complete Cayley graph structure and the initialisation of the virtual nodes. Furthermore, we show that our proposed method is MPNN in- variant by setting the underlying model to GCN [45] and GIN [39]. The cho- sen hyperparameters are in line with es- tablished foundations [9, 44] for each dataset respectively. Notably, the EGP paper limits its empirical analysis to only the graph classification tasks from OGB with GIN as the backbone GNN. Refer to Appendix (Section A) for more detail on our experimental setting and hyperparameters used for the OGB and TUDataset."}, {"title": "6 Conclusion", "content": "In this work, we presented Cayley Graph Propagation (CGP), an efficient propagation scheme that mitigates over-squashing. CGP utilises the complete Cayley Graph structure to guarantee improved information flow between nodes in the input graph. We highlight the advantages of Cayley graphs as expanders and regular graphs. We show that by truncating the Cayley graphs to align with the input graph, as suggested in Expander Graph Propagation, the resulting graph may contain bottlenecks. This is in contrast to the Cayley graph we use, which is guaranteed to be bottleneck-free. We demonstrate the effectiveness and efficiency of CGP compared to EGP and other rewiring approaches, over multiple real-world datasets, including large-scale and long-range datasets."}, {"title": null, "content": "Limitations and Future Work. One limitation of our proposed model is the performance on datasets containing graphs with a comparatively higher node-to-edge ratio. For this reason, one such avenue for future work is aligning the Cayley graph edges such that that they retain the inductive bias of the task [33]. Additionally, it would be interesting to see how CGP performs in other tasks that utilise expander graphs, including but not limited to temporal graph rewiring [31]. Furthermore, concurrent work has used virtual nodes as the focal point of their proposed methods [53, 54] with Southern et al. [55] analysing virtual nodes' role within the context of over-squashing. Therefore, we theorise an interesting setting would be applying these authors' approaches to the additional virtual nodes retained from the complete Cayley graph structure."}]}