{"title": "Synthetic Data Generation for Augmenting Small Samples", "authors": ["Dan Liu", "Samer El Kababji", "Nicholas Mitsakakis", "Lisa Pilgram", "Thomas Walters", "Mark Clemons", "Greg Pond", "Alaa El-Hussuna", "Khaled El Emam"], "abstract": "Background: Small datasets are common in health research. However, the generalization performance of\nmachine learning models is suboptimal when the training datasets are small. To address this, data\naugmentation is one solution and is often used for imaging and time series data, but there are no\nevaluations on its potential benefits for tabular health data. Augmentation increases sample size and is\nseen as a form of regularization that increases the diversity of small datasets, leading them to perform\nbetter on unseen data.\nObjectives: Evaluate data augmentation using generative models on tabular health data and assess the\nimpact of diversity versus increasing the sample size.\nMethods: Using 13 large health datasets, we performed a simulation to evaluate the impact of data\naugmentation on the prediction performance (as measured by the AUC) on binary classification gradient\nboosted decision tree models. Four different synthetic data generation models were evaluated. We also\ndeveloped a decision support model to help analysts determine if augmentation will improve model\nperformance and demonstrate the application of the decision model for augmentation on seven small\nreal datasets. A comparison of augmentation with resampling (which is a proxy for a larger dataset with\nminimal impact on diversity) was performed.\nResults: Augmentation improves prognostic performance for datasets that: have fewer observations, with\nsmaller baseline AUC, have higher cardinality categorical variables, and have more balanced outcome\nvariables. No specific generative model consistently outperformed the others. Our decision support model\nhad an AUC of 0.77 and can be used to inform analysts if augmentation would be useful. For the seven\nsmall application datasets, augmenting the existing data results in an increase in AUC between 4.31%\n(AUC from 0.71 to 0.75) and 43.23% (AUC from 0.51 to 0.73), with an average 15.55% relative\nimprovement, demonstrating the nontrivial impact of augmentation on small datasets (p=0.0078).\nAugmentation AUC was higher than resampling only AUC (p=0.016). The diversity of augmented datasets\nwas higher than the diversity of resampled datasets (p=0.046).\nConclusions: This study demonstrates that data augmentation using generative models can have a marked\nbenefit in terms of improved predictive performance for machine learning models, but only for datasets\nthat meet baseline data size and complexity criteria. Our decision model can help analysts decide if\naugmentation can be useful, and if it is, then they can follow our recommended method for finding the\nappropriate level of augmentation to apply. Furthermore, augmentation performed better than having a\nlarger dataset, which is consistent with the argument that greater data diversity due to augmentation is\nbeneficial.", "sections": [{"title": "1. Introduction", "content": "Many machine learning (ML) clinical prediction models are being trained on datasets that are too small.\nSpecifically, a median of 12.5 events per predictor variable has been reported in the literature [1] and 1.7\nfor oncology ensemble models [2]. However, to achieve stability while training ML models more than 200\nevents per predictor variable are often required [3], and the vast majority of ML modeling studies in\noncology did not meet the minimum recommended sample sizes [4].\nTo address this data scarcity problem, there is a growing interest in using data augmentation to simulate\nadditional observations from existing data [5]. This augmentation process increases the sample size of the\ndataset, which by itself is expected to improve ML model prognostic performance [3]. Augmentation can\nalso be seen as a form of regularization [6], where the simulated data increase the diversity of the original\ndataset by generating more and different examples from the same population. Therefore, augmentation\ncould improve the prediction accuracy on the unseen data and enable ML models trained on augmented\ndata to achieve better generalization performance.\nData augmentation is a common practice for imaging data [5,7,8] where additional synthetic imaging\nsamples can boost model accuracy [9\u201317]. Augmentation has also been applied to time series datasets\n[18,19]. However, there is a dearth of comprehensive evaluations, applications of, and guidance on\naugmentation methods for tabular data. Tabular data are ubiquitous in practice, including in the health\ndomain [20].\nAugmentation techniques have been applied to address different types of data scarcity problems.\nOversampling methods are commonly used, such as the synthetic minority over-sampling technique\n(SMOTE) [21] and a few variants of SMOTE [22\u201325], and these enlarge the original data by interpolation.\nHowever, they are typically applied in the case of outcome imbalance. When there is covariate imbalance,\nwith certain groups under-represented in the data, generative models have been used to mitigate the\nrepresentation bias that is introduced [26]. For augmenting overall records, methods such as sampling\nwith replacement, sequential synthesis using decision trees [27], generative adversarial networks (GAN)\n[28], and variational autoencoders [29] have been evaluated with encouraging results [30\u201334], although\nsome deep learning architectures were found to be unstable [35]. Augmentation methods have also been\napplied to small clinical trial datasets [36-38].\nIn the current paper, we develop and evaluate an augmentation scheme for tabular health data.\nSpecifically, we make two contributions represented as the two parts of the study. First, we examine the\naugmentation performance of four synthetic data generation (SDG) methods on the predictive"}, {"title": "2. Methods", "content": "Our study consists of a simulation and evaluation of the extent to which augmentation can improve the\npredictive performance of GBDTs by answering the following questions:"}, {"title": "2.1 Overview of Simulation and Evaluation Processes", "content": "The overall workflows for part 1 is shown in Figure 1, and part 2 in Figure 2.\nFor part 1, we start with a large population dataset P and randomly split it into a training dataset T and a\ntest dataset P\\T with a 70:30 split for train:test. The test set represents unseen patients that we would\nuse to evaluate augmented data on.\nWe then draw a simple random sample (step A) of size no from the training dataset (the base dataset),\nwhich is augmented using a generative model, also called a synthesizer (step B), with a set of additional\nn' records. The augmented dataset of size n = no + n' (step C) is used to train a binary GBDT model (step\nD) [39]. Tree-based models are the most common type of ML prognostic methods used in clinical research\n[1], they perform better than linear models, such as logistic regression [40-44], and have also been found\nto perform better than deep learning models on tabular datasets [45,46]. The performance of that trained\nmodel is evaluated on the test dataset using the AUC (step E). This process is repeated for multiple values\nof no.\nSince augmentation does not always positively contribute to prognostic model performance\nimprovement, a decision support model is required to allow end-users to decide whether to attempt"}, {"title": "2.2 Datasets", "content": "We have two sets of data corresponding to the two parts of our study.\nThe population real world datasets that were used are summarized in Table 1A. These datasets cover\nheterogeneous domains, including public health, hospital discharge, infant and maternal health, adverse\nevents, ICU, population health surveys, and insurance claims. The table provides an overview of the\ndatasets, the original number of observations, the number of observations after removing those with any\nmissing values in the outcome variable and the number of variables included in the binary classification\nmodels used to predict the outcome. A detailed description of each preprocessed dataset and the binary\nworkload used for modeling can be found in the appendix. The number of predictor variables in the\nworkloads is consistent with what is seen in the clinical research literature [1].\nFor the second part, we show the seven smaller datasets that we use for our application case studies and\ncomparisons in Table 1B."}, {"title": "2.3 Augmentation Scheme", "content": "Given a population dataset, the first step is to split it into training and testing datasets, where the training\ndataset is used for subsequent sampling, augmentation and ML modeling, while the testing data is\nretained for performance evaluation. In our augmentation scheme, outcome stratified random sampling\nis applied to draw 40 samples (base datasets) of sizes no without replacement, from the training data,\nwhere no \u2208 {20, 30, 40, 50, 60, 70, 80, 90, 100, 150, 200, 250, 300, 350, 400, 450, 500, 550, 600, 650, 700,\n750, 800, 850, 900, 950, 1000, 2000, 3000, 4000, 5000, 6000, 7000, 8000, 9000, 10000, 20000, 30000,\n40000, 50000}. Then, each of the 40 base datasets is used to train a specific generative model.\nSubsequently, the synthetic records were simulated from that generative model with sizes according to\nthe following geometric series. Let b ~ N(\u03bc = 1.5, \u03c3\u00b2 = 0.005) be a random variable that follows a normal\ndistribution with a mean of 1.5 and a standard deviation of 0.005. The geometric series has more samples\nat low values and less at higher values as we expect there will be more variability at the lower end of the\nrange."}, {"title": "2.4 Machine Learning Analytic Workload", "content": "In this study, the chosen workload ML model is a light gradient boosting machine (LGBM) [39]. Model\ntuning used 5-fold cross-validation and Bayesian optimization [47]. The range for the tuning parameters\nwas previously suggested [48\u201351], and these are summarized in the appendix. High cardinality variables\nwere converted to embeddings [52] using a scheme similar to target encoding."}, {"title": "2.5 Synthetic Data Generation Methods", "content": "We used four commonly applied generative modeling methods to generate new observations for\nstructured tabular data, namely, sequential decision trees [27,53\u201355], Bayesian networks [56\u201359],\nconditional generative adversarial network [60] and variational autoencoders [60]. The first method was\nimplemented using Aetion\u00ae Generate, a commercial product from Aetion\u00b9, and the last three methods\nwere implemented using an open-sourced Python package Synthcity [61]. Our implementation, which is\npublicly available, provides further pre-processing and post-processing on top of Synthcity. In the\nexperiments, the variables to be synthesized in each dataset are only those that were used in the analysis\n(last column in Table 1)."}, {"title": "2.5.1 Sequential Decision Trees", "content": "Similar to using a chaining method for multi-label classification problems, sequential decision trees (SEQ)\ngenerate synthetic data using conditional trees in a sequential fashion [27,62,63]. It has been commonly\nemployed in the healthcare and social science domains for data synthesis [30,53,54,64-69]. The details of\nthe implementation procedures can be referred to [27]."}, {"title": "2.5.2 Bayesian Networks", "content": "Bayesian Networks (BN) are models based on Directed Acyclic Graphs that consist of nodes representing\nthe random variables and arcs representing the dependencies among these variables. To construct the\nBN model, the first step is to find the optimal network topology, and then to estimate the optimal\nparameters [56]. Starting with a random initial network structure, the Hill Climb heuristic search is used\nto find the optimal structure. Then, the conditional probability distributions are estimated using the\nmaximum a posteriori estimator [70]. Once the network structure and the parameters are estimated, we\ncan initialize the nodes with no incoming arcs by sampling from their marginal distributions and predict\nthe rest of the connected variables using the estimated parameters."}, {"title": "2.5.3 Conditional Generative Adversarial Network", "content": "A basic generative adversarial network (GAN) consists of two artificial neural networks (ANNs), a\ngenerator and a discriminator [28]. The generator and the discriminator play a min-max game. The input\nto the generator is noise, while its output is synthetic data. The discriminator has two inputs: the real\ntraining data and the synthetic data generated by the generator. The output of the discriminator indicates\nwhether its input is real or synthetic. The generator is trained to 'trick' the discriminator by generating\nsamples that look real. On the other hand, the discriminator is trained to maximize its discriminatory\ncapability.\nAmong all the variations of GAN architectures, the conditional tabular GAN (CTGAN) is often used in\ntabular data synthesis [71]. CTGAN builds on conditional GANs by addressing the multimodal distributions\nof continuous variables and the highly imbalanced categorical variables [60]. CTGAN solves the first\nproblem by proposing a per-mode normalization technique. For the second problem, each category of a\ncategorical variable serves as the condition passed to the GAN."}, {"title": "2.5.4 Variational Autoencoder", "content": "Variational autoencoders (VAE) use ANNs and involve two steps (encoding and decoding) to generate new\nsamples [29]. First, an encoder is generated to compress input data into a lower-dimensional latent space,\nin which the data points are represented by distributions. The second step is a decoding process, in which\nnew data samples are reconstructed as output from the latent space. The neural network is optimized by\nminimizing the reconstruction loss between the output and the input. VAEs are known to generate\ncomplex data of various types due to its ability to learn more complex distributions [72]. Many variants\nhave been proposed as an extension of VAE, such as triplet-based VAE [73], conditional VAE [74], and"}, {"title": "2.6 Decision Support Model for Augmentation", "content": "Based on the characteristics of the input base datasets, a decision support model was trained to\nrecommend whether extra synthetic data should be simulated and added to the base dataset (the\n\"augmentation recommendation\u201d). The decision support model would be used by an analyst to determine\nwhether augmentation is likely to improve the performance of their prognostic ML model. Given a\ndataset, if the decision model recommends augmentation, then the four generative models would be\nused to create the additional synthetic data for multiple values of n', and the augmented dataset with the\nhighest AUC gain would then be chosen. If the decision model does not recommend augmentation, then\nthe analyst can save resources as augmentation is not likely to be beneficial.\nThe outcome for this decision support model was determined by examining all of the simulation results\nfor each no value for every dataset and every generative model, and a binary value was selected to indicate\nthat for this {no, dataset, generative model} combination augmentation improved AUC over the baseline\n(a one outcome) or not (a zero outcome). This resulted in 520 observations.\nWhether a dataset will benefit from a certain amount of augmentation will depend on its complexity. For\nexample, a simple dataset, which conceptually can mean a small dataset with few low cardinality\ncategorical variables, is unlikely to have a marked increase in diversity after augmentation. This is because\nthe space of possible values on the categorical variables is small. Whereas a more complex dataset with\nmany high cardinality variables is likely to experience much more increases in diversity with augmentation,\nand hence would perform better on unseen data.\nPrevious work on data complexity metrics [76,77] and methods for sample size calculation that take data\ncomplexity into account [78,79] have defined a set of metrics that we considered for our augmentation\ndecision model. We propose that dataset complexity can be characterized by the following variables: the\nbase dataset size no, the number of predictor parameters, outcome distribution, standardized entropy,\nmutual information, separability measure and the AUC of the base dataset. These additional variables are\ndefined as:\n\u2022 Base dataset size no. The number of records in the original dataset.\n\u2022 Degrees of freedom. This is given a value of 1 for a numeric variable, and a categorical variable\nwith k levels gives k - 1."}, {"title": "2.7 Evaluation of Augmentation", "content": "We illustrate the proposed decision support model and augmentation method by applying it in real\nsituations with small datasets and assess whether this results in an improvement in the performance of\nthe ML model.\nSeven real datasets were used: the Hot Flashes dataset, Danish Colorectal Cancer Group dataset, Breast\nCancer Coimbra dataset, Breast Cancer dataset, Colposcopy/Schiller dataset, Diabetic Retinopathy\ndataset and Thoracic Surgery dataset. These datasets vary across dimensions and complexity. The detailed\ndescriptions of the datasets are summarized in the Appendix.\nA nested 5-fold cross-validation (CV) approach was applied for model training and prediction, which has\nbeen shown to yield almost unbiased estimates [82\u201384]. Each original dataset was first preprocessed for\nthe recommendation of augmentation from the decision support tool and if that was positive it randomly\ndivided into 5-folds of training and testing sets. The characteristics of the analysis dataset were measured,\nincluding the sample size, imbalance factor and degrees of freedom. The baseline AUC was determined\nas the average value of AUC obtained from the 5-fold training sets. We train an LGBM model to examine\nthe association between the outcome of interest and the data complexity measures. The\nhyperparameters of LGBM models were tuned and optimized using Bayesian optimization [47]. The range\nfor the tuning parameters, specific to each model, was previously suggested [48\u201351]. Note that\naugmentation was performed separately for each training partition in the outer loop to avoid data leakage\nthat would result in optimistic model performance. A range of values for n' from 7 to 1 million was"}, {"title": "2.8 Evaluation of Diversity", "content": "The objective here was to determine if improvements in the AUC of augmented data were due to the\nlarger sample size or due to the generative models increasing the diversity of the datasets (which is the\nmechanism described in the literature)."}, {"title": "2.8.1 Measuring Diversity", "content": "Diversity is an important evaluation metric to assess the quality of generated synthetic data and is\nsometimes defined as the proportion of real data covered by the synthetic data [61,62]. However, in our\nstudy, we are more interested in identifying synthetic data records that are significantly different from\nthe original samples. In other words, a new data record is defined to be diverse if it is different (i.e., the\nextent to which it is an outlier or an anomaly) from the original sample. It is necessary to find an effective\napproach to detect the anomaly records in one dataset with reference to another one.\nSince diversity is measured at the dataset level rather than an individual record level, one way to\nconceptualize diversity is to compare the multivariate variation in the original data and the augmented\ndata. If augmentation results in greater variation, then that would be an indicator of greater data diversity,\nseveral versions of multivariate coefficients of variation were introduced to measure the variability of\npopulations using the characteristics of the numeric variables [63\u201366]. Another study proposed a method\nto determine the variability specifically for categorical data [67]. However, these methods are restricted\nto one type of variable and our datasets have both categorical and numeric variables. An alternative\napproach is to examine methods for assessing data shift. Kamulete developed a data-driven approach,\ncalled D-SOS, to detect non-negligible adverse shifts in a sample using outlier scores [68]. In contrast to\nother statistical tests, D-SOS focuses on identifying distributions that are not benign but significantly\nshifting from the reference sample by placing more weights on instances in the outlying regions of the\nsample data. However, the contamination rate that aims to detect non-negligible adverse shifts is\ndistribution-based and therefore, unsuitable to our context, which is to capture the amount of new and\ndiverse observations."}, {"title": "2.8.2 Evaluating Impact of Diversity", "content": "In addition to the four generative models, we included the bootstrap method as another approach to\naugment the base dataset by resampling the original records. The purpose of including the bootstrap\nmethod is to rule out the influence of increasing data size. The size of the additional data that were\nbootstrapped was the same as the amount of synthetic data generated from the generative model that\nled to the optimal performance.\nWe also performed the diversity analysis for the seven datasets to show the quantity of the diverse\nrecords in the augmented data that were generated using either the generative models or bootstrap. For"}, {"title": "3. Results", "content": null}, {"title": "3.1 Overall Augmentation Performance", "content": "In this section, the performance of data augmentation against the size of synthetic data n' in 40 different\nno scenarios is summarized. To make the trends more interpretable and visible, the scales for the y-axis\nare varied, and the logarithm is taken for n'. Local regression was used to fit a smooth curve for each\ngenerative model.\nIn the main body of the paper, we present results for the BSA and FAERS datasets. The results for the\nremaining datasets are included in the appendix. These two datasets were selected for inclusion in the\nmain body since the former is simple data and the latter is quite a complex dataset (with multiple variables\nwith high cardinality). They illustrate the findings across the range of data complexity. The conclusions\ndrawn from these two datasets are consistent with those from the other datasets.\nIn Figure 2 and Figure 3, it can be clearly seen that the augmentation can improve the performance\nmeasured by AUC, as more synthetic data are incorporated, especially for small and medium no. In fact,\nthe improvements in model performance as measured by the AUC can be nontrivial, in some cases\nexceeding absolute increases of 0.1. For the large no, the improvement from augmentation is less or there\nis even a deterioration. In addition, the performance of SDG models varies significantly across different no\nand base datasets, demonstrating the importance of identifying the most appropriate model in a specific\nsituation. Moreover, compared to the BSA dataset, the FAERS dataset benefits more from data\naugmentation, as the highest no with noticeable improvement is relatively larger, around no = 3000,\nwhereas the highest no with noticeable improvement for the BSA dataset is approximately 650. Since the\nFAERS dataset is more complex with higher cardinality variables, further augmentation may generate\nmore plausible values from the population, which leads to a more diverse augmented dataset compared\nto the BSA dataset."}, {"title": "3.2 Augmentation Decision Support Tool", "content": "Table 2 presents the LOOCV results of AUC and prediction accuracy for the five methods on the decision\nsupport model. The best model is logistic regression, which is highlighted in bold. It results in the highest\nAUC of 0.7661 and a prediction accuracy of 76.15%, indicating its superior performance over other ML\nmodels in terms of the ability to recommend augmentation (or not). One possible reason for LR\nperforming best might be that the underlying relationships between the data characteristics and\naugmentation indicator tend to be linear, and logistic regression is well suited to capture such linear\nrelationships. Random forest and LGBM perform similarly in ranking after logistic regression, while the\nrest of the ML models are less favorable in predicting the augmentation choice for new datasets."}, {"title": "3.3 Evaluation of Augmentation and Diversity", "content": "After calculating the data characteristics for the analysis datasets, our decision model suggested that\naugmentation should be performed for all the case study datasets. The four generative models were\nemployed to simulate the additional datasets. Table 4 presents the augmentation results for each dataset,\nthe generative model that leads to the optimal performance, the amount of synthetic data records\nneeding to be generated to achieve the optimal performance and the performance using the bootstrap\nmethod.\nThe baseline AUC values are within the range from poor to good. The additional synthetic data sizes vary\ndepending on both the generative model that was used and the dataset. As expected, the best generative\nmodel is not uniform.\nThe relative improvement in AUC due to generative model augmentation is remarkably high ranging from\n4.3% to 43.23% (average 15.55%), indicating a substantial gain in the model performance after\naugmentation (baseline AUC vs augmented AUC: p=0.0078). The resampling augmentation generally\nyields a much lower AUC, compared to the synthetic data generative models and on some occasions is\neven worse than the baseline scenario without augmentation (augmented AUC vs resampled AUC:\np=0.016). Increasing the sample size by resampling the original data often does not contribute to the\nimprovement of model performance as much as the other synthetic data generative models."}, {"title": "4. Discussion", "content": null}, {"title": "4.1 Summary", "content": "The availability of health data for research purposes is limited, and these datasets are often small.\nHowever, training of ML models requires large amounts of data to obtain optimal performance on unseen\ndata, and training on datasets that are too small can lead to model instability [85], and to overfitting and\nan inability to generalize predictions to unseen data [3,86] even under ideal conditions (e.g., no data shift\nor drift). Consequently, the conclusions drawn from such models may be unstable and inaccurate. In such\ncases, data augmentation can be beneficial by simulating more, and more diverse, data based on the\nexisting data.\nAlthough it has been receiving increasing attention in recent years, especially in imaging data and time\nseries data applications, tabular data augmentation has not been extensively evaluated despite data\naugmentation being one of the primary use cases for synthetic data generation methods [87]. In this\nstudy, we fill this gap by evaluating the benefits of data augmentation for tabular health data.\nOur simulations show that augmenting existing data can enhance the ML performance as measured by\nAUC, especially for smaller, more balanced, more complex datasets or datasets with lower baseline AUC.\nExcessive augmentation is not necessarily beneficial. The appropriate level of augmentation that\nmaximizes performance differs for each dataset. However, the benefits of augmentation are less obvious\nor even detrimental for large datasets.\nOur interpretation of this phenomenon is that with small or moderate data size to start, the simulated\ndata positively contributes by increasing the size and diversity, and thus, more likely adds the information\nthat is similar to the unseen dataset. In contrast, for a large base dataset, the increase in size has less\nmarginal prognostic benefit and the dataset may already contain sufficiently diverse information, and\nincorporating more simulated data is less likely to provide useful diversity. In fact, it may be increasing the\nunnecessary noise in the current dataset. Moreover, the simpler datasets with fewer categorical variables\nand lower cardinality were found to benefit less from augmentation, and this is arguably because the\nspace to increase diversity is limited (i.e., simulated records will look more like current records rather than"}, {"title": "4.2 Recommendations for Practice and Research", "content": "For datasets where the baseline AUC is high, augmentation may not provide a significant advantage.\nHowever, where the baseline AUC is medium or small, and where datasets are in the 100 to 3000\nobservations range, augmentation can potentially improve the performance of a model's AUC, sometimes\nby a considerable amount. Datasets with high cardinality categorical variables can also benefit from\naugmentation. In contrast, augmentation will likely be less beneficial for large and simple datasets with\nstrong relationships with the outcome (i.e., higher baseline AUC).\nAnalysts can try different degrees of augmentation using multiple generative models and evaluate them\non holdout data to determine which amount of augmentation can maximize the prognostic performance.\nWe recommend using the data characteristic measures and the logistic regression decision support model\nto determine the necessity of augmentation for a given dataset.\nIt should be noted that the training dataset for the generative models should be separate from the dataset\nthat is used for testing. This is easier to do in a simple train/test split scenario. However, if augmentation\nis used in the context of, say, 5-fold cross-validation then the generative models should be trained on the\n4/5 training splits each time and evaluated on the remaining 1/5 split. This will ensure that there is no\ndata leakage which would result in optimistic results that would not carry to unseen data in subsequent\napplications. For the final augmented dataset, the determined n'_max simulated records should be\nconcatenated to the original dataset."}, {"title": "4.3 Limitations and Future Work", "content": "Evaluating the performance of each dataset at different levels of augmentation can be computationally\nintensive. This means that the processing time to determine the best level of augmentation may not be\nsmall in practice.\nOur analysis assumed that resampling with replacement was a good proxy for increasing the sample size\nwithout increasing diversity. The reasoning was that adding observations from the same distribution\nwould have a minimal impact on diversity.\nWhen datasets are very small, the types of generative models that we used in our study have a higher risk\nof overfitting. However, the data dimensionality that was used has also tended to be low which is a\nmitigating factor. Nevertheless, future work should examine generative models that are suited for small\ndatasets, such as those based on pre-trained models."}, {"title": "Ethics", "content": "This project was approved by the Research Ethics Board of the Children's Hospital of Eastern Ontario\nResearch Institute protocol 24/80x. The hot flashes data analysis was approved by the Ottawa Health\nSciences Research Ethics Board protocols OHSN REB #20210727-01H and OHSN REB #20210827-01H. For\nthe DCCG dataset, Danish Data Protection Agency (Datatilsynet) approval was obtained (RN-2018-94)."}, {"title": "Clinical Trial Registration", "content": "Clinical trial number: not applicable."}, {"title": "Funding", "content": "This research is funded by the Canada Research Chairs program through the Canadian Institutes of Health\nResearch, a Discovery Grant RGPIN-2022-04811 from the Natural Sciences and Engineering Research\nCouncil of Canada, and the Canadian Children Inflammatory Bowel Disease Network. LP is funded by the\nDeutsche Forschungsgemeinschaft (DFG, German Research Foundation) \u2013 530282197."}, {"title": "Competing Interests Statement", "content": "KEE is the Scholar-in-Residence at the Office of the Information and Privacy Commissioner of Ontario. KEE\nholds shares in Aetion, which provided the sequential synthesis generative model software that was used\nin this study."}, {"title": "Author Contributions", "content": "DL, SK, NM. LP, and KEE designed the study and performed the analysis. TW, MC, AEH, and KEE provided\nand interpreted the datasets. GP consulted on the analysis. All authors contributed to writing the paper."}, {"title": "Data Availability", "content": "The following provides information on the availability of each of the datasets used in this study:\n1. Better Outcomes Registry & Network (BORN) |\nThe BORN collects Ontario's prescribed perinatal, newborn and child registry with the role of\nfacilitating quality care for families across the province. It can be accessed through a data request at\nhttps://bornontario.ca/en/data/data.aspx.\n2. Basic Stand Alone (BSA)\nThe BSA inpatient claims dataset is about claim-level information that each record is an inpatient\nclaim incurred by a 5% sample of Medicare beneficiaries. The dataset is publicly available at\nhttps://www.cms.gov/data-research/statistics-trends-and-reports/basic-stand-alone-medicare-\nclaims-public-use-files/bsa-inpatient-claims-puf.\n3. California State Hospital Discharge\nThe California dataset contains the patient's hospital 2008 discharge data from California, State\nInpatient Databases (SID), Healthcare Cost and Utilization Project (HCUP), Agency for Healthcare\nResearch and Quality [93], and is available for purchase at https://hcup-\nus.ahrq.gov/tech assist/centdist.jsp.\n4. Canadian Community Health Survey (CCHS)\nThe CCHS data are Canadian population-level information concerning health status, health system\nutilization and health determinants collected by Statistics Canada through telephone survey. The\navailability of CCHS data is restricted and requires an access request at\nhttps://www150.statcan.gc.ca/n1/pub/82-620-m/2005001/4144189-eng.htm.\n5. COVID-19\nThe COVID-19 dataset collects Canadian health records of COVID-19 gathered by the Public Health\nAgency of Canada and is available at Esri Canada (https://resources-\ncovid19canada.hub.arcgis.com/).\n6. FDA Adverse Event Reporting System (FAERS)\nThe FAERS is a database comprising the information on adverse events and medication error reports\nsubmitted to FDA and can be downloaded at https://open.fda.gov/data/faers/.\n7. Florida State Hospital Discharge\nThe Florida dataset contains the patient's hospital 2007 discharge data from Florida, State Inpatient\nDatabases (SID), Healthcare Cost and Utilization Project (HCUP), Agency for Healthcare Research and\nQuality [93], and is available for purchase at https://hcup-us.ahrq.gov/tech assist/centdist.jsp.\n8. MIMIC-III\nMIMIC-III is a large database that contains deidentified health-related data associated with over\nforty thousand patients who stayed in critical care units of the Beth Israel Deaconess Medical Center\nbetween 2001 and 2012 [94,95]. The access to the MIMIC database is upon signing a data use\nagreement with PhysioNet at https://physionet.org/content/mimiciii/1.4/ [96].\n9. New York State Hospital Discharge\nThe New York dataset contains the patient's hospital 2007 discharge data from New York, State\nInpatient Databases (SID), Healthcare Cost and Utilization Project (HCUP), Agency for Healthcare\nResearch and Quality [93], and is available for purchase at https://hcup-\nus.ahrq.gov/tech assist/centdist.jsp."}]}