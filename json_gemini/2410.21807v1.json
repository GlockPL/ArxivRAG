{"title": "A Fresh Look at Generalized Category Discovery through Non-negative Matrix Factorization", "authors": ["Zhong Ji", "Shuo Yang", "Jingren Liu", "Yanwei Pang", "Jungong Han"], "abstract": "Generalized Category Discovery (GCD) aims to classify both base and novel images using labeled base data. However, current approaches inadequately address the intrinsic optimization of the co-occurrence matrix A based on cosine similarity, failing to achieve zero base-novel regions and adequate sparsity in base and novel domains. To address these deficiencies, we propose a Non-Negative Generalized Category Discovery (NN-GCD) framework. It employs Symmetric Non-negative Matrix Factorization (SNMF) as a mathematical medium to prove the equivalence of optimal K-means with optimal SNMF, and the equivalence of SNMF solver with non-negative contrastive learning (NCL) optimization. Utilizing these theoretical equivalences, it reframes the optimization of A and K-means clustering as an NCL optimization problem. Moreover, to satisfy the non-negative constraints and make a GCD model converge to a near-optimal region, we propose a GELU activation function and an NMF NCE loss. To transition A from a suboptimal state to the desired A*, we introduce a hybrid sparse regularization approach to impose sparsity constraints. Experimental results show NN-GCD outperforms state-of-the-art methods on GCD benchmarks, achieving an average accuracy of 66.1% on the Semantic Shift Benchmark, surpassing prior counterparts by 4.7%.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, deep learning models [1]\u2013[6] have made substantial strides, primarily attributed to the proliferation of large-scale, meticulously annotated datasets. However, these models fundamentally operate under a \u201cclosed-world\u201d assumption, limiting their efficacy in real-world scenarios where data typically encompasses a blend of seen and unseen categories [7]\u2013[9]. To address this limitation, the field of Generalized Category Discovery (GCD) [10], [11] has emerged, focusing on the development of methodologies capable of recognizing both seen and unseen categories utilizing partially labeled data, while eschewing reliance on additional modal information.\nDespite significant advancements, current GCD approaches [12]\u2013[14] continue to confront formidable challenges. A particularly salient issue is the suboptimal class-level and domain-level separability achieved when fine-tuning self-supervised pre-trained models through Contrastive Learning(CL). This separability lacks the necessary clarity and orderliness to adequately reflect the sparsity of intra-class similarity within both the base and novel domains, as well as the non-confounding nature within base-novel interaction domains.\nTo further elucidate, we construct the normalized co-occurrence matrix A in Fig. 1 based on the optimized feature space to articulate the cosine similarity between base and novel samples. Previous approaches [15], [16], which merely constrain the feature space through contrastive learning, typically achieve a similarity matrix akin to that shown in the top-left corner of Fig. 1. Although these methods yield commendable GCD performance, they fall short of perfection. In the GCD scenario, if a model is optimized to its theoretical optimum, an ideal similarity matrix A*, as illustrated in the upper right corner of Fig. 1, must satisfy two key criteria: (1) Intra-class Sparsity: Given that each class constitutes only a small portion of the data in its respective domain, the submatrix regions corresponding to the base and novel domains must exhibit sufficient sparsity. (2) Base-Novel Insulation: The interaction regions between base and novel domains must demonstrate significant dissimilarity, with similarities approaching zero, as only with substantial differences can a GCD model accurately identify them during clustering."}, {"title": "II. RELATED WORKS", "content": "Symmetric Non-negative Matrix Factorization (SNMF) has emerged as a crucial extension of traditional Non-negative Matrix Factorization (NMF) [17]\u2013[20], specifically tailored for scenarios where the data matrix is symmetric and non-negative, such as in similarity or adjacency matrices. SNMF decomposes a high-dimensional symmetric matrix into a product of lower-rank, symmetric, and non-negative matrices, thereby enhancing interpretability and robustness in capturing latent structures. The significance of SNMF has been demonstrated across various domains. In clustering [21], [22], SNMF outperforms traditional methods by providing a more nuanced and interpretable decomposition of data relationships. It has also proven effective in data representation [23], [24], where compact and interpretable representations are essential for downstream tasks. Additionally, SNMF excels in feature selection [19], [25], [26], enabling the identification of discriminative features under non-negativity constraints, which is crucial for maintaining the interpretability and relevance of the selected features. Furthermore, SNMF has found significant applications in community detection [27], [28], where its symmetric nature is particularly advantageous for identifying densely connected sub-networks within large graphs. These applications highlight SNMF's ability to enhance both the theoretical understanding and practical utility of non-negative matrix factorization techniques, making it a powerful tool in a variety of data-driven fields.\nGeneralized Category Discovery (GCD), an extension of the Novel Category Discovery (NCD) paradigm [7]\u2013[9], addresses the challenge of discovering novel, previously unseen categories while leveraging labeled data from seen categories. GCD aligns with open-world semi-supervised learning [29], [30] by jointly handling both seen and unseen categories in a dynamic, open environment. Current GCD approaches can be broadly classified into two categories: non-parametric and parametric methods. Non-parametric methods [10], [16], [31] typically employ clustering techniques, such as K-means, to separate unseen classes, while relying on minimal assumptions about the data distribution. In contrast, parametric methods [12], [13], [15] incorporate implicit clustering within learned classifiers, enabling more sophisticated modeling of category discovery by learning discriminative features for unseen categories. Despite the widespread adoption of clustering in both explicit and implicit GCD approaches, existing methods have not fully addressed the specific optimization goals of GCD, particularly concerning the nature of A and how K-means clustering should be optimized, thus limiting the full potential. Our NN-GCD framework addresses this limitation both theoretically and practically by transforming K-means clustering into an optimizable NCL problem through SNMF, which provides a more cohesive and effective strategy."}, {"title": "III. PRELIMINARIES", "content": "In this section, we provide a detailed introduction to the media involved in proving the equivalence of optimal NCL and A*. This exposition primarily focuses on the foundational concepts of Non-negative Matrix Factorization (NMF) and its symmetric variant, SNMF.\nNon-negative Matrix Factorization complements clustering by decomposing a non-negative matrix V \u2208 $\\mathbb{R}^{m \\times n}$ into the product of two lower-rank non-negative matrices W \u2208 $\\mathbb{R}^{m \\times k}$ and H \u2208 $\\mathbb{R}^{k \\times n}$, where typically k < min(m,n) [17]. The canonical NMF optimization problem is formulated as:\n$\\min_{W\\geq0,H\\geq0} ||V-WH||_F^2$,\nwhere $|| \\cdot ||$ represents the Frobenius norm.\nSymmetric Non-negative Matrix Factorization extends NMF to symmetric matrices where V = V\u1d40 [32]. SNMF approximates V as $HH^T$, with H being non-negative:\n$\\min_{H\\geq0} ||V - HH^T ||_F^2$.\nThis formulation inherently aligns with the intrinsic clustering nature of GCD tasks, preserving data symmetry and maintaining inherent relationships. As GCD methodologies advance, integrating SNMF with high-dimensional, multi-modal data presents promising research directions. This synergy enhances our capacity to uncover and interpret complex patterns across diverse domains. SNMF's application in GCD contexts may revolutionize class discovery, particularly where traditional clustering methods face limitations due to data complexity or high dimensionality.\nNon-negative Contrastive Learning is a variant of contrastive learning that imposes non-negativity constraints on the network outputs during the optimization process [33]. The NCL loss function is defined as:\n$L_{NCL}(f) = -2\\mathbb{E}_{x,x^{+}} [f^{+}(x)^Tf^{+}(x^{+})] + \\mathbb{E}_{x,x^{-}} [f^{+}(x)^Tf^{+}(x^{-})]^2$,\nwhere $f^+(x)$ denotes the non-negative output of the neural network for input x.\nThe first term encourages similar representations for positive pairs, promoting intra-class compactness, while the second term penalizes similar representations for negative pairs, ensuring inter-class separability. NCL aims for sparse and disentangled representations, enhancing the model's generalization ability, which aligns with the requirements of the GCD tasks for generalizing from old to new classes and disentangling features between them."}, {"title": "IV. THEORETICAL INSIGHTS", "content": "In this section, to establish the theoretical foundation for the equivalence among optimal K-means clustering, SNMF, and NCL, we derive Theorem 1 and Theorem 2.\nA. Equivalence of Kernel K-Means and SNMF\nTheorem 1 (Equivalence of optimal Kernel K-Means and optimal SNMF). Let {$x_i$}$_{i=1}^N$ be a dataset transformed into a Reproducing Kernel Hilbert Space (RKHS) via an optimal neural network $f^*$, resulting in optimal kernel matrix $A^* = f^*(X)^T f^*(X)$, which is unnormalized $\\overline{A}^*$. The optimal Kernel K-means objective can be reformulated as:\n$\\min L_{A^*} = \\min(\\sum_i ||f^*(x_i)||^2 - \\sum_{k\\, i,j \\in C_k} f^*(x_i)^T f^*(x_j) h_k)$,\n$\\ = \\min Tr(A^{*T}A^{*}) - \\min Tr(H^T A^* H)$,\nwhere H = ($h_1$,...,$h_k$) with $h_k h_l = \\delta_{kl}$, $H^T H=I$, and $A^{*T} A^* = const$.\nExpanding the optimal kernel k-means loss:\n$\\min L_{A^*} = const - \\min Tr(H^T A^* H)$\n$\\= \\min(||A^*||_F^2 - 2Tr(H^T A^* H) + ||H^T H||_F^2)$.\nThis is equivalent to an instance of optimal SNMF:\n$\\min ||A^* - HH^T||_F^2$.\nProof. To demonstrate the equivalence between NMF and K-means clustering in a kernelized setting, consider a dataset {$x_i$}$_{i=1}^N$ mapped into a Reproducing Kernel Hilbert Space (RKHS) through a transformation f(x). The optimization problem for Kernel K-means in this space is expressed as:\n$\\min L_{K}(f) = \\sum_i ||f(x_i)||^2 - \\sum_k \\frac{1}{n_k} \\sum_{i,j \\in C_k} f(x_i)^T f(x_j)$,\nwhere the term $\\sum_i ||f(x_i)||^2$ remains constant for a fixed network architecture and can thus be omitted from the optimization process.\nThe clustering solution is encoded by the matrix H consisting of K indicator vectors, where:\nH = ($h_1$,...,$h_k$), $h_k h_l = \\delta_{kl}$,\nand each vector $h_k$ is normalized as:\n$h_k = (0,...,0, \\underbrace{1,...,1}_{n_k}, 0,...,0)^T \\frac{1}{\\sqrt{n_k}}$.\nThis formulation leads to the Kernel K-means objective being rewritten as:\n$L_{A} = Tr(A^{*T}A^{*}) - Tr(H^T A^* H)$,\nwhere $A^* = [f^*(x_1),...,f^*(x_l),...,f^*(x_n)]$, $f^*(X) = [f^*(x_1),...,f^*(x_l),...,f^*(x_n)]^T$ and $f^*(X) = f^*(X)^T f^*(X)$.\nSince the term $Tr(A^{*T}A^{*})$ is constant for a fixed network, the objective is to maximize $Tr(H^T A^* H)$ subject to $H^T H=I$ and $H \\geq 0$. This is equivalent to solving:\n$\\min ||A^* - HH^T||_F^2$.\nThis problem corresponds to Symmetric Non-negative Matrix Factorization (SNMF), where a non-negative matrix V > 0 is factorized as V \u2248 $FF^T$, with $F \\geq 0$. In the context of SNMF, A* represents the data matrix, and $HH^T$ is the projection onto the cluster centroids. The orthogonality constraint $H^T H = I$ enforces the orthogonality of these centroids.\nWhile SNMF may not exactly hold the equivalence to K-means in suboptimal cases, under optimal conditions, SNMF approaches the orthogonality properties required for K-means clustering, leading to their approximate equivalence."}, {"title": "B. Equivalence of SNMF and NCL", "content": "Following prior work [33], [35], we establish the equivalence between SNMF on A and NCL. Let $\\overline{A}$ = $D^{-1/2}AD^{-1/2} \\in \\mathbb{R}^{N\\times N}$ be the normalized adjacency matrix, where A is the co-occurrence matrix of augmented samples x \u2208 X, with entries $A_{x,x'} := P(x, x') = \\mathbb{E}_{A}[A(x|x)A(x'|x)]$ representing the co-occurrence probability of samples x and $x'$. Building on this foundation, we propose Theorem 2.\nTheorem 2. Given a normalized adjacency matrix $\\overline{A} \\in \\mathbb{R}^{N\\times N}$ representing the probabilities of co-occurrence of augmented data samples, and a representation function f: $\\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ subject to non-negativity constraints $f^+(x) \\geq 0$ for all x \u2208 X, performing SNMF on $\\overline{A}$ is equivalent to the NCL. Specifically, the SNMF problem minimizes the loss:\n$L_{SNMF}(F) = ||\\overline{A} - F^+{F^+}^T||_F^2$\n$\\ = \\sum_{x,x^+} \\frac{P(x, x^+)^2}{P(x)P(x^+)} + \\sum_{x,x^-} \\frac{P(x, x^-)^2}{P(x)P(x^-)}$\n$-2\\sum_{x,x^+} \\frac{P(x, x^+)}{\\sqrt{P(x)P(x^+)}} F^+_x F^+_{x^+}$\n$\\ + \\sum_{x,x^-} \\sqrt{P(x)P(x^-)} (F^+_x)^T (F^+_{x^-})$\n$\\ = L_{NCL} + const$,\nwhere $F^+ \\geq 0$ and each row vector $(F^+)_{x,:} = \\sqrt{P(x)}f^+(x)^T$.\nTherefore, minimizing $L_{SNMF}(F)$ is equivalent to minimizing $L_{NCL}(f)$ up to an additive constant.\nProof. Representation learning aims to train an encoder function f : $\\mathbb{R}^d \\rightarrow \\mathbb{R}^k$ that maps input data x \u2208 $\\mathbb{R}^d$ to compact representations z \u2208 $\\mathbb{R}^k$. In contrastive learning, positive sample pairs ($x,x^+$) are generated by augmenting the same sample x ~ $P(x)$, where augmentation follows the distribution A(x). Negative samples $x^\u2212$ are drawn independently from the marginal distribution P(x).\nWe define the normalized adjacency matrix $\\overline{A} = D^{-1/2}AD^{-1/2}$, where A \u2208 $\\mathbb{R}^{N\\times N}$ represents the co-occurrence matrix of augmented samples x \u2208 X, with entries $A_{x,x'} := P(x,x') = \\mathbb{E}_{A}[A(x|x)A(x'|x)]$. The objective of contrastive learning is to align the features of positive pairs while ensuring the separation of features for negative pairs. The InfoNCE loss is defined as:\n$L_{NCE}(f) = -\\mathbb{E}_{x,x^+, [x_i]^M_{i=1}} \\{log(\\frac{exp(f(x)^Tf(x^+))}{\\sum_{i=1}^M exp (f(x)^T f(x_i)) + exp(f(x)^T ((\\xi_i) (x))))} \\}$.\nFor theoretical analysis, we use the spectral contrastive loss:\n$L_{sp}(f) = -2\\mathbb{E}_{x,x^+} f(x)^T f(x^+) + \\mathbb{E}_{x,x^-} (f(x)^T f(x^-))^2$.\nImposing a non-negative constraint on the outputs transforms the spectral contrastive loss into a non-negative form:\n$L_{NCL}(f) = -2\\mathbb{E}_{x,x^+} f^+(x)^T f^+(x^+) + \\mathbb{E}_{x,x^-} (f^+(x)^T f^+(x^-))^2$,\nwhere $f^+(x) \\geq 0$ for all x \u2208 X.\nGiven that the normalized co-occurrence matrix $\\overline{A}$ is a non-negative symmetric matrix, we apply SNMF to A to derive non-negative features $F^+$:\n$L_{NMF}(F) = ||\\overline{A} - F^+{F^+}^T||_F^2, F^+ \\geq 0$.\nThe SNMF problem in Eq. (16) is equivalent to the non-negative spectral contrastive loss in Eq. (15), where $(F^+)_{x,:} = \\sqrt{P(x)}f^+(x)^T$. Expanding the terms:\n$L_{NMF}(F) = \\sum_{x,x^+} (\\frac{P(x,x^+)}{\\sqrt{P(x)P(x^+)}})^2 - 2 \\sum_{x,x^+} (\\frac{P(x,x^+)}{\\sqrt{P(x)P(x^+)}})^T \\sqrt{P(x)}f^+(x) \\sqrt{P(x^+)}f^+(x^+)$\n$\\ + \\sum_{x,x^-} (\\sqrt{P(x)P(x^-)} (f^+(x)^T f^+(x^-)))^2$\n$\\ = \\sum_{x,x^+} \\frac{P(x,x^+)^2}{P(x)P(x^+)} - 2 \\sum_{x,x^+} P(x,x^+) f^+(x)^T f^+(x^+)$\n$\\ + \\sum_{x,x^-} P(x)P(x^-) (f^+(x)^T f^+(x^-))^2$\n$\\ = L_{NCL} + const$.\nThus, we have shown that contrastive learning with non-negative constraints performs symmetric non-negative matrix factorization on the normalized adjacency matrix.\nIntegrating these findings, we underscore the equivalence between optimal K-means clustering and optimal SNMF and the equivalence between SNMF and NCL during the optimization. This provides a direct pathway for optimizing the underlying mechanisms of GCD models."}, {"title": "V. METHOD", "content": "In this section, we detail the proposed NN-GCD framework, which refines existing GCD methodologies.\nTask Definition: The GCD task aims to accurately recognize both previously base (old) and novel categories [10]. The training dataset D comprises a labeled subset $D_l$ and an unlabeled subset $D_u$. Specifically, the labeled set $D_l = \\{(X_i, Y_i)\\}_{i=1}^{N_l} \\subset X \\times Y_l$ includes samples from base classes $Y_l = C_{old}$, while the unlabeled set $D_u = \\{x_u;\\}_{u=1}^{N_u} \\subset X_u$ includes samples from both base and novel classes $Y_u = C_{old} \\cup C_{new}$. Here, C represents the complete set of categories within D, which can be predefined or identified through existing methodologies.\nA. Non-negative Activated Neurons\nIn the preceding theoretical section, we elucidate the equivalence between optimal clustering and optimal SNMF, as well as the optimal NCL. It proves that the optimal A* and clustering performance can be obtained through optimizing NCL. Therefore, in this subsection, we propose a more optimal network architecture based on NCL tailored for the GCD scenario, ensuring the non-negativity constraint.\nTo enforce this constraint, the neural network outputs are reparameterized. Following a standard neural network encoder f\u2014comprising a feature extractor F and a multi-layer perceptron (MLP) projection head g\u2014as used in contrastive learning, a non-negative transformation \u03c6(\u00b7) (satisfying \u03c6(x) \u2265 0, \u2200x) is introduced:\n$f^+(x) = \\phi(f(x))$.\nPotential choices of \u03c6 include ReLU, sigmoid, etc. We find that the simple ReLU function, ReLU(x) = max(x, 0), compared to other operators with non-negative outputs, imposes a very stringent constraint through its zero-output effect for inputs less than zero.\nAlthough ReLU-activated features exhibit absolute non-negativity, they often encounter the issue of dead neurons during training [36]. During NCL optimization, the similarity between samples of different classes is driven to zero: $f^+(x)^Tf^+(x') \\rightarrow 0$. However, the problem of dead neurons reduces the feature representation capability, forming all-zero activations that lead to all-zero features. This results in A being sufficiently sparse but with weak class separability, significantly hindering the generalization required for GCD tasks and causing confusion among different classes. To solve this problem, we propose to use GELU [37], which can be considered a smooth form of ReLU:\n$GELU(x) = x \\cdot \\Phi(x) = x \\cdot \\frac{1}{2} (1 + erf(\\frac{x}{\\sqrt{2}}))$,\nwhere \u03a6(x) is the cumulative distribution function of the standard normal distribution.\nAccording to [38], unlike ReLU, GELU exhibits Lipschitz continuity and smoothness, which mitigates the dead neuron issue and benefits model generalization [39]. As shown in Fig. 2, GELU preserves the input values, enabling non-zero activations during optimization. This allows $f^+(x)$ and $f^+(x')$ to form non-zero orthogonal solutions. The non-zero orthogonality of features from different classes ensures that inter-class cosine similarity approaches zero while intra-class cosine similarity remains close to one, thereby preventing feature collapse, especially between base and novel classes. Ultimately, this achieves class separability of A*, significantly enhancing GCD performance.\nTo more reliably generate non-zero activations, we stop the gradient updates for the teacher head $g_t$ and update the teacher head parameters $O_t = \\{o_t,m_t\\}$ [40] using the Exponential Moving Average (EMA) method, specifically updating $O_t$ as $O_t = w(t)O + (1 - w(t))O$, where O refers to the trainable parameters of the student head $g_s$. The hyperparameter w(t) is updated using a cosine decay schedule, defined as $w(t) = w_{max}((\\frac{1-min}{1}) cos(\\frac{t\\pi}{T}))/2$.\nB. NMF NCE Loss\nThe introduction of the Non-negative Activated Neurons mechanism enforces the non-negative output constraint in NCL, enabling the use of contrastive learning-based loss functions and gradient descent to find the optimal A*. To learn the representations of the data, we follow SimGCD [11] and combine self-supervised and supervised learning. We combine the SimCLR [41] loss and the cross-entropy loss between predictions and pseudo labels:\n$L_{SSL} = L_{SimCLR} + L_{pseudo}$.\nFor labeled data, we use a supervised learning loss that combines the SupCon [42] loss and the cross-entropy loss between predicted and ground-truth labels:\n$L_{SL} = L_{SupCon} + L_{CE}$.\nWe calculate the SimCLR loss as follows:\n$L_{SimCLR} = \\frac{1}{|B|} \\sum_{i \\in B} -log \\frac{exp(h_i \\cdot h_i/\\tau)}{\\sum_{j \\in B} exp(h_i \\cdot h'_j/\\tau)}$,\nwhere $x_i$ and $x'_i$ are two views of the same image in a mini-batch B, $h_i = g(F(x_i))$, and \u03c4 is a temperature value. F is the feature extractor ViT, and g is the MLP projection head. The SupCon loss is calculated as follows:\n$L_{SupCon} = \\frac{1}{|B_L|} \\sum_{i \\in B_L} \\frac{1}{|N_i|} \\sum_{q \\in N_i} - log \\frac{exp(h_i \\cdot h_q/\\tau)}{\\sum_{j \\notin N_i} exp(h_i \\cdot h'_j/\\tau)}$,\nwhere $B_L$ is the labeled subset of B, and $N_i$ is the set of indices of images sharing the same label as $x_i$.\nWe utilize two additional loss functions to train the feature extractor, following [11] and applied in a self-distillation fashion [43]. Specifically, for a total number of categories K, we randomly initialize a set of prototypes $C = \\{c_1,...,c_K\\}$, each representing one category. During training, we calculate the soft label for each augmented view $x_i$ by performing softmax on the cosine similarity between the hidden feature $z_i = F(x_i)$ and the prototypes C, scaled by 1/Ts:\n$p_i^{(k)} = \\frac{exp((1/\\tau_s)((z_i/||z_i||_2)^T(c_k/||c_k||_2)))}{\\sum_{k'} exp ((1/\\tau_s)((z_i/||z_i||_2)^T(c_{k'}/||c_{k'}||_2)))} $.\nC. Hybrid Sparse Regularization\nThe optimization target A* in NCL is highly sparse, necessitating the minimization of similarity among dissimilar negative samples ($f^+(x)^Tf^+(x') \\rightarrow 0$). This results in $f^+(x)$ and $f^+(x')$ forming non-zero orthogonal solutions, leading to zero activation in the irrelevant dimensions of the features, thus requiring inherent sparsity in NCL features. Imposing sparsity constraint regularization on NCL maintains feature sparsity during optimization, enabling better convergence to the optimal solution. To achieve this, we propose the Hybrid Sparse Regularization:\n$L_{HSR}(W) = \\gamma (\\beta ||W||_1 + (1 - \\beta)(||W||_{2,1} - ||W||_F^2))$,\nHere, \u03b2 and \u03b3 act as balancing parameters. The regularization term mitigates the deficiencies of the 1-norm and handles feature redundancy, a common issue with the l2,1-norm.\nThe purpose of $||W||_1$ is to enhance the sparsity of the parameter matrix, while the subsequent term($||W||_{2,1} - ||W||_F^2$), maintaining row sparsity while controlling the overall energy of the matrix. The modified l2,1 norm, $||W||_{2,1}$, aims to constrain row sparsity, thereby reducing redundancy, while the squared Frobenius norm, $||W||_F^2$, controls the overall energy of the parameter matrix.\nThe total loss function for NN-GCD is then equal to:\n$L_{NN-GCD} = (1 - \\lambda)L_{SSL} + \\lambda L_{SL} + L_{HSR}(W)$.\nBy employing the aforementioned methods, the optimal features and A* are achieved. The results are visualized as follows: Figure 3 highlights our method's non-negative, sparse features, marked by mostly zero eigenvalues and selective large positives. By refining NCL through a specialized loss function and sparse regularization, we achieve optimal A* with class-separable features. Additionally, Figure 4 underscores superior sparsity in both base and novel regions, nearing unity in the base-novel area, thus outperforming predecessors."}, {"title": "VII. CONCLUSION", "content": "In this paper, we present NN-GCD, a novel framework tackling GCD complexities. Through the meticulous optimization of the co-occurrence matrix A, and by harnessing the synergistic theoretical underpinnings of K-means clustering, SNMF, and NCL, we have devised a Non-negative Activated Neurons mechanism. This innovative construct, further reinforced by the NMF NCE loss coupled with Hybrid Sparsity Regularization, facilitates an unprecedented level of intra-class compactness and inter-class differentiation, culminating in the derivation of the optimal A*. Rigorous empirical evaluations affirm the NN-GCD framework's superior efficacy, solidifying its status as a vanguard methodology in the realm of advanced category discovery."}]}