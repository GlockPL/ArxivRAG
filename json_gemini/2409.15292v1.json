{"title": "SketcherX: AI-Driven Interactive Robotic drawing with Diffusion model and Vectorization Techniques", "authors": ["Jookyung Song", "Mookyoung Kang", "Nojun Kwak"], "abstract": "We introduce SketcherX, a novel robotic system for personalized portrait drawing through interactive human-robot engagement. Unlike traditional robotic art systems that rely on analog printing techniques, SketcherX captures and processes facial images to produce vectorized drawings in a distinctive, human-like artistic style. The system comprises two 6-axis robotic arms: a face robot, which is equipped with a head-mounted camera and Large Language Model (LLM) for real-time interaction, and a drawing robot, utilizing a fine-tuned Stable Diffusion model, ControlNet, and Vision-Language models for dynamic, stylized drawing. Our contributions include the development of a custom Vector Low Rank Adaptation model (LoRA), enabling seamless adaptation to various artistic styles, and integrating a pair-wise fine-tuning approach to enhance stroke quality and stylistic accuracy. Experimental results demonstrate the system's ability to produce high-quality, personalized portraits within two minutes, highlighting its potential as a new paradigm in robotic creativity. This work advances the field of robotic art by positioning robots as active participants in the creative process, paving the way for future explorations in interactive, human-robot artistic collaboration.", "sections": [{"title": "I. INTRODUCTION", "content": "SketcherX is a robotic system designed to interact with people by drawing their portraits. When a user sits in front of the robot, a camera mounted on the robot's head captures the user's face. This captured image is then transformed into the robot's distinctive drawing style. The transformed image is further converted into vector strokes, enabling the robot's arm to draw the portrait like a human artist would. This interactive experience allows users to witness the creative act of the robot, redefining the value of robots as interactive media. As shown in Fig. 1, SketcherX was successfully exhibited at CES 2023 and attracted a lot of attention.\nWhile robotic drawing is not a new concept, previous attempts, such as those by (Birsak et al., 2018) and (Rob and Carter, 2024), primarily involved robots recording bitmap data onto analog media using predefined rules, a process more akin to \"printing\" than true \"drawing.\" Some research, such as Frida (Schaldenbrand et al., 2023), has focused on imbuing robotic movements with creative significance and the ability to perform artistic acts. However, the outcomes of these efforts, while considered artworks, often lack a distinct robotic identity.\nOur objective is to ensure that the drawings produced by SketcherX are perceived as creations of an autonomous entity. This requires the following characteristics in the media creations:\n1) The creating entity must have an independent identity and interact with people.\n2) The process of creation should be demonstrated in a manner comprehensible to humans.\nConsidering these characteristics, we developed two robotic arms: one with a face that can interact with people and the other capable of drawing. The arm responsible for the face uses a large language model (LLM) to communicate with users and is equipped with a rear-projection mask to display facial movements. The robot's face is dynamically rendered using a 3D simulator built in Unity, allowing it to express emotions through facial expressions and gestures based on the context of the conversation.\nThe drawing arm is designed to effectively demonstrate and communicate the drawing process to the user. After capturing the facial features of the person, the image is processed through a diffusion-based image processing technique to transform it into the robot's unique drawing style. This raster image is then mapped into a vector image which is again converted into stroke parameters interpretable by the robot using the KUKA PRC program. A crucial aspect of this vectorization is ensuring that the raster image consists of clean lines, where curves are represented as long, continuous strokes instead of multiple segments.\nTo achieve this, we introduced a fine-tuning approach for the Stable Diffusion model, applying context consistency loss during pairwise training to create the Vector LoRA. This Vector LoRA is specifically designed to simplify the image into vector-friendly strokes while maintaining the artistic integrity of the style. A key innovation of our work is the compatibility of Vector LoRA with other Style LoRA modules, enabling us to retain various artistic styles while producing vectorized outputs. This flexibility allowed us to collaborate with renowned Korean"}, {"title": "II. DYNAMIC HUMAN-ROBOT INTERACTION", "content": "SketcherX features two 6-axis KUKA (Bischoff et al., 2010) robots, one of which serves as the Face Robot. The Face Robot's head is equipped with a camera that detects human faces, initiating image processing upon detection. To create a realistic robotic face, the face content is 3D-modeled using Unity, with a strategically positioned projector behind the face for seamless projection mapping onto the sculpture, aided by display calibration techniques.\nUser speech is captured by a front-mounted microphone and converted to text via Microsoft Azure's Speech-to-Text (STT) service. The transcribed text is then processed by OpenAI's GPT-4 model (Achiam et al., 2023), allowing the robot to respond to user inquiries. During this interaction, the robot analyzes the conversation to detect the user's emotions, triggering corresponding facial expressions such as laughing, frowning, or crying\u2014and robotic motions like dancing or nodding, enabling dynamic, responsive interactions."}, {"title": "III. METHOD FOR ROBOTIC VECTOR SKETCHING", "content": "To visually demonstrate the robot's drawing process and create unique portraits, we developed a specialized drawing algorithm. This algorithm had to 1) accurately capture human features in a distinctive drawing style and 2) render the drawing in continuous, sequential strokes. To achieve these preferable conditions, we propose fine-tuning Stable Diffusion models using strategies such as context consistency loss and pair-wise fine-tuning with Dreambooth (Ruiz et al., 2023). We also integrated ControlNet (Zhang et al., 2023) and Vision-language (VL) models to enhance the depiction of human features. The overall drawing process is illustrated in Fig. 2."}, {"title": "A. Text-to-image model-based stroke line image generation", "content": "To achieve clean and precise vectorization results, we leverage a text-to-image model that outputs well-defined line art, essential for emulating the sequential pen actions of human sketching. This vectorization process is implemented using parameterized basic shapes, such as Bezier curve control points or polygon vertices, making it resolution-independent and allowing for the determination of end-to-end positional points of strokes. We employ the method from Mo et al. (2021), which utilizes a Recurrent Neural Network (RNN) for step-by-step stroke prediction and a dynamic window to maintain the full resolution of the input. The strokes are generated using a Convolutional Neural Network (CNN) encoder and an RNN decoder, which outputs the stroke parameters. These are then refined through differential rendering based on Bezier curves and differential pasting, converting the resulting SVG files into data interpretable by the KUKA PRC robotic system.\nA major challenge is ensuring that the vectorized output is simplified, with curves represented as single, continuous strokes rather than multiple segments. This requires the sketch to maintain consistent line thickness and be free of shadows or shading, ensuring clean lines with precise start and end points. To achieve this level of precision, we propose fine-tuning the stable diffusion model using our proposed context consistency loss.\n1) Pair-wise fine-tuning with context consistency loss: We adopted Dreambooth (Ruiz et al., 2023), an effective method for fine-tuning with a small number of images, to fine-tune the Stable Diffusion model using a customized LoRA. LORA (Hu et al., 2021) (Low-Rank Adaptation) is additional trainable low-rank matrices, updating a diffusion model's weight matrix W. We developed \"Vector LoRA,\" which transforms images into simple stroke line arts suitable for vectorization. This approach allows for easy combination with other style-specific LoRAs, enabling the production of images in diverse styles that are ideal for vectorization.\nOur findings indicate that pairwise training of regulariza- tion images with style images during training enhances the expression of the desired style. In Dreambooth, fine-tuning for a specific style involves appending a unique identifier to the subject's class in the image's text prompt, while applying class-specific prior preservation loss to maintain the semantic prior of the class.\nWe revised this idea to develop pair-wise training. Our model's input is a face captured by a camera, which we aim to convert into a vectorized image with clear constraints. The training data consists of pairs of style images (lineart drawing of a human face with the desired style) and regularization images (human face data). We denote this pair as (Xstyle, Xreg). The text prompt Pstyle for the style image includes a description and a unique identifier, while the prompt Preg for the regularization image includes only the description. Pre-trained text-to-image diffusion model \u00eee is fine-tuned using these image pairs, with text prompt pair which is conditioned within a text encoder \u0393, where Cstyle = \u0393(Pstyle) and Creg = \u0393(Preg). Stable diffusion model denoises the latent zt = Atx + \u03c3\u03c4\u03b5, where at, \u03c3\u03c4 are terminology for noise sampler. We simultaneously trained the model with the ground truth image pair (Xstyle, Xreg) to enable the model to learn the differences between the two images more closely. The reconstruction loss is as follows:\n$L_{rec} = \\lambda_1 ||\\hat{x}_0(\\alpha_t x_{style} + \\sigma_t \\epsilon, C_{style}) - x_{style} ||_2 $\n$+ \\lambda_2 ||\\hat{x}_0(\\alpha_t x_{reg} + \\sigma_t \\epsilon, C_{reg}) - x_{reg}||_2, $\n(1)"}, {"title": "2) LORA merging of Vector LoRA and Style LoRA", "content": "Vector LORA alone can transform an image into a style suitable for vectorization. Moreover, its compatibility with other style-specific LoRAs allows for flexible and diverse stylistic transfor- mations, enabling the creation of images in various styles ideal for vectorization. Specifically, LoRA updates weight matrix W\u2208 Rn\u00d7m in the diffusion model \u00ee by adding a low-rank term, i.e., W' = W + BA, where B \u2208 Rn\u00d7r and A\u2208 Rrxm for r < min(n, m). When merged with Vector LoRA and Style LORA, the updated W' weight of \u00ee is given by:\nW' = W + (w \u00d7 BvAv + ws \u00d7 BsAs),\n(4)\nwhere \u0392\u03c5, \u0391\u2082 are the matrices of low-rank factor in Vector LORA, Bs, As are the matrics of Style LoRA and wr and ws are balancing parameters. Fig. 4 compares the SVG conversion results when using only the Style LoRA and when using both the Vector LoRA and Style LoRA. The results demonstrate that Vector LoRA is highly effective for generating smooth stroke lines. Additionally, it shows that Vector LoRA is compatible with various Style LoRAs, allowing for diverse stylistic transformations. This flexibility allowed us to collaborate with renowned manga artist Lee Hyun-Se, creating a robot exhibition that drew portraits in his distinctive style. The exhibition received significant attention and spotlight from various media outlets."}, {"title": "B. Techniques for Human Feature Extraction", "content": "Accurately capturing human features is essential for portrait drawing. Initially, we use StyleGAN v2 (Karras et al., 2020) with StyleCLIP (Patashnik et al., 2021) to modify features such as expressions and accessories. However, these models often overlook details not present in the CelebA-HQ dataset, such as clothing details and diverse accessories, resulting in limited diversity. Furthermore, the requirement for precise facial alignment makes it challenging to capture upper body features."}, {"title": "IV. CONCLUSION", "content": "We developed an innovative robotic system for creating personalized portraits through interactive user engagement. Combining advanced diffusion-based image processing with real-time human-robot interaction, our platform captures a person's likeness with a head-mounted camera, processes the image into a distinctive artistic style, and uses a robotic arm to draw the portrait in a human-like manner.\nUnlike previous robotic art methods that rely on analog printing, our system emphasizes the robot's autonomous identity and interactivity. Integrating Large Language Model (LLM) technology, the robot can engage in meaningful conversations, enhancing user experience. The custom drawing algorithm, fine-tuned stable diffusion model, and ControlNet and Vision Language models ensure portraits are unique and true to the subject's features.\nOptimized for efficiency, our system delivers high-quality portraits in about two minutes, with the Vector LoRA enabling seamless adaptation to various artistic styles. This work showcases a new paradigm in robotic creativity, where robots actively participate in the artistic process, moving beyond mere reproduction. Future work will further refine the system's artistic capabilities and explore broader applications in creative domains."}, {"title": "V. SUPPLEMENTARY MATERIALS", "content": "A. Hardware composition\nThe hardware configuration for the robot system is as follows.\nThe detailed hardware composition is depicted in Figure 5. We utilized two KUKA (KR4, KR6) models to create a Drawing Robot and a Face Robot. The Face Robot is equipped with a Face Mask at its end effector, where a 3D face is mapped onto the mask using the Unity engine, facilitated by a projector positioned behind the mask. An RGB camera is mounted on the top of the mask, simulating the robot's head, to capture human facial images.\nThe Drawing Robot is equipped with a pen gripper, fabricated using a 3D printer, at its end effector. A pen is inserted into this gripper, enabling the robot to draw. The system is supported by three workstations, each equipped with Core i7 processors and RTX 4070Ti GPUs. One workstation handles machine learning processing, another is responsible for face mapping, and the third serves as the master PC, managing communication between the KUKA robots and the workstations, as well as overseeing the entire system. DeviceNet is used to transmit coordinate data from the PC to the robot control system, ensuring precise execution of tasks.\nB. Training Dataset and Experiment Settings\nTo create the Vector LoRA, we employed the DreamBooth fine-tuning method. Initially, we prepared a dataset consisting of 50 image pairs. After training the model, we further refined it using augmented data generated by the model itself. Each image pair consists of a regularization image, where a person is facing forward, and a style image, which is a traced version of the same forward-facing photograph. The tracing technique was used for the style image to ensure that the vectorized image is composed of a continuous one-line stroke outlining the person. For the regularization images, we generated virtual individuals facing forward using StyleGAN v2. Figure 6 illustrates an example of these image pairs.\nDuring the first 500 iterations, we trained the model using the initial 50 image pairs. For the remaining 500 iterations, we incorporated an additional 200 augmented data pairs generated by the model. The weight terms for the reconstruction loss 1, denoted as A\u2081 and X1, were both set to 0.5. The weights for the overall loss 3, the wt of the reconstruction loss and wt of the context consistency loss were also set to 0.5 during the first 500 iterations. However, for the subsequent 500 iterations, we adjusted the weights to 0.8 for the reconstruction loss and 0.2 for the context consistency loss, focusing on enhancing the precision of the output.\nC. Performance Comparison Across Different Artistic Styles\nThe Figure 7 presented is an SVG output created by combining five different Style LoRAs with a Vector LoRA. Each column represents outputs with the same style. As discussed in the main text, our stroke transformation is crucial for producing clean lines composed of a single stroke, ensuring that the robot can accurately render the image. The role of the Vector LoRA is to facilitate vectorization while maintaining the distinctive style of each Style LoRA. When the figure is enlarged, the clarity and precision of the lines that compose each image become evident. Since the figure is the result of vectorization, it accurately corresponds to what the robot would draw on paper. The main contribution lies in the ability to effectively capture human features while transforming them into the desired style.\nD. Impact of ControlNet\nUsing the SDEdit method, specifically the img2img approach alone, was insufficient to capture all the unique characteristics of the user in a precise portrait. Figure 8 compares the different outcomes achieved by adjusting the ControlNet weight. When the weight is set to 0, the portrait reflects only coarse characteristics, such as the user's gender, but fails to capture deeper features. We found that a weight of 0.8 produced the most optimal results. However, this weight may need to be adjusted depending on the style LoRA used in conjunction with it.\nE. Artwork Examples and Exhibition History\nFigure 9 shows the examples of portraits created by SketcherX. These portraits effectively capture the unique characteristics of individuals, adapting to various styles based on the nature of the exhibition. SketcherX tailors its artistic approach to suit the theme of each exhibition, producing portraits that resonate with the audience.\nFigure 10 is a depiction of SketcherX during an actual exhibition, showcasing its interaction with attendees. Since its debut at CES 2023, SketcherX has undergone continuous development, evolving into its current form. It has been exhibited at various venues, including The Hyundai Seoul, Galleria Department Store in Apgujeong, Hyundai Department Store in Pangyo, Lee Hyun-se's Road: The Legend of K-Webtoon, and Museum-X in Sokcho. Through these exhibitions, SketcherX has engaged with the public and received positive recognition for its approach as an interactive media.\nVI. CREDIT ATTRIBUTIONS\nThis project was performed by XORBIS R&D Center. We would like to express our sincere gratitude to the following individuals for their invaluable contributions to this project. Sukhwan Choi for system integration and robotics program- ming, Jeewoong Lieu for product and mechanical design, Jiwoong Ryu and Kimyung Lee for face software and 3d programming, Solmi Kim and Gyeongwon Joo for robot face and 3d modeling design, and Seunghun Mok for chat system integration. We also acknowledge Eunjung Yoo and Chaewon Kim for curation, Dr. Kyounghun Lim for design, Dr. Jungwoo Chae for his support in creative directing."}]}