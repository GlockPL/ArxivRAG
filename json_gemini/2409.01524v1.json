{"title": "S3C-MATH: Spontaneous Step-level Self-correction Makes Large Language Models Better Mathematical Reasoners", "authors": ["Yuchen Yan", "Jin Jiang", "Yang Liu", "Yixin Cao", "Xin Xu", "Mengdi Zhang", "Xunliang Cai", "Jian Shao"], "abstract": "Self-correction is a novel method that can stimulate the potential reasoning abilities of large language models (LLMs). It involves detecting and correcting errors during the inference process when LLMs solve reasoning problems. However, recent works do not regard self-correction as a spontaneous and intrinsic capability of LLMs. Instead, such correction is achieved through post-hoc generation, external knowledge introduction, multi-model collaboration, and similar techniques. In this paper, we propose a series of mathematical LLMs called S\u00b3C-MATH, which are able to perform Spontaneous Step-level Self-correction for Mathematical reasoning. This capability helps LLMs to recognize whether their ongoing inference tends to contain errors and simultaneously correct these errors to produce a more reliable response. We proposed a method, which employs a step-level sampling approach to construct step-wise self-correction data for achieving such ability. Additionally, we implement a training strategy that uses above constructed data to equip LLMs with spontaneous step-level self-correction capacities. Our data and methods have been demonstrated to be effective across various foundation LLMs, consistently showing significant progress in evaluations on GSM8K, MATH, and other mathematical benchmarks. To the best of our knowledge, we are the first to introduce the spontaneous step-level self-correction ability of LLMs in mathematical reasoning.", "sections": [{"title": "Introduction", "content": "Reasoning is one of the essential foundational abilities of large language models (LLMs), showing the capacity of LLMs to tackle complex real-world problems. Nowadays, researchers are increasingly focusing on the performance of LLMs in specific reasoning tasks like mathematics, code, logic, common-sense, etc. (Sun et al. 2024). Mathematics is one of the significant branches of reasoning ability of LLMs, and solving a mathematical problem can demonstrate an LLM's ability to decompose, reason, and summarize complex problems. In recent work, the chain-of-thought (CoT) inference (Wei et al. 2023) has been proven to be a method that can significantly enhance an LLM's ability to solve reasoning problems (Suzgun et al. 2022; Wang and Zhou 2024). CoT induces the model to explicitly output the reasoning process, gradually deriving a series of intermediate processes or sub-goals, thereby enabling the model to correctly answer reasoning questions. However, during the step-by-step reasoning process, an LLM may still generate errors (Chen et al. 2024; Wang et al. 2024). These errors can be propagated to subsequent reasoning processes, leading to incorrect outputs from the LLMs.\nTo alleviate potential errors that may occur during model reasoning, self-correction methods have been proposed. These methods can check whether the model makes mistakes during the reasoning process, pinpoint the errors, and enable the model to generate a better answer based on the provided feedback (Kamoi et al. 2024; Pan et al. 2024). Existing research has already leveraged such error correction processes to make the model's response more reliable(Puerto et al. 2024; Paul et al. 2024; Xu et al. 2024a). The crux of self-correction lies in how to generate feedback for the model's output, and how to generate a better response based on this feedback (Tyen et al. 2024).\nIn terms of generating feedback, for some specific tasks, external tools or knowledge can be used as feedback. For instance, in the task of code generation, the code generated by the model can be handed over to the compiler. If the code cannot compile, the model can be informed that an error exists, and the error stack can be provided to the LLMs (Zheng et al. 2024; Li et al. 2024a). For tasks that are difficult to judge with external tools, some research provides the model's original answer to more capable models such as GPT4 or a trained critic model specifically for error detection, thereby generating corresponding feedback (Zhang et al. 2024b). Such self-correction is usually applied at the instance-level, i.e., after the model's complete output, feedback generation and error correction are completed. Given that most reasoning tasks are solved in a CoT manner, these self-correction methods have begun to be applied at the step-level, providing the model with more granular signals. We illustrate these self-correction methods in panels (a) and (b) of Figure 1.\nHowever, we believe that the aforementioned methods still seem unnatural in LLMs' inference. Utilizing multiple models to address a single problem does not provide a fair assessment of the LLM's reasoning ability, and such an approach cannot be considered an end-to-end capability of the model. Therefore, some researchers have begun to propose implementing all steps of self-correction within the same model, using multi-task learning to enable a single model to master both problem-solving and error correction tasks (Madaan et al. 2023; Liu et al. 2024). Yet, we do not think this is enough. Multi-stage processing requires LLMs to generate several times, thus increasing inference time and computational costs. Moreover, this behavior is not spontaneous from the model itself, but a preset fixed process in multi-stage processing, which cannot demonstrate the model's capacity for error recognition. Based on these considerations, we propose spontaneous step-level self-correction capability for LLMs, which allows the LLM to spontaneously identify errors in its on-going output and correct them immediately, ultimately yielding more reliable responses. This process is illustrated in panel (c) of Figure 1.\nIn this paper, we propose an ingenious method for constructing self-correction data to achieve spontaneous step-level self-correction. This method utilizes existing step-by-step reasoning instruction data, generating potentially erroneous steps through sampled generation, validating whether there are indeed errors in the steps through the pass@k validation. We insert the erroneous steps and the markers used to trigger self-correction into the correct steps in the existing data and construct S\u00b3C-MATHQA, which includes 532K self-correction data, based on Meta-MathQA (Yu et al. 2024). In order to make self-correction more accurate and generalize to more mathematical problems, we annotate each correction case in a fine-grained manner, adding the reflection and improvements. During the training stage, we mix S\u00b3C-MATHQA and the original 395K MetaMathQA to create 927K SFT data, and use loss-masks to ignore the loss of the erroneous steps. This method ensures that the LLMs are introduced with a new self-correction capability while maintaining their original effectiveness. Our experiments show stable and consistent improvements across multiple mathematical datasets such as GSM8K (Cobbe et al. 2021) and MATH (Hendrycks et al. 2021) on both generalist LLMs like Meta-Llama-3-8B (Meta AI 2024) and math-specialized LLMs like DeepSeek-Math (Shao et al. 2024). Our main contributions are as follows:"}, {"title": "Related Works", "content": "Mathematical reasoning, one of the foundational abilities of LLMs, demonstrates the model's capability to solve complex real-world mathematical problems, which can be enhanced during various stages of LLMs' training. In the pre-training stage, continuing pre-training on large amounts of mathematical corpora can improve the internal mathematical reasoning ability of LLMs from a knowledge perspective (Ying et al. 2024; Shao et al. 2024; Zhou et al. 2024). During supervised instruction tuning, mathematical abilities could be further enhanced by training on problem-answer pairs, leading to the same answer formatting and better reasoners (Yu et al. 2024; Li et al. 2024b). For the preference alignment stage, preference optimization (Chen et al. 2024) and reinforcement learning (Lightman et al. 2023; Wang et al. 2024) are used to improve LLMs at a fine-grained level.\nSelf-correction is an advanced technique to enhance an LLM's output by refining its initial response during the inference process, which can be implemented through variousmethodologies (Kamoi et al. 2024; Pan et al. 2024). Strategies of self-correction center around the concept of feedback generation. Essentially, the LLM must evaluate whether errors exist in its initial output, identify these errors, and pinpoint the steps needed for correction. The timing of self-correction can be categorized into two main types: post-hoc and real-time (Paul et al. 2024; Jiang et al. 2023). In the post-hoc approach, feedback is generated after the initial inference, based on which the LLM is prompted to regenerate the response. In contrast, the real-time approaches integrate feedback into the generation context during the inference process, enabling the LLM to produce a completion with the necessary corrections on the fly. Additionally, feedback of the LLMs' contents could be generated in two types: external knowledge and tools to such as code compilers (Zheng et al. 2024; Li et al. 2024a) or LLMs themselves. Feedback generation by LLMs can be bifurcated into cross-model and same-model approaches. In the cross-model approach, a more advanced LLM or a dedicatedly trained critic model is employed to produce the feedback (Li, Patel, and Du 2024; Cohen et al. 2023; Du et al. 2023). On the other hand, the same-model approach utilizes the same LLM for both the initial generation and the feedback, albeit with different prompts to guide the correction process (Thorne and Vlachos 2021; Tyen et al. 2024; Paul et al. 2024).\nSynthesized data generated from existing LLMs has been proven effective for training another LLM (Yu et al. 2024; Liu and Yao 2024; Li et al. 2024b; Zhu et al. 2024; Xu et al. 2024b; Zhou et al. 2024), which can be used in both pre-training and supervised fine-tuning. For pre-training, data can be synthesized to expand the scope of mathematical knowledge that LLMs can learn from. For instance, continuing pre-training on the synthesized Jiuzhang dataset (Zhou et al. 2024) significantly improves LLMs' performance on multiple mathematical benchmarks. For supervised fine-tuning, there are also various methods to synthesize supervised instruction pairs. For example, MetaMathQA (Yu et al. 2024) augments questions from the original training set and generates answers for those bootstrapped questions. More recently, KPMath (Zhu et al. 2024) extracts key points from existing data to analyze and synthesize more complex mathematical word problems."}, {"title": "Approach", "content": "The main works of this paper can be divided into two parts. The first part is to construct self-correction SFT data S\u00b3C-MATHQA based on existing step-by-step mathematical instruction pairs. The second is to use S3C-MATHQA to carry out instruction fine-tuning, ultimately enabling the model to acquire spontaneous step-level self-correction capabilities."}, {"title": "Data Construction", "content": "We divide our proposed self-correction data construction process into two steps. The first step involves generating erroneous steps from existing correct CoT steps, which will be used to create step-level self-correction instances. The second step concerns reflections and improvements for the above step-level self-correction instances, which serve as detailed guidance for LLMs to better acquire the ability to dospontaneous step-level self-correction. In this paper, we use the MetaMathQA (Yu et al. 2024) dataset, which includes 395K mathematical reasoning step-by-step samples, as the seed dataset.\nBefore we start, we carry out instruction tuning on the Meta-Llama-3-8B (Meta AI 2024) using the MetaMathQA data, resulting in our model M used for sampling erroneous steps. Since we need to sample on the training set, in order to mitigate the reduction of diversity caused by model over-fitting, we adopt the idea of cross-validation and conduct a 5-fold cross-training. That is, we evenly divide the MetaMathQA data into five parts, use four parts of the data to train the model, and the remaining one part of the data for sampling erroneous steps. This process is repeated five times on different parts of the data. Hereafter, we denote the data used for model training as s, and the remaining data used for sampling erroneous steps as 5. The model that undergoes instruction tuning on s is denoted as Ms.\nFirst, we use the trained model Ms to perform step-wise sampling on the split-out data 3, obtaining a set of steps that may contain errors. The specific method is that for each step $x_i$ in an SFT sample response x, we concatenate all the steps from the first to the ith step into a context and give it to Ms for continuation, until the model outputs the next step, obtaining $candidates(\\hat{x_{i+1}})$. We use a high temperature and sample multiple steps at the same time to ensure the diversity of steps. The candidates of the (i+1)th steps of case x could be represented as:\n$candidates(x_{i+1}) = M(x_1 \\oplus . . . \\oplus x_i), x \\in \\S$\nwhere $\\oplus$ indicates concatenation of existing correct steps, and $M(x_1... \\oplus x_i)$ represents the set of responses generated by Ms for $x_1 \\oplus ... \\oplus x_i$ k times.\nNext, we will use the trained model to evaluate the correctness of the generated candidates $x_{i+1}$. Specifically, for each step i+1 in the candidate set, we concatenate the correct preceding steps $x_1,..., x_i$ as context before the step $\\hat{x_{i+1}}$, and ask the model Ms to continue generating until the model actively ends the generation (i.e., encounters the <end-of-sequence, token). We will determine whether the step is correct based on whether the answer generated by model Ms matches the standard answer. In order to make more accurate judgments and reduce the cases where correct steps are mistakenly considered as wrong, we use pass@k to evaluate a step. Specifically, we sample multiple model outputs with a high temperature of 1.0 for 16 times, and only when none of the contents generated by the model match the correct answer do we consider the step to be wrong. Whether $\\hat{x_{i+1}}$ is correct can be represented as:\n$\begin{aligned}\\ Correct &\\text{ if } \\exists o \\in M^k(x_1 \\oplus ... + x_i \\oplus \\hat{x_{i+1}}), A(o) = A(gt) \\\\[-1ex] Wrong &\\text{ if } \\forall o \\in M^k (x_1... x_i \\oplus \\hat{x_{i+1}}), A(o) \\neq A(gt)\n\\end{aligned}$\nwhere A(o) represents the answer extraction for the output o, while A(gt) stands for the answer extraction for the golden truth."}, {"title": "Reflection and Improvement Generation", "content": "Based on the correct steps in the existing data and the erroneous steps we sampled in the previous section, we inserted the erroneous steps and the flags used to trigger self-correction into the correct steps, thus forming the direct self-correction data. However, another significant challenge of self-correction is how the LLMs can produce more accurate content when they know that errors already exist. To enable the LLMs to better learn to correct the identified mistakes, we annotated the acquired step-level self-correction samples in two ways, reflection and improvement, using Meta-Llama-3-70B-Instruct (Meta AI 2024). Reflection involves the model analyzing where errors occurred in the existing answers, while improvement entails generating ways to improve based on the existing output and reflection. When generating these two annotations, we provide the model with a prompt, as well as a question, previous step, wrong step, and correct step, and ask the model to make annotations based on this information. The prompt we used is shown in Appendix A. We present a data example and its components in Figure 3."}, {"title": "Tuning and Evaluation", "content": "After constructing the aforementioned data, we implement SFT on it. We test our trained model on multiple mathematical evaluation benchmarks to validate the effectiveness of our data and methods."}, {"title": "Analysis", "content": "Since we introduced new data into the original Meta-MathQA dataset, even though our data's queries all come from MetaMathQA, there is still a possibility of changing the query distribution of the SFT data. Our error path sampling method may generate more self-correction data on more challenging problems, thus more difficult queries may appear more frequently in our SFT data. To ablate the impact of these situations on our results and to prove that our improvement comes from self-correction itself rather than the distribution changes caused by sampling, we conducted the following ablation experiment. The specific method of this experiment is that, based on the queries of the 532K self-correction data we generated, we over-sampled the original data from MetaMathQA, resulting in a total of 927K data. The distribution of queries in the over-sampled data is the same as that in our proposed 927K data.\nOne of the key features of our method is to directly construct self-correction data from the existing step-by-step mathematical data. Therefore, in our correction data, apart fromthe erroneous step and the corresponding correction behavior, the remaining correct steps are completely consistent with the original SFT data. This allows us to seamlessly introduce the intrinsic self-correction ability into the LLMs. Currently, in exploring the mathematical capabilities of the LLM, scholars use Monte Carlo Tree Search (MCTS) for step sampling (Wang et al. 2024; Zhang et al. 2024a), which allows for more diverse steps and decouples from the existing SFT data. To demonstrate that our direct sampling of erroneous steps from existing SFT data and construction of self-correction data is more effective, we designed this ablation experiment. We implemented an MCTS to generate correct and incorrect steps for all queries in MetaMathQA, and used these steps to construct self-correction data. For a fair comparison, we also included the original MetaMathQA data and constructed MCTS self-correction data of the same scale as the data volume as S\u00b3C-MATHQA."}, {"title": "Conclusion", "content": "In this paper, we introduce a novel capability for LLMs, the spontaneous step-level self-correction ability, which allows LLMs to recognize errors in their outputs in real-time and correct them simultaneously, thereby generating a more reliable answer. We propose a method for constructing training data for this capability based on wrong step sampling and build the S\u00b3C-MATHQA based on MetaMathQA. We applied SFT on both the generalist and math-specialized LLMs using the proposed S3C-MATHQA, achieving consistent and stable improvements on multiple mathematical benchmarks. Our work demonstrates that LLMs can possess a spontaneous step-level self-correction ability. There are still areas for improvement in our work, such as generating diverse steps when sampling error steps, examining the quality of reflection and improvement, etc. In the future, we will continue on this work to increase the accuracy during the self-correction process and extend this method to more reasoning tasks, truly generalizing this ability and building a better LLM reasoner."}]}