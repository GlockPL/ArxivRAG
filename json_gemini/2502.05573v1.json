{"title": "Low-Rank Agent-Specific Adaptation (LoRASA) for Multi-Agent Policy Learning", "authors": ["Beining Zhang", "Aditya Kapoor", "Mingfei Sun"], "abstract": "Multi-agent reinforcement learning (MARL) often relies on parameter sharing (PS) to scale efficiently. However, purely shared policies can stifle each agent's unique specialization, reducing overall performance in heterogeneous environments. We propose Low-Rank Agent-Specific Adaptation (LoRASA), a novel approach that treats each agent's policy as a specialized \u201ctask\" fine-tuned from a shared backbone. Drawing inspiration from parameter-efficient transfer methods, LoRASA appends small, low-rank adaptation matrices to each layer of the shared policy, naturally inducing parameter-space sparsity that promotes both specialization and scalability. We evaluate LoRASA on challenging benchmarks including the StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo), implementing it atop widely used algorithms such as MAPPO and A2PO. Across diverse tasks, LoRASA matches or outperforms existing baselines while reducing memory and computational overhead. Ablation studies on adapter rank, placement, and timing validate the method's flexibility and efficiency. Our results suggest LoRASA's potential to establish a new norm for MARL policy parameterization: combining a shared foundation for coordination with low-rank agent-specific refinements for individual specialization.", "sections": [{"title": "1. Introduction", "content": "A canonical paradigm in MARL is Centralized Training and Decentralized Execution (CTDE), where agents learn with access to global information but execute policies independently.\nWithin CTDE, a standard approach is parameter sharing (PS), which significantly cuts down on resource requirements by training a single policy network for all agents.\nDespite its efficiency, PS can compromise the specialized behaviors needed in heterogeneous or role-based scenarios. Simple fixes, such as tagging states with agent identifiers, rarely capture deeper skill differences. Merely appending an ID to observations seldom suffices to uncover such diverging policies\u2014an agent must not only \"know\" it has a particular identity but also adapt its policy to exploit that identity effectively. Furthermore, even in homogeneous scenarios, such as StarCraft with agents of the same unit type, we empirically found that agents also require non-identical behaviors, see Sec A.3.\nTo enable better heterogeneous behaviors in multi-agent systems, researchers have explored approaches like selective parameter sharing (SePS) and adaptive parameter sharing (AdaPS). SePS clusters agents based on behavioral similarities, assigning a shared policy network to each cluster, while AdaPS dynamically selects specialized subnetworks from a shared architecture. However, SePS often struggles in dynamic environments where agent roles evolve or unique edge cases arise, as its static clustering framework cannot adapt to changes. Similarly, AdaPS can over-prune critical parts of the shared network, limiting agents' ability to leverage common knowledge in unforeseen situations. This lack of adaptability can significantly impact performance in complex environments where flexibility is paramount. For example, in disaster response scenarios, drones performing routine tasks like surveying may be effectively managed by SePS or AdaPS, but these methods often fail to address rare, specialized tasks such as hazardous material containment or rescue operations, where more nuanced specialization is required.\nFully distinct policies, such as those used in Non-Parameter Sharing (NPS), assign unique parameters to each agent. While this allows for full specialization, it forces each neural network to learn similar representations independently, often with limited data per agent, leading to sample inefficiency. Moreover, the approach is computationally and memory-intensive, making it impractical for large number of agents (see time and memory requirement of NPS in Figure 3).\nIn light of these challenges, there is a strong incentive to develop methods that preserve the efficiency and coordination benefits of parameter sharing while allowing agents to specialize effectively. In this paper, we introduce a novel perspective on MARL by framing it as a multi-task learning (MTL) problem, where each agent's policy is treated as a distinct task requiring specialized behavior. Unlike previous multi-task MARL approaches that aim to generalize across similar tasks to facilitate adaptation, our approach focuses on fostering diverse agent behaviors within a unified multi-agent objective. For instance, in a disaster-stricken city, rescue robots share the common skill of navigating debris-filled environments but specialize in tasks like clearing rubble, delivering medical supplies, or locating survivors. These distinct roles require specialized policies that cannot be effectively captured by a single shared network or simple agent identifier augmentations.\nTo address this need for efficient specialization, we propose Low-Rank Agent-Specific Adaptation (LoRASA), refer Figure 1, inspired by LoRA, a parameter-efficient fine-tuning method originally developed for large-scale natural language models. LoRA introduces lightweight, low-rank adaptation matrices that are added to pretrained weights, enabling task-specific refinements while preserving the core shared knowledge. Extending this concept to MARL, LORASA fine-tunes the shared policy with minimal overhead by constraining adaptations to a low-rank subspace. This induces parameter-space sparsity (Sec A.3), allowing each agent to specialize without the computational and memory burdens of assigning unique, full-rank parameters (see plots in Figure 3 for further evidence).\nOur work makes the following main contributions:\n\u2022 LORASA (in Sec 2): We introduce a novel low-rank adaptation mechanism for MARL, positioning the problem as a multi-task fine-tuning scenario to achieve both scalability and agent-specific specialization.\n\u2022 Comprehensive Empirical Evaluation (refer Sec 3.4): On two challenging benchmarks-StarCraft Multi-Agent Challenge (SMAC) and Multi-Agent MuJoCo (MAMuJoCo)-LoRASA consistently matches or outperforms other parameter-sharing variants based on strong baselines (MAPPO, A2PO) while utilizing fewer resources. Our extensive ablation studies on adapter rank, fine-tuning timing, and layer-wise placement provide actionable guidelines, reinforcing LoRASA's practicality for real-world deployment.\n\u2022 Parameter-Space Sparsity and Heterogeneous Behaviors (see Sec A.3): LoRASA leverages parameter-space sparsity through low-rank updates, enabling agents to exhibit diverse, specialized behaviors while retaining the efficiency and coordination benefits of a shared policy.\nTogether, these contributions highlight a paradigm shift in MARL from rigid, fully shared or fully distinct policies to a flexible, low-rank adaptation framework. LoRASA enables agent-specific specialization while preserving the coordination benefits of a shared policy, offering a scalable and efficient solution for diverse and large-scale MARL applications that demand both flexibility and resource efficiency."}, {"title": "2. Methodology", "content": "2.1. Preliminaries\nMulti-Agent Reinforcement Learning (MARL). We consider cooperative MARL problems modeled as Partially Observable Markov Games (POMGs), where each agent i observes Oi,t \u2208 Oi, selects an action ai,t \u2208 Ai according to its policy \u03c0i(ai,t | Oi,t; \u03b8i), and receives a shared reward rt at time t. The objective is to maximize E[\u2211to vtrt], focusing on cooperative tasks.\nCentralized Training & Decentralized Execution (CTDE). In this work, we adopt CTDE, where N agents train with access to global information (joint observations, rewards) yet execute independently based on local observations. This setup is a natural fit for real-world multi-agent scenarios demanding high scalability and local autonomy. Under CTDE, the joint policy \u03a0 factorizes as\n$$\\Pi(a | o) = \\Pi \\pi_i(a_i | o_i; \\theta_i)$$\n$${i \\in N}$$\nIn PS approaches, \u03b8i = \u03b8shared for all i \u2208 N, while in NPS, each agent has distinct parameters. Our method, LORASA, stands at an intermediate point, blending the resource-efficiency of PS with the flexibility of NPS.\nLow-Rank Adaptation (LoRA). LORA was introduced for parameter-efficient fine-tuning in large-scale language models. It adds a low-rank update \u03b4W = ABT to each weight matrix W, where A \u2208 Rd\u00d7r and B\u2208 Rk\u00d7r with r \u226a min(d, k). Critically, only A and B are trained, while W remains fixed. In our setting, we treat the shared policy as pretrained and each agent's specialized adaptation as a separate task. Thus, LoRA naturally encodes agent-specific deviations from a common baseline without replicating entire networks.\n2.2. LORASA: Low-Rank Adaptation for MARL\nTheoretical and Conceptual Insights. Recent studies in deep reinforcement learning suggest that the effective dimensionality of learned policies can be much lower than the total parameter count. In cooperative MARL, agents often assume distinct roles (e.g., scouting vs. attacking), indicating these policy variations lie in a smaller subspace of the full parameter space. By restricting agent-specific updates to an r-rank matrix, LoRA formally encodes each agent's deviations from a shared backbone within this lower-dimensional subspace. This design not only retains the bulk of the pretrained policy's knowledge but also efficiently captures heterogeneous behaviors without duplicating entire networks.\nProposition 2.1. Assume that in a cooperative multi-agent reinforcement learning (MARL) setting, the agent-specific parameter deviations lie within or near an r-dimensional affine subspace of the full parameter space. Then, applying a rank-r low-rank adaptation (LoRA) to the shared backbone's weights can approximate the optimal agent-specific policies with a bounded error in the least-squares sense.\nThis proposition is supported by the Eckart-Young-Mirsky theorem, which states that the best rank-r approximation of a matrix minimizes the Frobenius norm of the approximation error. Confining agent-specific offsets to a rank-r subspace thus balances scalability and expressiveness: each agent can specialize sufficiently to capture its role-specific deviations while still sharing the bulk of learned features. In practice, this translates to improved scalability, merging the resource efficiency of parameter sharing with the fine-grained specialization of non-parameter sharing in a single low-rank framework.\nWeight Parameterization in the Actor Network. Consider a recurrent actor network with fully connected (FC) layers and a recurrent unit (GRU or LSTM). Let \u03b8l \u2208 Rd\u00d7ke be the weight matrix at layer l. We add a low-rank adaptation \u03b4\u03b8l = AlBlT, where Al \u2208 Rd\u00d7r and Bl\u2208 Rke\u00d7r. These matrices are trained specifically for each agent, while the shared backbone \u03b8\u00ba remains frozen. We emphasize linear transformations in the recurrent pathway (input-to-hidden and hidden-to-hidden), leaving biases and layer-norm parameters fixed for simplicity. Nonetheless, even applying LoRA solely to linear transformations gives agents ample capacity to adapt their recurrent dynamics.\nAction Spaces and Final Layer Adaptations. For continuous and constrained action spaces, the actor network outputs the mean and log-std of a squashed Gaussian distribution. We apply LoRA to the weight matrices of the final fully connected (FC) layers responsible for generating both the mean and the log-std. This allows each agent to tailor its exploration strategy through agent-specific low-rank adaptations without duplicating entire networks. For discrete action spaces, we apply LORA to the final FC layer that produces action logits, enabling agent-specific adjustments to discrete action probabilities.\nBy focusing LoRA on these output layers, agents can refine their decision-making to match specialized roles (e.g., scouting vs. attacking) while maintaining the efficiency and coordination benefits of a shared policy backbone.\n2.3. Training Procedure\nLORASA consists of two main steps: Shared Policy Pretraining for learning shared knowledge and LoRA Fine-Tuning for agent-specific specialization.\nPhase 1: Shared Policy Pretraining. We first train a single shared policy \u03b8shared using a standard multi-agent reinforcement learning (MARL) algorithm (e.g., MAPPO, A2PO). During this phase, the system behaves like a PS method, where all agents rely on the same policy. To evaluate the robustness of the shared policy, we track key performance metrics such as cumulative returns and win rates. These metrics quantify the policy's ability to exhibit effective behaviors across tasks. Once \u03b8shared shows consistent improvement and meets predefined performance thresholds, we consider it sufficiently trained for downstream adaptation. At this point, we transition to fine-tuning, allowing agents to specialize their policies while retaining shared knowledge.\nPhase 2: LoRA Fine-Tuning. In this phase, we introduce LORA adapters {Ai, Bi} for each agent i, while keeping the shared policy \u03b8shared frozen:\n$\\Vl: \\theta_i^l = \\theta_{shared}^l + A_i B_i^T, A_i \\in R^{d \\times r}, B_i \\in R^{r \\times k}.$$\nThe low-rank dimension r controls the level of specialization:\n\u2022 Larger r enables more expressive adaptations but increases resource costs.\n\u2022 Smaller r keeps agents closer to the shared policy, ensuring efficiency.\nTuning r balances expressivity vs. efficiency-higher r approaches NPS-like fine-tuning, while lower r retains parameter-sharing benefits. During fine-tuning, agents update only \u03b4\u03b8 = AiBiT using their own trajectories, allowing specialization atop a shared backbone. This framework merges PS's scalability with NPS's adaptability, achieving specialization without excessive overhead.\n2.4. Algorithms for LORASA\nAlgorithmic Details. Algorithms 1, 2, and 3 outline our approach. In Algorithm 1, we train the shared policy exactly as in standard CTDE-based MARL. Algorithm 2 then enables agent-specific specialization by updating LORA adapters for each agent's actor network. Finally, Algorithm 3 merges the LoRA updates into the backbone weights at execution time for efficient inference.\nChoice of Baseline Algorithms (MAPPO and A2PO). We implement LoRASA on top of two distinct CTDE actor-critic methods: MAPPO extends PPO to multi-agent settings with a centralized critic and typically uses shared policy parameters, while A2PO sequentially updates each agent's policy, mimicking NPS. By applying LoRASA to both methods, we illustrate how a low-rank adaptation framework bridges these two extremes-offering parameter efficiency and fine-grained specialization within the same architecture.\nRank as a Bridge between PS and NPS. Varying r seamlessly interpolates between pure parameter sharing (r = 0) and fully distinct policies. As r grows, each agent's policy deviates more from the shared backbone, capturing complex role-specific behaviors without wholly duplicating the network. We show empirically that moderate r values are sufficient for notable performance gains while retaining a low overhead in memory and computation.\n2.5. Computational and Memory Efficiency\nDuring pretraining, LoRASA behaves like conventional PS, incurring no extra cost. Once fine-tuning begins, each agent introduces \u2211er(de + ke) additional parameters, far fewer than duplicating entire networks (as in NPS). At inference, these adapters can be merged with the shared backbone (Algorithm 3), meaning the final memory footprint remains close to a single policy, scaled only by small, rank-dependent matrices. Figures 2 and 3 show that LORASA achieves higher performance than naive PS at a fraction of NPS's overhead, confirming its scalability.\nFurthermore, by harnessing low-rank subspace adaptation, LORASA offers an attractive middle ground\u2014yielding heterogeneous agent behaviors with minimal resource demands. Overall, LORASA expands the design space for cooperative multi-agent RL, moving beyond rigidly shared or fully distinct parameters toward a more adaptive paradigm."}, {"title": "3. Experimental Setup", "content": "We evaluate LoRA-based multi-agent reinforcement learning (MARL) across diverse continuous and discrete environments, detailing our tasks, baselines, metrics, and key findings. Our experiments demonstrate LoRASA's ability to bridge the gap between purely shared (PS) and fully distinct (NPS) policies, reducing resource overhead while retaining agent-specific specialization.\n3.1. Environments and Tasks\nMAMuJoCo (Continuous Control). We first consider MAMuJoCo, where each agent controls specific joints of a multi-limbed robot. Actions are continuous torques, and observations include local joint information (positions, velocities, etc.) plus agent IDs. Episodes run for up to 1000 steps or until the robot becomes inactive (e.g., falls). We benchmark on Half Cheetah 2x3, Walker 3x2 and Ant 4x2 under partial observability. For Humanoid 9|8, we follow prior work in providing global states to avoid degenerate solutions under severe partial observability. These tasks vary significantly in coordination needs, aiming to test LoRASA's generality in continuous multi-agent control.\nSMAC (Discrete Combat). For discrete actions, we utilize the StarCraft Multi-Agent Challenge (SMAC). Unlike SMAC-v2, which randomly samples agents across episodes, SMAC maintains consistent agent assignments. This consistency is crucial for training agent-specific parameters, as random sampling in SMAC-v2 could lead to some agents being trained more frequently than others, introducing unwanted complexity. In SMAC, each agent controls a StarCraft II unit with observations that include local surroundings, partial enemy and ally information, and agent IDs. We evaluate our approach on maps such as 3s5z, 1c3s5z, 3s5z_vs_3s6z, and MMM2. These scenarios require specialized roles\u2014for instance, medics versus frontline marines-providing a robust test of LoRASA's ability to learn heterogeneous behaviors without necessitating fully separate policies."}, {"title": "3.2. Baselines and Comparisons", "content": "We compare LoRASA to four baselines spanning full sharing, partial sharing, and fully separate parameters:\n\u2022 PS + ID. A single shared policy for all agents, with agent IDs appended to observations. Highly memory-efficient but often fails to capture diverse agent roles.\n\u2022 NPS. Each agent trains an entirely separate network, allowing maximal specialization at the cost of significant resource overhead.\n\u2022 SePS. Clusters agents by similarity and assigns a shared policy per cluster. Reduces the overhead of NPS but may perform suboptimally when agent diversity is high.\n\u2022 MTL. Multi-task learning with partial sharing, typically restricting specialization to the final layer while sharing other layers between agents across tasks.\nLORASA comprises two methods, PS+LORA and SePS+LORA, which build on top of PS and SePS, respectively. By contrast, LoRASA uses low-rank adaptation matrices on a shared backbone, aiming for near-NPS specialization with far lower parameter and computational demands."}, {"title": "3.3. Evaluation Metrics and Protocol", "content": "Metrics. We measure cumulative episode return for MAMuJoCo and episodic win rate for SMAC (and episodic return see Appendix Figure 5). We also report the total parameter count and wall-clock training and inference time, reflecting LoRASA's resource efficiency.\nTraining Protocol. All methods train for up to 12 million steps, repeated over 5 random seeds for reliability. LoRA fine-tuning begins after a shared-policy pretraining phase, typically when the shared policy begins to demonstrate improved learning (see ablation studies in Figure 4). We choose this checkpoint to balance the need for core coordination strategies (captured by the shared policy) against leaving sufficient room for agent-specific refinements. Appendix A.4 provides additional implementation details."}, {"title": "3.4. Overall Performance and Resource Usage", "content": "Performance Across Benchmarks. Figure 2 compares LORASA-based approaches (PS+LoRA, SePS+LoRA) with the baselines on MAMuJoCo and SMAC. LoRASA frequently outperforms naive PS and, in many tasks, matches or surpasses NPS-yet at a fraction of NPS's parameter overhead. Under A2PO, SePS+LORA and PS+LORA achieves top scores on Walker 3x2 and Ant 4x2, reflecting its ability to adapt efficiently within a shared architecture. MAPPO shows similar trends, with PS+LoRA leading in tasks like Walker and Ant. Even in tasks where MTL and NPS performs strongly (e.g., Half Cheetah in MAPPO and A2PO respectively), LoRASA remains competitive but requires significantly less computation than NPS. Notably, all methods trained using MAPPO-including LoRASA-failed to make meaningful progress on the exceptionally challenging Humanoid 9|8 task. This consistent struggle highlights the extreme complexity of this scenario under the MAPPO framework.\nSMAC tasks show a similar pattern: LoRASA-based methods often tie or exceed the strongest baselines in scenarios like 3s5z and MMM2. Although certain maps (e.g., 3s5z_vs_3s6z) still favor naive PS, LoRA-based approaches remain highly effective, underscoring LoRASA's adaptability across different MARL challenges.\nResource Efficiency. Figure 3 compares parameter counts and runtime. While PS is cheapest, it often underperforms in roles requiring specialization. NPS, though powerful, scales poorly in both memory and wall-clock time. LORASA achieves strong performance similar to (and sometimes exceeding) NPS with far fewer additional parameters. This is especially evident when scaling from 4 to 8 agents, where NPS overhead spikes but LoRASA's cost grows moderately thanks to its low-rank updates. These findings highlight LoRASA as the \"sweet spot\" in MARL: it achieves the expressiveness and performance of NPS while retaining the parameter efficiency and scalability of PS, making it a practical, resource-friendly solution for large-scale MARL systems."}, {"title": "3.5. Ablation Studies", "content": "Our ablation experiments highlight three key dimensions-fine-tuning checkpoints, LORA rank, and adapter placement-that underscore LoRA's capacity to systematically unlock agent-specific policies while preserving the advantages of PS. We present ablation"}, {"title": "3.6. Discussion, Limitations and Future Work", "content": "LORASA significantly boosts performance and efficiency but comes with caveats. It relies on careful hyperparameter tuning-particularly rank selection and fine-tuning checkpoints-and depends on a robust pretrained shared policy. Fixed ranks across layers and agents may not capture the full diversity of highly heterogeneous or dynamic tasks. Moreover, LoRASA currently focuses on actor networks, leaving biases, normalization layers, and critic components unadapted. However, this approach can be extended to value-based methods; for instance, applying LoRASA to the utility functions in QMix enables agent-specific adaptations in value estimation, facilitating more nuanced coordination without requiring entirely separate policies. Future work can address these limitations by exploring dynamic rank adaptation per layer and agent type, and by extending LoRA to additional network components such as biases, normalization layers, and critic networks. Specifically, AdaLoRA decomposes \u03b4W using singular value decomposition to dynamically reduce the rank by removing less important singular values, while DyLoRA introduces adaptive mechanisms that train LoRA blocks across a range of ranks. Additionally, investigating alternating updates between shared and LoRA parameters, and integrating LORA with hierarchical or adversarial policy architectures, could further generalize low-rank specialization. Extending this framework to competitive and adversarial multi-agent systems remains a promising direction, potentially enabling effective specialization in non-cooperative settings. These avenues promise to enhance LoRASA's adaptability and robustness in large-scale, complex MARL applications. In challenging scenarios where the underlying algorithm struggles to learn a solid foundation , LoRASA's effectiveness diminishes."}, {"title": "4. Related Work", "content": "Parameter sharing (PS) is a common strategy in MARL that reduces computational complexity by using a single policy or value network for all agents. However, standard PS often fails in heterogeneous environments where agents require diverse behaviors. Dynamic sharing methods improve adaptability by assigning specific network modules based on predefined clusters, but they increase computational overhead, depend on role assumptions, and can introduce training instability-especially when agent roles change rapidly. In dynamic sharing methods, each agent's parameter subset can be significantly smaller than other policy baselines, making it unclear whether performance gaps stem from suboptimal selection or insufficient capacity. This scale discrepancy complicates direct comparisons with other parameter sharing approaches and is thus left out of the scope of this study. Techniques like agent identification or regularization-based methods attempt to differentiate agents within a shared network, but often lack sufficient representational capacity or add complexity and tuning burdens. In contrast, our approach embeds a low-rank structure directly into shared parameters, inducing sparsity and enabling agent-specific specialization without requiring dynamic reconfiguration, clustering, or heavy regularization.\nSelective Experience Sharing. Selective experience-sharing methods improve data efficiency by exchanging only relevant trajectories, reducing communication overhead and accelerating learning. However, they do not address policy expressiveness, as agents may still be constrained by a single shared model. In contrast, LoRASA operates at the parameter level, ensuring that even with fully shared transitions, low-rank offsets allow agents to develop specialized behaviors in an r-dimensional subspace. Thus, experience sharing enhances sample efficiency, while LoRASA enables representational flexibility, making them complementary rather than conflicting approaches.\nNetwork Pruning techniques sparsify shared networks to lower resource usage. However, removing parameters outright risks discarding critical features needed by certain agents, especially in tasks requiring rare but crucial skills. Our work takes the opposite route: we add low-rank modules to a shared backbone, preserving the base network and preventing irreversible performance degradation from over-pruning. This approach naturally balances expressiveness and efficiency by localizing agent-specific adaptations in small, learnable subspaces.\nNon-Parameter Sharing (NPS) policies allow maximal specialization, but scale poorly in agent-heavy systems due to their linear growth in parameters and slower training due to re-learning of common behaviors. Despite their strong performance, these methods are often untenable for large populations of agents. In contrast, our low-rank approach approximates the benefits of NPS\u2014i.e., agent-specific customization-while retaining the resource efficiency of a shared framework.\nMARL as Multi-Task RL methods often aim to transfer knowledge such as shared decision-making modules, task representations, or agent-interactions across distinct tasks rather than to accommodate diverse roles within a single shared task. This makes them less suited for MARL scenarios where agents differ significantly but still collaborate on one global objective. In contrast, our work explicitly treats each agent as a separate \"task\", applying parameter-efficient fine-tuning via low-rank adapters. Unlike approaches that only adapt output layers, we modify internal layers as needed to capture nuanced agent behaviors without incurring the high overhead of duplicating entire networks."}, {"title": "5. Conclusion", "content": "We introduce LoRASA, a novel approach in MARL that integrates LoRA into parameter-sharing frameworks. LoRA enables scalable, specialized policies by constraining agent-specific updates to low-dimensional subspaces, effectively combining the efficiency of shared backbones with the expressiveness of near-independent policies. Our extensive experiments on MAMuJoCo and SMAC benchmarks demonstrate that LoRA-based methods consistently outperform or match specialized baselines like NPS, while significantly reducing both parameter and computational overhead. Ablation studies reveal that (1) Deeper Network Layers are essential for performance gains. (2) Optimal Transition Timing occurs when LoRA fine-tuning begins once the shared policy achieves competent, non-plateaued performance. (3) A LORA Rank of 8 effectively balances capacity and efficiency, scaling appropriately with task complexity. These findings provide practical guidelines for integrating LORA into MARL pipelines. Future work will explore dynamic rank adaptation per layer and agent type, alternating updates between shared and LoRA parameters, and extending LoRA to critic networks and adversarial multi-agent systems, thereby enhancing adaptability and robustness in complex MARL environments."}, {"title": "A. Appendix", "content": "A.1. Pseudocode\nAlgorithm 1 Phase 1: Shared Policy Pretraining\nInput: N: number of agents, Env, Algorithm (e.g., MAPPO/A2PO), 0shared: shared parameters, Psteps: pretraining steps\nOutput: Pretrained @shared\n1: Initialize @shared:\n2: for step \u2190 1 to Psteps do\n3: Collect joint trajectories (obs, actions, rewards, next_obs) from Env\n4: Oshared \u2190 Algorithm.update_shared(@shared, trajectories)\n5: end for\noutput shared\nAlgorithm 2 Phase 2: LoRA-Based Fine-Tuning\nInput: N: number of agents, Env, Algorithm, pretrained @shared, rank r, Fsteps\nOutput: Agent-specific LoRA adapters {A, B}1\nIntroduce LoRA adapters A, B; freeze @shared\nAdexr, B\u2190 Random(ke \u00d7 r)\nfor step \u2190 1 to Fsteps do\nCollect trajectories (obs, a, rewards, next_obs)\nfor i\u2190 1 to N do\n(A, B) \u2190 Algorithm.update_agent_lora(@shared, A, B, trajectories[i], r)\nend for\nend for\noutput {A, B}1\nAlgorithm 3 Inference with LoRA\nInput: Pretrained @shared, LoRA adapters {A, B}, agent observations {0}1\nOutput: Actions {a}1\n1: for i \u2190 1 to N do\n2: for layer l in actor network do\n3: Oared + ABT\n4: end for\n5: end for\nN\n6: {a}1 \u2190 Actor.select_action({0}1, {A, B}1)\nN\noutput {a}-1"}, {"title": "A.3. Further Analysis", "content": "Heterogeneous nature of agent policies Inspection of the policy activation heatmaps indicates that the initial layers of each agent's policy are comparatively similar\u2014both relative to the shared baseline and among different agents. This aligns closely with our layer ablation findings, where adapting these early layers alone provides only modest returns. These early, near-identical activations suggest that fundamental state or feature extraction is largely universal, capturing environmental signals (e.g., basic positional inputs in MAMuJoCo or unit attributes in SMAC) that all agents need in a shared manner.\nIn contrast, later layers exhibit far more divergence in the heatmaps, signaling agent-specific computations that reflect distinct strategic roles. For example, applying LoRA to mid-to-high layers substantially boosts performance, underscoring that the majority of adaptive capacity is needed where the policy makes higher-level decisions (e.g., unit targeting in SMAC or joint coordination in MAMuJoCo). Further, adapting all layers emerges as the best configuration, indicating that even though initial layers are mostly similar\u2014some specialized nuance in lower layers can still yield incremental gains when combined with deeper-layer updates, consistent with the agent-specific differences revealed in Figures 9-20.\nTo quantify these visual distinctions, we measure the Wasserstein distance between policy distributions across agents (see Figure 7 and 6). Two key observations arise:\nAgents with Similar Roles but Divergent Strategies. Units of the same \"category\" often have smaller pairwise Wasserstein distances, suggesting a shared skill set or baseline. However, even among these nominally similar agents, divergences can arise-particularly in later layers-because each agent may develop a unique strategy. This behavior echoes recent work in multi-task RL that finds role similarity does not preclude agent-specific policy refinements when higher-level decisions are at play.\nDifferent Categories, Greater Distances. When comparing agents of distinct roles, the Wasserstein distance grows larger. This supports the notion that LoRA fosters substantial heterogeneity for roles requiring fundamentally different behaviors. Our ablation results reinforce that focusing LoRA updates on deeper layers, where these role-specific divergences manifest, provides significant performance gains.\nSparsity analysis Figure 8 demonstrates the sparsity introduced by LORASA in the policy parameter space. The percentage of policy parameters above various threshold values is significantly higher for the shared policy (|$@_{shared}$|) compared to LORASA-adapted parameters (|$@$|). This suggests that LoRASA fine-tuning effectively prunes the parameter space by focusing updates on a smaller, behaviorally critical subspace.\nAt lower thresholds, both shared and LoRA-adapted parameters maintain a higher proportion of active values. However, as the threshold increases, the LoRA curve drops off more sharply than the shared policy curve. This indicates that LORA adaptations primarily influence low-magnitude adjustments, reinforcing its role as a lightweight mechanism for agent-specific fine-tuning without unnecessarily inflating parameter magnitudes.\nThis analysis ties the sparsity observation to LoRASA's broader benefits, reinforcing its practical and theoretical strengths in MARL."}]}