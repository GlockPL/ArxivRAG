{"title": "Creating Healthy Friction: Determining Stakeholder Requirements of Job Recommendation Explanations", "authors": ["Roan Schellingerhout", "Francesco Barile", "Nava Tintarev"], "abstract": "The increased use of information retrieval in recruitment, primarily through job recommender systems (JRSs), can have a large impact on job seekers, recruiters, and companies. As a result, such systems have been determined to be high-risk in recent legislature. This requires JRSs to be trustworthy and transparent, allowing stakeholders to understand why specific recommendations were made. To fulfill this requirement, the stakeholders' exact preferences and needs need to be determined. To do so, we evaluated an explainable job recommender system using a realistic, task-based, mixed-design user study (n = 30) in which stakeholders had to make decisions based on the model's explanations. This mixed-methods evaluation consisted of two objective metrics - correctness and efficiency, along with three subjective metrics - trust, transparency, and usefulness. These metrics were evaluated twice per participant, once using real explanations and once using random explanations. The study included a qualitative analysis following a think-aloud protocol while performing tasks adapted to each stakeholder group. We find that providing stakeholders with real explanations does not significantly improve decision-making speed and accuracy. Our results showed a non-significant trend for the real explanations to outperform the random ones on perceived trust, usefulness, and transparency of the system for all stakeholder types. We determine that stakeholders benefit more from interacting with explanations as decision support capable of providing healthy friction, rather than as previously-assumed persuasive tools.", "sections": [{"title": "1. Introduction", "content": "Recommender systems have found their way into many aspects of daily life. Even highly impactful decisions, such as the job that one applies to, and vice versa, the applicants a company considers, are often influenced by so-called job recommender systems. When it comes to such impactful scenarios, blindly relying on algorithms to make the correct decision can be risky and may lead to unintended consequences. As a result, there is a growing demand for explainable artificial intelligence (XAI) within the field of job recommendation, which aims to provide transparent and interpretable insights into the decision-making process of such systems [1, 2, 3].\nMost research on XAI, however, focuses on assisting developers and other users with prior knowledge of AI [4, 5], with the amount of user-centered research staying rather limited [6, 7, 8]. Considering that the overwhelming majority of users of recommender systems tend to be lay users, such in-depth, technical, and often complicated explanations offer little value. Therefore, it is crucial to design explanations in such a way that they are accessible not just to Al experts, but also to everyday users with different levels of expertise. Within job recommender systems, the everyday users are threefold: candidates - those looking for a job; recruiters - those whose job it is to match candidates to vacancies; and company representatives - those who are responsible for hiring in companies. Considering these stakeholders all perform different tasks, their explanation requirements also tend to differ, making tailor-made solutions and compromises necessary. Previous work investigated the preferences related to three explanation types (textual, bar chart, and graph-based) of each stakeholder type, providing insights useful for the design of such tailor-made solutions [9]. In this paper, we aim to determine the actual benefit those solutions offer to the different stakeholders. As a result, this paper aims to answer the following research question:\nRQ: To what degree does the combination of multiple explanation types assist job recommender system stakeholders in their daily tasks? To answer this question, we conducted a pre-registered\u00b9 mixed design user study (n = 30), wherein participants from each stakeholder type interacted with a recommendation environment in which they could combine multiple explanation types pertaining to predicted matches between candidates and vacancies, or choose to look at the different types of explanations independently. Using this environment, we explored the following sub-questions:\nSQ1: To what extent do the explanations assist the stakeholders in their decision-making process?\nSQ2: What is the impact of different explanation components (textual, bar chart, and graph-based) on the stakeholders' understanding of the explanation?\nSQ3: How can the explanations be improved to better assist the different stakeholders?\nOur results show that all three stakeholder types find the inclusion of explanations in job recommender systems beneficial. However, they still largely rely on their knowledge and intuition, often disregarding the explanations, or interpreting them differently than intended by the model. Furthermore, based on our thematic analysis, we find that candidates and recruiters prefer text-based explanations, while company representatives lean more toward visualizations. All stakeholders agreed that the explanations could be improved by showing a more direct relation to the CV/- vacancy for which recommendations were made, e.g., by reiterating exact phrases in the explanation, and showing how those were incorporated. How different features contributed to the recommendation (positively/negatively, and to what extent) should also be made explicit, to minimize"}, {"title": "2. Related Work and Hypotheses", "content": "As determined by the European Union in the AI act [10], the usage of AI in recruitment can be considered a high-risk scenario. Due to the large impact that career choices can have on individuals' lives, as well as the fact that recruitment often deals with large amounts of sensitive data, job recommender systems require a more tailored approach compared to less impactful recommender systems (e.g., music recommenders) [11]. However, current state-of-the-art approaches often fail to make use of such tailored approaches, causing aspects such as explainability to be largely ignored in current literature [1]. In this section, we provide an overview of the current works of explainability in job recommendation, both for experts and lay users. Additionally, we formulate hypotheses for our research questions based on existing literature."}, {"title": "2.1. Explainable job recommendation", "content": "While a number of previous works have incorporated explainability within their JRSs, the explanations often have limited expressive value or were not the main focus of the system [12, 13, 14]. Even when explainability has been included, authors usually fail to consider all stakeholders, tailoring the explanations to only one group (e.g., developers or users only) [6]. Furthermore, explanations are often solely evaluated anecdotally, leaving their quality up for debate [15]. One could argue that easy-to-understand explainability should be at the core of the models' design in a high-risk, multi-stakeholder domain such as recruitment. Previous research, however, often does not explicitly consider the understandability of their explanations: while their models can technically explain some part of their predictions, the explanations tend to be unintuitive and/or limited, either staying too vague [12, 13] or being hard to understand [14] for the intended users. Furthermore, baselines are rarely used for evaluating the effect of explanations, leaving their actual added benefit up for debate. It has been shown that lay users may positively evaluate explanations, even when they do not properly understand them [16]. By comparing two explanations (in our study, a random baseline vs a real explanation) in the same environment, it is possible to determine whether explanations actually add value for the users. Although explanations should always be available [10, 17], they will not necessarily always be useful [18]. Explanations have been shown to be mostly used whenever the user finds themselves in contention (e.g., whenever they disagree with the recommendations or do not find any of the items suitable) [19]. As a result, these difficult choices are likely to be alleviated by proper explanations, allowing the users to make the correct decision more quickly and often. Thus, whenever the explanation helps the user make a decision, it will also improve their view of the system as a whole. This leads to the following hypotheses for SQ1 (To what extent do the explanations assist the stakeholders in their decision-making process?):\nH1a: When provided with the real explanations, participants will be able to find matches more quickly, and make the correct decision more often, compared to when they are provided the random explanations."}, {"title": "H1b:", "content": "Participants will respond more positively to a recommendation environment that includes the real explanations than to a similar one that includes random explanations. This will improve metrics such as perceived trust, transparency, and usefulness."}, {"title": "2.2. Explanations for lay users", "content": "When dealing with users with limited Al knowledge (e.g., recruiters, job seekers, and most company representatives), having clear, straightforward explanations is crucial [20]. While explainability methods such as feature attribution maps (usually in the form of bar charts) can be sufficient for Al experts to get a better understanding of a model, this is not necessarily the case for lay users. Although such 'technical' explanations often look intuitive, they can be deceptive by giving the users a false sense of understanding [16]. In another study, specifically on job recommendations, textual explanations were found to be preferred by the majority of lay users [9], but those take additional time to read and understand, limiting their real-world usability. On the other hand, visual explanations tend to include more detail, allowing more experienced users (such as company representatives) to get more value from them. As a result, hybrid combinations of explanation interfaces can be used to make the explanations feel accessible, while still being sufficiently comprehensive. Even when using hybrid combinations, however, unique characteristics of each stakeholder type still play a role. We expect the same preferences to be indicated in our study, as we conduct our experiment in a similar (but more realistic) setting. This leads us to formulate the following hypothesis for SQ2 (What is the impact of different explanation components (textual, bar chart, and graph-based) on the stakeholders' understanding of the explanation?):\nH2: Candidates and recruiters will mainly use textual explanations to understand the recommendations, while company representatives prefer graph-based explanations. The bar chart will be considered useful as a supportive tool, but will be insufficient as a sole explanation by all stakeholders."}, {"title": "2.3. Implementing lay user explanations", "content": "When trying to explain recommendations to lay users, multiple design factors need to be considered. Not just the way in which explanations are presented, but also how they are generated, should be carefully taken into account beforehand. Previous works have shown that model-agnostic explainability methods, such as LIME [21] and SHAP [22], can fall short when trying to support non-expert users [23, 24]. Common feature attribution methods, such as LIME, tend to provide limited expressiveness (i.e., they only show the extent to which different features contributed to the prediction, but do not include any sort of interaction or higher-level relations between the features). On the contrary, model-intrinsic methods, such as attention [25] and integrated gradients [26], can be quite intuitive, even to people with less expertise [20]. Additionally, such methods lend themselves to the use of graph-based models, which can use knowledge graphs to incorporate additional expressiveness by actually including such high-level relations between features [27]. However, importance weights like attention and integrated"}, {"title": "H3a:", "content": "The explanations frame the model's reasoning in a 'positive' way, which could be perceived as confusing, lowering the usefulness and transparency of - and trust in - the model. All stakeholders will therefore benefit from additionally including negative attention weights."}, {"title": "H3b:", "content": "There are no comparative (i.e., list-wise) explanations available for the list of recommendations. An additional explanation that explains why recommendation X was ranked higher than Y and Z will therefore be desirable for all stakeholder types."}, {"title": "2.4. User studies for evaluation", "content": "While some offline evaluation metrics exist for explanations [15], it is common practice to evaluate explanations in a realistic setting using individuals from the group that is supposed to use the explanations. The subjectivity of user preference generally requires an approach that allows participants to freely express themselves, as forcing participants to choose from pre-determined answers is likely to lack depth during the formative phase of a system. Therefore, studies evaluating user experience and preference of AI-systems often use a combination of think-aloud protocols and (semi-)structured interviews. For instance, Degen et al. [30], conducted interviews with 11 energy engineers to design an explainable system for such highly expert users. Furthermore, Nelson et al. [31] used semi-structured interviews to get insights from 48 patients on the use of AI for skin cancer screening. Similarly, Zhu et al. [32] combined a think-aloud protocol (i.e., having participants say their thoughts out loud during the experiment) with post-test interviews to evaluate the user experience of an AI-based financial advisory system of 24 users with strongly diverse demographics - both in terms of personal characteristics and level of expertise."}, {"title": "2.5. Contributions", "content": "This paper aims to evaluate explanations for job recommendations in a realistic setting with a sufficient sample of stakeholders. This is done with the aim of addressing the lack of multi-stakeholder focus and evaluation that is present in previous research. Firstly, we create a novel explainable job recommender system that can generate unique explanations for different stakeholders tailored to their specific needs. Furthermore, we evaluate these explanations through a user study in which the participants have to complete a task that mimics their day-to-day activities. Specifically, we compare a combination of different objective and subjective metrics across two settings, one in which participants see genuine explanations, and one in which they see randomized explanations, in an attempt to determine the explanations' real-world impact."}, {"title": "3. Methodology", "content": "To answer our research question, we performed a pre-registered user study. We created an online environment that allows the different stakeholders to perform tasks that are similar to their day-to-day tasks, e.g., looking for suitable candidates, finding interesting vacancies, or matching candidates to vacancies. An overview of the environment can be seen in Fig. 1. A total of 10 participants from each stakeholder type were asked to use the environment, for a total sample of 30 individuals. Previous works utilizing similar approaches showed that, due to the quality and amount of data collected through qualitative user studies, such a sample size is sufficient [30, 33, 31, 32]. The participants were recruited from a wide range of backgrounds (e.g., area of expertise, age, gender identity, etc.) to mitigate possible biases from arising. Participants were gathered in two ways: through personal and professional networks, as well as in collaboration with Randstad N.V.2 (Randstad), the world's largest recruitment agency. They were asked to participate en masse over e-mail and were provided an information letter including details on what the research would entail. Our final sample consisted of 14 women, 15 men, and 1 non-binary person with an average age of 37.4 years (SD = 13.486,1 = 22, h = 69). The participants had widely varying backgrounds (e.g., IT, HR, finance, sociology, law, marketing), leading to levels of expertise w.r.t. Al ranging from no knowledge whatsoever to a Master's degree in a related field. A full overview of the descriptives per stakeholder type can be seen in Table 1. Both qualitative and quantitative data were gathered during the experiment through a previously validated (semi-structured) interview guide [9]. This allowed participants to freely speak their minds, while also allowing for statistical analyses to be performed."}, {"title": "3.1. Procedure", "content": "Participants were given the choice to conduct the experiment online or in person. All but one participant preferred to participate online; as a result, 29 out of the 30 interviews"}, {"title": "3.2. Model and data", "content": "The heterogeneous explainable graph neural network (eGNN) used to generate the recommendations was implemented using PyTorch geometric [34]. Its architecture builds upon existing model designs [35, 36, 37, 28], but is altered to allow for heterogeneous data to be considered, while also generating separate predictions and explanations for both the user (candidate) and provider (company). Our implementation consisted of a node and edge embedding layer for both textual (based on DPR [38]) and categorical data. Textual data was pre-tokenized to adhere to PyTorch Geometric's data conventions. Then, during inference, the tokens were retrieved, embedded, and readded to the graph. Categorical data was embedded using one-hot encoding. After having embedded the non-numerical data, the entire graph was fed into a general sub-graph embedding layer (using a GATv2conv [35]). The output of this layer was a generic embedding of the entire sub-graph as a whole, which was then fed into two parallel stakeholder-specific scoring layers (using Graph Transformers [36]). Both parallel stakeholder-specific layers provided a 'matching score' based on the sub-graph embedding as their outputs, which were then combined and fed into a linear layer to make a final prediction. The model was trained on a public job recommendation dataset provided by Zhaopin, China's largest online recruitment platform. This publicly available dataset contains (i) 4.5 thousand job seekers, who are represented by features such as their age, education, experience, and desires (e.g., preferred city and industry); (ii) 4.78 million job postings, which contain information on the specific job, as well as general details of the company; and (iii) 700 thousand recorded interactions between the two, which consist of four stages: no interaction, browsed (either party showed interest by looking at the other's CV/posting online), delivered (the parties were presented to each other), and satisfied (the parties were actually matched up). To use these labels as ground truth values in the user experiment, we considered the highest-rated item (candidate or job) in the list to be the 'correct' answer, with the second highest-rated item to be the second-best answer, etc. making sure there were no ties for the highest-rated position.\nWe converted this tabular dataset to a knowledge graph using a manually defined ontology. The ontology was created based on the relations between the available features in the dataset. We converted feature types to edges, and feature values to nodes. E.g., if a candidate had the value 'transport' for their 'current industry' feature, the resulting triple would be (candidate, worksInIndustry, transport). This allowed for previously non-existing connections between candidates and vacancies to arise (e.g., a path from a candidate to a vacancy based on the candidate sharing a common skill with another person who had previously fulfilled the position). This approach led to a final knowledge graph consisting of over 280 thousand nodes and nearly 1.6 million edges, with every node having 5.6 neighbors on average.\nBy then creating sub-graphs from this knowledge graph through the use of a k-random walk algorithm (k = 7, 50 walks per match) [39] between job seekers and vacancies, we created a graph ranking dataset. This was done offline and before the experiment was conducted. We trained the eGNN on this newly created dataset by performing a grid"}, {"title": "3.3. Variables", "content": "To determine the objective benefit of adding explanations to the recommendations, we compared multiple metrics that represent different aspects of usability and usefulness of the environment. These metrics were compared for the two independent variables present in our design:\nScenario: the type of explanation presented to the user (within-subject, categorical: random or real);\nStakeholder type: the participant's stakeholder type (between-subject, categorical: candidate, recruiter, or company representative).\nFor these settings, we then compare the values of the five dependent variables:\nCorrectness: whether the decision of the participant is correct, based on the ground truth values in the data (scale from 0-3, where higher values are more correct);\nEfficiency: the time in seconds it takes the participants to decide;\nTransparency: how much the explanation helps the participant understand how the model made the recommendation (scale from 1-10);\nUsefulness: the extent to which the explanation helps the participant actually make a final decision (scale from 1-10);"}, {"title": "4. Results", "content": "SQ1: To what extent do the explanations assist the stakeholders in their decision-making process?\nCorrectness When focusing on correctness, we find that participants often did not change their decision in between rounds (only 7 participants switched between rounds), meaning the difference in correctness between the random and real explanations was small, and not significant (companies: U = 42, n = 20, p = .579, candidates: U = 51, n = 20, p = 1.000, recruiters: U = 40, n = 20, p = .481). However, the trend was for the random explanations to lead to more correct answers than the genuine ones (c.f., Table 2). Participants often used their own knowledge to come to a conclusion, even if that knowledge did not align with the explanation. For example, the candidate most often selected by company representatives and recruiters had work experience similar to the vacancy. While this experience was also the most important feature for the real explanation, it was of below-average importance in the random explanation. Regardless, the participants still described it as the main argument of the model, even when viewing the random explanation. In other words, they considered the weighted arguments provided by the model based more on their own intuition, rather than the importance prescribed by the model. This decision-making process occurred with a broad range of participants, especially those who were more reluctant to trust the system.\nEfficiency In terms of efficiency, both scenarios performed similarly (companies: U = 53, n = 20, p = .853, candidates: U = 40.5, n = 20, p = .481, recruiters: U = 44, n = 20, p = .684). Regardless of the order in which the scenarios were presented, the one shown second usually led to a faster decision. This shows that, since the participants had already decided in the first round, they only had to confirm this decision in the second round (as the same list of options was presented in both rounds). We conducted a post-hoc analysis to better determine the contribution of order effects on efficiency (Section 5.1.1). Furthermore, the participants largely indicated that they had become more familiar with the environment the second time, making them more adept and efficient when comprehending the explanations (e.g., P16: \"Yes, well, it does take some time; you need to get into it for a bit. But it's slowly starting to make sense now. Indeed, it's not something you immediately grasp and say, oh yes, that's how it works\")."}, {"title": "Transparency", "content": "Regarding the perceived transparency, we notice that most participants had some difficulty in finding the difference between the two models; largely because the same list of recommendations was presented for both. Recall that participants viewed the same underlying data and only the importance of different features changed. However, upon further inspection, participants did determine the genuine explanations to be slightly better at explaining the match, mostly due to them feeling more 'sensible' and 'descriptive.' This difference in transparency was, however, not statistically significant (companies: U = 57, n = 20, p = .684, candidates: U = 52, n = 20, p = .912, recruiters: U = 59.5, n = 20, p = .481).\nParticipants who used the graph-based explanation more, found the difference in edge and node weights provided by the genuine explanation to be useful when mentally parsing the graph. Since the difference between path weights was larger in the real scenario (i.e., in the random setting, most paths had fairly equal weights - roughly $\\frac{1}{N}$ with N being the number of paths. Conversely, in the real setting, paths determined to be important by the model had noticeably more weight than the rest - e.g., > 0.9), they could more quickly determine what arguments the model had determined to be important, after which they could judge if they agreed with those arguments."}, {"title": "Usefulness", "content": "The real explanations also had a positive, but not significant, impact on participants' perceived usefulness (companies: U = 55, n = 20, p = .739, candidates: U = 56.5, n = 20,p = .631, recruiters: U = 63,n = 20, p = .353). In both the random and real scenarios, participants indicated the explanations as being helpful as a push in the right direction, but insufficiently detailed to base an actual decision on. Participants who used the textual explanation most found very little difference in the degree to which the explanations helped them make a decision. This was caused by the fact that both texts included the same arguments (because both were based on the same data), mainly differing in different aspects being described as more or less important. However, this difference was quite nuanced, presenting itself in subtle differences of phrasing (e.g., \"somewhat important\" vs. \"very important\"), causing it to not be noticed much. In the bar chart and graph, the difference in perceived usefulness was more noticeable, and participants who focused mainly on those explanation types rated the real explanation as slightly more useful."}, {"title": "Trust", "content": "Regarding the perceived trust, the recommendations were rated as slightly more trustworthy when the participants were presented with real explanations because those were perceived as better at explaining why a match was made. However, this increase was not statistically significant for any stakeholder group (companies: U = 55.5, n = 20, p = .684, candidates: U = 59, n = 20, p = .529, recruiters: U = 52.5, n = 20, p = .853). One thing that limited the trust in both scenarios was the inclusion of other candidates in the explanation (e.g., including that a candidate with similar skills to the recommended candidate has also fulfilled a specific vacancy in the past), which often led to confusion and uncertainty. This was perceived as a weak or nonsensical argument by participants, which decreased the trust they had in the system. Some participants mentioned this could be improved by rephrasing the argument to be more general, rather than referring to individuals, e.g.,"}, {"title": "SQ2:", "content": "What is the impact of different explanation components on the stakeholders' understanding of the explanation? While trying to find a match, recruiters and candidates strongly preferred the textual explanations, as they found those easiest to work with. Especially those without a 'technical' background were reluctant when using the visual explanations, as those required participants to compare and evaluate different numbers. Some participants even ignored the visualizations entirely, as they found the textual explanations to be sufficient. Company representatives indicated that the graph-based explanation was helpful, but the opinion was split, and understanding how to read the graph was often reliant on reading the text as well. The complexity of the graphs was exacerbated by overlapping edges, which added an additional layer of difficulty (as they required extra effort to be understood properly). However, once the company representatives had familiarized themselves with the environment, most indicated that they could see themselves using the graph a lot more in the future, as it did give them a quick overview of the connections between the vacancy and candidates. Some participants mentioned the graph-based explanation could be made more user-friendly by relating it more directly to the vacancy at hand, e.g., if they could customize the outgoing edges of the vacancy, limiting the graph to connections directly related to what they considered the most important aspects of the vacancy. The bar chart was not used much by most stakeholders, only being used actively by a handful of participants, as it provided too little context to substantiate a decision. Since a lot of the explanations were based on connections between different data points, simply having feature attributions did not paint a sufficiently comprehensive picture. Those who did use the bar chart, used it mostly as a supporting tool that could help them determine what parts of the textual explanation were most important."}, {"title": "SQ3:", "content": "How can the explanations be improved to better assist the different stakeholders? Most participants indicated that the explanations could be helpful when providing an overview of why things were recommended. However, because the explanations only touched on a limited number of features (i.e., at most three paths in the graph) they found them hard to trust, as there could be additional factors that they would consider important, but had not been considered by the AI. As a result, participants overwhelmingly indicated that the explanations could be used as a 'push in the right direction, but would need additional verification/exploration to be actually used. Furthermore, the explanations were indicated to be too 'generic' by multiple participants. They indicated that explanations would be more useful if they included explicit references to the CV/vacancy (e.g., \"The vacancy requires two years of experience, which the candidate possesses\"). This was especially important for hard requirements (such as minimal education or experience), which should be verified before considering the rest of the recommendation."}, {"title": "5. Discussion", "content": "We now discuss our results relating to the three sub-research questions and their accompanying hypotheses. We also"}, {"title": "5.1. Assisting in decision-making", "content": "Based on our findings we reject both hypotheses Hla (real explanations will help participants find matches more quickly, and make the correct decision more often, compared to random explanations) and H1b (participants will respond more positively to a recommendation environment that includes the real explanations). For Hla, we found no evidence that the real explanations allowed participants to make the correct decision more often compared to the random explanations, and we merely found a weak trend that the real explanations enabled participants to decide more quickly.\nThere was a large difference in efficiency between the first and second rounds, regardless of which of the two scenarios was presented first. Since the lists of items shown in both scenarios were identical (with only their content changing), the participants simply looked for any large differences that could change their minds, rather than going through the full explanations a second time (sometimes even explicitly stating that they would not need to give the explanations another look). We return to the measurement of efficiency in the post-hoc analysis in Section 5.1.1.\nAlthough we did find some trends in line with H1b, these findings were not statistically significant. We determine that this is in large part caused by the fact that most participants did not actively engage with the explanations. While most participants gravitated to the textual explanations, they often used these explanations to look for additional information on the candidate and vacancy, rather than to understand why a match was made. They would then use the information they had gathered from the explanations to manually decide for themselves, regardless of what was mentioned in the explanations. This behavior was exacerbated by the fact that the difference between scenarios for textual explanations was relatively small, e.g., alterations in phrasing, such as changing from \"very important\" to \"with limited impact.\" These small differences often went unnoticed, causing participants to rely on their own expertise rather than the model's recommendation. Therefore, when using textual explanations to substantiate a recommendation, the phrasing should be precise, strongly stressing to what degree, and in what way, different factors contributed to the recommendation; simply listing arguments seems insufficient."}, {"title": "5.1.1. Post-hoc analyses", "content": "We additionally investigated whether the lack of difference in efficiency was influenced by the fact that participants were exposed to both scenarios. To do this, we only consider the values from the first run of each participant, which was randomly selected to be real or random with a 50% chance). When doing so, we did not find any statistically significant differences in terms of efficiency for any of the stakeholder types (U = 11.0, N = 10, p = .914 for recruiters, U = 8.5, N = 10, p = .667 for candidates, and U = 15.0, N = 10, p = .690 for company representatives). As in the analysis with the full sample, the trend was toward real explanations allowing the participants to decide more quickly. This lends further support to the conclusion that the difference between the two conditions is small and unlikely to lead to large differences in efficiency.\nSurprisingly, there was a trend for participants to make incorrect decisions more often when presented with real explanations. To better understand which kinds of mistakes were made, we analyzed the justifications given by participants who incorrectly switched when presented with the real explanation, or correctly switched when presented with the random explanation.\nWe find that the random explanation was more likely to enable participants to decide using their prior knowledge:\nP23: \"Yes, because a little more emphasis is placed on his qualifications in accounting and finance, and his experience\"\nIn contrast, the real explanations were more likely to steer them into a specific decision:\nP14: \"At least, because those lines are thicker, they reflect the pattern much more of, hey, what are the core elements? And then I actually see this line compared to these two lines, then I think you see very clearly that connection is much stronger with [incorrect candidate] than it is with [correct candidate]\"\nOnce more, this indicates a lack of engagement with the explanations from the participants. As long as there was no large discrepancy between the participants' initial decision and the explanation provided by the model, they would use the explanation to justify their choice, regardless of whether or not that justification was grounded in the content of the explanation. Considering the random explanations did not have any \"strong\" arguments in the subgraph (i.e., paths with a significantly higher attention value attributed to them), participants were more often able to disregard the model's arguments and use their prior knowledge instead. Therefore, we conclude that participants experienced more (healthy) 'friction' when interacting with the real explanations (as those sometimes disagreed with them), while they could nearly always justify their personal reasoning using the random explanations (as those were not as 'decisive' with their weights)."}, {"title": "5.2. Explanation components' impact", "content": "Considering the responses to the interview questions, we can accept H2 (Candidates and recruiters will mainly use textual explanations company representatives will gain more from graph-based explanations). Although company representatives were more split on the graph-based explanations than expected, they still viewed it more positively than the other stakeholders. While some struggled with it initially, they could see its benefits after comparing it to the text. Especially those with more technical backgrounds (engineering, finance, AI, etc.) were quick to master the graphs. As expected, candidates and recruiters stuck mostly to textual explanations, indicating that those were sufficiently expressive while not being overwhelming or intimidating. For some, the texts were sufficient, and those participants did not view the visual explanations at all (three participants in total). This again shows that most participants were more inclined to use the textual explanations to create a clear image of the situation rather than actually considering it as an explanation. I.e., they often picked the facts from the textual explanation (e.g., \"this candidate has X work experience,"}, {"title": "5.3. Improving the explanations", "content": "As for H3a (... All stakeholders will benefit from additionally including negative attention weights)", "hypothesis": "all stakeholders will benefit from additionally including negative attention weights. Although not mentioned explicitly by any participant, it is clear that most struggled to make sense of some of the weights in the explanation (e.g., being confused why something they considered irrelevant had a relatively high weight), showing that it was unclear to them that this was a strong argument against the match. This was likely exacerbated by the fact that some participants (incorrectly) assumed the five matches presented in the environment were the top five recommendations. They, therefore, assumed that all arguments presented by the explanation were meant to convince them of why the item was a good match, rather than possibly explaining why it was not.\nOn the other hand, we found no evidence for H3b (An explanation that explains why recommendation X was ranked higher than Y and Z will be desirable for all stakeholders), as an additional explanation that justifies why one recommendation was ranked higher than another was not desirable for all stakeholder types within the current interface. The participants sometimes struggled to choose a single best option (candidate or vacancy), but did so anyway by manually evaluating the explanations for the individual items and analyzing the possibilities. Rather than needing clarification of the difference in ratings between two items, the participants used their own reasoning and prior knowledge (e.g., regardless of what the model said, what they would consider to be relevant work experience) to determine which item was most suitable. Considering participants were sometimes already overwhelmed by the amount of"}]}