{"title": "ELECTRA and GPT-40:\nCost-Effective Partners for Sentiment Analysis", "authors": ["James P. Beno"], "abstract": "Bidirectional transformers excel at sentiment\nanalysis, and Large Language Models (LLM)\nare effective zero-shot learners. Might they per-\nform better as a team? This paper explores col-\nlaborative approaches between ELECTRA and\nGPT-40 for three-way sentiment classification.\nWe fine-tuned (FT) four models (ELECTRA\nBase/Large, GPT-40/40-mini) using a mix of re-\nviews from Stanford Sentiment Treebank (SST)\nand DynaSent. We provided input from ELEC-\nTRA to GPT as: predicted label, probabilities,\nand retrieved examples. Sharing ELECTRA\nBase FT predictions with GPT-40-mini signifi-\ncantly improved performance over either model\nalone (82.74 macro F1 vs. 79.29 ELECTRA\nBase FT, 79.52 GPT-40-mini) and yielded the\nlowest cost/performance ratio ($0.12/F1 point).\nHowever, when GPT models were fine-tuned,\nincluding predictions decreased performance.\nGPT-40 FT-M was the top performer (86.99),\nwith GPT-40-mini FT close behind (86.77) at\nmuch less cost ($0.38 vs. $1.59/F1 point). Our\nresults show that augmenting prompts with pre-\ndictions from fine-tuned encoders is an efficient\nway to boost performance, and a fine-tuned\nGPT-40-mini is nearly as good as GPT-40 FT\nat 76% less cost. Both are affordable options\nfor projects with limited resources.", "sections": [{"title": "Introduction", "content": "Sentiment analysis\u2014the computational study of\nopinions, attitudes, and emotions in text (Medhat\net al., 2014)-has seen major advances from trans-\nformer architectures (Vaswani et al., 2017). Bidi-\nrectional encoders like BERT (Devlin et al., 2019),\nROBERTa (Liu et al., 2019), and ELECTRA (Clark\net al., 2020) excel at sentiment analysis when fine-\ntuned, and Large Language Models (LLM) like\nGPT (Radford et al., 2018) are strong zero-shot\nand few-shot learners (Kheiri and Karimi, 2023).\nRecent work has explored collaboration between\nthese models, such as using GPT to augment data of\nminority classes before fine-tuning with RoBERTa\n(Kok-Shun et al., 2023), using GPT for aspect\nextraction and RoBERTa for sentiment scoring\n(Qian et al., 2024), and escalating to LLMs when\nROBERTa classification confidence was low (An-\ndrade et al., 2024). However, leveraging predic-\ntions from fine-tuned encoders to enhance LLMS\nremains under-explored.\nThis research investigates collaborative ap-\nproaches between ELECTRA and GPT-40 models\n(OpenAI, 2024b,c) for three-way sentiment clas-\nsification (negative, neutral, positive) of reviews.\nOur research focused on the following hypotheses:\nProviding predictions from a fine-tuned ELECTRA\nas context to a GPT model will improve classi-\nfication performance (H1). The improvement in\nperformance will be less for a fine-tuned GPT (H2).\nThe format of predictions in the prompt will affect\nperformance (H3). Including similar examples in\nthe prompt will improve performance (H4).\nThese hypotheses build on ELECTRA's strength\nin capturing nuanced sentiment patterns when fine-\ntuned (Clark et al., 2020; Potts et al., 2021; B\net al., 2023), and GPT's versatility through in-\ncontext learning (Radford et al., 2019; Liu et al.,\n2019; Koco\u0144 et al., 2023; OpenAI, 2024a)\u2014they\ncan perform well across diverse tasks when given\nthe appropriate context through prompting (Liu\net al., 2023; Khattab et al., 2024). Although they\nmay struggle with emotion and nuance (Koco\u0144\net al., 2023), retrieved examples can improve per-\nformance (Zhang et al., 2023).\nTo test these hypotheses, we established four\nbaselines and conducted 23 experiments across\nthree sentiment classification datasets: Stanford\nSentiment Treebank (SST), and DynaSent Rounds\n1 and 2. We used ELECTRA Base/Large and GPT-\n40/40-mini, each of which were fine-tuned (FT) on\na merge of SST and DynaSent reviews.\nWe investigated the effects of different prompt\naugmentation scenarios using DSPy (Khattab et al.,\n2024): sharing the predicted class text label, the"}, {"title": "Prior Literature", "content": "2.1 MLMs and ELECTRA\nMasked Language Models (MLM) like BERT\n(Bidirectional Encoder Representations from Trans-\nformers) (Devlin et al., 2019) employed bidirec-\ntional encoding to obtain holistic representations\nof text. ROBERTa (Robustly Optimized BERT Pre-\ntraining Approach) (Liu et al., 2019) optimized the\npre-training approach, but both models were ineffi-\ncient because learning only occurred in about 15%\nof the tokens that were masked.\nThis led to the development of ELECTRA (Ef-\nficiently Learning an Encoder that Classifies To-\nken Replacements Accurately) (Clark et al., 2020).\nELECTRA was pre-trained with two models using\nreplaced token detection. As a result, it learned\nfrom all tokens and had comparable or better per-\nformance in a variety of tasks with less compute.\nELECTRA was found to be a top performer in\nsentiment classification on datasets such as SST\n(Clark et al., 2020), DynaSent (Potts et al., 2021),\nand IMDB movie reviews (B et al., 2023). It's\nfor these reasons that we chose to use ELECTRA\nin our research, in addition to observing a perfor-\nmance gain relative to ROBERTa in early trials.\n2.2 GPT Models\nBidrectional transformers seemed to have an edge\nover early autoregressive models like GPT (Rad-\nford et al., 2018) for sentiment analysis. But that\nedge is being whittled away by the successors of\nGPT pre-trained at a massive scale: GPT-3, GPT-\n3.5, GPT-4, and GPT-40 (OpenAI, 2024a,b,c).\nFor sentiment analysis of social media posts,\nKheiri and Karimi (2023) found that GPT mod-\nels significantly outperformed a number of prior\nmodels on the SemEval 2017 dataset. In contrast,\nKoco\u0144 et al. (2023) found that, although ChatGPT\nis versatile and competent across a wide range of\ntasks, it did not perform as well as RoBERTa\u2014\nespecially for pragmatic tasks involving detection\nof emotional and contextual nuances. They pro-\npose that fine-tuning ChatGPT may be necessary,\nwhich we explore in this research.\n2.3 Collaborative Approaches\nRecent work has revealed several promising ap-\nproaches for collaboration between these models.\nKok-Shun et al. (2023) explored a unique frame-"}, {"title": "Data", "content": "Models were trained and evaluated on a merge of\nmovie reviews from the Stanford Sentiment Tree-\nbank (SST) (Socher et al., 2013) and business re-\nviews from DynaSent Rounds 1 and 2 (Potts et al.,\n2021). See Table 1 for examples. By default, SST\nis a five-way classification (positive, somewhat pos-\nitive, neutral, somewhat negative, negative). The\npositive and negative classes were combined to\nproduce SST-3 (positive, neutral, negative).\nThe SST-3, DynaSent R1, and DynaSent R2\ndatasets were randomly mixed to form a Merged\ndataset with 102,097 Train examples, 5,421 Valida-\ntion examples, and 6,530 Test examples. See Table\n2 for the distribution of labels, and Table 3 for a\nbreakdown of sources.\nIt's worth noting that the source datasets all have\nclass imbalances. Merging the data helps mitigate\nthis imbalance, but there is still a majority of neu-\ntral examples in the training split. Another poten-\ntial issue is that the models will learn the dominant\ndataset, which is DynaSent R1. As a test, the mi-\nnority classes were over-sampled to create a new\nbalanced dataset. When this was evaluated, the\nperformance did not improve."}, {"title": "Models", "content": "Four models were fine-tuned and evaluated in this\nresearch, both individually and in collaboration\nwith each other: ELECTRA Base and Large, and\nGPT-40 and 40-mini. See Table 4 for details.\nELECTRA (Clark et al., 2020) was chosen as the\nbidirectional transformer because its pre-training\narchitecture gives it an advantage over MLMs. It\nalso outperformed ROBERTa in early trials. We\nevaluated both the Base (110M parameters) and\nLarge (335M parameters) variants.\nTo function as a classifier, ELECTRA's output\nis sent through a mean pooling layer. A classifier\nhead is appended with 2 hidden layers of dimen-\nsion 1024, and a final output dimension of 3. Swish\nGLU (Shazeer, 2020) was used as the hidden ac-\ntivation function, and dropout layers were added\nwith a rate of 0.3. See Appendix A for more details\non the model architecture and hyper-parameters.\nFor comparison and collaboration, two GPT\nmodels were used via OpenAI's API: GPT-40 (Ope-\nnAI, 2024b) and GPT-40-mini (OpenAI, 2024c).\nAlthough the full specifications are not public, they\nare state-of-the-art autoregressive language mod-\nels with strong zero-shot capabilities. GPT-40 is\ndescribed as a \u201chigh-intelligence flagship model\nfor complex, multi-step tasks.\u201d GPT-4o-mini is\ndescribed as an \"affordable and intelligent small\nmodel for fast, lightweight tasks.\u201d"}, {"title": "Methods", "content": "Our research progressed through the following\nstages. Code and datasets are available at: https:\n//github.com/jbeno/sentiment.\nELECTRA Baseline & Fine-tuning\nWe first developed a training pipeline to sup-\nport interactivity and distributed training across\nmultiple GPUs. Training progress was tracked\nthrough Weights and Biases so we could monitor\ntrain/validation metrics (loss, macro F1, accuracy)\nacross epochs. The final models were selected"}, {"title": "GPT Data Preparation & Fine-tuning", "content": "To use OpenAI's fine-tuning API, we converted the\nMerged training data to JSONL format that defined\nthe System, User, and Assistant roles. We noticed\nthat if the context at inference time varied even\nslightly from the fine-tuning context, performance\nwould suffer. So we created three templates to\nenable better comparisons between fine-tuned and\ndefault models using the same DSPy signatures\n(see Appendix B):\n\u2022 Minimal (FT-M): No prompt other than Sys-\ntem role.\n\u2022 Prompt (FT): Default fine-tuning. User role\nincluded full DSPy prompt.\n\u2022 Prompt with Label (FT-L: User role in-\ncluded DSPy prompt with ELECTRA pre-\ndicted label.\nWe included the ELECTRA predictions in the\nthird template to align the fine-tuning context with\nthe inference time context, but also to provide an\nopportunity for the GPT models to learn from the\nELECTRA predictions. In total there were 6 fine-\ntuning jobs (see Table 5)."}, {"title": "DSPy Signatures & Modules", "content": "Using DSPy, we explored a variety of approaches\nto integrating ELECTRA's output into GPT's"}, {"title": "Results", "content": "Our experiments revealed significant differences\nin performance across baseline, fine-tuning, and\ncollaborative scenarios. See Table 6 for the results.\nBaselines. Regarding baselines, both GPT mod-\nels outperformed the ELECTRA classifiers, with\nGPT-40 achieving 80.14 macro F1 and GPT-40-\nmini scoring 79.52, compared to ELECTRA Base\n(69.65) and Large (67.88). This demonstrates the\nstrong zero-shot capabilities of the GPT models.\nFine-tuning. Fine-tuning improved perfor-\nmance across all models. ELECTRA Base's macro\nF1 increased from 69.65 to 79.29, while ELECTRA\nLarge showed even greater gains, improving from\n67.88 to 82.36. This improvement is the result of\nfine-tuning all layers\u2014the baselines had the same\nclassifier head. The fine-tuned GPT models had the\nhighest scores (see Figure 2), with GPT-40-mini\nFT rising from 79.52 to 86.77, and GPT-40 FT-M\nachieving 86.99 with the minimal template.\nSharing Predictions. The effect of adding\nELECTRA predictions to GPT prompts depended\non if the GPT model was fine-tuned (see Figure\n1). Sharing ELECTRA Base predictions with GPT-\n40-mini (not fine-tuned) significantly improved the\nmacro F1 from 79.52 to 82.74 (p < 0.0001, McNe-\nmar's test), a +3.22 gain. There was an even greater\ngain of +3.97 points when ELECTRA Large predic-\ntions were shared (from 79.52 to 83.49, p < 0.0001).\nSimilarly, including ELECTRA Large predictions\nwith GPT-40 improved the macro F1 from 80.14 to\n81.57 (p = 0.0106), a smaller +1.43 gain.\nHowever, sharing ELECTRA predictions with\nfine-tuned GPT models actually decreased perfor-\nmance. GPT-40-mini's macro F1 dropped from\n86.77 to 81.42 with ELECTRA Base predictions,\nand to 82.02 when fine-tuned with the predictions"}, {"title": "Analysis", "content": "H1. Sharing predictions would boost perfor-\nmance. The significant improvement in GPT-40-\nmini's performance when augmented with ELEC-\nTRA Base FT or Large FT predictions strongly\nsupports H1. We also saw a similar boost for GPT-\n40 with ELECTRA Large FT predictions.\nHowever, following ELECTRA's predictions\nhad mixed results. When GPT-40-mini changed its\ndecision and followed ELECTRA Base FT, it was\ncorrect 548 times and wrong 412 times (+136 net\nimprovement, 57.08% success rate). When GPT-\n40 changed its decision and followed ELECTRA\nLarge FT, it was correct 521 times and wrong 481"}, {"title": "Conclusion", "content": "This research investigated collaborative approaches\nto sentiment classification between bidirectional\ntransformers and LLMs. Our results show that\naugmenting prompts with predictions from a fine-\ntuned ELECTRA can significantly improve perfor-\nmance when the GPT model is not fine-tuned-up\nto +3.97 points of gain in the macro F1 score. In-\ncluding probabilities or similar examples enhanced\nperformance for GPT-40, especially on challeng-\ning datasets. However, this collaborative benefit\ndisappeared when the GPT models were fine-tuned.\nOur findings offer several cost-effective paths\nfor sentiment analysis projects. For organizations\nthat can fine-tune via API, GPT-40-mini FT offers\nnearly equivalent performance to GPT-40 FT-M\n(86.77 vs 86.99 macro F1) at 76% lower cost ($0.38\nvs $1.59/F1 point). For those with data privacy\nconcerns or resource constraints, GPT-40-mini with\nELECTRA Base FT had the best cost/performance\nratio ($0.12/F1 point). Projects that need to stay\ncompletely local can fine-tune ELECTRA Large,\nwhich outperformed both base GPT models.\nFuture work could explore optimization of\ninference-time prompts through DSPy, and alter-\nnate System role instructions during fine-tuning.\nIn addition, this collaborative approach could be\nextended to different datasets/domains, classifica-\ntion tasks, and model pairings. There may also be\npotential for including multiple predictions from\nan ensemble of models. A new collaborative sce-\nnario would be fine-tuning GPTs on the ELECTRA\noutput representations."}, {"title": "Limitations", "content": "The cost/performance evaluation only considered\nthe fine-tuning costs to achieve the reported macro\nF1 on the test set. In practice, there would be\nongoing costs for inference time API calls.\nResource and time constraints prevented us from\nexploring every possible collaborative scenario.\nOnce we saw ELECTRA Large FT performed bet-\nter than ELECTRA Base FT, we only evaluated the\noutput from Large in the different prompt contexts\nfor both GPT-40 and GPT-40-mini."}]}