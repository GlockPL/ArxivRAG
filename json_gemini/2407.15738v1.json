{"title": "Parallel Split Learning with Global Sampling", "authors": ["Mohammad Kohankhaki", "Ahmad Ayad", "Mahdi Barhoush", "Anke Schmeink"], "abstract": "The expansion of IoT devices and the demands of Deep Learning have highlighted significant challenges in Distributed Deep Learning (DDL) systems. Parallel Split Learning (PSL) has emerged as a promising derivative of Split Learning that is well suited for distributed learning on resource-constrained devices. However, PSL faces several obstacles, such as large effective batch sizes, non-IID data distributions, and the straggler effect. We view these issues as a sampling dilemma and propose to address them by orchestrating the mini-batch sampling process on the server side. We introduce the Uniform Global Sampling (UGS) method to decouple the effective batch size from the number of clients and reduce mini-batch deviation in non-IID settings. To address the straggler effect, we introduce the Latent Dirichlet Sampling (LDS) method, which generalizes UGS to balance the trade-off between batch deviation and training time. Our simulations reveal that our proposed methods enhance model accuracy by up to 34.1% in non-IID settings and reduce the training time in the presence of stragglers by up to 62%. In particular, LDS effectively mitigates the straggler effect without compromising model accuracy or adding significant computational overhead compared to UGS. Our results demonstrate the potential of our methods as a promising solution for DDL in real applications.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT advances in Deep Learning (DL), driven by growing architecture sizes, have significantly impacted areas like audio signal processing, natural language processing, and smart healthcare [1]\u2013[5]. This progress relies on large data volumes and substantial computing resources [6]\u2013[8]. The expansion of Internet-of-Things (IoT) devices, such as smartphones and wearables, provides extensive data and computing capabilities, yet privacy laws often restrict access to this data [6]. Distributed Deep Learning (DDL) frameworks have emerged to address these challenges by enabling collaborative model training across multiple devices while preserving data privacy.\nFederated Learning (FL) [9] is a prominent framework in DDL. In Federated Averaging (FedAvg), clients train model copies locally and periodically aggregate these models by averaging their parameters, facilitating global model convergence [9]. However, the high resource demands of FL restrict its implementation in edge computing and IoT environments, primarily due to substantial computational and communication burdens. Deploying FL on edge devices is further complicated by the straggler effect [10], which causes significant delays.\nAdditionally, FL struggles with datasets that are small or non-independent and identically distributed (non-IID). Strategies such as client selection [11] and model quantization [12] help mitigate these issues, but do not resolve them completely.\nSplit Learning (SL) [13], [14] addresses some limitations of FL by partitioning neural network training between client devices and a central server. Clients conduct initial Forward Propagation (FP) on local data and send intermediate results to the server, which completes the FP, initiates the Backward Propagation (BP) and returns gradients to the clients to complete the BP. SL enhances data privacy, reduces computational demands on the client-side, and increases the convergence speed compared to FL, making it well-suited for edge and IoT applications [13], [15], [16]. However, it incurs high training and communication latency due to its sequential nature [15]. While SL performs better than FL with non-IID data [16], [17], it suffers from catastrophic forgetting as the number of clients increases [18].\nTo address training latency in SL, advanced derivatives of SL like Split Federated Learning (SFL) [19]\u2013[21] and Parallel Split Learning (PSL) [22]\u2013[26] have been proposed. SFL combines the techniques of SL and FL to facilitate parallel model training across multiple devices. This approach leverages the advantages of both FL and SL. However, it encounters increased communication overhead compared to SL because it requires both the transmission of intermediate model results from clients to the server and the aggregation of updated models back to clients. Furthermore, SFL necessitates that the server maintains individual model copies for each client, which complicates scalability. Although variants such as SFLv2, SFLv3 [20], and SFLG [21] aim to mitigate these drawbacks, practical challenges persist.\nPSL enhances parallelism without requiring multiple server model instances or client model aggregation. However, PSL still faces issues with large effective batch sizes, non-IID data, and the straggler effect:\n1) Large Effective Batch Size Problem [27]: Also known as the Server-Client Update Imbalance Problem [25], where the effective batch size scales with the number of clients.\n2) Non-Independent and Identically Distributed Data: Devices contribute data of varying distributions and sizes, causing batch distributions to deviate from the overall data distribution and leading to poor generalization.\n3) Straggler Effect: Due to the need for synchronization, slower devices delay the training process, as the processing time of a single batch is determined by the slowest client, impacting throughput and total training time [10]."}, {"title": "II. RELATED WORK", "content": "PSL enables the parallel execution of the FP and BP on the clients and enhances scalability by avoiding the need for the server to maintain separate model copies for each client. Nevertheless, PSL faces the client decoupling problem [27] (also known as client detachment problem [25]), where the the information flow between clients is decoupled during the BP due to the independent execution of the BP on each client. Jeon and Kim [22] use gradient averaging over all clients to address the client decoupling problem. They chose the local batch sizes fixed and proportional to the dataset size of each client. Lyu et al. [31] propose averaging the clients' local loss function outputs instead of their local gradients. This approach aligns with that of [22] due to the linearity of the gradient operator, but enhances privacy as clients can compute the local loss functions without sharing labels. [22] and [31] show consistent convergence and improved accuracy in both IID and\nnon-IID settings, but do not address the large effective batch size problem.\nPal et al. [27] propose SplitAvg to address the client decoupling problem. This method generalizes the approach in [22] by allowing averaging only a subset of the local gradients. Opposed to [22] and [31], the authors observe that using all clients for gradient averaging, the model may not converge. To address the large effective batch size problem, the authors propose scaling the learning rate on the server side while keeping the local batch size fixed and identical for all clients. Following the line of work from [26], Oh et al. [25] propose using smashed data augmentation, local loss function design and FedAvg for the client models to overcome the large effective batch size problem and client model decoupling.\n[27] and [25] provide promising solutions for the large effective batch size problem, but assume that the effective batch size scales with the number of clients. This assumption conflicts with the diminishing returns of larger batch sizes for gradient estimation [32] and the benefits of smaller batch sizes [33]. Furthermore, the empirical results are reported only for IID settings. They do not address the non-IID data problem, which is a significant challenge in PSL. For general SL, Cai and Wei [34] propose a method to handle non-IID data with a distillation loss function. The bargaining game approach for personalized SL [35] balances energy consumption and privacy in non-IID settings. However, these methods can not be directly applied to PSL.\nOther efforts to improve PSL focus on reducing training and communication latency [23], [24], [36]. Methods include last-layer gradient aggregation, resource optimization, client clustering and cut-layer selection. However, these methods do not address the straggler effect, which remains a significant challenge in PSL. A notable work in this direction is the BOA algorithm [37], which dynamically adjusts the batch size of slow clients in distributed learning systems to reduce the straggler effect.\nDifferent from other works, we approach the large effective batch size problem, the Non-IID data problem and the straggler effect as a sampling dilemma. By orchestrating the batch sampling process on the server side we allow variable local batch sizes and decouple the number of clients from the effective batch size. This enables global sampling methods to account for Non-IID data by reducing the deviation between the global batches and the overall data distribution and to mitigate the straggler effect by controlling the concentration of stragglers in global batches."}, {"title": "III. PRELIMINARIES", "content": "We consider the PSL setting from [22] where a server and K\u2208 N clients train a neural network model with parameter vector w collaboratively using the PSL framework. The set of clients is represented by K = {1,2,..., K}. We focus on supervised classification tasks, where the dataset \n\\(D_k\\) of each client \\(k \\in K\\) consists of input-label pairs \n\\(D_k = \\{(x_{k,i}, y_{k,i})\\}_{i=1}^{D_k}\\), where \\(D_k \\in \\mathbb{N}\\) is the dataset size, \n\\(x_{k,i}\\) is the input data, and \\(y_{k,i} \\in \\mathcal{M}\\) is the ground truth class label of \\(x_{k,i}\\). Here, \\(\\mathcal{M} = \\{1, 2, ..., M\\}\\) represents all classes and \\(M \\in \\mathbb{N}\\) is the number of classes. The overall dataset \n\\(D_0 = \\bigcup_{k=1}^K D_k\\) is the union of all clients' datasets, with a total size \\(D_0 = \\sum_{k=1}^K D_k\\). We refer to the distribution of the datapoints in \\(D_0\\) as the empirical distribution. We define the class counting function \\(c : \\mathbb{N} \\times \\mathbb{N} \\rightarrow \\mathbb{N}\\) with \\(N \\in \\mathbb{N}\\) as\n\\[c(y, m) = \\sum_{i=1}^{D_k} \\delta(y_i, m)\\]\nwhere \\(\\delta(i, j) = \\begin{cases}\n1 & \\text{if } i = j \\\\\n0 & \\text{otherwise}\n\\end{cases}\nThe class distribution of a client k with class la-\nbel vector \n\\(y_k \\in \\mathcal{M}^{D_k}\\) is denoted as \\(\\beta_k \\in \\mathbb{R}^M\\)\nwith \\(\\beta_k = (c(y_k, 1), c(y_k, 2), . . ., c(y_k, M))^T\\). The over-\nall class distribution of \\(D_0\\) with class label vector \\(y_0 \\in \\mathcal{M}^{D_0}\\)\nis denoted as \\(\\beta_0 \\in \\mathbb{R}^M\\) according to the same definition.\nEvery epoch consists of \\(T \\in \\mathbb{N}\\) optimization steps. The set\nof optimization steps is denoted as \\(\\mathcal{T} = \\{1, 2, . . ., T\\}\\). We use\nthe subscript k for clients and the superscript t for optimization\nsteps. If the distinction is unnecessary, we omit the indices for\nbrevity. Each client has a local batch size \\(B_k \\in \\mathbb{N}\\). The global\nbatch size \\(B = \\sum_{k=1}^K B_k\\) is the total number of data points\nthat arrive at the server in each optimization step. Accordingly,\nwe differentiate between local batches \\(B_k^{(t)} \\subseteq D_k\\) and the\nglobal batch \n\\(B^{(t)} = \\bigcup_{k=1}^K B_k^{(t)}\\) in each optimization step.\nPSL mirrors SL by splitting the model parameters into a client-side lower segment \\(\\mathbf{w}_c\\) and a server-side upper segment \\(\\mathbf{w}_s\\) with \\(\\mathbf{w} = (\\mathbf{w}_s, \\mathbf{w}_c)\\). The first layer on the server side is referred to as the cut layer. Before training, the server sends \\(\\mathbf{w}_c\\) to the clients, each storing a copy \\(\\mathbf{w}_{c,k}\\) with identical initial values. The function \\(f(x; \\mathbf{w})\\) maps x to a predicted value according to the given model parameters w. Hence, the global model is given as \\(f(x;\\mathbf{w}) = f(f(x;\\mathbf{w}_c); \\mathbf{w}_s)\\). When the argument of f is a dataset or a subset of it, e.g. a batch B with batch size B, it operates element-wise on the input data and is defined as:\n\\[f(B; \\mathbf{w}) = \\{(f(x_i; \\mathbf{w}), y_i)\\}_{i=1}^{B}\\]\nThe goal is to find the optimal model parameters \\(\\mathbf{w}^* = (\\mathbf{w}_s^*, \\mathbf{w}_c^*)\\) that minimize the global loss function \\(L(D_0; \\mathbf{w})\\):\n\\[\\mathbf{w}^* = \\arg \\min_{\\mathbf{w}} L(D_0; \\mathbf{w})\\]\n\\[\\mathbf{w}^* = \\arg \\min_{\\mathbf{w}} \\frac{1}{D_0} \\sum_{k=1}^K \\sum_{i=1}^{D_k} L((x_{k,i}, y_{k,i}); \\mathbf{w})\\]\n\\[\\mathbf{w}^* = \\arg \\min_{\\mathbf{w}} \\frac{1}{D_0} \\sum_{k=1}^K \\sum_{i=1}^{D_k} L((f(x_{k,i}; \\mathbf{w}_c), y_{k,i}); \\mathbf{w}_s)\\]\nThe mini-batch Stochastic Gradient Descent (SGD) algo- rithm or one of its variants are typically used to optimize the model parameters, alternating between FP and BP. Each optimization step \\(t \\in \\mathcal{T}\\) involves the following substeps."}, {"title": "IV. PARALLEL SPLIT LEARNING WITH GLOBAL SAMPLING", "content": "A good sampling method should ensure that the expected distribution of the global batches aligns with the empirical distribution. If the global batches frequently deviate from the empirical distribution, the trained model may become biased and generalize poorly. We measure the quality of a sampling method by the deviation of the generated global batches from the empirical distribution, referred to as batch-deviation. By analyzing how the choice of local batch sizes affects the batch deviation, we determine the UGS method that is bounded at least as tight as to central uniform sampling in terms of batch deviation and decouples the effective batch size from the number of clients. All proofs are provided in Section B.\nA. Deviation Analysis\nAssume we are given the complete dataset \\(D_0\\) and train our model using central learning. In this scenario, the model is\ntypically trained using batches B of size B that are sampled uniformly at random. Although uniform sampling does not guarantee minimal batch deviation, it is common due to its simplicity and efficiency. To simplify our analysis, we consider the overall class distribution \\(B_0\\) instead of the empirical distribution, assuming that the class distribution acts as a proxy for the empirical distribution. We define the batch deviation as the \\(L_1\\) distance between the class distribution of a batch B with class label vector y and \\(B_0\\):\n\\[d(B, \\beta_0) = \\sum_{m=1}^M | \\frac{\\sum c(y, m)}{B} - \\beta_{0,m} |\\]\nLet \\(Y = (Y_1, Y_2, . . ., Y_M)^T\\) be a random vector represent- ing the class counts in a batch B sampled from \\(D_0\\) uniformly at random with \\(Y \\sim Multinomial(B, \\beta)\\). Using the Chebychev inequality, we can derive an upper bound on the probability of B deviating from \\(\\beta_0\\) by at least \\(\\epsilon\\):\nLemma 1. For all \\(\\epsilon > 0\\) and \\(m \\in \\mathcal{M}\\) it holds that\n\\[P(\\frac{Y_m}{B} - \\beta_{0,m} | \\geq \\epsilon) \\leq \\frac{Var(Y_m)}{B^2 \\epsilon^2}\\]\nOur goal is to determine a sampling method for PSL with a deviation bound at least as tight as the one from central uniform sampling. In the regular PSL framework, a local batch size \\(B_k\\) is assigned to each client k and does not change during training. The class distribution of each local batch B' is modelled as a multinomial distribution with parameters \\(B_k\\) and \\(\\beta_k\\). Let \\(Y' = (Y_1, Y_2, ...,Y_M)^T\\) be a random vector representing the class counts in the corresponding global batch B'. Since the \\(B_k\\) are sampled independently, Y' is distributed according to the sum of the multinomials:\n\\[Y' \\sim \\sum_{k=1}^K Multinomial (B_k, \\beta_k)\\]\nUsing the Markov inequality, we can derive an upper bound on the probability of B' deviating from \\(\\beta_0\\) by at least \\(\\epsilon\\) and relate it to B, i.e., batches generated by central uniform sampling:\nLemma 2. For all \\(\\epsilon > 0\\) and \\(m \\in \\mathcal{M}\\) it holds that\n\\[P(\\frac{Y'_m}{B} - \\beta_{0,m} | \\geq \\epsilon) \\leq \\frac{Var(Y'_m) + (E[Y'_m] - E[Y_m])^2}{B^2 \\epsilon^2}\\]\nThe upper bound loosens when the bias term \\((E[Y'_m] - E[Y_m])^2\\) increases. Since \\(E[Y'_m] = \\sum_{k=1}^K B_k\\beta_{k,m}\\)\nand \\(E[Y_m] = B\\beta_{0,m}\\), this implies that even small differences\nin the \\(B_k\\) or \\(\\beta_{k,m}\\) can lead to a significant deviation in the\nglobal batches.\nTheorem 1. If \\(B_k = \\frac{B \\cdot D_k}{D_0}\\) or \\(\\beta_{k,m} = \\beta_{0,m}\\) for all \\(k \\in K\\) and \\(m \\in \\mathcal{M}\\) then it holds that\n\\[P(\\frac{Y'_m}{B} - \\beta_{0,m} | \\geq \\epsilon) \\leq \\frac{Var(Y_m)}{B^2 \\epsilon^2}\\]\nfor all \\(m \\in \\mathcal{M}\\).\nTheorem 1 shows that choosing the \\(B_k\\) proportionally to the local dataset sizes ensures that the upper bound is at least as tight as the one from central uniform sampling in Lemma 1. Only if the client datasets are IID \\(\\beta_{k,m} = \\beta_{0,m}\\), any viable choice of local batch sizes \\(B_k\\) ensures the tightness of the deviation bound. Therefore, choosing the same local batch size \\(B_k = B'\\) for all clients k as in [27] can lead to an increased batch deviation when the client datasets are non-IID. On the other hand, if \\(\\frac{B \\cdot D_k}{D_0} \\notin \\mathbb{N}\\), the local batch sizes must be converted to integers. This can increase the deviation of the global batches in a non-IID setting, since the premise of Theorem 1 may not hold anymore throughout the training process.\nB. Uniform Global Sampling (UGS)\nWe propose the UGS method, which ensures that Theorem 1 holds independent of local dataset sizes or local data distributions. The complete UGS method is outlined in Algorithm 1. In UGS, the local batch sizes are variable and determined by the server for each training step separately. It is not required to fix the local batch sizes right from the start. The server only requires a global batch size B to be specified.\nUGS is invoked before each epoch of PSL. Each client receives a local batch size vector \\((B_1^{(t)}, B_2^{(t)}, ..., B_T^{(t)})^T\\) from the server such that \\(\\sum_{k=1}^K B_k^{(t)} = B\\) for all \\(t \\in \\mathcal{T}\\). The \\(B_k^{(t)}\\) specify the local batch size of client k at training step t. If \\(B_k^{(t)} = 0\\), the client k does not contribute to the global batch at training step t. UGS uses the following generative process to sample the global batches \\(B^{(t)}\\) in each training step t:\n1) Choose \\(\\pi = (\\frac{D_1}{D_0}, \\frac{D_2}{D_0},..., \\frac{D_K}{D_0})\\)\n2) For each \\(i \\in \\{1, 2, . . ., B\\}\\):\na) Choose a client \\(z_i^{(t)} \\sim Categorical(\\pi)\\)\nb) Choose a class \\(y_i^{(t)} \\sim Categorical(\\beta_{z_i^{(t)}})\\)\nThe key idea is to first sample a client \\(z_i^{(t)} \\in K\\) according to a categorical distribution with parameters \\(\\pi\\) and then sample a class \\(y_i^{(t)} \\in \\mathcal{M}\\) from the chosen client. Since \\(\\sum_{k=1}^K \\pi_k = 1\\), UGS corresponds to sampling from a mixture of categorical distributions with the mixing probabilities \\(\\pi\\). In our context, \\(\\pi\\) can be interpreted as the client selection probabilities. Note that the categorical distribution is used iteratively instead of sampling in one step from the multinomial distribution. This is because the clients actually sample uniformly without replacement. If a client k has no data left, the server needs to update the client selection probabilities \\(\\pi\\) by setting \\(\\pi_k = 0\\) and renormalizing the remaining probabilities before continuing the sampling process.\n\\(z^{(t)} = (z_1^{(t)}, z_2^{(t)},...,z_B^{(t)})\\) are latent variables that represent the clients from which the \\(y_i^{(t)}\\) are sampled. This allows the sampling process to be split between the server and the clients, where the server executes 2a) and the clients execute\n2b). Since steps 2a) and 2b) are independent, we can postpone step 2b) and pre-compute the \\(B_k^{(t)}\\) for all \\(k \\in K\\) and \\(t \\in \\mathcal{T}\\). Accordingly, we count the occurrences of each client k in \\(z^{(t)}\\) with \\(B_k^{(t)} = c(z^{(t)}, k)\\) right after step 2a). The server then sends the \\(B_k^{(t)}\\) to the clients.\nOnce training starts, the clients can execute the step 2b) on demand, i.e., sampling local batches of size \\(B_k^{(t)}\\). This equals substituting \\(B_k\\) with \\(B_k^{(t)}\\) in the first step of PSL in Section III. The rest of the training process remains unchanged. Consequently, the effective batch size B becomes a hyperparameter that is no longer dependent on the number of clients or their respective dataset sizes, addressing the large effective batch size problem.\nCorollary 1. Let \\(\\pi = (\\frac{D_1}{D_0}, \\frac{D_2}{D_0},..., (\\frac{D_K}{D_0})^T\\). If\n\\[Y'_m \\sim B\\sum_{k=1}^K \\pi_k Categorical(\\beta_k)\\]\nthen\n\\[P(\\frac{Y'_m}{B} - \\beta_{0,m} | \\geq \\epsilon) \\leq \\frac{Var(Y_m)}{B^2 \\epsilon^2}\\]\nfor all \\(m \\in \\mathcal{M}\\).\nCorollary 1 shows that Theorem 1 holds for the global batches generated by this process. Therefor, UGS ensures that the batch deviation bound is as tight as the one from central uniform sampling even in the presence of non-IID data distributions.\nC. Straggler Mitigation\nWhile UGS addresses the issue of large effective batch sizes and non-IID data distributions, it does not consider the presence of stragglers. Intuitively, by increasing the client selection probability \\(\\pi_i\\) of a straggler client k', we can reduce the number of global batches to which the straggler client contributes. This reduces the total training time, as the straggler clients' datasets deplete faster and they appear less frequently in global batches.\nHowever, adjusting the \\(\\pi\\) can increase the batch deviation, as Corollary 1 would no longer hold. The choice of \\(\\pi\\) is thus crucial to balance the trade-off between the batch deviation and the concentration of stragglers in the global batches. To address this issue, we treat \\(\\pi\\) as a parameter to be optimized with respect to the desired trade-off. Inspired by the Latent Dirichlet Allocation model [28], we introduce a Dirichlet prior distribution over \\(\\pi\\) with concentration parameter \\(\\alpha \\in \\mathbb{R}^K\\) and \\(\\alpha_k > 0\\). As a conjugate to the categorical distribution, the Dirichlet distribution is a suitable choice for the prior distribution. The choice of \\(\\alpha\\) is crucial to balance the trade-off between batch deviation and total training time. For now we assume that \\(\\alpha\\) is given and will discuss its initialization in the next subsection. The prior distribution over \\(\\pi\\) is defined as:\n\\[P(\\pi | \\alpha) = \\frac{1}{B(\\alpha)} \\prod_{k=1}^K \\pi_k^{\\alpha_k - 1}\\]\nwhere B(\\(\\alpha\\)) is the Beta function. Furthermore we have:\n\\[p(z | \\pi) = \\pi_\\alpha \\text{ and } p(y | z, \\beta) = \\beta_{\\alpha \\mu}\\]\nThe goal is to estimate the mixture proportions \\(\\pi^*\\) that maximize the posterior distribution \\(P(\\pi | y, \\alpha, \\beta)\\), so that the global batches that are generated using \\(\\pi^*\\) are less likely to deviate from y while respecting the prior beliefs given by \\(\\alpha\\). Here \\(y \\in \\mathcal{M}^N\\) with sample size \\(N \\in \\mathbb{N}\\) is the observed and incomplete data we want to fit our model to. For an accurate estimation the class distribution of y should match the overall class distribution as closely as possible. This can be formulated as a MAP estimation problem:\n\\[\\pi^* = \\arg \\max_{\\pi} P(\\pi | y, \\alpha, \\beta)\\]\n\\[= \\arg \\max_{\\pi} P(y | \\pi, \\beta) \\cdot P(\\pi | \\alpha)\\]\n\\[= \\arg \\max_{\\pi} ln P(y | \\pi, \\beta) + ln P(\\pi|\\alpha)\\]\n\\[= \\arg \\max_{\\pi} ln \\{ \\sum_{z \\in K^N} P(y, z | \\pi, \\beta)\\} + ln P(\\pi |\\alpha)\\]\nDue to the sum inside the logarithm in Equation (2), a closed-form solution for the MAP estimate is not available. Instead, we use the Expectation-Maximization (EM) algorithm [29] to iteratively estimate the mixture proportions \\(\\pi^*\\)The EM algorithm is a two-step iterative optimization algorithm that alternates between the Expectation (E) step and the Maximization (M) step to seek the maximum likelihood estimate of a marginal likelihood function. We use the MAP variant of the EM algorithm. It starts with an initial parameter value \\(\\pi^{old} \\sim P(\\pi |\\alpha)\\) and iteratively refines the estimate until convergence. The E-step computes the expected value of the log likelihood \\(P(y, z | \\pi, \\beta)\\) w.r.t. \\(\\pi^{old}\\) and is given by:\nAlgorithm 2 EM Algorithm for MAP Estimation\nInput: \\(y, \\pi, \\beta, \\alpha, \\tau\\)\nOutput: \\(\\pi^*\\)\n1: \\(\\pi^{old} \\leftarrow \\pi_0\\)\n2: \\(\\pi^{new} \\leftarrow \\pi\\)\n3: \\(N \\leftarrow |y|\\)\n4: \\(\\alpha_0 \\leftarrow \\sum_{k=1}^K \\alpha_k\\)\n5: \\(v \\leftarrow (c(y, 1), c(y, 2), . . ., c(y, M))^T\\)\n6:\n7: while \\(\\| \\pi^{new} - \\pi^{old} \\|_2 > \\tau\\) do\n8: \\(\\pi^{old} \\leftarrow \\pi^{new}\\)\n9: for k= 1 to K do\n10: Compute \\(\\Gamma_k\\) using (5)\n11: \\(N_k \\leftarrow v \\Gamma_k\\)\n12: \\(\\pi_k^{new} \\leftarrow \\frac{N_k+\\alpha_k-1}{N+\\alpha_0-K}\\)\n13: end for\n14: end while\n15: return \\(\\pi^{new}\\)\nD. Initialization of Concentration Parameters\nThe EM algorithm is sensitive to the initialization of the parameters and may converge to different local maxima depending on the initialization. We initialize \\(\\alpha\\) in two steps. First, we set \\(\\alpha_k = \\frac{D_k}{D_0} \\cdot N\\). This initial step is independent of the stragglers and provides a good starting point for optimization. The expected value for \\(\\pi_k\\) with this initialization is \\(\\frac{D_k}{D_0}\\), which aligns with Corollary 1 and ensures that the \\(\\pi_k\\) are most likely proportional to the dataset sizes. Our heuristic initialization keeps the \\(\\alpha_k\\) values in a range where they neither dominate the \\(N_k\\) in Proposition 1 nor are they insignificant, ensuring \\(N_k \\leq \\alpha_k\\).\nIn the second step, we adjust the \\(\\alpha_k\\) for each client k to account for the delay times given a hyperparameter \\(\\Delta > 0\\):\n\\[\\alpha_k \\leftarrow \\alpha_k \\cdot exp\\{\\Delta \\cdot \\frac{(\\omega_k - \\bar{\\omega})}{\\sigma_\\omega}\\}\\]\nwhere \\(\\bar{\\omega} = \\frac{1}{K} \\sum_{k=1}^K \\omega_k\\) and \\(\\sigma_{\\omega} = \\sqrt{\\frac{1}{K-1}\\sum_{k=1}^K(\\omega_k - \\bar{\\omega})^2}\\)\nIn this adjustment, the z-score \\(\\frac{(\\omega_k - \\bar{\\omega})}{\\sigma_\\omega}\\) standardizes the delay times \\(\\omega_k\\) by measuring how many standard deviations each\n\\([L(B^{(t)}; w_s) = \\frac{1}{B} \\sum_{i=1}^B \\sum_{m=1}^M y_{i,m} log (\\hat{y}_{i,m})\\]\nwhere \\({\\hat{y}}_i = f(f(x_i; w_c); w_s)\\)\nFor LDS we chose a convergence threshold of \\(\\tau = 1 \\times 10^{-5}\\)\nand set \\(R = 0\\) for all experiments. We always used the overall\nclass labels of \\(D_0\\) as y for the EM algorithm. All experiments\nwere conducted on a workstation with an Intel Core i9-13900\nCPU, 64 GB of DDR5 RAM and an NVIDIA GeForce A4000\nGPU with 16 GB of VRAM. Appendix one text goes here. You\ncan choose not to have a title for an appendix if you want by\nleaving the argument blank"}]}