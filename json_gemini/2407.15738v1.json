{"title": "Parallel Split Learning with Global Sampling", "authors": ["Mohammad Kohankhaki", "Ahmad Ayad", "Mahdi Barhoush", "Anke Schmeink"], "abstract": "The expansion of IoT devices and the demands of Deep Learning have highlighted significant challenges in Distributed Deep Learning (DDL) systems. Parallel Split Learning (PSL) has emerged as a promising derivative of Split Learning that is well suited for distributed learning on resource-constrained devices. However, PSL faces several obstacles, such as large effective batch sizes, non-IID data distributions, and the straggler effect. We view these issues as a sampling dilemma and propose to address them by orchestrating the mini-batch sampling process on the server side. We introduce the Uniform Global Sampling (UGS) method to decouple the effective batch size from the number of clients and reduce mini-batch deviation in non-IID settings. To address the straggler effect, we introduce the Latent Dirichlet Sampling (LDS) method, which generalizes UGS to balance the trade-off between batch deviation and training time. Our simulations reveal that our proposed methods enhance model accuracy by up to 34.1% in non-IID settings and reduce the training time in the presence of stragglers by up to 62%. In particular, LDS effectively mitigates the straggler effect without compromising model accuracy or adding significant computational overhead compared to UGS. Our results demonstrate the potential of our methods as a promising solution for DDL in real applications.", "sections": [{"title": "I. INTRODUCTION", "content": "RECENT advances in Deep Learning (DL), driven by growing architecture sizes, have significantly impacted areas like audio signal processing, natural language processing, and smart healthcare [1]\u2013[5]. This progress relies on large data volumes and substantial computing resources [6]\u2013[8]. The expansion of Internet-of-Things (IoT) devices, such as smartphones and wearables, provides extensive data and computing capabilities, yet privacy laws often restrict access to this data [6]. Distributed Deep Learning (DDL) frameworks have emerged to address these challenges by enabling collaborative model training across multiple devices while preserving data privacy.\nFederated Learning (FL) [9] is a prominent framework in DDL. In Federated Averaging (FedAvg), clients train model copies locally and periodically aggregate these models by averaging their parameters, facilitating global model convergence [9]. However, the high resource demands of FL restrict its implementation in edge computing and IoT environments, primarily due to substantial computational and communication burdens. Deploying FL on edge devices is further complicated by the straggler effect [10], which causes significant delays.\nAdditionally, FL struggles with datasets that are small or non-independent and identically distributed (non-IID). Strategies such as client selection [11] and model quantization [12] help mitigate these issues, but do not resolve them completely.\nSplit Learning (SL) [13], [14] addresses some limitations of FL by partitioning neural network training between client devices and a central server. Clients conduct initial Forward Propagation (FP) on local data and send intermediate results to the server, which completes the FP, initiates the Backward Propagation (BP) and returns gradients to the clients to complete the BP. SL enhances data privacy, reduces computational demands on the client-side, and increases the convergence speed compared to FL, making it well-suited for edge and IoT applications [13], [15], [16]. However, it incurs high training and communication latency due to its sequential nature [15]. While SL performs better than FL with non-IID data [16], [17], it suffers from catastrophic forgetting as the number of clients increases [18].\nTo address training latency in SL, advanced derivatives of SL like Split Federated Learning (SFL) [19]\u2013[21] and Parallel Split Learning (PSL) [22]\u2013[26] have been proposed. SFL combines the techniques of SL and FL to facilitate parallel model training across multiple devices. This approach leverages the advantages of both FL and SL. However, it encounters increased communication overhead compared to SL because it requires both the transmission of intermediate model results from clients to the server and the aggregation of updated models back to clients. Furthermore, SFL necessitates that the server maintains individual model copies for each client, which complicates scalability. Although variants such as SFLv2, SFLv3 [20], and SFLG [21] aim to mitigate these drawbacks, practical challenges persist.\nPSL enhances parallelism without requiring multiple server model instances or client model aggregation. However, PSL still faces issues with large effective batch sizes, non-IID data, and the straggler effect:\n1) Large Effective Batch Size Problem [27]: Also known as the Server-Client Update Imbalance Problem [25], where the effective batch size scales with the number of clients.\n2) Non-Independent and Identically Distributed Data: Devices contribute data of varying distributions and sizes, causing batch distributions to deviate from the overall data distribution and leading to poor generalization.\n3) Straggler Effect: Due to the need for synchronization, slower devices delay the training process, as the processing time of a single batch is determined by the slowest client, impacting throughput and total training time [10]."}, {"title": "II. RELATED WORK", "content": "PSL enables the parallel execution of the FP and BP on the clients and enhances scalability by avoiding the need for the server to maintain separate model copies for each client. Nevertheless, PSL faces the client decoupling problem [27] (also known as client detachment problem [25]), where the the information flow between clients is decoupled during the BP due to the independent execution of the BP on each client. Jeon and Kim [22] use gradient averaging over all clients to address the client decoupling problem. They chose the local batch sizes fixed and proportional to the dataset size of each client. Lyu et al. [31] propose averaging the clients' local loss function outputs instead of their local gradients. This approach aligns with that of [22] due to the linearity of the gradient operator, but enhances privacy as clients can compute the local loss functions without sharing labels. [22] and [31] show consistent convergence and improved accuracy in both IID and"}, {"title": "III. PRELIMINARIES", "content": "We consider the PSL setting from [22] where a server and $K \\in \\mathbb{N}$ clients train a neural network model with parameter vector $w$ collaboratively using the PSL framework. The set of clients is represented by $\\mathcal{K} = \\{1,2,..., K\\}$. We focus on supervised classification tasks, where the dataset $\\mathcal{D}_k$ of each client $k \\in \\mathcal{K}$ consists of input-label pairs $\\mathcal{D}_k \\triangleq \\{(x_{k,i}, y_{k,i})\\}_{i=1}^{D_k}$, where $D_k \\in \\mathbb{N}$ is the dataset size, $x_{k,i}$ is the input data, and $y_{k,i} \\in \\mathcal{M}$ is the ground truth class label of $x_{k,i}$. Here, $\\mathcal{M} = \\{1, 2, ..., M\\}$ represents all classes and $M \\in \\mathbb{N}$ is the number of classes. The overall dataset $\\mathcal{D}_0 = \\bigcup_{k=1}^{K} \\mathcal{D}_k$ is the union of all clients' datasets, with a total size $D_0 = \\sum_{k=1}^{K} D_k$. We refer to the distribution of the datapoints in $\\mathcal{D}_0$ as the empirical distribution. We define the class counting function $c : \\mathbb{N} \\times \\mathbb{N} \\rightarrow \\mathbb{N}$ with $N \\in \\mathbb{N}$ as\n\n$c(y, m) = \\sum_{i=1}^{D_k} \\delta(y_i, m)$\n\nwhere $\\delta(i, j) = \\begin{cases} 1 & \\text{if } i = j \\\\ 0 & \\text{otherwise} \\end{cases}$\nThe class distribution of a client $k$ with class label vector $y_k \\in \\mathcal{M}^{D_k}$ is denoted as $\\beta_k \\in \\mathbb{R}^M$ with $\\beta_k = (c(y_k, 1), c(y_k, 2), ..., c(y_k, M))^T$. The overall class distribution of $\\mathcal{D}_0$ with class label vector $y_0 \\in \\mathcal{M}^{D_0}$ is denoted as $\\beta_0 \\in \\mathbb{R}^M$ according to the same definition.\nEvery epoch consists of $T \\in \\mathbb{N}$ optimization steps. The set of optimization steps is denoted as $\\mathcal{T} = \\{1, 2, ..., T\\}$. We use the subscript $k$ for clients and the superscript $t$ for optimization steps. If the distinction is unnecessary, we omit the indices for brevity. Each client has a local batch size $B_k \\in \\mathbb{N}$. The global batch size $B = \\sum_{k=1}^{K} B_k$ is the total number of data points that arrive at the server in each optimization step. Accordingly, we differentiate between local batches $B_k^{(t)} \\subset \\mathcal{D}_k$ and the global batch $B^{(t)} = \\bigcup_{k=1}^{K} B_k^{(t)}$ in each optimization step.\nPSL mirrors SL by splitting the model parameters into a client-side lower segment $w_c$ and a server-side upper segment $w_s$ with $w = (w_s, w_c)$. The first layer on the server side is referred to as the cut layer. Before training, the server sends $w_c$ to the clients, each storing a copy $w_{c,k}$ with identical initial values. The function $f(x; w)$ maps $x$ to a predicted value according to the given model parameters $w$. Hence, the global model is given as $f(x;w) = f(f(x;w_c); w_s)$. When the argument of $f$ is a dataset or a subset of it, e.g. a batch $B$ with batch size $B$, it operates element-wise on the input data and is defined as:\n\n$f(B; w) = \\{(f(x_i; w), y_i)\\}_{i=1}^B$\nThe goal is to find the optimal model parameters $w^* = (w_s^*, w_c^*)$ that minimize the global loss function $\\mathcal{L}(\\mathcal{D}_0; w)$:\n\n$w^* = \\arg \\min_{w} \\mathcal{L}(\\mathcal{D}_0; w)$\n\n$\\mathcal{L}(\\mathcal{D}_0; w) = \\arg \\min_{w} \\frac{1}{D_0} \\sum_{k=1}^{K} \\sum_{i=1}^{D_k} L((x_{k,i}, y_{k,i}); w)$\n\n$= \\arg \\min_{w} \\frac{1}{D_0} \\sum_{k=1}^{K} \\sum_{i=1}^{D_k} L((f(x_{k,i}; w_c), y_{k,i}); w_s)$\nThe mini-batch Stochastic Gradient Descent (SGD) algo-rithm or one of its variants are typically used to optimize the model parameters, alternating between FP and BP. Each optimization step $t \\in \\mathcal{T}$ involves the following substeps."}, {"title": "IV. PARALLEL SPLIT LEARNING WITH GLOBAL SAMPLING", "content": "A good sampling method should ensure that the expected distribution of the global batches aligns with the empirical distribution. If the global batches frequently deviate from the empirical distribution, the trained model may become biased and generalize poorly. We measure the quality of a sampling method by the deviation of the generated global batches from the empirical distribution, referred to as batch-deviation. By analyzing how the choice of local batch sizes affects the batch deviation, we determine the UGS method that is bounded at least as tight as to central uniform sampling in terms of batch deviation and decouples the effective batch size from the number of clients. All proofs are provided in Section B.\nA. Deviation Analysis\nAssume we are given the complete dataset $\\mathcal{D}_0$ and train our model using central learning. In this scenario, the model is"}, {"title": "V. SIMULATION RESULTS", "content": "This section provides a comprehensive evaluation of the proposed UGS and LDS methods in terms of model performance and training time. We compare PSL extended by our sampling methods with several existing frameworks, focusing on metrics such as test accuracy, convergence behavior, and Time-Per-Epoch (TPE). Furthermore, we investigate the effectiveness of LDS for mitigating stragglers, and analyze its time complexity under various conditions."}]}