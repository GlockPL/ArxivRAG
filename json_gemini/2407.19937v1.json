{"title": "AOTree: Aspect Order Tree-based Model for Explainable Recommendation", "authors": ["Wenxin Zhao", "Peng Zhang", "Hansu Gu", "Dongsheng Li", "Tun Lu", "Ning Gu"], "abstract": "Recent recommender systems aim to provide not only accurate recommendations but also explanations that help users understand them better. However, most existing explainable recommendations only consider the importance of content in reviews, such as words or aspects, and ignore the ordering relationship among them. This oversight neglects crucial ordering dimensions in the human decision-making process, leading to suboptimal performance. Therefore, in this paper, we propose Aspect Order Tree-based (AOTree) explainable recommendation method, inspired by the Order Effects Theory from cognitive and decision psychology, in order to capture the dependency relationships among decisive factors. We first validate the theory in the recommendation scenario by analyzing the reviews of the users. Then, according to the theory, the proposed AOTree expands the construction of the decision tree to capture aspect orders in users' decision-making processes, and use attention mechanisms to make predictions based on the aspect orders. Extensive experiments demonstrate our method's effectiveness on rating predictions, and our approach aligns more consistently with the user's decision-making process by displaying explanations in a particular order, thereby enhancing interpretability.", "sections": [{"title": "Introduction", "content": "With the overwhelming amount of information on the Internet, recommender systems that aim to provide users with suitable items (products or services) by predicting a user's interest have been widely integrated into e-commerce, social network, and other web applications (Karn et al. 2023; Zheng et al. 2023). Recommender systems play a crucial role in alleviating the information overload problem, thereby rendering the decision-making process more user-friendly and enhancing the overall user experience in domains such as online shopping, social interaction, and more. The dual objectives of recommender system development encompass not only accurately predicting user preferences but also enhancing user satisfaction and trust. This dual focus motivates research to emphasize both recommendation accuracy and user-centric metrics, with interpretability standing out as an important one. This involves creating explainable recommender systems capable of not just suggesting items but also providing users with explanations for those recommendations (Tan et al. 2021). For instance, in an online shopping scenario, a recommendation for a mobile phone could include an explanation like \"This phone is purchased by 80% of your peers.\" \nIn the realm of explainable recommender systems, item reviews emerge as a valuable resource for generating explanations. Reviews offer insights from users' perspectives, serving a dual purpose: 1) a user's reviews contribute to characterizing their preferences (Li et al. 2017), and 2) an item's reviews can be leveraged to generate explicit explanations for its recommendation (Mcauley and Leskovec 2013). So many recent studies in recommendations have focused on using advanced techniques like multi-view deep learning approaches to analyze features from user reviews as well as item reviews, and integrate them into recommender systems (Elkahky, Song, and He 2015; Fan et al. 2022). The key concept involves using attention mechanisms to capture the importance of different aspects reflected in user preferences and item characteristics from reviews and generating explanations accordingly (Zhang et al. 2014b; Pan et al. 2022). For instance, considering a user's emphasis on the aspect of price in their reviews, the explanation could be framed as \u201cWe recommend this phone to you because its price matches with your taste on affordability.\" \nDespite the success of existing review-based explainable recommender systems, a crucial factor often overlooked is the aspect order in users' decision-making processes. The \"Order Effects Theory\" suggests that humans tend to follow a specific order of factors when making decisions (Anderson 1965), and the sequence in which factors are considered can influence decisions. For instance, a mobile photography enthusiast may prioritize the camera when buying a phone, while students might prioritize appearance and price over other factors. The different orders of considered aspects can significantly reveal users' preferences. Existing methods in explainable recommendations primarily focus on decisive factors in the decision-making process but neglect the ordering relationship between these factors. \nIncorporating aspect order into review-based explainable recommendations poses key challenges. Firstly, constructing aspect order is intricate due to its personalized and dynamic nature for both users and items across diverse situations, presenting complexities in modeling. Users exhibit different decision orders, and likewise, various items entail distinct orders of consideration during selection. Furthermore, users' contemplation of each aspect is not predetermined but dynamically influenced by contextual factors such as item characteristics and prior decisions (Meshram, Gopalan, and Manjunath 2016). Secondly, integrating aspect orders from both users and items for interpretable recommendations is non-trivial, given the inherent trade-off between explainabil-"}, {"title": "Related Works", "content": "Since our work is to apply the Order Effects Theory to the construction of explainable recommender systems, we divide the related work into two subsections: (a) Explainable Recommender Systems; and (b) Order Effects Theory. \nExplainable Recommender Systems. In the domain of explainable recommender system research, there are two types of dominant models, namely model-agnostic and model-intrinsic approaches (Lipton 2018). Since there is generally a trade-off between performance and transparency in recommender systems, these two approaches have their own advantages and shortcomings regarding performance and transparency (Zhang, Chen et al. 2020). \nThe model-agnostic approach, also named post-hoc explanation approach (Peake and Wang 2018), allows the recommendation model to be a black box and generates ex-"}, {"title": "Preliminary Analysis", "content": "In this section, we explore the applicability of the Order Effects Theory within the realm of recommendation, analyzing the ordering relationship among user concerns in reviews. The analysis is conducted on the Yelp dataset, which is a publicly available user review dataset and commonly used for recommendation tasks. The statistics of the dataset after preprocessing are shown in Table 1. Following the sentiment analysis toolkit, named Sentires, built by Zhang et al. (2014b), the triplets (Aspect, Opinion, Sentiment) were extracted from the reviews to represent each datum, wherein Aspect is the focus in our analysis. This phrase-level sentiment analysis toolkit is a widely used tool for aspect analysis in recommendation system due to its accuracy and ease of use in capturing aspect and sentiment (Zhang, Chen et al. 2020; Zhang et al. 2014a). Based on Yelp dataset, we present a scenario where users make decisions regarding restaurants, highlighting the existence of order effects and reveal its difference among users/items. This comprehensive analysis yields valuable insights into the applicability of the Order Effects Theory. \nWe show the following randomly picked review from the Yelp dataset (by user Hie3_7_Nan3R3HaygoCFvA for item VitNqJm8DIjw5D-Q-aiENQ). For brevity, we display only key sentences containing the filtered extracted aspects (highlighted in bold). Aspect order can be considered as the order of the aspects that appeared as the sequence of the user's decision-making process (Arapoff 1967). \nI've been here twice now, once for dinner and another time for the lunch buffet. The food is quite good, with nice hits of spice and savory flavors. The prices are average. \nAs shown in the review, the user initially emphasizes the aspect of lunch for mealtime, followed by considerations of the restaurant's food and flavor to evaluate taste. Subsequently, the aspect of price is addressed in later sentences, leading to the final experience step by step. The decisive chain can be identified as the aspect order: {lunch, food, flavor, price} (denoted as $O_{11}$). Similarly, another review for the same above user can be randomly picked and displayed as an aspect order of {lunch, food, price} (denoted as $O_{12}$). Further, for a different randomly selected user (user_id is -_FCaLa5eYXedOotc7J18Q), one aspect order can be extracted as {food, service, food, chicken, service, table, spot} (denoted as $O_{21}$). It is obvious to see that the consistency of the two aspect orders for the same user ($0_{11}$ and $O_{12}$) is much higher than that for different users ($O_{11}$ and $O_{21}$), suggesting that the intra-consistency (within one user) of aspect order is higher than the inter-consistency (between different users). \nWe further aim to generalize the above findings by randomly selecting 10,000 users from Yelp and then constructing 10,000 pairs of intra-user reviews (two reviews from the same user) and 10,000 pairs of inter-user reviews (two reviews from two different users). For each pair of intra-user or inter-user reviews, we utilize the NDCG metric (Kanoulas and Aslam 2009) (the details of NDCG are described in Experiment Section) to evaluate the consistency of aspect order between the two reviews (named $intra\\_cons_{user}$ or $inter\\_cons_{user}$, respectively). To highlight the difference between a user's $intra\\_cons_{user}$ and $inter\\_cons_{user}$, we construct a measurement named $consistency\\_dis_{user}$ which is computed as $intra\\_cons_{user}$ minus $inter\\_cons_{user}$. It can reflect whether a user has a specific order preference. The CDF (Cumulative Distribution Function) of $consistency\\_dis_{user}$ among the 10,000 users is shown Figure 1 (a). From the figure, we can see the intra_consuser of nearly 70% users is greater than their corresponding inter_consuser (p < 0.001), indicating that most users have specific aspect order considerations. We can define such users as Sensitive Users. Moreover, it suggests that a proportion of users are Strong Sen-"}, {"title": "Methodology", "content": "In this section, we introduce our proposed Aspect Order Tree-based explainable recommendation method AOTree, which aims to simulate human decisive behavior according to Order Effects Theory. We first present the Problem Formulation of this work followed by the general architecture shown in Figure 3, which consists of three key modules: 1) the AOTree Generator, 2) the Aspect Order Generator, and 3) the final Predictor."}, {"title": "Problem Formulation", "content": "In this work, we target at the rating prediction task. Let $U = \\{U_1, U_2, ..., U_m \\}$, $V = \\{V_1, V_2, ..., V_n \\}$ denote the user set and item set, respectively, where m and n represent the number of users and items. Then, $r_{i,j}$ is the rating value for a user $u_i$ toward an item $v_j$. And a set of observed data is represented as $O = \\{(u, v)|u \\in U, v \\in V, u \\text{ has reviewed } v\\}$. \nThe construction of the sentiment set, represented by triplets (Aspect, Opinion, Sentiment), follows the same procedure outlined in the Data Analysis Section. Suppose we finally get the aspect set $A = \\{a_1, a_2, ..., a_l\\}$, where l denotes the number of aspects. Then, we construct the user aspect matrix $X \\in R^{m*l}$ as well as the item aspect matrix $Y \\in R^{n*l}$ as (Zhang et al. 2014a): \n$X_{i,k} = \\begin{cases} 0, \\text{ if } u_i \\text{ did not mention aspect } a_k, \\\\ \\frac{2}{1+(N-1)} (1+\\exp(-f_{i,k}s_{i,k})), \\text{ otherwise.} \\end{cases}$ \n$Y_{j,k} = \\begin{cases} 0, \\text{ if aspect } a_k \\text{ was not mentioned for item } v_j, \\\\ \\frac{2}{1+(N-1)} (1+\\exp(-f_{j,k}s_{j,k})), \\text{ otherwise.} \\end{cases}$ \nN denotes the rating scale, which is 5 in our datasets. $f_{j,k}$ is the frequency of user $u_i$ mentioning aspect $a_k$, and $s_{j,k}$ is the average sentiment calculated from the sentiment set. Since $X_{i,k}$ and $Y_{j,k}$ measure the importance of aspect $a_k$ to user $u_i$ and item $v_j$, respectively, we thus call them \u201caspect importance\" in this paper and X and Y are called individual aspect importance matrices."}, {"title": "Group Aspect", "content": "Before building AOTree, we first process the user/item aspect matrix into a general representation for all users/items rather than construct trees for each user/item in order to reduce the time complexity. We take the user AOTree genera-"}, {"title": "AOTree Generator", "content": "The tree-based model is considered to be explainable and transparent when applied to recommendation tasks (Zhang, Chen et al. 2020). Also, it is easy for humans to simulate and thus understand and trust (Wu et al. 2017). Rather than learning features for prediction like other deep learning methods, the tree-based model learns decision rules from datasets. However, the traditional decision tree method aims to solve classification problems, which is not applicable to our situation. So, we design our own decision rules to split items/users according to the matching degree within aspect quality/preference value to find an aspect order representing the considering decision process of users. \nDue to the variances of aspect order effects suggested in the Preliminary Analysis Section, we personalize the order construction for both users and items when building the AOTree, respectively. To be more specific, when building the User-AOTree, we could split user sets and get the considering aspect sequence for each user by matching the general item aspect importance vector $\\hat{Y}$ with the individual user aspect importance matrix X, meaning how the sequence of general item quality is shown according to different users. \nThe Item-AOTree can be constructed in the same way with the corresponding general user aspect importance vector $\\hat{X}$ with the individual item aspect importance matrix Y. \nIn general, inspired by the decisive process of Markov Decision Processes (MDP), the goal is to maximize its reward stream, and thus, each decisive step is chosen based on the best current state (Shani et al. 2005). There are various approaches existing for obtaining optimal policy, exactly or approximately. In our scenario, each step of our aspect order is defined as the chosen aspect, which is based on the minimal Split Expense for the optimal Split Value. Specifically, our AOTree is constructed as the following three steps: \n1. Calculate Split Value (SV) for each aspect, representing the optimal split criteria to match user preference and item characteristic corresponding to each aspect; \n2. Calculate Split Expense (SE) for each aspect with its corresponding SV, representing the reward for each aspect;"}, {"title": "Calculate Split Value (SV)", "content": "We first traverse all aspects to calculate the split value for each aspect, representing the matching degree between the user's preference and the item's quality. Rather than directly using values in the general item aspect importance vector (\u0176) to split nodes, which may cause biases among different users, we focus on the degree of ranking matching between users and items on each aspect (Nevo 1989). \nIn detail, for each aspect, we first get the item quality rank position ($P_{Ik} = 1,2,..., l$) according to the general item aspect importance vector (\u0176) and find the corresponding user rank position ($P_{Uk}$) in the individual user aspect importance matrix (X) for the same aspect ($a_k$). $P_{Ik}$ and $P_{Uk}$ satisfy the Equation 2, which means the item's quality rank matches the user's preference rank. \n$\\frac{P_{Ik}}{l} = \\frac{P_{Uk}}{m}.$ (2) \nAs the values of $P_{Ik}$, m and l are already known, the corresponding user rank position ($P_{Uk}$) could be calculated. Then, the Split Value (SV), the value in the individual user aspect importance matrix (X) at the corresponding position, could be obtained by the following equation inspired by the matching-ranking technique (Nevo 1989): \n$\\frac{P_{Uk} - P_{Ukl}}{P_{Ukr} - P_{Ukl}} = \\frac{SV_k - X_{P_{Ukl},k}}{X_{P_{Ukr},k} - X_{P_{Ukl},k}}$ (3) \nAs $P_{Uk}$ is obtained as a decimal, $P_{Ukr}$ and $P_{Ukl}$ represent the right and the left integer rank position index of $P_{Uk}$ ($P_{Ukr} > P_{Uk} > P_{Ukl}$). And $X_{P_{Ukr},k}$ and $X_{P_{Ukl},k}$ are the corresponding values in the individual user aspect importance matrix (X) for aspect $a_k$. As $SV_k$ is the only unknown value in Equation 3, it can be easily derived given other values. \nThe process can be illustrated as a specific example in Figure 4 for a better understanding. Take aspect $a_1$ as one of the traversals, so the k is 1 in this example. Then, the value for $P_{I1}$, l and m are 2 (item quality rank position for $a_1$), 6 (number of aspects) and 5 (number of users), and we can obtain $P_{U1} = m * P_{I1}/l = 5 * 2/6 = 1.667$ (according to Equation 2). The corresponding value for $P_{U1r}$, $P_{U1l}$ are 2 and 1, then $X_{P_{U1r},k}$ and $X_{P_{U1l},k}$ are 3.544 and 2.211, respectively corresponding to User2 and User5. And the final SV value for aspect $a_1$ is $SV_1 = \\frac{(1.667-1)}{(2-1)} * (3.544 -$"}, {"title": "Calculate Split Expense (SE)", "content": "Then, we decide which aspect should be chosen as the node split criterion and split users by comparing the corresponding aspect value in the individual user aspect importance matrix (X) with SV based on the chosen aspect. Take $a_k$ as an example, if the aspect importance value for the user ($X_{ik}$) is larger than $SV_k$, the user is split into the right node, otherwise, into the left node. Our goal is to find one aspect that could make the best matching between user preference and item quality, simultaneously ensuring the robustness of the decision tree. Targeting the rationality and robustness of the decision tree construction, we specify the split principles into the following three components, $SE_1$, $SE_2$ and $SE_3$ aiming at the child nodes after splitting the node. The specific division principles refer to classification and clustering methods, such as Support Vector Machine (SVM) and K-Nearest Neighbors (KNN), aiming to enhance intra-class similarity while promoting inter-class dissimilarity: (suppose $a_k$ is the split criterion for the current node): \n1) $SE_1$: users in the same child node should be similar on aspect $a_k$, i.e., with minimal distance: \n$\\min SE_1 = \\frac{1}{i}\\sum_i X_{ik} - \\hat{X_k},$ (4) \nwhere $\\hat{X_k}$ and $X_{ik}$ are the average aspect importance value in the individual user aspect importance matrix (X) and the specific value for user i, both on aspect $a_k$. \n2) $SE_2$: users in the different child node groups should be different on aspect $a_k$, i.e., with maximal distance: \n$\\max SE_2 = |\\hat{X}_{right,k} - \\hat{X}_{left,k}|,$ (5) \nwhere $\\hat{X}_{right,k}$ and $\\hat{X}_{left,k}$ are the average aspect importance values for aspect $a_k$ in the right and left child node. \n3) $SE_3$: users in the same child node group should be different on aspects except $a_k$, to continually split users: \n$\\max SE_3 = \\sum_o^{l-k} \\sum_i | X_{io} - \\hat{X_o}|,$ (6) \nwhere $X_{io}$ is the aspect importance for user i on aspect except $a_k$, and $\\hat{X_o}$ is the average aspect importance for all users on aspect $a_o$. \nThen, we could get the Split Expense (SE) value by traverse all aspects according to the corresponding Split Value (SV) as follows: \n$SE = NV * SE_l * SE_r,$ (7) \nwhere NV represents the normalization constant to normalize the value of SE, and we set it to $10^{-N_l*N_r}$ in our method, where $N_r$ and $N_l$ are the number of users in each child node. The subscripts \"r\" and \"l\" denote the values of the right and left child nodes, respectively. $SE_l$ and $SE_r$ are Split Expense (SE) for the left and right nodes after splitting, which could be obtained by: \n$SE_l = \\frac{SE_{l1}}{SE_{l2} * SE_{l3}}, SE_r = \\frac{SE_{r1}}{SE_{r2} * SE_{r3}}$ (8) \nChoose Aspect and Split Value We finally choose the aspect with the minimal Split Expense (SE) in order to satisfy our goal: \n$k = arg \\min_{k} SE.$ (9) \nThen the aspect $a_k$ is the split aspect for this node, and the corresponding Split Value (SV) can be obtained by Equation 3."}, {"title": "Build AOTree", "content": "We continue the above three steps to decide on each node with a split aspect and its corresponding Split Value (SV). The criteria for stopping the division is until each node has only one item or the tree depth has reached the predefined threshold. \nThe obtained User-AOTree represents how the sequence of general item quality is shown according to different users. By deciding the split aspect for each node, we could construct aspect order for certain user sets. In each leaf node, users with similar aspect quality share the same aspect order (e.g., $UP_i = \\{N_1, N_2, ..., N_e\\}$ for user $u_i$, where N and e denote the aspect id and the path length). \nSimilarly, the Item-AOTree can be built following the same process (e.g., the aspect order is $IP_j = \\{N_1, N_2, ..., N_e\\}$ for user $v_j$), representing how the sequence of general user preference is shown according to different items."}, {"title": "Aspect Order Generator", "content": "For each interaction (user-item pair), we could convert it into aspect order $UP_i$ for $u_i$ and aspect order $IP_j$ for $v_j$ according to the User-AOTree and Item-AOTree, respectively. The above two orders are considered separately from the user and item sides, providing information for the final decision sequence. So we combine these two considering orders by a simple average: \n$Ind_k = \\frac{Ind_{UP,k} + Ind_{IP,k}}{2}$ (10) \nwhere $Ind_{UP}$ and $Ind_{IP,k}$ denote the ranking index of aspect $a_k$ in $UP_i$ and $IP_j$, respectively. Then, the average aspect index Indk can be sorted to construct the final aspect sequence $TP = \\{TP_1,TP_2, ..., TP_e\\}$, and each element is the id for aspect, representing the decisive aspect process for certain user toward certain item. We fill each sequence into a fixed-length e with random aspect id if the length is less than e. \nCorresponding to the aspect id sequence TP, we could get the final user or item aspect importance sequence USeq or ISeq by picking the value of the corresponding aspect id from the original individual user/item aspect importance matrix (X/Y). For example, the value for USeq = $\\{USeq_1, USeq_2, ..., USeq_e\\}$ is picked from $X_i$ based on the aspect id in TP, which means, $U Seq_i$ is equal to $X_{i,TP_i}$."}, {"title": "Prediction with Aspect Order", "content": "The final rating prediction result is obtained mainly by two components, the decision process information, which is the core part of our method, and the user/item context information. In the learning process, we refer to the traditional latent factor method, but extend with our designed components: applying position embedding and self-attention layer to the sequence, and then the final ratings are obtained with a linear model. \nPosition Embedding Layer Given the aspect order generated by AOTree (TP), which is represented by aspect id, we project each id into an embedding vector with size d. Then, we could obtain the path embedding TP = $\\{TP_1, TP_2, ..., TP_e\\} \\in R^{e \\times d}$. \nIn order to highlight the position of each aspect in the sequence, we consider combining our sequence with a position embedding vector. Inspired by Kang and McAuley (2018), we add a position embedding $E = \\{E_1, E_2, ..., E_e\\} \\in R^{e \\times d}$"}, {"title": "Experiment", "content": "In this section, we present experimental setup, comprehensive experimental results and detailed analysis, aiming at answering the following research questions (RQs): \n\u2022 RQ1: Does AOTree outperform the state-of-the-art methods in terms of recommendation accuracy? \n\u2022 RQ2: Does the considering aspect order obtained by AOTree help to improve recommendation accuracy? \n\u2022 RQ3: Does the considering aspect order obtained from AOTree help to improve explainability?"}, {"title": "Experimental Setup", "content": "Dataset. The experiments are conducted on five publicly available user review datasets from Amazon and Yelp. For Amazon, we chose four common datasets: Cells Phones and Accessories, Office Product, Patio, Lawn and Garden and Digital Music. \nWe first adopt the sentiment analysis toolkit (Zhang et al. 2014b) and only keep users/items with more than $T_u$/$T_i$ review records due to the sparsity of the datasets (Wang et al. 2018a; Pan et al. 2022). As we focus on the aspects extracted from reviews, we also set $T_a$ to the minimal number of appearing times for each aspect. The specific value of the threshold and the statistics of the datasets after preprocessing are shown in Table 1. The Density value denotes the average number of aspects mentioned by users. It is worth noting that the $T_u$/$T_i$ for Digital Music and Yelp are set differently in order to keep a relatively consistent density value. In the experiments, each dataset is divided into training set, validation set, and test set by 8:1:1. \nCompared Methods. In order to validate the performance of our proposed AOTree method, several baselines are selected for comparison. Specifically, we choose several commonly-used traditional and state-of-the-art methods in the field of recommender systems, such as the classic benchmark method (MF), tree-based methods (XGBoost and TEM), review-based method (NARRE, R3), and aspect-based prediction methods (SULM, ANR and ERRA). All the compared methods are evaluated based on the code published by the authors with careful hyper-parameter tuning. The compared methods are: \n\u2022 MF (Koren, Bell, and Volinsky 2009): Matrix Factorization (MF) is a classic rating prediction method using bias terms and latent features for prediction. \n\u2022 XGBoost (Chen and Guestrin 2016): XGBoost is the state-of-the-art tree-based method. The final tree captures complex feature dependencies, that is the cross feature combination for different paths. \n\u2022 TEM (Wang et al. 2018b): Tree-enhanced Embedding Method (TEM) is also a tree-based model. It is constructed into two cascade parts with GBDT and an easy-to-interpret attention network, making the recommendation process fully transparent and explainable. \n\u2022 NARRE (Chen et al. 2018): Neural Attentional Regression model with Review-level Explanations (NARRE) is a widely used state-of-the-art explainable recommendation method. This method focuses on the reviews usefulness and uses an attention mechanism to learn the importance weights over different reviews. \n\u2022 R3 (Pan et al. 2022): Recommendation via Review Rationalization (R3) is a causal-aware explainable method, which extracts rationales from reviews via a rationale generator to alleviate the effects of spurious correlations in recommendation. Then, the final recommendation and causal-aware explanation can be generated according to the rationales. \n\u2022 SULM (Bauman, Liu, and Tuzhilin 2017): Sentiment Utility Logistic Model (SULM) is a basic aspect-based model, which uses sentiment analysis of user reviews to identify the most valuable aspects for recommendation."}, {"title": "Implementation Details", "content": "For the compared algorithms, we follow the corresponding papers to initialize all the hyperparameters and carefully tune them to achieve optimal performance. We use Adaptive Moment Estimation (Adam) (Kinga, Adam et al. 2015) to optimize our method, with learning rate in \\{0.00001, 0.0005, 0.001, 0005\\}, L2 regularization coefficient in \\{10^-3, 10^-4, 10^-5, 10^-6, 10^-7\\} and dropout rate in \\{0, 0.2, 0.4, 0.6, 0.8\\}. We use early stopping with 10 epochs and report the final results from the best-performing model on validation sets. More specifically, other hyperparameters are searched with the batch size in \\{16, 32, 64, 128\\}, the latent factors number in the range [5, 20]. Further, in order to control the expense of constructing the AOTree, the max depth of the tree is limited to \\{5, 10, 15,..., 100\\} and the construction process does not require training."}, {"title": "Results", "content": "Overview Performance (RQ1) \nWe first report the overall performance of AOTree in Table 2 and Table 3, which show the MSE and NDCG results according to the competitors. From the results, AOTree significantly outperforms all the other competitors on all datasets (p < 0.05), which could answer RQ1. The performance of R3 method ranks second due to the dependence on causal relations rather than spurious correlations. And NARRE and ANR also show quite good performance, especially in Cell Phones and Accessories and Office Product datasets, which demonstrates that attention mechanism can capture implicit finer-grained properties from reviews. TEM captures the explicit cross features, which ignore the relationship from the \u201cordering\u201d dimension, and it only shows a slight advantage in Office Product. By exploring the \"ordering\" dimension among aspects, the AOTree is constructed by using sequence property and shows promising performance. From the ranking score results on NDCG compared with SULM, the AOTree method still significantly outperforms"}, {"title": "Effectiveness of the Aspect Order (RQ2)", "content": "For RQ2, we perturb the obtained aspect order to demonstrate the superiority of the current order. However, the results are not stable for the whole dataset. So we split users according to the MSE results shown in Table 2. To be specific, we focus on the samples with MSE less than the average value in the training set. If all interactions for one user meet the above criterion, the user will be selected. In fact, this is a very strict partition criterion since it requires each of a user's interactions meets such a criterion, and the selected users should have strong order effect. So we mark the selected users as Identified Strong Sensitive Users and the others as Identified non-Strong Sensitive Users. Our analysis suggests that the fraction of Identified Strong Sensitive Users reaches 20% to 30% in different datasets (20.37% in Cells Phones and Accessories, 26.61% in Office Product, 21.60% in Patio, Lawn and Garden, 21.05% in Digital Music, and 31.57% in Yelp). Such a proportion is close to that of the Strong Sensitive Users with consistency_disuser higher than 0.5 in our Preliminary Analysis, indicating our method's effectiveness in capturing order effect. \nThen, for each user, we verify the effectiveness of the captured aspect order for each interaction by adopting two perturbation operations related to the \"ordering\u201d: 1) shuffling the whole order to verify whether the sequence generated by the model is effective, and 2) replacing the first five aspects with random ones to see whether the Primacy principle contributes. The final results are shown in Table 4. From the results, we can obtain the following findings. For the Identified Strong Sensitive Users, the results after perturbation show higher MSE values, which suggests the superiority of the aspect order obtained by our method. While for the Identified non-Strong Sensitive Users, the above conclusion seems to be dismissed since the results after perturbation do not make a significant difference."}, {"title": "Explainability of Aspect Order (RQ3)", "content": "As we seek for a specific form of interpretability known as human simulability, we finally verify the explainability of the AOTree model according to the simulability, that is, to attain the matching degree between the obtained aspect order and the original aspect sequence that appeared in the reviews. We compare the importance and order of the first five aspects to prove that our method could predict users' decision process displayed as reviews. The importance is represented by calculating the coverage of the aspects, and the order is verified by numerical metrics of NDCG and F1. \nWe choose TEM, SULM and ERRA as the compared methods in the explanability evaluation because they also focus on using the extracted aspects as explanations. The aspect order of TEM and SULM is constructed according to the rank of the attention weights for each aspect and the aspect order of ERRA is extracted by the aspects appeared in the generated explanations. The results are shown in Table 5. For the Num% value, we can see that AOTree significantly outperforms the other three methods on all the datasets, which means that our method has the advantage of more accurately discovering the considering factors in the reviews. The advantage is quite significant, especially for the Yelp dataset (p < 0.01). And for the NDCG@5 and F1@5 metrics, although TEM performs better on Cells Phones and Accessories and Digital Music dataset, the coverage value is quite low (only 7.10% and 5.43%). The reason for this abnormal result is that in these two datasets, the proportion of sensitive users is the lowest (20.37% in Cells Phones and Accessories and 21.05% in Digital Music), resulting in the order effects generated by the model not fully matching the results in actual user reviews. Also, ERRA performs relatively good results on Cells Phones and Accessories, which show its advatange on aspect prediction due to the aspect enhancement component. In summary, we can conclude that AOTree outperforms TEM and SULM in terms of explainability."}]}