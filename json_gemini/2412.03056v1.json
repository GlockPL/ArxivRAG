{"title": "Point-GN: A Non-Parametric Network Using Gaussian Positional Encoding for Point Cloud Classification", "authors": ["Marzieh Mohammadi", "Amir Salarpour"], "abstract": "This paper introduces Point-GN, a novel non-parametric network for efficient and accurate 3D point cloud classification. Unlike conventional deep learning models that rely on a large number of trainable parameters, Point-GN leverages non-learnable components\u2014specifically, Farthest Point Sampling (FPS), k-Nearest Neighbors (k-NN), and Gaussian Positional Encoding (GPE)\u2014to extract both local and global geometric features. This design eliminates the need for additional training while maintaining high performance, making Point-GN particularly suited for real-time, resource-constrained applications. We evaluate Point-GN on two benchmark datasets, ModelNet40 and ScanObjectNN, achieving classification accuracies of 85.29% and 85.89%, respectively, while significantly reducing computational complexity. Point-GN outperforms existing non-parametric methods and matches the performance of fully trained models, all with zero learnable parameters. Our results demonstrate that Point-GN is a promising solution for 3D point cloud classification in practical, real-time environments.", "sections": [{"title": "1. Introduction", "content": "Point cloud classification is a critical task in 3D data analysis and has been widely employed in various fields, including object detection [45, 47], 3D reconstruction [36], robotics [29], and medicine [5, 39]. Unlike 2D images that are structured in regular grids, point clouds consist of unordered and irregular sets of points, presenting unique challenges for efficient and accurate analysis. The unordered nature and high dimensionality of point clouds make traditional 2D image processing techniques unsuitable for this task, thereby necessitating specialized algorithms that can handle the unique structure of 3D data.\nTo address the high computational and memory demands of parametric models, recent research has explored non-parametric methods. These approaches, such as PointClip [42], which uses pre-trained 2D models for point cloud classification, and Point-NN [43], which avoids learnable weights, aim to reduce parameter sets and improve efficiency. However, these methods often trade off computational efficiency for classification accuracy, highlighting the need for further improvement.\nIn this paper, we propose a novel non-parametric method for point cloud classification that introduces a Gaussian function for positional embedding, as shown in Fig. 1. This Gaussian embedding enhances classification accuracy by capturing local geometric structures without requiring extra parameters or retraining. Integrated into the non-parametric framework of PointNet++ with Farthest Point Sampling (FPS) and k-Nearest Neighbors (k-NN), it improves performance while maintaining computational efficiency.\nOur contributions are threefold:\n\u2022 We introduce a Gaussian embedding function to the non-parametric framework, which significantly improves classification accuracy without adding computational overhead.\n\u2022 We simplify the network design, eliminating unnecessary complexities while maintaining performance comparable to state-of-the-art parametric models, offering a more efficient alternative for resource-constrained environments.\n\u2022 We demonstrate the efficiency and scalability of our approach through extensive experiments on popular benchmarks, including ModelNet40 [34] and ScanObjectNN [28], achieving competitive performance without additional costs in terms of memory or computational requirements.\nThis method offers a practical solution for efficient, accurate point cloud classification, ideal for real-time applications with limited resources, such as robotics and autonomous systems [1, 14], where both accuracy and efficiency are essential."}, {"title": "2. Related works", "content": "In this section, we provide an overview of prior approaches to 3D point cloud classification, focusing on both projection-based and point-based methods, and introduce recent advancements in positional encoding techniques, which have significantly impacted model performance and efficiency."}, {"title": "2.1. 3D Point Cloud Classification", "content": "3D point cloud classification methods can be divided into projection-based and point-based approaches.\nProjection-based methods convert 3D point clouds into 2D representations, such as depth maps [3, 4, 26] or voxel grids [9, 12, 31], enabling the use of 2D image processing techniques. However, they often lose spatial details due to the sparsity and incompleteness of point clouds.\nPoint-based methods process raw 3D point clouds directly, preserving geometric information. PointNet [20] processes points independently and aggregates global features using max pooling but struggles with local geometric details. PointNet++ [21] improves this by introducing a hierarchical architecture for capturing local features. Other advancements include convolutional [7, 18, 24, 27] and graph-based models [10, 32], as well as attention and transformer-based methods [37, 40, 46] that model long-range dependencies.\nHowever, these models are computationally intensive, limiting their use in real-time applications. Efficient methods like ShellNet [44] and RandLA-Net [6] reduce memory usage, but still face challenges with large-scale data. Convolutional models like KPConv [27] reduce memory overhead through sparse convolutions but still require significant resources.\nInspired by Point-NN [43], we propose a novel non-parametric model for point cloud classification that improves feature extraction without introducing additional trainable parameters, thus enhancing both efficiency and scalability."}, {"title": "2.2. Positional Encodings", "content": "Positional encoding was first introduced in the Transformer architecture [30] to inject positional information into input sequences, such as words in a sentence, using sinusoidal functions. This method has since been widely adopted in natural language processing and computer vision, where capturing spatial relationships is critical. In point cloud processing, positional encoding allows the model to retain spatial awareness in 3D space, particularly for applications involving coordinate-based models, such as 2D image synthesis [16] or 3D scene reconstruction [17].\nIn 3D point cloud processing, positional encoding plays a crucial role in capturing the underlying geometric relationships between points. One notable application is in Neural Radiance Fields (NeRF) [13], where sinusoidal encoding transforms input coordinates into higher-dimensional feature spaces, enabling the accurate reconstruction of fine-grained details in 3D scenes. Such approaches demonstrate the importance of encoding spatial information when dealing with high-frequency signals, as it"}, {"title": "3. The Proposed Method", "content": "In this section, we present the details of Point-GN, our Non-Parametric Network that utilizes Gaussian Positional Encoding (GPE) for point cloud classification.\nTo ground the discussion, we first revisit the fundamental concepts behind 3D point clouds and how they are typically classified. We then elaborate on the design of our non-parametric feature encoder and classifier, which are integral components of Point-GN."}, {"title": "3.1. Background", "content": "A 3D point cloud is a collection of points in 3D space representing the shape or structure of an object or scene. Each point p\u2081 = (Xi, Yi, Zi) \u2208 R\u00b3 is defined by its coordinates and may have additional attributes, such as color or surface normal vectors. Given a point cloud P = {P1, P2,..., PN}, where N is the number of points, each point p\u2081 is represented by its coordinates (xi, Yi, zi). An encoder extracts meaningful information from the point cloud into a compact representation:\nEncoder(P) = F \u2208 Rd\nwhere F is the feature vector that encodes the essential characteristics of the point cloud into a d-dimensional space.\nFor classification, the feature vector F is fed into a classifier that maps the feature vector to C classes, producing a vector of logits y \u2208 RC:\nClassifier(F) = y = (Y1, Y2, ...,YC)\nThe predicted class c is determined by selecting the class with the highest score:\nc = arg max Yi\ni"}, {"title": "3.2. Gaussian Positional Encoding (GPE)", "content": "Gaussian Positional Encoding (GPE) embeds spatial information into the feature representation of individual points in the 3D point cloud. By transforming raw point coordinates into a higher-dimensional space, GPE provides the model with richer spatial context without introducing learnable parameters. The encoding is formulated as:\nVx(xi, Uj) = exp(-\\frac{|| xi - vj ||^2}{2\\sigma^2})\nVy (Yi, vj) = exp(-\\frac{|| Yi - vj ||^2}{2\\sigma^2})\nVz(Zi, Uj) = exp(-\\frac{|| Zi - vj ||^2}{2\\sigma^2})\nwhere vj are predefined reference points, and o is the standard deviation that controls the focus on local vs. global spatial information. A smaller o captures local detail, while a larger o captures broader spatial patterns.\nThe encoded feature vector for each point is:\nV(Pi) = [\u221ax(xi, vj), Vy (Yi, vj), Vz (zi, vj)]j=1\nwhere V is the number of reference points along each axis."}, {"title": "3.3. Non-Parametric Feature Encoder", "content": "In our approach, the non-parametric feature encoder leverages Gaussian Positional Encoding (GPE) to capture and aggregate spatial information from 3D point clouds. This hierarchical encoder adapts to various input configurations without relying heavily on learned parameters, making it flexible across different tasks."}, {"title": "3.3.1 GPE Embedding", "content": "The embedding process begins by applying GPE to each point p\u2082 in the point cloud P. This transformation maps the raw 3D coordinates into a higher-dimensional feature space, enhancing the model's ability to understand spatial relationships. As a result, each point p\u2081 is represented by a richer feature vector (pi), capturing its spatial relationships more effectively.\nThis transformation expands the original 3D coordinates into a V \u00d7 3-dimensional space, where V represents the number of reference points vj used in the encoding. These reference values are strategically chosen or learned by the model, often distributed uniformly between -1 and 1. The parameter \u03c3, controlling the width of the Gaussian function, determines how spatial information is captured. A smaller \u03c3 focuses on local details, while a larger \u03c3 captures more global spatial relationships. This flexibility enables the GPE to balance the capture of both local and global spatial information effectively.\nThe GPE embedding serves as the foundational step in our non-parametric feature encoder. By transforming raw 3D point clouds into a feature space rich in spatial context, (pi) enhances the model's ability to recognize and utilize the underlying geometric structures of the data. This process is integral to the effectiveness of the encoder, leading to improved performance in tasks such as object recognition, segmentation, and classification, where a deep understanding of both local and global spatial relationships is essential."}, {"title": "3.3.2 Local Grouper", "content": "After GPE embedding, feature extraction proceeds through multiple stages, each involving a local grouper, GPE aggregation, and neighbor pooling. At each stage, the input point cloud from the previous stage is represented as {Pi, (Pi)}1, where p\u00bf \u2208 R\u00b3 denotes the coordinates of point i and y(pi) \u2208 RV\u00d73 represents the GPE-embedded features of that point.\nThe process begins with Farthest Point Sampling (FPS) to downsample the number of points from N to N/2, selecting a subset of local center points:\n{Pj, (j)}=1 = FPS ({pi, Y(Pi)}=1)\nNext, the downsampled point coordinates p; and the original point coordinates pi are used by the K-Nearest Neighbors (KNN) algorithm to find the K nearest neighbors for each downsampled point pj. The indices of these nearest neighbors are used to retrieve the corresponding coordinates and features:\nidxj = KNN(Pj, Pi)\nThe retrieved coordinates and features are:\nNj = retrieve ({pi}=1, idxj) \u2208 RK\u00d73\n\u0393j = retrieve ({\\(pi)}=1, idxj) \u2208 RK\u00d7(V\u00d73)\nHere, P; represents the gathered coordinates, and j represents the gathered features for the point pj. The retrieved coordinates P; and features \u0393j are then normalized using the mean and standard deviation of each point's neighbors. These normalized coordinates and features are then passed to the next stage for further processing."}, {"title": "3.3.3 GPE Aggregation", "content": "The features from the Local Grouper are then fed into the GPE Aggregation module. Here, GPE is applied to the retrieved coordinates P; to encode spatial information. These encoded features are then weighted and combined with retrieved features \u0393j, emphasizing points closer to the center. The updated feature representation is:\n\u0393; \u2190 \u0393; + (P\u0632) (\u0632(\nIn this formulation, \u03b3(Pj) represents the encoded spatial information of the retrieved neighbors, and \u0393; represents the features retrieved from the nearest neighbors. The element-wise multiplication ensures that the final aggregated features are influenced by both local features and spatial encoding, effectively capturing detailed local geometry while preserving spatial relationships."}, {"title": "3.3.4 Neighbor Pooling", "content": "Following GPE Aggregation, the neighbor pooling process aggregates features using both mean and max operations across the neighbor dimension. For each point, the pooled features are calculated as:\n\u03a6\u0698 = Mean(\u0393) + Max(Tj), j\u2208 {1, ...,N/2(\nHere, Mean(\u0393) and Max(\u0393j) are permutation-invariant operations, ensuring that the order of neighbors does not affect the pooled features."}, {"title": "3.3.5 Aggregation Across Stages", "content": "The non-parametric feature encoder includes four stages, each producing pooled features \u0424. After processing through all stages, global pooling is applied to the results from each stage. The final feature vector F for the non-parametric feature encoder is obtained by concatenating the global mean and max features from all four stages:\nF = [Mean() + Max(3)]\ns=1\nThis formulation captures and aggregates spatial and feature information across multiple levels by combining the mean and max features from each stage."}, {"title": "3.4. Non-Parametric Classifier", "content": "To preserve the non-parametric nature of Point-GN, we adopt a similarity-based classification approach [43]. Given a test point cloud, its feature vector Ftest is compared to the feature embeddings Ftrain from the training set. The classifier computes a similarity score between the test and training embeddings, which is then used to assign a class label based on the closest matching features. This similarity-driven mechanism avoids the need for traditional parametric models, maintaining the flexibility and efficiency of the non-parametric framework."}, {"title": "3.4.1 Feature Representation and Label Embedding", "content": "Given a training set of M point clouds {Pm}M=1, each belonging to one of C categories, we first extract a global feature vector Fm for each point cloud Pm using the non-parametric encoder. The corresponding labels {Ym}m=1 are transformed into embedded label vectors Lm.\nThe process of storing these feature and label embeddings is shown in Fig. 3. The feature embeddings are stored in the global feature matrix Ftrain, and the label embeddings are stored in the label matrix Ltrain, both defined as follows:"}, {"title": "3.4.2 Similarity-Based Classification", "content": "For a test point cloud Ptest, the non-parametric encoder generates the feature vector Ftest. We compute the similarity between Ftest and the stored training features in Ftrain using the following equation:\nSim = Ftest Ftrain\nThe similarity scores in Sim are used to weight the corresponding label embeddings from Ltrain. The final classification logits are computed using the following activation function:\nylogits = exp(-\\gamma. (1 \u2013 Sim)) \u00b7 Ltrain\nHere, y is a scaling factor, and exp(-\u03b3\u00b7(1-Sim)) serves as the activation function adapted from Tip-Adapter [41], where higher similarity scores result in stronger contributions from the corresponding labels in Ltrain."}, {"title": "3.4.3 Classification Decision", "content": "The predicted class label is determined by applying an activation function to the logits, selecting the category with the highest value. In our case, we use a softmax activation for this final step:\nc = arg max(softmax(ylogits))\nThrough this similarity-based label integration, the classifier is able to effectively differentiate between various point cloud instances using a simple and efficient mechanism."}, {"title": "4. Experiments", "content": "In this section, we benchmark the performance of Point-GN against state-of-the-art methods for 3D shape classification. We conduct experiments on two widely recognized datasets: ModelNet40 [34] and ScanObjectNN [28]. These datasets were selected for their complementary characteristics: ModelNet40 comprises clean, synthetic"}, {"title": "4.1. Experimental Setup", "content": "We evaluate the performance of our Point-GN method on a system equipped with an NVIDIA RTX 4090 GPU. Although Point-GN is a non-parametric method and does not require traditional model training, the GPU significantly accelerates the inference process, allowing for efficient evaluation across the large and diverse ModelNet40 [34] and ScanObjectNN [28] datasets. This high-performance hardware ensures that our approach can handle the complexity and size of real-world 3D data, providing rapid evaluations during the benchmarking process."}, {"title": "4.2. Dataset Details", "content": "The ModelNet40 [34] dataset consists of 12,311 CAD models across 40 object categories, split into 9,843 samples for training and 2,468 for testing. This dataset is widely used for point cloud classification due to its clean, synthetic nature, providing a controlled environment for benchmarking.\nIn contrast, the ScanObjectNN [28] dataset presents a more challenging real-world scenario, with 2,902 samples across 15 object categories. Objects in ScanObjectNN are often occluded, cluttered, or contain background noise, providing a closer simulation to real-world 3D data.\nThe dataset is divided into three official subsets: OBJ-BG, which contains objects with background noise, OBJ-ONLY, with objects without background, and PB-T50-RS, featuring partial occlusions and transformations. These sub-"}, {"title": "4.3. Shape Classification on ModelNet40", "content": "We evaluate the performance of Point-GN on the ModelNet40 [34] dataset in Tab. 1. Point-GN achieves an accuracy of 85.3%, demonstrating strong performance in synthetic 3D shape classification. This result highlights Point-GN's ability to effectively capture both local and global geometric features, all while maintaining a minimal model complexity.\nWhen compared to the non-parametric Point-NN [43], Point-GN shows a +3.5% improvement in accuracy, despite having zero trainable parameters. This demonstrates the effectiveness of our approach in extracting meaningful features without relying on large parameter counts. Furthermore, Point-GN achieves an inference speed of 301 samples/second (measured on our system), ensuring high efficiency for real-time applications. This is especially notable when compared to parametric models like PointMLP [11], which requires 12.6M parameters to achieve a slightly higher accuracy of 94.1%.\nThe combination of competitive accuracy and exceptional computational efficiency makes Point-GN an attractive choice for resource-constrained environments, where real-time performance and minimal model complexity are crucial."}, {"title": "4.4. Shape Classification on ScanObjectNN", "content": "On the ScanObjectNN [28] dataset (Tab. 2), Point-GN demonstrates superior performance in real-world scenarios, outperforming most existing methods across all three official splits: OBJ-BG, OBJ-ONLY, and PB-T50-RS. Notably, Point-NN [43], the only other non-parametric method for comparison, is significantly outperformed by Point-GN. In the most challenging split, PB-T50-RS, Point-GN achieves a +21.5% improvement in accuracy over Point-NN, highlighting its robustness to occlusions and background noise in real-world data.\nRather than using different setups for each split, we adopted a single configuration for all three splits and aimed to find the best average performance. The average accuracy across the three splits for Point-GN is 85.89%, demonstrating its consistency across varying conditions. This approach ensures we identify a setup that works best on average, rather than optimizing separately for each split.\nWhen compared to fully trained models (shown in blue in Tab. 2), Point-GN consistently outperforms most, achieving higher accuracy across all splits. In particular, it surpasses models like PointNet [20], PointNet++ [21], and PointMLP [11] by substantial margins. The difference between Point-GN and the top-performing model, PointMetaBase-S [8], is less than 2%, indicating that Point-GN competes closely with state-of-the-art methods despite having zero trainable parameters.\nThese results underscore the power of Point-GN's non-parametric design, which offers competitive performance while maintaining computational efficiency and avoiding the complexity of parameter-heavy models."}, {"title": "4.5. Few-shot Classification on ModelNet40", "content": "In the few-shot classification task on ModelNet40 [34] (Tab. 3), Point-GN outperforms existing methods, demonstrating the best performance in both the 5-way and 10-"}, {"title": "4.6. Computational Complexity Analysis", "content": "We evaluate the computational efficiency of Point-GN in Fig. 4, where we compare its inference speed to Point-NN [43] by running both models on our system. Despite having zero trainable parameters, Point-GN demonstrates significantly faster inference across both the ScanObjectNN [28] and ModelNet40 [34] datasets. This efficiency makes Point-GN ideal for real-time applications, such as autonomous driving and robotic perception.\nIn contrast, Point-NN [43], while also non-parametric,"}, {"title": "4.7. Ablation Study", "content": "We conducted an ablation study to evaluate the impact of different model configurations on the performance of Point-GN. Specifically, we examined the effect of four key factors: the dimension of Gaussian Positional Encoding (GPE), the number of neighbors k in the k -Nearest Neighbors algorithm, the standard deviation o of the Gaussian function, and the number of stages in the model.\nEffect of GPE Dimension. The dimensionality of the Gaussian Positional Encoding (GPE) significantly affects model performance. As shown in Fig. 5, increasing the dimension up to 27 improves accuracy, reaching a peak of 85.3%. However, beyond 45, further increases in dimension result in diminishing returns, with accuracy slightly declining. This suggests that while higher dimensions can capture more complex features, excessively high dimensions add unnecessary complexity without substantial performance gains.\nImpact of Sigma (\u03c3). The o parameter of the Gaussian kernel (\u03c3) determines the degree of locality in feature aggregation. For the ModelNet40 dataset, \u03c3 = 0.3 and \u03c3 = 0.35 yield the highest accuracy, effectively preserving both local and broader spatial contexts. On the other hand, For the ScanObjectNN dataset, we find that o = 0.4 is most effective, achieving the best accuracy by maintaining a balance between capturing fine-grained details and minimizing noise. Smaller values (\u03c3 < 0.3) fail to aggregate sufficient local features, while larger values (\u03c3 > 0.4) introduce excessive smoothing, negatively impacting performance. These trends are illustrated in Fig. 5.\nEffect of Number of Neighbors (k). The number of neighbors k used in the k-NN algorithm also plays a critical role in model performance. Our experiments show that k = 120 offers the best trade-off between computational efficiency and accuracy. Smaller values, such as k = 70, fail to capture sufficient local context, while larger values, like k = 130, introduce irrelevant neighbors that confuse the model, hindering its ability to discern fine-grained geometric features. The performance comparison is shown in Fig. 5.\nEffect of Number of Stages. Increasing the number of stages in the model generally improves accuracy. The 4-stage configuration achieves the highest accuracy of 85.3%, suggesting that deeper models are better equipped to capture complex spatial relationships and improve classification performance."}, {"title": "5. Conclusion", "content": "In this paper, we introduced Point-GN a novel non-parametric network for 3D point cloud classification that combines Gaussian Positional Encoding (GPE) with non-learnable components such as FPS and KNN to efficiently capture both local and global geometric structures. By eliminating the need for learnable parameters, Point-GN provides a highly efficient and lightweight model suitable for real-time and resource-constrained environments. Experimental results on ModelNet40 and ScanObjectNN demonstrate that Point-GN achieves competitive accuracy, outperforming existing non-parametric methods while requiring zero trainable parameters and delivering fast inference speeds.\nFor future work, we plan to extend the non-parametric framework of Point-GN by incorporating additional point cloud features and exploring its potential in more complex 3D tasks, such as semantic segmentation and object detection, further enhancing the model's versatility and real-world applicability."}], "equations": ["Encoder(P) = F \u2208 Rd", "Classifier(F) = y = (Y1, Y2, ...,YC)", "c = arg max Yi", "Vx(xi, Uj) = exp(-\\frac{|| xi - vj ||^2}{2\\sigma^2})", "Vy (Yi, vj) = exp(-\\frac{|| Yi - vj ||^2}{2\\sigma^2})", "Vz(Zi, Uj) = exp(-\\frac{|| Zi - vj ||^2}{2\\sigma^2})", "V(Pi) = [\u221ax(xi, vj), Vy (Yi, vj), Vz (zi, vj)]j=1", "{Pj, (j)}=1 = FPS ({pi, Y(Pi)}=1)", "idxj = KNN(Pj, Pi)", "Nj = retrieve ({pi}=1, idxj) \u2208 RK\u00d73", "\u0393j = retrieve ({\\(pi)}=1, idxj) \u2208 RK\u00d7(V\u00d73)", "\u0393; \u2190 \u0393; + (P\u0632) (\u0632(", "\u03a6\u0698 = Mean(\u0393) + Max(Tj), j\u2208 {1, ...,N/2(", "F = [Mean() + Max(3)]\ns=1", "Ftrain =  Fm", "Ltrain = Lm", "Sim = Ftest Ftrain", "ylogits = exp(-\\gamma. (1 \u2013 Sim)) \u00b7 Ltrain", "c = arg max(softmax(ylogits))"]}