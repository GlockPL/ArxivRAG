{"title": "Hierarchical Reinforcement Learning for Temporal Abstraction of Listwise Recommendation", "authors": ["Luo Ji", "Gao Liu", "Mingyang Yin", "Hongxia Yang", "Jingren Zhou"], "abstract": "Modern listwise recommendation systems need to consider both long-term user\nperceptions and short-term interest shifts. Reinforcement learning can be applied\non recommendation to study such a problem but is also subject to large search space,\nsparse user feedback and long interactive latency. Motivated by recent progress in\nhierarchical reinforcement learning, we propose a novel framework called mccHRL\nto provide different levels of temporal abstraction on listwise recommendation.\nWithin the hierarchical framework, the high-level agent studies the evolution of\nuser perception, while the low-level agent produces the item selection policy by\nmodeling the process as a sequential decision-making problem. We argue that\nsuch framework has a well-defined decomposition of the outra-session context\nand the intra-session context, which are encoded by the high-level and low-level\nagents, respectively. To verify this argument, we implement both a simulator-\nbased environment and an industrial dataset-based experiment. Results observe\nsignificant performance improvement by our method, compared with several well-\nknown baselines. Data and codes have been made public.", "sections": [{"title": "Introduction", "content": "In recent years, hybrid Recommender systems (RS) have become increasingly popular to overcome\ninformation redundancy and have been widely utilized in a variety of domains (e.g. products,\narticles, advertisements, music, and movies). Hybrid RS selects a list of\ncontents (usually called 'items') from overwhelmed candidates for exhibition to meet user preferences,\nwith expected clicks, dwell times or purchases increased. Although with substantial achievement,\nmore advanced recommendation techniques are always necessary because of the complexity of\nbusiness scenarios, including listwise, cold-start, heterogeneous contents, spatiotemporal-aware\n(STA) recommendations, etc. Among these, the listwise recommendation is always an important\nissue, not only due to the wide application of Top-K recommendation or feed\nrecommendation but also the computational complexity with consideration of\nmutual-item influence, and position bias.\nReinforcement Learning (RL) is a natural, unified framework to learn interactively with the environ-\nment and maximize the global and long-term rewards, which highlights some promising solutions of\nlistwise ranking. According to the formulation manner of the Markov Decision Process (MDP), we\nconclude that most such works can be classified into two main categories:\n1. Define the user perception/preference as state and the entire recommendation list as action,\nand model the user interest shift as state transition"}, {"title": "Related Work", "content": "RL has been widely used in recommendation tasks, while their definitions of MDP can be diversified\ninto two categories. The first categories models the user preference as state and recommendation\nthe entire item list as action, such as in which the state transit depicts the user interest shift. On the other hand, the second category is\nto model the ranking steps of items as state, and the selection of the next favorite item is action.\nSuch methods include which aims to model the mutual influence\nbetween items and listwise bias. The intuition of combining the advantages of two types of MDPS\nmotivates our idea of mccHRL.\nThere are also RL works aiming to fix some special recommendation issues, such as location or\nPOI-based information cold start heterogeneous items\nuser long-term engagement or fairness of\nitems Our work shares similar interests to the POI-based recommendation with However, most of them work on the next-location\nrecommendation given sparse spatial information, to learn a reasonable user-location relationship\nmatrix, while we aim to provide an end-to-end solution of session recommendation with spatial\ninformation implicitly considered in our recommending policy."}, {"title": "Hierarchical Reinforcement Learning", "content": "Hierarchical Reinforcement Learning (HRL) has a hierarchical structure of RL layers in order to solve\nmore complex tasks, reduce the searching space, provide different levels of temporal abstractions,\nor deal with sparse rewards. HRL methods can be classified into two categories by the coordinated\nway between different levels. The first is the goal-conditional framework in which a high-level agent learns\nthe goal to drive the low-level agent; the second is sub-task discovery framework, the high-level\nagent might provide some options (or skill) to reduce the search space of low-level agent Our methodology belongs to the first category in which the high-level\nagent studies the user perception embedding as the low-level agent decision basis.\nHRL methods also vary in hierarchical structures. Choices include multi-layer DQN multi-layer policy or multi-layer actor-critic Our\nmodel is similar to hierarchical actor-critic (HAC) but reduces its complexity for practical edge\ndeployment."}, {"title": "HRL-based Recommendation", "content": "There are also attempts to apply Hierarchical Reinforcement Learning (HRL) on recommendation or\nsearch Takanobu et al. uses the\nHigh-Level RL as the heterogeneous search source selector; similarly, High-Level RL in HRL-rec\nXie et al. is the content channel selector. MaHRL uses the High-Level RL\nto model the user perception state, to optimize multiple recommendation goals. All of these methods\ndefine the Low-Level RL as the item selector, which is similar to our framework. We have a similar\ndefinition of High-Level RL with MaHRL, however, our difference includes that (1) our target is\nto improve a session-based, STA recommendation CTR performance instead of multiple labels; (2)\nwe provide a natural formulation of temporal abstraction to solve the sparse reward issue of listwise\nrecommendation; (3) we deploy the Low-Level part of HRL on the edge side to further improve the\nmethodology throughput."}, {"title": "Edge-based Recommendation", "content": "Edge Computing (might also be named Edge Intelligence, Edge AI, or on-device AI), in contrast\nto Cloud computing, has been widely studied recently years Efforts have been\nmade on applications including IoT, 5G, and auto-driving. However, this field still is at its early\nstage with most efforts focusing on the lightweight and deployment of edge models For example, EdgeRec works on the split-deployment which places the memory-\nconsuming embedding module on the cloud while the lightweight recommender is inferenced on the\ndevice.\nThere has been increasing attention on Edge-Cloud collaboration, either privacy-primary such as\nFederated Learning, or efficient-primary. We are interested in efficient primary methods, to improve\nthe recommender's personalization. Such efforts including COLLA DCCL Yao\net al. For example, COLLA designed the cloud model as\na global aggregator distilling knowledge from many edge models; MC2-SF proposes a slow-fast\nlearning mechanism on RS, with the bidirectional collaboration between Edge and Cloud. However,\nedge-based RL has not been widely studied so far. Instead, RL is often utilized as an aside system to\nEdge Computing, to help service offloading, task scheduling, or resource allocation Therefore, our work can be regarded as the pioneering study on edge-based RL, with the slow-fast\ncollaborative study mechanism between cloud and edge, similar to Madan et al. To differentiate from the traditional edge AI methods, we use the notation 'mobile' instead of\n'edge' since most impressions of internet RS are on the users' mobile devices (smartphones or pads)."}, {"title": "Preliminary and Problem Formulation", "content": "This section illustrates key concepts of our approach, including the system configuration, formulation,\nand detailed structure of mccHRL, as well as some necessary preliminaries."}, {"title": "Listwise Recommending Scenario", "content": "There is assumed to be a listwise recommendation task that exhibits $K$ items upon each user query,\nin which the session length $K$ is a pre-determined, fixed integer. User clicks response to exposed\nitems within a session is then $c = \\{c_0, c_1,\\ldots,c_K\\}$ in which each $c_k$ is a binary variable, $k \\in [0, K]$.\nThe recommendation objective is generally to maximize the global session-wise click-through rate\n(CTR).\nWe first encoder each item into the embedding vector $e \\in \\mathbb{R}^L$. Then a session recurrent encoder\n(SRE) is employed to encode the session's historically exposed item sequence (We simply use the\nnotation 'historical sequence' in the following contexts for briefly) into the same embedding space,\n$1 := \\{e_0, e_1,\\ldots,e_k\\} \\in \\mathbb{R}^L$. More details of SRE will be introduced in Section 4.1.\nFor personalization purpose, the user general profile, the user on-device feature, and the STA\ninformation are encoded into $u$, $m$ and $c^o$. respectively. The superscript $o$ denotes the outra-session\ncontext, to differentiate from the intra-session context ($c^i$) which is encoded inside the actor and will\nbe mentioned in the later contexts."}, {"title": "Reinforcement Learning", "content": "Reinforcement Learning (RL) is an interactive learning technology that is generally built on Markov\nDecision Process (MDP). MDP can be represented with a four-tuple of $M_t := (S, A, R, T)$, where\n$S$ is the set of state $s$, $A$ is the set of action $a$, $R$ is the set of reward $r$, and $T(s_{t+1}|s_t, a_t)$ is the\ntransition function of $s_{t+1}$ after executing $a_t$ on $s_t$. The subscript $t$ of $M$ works as the step indicator.\nRL optimizes a long-term objective which is defined as the discounted accumulated rewards $J =$\n$\\sum_{t=0}^\\infty \\gamma^t r_t$, where $\\gamma \\in [0, 1)$ is the discount factor. The goal of reinforcement learning is to learn a\npolicy $\\pi(a|s)$ which maximizes $J$. In this work, we employ the classical model-free and off-policy\nalgorithm called the Deep Deterministic Policy Gradient (DDPG) method Lillicrap et al. to\nsolve this problem."}, {"title": "Deep Deterministic Policy Gradient", "content": "In this paper, we implement RL with a classical model-free and off-policy algorithm called the Deep\nDeterministic Policy Gradient (DDPG) method Lillicrap et al. Here we briefly review its\nalgorithmic details which is the basis of later derivation.\nThe DDPG is borrowed to get the recommended score of the item (i.e., action) and the long-term\nestimate return (i.e., Q-value).\nDDPG has a typical actor-critic architecture in which the actor is the parametric policy network\n$\\pi_{\\theta}(a|s)$ while the critic is a state-action value function\n$Q_w(s, a) = \\sum_{i=t}^\\infty \\gamma^{i-t}E(r_i|S_t = s, a_t = a)$ (1)\nin which $\\theta$ and $w$ are trainable parameters. DDPG also keeps a target actor and a target cricic network\nwith their trainable parameters $\\theta'$ and $w'$ as instead.\nWith target networks fixed, $w$ can be updated by minimizing\n$y_t = r_t + \\gamma Q_{w'}(s_{t+1}, \\pi_{\\theta'}(s_{t+1}))$ (2)\nthen $w$ is updated by minimizing\n$L = E_{s \\sim d^{\\pi}} (r_t + \\gamma Q_{w'}(s_{t+1}, \\pi_{\\theta'}(s_{t+1})) - Q_w(s, \\pi_{\\theta}(s)))^2$ (3)\nwhich is the famous Bellman Equation. $\\theta$ is updated by the policy gradient\n$\\nabla_{\\theta}J = E_{s \\sim d^{\\pi}}\\nabla_a Q_w(s, a)|_{a=\\pi_{\\theta}(s)}\\nabla_{\\theta}\\pi_{\\theta}(s)$ (4)\nwhere $d^{\\pi} (s)$ is the discounted distribution of state $s$ sampling by policy $\\pi$. Target network parameters\nare softly updated by\n$\\theta' \\leftarrow \\tau \\theta' + (1 - \\tau) \\theta$\n$w' \\leftarrow \\tau w' + (1 - \\tau) w$ (5)\nin a pre-defined time interval with $\\tau \\in (0, 1)$.\nThen $\\theta$ can be updated as\n$\\theta \\leftarrow \\theta + \\eta E_{s \\sim d^{\\pi}} [\\nabla_{\\theta}\\pi_{\\theta}(s)\\nabla_a Q_w(s, a)|_{a=\\pi(s)}]$ (6)\nwith $\\eta$ as the learning rate."}, {"title": "Gated Recurrent Unit", "content": "Gated Recurrent Unit (GRU) is a special type of RNN that can sequentially capture the item-\ndependency impacts. It is found that GRU has better stability with fewer parameters than general\nGRN, therefore is usually used in recommendation studies The core logic of GRU is the following:\n$z_t = \\sigma(W_z \\cdot [h_{t-1}, x_t])$\n$r_t = \\sigma(W_r \\cdot [h_{t-1}, x_t])$\n$\\hat{h_t} = tanh(W_h \\cdot [r_t h_{t-1}, x_t])$\n$h_t = (1 - z_t) h_{t-1} + z_t(\\hat{h_t})$ (7)\nwhere $z_t, r_t$ are update and reset gates."}, {"title": "The mccHRL Framework", "content": "Recalling the discussion in Section 2, the listwise ranking problem can be defined as two MDPs in\nvaried time scales:\n1. The user state evolution at the real physical time $t$, with the notation of MDP as $M_t$;"}, {"title": "Method", "content": "This section introduces the training mechanisms of mccHRL, with summarizing important\nsymbols frequently used in the paper. We inherite the famous actor-critic RL architecture. However,\nhere we let the HRA to be the critic and the LRA to be the actor solely, to reduce the computational\ncomplexity."}, {"title": "Session Recurrent Encoder", "content": "The session recurrent encoder (SRE) is an incremental function that aims to provide a session\nembedding with arbitrary session length and can be applied both in the learning and planning stages.\nHere we utilize the famous GRU structure, with $e_k$ as the input:\n$l_{k+1} = SRE(l_k, e_k), k = 0,\\ldots, K - 1$ (9)\nthe encoded session embedding $l_k$ is the latent state of GRU which has the same dimension of $e_k$."}, {"title": "High-Level Agent", "content": "The HRA performs as the critic in our mccHRL. It is modeled with a parametric value function\n$Q_w(s^h, a^h)$.  Historical click sequences\n$\\left\\{e_t\\right\\}$ are extracted from the cloud database, each encoded by SRE and then processed by a DIN\nstructure with the last $l$ as the target tensor. The DIN output concatenates with $u$\nand $c^o$ and goes into first an FM layer first then an MLP. The last activation function of MLP is tanh\nwhich generates the Q-value. At the beginning of a session service, HRA also transmits $u$, $c^o$, and\nSRE to LRA, as indicated by Figure 3. The output of $Q_w$ then provides an implicit function of LRA\ngoals $g$, as indicated by Arrow III."}, {"title": "Low-Level Agent", "content": "shows the actor-network structure on the right, which implements our LRA policy func-\ntion $\\pi_{\\theta}(a^l|s^l)$. Since the size of the edge model is significantly limited by the on-device hard-\nware, we design a relatively simple network structure. The local intra-session exposed item\nsequence $\\{e_0, e_1,\\ldots,e_k\\}$ is again encoded by SRE, while the on-device user profile sequence\n$\\{m_0, m_1,\\ldots, m_k\\}$ is encoded by another GRU unit. Concatenation of the last latent states of SRE\nand GRU forms the intra-session context $c^i$. We further concatenate $u$, $c^o$ and $c^i$ together and feed\ninto a MLP which generates $\\hat{e}_k \\in \\mathbb{R}^L$, as $a^l$. During the training stage, gradient back-propagation of\nSRE in the actor is blocked to reduce the computational complexity.\nDuring the service stage, $\\hat{e}_k$ is dot multiplied with the embeddings of unselected items and generates\nscalar scores, then greedily selects the optimal item:\n$k = arg \\max\\{\\hat{e}_k e_{k'}\\}, k' = k + 1,\\ldots, K'$ (10)\nin which $K'$ is the remained item candidate set size (and can be assumed to be larger than $K$). The\n$k$th item is appended to the end of recommendation, then the step moves forward from $k$ to $k + 1$\nuntil all $K$ items are determined."}, {"title": "Mechanism", "content": "reviews the aforementioned interactive pattern of HRA and LRA. HRA operates at the real\nworld time $t$, observes the current user states, and provides the user favorable preference by the\nHigh-Level action at once a session service is requested. The HRA training is executed on the cloud\nside based on all users' logs, and policy updates once the session ends with the real, non-delayed"}, {"title": "Training with Generic Environment", "content": "Algorithm 1 shows the training mechanism of our mccHRL. Considering the listwise mechanism,\nWe assign the Low-Level actor an on-policy strategy and the High-Level critic an off-policy strategy.\nSimilar to DDPG, we also have a target critic and a target actor."}, {"title": "Training with Offline Data", "content": "Algorithm 1 is an interactive learning method with both cloud and edge environments. Unfortunately,\nit is often not practical for direct online deployment and learning on large-scale industrial systems,\nespecially for online learning on users' mobile devices. Therefore, it is of great importance to develop\nan algorithm that can learn from the offline data, or at least use the offline training as a warmup start.\nIn this paper, we assume offline data can be retrieved and reformulated in the temporal order, with\neach line in the form of $\\left[s, a, r, \\{e_k\\}, \\{m_k\\}, s_{t+1}^l, a_{t+1}^l\\right]$.\nRL study on such data calls for a special research direction called offline RL, or batch-constrained\nRL, which learns an optimal policy from a fixed dataset by implementing the out-of-distribution\ncorrections for off-policy RL Since our HRA has an off-policy strategy, we could\nemploy similar ideas to train the HRA critic offline. Here we refer to the conservative Q-learning\n(CQL) Kumar et al. , to have a regularizer in the TD loss of the Q-function:\n$L_{off}(s, a, \\pi_{\\theta}) = E_{a \\sim \\pi_{\\theta}} [Q_w(s, a) - \\frac{\\exp Q_w(s, a)}{\\sum \\exp Q_w(s, a)}]$ (11)\nin which $Z$ is the normalizing factor of $Q$ and $\\pi_{\\beta}$ is the sampling policy in data. This formulation\nindicates that we also minimize the soft-maximum of $Q$ at the current solved policy ($\\pi_{\\theta}(a|s)$) w.r.t\nthe sampling policy $a^h$, therefore the learning becomes 'conservative'. According to Algorithm 1 in\nsuch a trick can be also applied in the actor-critic framework. Here we present\nour offline version of Algorithm 1 which is specified in Algorithm 2."}, {"title": "Experiments", "content": "As an on-policy RL-based methodology, our mccHRL should be evaluated by online experiments\nand business performance indicators such as CTR. However, the industrial recommendation system\nis complex and connected with enormous impressions and incomes, which makes thrugh online\nexperiment difficult. As a remedy, we design two offline evaluation strategies. We demonstrate the\neffectiveness of mccHRL by both offline and live experiments, as well as ablation and sensitivity\nstudies."}, {"title": "Experimental Environments", "content": "In order to evaluate our RL-based method, ideally we need a simulator with both cloud and edge\nenvironments, which are interactively learnable by RL agents. Unfortunately, to the best of our\nknowledge, there is no such test-purpose cloud-edge request simulator. Instead, we provide two\nremedy solutions for offline experiments, before the online deployment:\n1. An RS simulator that is generated by a small-scale public dataset. We arbitrarily define\nsome spatial and temporal related features as 'edge features' which is not observable by the\ncloud. Other features are manually implemented with some transmission latency between\nthe edge and the cloud. The average reward during the test stage can be used as a metric for\nsuch a simulation-based system."}, {"title": "Simulator-based Experiment.", "content": "Movielens 3 is a user-movie rating dataset including the behavior data, the user feature, and item\nfeature data. Samples are labeled with users' ratings on movies from 1 to 5.\nBased on the MovieLens-100k dataset, We build a simulator based on the methodology introduced in\n with the averaged ratings behaving as the RL rewards. The average rating is 3.53\nover all samples. Among the user features, the spatial features including 'occupation' and 'zip code'\nare set as edge features, which can not be accessed by the cloud. We further assume the user cloud\nsequence has a fixed latency with edge sequence, i.e., the cloud sequence is delayed by 6 items the\nedge sequence. By these configurations, we build an approximated mobile-cloud simulator, with an\nexplicit transmission latency."}, {"title": "Dataset-based Experiment.", "content": "Because of the industrial limitations, interactive training of on-device RL is temporarily impractical.\nInstead, we conduct an industrial-scale experiment based on the offline dataset with realistic on-\ndevice features. Here we employ a content-based feed recommendation dataset 4, sampling from\nthe Taobao front-page content-base recommendation and has been declared open-sourced in Tianchi.\nOn-device features are aligned with the recommended item and corresponding click labels, and\nthe data is reorganized into a session-wise form. The recommendation objective is to maximize\nthe click-through rate (CTR). We denote user app-related behaviors sampled on the device as $m$,\nincluding app IDs, stay-times, and LBS information in the granularity of districts. The spatiotemporal\ninformation in $c^o$ including POI, province, city, day, hour, and workday/weekend labels. We further\nassume the user cloud sequence has a fixed latency with edge sequence, i.e., the cloud sequence is\ndelayed by 10 items the edge sequence."}, {"title": "Evaluation Metrics", "content": "We define different evaluation metrics according to the characteristics of two experimental environ-\nments:\n\u2022 Simulator-based Experiment.: the expected reward indicates the policy performance in an\ninteractive environment. The rating employed for evaluation is retrieved from the most\nsimilar occurrence (with a cos similarity by user and item embeddings) in the dataset.\nSuch test stage is repeated for 50 rounds and the final averaged metric is reported. We\nevaluate it by the average rating during the test stage of the experiment, which is denoted by\nS-<rating>.\n\u2022 Dataset-based Experiment.: the general solution is to convert the online CTR maximization\ninto an offline binary classification problem, i.e. predict if the user clicks or not. We use\nArea Under the ROC Curve (AUC) to evaluate this classification performance, in which we\nemploy the actor's servicing score as the prediction and the ground-truth click record as the\nlabel. We name it as D-AUC."}, {"title": "Implementation Details", "content": "Table 3 shows some hyper-parameters of two experiments. Further implementation details are listed\nbelow."}, {"title": "Simulator-based Experiment.", "content": "MLP in critic has hidden units of [32, 16, 1] with the last activation as tanh; while MLP in actor\nhas hidden units of [128, K]. The latent state dimension is 16 for both GRU and SRE components.\nWe use an Adam optimizer to interactively train the critic and actor, with a learning rate of 0.0001.\nPerformance is evaluated by iterating all users among the training set, while for each ground truth item,"}, {"title": "Dataset-based Experiment.", "content": "The DIN layer of critic uses a multi-head target attention structure, with the last session embedding\nvector as the target. The attention mode is cosine. The deepFM layer of critic is composed of linear\nterms and 2nd-order cross terms. MLP in critic has hidden units of [128, 64, 32, 1] with the last\nactivation as tanh; while MLP in actor has hidden units of [64, 32, K] with all activation as relu.\nThe latent state dimension is 16 for both GRU and SRE components. We use the Adam optimizer\nwith a learning rate of 0.0001, 1st-regularization weight of 0.0001 and 2nd-regularization weight of\n0.00001. Data on the last day (day = 10) is classified into the test set while the other consists of the\ntraining set. To help the training converge, we first conduct 3 epochs of supervised learning, with the\nbinary cross-entropy loss of the click label. RL then conducts 2 more epochs. The training ends with\nconvergence, costing about 17000 steps. Training is ruuning on an industrial-scale, parallel training\nplatform, with each experiment allocated with 8 GPUs and 20000M RAM."}, {"title": "Baselines", "content": "The following experimental baselines are employed to compare with our mccHRL:\n\u2022 Random: the policy picks the recommended item from the candidates set randomly. Only\nimplemented in the simulator experiment.\n\u2022 DIN: the Deep Interest Network Zhou et al. , as a standardized solution of industrial\npointwise CTR model.\n\u2022 DIN(ideal): the DIN model (which is deployed on the cloud) can have full access to edge\nfeatures with exact zero latency. Therefore, it indicates an unrealistic performance upper\nbound for a generic supervised model. Only implemented in the industrial experiment.\n\u2022 GRU4rec: a session-based method with a latent state calculated by GRU therefore considers the intra-session context.\n\u2022 LIRD: a DDPG-based RL listwise ranking framework\n\u2022 MC2-SF: a slow-fast training framework based on moble-cloud collaboration Chen et al.\nwith mobile features incorporated and considered."}, {"title": "Offline Results", "content": "We compare the offline experimental performance of mccHRL and baselines in Table 4. mccHRL\noutperforms other baselines in both experiments. Specifically, the improvement of mccHRL on the\nindustrial experiment is more evident, since this experiment has substantial on-edge features, and\nuser behaviors are more responsible for session-wise exhibition and industrial computational latency.\nComparison with baselines could highlight more information. For example, LIRD is the second-best\nmethod in the simulator experiment since its RL nature fits the interactive environment. However, it"}, {"title": "Ablation Study", "content": "Compared with previously published works, our mccHRL has several novel components therefore\ntheir ablation studies are necessary. Table 4 also illustrates the results of the following attempts:\n\u2022 mccHRL(wo edge): the mccHRL approach with only cloud-based features.\n\u2022 mccHRL(wo actor): the mccHRL but without the actor. We simply use $\\arg \\max Q$ to\ndetermine the favorable item selection.\n\u2022 mccHRL(wo critic): the mccHRL but without the critic. An item selection policy is trained\noffline and is deployed on the device for service.\nNot surprisingly, mccHRL still has the best performance, suggesting that the critic, the actor, and\nedge-based features are all crucial. Note that mccHRL(wo critic) has the worst performance since it\nsuffers from the sparse reward problem."}, {"title": "Sensitivity Study", "content": "There is always a trade-off between performance and speed for the edge-based model, due to limited\nresources on the edge side. To have a clear picture, we choose an important factor, the user behavior\nsequence length kept by the edge model, to have sensitivity analysis. We inspect its impact from the\nfollowing two aspects:\n1. The latency between cloud and edge sequence update. Since the detailed value of time\nlatency is highly subject to the cloud and edge real-time conditions, here we pick an\napproximate number, the number of items such that cloud is behind the edge sequence. With\nthis number larger, LRA could maintain more information but with a more computational\ncost.\n2. The length of behavior sequence studied by $s^l$ in LRA. If this number is larger, LRA could\nprovide more historical information to policy but the model size is expected to increase.\nshows the sensitivity results of the above two factors. Not surprisingly, the model per-\nformance deteriorates as latency becomes larger according to Figure 4 (Left), indicating that the\nedge model memory is a positive factor. However, improving the model memory will challenge the\ndeployment ability on the device, as indicated by Figure 4 (Right). Based on our practical experience,\nthe edge model should generally be no bigger than 3.5MB to avoid unreasonable mobile overhead,\nwhich indicates our current choice of $\\hat{N}^l = 50$ is almost optimal."}, {"title": "Live Experiment", "content": "We deploy our algorithm on a world-leading mobile application and execute an A/B live test. The\nexperiment lasts a week. We randomly select a pool of users from different geological areas to\nconduct our mobile-based deployment. Compared with the live baseline, we observe the global CTR\nincreased by 1.2% and the total views increased by 1.5%."}, {"title": "Discussion and Limitations", "content": "This work is motivated by decoupling the two time scales of the listwise recommendation: the\nphysical time step (for user preference update) and the virtual step (for the ranked item selection).\nOur methodology adopts the fast-slow system by learning the above two mechanisms simultaneously\nby the HRL framework. We also take advantage of the edge computing technique by implement the\nlow-level agent on the mobile device side, in which the localized and STA user information can be\nconsider and feedback before uploading to the cloud. Therefore, our method provides the theoretical\noptimal solution for such recommending scenarios.\nHowever, one potential drawback is the extra computational burden brought by the mccHRL frame-\nwork. Here we perform a simple time complexity analysis for a brief discussion. Given the physical\ntime steps as T and the virtual time steps as K (the session length), inference of a generic pointwise\nrecommendation model requires $O(K * T)$, in which the model is called with the user and item\npair independently. In our mccHRL, the time complexity would be $O(K^2 * T)$ since we need to\ncalculate the session contextual information recurrently. When K is too large (e.g. larger than 100),\nmccHRL might be inefficient and impractical. Fortunately, in the industrial implementation, the user\nrequest is usually divided into sequential page requests. This solution reduces the actual K therefore\ncould alleviate the efficiency issue. The mobile-device transmission could also be a problem which is\nalready discussed in Section 5.7.\nAnother important potential improvement is to consider the user's instant response within a session and\nmake immediate adjustments for ranking results. Such considerations will add some user-interactive\nrewards for Low-Level RL, and our method then becomes a unified framework of cloud-based ranking\nand mobile-based reranking, which is a promising future direction."}, {"title": "Conclusion", "content": "In this paper, we build a unified framework to deal with the user preference mode and the listwise\nitem ranking simultaneously, by a novel hierarchical RL methodology. We reinforce the state\nMarkovness by edge features consideration and design a mobile-cloud collaboration mechanism. The\ngoal-conditional mechanism is utilized to synchronize the user preference solved by high-level RL\nto the low-level RL. The outra- and intra-session contexts are decoupled and studied in two time\nscales. We implement both a simulator and an offline industrial dataset to verify our methodology,\nbefore the formal live experiment. Our study shed some lights on the applications of HRL on listwise\nrecommendation, and future studies can be developed to further implement RL interactively in large\nindustrial environments."}]}