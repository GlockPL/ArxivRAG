{"title": "SEG:Seeds-Enhanced Iterative Refinement Graph\nNeural Network for Entity Alignment", "authors": ["Wei Ai", "Yinghui Gao", "Jianbin Li", "Jiayi Du", "Tao Meng", "Yuntao Shou", "Keqin Li"], "abstract": "Abstract-Entity alignment is crucial for merging knowledge\nacross knowledge graphs, as it matches entities with identical\nsemantics. The standard method matches these entities based\non their embedding similarities using semi-supervised learning.\nHowever, diverse data sources lead to non-isomorphic neigh-\nborhood structures for aligned entities, complicating alignment,\nespecially for less common and sparsely connected entities.\nThis paper presents a soft label propagation framework that\nintegrates multi-source data and iterative seed enhancement,\naddressing scalability challenges in handling extensive datasets\nwhere scale computing excels. The framework uses seeds for\nanchoring and selects optimal relationship pairs to create soft\nlabels rich in neighborhood features and semantic relationship\ndata. A bidirectional weighted joint loss function is implemented,\nwhich reduces the distance between positive samples and differ-\nentially processes negative samples, taking into account the non-\nisomorphic neighborhood structures. Our method outperforms\nexisting semi-supervised approaches, as evidenced by superior\nresults on multiple datasets, significantly improving the quality\nof entity alignment.", "sections": [{"title": "I. INTRODUCTION", "content": "Knowledge graph is a large-scale structured semantic\nknowledge base used to describe concepts in the physical\nworld and their interrelationships, aggregate information and\ndata, and link relationships on the Web into knowledge\n[1]-[8]. Knowledge graphs contain rich information such\nas entity attributes, semantic relationships, and entity types.\nThere are many large knowledge graphs, such as DBpedia\n[9]and YAGO [10], which have been widely used in natu-\nral language processing, information retrieval, and intelligent\nquestion answering. Due to the frequent inclusion of many\nentities and relationships in knowledge graphs, there is also a\nclose connection with scale computing. However, knowledge\ngraphs are inherently incomplete. If downstream applications\nneed support, integrating various KGs for knowledge fusion\neffectively solves this problem. Researchers have proposed\nmany entity alignment methods. People have gradually shifted\nfrom traditional TransE [11]\u2013[18] methods to deep learning-\nbased modeling of the graph structure of knowledge graphs.\nThe methods based on deep learning [19]\u2013[25] mainly follow\nthe learning embedding-similarity calculation-greedy matching\nmode, which is to use seed entities to automatically extract\nimplicit features of entities from different KGs using neural\nnetworks and then embed them into the same implicit space\nand calculate the similarity between different entity embed-\ndings. However, in actual knowledge graphs, due to the hetero-\ngeneity of data sources, entities usually exhibit non-isomorphic\nneighborhood structures, and people have begun to realize that\nmore is needed to consider entity embeddings for alignment.\n[26]-[30] considered the positive interaction between entity\nalignment and relationships in a semi-supervised context and\nmined relational information to assist in alignment. [31], [32]\nconsiders the simultaneous roles of entities and relationships.\nAlthough these scholars have attached varying degrees of\nimportance to the importance of relationships in alignment,\nmost methods still ignore two issues:\nFirst, the difference between entities and relations is ig-\nnored. In the mainstream methods, embedding-similarity-\nmatching, similarity methods can also be divided into two\ntypes: based on equivalent entities having similar attributes\n[33] and based on equivalent entities having similar adjacent\nentities. However, in these methods, the propagation of align-\nment information focuses more on the propagation from the\nsimilarity of entities to relations. Not only does it largely\nignore the adequate auxiliary alignment information of the\nrelational semantics itself, but there is also a problem of\ninformation loss. More importantly, it ignores the differences\nbetween entities and relations, which leads to inconsistent\ninformation. As shown in Fig 1, if only considering the\npropagation of similarity in entities, then Alan Turing and\nAlan Turing, Stephen Hawking, and Stephen Hawking can\nbe fully aligned, but in relations, alma-mater, and graduate,\nstudent and worker cannot be aligned. GCN-based frameworks\nalso use relations as weights [34] or information. However,\nnoise is inevitably mixed in when the same model is used to\ntrain the representation of entities and relations jointly. The\nintegration of relations into entity representation will cause\nthe entity representation to be blurred and smooth.\nSecond, there is an obvious problem in current research:\nover-emphasis on reducing the distance between positive sam-\nples while ignoring the distance between negative samples,\nespecially when facing pseudo-aligned entities with high sim-\nilarity; there needs to be more differentiated treatment of\nthese pseudo-entities. In cross-knowledge graph alignment,\neffectively dealing with the non-isomorphic neighborhood\ndifferences between positive and negative samples is a sig-\nnificant challenge. In order to overcome the limitations of\nexisting methods, this paper proposes a soft label propagation\nframework based on multi-source fusion combined with an\niterative process of seed enhancement. First, we conduct dual-\nangle modeling based on entities and relations and fuse the\nbest-matching relationship pairs selected in both entity and\nrelationship modes to generate soft labels. These soft labels\ncontain rich entity neighbor features and integrate auxiliary\ninformation of relationship semantics, avoiding the problem\nof auxiliary information loss. Secondly, the soft labels fused\nby multi-source information propagate alignment information,\nand the topological structure and neighborhood differences\nare effectively captured. On this basis, a bidirectional adap-\ntive weighted joint loss function is introduced, which not\nonly shortens the distance between positive samples through\ncommonalities but also widens the distance between high-\nsimilarity negative samples to varying degrees and more\ncomprehensively considers the commonalities and differences\nin the alignment process. Some contributions are as follows:\n\u2022 Obtaining soft labels from the perspectives of entities\nand relations can reveal shared entity features and mine\nthe rich information of the relationship semantics itself,\nthereby improving alignment accuracy.\n\u2022 The difference problem of non-isomorphic neighborhoods\nis solved by effectively shortening the distance between\npositive samples and using a weighted joint loss function\nto differentiate and distance negative samples.\n\u2022 Our method, tested on real datasets, significantly boosts\nmodel performance, proving highly feasible and valuable\nfor entity alignment across knowledge graphs.\nThe remainder of this paper is organized as follows: Sec-\ntion II reviews related work, Section III provides a detailed\ndescription of the proposed method, Section IV presents the\nexperimental setup and results, and finally, Section V offers a\nbrief conclusion."}, {"title": "II. RELATED WORK", "content": "In this part, we examine existing scholarly contribu-\ntions about entity alignment, encompassing conventional\ntranslation-based tactics as well as contemporary graph rep-\nresentational techniques that leverage Graph Convolutional\nNetworks (GCNs) [35] [36]."}, {"title": "III. PROPOSED METHOD", "content": "We introduce the SEG methodology, whose structural out-\nline is depicted in Fig 2. Entity encoding and alignment can-\ndidate generation.Soft label screening: The soft labels of the\ntwo modalities are fused, and the screening strategy is used to\nobtain the final accurate soft labels.Pruning when performing\nneighborhood aggregation based on soft labels: Propagate the\nalignment relationship in the soft labels to the neighborhood\nstructure of the candidate alignment.Bidirectional weighted\njoint mechanism: Use a weighted method to differentiate neg-\native samples to guide positive and negative sample learning\nbetween two knowledge graphs.Bidirectional iterative collab-\norative training.\nIn this study, we consider the scenario where two distinct\nknowledge graphs are provided as inputs. A knowledge graph\nis structured as KG = (E, R,T), where E denotes the set of\nentities, R represents the set of relations, and T is a collection\nof triples defined as {(e1,r,e2) \u20ac1,62 \u20ac E,r \u2208 R}. The\nsource knowledge graph is denoted by kG\u2081 = (E1, R1, T1);\nsimilarly, the target knowledge graph is denoted by KG2 =\n(E2, R2, T2). The set of seed alignment entities is given by\nAlignseed = {(u, v)|u\u2208 E\u00b9, v \u2208 E\u00b2, u \u2194 v}, indicating that\nentity u from KG\u2081 and entity v from KG2 are identical, with\n\u2192 signifying equivalence. To initiate the process, we employ a\npre-trained model to assign initial embeddings to the entities."}, {"title": "A. Graph representation learning", "content": "The Graph Attention Network serves as a feature extrac-\ntion mechanism, introducing an innovative neural network\nframework for processing data structured as a graph. This\narchitecture comprises an input layer, an intermediary hidden\nlayer, and a final output layer. Within each hidden layer, the\nrepresentation of a node is determined by its interaction with\nthe surrounding nodes' information, necessitating ongoing\nupdates to the hidden layer's state. The computation is based\non the following formula:\n$H^{(l+1)} =  H^{(l)}  . \u03c3 (\\sum_{e_{j} \\in N_{i}}  \u03b1_{ij} H_{e_{j}}^{(l)}),$ (1)\nwhere $H^{(l+1)}$ and $H^{(l)}_{e_{i}}$ represent the embedding of the\nentity at the l layer, l represents the number of layers, and $d^{(l)}$"}, {"title": "B. Soft Labels", "content": "represents the weight parameter of the l layer,\u03c3 represents the\nReLU activation function.\n$S_{ij} = \\frac{exp(S_{ij})}{\\sum_{H_{ek} \\in N(H_{ei})} exp(S_{ik})},$ (2)\n$S_{ij} =  \u03b1 t([H_{ei} || H_{ej}] \\odot R_{t}),$ (3)\n$\\sum_{(H_{ei}, R_{t}, H_{ej}) \\in T_{ij}}$\nwhere $T_{ij}$ represents the set of relation triplets with $e_{i}$ as\nthe head entity and $e_{j}$ as the tail entity, a is the attention pa-\nrameter, || are vector concatenation, \u2299 element multiplication\nand LeakyReLU respectively function.\n$R_{t} = \u03c3(\\frac{\\sum_{H_{e} \\in T_{t}} W_{w}^{H_{e}}}{\\|T_{t}\\|} , \\frac{\\sum_{H_{e_{i}} \\in W_{t}} H_{e_{i}}}{\\|W_{t}\\|}) ,$ (4)\nWhere $T_{t}$ and $W_{t}$ represent the size of the head entity set\nand the tail entity set connected by $R_{t}$, respectively, b is the\nattention parameter.\n$H_{e_{i}} = W_{n}.n(H_{e_{i}}) + b^{2}.$ (5)\nw and b are trainable weight matrices, and their output\nmatrices are represented as $H_{e_{i}}$.\nAlignment candidates. After obtaining the embedding repre-\nsentation of the entity through GAT, the embedding represen-\ntation of the two heterogeneous knowledge graphs is obtained\nrespectively. E1 = {$e_{1}^{1}, e_{2}^{1}, e_{3}^{1}...,e_{n}^{1}$},E\u00b9 \u2208 KG1, E2 =\n{$e_{1}^{2}, e_{2}^{2}, e_{3}^{2}, ..., e_{n}^{2}$}, E2 \u2208 KG2,, and then the candidate entity\npairs need to be matched, that is, $e_{1}^{1} \u2194 e_{3}^{2}$, cosine similarity is\nused to evaluate the similarity between entities. The similarity\ncalculation results of all embeddings in E\u00b9 and E\u00b9 can be cal-\nculated according to the cosine similarity, and the calculation\nresults are saved in the similarity matrix cosine_distances. The\ncalculation formula is as follows:\n$Sim(e_{i},e_{j}) = \\sum_{EKG_{i}EKG_{j}}  \\frac{e_{i}.e_{j}}{\\|e_{i}\\| . \\|e_{j}\\|} ,$ (6)\nSim \u2208 [-1,1], Only the similarity in the matrix >= 0.95\nis selected as the Candidate pre-selected set. However, these\ncandidate sets only represent possible matching entity pairs,\nand there may even be one-to-many situations that cannot\nguarantee the accuracy of the final alignment.\nBased on Entity propagate similarity. taking the aligned\nseeds of KG1 and KG2 as anchors, for any seed pair\n($e_{a}^{d}, e_{a}^{f}$) \u2208 Alignentity, we construct the neighbourhood node\nsets of $e_{d}^{a}$ and $e_{f}^{a}$, n($e_{d}^{a}$)\u2208 N($e_{d}^{a}$) and n($e_{f}^{a}$)|e \u2208 N($e_{f}^{a}$),\n($e_{a}^{d}, r_{a1}, \\overline{e_{1}}$) or ($\\overline{e_{1}}, r_{a1}, e_{a}^{d}$). n($\\overline{e_{1}}$) represents the i-th neighbor\nnode of $e_{d}^{a}$, and $r_{a1}$ represents the connection between the\ntwo. $e_{f}^{a}$ represents the j-th neighbor node of $e_{f}^{a}$, ($e_{a}^{f}, r_{b1}, \\overline{e_{i}}$),\n($\\overline{e_{i}}, r_{bi}, e_{a}^{f}$), $r_{bi}$ is the same. Here, the above cosine simi-\nlarity is also used to calculate the similarity of the nodes\n($e_{d}^{a}, e_{f}^{a}$) in the neighborhood of the seed pair. In order to\nensure that the relationship pairs screened by the neighborhood\ncandidate alignment set are highly accurate and convenient\nfor subsequent training, two standards are set here: First,\nthe neighborhood alignment nodes need to meet the set\nSime similarity threshold and the Matche matching number\nthreshold of the derived relationship. Second, we must ensure\nthat the neighborhood nodes are aligned one-to-one. Only the\nneighborhood pairs with the highest similarity are selected\nwhen encountering a one-to-many or many-to-one situation.\nSum1 = {Sime >= 0.98} + {Matche >= 10}, (7)\nSum\u2081 represents the soft label set screened based on the entity\nembedding model, Sime represents the cosine similarity of the\nneighborhood node pair, and match is the number of repeti-\ntions of the relationship between the entities. For example:\n($e_{a}^{d}, e_{f}^{a}$) \u2192 ($e_{a}^{d}, r_{a1},\\overline{e_{1}}$), ($e_{f}^{a}, r_{b1},\\overline{e_{i}}$) \u2192 Sim($\\overline{e_{1}},\\overline{e_{i}}$) >=\n0.98 and Match($r_{a1}, r_{b1}$) >= 10,that is, keep this relationship\npair in the set Sum\u2081.\nBased on relation propagate similarity. In this section, the\nBert model is introduced to encode the relations of the KG\ngraph, and the semantic features of the relations are fully\ncaptured using the name and description of the relations.\n$H = Bert(RD),$ (8)\nRD: Relation Description. Similarly, setting the cosine similar-\nity and matching threshold also requires one-to-one matching.\nSum2 = {Simr >= 0.98} + {Matchr >= 600}, (9)\nSum2 represents the number of soft labels selected based\non the relation embedding model, Sim, represents the cosine\nsimilarity of the relation pair, and Matchr is the number of\ntimes this relation pair appears repeatedly. Sum = Sum\u2081 +\nSum2, Order(Sum2) > Order(Sum1), Sum2has a higher\npriority than Sum2."}, {"title": "C. Pruning Based on Soft Labels", "content": "Soft labels enhance alignment precision by guiding neigh-\nborhood matching and are selectively retained based on\nmatch quality. GCN integrates this neighborhood data, while\nHighway networks combine entity names with neighborhood\ninformation to refine the final embeddings."}, {"title": "D. Bidirectional Weighting Mechanism", "content": "Inspired by contrastive learning, we have engineered a\nbidirectional weighting mechanism based on similarity aimed\nat optimizing the weight distribution of samples during the\nentity alignment process. This mechanism adjusts the weights\nof negative samples differentially to achieve a precise approx-\nimation of positive samples and effectively optimize negative\nsamples. Positive samples are actual alignments Alignentity,\nwhile negative samples are selected from the top 50 nodes\nwith the highest similarity to the positive samples in either\nKG\u2081 or KG2. Notably, due to the most significant risk of\nmisalignment, the node with the highest similarity is assigned\nthe highest weight penalty."}, {"title": "IV. EXPERIMENTS", "content": "This section presents the experimental settings, including\nexperimental datasets, evaluation metrics, model variants, and\nexperimental implementation. The model in this paper is\ntested on the DBP15K datasets to study further the essential\ncomponents of this model and their contributions."}, {"title": "A. Datasets", "content": "DBP-15K is built on DBpedia and includes three cross-\nlanguage DBpedia datasets: ZH-EN (Chinese-English), JA-EN\n(Japanese-English), and FR-EN (French-English)."}, {"title": "B. Baseline", "content": "To assess the performance of our model, we conducted com-\nparisons against current leading entity alignment techniques.\nBroadly, these can be categorized as follows: Traditional\nEntity Alignment Methods. SEA proposes a semi-supervised\nentity alignment method that uses labeled entities and rich\nunlabeled entity information for alignment. BootEA proposes\na Bootstrap-based entity alignment method. Relation-based\nEntity Alignment Methods. RAGA proposed a framework for\ncapturing entity and relation interactions based on relation-\naware graph attention networks. RANM [40] also demon-\nstrates exceptional performance in leveraging relational assis-\ntance for entity alignment."}, {"title": "C. Evaluation setting", "content": "To ensure that the model remains optimal, five-fold cross-\nvalidation based on the training set (20%), validation set\n(10%), and test set (70%) was used, so the reported per-\nformance is the average of five independent training runs,\nand the training/validation/test datasets are shuffled in each\nround. The number of GAT layers is 2, the similarity threshold\nof entity embedding is 0.95, and the maximum number of\nneighbor nodes for alignment seeds is 982. The final training\nparameters are selected between the best performance period\non the validation set (based on \"Hits@1\") and the last period\n(up to 1500 periods). The experiments were run on NVIDIA\n4090 16 GB."}, {"title": "D. Ablation Studies", "content": "Our model exceeds baselines, as detailed in the table. Abla-\ntion studies adjusted parameters like GNN layers and learning\nrate, showing that the soft labels module enhanced DBP-15K\naccuracy by 0.3%-1%, especially in the ZH-EN subset with\na 1% gain. Additionally, the Bidirectional Weighting module\nraised DBP-15K accuracy by 0.2%-0.9%, contributing to a\nconsistent 0.7%-1.3% overall improvement across all subsets."}, {"title": "E. Robustness Analysis", "content": "In assessing our model's robustness, we tested the impact of\nGNN layer count and learning rate. While more GNN layers\nslightly lowered entity alignment accuracy but kept overall\nperformance stable, higher learning rates (0.01, 0.005, 0.001)\nsignificantly reduced performance due to training instability\nand convergence challenges."}, {"title": "V. CONCLUSION", "content": "This paper presents the SEG framework for cross-lingual\nentity alignment in knowledge graphs, which enhances entity\ninformation through a multi-source fusion mechanism and\nutilizes semantic auxiliary information from relations to assist\nin entity alignment. Additionally, it employs a bidirectional\nweighting mechanism to improve the model's differential\nprocessing of negative samples, complemented by a semi-\nsupervised iterative mechanism that continually expands and\nstrengthens alignment seeds. The robustness of this method has\nbeen validated on datasets, and future efforts will concentrate\non refining relation representation and entity enhancement\ntechniques to enhance alignment accuracy."}]}