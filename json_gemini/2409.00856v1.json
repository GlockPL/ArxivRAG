{"title": "Benchmarking LLM Code Generation for Audio Programming with Visual Dataflow Languages", "authors": ["William Zhang", "Maria Leon", "Ryan Xu", "Adrian Cardenas", "Amelia Wissink", "Hanna Martin", "Maya Srikanth", "Kaya Dorogi", "Christian Valadez", "Pedro Perez", "Citlalli Grijalva", "Corey Zhang", "Mark Santolucito"], "abstract": "Node-based programming languages are increasingly popular in media arts coding domains. These languages are designed to be accessible to users with limited coding experience, allowing them to achieve creative output without an extensive programming background. Using LLM-based code generation to further lower the barrier to creative output is an exciting opportunity. However, the best strategy for code generation for visual node-based programming languages is still an open question. In particular, such languages have multiple levels of representation in text, each of which may be used for code generation. In this work, we explore the performance of LLM code generation in audio programming tasks in visual programming languages at multiple levels of representation. We explore code generation through metaprogramming code representations for these languages (i.e., coding the language using a different high-level text-based programming language), as well as through direct node generation with JSON. We evaluate code generated in this way for two visual languages for audio programming on a benchmark set of coding problems. We measure both correctness and complexity of the generated code. We find that metaprogramming results in more semantically correct generated code, given that the code is well-formed (i.e., is syntactically correct and runs). We also find that prompting for richer metaprogramming using randomness and loops led to more complex code.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have shown promising results for code generation. To date, significant research has focused on their application to traditional software engineering-style coding problems (Chen et al. 2021; Liu et al. 2023a). These applications of LLM code generation have been focused on software with impressive levels of complexity that can be used in real-world, professional software systems deployments. However, one of the most exciting parts of LLM code generation has been the adoption by non-professional programmers into their workflow.\nIn support of a diversity of users of LLM code generation, we explore the use of LLMs for coding in the context of visual dataflow programming languages for audio. Visual dataflow languages for the arts, such as MaxMSP for audio (Cycling '74 2023), Grasshopper for 3D modelling (Celani and Vaz 2012), and shader graphs for video game visual artists (Jensen et al. 2007), have allowed many non-traditional programmers to leverage the power of computation in their work. Just as LLM code generation is delivering enormous benefit to traditional software engineers (Jimenez et al. 2023), it is critical that we also enable LLM code generation to benefit non-traditional programmers in their preferred programming environments.\nIf LLM code generation focuses only on text-based languages, which may lie outside the area of expertise of some users, the generated code then functions effectively as a blackbox for some users\u2014restricting the user who prefers visual programming. Code comprehensibility and the ability to modify the generated computational artifacts is critical in the world of automatic program construction (Santolucito et al. 2018a,b; Santolucito, Hallahan, and Piskac 2019; Hempel, Lubin, and Chugh 2019; McNutt et al. 2023; Ferdowsi et al. 2023). Generating code in languages that allow for users to make manual edits can help preserve the flexibility and expressiveness of their workflow.\nThe main contributions of this work are as follows:\n1. Propose a benchmark set for audio digital signal processing (DSP) and use it to evaluate LLM code generation.\n2. Evaluate 600 code generations over two languages-one of our own design and one industry standard language for audio DSP-for visual node-based audio programming across three different levels of code representation on this benchmark set.\n3. Define a metric that measures if LLM-generated code is semantically correct, given that it is well-formed.\n4. Provide an analysis of LLM code generation comparing direct JSON generation to metaprogramming for visual dataflow languages that may guide future work in the area, finding that metaprogramming results in more semantically correct code given it is well-formed."}, {"title": "Background and Related Work", "content": "The topic of using LLM for creative work has seen an explosion of interest. There have been new languages proposed that use LLMs as primitives in the language. For example, Jigsaw is a visual language for the construction of LLM pipelines (Lin and Martelaro 2023). Other work has focused on the intersection of media arts code and LLM code generation, for example in P5.js (Wang et al. 2024), a text-based language for web-based animations. Spellburst is an interface for creative coding assisted by LLMs (Angert et al. 2023), which gives users a visual interface for tracking their interaction with the LLM and moving between the LLM modality, and the code modality of creation.\nIn the domain of audio programming with visual languages, designing an LLM-based assistant for MaxMSP is a crucial step to make the language more accessible to users with limited programming experience (Zhang 2024). However, there is limited work in the best methods of LLM code generation specifically for visual languages, especially in the creative coding space. An important part of LLM-based code generation is that the generated code be understandable (Ferdowsi et al. 2023). Generating quality code for visual languages is an important direction, as some users find the visual language to be a more understandable code format than text-based languages.\nWe focus on visual dataflow languages for audio programming. We explore two languages in this context.\nFirst, we use MaxMSP (Cycling '74 2023), a visual dataflow language designed for interactive audiovisual projects. MaxMSP is one of the most popular languages in the computer-based music community. To complement the visual language of MaxMSP, we also generate code in MaxPy, a Python library for metaprogramming (writing code that generates code) in MaxMSP.\nSecond, we use the Web Audio API, a JavaScript API and one of the most popular frameworks for in-browser audio programming. To complement the text-based Web Audio API, we also generate code in the visual programming language Wavir, a node-based editor for Web Audio.\nWe provide here a short introduction to each language, highlighting the language features that are most relevant to our investigation of code generation.\nWe also give example code snippets for the same program (additive synthesis on a sine wave at 440 Hz with 3 partials above the fundamental) in each language as shown in Fig. 1, Fig. 2, Fig. 3, Fig. 4, Fig. 5, and Fig. 6.\nMaxMSP is a visual programming language, with each project represented in an interface of nodes and connections called a MaxMSP program (called a \"patch\" in the MaxMSP community). This visual nature makes the language accessible to users, but this visual-first representation also presents challenges for existing text-based LLM code generation. MaxMSP files are saved in JSON format that captures objects, connections, and spatial layout (the placement of nodes).\nMaxPy (Liu et al. 2023b) is a Python package that allows users to create Max patches in Python through metaprogramming. Users can use Python to programmatically construct MaxMSP patches-a MaxPy program generates a JSON file that is a MaxMSP patch.\nWavir is a visual dataflow language of our own design that complements Web Audio and is inspired by MaxMSP (wav 2024). It is available at wavir.io. Wavir's semantics are approximately the same as MaxMSP. Nodes can be sources of sound (oscillators and noise), processors (filters or gains), or constants to modify other nodes, both as float numbers or as musical note names. There are also input nodes for keyboard inputs and output nodes that display frequency and time visualizers or feed the signal to Web Audio's universal output. Nodes are connected to each other and finally to the output to generate sound. One key difference between Wavir and MaxMSP is that spatial layout is absent from Wavir's JSON encoding.\nThe Web Audio API used in Wavir is a JavaScript API for processing and synthesizing audio in web applications. This API provides a way to manipulate audio through a modular routing system consisting of audio nodes that when connected form an audio processing graph. Different nodes represent audio processing operations. The dataflow model of the Web Audio API can be represented visually as a directed graph, where nodes are vertices, and connections between nodes are edges. The Web Audio API functions similarly to MaxPy as a metaprogramming language for programmatically constructing audio DSP graphs in Wavir.\nFor both MaxPy and Web Audio, we also define and use a level of code generation called rich code, where we prompt for code with more complexity. We define rich code as a level of metaprogramming that produces more sophisticated programs by taking advantage of features only available in the text-based representation. In our experiment, we prompt the LLM to use for loops and random functions in our rich code trials. While MaxMSP has a random number object, a metalanguage such as MaxPy can use Python's random library to generate random values directly instead of adding another node to the Max patch. Moreover, the ability to generate large amounts of repetitive code in MaxMSP with a Python loop is one of the motivating factors of behind MaxPy's creation (Liu et al. 2023b). For large Max patches, this is more efficient than placing each repeated node individually through the visual editor.\nExamples of rich code compared with \"normal code\" (i.e., text-based code that does not leverage metalanguage features) are shown in Fig. 3 vs. Fig. 2 and Fig. 6 vs. Fig 5."}, {"title": "Methodology", "content": "We investigate the following research questions:\nRQ1: How does the correctness (as measured by pass@k) of the generated code differ between metaprogramming, rich metaprogramming, and JSON as the target of LLM generation for visual dataflow programming languages for audio\nRQ2 How does the complexity of the generated code differ between metaprogramming, rich metaprogramming, and JSON as the target of LLM generation for visual dataflow programming languages for audio?\nTo investigate these questions, we divided our two audio programming languages of MaxMSP and Web Audio into three code generation methods each: JSON generation, metaprogramming, and rich metaprogramming.\nThus, we experimented over six language categories:\nMax MSP\n1) MaxMSP JSON; 2) MaxPy; 3) MaxPy Rich Code\nWeb Audio\n4) Wavir JSON; 5) Web Audio; 6) Web Audio Rich Code\nWe prompted LLM assistants to generate code using each of these categories for a defined benchmark set of sound projects and evaluated the output on correctness and complexity. We describe our assistants, benchmark set, prompting methods, and evaluation metrics in the sections below."}, {"title": "Assistants Used", "content": "For each language representation (MaxMSP JSON, MaxPy, Wavir JSON, Web Audio), we created a custom LLM assistant using the OpenAI Assistants API (OpenAI 2024). These assistants used the gpt-4-0125-preview model. The purpose of using the Assistants API is to provide the LLM with familiarity in these languages, which it may not have out of the box. Our assistants were each provided with a knowledge base consisting of several code examples in their target generation language and a \"knowledge document\".\nA knowledge document is an instruction file (e.g. txt, pdf) uploaded to an OpenAI Assitant's knowledge base through the Assistants API. It contains instructions specific to the Assistant's task, ensuring the assistant follows the outlined guidelines for formatting code or JSON, such that it will not generate any unsupported methods, nodes, or data values. The Assistant retrieves it on each run. Generally, a knowledge document is simply a prefix to every prompt. An example instruction from the MaxPy knowledge document is as follows: \"These are the ONLY methods of a MaxPatch object: patch = mp.MaxPatch(), patch.place, patch.connect, patch.save. Any other method will not compile.\""}, {"title": "Benchmark Set", "content": "We chose a set of 10 audio programming projects to test code generation across each language category. Our set includes 5 specific benchmarks and 5 creative benchmarks. Specific benchmarks test the implementation of highly-defined, low-level tasks, while creative benchmarks test open-ended creative code possibilities. Our benchmark set asks for implementations of the following sounds:\nSpecific Benchmarks 1) additive synthesis; 2) AM synthesis; 3) FM synthesis; 4) an LFO; 5) filtered noise\nOpen-ended Benchmarks 6) a church bell; 7) a telephone dial tone; 8) a bird call; 9) the sound of waves hitting the ocean; 10) a babbling brook\nThese benchmark examples were chosen to test the application of foundational computational sound knowledge in projects that are concise enough for a short LLM-generated code sample to implement. Each of the open-ended benchmarks correspond directly to a specific benchmark in terms of the typical technique used. For example, a church bell sound is often constructed with additive synthesis, and a telephone dial tone is often constructed with AM synthesis. For each language category, we generated 10 code samples of each benchmark. Thus, we generated 100 code samples total for each language category and 600 code samples total across all our trials. Each of these were manually inspected and evaluated."}, {"title": "Prompting", "content": "To prompt the MaxPy, MaxMSP JSON, and Web Audio assistants, we wrote a Python script to call the OpenAI Python library (OpenAI 2024). This script creates a new thread for each trial to clear the context, inputted the prompt as the message, parsed the assistant's output for the code and saved it to a new file. As Max patches are JSON files with a .maxpat extension, we saved the generated JSON file into a .maxpat file and opened it in Max to evaluate. With generated MaxPy scripts, we ran the script, which then produced a .maxpat file that we opened manually and evaluated. For Web Audio, we ran the generated JavaScript code in our browser to evaluate."}, {"title": "Evaluation Process", "content": "Correctness To answer RQ1, we use the pass@k metric, which measures the probability of success and considers a task successful if any of the k code samples generated pass the test. To evaluate pass@k, we generate n samples per task, count the number of correct samples c < n, and calculate pass@k, for some k (Chen et al. 2021) as shown below:\n$pass@k := \\mathbb{E}\\left[\\frac{\\binom{n-c}{k} }{\\binom{n}{k}}\\right]$\n(1)\nA key difference in our use of pass@k compared to prior work is that we do not have unit tests to measure correctness. Not only are unit tests difficult to implement for audio DSP across languages, where sample rates and language construct implementations (e.g. low pass filter) may vary, but unit tests are too restrictive for a creative coding task. For example, it is difficult to formalize a test for \"the sound of waves hitting the ocean\u201d. Instead, we measure correctness through manual human inspection of the generated solutions. Due to this labor-intensive evaluation strategy, we limit ourselves to n = 10 per benchmark and k \u2264 3 to measure pass@1 and pass@3.\nFurthermore, Chen et al. optimize temperature for each value of k, since with large values of k (e.g., pass@100) a higher temperature is optimal as the samples generated have more diversity and only one sample need be correct (Chen et al. 2021). However, because we only used small values of k \u2264 3, we did not find it necessary to tune this hyperparameter for temperature. Instead, we use the default temperature setting of 1, giving us a conservative measure of pass@k\u2014this may be higher with hyperparameter tuning.\nSix authors of the paper participated in code evaluation. Code evaluation was conducted in pairs. Each pair generated and evaluated the code for the benchmarks in their assigned languages. One pair was assigned MaxMSP JSON, MaxPy, and rich MaxPy, one pair was assigned Web Audio and rich Web Audio, and one pair was assigned Wavir.\nOur manual evaluation process was as follows: First, the evaluator pair was provided with a correct example of each benchmark sound in the target language, coded by a developer on our team. For generated code samples of specific benchmarks, each individual evaluator judges whether the code and the sound it produces resembles the correct example of the benchmark. To be correct, a specific benchmark needs to both produce the sound and have the correct nodes and connections in its code for that particular benchmark.\nFor generated code samples of open-ended benchmarks, each individual evaluator judges whether the sound produced resembles, to their ears, the intended sound. In this case, the exact code implementation is less relevant because there are multiple ways to code the sound. There are also many forms an open-ended sound such as a \u201cbird call\" could take while still being considered correct.\nWe used the following inter-rater reliability process: if the evaluators agree on pass/fail, then they mark the code sample accordingly. If one or both evaluators are unsure about a sample, or if they disagree, the pair consults as a team consisting of at least one additional evaluator. The team decides through discussion whether to mark the sample correct.\nThe team repeats this process for each code sample for each benchmark in the language category. The total number of samples marked correct across specific benchmarks, creative benchmarks, and all benchmarks is the variable c used to calculate specific, creative, and overall pass@k scores.\nComplexity To answer RQ2, we defined complexity as node count. We explain this metric as a measure of complexity in audio DSP in Threats to Validity."}, {"title": "Results", "content": "Here we analyze our results through summary statistics. We provide the full evaluation set, with all code generations and raw evaluation scores provided at\nhttps://github.com/williamyzhang/MaxPy-Wavir-LLM-Generated-Code"}, {"title": "Correctness", "content": "MaxMSP We calculated the standard pass@k metric for each language across all benchmarks (n = 100). The pass@1 score is 0.300 for MaxMSP JSON, 0.310 for MaxPy normal code, and 0.200 for MaxPy rich code. However, the standard pass@k score does not differentiate between semantically incorrect and syntactically incorrect code-a significant difference for LLM-assisted creative coding (see Discussion). To understand the generation better, we separate correctness into two possibilities: well-formedness and semantic correctness.\nFor a generation to be well-formed the code must run without error. In the case of MaxPy, this means the Python code produces a valid MaxMSP patch. In the case of MaxMSP JSON generation, this means that the JSON produces a valid MaxMSP patch when opened in Max. Well-formedness is a similar notion to the more commonly used term of syntactical correctness, where code is free of syntactic problems (Corso et al. 2024). However, well-formedness is a stronger notion than syntactic correctness as code must run to be well-formed, whereas syntactically correct code may still not run. The number of generations for each language that are well-formed is shown in Fig. 7.\nFor a generation to be semantically correct, it must correctly implement the benchmark, as judged by our evaluators. The number of generations that are semantically correct for each language is shown in Fig. 8. This definition of correctness is used to define c in the calculation of pass@k.\nWe calculated pass@k scores again where n was set to the number of well-formed code samples for each language, rather than the total number of samples generated. Here, the pass@1 score is 0.345 for MaxMSP JSON, 0.463 for MaxPy normal code, and 0.417 for MaxPy rich code.\nFor the MaxMSP languages, when we calculate pass@k as semantic correctness over all code samples, it is unclear whether metaprogramming or direct JSON generation results in more correct code, while rich metaprogramming performs worse than both normal metaprogramming and direct JSON generation.\nHowever, when we calculate pass@k as semantic correctness over only well-formed samples, both normal and rich metaprogramming result in higher rates of correctness over direct JSON generation. Rich metaprogramming still performs worse than normal metaprogramming.\nWeb Audio Over all code samples (n = 100), the pass@1 score is 0.080 for Wavir JSON, 0.380 for Web Audio normal code, and 0.410 for Web Audio rich code.\nCalculating pass@k over only the well-formed samples, the pass@1 score is 0.533 for Wavir JSON, 0.760 for Web Audio normal code, and 0.732 for Web Audio rich code.\nFor the Web Audio languages, metaprogramming results in higher correctness than direct JSON generation across both pass@k metrics. Rich metaprogramming performs either better or worse depending on the metric used.\nConclusions To answer RQ1, metaprogramming and rich metaprogramming result in more correct code than direct JSON generation when we measure semantic correctness over only well-formed samples. Results are less conclusive when using a standard pass@k metric of semantic correctness over all generated samples. We will explain why calculating pass@k as semantic correctness over well-formed samples is a useful metric and how the research community can use these results in the Discussion."}, {"title": "Complexity", "content": "Complexity was measured as the average number of nodes per compiled code sample for each benchmark. The significance of the difference in complexity between each pair of languages was calculated using a one-sided Wilcoxon signed-rank test across the averages for each benchmark.\nMaxPy Rich Code had a statistically significantly higher node count than either MaxMSP JSON or normal MaxPy, with p-values of 0.002 and 0.001 respectively. Web Audio Rich Code also had a statistically significantly higher average node count than either Wavir JSON or normal Web Audio, with p-values of 0.02 and 0.01 respectively.\nNormal MaxPy had a statistically significantly higher average than MaxMSP JSON at p = 0.05. However, Wavir JSON had a slightly higher average node count than Web Audio, but the difference was not statistically significant.\nConclusions To answer RQ2, rich metaprogramming as the target of LLM generation results in the most complex code. We found no consistent difference between normal metaprogramming and JSON generation in complexity."}, {"title": "Discussion", "content": "Here we explain our new correctness metric, the applications of our results, and potential limitations of our study.\nA New Correctness Metric\nWe calculated two different pass@k metrics in our evaluation, and we derived our conclusion that metaprogramming results in more semantically correct code given it is well-formed from the second metric (semantically correct over only well-formed samples). Here, we will justify that this second metric is a reasonable measurement of correctness with practical implications.\nMost evaluations of LLM-generated code rely on a correctness metric that considers code as correct if it passes the unit test, and incorrect otherwise (Chen et al. 2021). Under this metric, semantic errors and syntax errors are treated the same. However, distinguishing between these two types of errors is significant for AI-based coding assistants. Corso et al. investigated both syntactic and semantic correctness of AI-generated code, concluding that a simple pass-fail check for correctness using test cases is insufficient, as AI assistants may write code that passes unit tests but does not properly implement the function requested (Corso et al. 2024).\nAn important concern with LLM-based coding assistants in the creative coding and visual language domain is their accessibility to non-programmers. For non-programmers, if an LLM-generated code sample is not producing the result desired, it is likely easier to regenerate another code sample rather than debug the broken code. A tool that is fully accessible to non-programmers would ideally not require directly debugging even a high-level programming language (Ferdowsi et al. 2023).\nWhen building an LLM-based coding assistant, it is easy to automate a check for well-formedness, as we only need to check for errors in the console. On the other hand, checking for semantic correctness in creative coding is far more difficult and subjective to some extent. The latter would require a unit test for whether a sound project coded by the AI assistant sounds convincingly like a bird call, and if it is the exact type of bird call the user desires. Thus, a hypothetical assistant can filter out samples that are not well-formed and return only well-formed samples to the user. It is important that the generated well-formed code has a high rate of semantic correctness. Therefore, semantic correctness over only well-formed samples is a practical metric for such an assistant.\nIn designing a creative coding assistant accessible to non-programmers, prioritizing code generation that yields semantically correct code, given that the generation is well-formed, is more useful than prioritizing semantically correct code over all generations.\nMetaprogramming as the Target of LLM Code Generation\nAs Wavir and MaxMSP are designed as visual languages in order to give users a high-level interface to computation, effective use of LLM-assisted tools that generate visual code could help further raise the level of abstraction for users. However, generating code (rather than a blackbox system), is important so that users maintain control over their output.\nMetaprogramming offers concision and efficiency over JSON in representing a visual language. To generate the same MaxMSP patch requires far more lines of code in JSON than in MaxPy. Moreover, metaprogramming is much more intuitive than JSON for organizing and connecting elements of the visual language, as it encodes the visual language in a high-level programming language instead of lower-level JSON data. For these reasons, metaprogramming is certainly preferable for human programmers when choosing a text-based representation of visual languages.\nOur results indicate that generated code for visual dataflow languages is more semantically correct at the metaprogramming level than in JSON representation, given it is well-formed. This suggests that metaprogramming is preferable for LLM-based coding assistants as well. This preference has important implications for LLM code generation. When the target of generation has multiple representations in text, a higher-level representation (i.e., more abstract, closer to natural language) may be more semantically correct, given the generation is well-formed, than lower-level data representations.\nBenefits of Complexity\nThe goal of measuring code complexity is most often to limit the complexity of programs, since more complex programs are harder to understand, more error-prone, and more difficult to debug (Corso et al. 2024; Lavazza, Morasca, and Gatto 2023). However, here we will argue for some advantages that complexity offers in creative coding.\nWith regard to creativity in computational systems, Wiggins formalizes the concept of a \u201cperfect or productive aberration\u201d, where new concepts outside of the existing domain produce valuable results (Wiggins 2006b,a). Applying this concept to creative coding, a productive aberration would result when a programmer uses the language in an unexpected way outside of their knowledge domain yet generates a useful or pleasing outcome. Much as productive aberrations in computational agents are useful for training and improving an AI (Wiggins 2006a), productive aberrations in creative coding allow the programmer to develop in knowledge and mastery of the language.\nFollowing the notion of productive aberrations, an LLM-based coding assistant that can offer more complexity would allow creative coders to use the language in ways outside of their knowledge. Specific to our domain, an assistant capable of generating programs with more nodes is more likely to result in unexpected outcomes or usages of the language that lead to productive aberrations.\nA historical example of the advantages of complexity is FM synthesis. In describing sound synthesis via frequency modulation, Chowning emphasized the complexity of the components involved in FM synthesis as a major contributor to the rich sounds produced by the technique (Chowning 1977). These complex sounds subsequently made FM central to the synth-dominated music of the 1980s.\nOf course, excess complexity will be hard to understand and inaccessible to beginners, so an LLM-based coding assistant that can generate complex code should also be able to generate simpler code if requested. For this purpose, the results of our study are relevant. Rich metaprogramming resulted in the most complex code, whereas the complexity of normal metaprogramming and direct JSON generation was approximately equal. Thus, metaprogramming can provide both a low level of complexity similar to that of JSON generation and a high level of complexity via prompting for rich code. The availability of multiple modes is an inherent advantage of metaprogramming over direct JSON generation, which lacks features such as loops and randomness that result in rich code.\nRich metaprogramming allows an LLM-based coding assistant to provide users with the creative exploration and development afforded by complexity. Meanwhile, switching between rich and normal modes of metaprogramming code generation would allow an LLM-based coding assistant to offer more complex or more basic solutions upon request."}, {"title": "Threats to Validity", "content": "Confounding Variables for Correctness The major internal threat to the validity of our study is the conflation of correctness because of the nature of the language with correctness because of the LLM having better training data.\nProgramming languages with more training data in the LLM's training corpus tend to perform better in evaluations of generated code, as defined by pass@k (Athiwaratkun et al. 2023; Cassano et al. 2022, 2024). GitHub is frequently used as a source of code examples for training LLMs on code (Kocetkov et al. 2022). With closed-source LLMs such as GPT-4, the training data used is not released to the public (OpenAI et al. 2024; Kocetkov et al. 2022). However, even with closed-source models such as Codex, a language's popularity on GitHub is a predictor of its performance (Cassano et al. 2022). Therefore, we classify the languages used in our paper from low- to high-resource languages based on the number of code examples available on GitHub.\nAs Fig. 11 shows, the number of semantically correct generations tends to increase for higher resource languages. As high-resource languages likely have far more representation in GPT-4's training corpus, the pass@k scores for high-resource languages such as Web Audio may be higher due to their greater available training data, rather than their characteristics as a language. MaxPy's better performance over MaxMSP in the metric of semantic correctness given syntactic correctness shows that higher correctness is not solely due to the amount of training data. Nonetheless, future research should control for the training data disparity between metaprogramming languages and direct JSON inputs.\nComplexity Metric While we used node count as our complexity metric, there are other more commonly used metrics for code complexity, such as cyclomatic complexity. In this section we justify our choice of node count over other metrics.\nCyclomatic complexity, as first defined by McCabe, measures complexity as the number of linearly independent paths through a program (McCabe 1976). However, cyclomatic complexity does not make sense in the audio DSP programs used in our study because these programs lack control flow. That is, there is only one path through the programs, since they are designed to generate one sound in one way, without branching into multiple possibilities. Without control flow, their cyclomatic complexity will be the same-1. Therefore, cyclomatic complexity is unfit as a measure of complexity for our study.\nAnother classic measure of complexity is lines of code (Lavazza, Morasca, and Gatto 2023). Lines of code has been shown to perform approximately as well as other metrics, including cyclomatic complexity, in predicting empirical measures of code understandability (Lavazza, Morasca, and Gatto 2023; Davis and LeBlanc 1988). As our study used multiple levels of representation of visual programming languages, however, measuring lines of code would not allow for comparison between the different languages across these levels of representation. Nonetheless, the logic behind lines of code is useful in our study, measuring complexity as the number of elements in a program (Davis and LeBlanc 1988).\nIn the context of visual node-based programming languages, a node is a base element of the program. The number of nodes can be a visual dataflow analog of lines of code for traditional coding languages. In fact, node count likely underestimates complexity, since with increased number of nodes there is also an exponential increase in the interactions between nodes. A program with 4 nodes is usually more than twice as complex as a program with only 2 nodes. Complexity scales more than linearly with the number of nodes. Therefore, node count is a conservative estimate of a lower bound on complexity."}]}