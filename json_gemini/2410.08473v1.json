{"title": "Deeper Insights into Deep Graph Convolutional Networks: Stability and Generalization", "authors": ["Guangrui Yang", "Ming Li", "Han Feng", "Xiaosheng Zhuang"], "abstract": "Graph convolutional networks (GCNs) have emerged as powerful models for graph learning tasks, exhibiting promising performance in various domains. While their empirical success is evident, there is a growing need to understand their essential ability from a theoretical perspective. Existing theoretical research has primarily focused on the analysis of single-layer GCNs, while a comprehensive theoretical exploration of the stability and generalization of deep GCNs remains limited. In this paper, we bridge this gap by delving into the stability and generalization properties of deep GCNs, aiming to provide valuable insights by characterizing rigorously the associated upper bounds. Our theoretical results reveal that the stability and generalization of deep GCNs are influenced by certain key factors, such as the maximum absolute eigenvalue of the graph filter operators and the depth of the network. Our theoretical studies contribute to a deeper understanding of the stability and generalization properties of deep GCNs, potentially paving the way for developing more reliable and well-performing models.", "sections": [{"title": "Introduction", "content": "Graph-structured structured data is pervasive across diverse domains, including knowledge graphs, traffic networks, and social networks to name a few [1, 2]. Several pioneering works [3, 4] introduced the initial concept of graph neural networks (GNNs), incorporating recurrent mechanisms and necessitating neural network parameters to define contraction mappings. Concurrently, Micheli [5] introduced the neural network for graphs, commonly referred to as NN4G, over a comparable timeframe. It is worth noting that the NN4G diverges from recurrent mechanisms and instead employs a feed-forward architecture, exhibiting similarities to contemporary GNNs. In recent years, (contemporary) GNNs have gained significant attention as an effective methodology for modeling graph data [6-11]. To obtain a comprehensive understanding of GNNs and deep learning for graphs, we refer the readers to relevant survey papers for an extensive overview [12-15].\nAmong the various GNN variants, one of the most powerful and frequently used GNNs is graph convolutional networks (GCNs). A widely accepted perspective posits that GCNs can be regarded as an extension or generalization of traditional spatial filters, which are commonly employed in Euclidean data analysis, to the realm of non-Euclidean data. Due to its success on non-Euclidean data, GCN has attracted widespread attention on its theoretical exploration. Recent works on GCNs includes understanding over-smoothing [16-19], interpretability and explainability [20\u201324], expressiveness [25-27], and generalization [28-41]. In this paper, we specifically address the generalization of GCNs to provide a bound on their generalization gap.\nInvestigating the generalization of GCNs is essential in understanding its underlying working principles and capabilities from a theoretical perspective. However, the theoretical establishment in this area is still in its infancy. In recent work [36], Zhang et al. provided a novel technique based on algorithmic stability to investigate the generalization capability of single-layer GCNs in semi-supervised learning tasks. Their results indicate that the stability of a"}, {"title": "Related Work", "content": "Theoretically, contemporary research on the generalization capability of GCNS predominantly employs methodologies such as Vapnik-Chervonenkis dimension (VC-dim) [30, 34], Rademacher complexity [31-35], and algorithmic stability [36, 37, 42, 43], as the mainstream categories revisited in this section. To provide a broader perspective, we also mention briefly other methodologies such as the classic PAC-Bayesian [38, 39], neural tangent kernels (NTK) [40, 41], algorithm alignment [44, 45], statistical physics and random matrix theory [46].\nVC-dim and Rademacher Complexity. In [30], Scarselli et al. examined the generalization capability of GNNs by providing upper bounds on the order of growth of the VC-dim of GNNs. While the VC-dim serves as a traditional concept for establishing learning bounds, its applicability does not account for the underlying graph structure. In [34], the authors also provided a generalization error bound for GNNs using VC-dim. However, the error bound based on VC-dim is trivial and fails to capture the beneficial impact of degree normalization. Esser et al. [34] explored the generalization upper bound using transductive"}, {"title": "Preliminaries and Notations", "content": "In this section, we provide a comprehensive description of the problem setup examined in this paper. Additionally, we present an extensive review of fundamental concepts related to uniform stability for training algorithms, which serve as the foundation for the subsequent analysis."}, {"title": "Deep Graph Convolutional Networks", "content": "Let G = (V, E, A) denote an undirected graph with a node set V of size N, an edge set & and the adjacency matrix A \u2208 \\mathbb{R}^{N\u00d7N}. As usual, L := D \u2013 A is denoted as its conventional graph Laplacian, where D \u2208 \\mathbb{R}^{NXN} signifies the degree diagonal matrix. Furthermore, g(L) \u2208 \\mathbb{R}^{N\u00d7N} represents a graph filter and is defined as a function of L (or its normalized versions). We denote by Cg = ||g(L)||_2 the maximum absolute eigenvalue of a symmetric filter g(L) or the maximum singular value of an asymmetric g(L).\nWe denote by X = (X_1, X_2, ..., X_N)^\\intercal \u2208 \\mathbb{R}^{N\u00d7d_o} the input features (do stands for input dimension) and xj \u2208 \\mathbb{R}^{d_o} the node feature of node j, while Cx = ||X||_F represents the Frobenius norm of X. For the input feature X, a deep GCN with g(L) updates the representation as follows:\nX^{(k)} = \\sigma(g(L)X^{(k-1)}W^{(k)}), k = 1,2, . . ., K,\nwhere X^{(k)} \u2208 \\mathbb{R}^{N\u00d7d_k} is the output feature matrix of the k-th layer with X^{(0)} = X, the matrix W^{(k)} \u2208 \\mathbb{R}^{d_{k\u22121}\u00d7d_k} represents the trained parameter matrix specific to the k-th layer. The function \u03c3(\u00b7) denotes a nonlinear activation function applied within the GCN model. For simplicity, we set a final output in a single dimension, that is, the final output label of N nodes is given by\ny = \\sigma(g(L)X^{(K)}w),\nwhere y \u2208 \\mathbb{R}^{N} and w\u2208 \\mathbb{R}^{d_K}. As defined above, the deep GCN (1) with learnable parameters\n\\theta = {W^{(1)}, W^{(2)}, ..., W^{(K)}, w}.\nis a K + 1 layers GCN with K hidden layers and a final output layer, and in the case of K = 0, it degenerates into the single-layer GCN studied in [36]."}, {"title": "The SGD Algorithm", "content": "We denote by D the unknown joint distribution of input features and output labels. Let\nS := {(x_j, y_j)}_{j=1}^m\nbe the training set i.i.d sampled from D and As be a learning algorithm for a deep GCN trained on S. For a deep GCN model (1) with parameters \u03b8 = {W^{(1)},..., W^{(K)}, w}, denote As(x) = f(x|\u03b8_s) = \u03c3(\u03b4_xg(L)X^{(K)}w) as the output of node x, where Os is the corresponding learned parameter and 8x is the indicator vector with respect to node x. For a loss function l : \\mathbb{R}\u00d7\\mathbb{R} \u2192 \\mathbb{R}+, the generalization error or risk R(As) is defined by\nR(A_s) := E_z [l(f(x|\u03b8_s), y)],\nwhere the expectation is taken over z = (x, y) ~ D, and the empirical error or risk Remp(As) is\nR_{emp}(A_s) := \\frac{1}{m} \\sum_{j=1}^m l(f(x_j|\u03b8_s), y_j).\nWhen considering a randomized algorithm As,\nE_{gen}(A_s) := E_A [R(A_s) \u2013 R_{emp}(A_s)]\ngives the generalization gap between the generalization error and the empirical error, where the expectation En corresponds to the inherent randomness of As. In this paper, As is considered to be the algorithm given by the SGD al- gorithm. Following the approach employed in [36], our analysis focuses solely on the randomness inherent in As arising from the SGD algorithm, while dis- regarding the stochasticity introduced by parameter initialization. The SGD algorithm for a deep GCN(1) aims to optimize its empirical error on a dataset S by updating parameters iteratively. For t \u2208 \\mathbb{N} and considering the parameters Ot-1 obtained after t - 1 iterations, the t-th iteration of SGD involves randomly drawing a sample (xt, yt) from the dataset S. Subsequently, parameters 0 are iteratively updated as follows:\n\u03b8_\u03c4 = \u03b8_{t-1} - \u03b7\u2207\u03b8l(f(x_t|\u03b8_{t\u22121}), y_t),\nwith the learning rate \u03b7 > 0."}, {"title": "Uniform Stability", "content": "For the sake of estimating the generalization gap egen(As) of As, we invoke the notion of uniform stability of As as adopted in [36, 52].\nLet\nS^{\\backslash i} := {(x_j, y_j)}_{j=1}^{i-1} \u222a {(x_j, y_j)}_{j=i+1}^{m}\nbe the dataset obtained by removing the i-th data point in S, and\nS^{i} = {(x_j, y_j)}_{j=1}^{i-1} \u222a {(x, y)} \u222a {(x_j, y_j)}_{j=i+1}^{m}\nthe dataset obtained by replacing the i-th data point in S. Then, the formal definition of uniform stability of a randomized algorithm As is given in the following.\nDefinition 1 (Uniform Stability [36]). A randomized algorithm As = f(x|\u03b8s) is considered to be \u00b5m-uniformly stable in relation to a loss function l when it fulfills the following condition:\nsup |E_A [l(y, y)] \u2013 E_A [l(y', y)]| \u2264 \u00b5_m,\nS,z\nwhere z = (x, y) ~ D, \u0177 = f(x|\u03b8s) and y' = f(x|\u03b8_{S^{\\backslash i}}).\nAs shown in Definition 1, \u00b5m indicates a bound on how much the variation of the training set S can influence the output of As. It further implies the following property:\nsup |E_A [l(y, y)] - E_A [l(y', y)]| \u2264 2\u00b5_m,\nS,z\nwhere z = (x, y) ~ D, y = f(x|\u03b8s) and y' = f(x|\u03b8_{S^{i}}).\nMoreover, it is shown that the uniform stability of a learning algorithm As can yield the following upper bound on the generalization gap Egen (As).\nLemma 1 (Stability Guarantees [36]). Suppose that a randomized algorithm As is pm-uniformly stable with a bounded loss function l. Then, with a probability of at least 1-8, considering the random draw of S, z with d\u2208 (0,1), the following inequality holds for the expected value of the generalization gap:\nE_{gen} (A_s) \u2264 2\u00b5_m + \\sqrt{\\frac{4m\u00b5_m + M}{2m} log {\\frac{2}{\\delta}}},\nwhere M is an upper bound of the loss function l, i.e., 0 \u2264 l(\u00b7, \u00b7) \u2264 M."}, {"title": "Main Results", "content": "This section presents an established upper bound on the generalization gap Egen (As) as defined in (2) for deep GCNs trained using the SGD algorithm. Notably, this generalization bound, derived from a meticulous analysis of the comprehensive back-propagation algorithm, demonstrates the enhanced insight gained through the utilization of SGD."}, {"title": "Assumptions", "content": "First, we make some assumptions about the considered deep GCN model (1), which are necessary to derive our results.\nAssumption 1. The activation function \u03c3: \\mathbb{R} \u2192 \\mathbb{R} is assumed to satisfy the following:\n1. ao-Lipschitz:\n|\u03c3(x) \u2013 \u03c3(y)| \u2264 \u03b1_o|x \u2212 y|, \u2200x, y \u2208 \\mathbb{R}.\n2. vo-smooth:\n|\u2207\u03c3(x) \u2013 \u2207\u03c3(y)| \u2264 \u03bd_o|x \u2212 y|, \u2200x,y \u2208\\mathbb{R}.\n3. \u03c3(0) = 0.\nWith these assumptions, the derivative of \u03c3, denoted by \u2207\u03c3, is bounded, i.e., |\u2207\u03c3| \u2264 \u03b1\u03c3, and thus ||\u2207\u03c3(X)||_F \u2264 \u03b1_r||X||_F holds for any matrix X. It can be easily verified that activation functions such as ELU and tanh satisfy the above assumptions.\nAssumption 2. Let \u0177 and y be the predicted and true labels, respectively. We denote the loss function l : [Ymin, Ymax] \u00d7 [Ymin, Ymax] \u2192 \\mathbb{R} by l(\u0177, y). Similar to [37], we adopt the following assumptions for l.\n1. The loss function l exhibits continuity with respect to the variables (\u0177, y) and possesses continuous differentiability with respect to \u0177.\n2. The loss function l satisfies ae-Lipschitz with respect to \u0177:\n|l(\u0177, y) \u2013 l(\u0177', y)| \u2264 \u03b1_l|\u0177 - \u0177'|, \u2200\u0177, \u0177\u02b9, y \u2208 [Ymin, Ymax].\n3. The loss function l meets ve-smooth with respect to \u0177:\n|\\frac{\u2202l(\u0177, y)}{\u2202\u0177} - \\frac{\u2202l(\u0177', y)}{\u2202\u0177}| \u2264 \u03bd_e|\u0177 - \u0177'|, \u2200 \u0177, y', y \u2208 [Ymin, Ymax].\nWith these assumptions, |\\frac{\u2202l(\u0177, y)}{\u2202\u0177}| \u2264 \u03b1e, and l is bounded, i.e., 0 \u2264 l(\u0177, y) < M.\nAssumption 3. The learned parameters {W^{(1)}, . . ., W^{(K)}, w} during the training procedure with limited iterations satisfies"}, {"title": "Generalization Gap", "content": "This section presents the main results of this paper. For convenience, the notations used in the result are summarized in Table 1. Under the assumptions made in Section 4.1, the bound on the generalization gap of deep GCNs is provided in the following theorem.\nTheorem 1 (Generalization gap for deep GCNs). Consider the deep GCN model, defined in equation (1), which comprises K hidden layers and utilizes g(L) as the graph filter operator. The model is trained on S using SGD for T iterations. Under Assumptions 1, 2 and 3 stated in Section 4.1, the following expected generalization gap is valid with a probability of at least 1 - \u03b4, where\n\u03b4\u2208 (0,1):\nE_{gen} (A_s) \u2264 \\frac{1}{\\sqrt{m}} {O((K+1)\u03b7\u03ba_1 + \u03b7\u03ba_2) \\frac{T}{2}} + M \\sqrt{\\frac{log {\\frac{2}{\\delta}}}{2}},\nwhere\n\u03ba_1 :=(\u03bd_\u03c3 \u03b1_l + \u03b1_\u03c3 \u03bd_e) (B\u03b1_oC_g)^{2K}C_x^2C_g^2 + \u03b1_l(B\u03b1_oC_g)^{K-1}\u03b1_\u03c3C_g^2C_x,\n\u03ba_2 := \u03bd_o (B\u03b1_oC_g)^{K}C_x^2C_g^2(\\sum_{j=0}^{K-1}(j+1)(B\u03b1_oC_g)^{2j}).\nA fundamental correlation between the generalization gap and the param- eters governing deep GCNs is induced by Theorem 1. This correlation implies that the uniform stability of deep GCNs, trained using the SGD algorithm, ex- hibits an increase with the number of samples when the upper bound approaches zero as the sample size m tends to infinity. Specifically, it is observed that if the value of Cg (presenting the largest absolute eigenvalue of a symmetry g(L) or the maximum singular value of an asymmetry g(L)) remains unaffected by the size N, a generalization gap decaying at the order of O(1/\\sqrt{m}) is obtained. To compare with the result in [36], let us discuss at length the role of g(L) and the hidden layer number K on the generalization gap.\nAccording to (7) and (8), \u043a\u2081 = O(C_g^{2K+2}) and K2 = O(C_g^{2K+1}). Therefore, the bound on the generalization gap of deep GCNs in Theorem 1 is\nE_{gen} (A_s) \u2264 \\frac{1}{\\sqrt{m}} (O(C_g^{2K+2}) \\frac{T}{2}} + M \\sqrt{\\frac{log {\\frac{2}{\\delta}}}{2}}.\nWhen K 0, the GCN model (1) degenerates into the single-layer GCN model considered in [36]. At this point, according to (9), we have\nE_{gen} (A_s) \u2264 \\frac{1}{\\sqrt{m}} (O(C_g^{2}) \\frac{T}{2}} + M \\sqrt{\\frac{log {\\frac{2}{\\delta}}}{2}}.\nwhich is the same as the result of [36].\nRemarks. Based on (9), we present certain observations regarding the impact of filter g(L) and the hidden layer number K on the generalization capacity of deep GCNs in (1)."}, {"title": "Normalized vs. Unnormalized Graph Filters", "content": "We examine the three most commonly utilized filters: 1) g_1(L) = A + I, 2) g_2(L) = D^{-1/2}AD^{-1/2} + I, and 3) g_3(L) = D^{-1}A + I. For the unnormalized filter 91, its maximum absolute eigenvalue is bounded by O(N). Con- sequently, as the value of m approaches the magnitude to N, the upper bound indicated by (9) tends towards O(N^p) for some p > 0, leading to an impractical upper bound when N become infinitely large. On the contrary, for two normalized filters g2 and g3, their largest absolute eigenvalues are bounded and independent of graph size N. Therefore, both filters yield a diminishing generalization gap at a rate of O(\\frac{1}{\\sqrt{m}}) as m goes to infin- ity. This discovery underscores the superior performance of normalized filters over unnormalized counterparts in deep GCNs. This observation is consistent with the findings in [36, 37]."}, {"title": "The Role of Parameter K", "content": "It is evident that, when the values of Cg and T are fixed, the upper bound (9) exhibits an exponential dependence on parameter K. This observation implies that a larger value K leads to an increase in the upper bound of the generalization gap, thereby offering valuable insights for the architectural design of deep GCNs. This finding diverges from the ones presented in [36, 37], as these studies do not account for generic deep GCNs and overlook the significance of the parameter K.\nFurthermore, based on Theorem 1, we give a brief analysis of the impact of dk (width of the k-th layer) on the generalization. Actually, the impact of dk on the generalization is reflected in its impact on B. More specifically, let us consider the case where parameters {W^{(1)}, ..., W^{(K)}, w} belong to the set Xe, where\n\u03a7\u03b5 := {W : ||W||_\u221e\u2264 \u03be},\ni.e., X\u0119 is the collection of all matrices whose elements' absolute values are all less than \u00a7. At this point, for W(k) \u2208 \\mathbb{R}^{d_{k-1}\u00d7d_k}, we have\nsup ||W^{(k)}||_2\u2264 sup ||W^{(k)}||_F\u2264\u03be\u221a{d_{k-1}d_k}."}, {"title": "Generalization Gap", "content": "Therefore, a larger dk (i.e., width of the k-th layer) results in a larger upper bound of ||W^{(k)} ||_2, which implies that a larger dk results in a larger B (see Assumption 3 in Section 4.1). Finally, Theorem 1 indicates that a larger B leads to a larger bound on the generalization gap, thus we conclude that a larger dk leads to a larger bound on the generalization gap. To justify this argument, we add some experimental studies in Section 5. The empirical results are consistent with our analysis."}, {"title": "Experiments", "content": "In this section, we conduct some empirical studies using three benchmark datasets commonly utilized for the node classification task, namely Cora, Cite- seer, and Pubmed [53, 54]. Table 3 summarizes the basic statistics of these datasets. In our experiments, we follow the standard transductive learning"}, {"title": "Conclusion and Further Remarks", "content": "This paper explores the generalization of deep GCNs by providing an upper bound on their generalization gap. Our generalization bound is obtained based on the algorithmic stability of deep GCNs trained by the SGD algorithm. Our analysis demonstrates that the algorithmic stability of deep GCNs is contingent upon two factors: the largest absolute eigenvalue (or maximum singular value) of graph filter operators and the number of layers utilized. In particular, if the aforementioned eigenvalue (or singular value) remains invariant regardless of changes in the graph size, deep GCNs exhibit robust uniform stability, resulting in an enhanced generalization capability. Additionally, our results suggest that a greater number of layers can increase the generalization gap and subsequently degrade the performance of deep GCNs. This provides guidance for designing well-performing deep GCNs with a proper number of layers [56]. Most impor- tantly, the result of single-layer GCNs in [36] can be regarded as a special case of our results in deep GCNs without hidden layers.\nWhile our study is primarily focused on exploring the fundamental princi- ples of generalizability and stability in the context of a simple deep GCN model framework, it can offer preliminary insights into several pressing issues that are the subject of recent attention in the GNN domain. These include: i) the over-smoothing problem, which stands as a pivotal challenge in the development of deep GNNs [57, 58], and ii) the design of advanced GNNs tailored for het- erophilic graphs, characterized by nodes whose labels significantly diverge from"}, {"title": "Proofs", "content": "The proofs of our main results are given in this section. We first make some statements about the notations used in the paper. WT denotes the transpose of"}, {"title": "Gradient computation for SGD", "content": "To work with the SGD algorithm, we provide a recursive formula for the gradient of the final output f(x0) at node x in the GCNs model (1) with respect to the learnable parameters.\n\u2022 For the final layer,\n\u2207wf(x|0) = \u2207_\u03c3(\u03b4_xg(L)X^{(K)}w) [\u03b4_xg(L)X^{(K)}]^T,\n\u2022 For the hidden layer k = 1, 2, ..., K,\n\u2207_{W^{(k)}} f(x|\u03b8) = [g(L)x^{(k-1)}] (\\frac{\u2202f(x|\u03b8)}{\u2202X^{(k)}}R^{(k)}),\nwhere R^{(k)} := \u2207_\u03c3(g(L)X^{(k-1)}W^{(k)}) and\n\\frac{\u2202f(x|\u03b8)}{\u2202X^{(k-1)}} = g(L) (\\frac{\u2202f(x|\u03b8)}{\u2202X^{(k)}}R^{(k)}) [W^{(k)}]^T,\nwith\n\\frac{\u2202f(x)}{\u2202X^{(K)}} = \u2207_\u03c3(\u03b4_xg(L)X^{(K)}w) [\u03b4_xg(L)]^Tw^T,\nThe notation represents the Hadamard product of two matrices. (A.1) and (A.4) are easy to verify, while (A.2) and (A.3) are not. In the following, a detailed procedure is provided to derive (A.2) and (A.3).\nFirst, since X_{ij}^{(k)} = \u03c3(\u03b4_xg(L)X^{(k-1)}W^{(k)}\u03b4_j),\n\\frac{\u2202X_{ij}^{(k)}}{\u2202W^{(k)}} = \\frac{\u2202\u03c3(\u03b4_xg(L)X^{(k-1)}W^{(k)}\u03b4_j)}{\u2202W^{(k)}}\n= \u2207_\u03c3 (\u03b4_xg(L)X^{(k-1)}W^{(k)} \u03b4_j) {g(L)X^{(k-1)} \u03b4_j\u03b4_i^T },\nand\n\\frac{\u2202X_{ij}^{(k)}}{\u2202X^{(k-1)}} = \\frac{\u2202\u03c3 (\u03b4_xg(L)X^{(k-1)}W^{(k)}\u03b4_j)}{\u2202X^{(k-1)}}\n= \u2207_\u03c3(\u03b4_xg(L)X^{(k-1)}W^{(k)}\u03b4_j) [g(L)\u03b4_id_j^T] [W^{(k)}]^T,"}, {"title": "Proof of Lemma 2", "content": "To prove Lemma 2, we first provide the following lemma to show the variation of output in each layer for two GCNs with different learned parameters \u03b8 = {W(1), W(2),...,W(K), w} and \u03b8\u2032 = {W(1)\u2032, W(2)\u2032, ..., W(K)\u2032, w\u2032}. Let X(k) and X(k)\u2032 be their output of the hidden layer, as well as f(x|\u03b8) and f(x|\u03b8\u2032) the final output of node x. The following lemma provides a bound of X(k) \u2013 X(k)\u2032 and f(x|\u03b8) \u2212 f(x|\u03b8\u2032) based on \u25b3\u03b8 = {\u25b3W(1), ..., \u25b3W(K), \u25b3w}.\nLemma 6. Consider two GCNs with parameters 0 and 0', respectively. Then, we obtain the following results for their variations.\n\u2022 Their variation of outputs in hidden layers \u25b3X(k) := X(k) \u2013 X(k)\u2032 (k =\n1,2,..., K) satisfies\n||\u25b3X^{(k)}||_F \u2264 B^{k-1}\u03b1_\u03c3C_g^kC_x(\\sum_{j=1}^{k}||\u25b3W(j)||_2).\n\u2022 Furthermore, for the final output of node x,\n|f(x|\u03b8) \u2212 f(x|\u03b8\u2032)| < B^K\u03b1_\u03c3C_g^{K+1}C_x||\u25b3\u03b8||_*."}, {"title": "Proof of Lemma 3 and Lemma 4", "content": "To prove Lemma 3 and Lemma 4, we should first prove the following lemma.\nLemma 7. Consider two GCNs with parameters 0 and 0', respectively. Then, their variation of gradients of f with respect to {W(1),...,W(K), w} satisfies\n||\u2207_wf(x|\u03b8) \u2013 \u2207_{w\u2032}f(x|\u03b8\u2032)||_F \u2264 (\u03bd_oB^{2K} \u03b1_\u03c3^{2K} C_g^{2K+2} C_x^2 + B^{K-1}\u03b1_\u03c3^{K+1}C_g^{K+1} C_x) ||\u25b3\u03b8||_*,\nand for k = 1, 2, ..., K,\n||\u2207_{W(k)} f(x|\u03b8) - \u2207_{W(k)} f(x|\u03b8\u2032)||_F\n\u2264(\u03bd_oB^{2K} \u03b1_\u03c3^{2K} C_g^{2K+2} C_x^2 + B^{K-1}\u03b1_\u03c3^{K+1}C_g^{K+1} C_x) ||\u25b3\u03b8||_* + \u03c1_k ||\u25b3\u03b8||_*,\nwhere\n\u03c1_k := \u03bd_e(B\u03b1_oC_g)^{K+k-1}C_x^2C_g^2(\\sum_{j=0}^{K-k-1}(B\u03b1_oC_g)^j).\nProof. First, according to the proof of (A.8) and (A.9), the following holds true for k = 1, 2, ..., K + 1:\n||X^{(k-1)}W^{(k)} \u2013 X^{(k-1)\u2032}W^{(k)\u2032}||_F <B^{k-1} \u03b1_\u03c3^{k-1} C_g^{k-1}C_x||\u25b3W^{(k)}||_2 + B||\u25b3X^{(k-1)}||_F\nB^{k-1} \u03b1_\u03c3^{k-1} C_g^{k-1}C_x(\\sum_{j=1}^k ||\u25b3W^{(j)}||_2),"}, {"title": "Proof of Theorem 3", "content": "Based on Lemmas 3 and 4, we detail the proof of Theorem 3 as follows.\nNote that (xt, yt) = (x1, y1) with probability \\frac{1}{m} and (xt, yt) \u2260 (x1, y1) with probability \\frac{m-1}{m}. By considering (3) and incorporating the probability of the two scenarios presented in Lemmas 3 and 4, using F and F' to denote f(xt|0t\u22121) and f(xt|0\u2032t\u22121), respectively, we have:\nEA [||\u25b3Wt||2] = (1-\\frac{1}{m})EA [||\u25b3Wt-1 - \u03b7(\u2207_wl(F, Yt) - \u2207_wl(F\u2032, Yt)||_2]\n+\\frac{1}{m}EA [||\u25b3Wt-1 - \u03b7(\u2207_wl(F, Yt) - \u2207_wl(F\u2032, Yt)||_2]\n<(1-\\frac{1}{m})EA [||\u25b3Wt-1||_2 + \u03b7 ||\u2207_wl(F, Yt) \u2013 \u2207_wl(F\u2032, Yt)||_F]\n+\\frac{1}{m}EA [||\u25b3Wt-1||_2 + \u03b7 ||\u2207_wl(F, Yt) \u2013 \u2207_wl(F\u2032, Yt)||_F].\nBased on Lemma 3 and Lemma 4,\nEA [||\u25b3Wt||2] <EA [||\u25b3Wt-1||2] + \u03b7\u03ba_1EA [||\u25b3\u03b8t\u22121||_*] +\n2\u03b7\u03b1_eB^K \u03b1_\u03c3^{K+1} C_g^{K+1} C_x/m\nSimilarly, for k = 1, 2, ..., K,\nEA [||\u25b3W_t^{(k)}||_2] <EA [||\u25b3W_{t-1}^{(k)}||_2] + \u03b7(\u03ba_1 + \u03c1_k)EA [||\u25b3\u03b8_{t\u22121}||_*] +\n2\u03b7\u03b1_eB^K \u03b1_\u03c3^{K+1} C_g^{K+1} C_x/m\nThen,\nEA [||\u25b3\u03b8_t||_*] =EA [||\u25b3Wt||2] + \\sum_{k=1}^K EA [||\u25b3W_t^{(k)}||2]\n<EA [||\u25b3Wt-1||2] + \u03b7\u03ba_1EA [||\u25b3\u03b8_{t\u22121}||_*] +\nK\n+ EA[||\u25b3W_t^{(k)}||2] + \u03b7(\u03ba_1 + \u03c1_k)EA [||\u25b3\u03b8_{t\u22121}||_*] +\nK\n= EA [||\u25b3\u03b8_{t-1}||_*] + (\u03b7\u03ba_1 + \u03b7\u03c1_k)EA [||\u25b3\u03b8_{t\u22121}||_*] +\nK\n2(K+1)\u03b7\u03b1_eB^K \u03b1_\u03c3^{K+1} C_g^{K+1} C_x/m\nwhere k_2 = \\sum_{k=1}^K \u03c1_k By (A.12), we have \u03ba_2 = \u03bd_o (B\u03b1_oC_g)^{K+k\u22121}C_x^2C_g^2\\sum_{j=0}^{K-k-1}(B\u03b1_oC_g)^j /(1)(B\u03b1_oC_g)^j), as defined in (8). Finally, since ||\u25b3\u03b8_0||_* = ||0_0 \u2013 \u03b8_0\u2032||_* = 0\nEA [||\u25b3\u03b8_T||_*] <\\sum_{t=1}^T (1 + (\u039a + 1)\u03b7\u03ba_1 + \u03b7\u03ba_2)^{t-1}.\nThis completes the proof of Theorem 3."}]}