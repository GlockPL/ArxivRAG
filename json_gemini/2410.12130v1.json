{"title": "ITER-AHMCL: ALLEVIATE HALLUCINATION FOR LARGE LANGUAGE MODEL VIA ITERATIVE MODEL-LEVEL CONTRASTIVE LEARNING", "authors": ["Huiwen Wu", "Xiaohan Li", "Xiaogang Xu", "Jiafei Wu", "Deyi Zhang", "Zhe Liu"], "abstract": "The development of Large Language Models (LLMs) has significantly advanced various AI applications in commercial and scientific research fields, such as scientific literature summarization, writing assistance, and knowledge graph construction. However, a significant challenge is the high risk of hallucination during LLM inference, which can lead to security concerns like factual inaccuracies, inconsistent information, and fabricated content. To tackle this issue, it is essential to develop effective methods for reducing hallucination while maintaining the original capabilities of the LLM. This paper introduces a novel approach called Iterative Model-level Contrastive Learning (Iter-AHMCL) to address hallucination. This method modifies the representation layers of pre-trained LLMs by using contrastive 'positive' and 'negative' models, trained on data with and without hallucinations. By leveraging the differences between these two models, we create a more straightforward pathway to eliminate hallucinations, and the iterative nature of contrastive learning further enhances performance. Experimental validation on four pre-trained foundation LLMs (LLaMA2, Alpaca, LLaMA3, and Qwen) finetuning with a specially designed dataset shows that our approach achieves an average improvement of 10.1 points on the TruthfulQA benchmark. Comprehensive experiments demonstrate the effectiveness of Iter-AHMCL in reducing hallucination while maintaining the general capabilities of LLMs.", "sections": [{"title": "Introduction", "content": "The development of large language models (LLMs) has led to impressive successes in a wide range of artificial intelligence (AI) applications, from commercial usage like GPT-4 [1] to scientific research fields [32]. The adoption of AI technologies, especially LLMs, is leading us into a groundbreaking era for scientific research. LLMs technology has unlocked a wide range of possibilities in scientific fields, from summarizing scientific literature reviews [20, 17, 3] to assisting in scientific writing [23, 2, 21] and constructing knowledge graphs [26, 9, 39].\nIn order to address the hallucination issue for LLMs, several types of research focus on measuring and mitigating hallucinated texts [16, 34, 27], representation editing and alignment before downstream tasks [45, 44, 36], and utilizing knowledge distillation methodology [25, 13, 31]. Although current methods have shown success in reducing hallucinations in LLMs, there is still room for improvement in increasing awareness of hallucinations and preserving knowledge.\nIn our work, we draw inspiration from feature editing methods with the vector guidance, which are presented in [45], to propose a feature editing approach with the model-level guidance to deal with hallucination. Building on the principles of contrastive learning discussed in [46, 38], we emphasize the importance of selecting both appropriate positive and negative guidance during model training. Our method effectively reduces hallucination across different LLMs while preserving the models' original language capabilities.\nEspecially, we designed a new approach called Iterative Model-level Contrast Learning (Iter-AHMCL), and the vital characteristic is the formulation of model guidance. First, we construct positive and negative data using corresponding templates. Next, we pre-train positive and negative guidance models based on the general vector-guidance-based representation editing method [46]. The goal of the positive model is to exhibit a favorable bias in hallucination evaluation by achieving a high score, while the negative model is trained to show the opposite bias. We then use these pre-trained positive and negative models as guidance to edit the representation layer, effectively controlling the model's tendency toward hallucination.\nFurthermore, we recognize the importance of adaptively updating the models that provide guidance. Better performance is achieved by evolving the LLM in tandem with the guidance models. To this end, we design a model-level iterative strategy, where the positive model is updated with one that performs better in hallucination evaluation, and the negative model is updated with one that performs worse. By leveraging the differences between these two models, we create a more direct pathway to reduce hallucinations. This iterative approach combined with contrastive learning further enhances overall performance.\nIn addition to improving performance in addressing hallucinations, our proposed Iter-AHMCL utilizes representation editing to adjust only the model's preferences related to hallucination problems, with minimal impact on its original capabilities. This is because the guidance in Iter-AHMCL is highly aligned with the direction relevant to hallucinations, while remaining orthogonal to other knowledge areas. We validate this claim through comprehensive evaluations from multiple perspectives. The overall procedure of Iter-AHMCL is illustrated in Figure 1.\nTo summarize, our contributions are listed as follows.\n\u2022 We introduce a novel approach, called Iter-AHMCL, to eliminate hallucination in LLMs while preserving their general capabilities. In Iter-AHMCL, we offer a new perspective by adaptive developing models with positive and negative feature representations, implementing model-level contrastive learning guided by these models.\n\u2022 We implement an iterative approach to update the guidance model and establish model-level guidance. This iterative strategy is broadly applicable to various LLMs. The code and all models will be released to publication.\n\u2022 We conduct comprehensive experiments with various LLM models, and the evaluation results demonstrate that our method effectively reduces hallucinations while preserving general capabilities."}, {"title": "Related Work", "content": "Hallucinations in LLMs occur when the model generates inaccurate or fictitious information, diverging from factual knowledge and occasionally producing responses that are not grounded in its training data [27]. Several studies have"}, {"title": "Representation Editing", "content": "Representation editing is a technique for modifying a model's preferences or performance by altering its trained representations. It is widely used in both traditional machine learning (ML) [33] and the rapidly advancing LLMs [35, 45, 44]. For example, in a machine learning scenario, [33] proposes a flexible unsupervised text attribute transfer framework that utilizes a Transformer-based autoencoder to learn latent representations and employs the Fast Gradient Iterative Modification algorithm to edit these representations until they align with the target attribute. In the context of LLMs, [45] introduces a method for controlling the model's preferences through dedicated alignment of the selected layer representations. [35] extends the representation editing method to jailbreak attacks and hallucination control scenarios. [44] accomplishes concept editing through adversarial representation engineering. [36] proposes Representation Editing (RED), a novel fine-tuning approach that modifies neural model representations, significantly reducing the number of trainable parameters while achieving results comparable to or exceeding those of full fine-tuning and other parameter-efficient fine-tuning (PEFT) methods. [19] pre-trains a multimodal encoder to produce text-aligned visual representations and designs a subject representation learning task that enables a diffusion model to generate new subject renditions using these representations. Although [35, 45, 44] present insightful approaches to concept editing through representation alignment, the performance in reducing hallucinations can be further enhanced."}, {"title": "Contrastive Learning", "content": "Contrastive learning is a self-supervised learning technique designed to learn useful representations of data by contrasting positive and negative samples. The core idea is to bring the representations of similar positive pairs closer together while pushing apart the representations of dissimilar pairs. This technique is widely applied to tasks such as image classification, especially when labels are scarce or costly to obtain. In [18], the author leverages the power of contrastive learning in supervised settings by bringing together points belonging to the same class in the embedding space while separating clusters of samples from different classes. In [10], the author introduces a momentum contrast method for unsupervised visual representation learning, viewing contrastive learning as a dictionary lookup. This approach involves building a dynamic dictionary using a queue and a moving average encoder to create an extensive and consistent dictionary. [42] proposes a graph contrastive learning framework for learning unsupervised representations of graph data across four settings: semi-supervised, unsupervised, transfer learning, and adversarial attacks. [37] extends graph contrastive learning to heterophilic graphs, where connected nodes have different class labels and features, by employing an asymmetric view of neighboring nodes. [5] presents a straightforward framework for contrastive learning of visual representations by introducing a learnable non-linear transformation between the representation and the contrastive loss. [41] introduces a decoupled contrastive learning loss that removes the positive term from the denominator, significantly enhancing learning efficiency. Recent research has applied contrastive learning to enhance LLMs for tasks such as few-shot text classification [43], unified representation extraction [24], and machine translation [38]. However, despite the widespread use of contrastive learning in traditional machine learning tasks, the primary challenge in adapting"}, {"title": "Methodology", "content": "In this section, we describe the main procedure of Iter-AHMCL. Throughout the paper, we use the following notations.\n{T, T+, T-} denotes the triplet consisting of the neural data, positive data, and negative data. T represents the set of them. M means the model to be fine-tuned. M+ is the positive guidance model, while M\u00af is the negative guidance model. {R, R+, R-} denotes the triplet consisting of the neural, positive, and negative feature representations. i is employed to indicate the iteration step in CL-IMG. M\u2021 represents the updated positive guidance model at step i. R+\nand R\u2212i denote the representation of updated positive and negative guidance representation at step i."}, {"title": "Motivation", "content": "In recent research [45, 44], the authors present a fine-tuning method to control model preferences through representation alignment, applying this technique to generate non-harmful and trustworthy responses. In the study by [45], the authors extract partial model layers to obtain representations and analyze the intermediate features of various concepts, such as honesty, fairness, and harmlessness. This enables them to edit and control the behavior of an LLM by directing a representation vector generated within the internal hidden layers. For example, when faced with a truthful question, the vector can be altered in two directions to influence the final answer: one direction to generate a more truthful response, and the other to develop a less truthful response. In this way, by editing the intermediate layer representations, one can alter the preference of the final output. The key is to find better guidance directions for the positive and negative components. In [45], the authors use a contrast vector-defined as the difference between representations generated by positive input data and those generated by negative data-to guide the direction. In [44], the authors train a discriminator to differentiate between positive and negative directions at the intermediate layers. However, these methods, which rely on sample-level guidance vectors, often depend heavily on carefully tuned hyperparameters and may fail to generalize to unseen data, leading to issues such as overfitting. To address this, we propose training two models as guidance, better learning the positive and negative directions in feature representations. The main objective is to enhance the separation between these directions, which can more effectively align with safety requirements while preserving the original capabilities of the model."}, {"title": "Contrast Triple Construction (CTC)", "content": "First, we define the contrast templates for fine-tuning the LLM. The fine-tuning data for Iter-AHMCL are constructed from the PKU-SafeRLHF dataset[7], available on Hugging Face. Each sample in the original training data consists of one prompt and two responses, along with labels. The label takes boolean values, indicating whether a response is safe or not. The existing sample T = {Ins, Res} contains two elements where Ins denotes the instruction and Res represents the response. We need to construct positive and negative instance pairs, which are crucial for the second phase of contrastive learning. Let Pos be the prompt of 'Please give a truthful answer' and Neg be the negative prompt of 'Please give an untruthful answer'. The positive instance is constructed by T+ = {Ins, Pos, Res} while the negative instance is formulated as T\u00af = {Ins, Neg, Res}. The input data of Iter-AHMCL is the triple {T, T\u00af, T+}."}, {"title": "Contrastive Learning with Contrasted Triple (CL-CT)", "content": "In this section, we elaborate on the strategy of CL-CT, the basic method for establishing guidance directions within the feature representation space. The input data is a triplet training sample, {T, T\u00af, T+}, consisting of the original sample T, the sample with a positive template T+, and the sample with a negative template T\u00af, as constructed in Sec. 3.2. Using the provided frozen model M, we follow existing work [45] to select several layers for representation extraction, while utilizing other layers to perform the fine-tuning. In other words, we pass the data triplet through the frozen model M, specifying certain layers to obtain a triplet of representations {R, R+, R\u00af}, where R = M(T) is the representation of the original data, R+ = M(T+) is the representation of the data with the positive template, and R\u00af = M(T\u00af) is the representation of the data with the negative template. To enhance the LLM's ability to distinguish between positive and negative samples, we diverge from the approach in [45], which constructs the loss solely as the l2 distance between the positive and negative representations (Eq. (1)). We further incorporate terms for the 12 distance between the positive and neutral representations, as well as between the negative and neutral ones. The goal is to enlarge the l2 distance between the neural representation R and the negative representation R\u00af (Eq. (3)) while eliminate the l2 distance between the original representation R and the positive representation R+ (Eq. (2)).\n\n$\nLLORRA = ||M(T^+) \u2013 M(T^-)||^2;\\\\\nL^+ = ||M(T) \u2013 M(T^+)||^2;\\\\\nL^- = ||M(T) \u2013 M(T^-)||^2.\n$\nCombine with the original LoRRA loss presented in [45], the loss function of CL-CT, denoted as L1, is\n\n$\nL_1 = L_{LORRA} + \\alpha L^+ - \\beta L^-,\n$\nwhere \u03b1 and \u03b2 are small non-negative constants. While this loss function has demonstrated improved effectiveness compared to the original LoRRA loss, we further amplify the influence of the guidance direction by proposing the development of a guidance model, which aids in extracting more accurate positive and negative directions."}, {"title": "Guidance Model Pre-training (GMP)", "content": "Before introducing the formulation of the new learning function, it is essential to elaborate on the training of the guidance model, which serves as a vital component. Therefore, in this section, we will discuss the pre-training procedure of the guidance model. To obtain better guidance, we train one positive guidance model and one negative guidance model, thereby enhancing the effectiveness of contrastive learning in CL-CT as presented in Sec. 3.3. The pre-training data consists of two sub-datasets derived from the PKU-SafeRLHF datasets [7]. The positive model M+ is trained with the goal of reducing hallucinations. Thus, the training loss is defined the same way as the representation editing loss in Eq. (4). However, the goal of the negative model M\u00af is to diminish its ability to generate responses to hallucination-related questions. Therefore, it has a negative objective compared to the editing loss and the positive guidance model training loss. The training loss for the negative guidance model is defined as in Eq. (5). The only difference lies in the coefficient terms for L+ and L\u00af. We set the coefficient for the positive l2 distance to be negative, while the coefficient for the negative 12 distance is set to positive, thereby increasing hallucination responses and creating a contrary model.\n\n$\nL_2 = L_{LORRA} - \\alpha L^+ + \\beta L^-,\n$\nwhere \u03b1 and \u03b2 are non-negative constants.\nThe formulation of M+ and M\u00af. The training strategy employs LoRA [12], which focuses on optimizing the low-rank components of each attention matrix. After completing the pre-training of the two models, we can obtain two adapters designed to provide positive and negative guidance. We then integrate these adapters with the frozen model M to create the positive guidance model M+ and the negative guidance model M\u00af."}, {"title": "Constrastive Learning with Model Guidance (CL-MG)", "content": "In this section, we discuss the application of the guidance model in contrastive learning. After obtaining the positive guidance model M+ and the negative guidance model M\u00af in Sec. 3.4, we utilize them to generate representations for the guidance loss. Specifically, the positive representation is produced by the positive guidance model using the data sample with a positive template, expressed as R+ = M+(T+). In contrast, the negative representation is generated by the negative guidance model using the negative data sample, represented as R\u00af = M\u00ae(T\u00af). Compared with the"}, {"title": "Contrastive Learning with Iterative Model Guidance (CL-IMG)", "content": "After establishing CL-MG, we observe that the guidance model can be further improved. Therefore, in this section, we outline the iterative process for updating the pre-trained M+ with more effective guidance models. This strategy is called CL-IMG. The long-term fine-tuning with CL-IMG is conducted using a continual learning strategy, incorporating feature editing training with an improved pre-trained guidance model. Since the positive training models and CL-MG share the same training loss and methodology, we iteratively update the positive models using the newly obtained best models from CL-MG. With this update, the loss function in the i-th round is defined as\n\n$\nL_{Iter} (i, T, T^+) = ||M(T) \u2013 M^\\ddagger (T^+)||^2;\\\\\nL_{Iter} (i, T, T^-) = ||M(T) \u2013 M^-(T^-)||^2,\n$\nwhere M+ is the positive model updated after i rounds. Meanwhile, the overall loss for contrastive learning with iterative model guidance is denoted as\n\n$\nL_{Iter} = L_{LoRRA} + \\alpha L_{Iter} - \\beta L_{Iter},\n$\nwhere \u03b1 and \u03b2 are small non-negative constants.\nTo summarize, we consolidate all the previously discussed components and describe the Iter-AHMCL in Algorithm 1. Our method encompasses several key steps: 1. Data Preparation: We construct sets of contrasting data and use this data to pre-train the positive and negative guidance models. 2. Guidance Model Utilization: We leverage the pre-trained guidance models to adjust the direction of the intermediate representations during the fine-tuning process. 3. Iterative Improvement: To enhance fine-tuning performance, we iteratively update the guidance model, ensuring it continuously adapts and improves, thereby maintaining peak performance while demonstrating flexibility and resilience.\nIt is important to highlight that the objective of Algorithm 1 is to reduce hallucination. Our goal is to develop a more effective positive model throughout the training procedure of Iter-AHMCL. Consequently, we adopt an asymmetric approach to iteratively update the pre-trained models: we focus on updating the positive guidance model while keeping the negative guidance model unchanged."}, {"title": "Experimental Analysis", "content": "In this section, we present the main results from comprehensive experiments to demonstrate the efficiency and effectiveness of our methods Iter-AHMCL. Through the experimental analysis, we aim to answer the following research questions:\n\u2022 RQ.1.GMP Training Procedure. How does the training of GMP perform, and what is the divergence between positive and negative representations?\n\u2022 RQ.2.Hallucination Reduction Effect. How does Iter-AHMCL reduce the hallucination of an LLM model?\n\u2022 RQ.3. General Capability Preservation. How does Iter-AHMCL preserve the model's knowledge and general language ability?\n\u2022 RQ.4.Iterative Process Benefits. How does the iterative procedure help LLMs reduce hallucination through representation editing?\n\u2022 RQ.5.Transferability of Guidance Model. Does the guidance model have transferability from one LLM foundation model to another?"}, {"title": "Experimental Settings", "content": "I) Data Preparation. We prepare two datasets for the overall training of Iter-AHMCL with the PKU-SafeRLHF dataset [7] and the Alpaca-instruction dataset [29]. The PKU-SafeRLHF dataset contains 83,400 samples, while the Alpaca-instruction dataset comprises 52,000 samples.\nII) Foundation Model Choice. Alpaca-native (Alpaca) [29] is an enhanced version of the LLaMa1-7B model, fine-tuned on synthetic data, developed by the Stanford team. LlaMA2-chat-hf (LLaMA2) [30] is an open-source collection of pre-trained and fine-tuned LLMs ranging in scale from 7B to 70B parameters, released in July 2023 by Meta. LlaMa3-chat-8b (LLaMA3) [8] is a suite of language models that natively support multilingual capabilities,\nIII) Compared Methods. 1) Foundation models refer to the original models downloaded from Hugging Face [15] without any further fine-tuning. 2) LoRRA [45] provides a method for editing representations using contrast vectors to enhance the model's ability to distinguish between positive and negative directions. 3) Pure Model Guidance (Pure-MG) fine-tunes the models using a loss derived from pure positive and negative model guidance\n\n$\nL_{pure} = \\alpha L^+_{MG} \u2013 \\beta L^-_{MG}.\n$\nIV) Hyper-parameters. We present the hyper-parameters for GMP (Sec. 3.4) and Iter-AHMCL (Sec. 3.6) in Table 1.\nV) Evaluation Methods. 1) TruthfulQA [22] is a benchmark designed to assess the accuracy and truthfulness of LLMs based on their generated responses to questions. The benchmark consists of 817 questions covering 38 diverse categories, including health, law, finance, and politics. We utilize the TruthfulQA [22] benchmark for evaluating hallucinations. According to the guidelines of TruthfulQA [22], we selected MC1 (Single-true) to evaluate the LLM model's capacity to identify factual statements. In MC1, the LLM is presented with a question and 4-5 answer choices. It undergoes a rigorous process to select the most probable correct answer, ensuring a thorough evaluation. The likelihood of each selection is computed independently, and the completion with the highest log probability is chosen. The reported score reflects the accuracy across all questions, with higher scores indicating better performance in reducing hallucinations. 2) MMLU [11], which stands for Measuring Massive Multitask Language Understanding, serves as a benchmark for assessing the performance of language models. This benchmark comprises approximately 16,000 multiple-choice questions spanning 57 academic disciplines, including mathematics, philosophy, and medicine. 3) C-Eval [14] is a comprehensive Chinese evaluation system designed to assess the advanced knowledge and reasoning skills of foundational models within a Chinese context. The system includes an extensive set of 13,948 multiple-choice questions across 52 distinct fields, covering various educational stages. We utilize six conventional subject categories: STEM, social sciences, humanities, other, average, and Avg (hard). The Avg (hard) category represents the mean score of the C-Eval hard benchmark, which includes subjects such as advanced mathematics, discrete mathematics, and college chemistry, all of which require significant reasoning skills for resolution. 4) Qwen API [4] evaluates the model's performance in terms of accuracy, coherence, safety, and usability for real-world applications. In the evaluation results, 'Gold-Ref' refers to scores assigned to standard responses, \u2018Relevance' identifies significant content, 'Fluency' focuses on sentence quality, 'Coherence' assesses structure and logic, and 'Consistency' checks for factual agreement."}, {"title": "Pre-training Effect of Guidance (RQ.1)", "content": "In this section, we present the pre-training performance of both the positive and negative guidance models. The training data for the guidance models is derived from the PKU-SafeRLHF datasets [7]. The loss function is constructed as shown in Eq. (8). Specifically, we set \u03b1 = 10 and \u03b2 = 1 for training the positive guidance model, and \u03b1 = 1 and \u03b2 = 10 for the negative one. We present the convergence behavior of the two models during pre-training in Figure 5 (Upper) and evaluate their performance using TruthfulQA [22] in Table 2."}, {"title": "Effect of Positive and Negative Representation (RQ.1)", "content": "To demonstrate the effect of the differences between positive and negative representations, we randomly sampled 500 triples from the contrastive dataset and recorded the l2 distance between the original representation and the positive representation, as well as the original representation and the negative one. We present the statistics of the recorded 500"}, {"title": "TruthfulQA Evaluation (RQ.2)", "content": "We present the evaluation results of the TruthfulQA Benchmark [22] for measuring the hallucination of trained models in Table 4. From Table 4, it is evident that our method, Iter-AHMCL, exhibits a significant improvement in the MC1 score based on the TruthfulQA evaluation compared to both the foundation models and the existing LORRA [45] method. Specifically, for the LLaMA2 foundation model, we improve the MC1 score by up to 19.43 points compared to the foundation model and by 3.82 points compared to LoRRA. A consistent improvement can also be observed for the Alpaca and LLaMA3 models. Notably, for the Alpaca model, the LoRRA method decreases the MC1 score, while our method increases it by up to 3.38 points. In conclusion, Iter-AHMCL demonstrates efficient and consistent improvement in the hallucination reduction task across the three foundation models."}, {"title": "Knowledge Evaluation (RQ.3)", "content": "To answer RQ.3, we present the knowledge evaluation of the model fine-tuned with our method, Iter-AHMCL, in comparison to the foundation models and the LoRRA models based on the benchmarks of the MMLU [11] and C-Eval [14] datasets. Both MMLU and C-Eval are designed to evaluate the model's performance across a wide range of subjects. MMLU focuses on multiple-choice questions in various academic disciplines, while C-Eval targets a comprehensive set of tasks that include both multiple-choice and open-ended questions to assess the models' capabilities in different linguistic and knowledge domains. The results for MMLU and C-Eval are presented in Figure 4. We observe that Iter-AHMCL exerts no significant negative influence on the knowledge evaluation, demonstrating the model capability-preserving property."}, {"title": "Model Evaluation by QWen API(RQ.3)", "content": "In this section, we discuss the use of the QWen LLM API for evaluation [4]. The process involves utilizing foundational or fine-tuned models to generate responses for CNN-DailyMail News Text Summarization [6]. Answer sheets are created from the model outputs and standardized answers, which are then input into the Qwen API. The API evaluates the answer sheets and provides results, which are subsequently analyzed statistically. The evaluation encompasses four aspects: Relevance, Fluency, Coherence, and Consistency.\nWe present the evaluation results in Table 5, based on the Qwen API. Our ongoing analysis of the Qwen scores, as shown in Table 5, begins by noting that the Gold-Ref achieves the highest scores across all four perspectives, reflecting the tailored evaluation methodology of Qwen. Furthermore, our method, Iter-AHMCL, consistently delivers stable results in all evaluation dimensions, slightly outperforming the Foundation models but not reaching the level of the Gold-Ref."}, {"title": "Iterative Process of Model Guidance (RQ.4)", "content": "This section demonstrates the benefits of iterative model guidance in addressing RQ.4. We take Iter-AHMCL applied to the foundation model Alpaca as an example. The training procedure is described in Algorithm 1. All training is conducted with a maximum of 1,250 iteration steps. The detailed iterative process of Iter-AHMCL and its improvements on the TruthfulQA Evaluation are shown in Table 6. By updating the positive guidance model, we iteratively enhance the MC1 score of the fine-tuned model. The long-term iterative process is illustrated in Figure 6, where the x-axis represents the training steps and the y-axis represents the MC1 score evaluated using TruthfulQA. From Figure 6, we observe that the score improves progressively, with oscillations occurring at the interchange points of the guidance model. The iterative updates of the positive guidance model may temporarily degrade the model's performance around"}, {"title": "Transferability (RQ.5)", "content": "In this section, we address RQ.5: whether a pre-trained guidance model based on one foundation model can be transferred for the CL-MG/CL-IMG learning to another foundation model. To explore this, we conducted experiments using a positive guidance model trained on the foundation model LLaMA2 and Iter-AHMCL trained on Alpaca. The TruthfulQA Evaluation MC1 scores are presented in the last row of Table 6. In this experiment, the positive guidance model used is Iter-AHMCL-LLaMA2 at iteration 6, with the negative guidance model remaining the same as in other experiments. The transfer experiments yield performance comparable to the first iteration of Iter-AHMCL using a positive guidance model tuned from a homogeneous guidance model, demonstrating the transferability."}, {"title": "Conclusion", "content": "In our paper, we introduce a novel method called Iter-AHMCL, designed to reduce hallucination in LLM models while preserving their overall performance. This approach involves fine-tuning the representations at specific layers to enhance the model's capabilities through constructive learning. Unlike existing strategies that rely on sample-level contrasts, we propose formulating guidance at the feature representation level using specifically trained positive and negative model guidance. This allows us to establish contrasts at both the sample and model levels. Furthermore, this contrastive learning approach can be conducted iteratively by continuously updating the positive guidance model. Our comprehensive experiments, which include evaluations of hallucination and language ability, demonstrate the efficiency and effectiveness of our proposed methods. In the future, we plan to investigate the transferability of Iter-AHMCL in the cross-domain scenarios."}]}