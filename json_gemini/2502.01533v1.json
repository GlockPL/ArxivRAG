{"title": "Transformers trained on proteins can learn to attend to Euclidean distance", "authors": ["Isaac Ellmen", "Constantin Schneider", "Matthew I.J. Raybould", "Charlotte M. Deane"], "abstract": "While conventional Transformers generally operate on sequence data, they can be used in conjunction with structure models, typically SE(3)-invariant or equivariant graph neural networks (GNNs), for 3D applications such as protein structure modelling. These hybrids typically involve either (1) preprocessing/tokenizing structural features as input for Transformers or (2) taking Transformer embeddings and processing them within a structural representation. However, there is evidence that Transformers can learn to process structural information on their own, such as the AlphaFold3 structural diffusion model. In this work we show that Transformers can function independently as structure models when passed linear embeddings of coordinates. We first provide a theoretical explanation for how Transformers can learn to filter attention as a 3D Gaussian with learned variance. We then validate this theory using both simulated 3D points and in the context of masked token prediction for proteins. Finally, we show that pre-training protein Transformer encoders with structure improves performance on a downstream task, yielding better performance than custom structural models. Together, this work provides a basis for using standard Transformers as hybrid structure-language models.", "sections": [{"title": "1 Introduction", "content": ""}, {"title": "1.1 Background", "content": "Transformers typically operate on sequential data, however many applications of Transformers benefit from an ability to learn geometric reasoning. For instance, ESM-2 (Lin et al., 2023) demonstrates that in order to effectively predict masked tokens in protein sequences, the model has learned some ability to predict protein structures. Other tasks such as image processing or even natural language processing may benefit from an internal representation of objects in 3D space. However, it is unclear how Transformers can learn to use 3D representations to perform spatial reasoning. To this end, custom structural Transformers have been created which model data as graphs and represent distance between nodes as edge features in order to perform SE(3)-invariant attention (Ingraham et al., 2019; Fuchs et al., 2020; Liao & Smidt, 2023; Liao et al., 2023).\nSE(3) invariance means that all functions of coordinates reduce to functions of relative distance. That is, for every function $f$ of two coordinates $x_1$ and $x_2$, there exists an equivalent function $g$ which depends only"}, {"title": "1.2 Prior work", "content": "There have been prior approaches to merge Transformers with SE(3)-(in/equi) variant models, especially for computational chemistry and 3D point clouds. Some methods add attention blocks to SE(3) GNNs to create SE(3)-invariant GNN Transformers (Fuchs et al., 2020; Liao & Smidt, 2023). These have shown good results on a number of tasks, however tend to be memory-intensive, particularly because attention is performed on edges, which grow as $n^2$ for fully-connected graphs. As a result, the graph connectedness of the GNNs is typically limited to k-nearest neighbours. In contrast, memory-efficient attention implementations such as FlashAttention (Dao et al., 2022; Dao, 2023) have enabled linear-memory standard Transformers.\nPrevious works have demonstrated that sequence-only protein Transformers can learn attention maps which correlate with physical contacts (Lin et al., 2023; Vig et al., 2020). However, these works do not formally model structure and so are limited by which contacts can be predicted from purely sequential patterns. To overcome this, Transformers are often paired with structural models, for instance methods such as ProSST (Li et al., 2024), ESM-IF (Hsu et al., 2022), and ESM3 (Hayes et al., 2024) use custom graph-based modules to create structural tokens which are fed into standard Transformers. In contrast, our work shows that standard Transformers are natively capable of using coordinates to model structure and measure distance.\nSimilarly, AlphaFold2 (Jumper et al., 2021) and ESMFold (Lin et al., 2023) use Transformers to preprocess protein sequences for structure prediction. Again, these preprocessed representations have been shown to correlate with structural contacts. AlphaFold2 makes this explicit during training by minimizing a distrogram loss which encourages the EvoFormer to learn structural contacts. However, it is unclear if these representa- tions are learning to explicitly embed coordinates in 3D and both models still require an SE(3)-equivariant GNN structure module to actually produce 3D structures.\nIn building AlphaFold3, DeepMind replaced AlphaFold2's SE(3)-equivariant structure module with linearly embedded coordinates fed into a diffusion transformer (Abramson et al., 2024). AlphaFold3's structure module uses inner product attention with a pair bias learned from the pair representation. At present, this still requires quadratic memory, however does indicate that nearly-standard Transformers with linearly embedded coordinates can learn on structure. Here, we explore how such linearly embedded coordinates can be used by standard Transformer attention modules to measure the Euclidean distance between tokens, and, in contrast to prior work, show that no modifications are necessary for the standard Transformer architecture to learn to perform structural reasoning."}, {"title": "2 Theory", "content": "Here we introduce our theory for how Transformers can learn to measure Euclidean distance. Throughout this section, we assume that all coordinates are small. This can be achieved by scaling the inputs and outputs manually or via learned weights in the linear input/output maps. For more detail, see Appendix A.1."}, {"title": "2.1 Gaussian spatial attention", "content": "Consider a pre-norm Transformer (Xiong et al., 2020) operating on a sequence of embedded positions, $E(x_i)$. For simplicity, assume that the key and query embeddings for all heads are trivial $(Q = K = I_a)$. Such mappings can easily be learned as long as the head dimension is at least $d$. Then, in the first layer of the Transformer, the attention matrix for all heads will be:\n$\\text{A}_{i,j} = SM(\\frac{LN(E(x_i)) \\cdot LN(E(x_j))}{\\sqrt{d}})$", "latex": ["\\text{A}_{i,j} = SM(\\frac{LN(E(x_i)) \\cdot LN(E(x_j))}{\\sqrt{d}})$"]}, {"title": "2.2 Spatial positional embeddings", "content": "For simplicity, we consider the case of 1 spatial dimension and provide 4-dimensional embeddings, which satisfy Equation 2. The 3 (or n) dimensional case is similar. Consider the embeddings:\n$E_{trig}(x) = (sin(x), \u2013 sin(x), cos(x), - cos(x))$\n$E_{lin}(x) = (x, -\u0445, 1, -1)$\n$E_{quad}(x) = (x, -x,1 - \\frac{x^2}{2},\\frac{x^2}{2} - 1)$", "latex": ["E_{trig}(x) = (sin(x), \u2013 sin(x), cos(x), - cos(x))", "E_{lin}(x) = (x, -\u0445, 1, -1)", "E_{quad}(x) = (x, -x,1 - \\frac{x^2}{2},\\frac{x^2}{2} - 1)"]}, {"title": "3 Experiments", "content": ""}, {"title": "3.1 Simulated points", "content": "To test our theory of how Transformers learn to measure distance, we designed a Transformer encoder which is truncated such that the output is the unnormalized attention matrix for a single head. A diagram of this model is shown in Figure 1. We computed the loss as the $l_1$ difference between the output matrix and the matrix $A_{i,j} = e^{-(\\frac{|x_i - x_j|}{p})^2}$. This corresponds to the prenormalized softmax of the negative square of the relative distance between points, as predicted by our theory. The data consisted of 10,000 \"structures\", each with five 3-dimensional points with coordinates randomly selected between 0 and 200. Unless otherwise indicated, the Transformer encoder for all experiments has three layers (is truncated at the third layer), an embedding dimension of 256, a feedforward dimension of 1,024, 8 heads, pre-normalization and ReLU activation. The models were trained with a batch size of 16 using the Adam optimizer with a peak learning rate of $4\\times10^{-4}$ which is reached after 4,000 warmup steps, and then is quadratically decayed.\nAs in AlphaFold3 (Abramson et al., 2024), we transform the input structures before they are passed through the model. Whenever a structure is loaded, its points are recentred, randomly rotated, and rescaled by a factor of $\\frac{1}{200}$. This has two benefits. First, recentering and rescaling the points ensures that all coordinates stay relatively small, even before the embedding layer has learned an appropriate mapping. Second, recentering and randomly rotating gives the model resilience to translations and rotations which encourages it to learn a distance measure which is truly SE(3)-invariant."}, {"title": "3.1.1 Transformers can attend to |\ud835\udc65\ud835\udc56 - \ud835\udc65\ud835\udc57|2", "content": "In Section 2, we show that Transformers are theoretically capable of learning Gaussian functions of distance. We experimented with learning $e^{-|x-x'|^p}$ for different powers $p$, ranging from 0.5 to 4, in increments of 0.5. Figure 2a shows the relationship between $p$ and the validation loss. As expected, Transformers can learn to reproduce the $e^{-|x-x'|^2}$ attention matrix most accurately which shows that a Gaussian is the most natural way for Transformers to learn to filter attention spatially."}, {"title": "3.1.2 Transformers need \ud835\udc5b + 2 embedding dimensions to learn distance in \u211d\ud835\udc5b", "content": "Next, we explored how large a spatial embedding must be to learn a good approximation of distance for $\\mathbb{R}^n$. For the case of 1 spatial dimension, we provide a theoretical 4-dimensional embedding. We trained models"}, {"title": "3.1.3 SE(3) transformations improve learned SE(3)-invariance", "content": "We investigated whether Transformers will learn to overfit training data in a low data regime and if this can be prevented. This could also correspond to a scenario where there is only a strong structural signal in a small number of training examples. We reduced the number of training points to 100 and measured the training and validation loss. To test the importance of data augmentation, we trained models with and without the random rotations. The raw coordinates clearly demonstrate overfitting while the randomly rotated coordinates show near perfect alignment between training and validation loss. Importantly, this form of data augmentation does not require creating new data points, only rotating the training data each epoch.\nWe measured the average $l_1$ distance between predictions of randomly rotated structures for the models trained with and without random rotations, as a measure of SE(3) divergence. In both cases the SE(3) divergence was almost the same as the validation loss, indicating that randomly rotating training structures reduces overfitting by encouraging models to learn an SE(3)-invariant measure of distance."}, {"title": "3.2 Proteins", "content": "Proteins are a natural fit for structural Transformers because they are composed of linear sequences embedded in 3D space. As such, their properties depend on both sequential and structural features. We considered two tasks in protein modelling: predicting masked tokens as a pretraining objective and predicting protein function conditioned on embeddings generated by pretrained models. For all protein experiments we used the GO PDB dataset from DeepFRI (Gligorijevi\u0107 et al., 2021) which comprises ~36K protein chains."}, {"title": "3.2.1 Pretraining a structural protein language model", "content": "To test a Transformer's ability to learn useful structural patterns in proteins, we trained an ESM/BERT- style (Rives et al., 2021; Devlin et al., 2019) model to complete masked token prediction. We trained two models: one with coordinates ('coords model') and the other without (\u2018non-coords model'). The version with coordinates added a linear embedding of the coordinates to the token embedding in the same way as the simulated experiments. Both models were very similar to the smallest publicly released ESM1 model, consisting of a 6-layer Transformer encoder with a hidden dimension of 768, 12 attention heads, a feedforward dimension of 2048, and GeLU activation (Hendrycks & Gimpel, 2023). As in ESM, we omitted dropout. To prevent the model from focusing too much on linear positional information, we used Sinusoidal Positional Encodings rather than Rotary Positional Encodings (Su et al., 2022). As is common in masked token prediction, we masked 15% of tokens. Of the masked tokens, 80% were replaced with a [MASK] token, 10% were replaced with a random amino acid, and 10% were left unchanged. We clustered the data by 50% sequence identity using MMSeqs2 (Steinegger & S\u00f6ding, 2017) and randomly held out 1% of the clusters to use as a validation set. We trained each model for 100 epochs with a fixed batch size of 24, resulting in approximately 150K updates. We used the Adam optimizer with 4,000 warmup steps to a peak learning rate of 2.3\u00d710-4, followed by inverse square decay. Each time a structure was loaded, its coordinates were recentred, randomly rotated, and rescaled.\nAs shown in Figure 4a, adding coordinates substantially improved the model, leading to a final training perplexity of 6.5 with coordinates vs 11.9 without. The final training loss for the version without coordinates (after 100 epochs) was surpassed by the version with coordinates after 8 epochs. Additionally, the final validation loss was surpassed after only 4 epochs which may indicate that the structural features learned early in training are more robust to dissimilarity in sequence space.\nWe also investigated the difference in sequence recovery rates between the two models. The total sequence recovery rate was ~23% for the non-coords model compared to ~38% for the coords model. Figure 4b shows a breakdown of the recovery rates per amino acid type. The recovery rate for the coords model was greater than or equal to that of the non-coords model for all amino acid types. The difference was particularly stark for glycine and proline, which may be related to their distinct backbone conformational preferences (Ho & Brasseur, 2005; Beck et al., 2008)."}, {"title": "3.2.2 Pretrained models learn to measure distance", "content": "In the model trained in the previous paragraph, there are three inputs to the model for each token: amino acid type, sequential position, and 3D coordinates. To test if the pretrained model with coordinates was learning to measure distance as predicted, we plotted the average attention paid by each pair of tokens across all heads in a layer as a function of distance. We also plotted the average attention paid to each token as a function of relative sequence distance. To isolate the effect of each feature, we fixed all amino acids to alanine. We also fixed the linear sequence index to a constant value for all tokens while measuring 3D dependence and fixed the 3D position to (0,0,0) while measuring linear dependence. For the distance measurements, we rounded each pairwise distance to the nearest Angstrom and computed the average for each distance value."}, {"title": "3.2.3 Protein function prediction", "content": "Finally, we tested whether the pretrained protein model embeddings could improve accuracy on a downstream task. We trained models to predict protein molecular function Gene Ontology labels (Ashburner et al., 2000)."}, {"title": "4 Conclusions", "content": "In this work we show that standard Transformers are capable of performing structural reasoning by learning an approximately SE(3)-invariant distance filter on attention. We predict that even linearly embedded positions can produce Gaussian attention filters of distance and validate this prediction using experiments on simulated points and proteins. The protein model naturally learns to use the 3D coordinates to measure distance which substantially improves its ability to predict masked tokens. The structural information also materially improves the model's ability to inform function prediction, providing even greater benefit than existing custom-built structural models.\nWe show that Transformers can learn to measure distance and operate as hybrid structure/language models. In contrast to many conventional structure models which are based on GNNs, Transformers do not explicitly model edges. This admits memory-efficient implementations such as FlashAttention (Dao et al., 2022; Dao, 2023) which allow for fast, fully-connected updates in linear memory. Most structure models store distance in edges which use quadratic memory for fully-connected graphs. Practically, this means that Transformers can perform structural reasoning on more highly connected structures, which may allow them to \"see\" more while making decisions.\nAs shown in Section 3.2.2, the pretrained protein model trained with coordinates showed a strong positional dependence in attention in early layers followed by a weak positional dependence in the last few layers. It is possible that this corresponds to the model identifying structural features such as secondary structure and local physics, before encoding these and performing long-range sequential processing. This corresponds with the contemporary trend of preprocessing structural information to create structural tokens for Transformers compared to the more traditional approach of using language model embeddings as input to structural GNNs.\nIn this work we explore two protein tasks: masked token prediction and function prediction. Virtually all protein learning tasks benefit from combined sequence and structure processing and so this work could be applied across areas including inverse folding, structure prediction, and arbitrary property prediction. As is common in tasks such as inverse folding, the input structures could include more atoms from the backbone. This could be achieved by simply projecting these atom coordinates to the input representation, unlike GNN- based methods which require explicitly including all pairwise distances in the edge features. Additionally, while proteins are a natural fit for structural Transformers due to their combined sequential and spatial data, there are many other possible applications of this type of model. Some of these include tasks with explicit 3D information such as small molecules and 3D objects. However, there are also tasks where learning an approximate relationship between entities in Euclidean space could help with reasoning, such as vision Transformers (Dosovitskiy et al., 2021) or even large language models."}, {"title": "5 Reproducibility Statement", "content": "Proofs for claims made in Section 2 are available in the Appendix. Code to replicate the experiments is available in an anonymous zipped directory in the supplementary materials. This includes code for models, data download/processing, model training, and evaluation."}, {"title": "A Appendix", "content": ""}, {"title": "A.1 Coordinates can be rescaled for better approximations", "content": "The validity of the approximations shown so far depends on the coordinates being small.  Figure A1 shows how well $(c LN(E_{lin}(x_1))) \\cdot cLN(E_{lin}(x_2))$ approximates a quadratic as a function of $c$ and how well the resulting exponential approximates a Gaussian. The scaling parameter $c$ can be learned by the input and output linear maps of the embedding or by the LayerNorm gain parameters. In this way, all coordinates can be rescaled such that the previous sections produce arbitrarily good approximations."}, {"title": "A.2 Embedding proofs", "content": ""}, {"title": "A.2.1 Trigonometric embeddings", "content": "Consider the embedding $E_{trig}$:\n$E_{trig}(x) = (cos(x), \u2013 cos(x), sin(x), \u2013 sin(x))$\nThen the mean, $\\mu(E_{trig}(x))$, is:\n$\\mu(E_{trig}(x)) = 0$\nand the variance, $\\sigma(E_{trig}(x))$, is:", "latex": ["E_{trig}(x) = (cos(x), \u2013 cos(x), sin(x), \u2013 sin(x))", "\\mu(E_{trig}(x)) = 0"]}, {"title": "A.2.2 Layer normalization can learn approximately quadratic functions of input", "content": "Consider the first-order approximation of $E_{trig}$, $E_{lin}$:\n$E_{lin}(x) = (1, -1, x, -x)$\nWe have\n$\\mu(E_{lin}(x)) = 1 \u2212 1 + x \u2212 x = 0$\n$\\sigma(E_{lin}(x)) = \\sqrt{\\frac{1}{4}(1^2 + (-1)^2 + x^2 + (\u2212x)^2)} = \\sqrt{\\frac{1}{2}(1+x^2)} \\approx \\sqrt{\\frac{1}{2}(1+\\frac{x^2}{4})} = \\frac{1}{\\sqrt{2}}(1 + \\frac{x^2}{8})$", "latex": ["E_{lin}(x) = (1, -1, x, -x)", "\\mu(E_{lin}(x)) = 1 \u2212 1 + x \u2212 x = 0", "\\sigma(E_{lin}(x)) = \\sqrt{\\frac{1}{4}(1^2 + (-1)^2 + x^2 + (\u2212x)^2)} = \\sqrt{\\frac{1}{2}(1+x^2)} \\approx \\sqrt{\\frac{1}{2}(1+\\frac{x^2}{4})} = \\frac{1}{\\sqrt{2}}(1 + \\frac{x^2}{8})"]}, {"title": "A.2.3 Gated linear units provide a better approximation", "content": "Lemma A.1. ReGLU and SwiGLU can produce functions of $x^2$. In particular:\n$ReGLU(x) + ReGLU(-x) = SwiGLU(x) + SwiGLU(-x) = x^2$\nProof.\n$ReGLU(x) + ReGLU(\u2212x) = max(0,x)x + max(0, \u2212x)x$\n$= max(\u2212x,x)x = xx = x^2$\nSimilarly,", "latex": ["ReGLU(x) + ReGLU(-x) = SwiGLU(x) + SwiGLU(-x) = x^2", "ReGLU(x) + ReGLU(\u2212x) = max(0,x)x + max(0, \u2212x)x", "= max(\u2212x,x)x = xx = x^2"]}, {"title": "A.4 Extended experiments", "content": "Here, we report the results of two additional experiments on predicting biological process (Table 2) and molecular function (Table 3) labels, also from the DeepFRI dataset. We were unable to compare these results to DeepFRI because the PDB-only results were not reported. As in the molecular function experiments, the inclusion of structure improved the performance of all models. In these experiments, the performance of the MLP and finetuned Transformers were comparable, and the sequence-only MLP outperformed the sequence-only Transformer for cellular component prediction. It is possible that this is the result of the more expressive Transformer model overfitting to sequence training data, which is then mitigated by the inclusion of structure."}, {"title": "A.5 Model parameter counts", "content": "In Table 4, we list the parameter counts for all models used in the paper. The simulated parameter counts will vary slightly as described in the details of each experiment. The pretrained models have the same number of parameters both with and without coordinates. The finetuned models have a slightly higher number of parameters than the pretrained because of the final linear layer which projects to the number of classes. The MLP parameter counts are relatively low because they are conditioned on the (fixed) pretrained embeddings."}]}