{"title": "Learning Infinitesimal Generators of Continuous Symmetries from Data", "authors": ["Gyeonghoon Ko", "Hyunsu Kim", "Juho Lee"], "abstract": "Exploiting symmetry inherent in data can significantly improve the sample efficiency of a learning procedure and the generalization of learned models. When data clearly reveals underlying symmetry, leveraging this symmetry can naturally inform the design of model architectures or learning strategies. Yet, in numerous real-world scenarios, identifying the specific symmetry within a given data distribution often proves ambiguous. To tackle this, some existing works learn symmetry in a data-driven manner, parameterizing and learning expected symmetry through data. However, these methods often rely on explicit knowledge, such as pre-defined Lie groups, which are typically restricted to linear or affine transformations. In this paper, we propose a novel symmetry learning algorithm based on transformations defined with one-parameter groups, continuously parameterized transformations flowing along the directions of vector fields called infinitesimal generators. Our method is built upon minimal inductive biases, encompassing not only commonly utilized symmetries rooted in Lie groups but also extending to symmetries derived from nonlinear generators. To learn these symmetries, we introduce a notion of a validity score that examine whether the transformed data is still valid for the given task. The validity score is designed to be fully differentiable and easily computable, enabling effective searches for transformations that achieve symmetries innate to the data. We apply our method mainly in two domains: image data and partial differential equations, and demonstrate its advantages.", "sections": [{"title": "1 Introduction", "content": "Symmetry is fundamental in many scientific disciplines, crucial for understanding the structure and dynamics of physical systems, datasets, and mathematical models. The ability to uncover and leverage symmetries has become increasingly important in machine learning and scientific research due to its potential to improve model efficiency, generalization, and interpretability. By capturing inherent symmetrical properties, models can learn more compact and informative representations, leading to improved performance in tasks like supervised learning [31, 29, 4, 7, 33], self-supervised learning [8, 15, 23], and generative models [19, 11, 18].\nPrevious methods for learning symmetry have often relied on the explicit parameterization of group representations based on predefined generators, which can be limited in capturing various symmetries, including transformations that do not align along the generators. For example, when searching for Lie group symmetries in images or physics data, existing methods [3, 34] parameterize a group action $g$ as the matrix exponential of a linear combination of linear or affine Lie algebra generators $L_i$ with their learnable coefficients $w_i$ as $g = \\exp(\\sum_i w_i L_i)$. In the affine transformations of images in"}, {"title": "2 Preliminaries: One-parameter Group", "content": "In this section, we present the basic definitions of a one-parameter group, which we use to parameter-ize the symmetric transformations learned from the data.\nConsider an unknown Euclidean domain $Z \\subset \\mathbb{R}^n$ and a smooth vector field $V : Z \\rightarrow \\mathbb{R}^n$. A path $\\gamma : I = (a, b) \\subseteq \\mathbb{R} \\rightarrow Z$ satisfying $\\frac{d}{ds}\\gamma(s) = V(\\gamma(s))$ for all $s \\in I$ is a curve that travels around the domain $Z$ with a velocity given by the vector field $V$. Along the curve $\\gamma$, a point $x = \\gamma(a_0)$ can be transported to $\\gamma(a_0 + s)$ by flowing along the vector field $V$ by time $s$. We define the flow $\\vartheta_V^s$ by $\\vartheta_V^s(x) = \\gamma(a_0 + s)$ of $V$ as in Figure 2. This flow is computed by solving an ODE\n$\\frac{d}{ds} \\vartheta_V^s (x) = V(\\vartheta_V^s(x))$        (1)\nwith initial condition $\\vartheta_V^0(x) = x$ for all $x \\in Z$ [20].\nThe flow $\\vartheta_V^s$ is governed by an autonomous ODE, i.e., an ODE independent of the temporal variable $s$. Due to properties of autonomous ODEs, the flow $\\vartheta_V^s$ exists uniquely and it is smooth in both variables $s$ and $x$. Assuming a mild condition on $V$, such as $V$ extends to a compactly supported vector field $\\overline{V}$ on $\\mathbb{R}^n$, the ODE does not terminate in $\\mathbb{R}^n$ in finite time and hence $\\vartheta_V^s$ is defined for all $s \\in \\mathbb{R}$. In that case, the flow satisfies a group equation\n$\\vartheta_V^{s_1+s_2}(x) = \\vartheta_V^{s_1} \\circ \\vartheta_V^{s_2}(x)$    (2)\nfor all $s_1, s_2 \\in \\mathbb{R}$. It means that the flow can be regarded as a group action of $\\mathbb{R}$ on $\\mathbb{R}^n$, transforming elements of $Z \\subset \\mathbb{R}^n$. For this reason, $\\vartheta_V^s$ is also called a one-parameter group, and the vector field $V$ is called an infinitesimal generator of the one-parameter group.\nOn $Z = \\mathbb{R}^n$, a constant vector field $V(x) = v \\in \\mathbb{R}^n$ gives rise to a translation $x \\rightarrow x + sv$. For a matrix $A \\in \\mathbb{R}^{n \\times n}$, a vector field $V(x) = Ax$ gives rise to an affine transformation $x \\rightarrow \\exp(sA)x$, where $\\exp$ is the matrix exponentiation. Multiple infinitesimal generators may span a vector space $\\mathfrak{g}$, and if $\\mathfrak{g}$ satisfies some algebraic condition (closure under the Lie bracket), then $\\mathfrak{g}$ forms a Lie algebra. Composing the elements of one-parameter groups of elements in $\\mathfrak{g}$ gives rise to a Lie group $G$. The correspondence between $G$ and $\\mathfrak{g}$ is called Lie group-Lie algebra correspondence.\nContinuous symmetries are commonly defined by a Lie group $G$, acting on some domain and keeping the transformed objects invariant with respect to some criterion. We model symmetries by specifying their infinitesimal generators whose composition of one-parameter groups comprises the symmetries of that domain.\nBelow, we describe two representative examples that will be discussed extensively in the remainder of the paper: images (interpreted as functions on 2D planes) and PDEs."}, {"title": "2.1 Images and Their Symmetries", "content": "Consider a rescaled image of the form $f : X = [-1,1]^2 \\subseteq \\mathbb{R}^2 \\rightarrow Y = [0,1]^3 \\in \\mathbb{R}^3$. The affine transformations on $\\mathbb{R}^2$ have the form $x = (x_1,x_2) \\rightarrow Ax + b$ for a matrix $A \\in \\mathbb{R}^{2 \\times 2}$ and a vector $b \\in \\mathbb{R}^2$. The Affine transformations form the 6-dimensional Affine group $Aff(2)$, and it has a corresponding 6-dimensional Lie algebra having a basis $\\{L_1,..., L_6\\}$ given as in Table 1. The symmetries of images are often exploited as a data augmentation strategy for learning image classifiers, under an assumption that the transforms do not alter the identity or semantics of the images to be classified."}, {"title": "2.2 PDEs and Their Symmetries", "content": "Given an $n$-dimensional independent variable $x = (x_1,\\cdots, x_n) \\in X \\subseteq \\mathbb{R}^n$ and an $m$-dimensional dependent variable $u = u(x) = (u_1(x),\\cdots, u_m(x)) \\in U \\subseteq \\mathbb{R}^m$, we denote by $u^{(i)}$ the collection of all $i$-th partial derivatives of $u$ with respect to $x$. A partial differential equation $\\Delta$ on $u(x)$ of order $k$ is defined by a set of algebraic equations $\\Delta(x, u, u^{(1)}, \\cdots, u^{(k)}) = 0$ involving all the variables and their partial derivatives. For example, two scalar independent variables $x, t \\in \\mathbb{R}$ and one scalar dependent variable $u(x, t) \\in \\mathbb{R}$ governed by equation $\\Delta = u_t + uu_x + u_{xxx} = 0$ gives the 1-dimensional Korteweg-de Vries (KdV) equation, where we denote partials using subscripts, e.g., $u_t = \\frac{\\partial u}{\\partial t}$ and $u_{xxx} = \\frac{\\partial^3 u}{\\partial x^3}$. The KdV equation is commonly used to model the dynamics of solitons, e.g. shallow water waves [35]. The KdV equation described above is an example of 1-dimensional scalar-valued evolution equation. Such an equation takes the form $u = u(x, t) \\in \\mathbb{R}$ with its governing equation of the form\n$u_t = F(x, t, u, u_x, u_{xx}, u_{xxx},\\cdots)$    (3)\nfor some function $F$. In this paper, we only deal with 1D scalar-valued evolution equation on a fixed periodic domain $x \\in [0, L]$.\nContinuous symmetries of PDEs are commonly parametrized by a one-parameter group on $X \\times U$. Denote $(\\xi, \\mu) = (\\xi[x, u], \\mu[x, u])$ an infinitesimal generator defined on $X \\times U$. Then the PDE $\\Delta$ possesses the infinitesimal generator of symmetry $(\\xi, \\mu)$ if the equation is still satisfied after transforming both the independent variable $x$ and the dependent variable $u$ [27, 5, 26]. Symmetries of PDEs are categorized by how the generators $(\\xi, \\mu)$ depend on $(x, u)$. The symmetry is a Lie point symmetry (LPS) if the value of $(\\xi, \\mu)$ at each point $(x, u(x))$ depends only on the point value $(x, u(x))$ itself. If $(\\xi, \\mu)$ also depends on the derivatives $u^{(1)},\\cdots, u^{(k)}$ at that point, it is called a Lie-B\u00e4cklund symmetry or generalized symmetry. If $(\\xi, \\mu)$ depends on integrals of $u$, then it is called a nonlocal symmetry. Finding an LPS of a PDE $\\Delta$ can be done algorithmically under some mild assumptions on $\\Delta$. However, there is no general recipe of finding Lie-B\u00e4cklund symmetries or nonlocal symmetries, and discovering such symmetries remains an active area of research."}, {"title": "3 Related Work", "content": "Symmetry discovery. Approaches to learning symmetries can be categorized by addressing two questions: (a) where do they search for symmetries, and (b) what are they aiming to learn. One line of research aims to learn ranges, focusing on determining the ranges of transformation scales that enhance learning when employed as augmentation techniques. For example, Benton et al. [3] learns transformation ranges of predefined transformations by treating them as learnable parameters and backpropagating through differentiable transformations.\nAnother line of research aim to learn subgroups of bigger candidate groups, typically a linear group $GL(n)$ or an affine group $Aff(n)$. For example, Desai et al. [10] use the Generative Adversarial Network (GAN) to search for symmetries, with the generator transforming data by group elements sampled from the candidate group and the discriminator verifying whether the transformed data sill lies in the data distribution. Similarly, Yang et al. [34] employ the GAN approach, but generator of GAN models infinitesimal generators instead of the subgroup itself. As an alternative, Moskalev et al. [24] proposed an idea of extracting symmetries from learned neural network by differentiating through it, and retrieved 2D rotation in the linear group using the rotation MNIST dataset.\nFinally, learning symmetries with minimal assumption, i.e. without assuming the infinitesimal generators are linear or affine, is an area of large interest. An early attempt of Rao & Ruderman [28] models infinitesimal generator by a learnable matrix from the pixel space to the pixel space, and learn 2D rotation by solving a task that compares original images and rotated ones, where the images are 5 \u00d7 5 random pixels. Sohl-Dickstein et al. [32] takes the similar approach with eigen-decomposing the learnable matrix. Dehmamy et al. [9] builds a convolution operation whose kernel encodes learnable infinitesimal generators, and retrieved 2D rotation from random 7 \u00d7 7 images by comparing original and transformed ones. Our work closely aligns with Liu & Tegmark [22] and Forestano et al. [13], which model one-parameter groups by an MLP and learn the symmetries from an invariant scalar quantity. To the best of our knowledge, learning correct symmetries with minimal assumption was only achieved with toy datasets, far from real-world datasets such as CIFAR-10."}, {"title": "4 Learning Continuous Symmetries with One-Parameter Groups", "content": "Given a learning task with a dataset $D \\subset A$ in an underlying space $A = \\{f|f : X \\rightarrow Y\\}$, we aim to model symmetry by a one-parameter group $\\vartheta_V^s$ acting on $A$, as explained in \u00a7 4.3. We define a continuous symmetry by stating that $\\vartheta_V^s$ is a symmetry of this task if there exists some $\\sigma > 0$ such that for any data point $f \\in D$ and transformation scale $s \\in [-\\sigma, \\sigma]$, the transformed data point $\\vartheta_V^s (f)$ remains valid for this task. We assume the existence of a differentiable validity score $S(\\vartheta_V^s, f) \\in \\mathbb{R}$, such that $\\vartheta_V^s (f) \\in A$ is valid if $S(\\vartheta_V^s, f) < C$ for a certain threshold $C \\in \\mathbb{R}$. Then, a one-parameter group $V$ is a symmetry of the task if $S(\\vartheta_V^s, f) < C$ for all $f \\in D$.\nThe validity score depends on the nature of the target task, though no strict criterion exists. As long as it is differentiable and the valid data aids learning, it is considered acceptable. For instance, we can define the validity based on a negative log-likelihood of a probabilistic model. In \u00a7 4.2, we discuss the validity scores to be used for image and PDE data.\nOnce a validity score is defined, we learn a symmetry $V^*$ by minimizing the validity scores of transformed data,\n$V^* = \\arg \\min_V \\mathbb{E}_{f \\sim D, s \\sim Unif([-\\sigma, \\sigma])} [S(\\vartheta_V^s, f)],$         (4)\nwhere the $\\arg \\min$ is taken over the entire class of smooth one-parameter groups. Since the learning is performed in function space, we appropriately constrain the function space using a regularizer, as described in \u00a7 4.4. Once symmetries are learned, they reveal the symmetrical properties of the target task, which can then be exploited to augment the training data."}, {"title": "4.2 Task-specific Definition of Validity Score S", "content": "Images. In image-related tasks, we define a validity score using a pre-trained neural network. Let $D$ be an image classification dataset consisting of data of the form $(f, y) \\in A \\times \\mathbb{R}$, where $f$ is an image and $y$ is a label. Also let $H_{cls} \\circ H_{fext} : A \\rightarrow \\mathbb{R}$ be a learned neural network, where we denote by $H_{fext} : A \\rightarrow \\mathbb{R}^k$ the feature extractor and $H_{cls} : \\mathbb{R}^k \\rightarrow \\mathbb{R}$ the classifier. We define the validity score $S(\\vartheta_V^s, f)$ as the cosine similarity between the features before and after the transformation:\n$S(\\vartheta_V^s, f) = sim(H_{fext}(\\vartheta_V^s (f)), H_{fext}(f)),$        (5)\nwhere $sim$ is the cosine similarity defined as $sim(v_1, v_2) = \\frac{v_1 \\cdot v_2}{||v_1|| ||v_2||}$ for all $v_1, v_2 \\in \\mathbb{R}^k \\backslash \\{0\\}$.\nNgrid\nPDEs. Let $u(x)$ be a solution of a given PDE $\\Delta$, discretized on a rectangular grid $X_{grid} = \\{x_i\\}_{i=1}^{N_{grid}}$. For a transformed data $\\vartheta_V^s(u)$, we measure the violation of the equality $\\Delta = 0$ to assess whether the transformed data is still a valid solution. Using an appropriate numerical differentiation method, we directly compute the value of the PDE, denoted as $\\Delta(\\vartheta_V^s(u))$, which represents the error of $\\vartheta_V^s (u)$ as a solution of $\\Delta$, taking a value $\\Delta(\\vartheta_V^s(u))_i$ at grid point $x_i$. The validity score is defined by the summation of all PDE errors across the grid points:\n$S(\\vartheta_V^s, u) = \\sum_i |\\Delta(\\vartheta_V^s(u))_i|.$    (6)"}, {"title": "4.3 Parametrization of One-Parameter Groups using Neural ODE", "content": "On a Euclidean domain $Z \\subset \\mathbb{R}^n$, we model an infinitesimal generator with an MLP $h_\\theta : Z \\subset \\mathbb{R}^n \\rightarrow \\mathbb{R}^n$. The infinitesimal generator $h_\\theta$ gives rise to a one-parameter group $\\vartheta_{h_\\theta}^s$. We sample a transformation scale $a \\sim Unif([-\\sigma, \\sigma])$ for a predefined hyperparameter $\\sigma \\in \\mathbb{R}_{>0}$. To transform a point $x \\in Z$ along this one-parameter group by an amount $a \\geq 0$, we use a numerical ODE solver to solve the ODE for $\\gamma : [0, a] \\rightarrow Z$ satisfying\n$\\gamma'(s) = h_\\theta(\\gamma(s)), \\forall s \\in [0, a], \\gamma(0) = x$   (7)\nand obtain a transformed data point $x' = \\vartheta_{h_\\theta}^a (x) = \\gamma(a)$. If $a < 0$, we compute $\\vartheta_{h_\\theta}^a (x) = \\vartheta_{-h_\\theta}^{-a} (x)$ by integrating $-h_\\theta$ instead of $h_\\theta$ using the ODE solver. We can backpropagate through the numerical ODE solver using the adjoint method [6] to learn $\\theta$.\nLet $f : X \\rightarrow Y$ be a data point on a domain $A$. As $A$ is a space of functions, na\u00efvely modeling symmetry on $A$ may ignore the geometry implied in the input space $X$. Instead, we define two transformations: $\\delta_X$ on $X$ and $\\delta_Y$ on $Y$, and induce a transformation of $f$ by\n$(\\vartheta_V(f))(x) = f(\\delta_X(x)), (\\delta_Y(f))(x) = \\delta_Y(f(x)),$  (8)\nwhere we abuse notation and write the transformed function as $\\vartheta_X(f)$ and $\\delta_Y(f)$. For an image represented as a discretized function $f : X \\rightarrow Y$ from $X = [-1,1]^2$ and $Y = [0,1]^3$, $\\vartheta_X$ corre-sponds to spatial transformations such as translation or rotation, and $\\delta_Y$ corresponds to color space transformations. For a PDE, a 1D scalar-valued evolution equation on a fixed periodic domain takes the form $u(x,t) \\in U = \\mathbb{R}$ with $(x, t) \\in [0, L] \\times [0,T] = X \\subseteq \\mathbb{R}^2$, and we parameterize an infinitesimal generator on a product space $X \\times U \\subset \\mathbb{R}^3$ by an MLP. Then, a transformation on $(x, t, u) \\in X \\times U$ induces a transformation on the solution of the PDE $u(x, t)$."}, {"title": "4.4 Objective Functions", "content": "Symmetry loss. Let $N_{sym}$ be the number of symmetries to be learned. Let $(h_\\theta^{(a)})_{a=1}^{N_{sym}}$ be the infinitesimal generators computed from a single MLP. For each $a \\in \\{1, ..., N_{sym}\\}$, we sample a transformation scale $s_a \\sim Unif([-\\sigma, \\sigma])$ to transform $f$ via numerical integration. The parameter $\\theta$ is optimized by minimizing the average validity score over the training data,\n$L_{sym}(\\theta) = \\sum_{a=1}^{N_{sym}} \\mathbb{E}_{f \\sim D, s_a \\sim Unif([-\\sigma, \\sigma])} \\bigg[S(\\vartheta_{h_\\theta^{(a)}}^{s_a} (f))\\bigg]$   (9)\nOrthonormality loss. Learning only with the symmetry loss may result in trivial solutions such as the zero vector field or the same vector field repeated in multiple slots. To prevent this, we introduce the orthonormality loss to regularize the model towards learning orthonomral vector fields. Specifically, given two vector fields $V_1, V_2 : Z \\rightarrow \\mathbb{R}^n$, we define an inner product as,\n$\\langle V_1, V_2 \\rangle = \\frac{1}{vol(Z)} \\int_Z w(x)(V_1(x) \\cdot V_2(x)) dx \\approx \\frac{1}{|Z_{grid}|} \\sum_{x_i \\in Z_{grid}} w(x_i) (V_1(x_i) \\cdot V_2(x_i)),$ (10)\nwith a suitable weight function $w(x) : Z \\rightarrow \\mathbb{R}$ and a discretized grid $Z_{grid}$ of $Z$ of size $|Z_{grid}|$. Given this definition, we first normalize each generator by its norm to ensure $||h_\\theta^{(a)}||_2 = 1$. Then we compute the orthonormality loss as,\n$L_{ortho}(\\theta) = \\sum_{1 \\leq a < b \\leq N_{sym}} \\Big\\langle sg\\big(h_\\theta^{(a)}\\big), h_\\theta^{(b)} \\Big\\rangle,$      (11)\nwhere $sg(\\cdot)$ denotes the stop-gradient operation to ensure that the constraint $\\langle h_\\theta^{(a)}, h_\\theta^{(b)} \\rangle = 0$ only affects the latter slot (b). By doing this, if the true number of symmetries $N_{sym}$ is less than or equal to the assumed number of symmetries $\\overline{N}_{sym}$, the learned symmetries will be aligned in the first $\\overline{N}_{sym}$ slots."}, {"title": "Lipschitz loss.", "content": "We further introduce inductive biases to the infinitesimal generators we aim to learn. For instance, an infinitesimal generator moving only a single pixel near the boundary by a large scale would be undesirable. This idea can be implemented using Lipschitz continuity. For a grid point $x_i \\in Z_{grid}$ and its neighboring point $x_j \\in nbhd(x_i) \\subset Z_{grid}$, we expect the vector field $V$ to satisfy the Lipschitz condition,\n$\\frac{||V(x_i) - V(x_j)||}{||x_i - x_j||} \\leq \\tau \\text{ where } Lips(V; x_i, x_j) = \\frac{||V(x_i) - V(x_j)||}{||x_i - x_j||}.$            (12)\nTo regularize the model toward the Lipschitz condition, we introduce the Lipshictz loss,\n$L_{Lips} (\\theta) = \\sum_{a=1}^{N_{sym}} \\sum_{x_i \\in Z_{grid}, x_j \\in nbhd(x_i)} \\max(Lips(h_\\theta^{(a)}; x_i, x_j) - \\tau, 0).$         (13)"}, {"title": "Total loss and loss-scale-independent learning.", "content": "We jointly minimize the three loss functions with suitable weights $w_{sym}, w_{ortho}, w_{Lips} > 0$ and learn the weights $\\theta$ of MLP using a stochastic gradient descent:\n$\\theta^* = \\arg \\min_\\theta w_{sym}L_{sym}(\\theta) + w_{ortho}L_{ortho}(\\theta) + w_{Lips} L_{Lips}(\\theta).$  (14)\nTo minimize the computational burden of hyperparameter tuning, we ensure that all the loss terms have a natural scale, i.e. a dimensionless scale independent of the context. For example, when penalizing the inner product in Equation 11, we apply arccos to the normalized inner product to ensure the loss term lies in $[0, \\pi/2)$. Similarly, the scale of the PDE validity score $S(\\vartheta_V^s, u)$ in Equation 9 depends on the scale of the data $u$. When penalizing it, we apply the log function so that the gradients are scaled automatically as $\\nabla_\\theta \\log(S(\\vartheta_V^s, u)) = \\nabla_\\theta S(\\vartheta_V^s, u) / S(\\vartheta_V^s, u)$.\nHere we describe the generic training process, but the actual implementation requires non-trivial task-specific designs, such as the choice of the weighting function $w(x)$ or the method for locating the transformed data on the target grid. We defer these details for image and PDE tasks to Appendix A."}, {"title": "4.5 Comparison With Other Methods", "content": "Here, we compare our method with other recent symmetry discovery methods. The differences mainly arise from (a) what they aim to learn (e.g., transformation scales or subgroups from a larger group) and (b) their assumptions about prior knowledge (e.g., complete, partial, or no knowledge of symmetry generators). Another important distinction is the viewpoint on symmetry: some methods learn symmetries that raw datasets inherently possess (implicit), while others learn symmetries from datasets explicitly designed to carry such symmetries (explicit).\nWe emphasize that our method excels in two key aspects: (a) our learning method reduces infinitely many degrees of freedom, (b) our method works with high-dimensional real-world datasets. For example, while LieGAN [33] and LieGG [24] reduce a 6-dim space (affine) to a 3-dim space (translation and rotation), ours reduces an$\\infty$-dim space to a finite one. L-conv [9] also does not assume any prior knowledge, but it is limited in finding rotation in a toy task, where it learns rotation angles of rotated images by comparing them with the original ones, which are 7x7 random pixel images."}, {"title": "5 Experiments", "content": "We use images of size 32 \u00d7 32 from the CIFAR-10 classification task. Since our method does not model discrete symmetry, we use horizontal flip with 50% probability by default. We train a ResNet-18 model, which will be used as the feature extractor $H_{fext}$ in Equation 5. The weight function on the pixels is computed as explained in Appendix A.1. We expect to find 6 affine generators and we use an MLP modeling 10 vector fields in the pixel space [-1,1]\u00b2 \u2286 \u211d\u00b2, expecting the first six learned vector fields to be the affine generators. We learn the Equation 14 using stochastic gradient descent with $w_{sym}=1$ and $w_{ortho}, w_{Lips}=10$. The parameter $\\sigma$, which controls the scale of transformation, is set to $\\sigma = 0.4$, and the Lipschitz threshold $T$ is set to $\\tau = 0.5$. Other details are described in Appendix B.1. We conducted three trials with random initializations and report the full results in"}, {"title": "5.2 PDEs", "content": "We follow the experimental setting of Brandstetter et al. [4], which use the Korteweg-de Vries (KdV) equation, the Kuramoto-Shivashinsky (KS) equation, and the Burgers' equation on a 1D periodic domain as experiments. They all have time translation, space translation, and the Galilean boost as LPSs. To consider PDEs with non-trivial and non-affine symmetries, we add two variants of the KdV, namely the nKdV equation and the cKdV equation. The nKdV is yielded by a nonlinear time translation of the original KdV, and the cKdV is the cylindrical KdV equation, having an extra time-dependent term. The equations are listed in Table 4, and their symmetries are listed in Table 5. Note that some symmetries are ruled out since we work within a fixed periodic domain, e.g., the scaling symmetry of the KdV. Since there are at most three symmetries, we open four slots in an MLP and learn symmetries using weights $w_{sym}, w_{Lips} = 1$ and $w_{ortho} = 3$."}, {"title": "6 Conclusion", "content": "We have introduced a novel method for learning continuous symmetries, including non-affine trans-formations, from data without prior knowledge. By leveraging Neural ODE, our approach models one-parameter groups to generate a sequence of transformations, guided by a task-specific validity score function. This approach captures both affine and non-affine symmetries in image and PDE datasets, enhancing automatic data augmentation in image classification and neural operator learning for PDEs. The learned generators produce augmented training data that improve model performance, particularly with limited training data.\nLimitation. However, despite its flexibility, our method requires careful selection of numerical methods, such as numerical differentiation and interpolation, to ensure stable training and the ODE integration can be computationally large for augmentation generation compared to other augmentation methods. While we focus on image symmetries and LPSs of PDEs, the method could potentially model other symmetries and domains with proper validity scores, suggesting future applications in learning complex symmetries, including conditional and non-local symmetries, in various data types."}, {"title": "A Implementation Details", "content": "Images are functions of the form $f : X \\rightarrow Y$, where $X = [-1,1]^2$ is the spatial domain and $y = [0, 1]^3$ is the normalized RGB domain. Pixels $X_{grid} = \\{x_i\\}_{i=1}^{N_{grid}}$ are discretized through a rectangular grid of $X$, and images $f$ are discretized on the pixel space by $f_i = f(x_i)$ for all $i$.\nw(x) in orthonormality loss. We first learn a symmetry on the spatial domain $X$ using a feature extractor $H_{hidden}$ taken from a pre-trained neural network. One obstacle is that in most image datasets, the main subjects of images are mostly located around the centers, and the regions close to the boundary are filled with backgrounds. In other words, each pixel has a different level of importance, and we may end up learning infinitesimal generators that only move boundary pixels and fix the center.\nWe take into account the importance of pixels using the weight function $w(x) : X \\rightarrow \\mathbb{R}_{>0}$. For a discretized image $f = \\{f_i\\}$ and for each pixel $x_i$, we measure a pixel sensitivity of the image $f$ at the $i$-th pixel up to the feature extractor $H_{fext}$ as\n$Sensitivity (f, x_i) = \\bigg| \\frac{\\partial H_{fext}(f)}{\\partial x_i} \\bigg| = \\bigg| \\frac{\\partial f_i}{\\partial x_i} \\frac{\\partial H_{fext}(f)}{\\partial f_i} \\bigg|$       (15)\nwhich can be computed by querying a Jacobian-vector product of $H_{fext}$ with respect to the image gradient at each pixel $\\frac{\\partial f_i}{\\partial x_i}$. We define the weight $w(x)$ as the average of pixel sensitivity across the dataset:\n$w(x_i) = \\mathbb{E}_{f \\sim D} [Sensitivity (f, x_i)] = \\mathbb{E}_{f \\sim D} \\bigg[ \\bigg| \\frac{\\partial f_i}{\\partial x_i} \\frac{\\partial H_{fext}(f)}{\\partial f_i} \\bigg| \\bigg]$   (16)\nHowever, computing this weight function $w$ needs $N_{grid} \\cdot N_{data}$ times of computation of Jacobian-vector products. Instead, we use a Gaussian kernel $\\kappa(x; \\overline{x}, \\sigma) = \\frac{1}{\\sqrt{2 \\pi} \\sigma} \\exp(-\\frac{||x - \\overline{x}||^2}{2 \\sigma^2})$, with a fixed $\\sigma = 0.1$ and the center $\\overline{x}$ sampled uniformly on $X$, and approximate the weight function by\n$w(x_i) \\approx \\mathbb{E}_{f \\sim D, \\overline{x} \\sim Unif(X)} \\sum_j \\bigg| \\frac{\\partial \\overline{x}_j}{\\partial f_j} \\frac{\\partial H_{fext}(f)}{\\partial \\overline{x}_j} \\bigg| \\kappa(x_i; \\overline{x}, \\sigma)$         (17)\nso that we compute the weight in a stochastic manner. Intuitively, it computes Jacobian-vector product for each larger pseudo-pixel represented by the Gaussian kernels instead of each individual pixel. We iterate over the dataset 20 times, and the computed weight function is shown in Figure 7.\nTraining details. The infinitesimal generator $h_\\theta^{(a)} = h_{\\theta}$ transforms each pixel $x$ into $x_i$ via ODE integration. In this process, the transformed pixels $\\{x_i\\}$ may no longer be located on the rectangular grid $X_{grid}$, so we use the bilinear interpolation method to resample the transformed image $\\vartheta_V^s (f)$ on $X_{grid}$. Note that the bilinear interpolation operation is differentiable, thereby we can train the MLP by minimizing the validity score $S(\\vartheta_V^s, f)$, which is the cosine similarity between the features of $f$ and $\\vartheta_V^s(f)$:\n$S (\\vartheta_V^s, f) = sim (H_{fext}(\\vartheta_V^s (f)), H_{fext}(f)).$  (18)"}, {"title": "PDE Dataset", "content": "Solutions of a 1D scalar-valued evolution equation on a periodic domain take the form $u(x", "t)": "X \\rightarrow U$ for $X = [0, L", "T": "and $U = \\mathbb{R}$, where $[0, L"}]}