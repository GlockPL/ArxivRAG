{"title": "ELMS: Elasticized Large Language Models On Mobile Devices", "authors": ["Wangsong Yin", "Rongjie Yi", "Daliang Xu", "Gang Huang", "Mengwei Xu", "Xuanzhe Liu"], "abstract": "On-device Large Language Models (LLMs) are revolutionizing mobile AI, enabling applications such as UI automation while addressing privacy concerns. Currently, the standard approach involves deploying a single, robust LLM as a universal solution for various applications, often referred to as LLM-as-a-Service (LLMaaS). However, this approach faces a significant system challenge: existing LLMs lack the flexibility to accommodate the diverse Service-Level Objectives (SLOs) regarding inference latency across different applications. To address this issue, we introduce ELMS, an on-device LLM service designed to provide elasticity in both the model and prompt dimensions of an LLMaaS. This system includes: A one-time neuron reordering technique, which utilizes the inherent permutation consistency within transformer models to create high-quality, elastic sub-models with minimal runtime switching costs. A dual-head compact language model, which efficiently refines prompts and coordinates the elastic adaptation between the model and the prompt. We have implemented this elastic on-device LLM service on several off-the-shelf (COTS) smartphones and evaluate ELMS using both standalone NLP/mobile-agent datasets and synthesized end-to-end traces. Across a range of SLOs, ELMS surpasses four strong baselines by up to 16.83% and 11.04% in absolute accuracy on average, with less than 1% Time-To-First-Token (TTFT) switching overhead, comparable memory usage, and fewer than 100 offline GPU hours.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) are ushering in a transformative era for mobile AI. A multitude of killer apps are built on top of LLMs, encompassing mobile UI automation [71], API-calling [23] and screen content comprehension [15]. For instance, one can easily place an order by simply saying \"Order a pizza now from Pizza Hut\" in smartphone.\nWith ever-growing privacy concerns, there is an emerging paradigm of mobile LLM deployment: local LLM as a system service, namely LLM-as-a-Service (LLMaaS) [1, 44, 47, 53, 82].\nAkin to location or notification services, LLM service runs as a standalone system service in background and serves LLM requests (text in, text out) from third-party apps. Such a paradigm is completely feasible thanks to the world knowledge and in-context learning abilities of LLMs. With the OS's visibility into the service, LLMaaS is also resource-efficient. Only one copy of LLM weights is needed in memory, preventing the device memory from being exhausted by app-specific models. With a converged model architecture and operator set, the LLM service can better enjoy the system level scheduling optimizations (e.g., batching or priority queues) and hardware accelerators (e.g., GPUs, DSPs, or NPUs). An industrial example is Google's Android AICore [1], a built-in on-device LLM service in Android OS that has been used by apps like GBoard smart reply and Pixel voice recorder. Similar paradigm can also be found in Apple [3] and Intel [8].\nElastic on-device LLM service. We identify a crucial yet unexplored system feature of on-device LLM service performance elasticity. Specifically, each app calling LLM service demands its own Service-Level Objective (SLO) on inference latency, yet a single static LLM cannot meet the diversified SLOs. Demand of elasticity is further exaggerated and complicated since LLM inference consists of two distinct stages: prefill (prompt processing speed) and decode (token generation speed), whose latencies are measured by Time-To-First-Token (TTFT) and Time-Per-Output-Token (TPOT), respectively. For instance, a chatbot [9] must behave both low TTFT and low TPOT to match human reading speed; a UI-automation agent [71, 84] typically requires a low TTFT and an acceptable TPOT, as TPOT can be overlapped with UI manipulations; a screen-event recorder [15] running in background only needs a tolerable TTFT/TPOT. Failing to meet an SLO leads to serious consequences: a significant degradation of user experience, or failure in the interactions between LLM agents and the environments/tools [3, 23, 84].\nIn a realistic LLM service with elastic feature, an app sends a prompt to the service as well as the SLO expected, and"}, {"title": "Challenges", "content": "However, designing an LLMaaS with efficient elasticity faces the following unique challenges.\n\u2022Costly runtime switching between elasticized models. Elastic service has to frequently switch between the sub-models at request level, yet traditional model elastification methods (e.g. structural pruning) often ignore this switching overhead [21, 42, 46, 72]. For instance, generated by SoTA structural pruning [46], a sub-model of LLaMA-7B with 20% parameters takes 8.2s to switch to 30% on Redmi K60 Champion. The root cause is that, to utilize the deeply optimized on-device NN inference libraries and hardware throughput, the interleaved and overlapping sub-models' weights must be re-layouted to contiguous format in memory before inference (or each sub-model must be maintained an unacceptable standalone copy in memory). In LLMaaS, this switching overhead is billed to TTFT since the switching can only be performed right after the LLMaaS receiving a request.\n\u2022 Sensitive prompt-model orchestration strategy. There exist multiple elastic strategies to meet an SLO, yet their output text quality could differ significantly. Exemplified with a real prompt from ARC_E dataset in Figure 5, although both elastic strategies (50%/20% prompt/model pruning vs. 20%/50% prompt model pruning) can meet the SLO, only the first strategy leads to a correct generated answer. Another instance is that with randomized strategy, the top5 API selection of Octopus dataset exhibits a 15.2% accuracy loss to the oracle strategy on an SLO with 50% TTFT and 80% TPOT of the full LLAMA-7B model. How to orchestrate the two dimensions of elastification to maximize the LLM output quality at request level has not been touched in prior study."}, {"title": "Our solution: ELMS", "content": "We present ELMS, a first-of-its-kind elastic on-device LLM service that tackles the above challenges through following novel techniques.\nOne-shot reordering of permutation consistent units (\u00a73.2). This technique performs pruning on a fine granularity"}, {"title": "2 Background and Motivations", "content": "With ever-growing privacy concerns, recently there is a trend proposing running a single yet powerful (e.g., 7B-size) LLM as a system service (LLMaaS) on mobile devices [1, 44, 47, 53, 82]. In doing so, various apps/agents share the same LLM, minimizing the efforts and resources of preparing and managing task-specific models.\nWe identify a key system challenge of LLMaaS: the LLM requests from different apps or tasks have diversified SLO demands on inference latency, yet current LLMs lack the elasticity to do so. For instance, as listed in Table 1, a chatbot must behave both low TTFT and TPOT in order to match human reading speed. A UI-automation agent requires a relatively low TTFT to generate the first action and an acceptable TPOT, since the following latency can be overlapped with the manipulation of UI elements and thus transparent to users. Besides, these mobile-agents typically only decode few tokens compared to the prompt length. Failing to provide satisfactory latency for a request leads to serious consequences: a significant degradation in user experience, or failure in the interactions between LLM agents and the environment/tools.\nHow to satisfy the heterogeneous demands of different LLM requests, while not degrade the LLM output quality"}, {"title": "2.2 Opportunities and challenges", "content": "We present several observations to show the opportunities and challenges of realizing elastic LLM service.\nObservation#1: LLM inference latency is influenced by two dimensions prompt and model. An LLM inference workload can be divided into two dimensions: prompt (activations) and model (weights). We conduct a measurement of LLaMA-7B inference on Redmi K60 Champion smartphone equipped with Snapdragon 8Gen2 SoC. We use 4 threads (big cores). The \u201cmodel size\u201d here represents a sub-model of LLAMA-7B [68] (e.g., 0.1 means 10% parameters). In Figure 2, we observe that both the two dimensions can influence LLM\ninference latency. TTFT is influenced by both prompt length and model size; TPOT is mainly determined by model size\u00b2.\nObservation#2: LLMs are elasticizable; yet elastic LLM does not necessarily translate to elastic LLM service. DNNs are known to be elasticizable: they can provide various compute-accuracy tradeoffs by a subset of their parameters (known as pruning [29, 46]). For instance, Sheared-LLAMA [76] demonstrates that pruning off 60% parameters of a 7B-size LLM can still retain 74% of original accuracy on ARC_E dataset [24]. LLMPruner [46] further shows that with lightweight Parameter-Efficient Fine-Tuning (PEFT) methods, the accuracy loss incurred by pruning could be recovered. Since pruning (and PEFT) generates sub-models that share the same superset of parameters, there is no overhead of extra memory or pre-training that mentioned in \u00a72.1.\nYet, the challenge is that, since the model upgrades/downgrades itself to adapt to various requests' SLOs, switching between these sub-models is not overhead-free. As shown in Figure 3a, although sub-models share the same superset of parameters, they are no longer contiguous in memory. One may change the deeply optimized dense kernels of on-device NN libraries (e.g., MNN [39] or mllm [12]) to handcrafted sparse kernels. However, these kernels typically undergo degraded performance without fully exploiting the parallelism of processors (e.g., mobile GPUs/NPUs or multi-core CPUs). Another compromising method is to perform a movement of parameter data for each model switching, as shown in Figure 3b. Although the switching overhead is mitigated from iteration/operator level to request level, it is still non-negligible. For instance, movement of LLaMA-7B's a Wo matrix (4096\u00d74096) takes 139 ms on Redmi K60 Champion smartphone in the worst case, and consequently the entire model suffers time overhead at seconds level.\nObservation#3: Prompts of on-device LLM service are also elasticizable. Intuitively, as a natural language sequence, a prompt could still preserve essential semantic information when refined to a shorter one. Especially, the prompts of LLM service callers tend to be verbosely designed in order to maximize the LLM's instruction following [52, 88] and in-context learning [26, 73] abilities. In other words, the prompt dimension can also be elasticized just like the model dimension. We showcase employing a commonly used prompt compression method LLMLingua2 [54] for Octopus [23] dataset, which contains traces of an on-device API-calling agent. LLMLingua2 identifies and preserves most semantically significant tokens by a dedicated language model. We report"}, {"title": "3 ELMS Design", "content": "ELMS aims to provide LLM service that adapts to a specific Service Level Objective (SLO) of resource constraint per request (prompt), while maximizing the service quality (i.e., LLM generation accuracy).\nSLO definition. In this paper, we define SLO of LLM service as a tuple < TTFT, \u03b6TPOT >, where \u03b6 is the compression ratio to full LLMaaS latency. The SLOs that an LLMaaS should serve is pre-defined by the service developers.\nWorkflow. ELMS's idea is to orchestrate model- and prompt-level elastification, both of which need offline preparation. To achieve this, its workflow is designed as a cloud, offline stage and a device, online stage, as shown in Figure 6.\nAt cloud offline stage, on one hand, the model is elasticized to various levels of sub-models that share the memory and can be cost-effectively switched (\u00a73.2). On the other hand, we on-cloud fine-tune a TLM for prompt elastification (\u00a73.3).\nAt device online stage, the elasticized LLM and fine-tuned TLM are deployed on mobile devices as a service. For each LLM service request, the prompt and the corresponding SLO are fed into the fine-tuned TLM. The TLM then outputs a compressed prompt and selects a sub-model with proper size. Finally, an LLM inference that satisfies the given SLO is performed atop the sub-model and the compressed prompt."}, {"title": "3.2 Model elastification", "content": "Units of a neuron network are called permutation consistent units if they can be reordered between each other in a block without affecting the block's input/output.\nThis property indicates that a dense operator kernel can equivalently process these units in arbitrary order without any runtime data re-layout. The rationale behind it is that the Reduce operator (e.g., sum/min/max) satisfies the commutative and associative laws. A basic block that contains such units is y = xW1 W2 in Figure 7. Its permutation consistent unit is a column of W\u2081 together with the corresponding row of W2. If we permute the weights as shown in Figure 7, the intermediate activation xW' will be permuted in response. Nevertheless, W\u2082 is also permuted in the same order, so the multiplication of MatMul operator can still be performed correctly. Since the following addition of MatMul operator is a Reduce operator, the calculated xW'1W' 2 is exactly the same\n\nAs shown in Figure 8, there are two types of permutation consistent units in the main weights of a Transformer layer, i.e., attention heads and MLP neurons, and they are independent to each other. Specifically, the contiguous columns/rows with the same indices in WQ, WK, Wv and Wo (i.e., an attention head) constitute a permutation consistent unit; a column/row with the same index in Wup and Wdown (i.e., an MLP neuron) also constitute a permutation consistent unit. The derivation process is similar to the example in Property 1, Figure 7 - the last operator of attention and MLP blocks is Reduce. Property 2 holds true for mainstream Transformer-based language models with variants like RoPE [59], biased QKV [20], GQA [27] or gated MLP [68].\n Based on Property 1 and Property 2, we propose a novel model elastification method as shown in Figure 9. Its key idea is to \u201catomize\u201d the Transformer into the units shown in Figure 8, and then group them to construct a series of sub-models that each is contiguous in memory.\n\u2022 Specifically, ELMS first profiles importance of each unit offline (detailed later). Since these units are permutation\nconsistent, ELMS freely reorders them by their importance in descending order (if importance is the higher the better), starting from the base address of the weight tensor. Notably, the reordering is only performed intra-block, e.g., reordering the unit of attention heads in the same attention block. Then, ELMS groups these units into sub-models. For example, in Figure 9, sub-model in contains units with indices (not address) \u201c1 5 8 2\u201d, and sub-model in contains \u201c1 5 8 2 3 4 7\". The sub-model sizes and numbers {level,..., levelM} are pre-defined by the LLMaaS developers. In practice, we set this to a fine enough granularity (a global ratio of 20% to 100% in a step size of 10%, by default). Such a ratio is evenly shared by all Transformer layers that need elastification. After that, low-rank adapters [34] are attached to each sub-model to recover potential generation quality loss (if there is any) of these sub-models. We elaborate such a recovery process in the following parts. So far, the LLM weights have been elasticized into a series of high-quality sub-models that run in various speed. We demonstrate the quality of generated sub-models in Figure 10a.\n\u2022 Online. In Figure 9b, the upgrading of model is performed in the following steps: ELMS first detaches the corresponding adapter from sub-model level, which has served the last request. Then, it moves the ending memory pointer of the weights tensor from the address of unit with index \"2\" to \"7\". After that, ELMS attaches another adapter to sub-model level, and the upgrading is finished. Such a process is very cost-effective on mobile devices \u2013 it does not involve any data movement compared to traditional pruning methods, and can still utilize deeply-optimized dense kernels provided by NN libraries. For instance, upgrading Wo to 4096\u00d74096 size only takes 2 ms on Redmi K60 Champion smartphone, while a naive pruning method must undergo a 140ms data movement.\nParameters of neuron networks are known to feature diverse importance. For instance, a weight element with higher magnitude may contribute more to NN capability [32, 36, 43]. Inspired by the concept of eXplainable-AI (XAI) [35, 57, 62], ELMS profiles unit importance with a more accurate method, i.e., by the next-token prediction loss function L on a calibration corpus C. The intuition behind XAI is that, if a unit is more important, it should make L larger on C when been removed. Specifically, we define importance of unit i as $impi = |L - Lw;=0|$. By Maclaurin series [63], we get\n$impi = |L - Lw;=0| = |\\frac{\\partial L}{\\partial W_i}W_i +o(||W_i||\u00b2)|$ (2)\nSince the second term is a higher-order infinitesimal, ELMS then takes $| \\frac{\\partial L}{\\partial W_i}Wi |$ as a estimation of unit importance. By default, C is set to a sub-set of bookcorpus [89], which is a general-purpose language-modeling dataset.\""}, {"title": "3.3 Prompt elastification", "content": "ELMS tackles the challenges mentioned in \u00a72.2 by a dual-head Tiny Language Model (TLM). As shown in Figure 11, the TLM is a variant of mobile-friendly pre-trained language model MobileBert [61], a compact model with only 20% parameters of BERT_base [25] yet just 0.7% accuracy loss on GLUE benchmark [69]. We make the following modifications. Firstly, the SLO of the current request is marked in natural language and inserted into the embedding layer of MobileBert as special tokens. For instance, \"[05]\" represents the prefill SLO is 50% TTFT; \u201c<08>\u201d is 80% TPOT. These special tokens are initialized to word vectors that orthogonal to each other. Secondly, the TLM is designed with two separate heads, named score-head and decision-head. The score-head treats each token of the prompt as a two-class classification problem, where each token can be classified\nSince MeetingBank [11] dataset to fine-tune the score-head. The dataset contains about 50M tokens"}, {"title": "4 Implementation", "content": "We build the elasticized LLM and the TLM on top of Pytorch [55]. We modify the modeling.py of Huggingface Transformers [74] library to identify the permutation consistent units and profile their importance. In doing so, ELMS can easily be made forward-compatible with LLMs released in the future. The offline elastification is performed on a cloud server equipped with 8 NVIDIA A40 GPUs.\nOn-device LLMaaS. We build an LLM service on top of mllm\u00b3 [12], which is a lightweight yet powerful on-device"}, {"title": "5 Evaluation", "content": "We conduct experiments on the following testbeds.\nOn cloud, we use a server with a 64-core CPU (750GB RAM) and 8 A40 GPUs (45GB HBM each). On device, we test ELMS across COTS smartphones listed in Table 2.\nWe test the following LLMs. (1) Two base LLMs: LLaMA-7B [68] and Llama3-8B [27]. (2) Two instruction-tuned LLMs: Vicuna-V1.5-7B [86] and Llama3-instruct-8B [27]. (3) One sub-7b LLM: Orca-mini-3B [51].\nWe randomly set 6 SLOs based on a stepwise sensitivity hierarchy as shown in Table 3. We also enumerate more SLOs in Figure 19b as supplementary experiments.\nELMS workload. We evaluate ELMS on both standalone datasets and end-to-end synthesized traces.\n\u2022Datasets. We select 4 representative datasets/benchmarks for on-device LLM evaluation: (1) ARC_E [24], a commonly used dataset for science knowledge QA and reasoning. (2) OBQA [49], a dataset for natural language and commonsense knowledge comprehension. (3) Octopus [23], an on-device API-calling benchmark that follows the natural language instruction of users and selects the most suitable functions. (4) LlamaTouch [84], a realistic and complicated on-device UI-automation agent benchmark that manipulates mobile apps following user instructions. Each entry is augmented by in-context learning in 5-shots. We report option select accuracy of ARC_E and OBQA, top-5 function (without parameters) selection accuracy of Octopus, and app invocation accuracy of LlamaTouch.\n\u2022End-to-end traces. We further synthesize end-to-end traces on top of the above datasets. In Table 3, we list 6 conceived apps in response to the pre-defined SLOs. The requests and"}, {"title": "5.2 End-to-end performance", "content": "We first evaluate end-to-end performance on traces in \u00a75.1.\nWe report the request-response accuracy when all requests' SLOs are met in a trace. We compare the correctness of the LLM's answer to groundtruth. We set three levels of trace skewness: \u03b1 = 0 (even), \u03b1 = 0.25 (towards relaxed) and \u03b1 = -0.25 (towards tight). Each trace is executed on three diverse COTS devices listed in Table 2. The results are averaged across these three devices and shown in Figure 14. ELMS significantly outperforms the baselines by 6.00%-16.83% (11.04% on average) in absolute accuracy. Compared to PFS, ELMS further involves prompt elastification and does not introduce costly switching overhead. Besides, PFS does not fully make use of the room below a given SLO. Another potential reason is that ELMS derives sub-models from a bulky one, which may feature stronger emergent ability than the"}, {"title": "Memory consumption", "content": "We discuss peak memory consumption in Figure 16a. Without loss of representativeness. we report LLaMA-7B in the trace with \u03b1 = 0 on Redmi K60 Champion. ELMS consumes on-par memory compared to the baselines (15-17GB). Notably, directly deploying all the dedicated size LLMs in memory is impractical. As marked as PFS(Ideal) in Figure 16a, it consumes 29.3GB memory in total, which is OOM on all the COTS devices in Table 2.\nWith the same setting, we report the breakdown latency of switching between different model elastification levels (i.e., sub-models) in Figure 16b. Switching time is actually part of TTFT, and a too long switching time will preempt the room of model/prompt capacity, leading to a lower accuracy. PFS (Swap) and LPruner incur unacceptable time overhead that up to 8.3/6.2 seconds per request. This is mainly attributed to the costly swapping and in-memory data movement. ELMS only takes 0.31 second to switch to a new sub-model. Such a number is lower than 1% of LLMaaS's average TTFT, thus being completely acceptable."}, {"title": "5.3 Performance on standalone datasets", "content": "With a specific SLO, we further report the performance on an entire standalone dataset to show ELMS's superiority. The switching overhead is dismissed as there is no upgrade/-downgrade. The results are obtained on cloud server with SLO statistics on Mi 14 smartphone, and are shown in Figure 15. We have the following observations. ELMS significantly outperforms all its baselines by up to 34% and on-average 22% on accuracy. Specifically, on all the SLOs, ELMS"}, {"title": "5.4 Offline stage overhead", "content": "We further analysis the offline overhead in Figure 17. We measure the elastification of LLaMA-7B for MI14 on our A40 cloud server. Compared to PFS that trains a dedicated LLM for each SLO, ELMS derives elasticized LLMs from the original LLaMA-7B. Thereby, as shown in Figure 17a, the entire offline stage of ELMS only consumes 68.3 GPU hours (translates to about $100 GPU renting price [7]), making it affordable for most LLMaaS developers. Compared to other baselines that also derive elasticized LLMs from the original one, ELMS takes 21.7-68.2 more GPU hours, which is acceptable since the offline stage is performed once for all. Note that in the above sections, we have demonstrated that ELMS delivers an elastic LLM service with much higher quality than these baselines.\nIn Figure 17b, we provide a detailed breakdown of ELMS's offline stage. The model recovery (\u00a73.2) and self-induced labelling (\u00a73.3) dominate the offline stage, taking 40.4/26.3 hours. The reason the latter requires tremendous time is due to the lower GPU utilization caused by interactions with the score-head and sub-models."}, {"title": "5.5 Sensitivity analysis", "content": "As shown in Figure 18, the data scale of elastification exhibits a marginal effect. Regarding to model recovery, we further collect training data from LaMini [75] dataset that akin to Alpaca-cleaned. The final accuracy only increases 2.9%/3.7% when the recovery corpus is 10\u00d7/20\u00d7 larger. Regarding to unit importance profiling and decision training data, we expand them from Bookcorpus and MMLU [33], respectively. We also observe the similar marginal effect.\nSince Meetingbank is currently the largest corpus for token importance scoring to the best of our knowledge, we leave expanding it as a further work. In a nutshell, ELMS's data scale achieves a strong and sweet spot for high-quality elastification."}, {"title": "5.6 Generalization to other SLOS", "content": "Although in this paper we identify LLM inference latency as the SLO of LLMaaS requests, the SLO can also been easily generalized to other metrics with our proposed elastification methods. Here we define another two SLOs: end-to-end request inference time SLOtime (i.e., prefill + decode stage) and inference energy consumption SLOenergy, each of which is with a step size of 10% full LLMaaS. The results are obtained on Mi 14 smartphone. As shown in Figure 21, ELMS consistently outperforms its baselines on these new SLOs. the rationale is that these SLOs are all a specific kind of resource requirement and can be break down into prompt- and model- level elastifications. We believe that ELMS can serve various metrics of SLOs for diverse apps' requirements."}, {"title": "5.7 On mobile SoC accelerators", "content": "Mobile devices (smartphones) are typically equipped with hardware accelerators (e.g., Adreno GPUs or Qualcomm NPUs) in their SoCs. These SIMD/SIMT DSAs achieve higher throughput and lower latency on traditional on-device DNNs, especially CNNs. Yet, to the best of our knowledge currently (Aug., 2024) none of them can seamlessly run prefill and decode of ELMS and all its baselines due to legacy SDK/hardware issues like dynamic shape, graph building or"}, {"title": "6 Related Work", "content": "Elastic neuron networks. Elastic neuron networks can change their capacity at runtime to dynamically adapt to various resource/accuracy constraints. Early exit networks [64, 67] only perform inference on bottom layers since they are empirically more critical. Yet, early exit is not suitable for elastic LLM service. On one hand, due to LLMs' autoregressive inference nature, a skipped layer's KV cache may be accessed later. On the other hand, using layers as the granularity for trade-offs is not fine-grained enough. Parameter sharing networks [28, 31, 72] generate memory-efficient sub-models that shares parameters with each other. For instance, NestDNN [28] and LegoDNN [31] create sub-models of CNNs via offline re-pretraining, which is costly for foundation models. Adaptivenet [72] employs a CNN-oriented supernet, which provides diverse accuracy-latency tradeoffs yet needs extensive pre-training and memory resources.\nActivation sparsity [22, 40, 45, 58] elasticitizes DNNs via sparsifying weights according to the NN inputs. An accuracy-latency trade-off can be achieved by setting the proportion of activated weights. Yet, they either focus on CNNs with relu [19] structure (e.g., convrelu++ [40], seernet [22]), or only accelerates the decoding stage of relu-based MLPs of LLMs. The LLM prefill stage cannot be elastictized due to the low locality. And the trade-off on decode stage is limited by both the sparse kernels and the attention module. Besides, the non-relu LLMs (e.g., LLaMA [68], Vicuna [86]) exhibit a poor performance without resource-extensive pre-training.\nIn a nutshell, ELMS outperforms the above work by employing an orchestrated prompt-model elastification, providing a high-quality accuracy-latency trade-off in both TTFT and TPOT for LLMaaS apps.\nEfficient on-device LLM inference. Tremendous work [12, 39, 66, 77, 79-81] shed light on resource-efficiently deploying LLMs on mobile devices. For instance, MLC-LLM [66] is an NN-compiler with operator- and kernel- level optimizations for mobile devices. MNN [39] and mllm [12] are on-device inference libraries for LLMs. PowerinferV2 [80] addresses the memory issue of mobile devices by introducing swapping and activation sparsity to LLM inference. Targeting at elastic LLM service, ELMS is orthogonal to these work, and can benefit from them to realize the vision of on-device LLMaaS."}, {"title": "7 Conclusion", "content": "This work has proposed ELMS, the first-of-its-kind elastic on-device LLM service that serves apps with diverse SLOs. ELMS incorporates two novel designs, i.e., one-shot reordering of permutation consistent units and dual-head tiny language model to fully unleash the potential of model- and prompt-elastification. ELMS significantly outperforms competitive baselines by up to 16.83% and 11.04% on average in accuracy."}]}