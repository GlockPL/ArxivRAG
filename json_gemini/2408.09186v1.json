{"title": "EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition", "authors": ["Qile Liu", "Weishan Ye", "Yulu Liu", "Zhen Liang"], "abstract": "Emotion recognition using electroencephalography (EEG) signals has garnered widespread attention in recent years. However, existing studies have struggled to develop a sufficiently generalized model suitable for different datasets without re-training (cross-corpus). This difficulty arises because distribution differences across datasets far exceed the intra-dataset variability. To solve this problem, we propose a novel Soft Contrastive Masked Modeling (SCMM) framework. Inspired by emotional continuity, SCMM integrates soft contrastive learning with a new hybrid masking strategy to effectively mine the \"short-term continuity\" characteristics inherent in human emotions. During the self-supervised learning process, soft weights are assigned to sample pairs, enabling adaptive learning of similarity relationships across samples. Furthermore, we introduce an aggregator that weightedly aggregates complementary information from multiple close samples based on pairwise similarities among samples to enhance fine-grained feature representation, which is then used for original sample reconstruction. Extensive experiments on the SEED, SEED-IV and DEAP datasets show that SCMM achieves state-of-the-art (SOTA) performance, outperforming the second-best method by an average accuracy of 4.26% under two types of cross-corpus conditions (same-class and different-class) for EEG-based emotion recognition.", "sections": [{"title": "Introduction", "content": "Emotions are human attitudinal experiences and behavioral responses to objective things, closely related to health conditions and behavioral patterns (Wang, Zhang, and Tang 2024). Compared to speech (Singh and Goel 2022), gestures (Noroozi et al. 2018), and facial expressions (Canal et al. 2022), electroencephalography (EEG) offers a more direct and objective measurement of human emotions by capturing brain activity across various scalp locations (Hu et al. 2019). Recently, researchers have increasingly emphasised EEG-based emotion recognition (Zhong, Wang, and Miao 2020; Zhao, Yan, and Lu 2021; Zhang, Liu, and Zhong 2022; Gao et al. 2024), aiming to advance the development of affective brain-computer interfaces (aBCIs). However, three critical challenges remain to be addressed in current approaches.\n(1) Dataset Specificity. Most existing EEG-based emotion recognition methods are typically designed for a single dataset, necessitating model retraining when the dataset changes. This requirement significantly limits the model's generalizability and scalability, hindering its application across different datasets. To tackle this issue, the concept of cross-corpus has been proposed, which is designed to be generalized across multiple datasets. A cross-corpus model is trained on one dataset and can be directly applied to another without the need for retraining from scratch. This concept, which originated in natural language processing (Schuller et al. 2010; Zhang et al. 2011), has been extended to various domains in recent years (Rayatdoost and Soleymani 2018; Chien, Yang, and Lee 2020; Ryumina, Dresvyanskiy, and Karpov 2022). Although existing EEG-based emotion recognition methods, such as BiDANN (Li et al. 2018), TANN (Li et al. 2021), and PR-PL (Zhou et al. 2023), have achieved superior performance in within-subject or cross-subject tasks, their efficacy significantly degrades in cross-corpus scenarios, where differences in data distribution across datasets far exceed the variability within a single dataset (Rayatdoost and Soleymani 2018).\n(2) Data Availability. Current approaches for cross-domain or cross-corpus EEG-based emotion recognition rely heavily on domain adaptation techniques, which depend extensively on the availability of labeled source data and un-labeled target data. For example, AD-TCN (He, Zhong, and Pan 2022) learned an asymmetric mapping that adapts the target domain feature encoder to the source domain, aiming to eliminate the complicated steps of target domain labeling and improve the model performance in cross-domain scenarios. Similarly, E2STN (Zhou et al. 2024) integrated content information from the source domain with style information from the target domain to create stylized emotional EEG representations. However, these methods require prior access to all labeled source data and unlabeled target data for model training, presenting a significant limitation due to data availability constraints (Liu et al. 2024a).\n(3) Ignorance of Emotional Continuity. Unlike domain adaptation techniques, contrastive learning (CL) achieves superior performance without relying on labeled data, and has demonstrated significant potential in enhancing feature representation capabilities across various domains (Chen et al. 2020; Radford et al. 2021; Zhang et al. 2022). Current"}, {"title": "Methodology", "content": "Given an unlabeled pre-training EEG emotion dataset $X = {x_i}_{i=1}^N$ with $N$ samples, where each sample $x_i \\in \\mathbb{R}^{C \\times F}$ contains $C$ channels and $F$-dimensional features, the goal is to learn a nonlinear embedding function $f_E$. This function is designed to map $x_i$ to its representation $h_i$ that best describes itself by leveraging the emotional continuity inherent in EEG signals. Ultimately, the pre-trained model is capable of producing generalizable EEG representations that can be effectively used across different EEG emotion datasets."}, {"title": "Model Architecture", "content": "The overall framework of SCMM is illustrated in Fig. 2, which includes three main modules: hybrid masking, soft contrastive learning, and aggregate reconstruction. Below, we detail the specific design of each module and the pre-training process of SCMM.\nHybrid Masking The selection of masking strategies is crucial for CL and masked modeling (Zhang et al. 2024; Liu et al. 2024b). For an input EEG sample $x_i \\in X$, most existing methods use random masking (Zhang, Liu, and Zhong 2022) or channel masking (Li et al. 2022) to generate the masked sample $Z_i$. The random masking strategy masks samples along the feature dimension, ignoring the inter-channel relationships of EEG signals. While a large masking ratio (e.g., 75%) can mask entire portions of certain channels, it complicates the modeling process due to significant information loss. Conversely, the channel masking strategy masks features across all dimensions of the selected channels, losing the relationships between different dimensional features. Neither approach captures both channel and feature relationships simultaneously. Therefore, we develop a new hybrid masking strategy to generate diverse masked samples by considering both channel and feature relationships.\nSpecifically, we first generate a random masking matrix $Mask_R \\in {0,1}$ with dimensions $C \\times F$, and a channel masking matrix $Mask_C \\in {0,1}$ with dimensions $C \\times F$, both derived from binomial distributions with masking ratios $r \\in (0, 1)$. Here, the element values in each row of $Mask_C$\nare either all 1s or all 0s. Next, we generate a probability matrix $U \\in [0, 1]$ with dimensions $C \\times 1$ for hybrid masking, which is drawn from a uniform distribution. The hybrid masking process is defined as:\n$X_{i,c} = \\begin{cases}\nX_{i,c} \\odot Mask_R, & \\text{if } \\mu < U_c \\leq 1 \\\\\nX_{i,c} \\odot Mask_C, & \\text{if } 0 \\leq U_c \\leq \\mu\n\\end{cases}$\nwhere $x_{i,c}$ represents the c-th channel of $x_i$, and $X_{i,c}$ is the corresponding masked sample. $\\odot$ denotes element-wise multiplication. $U_c$ is the probability value in the c-th row, and $\\mu$ is a probability threshold that controls the weights of the two masking strategies. By integrating the hybrid masking strategy in SCMM, we enhance the diversity of masked samples, encouraging the model to learn richer and more robust feature representations that account for both channel and feature relationships within EEG signals. Figure 3 illustrates the differences between three masking strategies.\nSoft Contrastive Learning Traditional hard CL treats augmented views generated from the same sample as positive pairs, and those from different samples as negative pairs (Chen et al. 2020; Eldele et al. 2021; Yue et al. 2022). During the computation of the contrastive loss, hard values (1 or 0) are assigned to sample pairs, as illustrated in Fig. 1(b) (Hard CL). However, we argue that this approach fails to account for the \u201cshort-term continuity\u201d characteristic inherent in human emotions, leading to inaccurate modeling of inter-sample relationships and hindering the generalizability of the learned embeddings."}, {"title": "Aggregate Reconstruction", "content": "To further capture the fine-grained relationships between different samples, we incorporate an aggregator for weighted aggregation and reconstruction. Current approaches for masked EEG modeling typically reconstruct the masked portion based on the unmasked portion of a single masked sample (Lan et al. 2024; Pang et al. 2024), following the learning paradigm of MAE (He et al. 2022). However, this single-sample reconstruction strategy overlooks the interactions between samples, leading to a complex and inadequate reconstruction process.\nTo overcome this limitation, we introduce an aggregator that improves the traditional single-sample reconstruction process. Specifically, we first calculate the cosine similarity between each pair of projected embeddings $(z_i, z_j)$ within a mini-batch, resulting in a similarity matrix $S$. Based on the pairwise similarities in S, the aggregator then performs weighted aggregation of the embedding $h_i$. The weighted aggregation process is defined as:\n$\\hat{h_i} = \\sum_{z' \\in Z \\{z_i\\}} \\frac{exp(sim(z_i, z')/T_c)}{\\sum_{z\" \\in Z \\{z_i\\}} exp(sim(z_i, z'')/T_c)} \\cdot h'_i,$\nwhere $\\hat{h'}$ $ \\in H\\{h_i\\}$ represents the encoded embedding corresponding to the projected embedding $z'$, and $H$ denotes the embedding space of the encoder $E$. This approach allows for a more comprehensive reconstruction by aggregating complementary information and incorporating similar features from different samples during the reconstruction process, while suppressing interference from irrelevant noise samples. Finally, the reconstructed embedding $\\hat{h_i}$ is fed into a lightweight decoder $D$ to obtain the reconstructed sample $\\hat{x_i}$. Following the masked modeling paradigm, we use Mean Square Error (MSE) as the reconstruction loss for model optimization, which is defined as:\n$L_{R,i} = ||x_i - \\hat{x_i}||^2$.\nSimilar to the soft contrastive loss $L_c$, the final reconstruction loss $L_R$ is computed by summing and averaging $L_{R,i}$ across all samples within a mini-batch."}, {"title": "The pre-training process of SCMM", "content": "During the pre-training process, SCMM is trained by jointly optimizing $L_C$ and $L_R$. The overall pre-training loss is defined as:\n$L = \\lambda_c L_C + \\lambda_R L_R,$\nwhere $\\lambda_c$ and $\\lambda_R$ are trade-off hyperparameters, which are adaptively adjusted according to the homoscedastic uncertainty of each loss item. Algorithm 1 details the pre-training process of the proposed SCMM."}, {"title": "Experiments", "content": "We conduct extensive experiments on three publicly available datasets, SEED (Zheng and Lu 2015), SEED-IV (Zheng et al. 2018), and DEAP (Koelstra et al. 2011), to evaluate the model performance of SCMM. These datasets are diverse in terms of EEG equipment, emotional stimuli, data specifications, labeling approaches and subjects, making them well-suited for assessing the model's efficacy in cross-corpus EEG-based emotion recognition tasks. In our experiments, we use differential entropy (DE) features as inputs. Detailed descriptions of the datasets and pre-processing procedures are provided in Appendix A."}, {"title": "Discussion", "content": "Generalization capability analysis To further validate the generalization capability of SCMM, we conduct additional experiments on the SEED and DEAP datasets under a different-class cross-corpus scenario, denoted as DEAP \u2192 SEED\u00b3, SEED\u00b3 \u2192 DEAP (Valence) and SEED\u00b3 \u2192 DEAP (Arousal). It is notable that the EEG acquisition equipment, emotional stimuli, data specifications, labeling approaches and subjects are completely different between the two datasets. Table 3 presents the experimental results of SCMM compared to existing methods. Specifically, for the DEAP \u2192 SEED\u00b3 experiment, SCMM achieves an accuracy of 91.70% with a standard deviation of 8.07%, outperforming the second-best method MAE by an accuracy of 8.01%. For the SEED\u00b3 \u2192 DEAP (Valence) and SEED\u00b3 \u2192 DEAP (Arousal) experiments, our model achieves classification accuracies of 73.96% and 72.66% with standard deviations of 6.75% and 5.67%, surpassing the second-best method MAE by 1.77% and 2.16% in accuracies. Experimental results demonstrate that the proposed SCMM maintains excellent performance even when the pre-training and fine-tuning datasets are completely different, highlighting its superior generalization capability.\nModel performance with limited fine-tuning data We further explore the model performance of SCMM on the SEED and SEED-IV datasets when fine-tuning labeled data is limited. Specifically, we randomly select 1%, 5%, 10% and 20% of labeled samples from the fine-tuning dataset for"}, {"title": "Conclusion", "content": "This paper proposes a novel self-supervised pre-training framework, Soft Contrastive Masked Modeling (SCMM), for cross-corpus EEG-based emotion recognition. Unlike traditional contrastive learning models, SCMM integrates soft contrastive learning with a hybrid masking strategy to effectively capture the \"short-term continuity\" characteristics inherent in human emotions, and produce stable and generalizable EEG representations. Additionally, an aggregator is developed to weightedly aggregate complementary information from multiple close samples, thereby enhancing fine-grained feature representation capability in the modeling process. Extensive experiments on three well-recognized datasets show that SCMM consistently achieves SOTA performance in cross-corpus EEG-based emotion recognition tasks under both same-class and different-class conditions. Comprehensive ablation study and parameter analysis confirm the superior performance and robustness of SCMM. Visualization results indicate that our model effectively reduces the distance between similar samples within the same category, and captures more fine-grained relationships across samples. These findings suggest that SCMM enhances the feasibility of extending the proposed method to real-life aBCI applications."}, {"title": "Appendix A Datasets", "content": "A.1 Dataset Description\nWe conduct extensive experiments on three well-recognized datasets, SEED (Zheng and Lu 2015), SEED-IV (Zheng et al. 2018), and DEAP (Koelstra et al. 2011), to evaluate the model performance of SCMM in cross-corpus EEG-based emotion recognition tasks. Table 6 provides a detailed description of the three datasets.\n(1) SEED (Zheng and Lu 2015) was developed by Shanghai Jiao Tong University. The dataset used a 62-channel ESI NeuroScan System based on the international 10-20 system to record EEG signals from 15 subjects (7 males and 8 females) under different video stimuli at a sampling rate of 1kHz. Each subject participated in 3 sessions. In each session, each subject was required to watch 15 movie clips consisting of 3 different emotional states: negative, neutral and positive. Each emotional state contains a total of 5 movie clips, corresponding to 5 trials.\n(2) SEED-IV (Zheng et al. 2018) used the same EEG acquisition equipment as the SEED dataset, but with different video stimuli, emotion categories and subjects. The dataset recorded EEG signals from 15 subjects under different video stimuli at a sampling rate of 1kHz. Each subject participated in 3 sessions. In each session, each subject was required to watch 24 movie clips containing 4 different emotions: sad, neutral, fear and happy. Each emotion contains a total of 6 movie clips, corresponding to 6 trials.\n(3) DEAP (Koelstra et al. 2011) was constructed by Queen Mary University of London. The dataset used a 128-channel Biosemi ActiveTwo System to record EEG signals from specific 32 channels of 32 subjects (16 males and 16 females) while watching 40 one-minute music videos at a sampling rate of 512Hz. The 40 one-minute videos elicited different emotions according to the valence-arousal dimension. Specifically, the valence-arousal emotional model, first proposed by Russell (Russell 1980), places each emotional state on a two-dimensional scale. The first dimension represents valence, ranging from negative to positive, and the second dimension represents arousal, ranging from calm to exciting. Participants rated valence and arousal using a continuous scale of 1 to 9 after watching each video clip."}, {"title": "A.2 Pre-processing Procedures", "content": "For the SEED and SEED-IV datasets, the raw EEG signals were initially downsampled to 200Hz and filtered through a bandpass filter of 0.3-50Hz to filter noise and remove artifacts. Then, the data were divided into multiple non-overlapping segments using sliding windows of 1 second for SEED and 4 seconds for SEED-IV, respectively. After that, we extracted differential entropy (DE) features for each channel of each segment at five frequency bands: Delta (1-4Hz), Theta (4-8Hz), Alpha (8-14Hz), Beta (14-31Hz), and Gamma (31-50Hz). Finally, the DE features from 62 channels and 5 bands were formed into a feature matrix of shape 62 \u00d7 5, which serves as input to the SCMM model. The extraction of DE features can be expressed as:\n$DE(x) = \\frac{1}{2}log(2\\pi e\\sigma^2),$\nHere, x represents an EEG signal segment of a specific length that approximately obeys a Gaussian distribution \u039d(\u03bc, \u03c3\u00b2), where o denotes the standard deviation of \u00e6, and e is the Euler constant.\nFor the DEAP dataset, the raw EEG signals were first downsampled to 128Hz and denoised by a bandpass filter of 4-45Hz. Subsequently, the data were segmented into multiple non-overlapping segments using a sliding window of 1s. Similar to the SEED and SEED-IV datasets, DE features were extracted for each channel of each segment at five frequency bands. Finally, the DE features from 32 channels and 5 bands were formed into a feature matrix of shape 32 \u00d7 5 as input to the model. During the experiments, we divided the continuous labels using a fixed threshold of 5 to convert them to binary classification tasks (low / high)."}, {"title": "A.3 Handling Different Number of Channels", "content": "Since the SEED-series datasets and the DEAP dataset contain different numbers of electrodes (channels), we require channel processing before inputting DE features into the model. Specifically, we consider the fine-tuning dataset as an anchor. When the number of channels in the fine-tuning dataset is less than in the pre-training dataset, we select data from the corresponding channels in the pre-training dataset and drop the data from the redundant channels as inputs (e.g., pre-training on SEED and fine-tuning on DEAP). Conversely, when the number of channels in the fine-tuning dataset is greater than in the pre-training dataset, we fill the missing channel data with zeros in the pre-training dataset to match the fine-tuning dataset (e.g., pre-training on DEAP and fine-tuning on SEED)."}, {"title": "Appendix B Implementation Details", "content": "To reduce computational load while maintaining model performance, we adopt a lightweight design for each module of SCMM. Specifically, we use a 3-layer 1D CNN for the encoder E and a 2-layer MLP for the projector P. For the lightweight decoder D, we utilize a single-layer MLP for reconstruction. Regarding hyperparameter selection in the pre-training stage, we set r to 0.5 and \u03bc to 0.1 for hybrid masking, i.e., the percentage of random masking and channel masking is 0.9 and 0.1, respectively. We use the negative of cosine similarity as the metric function Dist(\u00b7,\u00b7), and we set a to 0.5, Tw to 0.05 and Te to 0.5 for soft contrastive learning. We use Adam as the optimizer with a learning rate of 5 \u00d7 10-4 and an L2-norm penalty coefficient 3 \u00d7 10\u22124. The pre-training process is conducted over 200 epochs with a batch size of 256. We save the model parameters 0 from the final epoch as the pre-trained SCMM. In the fine-tuning stage, we input the encoded embeddings hi into a classifier consisting of a 2-layer fully connected network for final emotion recognition. The Adam optimizer is utilized with a learning rate of 5 \u00d7 10-4 and a weight decay of 3 \u00d7 10-4. The number of fine-tuning epochs is set to 50 for the SEED and SEED-IV datasets and 500 for the DEAP dataset, with a batch size of 128. For efficient deployment and testing of the model, the pre-trained SCMM is optimized solely using cross-entropy loss during fine-tuning. All experiments"}, {"title": "Appendix C Baseline Methods and Experimental Settings", "content": "C.1 Baseline Methods\nWe compare the proposed SCMM against eight competitive methods, including four traditional deep learning methods: BiDANN (Li et al. 2018), TANN (Li et al. 2021), PR-PL (Zhou et al. 2023) and E2STN (Zhou et al. 2024), as well as four self-supervised learning models: SimCLR (Chen et al. 2020; Tang et al. 2021), Mixup (Zhang et al. 2018; Wickstr\u00f8m et al. 2022), MAE (He et al. 2022) and JCFA (Liu et al. 2024a). Details of the eight baseline methods are summarized as follows.\n\u2022 BiDANN (Li et al. 2018): The bi-hemispheres domain adversarial neural network mapped the EEG data of both left and right hemispheres into discriminative feature spaces separately to address domain shifts in EEG-based emotion recognition tasks.\n\u2022 TANN (Li et al. 2021): The transferable attention neural network is a novel transfer learning methods which learned the discriminative information from EEG signals using local and global attention mechanisms.\n\u2022 PR-PL (Zhou et al. 2023): The prototypical representation based pairwise learning framework adopted pairwise learning to model the relative relationships between EEG sample pairs in terms of prototypical representations, addressing the critical issues of individual differences and noise labels in cross-subject scenarios.\n\u2022 E2STN (Zhou et al. 2024): The emotional EEG style transfer network integrated content information from the source domain with style information from the target domain, achieving superior performance in cross-corpus EEG-based emotion recognition tasks.\n\u2022 SimCLR (Chen et al. 2020; Tang et al. 2021): A seminal work in self-supervised contrastive learning, first proposed for computer vision, has been extended to human activity recognition.\n\u2022 Mixup (Zhang et al. 2018; Wickstr\u00f8m et al. 2022): A novel CL-based data augmentation method that aimed to correctly predict the mixing proportion of two samples, has been applied to time series analysis.\n\u2022 MAE (He et al. 2022): A groundbreaking work in the field of mask modeling, which proposed to reconstruct the masked portion based on the unmasked portion of the masked sample, has achieved remarkable success in a wide range of fields.\n\u2022 JCFA (Liu et al. 2024a): The joint contrastive learning framework performed joint contrastive learning across two domains to synchronize the time- and frequency-based embeddings of the same EEG sample in the latent time-frequency space, achieving state-of-the-art performance in cross-corpus scenarios.\nTo ensure a fair comparison, we adopt the same encoder, projector, decoder, and classifier structures for SimCLR, Mixup and MAE as used in SCMM. We use the default hyperparameters reported in the original paper for all methods in our experiments, unless otherwise specified. Additionally, for BiDANN, TANN, PR-PL, E2STN, MAE, and SCMM, the input samples are preprocessed 1-s DE features. In contrast, SimCLR, Mixup, and JCFA use preprocessed 1-s EEG signals as inputs, in accordance with the specific design of each model."}, {"title": "C.2 Experimental Settings", "content": "We adopt a cross-corpus subject-independent experimental protocol in the experiments, following the setup used in"}, {"title": "E.2 Probability Threshold", "content": "Table 10 presents the model performance using different probability thresholds (ranging from 0 to 1) for hybrid masking. Here, \u03bc = 0 refers to the exclusive use of the random masking strategy, while \u03bc = 1 indicates the exclusive use of the channel masking strategy. Experimental results show that SCMM achieves the best performance in the SEED-IV\u00b3 \u2192 SEED\u00b3 and SEED\u00b3 \u2192 SEED-IV3 experiments when the probability threshold is set to \u03bc = 0.1 (i.e., the ratio of random masking and channel masking is 9:1)."}, {"title": "E.3 Metric Function", "content": "We evaluate the model performance of SCMM using different metric functions in soft assignments w(\u00b7, \u00b7). Experimental results in Table 11 indicate that SCMM performs best on the SEED and SEED-IV 3-category datasets when cosine similarity is used as the metric function."}, {"title": "E.4 Upper Bound", "content": "We assess the impact of different upper bounds on calculating soft assignments w(., .). To restrict the soft assignments to the range of 0 to 1, we explore the model performance of SCMM with upper bounds of 0.25, 0.5, 0.75, and 1. Experimental results in Table 12 illustrate that the proposed SCMM achieves the best performance in the SEED-IV\u00b3 \u2192 SEED\u00b3 experiment when the upper bound is set to a = 0.5."}, {"title": "E.5 Sharpness", "content": "Table 13 explore the impact of different sharpness parameters in the SEED-IV\u00b3 \u2192 SEED\u00b3 experiment, as shown in Table 13. Experimental results indicate that our model achieves the best performance with a sharpness parameter of Tw = 0.05."}, {"title": "E.6 Temperature", "content": "We conduct comparison experiments on the SEED and SEED-IV 3-category datasets to explore the impact of temperature parameter on model performance. Experimental results in Table 14 show that SCMM consistently achieves the best performance in both experiments when the temperature parameter is set to Te = 0.5."}, {"title": "F Model Performance and Computational Complexity Analysis of SCMM", "content": "To investigate the trade-off between model performance and computational complexity in few-shot scenarios, we assess the classification accuracy, time cost, and the number of trainable parameters of SCMM when fine-tuning with limited labeled data. Experimental results in Table 15 show that the model performance of SCMM significantly improves as the number of fine-tuning labeled samples increases. Meanwhile, the inference time cost remains low due to our lightweight design. Additionally, our model achieves superior classification performance in cross-corpus EEG-based emotion recognition tasks with very few parameters. In summary, the proposed SCMM effectively balances model performance and computational complexity."}]}