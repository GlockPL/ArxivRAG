{"title": "EEG-SCMM: Soft Contrastive Masked Modeling for Cross-Corpus EEG-Based Emotion Recognition", "authors": ["Qile Liu", "Weishan Ye", "Yulu Liu", "Zhen Liang"], "abstract": "Emotion recognition using electroencephalography (EEG) signals has garnered widespread attention in recent years. However, existing studies have struggled to develop a sufficiently generalized model suitable for different datasets without re-training (cross-corpus). This difficulty arises because distribution differences across datasets far exceed the intra-dataset variability. To solve this problem, we propose a novel Soft Contrastive Masked Modeling (SCMM) framework. Inspired by emotional continuity, SCMM integrates soft contrastive learning with a new hybrid masking strategy to effectively mine the \"short-term continuity\" characteristics inherent in human emotions. During the self-supervised learning process, soft weights are assigned to sample pairs, enabling adaptive learning of similarity relationships across samples. Furthermore, we introduce an aggregator that weightedly aggregates complementary information from multiple close samples based on pairwise similarities among samples to enhance fine-grained feature representation, which is then used for original sample reconstruction. Extensive experiments on the SEED, SEED-IV and DEAP datasets show that SCMM achieves state-of-the-art (SOTA) performance, outperforming the second-best method by an average accuracy of 4.26% under two types of cross-corpus conditions (same-class and different-class) for EEG-based emotion recognition.", "sections": [{"title": "Introduction", "content": "Emotions are human attitudinal experiences and behavioral responses to objective things, closely related to health conditions and behavioral patterns (Wang, Zhang, and Tang 2024). Compared to speech (Singh and Goel 2022), gestures (Noroozi et al. 2018), and facial expressions (Canal et al. 2022), electroencephalography (EEG) offers a more direct and objective measurement of human emotions by capturing brain activity across various scalp locations (Hu et al. 2019). Recently, researchers have increasingly emphasised EEG-based emotion recognition (Zhong, Wang, and Miao 2020; Zhao, Yan, and Lu 2021; Zhang, Liu, and Zhong 2022; Gao et al. 2024), aiming to advance the development of affective brain-computer interfaces (aBCIs). However, three critical challenges remain to be addressed in current approaches.\n(1) Dataset Specificity. Most existing EEG-based emotion recognition methods are typically designed for a single dataset, necessitating model retraining when the dataset changes. This requirement significantly limits the model's generalizability and scalability, hindering its application across different datasets. To tackle this issue, the concept of cross-corpus has been proposed, which is designed to be generalized across multiple datasets. A cross-corpus model is trained on one dataset and can be directly applied to another without the need for retraining from scratch. This concept, which originated in natural language processing (Schuller et al. 2010; Zhang et al. 2011), has been extended to various domains in recent years (Rayatdoost and Soleymani 2018; Chien, Yang, and Lee 2020; Ryumina, Dresvyanskiy, and Karpov 2022). Although existing EEG-based emotion recognition methods, such as BiDANN (Li et al. 2018), TANN (Li et al. 2021), and PR-PL (Zhou et al. 2023), have achieved superior performance in within-subject or cross-subject tasks, their efficacy significantly degrades in cross-corpus scenarios, where differences in data distribution across datasets far exceed the variability within a single dataset (Rayatdoost and Soleymani 2018).\n(2) Data Availability. Current approaches for cross-domain or cross-corpus EEG-based emotion recognition rely heavily on domain adaptation techniques, which depend extensively on the availability of labeled source data and unlabeled target data. For example, AD-TCN (He, Zhong, and Pan 2022) learned an asymmetric mapping that adapts the target domain feature encoder to the source domain, aiming to eliminate the complicated steps of target domain labeling and improve the model performance in cross-domain scenarios. Similarly, E2STN (Zhou et al. 2024) integrated content information from the source domain with style information from the target domain to create stylized emotional EEG representations. However, these methods require prior access to all labeled source data and unlabeled target data for model training, presenting a significant limitation due to data availability constraints (Liu et al. 2024a).\n(3) Ignorance of Emotional Continuity. Unlike domain adaptation techniques, contrastive learning (CL) achieves superior performance without relying on labeled data, and has demonstrated significant potential in enhancing feature representation capabilities across various domains (Chen et al. 2020; Radford et al. 2021; Zhang et al. 2022). Current"}, {"title": "Methodology", "content": "CL-based methods for EEG-based emotion recognition consider an anchor and its augmented views as positive pairs, while treating all other samples as negatives, as shown in Fig. 1(b) (Hard CL). For example, CLISA (Shen et al. 2022) employed contrastive learning to minimize the inter-subject differences by maximizing the similarity in EEG representations across subjects. JCFA (Liu et al. 2024a) performed joint contrastive learning across three domains to align the time- and frequency-based embeddings of the same sample in the latent time-frequency space, achieving state-of-the-art (SOTA) performance in cross-corpus scenarios. However, psychological and neuroscientific studies have shown that emotions exhibit significant \"short-term continuity\" characteristics (Davidson 1998; Houben, Van Den Noortgate, and Kuppens 2015). In other words, human emotions are relatively stable over certain periods, with sudden changes being rare. As illustrated in Fig. 1(a), a high cosine similarity is maintained between an anchor sample $x_i$ and its neighboring sample $x_j$, and even a distant sample $x_k$ separated by extended periods (e.g., 60 seconds). Given this nature of emotions, we propose that the definition of positive pairs in CL-based EEG analysis should extend beyond just the anchor and its augmented views. Instead, it should include a broader range of similar samples, especially those that are temporally proximal, as shown in Fig. 1(b) (SCMM). However, existing methods like JCFA (Liu et al. 2024a), which follow the traditional CL paradigm (Chen et al. 2020), may incorrectly pull apart similar but not identical samples, thus failing to capture the inherent correlations of EEG signals.\nTo address the aforementioned three critical issues, we propose a novel Soft Contrastive Masked Modeling (SCMM) framework for cross-corpus EEG-based emotion recognition. Unlike traditional hard CL shown in Fig. 1(b), SCMM considers emotional continuity and incorporates soft assignments of sample pairs. This approach enables the model to identify the fine-grained relationships between samples in a self-supervised manner, thereby enhancing the generalizability of EEG representations. Comprehensive experiments on three well-recognized datasets demonstrate that SCMM consistently achieves SOTA performance, highlighting its superior capability and stability. In summary, the main contributions of SCMM are outlined as follows:\n\u2022 Inspired by the nature of emotions, we propose a novel SCMM framework to address cross-corpus EEG-based emotion recognition. This approach assigns soft weights to sample pairs during contrastive learning to capture the similarity relationships between different samples. As a result, better feature representations of EEG signals are learned in a self-supervised manner. To the best of our knowledge, this is the first study to introduce soft contrastive learning into EEG-based emotion recognition.\n\u2022 We also develop a new hybrid masking strategy to generate diverse masked samples by considering both channel and feature relationships, which is essential for enhancing contrastive learning. Additionally, we introduce an aggregator that weightedly aggregates complementary information from the embeddings of multiple close samples, enabling fine-grained feature learning and improving the model's overall capability.\n\u2022 We conduct extensive experiments on three well-known datasets (SEED, SEED-IV, and DEAP), showing that SCMM achieves consistent SOTA performance compared to eight competitive methods, surpassing the second-best method by an average accuracy of 4.26%.\nProblem Definition\nGiven an unlabeled pre-training EEG emotion dataset $\\mathcal{X} = \\{x_i\\}_{i=1}^N$ with $N$ samples, where each sample $x_i \\in \\mathbb{R}^{C \\times F}$ contains $C$ channels and $F$-dimensional features, the goal is to learn a nonlinear embedding function $f_e$. This function is designed to map $x_i$ to its representation $h_i$ that best describes itself by leveraging the emotional continuity inherent in EEG signals. Ultimately, the pre-trained model is capable of producing generalizable EEG representations that can be effectively used across different EEG emotion datasets."}, {"title": "Model Architecture", "content": "The overall framework of SCMM is illustrated in Fig. 2, which includes three main modules: hybrid masking, soft contrastive learning, and aggregate reconstruction. Below, we detail the specific design of each module and the pre-training process of SCMM.\nHybrid Masking The selection of masking strategies is crucial for CL and masked modeling (Zhang et al. 2024; Liu et al. 2024b). For an input EEG sample $x_i \\in \\mathcal{X}$, most existing methods use random masking (Zhang, Liu, and Zhong 2022) or channel masking (Li et al. 2022) to generate the masked sample $\\tilde{x_i}$. The random masking strategy masks samples along the feature dimension, ignoring the inter-channel relationships of EEG signals. While a large masking ratio (e.g., 75%) can mask entire portions of certain channels, it complicates the modeling process due to significant information loss. Conversely, the channel masking strategy masks features across all dimensions of the selected channels, losing the relationships between different dimensional features. Neither approach captures both channel and feature relationships simultaneously. Therefore, we develop a new hybrid masking strategy to generate diverse masked samples by considering both channel and feature relationships.\nSpecifically, we first generate a random masking matrix $Mask_R \\in \\{0, 1\\}$ with dimensions $C \\times F$, and a channel masking matrix $Mask_C \\in \\{0, 1\\}$ with dimensions $C \\times F$, both derived from binomial distributions with masking ratios $r \\in (0, 1)$. Here, the element values in each row of $Mask_C$ are either all 1s or all 0s. Next, we generate a probability matrix $U \\in [0, 1]$ with dimensions $C \\times 1$ for hybrid masking, which is drawn from a uniform distribution. The hybrid masking process is defined as:\n$\\tilde{x}_{i,c} =\\begin{cases}x_{i,c} \\odot Mask_R, & \\text{if } \\mu < U_c \\leq 1\\\\x_{i,c} \\odot Mask_C, & \\text{if } 0 \\leq U_c \\leq \\mu\\end{cases}$  (1)\nwhere $x_{i,c}$ represents the c-th channel of $x_i$, and $\\tilde{x}_{i,c}$ is the corresponding masked sample. $\\odot$ denotes element-wise multiplication. $U_c$ is the probability value in the c-th row, and $\\mu$ is a probability threshold that controls the weights of the two masking strategies. By integrating the hybrid masking strategy in SCMM, we enhance the diversity of masked samples, encouraging the model to learn richer and more robust feature representations that account for both channel and feature relationships within EEG signals. Figure 3 illustrates the differences between three masking strategies.\nSoft Contrastive Learning Traditional hard CL treats augmented views generated from the same sample as positive pairs, and those from different samples as negative pairs (Chen et al. 2020; Eldele et al. 2021; Yue et al. 2022). During the computation of the contrastive loss, hard values (1 or 0) are assigned to sample pairs, as illustrated in Fig. 1(b) (Hard CL). However, we argue that this approach fails to account for the \u201cshort-term continuity\u201d characteristic inherent in human emotions, leading to inaccurate modeling of inter-sample relationships and hindering the generalizability of the learned embeddings."}, {"title": "Datasets", "content": "To address this issue, we propose defining soft assignments for different sample pairs, as shown in Fig. 1(b) (SCMM). We first input $x_i$ and $\\tilde{x_i}$ into an encoder $E$ that maps samples to embeddings, denoted as $h_i = E(x_i)$ and $\\tilde{h_i} = E(\\tilde{x_i})$. These embeddings are then projected into a latent space $Z$ using a projector $P$, resulting in $z_i = P(h_i)$ and $\\tilde{z_i} = P(\\tilde{h_i})$. Next, we perform soft contrastive learning in $Z$ using $z_i$ and $\\tilde{z_i}$. Specifically, for a given pair of samples $(x_i, x_j)$, we first calculate the normalized distance $D(x_i, x_j)$ between $x_i$ and $x_j$ in the original data space as:\n$D(x_i, x_j) = Norm(Dist(x_i, x_j)) \\in [0, 1],$  (2)\nwhere $Dist(\\cdot, \\cdot)$ is a metric function used to measure the distance between sample pairs, and $Norm(\\cdot)$ denotes min-max normalization. In the experiments, we take the negative of cosine similarity as the metric function. Based on the normalized distance $D(x_i, x_j)$, we then define a soft assignment $w(x_i, x_j)$ for each pair of samples $(x_i, x_j)$ using the sigmoid function $\\sigma(x) = 1/(1 + exp(-x))$:\n$w(x_i, x_j) = 2 \\alpha \\cdot \\sigma(-D(x_i, x_j) / \\tau_w),$  (3)\nwhere $\\alpha \\in [0, 1]$ is a boundary parameter that controls the upper bound of soft assignments. $\\tau_w$ is a sharpness parameter, where smaller values of $\\tau_w$ result in greater differences in $w(x_i, x_j)$ between sample pairs, and vice versa. Leveraging the soft assignments for all sample pairs, we propose a soft contrastive loss to refine the traditional hard contrastive loss. Specifically, for a pair of projected embeddings $(z_i, \\tilde{z_i})$, we first calculate the softmax probability of the relative similarity among all similarities as:\n$p(z_i, \\tilde{z_i}) = \\frac{exp(sim(z_i, \\tilde{z_i}) / T_c)}{\\sum_{z' \\in Z \\setminus \\{z_i, \\tilde{z_i}\\}} exp(sim(z_i, z') / T_c)},$  (4)\nwhere $sim(\\cdot, \\cdot)$ refers to the cosine similarity, and $T_c$ is a temperature parameter used to adjust the scale. Based on $p(z_i, \\tilde{z_i})$, the soft contrastive loss is then defined as:\n$\\mathcal{L}_{C, i} = -log p(z_i, \\tilde{z_i}) - \\frac{\\sum_{x' \\in \\Omega \\setminus \\{x_i, \\tilde{x_i}\\} \\atop z' \\in Z \\setminus \\{z_i, \\tilde{z_i}\\} } w(x_i, x') log p(z_i, z')}{\\sum_{x' \\in \\Omega \\setminus \\{x_i, \\tilde{x_i}\\} } 1},$  (5)\nwhere $\\Omega = \\mathcal{X} \\cup \\tilde{\\mathcal{X}}$ represents the union of the data spaces of the original and masked samples. By assigning soft weights to different sample pairs, the model is encouraged to better capture the inherent correlations across different samples. During the training process, the final soft contrastive loss $\\mathcal{L}_{C}$ is computed by summing and averaging $\\mathcal{L}_{C, i}$ across all samples within a mini-batch. Notably, when $\\forall w(x_i, x') = 0$, the soft contrastive loss reduces to the traditional hard contrastive loss.\nAggregate Reconstruction To further capture the fine-grained relationships between different samples, we incorporate an aggregator for weighted aggregation and reconstruction. Current approaches for masked EEG modeling typically reconstruct the masked portion based on the unmasked portion of a single masked sample (Lan et al. 2024; Pang et al. 2024), following the learning paradigm of MAE (He et al. 2022). However, this single-sample reconstruction strategy overlooks the interactions between samples, leading to a complex and inadequate reconstruction process.\nTo overcome this limitation, we introduce an aggregator that improves the traditional single-sample reconstruction process. Specifically, we first calculate the cosine similarity between each pair of projected embeddings $(z_i, z_j)$ within a mini-batch, resulting in a similarity matrix $S$. Based on the pairwise similarities in $S$, the aggregator then performs weighted aggregation of the embedding $h_i$. The weighted aggregation process is defined as:\n$h_i' = \\sum_{z' \\in Z \\setminus \\{z_i\\}} \\Big( \\frac{exp(sim(z_i, z') / T_c)}{\\sum_{z'' \\in Z \\setminus \\{z_i\\}} exp(sim(z_i, z'') / T_c)} \\cdot h' \\Big),$  (6)\nwhere $h' \\in \\mathcal{H} \\setminus \\{h_i\\}$ represents the encoded embedding corresponding to the projected embedding $z'$, and $\\mathcal{H}$ denotes the embedding space of the encoder $E$. This approach allows for a more comprehensive reconstruction by aggregating complementary information and incorporating similar features from different samples during the reconstruction process, while suppressing interference from irrelevant noise samples. Finally, the reconstructed embedding $h_i'$ is fed into a lightweight decoder $D$ to obtain the reconstructed sample $\\tilde{x_i}$. Following the masked modeling paradigm, we use Mean Square Error (MSE) as the reconstruction loss for model optimization, which is defined as:\n$\\mathcal{L}_{R, i} = ||x_i - \\tilde{x_i}||^2.$  (7)\nSimilar to the soft contrastive loss $\\mathcal{L}_{C}$, the final reconstruction loss $\\mathcal{L}_{R}$ is computed by summing and averaging $\\mathcal{L}_{R, i}$ across all samples within a mini-batch.\nThe pre-training process of SCMM During the pre-training process, SCMM is trained by jointly optimizing $\\mathcal{L}_{C}$ and $\\mathcal{L}_{R}$. The overall pre-training loss is defined as:\n$\\mathcal{L} = \\lambda_c \\mathcal{L}_C + \\lambda_R \\mathcal{L}_R,$  (8)\nwhere $\\lambda_C$ and $\\lambda_R$ are trade-off hyperparameters, which are adaptively adjusted according to the homoscedastic uncertainty of each loss item. Algorithm 1 details the pre-training process of the proposed SCMM."}, {"title": "Experiments", "content": "We conduct extensive experiments on three publicly available datasets, SEED (Zheng and Lu 2015), SEED-IV (Zheng et al. 2018), and DEAP (Koelstra et al. 2011), to evaluate the model performance of SCMM. These datasets are diverse in terms of EEG equipment, emotional stimuli, data specifications, labeling approaches and subjects, making them well-suited for assessing the model's efficacy in cross-corpus EEG-based emotion recognition tasks. In our experiments, we use differential entropy (DE) features as inputs. "}, {"title": "Discussion", "content": "Generalization capability analysis To further validate the generalization capability of SCMM, we conduct additional experiments on the SEED and DEAP datasets under a different-class cross-corpus scenario. It is notable that the EEG acquisition equipment, emotional stimuli, data specifications, labeling approaches and subjects are completely different between the two datasets. Experimental results demonstrate that the proposed SCMM maintains excellent performance even when the pre-training and fine-tuning datasets are completely different, highlighting its superior generalization capability."}, {"title": "Conclusion", "content": "This paper proposes a novel self-supervised pre-training framework, Soft Contrastive Masked Modeling (SCMM), for cross-corpus EEG-based emotion recognition. Unlike traditional contrastive learning models, SCMM integrates soft contrastive learning with a hybrid masking strategy to effectively capture the \"short-term continuity\" characteristics inherent in human emotions, and produce stable and generalizable EEG representations. Additionally, an aggregator is developed to weightedly aggregate complementary information from multiple close samples, thereby enhancing fine-grained feature representation capability in the modeling process. These findings suggest that SCMM enhances the feasibility of extending the proposed method to real-life aBCI applications."}, {"title": "Appendix A Datasets", "content": "A.1 Dataset Description\nWe conduct extensive experiments on three well-recognized datasets, SEED (Zheng and Lu 2015), SEED-IV (Zheng et al. 2018), and DEAP (Koelstra et al. 2011), to evaluate the model performance of SCMM in cross-corpus EEG-based emotion recognition tasks.\n(1) SEED (Zheng and Lu 2015) was developed by Shanghai Jiao Tong University. The dataset used a 62-channel ESI NeuroScan System based on the international 10-20 system to record EEG signals from 15 subjects (7 males and 8 females) under different video stimuli at a sampling rate of 1kHz. Each subject participated in 3 sessions. In each session, each subject was required to watch 15 movie clips consisting of 3 different emotional states: negative, neutral and positive. Each emotional state contains a total of 5 movie clips, corresponding to 5 trials.\n(2) SEED-IV (Zheng et al. 2018) used the same EEG acquisition equipment as the SEED dataset, but with different video stimuli, emotion categories and subjects. The dataset recorded EEG signals from 15 subjects under different video stimuli at a sampling rate of 1kHz. Each subject participated in 3 sessions. In each session, each subject was required to watch 24 movie clips containing 4 different emotions: sad, neutral, fear and happy. Each emotion contains a total of 6 movie clips, corresponding to 6 trials.\n(3) DEAP (Koelstra et al. 2011) was constructed by Queen Mary University of London. The dataset used a 128-channel Biosemi ActiveTwo System to record EEG signals from specific 32 channels of 32 subjects (16 males and 16 females) while watching 40 one-minute music videos at a sampling rate of 512Hz. The 40 one-minute videos elicited different emotions according to the valence-arousal dimension. Specifically, the valence-arousal emotional model, first proposed by Russell (Russell 1980), places each emotional state on a two-dimensional scale. The first dimension represents valence, ranging from negative to positive, and the second dimension represents arousal, ranging from calm to exciting. Participants rated valence and arousal using a continuous scale of 1 to 9 after watching each video clip.\nA.2 Pre-processing Procedures\nFor the SEED and SEED-IV datasets, the raw EEG signals were initially downsampled to 200Hz and filtered through a bandpass filter of 0.3-50Hz to filter noise and remove artifacts. Then, the data were divided into multiple non-overlapping segments using sliding windows of 1 second for SEED and 4 seconds for SEED-IV, respectively. After that, we extracted differential entropy (DE) features for each channel of each segment at five frequency bands: Delta (1-4Hz), Theta (4-8Hz), Alpha (8-14Hz), Beta (14-31Hz), and Gamma (31-50Hz). Finally, the DE features from 62 channels and 5 bands were formed into a feature matrix of shape 62 \u00d7 5, which serves as input to the SCMM model. The extraction of DE features can be expressed as:\n$DE(x) = \\frac{1}{2}log(2\\pi e \\sigma^2),$  (9)\nHere, $x$ represents an EEG signal segment of a specific length that approximately obeys a Gaussian distribution $\\mathcal{N}(\\mu, \\sigma^2)$, where $\\sigma$ denotes the standard deviation of $x$, and e is the Euler constant.\nFor the DEAP dataset, the raw EEG signals were first downsampled to 128Hz and denoised by a bandpass filter of 4-45Hz. Subsequently, the data were segmented into multiple non-overlapping segments using a sliding window of 1s. Similar to the SEED and SEED-IV datasets, DE features were extracted for each channel of each segment at five frequency bands. Finally, the DE features from 32 channels and 5 bands were formed into a feature matrix of shape 32 \u00d7 5 as input to the model. During the experiments, we divided the continuous labels using a fixed threshold of 5 to convert them to binary classification tasks (low / high).\nA.3 Handling Different Number of Channels\nSince the SEED-series datasets and the DEAP dataset contain different numbers of electrodes (channels), we require channel processing before inputting DE features into the model. Specifically, we consider the fine-tuning dataset as an anchor. When the number of channels in the fine-tuning dataset is less than in the pre-training dataset, we select data from the corresponding channels in the pre-training dataset and drop the data from the redundant channels as inputs (e.g., pre-training on SEED and fine-tuning on DEAP). Conversely, when the number of channels in the fine-tuning dataset is greater than in the pre-training dataset, we fill the missing channel data with zeros in the pre-training dataset to match the fine-tuning dataset (e.g., pre-training on DEAP and fine-tuning on SEED)."}, {"title": "Appendix B Implementation Details", "content": "To reduce computational load while maintaining model performance, we adopt a lightweight design for each module of SCMM. Specifically, we use a 3-layer 1D CNN for the encoder E and a 2-layer MLP for the projector P. For the lightweight decoder D, we utilize a single-layer MLP for reconstruction. Regarding hyperparameter selection in the pre-training stage, we set r to 0.5 and $\\mu$ to 0.1 for hybrid masking, i.e., the percentage of random masking and channel masking is 0.9 and 0.1, respectively. We use the negative of cosine similarity as the metric function $Dist(\\cdot, \\cdot)$, and we set $\\alpha$ to 0.5, $\\tau_w$ to 0.05 and $T_c$ to 0.5 for soft contrastive learning. We use Adam as the optimizer with a learning rate of 5 \u00d7 10-4 and an L2-norm penalty coefficient 3 \u00d7 10\u22124. The pre-training process is conducted over 200 epochs with a batch size of 256. We save the model parameters $\\theta$ from the final epoch as the pre-trained SCMM. In the fine-tuning stage, we input the encoded embeddings $h_i$ into a classifier consisting of a 2-layer fully connected network for final emotion recognition. The Adam optimizer is utilized with a learning rate of 5 \u00d7 10-4 and a weight decay of 3 \u00d7 10-4. The number of fine-tuning epochs is set to 50 for the SEED and SEED-IV datasets and 500 for the DEAP dataset, with a batch size of 128. For efficient deployment and testing of the model, the pre-trained SCMM is optimized solely using cross-entropy loss during fine-tuning. All experiments"}, {"title": "Appendix C Baseline Methods and Experimental Settings", "content": "C.1 Baseline Methods\nWe compare the proposed SCMM against eight competitive methods, including four traditional deep learning methods: BiDANN (Li et al. 2018), TANN (Li et al. 2021), PR-PL (Zhou et al. 2023) and E2STN (Zhou et al. 2024), as well as four self-supervised learning models: SimCLR (Chen et al. 2020; Tang et al. 2021), Mixup (Zhang et al. 2018; Wickstr\u00f8m et al. 2022), MAE (He et al. 2022) and JCFA (Liu et al. 2024a). Details of the eight baseline methods are summarized as follows.\n\u2022 BiDANN (Li et al. 2018): The bi-hemispheres domain adversarial neural network mapped the EEG data of both left and right hemispheres into discriminative feature spaces separately to address domain shifts in EEG-based emotion recognition tasks.\n\u2022 TANN (Li et al. 2021): The transferable attention neu-tral network is a novel transfer learning methods which learned the discriminative information from EEG signals using local and global attention mechanisms.\n\u2022 PR-PL (Zhou et al. 2023): The prototypical representa-tion based pairwise learning framework adopted pairwise learning to model the relative relationships between EEG sample pairs in terms of prototypical representations, addressing the critical issues of individual differences and noise labels in cross-subject scenarios.\n\u2022 E2STN (Zhou et al. 2024): The emotional EEG style transfer network integrated content information from the source domain with style information from the target domain, achieving superior performance in cross-corpus EEG-based emotion recognition tasks.\n\u2022 SimCLR (Chen et al. 2020; Tang et al. 2021): A semi-nal work in self-supervised contrastive learning, first pro-posed for computer vision, has been extended to human activity recognition.\n\u2022 Mixup (Zhang et al. 2018; Wickstr\u00f8m et al. 2022): A novel CL-based data augmentation method that aimed to correctly predict the mixing proportion of two samples, has been applied to time series analysis.\n\u2022 MAE (He et al. 2022): A groundbreaking work in the field of mask modeling, which proposed to reconstruct the masked portion based on the unmasked portion of the masked sample, has achieved remarkable success in a wide range of fields.\n\u2022 JCFA (Liu et al. 2024a): The joint contrastive learning framework performed joint contrastive learning across two domains to synchronize the time- and frequency-based embeddings of the same EEG sample in the latent time-frequency space, achieving state-of-the-art perfor-mance in cross-corpus scenarios.\nTo ensure a fair comparison, we adopt the same encoder, projector, decoder, and classifier structures for SimCLR, Mixup and MAE as used in SCMM. We use the default hyperparameters reported in the original paper for all methods in our experiments, unless otherwise specified. Additionally, for BiDANN, TANN, PR-PL, E2STN, MAE, and SCMM, the input samples are preprocessed 1-s DE features. In contrast, SimCLR, Mixup, and JCFA use preprocessed 1-s EEG signals as inputs, in accordance with the specific design of each model.\nC.2 Experimental Settings\nWe adopt a cross-corpus subject-independent experimental protocol in the experiments, following the setup used in"}, {"title": "Appendix D Masking Strategy", "content": "This paper introduces a novel hybrid masking strategy to generate diverse masked samples by considering both channel and feature relationships. To compare our approach with traditional masking strategies, we explore three different masking rules: random masking, channel masking, and hybrid masking. \n(1) Random Masking: Generate masks using a binomial distribution to randomly mask samples in the feature dimension, setting the values of the masked features to zero.\n(2) Channel Masking: Generate masks using a binomial distribution to randomly mask samples in the channel dimension, setting the values of all features within the masked channels to zero.\n(3) Hybrid Masking: Generate a probability matrix using a uniform distribution that proportionally mixes masks generated from random masking and channel masking.\nOur proposed hybrid masking strategy is highly flexible and can be extended to various datasets by integrating multiple masking strategies in different ratios, which is exceptionally suitable for data with rich semantic information. This approach effectively generates more diverse masked samples, encouraging the model to comprehensively capture the inherent relationships of the data."}, {"title": "Appendix E Hyperparameter Analysis", "content": "We conduct comprehensive experiments to verify the hyperparameter sensitivity of SCMM on the SEED and SEED-IV 3-category datasets. The hyperparameters examined include the masking ratio r", "9": 1}]}