{"title": "Using Machine Learning to Discover Parsimonious and Physically-Interpretable Representations of Catchment-Scale Rainfall-Runoff Dynamics", "authors": ["Yuan-Heng Wang", "Hoshin V. Gupta"], "abstract": "Despite the excellent real-world predictive performance of modern machine learning (ML) methods, many scientists remain hesitant to discard traditional physical-conceptual (PC) approaches due mainly to their relative interpretability, which contributes to credibility during decision-making. In this context, a currently underexplored aspect of ML is how to develop \u201cminimally-optimal\u201d representations that can facilitate better \u201cinsight regarding system functioning\u201d. Regardless of how this is achieved, it is arguably true that parsimonious representations better support the advancement of scientific understanding. Our own view is that ML-based modeling of geoscientific systems should be based in the use of computational units that are fundamentally interpretable by design.\nThis paper continues our exploration of how the strengths of ML can be exploited in the service of better understanding via scientific investigation. Here, we use the Mass Conserving Perceptron (MCP) as the fundamental computational unit in a generic network architecture consisting of nodes arranged in series and parallel to explore several generic and important issues related to the use of observational data for constructing input-state-output models of dynamical systems. In the context of lumped catchment modeling, we show that physical interpretability and excellent predictive performance can both be achieved using a relatively parsimonious \u201cdistributed-state\u201d multiple-flow-path network with context-dependent gating and \u201cinformation sharing\u201d across the nodes, suggesting that MCP-based modeling can play a significant role in application of ML to geoscientific investigation.", "sections": [{"title": "1. Introduction, Background and Scope", "content": ""}, {"title": "1.1. Introduction", "content": "\\[1] The system theoretic (so-called \u201cblack box\u201d) approach to catchment-scale rainfall-runoff (RR) modeling can represent the dynamical behaviors of such systems without the need to incorporate prior knowledge regarding their internal form and functioning (Bunge, 1963). In fact, the Kolmogorov neural network existence theorem states that a three-layer feed-forward Artificial Neural Network (ANN) meets the requirements to be a universal function mapping, so that any multivariate function can be closely approximated by an ANN having only a finite number of nodes in the hidden layer (Kolmogorov, 1957; Hecht-Nielsen, 1987). In this regard, modern machine-learning (ML) provides a viable alternative to Physical/Conceptual (PC) modeling for simulating RR process, and can do this without explicitly representing the internal hydrologic structures of watersheds, while instead focusing primarily on achieving high predictive accuracy (Sorooshian, 1983).\n[2] However, despite the excellent predictive performance of modern ML methods in real-world applications, many scientists remain hesitant to replace/discard traditional PC-based approaches due mainly to their relative interpretability (model transparency), which contributes to credibility during decision-making (Rudin, 2019). This issue has prompted recent research into \u201cexplainable Al\u201d using methods such as Local Interpretable Model-Agnostic Explanations (LIME; Althoff et al., 2021), Shapley Additive Explanations (SHAP; Yang & Chui 2021), and the feature-importance-based Expected Gradient and Additive Decomposition method (Jiang et al., 2022). Meanwhile, Knowledge Guided Machine Learning (KGML; Willard et al., 2022) also provides a potential approach to enhanced interpretability (see detailed summary in Varadharajan et al., 2022) by transforming the \"black box\" models into \"glass (clear) box\u201d models (Rai, 2020), with the goal of achieving physically consistent and generalizable predictions at minimal model development cost.\n[3] In this context, it is our opinion that a currently underexplored aspect is the investigation of \u201cminimally-optimal\" network architectures that can facilitate better \u201cinsight regarding system functioning\u201d. This is in contrast to models that are based in poorly-interpretable computational units and generic architectures. One approach is to implement methods for network pruning/compression that can help to identify efficient low-rank sub-networks (Schotth\u00f6fer et al., 2022; Zangrando et al., 2023) thereby facilitating the quantification of information content (Tishby & Zaslavsky, 2015; Shwartz-Ziv & Tishby, 2017) and potentially enhancing the interpretability, and hence credibility, of ML-based approaches used for decision making. An alternative is to construct progressively more complex, but still architecturally generic, minimal description length representations until a satisfactory level of performance is achieved (Stanley & Miikkulainen, 2002). Our own view is that a more productive approach is to base the entire ML-based modeling approach in the use of computational units that are fundamentally interpretable by design (Wang & Gupta, 2024a,b). Regardless of which strategy is adopted, it is arguably true that parsimonious representations better support the advancement of scientific understanding (Weijs & Ruddell, 2020), based on which network architectures can be progressively augmented (as necessary and appropriate) during training (Hsu et al., 1995; Chen & Chang, 2009).\n[4] This paper is third in a series that seeks to explore how the strengths of machine learning can be exploited in the service of achieving better understanding via scientific investigation. In Section 1.2, we provide some foundational background, after which we discuss the goals and scope of the studies reported here (Section 1.3), and the organization of this paper (Section 1.4)."}, {"title": "1.2. Background", "content": "[5] There is a long history to the development of hydrologic models (see Singh, 1988; Clark et al., 2008; Fenicia et al., 2011; Gupta et al., 2012). In this regard, ANNs have been used for hydrologic prediction since at least the 1990's (Daniell, 1991; Halff et al., 1993; Hsu et al., 1995; Smith & Eli, 1995) and have consistently been shown to outperform PC-based models as evaluated by a variety skill metrics (Hsu et al., 1995). Such performance can attributed to the fact that, whereas the architecture of a knowledge-based PC models must be pre-specified/designed to be consistent with physical principles/laws (such as mass and energy conversation), a data-driven model can \u201clearn\u201d an effective representation of the appropriate internal architectural features, via iterative adjustment/training of model weights so as to extract the relevant information from data (Xu & Liang, 2021).\n[6] The past few years has seen extremely rapid development and application of machine/deep learning (ML/DL) artificial intelligence (AI) by the hydro-geo-scientific community (Shen, 2018). In particular, since its early application to RR modeling in the 1990's (Hsu et al., 1995), the recurrent neural network (RNN) architecture has gained considerable attention and popularity. This is particularly true of the Long Short Term Memory network (LSTM; Hochreiter & Schmidhuber, 1997) which can learn to model the long-term dependencies that characterize storage effects such as snowpack accumulation and melt (Kratzert et al., 2018). In terms of predictive accuracy, as well as ability to provide predictions in ungauged basins, the standard LSTM formulation remains the \"state-of-the-art\" for catchment-scale RR modeling (Kratzert et al., 2019b), although the recently developed transformer neural network (TNN; Vaswani et al., 2007) also shows promise for hydrological applications (Li et al., 2022; Liu et al., 2024b; Koya & Roy, 2024).\n[7] Notably, the LSTM-based approach has consistently been shown to outperform PC-based models for RR modeling (Kratzert et al., 2019a; Mai et al., 2022; Arsenault et al., 2023), and to be a viable surrogate for traditional data assimilation (Nearing et al., 2022), while enabling the leveraging of multiple sources of data regarding various meteorological variables (Kratzert et al., 2021) at multiple time scales (Gauch et al., 2021; Feng et al., 2020). Combined with time-efficient computation, these capabilities make LSTM-based modeling highly attractive for operational hydrologic forecasting (Harrigan et al., 2023; Sabzipour et al., 2023), especially at the global scale (Nearing et al., 2023; Kratzert et al., 2024).\n[8] The exponential rate of development and widespread access to ML technology (and especially the power of differentiable programming; Baydin et al., 2018), has spawned considerable innovation in the development of modeling strategies and tools that combine physical principles with Al (Shen et al., 2023). One major research thread involves the use of data-driven methods for postprocessing the outputs of PC-based models (Nearing et al., 2020b; Frame et al., 2021). Post-processing is a relatively simple way to improve predictive accuracy in cases where the PC-based model encodes information provided by human knowledge that the ML algorithm is unable to extract from data.\n[9] A second major thread aims to enhance model behavioral expressivity by adding functional complexity to the PC-based model (Jiang et al., 2020; Feng et al., 2022; Bennett & Nijssen, 2021). The model is improved by implementing operational neural network layers to learn improvements to the existing model parameterization (Tsai et al., 2021) or to provide substitutes for poorly understood process relationships (H\u00f6ge et al., 2022; Feng et al., 2023).\n[10] A third major thread involves (partially) modifying the internal neural network architecture (Nourani, 2021) for the purpose of improving the regionalization ability of the model (e.g., EA-LSTM; Kratzert et al., 2019b). This approach encodes physical principles such as mass conservation (MC-LSTM; Hoedt et al., 2021; Frame et al., 2022), together with regularization to constrain (for example) evapotranspiration loss (Wi & Steinschneider, 2024), or to add an attention mechanism that helps to identify important catchment specific features (e.g., Hydro-LSTM; De la Fuente et al., 2024a). Overall, such investigations have enlivened the community and contributed to rapid advances in the Earth, Environmental, and Geosciences (Fleming et al., 2021)."}, {"title": "1.3. Goals and Scope", "content": "[11] This study builds upon our previous work reported in Wang & Gupta (2024a, b). In the first (Wang & Gupta, 2024a) we proposed a physically-interpretable computational unit (named the Mass Conserving Perceptron) to be used as a component (node) in neural networks that can directly learn the functional nature of physical processes from available data (as in machine \u201clearning\u201d) using off-the-shelf ML technologies, while being regularized to obey conservation principles at the nodal level. The purpose was to explore the behavioral expressivity, interpretability, and performance achievable by a single MCP node (a single cell-state model) enabled by the learnable gating mechanism, and where all of the architectural complexity was expressed only through those gating functions. In particular, we demonstrated how prior knowledge and/or hypotheses regarding system dynamics can be progressively encoded into a simple MCP-based single-node model, thereby enabling the scientist to test different hypothesis regarding the internal functioning of the catchment (Gong et al., 2013; Nearing et al., 2020\u03b1).\n[12] Next, in Wang & Gupta (2024b) we showed that the MCP can be used as a building block for constructing more complex, but parsimonious, directed graph architectures consisting of node (state variable) and link (flow path) subcomponents (Gupta & Nearing, 2014), that obey conservation principles and are conceptually-interpretable in the traditional PC sense, while achieving comparable performance to purely data-based models. The purpose was to show how ML can be used to effectively combine theory-based prior information with novel information extracted from data, thereby enabling hypotheses testing regarding appropriate system architecture (numbers of dynamical state variables and their interconnectedness) and an examination of how information is increased, decreased, or altered during stagewise model development (Fenicia et al., 2008; Kavetski & Fenicia, 2011; Nearing & Gupta, 2015; Gharari et al., 2021).\n[13] In this work, we seek to explore a number of generic and important issues related to the use of time series data for the construction of dynamical input-state-output models. Accordingly, instead of using physical-conceptual principles and/or theory to guide specification of the form of the directed graph network architecture, wherein the nodes (cell states) and links (pathways) are pre-emptively assigned conceptual meaning at the time of model specification based on physical-conceptual understanding/theory (e.g., as was done in Wang & Gupta, 2024b), we follow the function approximation paradigm employed by modern ML wherein a generic network architecture is implemented consisting of \u201cbasis function nodes\u201d arranged in series and parallel. By using the MCP node as the fundamental computational unit, we are able to explore (in the context of a lumped catchment system) several generic and important modeling issues such as:\n1) How many network \u201clayers\u201d, \u201cnodes\u201d (cell-states) and \"links\" (flow pathways) are potentially necessary/sufficient to accurately model the input-state-output dynamics of a given system?\n2) Is a \"distributed-input\" or a \u201cdistributed-state\" representation (or some hybrid combination of the two) a more suitable approach to network regularization?\n3) Consistent with physical understanding that water balance closure at the overall catchment-scale is typically impossible to assert with any degree of confidence, is there potential benefit to relaxing mass conservation at the overall \u201cnetwork\u201d level while maintaining mass-conservation at the \u201cnodal\u201d level?\n4) Is there benefit to allowing the nodal \"gates\" to be informed about the entire distribution of \u201cmoisture\" across the system when determining what the time-varying (context-dependent) output and loss gate conductivities should be at each time step?\n5) How much interpretability can be achieved/maintained by such a network while permitting it to pursue the aim of optimal predictive performance?\n6) How does such predictive performance compare to that obtained using \"purely data-based\u201d and/or conventional \u201cphysical-conceptual\" models?\n7) Are there potential benefits to \u201ctraining\u201d and then \u201cpruning\u201d such MCP-based machine-learning networks?\n[14] To our knowledge, no similar efforts have been made in this regard. While generic RNN-based networks (Nearing et al., 2021; Kratzert et al., 2024) are capable of very high levels of predictive performance, we follow Occam's Second Razor (Domingos, 1998) in suggesting that it is sensible to \u201cthink twice\" before abandoning interpretability in favor of non-understandable complexity. As with our previous work (Wang & Gupta, 2024a,b), our scope here remains restricted to an examination of interpretable architectural complexity at a single location, rather than universal applicability across large samples of catchments. Combined with our previous findings, the results reported here form the necessary pre-requisite for broader application to multiple hydro-climatic regimes (work in progress), and to eventually tackling the problem of interpretable ML-based modeling for prediction in ungaged basins (PUB; Sivapalan et al., 2003; Hrachowitz et al., 2013).\""}, {"title": "1.4. Organization of the paper", "content": "[15] In Section 2 we briefly recap relevant details regarding the physically-interpretable Mass-Conserving Perceptron and its analogical relationship to the fundamental computational component of the data-based LSTM network. Section 3 outlines the data, methods, and MCP-based network architectures explored in this study, and discusses the \u201coverall interpretability\u201d associated with such network architectures. Sections 4 and 5 discuss our findings and results. In particular Section 5 explores the value of \u201cinformation sharing\" across nodes of the network, a feature that is built-in to the standard LSTM architecture but is not commonly found in PC-based representations of geoscientific (e.g., hydrological) systems. Section 6 presents a benchmark comparison against both MCP-based models introduced in Wang & Gupta (2024b) and the standard LSTM networks, while Section 7 illustrates the interpretability of MCP-based neural networks. Finally, in Section 8, we conclude with a discussion of implications and directions for future work."}, {"title": "2. Methodology", "content": ""}, {"title": "2.1 The Mass-Conserving Perceptron (MCP)", "content": "[16] In Wang & Gupta (2024a), we proposed the mass-conserving perceptron (MCP) as an ML-based physically-interpretable computational unit that is isomorphically similar to a single node of a generic gated recurrent neural network, but is different in that it enables mass flows to be conserved at the nodal level. Figure 1b illustrates the architecture of the MCP node. The node represents mass-conservative system dynamics via the discrete time update equation:\nXt+1=XtOt - Lt + Ut\n(1)\nwhereby the mass state Xt+1 of the system (node) at time step t + 1 is computed by adding the mass of input flux Ut that enters the node, and subtracting the masses of output fluxes Ot and Lt that leave the node, during the time interval from t to t + 1. For example, in the context of spatially-lumped catchment-scale RR modeling, Ut can represent the precipitation mass input flux, and Lt and Ot can represent the evapotranspirative and streamflow mass output fluxes from the system control volume represented by the node.\n[17] It is assumed that Ot and Lt depend on the value of the state Xt through the process parameterization equations Ot = G \u2022 Xt and Lt = G\u30fbXt, where Ge and Gt are context-dependent (see later) time-varying \u201coutput\u201d and \u201closs\u201d conductivity gating functions respectively, so that Eqn (1) can be rewritten as:\nXt+1 = Xt - G\u2022X \u2212 G\u00a5\u2022 Xt + Ut\nXt+1 = GfXt + Ut\n(2a)\n(2b)\nwhere Gf is the \u201cremember\u201d gate, represents the fraction of the state Xt that is retained by the system from one time step to the next. To ensure physical realism, we require that the time-evolving values of each of these gates (Ge, G\u0142 and G) remain both non-negative and less than or equal to 1.0 at all times. Further, to ensure conservation of mass we require that G + G + G = 1, which means that the remember gate is computed from knowledge of the output and loss gates as G = 1 \u2212 G \u2013 G\u0269 ; this of course places a strict constraint on the relative values that Ge and G\u2021 can take on. Now, assuming knowledge of the initial mass state of the system Xo, and given the time history of inputs U\u2081, ..., Ut, Eqn (2) can be used to sequentially update the state Xt of the system if the time-evolving values of the gating functions Ge and Gt (and therefore G) are also provided."}, {"title": "2.2 Long Short-Term Memory Network (LSTM)", "content": "[22] As a data-driven benchmark to evaluate performance of the MCP-based networks tested in this study (Nearing et al, 2020) we use the LSTM network, adapted from code provided by Kratzert et al. (2019). The LSTM network is a type of recurrent neural network that includes memory cells that can store information over long periods of time, and that uses three gating operations (input, forget, output) as shown in Figure 1c. The mathematical formulation of the LSTM network is provided in the supplementary materials.\n[23] Given an input sequence x = [x[1], x[2] ... ..., x[T]] with T time steps, where each element x[t] is a vector containing input features (model inputs) at time step t (1 \u2264 t \u2264 T), Eqns (3-8) specify a single forward pass through the LSTM:\ni[t] = \u03c3(bix[t] + w\u2081h[t \u2212 1] + a\u2081)\n(3)\nf[t] = \u03c3(bfx[t] + wsh[t \u2013 1] + af)\n(4)\ng[t] = tanh(bgx[t] + wgh[t \u2212 1] + ag)\n(5)\no[t] = \u03c3(box[t] + wh[t - 1] + a\uff61)\n(6)\nc[t] = f[t]c[t \u2013 1] + i[t] \u2299g[t]\n(7)\nh[t] = o[t] tanh (c[t])\n(8)\nwhere i(t), f(t), o(t) are the input, forget and output gates respectively, g(t) is the cell input, x(t) is the network input at time step t (1 \u2264 t \u2264 T), and h(t-1) is the recurrent input. The terms c(t) and c(t \u2212 1) indicate the cell states at the current and previous time step. At the first-time step, the hidden and cell states are initialized as vectors of zeros. The terms a, w and b are learnable parameters for each gate, with subscripts referring to which gate the particular weight matrix, or bias vector corresponds to. The sigmoid activation function \u03c3 (\u00b7) outputs a value between 0 and 1, while the hyperbolic tangent activation function tanh (*) outputs a value between -1 and 1. The symbol indicates element-wise multiplication.\n[24] The values of the cell states can be modified by the forget gate f(t), which can delete states. The cell update g(t) can be interpreted as information that is added, while the input gate i(t) controls into which cells new information is added. The output gate o(t) controls which of the information, stored in the cell states, is output. Note that the cell states c(t) characterize the memory of the system, and its simple linear interaction with the remaining LSTM cell helps to prevent the issue of exploding or vanishing gradients during the back-propagation step of network training (Hochreiter & Schmidhuber, 1997). The output of the final LSTM layer h(t) is connected through a dense layer to a single output neuron, which computes the final output y(t) prediction, as indicated by Eqn 9:\ny = bahn + ad\n(9)\nwhere ba and a\u0105 are learnable weights and bias terms of a dense output layer."}, {"title": "2.3 Isomorphic Relationship Between Architectures", "content": "[25] As discussed above (Section 2.1), the MCP unit is structurally isomorphic to the representation of a simple physical RR system expressed as an RNN. Eqns (10-16) show how the MCP unit is isomorphically similar to that of the LSTM:\ni(t) = G = 1.0\n(10)\nf(t) = G = 1.0 \u2013 G \u2013 G\u00a5\n(11)\ng(t) = Ut\n(12)\no(t) = G = \u03ba\u03bf \u2299 \u03c3(boc(t) + a\uff61)\n(13)\nl(t) = G = \u03ba\u2081 \u2299 \u03c3(b\u2081PETt + a\u2081)\n(14)\nc(t + 1) = Xt+1 = f(t) \u2299 Xt + i(t) g(t)\n(15)\nO(t) = o(t) Xt\n(16)\n[26] Note that whereas the input gate i(t) = G (Eq 10) is set to 1.0, thereby indicating that all of the input mass enters the cell state, one can also create a \u201cbypass\u201d gate by defining GP = 1 \u2013 G\u00ec that allows some quantity of the input mass to bypass the unit. The LSTM forget gate f(t) is re-interpreted (Eqn 11) as a \u201cremember gate\u201d Gf since its value (which varies between 0 and 1) indicates the extent to which the system retains water at each time step. Accordingly, the cell update g(t) is equal to the mass input Ut to the unit at any given time (Eqn 12). The output gate o(t) = Go, and the newly proposed loss gate l(t) = G+, are simply parameterized here as being dependent on the current timestep cell state and potential evapotranspiration respectively (Eqns 13-14); more complex dependencies could also be envisioned and implemented. The mass output 0(t) is computed as a fraction o(t) = Ge of the current timestep internal state X\u0165 (Eq 16). Finally, the cell state c(t + 1) = Xt+1 is updated to represent how much of the mass is retained by the unit, and augmented by the incoming input mass at the current time step (Eq 15). As such, given the constraints imposed on the values of its gating functions (G + G + G = 1 and 0 \u2264 G, G, G \u2264 1), each node of an MCP-based network can be understood to function like a mass-constrained version of a node of an LSTM network.\n[27] It is important to note that, compared to the Nash linear reservoir tank (Nash, 1957), the MCP can be viewed as a non-linear reservoir with evapotranspirative loss, where the time-constant conductivity parameter is replaced by a time-variable gating function. In this context, the mathematical structure of the simple mass-balance linear reservoir model is isomorphically similar to the MCP (and by extension, LSTM), with adjustments to the remember gate (Eq 17) and output gate (Eq 18), and by removing the loss gate. These relationships are clearly illustrated in Figure 1a.\nf(t) = G = 1.0 \u2013 G\n(17)\no(t) = G = ko\n(18)"}, {"title": "3. Experimental Setup", "content": ""}, {"title": "3.1 Study Catchment and Data Set", "content": "[28] All of the experiments reported in this study use the Leaf River data set (compiled by the US National Weather Service), consisting of 40 years (WY 1949-1988) of daily data from the humid, 1944 km\u00b2, Leaf River Basin (LRB) located near Collins in southern Mississippi, USA. The dataset consists of cumulative daily values of observed mean areal precipitation (PP; mm/day), potential evapotranspiration (PET; mm/day), and watershed outlet streamflow (QQ; mm/day). The dataset has been widely used for model development and testing by the hydrological science community."}, {"title": "3.2 Data Splitting", "content": "[29] As discussed by Shen et al. (2022), it is important to use only a portion of the available data D for making decisions about the model representation (choices regarding model structure and parameter values), while retaining a separate portion for testing the validity of those choices. Here, we follow the data-splitting procedure reported in Wang & Gupta (2024a) and adopt the robust data allocation method developed by (Zheng et al., 2022) that partitions the data (D) to ensure distributional consistency of the observational streamflow records across three subsets of the data to be used for training (Dtrain), selection (Dselect), and testing (Dtest). This contributes to ensuring that model performance is relatively consistent across each of three independent sets (Chen et al., 2022; Maier et al., 2023), and enables us to reasonably neglect the need for procedures such as k-fold cross validation.\n[30] For all experiments, we set the Dtrain: Dselect: Dtest partitioning ratio to be 2:1:1 respectively. The data splitting procedure first sorts the streamflow data based on magnitude. Next, the timestep associated with the largest streamflow magnitude is paired with the timestep associated with the smallest streamflow value, continuing with the next largest and smallest values and so on, until all time steps have been paired. These pairs are then sequentially allocated, in the abovementioned ratio, to the three independent sets (following the sequence of Dtrain \u2192 Dtest \u2192 Dselect \u2192 Dtrain etc.) until all pairs have been assigned. Overall, given 14,610 time-steps/days in the 40-year LRB dataset, this results in a training subset consisting of 7,306 timesteps, and selection and testing subsets consisting of 3,652 time-steps each."}, {"title": "3.3 Metrics Used for Training and Performance Assessment", "content": "[31] The metric used for model training was the Kling-Gupta Efficiency (KGE; Eqn. 19) (Gupta et al., 2009). As in Wang & Gupta (2024a,b), each model architecture was trained 10 times with random initialization of the parameters, from which the one having the highest scaled KGE score (KGEss; Eqn. 20) (Knoben et al., 2019) computed on the selection set was retained. Performance assessment was conducted using KGEss and the components of KGE (Eqns 21-23):\nKGE = 1 - \u221a((pKGE \u2014 1)2 + (BKGE \u2014 1)2 + (KGE \u2014 1)2)\n(19)\nKGESS = 1\n(1-KGE)\n\u221a2\n(20)\nAKGE\n\u03c3\u03b5\n(21)\n\u03c30\nBKGE =\n\u03bc\u03b5\n(22)\n\u03bc\u03bf\nPKGE\nCovso\n\u03c3\u03b5\u03c3\u03bf\n(23)\nwhere \u03c3\u03b5 and \u03c3\u03bf are the standard deviations, and \u03bcs and \u03bc\u3002 are the means, of the corresponding data-period simulated and observed streamflow hydrographs respectively and, similarly, Covso is the covariance between the simulated and observed values. Note that KGE (and therefore KGE55) is maximized when aKGE, BKGE and PKGE are all 1.0.\n[32] Although aKGE and \u1e9eKGE are both optimal at 1.0, their values can be larger or smaller than this optimal value which lead to ambiguity when conducting model comparisons using these metrics. Here, we define AKGE and BKGE as shown in Eqns (24-25) to circumvent this problem through allowing 1.0 to be upper bound value.\nAKGE = 1 \u2212 |1 \u2013 AKGE|\n(24)\nBKGE = 1- |1 \u2013 BKGE|\n(25)"}, {"title": "3.4 Training Procedures, Algorithm and Hyperparameter Selection", "content": "[33] The training procedures were also adopted from Wang & Gupta (2024a,b). To initialize the model cell-states, we used a \u201cthree-year\u201d spin-up period that sequentially repeats the first water year data (WY 1949) three times at the start of the overall 40-year simulation period. This helps to minimize the potential effects of state initialization errors (De la Fuente et al., 2023). The gradient-based ADAM optimization algorithm (Kingma & Ba, 2014) was used for model training (i.e., to determine optimal values for the parameters of the gating functions). The training metric and its gradient were computed using the streamflow values/timesteps assigned to the training subset."}, {"title": "3.5 Basic Network Architectures Tested", "content": "[34] Wang & Gupta (2024a) explored the expressive power of a single MCP node (cell-state) while enabling the gating operations to be represented with various levels of functional complexity. We showed that the basic mass-conserving node, represented as MC{OLcon}, is able to provide the bare minimum amount of complexity/flexibility required to achieve good predictive accuracy while also being physically-interpretable. This unit uses simple sigmoid activation functions (indicated by subscript \u03c3) for the construction of the output and loss gates (Or and Lo respectively), while constraining (indicated by superscript con ) the evapotranspirative loss flux to be less than or equal to PET.\n[35] Subsequently, Wang & Gupta (2024b) showed how the MCP unit can be used as a basic building block for constructing and testing a variety of multi-node (multi-cell-state) conceptually-interpretable representational hypotheses (architectures) for the spatially-lumped catchment-scale Leaf River system. As in Wang & Gupta (2024a), the models were constructed and trained using readily-available ML technologies.\n[36] Here, we explore the predictive performance achievable by use of basic MCP units as the building blocks of \"interpretable neural networks\u201d (INN), where the network architectures consist of layers of fully-connected \"nodes\u201d as is common, for example, when building LSTM models, and where the main hyperparameters to be tuned are the numbers of layers and the numbers of nodes in each layer. To achieve additional expressive power, we modified the loss gate Loon so that (in addition to PET) it is also informed by the value of the cell-state, and refer to this unit using the notation MC{OLcor}, where the additional + symbol indicates this augmentation. Hereafter, unless otherwise mentioned, all nodes in the MCP-based networks are of the augmented MC{OLcor} type. In Section 4, we show that this modification results in a significant performance improvement."}, {"title": "3.5.1 Distributed-Input (DI) Network", "content": "[40] We use the notation MNDI(...) to indicate a network where the input distribution weights win are constrained to all be positive and sum to one (i.e., \u2211j win = 1, win \u2265 0 for all j), while the output aggregation weights are all identically equal to one (i.e., wput = 1 for all k).\n[41] This architecture can be interpreted as allocating the total system input (here precipitation) in different fractions along different flow paths. For example, we might imagine that different fractions of the rain fall on impermeable ground, grassland, and forested portions of the catchment. Accordingly, the nodal cell-states in the first layer represent different degrees of surface \u201cwetness\u201d or \u201cmoisture storage\u201d associated with each of these fractional portions. By setting all the output weights to equal one, we simply aggregate together the outputs (flow components) generated by each flow path. Overall, this network type ensures mass conservation at both the individual nodal level and the overall network level."}, {"title": "3.5.2 Distributed-State (DS) Network", "content": "[42", "k).\n[43": "This can be interpreted as a crude way of modeling the system in a probabilistic manner, where the \u201clumped overall average\u201d (single statistic) value of surface \u201cmoisture\u201d is inadequate to describe the \u201cstate\u201d of the system and instead a \u201cdistributional description\u201d is required to better represent the dynamics of the system. This concept is analogous to that encoded by the Probability Distributed Store component (Moore, 2007) used in versions of the HyMod and other conceptual"}]}