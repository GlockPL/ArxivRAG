{"title": "Codenames as a Benchmark for Large Language Models", "authors": ["Matthew Stephenson", "Matthew Sidji", "Beno\u00eet Ronval"], "abstract": "In this paper, we propose the use of the popular word-based board game Codenames as a suitable benchmark for evaluating the reasoning capabilities of Large Language Models (LLMs). Codenames presents a highly interesting challenge for achieving successful AI performance, requiring both a sophisticated understanding of language, theory of mind, and epistemic reasoning capabilities. Prior attempts to develop agents for Codenames have largely relied on word embedding techniques, which have a limited vocabulary range and perform poorly when paired with differing approaches. LLMs have demonstrated enhanced reasoning and comprehension capabilities for language-based tasks, but can still suffer in lateral thinking challenges. We evaluate the capabilities of several state-of-the-art LLMs, including GPT-4o, Gemini 1.5, Claude 3.5 Sonnet, and Llama 3.1, across a variety of board setups. Our results indicate that while certain LLMs perform better than others overall, different models exhibit varying emergent behaviours during gameplay and excel at specific roles. We also evaluate the performance of different combinations of LLMs when playing cooperatively together, demonstrating that LLM agents are more generalisable to a wider range of teammates than prior techniques.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, Large Language Models (LLMs) have seen rapid advancement, adoption and experimentation across a wide range of research fields [1]. Games are no exception to this, with many researchers attempting to utilise this new technology for novel game playing and content creation applications [2], [3]. The emergent reasoning capabilities of LLMs, as demonstrated across various cognitive and symbolic tasks, has led to further investigations into the abilities of LLMs to not only enhance the in-game experience, but also to play games autonomously [4], [5], [6], [7], [8]. However, the complex spatial reasoning and strategic planning aspects of most traditional board and video games are known to be particularly challenging for LLMs [9].\nIn contrast to pure strategy games such as Chess or Go that have been used as AI benchmarks [10], [11], the language centred nature of LLMs make them much more suited to games that utilise natural language as part of the core gameplay. Many modern multi-player games permit natural language conversations between players as a means to discuss strategies or form alliances, but this aspect is often not considered by traditional AI agents [12]. LLMs offer a new approach to developing agents for language-based games, as well as games where players can benefit from being able to communicate with each other.\nTo explore the potential of LLMs for language-based games, we have selected the board game Codenames as our benchmark [13]. Codenames is a popular team-based party game that requires cooperation, natural language understanding, and epistemic reasoning abilities to play effectively. Players work in asymmetric two-person teams, where one player (codemaster) provides a single word clue that attempts to link a variety of other words together. The second player (guesser) must then select from a board of several possible words, those which they feel are most associated with the provided clue. The full rules for playing Codenames are described in Section II, for any readers who are unfamiliar with this game.\nA simplified version of Codenames was previously utilised for a short-lived AI competition in 2019, that focused on developing more traditional natural language processing techniques (such as semantic word associations) [14]. While these approaches performed well when playing with another agent using the same word association strategy, they perform significantly worse when paired with a teammate utilising an alternative technique [15]. LLMs may provide a solution to this problem, demonstrating emergent natural-language and theory of mind reasoning capabilities across a wide range of prior domains [16].\nIn this paper, we present an updated version of the Codenames AI framework that replicates the full rules of the original board game. We then benchmark the performance of several state-of-the-art LLMs, as well as the more traditional word-vector approaches, to play Codenames alongside a variety of teammates and opponents. This evaluation explores whether LLMs are inherently able to understand the rules of the game provided to them, along with their ability to provide meaningful and generalisable clues that work for a range of potential teammates. Our results demonstrate that, while current LLMs are not able to outperform more traditional NLP agents when cooperating with an identical technique, their performance is significantly less hindered when paired with other agents. It is also apparent that each of the LLMs tested have a different emergent playstyle, with some playing more cautiously or risky than others, leading to interesting result combinations when paired together. We also provide preliminary results for OpenAI's recent ol-preview model, which demonstrated a significant performance improvement over other LLMs."}, {"title": "II. CODENAMES AI FRAMEWORK", "content": "Before discussing related work, we will first explain our updated Codenames AI framework. One of the contributions of this research is that the previous Codenames AI framework has been extended to replicate the full game rules, along with support for LLM controlled agents. This section describes the rules of Codenames for readers who are unfamiliar with it, along with the necessary changes made to the Codenames AI framework to support the complete game ruleset. The updated framework code is available, alongside our presented results, at the following repository.\n\n\nA. Codenames Rules\n\nThe following four sub-sections describe the complete rules for Codenames. The format and wording used for these rule descriptions matches the input prompt provided to all LLM agents within our updated Codenames Al framework. For additional clarification, Figure 1 shows an example board setup from the codemaster's perspective. The guesser is presented the same board but with the identities (i.e., colours) of the words hidden.\n\n\nOverview: Codenames is a word-based game of language understanding and communication. Players are split into two teams (red and blue), with each team consisting of a Codemaster and Guesser.\n\n\nSetup: At the start of the game, the board consists of 25 English words. The Codemaster on each team has access to a hidden map that tells them the identity of all of the words (Red, Blue, Civilian or Assassin). The Guessers on each team do not have access to this map, and so do not know the identity of any words. Players need to work as a team to select all their words in as few turns as possible, while minimising the number of incorrect guesses.\n\n\nTurns: At the start of each team's turn, the Codemaster supplies a clue and a number (the number of words related to that clue). The clue must:\n\n\nBe semantically related to the words the Codemaster\nwants their Guesser to guess.\n\n\nBe a single English word.\n\n\nNot derive, or be derived from, one of the words on the\nboard.\n\nThe clue number must be greater than or equal to zero. The Guesser then selects from the remaining words on he board, based on which word is most associated with the Codemaster's clue. The identity of the selected word is then revealed to all players. If the Guesser selected a word that is their team's colour, then they may get to select another word. The Guesser must always make at least one guess each turn, and can guess up to one word more than the number provided in the Codemaster's clue. The only exception to this is if the Codemaster's clue number is zero, then there is no limit on the maximum number of guesses. If a Guesser selects a word that is not their team's colour, their turn ends. The Guesser can choose to stop selecting words (ending their turn) any time after the first guess.\n\n\nEnding: Play proceeds, passing back and forth, until one of three outcomes is achieved:\n\n\nAll of the words of your team's colour have been selected\n(your team wins).\n\n\nAll of the words of the other team's colour have been\nselected (your team loses).\n\n\nYour team selects the assassin word (your team loses).\n\n\n\nB. Differences from Previous Framework\n\nThe previous Codenames AI framework provided a simplified version of the above rules, which differs from the full game in two key aspects:\nFirstly, the previous framework provided a single team cooperative version of the game, where only the red team gives clues and makes guesses. The red team is then scored at the end of the game based on the number of turns needed to select all red words (i.e., a lower score is better) with a loss resulting in a maximum score of 25 points. This rule simplification makes it easier to evaluate a single team of agents without needing to worry about the opposing team, but also removes some of the game's deductive and strategic reasoning elements. Blue and civilian words function almost identically in this single team version, with the only difference being that the red team loses if they somehow select all blue words (which essentially never happens). This lowers the potential impact that incorrect word selections have on a team's chance of winning, and reduces the game's overall strategic depth. Team's being given the maximum score of 25 points if they lose also means that selecting the assassin word can have a huge impact on a team's average overall performance, and thus incentives slow and cautious strategies.\nOur revised framework provides the option for both the single team (previous framework) and two teams (full rules) versions of Codenames.\nSecondly, the previous framework did not permit the guesser to deviate from the number of guesses specified in the codemaster's clue. The original Codenames rules state that, assuming that an incorrect word is not selected, the guesser can select any number of words between one and one more than the number of guesses specified by the codemaster (e.g., if the codemaster's clue specifies the number three, then the guesser can select a minimum of one and a maximum of four words). The codemaster is also able to provide a clue number of zero, which allows the guesser to make as many guesses as they like (although this almost never happens in regular play). Our updated framework allows the guesser to stop guessing after each word selection, up to one more than the clue number provided by the codemaster. This provides another interesting strategic choice for the guesser, allowing them to stop early if they are unsure what word to select next or stop late if they wish to make a risky extra guess.\n\n\nC. Framework Limitations\n\nDespite our best attempts to adhere to the full Codenames rules described above, there are still some remaining ambiguities that need to be addressed. Firstly, the rules of Codenames state the the provided clue cannot derive, or be derived from, one of the words on the board. What exactly counts as a derivative word is a subjective decision, and thus cannot be easily defined in our framework. In the official rulebook for Codenames, this invalid clue rule extends to include compound words or non-English words, using the clue number as an additional hint, and providing clues that do not relate to the meaning of words. These requirements are hard to define objectively, and have thus been excluded from our framework. The only restriction we enforce is that clues cannot contain or be contained within any of the words still available on the board (i.e., no substrings allowed). This does mean that agents could effectively \u201ccheat\" by subverting this rule (such as deliberately misspelling words) although this is unlikely to occur without deliberate human influence.\nAnother rule from the original Codenames game relates to the penalty for invalid clues. In the official rulebook, if the codemaster gives an invalid clue their turn ends immediately and the other team's codemaster gets to identify one of their team's words for free. However, the imperfect nature of LLMs means that they have an increased risk of providing a clue in an invalid format (such as providing additional text in their response). We therefore decided to relax this rule, and instead simply ask the codemaster to try again if their original response was invalid. However, If the codemaster fails to provide a valid clue 10 times in a row, then a default empty string is chosen as the clue with the clue number being set to 1. Likewise, if the guesser fails to guess a valid word on the board 10 times in a row, then one of the remaining words is chosen at random. During our experiments, no codemaster ever failed to give a valid clue within this defined limit. However, the guesser agent would very occasionally fall into a repeated loop of providing a response that wasn't on the board and would eventually have a word selected at random (although this happened only a handful of times across all experiments).\""}, {"title": "III. RELATED WORK", "content": "In this section, we discuss alternative benchmarks that have previously been used to evaluate the performance of LLMs, and why we feel that Codenames is a novel and suitable benchmark alternative. We also present some of the previous approaches to developing AI approaches for Codenames.\n\n\nA. Benchmarks for LLMs\n\nWith the ever growing number of LLMs being released every month, being able to empirically evaluate their performance has become an increasingly critical challenge. To understand how good these models are across different domains and applications, researchers often rely on multiple benchmarks that focus on different LLM capabilities or tasks.\nOne of the most popular benchmarks for evaluating the general knowledge and language understanding of LLMs is the Massive Multitask Language Understanding (MMLU) test [17]. This benchmark contains numerous multiple-choice questions grouped into 57 individual topics, that LLMs are required to interpret and answer correctly. Other benchmarks such as HellaSwag [18] or BIG-Bench Hard (BBH) [19] also focus on language comprehension, but additionally assess general and common-sense reasoning abilities [20]. For HellaSwag, given the start of the sentence, the LLM must select the logical completion among different available choices. BBH considers 23 task groups, each consisting of several examples, that have been identified as especially challenging LLMs.\nAs a subcategory of reasoning, LLMs can also be evaluated on their strategic capabilities [21]. This ranges from their playing skill for different types of games (conversational, board, card, or electronic games) to societal and economic simulations. One of the key aspects for this type of evaluation is the presence of at least one other agent that can influence the environment, which thus affects the decisions made by the LLM. Beyond evaluating general reasoning or text generation, benchmarks have also been proposed to assess the capabilities of LLMs for specific tasks or scenarios. TyDi QA [22] is an- other benchmark consisting of questions and expected answers but presented in a variety of world languages, intended to evaluate the multilingual abilities of LLMs. Another example of a precise domain benchmark is the MATH test set [23], which (as the name would imply) assesses LLM performance on different mathematical problems.\nThis section has covered only a handful of the many LLM benchmarks that currently exist, however it is apparent that few of them evaluate more abstract language understanding and reasoning capabilities of LLMs outside of knowledge assessment. We propose that the use of Codenames as an LLM benchmark allows for the simultaneous assessment of language understanding, strategic reasoning, and theory of mind capabilities."}, {"title": "B. Reasoning Skills in Codenames", "content": "In this section, we highlight some of the the important rea- soning skills required to play Codenames effectively, and why this makes it a novel and effective benchmark for evaluating LLMs.\nThe first type of reasoning is that of language or word under- standing. We further distinguish this into inductive reasoning for the codemaster and deductive reasoning for the guesser. We designate the generation of a clue by the codemaster as inductive reasoning, as it requires forming a general concept (the clue) from specific provided examples (the words on the board). This can also be seen as an optimisation problem, where the codemaster has to find a clue related to the largest number of their team's words, while also avoiding associations with any other non-team words. In contrast, the guesser has to demonstrate deductive reasoning to identify specific words on the board based on their association with the provided clue. Demonstrating robust inductive and deductive reasoning remains a challenging task for LLMs, although recent works involving prompt engineering are encouraging and show the need for benchmarks that evaluate this capability [24], [25], [26].\nThe second type of reasoning present in Codenames is strategic reasoning. The codemaster may decide to give a more risky or cautious clue to the guesser, commonly exemplified as the number associated with the clue word. A higher clue number would allow the guesser to select more words, but can also increase the risk of them selecting one of the words for the other team (or even the assassin word). The guesser may also make strategic decisions, by deciding whether to stop guessing early when unsure of the next word, stopping at the number specified by the codemaster, or making an extra guess. The decision whether to play risky or cautious should ideally depend on the current state of the game, as a team that is further ahead than the opposing team can afford to play safer, while a team that is behind or close to losing make choose to go for a \u201cHail Mary\u201d final attempt. We should expect intelligent players to take all these factors into account when making their decisions and adapt to changes in relative team performance throughout the game. Recent research has indicated that LLMs may possess the capability for generalisable strategic reasoning in simple test cases, indicating their potential suitability for Codenames [27].\nLastly, Codenames also requires cooperative and epistemic reasoning. Both the codemaster and the guesser have to play with the each other in order to win, despite not being able to communicate outside of their defined actions. These Partial Information, Restricted Communication, Cooperative (PIRCC) games have recently garnered interest in the AI literature due to the complexity of creating capable AI players, and humans' current superior performance to many AI [28]. To perform in these settings effectively, each agent needs to internalise what their teammate, and to a lesser extent the opposing team, may or may not think based on their actions (i.e., what is their current mental model). To perform better at the game, this reasoning needs to be applied when inducing suitable clues (codemaster) and deducing words on the board (guesser). This reasoning also allows players to adapt their strategy and playstyle based on the performance and actions of their teammate. For example, a player may give a clue about medieval history because their teammate is knowledgeable on the subject, or a highly skilled codemaster may need to reduce their clue complexity when paired with a novice guesser. LLMs may be able to use this type of reasoning to play Codenames better than other more traditional agents, having previously demonstrated some theory of mind capabilities [16], [29], although there remains debate over whether this reflects true possession of a theory of mind [24], [25].\nBased on the multiple interconnected reasoning capabilities involved in playing Codenames, we believe that this game provides a complex and nuanced task that assesses multiple facets of cognitive intelligence."}, {"title": "C. Codenames AI", "content": "There is growing interest in the game Codenames from AI research due to its demand for multi-modal language understanding, asymmetric cooperation, theory of mind, and epistemic reasoning [15]. Introduced by Kim et al. (2019) the first Codenames AI competition employed word embedding techniques such as word2vec and GloVe models. These models achieved 100% accuracy when paired with themselves as teammates, but saw a drop in performance when working with teammates using different models. As an extension of this work Jaramillo et al. (2020) utilised term frequency - inverse document frequency (TF-IDF), Naive-Bayes, and the GPT-2 Transformer models. They report that the transformer model achieved the same or better accuracy compared to agents developed by Kim et al. (2019). When tested with human participants, the transformer model was preferred over other agents [30].\nKoyyalagunta et al. (2021) developed multiple methods to improve Codenames AI performance when paired with humans. Their aims was to create agents that produce more human-interpretable clues, as previous agents would often give clues that were nonsensical to human guessers but would be correctly guessed when paired with another word embedding agent [31]. Other researchers have focused on agents with the ability to adapt in real time to their teammates. Archibald et al. (2024) created the Adaptive Codenames Ensemble (ACE) which changes the Codenames agent it produces clues with based on the teammates guesses [32]. Archibald et al. (2024) also proposed a Noisy Communication Model (NCM) which deliberately adds \"noise\" to a clue in order to increase it's generalisability to unknown teammates [33].\nPrompting techniques for LLMs have also been used to improve Codenames AI performance. Ozturkler et al. (2023) use ThinkSum, a prompting technique used to promote de- ductive reasoning, to improve Codenames guesser agents. They showed a 20% improvement in score compared to few-shot prompting for guesser agents [34]. Most recently, Sidji et al. (2024) compared the performance of various prompt engineering approaches on Codenames using OpenAI's GPT-4-1106 as the base LLM. This included prompting techniques such as Chain of Thought [35], Self Refine [36], and Solo Performance [37], [26]. While these prompting techniques had a measurable impact on the agent's playstyle, typically resulting in more risky or cautious clues, none of them were able to produce a significant improvement to overall per- formance. Rather than exploring the effectiveness of prompt engineering techniques, our presented research instead aims to explore the inherent abilities of different state-of-the-art LLMs to play Codenames effectively when paired with or against other alternative models."}, {"title": "IV. EXPERIMENTS", "content": "This section describes the selected AI agents, Codenames game versions, and evaluation process carried out using our updated Codenames AI framework. The primary purpose of these experiments was to determine the current performance of state-of-the-art LLMs for playing Codenames, compared to traditional word-vector approaches.\n\n\nA. Game Playing Agents\n\n\nLLM Agents: The following LLM families and versions\nwere initially selected for evaluation:\n\n\nGPT (OpenAI)\n\n\n01-preview (2024-09-12)\n\n\n01-mini (2024-09-12)\n\n\nGPT-4o (2024-08-06)\n\n\nGPT-3.5-turbo (0125)\n\n\nGemini (Google Deepmind)\n\n\nGemini-1.5 (Pro 002)\n\n\nClaude (Anthropic)\n\n\nSonnet-3.5 (2024-10-22)\n\n\nHaiku-3.5 (2024-10-22)\n\n\nLlama (Meta)\n\n\nLlama-3.1 (70B-Instruct)\n\n\nLlama 3.2 (8B-Instruct)\n\n\nPhi (Microsoft)\n\n\nPhi-3-medium (128k-Instruct)\n\n\nMistral AI (Mistral)\n\n\nMistral-0.3 (7B-Instruct)\n\n\nMixtral-0.1 (8x7B-Instruct)\n\nHowever, it became apparent during preliminary testing that many of the smaller models were not able to correctly adhere to the game's specified rules and output format, resulting in repeated invalid clues or guesses that frequently devolved into pure random play. As such, only the six LLMs highlighted in bold (o1-preview, o1-mini, GPT-4o, Gemini-1.5, Sonnet-3.5 and Llama-3.1) were able to consistently follow the game's rules. Scores for the other LLMs were significantly worse because of this, and we thus chose to only present our findings and analysis for these six higher performing LLMs.\n\n\nWord-Vector Agents: In addition to the LLM agents mentioned above, we also evaluated three word-vector agents supplied with the previous Codenames AI framework:\n\n\nWord2Vec (threshold = 0.7).\n\n\nGloVe (300d)\n\n\nCombined (300d, threshold = 0.7)\nThese agents utilise word embedding approaches based on learned vector semantics from a provided training corpus [15]. Word2Vec is based on a 300 dimensional pre-trained skip- Gram model trained on the Google News corpus [38]. GloVe (Global Vectors for Word Representation) is an alternative approach that additionally considers the co-occurrence of words within a defined context [39]. Combined utilises both approaches by concatenating the Word2Vec and GloVe vectors together [40].\nOne crucial limitation of these word-vector agents is that, due to the fact that their approaches utilise a pre-defined corpus of words for determining suitable clues / guesses, they are unable to interpret any words which are not present in their training set. Preliminary testing found that these agents will often produce errors when paired with LLM teammates, which have a much wider ranging vocabulary from which to select clues. Because of this, we were unable to produce reliable performance results for games with both LLM and Word- Vector agents, and have instead chosen to evaluate each agent group separately.\n\n\n\nB. Game Versions\n\nTwo different versions of Codenames were utilised for our agent evaluations, based on the previous and updated versions of the Codenames AI framework.\n\n\nSingle Team (cooperative): Played using the same scor- ing system as the previous Codenames AI framework, where a single team (red codemaster/guesser) attempts to identify all red words in as few turns as possible. Teams are awarded a score at the end of the game based on the number of turns taken (lower score is better). The only exception to this is if the guesser selects all blue words or the assassin word, which results in a maximum score of 25 points.\n\n\nTwo Teams (competitive / cooperative): Played using the full set of rules from the original Codenames game, where two teams (red codemaster/guesser and blue codemaster/guesser) attempt to identify all words of their team's colour first. Selecting the assassin word results in an immediate win for the other team. Guessers can also inadvertently help the opposing team win if they accidentally select any words of their colour. Rather than using a scoring system, this version measures success in terms of overall win-rate.\n\n\n\nC. Evaluation Procedure\n\nBased on the agent and game versions described above, the following evaluation experiments were conducted:\n\n\nSingle Team: This experiment compared the perfor- mance of different LLM and word-vector agent combinations, acting as both the codemaster and guesser, for the single team version of Codenames. Due to the previously mentioned issues with word-vector agents failing to understand clues provided by LLM agents, these two groups of agents were evaluated separately. In addition, due to the high usage costs currently required to run OpenAI's recent ol models, the ol-preview and o1-mini LLMs were only evaluated when paired with themselves.\n\n\nTwo Teams: This experiment evaluated how different teams of the same agent (i.e., identical models for the codemaster and guesser) performed on the new two teams version of Codenames (i.e., the full Codenames rules). While it would have been beneficial to compare how teams of different agents also performed, this would have led to an exponential increase in the number of agent combinations and associated costs. Similar to the single team version, LLM and word-vector agent groups were evaluated separately. We also chose to exclude the o1 models from this experiment due to the aforementioned high usage costs.\nEach of the agent and game version combinations mentioned above were evaluated over 100 trials (using random seeds 0-99 inclusive). All code utilised for this experiment, along with full quantitative results, input prompts and output responses, is available in the provided public code repository. All experiments were conducted on an AMD Ryzen Thread-ripper PRO 5955WX, with an RTX A6000 and 256GB of RAM (although the specific hardware utilised should not have an impact on the produced results). Due to the unpredictable nature of the LLMs used, exact replication of our results is likely not possible even if the same parameters are applied. However, we have found that the general trends in our results are consistent over multiple experiment runs.\nRegarding the costs associated with replicating these experiments, while the Llama models are open-source, the GPT, Gemini and Claude models are currently only available using a paid service offered by each provider. As of the time of writing, the costs incurred as a result of these experiments total approximately $514 USD for GPT (OpenAI), $141 USD for Gemini (Google Deepmind), and $177 USD for Claude (Anthropic). Please note that this does not include the additional subscription fee required to access the OpenAI API. The bulk of these costs was attributed to the new ol-preview model provided by OpenAI, which cost $288 USD alone for running 100 trials of the single team game version. While costs for using these models are expected to reduce over time, this can still be a prohibitively expensive requirement for any students or self-funded researchers wishing to experiment with these same models."}, {"title": "V. RESULTS", "content": "This section provides results for the agent evaluation experiments described above.\n\n\nA. Single Team Version\n\nSummative results for experiments utilising the single team version of Codenames are shown in Table I. Definitions for each column are provided as follows:\n\n\nModel Pair: Defines the two models being applied, with\nthe former specifying the codemaster and the latter spec-\nifying the guesser (i.e., codemaster - guesser). Results for\neach row were calculated from 100 individual trials.\n\n\nMean: Mean score. Note, a team's score is equal to the\nnumber of turns needed to identify all red words (i.e., a\nlower value is better), with the exception that selecting\nthe assassin word results in a score of 25 points.\n\n\nMedian: Median score.\n\n\nMin: Minimum score.\n\n\nStd Dev: Standard deviation in the score.\n\n\nLoss: Percentage of games that ended in a loss (i.e.,\nselecting the assassin word).\n\n\nBlue avg: Average number of blue words selected in each\ngame (standard deviation provided in brackets).\n\n\nCivilian avg: Average number of civilian words selected\nin each game (standard deviation provided in brackets).\n\n\nClues avg: Average clue number given by the codemaster\neach turn (standard deviation provided in brackets).\n\n\nGuesses avg: Average number of guesses made by the\nguesser each turn (standard deviation provided in brack-\nets).\n\n\nStop Early: Percentage of turns where the guesser chose\nto voluntarily stop guessing before reaching the clue\nnumber provided by the codemaster.\n\n\nStop Late: Percentage of turns where the guesser chose\nto guess one word more than the clue number provided\nby the codemaster.\n\n\n\nB. Two Teams Version\n\nSummative results for experiments utilising the two teams version of Codenames are shown in Table II. Definitions for each column are provided as follows:\n\n\nModel Pair: Defines the two models being applied for\neach team (both codemaster and guesser), with the former\nspecifying the red team's models and the latter specifying\nthe blue team's models (i.e., red vs. blue). Results for\neach row were calculated from 100 individual trials.\n\n\nWin-rate: Percentage of games won by the red and blue\nteams respectively.\n\n\nAssassin losses: Percentage of games lost due to selecting\nthe assassin word, for the red and blue teams (respec-\ntively)."}, {"title": "VI. DISCUSSION", "content": "A. Quantitative Results\n\n\nSingle Team Version: Looking first at the performance\nof the LLMs for the single team version of Codenames", "Version": "Looking at the result for the two teams version of Codenames, see Table II, we can see some interesting differences in the performance of certain LLMs compared to the single team version. Firstly, in would appear that riskier codemasters (e.g., sonnet-3.5) perform much better, likely due to the reduced impact that selecting an assassin word has on a team's average win-rate. Looking at the overall win-rates for"}]}