{"title": "GBFRS: Robust Fuzzy Rough Sets via Granular-ball Computing", "authors": ["Shuyin Xia", "Xiaoyu Lian", "Binbin Sang", "Guoyin Wang", "Xinbo Gao"], "abstract": "Abstract-Fuzzy rough set theory is effective for process-ing datasets with complex attributes, supported by a solidmathematical foundation and closely linked to kernel meth-ods in machine learning. Attribute reduction algorithms andclassifiers based on fuzzy rough set theory exhibit promisingperformance in the analysis of high-dimensional multivariatecomplex data. However, most existing models operate at thefinest granularity, rendering them inefficient and sensitive tonoise, especially for high-dimensional big data. Thus, enhancingthe robustness of fuzzy rough set models is crucial for effectivefeature selection. Muiti-garanularty granular-ball computing, arecent development, uses granular-balls of different sizes toadaptively represent and cover the sample space, performinglearning based on these granular-balls. This paper proposesintegrating multi-granularity granular-ball computing into fuzzyrough set theory, using granular-balls to replace sample points.The coarse-grained characteristics of granular-balls make themodel more robust. Additionally, we propose a new methodfor generating granular-balls, scalable to the entire supervisedmethod based on granular-ball computing. A forward searchalgorithm is used to select feature sequences by defining thecorrelation between features and categories through dependencefunctions. Experiments demonstrate the proposed model's ef-fectiveness and superiority over baseline methods. The sourcecodes and datasets are both available on the public link:https://github.com/lianxiaoyu724/GBFRS", "sections": [{"title": "I. INTRODUCTION", "content": "LASSIC rough sets can only handle symbolic data sets,requiring discretization of continuous and numerical at-tributes before data reduction, which may lead to the loss ofclassification information [8], [20]. Fuzzy rough sets (FRS),proposed by Dubois and Prade [14], provide an effectivemethod to overcome data discretization issues and can handlecontinuous or numerical data sets without preprocessing. FRSis an effective tool for measuring sample uncertainty [15],characterizing uncertain data, and selecting informative fea-tures for downstream learning tasks. FRS has been applied todimensionality reduction [12], [30], [44], classification [10],[20], regression analysis [4], etc. In recent years, scholarshave proposed various FRS-based feature selection models [6],[16], [19], [22], [24], [41], using fuzzy dependence functionsto characterize the discriminative ability of feature subsets.However, this approach can lead to unstable classificationperformance on noisy data sets.\nNoise samples often appear in the data, so this is one of thecauses of data uncertainty. Generally speaking, there are twotypes of noise samples in data [10]. One is that the conditionalattributes of the sample are abnormal (i.e., attribute noise)[26], and the other is that the decision-making attributes ofthe sample are abnormal (i.e., quasi-noise) [37]. Since fuzzyrough sets are very sensitive to noise in uncertainty data, theevaluation of uncertainty is not accurate. The quality of datacan be improved by improving the anti-noise performance offuzzy rough set models [3], [5]. To improve the robustness tonoise, Hu et al. [17] discussed the properties of the model andconstructed a new dependency function from the model, usingthis function to evaluate and select features, and developed anew fuzzy rough set model called soft fuzzy rough set. Sincefuzzy rough sets are very sensitive to noise in uncertainty data,the evaluation of uncertainty is not accurate. An et al. [1]proposed a probabilistic fuzzy rough model by consideringthe distribution information of the data. On this basis, fuzzyrough sets based on probabilistic particle distance are proposedto reduce the impact of noisy data, but the calculation amountis large and the relationship between sample labels within theneighborhood of the sample [2] is ignored. Cornelis et al. [11]proposed a mitigation method, which is to determine the mem-bership degree of the upper and lower approximations throughan ordered weighted average operator. Wang et al. [32] furtherstudied the equivalent expression of granular variable precisionfuzzy rough sets based on fuzzy (co) meaning. It promotes thefurther development of fuzzy rough theory from a practicalperspective. In addition, many studies consider dividing thesample set into different regions to improve the efficiency androbustness of feature selection. Hu et al. [18] introduced aneighborhood rough set model to divide the sample set into adecision positive area and a decision boundary area and proposed a forward greedy strategy for searching feature subsets.This strategy minimizes the neighborhood decision error rateand accordingly minimizes the classification complexity of theselected feature subset. Wang et al. [31] used the concept ofthe fuzzy neighborhood to define fuzzy decision-making ofsamples, and introduced parameterized fuzzy relationships tocharacterize fuzzy information particles. A novel rough setmodel for feature subset selection is constructed using therelationship between fuzzy neighborhoods and fuzzy decision-making. And Wang et al. [29] used the concept of fuzzyneighborhood to define fuzzy decision-making of samples, and"}, {"title": "II. RELATED WORK", "content": "In this section, we review some basic concepts of classicfuzzy rough sets [7], [23] and related content of granular-ballcomputing.\nA. Basics of fuzzy rough set\nFuzzy rough sets can handle complex uncertainties, wheretho objects may have similar or identical condition attributedescriptions but belong to different decision-making classes.Assume U is a domain of discourse, mapping A (): U\u2192[0, 1], then A is called a fuzzy set on U.\nDefinition 1. Given $U = {X_1, X_2, ..., X_n}$ is a set of samplesand A is a set of real-valued attributes. Any $a \\in A$ caninduce a fuzzy binary relation $R_a$ on U. If it satisfies, we saythat $R_a$ is a fuzzy similarity relation\n(1) Reflexivity: $R_a(x,x) = 1, \\forall x \\in U$;\n(2) Symmetry: $R_a(x,y) = R_a(y, x), \\forall x, y \\in U$."}, {"title": "III. GRANULAR-BALL FUZZY ROUGH SET", "content": "A. Motivation\nMost of the existing fuzzy rough set methods are aimedat sample points and need to calculate similarity matricesand upper and lower approximations between all objects,involving a large number of distance calculations and matrixoperations, especially on large-scale data sets, which willlead to computational complexity and time costs increasesignificantly. Fuzzy rough sets are also sensitive to noise pointsin the data, affecting the accuracy of lower approximationsand, consequently, attribute importance assessment.\nAs an example of the lower approximation, Fig. 5 showsthe effect of noise points in the fuzzy rough set on the lowerapproximation. For a point x, if we want to determine thedegree to which it belongs to a certain class $D_i$, it depends on the nearest heterogeneous boundary to it. In the figure, $x$is located within class $D_1$, but there is a noise point y nearit. Since point y is very close to x, it may be mistaken asthe nearest heterogeneous boundary of x, thus affecting thelower approximation of x. Similarly, point z actually belongsto class $D_2$, but its location is close to class $D_1$, so it islikely to be regarded as a noise point. Although point z hasan upper approximation of 1 (i.e. very much belongs to class$D_2$) because its nearest point is itself, it may actually belong toclass $D_1$. The upper and lower approximations of fuzzy roughsets are values, while the upper and lower approximations ofrough sets are set. x represents an arbitrary point, and y is"}, {"title": "B. Granular-ball Fuzzy Rough Set", "content": "GBFRS introduces granular-ball computing into fuzzyrough sets, so it must follow the granular-ball calculationmodel (2). Each granular-ball consists of two parameters:center and radius, which are defined as:\nDefinition 3. A granular-ball GB = {$x_i$, i = 1, ..., N}, where$x_i$ denotes the samples in GB and N is the number of samplesin GB. GB's center c and radius r get values by:\n$C = \\frac{1}{N} \\sum_{i=1}^{N} x_i$; $r= \\frac{1}{N} \\sum_{i=1}^{N} ||x_i - c||$;\nwhere $||x_i - c||$ denotes the Euclidean distance from $x_i$ to $c$.\nThe quality of a granular-ball is assessed based on its\"purity\", defined as the proportion of samples belonging to thedominant category within the ball. For instance, if a granular-ball contains 100 samples categorized into two classes, with80 samples labeled as '+1' and 20 samples labeled as '-1', thepurity is determined by the ratio of the dominant category'ssamples to the total samples within the ball. In this example,the label '+1' represents the majority with 80 samples out of100, resulting in a purity of 0.8, thus assigning the label '+1'to the granular-ball. Granular-balls replace sample points todescribe the entire data set, and it is necessary to ensure thatthe quality of each ball reaches the requirements.\nThe structure of the data set for classification learning isrepresented as \u3008U, A, D\u3009, where U is a non-empty set ofsamples, and A is a set of real-valued attributes. An attributea \u2208 A is a mapping defined on U, that is, a : U \u2192 $R^+$\u222a{0}.R is the field of real numbers. D is the decision attribute.Suppose there is a data set U = {$x_i$, i = 0,1,..., n}, andconvert all sample points into a set of granular-balls as G ={$GB_i$, i = 0,1,..., m}, then the sample space is transformedinto U' = G. The fuzzy similarity relationship based ongranular-balls is as follows:"}, {"title": "D. Algorithm Design", "content": "This section discusses the algorithm design of GBFRS forfeature selection based on weighted granular-ball dependency.The algorithm is divided into two parts.\nThe first part is for granular-balls generation. Given adata set D = {$X_1,X_2, ..., X_n$} with attribute set B, and apurity threshold given as T. First, the data set is divided"}]}