{"title": "Causal World Representation in the GPT Model", "authors": ["Raanan Y. Rohekar", "Yaniv Gurwicz", "Sungduk Yu", "Vasudev Lal"], "abstract": "Are generative pre-trained transformer (GPT) models only trained to predict the next token, or do they implicitly learn a world model from which a sequence is generated one token at a time? We examine this question by deriving a causal interpretation of the attention mechanism in GPT, and suggesting a causal world model that arises from this interpretation. Furthermore, we propose that GPT-models, at inference time, can be utilized for zero-shot causal structure learning for in-distribution sequences. Empirical evaluation is conducted in a controlled synthetic environment using the setup and rules of the Othello board game. A GPT, pre-trained on real-world games played with the intention of winning, is tested on synthetic data that only adheres to the game rules. We find that the GPT model tends to generate next moves that adhere to the game rules for sequences for which the attention mechanism encodes a causal structure with high confidence. In general, in cases for which the GPT model generates moves that do not adhere to the game rules, it also fails to capture any causal structure.", "sections": [{"title": "1 Introduction", "content": "In recent years, the generative pre-trained transformer (GPT) model [Radford et al., 2018] has demonstrated high-quality generative capabilities, as perceived by humans. Although this model is trained to generate one token at a time, it have been demonstrated to perform a range of tasks beyond next-token prediction, such as visual understanding and even symbolic reasoning [Liu et al., 2024, Team et al., 2023, Chowdhery et al., 2023]. Are these emergent abilities [Li et al., 2023] or merely a 'mirage' resulting from the choice of metric and task [Schaeffer et al., 2024]?\nIn this paper we suggest that there is no restriction in the GPT architecture for learning conditional independence (CI) relations between tokens in a sequence. Moreover, under certain assumptions, a causal structure is directly entailed from these CI relations. One may ask whether this absence of restriction results in implicitly learning a causal model of the world during the pre-training procedure of GPT. Assuming that both a causal world model and a model based on surface statistics are sufficient solutions, one possibility is that a causal world model is a more compact representation, making it more likely to be learned during pre-training (Occam's razor). While surface statistics models may distribute weights uniformly across a wide range, a causal structure imposes constraints that narrow the range of possible weight. If so, what are the assumptions underlying this causal world model?\nRecently, Rohekar et al. [2024] derived a causal interpretation for unmasked self-attention in BERT models [Devlin et al., 2019]. In this paper we follow a similar approach, with several differences, and propose a causal interpretation of the masked attention mechanism in GPT. We additionally define a corresponding causal world model. The ABCD method Rohekar et al. [2024] is adapted and used to learn causal structures of which the induced dependency-relations are encoded in the attention matrices in GPT. We then ask whether errors generated by GPT are correlated with the uncertainty in representing the causal structure by the attention matrices. To this end, we define a metric based on the entropy of p-values of CI tests that are used for inferring the causal structures."}, {"title": "2 Preliminaries", "content": "In this section we provide notations and descriptions of self attention in the GPT architecture, and structural causal models. Matrices are written in bold, vectors in bold-italic, and models in calligraphic font. A summary of the main symbols that are used in this paper is given in Table 1."}, {"title": "2.1 Attention in GPT", "content": "Attention is a mechanism that estimates network weights with respect to the context in an input sequence of tokens [Schmidhuber, 1992]. In a GPT model, which is based on the decoder part of the Transformer architecture [Vaswani et al., 2017], an attention layer estimates an $n \\times n$ lower-triangular (masked) attention matrix A given an input sequence of $n$ tokens. The input sequence is in the form of an $n x d$ matrix Y, where the i-th row vector is $Y(i, \\cdot)$, is an embedding (representation) of the i-th token in $d$ dimensions. The attention matrix is estimated by $A = \\text{softmax}(YW_QW_K Y^\\text{T})$, such that A is lower triangular and the rows sum to 1. In addition to the attention weights, the attention layer calculates a values matrix, $V = YW_V$, where row $V(i, \\bullet)$ is the value vector of the $i$-th token. Then, the output embeddings are\n$Z = AV$,\nwhere the $i$-th row, $Z_i$, is the embedding of the $i$-th output token. In a GPT, several attention layers are stacked, and pre-trained such that the $i$-th output embedding in the last layer predeicts the $(i + 1)$-th input token. That is, predicts the next input token.\nIt is important to note that in the GPT architecture, the embedding of one token is influenced by another token only by the attention matrix, A. In addition, note that an attention matrix A is estimated uniquely for each input sequence of tokens, using weight matrices $\\{W_{QK}, W_V\\}$ learned commonly for all in-distribution input sequences."}, {"title": "2.2 Structural Causal Model", "content": "A structural causal model (SCM) is a model that can encode causal mechanisms in a domain [Pearl, 2009, Spirtes et al., 2000, Peters et al., 2017] and explain data samples generated from these causal mechanisms [Pearl and Mackenzie, 2018]. An SCM is a tuple $\\{\\mathcal{U}, \\mathcal{X}, \\mathcal{F}, P(\\mathcal{U})\\}$, where $\\mathcal{U} = \\{U_1,...,U_m\\}$ is a set of latent exogenous random variables, $\\mathcal{X} = \\{X_1, ..., X_n\\}$ is a set of endogenous random variables, $\\mathcal{F} = \\{f_1, ..., f_n \\}$ is a set of deterministic functions describing the values X given their direct causes, and $P(\\mathcal{U})$ is the distribution over $\\mathcal{U}$. Moreover, each endogenous variable $X_i$ has exactly one unique exogenous cause $U_i$ ($m = n$). The value of an endogenous variable $X_i, \\forall i \\in [1, . . ., n]$ is determined by\n$X_i\\leftarrow f_i(Pa_i, U_i)$"}, {"title": "3 A Causal Interpretation of GPT", "content": "We describe the masked attention in GPT as a mechanism that infers correlations between tokens of a given sequence, where these correlations are induced by a causal structure underlying the output sequence tokens. We then describe a method for learning this causal graph."}, {"title": "3.1 A Relation between GPT and SCM-based World Model", "content": "Rohekar et al. [2024] derived a causal interpretation of models [Devlin et al., 2019] (also demostrated for recommender systems [Nisimov et al., 2022]). We follow a similar approach, with several important modifications and extensions, to derive a causal interpretation to GPT. First, unlike BERT, which is pre-trained to predict the input sequence, GPT is pre-trained to predict the next tokens in the sequence. Given an input sequence of tokens, $\\{t_0, ..., t_{n-1}\\}$, GPT predicts tokens $\\{t_1,..., t_n\\}$. An attention matrix A and the corresponding values matrix V have $n$ rows corresponding to input tokens $\\{t_0,..., t_{n-1}\\}$ and the output embeddings of of these tokens are the rows of matrix $Z = AV$. Note that $V = YW_V$, where $W_V$ is a weight matrix fixed for all input sequences, and Y is input embedding of a specific sequence tokens. Each column of $W_V$ can be viewed as an independent vector onto which the input embeddings are projected. That is $V(i, j)$ is the projection of token $t_i$ input embedding $Y(i, \\cdot)$ on, common to all in-distribution sequences, vector $W_V(\\cdot, j)$. At inference, each attention matrix of the last attention layer, A, is extracted and a lower uni-triangular matrix is calculated, $D^{-1}A$, where $D = \\text{diag}(A)$. Then estimate the covariance matrix\n$C = [D^{-1}A] [D^{-1}A]^{\\text{T}}.$"}, {"title": "3.2 GPT for Zero-Shot Causal Structure Learning", "content": "The causal interpretation presented in this paper leads to a view in which each attention module represents associations (correlations) between input tokens that are induced by the underlying causal structure. Although this allows only rung-1 inference in the ladder of causation [Pearl and Mackenzie, 2018], under certain assumptions, many of the underlying causal relations can be extracted, even in the presence of latent confounders and selection bias [Spirtes et al., 2000]. These relations are generally represented in a type of causal structure called partial ancestral graph (PAG) [Richardson and Spirtes, 2002]. We follow a procedure called ABCD, proposed by Rohekar et al. [2024] with several modifications. First, since the causal (topological) order is given (restricted by the masked attention in GPT) we can apply causal discovery recursively to efficiently learn the causal structure. To this end we call the ICD algorithm [Rohekar et al., 2021] (other algorithms can be used [Spirtes et al., 2000, Colombo et al., 2012, Claassen et al., 2013, Yehezkel and Lerner, 2009, Rohekar et al., 2018, Nisimov et al., 2021]) to reconstruct a causal structure in each recursive iteration (Algorithm 1). The result is a PAG structure. Thus, a causal structure for a specific output sequence can be learned in a zero-shot manner directly from the attention matrix."}, {"title": "3.3 Causal Structure Confidence", "content": "In this section we derive a metric that describe how compatible a sequence is with the causal world model implicitly encoded by GPT. Given an output sequence of tokens, S, and a causal structure $\\mathcal{G}$ recovered from the last attention layer A, can we score the confidence in this causal structure? Recall that in the proposed world model each sequence has its own causal structure, and each causal structure may have latent variables. It is not clear how to calculate likelihood $P(S|\\mathcal{G})$.\nWe propose the following approach. A causal structure-learning algorithm performs multiple statisti-cal tests of conditional independence (CI) using the covariance matrix estimated from the attention matrix. These CI tests calculate p-values and compare them against a predetermined threshold of significance level ($\\alpha$). It is important to note that there is a one-to-one correspondence between the"}, {"title": "4 Experiments and Results", "content": "We use an experiment setup in which the world layout and rules are well defined and known but were not used during training with samples from this world (legal samples). We measure how well attention in the learned GPT model represents a causal model using Equation 9, and whether it is correlated with the ability of the model to generate tokens that adhere to the world rules (legal sequences)."}, {"title": "4.1 Setup", "content": "We examine a GPT model trained by Li et al. [2023], for predicting the next move given a sequence of consecutive moves in the Othello strategy board game. They trained the model on approximately 132,000 real-world sequences, where it is assumed the players played with the intention of winning. No information about the game board layout or game rules was used in their training process. For example, positional encoding was not used. In our experiments we use a test set that is not in-distribution with respect to the training set, but in-distribution with respect to the game rules. As a test set we use 1,000 random sequences of legal moves. That is, each sequence consists of moves that adhere to the Othello game rules but without considering any strategy of winning the game as in the training set. In other words, the support of the test distribution is not a subset of the support of the training distribution, $\\text{supp}(\\mathcal{P}_{\\text{train}}) \\nsubseteq \\text{supp}(\\mathcal{P}_{\\text{test}})$. This enables evaluating whether the model implicitly encoded the game rules. In Figure 1 we plot the accuracy of the model in generating a legal next move (vertical axis) as a function of test sequence size (horizontal axis). Although the average accuracy of the model is 95% (dashed red line), it is not uniformly distributed across different sequence lengths. For example, given a sequence of 15 moves, GPT generates a legal 16-th move in 88% of the times (adheres to the game board state and rules). It is evident that the accuracy is significantly lower for input sequence lengths in the range [10, 30] (lower than the average 95%). By definition of the Othello game rules, at the beginning of a game there are only four legal moves, and as this game unfolds, the number of possible legal moves increases before finally decreasing again as the number of vacant spaces on the board decreases. It might be that memorization of surface statistics can take place at the beginning and end of the game. We therefore report experiment results for input sequences with sizes in the range [10,30] (gray area) where the accuracy is lower than the average. For implementation and empirical evaluation we used the Causality Lab: github.com/IntelLabs/causality-lab package."}, {"title": "4.2 Legal Predictions vs. Structural Confidence", "content": "Is there a relation between the legality of predicted tokens, with respect to a set of world rules, and compatibility of attention matrices with a causal graph? Recall that the model was not trained"}, {"title": "4.3 Ablation Study", "content": "Next we examine if conditional independence tests from which the causal structure is entailed provide an advantage over pair-wise correlations directly represented by elements in the attention matrix. To this end we calculate the confidence score using p-values of a) all pair-wise correlations (from raw attention-matrix elements)\u2014empty conditioning set, b) CI-tests having exactly one node in the conditioning set, c) all CI-tests having empty or exactly one node in the conditioning set, and d) CI-tests used to reconstruct the causal structure without limiting conditioning set sizes. The results are depicted in Figure 3, with corresponding sub-figures. Vertical axis describe the difference between structural confidence averaged over legal and illegal predictions. Error bars indicate 95% confidence intervals. Horizontal axis indicate sequence length. It is evident that relying solely on raw attention values, case a), the difference between legal and illegal generated tokens in not statistically significant, except for sequence length 20. Relying solely on CI-test with exactly one node in the conditioning set, case b), the difference between the structural confidence is positive for all tested sequence lengths but statistically significant only for sequences lengths 17. When employing pair-wise correlations and CI tests with exactly one node in the conditioning tests, the result is statistically significant for both sequence lengths 17 and 20, implying that these two types of tests are complementary. Finally, it is evident that using CI-tests needed to learn the causal graph, without limiting the conditioning set sizes, case d), provide the best results where sequence lengths in [15, 22] are statistically significant and the difference between legal and illegal is positive in all sequence lengths."}, {"title": "5 Conclusions", "content": "We presented a causal interpretation of GPT that may explain apparent emergence of world model in recent studies. Following this interpretation, we described a method that utilizes the the triangular form of the attention matrices in GPT to efficiently recover the causal structures for input sequences (zero-shot causal-discovery). Finally, using experiments in a controlled environment of the Othello board game we demonstrated that GPT implicitly learns to represent causal structures in attention heads. Specifically, in cases where the confidence in recovering any structure from the attention matrices is low, GPT generally fails to predict a token that adheres to the Othello board game rules. In future work, these results may provide insights on the sources of hallucination in GPT-based models and may lead to deriving a method to detect them."}, {"title": "A Additional Preliminaries", "content": "In Table 1 we provide common symbols and their meaning used in this paper."}, {"title": "A.1 Covariance of Endogenous Nodes in SCM", "content": "The covariance matrix of endogenous variables in a linear-Gaussian SCM, as given in Equation 6, is derived as\n$C_X = E[(X - \\mu_X)(X - \\mu_X)^T] =\\newline = E[((I - G)^{-1} (U - \\mu_U)((I - G)^{-1})^T] =\\newline = E[((I - G)^{-1} (U - \\mu_U)(U - \\mu_U)^T ((I - G)^{-1})^T] =\\newline = [((I - G)^{-1})] C_U [((I - G)^{-1})^T],$ where $\\mu_X = ((I - G)^{-1}) \\mu_U$."}, {"title": "A.2 Definitions", "content": "Definition 1 (Causal Markov) In a causally Markov graph, a variable is independent of all other variables, except its effects, conditional on all its direct causes.\nDefinition 2 (Faithfulness) A distribution is faithful to a graph if and only if every independence relation true in the distribution is entailed by the graph."}, {"title": "B Recursive Causal Discovery for GPT", "content": "In Algorithm 1 we describe an efficient causal discovery algorithm that utilizes the causal order restricted by GPT by masking attention matrices, forcing them to a lower-triangular form. That is, in a sequence of tokens, $\\{t_1, ..., t_n\\}$, token $t_l$ is not an ancestor of token $t_{l-1}$ for all $l > 1$. In line 2, the last token is popped from the sequence and placed in $t_n$ resulting in a shorter sequence S'. Then, a recursive call is made in in line 3 to learn the structure over tokens in S'. Note that since it is ensured that $t_n$ is not an ancestor of any token in S' the skeleton and v-structure relations of $\\mathcal{G'}$ is ensured not to be change when adding $t_n$ to the graph [Spirtes et al., 2000]. In lines 4\u20136 token $t_n$ is connected to every node in $\\mathcal{G'}$. Finally, using the ICD algorithm [Rohekar et al., 2021] edges between $t_n$ and the rest of the graph are learned (removed if conditional independence is found) and the graph is oriented [Zhang, 2008]."}]}