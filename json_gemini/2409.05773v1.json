{"title": "Creativity and Visual Communication from Machine to Musician: Sharing a Score through a Robotic Camera", "authors": ["Ross Greer", "Laura Fleig", "Shlomo Dubnoy"], "abstract": "This paper explores the integration of visual communication and musical interaction by implementing a robotic camera within a \"Guided Harmony\" musical game. We aim to examine co-creative behaviors between human musicians and robotic systems. Our research explores existing methodologies like improvisational game pieces and extends these concepts to include robotic participation using a PTZ camera. The robotic system interprets and responds to nonverbal cues from musicians, creating a collaborative and adaptive musical experience. This initial case study underscores the importance of intuitive visual communication channels. We also propose future research directions, including parameters for refining the visual cue toolkit and data collection methods to understand human-machine co-creativity further. Our findings contribute to the broader understanding of machine intelligence in augmenting human creativity, particularly in musical settings.", "sections": [{"title": "Introduction", "content": "Group music-making relies on a rich exchange of information between musicians, requiring multiple channels for sending and receiving messages, and agreed-upon conventions for which messages (or, more often, whose messages) will be given priority and attention to form a cohesive musical output. These messages come in a variety of forms: conversations before a rehearsal, eye contact and head nods during performance, collective breathing and motion at the onset of a phrase, adjustments to pitch to match what is heard from neighbors, and more. This communication, verbal or otherwise, contains two components: (1) an encoder who shares information with a predetermined code, and (2) a decoder who observes and responds. Successful communication, then, means that the encoder's intention is known to both the encoder and the decoder. The extent to which the sender's intent matches the receiver's perceived intent can be defined as the functional achievement of the communicational act, i.e., how successful it was [16]. The act of creating music with one or more others can be thought of as a kind of conversation. Consequently, musicians work with various verbal, non-verbal, and musical cues that assist them in transmitting information to, i.e., communicating with, their fellow musicians. Generally, a musical cue is \"any verbal, visual, or auditory signal that directly influences the nature and direction of the performance\" with the aim to \"time the aesthetic tone\" and \"give a desired quality of sound\" [11]. While much of musical communication can occur over audio channels, nonverbal modes of communication also lead to rich musical expression. Physical gestures are used to indicate attacks and releases; eye contact is used to draw attention and importance [13]; posture changes convey prominence for balance. Especially in ensemble settings, the conductor exercises musical communication in this strictly visual manner.\nBuilding on this understanding of musical communication, we explore the emerging paradigm of co-creative human-machine interaction, which extends the notions of control of generative AI to communication for mutual understanding of the goals, intent, and the contextual framework in which the human and machine operate. When multiple parties collaborate creatively, individual agents' ideas build upon each other. Unlike simply dividing up tasks, co-creativity tends to lead to more inventive solutions, embodying the principle that the whole is greater than the sum of its parts [6]. Specifically, we investigate how to facilitate co-creative behaviors between humans and machines through multimodal interactions. In our musical setting, we interpret visual cues with contextual meta-score data for the creation of more sophisticated and context-aware AI improvisation systems.\nIn this paper, we introduce an initial case study by creating a musical game in such a framework. Toward continued research in human-machine co-creativity, we propose additional compositional games and extensions within this framework."}, {"title": "Related Research", "content": "How can a human or a robot communicate visually in a musical context? This section examines available visual modes of musical communication for humans and machines, aiming to integrate them effectively into our co-creative framework."}, {"title": "Musical Games and Improvisation", "content": "First, we turn our attention to human-to-human communication within musical games. Musical games have an established history as exploratory tools for understanding machine processes and creativity [14][24][18][8], and in this section, we explore how specific improvisational experiences such as conduction [22], Soundpainting [23], Cobra [21], and public conducting facilitate real-time, collaborative music-making."}, {"title": "Musical Robots", "content": "Having examined the foundations of musical games, we now shift our focus to existing research on musical robots and their ability to interact with humans using visual cues.\nOne well-known example of a musical robot is Shimon, the robotic improvisational marimba player developed at Gil Weinberg's Georgia Tech Center for Music Technology. Shimon has mallets as arms and an expressive head with a digital video camera. It \"matches the human's playing style, tempo, and harmony in real time, while extending on the human's playing and contributing its own musical phrases and ideas\" [15]. Importantly, Shimon, while not exactly humanoid, can convey emotions. Head bobs visualize Shimon's internal beat, it makes eye contact to facilitate turn-taking, and it imitates human-like behavior through blinking and breathing [15].\nMost research into human-robot interaction focuses on how robots can understand human gestures through computer vision, with considerably less research into what kinds of robotic gestures are recognizable by humans. Visual intake of robotic action activates the same mirror neuron system in humans that fires when seeing biological action. This implies that it should be possible for robot mimicry of human movements to be accurately interpreted by a human observer [4].\nThe specifics of how a robot might appropriately encode cues to send to a human musician greatly depend on the robot's physical structure. Given our research objectives, a PTZ (Pan-Tilt-Zoom) camera was selected as our robotic system for several reasons. While being a minimal system, it can mimic the act of looking at something, which is essential for interactive communication. Additionally, it can take in \"egocentric\" camera input, functioning as another agent in the musical space. Finally, a PTZ camera provides a reasonable aesthetic for stage presence, making it a practical and visually appropriate choice.\nAs of writing, there is no existing literature on imitating human head movements with a PTZ camera. However, combining the findings mentioned above leads to several proposed camera movements that might appropriately model corresponding human movements. Nodding and head shaking are easily representable through fast vertical and horizontal movements, respectively. Addition-"}, {"title": "Creativity", "content": "Cognitive scientist Margaret Boden defines creativity as \"the ability to generate ideas/artifacts that are new, surprising, and valuable\" [2]. Computer scientist J\u00fcrgen Schmidhuber provides an alternative definition of the creative process as one that maximizes some reward for the creation of novel patterns, by which reinforcement learning methods may drive the continued creation of novel patterns. When considering group creativity, however, the ability to generate comes not necessarily from individual agents but from their interactions. In this way, having an idea of a fixed reward or creative goal may not be sufficient since observation of the output of one agent may create new unconsidered possibilities for other creative agents in the process. This pattern can be observed on a large time scale, by which new research developments create further opportunities for creative and innovative responses; in this framework, we consider this phenomenon on the shortened and specific scale of group artistic creativity in music-making. As a recent and highly specific example, TikTok duets are a popular cultural media phenomenon, by which a soundbite from one user, deliberate or not, may be \"dueted\" with, via direct audio overlay, sampling, remixing, or other means, creating a new set of creative output from initial creative source material56. Popular music jam sessions feature this same phenomenon, on a more \"real-time\" production scale than social media affords. Similar to the structured improvisation \"games\" described above, these collaborative creative acts build around one or more initial musical segments, sounds, or seeds, in complete or partial form, that are used as a leading material or inspiration to start a musical process, which then evolves as free-form or according to a pre-designed plan.\nIn this research, we explore collaborative music-making with a centralized guidance agent, analogous to a conductor. We maintain the idea of adaptive agents (human or a generative machine bestowed with musical knowledge), whose creativity is coordinated through a centralized reward agent whose policy is defined through a partial musical score, but with a degree of freedom of moving along this score according to feedback received from the performing agents themselves. This model incorporates a bi-directional communication between the performing musicians and the conductor, whose goal is to coordinate an optimal joint performance within the constraints of the pre-defined score. Since the \"optimality\" is defined not only in terms of following the musical score directives, but also incorporates instantaneous interaction, the model incorporates signals from visual interaction, gestures, and possibly future emotional and other meta-musical features. Moreover, the framework itself allows flexible or alternative score choices or musical trajectories, such as an open musical form or idea of open art in general [10]."}, {"title": "Case Study: \u201cGuided Harmony\"", "content": "Based on the above, we have designed a \"guided harmony\" musical game that utilizes a bidirectional codebook of visual cues to facilitate musician-to-machine and machine-to-musician communication. This game is intended to be an initial proof-of-concept for using a robotic camera for this, so we are limiting ourselves to the following modes of communication:\n- The only human cue is a raised hand, which expresses discontentment with the current state. Since musical instruments put humans in poses that are not commonly trained on in pose detection systems, choosing a large enough cue for standard pose detection libraries to identify robustly and reliably is vital. (Incidentally, this highlights the need to create datasets of images of humans playing instruments.) Our prototype system uses MMPose Coco Wholebody, which has been used in applications which make decisions from modeled keypoints of human pose, including those of the hands and face [20] [12].\n- Additionally, this game limits machine movements to finding and centering on a specific musician (\u201ceye contact\"), nodding, and shaking its \u201chead\u201d. As a result, the joint output is human-only audio.\nIn our case study, the machine acts similarly to Collier's conducting, with a score guiding the performance. However, musicians retain some control, like in Cobra, where they can signal discontentment with the current musical state.\nThis guided harmony game has the concrete goal of having a machine guide a group of human musicians through a set score of sustained chords. This score is known to the machine and hidden from musicians (similarly to how Jacob Collier alone decides what chord progressions to move the audience through). The machine provides an initialization harmony to the (human) musicians, who sustain the sound. Musicians have only one communication channel to the machine, with only two states: \"content\" or \"not content.\" The cue for \"not content\" is a raised hand. When the machine receives a \"not content\" cue (i.e., detects a raised hand) from any musician, it provides instructions to all musicians to adjust the harmony in a specific way. These are communicated through a two-part message - who should adjust, and how they should adjust. Currently, adjustment is constrained to the following:\n- up a half step,\n- up a whole step,\n- down a half step,\n- down a whole step, and\n- no change.\nAfter communicating individual instructions, the machine then provides a cue to all musicians to implement their instructed change. Figure 2 provides illustration of some of these robotic gestures.\nAs a concrete example to illustrate the game more clearly, imagine a score with a C Major chord as measure one and an F Major chord as measure two."}, {"title": "In Support of Learning Methods for Co-creativity: ImproVision Extensions", "content": "Due to the subjective nature of creativity, the level of expertise required of human agents, and difficulty in controlling for co-factors affecting human expression, there is a general lack of data supporting the learning of human-machine (and even human-human) interactions in co-creative artistic endeavors [9]. In this section, we describe possible extensions of our initial musical \"Guided Harmony\" game, some data collection methods, and expected benefits and capabilities enabled by the collection of such data. We aim to make these descriptions specific enough to facilitate data collection and demonstration with sufficient detail, but presented in a way that allows for adaptation to more general application or variation."}, {"title": "Nonverbal Toolkit for Musicians", "content": "Needless to say, using a raised hand as a cue is not an ideal choice. Raising one's hand disrupts musical output, as many instruments require both hands for sustained sound. Accordingly, to provide a control signal to a machine improvisation tool, we identify the need to design a toolkit with the following properties based on our initial demo:\nSignals must be nonverbal,"}, {"title": "Enhanced Focus on Score Generation and Creativity", "content": "Rather than have the machine parse a human-composed score, it would be a logical extension of the existing system to give the machine agency in creating the score through integration with music generation models. Allowing the machine to recognize audio cues or, more generally, identify the chords being played would also make the system more robust against human error (i.e., if a musician misinterprets or forgets a cue, the system would be able to hear the mistake and adapt future cues to return to the intended chord progression).\nWe also imagine a scenario in which the machine's music generation algorithm attempts to infer the given musicians' chord preferences based on the time elapsed between a new chord being played and someone raising their hand. By targeting the optimization of group enjoyment, the creative process becomes more directed toward a clear objective, and may even reveal quantitative insights about the performers' (or population) musical aesthetic preferences."}, {"title": "Identification of the Improvisational Leader", "content": "In cases when an ensemble is improvising together, there is a natural amount of turn-taking in allowing a soloist to lead ensemble play. If a machine agent can communicate, suggest, and direct such turn-taking, it enables the machine to be part of the musical flow, creating its own sequences of solos and, at times, directing action to support its own musical output. Toward this, with the use of a PTZ camera or other directional indicator, we propose demonstration of a system that actively listens to ensemble musical output and uses its communication channel to offer a suggestion (or direction) to the identity of the next lead player."}, {"title": "Silent Musical Charades", "content": "Nonverbal communication of musical intent, pleasure, and frustration is difficult to capture, as many humans wear different physical expressions to communicate these ideas. Moreso, trained musicians tend to communicate these ideas in a multimodal setting, letting their sounded output co-communicate both the intended music and also their intent, coherence, or frustration with their stand partner.\nWe also consider that the training of a musical improvisation system may benefit from a scalar reward function, to either provide a positive, encouraging reward for the system to continue on its path, or a negative, discouraging reward for the system to try something different.\nWhile the aforementioned toolbox provides one way for this to be communicated, it fails to be naturalistic as it is a pre-programmed codebook of control commands. Here, we propose a scheme for the collection of data which can represent the user's frustration and intent in a nonverbal manner.\n- Musician A and B begin with some shared repertory knowledge. For example, these may be musicians who had similar orchestral experience, played in the same jazz band, studied conservatory solo pieces on the same instrument, etc.\n- Musician A is holding their instrument as if playing and imitates playing but will not create any sound.\n- Musician A is given a selection of musical repertoire, and, in the style of charades, wins when Musician B guesses which piece was given.\nIt is our hope that, in facilitating this demonstration, a few patterns may emerge:\n- Musician A will express \"pleasure\" in a naturalistic, nonverbal way when Musician B provides a \"nearby\" guess.\n- Musician A will express \"frustration\" in a naturalistic, nonverbal way when Musician B provides a \"distant\" guess or fails to note some salient feature.\n- Musician A will communicate in a way that is possible given the occupation of their instrument."}, {"title": "Conclusion", "content": "To conclude, the presented research and framework explores the intersection of visual communication and musical interaction by implementing a robotic camera in a \"Guided Harmony\" musical game. By leveraging bidirectional communication between human musicians and a robotic system, we demonstrate the potential for co-creative human-machine interaction in a musical context. The game underscores the importance of robust and intuitive communication channels for enhancing the collaborative creative process.\nA natural progression of this work would be refining the visual cue toolkit to accommodate the diverse needs of different musical instruments and settings, allowing the machine to participate actively by integrating music generation algorithms and closed-loop audio feedback, and collecting more data on human-machine interactions in these settings. Stated more generally, remaining challenges include\n1. defining and expanding the amount of signals or gestures conveyed during performance,\n2. increasing the agency of the performers and the conductor,\n3. specifying the score in terms of partial information (graphic scores [17]),\n4. formalizing the computational language of writing the interaction (temporal logic, iscore, petri-nets [19]), and\n5. learning creative interaction patterns.\nIn further research, a basic difficulty is defining optimal co-creative interaction; possible definitions include maximizing the amount of influence that performers have on each other, creating the most informative joint result with least constraint on individual expression (least effort communications), etc. These interactions may be measured through directed information, transfer entropy [7], communication inside the group, and information contents of the overall (joint) message. Ultimately, the presented research and derivative explorations contribute to the broader understanding of how AI can enhance human creativity in musical contexts."}]}