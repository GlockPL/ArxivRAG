{"title": "Task Offloading in Vehicular Edge Computing using\nDeep Reinforcement Learning: A Survey", "authors": ["Ashab Uddin", "Ning Zhang", "Ahmed Hamdi Sakr"], "abstract": "The increasing demand for Intelligent Transportation Systems (ITS) has introduced significant challenges in managing the\ncomplex, computation-intensive tasks generated by modern vehicles while offloading tasks to external computing infrastructures\nsuch as edge computing (EC), nearby vehicular, and UAVs has become influential solution to these challenges. However, traditional\ncomputational offloading strategies often struggle to adapt to the dynamic and heterogeneous nature of vehicular environments.\nIn this study, we explored the potential of Reinforcement Learning (RL) and Deep Reinforcement Learning (DRL) frameworks to\noptimize computational offloading through adaptive, real-time decision-making, and we have thoroughly investigated the Markov\nDecision Process (MDP) approaches on the existing literature. The paper focuses on key aspects such as standardized learning\nmodels, optimized reward structures, and collaborative multi-agent systems, aiming to advance the understanding and application\nof DRL in vehicular networks. Our findings offer insights into enhancing the efficiency, scalability, and robustness of ITS, setting\nthe stage for future innovations in this rapidly evolving field.", "sections": [{"title": "I. INTRODUCTION", "content": "In recent years, the automotive industry has undergone a significant transformation with the integration of various embedded\nsensors, processing units, and wireless communication modules to enhance the overall driving experience by making it safer,\nmore efficient, and more comfortable [1]. Consequently, the degree of data dependency, the workload associated with data\npreprocessing, as well as the computational intensity and delay sensitivity, have increased across various areas of vehicular\ntechnology, including autonomous driving, real-time traffic management, and advanced driver-assistance systems (ADAS).\nMoreover, due to the dynamic nature of vehicular environments, significant challenges to the effective offloading and processing\nof tasks are exacerbated by factors such as high mobility, variable network conditions, and stringent latency requirements [2].\nTo address the challenges of onboard vehicle computational limitations, latency sensitivity and energy scarcity, computation\noffloading enables vehicles to delegate resource-intensive tasks to more capable external computing infrastructures, such as\ncentralized cloud computing (CC), Mobile Edge Computing (MEC) and FOG [3]\u2013[7]. MEC brings computation closer to the\nuser by deploying resources at the network edge, including Roadside Units, onboard vehicle systems, and other infrastructure\nlike parking and gas stations [9]. The integration of MEC into the vehicular ecosystem is becoming increasingly critical, as\nit aligns with the low-latency, high-reliability requirements essential for the emerging 5G and future 6G networks [10], [11]\nwhile Fog computing, on the other hand, enables cooperation among edge devices creating an intermediate bridge between\nedge device and cloud [12].\nIn order to facilitate communication between these computing resources, vehicles rely on Vehicle-to-Vehicle (V2V) and\nVehicle-to-Infrastructure (V2I) communication protocols. These protocols leverage advanced radio access technologies (RATs)\nsuch as dedicated short-range communications (DSRC) and cellular networks, including LTE and the emerging new radio\n(NR) standards, to ensure reliable and efficient data exchange in vehicular networks [13]. Moreover, As vehicles and mobile\nusers move across different geographic areas, the ongoing services hosted on nearby edge servers can experience significant\nperformance degradation, a drop in Quality of Service (QoS), and even interruptions if they remain tied to a server that\nthe user has moved away from. Service migration [14] addresses these challenges by dynamically relocating services to the\nmost appropriate edge server, ensuring that the services remain close to the user and continue to function smoothly. This\nprocess is crucial to maintain seamless service continuity, minimizing latency, optimizing network performance,and balancing\nthe migration costs.\nDue to the massive interconnected and heterogeneous nature of vehicular networks, efficient resource management [15], [16]\nis essential to ensure service quality and reliability and optimal usage of computational resources, meeting the stringent latency\nand reliability requirements. Advanced techniques, such as predictive analytics and machine learning, can be employed to\nanticipate resource demands and optimize task distribution across available computing infrastructures, including edge servers,\nfog servers, cloud and nearby users. By dynamically balancing the load and prioritizing critical tasks, these approaches help\nprevent resource bottlenecks, reduce processing delays, and enhance overall system performance."}, {"title": null, "content": "In recent years, edge computing and computational offloading have emerged as essential solutions to address challenges in\nmodern vehicular, mobile, and IoT networks. Researchers have explored various architectures such as MEC, ad hoc systems,\nUAV networks, and cloudlets to improve performance, reduce latency, and optimize resource management. Surveys such as\nthose by Abbas et al. [17] and Shi et al. [18] emphasized MEC's role in latency reduction, resource allocation, and privacy\nenhancement, with applications in smart homes, augmented reality, and healthcare. Comparative studies like Nouhas et al [19]\nhighlighted the strengths of hybrid Cloud-Edge models, while others, like Deepak et al. [20], Liu et al. [21], and Wang et\nal. [14], stressed the need for advancements in resource allocation, Edge Intelligence, and blockchain integration for security.\nStudies on vehicular networks, including the work of Hou et al. [7], examined AI and RL for dynamic task optimization and the\npotential of Digital Twin Edge Networks (DITEN), and Mao et al. [3] studied the feasibility of server deployment. Furthermore,\nVehicular fog computing (VFC) and heterogeneous Vehicular Networks (HetVNETs) have been explored to leverage vehicular\nresources and improve connectivity while the integration of MEC with SDN and NFV, has been discussed by Haibeh et\nal. [8] followed by Filali et al. [9], further highlighted advances in network performance and scalability, particularly in 5G\nenvironments.\nTraditional optimization techniques such as convex optimization [22], evolutionary algorithms [23], and game-theory [24]\napproaches face significant challenges in solving the problem of mixed integer nonlinearty, along with with the increasing size\nand complexity of vehicular networks. These methods often struggle with scalability, adaptability to dynamic environments,\nand computational efficiency, especially in systems with large state-action spaces and non-convex dynamics. In contrast, deep\nreinforcement learning (DRL) offers a promising solution by simultaneously learning state transitions and optimizing decision-\nmaking policies in real time. DRL's ability to handle both centralized and multi-agent frameworks enables it to adapt to diverse\nvehicular scenarios, making it well-suited for dynamic and uncertain environments.\nConsidering the solution perspective through DRL, Nguyen et al. [25] highlighted DRL's potential for MEC-enabled AANS\nbut emphasized challenges with RIS and NOMA integration while Chen et al. [26] noted MDP limitations in IoT, advocating\nrobust frameworks for decentralized and multi-agent systems. Meanwhile, Liu et al. [4] identified scalability and coordination\ngaps in vehicular networks, suggesting hierarchical DRL models for efficient offloading. Focusing on One of the key challenges\nthat arise in vehicular networks, such as high mobility and the non-stationary nature of the environment, Althamary et al. in\n[27] detailed how MARL frameworks can provide robust solutions, offering examples of MARL's application to V2V, V2I, and\nV2X communications. Another key challenge i.e edge caching, has been examined by Zhu et al. in [28] illustrating how DRL\ncan improve caching strategies in mobile networks by making intelligent, data-driven decisions based on user behavior and\nnetwork conditions. Finally, Li et al. [29] presented a comprehensive survey of Multi-Agent Reinforcement Learning (MARL)\nin future Internet technologies, focusing on challenges like dynamic network access, transmit power control, and computation\noffloading.\nStudies on DRL highlights not only potential solutions for mixed integer nonlinear problems, but also demonstrates a\ngreater ability to adapt and cope with varying conditions and large networks. However, A key challenge that remains under-\nexplored in current DRL research is the examination of MDP formulations with learning methods that fully account for the\ncomplexities of dynamic environments while meeting system-wide goals. Effective coordination of MDP formulations with\nstate-action transitions and reward structures tailored to specific DRL approaches is essential for optimizing performance.\nThis includes designing reward functions that not only guide agents toward local objectives but also ensure overall system\nefficiency. In multi-agent systems, uncoordinated individual decisions often lead to suboptimal outcomes, such as resource\ncontention and inefficiency. Furthermore, aggregating individual POMDPs into a coherent global MDP is complex, requiring\nan understanding of how these MDPs interact, synchronize, or merge to form a valid and accurate representation of the whole\nenvironment. Studying this aggregation process is crucial for optimizing multi-agent systems' performance. Mechanisms that\nfoster collaboration among agents are vital to prevent locally optimal decisions from leading to globally suboptimal results.\nDRL studies show promise in solving mixed integer nonlinear problems and adapting to dynamic, large-scale networks.\nHowever, challenges remain in formulating MDPs that address environmental complexities and system-wide goals., and at the\nsame time effective reward structures are crucial for balancing local objectives with overall efficiency. Morover, in multi-agent\nsystems, uncoordinated decisions often cause resource contention and inefficiency while aggregations of individual POMDPS\ninto a coherent global MDP is particularly complex, requiring synchronization and interaction analysis to optimize performance.\nFurthermore, uniform learning among agents is a potential bottleneck that need to be addressed and analyzed with performance\ndynamics.\nOur research objectives are to investigate how MDP formulations can be optimized to enhance task offloading and decision-\nmaking under uncertainty. Critical challenges such as value function or quality function approximation, policy convergence,\nexploration and exploitation trade off, central and decentral learning approach for MARL, uniform learning accross agents\nand reward function design are analyzed within a multi-agent context. By addressing these issues, we aim to improve the\noverall performance of MDP-based DRL methods, offering new solutions for both local and global optimization in dynamic,\nmulti-agent environments.\nThe key contributions of this work are summarized as follows:\n\u2022 We have investigated Markov Decision Process (MDP) formulations across different studies, underlining the gap and\naddressing simplified assumptions."}, {"title": null, "content": "\u2022 Our study reveals that the variability in reward functions remains an unresolved research question in many works, i.e. the\ntrade-offs between latency and energy that fail to optimize both metrics simultaneously and fairness among multiagents.\n\u2022 We analyze value function approximation techniques for value iteration and policy gradient DRL models in vehicular\nnetworks, examining the impact of discrete vs. continuous action spaces and stochastic vs. deterministic policies. Our\nfindings highlight their influence on scalability and stability in single and multi-agent offloading, addressing real-time\nchallenges in autonomous driving and intelligent transportation systems.\n\u2022 Our study provides a comprehensive context for collaborative decision-making among multiple agents in vehicular\nnetworks, addressing learning architectures such as centralized and decentralized models. We investigate the impact of\ndynamic environmental changes on learning and coordination, enabling agents to adapt effectively to fluctuating conditions\nsuch as network topology, user mobility, and resource availability.\n\u2022 Our work offers valuable insights for future research on MDP objectives, collaborative and uniform learning in vehicular\ntask offloading scenarios for both single and multi agent scenerios.\nThe rest of the paper is organized as follows: Section III discusses the Computing Paradigms for Vehicular Networks,Section\nIV delves into the type connectivity of computational resources, and section V introduces the Basics of Deep Reinforcement\nLearning (DRL), offering a foundational understanding of RL approaches. Consequently Section VI presents a Comprehensive\nReview of the Literature, and finally, Section VIII provides the Conclusion, summarizing the paper's contributions and the\npotential impact of DRL on enhancing vehicular network performance."}, {"title": "II. COMPUTING PARADIGM FOR VEHICULAR NETWORK", "content": "The evolution of vehicular networks has introduced smart vehicles supported by advanced information and communication\ntechnologies, enabling significant innovations in communication, computing, and caching. Despite these advancements, vehicles\noften face constraints such as limited computational resources and finite onboard battery life. To address these challenges,\npowerful computing paradigms like Centralized Cloud (CC), Edge Cloud (MEC), and Vehicular Cloudlet (VC),UAV, Sattelite\nhave been developed, significantly enhancing the performance of vehicular applications through computation offloading."}, {"title": "A. Centralized Cloud (CC)", "content": "Cloud computing is a transformative model that enables ubiquitous, convenient, and on-demand network access to a shared\npool of configurable computing resources. These resources include networks, servers, storage, applications, and services, which\ncan be rapidly provisioned and released with minimal management effort or interaction with service providers. One of the"}, {"title": null, "content": "key advantages of cloud computing is its ability to enhance computing performance by leveraging abundant, innovative, cost-\neffective, and scalable storage and computeing solutions, facilitating a wide range of applications [16]. Despite its numerous\nbenefits, the physical distance between cloud servers and users can introduce delays, which may degrade the performance\nof latency-sensitive applications. Additionally, the transmission of large amounts of data can overburden limited network\nbandwidth, leading to potential congestion and higher transmission costs [30]."}, {"title": "B. Edge Cloud (EC)", "content": "Edge cloud computing is an extension of cloud computing that brings computational resources closer to the data source\nor end-users. Edge cloud computing is particularly beneficial for applications requiring real-time data processing, such as\nautonomous driving, augmented reality, and smart grids. Edge cloud can be referred as:\n1) Mobile Edge Computing (MEC):: Mobile Edge computing [31] refers to the processing of data near the source of data\ngeneration, such as IoT devices, sensors, and smart gadgets. This proximity ensures low latency, real-time data processing,\nand reduced bandwidth usage and enables cloud computing capabilities and an IT service environment at the edge of the\ncellular network i.e placed in road side unit, a nearest coputational source from vehicles 1. MEC allows network operators to\nopen their radio access networks (RAN) to authorized third parties, such as application developers and content providers. This\nis particularly beneficial for latency-sensitive applications and services that require rapid data processing close to the mobile\nusers.\n2) Fog Computing(FC):: Fog computing [32], on the other hand, acts as an intermediary layer between the edge devices\nand the central cloud. It extends cloud computing capabilities to the edge of the network, providing additional computational\npower, storage, and networking services closer to the data sources but little i.e placed in parking station far from MEC .It\ninvolves the use of decentralized computing infrastructure in which data, compute, storage, and applications are distributed in\nthe most logical, efficient place between the data source and the cloud. Fog computing is particularly useful for applications\nthat require low latency and are bandwidth-sensitive, providing computation, storage, and networking services between end\ndevices and traditional cloud data centers.\n3) On Board Vehicle Server:: A vehicular cloud or On Board Vehicle Server [33] is a mobility-enhanced, small-scale cloud\ndatacenter mounted onboard vehicles, designed to support resource-intensive and interactive mobile applications. Vehicles with\nunderutilized resources create a distributed network by leveraging their computational, storage, and communication capacities\nas mobile nodes. This setup enables data processing to occur closer to the point of need, reducing reliance on distant data\ncenters. The decentralized structure of VCC enhances real-time applications in transportation, such as traffic management and\nautonomous driving, by improving network efficiency and reducing latency.\n4) Unmanned Aerial Vehicles (UAV): Unmanned Aerial Vehicles (UAVs) or drones [34], have become a critical asset\nin enhancing vehicular networks and Intelligent Transportation Systems (ITS) by providing dynamic, aerial platforms for\ndata collection, communication, and environmental monitoring. UAVs can be rapidly deployed in areas lacking ground-based\ninfrastructure, such as rural or disaster-stricken regions, to monitor traffic conditions, detect accidents, and relay real-time\ninformation to traffic management centers or directly to vehicles. Additionally, UAVs serve as mobile communication relays,\naugmenting traditional networks in high-density urban areas or large-scale events, thereby reducing latency and improving data\nthroughput. Despite their potential, challenges such as regulatory constraints, safety concerns, and limited flight endurance\nmust be addressed to ensure their effective integration into vehicular networks."}, {"title": "III. NETWORK TOPOLOGY FOR VEHICULAR NETWORK", "content": "Vehicular networks are rapidly evolving, driven by the increasing demands for low-latency communication, real-time data\nprocessing, and efficient resource management. With the rise of connected and autonomous vehicles along with multiple types\nof computing paradigm, different connectivity topologies, such as central, distributed, hierarchical, ad-hoc, and heterogeneous,\nenable efficient data handling across various layers of the network. Each of these topologies comes with its own strengths and\nweaknesses, depending on factors like latency requirements, the density of vehicles, and the availability of infrastructure. This\nsection delves into these diverse topologies, exploring how they can be optimized for vehicular networks to enhance overall\nsystem performance ."}, {"title": "A. Central MEC", "content": "A single central MEC [35] system offers strong centralized control, efficiently coordinating resources like radio bandwidth\nand computational power. Deploying the MEC server at a base station (centre of the network) allows for task processing close\nto the user, reducing latency compared to traditional cloud computing. Centralized management simplifies resource allocation,\nenabling the central MEC server to use channel state information (CSI) and computation requests to optimize. However, this\nsetup faces challenges in scalability and resource limitations, as multiple users share the MEC server's resources, potentially\nleading to bottlenecks, especially in high-traffic situations. Distributed MEC systems, with multiple edge servers, address these\nissues by distributing the load, enhancing scalability, and reducing latency."}, {"title": "B. Distributed MEC", "content": "Distributed MEC [36] offers a solution to challenges in vehicular networks by dispersing computational resources across\nmultiple edge nodes, such as vehicles and roadside units (RSUs), instead of relying on a single centralized server. This setup\nsignificantly reduces latency by enabling data processing closer to its source, which is essential for real-time applications like\nautonomous driving and traffic management. By distributing tasks across nodes, the system better adapts to the mobility and\nresource variability in vehicular networks, improving scalability and resilience. Security and privacy are also enhanced, as data\ncan be processed locally, minimizing transmissions to distant servers."}, {"title": "C. Hierarchical MEC Network", "content": "Distributed edge computing systems face challenges [18] such as resource fragmentation, lack of centralized control, and\nissues with latency and data consistency, especially in environments with numerous edge nodes. Hierarchical edge computing\n[21] addresses these issues by organizing resources across layered levels, from local edge devices to centralized cloud systems.\nThis structure optimizes resource management by processing simpler tasks locally and offloading complex ones to higher\ntiers, reducing latency, network congestion, and energy use. Technologies like Software-Defined Networking (SDN), Network\nFunction Virtualization (NFV), and network slicing enhance control and flexibility, improving traffic flow, task scheduling, and\nefficiency. The hierarchical model also supports load balancing and fault tolerance, making it particularly suited for large-scale\nsettings like smart cities. Hierarchical network may include:\n1) Ad-hoc Networks in MEC: In MEC, ad-hoc networks formed by VANETS (Vehicular Ad-hoc Networks) [37], [38] and\nUAVs (Unmanned Aerial Vehicles) [39], [40] are essential for enabling decentralized, real-time data processing closer to the\nsource. These networks operate independently of centralized infrastructure, offering flexibility and adaptability in dynamic\nenvironments. Edge servers on vehicles or UAVs forming ad-hoc nature allows rapid, direct communication between vehicles\n(V2V), vehicles and UAVs (V2U), or UAVs (U2U), forming a dynamic and resilient network. This setup enhances robustness\nand scalability, enabling task offloading to nearby nodes even in areas with limited fixed infrastructure like disaster zones,\nrural areas, and dense urban spaces.\n2) Heterogeneous MEC: Heterogeneous MEC refers to a topology that integrates different types of network elements,\nincluding vehicles, RSUs, satellites, and cloud servers. This approach ensures that vehicular networks can support a variety\nof communication protocols and device capabilities, allowing for more robust and flexible architectures. Heterogeneous MEC\nis particularly useful for long-range communication and applications that require global coverage, such as integrating satellite\nlinks for vehicular communication in remote areas [41]. By combining diverse technologies, this topology provides seamless\nconnectivity, even in highly variable and dynamic network conditions."}, {"title": "IV. DRL BASICS", "content": "In reinforcement learning Fig. 3, the agent interacts with the environment by observing states, taking actions, and receiving\nrewards. The goal is to maximize long-term expected rewards. This process is modeled as a Markov Decision Process (MDP),\ndefined by the tuple (S, A, P, R, \u03b3) [42]:\nS: State space A: Action space P: Transition probabilities R: Rewards \u03b3: Discount factor The transition probability p(s', r |\ns, a) depends only on the current state and action, not past states a fundamental property of Markov Process. Policies can be\ndeterministic, a direct measure (\u03b1 = \u03c0(s)) or stochastic, a probability distribution (\u03c0(\u03b1 | s)). The state value function, V(s),\nrepresents how good a state is when following a policy \u03c0. It is defined as:"}, {"title": null, "content": "$Vn(s) = \\sum\\limits_{a} \u03c0(a|s) \\sum\\limits_{s',r} p(s'|s,a)(r + \u03b3V(s'))$"}, {"title": null, "content": "(1)\n$V\u03c0(s) = \\sum\\limits_{a} \u03c0(a | s)Q(s,a)$"}, {"title": null, "content": "(2)\nThe action value function, Q\u3160(s, a), represents how good a state-action pair is when following a policy \u03c0. It is defined as:"}, {"title": null, "content": "$Q\u03c0(\u03c2, \u03b1) = \\sum\\limits_{s',r} p(s'|s,a)(r + \u03b3V(s'))$"}, {"title": null, "content": "This can be simplified to:"}, {"title": null, "content": "$Q(s,a) = \\sum\\limits_{s',r} p(s'|s,a) [r+y \\sum\\limits_{a'} \u03c0(a' | s')Qz(s',a')]$"}, {"title": null, "content": "The Bellman optimality equation [43] for the value function is given by:"}, {"title": null, "content": "$V\u03c0* (8) = max \\sum\\limits_{a} \\sum\\limits_{s'} p(s'|s, a) V*(s')$"}, {"title": null, "content": "(3)\nSimilarly, the Bellman optimality equation for the action-value function is:"}, {"title": null, "content": "$Q\u03c0= (s, a) = \\sum\\limits_{s',r} p(s' | s,a) [r+ymax Q- (s', a')]$"}, {"title": null, "content": "(4)"}, {"title": "B. Value Function Approximation:Value Iteration", "content": "Value Function Approximation (VFA) [44] uses supervised learning to map states (or state-action pairs) to value functions.\nThe parameters @ are updated based on observed data.\nThe loss function for VFA is:"}, {"title": null, "content": "$L(0) = B_{(s,r,s')} (r + \u03b3V(s'; 0) \u2013 V (s; 0)) ^{2}$"}, {"title": null, "content": "(5)\nFor action-value function approximation:"}, {"title": null, "content": "$L(0) = B_{(s,a,r,s')} [(x (r + ymax Q(s',a';0) - Q(s,a;0))^{2}]$"}, {"title": null, "content": "(6)"}, {"title": null, "content": "1) Deep Q-Network (DQN) with Variant: The Deep Q-Network (DQN) [45] is an off-policy reinforcement learning algorithm,\nmeaning that it learns the optimal policy from the experience buffer, collected following old policy. In DQN, a neural network\nis used to estimate current state action-values (Q-values). To stabilize training, a target network is updated less frequently\nand used to calculate target Q- values, reducing the instability caused by correlations in the training data. Double DQN [46]\naddresses the overestimation bias by using the main network to select actions and the target network to evaluate the selected\nactions, further refining the Q-value estimates while Dueling DQN improves the model by separately estimating the state value\nand advantage functions, which are then combined to produce more robust action-value predictions, enhancing performance\nand generalization."}, {"title": "C. Value Function Approximation: Policy Gradient", "content": "The gradient of the value function is the expectation of the trajectory return times the accumulated score function [42]:"}, {"title": null, "content": "$VeV (50) x \\sum\\limits_{SES} d(s) \\sum\\limits_{\u03b1\u0395\u0391} Q\u03c0 (s, a) Ve\u03c0(\u03b1 | s)$"}, {"title": null, "content": "$\\sum\\limits_{SES} \\sum\\limits_{\u03b1\u0395\u0391} \u03c0(\u03b1 | s)Q\" (s, a)\u2207e log \u03c0(\u03b1|s)$\n= d(s)"}, {"title": null, "content": "= \u0395\u03c0 [Q\" (St, At) Velog \u03c0\u03b8 (At | St)]"}, {"title": null, "content": "(7)"}, {"title": null, "content": "1) Actor-Critic Method: The Actor-Critic [47] method method involves two networks: the Actor, which generates ac-\ntions(policy) based on the current state, and the Critic, which estimates the expected return for that state following maximum\nquality function Q(s, a). The Critic updates its value estimates by minimizing the Temporal Difference error for bellman\nequation, which accounts for the immediate reward and the next state's value. The Actor's policy is refined using the gradient\nof the objective function, guided by the Critic's feedback. This interaction helps the agent learn an optimal policy to maximize\nrewards.\n2) Trust Region Policy Optimization (TRPO): A variant of the Actor-Critic method, Trust Region Policy Optimization\n(TRPO) [48], is often called on policy method because of policy updates with the experienece collected by current policy,\nand utilize restricted policy updates during each iteration to ensure stability. It does so by applying a constraint on the change\nin policy, measured by the KL divergence between the new and old policies. The objective is to optimize the policy while\nkeeping the policy changes within a specified limit to prevent large, unstable updates. This approach helps in maintaining more\nreliable and consistent improvements during training."}, {"title": null, "content": "3) Proximal Policy Optimization (PPO): Another variant of the Actor-Critic method (on policy), Proximal Policy Opti-\nmization (PPO) [49], restricts updates to ensure stability. The objective function balances policy improvement by taking the\nminimum of the advantage function and a clipped version to prevent excessive changes. Additionally, to encourage exploration,\nthe objective is refined by incorporating an entropy term, which promotes a diverse set of actions. This approach helps maintain\na balance between exploration and exploitation, leading to more stable and efficient learning.\n4) Soft Actor-Critic (SAC): The Soft Actor-Critic (SAC) [50], an off-policy algorithm uses three key components: a policy\nnetwork, Q-networks, and a value network. The policy network generates actions based on the current state, optimized by\nmaximizing expected returns and incorporating an entropy term to encourage exploration. The Q-networks estimate expected\nreturns for state-action pairs and are updated using the soft Bellman residual, which considers immediate rewards and future\nvalues. The value network predicts the overall value of a state, stabilizing training by minimizing the Temporal Difference (TD)\nerror. These components work together to balance exploration and exploitation, enabling the learning of effective policies.\n5) DDPG: The Deep Deterministic Policy Gradient (DDPG) [51] is an off-policy algorithm designed for environments with\ncontinuous action spaces. It involves an Actor network that generates deterministic actions based on the current state, meaning\nthe actions are not probabilistic but directly selected according to the policy. The Critic network evaluates these actions by\nestimating the expected returns, and it updates its estimates by minimizing the Temporal Difference error. The Actor's policy\nis refined using gradients that drive the actions toward those that maximize the Q-value. To ensure stable learning, both the\nActor and Critic networks have corresponding target networks that are updated gradually. This framework allows DDPG to\nlearn effective and stable policies in environments requiring deterministic decisions and continuous actions.\n6) TD3: The Twin Delayed Deep Deterministic Policy Gradient (TD3) [52] algorithm improves upon the Deep Deterministic\nPolicy Gradient (DDPG) by addressing overestimation bias and instability in continuous action spaces. TD3 uses two Critic\nnetworks, selecting the smaller Q-value to reduce overestimation. It delays policy updates, updating the Actor less frequently\nthan the Critics to avoid unstable updates. Additionally, TD3 adds noise to target actions (Target Policy Smoothing) to prevent\noverfitting. These enhancements make TD3 more stable and reliable, leading to better performance in continuous action\nenvironments."}, {"title": "D. MultiAgent DRL", "content": "In Vehicular Edge Computing (VEC), objectives such as latency reduction, energy efficiency, load balancing, scalability,\nreliability, and data security can be achieved through a single-agent approach with a fully observable MDP or via a multi-agent\nsystem using a partially observable Markov decision process (POMDP). Single Agent RL centrally optimizes latency, energy,\nand task allocation for multiple vehicles or servers while multiagent Reinforcement Learning (MARL) [53], [54] involves\nmultiple agents interacting within a shared environment, each optimizing the set objectives:\n\u2022 Collaborative MARL: Agents cooperate to achieve a shared/common goal, e.g., optimizing system latency and energy\nwithout regard for individual interests [55].\n\u2022 Cooperative MARL: Agents act independently to achieve own objective, but also consider cooperation to each other e.g.,\noptimizing overall system utility with cooperative channel sensing mechanism [56].\n\u2022 Competitive MARL: Agents compete with each other solely to maximize their own objectives, e.g., competing for channel\nor resource access [57].\n1) Key Challenges of MARL: In multi-agent environments, agents often operate under Partial Observability, making decisions\nbased on local information, modeled by Partially Observable Markov Decision Processes (POMDPs). This leads to the non-\nstationarity [58] issue, where agents learn and update their policies concurrently. As one agent adapts, others are doing the\nsame, causing the environment to change continuously. This violates the assumption that state transitions and rewards are going\nto be stationary, making it difficult for any agent to achieve stable learning. In addition to non-stationarity, scalability [59] is\na critical challenge in multi-agent systems. As the number of agents increases, the joint action space expands exponentially,\nmaking it computationally expensive to calculate optimal policies. While Deep Neural Networks (DNNs) are used in Multi-\nAgent Reinforcement Learning (MARL) to approximate large action spaces and enhance scalability, they introduce challenges\nin terms of convergence due to the complexity of deep learning theory. As the system size grows, ensuring efficient and\nstable learning remains a major concern for MARL algorithms, requiring advanced methods to handle the trade-offs between\ncomplexity and performance. The multi agent approach can be referred as the followings:\n2) Centralized Training Decentralized Execution (CTDE): In CTDE, [60] a centralized critic uses global information to\noptimize the policies of all agents during training, ensuring that the system can handle large-scale environments efficiently.\nHowever, during execution, agents act independently, which reduces communication overhead and enables decentralized\ndecision-making. Moroever, by using global information during centralized training, CTDE helps mitigate the non-stationarity\nproblem, as agents learn while considering the actions and policies of others. This results in more stable learning because\nthe centralized critic can account for the evolving environment and ensure policy convergence, even when agents interact\ndynamically. Following are two CTDE method:\n\u2022 MultiAgent DDPG: MADDPG ( [61])is a multi-agent reinforcement learning algorithm based on the Deep Deterministic\nPolicy Gradient (DDPG) framework. It follows the Centralized Training and Decentralized Execution (CTDE) paradigm,"}, {"title": null, "content": "where each agent has a critic that has access to global state and action information from all agents during training.\nThis access helps improve the evaluation of actions in complex multi-agent environments", "COMA)": "As the central critic in MADDPG evaluates Q based on the\noverall state and joint actions", "62": "is an actor-critic algorithm\ndesigned to address the credit assignment problem in cooperative multi-agent environments. Unike MADDPG", "VDN)": "VDN [63"}]}