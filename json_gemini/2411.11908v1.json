{"title": "LLM4DS: Evaluating Large Language Models for Data Science Code Generation", "authors": ["Nathalia Nascimento", "Everton Guimaraes", "Sai Sanjna Chintakunta", "Santhosh Anitha Boominathan"], "abstract": "The adoption of Large Language Models (LLMs) for code generation in data science offers substantial potential for enhancing tasks such as data manipulation, statistical analysis, and visualization. However, the effectiveness of these models in the data science domain remains underexplored. This paper presents a controlled experiment that empirically assesses the performance of four leading LLM-based AI assistants-Microsoft Copilot (GPT-4 Turbo), ChatGPT (01-preview), Claude (3.5 Sonnet), and Perplexity Labs (Llama-3.1-70b-instruct)-on a diverse set of data science coding challenges sourced from the Stratacratch platform. Using the Goal-Question-Metric (GQM) approach, we evaluated each model's effectiveness across task types (Analytical, Algorithm, Visualization) and varying difficulty levels. Our findings reveal that all models exceeded a 50% baseline success rate, confirming their capability beyond random chance. Notably, only ChatGPT and Claude achieved success rates significantly above a 60% baseline, though none of the models reached a 70% threshold, indicating limitations in higher standards. ChatGPT demonstrated consistent performance across varying difficulty levels, while Claude's success rate fluctuated with task complexity. Hypothesis testing indicates that task type does not significantly impact success rate overall. For analytical tasks, efficiency analysis shows no significant differences in execution times, though ChatGPT tended to be slower and less predictable despite high success rates. For visualization tasks, while similarity quality among LLMs is comparable, ChatGPT consistently delivered the most accurate outputs. This study provides a structured, empirical evaluation of LLMs in data science, delivering insights that support informed model selection tailored to specific task demands. Our findings establish a framework for future AI assessments, emphasizing the value of rigorous evaluation beyond basic accuracy measures.", "sections": [{"title": "I. INTRODUCTION", "content": "Large Language Models (LLMs) have emerged as transformative tools with the potential to revolutionize code generation in various domains, including data science [1]-[5]. Their ability to generate human-like text and code opens up possibilities for automating complex tasks in data manipulation, visualization, and analytics. As data science projects often require extensive coding efforts that are time-consuming and demand significant expertise, leveraging LLMs could greatly enhance productivity and accessibility in this field. However, the effectiveness and reliability of LLM-generated code for data science applications remain underexplored, necessitating a thorough evaluation.\nWhile previous studies have evaluated LLMs in general programming tasks using platforms like LeetCode [6]\u2013[9], the HumanEval benchmark [10], and GitHub Projects [11], Gu et al. [12] identified a notable gap in approaches to evaluate domain-specific code generation. They demonstrated that LLMs exhibit sub-optimal performance in generating domain-specific code for areas such as web and game development, due to their limited proficiency in utilizing domain-specific libraries. This finding underscores the need for more focused evaluations that consider the unique challenges of specialized domains like data science, which involve tasks such as handling datasets, performing complex statistical analyses, and generating insightful visualizations areas not fully represented in general programming assessments.\nThis paper addresses this gap by providing an empirical evaluation [13] of multiple LLMs on diverse data science-specific coding problems sourced from the Stratascratch platform [14]. The controlled experiment involves four main steps: (i) selecting 100 Python coding problems from Stratascratch, distributed across three difficulty levels (easy, medium, hard) and three problem types (Analytical, Algorithm, Visualization); (ii) transforming these problems into prompts following the optimal prompt structure for each type; (iii) using these prompts for each AI assistant to generate code solutions; and (iv) evaluating the generated code based on correctness, efficiency, and other relevant metrics.\nOur research seeks to answer the following question: How effective are LLMs for data science coding? By systematically assessing the performance of these AI assistants, we aim to identify their strengths and limitations in automating code generation for data science problems.\nOur contributions are multifold:\n1) We provide an empirical evaluation of multiple LLMs on data science-specific coding problems, filling a critical gap in current research.\n2) We assess Stratacratch as a platform to benchmark LLMs for data science code generation, evaluating its suitability and potential as a standardized dataset for LLM performance in this domain."}, {"title": "II. RELATED WORK", "content": "In the realm of code generation, prior studies have evaluated LLMs like ChatGPT and GitHub Copilot using platforms such as HumanEval Benchmark, LeetCode, and Github. Nascimento et al. [7] compared code generated by ChatGPT against human-written solutions, assessing performance and memory efficiency. Kuhail et al. [8] evaluated ChatGPT on 180 LeetCode problems, providing insights into its capabilities and limitations. Coignion et al. [9] investigated different LLMs on general coding problems from LeetCode, focusing on performance metrics. Nguyen and Nadi [6] assessed GitHub Copilot's code generation on 33 LeetCode problems, evaluating correctness and understandability.\nBeyond traditional programming tasks, LLMs have been applied in data science-specific domains, where recent research has explored the models' capacity to handle complex queries and data manipulation tasks. Troy et al. [15] demonstrated that LLMs could generate SQL statements for cybersecurity applications, specifically highlighting their capability in structured query generation. In another study, Malekpour et al. [16] introduced an LLM routing framework designed for text-to-SQL tasks, optimizing the selection of models based on cost-efficiency and accuracy. Li et al. [3] identified limitations even in advanced models like GPT-4, noting that these models achieved only 54.89% execution accuracy on complex text-to-SQL queries significantly below the human benchmark of 92.96%. Additionally, Kazemitabaar et al. [5] delved into the challenges of data analysis with conversational AI tools like ChatGPT, identifying difficulties users face in verifying and guiding AI-generated results for desired outcomes.\nLai et al. [4] proposed the DS-1000 benchmark, a dataset specifically crafted for evaluating code generation in data science contexts. DS-1000 comprises 451 unique data science problems sourced from StackOverflow and spans seven essential Python libraries, including Numpy and Pandas. A key feature of this benchmark is its emphasis on problem perturbations, aimed at reducing the risk of model memorization. The dataset accounts for the unique challenges of data science tasks, which often lack executable contexts, may depend on external libraries, and can have multiple correct solutions. Lai et al. demonstrated the effect of different types of problem perturbations by testing models like Codex, InCoder, and CodeGen, with the best accuracy being 43.3% achieved by Codex-002. However, while DS-1000 provides a robust dataset for testing, Lai et al. do not perform a comparative empirical evaluation across multiple LLMs, leaving open questions about how current models fare on this benchmark.\nDespite these advancements, much of the current research has been limited to either general coding tasks or SQL-specific applications. The nuances of data science problems\u2014ranging from data manipulation and complex analyses to visualization-remain underexplored in LLM evaluations. Our work addresses this gap by conducting an empirical experiment using four leading LLMs on a set of data science problems extracted from the Stratacratch dataset, encompassing various difficulty levels and problem types. Unlike prior studies, which primarily introduce benchmarks or focus on specific task categories, our approach offers a detailed examination of LLM performance across a broader spectrum of data science challenges."}, {"title": "III. CONTROLLED EXPERIMENT", "content": "In line with the controlled experiment methodology by Wohlin et al. [13], our study aims to evaluate and compare the effectiveness of four prominent LLM-based AI assistants-Microsoft Copilot (GPT-4 Turbo), ChatGPT (01-preview), Claude (3.5 Sonnet), and Perplexity Lab (Llama-3.1-70b-instruct) in solving data science coding tasks sourced from the Stratascratch platform [14].\nEffectiveness in this context refers to the degree to which these models achieve desired outcomes across four key aspects: success rate, efficiency, quality of output, and consistency. Specifically, we define:\n\u2022 Success Rate as the proportion of correctly generated code solutions, measured by the percentage of solutions that achieve the correct result regardless of the number of attempts;\n\u2022 Efficiency as the runtime execution speed of the generated solution;\n\u2022 Quality of Output as the alignment of generated solutions with expected outcomes, particularly for visualization tasks;\n\u2022 Consistency as the reliability of each model's performance across varying difficulty levels and task types."}, {"title": "A. Research Questions, Hypotheses, and Metrics", "content": "To systematically explore effectiveness, we structured our investigation around specific research questions, each accompanied by testable hypotheses and relevant evaluation metrics. Table I details these research questions, hypotheses, and corresponding metrics."}, {"title": "B. Variables Selection", "content": "To structure our analysis, we identified key variables that allow us to examine the performance of each AI assistant across different problem types and difficulty levels.\nThe independent variables in this study, which we controlled or varied, include:\n\u2022 LLM-based AI assistants: The four AI models under evaluation-Microsoft Copilot, ChatGPT, Claude, and Perplexity Lab.\n\u2022 Difficulty level of coding problems: Easy, Medium, Hard.\n\u2022 Type of Data Science task: Analytical, Algorithm, Visualization.\nThe dependent variables are the metrics we measured to assess each AI assistant's effectiveness:\n\u2022 Success rate: The percentage of correct solutions generated by each LLM, regardless of the number of attempts.\n\u2022 Running time: Execution time of code for Analytical questions.\n\u2022 Graph similarity scores: Similarity between generated and expected graphs for Visualization questions.\nThese variables connect directly to the research questions and metrics outlined in Table I, allowing us to systematically investigate the impact of each independent variable on the AI models' performance."}, {"title": "IV. EXPERIMENT OPERATION", "content": "The experiment was conducted over the span of two months. For each of the four LLMs, we generated a solution for each of the 100 selected problems, resulting in a total of 400 generated coding solutions. Two researchers manually interacted with the AI assistants by inputting the prompts into their respective interfaces. They then copied the generated code and submitted it to the Stratascratch platform to assess its correctness and functionality. The researchers recorded whether the solution worked as intended and noted any necessary adjustments. Since Stratascratch provides execution time only for Analytical questions and similarity scores for Visualization questions, we collected these specific measurements accordingly.\nThe overall process of our controlled experiment consists of 11 steps, as illustrated in Figure 1:\n1) Select the problem source: We chose Stratascratch [14] as the platform for sourcing data science problems.\n2) Select one problem per task category for prompt engineering: One problem from each data science task category (Analytical, Algorithm, Visualization) was selected to refine our prompt templates.\n3) Prompt Engineering with feedback loop: We performed prompt engineering by iteratively adjusting the prompts and assessing the performance of different LLM versions, creating optimal prompt structures for each task type.\n4) Selection of AI assistants and LLMs: Four AI assistants, each utilizing a different LLM, were selected for the experiment.\n5) Definition of final prompts: The finalized prompt templates were established for each problem type based on the prompt engineering process.\n6) Selection of 100 Data Science problems: We selected 100 data science problems covering various topics across the three task types to ensure a comprehensive evaluation.\n7) Creation of prompts: The selected problems were incorporated into the prompt templates, resulting in 100 tailored prompts.\n8) Execution with AI assistants: Each prompt was executed using the four AI assistants, and the generated Python code was saved.\n9) Submission to Stratascratch platform: The generated code solutions were submitted to the Stratascratch platform interface for evaluation.\n10) Execution and result collection: The code was executed on Stratascratch, and the results were saved into a results dataset.\n11) Data analysis: We compared the performance results of the four LLMs to analyze their effectiveness."}, {"title": "A. Dataset: Selection of Data Science Problems", "content": "For our study, we selected the Stratascratch [14] platform as the source of data science coding problems. Stratascratch is a platform that aggregates real-world data science interview questions from various companies, providing a diverse set of problems that are representative of typical tasks encountered in data science, such as data manipulation, algorithm development, and data visualization.\nStratascratch problems are organized into three difficulty levels (easy, medium, and hard) and three main types, each addressing unique aspects of data science problem-solving:\nAnalytical: These problems involve tasks requiring data analysis and manipulation using tools like pandas and SQL. Topics include data aggregation, filtering, conditional expressions, and data formatting.\nAlgorithm: These challenges focus on computational problem-solving and algorithm development. Topics in this category include array manipulation, linear regression, probability, graph theory, recursion, and optimization techniques.\nVisualization: These problems require the creation of charts and graphs to represent data insights visually. Topics cover distribution analysis, time-series trend analysis, spatial data visualization, and comparison of categorical and numerical data."}, {"title": "B. Prompt Engineering: Transforming Problems into Prompts", "content": "This step started by selecting one problem from each task category-Analytical, Algorithm, and Visualization-for prompt development. These problems were outside our main dataset to avoid biasing the evaluation results. During this phase, we experimented with various prompt structures and observed the models' outputs. Initially, the models often generated code that included datasets or functions not specified in the problem descriptions. To address this, we iteratively refined the prompts by introducing specific instructions and constraints.\nWe tested several LLMs during prompt engineering, including some not selected for the main experiment. Some LLMs, such as Gemini (1.5 Flash), could not produce functional code even for easy problems, despite multiple prompt refinements. Others, like YouChat Pro, was capable but was not included in the final selection to avoid redundancy.\nTo ensure consistency and minimize subjectivity, we automated the conversion of problem descriptions into prompts. This involved creating prompt templates tailored to each problem type-Analytical, Algorithm, and Visualization-which addressed the unique requirements of each category. This section illustrates the prompt template used for Visualization problems. The templates for Algorithm and Analytics tasks are available in [17]. Our automated prompt generation system parsed the problem descriptions and inserted the information into the appropriate template based on the problem type."}, {"title": "C. Code Generation and Execution", "content": "In this experiment, we presented 100 problem prompts to four AI assistants-Microsoft Copilot, ChatGPT, Claude, and Perplexity Labs-generating a total of 400 code solutions (100 problems per assistant). For each problem, a new chat thread was initiated with the AI assistant to ensure no influence from previous interactions. Each AI assistant was given up to three attempts per problem, guided by feedback such as \"Not worked\" (which yielded better results with ChatGPT and Copilot) or \"Wrong answer\u201d (more effective with Claude and Perplexity) to prompt improvements.\nTo evaluate the solutions, we executed them on the Stratascratch platform and recorded the metrics provided by the platform, depending on the type of problem. For visualization problems, for example, the platforms calculates the similarity of the generated graphs with the expected outputs."}, {"title": "V. ANALYSIS AND INTERPRETATION", "content": "This section presents the statistical analysis of data collected during the experiment. The dataset includes information such as problem IDs, the code generated by each LLM, and associated performance metrics, which is available in full for reproducibility in [17].\nFigure 4 provides a general overview of the LLMs' assertiveness across all tasks and difficulty levels. This initial visualization offers a preliminary look at overall trends, while more detailed analyses follow for each research question (RQ)."}, {"title": "A. RQ1: Success Rate of LLMs in Solving Data Science Problems", "content": "As shown in Figure 5, ChatGPT achieves the highest success rate (72%), followed by Claude (70%) and Perplexity (66%), with Copilot at 60%. These percentages represent the proportion of correct solutions generated by each LLM, including those needing minor code edits.\nHypothesis Testing: To assess each model's success rate, we conducted a one-tailed binomial test with baseline thresholds of 50%, 60%, and 70%, determining if each LLM's success rate significantly exceeded these benchmarks. This non-parametric test, suitable for binary outcomes (correct/incorrect), provides insight into each LLM's performance relative to random chance [13]. Additionally, we evaluated whether there was a significant difference in success rates between the LLMs by applying the Friedman test, followed by pairwise Wilcoxon tests where a significant difference was detected.\nAs Table II shows, all LLMs perform significantly above the 50% threshold, confirming baseline effectiveness in solving coding tasks. At the 60% baseline, only ChatGPT and Claude reach statistical significance, suggesting enhanced reliability for typical tasks. No LLM achieves significance at the 70% baseline, indicating limitations in sustaining very high success rates across diverse challenges.\nTo explore differences between LLMs, we applied the Friedman test, which detected significant variation in success rates across models (p = 0.0384). We followed up with post-hoc Wilcoxon pairwise comparisons, identifying a statistically significant difference between ChatGPT and Copilot, with ChatGPT achieving a significantly higher success rate (corrected p-value: 0.0437), as depicted in the heatmap of Figure 6. No other significant differences were observed among models.\nBased on these tests, we conclude:\nFor hypotheses H01 and H01a:\n\u2022 At the 50% baseline, all LLMs exhibit success rates significantly above 50%, supporting the conclusion that each model performs better than random chance in solving data science coding problems.\n\u2022 At the 60% baseline, only ChatGPT and Claude show success rates significantly above this level, indicating that these two models exhibit greater reliability across general coding tasks.\n\u2022 At the 70% baseline, no LLM meets statistical significance, suggesting a possible limitation in achieving consistently high success rates across diverse coding challenges.\n\u2022 Friedman Test and Wilcoxon Post-hoc Test: Significant differences were found between models, with ChatGPT achieving a success rate significantly higher than that of Copilot.\nIn summary, RQ1 indicates that ChatGPT and Claude exhibit the most consistent performance, particularly ChatGPT, which leads in relative success. These findings suggest that ChatGPT and Claude may be preferable for tasks demanding higher success rates, while highlighting the difficulty for LLMS in consistently achieving a 70% success rate across diverse challenges."}, {"title": "B. RQ2: Does the difficulty level of coding problems (easy, medium, hard) influence the success rate of the different LLMs?", "content": "As shown in Figure 7, the success rates of each LLM vary across different difficulty levels. Claude achieves the highest success rate on easy and medium problems, while ChatGPT excels on hard problems, suggesting its robustness with advanced challenges. Copilot consistently shows the lowest success rate across all difficulty levels, indicating a potential limitation in handling more complex tasks.\nHypothesis Testing: Chi-Square tests were performed to evaluate the effect of difficulty level on each LLM's success rate. Results show that difficulty level significantly impacts the success rates of Perplexity and Claude (p < 0.05), suggesting that their performance fluctuates with problem complexity. In contrast, Copilot and ChatGPT demonstrate consistent success rates across all difficulty levels, indicated by non-significant results.\nBased on these tests, we conclude:\nFor the hypothesis HO_2, we reject it for Perplexity and Claude, indicating that difficulty level significantly affects their success rates. For Copilot and ChatGPT, we fail to reject HO_2, suggesting consistent performance across varying difficulty levels.\nTo further explore comparative performance, we conducted the Friedman test across all models at each difficulty level. Although the overall test did not show significant differences in success rates across LLMs for each level, pairwise Wilcoxon tests highlighted a significant difference between ChatGPT and Copilot for hard problems (p = 0.0196), indicating ChatGPT's superior performance on more challenging tasks."}, {"title": "C. RQ3: Does the type of data science task (Analytical, Algorithm, Visualization) influence the success rate of the different LLMs?", "content": "Figure 8 illustrates the success rate of each LLM across different task types. ChatGPT demonstrates the highest success rate in analytical and algorithm tasks, while Perplexity and Claude achieve similar levels in visualization tasks. Although ChatGPT performs particularly well in analytical and algorithm tasks, statistical tests reveal no significant overall success rate differences among the models across task types, except between ChatGPT and Copilot.\nThe Chi-Square test results in Table III show that task type does not significantly impact the success rate for any LLM, with all p-values exceeding the 0.05 threshold. This finding suggests that each model's performance remains relatively stable across analytical, algorithmic, and visualization tasks.\nFor hypothesis HO_3, we fail to reject it for all models, indicating that task type does not significantly impact success rate overall. However, post-hoc comparisons reveal that ChatGPT performs significantly better than Copilot in analytical and algorithm tasks."}, {"title": "D. RQ4: For Analytical questions, do the LLMs differ in the efficiency (running time) of the code they generate?", "content": "For a fair comparison, Figures 9 and 10 include only the results from problems successfully solved by all LLMs, as the platform does not compute execution times for solutions that did not work. Accordingly, Claude has the lowest median execution time, indicating its solutions generally execute faster than the other models' solutions, followed by Copilot and Perplexity. ChatGPT has the highest median execution time, suggesting that on average, it takes longer to execute analytical tasks than the other models. ChatGPT displays the largest interquartile range (IQR), indicating significant variability, whereas Copilot, Perplexity, and Claude have narrower IQRs, suggesting more consistent execution times. This analysis suggests Claude is generally faster and more consistent for analytical tasks, while ChatGPT may offer less predictability in execution time.\nHypothesis Testing: To assess whether these observed differences are statistically significant, we conducted a Kruskal-Wallis test, as the Kruskal-Wallis test is a non-parametric method suitable for comparing the distributions of independent groups, particularly their central tendencies, when data is not normally distributed.\nThe test, conducted using the scipy.stats library, resulted in a Kruskal-Wallis statistic of 0.6947 and a p-value of 0.8744. With a p-value exceeding the significance level of 0.05, we fail to reject the null hypothesis. This suggests that there are no statistically significant differences in the median execution times across the LLMs for Analytical questions.\nFor RQ4, we fail to reject H0_4, indicating that the LLMs do not differ significantly in the efficiency (running time) of the code they generate for Analytical questions."}, {"title": "E. RQ5: For visualization tasks, do the LLMs differ in the quality (similarity) of the visual outputs they produce compared to expected results?", "content": "As depicted in Figures 11 and 12, ChatGPT achieves the highest median similarity score among the commonly solved problems, indicating that its outputs are closest to the expected results. Additionally, ChatGPT displays the narrowest interquartile range (IQR), highlighting its consistency. These findings suggest that ChatGPT delivers more reliable quality in generating visual outputs that closely match the expected results.\nHypothesis Testing: To statistically analyze differences in similarity scores among the LLMs, we conducted a Kruskal-Wallis test.\nKruskal-Wallis Test Results:\n\u2022 Kruskal-Wallis Statistic: 0.8287\n\u2022 p-value: 0.8426\n\u2022 Conclusion: The p-value above 0.05 suggests no statistically significant differences in similarity scores between the LLMs. This indicates that while there are observed differences in mean similarity scores and variability (with ChatGPT achieving the highest mean and most consistent performance), these differences are not statistically significant across LLMs at the 5% significance level.\nBased on these results, we conclude:\nFor hypothesis HO_5, we fail to reject the null hypothesis, indicating that there is no significant difference in the similarity quality of generated visualization outputs among the LLMs."}, {"title": "F. Threats to Validity", "content": "As usual in empirical studies, our study acknowledges several threats that may impact the interpretation and generalization of the results.\n1) Internal Validity: A key concern is the undisclosed nature of the LLMs' training data. Without access to this information, we cannot confirm whether the generated solutions are novel or based on memorized content. Even though we selected new problems from Stratascratch, similar or identical problems might exist in the models' training data, potentially inflating their apparent effectiveness.\nPrompt design is another factor influencing outcomes. As noted by White et al. [18], the formulation of prompts can significantly affect LLM outputs. While we endeavored to use consistent prompts derived from original problem descriptions, variations could lead to different results.\nTo address potential subjectivity in converting problems to prompts, we developed standardized prompt templates for each task type. These templates ensured that all AI assistants received clear, consistent instructions, allowing for a fair comparison of performance.\n2) External Validity: The generalizability of our findings is limited by the scope of problems used. Our study focused on 100 Python coding problems from a single platform, which may not represent the full spectrum of data science tasks. To enhance external validity, future research should incorporate a wider range of problems from multiple sources.\n3) Construct Validity: We did not formally assess the expertise of the researchers conducting the experiment, which could introduce subjectivity, particularly in interpreting and evaluating the AI-generated code. Although guidelines were established for acceptable code modifications\u2014allowing only minor edits to resolve execution issues-differences in coding proficiency among researchers could influence the assessment.\n4) Conclusion Validity: These threats may affect the validity of our conclusions. While our study offers insights into the capabilities and limitations of LLMs in data science code generation, the results should be interpreted with caution. Further research addressing these limitations is necessary to strengthen the confidence in the findings."}, {"title": "VI. DISCUSSION", "content": "Through a series of hypothesis tests, we investigated each model's effectiveness across different problem types and difficulty levels. The findings underscore both the strengths and limitations of these LLMs in addressing data science challenges, providing insights into which models may be most suitable for specific scenarios in data science workflows. The results highlight that:\n\u2022 Success Rate: Empirical evidence from our tests indicates that each LLM exceeded the 50% baseline success rate, confirming effectiveness beyond random chance. At the 60% baseline, only ChatGPT and Claude achieved significantly higher success rates, reinforcing their reliability in general coding contexts. However, none of the models reached the 70% threshold, suggesting limitations in consistently achieving high accuracy across diverse data science task types. ChatGPT achieved the highest overall success rate and performed consistently well on harder questions, with descriptive analysis suggesting strong outcomes in analytical and algorithmic tasks, reflecting its robustness in complex data science scenarios. Claude also demonstrated solid performance, particularly on easier and medium-difficulty tasks, as well as in visualization tasks, indicating versatility across various problem types. Perplexity and Copilot, while showing lower success rates on more complex tasks, displayed consistent performance on simpler tasks, highlighting their potential for straightforward data science workflows.\n\u2022 Efficiency (Execution Time): For analytical tasks, the Kruskal-Wallis test on execution times revealed no statistically significant differences among the models, suggesting that efficiency, in terms of runtime, is relatively comparable across these LLMs. This finding implies that while execution time may vary, it may not be a decisive factor in model selection for tasks where accuracy and complexity are primary concerns. Despite the lack of empirical significance, the median execution times indicate some practical trends: Claude had the lowest median execution time, suggesting it generally runs faster than the other models, followed by Copilot and Perplexity. ChatGPT had the highest median execution time, indicating slower performance on average.\n\u2022 Quality of Output (Image Similarity for Visualization): In visualization tasks, where models were evaluated based on similarity scores to expected outputs, ChatGPT achieved the highest median similarity score. However, statistical tests indicated no significant differences between the models.\n\u2022 Consistency Across Difficulty Levels and Task Types: Empirical analysis reveals that ChatGPT maintains consistent performance regardless of task complexity, providing reliable success rates across both simple and complex tasks. In contrast, Perplexity's and Claude's success rates were significantly influenced by task difficulty, with better outcomes on less complex tasks. Copilot also demonstrated consistency across difficulty levels, though with generally lower success rates than ChatGPT. Additionally, our tests indicate that task type (analytical, algorithmic, visualization) does not significantly impact success rates for any model, suggesting stable performance across different data science task types. This consistency makes ChatGPT a dependable choice when task complexity is uncertain."}, {"title": "VII. CONCLUSION", "content": "This study presents a controlled experiment evaluating the effectiveness of four prominent LLM-based AI assistants-Microsoft Copilot (GPT-4 Turbo), ChatGPT (01-preview), Claude (3.5 Sonnet), and Perplexity Labs (Llama-3.1-70b-instruct) in data science coding tasks. Effectiveness was measured by each model's success rate, execution efficiency, visual output quality, and consistency across difficulty levels and task types.\nWith success rates exceeding 50% for all models, this research provides valuable insights into LLM performance in data science. At the 60% baseline, only ChatGPT and Claude achieved significantly higher success rates, highlighting their reliability in general coding tasks. However, our findings indicate that only ChatGPT consistently maintains performance across different difficulty levels, whereas Claude's success rate is significantly affected by task difficulty, suggesting its performance may vary with more complex tasks.\nNo evidence suggests that task type affects LLM success rates, though ChatGPT (01-preview) significantly outperforms Copilot (GPT-40) for analytical and algorithm tasks. This nuanced understanding of each model's strengths enables more strategic LLM selection tailored to specific needs. Additionally, this study underscores the value of rigorous hypothesis testing in Al evaluation, setting a template for assessing models beyond basic accuracy metrics."}, {"title": "VIII. FUTURE WORK", "content": "Our study opens several avenues for future research to enhance the application of LLMs in data science code generation.\nA. Exploring Complex and Real-World Data Science Tasks\nEvaluating LLMs on sophisticated, real-world data science tasks such as implementing machine learning models with libraries like Scikit-learn or TensorFlow, handling large datasets, and working with unstructured data-could provide deeper insights into their capabilities and limitations. For instance, Nascimento et al. [19] demonstrated the use of an LLM to replace a learning algorithm that involved neural networks optimized through genetic algorithms. While their experiment was preliminary, it highlighted the potential of LLMs to automate complex coding solutions, suggesting that these models could extend beyond basic scripting to more advanced tasks. Exploring tasks like multivariate analysis, time series forecasting, and dynamic optimization could further test LLM proficiency. Testing in practical settings uncovers challenges that controlled experiments may not fully capture.\nB. Expanding Model Diversity and Dataset Coverage\nWe could extend this analysis by integrating additional LLMs and incorporating data science-specific coding challenges from various platforms, such as LeetCode, with tasks like data manipulation, cleaning, and SQL queries. To capture a broader range of data science skills, the dataset could also include non-coding tasks, such as interpretation and analysis questions, as provided by Stratacratch.\nAdditionally, we could integrate recently released questions in Stratascratch that use Polars DataFrame [20] for data manipulation-a high-performance library designed for efficient data handling in Python. We could further expand the dataset by incorporating problems from DS-1000 [4], which includes a diverse selection of data science problems sourced from StackOverflow. Following recommendations by Lai et al. [4], introducing customized variations of existing problems would help reduce model memorization, enhancing the rigor of the evaluation environment.\nC. Expanding Evaluation Metrics\nFuture work could expand LLM evaluation by integrating software engineering metrics like code complexity, maintain-ability, and readability. Code similarity analysis could assess alignment with industry standards, while qualitative reviews by data scientists would add valuable insights, particularly for visualization tasks where image similarity metrics may fall short.\nD. Investigating Prompt Engineering and Ensuring Repro-ducibility\nPrompt engineering significantly influences LLM outputs. Future research should examine how different prompt formulations affect code generation quality and consistency. Employing methodologies where LLMs simulate multiple users [21] could shed light on the impact of varying professional experiences and prompt designs. Addressing the non-deterministic nature of LLMs by controlling parameters like temperature settings could improve reproducibility, leading to more consistent and reliable evaluations.\nE. Exploring Further Research Questions\nEven using the same dataset, many additional questions and hypotheses remain to be explored. For instance, while we assessed the impact of problem difficulty and type on LLM success rates, further analysis could focus on establishing baseline success rates for each problem type and difficulty level. Given the general success baseline of 60%, future research might explore optimal baseline thresholds specific to each type and level of task. Beyond success rates, this dataset also allows for an in-depth exploration of efficiency (execution times) and similarity scores for each difficulty level, providing a more comprehensive view of model performance in diverse task complexity. Additionally, information on the number of attempts (up to three) and instances of minor edits provides data for assessing error types (syntax and logic errors), retry patterns, and the models' adaptability to user feedback. Our dataset also includes specific topics within each question type, allowing for a more granular analysis that could reveal topic-specific strengths or limitations of each LLM."}]}