{"title": "GALA: Graph Diffusion-based Alignment with Jigsaw for Source-free Domain Adaptation", "authors": ["Junyu Luo", "Yiyang Gu", "Xiao Luo", "Wei Ju", "Zhiping Xiao", "Yusheng Zhao", "Jingyang Yuan", "Ming Zhang"], "abstract": "Source-free domain adaptation is a crucial machine learning topic, as it contains numerous applications in the real world, particularly with respect to data privacy. Existing approaches predominantly focus on Euclidean data, such as images and videos, while the exploration of non-Euclidean graph data remains scarce. Recent graph neural network (GNN) approaches can suffer from serious performance decline due to domain shift and label scarcity in source-free adaptation scenarios. In this study, we propose a novel method named Graph Diffusion-based Alignment with Jigsaw (GALA), tailored for source-free graph domain adaptation. To achieve domain alignment, GALA employs a graph diffusion model to reconstruct source-style graphs from target data. Specifically, a score-based graph diffusion model is trained using source graphs to learn the generative source styles. Then, we introduce perturbations to target graphs via a stochastic differential equation instead of sampling from a prior, followed by the reverse process to reconstruct source-style graphs. We feed the source-style graphs into an off-the-shelf GNN and introduce class-specific thresholds with curriculum learning, which can generate accurate and unbiased pseudo-labels for target graphs. Moreover, we develop a simple yet effective graph-mixing strategy named graph jigsaw to combine confident graphs and unconfident graphs, which can enhance generalization capabilities and robustness via consistency learning. Extensive experiments on benchmark datasets validate the effectiveness of GALA. The source code is available at thttps://github.com/luo-junyu/GALA.", "sections": [{"title": "1 INTRODUCTION", "content": "Graph neural networks (GNNs) have recently demonstrated superior performance in graph-level representation learning, which facilitates various graph machine learning problems. Among them, graph classification aims to predict the labels of the whole graphs [1], [2], [3], [4], which has various real-world applications, such as molecule property prediction [5], [6] and social network analysis [7], [8]. Typically, these methods adopt the message-passing mechanism to generate informative node-level representations [9]. These representations are subsequently summarized into graph-level representations via summarization operators for classification.\nIn spite of the promising performance of GNN approaches, they typically operate under the assumption that training and testing graphs come from the same distribution. However, in real-world applications, this is rarely the case. For example, in the discovery of novel molecules in unexplored environments, particularly in response to urgent events, there is often a substantial domain shift from existing training data, resulting in inferior testing performance. One potential solution is to use graph domain adaptation methods, which combine GNN with domain alignment techniques. These methods usually utilize adversarial learning [10] to implicitly minimize distribution differences, or use cross-domain graph contrastive learning [11] for explicit alignment.\nHowever, data privacy has become an important issue in both daily life and scientific research [12]. This is evident in the practices of AI companies, which generally release only pre-trained models rather than the entire dataset. Recent graph domain adaptation approaches typically require complete access to both the source and target graphs [13], [11], which can be impracticable due to privacy regulations. Towards this end, this paper investigates source-free graph domain adaptation, a practical but under-explored problem, which adapts an off-the-shelf source model to unlabeled target graphs without accessing the source graphs. In addition, our scheme is data-efficient, as we sufficiently leverages the open-source pre-trained models.\nIn literature, various works have focused on source-free domain adaptation [14], [15], [16], [17], [18]. These strategies usually employ self-supervised techniques for semantic exploration in the target domains and conduct domain alignment based on training statistics and weight matrices. However, these works often concentrate on normal Euclidean data, such as images and text, while this problem remains under-explored on non-Euclidean graph data. In fact, developing a source-free graph domain adaptation framework meets two significant challenges. (1) How to sufficiently align two graph domains without having access to the source graphs? The graph samples contain complicated structures, including various nodes and topologies [19]. The complicated structures and rich semantics lead to a hierarchical domain shift, which is difficult to capture [20]. Even worse, we are unable to explicitly align two domains by minimizing the distribution divergence, since source graphs are unavailable. (2) How to overcome the label scarcity in the target domain? In real-world applications, we can only access limited unlabeled target graphs, since labeling graph-structured data can be very costly [21]. Previous techniques often adopt pseudo-labeling for additional supervision. Given the complexities inherent in graph structures, these pseudo-labels probably contain errors and are skewed toward the dominant class.\nTo tackle these two challenges, this paper proposes a novel method named Graph Diffusion-based Alignment with Jigsaw (GALA), tailored for source-free graph domain adaptation. The main idea of our GALA is to convert the target graphs into source-style graphs using a graph diffusion model. Specifically, we first employ source graphs to train a score-based graph diffusion model, which connects intricate source graph structures with a prior distribution using a stochastic differential equation (SDE). As shown in Figure 1, we use standard diffusion models to generate new data from a prior distribution, while we perturb target graphs using forward SDE, and then reconstruct source-style graphs using reverse SDE with source styles. In this way, we bridge target graphs with the source domain in the protection of data privacy.\nIn addition, to generate more accurate and impartial pseudo-labels under label scarcity, we introduce adaptive class-specific thresholds, which will progressively increase with the spirit of curriculum learning. Target graphs are regarded as confident if their confidence scores are above the threshold. To exploit unconfident graphs, we provide a simple yet effective graph-mixing method called graph jigsaw, which executes graph clustering based on community detection to select a subgraph from each graph. Subsequently, a confident graph and an unconfident graph exchange subgraphs, much like a jigsaw puzzle. Mixed and original graphs are assumed to have consistent predictions for model generalization and robustness. We conduct comprehensive experiments on benchmark datasets and experiments demonstrate the superiority of the proposed GALA compared with existing baselines.\nTo summarize, the contribution of this work is as follows:\n\u2022 We explore a practical but under-explored problem of source-free graph domain adaptation. To the best of our knowledge, we are the first to explore this problem.\n\u2022 We propose a novel framework named GALA, which transforms target graphs into source-style using a graph diffusion model to mitigate the domain shift in data.\n\u2022 To overcome the label scarcity, we not only introduce adaptive class-specific thresholds with curriculum learning to learn accurate and unbiased pseudo-labels, but also utilize graph jigsaw with consistency learning for model generalization and robustness.\n\u2022 Extensive experiments on a range of benchmark datasets demonstrate the superiority of the proposed GALA compared with various competing baselines. Extensive ablation studies and visualization further validate our superiority."}, {"title": "2 RELATED WORK", "content": "In this section, we briefly review three related topics, i.e., graph classification, graph domain adaptation, and source-free domain adaptation."}, {"title": "2.1 Graph Classification", "content": "Graph neural networks (GNNs) have remarkable performance in graph classification by mapping graph-structured data into embedding vectors [3], [22], [4], [23]. Among various GNNs, message passing neural networks have been the most prevalent [9], [24], which update node representations via neighborhood aggregation. Recently, graph kernels have been incorporated into GNNs to learn from various substructures such as motifs and paths [25]. To generate graph-level representations, various graph pooling operators are proposed using the attention mechanism [26], reinforcement learning [27], and graph clustering [28]. In addition, various semi-supervised methods [29] are proposed for data-efficient graph-level learning [30]. Despite extensive progress, existing methods mostly assume that training and test graphs come from the same distribution, which is usually not the case in real-world applications. Therefore, we adopt diffusion adaptation to transform target graphs into source-style graphs effectively."}, {"title": "2.2 Graph Domain Adaptation", "content": "Graph domain adaptation has attracted substantial interest in recent research [31], [32], [33], [34], [35], [36], [37], [38], [39]. Early efforts typically focus on node-level adaptation [40], [41], [42], [13], where knowledge is transferred from a labeled source graph to an unlabeled target graph. These strategies usually utilize improved GNN encoders and adversarial learning for domain-invariant representations. For instance, UDA-GCN [40] uses dual graph encoders to explore semantics from multiple perspectives and then combines them with the attention mechanism. In contrast to node-level, graph-level adaptation would encounter numerous graphs with complex distribution shifts [43], [44]. The recent CoCo [11] combines graph kernel networks with GNNs and employs a coupled graph contrastive learning framework to tackle domain disparities. StruRW [45] utilizes a structural reweighting method to address the domain shift caused by graph structure and node attributes. SA-GDA [21] uses spectral augmentation and dual graph convolutional networks for graph domain adaptation. UDANE [46] leverages the transferable node representation to transfer knowledge across domains. P-Mixup [47] enforces the target domain progressively moving to the source domain for effective domain transfer. However, these works generally require access to both source and target graphs, which could be impractical in real-world scenarios with strict privacy restrictions. To this end, we investigate source-free graph domain adaptation, which adapts an off-the-shelf model to an unlabeled target domain."}, {"title": "2.3 Source-Free Domain Adaptation", "content": "Source-free domain adaptation seeks to transfer a model trained on a source domain to an unlabeled target domain without access to source data. Existing approaches can be essentially categorized into self-training approaches and domain alignment approaches. Self-training approaches typically employ pseudo-labeling and mutual information maximization to discover semantics from unlabeled target data [14], [48], [15]. For instance, SHOT [15] generates prototypes of classes in the hidden space, which can be used to train the nearest centroid classifier for unlabeled target data. In contrast, domain alignment approaches attempt to explore stored batch statistics and weight matrices for reduced distribution shift [16], [49]. However, these methods are designed for visual data, which performs poorly on non-Euclidean graph data. Towards this end, we propose a diffusion-based source-free graph domain adaptation method, which transforms target graphs back to the source domain via a graph diffusion model."}, {"title": "3 PRELIMINARIES", "content": ""}, {"title": "3.1 Problem Definition", "content": "We denote a graph as $G = (V, E)$ where $V$ denotes nodes and $E$ denotes edges. Each graph is associated with a node attribute matrix $X \\in \\mathbb{R}^{|V|\\times d_f}$ where $d_f$ is the attribute dimension. We denote a labeled source domain as $D^{so} = \\{(G^{so}_i, y^{so}_i)\\}_{i=1}^{N_{so}}$ where $G^{so}$ stands for the $i$-th source graph and $y^{so}_i$ is its label. An unlabeled target domain is $D^{ta} = \\{G^{ta}_j\\}_{j=1}^{N_{ta}}$ where $G^{ta}_j$ denote the $j$-th target graph. While two domains share the same label space, they have different data distributions. In the problem of source-free graph domain adaptation, we initially pre-train neural network models using source data and subsequently learn a model to predict the labels of target graphs. Importantly, to safeguard data privacy, the source data remains inaccessible during the adaptation on the target domain."}, {"title": "3.2 Graph Neural Networks", "content": "Graph neural networks (GNNs) are commonly used to encode graph-structured data based on a message-passing mechanism. In particular, given a graph $G$, we denote the node representation of $v \\in V$ at the layer $k$ as $h^{(k)}_v$ and formulate the updating rule as follows:\n$h^{(k)}_v = AGG^{(k)} (\\{ h^{(k-1)}_u : u \\in S(v) \\}),$   (1)\n$h^{(k)}_{S(v)} = COM^{(k)} (h^{(k-1)}_v, \\{ h^{(k-1)}_u : u \\in S(v) \\}),$  (2)\nwhere $S(v)$ represents the neighboring nodes of $v$ and $h_{S(v)}^{(k)}$ denotes the neighborhood representations for $v$. $AGG^{(k)}$ and $COM^{(k)}$ denote the aggregation and combination operators. Finally, a global pooling function is used to summarize the node representations into a graph-level representation,\n$z = GP (\\{ h^{(K)}_i\\}_{i=1}^{|V|}),$   (3)\nin which $GP(\\cdot)$ is for global pooling. Finally, we utilize an MLP classifier with softmax activation, i.e., $HEAD(\\cdot)$ to generate label distributions:\n$p = \\Phi(G) = HEAD(z),$  (4)\nwhere $\\Phi$ represents the whole graph neural network. In our setting, we would first train an off-the-shelf GNN model using source graphs by minimizing the cross-entropy objective as a preliminary:\n$\\mathcal{L}_{source} = -\\frac{1}{|D^{so}|} \\sum_{G^{so} \\in D^{so}}  \\log p^{G^{so}}[y^{G^{so}}],$  (5)\nwhere $p^{\\Phi}(G)$ denotes the output of the GNN."}, {"title": "4 THE PROPOSED APPROACH", "content": ""}, {"title": "4.1 Framework Overview", "content": "This paper introduces GALA for source-free graph domain adaptation. The model, as illustrated in Figure 2, does not have access to source graphs when learning from target graphs. To solve the problem, we are required to sufficiently align two graph domains and overcome the label scarcity on the target domain. At a high level, GALA transforms target graphs into source-style with a graph diffusion model. Specifically, source graphs are utilized to learn a score-based graph diffusion model, which will be fixed to keep the so source styles. On this basis, we perturb target graphs by forwarding a stochastic differential equation (SDE) and then reconstruct source-style graphs using the reverse process. Moreover, we adapt the off-the-shelf GNN model with class-specific thresholds through curriculum learning. This aids in generating accurate and class-balanced pseudo-labels for each target graph. Then, we introduce our proposed GALA in detail as below."}, {"title": "4.2 Graph Diffusion Model for Domain Alignment", "content": "The principal challenge faced in graph domain adaptation is the alignment of source and target domains. This becomes increasingly challenging in our scenarios since we cannot simultaneously access the source and target data. Our approach is to employ a generative model for source-style graphs that can make precise predictions using the pre-trained GNNs. To achieve this, we introduce an off-the-shelf graph diffusion model in the source domain, with the ability to generate graphs with source style. Subsequently, the target data can be fed into the diffusion model to generate a source-style graph via the reverse process."}, {"title": "4.2.1 Graph Diffusion Model", "content": "We first introduce our graph diffusion model, which reconstructs the graph structure using an SDE [50], [51], [52], [53]. As the most crucial component, the graph structure is expressed via the adjacency matrix as a variable from the timestamp 0 to 1 with an interval $\\Delta t$ when adding noise, i.e., $A(t)$. Then, the SDE for the adjacency matrix is:\n$dA(t) = -\\frac{1}{2}\\beta(t)A(t)dt + \\sqrt{\\beta(t)}dw,$ (6)\nwhere $\\beta(t)$ is a scalar of adding noise and $w$ is a Wiener process indicating random noise. The key of an effective diffusion model is to obtain the score function, i.e., the gradient of the likelihood $\\nabla_{A(t)} \\log p_{0t} (A(t) | A(0))$ where $A(t) \\sim p_{0t}(A(t)|A(0))$. Here, we utilize a score-based GNN $p(A(t), t)$ to approximate the ground truth by minimizing the following objective:\n$\\mathcal{L}_s = \\mathbb{E}_t \\{x(t)\\mathbb{E}_{A(0)} \\mathbb{E}_{A(t)|A(0)} [||p(A(t), t) - \\nabla_{A(t)} \\log p_{0t}(A(t) | A(0)) ||^2]\\},$ (7)\nwhere $x(t)$ is a positive weighting function. By minimizing Eq. 7 on the source domain, we can obtain a graph diffusion model with source styles embedded."}, {"title": "4.2.2 Graph-based Score Function", "content": "Different from diffusion models in computer vision, our approach requires incorporating graph-structural information during propagation. Combining a random walk with a message-passing mechanism, we develop an effective graph-based score function $p(A(t), t)$. Specifically, to better learn structural information, we first discretize the adjacency matrix and then utilize a random walk to generate edge representations $e_{mn}$:\n$\\hat{A}(t) = 1_{A(t)>1/2},$ (8)\n$e^{(0)}_{mn} = \\phi([R_{mn}^1, R_{mn}^2..., R_{mn}^r]),$ (9)\nwhere $R^k_{mn} = (A(t)D^{-1}(t))^k$ indicates the likelihood of starting and end points for every $k$-length walk. $r$ denotes the maximal random walk length and $D^{-1}(t)$ is the degree matrix of $A(t)$. $\\phi(\\cdot)$ is an MLP to map these likelihoods into the embedding space. Besides edge information, we also extract node representations $f^{(0)} = [e_{ii}, d_i]$ where $d_i$ is the one-hot degree vector, and then utilize a message passing neural network with to update $f$ as:\n$f^{(k)}_{S(v)} = AGG^{(k)} (\\{ f^{(k-1)}_j : j \\in S(i) \\})$ (10)\n$f^{(k)}_i = COM^{(k)} (f^{(k-1)}_i, f^{(k)}_{S(v)}).$ (11)\nFinally, we update edge representations as:\n$e^{(k+1)}_{ij} = COM^{(k)} (e^{(k)}_{ij}, f^{(k)}_i, f^{(k)}_j),$ (12)\nwhere $COM^{(k)}$ is a different combination operator to update the edge representation. After stacking $K_d$ layers, we utilize an MLP to generate the estimated adjacency matrix."}, {"title": "4.2.3 Target Graph Adaptation", "content": "By sampling from the prior distribution, the well-trained graph diffusion model can generate new graphs carrying source styles. Instead of sampling from the prior distribution $p_t(A(1))$, GALA adds noise to the target graphs and then uses the reverse process to endow the target graphs with source styles. This process only utilizes the trained diffusion model, effectively addressing the domain shift while protecting the privacy of source graphs.\nIn detail, we perturb the adjacency matrix of the target graph, which generates noise with target semantics,\n$A(t_{recon}) \\sim p_{t_{recon}} (A(t_{recon})|A(0)).$  (13)\nwhere $t_{recon}$ is the starting point of reconstruction. Consequently, we utilize an iterative process to alter the noisy input $A(t_{recon})$ for reconstruction with the reverse process. The reserve SDE can be written as\n$da(t) = \\beta(t)A(t) - \\beta(t)\\nabla_{A(t)} \\log p_t(A(t)) dt + \\sqrt{\\beta(t)}dw,$ (14)\nwhere $w$ denotes a standard Wiener process for backward. On this basis, the updating process of the generated adjacency matrix $\\hat{A} (t - \\Delta t)$ can be:\n$\\hat{A} (t - \\Delta t) \\sim p(\\hat{A} (t - \\Delta t)|\\hat{A} (t)).$  (15)\nIn our implementation, the Euler-Maruyama algorithm is employed to simply the propagation as:\n$\\hat{A} (t - \\Delta t) = \\hat{A}(t) + \\frac{\\beta(t)}{2} A(t) \\Delta t + \\beta(t) \\hat{p}_{se} (\\hat{A}(t), t) + \\sqrt{\\beta(t)} \\sqrt{\\Delta t} z_t,$ (16)\nwhere $z_t \\sim N(0, 1)$. Finally, the adjacency matrix $\\hat{A}(0)$ is for the reconstructed graph, which is denoted as $\\hat{G}^{ta}$. In this manner, we can adapt the target graphs into source-type graphs from a data-centric perspective, which can facilitate accurate semantics exploration using the out-of-shelf GNN."}, {"title": "4.3 Unbiased Pseudo-labeling with Curriculum Learning", "content": "To overcome label scarcity in the target domain, we turn to pseudo-labeling techiniques. Note that after transforming target graphs into source-style graphs, we can use the pre-trained GNN with a fixed threshold to generate target graph pseudo-labels [54]. However, these pseudo-labels from the pre-trained GNN could be biased toward the dominant class. Worse still, GNNs could produce overconfident pseudo-labels due to the potential class competition from normalization operators. To avoid this, we propose unbiased pseudo-labeling with class-specific thresholds for accurate and unbiased pseudo-labels. In addition, a curriculum learning approach is employed, which initially concentrates on all graph samples and then progressively focuses on more reliable ones. In other words, we increase the threshold to reject samples from easy to hard.\nPrimarily, we generate the label distribution $p_{t_a}^{\\Phi}$ using the pre-trained GNN for each source-style target graph $\\hat{G}^{ta}$, i.e.,\n$p_{t_a}^{\\Phi} = \\Phi(\\hat{G}^{ta}),$ (17)\nwhere $\\hat{G}^{ta}$ is the output of the diffusion model. To generate the rigid and class-balanced pseudo-labels, we first calculate the confidence distribution for each class and subsequently establish an adaptive threshold based on the maximum. In formulation, the confidence of each sample is defined as:\n$s^{ta} = \\max_c p_{t_a}^{\\Phi}[c].$ (18)\nThen, the maximum confidence scores for class $c$ are:\n$M_c = \\max\\{ s^{ta}| \\text{argmax}_c' p_{t_a}^{\\Phi}[c'] = c\\}.$ (19)\nThe adaptive threshold for class $c$ is defined as:\n$\\mathcal{T}_c = M_c - \\alpha(e),$  (20)\nwhere $\\alpha(e)$ is shared across different classes. Following the spirit of curriculum learning, we would increase $\\alpha(e)$ linearly according to epoch number $e$, which gradually selects graphs with higher confidence. Consequently, this would yield a collection of confident target graphs:\n$\\mathcal{C} = \\{ G^{ta} | c = \\text{argmax}_c' p_{t_a}^{\\Phi}[c'], s^{ta} > \\mathcal{T}_c \\}.$ (21)\nThe standard cross-entropy loss is minimized in $\\mathcal{C}$:\n$\\mathcal{L}_{sup} = -\\frac{1}{|\\mathcal{C}|} \\sum_{G^{ta} \\in \\mathcal{C}} \\log p^{\\Phi} \\left[ \\hat{y}^{t_a} \\right],$  (22)\nwhere $\\hat{y}^{ta}$ denotes the pseudo-label of $G^{ta}$. Our pseudo-labeling strategy provides a reliable and unbiased optimization process with reduced error accumulation."}, {"title": "4.4 Consistency Learning with Graph Jigsaw", "content": "Despite the effectiveness of pseudo-labeling, there could be a large number of unconfident target graphs without sufficient exploration. To tackle this, we aim to expand the dataset by combining confident and unconfident graphs. Recently, a number of works have been developed on graph Mixup [55], [56], which combines graph representations in the latent space. However, these methods cannot promise to generate realistic graphs and are dependent on GNN encoders. In contrast, we employ a simple yet effective augmentation strategy, graph jigsaw, that works in the graph space. Graph jigsaw uses a community detection algorithm to derive a small subgraph from each graph and then exchange them in a jigsaw puzzle mechanism. Finally, we propose consistency learning to ensure that post-augmentation predictions remain consistent, thereby enhancing robustness and generalization.\nIn detail, we adopt a community detection algorithm, i.e., the Louvain algorithm [57] to partition graphs into diverse clusters. The benefit of this algorithm is unnecessary to decide the cluster number. Then, we randomly choose a cluster, from each confident graph $G^{ta}_s$ and denote the complementary part and subgraph as $\\mathcal{G}_{s,1}$ and $\\mathcal{G}_{s,2}$. In parallel, each unconfident graph $G^{ta}_u$ can be segmented into $\\mathcal{G}_{s,1}$ and $\\mathcal{G}_{s,2}$. We exchange semantics by combining $\\mathcal{G}^{ta}_{s,1}$ and $\\mathcal{G}^{ta}_{u,2}$ to generate augmented graph $\\tilde{\\mathcal{G}}^{ta}_{s,1}$, and $\\mathcal{G}^{ta}_{u,1}$ would be connected to generate graph $\\tilde{\\mathcal{G}}_u$. A detailed example can be seen in Figure 2.\nThen, consistency learning is leveraged to encourage the mixed graph to yield similar predictions. In formulation, the learning objective is written as:\n$\\mathcal{L}_{con} = -\\frac{1}{|\\mathcal{S}|} \\sum_{G^{ta} \\in \\mathcal{S}} \\log p^{\\Phi} \\left[ \\hat{y}^{t_a} \\right] - \\frac{1}{|\\mathcal{U}|} \\sum_{G^{ta} \\in \\mathcal{U}} KL\\left(p^{\\Phi}(G^{ta}_u) || p^{\\Phi}(\\tilde{\\mathcal{G}}^{ta}_u) \\right),$  (23)\nwhere $p^{\\Phi}$ is the label distribution of the augmented view and $\\mathcal{U} = D^{ta} /\\mathcal{S}$ are unconfident graphs. Moreover, pseudo-labels are adopted to supervise confident samples, while the KL divergence of predictions between unconfident graphs and their augmented views are minimized. Our GALA enjoys the regularization from consistency learning to learn from extensive unconfident graph samples, which can further minimize the information loss."}, {"title": "4.5 Summarization", "content": "In a nutshell, the final loss of training our GALA is summarized as:\n$\\mathcal{L} = \\mathcal{L}_{sup} + \\mathcal{L}_{con}.$ (24)\nAs a preliminary, we adopt an off-the-shelf GNN and graph diffusion model trained on the source domain. Subsequently, the loss objective is minimized in the target domain. The overall algorithm is summarized in Algorithm 1.\nComplexity Analysis. We assume that $|D^{ta}$ is the number of target graphs, d is the feature dimension, $|V|_a$ is the average number of nodes, W is the reconstruction steps. The computational complexity of the diffusion is $\\mathcal{O}(K_d |D^{ta}| |V|_a d^2 W)$. The GNN model takes $\\mathcal{O}(K |D^{ta}| |V|_a d^2)$ where K is the layer number. Therefore, the complexity of our GALA is $\\mathcal{O}((K + K_dW)|D^{ta}| |V|_a d^2)$ which is linear to both $|V|_a$ and $|D^{ta}$."}, {"title": "5 EXPERIMENTS", "content": ""}, {"title": "5.1 Experimental Settings", "content": "Datasets. We perform experiments on real-world benchmark datasets with source-free domain adaption settings. In order to demonstrate the performance of our approach in various scenarios, we conducted experiments on both dataset split and cross-dataset source free domain adaptation. In ENZYMES, Mutagenicity, PROTEINS, and FRANKENSTEIN datasets, we split the data, thereby introducing domain discrepancies. Then, we perform source-free domain adaptation across the sub-datasets. As for the COX2 and BZR datasets, we directly conduct source-free domain adaptation on the sub-datasets associated with each of them.\n\u2022 ENZYMES [58] is a bioinformatic data set comprising 600 tertiary protein structures. It constitutes a classic collection of data rooted in the structural information of biological molecules and proteins.\n\u2022 Mutagenicity [59] encompasses a multitude of molecular structures, each intricately paired with its corresponding Ames test data, amounting to a total of 4337 molecular structures.\n\u2022 PROTEINS [60] contain protein data in graph form, where each label indicates whether a protein is a non-enzyme. The amino acids serve as nodes. The edges exist when the distance between two nodes is less than 6 angstroms.\n\u2022 FRANKENSTEIN [61] is a composite dataset that amalgamates the BURSI and MNIST datasets. Each data sample is depicted as a graph, with the chemical atom symbols representing vertices and the bond types representing edges.\n\u2022 COX2 [62]. We engage the COX2 and COX2_MD datasets, consisting of 467 and 303 inhibitors specifically targeting cyclooxygenase-2. Within these datasets, individual graphs serve as graphical representations of distinct chemical compounds. These graphs feature edges annotated with distance values, while the vertex labels are indicative of the atom types found in the compounds.\n\u2022 BZR [62]. We use BZR and BZR_MD datasets, which consist of 405 and 306 ligands designed to interact with the benzodiazepine receptor. The process of graph construction employed in these datasets is analogous to that of the COX2 dataset.\nDomain Settings. This work follows the setting of source-free domain adaptation [48], [15] in the image classification task. The division of the dataset is executed by using graph density, which is defined as the ratio of the number of existing edges to the number of potential edges. It can be calculated as\n$D = \\frac{2|E|}{|V|(|V| - 1)},$ (25)\nwhere |E| is the number of edges and |V| is the number of nodes in the graph G. Within each sub-dataset, the ratio of the training set to the test set is 8:2. Each dataset is split according to the above instructions, leading to domain discrepancies among these sub-datasets and subsequently yield inferior cross-domain performance."}, {"title": "5.3 Performance Comparison", "content": "Table 1, 2, 3 and 4 present the performance of different methods. We observe the following:\n1) Source-free domain adaptation presents a challenging task, as the inferior accuracy in the target domain highlights the necessity of investigating this problem. Both domain shifts and the unavailability of source data impose constraints on the model's capabilities, rendering prior research inapplicable.\n2) Semi-supervised methods (e.g., InfoGraph) perform generally better than source-only methods. Semi-supervised methods can utilize both labeled data (source domain) and unlabeled data (target domain). Note that in practical scenarios, it is often not feasible to access the source data, as only off-the-shelf models are available. Nevertheless, semi-supervised methods exhibit poor stability and performance, as the high standard deviation in experiments. This is attributed to the absence of consideration for domain shift between the source domain and the target domain.\n3) Source-free method PLUE performs better than the other approaches, especially in tackling the challenges posed by more complex domain adaptation tasks. Despite PLUE being the state-of-the-art method for source-free domain adaptation in image classification tasks, it is noteworthy that this method is not devised for graph data and substantial domain shifts. Consequently, there still remains a need for further improvements.\n4) GALA has yielded significant improvements, both within source-free domain adaptation on the split sub-datasets and cross-datasets. Notably, GALA demonstrates an average improvement of 5.3% in cross-dataset experiments when compared to the best-performing compared baseline methods. Furthermore, GALA exhibits improvements, especially in cases of poorly performing domain adaptation sub-tasks.\nThe improvements in GALA can be attributed to two primary factors: data transformation and domain transfer. 1) Diffusion-based data transformation allows target data to transform to the source domain while preserving semantics. This mitigates the adverse effects of domain shift and facilitates better predictions in the source domain. 2) The integration of graph jigsaw and proposed pseudo-label methods significantly helps the model learn more robust representations during the adaptation process, thereby enhancing the adaptation of the model."}, {"title": "5.4 Visualization", "content": "In order to further analyze the effectiveness of GALA, we investigate the two newly proposed modules, i.e. the graph diffusion and the graph jigsaw."}, {"title": "5.5 Ablation Study", "content": "We investigate the effectiveness of GALA in three aspects: the graph diffusion model for adaptation (GDM), unbiased pseudo-labeling with curriculum learning (UPCL), and consistency learning with graph jigsaw (CLGJ). The average classification results on two datasets are summarized in Table 6 and we have four results. Firstly, it can be observed that the whole model M4 achieves the best performance. As such, adaptation is required from both data and models. The collaborative interaction of these three modules can enhance the effectiveness. Secondly, different components can play distinct roles. GDM appears to be a notably effective component (M1), suggesting that data adaptation serves a crucial role in handling distribution shifts, the significant challenge in source-free domain adaptation. Thirdly, the role of UPCL varies in magnitude, depending on the bias present in pseudo-labels. plays a more important role in ENZYMES (M2), indicating a more serious biased of pseudo-labels. Nonetheless, consistently, they all play a significant role in the overall result of source-free domain adaptation. Fourthly, we can observe that CLGJ shows relatively consistent efficacy across both datasets (M3), which represents the gain from consistency learning with graph jigsaw.\nIn addition, to empirically analyze the effectiveness of UPCL, we conducted studies on the ENZYMES (E0\u2192E3), focusing on the confidence set selection in the target domain. As shown in Figure 8, the traditional pseudo-labeling methods can generate biased confidence sets that are skewed toward easier categories, which negatively affects adaptation in the target domain. In contrast, our approach to unbiased pseudo-labeling shows significant effectiveness in preventing such biases."}, {"title": "5.6 Scalability Analysis", "content": "Following to the experimental setup in Section 5.2, we analyze the scalability of our proposed GALA using the PROTEINS dataset, as shown in Figure 9. The experiments are conducted on a single RTX 3090 GPU, where we sample data from different scales of graph data and report the adaptation latency per batch. The results show that our method exhibits high efficiency as graph size increases, outperforming other methods. This highlights the ability of our method to achieve efficiency and high performance when handling large-scale graph data."}, {"title": ""}]}