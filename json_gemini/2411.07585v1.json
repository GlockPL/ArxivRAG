{"title": "Reinforcement Learning Framework for Quantitative Trading", "authors": ["Alhassan S. Yasin", "Prabdeep S. Gill"], "abstract": "The inherent volatility and dynamic fluctuations within the finan-cial stock market underscore the necessity for investors to employa comprehensive and reliable approach that integrates risk manage-ment strategies, market trends, and the movement trends of individ-ual securities. By evaluating specific data, investors can make moreinformed decisions. However, the current body of literature lackssubstantial evidence supporting the practical efficacy of reinforce-ment learning (RL) agents, as many models have only demonstratedsuccess in back testing using historical data. This highlights the ur-gent need for a more advanced methodology capable of addressingthese challenges. There is a significant disconnect in the effectiveutilization of financial indicators to better understand the potentialmarket trends of individual securities. The disclosure of successfultrading strategies is often restricted within financial markets, re-sulting in a scarcity of widely documented and published strategiesleveraging RL. Furthermore, current research frequently overlooksthe identification of financial indicators correlated with variousmarket trends and their potential advantages.\nThis research endeavors to address these complexities by en-hancing the ability of RL agents to effectively differentiate betweenpositive and negative buy/sell actions using financial indicators.While we do not address all concerns, this paper provides deeperinsights and commentary on the utilization of technical indica-tors and their benefits within reinforcement learning. This workestablishes a foundational framework for further exploration andinvestigation of more complex scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "The primary goal for investors in financial markets has always beenthe same, minimize trading risk and maximize profits, in the formof returns. Achieving such an objective involves using a systematicapproach to predict the price of a security and the type of trendsthat are to follow. This is a complex and dynamic task, which makesit hard to understand which factors to include and which ones toexclude when making a decision. The research conducted in thissub-field has been primarily around designing automated tradingsystems which take over the responsibility of fundamental investorwhile also providing greater liquidity into the financial markets.\nBoth supervised and unsupervised learning techniques have beenused in the past, however, they have provided rather unfavorableresults. Reinforcement Learning (RL) has gained attention in thelast several years in this particular application, potentially offer-ing novel solutions that allow for a more accurate price forecast,\nthereby allowing the trading agent to successfully interact with itsenvironment to make optimal decisions. Additionally, the dynamicnature of financial data aligns well with the Markov Decision Pro-cess (MDP), which serves as the basis in addressing reinforcementlearning challenges. The Markov Decision Process helps to evaluatewhich actions the agent should take considering the observationstate and the environment that the agent is interacting with. TheMarkov Decision Process states that the future state will depend onthe current state and action, it assumes all necessary informationis captured in the current state. However, this can be problematicin the sense that time-series data is never static. That is to say theMarkov Decision Process does not capture all relevant informationfrom a historical perspective.\nIn particular, ordinary stock market data consists of open, close,high, low, and volume per security, which is commonly presentedin a sequential format over various time intervals. Given the natureof time-series data and its sensitivity to subtle changes, the intro-duction of technical indicators was proposed to help with creatingmore robust trading strategies. Furthermore, this helps to identifywhat characteristics and patterns bullish and bearish scenarios mayentail. Today, the challenge remains in effectively utilizing theseindicators to reliably forecast future price movements. The presenceof false signals adds another layer of complexity into the mix. Toaddress this, advanced techniques such as dimensionality reduction,feature selection, and extraction have been used through Convolu-tional Neural Networks (CNN), Recurrent Neural Networks (RNN),Long Short Term Memory (LSTM), and more.\nIn the experiments section, the models utilized 20 technical in-dicators as inputs. A more detailed explanation of many of theseindicators can be found in the Technical Analysis book by John J.Murphy [7]."}, {"title": "2 PROBLEM STATEMENT AND MOTIVATION", "content": "Reinforcement learning can be leveraged by individuals to developmore robust trading strategies. The competition that exists todaymakes it hard for individuals to compete due to the limited compu-tational resources. A quantitative hedge fund on the other hand hasa wide array of resources that make it easier for them to conductresearch, develop, and refine strategies. The core motivation of thispaper is primarily due to the lack of knowledge given the limitednumber of publications in this space. Quantitative hedge funds andeven individual investors are hesitant to reveal any trading strate-gies to the general public as this will eliminate their competitiveedge.\nOur work showcases the considerations one needs to take intoaccount when using reinforcement learning with indicators. Thefindings in this paper create a foundation examining key compo-nents such as data pre-processing, back testing (using Backtest-ing.py), reward function, normalization methods, and much more."}, {"title": "3 BACKGROUND OVERVIEW OF THE WORK", "content": "This work is not intended to help individuals with financial gains.A thorough exploration of the limitations are discussed and subjectmatter experts can further develop this work."}, {"title": "3.1 Background: RL in Quantitative Trading", "content": "Reinforcement Learning (RL) has shown significant promise inthe realm of quantitative trading, outperforming other machinelearning methodologies in many cases. In this context, RL agentsare designed to interact with the financial market environment,receiving feedback in the form of rewards or penalties based ontheir trading actions. The primary objective of these agents is tomaximize their cumulative rewards over time, enabling them tomake more accurate and optimal trading decisions.\nA key advantage of RL in quantitative trading is its ability toprocess and analyze vast amounts of financial data, which can leadto more informed trading strategies. [10] However, an overabun-dance of data can sometimes overwhelm the RL agents, leading toless effective decision-making. This necessitates the developmentof advanced methodologies to filter and prioritize relevant informa-tion. Moreover, RL's effectiveness is often measured by its ability topredict future market movements and generate profitable tradingstrategies. The introduction of a discount factor, \u03b3 (Figure 1), helpsaccount for the uncertainty of future rewards, balancing immediategains with long-term potential."}, {"title": "3.2 Background: Literature on Indicators", "content": "A financial indicator in the context of a reinforcement learningagent is a measure that will help provide insights into the futureforecast of a security with respect to price, ultimately helping betterunderstanding trends, changes, and rapid fluctuations in the market.Current research does not provide a structured methodology orapproach into indicator selection, thus these indicators are selectedwithout conducting an in-depth examination of the pros and cons.This limits the performance of the agent, but also fails to providedthe agent with an observation space that is ideal to optimize decisionmaking. Therefore, it is crucial to analyze the impacts of variousindicators on separate trading strategies.\nThe universal issue with indicators stems from the contrast be-tween short and long time frames. Short-term indicators reactquickly to price changes (prone to a larger degree of volatility),"}, {"title": "3.3 Discrete and Continuous Action Spaces", "content": "An overview of discrete and continuous action spaces are impor-tant to better understand the problem statement. [10] A discreteor continuous action space can be used in this specific applicationdepending on the desired objective. A discrete action space can bedefined as a buy or sell action from the RL agent, similar to whatthis work has explored. On the contrary, a continuous action spacecan be applied such that various actions take place defined by arange of 0 to 1. For example, a zero could indicate a sell action and aone could indicate a buy action. Something within the range could"}, {"title": "4 LITERATURE REVIEW", "content": "A study published in 2016 used a deep reinforcement learningapproach without relying on traditional technical indicators. [2]By restructuring a Recurrent Deep Neural Network (RDNN), thestudy enabled simultaneous environment perception and recurrentdecision-making in real-time trading. Deep Learning was utilizedfor feature extraction and other fuzzy learning principles helped tohandle input data. While the solution did achieve profitability andoutperformed conventional strategies in various market conditionsand in certain time periods, the study's evaluation was limited to asmall set of contracts. The study was not able to provide evidenceof the strategy performance in live trading.\nAnother study integrated deep reinforcement learning alongwith sentiment analysis to demonstrate its efficacy in learning stocktrading strategies. [1] This two-prong approach helps to minimizethe uncertainty associated with pure quantitative trading strategiesusing only algorithms. The approach outlines employing DDPG forthe reinforcement learning agent and an RCNN for sentiment anal-ysis to help with enhancing the model's predictive capabilities. Theexperiments conducted do not demonstrate a significant increasein profits when comparing against the market average, but theydo analyze the agent's ability to learn trading strategies. There arealso some limitations in the reward function as it does not take riskfactors into account. The reward function is binary such that theagent receives a reward of 1 if the action taken is profitable and a 0if the action is not. While this may be effective in the short-termthere is no methodology that can prove it's outcome to be effectivein long-term trading.\nAnother study introduced a novel reinforcement learning frame-work for financial portfolio management, one that doesn't rely onfinancial models. [4] The framework is made up of the Ensembleof Identical Evaluators (EIIE) topology, a Portfolio-Vector Mem-ory (PVM), an Online Stochastic Batch Learning (OSBL) scheme,and an explicit reward function designed to maximize returns andmanage risks effectively. Implemented with Convolutional Neural\nNetwork (CNN), a basic Recurrent Neural Network (RNN), andLong Short-Term Memory (LSTM). The framework is tested withthree back-test experiments in a 30-minute trading window withinthe cryptocurrency market. While the results showcased impressivereturns, the study merely focused on the cryptocurrency marketand not traditional financial markets.\nThis approach evaluates nine different environmental variables,which includes open, close, high, low, previous day's closing price,trading volume, turnover, percentage change, and absolute changein stock price. [11] The paper introduces three main states for thetrading agent: Buy, Sell, or Hold. It highlights the effectivenessof the double average trading strategy in assisting agents to takeactions that will be associated with the highest return/reward. Bothshort-term and long-term moving averages are used to determineoptimal buying and selling points in the market. Furthermore, thevast majority of research has showcased models that operate bestwith a long-term returns perspective, the short-term is far moredifficult to deliver optimal results consecutively. [5]"}, {"title": "4.1 Considerations", "content": "Normalizing and scaling different environmental variables createpotential challenges such that each variable would need to con-tribute equally to the agent's decision-making process. Inconsistentscaling can lead to an inaccurate model in the long-term. Changingany of these indicators can also potentially influence the rewards re-ceived by the agent, requiring careful consideration and adjustmentof the reward function itself. [5] This is another example of a paperthat conducted rigorous back testing to ensure optimal results, butthe question lies in determining what the hold state entails.\nWhen we think of reinforcement learning and discrete actionspaces in particular, it should be noted that RL agent doesn't exactlyunderstand the difference between a hold state and a do-nothingstate. The agent does not understand what hold means unless that isspecifically provided as an input to the agent. However, if the agentis not provided this as an input, then this is likely a do-nothingstate, as opposed to a hold state. Do nothing is problematic in thesense that the agent does not understand whether to be in a buyor sell action, or maybe even requires more data before being ableto take make an optimal decision. It becomes even more complexwhen considering if the agent is currently holding any assets or not,as that is essentially a do-nothing state when no assets are held atany given timestep. This is different than the agent already havingassets and deciding to hold those assets. It was unclear whetherthe hold state presented any value as the agent must effectivelyexamine the trade-off between gains and risks for a dynamic andever-changing dataset."}, {"title": "5 METHODOLOGY", "content": "Our work explores potential methods such that reinforcement learn-ing agents can continue to learn and adapt to the environment asthe space changes. We investigate how a reinforcement learningagent can utilize financial indicators in specific market conditionsand trends to enhance overall trading accuracy. By understand-ing the correlation between various indicators, we can selectivelyidentify which indicators provide new information to the RL agent."}, {"title": "5.1 State Space", "content": "The state space in the trading environment consists of historicalprice data and technical indicators. The dimensionality is deter-mined by the length of the historical data window and the numberof indicators. The historical data window is pulled using a YahooFinance API, which helped to specify a period of interest."}, {"title": "5.2 Action Space", "content": "The action space was defined using gym-anytrading, a library thatextends the OpenAI Gym framework to provide environments forsimulating and testing algorithmic RL trading strategies. By default,this library makes the action space discrete such that the actionsconsist of either buy or sell actions. The reinforcement learningagent will always be in the market and taking one of those actionsat all times. The agent does not occupy a hold or do-nothing state."}, {"title": "6 REWARD FUNCTION", "content": "Extending on the definition of the action space above, the rewardfunction calculates the reward based on the action taken by theagent. Specifically, the reward is positive if the action results in atrade that generates profit and negative if the action results in aloss. The study proposed three potential reward functions to trainthe RL agent to trade, which included immediate, terminal, andfinal rewards.\nThe first reward function (immediate), evaluates the log price dif-ference of the current time step by the previous time step. It checksis initiated based on the current position and thenwill calculate the price difference between the current price andthe last trade price. If the current position is a long position, theprice difference is added to the step reward.\nThe equation can be written as:\n$\\log{\\frac{p_i}{p_{i-1}}}$\nThe second reward function initializes the reward to be zero.The reward is specifically calculated when there is a change in theaction (long/short). If the current action is the same as the previous"}, {"title": "6.1 Reward Function Limitations", "content": "Three separate reward functions were discussed in the previoussection. The first reward function was simple, taking the differencebetween the previous price and current price. However, the problemwith giving immediate rewards to the RL model is that it tends tooptimize for the smallest possible gains on each trade, rather thanconsidering long-term benefits. Providing an immediate rewardat every time step makes the model more prone to issues such asover fitting and/or greater variance in learning. Similarly, providingthe model with only a final reward means the model will strugglewith distinguishing a good trade versus a bad trade. This limits themodel's learning ability as it is not optimized to perform well in thefuture. Careful consideration in structuring the reward function iscrucial for developing an RL model, whether it's designed for theshort-term trading or long-term."}, {"title": "7 UTILIZING TECHNICAL INDICATORS", "content": "Our work examines and assesses 20 different technical indicators(using TA-Lib) as RL inputs and normalizes them utilizing a min/-max scalar. The indicators that were used included SMA, OBV,Momentum, Stochastic Oscillator, MACD, CCI, ADX, TRIX, ROC,SAR, TEMA, TRIMA, WMA, DEMA, MFI, CMO, STOCHRSI, UO,BOP, and ATR. The work uses DummyVecEnv to vectorize thecustom environment. This vectorized environment helps the RLalgorithms with the execution of multiple environments, which canlead to faster convergence and greater efficiency. The DummyVe-cEnv is a vectorized wrapper for multiple environments, allowingeach environment to be called in sequence on the Python process."}, {"title": "8 NORMALIZATION METHODS", "content": "Four separate experiments were conducted to measure the patterndifferences between normalizing all indicators with Min-max, Z-Score, Sigmoid, and L2."}, {"title": "8.1 Min-max", "content": "The Min-max method is a popular and widely used generic nor-malization method, where the method scales values of a featurebetween 0 and 1. Simply subtract the minimum value of the featurefrom each value and then divide by the range.\nMin-max Normalization = $\\frac{x - min(x)}{max(x) - min(x)}$"}, {"title": "8.2 Z-Score", "content": "Z-Score Normalization = $\\frac{x - \\mu}{\\sigma}$"}, {"title": "8.3 Sigmoid", "content": "The sigmoid normalization method involves transforming micro-array expression values using a sigmoid function. The functionscales the values between 0 and 1 then adjusts the values basedon their distance from the mean and the variability of the data bysample standard deviation.\nNormalized Value =$\\frac{1}{1 + e^{-(x-\\mu)/\\sigma}}$"}, {"title": "8.4 L2", "content": "The L2 approach will calculate the square root of the squared valuesof the vector element.\nNormalized Vector =$\\frac{vector}{\\sqrt{\\sum_{i=1}^{n}(vector[i])^{2}}}$"}, {"title": "8.5 Window Normalization", "content": "A separate function was created to help retrieve a window of signalfeatures, normalizes them using a log transformation scaled by 10,and then return an observation.\n$S_{ij}$ = log $\\frac{S_{ij}}{S_{00}} \\times 10$"}, {"title": "8.6 Summary", "content": "While our study observed various data pre-processing methods,their impact on pattern recognition and indicator relationships ap-peared to be marginal. However, the notable potential emerged inthe context of utilizing PPO, A2C, and DQN algorithms for trainingthree different models. Investigating how these algorithms inter-pret these relationships and their influence on executing trades isexplored in the next section."}, {"title": "9 INDICATORS CORRELATION MATRIX", "content": "This correlation matrix in Figure 4 provides coverage and insightsinto the relationships (highly correlated or not highly correlated)between the technical indicators in our analysis. Specific patternsinclude RSI showing strong positive correlation with other indica-tors including Momentum, Stochastic Oscillator, MACD, CCI, ROC,CMO, STOCHRSI, and UO. This suggests that these indicators movein the same direction as RSI. Other indicators such as ADX, TRIX,SAR, TEMA, TRIMA, WMA, DEMA, MFI, BOP, and ATR show weakcorrelations with the majority of the other indicators.\nThere were three other normalization methods tested on theseindicators to gain a better insights into how the correlation rela-tionships would change. Results concluded that all three of thesenormalization methods provided similar information as the MinMaxmethod. Figure 4 showcases the level of correlation of indicatorsmeasured, all normalized on the same Min-max scale. It should benoted that this may be an ineffective way to measure the correla-tion relationships among all technical indicators. For example, amomentum indicator may be normalized differently than a volumeindicator."}, {"title": "10 TRAINING DATA", "content": "The initial training dataset was compiled from two years of daydata for APPL from 2020-01-01 to 2022-01-01. This data was usedto train each of the three models, each uniquely utilizing a separatealgorithm. The study zoomed in on Actor-Critic, Proximal PolicyOptimization, and the Deep Q-Network as these algorithms havebeen proven effective in past studies and work well with our discreteaction space. Each of the three models were trained on one milliontime steps.\nThe agent and the environment engage in a series of interactionsthrough a sequence of time steps. Then at each time step t, the"}, {"title": "11 EXPERIMENTS AND RESULTS", "content": "agent will receive the state of the environment and a reward thatis associated with the previous action. The agent will then take anaction."}, {"title": "11.1 Actor-Critic", "content": "As shown in Figure 5, over a two-year period, the A2C approachutilizing the MlpPolicy performed poorly when comparing againstthe models utilizing DQN and PPO. One of the inherent problemswith A2C is that a large sum of data is required for the model to learneffectively. A2C is also prone to suffer from convergence issues,particularly because it is based on gradient descent. This meansthe parameters are updated following the steepest improvement inoverall performance. With respect to time-series data, A2C posesmany problems and its performance is able to verify just that."}, {"title": "11.2 Proximal Policy Optimization", "content": "The Proximal Policy Optimization (PPO) algorithm [9] had troublerecognizing when to initiate a buy action or a sell action. There arecertain instances where it made reasonable trades, but ultimatelyfailed to clearly distinguish a profitable buy or sell action. As shownin Figure 6, the PPO model made the greatest number of tradeswithin the 2-year period compared to the other algorithms."}, {"title": "11.3 Hyper Parameters", "content": "Table 1 showcases the five different hyper parameters beingexamined. To better assess the model's ability to explore and learn,the study tested different learning rates to identify any patterns orchanges with respect to the performance of the training set."}, {"title": "11.4 Learning Rate", "content": "Next, the study tested different learning rates, denoted as a. specif-ically on the DQN model as it was the most stable model duringtraining. Initial tests on training data concluded that utilizing alarger learning rate helped the agent with overall performancecompared to a smaller learning rate. Utilizing a larger learning ratehelps with exploration, however, this learning rate can be modi-fied to become smaller after the model shows greater evidence ofstability."}, {"title": "11.5 Hyper Parameter Results", "content": "Figure 8 showcases utilizing DQN with multiple changes, addingparameters such as learning rate, buffer size, batch size, gamma, andtarget update interval. For example, the batch size is the numberof experiences that are sampled from the buffer and then used toupdate weights during the iterations. After changing the learningrate to le-4, the model began to better distinguish profitable trades.Table 2 reflects this as there was a 13.5 percent return over thesame 2-year period. As the table presents many different results,identifying the return percentage, volatility, Sharpe ratio, and winrate are the most important metrics to examine in our evaluation."}, {"title": "11.6 Results continued", "content": "In the next experiment, the learning rate was modified from le-4to le-2, while all other parameters were left unchanged as shownin Table 3. The difference between these two learning rates is thatle-2 promotes quicker convergence as the learning rate is higherand the network will adjust weights much faster. Conversely, asmaller learning rate, such as le-4 will provide more stable updatesto the weights. Given the updates are smaller, the overall trainingtime does increase when utilizing a smaller learning rate. Betterunderstanding the trade-offs between exploration and exploitationis what the study explored in this portion of the paper."}, {"title": "11.7 INTERVALS & STRATEGY DEGRADATION", "content": "The current work here presents several avenues for enhancementand deeper exploration. Zooming in to one particular area is dataintervals. Further broadening the scope of testing towards more\ngranular data intervals such as hourly, minutes, or seconds can helpcapture more intricate details and provide greater insights into theperformance of each of the trained RL agent's. Additionally, it isimportant to conduct a comparative analysis among the differentmodels in varying market conditions. For example, investigatingwhether DQN outperforms PPO during downtrends and under-standing the underlying reasons behind such performances couldyield better insights. Furthermore, incorporating a diverse rangeof policies from Stable-Baselines3 alongside hyper-parameter tun-ing, exploration/exploitation strategies are crucial steps requiredto refine these models. Models will need to be continuously refinedas the financial markets provide substantial amounts of new dataduring trading hours.\nAnother universal limitation is strategy degradation. While asingle strategy may be back tested using historical data, the futureperformance of a strategy will always be uncertain. Utilizing amore comprehensive approach such as an ensemble approach helpsto minimize the risk associated with training the RL agent on asingle algorithm, but does not adequately address the problem withstrategy degradation."}, {"title": "12 STRATEGY DEVELOPMENT", "content": "When developing RL trading strategies, there are many consid-erations one will need to make. This work provides some of thekey areas to consider when developing RL trading strategies. Anindividual can immediately replicate this work and begin to explorewith different stock tickers. There are many strategies that can bedeveloped and tested using Backtesting.py and setting stop losses,order sizing, and other specific trading features can help simulatea real-world trading environment utilizing RL. One can furtherexpand on using all 200+ technical indicators as inputs and identifythe best normalization schemes for those indicators. For example,normalization will be different for Moving Average ConvergenceDivergence (MACD), a lagging indicator, as the range is between-1.0 and 1. A Max-Min normalization method scales all the databetween 0 and 1, which can become problematic. One should care-fully select indicators and normalize them appropriately to limitresults from becoming distorted."}, {"title": "13 CONCLUSION", "content": "Financial markets are characterized by their inherent volatilityand dynamic fluctuations, making it crucial for investors to adoptcomprehensive strategies that incorporate risk management, mar-ket trends, and movements of individual securities. Current litera-ture aims for robustness and highlighting the need for advancedmethodologies to bridge the gap for reinforcement learning (RL) inquantitative trading. Key contributions of this work for enhancingfoundational framework for reinforcement learning in financialtrading:\n\u2022 Emphasis on Technical Indicators: This work underscores theimportance of financial indicators in enhancing RL agents'decision-making processes. By leveraging these indicators,investors can better understand market trends and makemore informed decisions.\n\u2022 Reinforcement Learning as a Promising Approach: The pa-per advocates for RL as a potent tool in quantitative trading, noting its ability to handle the complexities and dynamicnature of financial data. The alignment of financial data withthe Markov Decision Process (MDP) is particularly notewor-thy, as it allows the agent to make optimal decisions basedon the current state and actions.\n\u2022 Challenges and Opportunities: While RL shows promise, thispaper highlights the challenges, such as the RL agent's poten-tial to be overwhelmed by excessive data, which can impedeits ability to recognize patterns and make favorable decisions.Addressing these challenges is crucial for developing moreeffective trading strategies.\n\u2022 Innovative Methodology: By focusing on the practical ap-plication of financial indicators within RL frameworks, thisresearch provides a foundational framework for improvingthe performance of RL agents. This involves:\n- Data pre-processing and normalization methods\n- Backtesting using tools like Backtesting.py\n- Analyzing the impacts of various indicators on tradingstrategies\n\u2022 Future Directions: The insights from this work serve as astepping stone for further exploration and refinement of RLin financial trading. The goal is to bridge the gap betweentheoretical models and real-world applications, ensuringthat RL agents can make accurate and beneficial tradingdecisions.\nThroughout the experiments conducted, the work leveragedthree different types of algorithms (A2C, PPO, and DQN) to test per-formance and experiment with tuning the hyper parameters to helpincrease stability and performance. Limitations such as strategydegradation and training on a limited set of data do exist. Therefore,it is crucial to test on a wide range of data and to recognize when astrategy is deteriorating."}]}