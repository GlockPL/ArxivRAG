{"title": "Brain-to-Text Decoding: A Non-invasive Approach via Typing", "authors": ["Jarod L\u00e9vy", "Mingfang (Lucy) Zhang", "Svetlana Pinet", "J\u00e9r\u00e9my Rapin", "Hubert Banville", "St\u00e9phane d'Ascoli", "Jean-R\u00e9mi King"], "abstract": "Modern neuroprostheses can now restore communication in patients who have lost the ability to speak or move. However, these invasive devices entail risks inherent to neurosurgery. Here, we introduce a non-invasive method to decode the production of sentences from brain activity and demonstrate its efficacy in a cohort of 35 healthy volunteers. For this, we present Brain2Qwerty, a new deep learning architecture trained to decode sentences from either electro- (EEG) or magneto-encephalography (MEG), while participants typed briefly memorized sentences on a QWERTY keyboard. With MEG, Brain2Qwerty reaches, on average, a character-error-rate (CER) of 32% and substantially outperforms EEG (CER: 67%). For the best participants, the model achieves a CER of 19%, and can perfectly decode a variety of sentences outside of the training set. While error analyses suggest that decoding depends on motor processes, the analysis of typographical errors suggests that it also involves higher-level cognitive factors. Overall, these results narrow the gap between invasive and non-invasive methods and thus open the path for developing safe brain-computer interfaces for non-communicating patients.", "sections": [{"title": "1 Introduction", "content": "The past decade has been marked by rapid progress in brain-computer interfaces (BCIs) for individuals who, after a brain lesion, have lost their ability to speak or communicate. In particular, several patients suffering from anarthria (Metzger et al., 2022; Moses et al., 2021), Amyotrophic Lateral Sclerosis (ALS) (Willett et al., 2023), or severe paralysis (Hochberg et al., 2012) have now been able to produce full sentences via a neuroprosthesis, which records and decodes neural activity from motor regions of the brain. Originally limited to decoding small sets of linguistic features (Herff et al., 2019; Angrick et al., 2019; Anumanchipalli et al., 2019; Moses et al., 2021; Card et al., 2024), words (Metzger et al., 2022), and gestures (Willett et al., 2021), the recent development of AI models has improved the precision and rapidity of brain-to-text decoding to a point of enabling natural language production at rates close to normal speech (Metzger et al., 2023; Wairagkar et al., 2024).\nHowever, such invasive neuroprostheses require a neurosurgical procedure, and thus expose patients to non-negligible risks of brain hemorrhage and infection (Chung et al., 2019; Bullard et al., 2020; Leuthardt et al., 2021; Baranauskas, 2014). Additionally, maintaining functional cortical implants over extended time periods remains challenging (Fekete et al., 2023; Zhou et al., 2024; Yasar et al., 2024). As a result, in their current form, invasive BCIs are not easily scalable for diagnosing or restoring communication in the large groups of non- or poorly-responsive patients (Owen et al., 2006; Claassen et al., 2019).\nNon-invasive BCIs could potentially address this challenge. However, they are usually based on scalp electroencephalography (EEG), whose limited signal-to-noise ratio (Mak and Wolpaw, 2009) requires users to perform complex tasks. For example, EEG-based BCIs typically require individuals to maintain their attention on flickering stimuli (Abiri et al., 2019) or to imagine moving their hand or foot over long time periods (Bodien et al., 2024) \u2013 two tasks known to produce EEG patterns that can be relatively easily detected by a linear classifier. Even so, decoding performance remains moderate. For instance, a public BCI benchmark (Chevallier et al., 2024) using EEG achieves an accuracy of only 43.3% on a four-class classification task with a motor imagery dataset (Yi et al., 2014). In sum, current non-invasive methods fall short of providing a fast and reliable BCI.\nTwo elements could address these challenges. First, magnetoencephalography (MEG), which measures the fluctations of magnetic fields elicited in the cortex, has higher signal-to-noise ratio than EEG (H\u00e4m\u00e4l\u00e4inen et al., 1993; Goldenholz et al., 2009; Baillet, 2017). Second, deep learning models trained to reconstruct natural language from MEG signals in language comprehension paradigms have recently demonstrated major improvements, especially in comparison to EEG (D\u00e9fossez et al., 2023). Together, these elements thus indicate that with modern AI techniques, high-quality MEG signals and natural language tasks could be combined to decode the production of language from non-invasive recordings of the brain.\nIn this study, we introduce Brain2Qwerty, an AI model trained to decode text production from non-invasive recordings of brain activity (Fig. 1). For this, we tasked 35 participants to type briefly memorized sentences on a keyboard, while their brain activity was recorded with either EEG or MEG. We then train Brain2Qwerty, a three-stage deep neural network trained to decode text from these brain signals and evaluate it on both EEG (20 participants, 146K characters, 23K words and 4K sentences) and MEG recordings (20 participants, 193K characters, 30K words and 5K sentences). Note that the present study does not delve into how the brain produce language during typing. This neuroscientific issue is addressed in a companion paper (Zhang et al., 2025)."}, {"title": "2 Results", "content": null}, {"title": "2.1 Linear Decoding", "content": "To verify that our typing protocol leads to the expected brain responses, we first focus on the differences in evoked responses elicited by left- and right-handed key presses (Fig. 2 A-B). The resulting topographies are typical of those associated with motor activity in the cortex (Donner et al., 2009). In addition, we trained a linear ridge classifier per subject to categorize left- vs right-handed responses at each time-sample relative to key presses. The classification accuracy peaks t=40 ms after the key press (Fig. 2). MEG achieves a peak accuracy of 74\u00b11.3% (\u00b1 reports standard-error-of-the-mean (SEM) across subjects), significantly outperforming EEG which achieves 64\u00b10.8% (Mann-Withney U test: p<10-7). We then trained the same model to classify the keys pressed. Character accuracy peaks around the same time, reaching a value of 22\u00b10.8% for MEG and 16\u00b10.5% for EEG, significantly above the chance level (14%). Overall, these findings confirm that the present protocol leads to the expected brain responses to key presses (Pinet and Nozari, 2020)."}, {"title": "2.2 Brain2Qwerty Performance", "content": "We next trained Brain2Qwerty, a new deep learning architecture, to decode individual characters from these M/EEG signals (see Methods in section 4.2) and evaluated both the hands-error-rate (HER) and the character-error-rate (CER). Brain2Qwerty achieves a CER of 32\u00b10.6% with MEG and 67\u00b11.5% with EEG. This performance reflects a substantial difference across recording devices (p<10-8). The best and worst EEG subjects reach a CER of 61\u00b12.0% across sentences and 71\u00b12.3%, respectively. Similarly, the best and worst MEG subjects reach a CER of 19\u00b11.1% and 45\u00b11.2%, respectively."}, {"title": "2.3 Comparing Brain2Qwerty to Baseline Models", "content": "How does Brain2Qwerty perform in comparison to classic baseline architectures? To address this issue we trained a linear model as well as EEGNet a popular architecture used in BCIs (Lawhern et al., 2018) \u2013 with the same approach, and compared their decoding performance to Brain2Qwerty's with a Wilcoxon test across subjects (Fig. 2 E-H). EEGNet outperforms the linear model on both HER (p=0.008) and CER (p<10-4) for MEG - although only on HER for EEG (p=0.03). However, EEGNet remains less effective than our model, which achieves, in comparison, a 1.14-fold improvement in CER with EEG (p<10-5) and a 2.25-fold improvement for MEG (p<10-6), respectively."}, {"title": "2.4 Brain2Qwerty Ablations", "content": "To validate our design choices, we then retrained different ablated versions of our model. Specifically, we re-trained and evaluated (i) the Convolutional Module (i.e. no transformer, no language model) and (ii) the Conv+Transformer (i.e. without Language Model) with the same hyperparameters. The Convolutional Module alone outperforms EEGNet both on EEG (HER: p=0.009, CER: p=0.03) and MEG (HER: p<10-5, CER: p<10-6). Adding the transformer only appears beneficial to CER, both for EEG (p<10-4) and MEG (p<10-6). Finally, the use of a Language Model module leads to an additional improvement of the CER of EEG (p<10-5) and MEG (p<10-6). Overall, these results show that the sentence-level contextualization provided by the transformer together with the leverage of natural language's statistical regularities effectively improves the decoding of individual characters."}, {"title": "2.5 Analyses of Decoded Sentences", "content": "The CER of all sentences for three representative participants recorded with MEG along with two example sentences from these subjects are displayed in Fig. 3. More decoding examples show that several sentences can be perfectly decoded for MEG (Tab. 1, right). Interestingly, some of these examples show that Brain2Qwerty's language model can correct the typographical errors of the participant. For example, EL BENEFICIO SUPERA LOS RIESGOS was perfectly decoded, even though the participants typed: EK BENEFUCUI SYOERA KIS RUESGIS. In comparison, the poor EEG decoding (Tab. 1, left) rarely leads to comprehensible text. Consistent with the statistical effects reported earlier, the examples in Tab. 2 highlight the impact of each module of our model, which together leads to perfect decoding after the language model."}, {"title": "2.6 Impact of Word Type and Frequency", "content": "To test whether Brain2Qwerty decodes words irrespectively of their grammatical type, we evaluated the CER for each part-of-speech (POS) categories separately (Fig. 4A). All POS categories are significantly better decoded than chance, with determiners exhibiting a remarkably low CER (17\u00b11.9%). This phenomenon may be due to two factors: their short length and their high frequency. To formally test this hypothesis, we first analyzed the impact of word frequency on CER (Fig. 4B). The results confirm that frequent words are better decoded than rare words (p=10-7). Interestingly, we verify that the words absent from the training set (out-of-vocabulary, OOV) can also be decoded, although with a relatively poor CER (68\u00b12.1%). Note that this may be due to the fact that on a random partition of train/validation/test splits, OOV words tend to be rare words.\nSecond, we evaluated whether the frequency of each character also impacts decoding. The results show a significant correlation between character frequency and decoding accuracy: R=0.85, p < 10-8 (Fig. 4C)."}, {"title": "2.7 Impact of Keyboard Layout", "content": "If Brain2Qwerty relies on brain activity from the motor cortex (as opposed to some amodal representations of language), then we expect its decoding errors to relate to the specific layout of the QWERTY keyboard. To test this hypothesis, we evaluated the confusion patterns of incorrectly predicted characters by analyzing the keyboard distance between decoded and actual key presses. The results show a strong Pearson correlation between physical distance and confusion rate: R=0.73, p=0.02 (Fig. 5A). To complement this analysis, we further perform a clustering of the last-layer embeddings of the convolutional module using scikit-learn's K-means clustering algorithm. When trained on two clusters, this unsupervised model fully separates the left-hand and right-hand keys. When using up to 10 clusters, the resulting partition remains consistent with the keyboard layout (Fig. 5B). This shows that the spatial layout of the keyboard is well encoded in the high-dimensional representations learned by our model. Overall, these results show that the decoder's mistakes tend to be confused with the keys that are physically close to the target letter on the QWERTY keyboard, suggesting that the decoder primarily relies on motor representations."}, {"title": "2.8 Impact of Typing Errors", "content": "Our protocol does not allow participants to correct their mistakes. Typing errors account for 3.9% of the keystrokes and are present in 65% of the sentences. Furthermore, they are associated with a specific behavior (Fig. 5C): the time taken to type a character as measured by the inter-key interval - doubles between correctly- (50\u00b17ms) and incorrectly-typed characters (114\u00b112ms, p=10-7). This well-known phenomenon (Logan and Crump, 2010) likely reflects hesitation or monitoring of mistakes. To evaluate the impact of typing error on decoding performance, we separately evaluated CER for correctly- and incorrectly-typed characters (Fig. 5D). The results show that with our Conv+Trans model, correctly-typed characters lead to a better CER (38%) than incorrectly-typed characters (65%, p=10\u22127). This result, however, may be partly driven by the contextualization enabled by the transformer. To minimize the impact of sentence context on this error analysis, we thus evaluate the performance of the Convolutional Module (Fig. 5D, right). Again, correctly-typed characters leads to a better CER (52%) than incorrectly-typed characters (71%, p=10-7). This result suggests that decoding performance diminishes when motor processes are inaccurately executed."}, {"title": "3 Discussion", "content": "The present study introduces a new method to decode the production of sentences from non-invasive brain recordings. With MEG, our Brain2Qwerty model achieves a character-error-rate (CER) of 32\u00b10.6% on average across subjects, with the best-performing participants reaching a CER as low as 19%. Our analyses indicate that this decoding benefits from two main factors. First, the use of MEG signals instead of EEG signals resulted in a two-fold improvement. Second, our deep learning architecture, combined with a pretrained character-level language model, substantially outperforms standard models.\nThis work directly stems from the recent progress in decoding natural language from non-invasive recordings of brain activity. In particular, D\u00e9fossez et al. (2023) showed that the perception of natural speech segments could be decoded, from MEG signals, with up to 41% top-10 accuracy (chance level=0.1%). Similarly, Tang et al. (2023) showed that the meaning of perceived sentences could be decoded from functional Magnetic Resonance Imaging (fMRI). Our Brain2Qwerty model shares several elements with these approaches, notably through the use of both a subject layer (D\u00e9fossez et al., 2023) and the use of a language model (Tang et al., 2023), although here restricted to a pretrained 9-gram character-level model. However, these two studies, which focus on decoding the perception of language rather than its production, remained limited in their downstream clinical applications.\nStudies that directly decode text production from non-invasive recordings remain rare. For example, Crell and M\u00fcller-Putz (2024) employed EEG to decode only 10 letters and achieved a character-error-rate of 75.8%, substantially higher than our 68% in the EEG setting with 29 characters. Similarly, EEG-based BCI benchmarks currently emphasize the constraints of poor signal quality and the variability across subjects (Chevallier et al., 2024). Our EEG findings are consistent with these observations. Beyond its mere metric performance, our approach is more efficient than traditional protocols used in non-invasive BCIs, such as the P300-speller (Marchetti and Priftis, 2014), SSVEP (Cheng et al., 2002), and fMRI-localizer (Owen et al., 2006). These methods have historically relied on handcrafted signal processing and shallow classifiers. In contrast, our approach is based on a task that is comparatively easier to use.\nWhile the decoding performance of Brain2Qwerty narrows the gap between non-invasive and invasive BCIs, this gap remains significant. In particular, for speech decoding, Metzger et al. (2023) achieved a rate of 79 words per minute and reported a CER of 15.2% on a dataset with 372 unique words, a vocabulary size comparable to ours. Willett et al. (2021) demonstrated typing speeds of 90 characters per minute with a CER below 6% and an offline CER under 1% when using a correction model. Both approaches rely on intracortical setups and require extensive recording sessions (11 hours per participant for Willett et al. (2021)). Consequently an important research avenue will be to scale and adapt those tasks for MEG experiments.\nSeveral challenges remain to be solved before the present method could be adapted to clinical applications. First, our model does not operate in real time. In particular, the transformer and language model here operate at the sentence level and thus require the trial to conclude before an output can be produced. In addition, the input of Brain2Qwerty requires the MEG segments to be aligned to keystrokes. Overall, a real-time architecture, akin to what is done with electromyography (Sivakumar et al., 2024) and speech recognition (D\u00e9fossez et al., 2023), remains necessary to make the present proof-of-concept applicable in real time.\nSecond, our study was conducted exclusively with healthy participants and with a strictly supervised model. Training indeed requires knowing both the timing and identity of each character. While this setup may be suitable for patients with neurodegenerative conditions who may still possess motor abilities, it is not applicable to locked-in individuals, who are completely unable to perform a typing task on a keyboard. Addressing this challenge may involve either adapting our typing task into an imagination task or designing AI systems capable of robust generalization across participants (Scotti et al., 2024).\nFinally, while MEG outperforms EEG, current MEG systems, including the one used in the present study, are not wearable. This, however, may be resolved by the development of new MEG sensors based on optically pumped magnetometers (OPMs) (Shah and Wakai, 2013; Schofield et al., 2022; Brickwedde et al., 2024).\nOverall, the present results serve as a stepping stone toward developing safer and more accessible non-invasive brain-computer interfaces, ultimately enabling solutions for individuals who have partially or completely lost the ability to communicate."}, {"title": "4 Methods", "content": "We aim to decode language production from non-invasive brain recordings. To achieve this, participants typed sentences on a keyboard while their brain activity was recorded using EEG or MEG. These two devices measure neural activity at a millisecond level, with EEG capturing electric fields and MEG detecting magnetic fields that are both generated by cortical neurons and recorded from sensors distributed across the scalp."}, {"title": "4.1 Experimental Protocol", "content": "Cohort. We recruited 35 healthly adult volunteers to participate in our study at the Basque Center on Cognition, Brain and Language (BCBL) in Spain. This group was composed of 23% of men and 77% of women with an average age of 31.6\u00b15.2 years. All participants were right-handed and skilled at typing. Participants had to type words they were hearing on a keyboard covered by a cardboard box. They were selected if their typing accuracy was above or equal to 80%. They are all native Spanish speakers with no declared prior history of neurological or psychiatric disorders. Their brain activity was recorded with either EEG or MEG for 0.88\u00b10.02 and 0.93\u00b10.01 hours respectively, amounting to a total of 17.7 and 21.5 hours of typing. Five participants took part in both EEG and MEG sessions. One participant was excluded from the MEG study due to the presence of a metallic component during the recording. Participants gave their informed consent and were compensated 12 euros per hour for their participation. This study was approved by the local ethics committee. The same dataset is used to investigate the underlying neural mechanisms driving this task in a companion paper (Zhang et al., 2025).\nDevices. The MEG system is a Megin system with 306 channels (102 magneto-meters and 204 planar gradiometers) recording at a sampling rate of 1 kHz, with an online high-pass filter set at 0.1 Hz and a low-pass filter at 330 Hz. The EEG system is an actiCAP slim from BrainVision\u00b9, with 64 channels (61 EEG channels and 3 ocular channels), a BRAINAMP DC amplifier, and sampled at 1 kHz with an online high-pass filter set at 0.02 Hz.\nStandard keyboards contain electronic and metallic parts that generate artifacts in the MEG. Consequently, we used a custom Magnetic-Resonance-compatible QWERTY keyboard from HybridMojo (LLC), and further modified it to replace the standard metallic springs with non-ferromagnetic silver-spring mechanisms.\nTask. Participants were seated in front of a projected screen (100 cm for MEG and 70 cm for EEG away from their eyes), and with our custom keyboard placed on a stable platform. The distance between M/EEG sensors and the keyboard was 70 cm. This setup ensured participants could type in a natural position. Each trial consisted of three steps: read, wait, type. First, a sentence was presented on the screen, with a rapid serial visual presentation protocol (RSVP; i.e. one word at a time). Each word was presented in a black font, in all upper-case, on a 50% gray background for a random duration between 465 and 665 ms without intervals between words. Second, after the disappearance of the last word of each sentence, a black fixation cross was displayed on the screen for 1.5 seconds. Third, the disappearance of the fixation cross signaled the start of the typing phase. No letters were presented on the screen during typing. Nevertheless, we added minimal visual feedback: a small black square at the center of the screen rotated clockwise by 10 degrees on every keystroke. This feedback ensured that eye movements were not correlated with linguistic features, as it is usually the case in left-to-right reading. Each session consisted of two blocks of 64 sentences each. The first four sentences of each session were training sentences and were different from the 128 unique sentences in the protocol. During the first two training sentences, participants received visual feedback while typing. The other two sentences were used to train them on the task (typing with minimal visual feedback).\nInstruction and stimuli. Participants were instructed to type the sentence that was presented in RSVP as accurately as possible without using backspaces to correct errors and while fixating on the center of the screen. To avoid using diacritic marks that occur in Spanish (e.g., \u00e9, \u00e1, \u00ed, \u00f3, \u00fa, \u00fc, and \u00f1), all words were presented in upper case, and participants were instructed to think of writing in upper case, and without accents. Participants pressed the return key at the end of the trial. Each session consisted of 128 unique Spanish sentences. All sentences were declarative Spanish sentences that contained between 5 and 8 words. They consisted of determiners, nouns, adjectives, prepositions and verbs. This led to a total of 4K sentences and 146K characters across participants for EEG, and 5.1K sentences and 193K characters for MEG."}, {"title": "4.2 Decoder", "content": "The goal for our decoding model is to predict each keystroke based on 0.5s windows of M/EEG signals. Formally, the objective is to learn a mapping from brain recordings to class probabilities: $f : \\mathbb{R}^{s\\times t} \\rightarrow [0,1]^C$, where s represents the number of M/EEG sensors, t denotes the number of M/EEG time samples in the window, and C is the number of available keys. We consider C = 29 distinct classes, which include all the letters of the Latin alphabet as well as three special classes: one for space, another for numbers, and the last for all other special characters."}, {"title": "4.2.1 Architecture", "content": "Our Brain2Qwerty model is composed of three successive modules (Fig. 1).\nConvolutional Module. The first building block is a modified convolutional model originally introduced by D\u00e9fossez et al. (2023). This model consists of four main parts. The first component employs a spatial attention mechanism to encode the relative positions of the sensors. The second introduces a subject-specific linear layer to account for differences between subjects. The third component is a convolutional neural network architecture, comprising 8 sequential blocks that employ a kernel size of 3 and a dilation period of 3, and incorporate skip connections, dropout regularization, and GELU activation functions. Finally, the temporal dimension is pooled with a single-head self-attention layer. Thus, for each window $X \\in \\mathbb{R}^{s\\times t}$, the CNN outputs $z \\in \\mathbb{R}^h$, with h = 2,048. For clarity, this module will hereafter be referred to as Convolutional Module (Conv)."}, {"title": "Transformer Module", "content": "The outputs of the Convolutional Module are then input to a transformer Module (Trans). The receptive field of the transformer is restricted to a unique sentence: $Z \\in \\mathbb{R}^{n\\times h}$. The transformer is used to refine the keystroke predictions by exploiting contextual information and consists of 4 layers with 2 attention heads per layer, maintaining consistent input and output dimensions. Finally, a linear layer projects the transformer's outputs to obtain the logits of each character $\\hat{Y} \\in \\mathbb{R}^{n\\times C}$."}, {"title": "Language Model", "content": "Finally, the output of the transformer $\\hat{Y}$ is input to a language model, so as to leverage the statistical regularities of natural language. For this, we used a 9-gram character-level model constructed using the KenLM library (Heafield, 2011) and pretrained on the Spanish Wikipedia Corpus (Wikimedia, 2005). This library optimizes both speed and memory efficiency by employing a prefix tree structure. At inference time, the language model is input with a sequence of predicted characters, and causally predicts the most likely next character given the preceding predicted ones.\nFormally, let $i \\in [1, 2, ..., n]$ denote the position of the character in a sequence of up to n characters and m = 9 be the order of the m-gram model. The probability of the next character $\\hat{c}_i$ given its preceding predictions $\\hat{c}_{i-m},..., \\hat{c}_{i-1}$ is estimated as a weighted combination between the transformer's logits and the probabilities of the language model:\n$P(\\hat{c}_i | \\hat{c}_{i-m},..., \\hat{c}_{i-1}) = P_{trans}(\\hat{c}_i) + \\alpha \\cdot P_{lm}(\\hat{c}_i | \\hat{c}_{i-m},..., \\hat{c}_{i-1}),$\nwhere $P_{trans}(\\hat{c}_i) = log\\big(softmax\\big([(\\hat{Y})]_i\\big)\\big)$ represents the contribution from the core model, $P_{lm}(\\hat{c}_i | \\hat{c}_{i-m},..., \\hat{c}_{i-1})$ represents the probabilities from the language model, and \u03b1 is the language model weight. We use a beam search of size 30 and a language model weight of 5 found using a grid search, ensuring a tradeoff between inference time and decoding accuracy. This language modeling module outputs a sequence of n characters that aim to regularize the predictions of the transformer with the statistics of natural language. Brain2Qwerty's final prediction is denoted as $\\tilde{Y} \\in \\mathbb{R}^{n\\times C}$."}, {"title": "4.2.2 Training", "content": "The Convolutional and Transformer modules are trained jointly with an unweighted cross-entropy loss in an end-to-end manner across all subjects, with the same hyperparameters for EEG and MEG recordings. This leads to a total of ~ 400M parameters (258M for Conv, 138M for Trans). The model is trained for 100 epochs with a batch size of 128 using the AdamW optimizer (Loshchilov and Hutter, 2019) with early stopping. We use the OneCycleLR scheduler (Smith and Topin, 2018) (weight decay=10-4; pct_start=0.1), warming up the learning rate to 10-4 over the first 10 epochs then decaying linearly. Training was conducted on a single NVIDIA Tesla V100 Volta GPU with 32 GB of memory. The total runtime for training one model is ~12 hours."}, {"title": "4.2.3 Evaluation", "content": "Hand Error Rate (HER). For analysis purposes and comparison with the classic BCI literature (e.g. Lebedev and Nicolelis (2006)), we first consider HER. This metric estimates whether the target and the predicted characters correspond to the same left/right-hand split of the keyboard. Specifically, keys to the left of Y, H, and B are assigned to the left-hand category, while those to the right (including Y, H, and B) are assigned to the right-hand category. For this evaluation, special characters, numbers and space are excluded, as participants may use both hands to type them.\nCharacter-error-rate (CER). CER is based on the Levenshtein distance which quantifies the minimum number of single-character edits required to transform the predicted sequence of keystrokes to the target sentence. A CER of 0 indicates perfect character-level accuracy. The formula for CER is given by $CER = \\frac{(s + d + a)}{n} \\times$, where s, d and a stand for substitutions, deletions, and additions in the n-character sentence, respectively. Unless stated otherwise, we compute the CER at the sentence level using the Levenshtein\u00b3 python library, and report the average CER across sentences."}, {"title": "4.2.4 Model comparison", "content": "Statistics. For statistical comparisons, we employed the non-parametric tests provided by the scipy package (Virtanen et al., 2020). For comparison across models within the same subjects, we used the Wilcoxon test. For comparison across subjects (e.g. for EEG versus MEG), we used the Mann Whitney U test. For time-course decoding, we further applied a false-discovery-rate (FDR) correction for multiple comparisons across time samples.\nBaseline Models. We consider two baseline models. The first one is a linear model implemented using the RidgeClassifierCV function from the scikit-learn library (Pedregosa et al., 2018), which was trained to predict characters from a single time sample of recording for each subject separately. The regularization parameter \u03b1 was selected through a nested cross-validation using a grid search logarithmic spanning from 10-2 to 108. This operation was repeated for each time sample between -0.5 and 0.5 seconds relative to the character onset.\nAs a second baseline, we need a model which uses the same setup as Brain2Qwerty, i.e. where all subjects are trained collectively and the temporal dimension is collapsed. We used EEGNet (Lawhern et al., 2018), a highly parameter-efficient model classically used in BCIs. We trained EEGNet with the same approach as our model. For our experiments, EEGNet is configured with a depth of 6 and a dropout rate of 0.3, which were selected based on a grid search over the validation set. EEGNet does not incorporate a subject-specific linear layer, which may compromise its performance in this particular setup where all subjects were trained collectively. To compute the chance level while accounting for the imbalance across characters, we evaluated the performance of a dummy model that always predicts the most frequent character.\nCompanion paper. We explore how the brain produce a hierarchy of language representations in a companion paper (Zhang et al., 2025). Note that Fig. 1 (left) and Fig. 2B are shared between these two studies."}]}