{"title": "PARAPHRASUS: A Comprehensive Benchmark for Evaluating Paraphrase Detection Models", "authors": ["Andrianos Michail", "Simon Clematide", "Juri Opitz"], "abstract": "The task of determining whether two texts are paraphrases has long been a challenge in NLP. However, the prevailing notion of paraphrase is often quite simplistic, offering only a limited view of the vast spectrum of paraphrase phenomena. Indeed, we find that evaluating models in a paraphrase dataset can leave uncertainty about their true semantic understanding. To alleviate this, we release PARAPHRASUS\u00b9 a benchmark designed for multi-dimensional assessment of paraphrase detection models and finer model selection. We find that paraphrase detection models under a fine-grained evaluation lens exhibit trade-offs that cannot be captured through a single classification dataset.", "sections": [{"title": "1 Introduction", "content": "Our study was set in motion by a serendipitous finding. Like many other researchers at the time, we were benchmarking large language models (LLMs). We had particular interest in the paraphrasing detection task and evaluated LLama and others LLMS (Dubey et al., 2024; Alves et al., 2024) on the paraphrase challenge dataset PAWS-X (Yang et al., 2019). We consider paraphrasing as an important and challenging task, hand in hand with Quintilianus (95, p. 117) who lived 2,000 years ago, and of course, with many papers presented at CL and ML conferences such as Bhagat and Hovy (2013); Zhou and Bhat (2021); Krishna et al. (2023). Interestingly, LLMs appear to under-perform much on PAWS-X. Their performance lies slightly above random coin flip, and they are vastly outperformed by smaller BERT-based models fine-tuned on the training split of PAWS-X.\nTherefore, we investigated whether from analyzing results on PAWS-X, we could make conclusions about the paraphrase detection understanding (or lack thereof) of different models. As what was merely intended as a sanity check at first, we repurposed data from the semantic text similarity (STS) task, and investigated the distribution of predicted paraphrases against the fine-grained similarity labels, where only the highest label of 5 denotes true paraphrases. In fact, moving away from 5, the semantic similarity decreases rapidly, so there clearly are no paraphrases below a certain level (e.g., 4). We confronted different models with this simple test and were surprised by the results (see Figure 1). Suddenly, the predictions of LLMs appeared more reasonable than what we took away from the benchmarking results, whereas the trained models that performed well on PAWS-X appeared na\u00efve and much too over-confident, assigning many paraphrase predictions to sentences that are semantically different. Indeed, \"predicting paraphrases is not easy\" (Vahtola et al., 2022).\nInspired by these pilot findings, we began a deeper empirical exploration into paraphrase notions and paraphrase detection models, the results of which are presented in the remainder of this paper. A main outcome of this is our PARAPHRASUS benchmark that allows the community to test"}, {"title": "2 Related Work", "content": "Paraphrasing. We find early excitement about paraphrases in the rhetorician Quintilianus (95)'s book, written 2000 years ago. Quintilianus suggests paraphrasing as an exercise for classrooms, praising the task as \u201cvaluable in virtue of its difficulty\", the goal is \u201cto rival and vie with the original in the expression of the same thoughts\" (p. 117). A useful taxonomy of different types of paraphrases can be found later in Bhagat and Hovy (2013)'s work, outlining 25 means of creating one, e.g., by metaphor substitution or function word variation (\"Pat gave a nice demo\u201d, \u201cPat's demo was nice\").\nClearly, there are many NLP applications where paraphrasing and understanding paraphrases plays a crucial role (Mallinson et al., 2017). Some of these are semantic search (Reimers and Gurevych, 2019), style transfer (Krishna et al., 2020), machine translation (Madnani et al., 2007), and plagiarism detection (Barr\u00f3n-Cede\u00f1o et al., 2013; Sharjeel et al., 2016). The evaluation of text generation is often interested in whether a candidate and the reference are paraphrases (e.g., Freitag et al., 2020; Thompson and Post, 2020; Nawrath et al., 2024).\nParaphrase Datasets Recognizing the importance of paraphrasing, prior research has developed methods for constructing datasets, including mining large corpora for multilingual paraphrases (Ganitkevitch et al., 2013; Ganitkevitch and Callison-Burch, 2014) and using back-translation to generate 50 million paraphrase pairs (Wieting and Gimpel, 2018). Such large-scale datasets are especially attractive for training models for each of the diverse paraphrase-related applications mentioned above. On the other hand, smaller datasets have been proposed, with a stronger focus on quality and evaluation of paraphrase judgment models. One of the earliest of such datasets is the MRPC corpus (Dolan and Brockett, 2005), this paraphrase classification dataset was constructed by mining the web and filtering the most promising candidates using lexicon-based classifiers. Two non-specialist human judges then labelled the sentence pairs, resulting in a test set of 1,730 sentence pairs, of which 66.5% were labelled positive.\nRecently, the PAWS Wiki dataset (Zhang et al., 2019) was introduced, providing an adversarial dataset for paraphrase classification through the application of word scrambling. They create challenging pairs by controlled word swapping and back-translation, followed by human quality check for fluency and paraphrase semantic. The dataset is split in 49,401 training sentence pairs and 8,000 test sentence pairs. PAWSX (Yang et al., 2019) takes 2000 of the PAWS Wiki test samples and creates a 7-way multilingual test set in German, Spanish, French, Chinese, Japanese, and Korean using human translators. While these minimal pairs are interesting to perform a focused evaluation, its adversarial mode of dataset creation can introduce limitations on the coverage of linguistic formulation variation and hinder generalization abilities. Typically, there is a quantity-quality tradeoff. With PARAPHRASUS we aim to get the best of the"}, {"title": "3 Proposed Benchmark", "content": "Paraphrases come in different flavors, and distinguishing types of paraphrases can be fuzzy (Bhagat and Hovy, 2013). Our benchmark therefore adopts an empirical approach to evaluating paraphrase detection models, according to three desiderata: i) It should reflect a broad spectrum of domains, to lower the impact of any superficial training that may have been acquired by a model. ii) It should reflect a broad spectrum of different flavors of"}, {"title": "3.1 The data: Ten Parts with Three Objectives", "content": "Our benchmark consists of 10 parts, with eight of them repurposed from various types of NLP tasks, such as natural language inference (NLI) and meaning representation annotation guidelines. By distributing the data across different domains, we aim to capture a wide range of paraphrase phenomena. An additional benefit of repurposing existing data could be that it helps alleviate concerns that the LLMs being evaluated may have incorporated this data during training. In addition to the repurposed data, two datasets are genuine paraphrase datasets, one of which is a human-annotated set of particularly challenging examples created by us. The benchmark is divided into three objectives, along which we structure its description:"}, {"title": "4 Classify paraphrases!", "content": "We evaluate the models on three binary paraphrase classification datasets, where each model must determine whether a given pair of sentences constitutes a paraphrase or not.\n\u2022 PAWSX: As mentioned in Section 2, PAWSX is a multilingual dataset containing lexically similar pairs of paraphrases and non-paraphrases.\n\u2022 MRPC: This is one of the first paraphrase datasets that has been released (Dolan and Brockett, 2005).\n\u2022 STS-H: We propose using challenging sentence pairs without relying on adversarial strategies. To achieve this, we select highly similar sentence pairs from the STS dataset, which are annotated on a 5-point Likert scale. Pairs that are highly similar or paraphrased are situated at the upper end of this scale. We select all pairs from 4-5 and carefully create a high-quality set of 338 paraphrase annotations.\nA semantics expert and a student annotated this subset independently, resulting in moderate-to-good Kappa of 0.63. Finally, disagreements were discussed and adjudicated. For the few cases where disagreements could not be resolved (13 out of 56 total), we assigned the non-paraphrase class. This decision was based on the reasoning that it is generally easier to argue that two sentences are not paraphrases than to prove the opposite."}, {"title": "Minimize paraphrase detection!", "content": "In a dataset that contains no paraphrases, the primary objective of the model is to minimize the prediction of paraphrases. However, to ensure the task remains challenging, the paired texts should exhibit some degree of similarity. In this study, we introduce the following five datasets.\n\u2022 SNLI, ANLI, XNLI\u2014 Natural Language Inference (NLI) repurposed: NLI asks whether a hypothesis follows a given premise, labeling the relationship with entailed, neutral, or contradiction. We select those pairs that either stand in a neutral relationship, or in a contradiction. Since typically the premise is longer than the hypothesis, we control this bias by adding the flipped pairs. The resulting data are denoted by SNLI, sampled from the first large-scale NLI dataset (Bowman et al., 2015);\nANLI taken from an adversarial version of the task (Nie et al., 2020); and the cross-lingual XNLI (Conneau et al., 2018) that allows us to study paraphrase detection models in the minimization task in different languages.\n\u2022 STS, SICK- Similarity repurposed: Orthogonally to how we created the STS-H data by using only extremely similar pairs, we now select only pairs that we know that they are not paraphrases. To ensure maximum data quality and lower the possibility of annotation confusions in the upper spectrum of the similarity Likert scale, we select only values 1-3 as negative pairs, excluding the range of 3 to 5. The resulting data are gathered from two datasets (Marelli et al., 2014; Cer et al., 2017)."}, {"title": "Maximize paraphrase detection!", "content": "If we know that a dataset contains only pairs of paraphrases, the natural expectation towards a model would be to maximize its paraphrase detection rate. We created two datasets to test this behavior:\n\u2022 TRUE: We create a dataset of simple, guaranteed paraphrases. The objective for a paraphrase model using this dataset is to correctly identify as many paraphrases as possible. To ensure the simplicity and accuracy of these examples, we leverage Abstract Meaning Representation annotation guidelines (Banarescu et al., 2013).2 AMR has the goal of mapping the same meaning to the same structure, and highlights this in its annotation guidelines by presenting graphs together with a group of semantically equivalent (but structurally different) sentences. For every unique meaning graph, we extract the n-sized set of represented sentences and create \u201c$\\binom{n}{2}$\" of unique paraphrase pairs. As the guidelines succinctly treat diverse phenomena, there can be short phrases, like \u201c20 km\u201d and \u201c20 kilometers\", but most of them are short sentences, e.g., \"The boy desires the girl to believe him.\u201d and \"The boy has a desire to be believed by the girl.\""}, {"title": "4.1 What can we learn from PAWSX?", "content": "We replicate the approach from the original PAWSX study by Yang et al. (2019) and fine-tune a multilingual encoder model, XLM-ROBERTa base (XLM-R) (Conneau et al., 2020), on the English Paws Wiki training set. Using default fine-tuning hyper parameters, we train for 6 epochs. During initial evaluation, we observed considerable variance between different fine-tuning seeds on PARAPHRASUS. To reduce the impact of this variance on our results, we report the average performance based on three high-performing checkpoints, each corresponding to a different seed, yielding a total of nine predictions."}, {"title": "4.2 What can we learn from LLMS?", "content": "Recent research has indicated that LLMs have developed a strong understanding of human language, which can be accessed through natural language prompts. In this study, we prompt LLama3 8B (Dubey et al., 2024) with three different paraphrase notions along with their in-context learning versions.\nExpressing different paraphrase notions. Given that multiple \u201cpersonalities\u201d can be elicited from LLMs (Chan et al., 2024), we leverage this property to design three distinct, minimalistic prompts that vary only in how the concept of \"paraphrase\" expressed. The first prompt (henceforth denoted as P1) is most straightforward, just asking the model to judge whether two sentences are paraphrases. The second prompt (P2) intends to emulate a person with background in semantics, asking the model whether the following two sentences are semantically equivalent. We speculate that this prompt triggers a model mode that very strictly judges the pairs, refraining from assigning a positive label if there is only a minor semantic difference. The third prompt (P3) aims to kindle a person who is primed from"}, {"title": "5 Main Benchmark Results", "content": "The main results on PARAPHRASUS are shown in Table 2.\nWhat model is best overall? Interestingly, our most simple LLM setup (P1) demonstrates the best overall result (21% errors on average) on PARAPHRASUS, outperforming the second best model that uses in-context examples from PAWSX (P1-ICL_K4) by 3 percentage points (pp). While the smaller fine-tuned XLM-R model excels on PAWSX with an error rate of only 15.2% it struggles to generalize across the other objectives and ranks last overall, showing an overall error of 27.5%.\nWhat model is best for specific objectives? Findings on PARAPHRASUS show that each model exhibits unique strengths. When examining the averages for each objective (right side of Table 2), LLama with prompt P3-ICL_K4 (Min!, 0.7% average error) outperforms others in the minimization objective, while LLama with prompt P1 (Max!, 11.8% average error) excels in the maximization objective. In the Classification objective, the trained XLM-R model achieves the best performance. However, it is important to note that this particular average is skewed due to the model's training on PAWSX. In the other two classification datasets, XLM-R is outperformed by LLama models, with up to a 15 percentage point error difference on STS-H (using prompt P2) and a 9.8 percentage point difference on MRPC (using prompt P1).\nZero Shot or In Context Learning In the zero shot experiments, the different paraphrase notions in the prompt lead to different levels of strictness and overall result, whereas in the ICL experiments the three prompts behave and perform similarly. Adding in-context examples to (P1) seems to calibrate the model to be stricter and reduce false positives, but at the cost of a 19.8 percentage point error difference in the maximization average, and a lower overall result."}, {"title": "6 Discussion", "content": "6.1 Can we improve the XLM-R training?\nIn the main results, we observed that fine-tuning a model on the PAWSX training set yields strong performance on the PAWSX test set. However, despite this success, the trained model ranks last overall on PARAPHRASUS. A natural question then is: Why did the model fail to generalize?\nIs it the adversarial character? We hypothesize that this may be due to the adversarial character of PAWSX. While its difficulty should ideally teach the model to handle even the most challenging paraphrase cases, the artificial construction of the dataset may have introduced biases. These biases could have led the model to rely on spurious correlations-similar to the \"Clever Hans\" effect -where the model learns to exploit patterns that do not generalize well, potentially explaining its struggles with broader generalization (Niven and Kao, 2019).\nAblation study A: Adding easy negatives to the training data. To test this hypothesis, we in-"}, {"title": "6.2 Human Paraphrase Understanding Study", "content": "When creating STS-H, we obtained independent annotations from two annotators before agreeing on a final gold standard. One annotator was a researcher with a background in semantics, and the"}, {"title": "6.3 Failure modes of LLM", "content": "When analyzing LLM performance on PARAPHRASUS using datasets containing only certified negatives or certified positives, we observed that while the error rate of the LLMs was low, it was never zero. Table 5 shows three pairs each from the repurposed NLI XNLI dataset (non-paraphrases only), as well as from the AMR-guidelines repurposed data (paraphrases only). In all these cases, all LLM prompts agree on the wrong decision. Interestingly, sometimes, the LLM seems tricked by seemingly simple active/passive variants and assigned a negative prediction, like X saddened Y / Y was saddened by X or a slight positional variation of an adverbial particle (look up X / look X up). At times, the model mistakenly labeled a contradiction as a paraphrase, as seen in the first pair where one sentence calls something a significant threat, while the other denies it was a threat.\nOverall, we should note that LLMs generally show low error rates on the NLI-repurposed data, demonstrating robustness against hard negatives. However, the error rate is higher on hard positives in the AMR-repurposed data. In 92% of cases where one or more LLM prompts make an error, at least one prompt still makes the correct decision. Despite this, our analysis indicates that LLMs can still be misled by relatively simple linguistic variations."}, {"title": "7 Conclusion", "content": "A key contribution of our work is the introduction of the PARAPHRASUS benchmark, which includes sentence pairs from ten domains and covers three paraphrase detection tasks. We are releasing this benchmark to the research community, with the goal of facilitating further exploration of paraphrase phenomena, evaluating paraphrase detection models and zero-shot capabilities of LLMs, and pinpointing specific areas for improvement.\nOur experiments using PARAPHRASUS revealed several interesting insights: 1. None of the tested LLM and classifier setups demonstrated consistently strong performance across the full spectrum of paraphrases captured by the benchmark, highlighting its objective nature and the need for continued system development. 2. Even advanced LLMs as LLama 3 failed to accurately and consistently detect paraphrases \u2013 in some cases, only the passivization of the verb is all it takes to confuse all tested prompt variants. 3. Improving training strategies for smaller, more efficient models is challenging, but not impossible. Specifically, carefully inserting a certain amount of easy negatives into a large training set of adversarial pairs helped to lower the average error. 4. Among all tested LLM prompting strategies, each exhibited specific strengths and weaknesses depending on the particular tasks within the PARAPHRASUS. However, the simplest prompt \u2013 directly asking whether the sentence pairs are paraphrases \u2013 performed best."}, {"title": "Limitations & Future Work", "content": "Our proposed dataset includes text pairs with different levels of semantic and lexical similarity from diverse domains and covers many interesting paraphrasing phenomena. Does it cover all possible phenomena related to paraphrases and paraphrasing? Clearly not. For example, an obvious limitation is its coverage of phenomena in languages other than English. While our dataset does include a total of 25,360 non-English samples they aren't spread across all ten parts of PARAPHRASUS (e.g., the AMR-sourced dataset is only English). Thus, expanding PARAPHRASUS to include more datasets in languages other than English, particularly low-resource languages, appears as a fruitful avenue for future work. We also invite the help of the community for this, and will allow the option to collaboratively work on extending PARAPHRASUS.\nWhile many resources were used to perform our evaluations, regarding our selection of LLMs, we limited ourselves to LLama3 Instruct 8B quantized to 4 bits. Similarly, the basic architecture for our experiments with trained models was limited to XLM-ROBERTabase. We believe that the benchmark can be viewed as an extended reasoning evaluation of LLMs, useful for comparing capabilities of different sizes and architectures. Specifically, among all the many ablations and different setups that we ran for both model types, there was none that showed consistently good performance."}]}