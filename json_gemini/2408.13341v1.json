{"title": "Toward Improving Synthetic Audio Spoofing Detection Robustness via Meta-learning and Disentangled Training with Adversarial Examples", "authors": ["ZHENYU WANG", "JOHN H. L. HANSEN"], "abstract": "Advances in automatic speaker verification (ASV) promote research into the formulation of spoofing detection systems for real-world applications. The performance of ASV systems can be degraded severely by multiple types of spoofing attacks, namely, synthetic speech (SS), voice conversion (VC), replay, twins and impersonation, especially in the case of unseen synthetic spoofing attacks. A reliable and robust spoofing detection system can act as a security gate to filter out spoofing attacks instead of having them reach the ASV system. A weighted additive angular margin loss is proposed to address the data imbalance issue, and different margins has been assigned to improve generalization to unseen spoofing attacks in this study. Meanwhile, we incorporate a meta-learning loss function to optimize differences between the embeddings of support versus query set in order to learn a spoofing-category-independent embedding space for utterances. Furthermore, we craft adversarial examples by adding imperceptible perturbations to spoofing speech as a data augmentation strategy, then we use an auxiliary batch normalization (BN) to guarantee that corresponding normalization statistics are performed exclusively on the adversarial examples. Additionally, A simple attention module is integrated into the residual block to refine the feature extraction process. Evaluation results on the Logical Access (LA) track of the ASVspoof 2019 corpus provides confirmation of our proposed approaches' effectiveness in terms of a pooled EER of 0.87%, and a min t-DCF of 0.0277. These advancements offer effective options to reduce the impact of spoofing attacks on voice recognition/authentication systems.", "sections": [{"title": "I. INTRODUCTION", "content": "I\nN recent years, ASV has been used extensively for per-\nsonal biometric authentication. An ASV system aims to\nverify an identity claim of an individual from their voice\ncharacteristics [1]. Spoofed voice attacks involve an attacker\nwho masquerades as the target speaker to gain access into the\nASV system [2], [3] for use of resources, services or devices.\nIn most cases, the zero-effort imposters can be easily caught\nby a general ASV system, but more sophisticated spoofing\nattacks pose a significant threat to system robustness and\ncredibility [4]. With easy access to biometric information of\npersonal voices, spoofing attacks are inevitable [5]. Such a\npotential system security breach represents a key reliability\nconcern of ASV systems. To address this, an audio spoofing\ndetection system generates countermeasure scores for each\naudio sample to distinguish between genuine (bona-fide)\nand spoofed speech, which allows for deployment of the\nASV system into real-world situations where diverse audio\nspoofing attacks could occur.\nSince 2015 [6]\u2013[9], the ASVspoof community has been at\nthe forefront of anti-spoofing research with a series of bian-\nnual challenges. Their aims are to foster progress in devel-\nopment of audio spoofing detection to protect ASV systems\nfrom manipulation. Existing audio spoofing detection sys-\ntems have been proposed to address two different mainstream"}, {"title": "II. RELATED WORK", "content": "This section presents a detailed investigation of existing\nstate-of-the-art countermeasures for audio synthetic spoof-\ning detection. The countermeasures are broadly classified\ninto three categories: conventional handcrafted features with\nmachine learning classifiers, enhanced deep learning ap-\nproaches, and state-of-the-art end-to-end approaches.\nResearchers in the spoofing detection community have\nworked on finding handcrafted features that reflect artefacts\nbased on phase spectrum, magnitude spectrum, pitch, group\ndelay, etc., to distinguish between spoofed and genuine\nspeech [60]\u2013[65]. Since feature extraction and classifiers are\ntwo main components of spoofing detection systems, the\nGaussian mixture model (GMM), its variants, and support\nvector machine (SVM) classifiers [60], [62], [66]\u2013[68] have\nbeen extensively explored for synthetic spoofing detection.\nHowever, it has been shown that efforts on complex machine-\nlearning-based classifiers are less effective than crafting in-\nformative features [69].\nThe Constant-Q Cepstral Coefficients (CQCC) [61] are ex-\ntracted with the constant-Q transform (CQT), which captures\nmanipulation artefacts that are indicative of spoofing attacks.\nPatel et al. proposed a combination of cochlear filter cepstral\ncoefficients (CFCC) and change in instantaneous frequency\n(IF) (i.e., CFCCIF) to detect genuine versus spoofed speech\n[70]. Additionally, improved classification performance was\nobserved when CFCCIF was combined with Mel frequency\ncepstral coefficients (MFCC) [71] features. For other effec-\ntive features, the high-dimensional magnitude-based features\n(i.e., log magnitude spectrum, and residual log magnitude\nspectrum) and phase-based features (i.e., group delay func-\ntion, modified group delay function, baseband phase dif-\nference, pitch synchronous phase, instantaneous frequency\nderivative) have been introduced in [72].\nArtefacts from synthetic speech reside in different sub-\nbands, therefore, subband processing is explored to extract\ndiscriminative features such as linear frequency cepstral co-\nefficients (LFCC) [14], energy separation algorithm instanta-\nneous frequency cepstral coefficients (ESA-IFCC) [69], and\nconstant-Q statistics-plus-principal information coefficient\n(CQSPIC) [73]. Kaavya et al. proposed another subband\nprocessing approach to perform a hierarchical scattering de-\ncomposition through a wavelet filterbank, then the absolute\nvalues of the filter outputs are used to yield a scalogram [74].\nPrevious studies in image processing have extensively\nexplored the concept of texture. It has been found that texture\ndescriptors such as local binary patterns (LBP) and local\nternary patterns (LTP) are effective for image classification\ntasks. Next, a novel countermeasure based on the analysis\nof sequential acoustic feature vectors using Local Binary\nPatterns (LBPs) was presented to detect LA attacks [67].\n[65] also employed relative phase shift features and MGDF-\nbased features to detect synthesized/converted speech. LBPs\nand MGDF [66] are less successful at differentiating between\ngenuine and spoofed samples because they are susceptible\nto noise, which generates patterns that are similar for both\nclasses.\nRecent efforts have witnessed a rise in utilization of\ndeep-learning-based methods to detect synthesized/converted\nspoofing attacks. Alzantot et al. [75] built three variants of\nResNet [31] that ingested different feature representations,\nnamely, MFCC, log-magnitude STFT, and CQCC. The fu-\nsion of three variants of ResNet (i.e., MFCC-ResNet, CQCC-\nResNet, and Spec-ResNet) has outperformed the spoofing de-\ntection baseline methods (i.e., LFCC-GMM, CQCC-GMM).\nWang et al. [59] used a 135-layer deep dense convolutional\nnetwork to detect voice transformation spoofing. Similarly,\nLai et al. [76] adopted two low-level acoustic features,\nnamely, log power magnitude spectra (logspec) and CQCC\nas input, where the DNN models hinged on variants of all the\nSqueeze-Excitation (SE) network and residual networks were\ntrained to detect spoofed speech. In [77], spectral log-filter-\nbank and relative phase shift features were taken as input\nto train DNN classifiers for synthetic spoofing detection. A\nfive-layer DNN classifier with a novel human log-likelihoods\n(HLL) scoring method was proposed, which was mathe-\nmatically proven to be more suitable for synthetic spoofing\ndetection than the classical LLR scoring method [78].\nConcerning feature engineering, it was found that utiliz-\ning the DNN-based model as a pattern classifier was less\neffective than using it for representation learning followed by\ntraditional machine learning classifiers (i.e., GMM or SVM\nas the classifier). In [79], a spoofing-discriminant network\nwas used to extract the representative spoofing vector (s-\nvector) at the utterance level. Next, the Mahalanobis distance,\nalong with normalization, was applied to the computed s-\nvector for LA attack detection. In [80], bottleneck features\nwith frame-level posteriors were extracted by the DNN-based\nmodel, followed by a standard GMM classifier built with\nacoustic-level features and bottleneck features. In [81], a light\nconvolutional gated recurrent neural network was used to\nextract utterance-level representations, later with extracted\ndeep features, back-end classifiers (i.e. linear discriminant\nanalysis (LDA), and its probabilistic version (PLDA), and\nSVM) performed the final genuine/spoofed classification). A\nsimilar approach has been proposed to learn spoofing identity\nrepresentations [82], where DNN-based frame-level features\nand RNN-based sequence-level features were incorporated in\nmodel training (i.e. LDA, gaussian density function (GDF),\nand SVM) for spoofing detection. Despite the extra computa-\ntion costs introduced by feature engineering, deep-learning-\nbased methods deliver better classification performances than\ntraditional methods.\nToday, end-to-end approaches have achieved state-of-the-art\nperformance in a variety of audio processing applications\n[83], [84]. Bypassing complex feature engineering, the end-\nto-end framework takes raw waveforms as input for represen-\ntation learning and yields corresponding classification deci-\nsions, which encapsulate pre-processing and post-processing\ncomponents within a single network [85], [86]. Muckenhirn\ndeveloped a convolutional neural network-based approach\nto learn features and then built a classifier in an end-to-end\nmanner [87]. A joint architecture called convolutional Long-\nShort Term Memory (LSTM) neural network (CLDNN) with\nraw waveform front-ends was proposed for spoofing detec-\ntion [88], [89]. In the literature [90], an end-to-end system\nbased on a variant of RawNet2 encoder [28] and spectro-\ntemporal graph attention networks [91] was used to learn\nthe relationship between cues spanning different sub-bands\nand temporal segments. Jung et al. developed an end-to-end architecture incorporated with a novel heterogeneous\nstacking graph attention layer, followed by a new max graph\noperation and readout scheme, to facilitate the concurrent\nmodelling of temporal-spectral graph attention for improved\nspoofing detection [92]. Following previous work [93] based\non a variant [94] of differentiable architecture search [95],\nGe et al. explored how to learn automatically the network\narchitecture towards a spoofing detection solution [96]. End-\nto-end approaches represent a new direction of anti-spoofing\nstudy."}, {"title": "III. METHODOLOGY", "content": "This section describes each of the relevant components for\nbuilding our proposed synthetic spoofing detection archi-\ntecture. This comprises the encoder for general represen-\ntation learning, attention modules for feature enhancement,\nand three specific optimization/training schemes to improve\nmodel accuracy, generalization ability to unseen attacks, and\nrobustness.\nInstead of using hand-crafted features as inputs [97], the\nRawnet2-based model operates directly upon the raw wave-\nform without preprocessing techniques [90], [98]. A variant\nof the RawNet2 model was introduced in [29] for the speaker\nembedding learning and applied subsequently for building\nspoofing detection systems [90], [99]. Here, we adopt that\nmodel to extract high-level representations \\(F\\in \\mathbb{R}^{C\\times S\\times T}\\)\n(C, S, and T are the number of channels, spectral bins, and\nthe temporal sequence length, respectively) from raw wave-\nforms. According to the literature [29], [30], [99], approaches\nequipped with a bank of sinc filters show superior effective-\nness in terms of both convergence stability and performance.\nTherefore, a sinc convolution layer is employed for front-end\nfeature learning. The sinc layer transforms the raw waveform\nin the time domain using a set of parameterized sinc functions\nthat are analogous to rectangular band-pass filters [100],\n[101]. Each filter within the filterbank possesses its center\nfrequencies based on a mel-scale. Cut-in and cutoff frequen-\ncies are fixed to alleviate over-fitting to training data due to\ntraining data sparsity or rather limited genres of different\nspoofing attacks (only 6 for the training and development\npartitions from the ASVspoof 2019 LA database).\nThe output of each filter is treated as a spectral bin,\nsubsequently, the output of the sinc layer is transformed into a\n2-dim time-frequency representation by adding a channel di-\nmension. The result is fed into stacked 2-dim residual blocks\n[31] with pre-activation [102] for high-level feature learning.\nEach residual block is comprised of a batch normalization\nlayer [103], a 2-dim convolution layer, scaled exponential\nlinear units (SeLU) [104], and a max pooling layer for\ndown-sampling. The specifics of our model configuration are\nsummarized in Tab. 1.\nThe fundamental building block of convolutional neural net-\nworks (CNNs) serve as the convolution operator, allowing\nnetworks to learn informative features by combining spa-\ntial and channel-wise information within the local receptive\nfields at each layer. Plug-and-play attention modules [33],"}, {"title": "B. ATTENTION MODULES", "content": "[34], [105] as an effective component can refine the inter-\nmediate feature maps within a CNN block, so as to boost\nthe model capacity. Researchers are of interest to formulate\neffective attention modules for feature enhancement, which\nenable networks to improve the quality of channel-wise or\nspatial encoding throughout the feature hierarchy.\nSqueeze-and-Excitation (SE) module can be integrated into\nresidual blocks for learning informative representations by\nthe insertion after a non-linearity following a convolution\n[33]. The module as a computational unit is comprised of\ntwo fully connected layers to learn the importance of each\nchannel, which is built on transforming by first compressing\nand then expanding the full average channel vector. Given\nthe intermediate feature map \\(x \\in \\mathbb{R}^{C\\times S\\times T}\\) of the Residual\nblock as input, the SE module first calculates the channel-\nwise mean statistics \\(e \\in \\mathbb{R}^{C}\\). Here, the c-th element of e is\n\\[e_c = \\frac{1}{ST} \\sum_{i=1}^{S} \\sum_{j=1}^{T} x_{cij},\\]\nwhere C, S, and T represent channel, frequency, and time\ndimensions. The SE module then scales this channel-wise\nmean by two fully connected layers to obtain the channel-\nwise attention weights s of the various channels:\n\\[s = \\sigma(W_2f(W_1e + b_1) + b_2),\\]\nwhere W and b denote the weight and bias of a linear layer.\nAlso, f(\u00b7) is the activate function of the rectified linear unit\n(ReLU), and \u03c3(\u00b7) is the sigmoid function.\nThe convolutional block attention module (CBAM) [34] ex-\ntends channel-wise attention into two separate dimensions,\nreferred to as the channel and spatial (frequency-temporal)\nattention modules. Next, the input feature maps are multi-\nplied by attention maps for adaptive feature refinement. With\nthe merits of a lightweight and effective module, the CBAM\ncan be integrated into any CNN-based architecture, which\nhas previously been successfully applied for speaker verifi-\ncation [106]. Given the input feature map \\(x \\in \\mathbb{R}^{C\\times S\\times T}\\), the\noverall attention process sequentially infers a 1-dim channel\nattention map \\(M_c \\in \\mathbb{R}^{C\\times 1\\times 1}\\) and a 2-dim frequency-\ntemporal attention map \\(M_{ft} \\in \\mathbb{R}^{1\\times S\\times T}\\). The feature refine-\nment process is formulated as,\n\\[x' = M_c(x) \\otimes x, \\\\ x'' = M_{ft}(x') \\otimes x',\\]\nwhere \u2297 denotes element-wise multiplication. The final re-\nfined output x'' is obtained by broadcasting the attention\nvalues (i.e., \\(M_c\\) and \\(M_{ft}\\)) along with the frequency-temporal\nand channel dimensions accordingly.\nInspired by attention mechanisms in the human brain based\non certain well-known neuroscience theories [107], the sim-\nple attention module (SimAM) [35] is proposed to opti-\nmize an energy function for encapsulating the relevance of\neach neuron. The parameter-free simple attention module\n(SimAM) has proven to be flexible and effective in enhancing\nthe learning capabilities of convolution networks with negli-\ngible computational costs [35], and subsequently applied in\nspeaker verification [108]. By optimizing an energy function\nto capture the significance of each neuron, it generates 3-dim\nattention weights for the feature map in a convolution layer.\n\\[e_t(W_t, b_t, y, x_i) = (y_t - \\hat{y}_t)^2 + \\frac{1}{M-1} \\sum_{i=1}^{M-1} (y_o - \\hat{y}_i)^2.\\]\nGiven the feature map \\(x \\in \\mathbb{R}^{C\\times S\\times T}\\) in a single channel, t\ndenotes the target neuron. xi is other neurons, where i is the\nindex over the frequency-temporal domain and M = S \u00d7 T\nis the number of neurons for each channel. Here, \\(\\hat{y}_t = W_t t +\nb_t\\) and \\(\\hat{y}_i = W_t x_i + b_t\\) are linear transforms for t and xi.\nEq. 4 obtains its minimal value when \\(\\hat{y}_t = y_o\\) and \\(\\hat{y}_i = y_t\\).\nConsidering yo and yt as two distinct values, for simplicity,\nbinary labels (i.e., 1 and -1) are assigned to yo and yt in the\nfinal energy function with a regularizer,\n\\[e_t(W_t, b_t, y, x_i) = \\frac{1}{M-1}\\sum_{i=1}^{M-1}(-1-(W_t x_i + b_t))^2 + (1 - (W_t t + b_t))^2 + \\lambda W_t^2.\\]\nThere are extensive computational resources needed to\noptimize each of the neuron's attention weights using a\ngeneral optimizer such as SGD. Fortunately, a closed-form\nsolution can be leveraged to derive the transform's weight\nWt and bias bt with optimal energy. Specifically, the minimal\nenergy of a neuron x in an input feature map \\(x \\in \\mathbb{R}^{C\\times H\\times W}\\)\nis formulated as:\n\\[e_x^* = \\frac{4(\\sigma^2 + \\lambda)}{(x-\\mu)^2 + 2\\sigma^2 + 2\\lambda},\\]"}, {"title": "C. BINARY CLASSIFICATION LOSS", "content": "where \\(\\mu = \\frac{1}{HXW} \\sum_{i=1}^{HXW} x_i, \\sigma^2 = \\frac{1}{HXW} \\sum_{i=1}^{HXW} (x - \\mu)^2\\), and \u03bb is a hyper parameter. Each neuron within a channel\nshares the statistics \u03bc and \u03c3, which hence significantly low-\ners computation costs. Given that research in neuroscience\ndemonstrates an inverse relationship between the energy of\nex and the significance of each neuron x [109], the refinement\nprocess of a feature map can be written as,\n\\[x = \\sigma(\\frac{1}{E}) \\otimes x,\\]\nwhere E groups all energy values of ex, with \u03c3(\u00b7) denoting\nthe sigmoid function. In this study, we inserted a SimAM\nafter the first convolution layer in each residual block of the\nbase model.\nIn this section, the fundamental cross-entropy loss with\nsoftmax and angular margin-based losses are discussed, and\nthe weighted additive angular margin loss is proposed for\nour binary classification. During training, each mini-batch\ncontains N utterances from either spoofed or genuine speech,\nwhose feature embedding vectors are \\(x_i \\in \\mathbb{R}^D\\), with the\ncorresponding spoofing identity labels being yi, where 1 <\ni \u2264 N and \\(y \\in \\{0,1\\}\\) (i.e., 0 denotes spoofed speech and 1\nrepresents the genuine).\nThe Softmax loss is comprised of a softmax function inte-\ngrated with a multi-class cross-entropy loss, which is formu-\nlated as,\n\\[L_s = -\\frac{1}{N} \\sum_{i=1}^{N} y_{i} log(\\frac{e^{W_{y_i}^T x_i}}{\\sum_{c=1} e^{W_{c}^T x_i}}) \\\\ = -\\frac{1}{N} \\sum_{i=1}^{N} log(\\frac{e^{W_{y_i}^T x_i}}{e^{W_{y_i}^T x_i} + e^{W_{1-y_i}^T x_i}}),\\]\nwhere W represents the weight vector of the last layer of the\nencoder trunk, and \\(W_0, W_1 \\in \\mathbb{R}^D\\) are the weight vectors\nof the spoofed class and genuine class, respectively. \\(W_{y_i}\\) is\nthe weight of the i-th sample with label yi. This loss function\nmerely computes penalties for classification error and does\nnot explicitly encourage intra-class compactness or inter-\nclass separation.\nThe softmax loss can be reformulated so that the posterior\nprobability only hinges on the cosine value of the angle\nbetween the weights and input vectors. With normalized unit\nvectors of W and y, the loss function termed as Normalized\nSoftmax Loss (NSL), is written as,\n\\[L_N = -\\frac{1}{N} \\sum_{i=1}^{N} log(\\frac{e^{||W_{y_i}|| \\cdot ||x_i|| cos(\\theta_{y_i},x_i)}}{\\sum_{j=1}^{N} e^{||W_j|| \\cdot ||x_i|| cos(\\theta_{j},x_i)}}) \\\\ = -\\frac{1}{N} \\sum_{i=1}^{N} log(\\frac{e^{||W_{y_i}|| \\cdot ||x_i|| cos(\\theta_{y_i},x_i)}}{\\sum_{j=1} e^{||W_j|| \\cdot ||x_i|| cos(\\theta_{j},x_i)}}).\\]\nwhere \\(\\cos(\\theta_{y_i},x_i)\\) denotes the dot product of normalized vec-\ntor W (\\(||W|| = 1\\)) and x (\\(||x|| = 1\\)). Next, \\(x_o = W_{y_i} x_i +\nb_y\\) describes the final linear transformation, where\n\\(x_i \\in \\mathbb{R}^D\\) is the penultimate linear layer's output (i.e., D-dim\nembedding) of the i-th sample with label yi and \\(x_o \\in \\mathbb{R}^2\\) is\nthe last linear layer's output. Finally, \\(W_{y_i} \\in \\mathbb{R}^D\\) denotes the\nyi-th column of the weight \\(W \\in \\mathbb{R}^{D\\times 2}\\) and by is the bias\nterm. This bias term \\(b_{y_i}\\) is set to 0 here, therefore, the linear\ntransformation is reformulated as \\(W x_i = ||W_{y_i}|| \\cdot ||X_i||\ncos d\\theta_{y_i}, x_i\\), where \\(\\theta_{y_i},i\\) is the angle between the weights and the\ninput feature. Likewise, this loss function has the same issue\nas the Softmax loss in that it only computes penalties based\non classification error. This results in embeddings which are\nlearned by the NSL as not being sufficiently discriminative.\nModifications are proposed here to mitigate this issue, where\nan additive margin is introduced with the AM-Softmax to\nmake the embedding space of the two classes close to their\nweights \\(W_0 - W_1\\) and \\(W_1 - W_0\\). The formula for the AM-\nSoftmax (CosFace) can now be written as,\n\\[L_C = -\\frac{1}{N} \\sum_{i=1}^{N} log(\\frac{e^{s(||W_i|||x_i||cos(\\theta_{y_i},i)-m)}}{e^{s(||W_i|||x_i||cos(\\theta_{y_i},i)-m)} + e^{s||W_{T-y_i}|||x_i||cos(\\theta_{1-y_i},i)}}) \\\\ = \\frac{1}{N} \\sum_{i=1}^{N} log(1 + e^{s(m-cos(\\theta_{y_i},i)+cos(\\theta_{1-y_i},i))}),\\]\nwhere s denotes a hyper-parameter that rescales up the gra-\ndient instead of the numerical values becoming too small\nwithin the training phase, which helps to expedite opti-\nmization. Feature maps are rescaled by s, where they are\naccordingly projected onto a hypersphere with radius s.\nFurthermore, an additive angular margin penalty m be-\ntween Wy and x\u2081 is also incorporated into the equation in\norder to simultaneously enhance the intra-class compactness"}, {"title": "D. META-LEARNING EPISODIC OPTIMIZATION", "content": "Meta-learning is focused on developing a task-oriented\nmodel to enhance the learning ability by conducting opti-\nmization within each subtask (i.e., an episode or a mini-\nbatch), instead of overall engagement for a given problem.\nA meta-subtask is composed of a support set and a query\nset. Examples in the support set are used for learning how\nto directly solve a subtask, while the query set is used\nfor subtask performance assessment. At each step in meta-\nlearning, model parameters are updated based on a randomly\nselected subtask. Since the network is presented with various\ntasks at each iteration, this enforces learning to distinguish\ninhomogeneous examples in general, rather than a specific\nsubset of examples. In realistic settings of the spoofing detec-\ntion, training data would contain N different types of spoofing\nattacks manipulated by various spoofing techniques (e.g.\nA01-A06 in the ASVspoof 2019 logical access (LA) dataset\n[8], [110]), but the unseen attacks could still occur in the\nevaluation phase. To simulate this situation during training,\nwe first randomly select K spoofing examples xs from each\nspoofing type respectively, along with 2K bona-fide examples\nxb. Next, one spoofing type is randomly included in the query\nset while keeping all other types in the support set within each\nsubtask. Here, 2K bona-fide examples are equally distributed\nbetween the query and support set. As a result, we obtain\nthe following support set \\(S = \\{x_s\\}_{(N-1)\\times K} \\cup \\{x_b\\}_{K}\\) and\nquery set \\(Q = \\{x_s\\}_{K} \\cup \\{x_b\\}_{K}\\). Given this formulation of\nsupport and query pairs in each episode, with a finite number\nof spoofing types of spoofing attacks enrolled into the model,\nthe spoofing attack types in the query set can now vary in\neach subtask.\nTo compare samples in the support set and query set, we\nuse the relation network [58], which parameterizes the non-\nlinear similarity metric using a neural network. Specifically,\nthe relation network simultaneously models the feature rep-\nresentation and metric over a set of subtasks in order to\ngeneralize to unseen spoofing attacks. Given the input sample\nand its corresponding label in terms of (x, y), samples from\nthe support set S and query set Q are fed through the encoder\nfo (see Sec. III-A). Next, an embedding fo(x) from the\nsupport set and an embedding fo(xj) from the query set are\nconcatenated to formulate one pair. Considering the number\nof samples in S (\\(|S| = NK\\)) and Q (\\(|Q| = 2K\\)), each\nsubtask/mini-batch is comprised of 2NK2 permutations as a\nset P of pairs for metric-based meta-learning. Finally, each\npair is processed by the relation module f, which yields\na scalar relation output score representing the similarity\nbetween the feature representation pair,\n\\[r_{i,j} = f_{\\phi}([f_{\\theta}(x_i), f_{\\theta}(x_j)]), \\]\nwhere [., .] denotes the concatenation operation, the network\nfo treats the relation score as a similarity measure [58],\ntherefore rij is defined as,\n\\[r_{i,j} =\\begin{cases}1, & \\text{if } Y_i = Y_j, \\\\ 0, & \\text{otherwise}.\\end{cases}\\]\nThe network fe and fo are jointly optimized using mean\nsquare error (MSE) objective as in [58], where the relation\nnetwork output is treated as the output of a linear regression\nmodel. The MSE loss for meta-learning here is written as,\n\\[L_M = \\frac{1}{2NK^2} \\sum_{i=1}^{NK} \\sum_{j=1}^{2K}(r_{i,j} - \\mathbb{I}(Y_i == y_j))^2.\\]\nAdditionally, we enforce the model to classify samples\nin both the support and query sets against the entire set of\nclasses in the training set. The entire meta-learning scheme\nwith global classification is depicted in Fig. 2. A hyper-\nparameter \u03bb balances the weighted AAM loss (Eq. 12) and\nthe MSE loss, where the fusion loss is hereby written as,\n\\[L_F = L_W + \\lambda L_M.\\]"}, {"title": "E. DISENTANGLED ADVERSARIAL TRAINING", "content": "Next, adversarial examples can be obtained by adding imper-\nceptible but malicious perturbations to the original training\ndata, which can compromise the accuracy of a well-trained\nneural network [111]. Adversarial examples are commonly\ntreated as a threat to neural networks. Here, we leverage\nboth original training data and corresponding adversarial ex-\namples to train networks for enhanced system performance.\nConsider the default adversary generation method, the Fast\nGradient Sign Method (FGSM), which has random perturba-\ntion and has been used for maximizing the inner part of the\nsaddle point formulation [112]. A more powerful multi-step\nattacker based on the projected gradient descent (PGD) (see\nEq. 17) is adopted here to produce adversaries on the fly [51].\nGiven an input training sample x \u2208 D with a corresponding\nground-truth label y, adversary generation is conducted in an\niterative manner as follows,\n\\[x^{adv} = \\Pi_{X} (x^{adv} + \\alpha sgn(\\nabla_x L(\\theta, x, y))), \\]\nwhere \u03a0 denotes a projection operator, S represents the\nallowed perturbation size that formalizes the manipulative\npower of the adversary, \u03b1 is the step size, L(\u00b7, \u00b7, \u00b7) stands for\nthe loss function, and \u03b8 indicates the model parameters. Eq.\n17 then illustrates one step of a multi-step attacker to generate\nadversaries.\nThe adversarial training framework proposed in [51] only\nused maliciously perturbed samples to train networks. Here,\nthe robust optimization objective illustrates a saddle point\nproblem composed of an inner maximization problem and\nan outer minimization problem written as,\n\\[arg \\min_{\\theta} E_{(x,y) \\sim D} (\\max_{\\delta \\in S} L(\\theta, x + \\delta, y)).\\\\]\nFor each training data sample x \u2208 D, a set of allowed\nperturbations \u03b4\u2208 S are introduced to formalize adversaries.\nSuch a training framework has merits as described in [54],\n[55], [113], but cannot generalize well to original training\ndata [51], [114].\nTo encourage full exploitation of the complementarity\nnature between original training data and corresponding\nadversarial examples, adversarial examples are treated as\naugmented data, and incorporated with the original data for\nmodel training. The learning objective is formulated as,\n\\[arg \\min_{\\theta,\\phi} E_{(x,y) \\sim D} (L_F(\\theta, \\phi, x, y)) + arg \\min_{\\theta} E_{(x,y) \\sim D} (\\max_{\\delta \\in S} L_W(\\theta, x + \\delta, y),\\]\nwhere LF and LW is referred to as Eq. 16 and Eq. 12,\nrespectively.\nEarlier studies on adversarial attacks have demonstrated that\ntraining using adversarial examples can cause label leaking\n(i.e., the neural network overfits to the specific adversary\ndistribution), which leads to compromised model perfor-\nmance [50] [111]. Under the assumption that adversarial\nexamples and original data come from different underlying\ndistributions, Xie et al. proposed disentangled training via an\nauxiliary batch norm (BN) to decouple the batch statistics\nbetween original and adversarial data in the normalization\nlayers during model training [56]. This approach would\nallow for better exploitation of the regularization power of"}, {"title": "IV. EXPERIMENT", "content": "The ASV spoof 2019 corpus on the Logical Access (LA) track\n[8", "110": "is adopted in this work to train and test models. The\ncorpus consists of three partitions, namely, training, devel-\nopment, and evaluation subsets, with each subset containing\ngenuine and spoofed samples. Different spoofing methods\n(i.e., voice conversion and speech synthesis) are employed\nto create spoofing attacks [115"}]}