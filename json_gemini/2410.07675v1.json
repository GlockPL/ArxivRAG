{"title": "ADVERSARIAL ROBUSTNESS OVERESTIMATION AND INSTABILITY IN TRADES", "authors": ["Jonathan Weiping Li", "Ren-Wei Liang", "Cheng-Han Yeh", "Cheng-Chang Tsai", "Kuanchun Yu", "Chun-Shien Lu", "Shang-Tse Chen"], "abstract": "This paper examines the phenomenon of probabilistic robustness overestimation in TRADES, a prominent adversarial training method. Our study reveals that TRADES sometimes yields disproportionately high PGD validation accuracy compared to the AutoAttack testing accuracy in the multiclass classification task. This discrepancy highlights a significant overestimation of robustness for these instances, potentially linked to gradient masking. We further analyze the parameters contributing to unstable models that lead to overestimation. Our findings indicate that smaller batch sizes, lower beta values (which control the weight of the robust loss term in TRADES), larger learning rate, and higher class complexity (e.g., CIFAR-100 versus CIFAR-10) are associated with an increased likelihood of robustness overestimation. By examining metrics such as the First-Order Stationary Condition (FOSC), inner-maximization, and gradient information, we identify the underlying cause of this phenomenon as gradient masking and provide insights into it. Furthermore, our experiments show that certain unstable training instances may return to a state without robust overestimation, inspiring our attempts at a solution. In addition to adjusting parameter settings to reduce instability or retraining when overestimation occurs, we recommend incorporating Gaussian noise in inputs when the FOSC score exceeds the threshold. This method aims to mitigate robustness overestimation of TRADES and other similar methods at its source, ensuring more reliable representation of adversarial robustness during evaluation.", "sections": [{"title": "INTRODUCTION", "content": "Adversarial robustness has emerged as a critical measure of security in machine learning models, particularly for deep neural networks. These models, while achieving state-of-the-art accuracy on various tasks, often exhibit vulnerabilities to adversarial examples-inputs specifically crafted to cause the model to make errors (Szegedy et al., 2014). Adversarial training (Goodfellow et al., 2015; Madry et al., 2018), a method that involves training a model on adversarial examples, has been developed as a primary defense mechanism to enhance adversarial robustness.\nIn a more recent development, Zhang et al. (2019) proposed a novel adversarial training method, TRADES (TRadeoff-inspired Adversarial Defense via Surrogate-loss minimization). Consider a data distribution D over pairs of examples x \u2208 Rd and labels y \u2208 [k]. As usual, fe represents the classifier, CE is the cross-entropy (CE) loss, and KL is the Kullback-Leibler (KL) divergence. For a set of allowed perturbations SC Rd that formalizes the manipulative power of the adversary,"}, {"title": "RELATED WORK", "content": "To defend against various adversarial attacks (Carlini & Wagner, 2017; Madry et al., 2018; Croce &\nHien, 2020a; Andriushchenko et al., 2020; Croce & Hein, 2020b), adversarial training (Goodfellow\net al., 2015; Madry et al., 2018) has been demonstrated to be effective in enhancing the robust-\nness with AutoAttack (Croce & Hein, 2020b) serving as a reliable evaluation method. (Croce et al.,\n2021) Adversarial training comes in different variants, including PGD-AT (Madry et al., 2018), ALP\n(Kannan et al., 2018), and LSQ (Shafahi et al., 2019), MMA (Ding et al., 2020), among others. On\nthe other hand, Tsipras et al. (2019); Zhang et al. (2019) have identified a trade-off between robust-\nness and accuracy, a phenomenon well-explained in theoretical aspects. Based on this, Zhang et al.\n(2019) developed TRADES, which currently serves as a classical baseline in adversarial training."}, {"title": "EXPERIMENTAL METHODS AND SETTINGS", "content": "RELEVANT HYPERPARMETERS\nWe identify several key hyperparameters that impact instability, which will be discussed in detail in Section 4.1. These are listed here in advance: (1) \u03b2 (referred to as in Equation 1), (2) batch size, and (3) learning rate. To better analyze the phenomenon, unless otherwise specified, our default setting is \u1e9e = 3, batch size = 256, and learning rate = 0.1. Also, the complexity of the dataset may also influence the results, with CIFAR-100 being used as the default. (The above setting is relatively reasonable and similar to (Zhang et al., 2019; Pang et al., 2021; Wu et al., 2024)) In addition, other experimental settings are provided in Appendix B.\nEVALUATION METHODOS\nAdversarial Attacks To measure the model's robustness, the most direct approach is to use adver- sarial attacks to test each sample and obtain the robust accuracy. Generally, Projected Gradient De- scent (PGD) (Madry et al., 2018), a well-known and highly effective white-box attack, is frequently used for validation to select checkpoints. Additionally, for more reliable robustness evaluation, Au- toAttack (Croce & Hein, 2020b) has become the mainstream benchmark method. It combines four different attacks: APGD-CE (Croce & Hein, 2020b), APGD-DLR (Croce & Hein, 2020b), FAB (Croce & Hein, 2020a), and Square Attack (Andriushchenko et al., 2020). Among these, Square Attack is of particular interest, as it is a black-box attack that is highly relevant in identifying the presence of gradient masking.\nFirst-Order Stationary Condition (FOSC) To measure the degree of gradient masking, we utilize FOSC (Wang et al., 2019) to assess the convergence capability of multi-step adversarial examples. Suppose we have a k-step adversarial example x*, its FOCS is defined as:\nStep-wise Gradient Cosine Similarity (SGCS) Another metric we refer to is SGCS Lee et al. (2021), which can also be used to compare the convergence stability. For the same k-step adversarial example xk, let xi be the resultant example from the i-th step of the attack, we have:"}, {"title": "IDENTIFYING ROBUSTNESS OVERESTIMATION AND GRADIENT MASKING", "content": "IDENTIFYING ROBUSTNESS OVERESTIMATION\nMotivated by the need to assess the reliability of TRADES, we conduct robust evaluations using different attacks. However, we discover that, under identical configurations, two distinct instances initialized with different random seeds can exhibit significant performance variation. We use PGD- 10 during validation to select the best checkpoint and perform a more rigorous evaluation at test time using the more reliable AutoAttack. The results (Table 1) show that robustness overestimation probabilistically occurs in certain instances, where the PGD-10 validation accuracy is significantly higher than the AutoAttack test accuracy. The main contributor to this discrepancy is Square Attack within AutoAttack, as we find that the black-box attack (Square Attack) outperforms the white- box attack (PGD), indicating the presence of gradient masking as the primary cause of the anomaly. For future reference, we refer to such anomalous cases as \"Unstable\u201d cases, highlighting that TRADES may unexpectedly induce gradient masking. Analyzing the causes and resolving this issue is the primary focus of this work.\nIn the preceding empirical observations, we identify that robustness overestimation is not uniformly present across all training instances, even when hyperparameters remain constant. This variability highlights the need for a deeper examination of the factors contributing to this phenomenon's instability. Therefore, we begin by exploring the hyperparameters (Table 2)that may influence this instability and find that \u1e9e (the coefficient regulating the trade-off between natural and adversarial loss), batch size, and learning rate all have a significant impact. We discover that smaller values of \u1e9e, smaller batch sizes, larger learning rates, and higher class complexity of the datasets (see Appendix C) exhibit a positive correlation with increased instability. In addition, the unstable cases can still occur when applying learning rate scheduling (see Appendix D).\nEXPLORING THE INSTABILITY OF GRADIENT MASKING\nTo more clearly depict gradient masking in unstable cases, we plot the data loss landscapes in Fig- ure 1 from the corresponding checkpoints. Typically, more robust models tend to have smoother loss surfaces, whereas less robust models exhibit more jagged landscapes (Chen et al., 2021). We"}, {"title": "ANALYZING AND INTERPRETING THE INSTABILITY", "content": "In this section, we analyze robustness overestimation and instability from three different perspec- tives: 1) inner maximization, 2) gradient information, and 3) self-healing. Through this analysis, we aim to understand the causes of instability and explore potential solutions. Also, from this point onward, we use the same seed of unstable instance as default settings in Figure 2 to better explain the instability (though this does not imply that these explanations are incidental). Additionally, we now focus on FOSC as a more effective tool for detecting unstable cases.\nTPGD CAUSES OVER-FITTING TO GRADIENT-BASED ATTACKS\nIn Figure 3, by monitoring the gap metric, defined as the difference between the clean training ac- curacy and the adversarial training accuracy, insights can be gained into the dynamics of robustness overestimation. In instances where the model becomes unstable, as indicated by an increase in the FOSC values, we observe a sudden drop in the gap metric. In some epochs, this gap even tem- porarily becomes negative, implying that the adversarial training accuracy has surpassed the clean training accuracy. This counterintuitive outcome suggests that the addition of adversarial pertur- bations through TRADES' PGD (TPGD) somehow aids the model in better classifying the cases, which is clearly abnormal.\nAs indicated in Equation 1, the inner-maximization step seeks to maximize the adversarial loss by increasing the KL divergence between the clean and adversarial logits. Conversely, the outer min- imization step aims to reduce this divergence. This push-and-pull dynamic in logit-logit relations, rather than the typical logit-label relations used in robust methods like PGD-AT (Madry et al., 2018), suggests the model may overfit to the unique characteristics of TPGD perturbations. Further analysis of these dynamics can be found in Appendix F.\nThe above hypothesis falls in line with the findings from the prior work by Kurakin et al. (2017), which discusses the learnable patterns of simpler attacks like Fast Gradient Sign Method (FGSM) (Goodfellow et al., 2015). However, in this case, the model is not learning any specific patterns; instead, it overfits the perturbations and converges at a point where most gradient-based attackers struggle to cause large logit differences due to obfuscated gradients.\nBATCH-LEVEL GRADIENT INFORMATION\nTo better understand the instability observed during training, we keep track of the weight gradient norm of the full loss (W_grad_norm), the gradient norm of the cross-entropy term (CE_norm), the"}, {"title": "SOLVING THE INSTABILITY", "content": "Careful hyperparameter tuning and rigorous evaluation may resolve the instability, but this approach requires extensive time spent on trial and error. Therefore, it is essential to design a method within"}, {"title": "LIMITATIONS", "content": "This paper focuses primarily on analyzing TRADES, a well-established adversarial training method. Although proper hyperparameter tuning may mitigate some of TRADES' potential issues, we be- lieve these challenges are linked to sampling probability and gradient masking-an issue that should not occur in an adversarial defense method-can still happen. Therefore, its existence should not be overlooked. Additionally, while many modern defense methods empirically outperform TRADES, it remains a foundational baseline for numerous adversarial training approaches. Researchers con- tinue to build upon TRADES to validate the effectiveness of their own methods. In this sense, TRADES must still maintain a degree of reliability, and this is exactly what we aim to challenge. Moreover, the logit pairing technique used in TRADES raises the question of whether it genuinely enhances robustness or if this is merely a misconception. This issue provides an opportunity for future researchers to further investigate, building on the analysis and solutions we have proposed."}, {"title": "CONCLUSION AND FUTURE WORK", "content": "In conclusion, we have identified the issue of probabilistic robustness overestimation in TRADES, analyzed its root causes, and proposed a potential solution. Hence, we believe that vanilla TRADES should not be fully trusted as a baseline for multi-class classification tasks without applying our so- lution techniques. Given that TRADES incorporates techniques similar to logit pairing-previously abandoned due to unreliable robustness and gradient masking-we found that TRADES exhibits similar issues. Future research can build upon this correlation by consolidating all these methods to provide a more unified analysis and explanation. Our findings offer a valuable reference for future research in adversarial training methods."}, {"title": "SOLUTION ALGORITHM", "content": "In Section 6, we explained the solution algorithm to address the instability. Here, we provide a detailed version of the pseudocode."}, {"title": "OTHER EXPERIMENTAL SETTINGS", "content": "About other configurations, we follow the similar settings of relevant work (Wu et al., 2024; Rice et al., 2020; Gowal et al., 2020; Pang et al., 2021). We perform adversarial training with a perturba- tion budget of \u20ac = 8/255 under the l\u221e-norm. During training, we use a 10-step TPGD (TRADES' PGD) adversary with a step size of a = 2/255. The models are trained using the SGD optimizer with Nesterov momentum of 0.9 and a weight decay of 0.0005. For AWP, we choose radius 0.005 as Wu et al. (2024; 2020); Gowal et al. (2020). For CIFAR-10/100 (Krizhevsky et al., 2009), we use 200 total training epochs, and simple data augmentations include 32 \u00d7 32 random crop with 4-pixel padding and random horizontal flip. As for TinyImageNet-200 (Le & Yang, 2015), we crop the image size to 64 \u00d7 64 and use 100 training epochs. All experiments were using the ResNet-18 model. Finally, we evaluate the models with PGD-10 at each epoch and select the checkpoint with the highest robust accuracy on the validation set for further experiments."}, {"title": "OTHER DATASETS", "content": "The issue of instability in TRADES is not limited to the CIFAR-100 (Krizhevsky et al., 2009) dataset. Instead, when experimenting on CIFAR-10 and Tiny-Imagenet-200 (Le & Yang, 2015), it is clear that robustness overestimation cases can still occur, as demonstrated in Table 4. Generally speaking, we find that instability is more prominent with larger class complexity."}, {"title": "RELATION BETWEEN FOSC AND SGCS", "content": "In this section, we demonstrate that a non-zero First-Order Stationary Condition (FOSC) value implies a Step-wise Gradient Cosine Similarity (SGCS) less than 1 for a PGD attacker in a toy case where the input x has dimension of 1. This is done to intuitively show that our hypothesis of poor convergence capability indicating a locally rugged loss landscape is logical, as the SGCS taking a smaller value implies non-aligned steps taken by the PGD attacker.\nWe work our proof on a PGD-k attacker taking input x with step-size a and perturbation bound e, assuming ka > 6. The perturbation ball is denoted X = [x \u2212 \u20ac, x + \u20ac] while the result from the i-th perturbation step is xi.\nAs demonstrated by Wang et al. (2019), we have the closed-form"}, {"title": "LOGIT DYNAMICS IN INNER MAXIMIZATION", "content": "In order to better understand the effects of maximizing the distance between clean logits and adver- sarial logits during inner maximization in the TRADES training algorithm, we performed experi- ments where the original TRADES' PGD (TPGD) was replaced with a standard PGD-10 attacker, which maximizes the distance between adversarial logits and the labels. This approach is similar to the algorithm in Wang et al. (2020), but without the misclassification weight.\nAs shown in Table 6, this modified algorithm does not lead to robustness overestimation or insta- bility. Thus, we infer that the dynamic of maximizing the difference between clean and adversarial logits is problematic. However, it should be noted that although this training algorithm avoids the issue of instability, it is not an ideal solution due to the significant decrease in clean accuracy."}, {"title": "GRADIENT NORM ANALYSIS", "content": "To better understand the behavior of model gradients during the TRADES training process (see Section 5.2), we additionally keep track of the following metrics, using methods similar to Liu et al. (2020):\n\u2022 W_Grad_Norm: The 12 norm of the gradient of the full TRADES loss in Equation 1 is denoted as:\nW_Grad_Norm = ||\u2207\u0398(CE(f\u0398(x), y) + \u03bb max\u03b4\u2208S KL(f\u0398(x), f\u0398(x + \u03b4)))||2"}, {"title": "ADVERSARIAL WEIGHT PERTURBATION", "content": "Hypothesizing that local ruggedness (as shown in Figure 1) causes the PGD-10 attacker to fail in finding effective adversarial examples, we attempted to use a sharpness-aware technique, specifically Adversarial Weight Perturbation (AWP) (Wu et al., 2020) as a solution.\nHowever, we found that adding this technique does not eliminate instability when used with the TRADES algorithm. In fact, as shown in Table 7, all training instances displayed robustness overes- timation and instability. We hypothesize that this is due to small rugged areas that are not smoothed out when flattening the landscape in general."}]}