{"title": "Repurposing the scientific literature with vision-language models", "authors": ["Anton Alyakin", "Jaden Stryker", "Daniel Alexander Alber", "Karl L. Sangwon", "Brandon Duderstadt", "Akshay Save", "David Kurland", "Spencer Frome", "Shrutika Singh", "Jeff Zhang", "Eunice Yang", "Ki Yun Park", "Cordelia Orillac", "Aly A. Valliani", "Sean Neifert", "Albert Liu", "Aneek Patel", "Christopher Livia", "Darryl Lau", "Ilya Laufer", "Peter A. Rozman", "Eveline Teresa Hidalgo", "Howard Riina", "Rui Feng", "Todd Hollon", "Yindalon Aphinyanaphongs", "John G. Golfinos", "Laura Snyder", "Eric Leuthardt", "Douglas Kondziolka", "Eric Karl Oermann"], "abstract": "Research in Al for Science often focuses on using modern Al technologies to augment components of the scientific process\u00b9, or in some cases, the entire scientific method\u00b2; how about Al for scientific publications3,4? Peer-reviewed journals are foundational repositories of specialized knowledge, written in discipline-specific language that differs significantly from general Internet content used to train most large language models (LLMs) and vision-language models (VLMs). We hypothesized that by combining a family of scientific journals with generative Al models, we could invent novel tools for scientific communication, education, and clinical care. We converted 23,000 articles from Neurosurgery Publications into a multimodal database - NeuroPubs - of 134 million words and 78,000 image-caption pairs to develop six datasets for building Al models. We showed that the content of NeuroPubs uniquely represents neurosurgery-specific clinical contexts compared with broader datasets and PubMed. For publishing, we employed generalist VLMs to automatically generate graphical abstracts from articles. Editorial board members rated 70% of these as ready for publication without further edits. For education, we generated 89,587 test questions in the style of the ABNS written board exam, which trainee and faculty neurosurgeons found indistinguishable from genuine examples 54% of the time. We used these questions alongside a curriculum learning process to track knowledge acquisition while training our 34 billion-parameter VLM (CNS-Obsidian). In a blinded, randomized controlled trial, we demonstrated the non-inferiority of CNS-Obsidian to GPT-40 (p = 0.1154) as a diagnostic copilot for a neurosurgical service. Our findings lay a novel foundation for Al with Science and establish a framework to elevate scientific communication using state-of-the-art generative artificial intelligence while maintaining rigorous quality standards.", "sections": [{"title": "1.1 Main", "content": "Scientific publications have existed in some form for millennia, stretching back to early works on medicine and mathematics from ancient Egypt. While the nature of the medium has shifted over the years, the primary purpose has always been the communication and storage of knowledge within scientific disciplines in higher education and professions across society. Since the earliest modern journal publication by the Royal Society in 16657, experiments in publishing have been driven by either the unique demands of the underlying fields or the emergence of novel media. In recent decades the dissemination of science has been impacted by the internet and rapidly-evolving media facilitated by it. Starting with the arXiv launch in 1991, preprints and whitepapers have become a common medium for sharing state-of-the-art scientific results for many fields in the physical, mathematical, and more recently - biomedical sciences (bioRxiv). Over the past few years, blog posts and graphical abstracts have emerged as novel means of communicating findings in a more widely available and digestible format, sometimes accompanied by peer-reviewed publications or preprint technical reports. The emergence of generative Al technologies in the form of large language models (LLMs) and vision-language models (VLMs) has raised new opportunities and challenges for scientific publishing\u2074."}, {"title": "1.1.1 Key contributions", "content": "Neurosurgery Publications is the journal of the Congress of Neurological Surgeons and a primary scholarly venue for the field of neurological surgery. It consists of three major journals, Neurosurgery, Operative Neurosurgery, and Neurosurgery Practice, and is published monthly by Wolters Kluwer under the current Editor in Chief, Dr. Douglas Kondziolka. We converted all three journals from Neurosurgery Publications into a large Al training dataset and performed an exploratory data analysis to compare the scientific content of our journal family to broader Al training datasets. Building from these datasets, we go on to make the following key contributions:\n(1) Publishing: We utilized a generalist VLM and our datasets to generate quick graphical abstracts for inclusion in Neurosurgery Publications (Fig. 1b).\n(2) Education: We generated tens of thousands of board review questions in the style of the American Board of Neurosurgery (ABNS) and subsequently utilized them for human and VLM training.\n(3) Generative Al training: We trained a specialty-specific VLM (CNS-Obsidian) using a novel curriculum learning pipeline with multiple choice question (MCQ) probes for a differential diagnosis task (Fig. 1c).\n(4) Clinical care as a diagnostic co-pilot: We deployed CNS-Obsidian with a chat interface as a point-of-care diagnostic co-pilot for neurological diagnoses and compared against a frontier VLM (GPT-40) in a blinded randomized controlled trial (Fig. 1d)."}, {"title": "1.2 Results", "content": "1.2.1 Scientific journals are a rich source of quality, domain-specific data\nWe identified Neurosurgery Publications as a potential source of high quality scientific text and image data due to the rigorous peer-review and publishing processes. We used a variety of acquisition, extraction, and filtering tools to assemble a dataset of texts and images by retrieving and processing all of the available Neurosurgery Publications articles. A total of 23,984 articles were converted into a base dataset consisting of 139 million words and 78,853 scientific figures with captions. We called the resulting base multimodal dataset NeuroPubs, and utilized it for subsequently exploratory data analysis and Al training dataset construction."}, {"title": "1.2.2 Vision-language models create production-ready graphical abstracts", "content": "Neurosurgery Publications encourages the creation of graphical abstracts to accompany published articles. We developed a pipeline for the automatic conversion of articles in NeuroPubs into graphical abstracts using CSS templates and iterative prompting of a frontier, generalist VLM (see Methods for details) (Fig. 2; Extended Data Fig. 1). One hundred automatically generated graphical abstracts were evaluated by Neurosurgery Publications Editorial Review Board members. Generated graphical abstracts were free of formatting errors 85% of the time, 99% of abstracts were factually correct, and 82% were found to be visually appealing. Graphical abstracts that were judged as being \u201cpublication ready\u201d had to meet all three criteria, and 70% of the generated abstracts passed this bar without any manual involvement in abstract creation."}, {"title": "1.2.3 Converting a specialty journal into a multimodal Al dataset", "content": "The base NeuroPubs dataset was converted into three task-specific datasets for VLM visual instruction fine tuning (IFT) (n=127,076 samples, 4.2M tokens), multiple-choice questions (MCQs) (n=89,587, 3.7M tokens) for human and VLM assessment, and cases with differential diagnoses (n=46,401, 0.4M tokens) for training a diagnostic co-pilot (Extended Data Fig. 2-3). We explored the content of NeuroPubs relative to the broader PubMed dataset to assess how much unique content was contained in our journal relative to the broader medical literature - noting that this divergence would likely be greater for broader, internet-scale datasets. We found a marked divergence between these datasets (Fig. 3a-c)."}, {"title": "1.2.4 NeuroPubs and generalist LLMs can make multiple choice questions for assessing medical trainees and VLMs", "content": "We assessed whether generalist VLMs, GPT-40 and Claude Sonnet-3.5, and NeuroPubs could be used to generate board examination questions for instructing both medical trainees and guiding VLM development. We generated 89,587 synthetic MCQ questions using frontier VLMs (see Methods). \u03a4\u03bf assess generated sample quality, we randomly sampled 50 synthetic questions generated by two different frontier models and 50 real questions from the Congress of Neurological Surgeons (CNS) Self-Assessment of Neurological Surgery (SANS) and used these to generate multiple 30 question mock exam blocks which randomly contained real and synthetic questions. We found that the average quality of questions made in a fully automated way is not yet at the level of manually created and curated ones with an average of 25% of the Al generated questions being rated as good board review questions compared to 72% of the Human generated questions (p < 10-7) (Extended Data Fig. 4). 54% of Al-generated questions misled at least one evaluator into thinking that they were Human-made, and 23% misled both."}, {"title": "1.2.5 Building multimodal neurosurgical artificial intelligence", "content": "We trained a specialty-specific VLM using NeuroPubs and a fully-autoregressive VLM chitecture17(Extended Data Fig. 5a). We extended the generalist medical curriculum training15 by introducing a third stage designed to learn specialty-specific knowledge through instruction finetuning, the skill of differential diagnosis, and answering specialty-specific multiple-choice questions (Fig 4a-b; Extended Data Fig. 5b). We performed extensive ablations and experiments, and used our MCQ performance as a means of quantitatively evaluating model knowledge acquisition during Stage 3 training as a probe for differential diagnosis abilities (Extended Data Fig. 6-8; also see Supplemental Methods: Ablations). Our best performing model matched GPT-40's performance on the held-out GPT-generated MCQs (p = 0.2347) (Fig. 5c) and substantially outperformed both GPT-40 and Claude Sonnet-3.5 on the Claude-generated MCQs (p = 0.0011 and 0.0004, respectively), despite being exclusively trained on the GPT-generated data (Fig. 5d). Zero-shot performance on the human-generated CNS-SANS questions improved from a baseline of 39.81% to 46.81%, but was unable to match the state-of-the-art frontier models (GPT-40 = 65.70%, p < 10-15) (Fig. 5e) which we hypothesized was likely due to data contamination within the frontier models. Including additional Claude-generated data in the model training improved the performance on Claude generated MCQs (p = 0.0427) but did not on GPT generated MCQs (p = 1.000) or SANS (p = 0.5193)."}, {"title": "1.2.6 Specialty-specific VLMs are effective diagnostic co-pilots at the point of care", "content": "We developed a platform for deploying VLMs internally within our health system. Using this platform we implemented a chatbot interface for our VLM (Extended Data Fig. 9). We launched a blinded, randomized controlled trial of our specialty VLM, CNS-Obsidian, vs. GPT-40 as point-of-care diagnostic co-pilots for three months of deployment (See Supplemental Information: Trial Protocol). For three months, neurosurgical residents at a major academic medical center were provided with a web-app that provided a chat interface for interacting with a VLM. The VLM was conditioned to provide a differential diagnosis based on user-provided visual and text inputs and the backend model was randomized between GPT-40 and CNS-Obsidian (Fig. 5a; Extended Data Fig.10, Supplementary Video 1 and 2). For our primary endpoint of noninferiority for a clinically helpful differential diagnosis CNS-Obsidian was found to have an upvote frequency of 40.62%, non-significantly lower than the GPT's 57.89%. (p = 0.1150) (Fig. 5b). For our secondary endpoint of follow-up conversation diagnostic helpfulness, CNS-Obsidian was also found to have non-significantly lower upvote frequency at 25.00% vs. 40.00% of GPT-40 (p = 0.1266) (Fig. 5c). Both models were assessed on the correctness of the generated differential diagnoses, measured as including the final diagnosis in the generated list at the time of the consult. Here, CNS-Obsidian achieved a score of 59.38% compared to GPT-40's of 65.79% (p = 0.3797) (Fig. 5d). Noting that GPT-40 tended towards broader and lengthier differentials (Fig. 5a), we corrected for length and found that CNS-Obsidian trended towards a higher rate of correct diagnoses 16.88%, compared to GPT-4o's of 10.69% (p = 0.9590) (Fig. 5e). For our other secondary endpoint of user engagement, assessed as the length of the continued conversation, CNS-Obsidian had an average conversation length of 2.50 compared to GPT-40's of 1.79 (p = 0.6719) (Fig 5f). Notably, there were a total of 70 chats (average of 0.75 per day; 38 randomized to GPT-40, 32 randomized to CNS-Obsidian) over the data collection period of 92 days. During this time period, 959 consults overall (10.42 per day) were seen by the Neurosurgery service."}, {"title": "1.3 Discussion", "content": "We present our attempt to repurpose scientific and medical journals using VLMs in an era of generative Al doing Al for Science by building Al with Science. By integrating Neurosurgery Publications with VLMs for publishing, education, Al modelling, and clinical care we show how repurposing quality scientific content can push the boundaries of generative Al. and create a mutually beneficial relationship between VLMs and the scientific literature. VLMs can augment scientific publications by rapidly generating novel and accurate educational and publishing content. In turn, scientific publications can augment VLMs with curated, multimodal data. By defining scientific journals as knowledge resources for human and Al training, we envision an exciting future for scientific publishing through the intelligent use of these tools.\nWe converted Neurosurgery Publications, the medical journal family of the Congress of Neurological Surgeons, into an Al training dataset and used that dataset to build a 34B parameter VLM using state-of-the-art techniques and a novel training regimen to emphasize differential diagnostic reasoning. To ensure a rigorous comparison on all downstream tasks, we held out entire articles from the training datasets to ensure that CNS-Obsidian never saw the underlying content at any step in its training. Due to unknowns surrounding the underlying training datasets of GPT-4o and Claude Sonnet-3.5 and known instances of data leakage 18,19, we believe this to be an exceptionally strict control for potential data contamination. We found that CNS-Obsidian performed comparably to the state-of-the-art frontier models, OpenAl's GPT-40 and Anthropic's Claude Sonnet-3.5, on real and synthetic domain specific (neurosurgery) multiple choice questions. These results emphasize the importance of in-domain data and that data is the most important aspect of modern Al efforts which overwhelmingly benefit from the massive availability of information on the internet. Where high quality medical information is typically not publicly available on the internet scientific publications present an immediate solution. Part of what makes publications so valuable is the multimodal paired image-text information, which is particularly rare. While efforts exist to directly scrape this content from Twitter16, or to utilize publicly available resources15, these efforts are likely noisier than scientific images with expertly authored captions such as those found in refereed journals.\nAs part of this project we conducted the first blinded, randomized controlled trial of VLM chatbots in a clinical setting. Unlike prior works that used randomization and blinding in simulated clinical settings20,21, our trial captured the complexities of real-world care. Both CNS-Obsidian and GPT-40 were seen as helpful at the point of care almost half of the time, and rated similarly on engagement despite GPT-40's longer and more verbose messages. While both models included the correct diagnosis in their differentials over half the time, CNS-Obsidian produced more precise differentials compared to GPT-40's broader and less specific differentials. While prior works on enabling LLMs for differential diagnostic reasoning utilized large, close source LLMs with prompting schemes 20, we directly finetuned our model on vision-language cases with paired differential diagnoses sampled from journal content, tailoring it more closely to the clinical environment.\nOne of the most significant findings of our RCT was the low utilization of the chatbot by neurosurgical participants . We attribute this low utilization rate due to the chatbot user interface itself - highly trained specialists would prefer automated solutions that free up their time to accomplish tasks rather than assistance with completing the tasks themselves. We hypothesize that one major barrier could be the chatbot interface itself, and the need to spend more time taking photos and texting responses which, for specialists, is largely unnecessary which might not hold true for generalist users.Notably, despite the differential diagnoses frequently being perceived as helpful, both cases frequently did not contain the final, official diagnosis."}, {"title": "1.4 Limitations", "content": "We acknowledge several key limitations in our work to \u201crepurposing scientific publications\u201d. We performed an exhaustive set of ablations to help identify the most beneficial training recipe and data mixture for our VLM configuration. However, we restricted ourselves to fully autoregressive models and did not explore cross attention-based alternatives22. Other potential model improvements, such as grounding strategies to more closely align fine-grained text and image features, are left to future researchers investigating medical VLM architectures. We also note that we restricted ourselves to utilizing only the content from a single journal family, Neurosurgery Publications, rather than the entirety of the neurosurgical literature. Comparisons against frontier models also were limited by our ability to evaluate for data leakage due to the lack of transparency surrounding the training datasets of these models. Our blinded RCT was limited by a low response rate, resulting in a relatively small sample size, and by its conduction at a single institution. These factors may limit the generalizability of our findings and underscore the need for future multi-center studies with larger numbers of participants. The low response rate itself is an interesting finding and raises several hypotheses as to ways of improving human-Al interactions in the medical setting, echoed by a recent study in Greece where surgeons had more negative perceptions of a GPT-4 based chatbot than their medical counterparts 23. One is by eliminating the need for conversational (chatbot) user-interfaces, as busy physicians may prefer to not use tools requiring substantial user-inputs in order to get a response. A second is that non-specialists may ultimately be more receptive audiences, where their broader scope of practice may see greater benefits from interacting with a chatbot's stored knowledge.\nA further limitation is our design choice to do full pre-training and finetuning of CNS-Obsidian, as compared to prompting and zero-shot inference with GPT-40. Furthermore, it may be possible that with additional techniques such as retrieval augmented generation (RAG)24, or additional uses of in-context learning or CoT prompting that both models could have improved performance as has been demonstrated in other works25. These observations highlight the need formore research into human-Al interactions in real-world environments as well as alternative user interfaces beyond chatbots to better align with specialist workflows."}, {"title": "1.5 Conclusion", "content": "We present here our results of doing Al with Science, and combining state of the art VLMs with a scientific journal family. Reinventing a scientific journal family, Neurosurgery Publications, with VLMs ultimately lead to potential benefits for both the journal and its scientific field. Automating the generation of graphical abstracts can expedite the communication of information, while the generation of board review questions can help with assessing educational achievement and minimizing the workloads of faculty members. Directly training journal content into the weights of a thirty-four billion parameter VLM lead to competitive results on specialty MCQ assessments with frontier models and in a real-world clinical deployment as a diagnostic co-pilot as compared to GPT-40, a closed-source VLM rumored to be in the trillions of parameters and widely acknowledged as the generalist state-of-the-art. These results suggest a possible role for mesoscale VLMs built by scientific and medical communities using unique community data resources to address unique community needs. While this project focuses on neurological surgery, it is easy to imagine alternative works for other areas of science and medicine."}, {"title": "Methods", "content": "2.1 IRB and Legal\nThis project was approved by the NYU Langone Institutional Review Board (i23-00510). This project was also reviewed by the leadership of Neurosurgery Publications and Wolters Kluwer. All parties agreed to the utilization of journal content for the purposes of this study. Self-Assessment of Neurological Surgery questions were utilized as a benchmark with the permission from the Congress of Neurological Surgeons.\n2.2. Data Acquisition and Processing\nWe built a pipeline to create a dataset from three neurosurgical journals: Neurosurgery n=21,219, Operative Neurosurgery n=2,618, Neurosurgery Practice n=147. We implemented two parallel extraction streams. For images, AWS Textract identified figure locations and generated bounding boxes for cropping. Our regex-based caption matcher paired images with candidate captions using geometric distances. For text, we used a combination of AWS Textract and Meta's Nougat 26 to extract unstructured texts from the PDFs. We used regular expressions to identify in-text mentions of each figure within the texts and stored these excerpts together with the figure's caption and metadata.\n2.3. Graphical Abstract Generation\n2.3.1 Generation Pipeline\nWe developed an automated pipeline to convert extracted manuscript content into standardized graphical abstracts. The pipeline implements a custom Cascading Style Sheet (CSS) profile designed to match the format of existing Neurosurgery graphical abstracts. Using Claude Sonnet-3.5, we engineered prompts to generate structured HTML summaries compatible with our CSS profile. The summaries were organized into six sections: Objectives, Background, Methods, Results, Discussion, and Conclusion. The model also selected up to two representative figures from each manuscript based on caption analysis.\n2.3.2 Abstract Human Evaluation\nWe evaluated the pipeline's performance using a random sample of 100 articles published between 2021-2024. Three members of the Neurosurgery Editorial Review Board assessed each generated abstract using three binary criteria:\n\u2022 Graphical Abstract is Properly Built? (0: No, 1: Yes)\n\u2022 Content is Factually Correct? (0: No, 1: Yes)\n\u2022 Graphical Abstract \"Looks Good\" for our Journal? [Good figure choice? Good facts to include?] (0: No, 1: Yes)\n2.4 Knowledge Translation Framework\n2.4.1 Filtering\nWe built an image content classification system using ResNet-5027 feature extraction followed by a linear classifier. In order to do so, we manually annotated 500 figures as one of three classes:\n\u2022 Class 2: Medical imaging (CT, MRI, X-ray, angiography)\n\u2022 Class 1: Clinical visuals (surgical fields, microscopy, anatomical drawings)\n\u2022 Class 0: Technical content (flowcharts, survival curves, tables)"}, {"title": "2.4.2 Conversion", "content": "We developed an automated pipeline using OpenAI GPT-40 and Anthropic Claude Sonnet-3.5 to convert specialized domain knowledge into high-quality training data for vision-language models:\n1. Instruction fine-tuning (IFT): Conversational pairs\n2. Multiple-choice (MC): Clinical vignettes with options\n3. Differential diagnosis (DDx): One-line case summaries with ranked diagnoses\nEach generation task used custom prompts with four randomly sampled few-shot examples from a pool of 10 examples (IFT used LLaVA-Med15 examples; MC and DDx were manually created by us). Models were prompted to create a user-assistant conversation, a multiple choice question with discussion, or a patient one liner with a differential diagnosis based on the image, caption, and in-text mention, but without explicit reference to the latter two. The pipeline included the target figure in the API call but excluded example figures."}, {"title": "2.5. Dataset Visualization", "content": "2.5.1 Data Cartography\nWe embedded the text portion of the IFT dataset obtained from passing Neurosurgery Publications through GPT, together with a dataset generated in the same methodology from PubMed15, into a shared two-dimensional space for visualization and data exploration purposes. To do so, we first used Nomic Embed Text v128, an open-source BERT-like text embedder that converts unstructured text into 512-dimensional vectors. We subsequently used tSNE29 to reduce the dimensionality to two dimensions. We obtained 12 largest clusters using the HBDSCAN30 hierarchical clustering and generated names for it by sampling texts from inside and outside the cluster and making a query to GPT-40 asking to come up with a name that unifies the themes of the cluster."}, {"title": "2.6 Multiple Choice Evaluation", "content": "2.6.1 MC Human Evaluation\nWe developed five surveys, each consisting of 10 authentic questions from the neurosurgical boards question bank (Self-Assessment for Neurological Surgeons, SANS) and 20 synthetic questions\u201410 generated by GPT-40 and 10 by Claude Sonnet 3.5\u2014presented in a blinded and randomized order. To maintain consistency with our image-based synthetic datasets, only image-associated questions from the SANS question bank were included. Synthetic questions were sourced exclusively from holdout datasets to ensure none were present during model training. Each survey was administered to one neurosurgery trainee (resident) and one attending neurosurgeon. For each question, participants answered two follow-up questions: (1) whether they believed the question was human- or"}, {"title": "2.7. Model Architecture and Training", "content": "2.7.1 Vision-Language Model Backbone\nLLAVA (Large Language and Vision Assistant) combines vision and language processing by aligning CLIP-derived image features with language embeddings, enabling interactive image understanding. We built on LLaVA-Next's improvements - including its multilayer projection, patch-based processing of large images, and enhanced pre-training. (Extended Data Fig. 4a) As a starting point, we used the 34B parameter version of LLaVA-Next based on Nous Hermes 2 Yi-34B available on HuggingFace Transformers.\n2.7.2 Three-Stage Curriculum Training\nWe developed a training protocol based on the LLaVA-Med curriculum training, but with a novel Specialization Stage 3. Stage 1 (\"alignment\") kept the model frozen while training only projection layers on PubMed-based figure-caption pairs. Stage 2 (\"medical knowledge integration\") unfroze both projection and language model components and train on general medicine PubMed-based conversations (IFT) dataset generated using GPT-40. Stage 3 (\"neurosurgical specialization\") maintained the same unfrozen components while training on our domain-specific, Neurosurgery Publications-based, and task-specific (IFT, MC, and DDx) datasets generated using GPT-40 and Claude Sonnet 3.5. (Extended Data Fig. 4b)\n2.7.3 Training Details\nThe training infrastructure used 104 H100s on NYU Langone's UltraViolet high-performance compute cluster. PyTorch FSDP was used for distributed data parallelization. We used bfloat16 precision, with learning rates of 1e-3 for Stage 1 and 1e-5 for Stages 2 and 3, and cosine scheduling. Mini-batch size per GPU was 4, with gradient accumulation steps of 4 during Stage 1, yielding effective batch sizes of 1664 for Stage 1 and 416 for Stages 2 and 3 across our distributed setup. Unlike LLaVA-Next, but similar to LLaVA-Med, we kept the vision encoder frozen due to training stability constraints. Data splits maintained paper-level separation with 95% training, 2.5% validation, and 2.5% test sets to prevent information leakage between splits. Validation was used to monitor loss, whereas test split was used to establish performance (see Methods 2.8.2).\n2.7.4 Training Length\nWe refer to different checkpoints of our model as [<# of Stage 1 epochs>, <# of Stage 2 epochs>, <# of Stage 3 epochs>] for brevity and convenience. We started with recreating LLaVA-Med's framework, but using LLaVA-Next architecture, with one epoch of Stage 1 training and three epochs of Stage 2 training, yielding our LLaVA-Next-Med [1, 3, 0]. In the initial attempt of creating a task-specific model, we trained this model on three epochs of GPT-Only Stage 3 datasets, yielding [1, 3, 3]. Through extensive experiments and ablation studies (See Supplemental Results: Ablations) we found that the standard training duration of 1 epoch for alignment and 3 epochs for medical and neurosurgical fine-tuning was insufficient for a model of this scale (Extended Data Fig. 5-6). Our best performing"}, {"title": "2.8 Evaluation", "content": "2.8.1 Benchmarking\nWe evaluated LLaVA-Next-Med and CNS-Obsidian against multiple baselines: LLaVA-Med, base LLaVA-Next, OpenAI GPT-40, and Anthropic Claude Sonnet 3.5. For our models, LLaVA-Med, as well as LLaVA-Next we used vLLM for deployment and made calls to the deployed model via requests interface. For GPT-40 and Sonnet 3.5 evaluations, we made direct API calls to the publicly available checkpoints. We used a local instance of LLaMA-70B to parse the models' chain-of-thoughts and convert them to single-letter answers for automated matching against ground truth. We used two-sided Fisher exact tests to establish significance or a lack of there-off throughout benchmarking.\n2.8.2 Synthetic Domain-Specific Questions\nWe created additional synthetic benchmarks from our held-out test data (2.5% of total), comprising 1,282 questions from the GPT-Synthetic dataset and 1,239 questions from the Claude-Synthetic dataset. Paper-level splitting during training ensured these test questions contained neither previously seen questions nor figures from training papers. We used these benchmarks to guide our decisions with ablations.\n2.8.3 Human-Made Domain-Specific Questions\nThe Self-Assessment of Neurological Surgery (SANS) questions formed our primary benchmark. These questions, designed by the Congress of Neurological Surgeons (CNS), are used by neurosurgery residents preparing for American Board of Neurological Surgeons standardized examinations. The CNS provided 3,965 questions for our evaluation set. Of these questions, 950 contained question-associated images, which formed our benchmark."}, {"title": "2.9 Randomized Control Trial", "content": "2.9.1 Interface and Evaluation Stack\nOur final evaluation component involved a randomized controlled trial (RCT) comparing the diagnostic and consultation performance of NYU-Obsidian to a PHI-safe version of GPT-40. \u03a4\u03bf facilitate this, we developed a full-stack application for blinded and randomized evaluation of clinician-facing LLMs. The user interface (Extended Data Fig. 8) was adapted from a publicly available framework Chatbot UI, implemented in React and Next.js, and extended with features such as secure authentication, medical reference number recording, image submission, and endpoint randomization for each new session. The system utilized Postgres for account and chat storage, Flask with SQLiteDB for authentication, vLLM for hosting the local model (CNS-Obsidian), and Kong as an API gateway to connect with the PHI-safe OpenAl GPT-40. Throughout the trial, the best-performing model at the beginning of the trial, CNS-Obsidian-base [1, 3, 3], was used, despite later improvements in training schema resulting in superior, longer-trained versions."}, {"title": "2.9.2 Randomization and Sample Size", "content": "Study participants included neurosurgery residents, fellows, attending physicians, and advanced practice providers, who interacted with a randomly assigned model in each conversation. The data was collected for three months, August 30th, 2024 through November 30th, 2024 on the Neurosurgery service at NYU Langone Health Tisch Hospital. All trainees and faculty in the department were invited to participate via email. They were instructed to interact with the software after finishing an encounter with a consult patient. Each chat was randomized independently of others using python's scipy binomial pseudorandomness.\nThe trial was not preregistered due to its observational nature as it's a user study of tools that were used after patient assessment, not before. The tools were not accessible during patient assessment. (See Supplemental Information: User Manual and Supplemental Information: Trial Protocol)\n2.9.3 Diagnostic Helpfulness\nClinicians submitted medical images alongside concise clinical summaries (\"one-liners\"). Models were prompted to generate a differential diagnosis in their first response, which clinicians then rated as either clinically helpful (\"thumbs up\") or not (\"thumbs down\"). Clinicians could optionally continue the conversation, rating subsequent responses as well. The primary endpoint of the study was the subjectively rated diagnostic helpfulness of the first response, defined as the frequency of \u201cthumbs up\". Secondary endpoints included the length of follow-up conversations and the subjective helpfulness of these interactions, as well as post-hoc differential accuracy.\n2.9.4 Differential Accuracy\nFor each submitted case, patient identifiers were recorded but omitted from model inputs. Ground truth diagnoses were retrospectively retrieved and compared against model-generated differential diagnoses. Accuracy of the differential diagnosis, measured as the proportion of cases where the true diagnosis appeared in the list of differentials served as an additional secondary endpoint. Evaluation involved a two-step automated process:\n1. GPT-40 extracted individual diagnoses from the unstructured model outputs into a structured list.\n2. GPT-40 determined whether the true diagnosis was included in this list.\nDuring analysis, GPT-4o's differentials were observed to be longer and less specific compared to CNS-Obsidian. To address this, we introduced an adjusted accuracy metric: the average proportion of correct diagnoses divided by the differential list's length.\n2.9.5 Statistical Analysis\nThe RCT was designed as a non-inferiority study using one-sided statistical tests:\n\u2022 Diagnostic Helpfulness (both primary and secondary): Fisher's exact test, testing the hypothesis that CNS-Obsidian is less helpful than GPT-40.\n\u2022 Diagnostic Accuracy: Fisher's exact test, testing the hypothesis that CNS-Obsidian is less accurate than GPT-40.\n\u2022 Length-Adjusted Accuracy: Mann-Whitney U test, comparing fractions of correct responses adjusted for differential list length.\n\u2022 Engagement Frequency: Mann-Whitney U test, comparing the frequency of clinician interactions to the number of submitted consults, testing the hypothesis that CNS-Obsidian is less engaging (has shorter chats)."}, {"title": "3.4 Contributions", "content": "E.K.O. conceptualized and supervised the project. A.A., D. Kurland, and K.S. collected journal publication data. A.A. and J.S. extracted, processed, filtered, and organized the data. J.S. developed the front-end and prompting for the graphical abstract-generation pipeline. R.F., T.H., and E.K.\u039f. evaluated generated graphical abstracts. B.D. handled data embedding, mapping, and visualizations. A.A. developed the data conversion pipeline. A.A. and S.S. created forms for human evaluation of the multiple-choice questions. A.S., D. Kurland, C.O., A.V., S.N., E.T.H., I.L., D.L., P.R., L.S., and D. Kondziolka manually evaluated the questions. A.A. developed and trained the models. J.S. built the model evaluation suite. A.A., J.S., and E.K.O. conducted benchmarking and ablations, benchmarking. J.S. and E.K.O. developed the randomized control trial (RCT) user interface and web stack. A.S., C.O., A.V., S.N., A.L."}]}