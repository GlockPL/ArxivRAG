{"title": "Utilizing Large Language Models to Optimize the Detection and Explainability of Phishing Websites", "authors": ["Sayak Saha Roy", "Shirin Nilizadeh"], "abstract": "In this paper, we introduce PhishLang, an open-source, lightweight Large Language Model (LLM) specifically designed for phishing website detection through contextual analysis of the website. Unlike traditional heuristic or machine learning models that rely on static features and struggle to adapt to new threats, and deep learning models that are computationally intensive, our model utilizes the advanced language processing capabilities of LLMs to learn granular features that are characteristic of phishing attacks. Furthermore, PhishLang operates with minimal data preprocessing and offers performance comparable to leading deep learning tools, while being significantly faster and less resource-intensive. Over a 3.5 months testing period, PhishLang successfully identified approximately 26K phishing URLs, many of which were undetected by popular antiphishing blocklists, thus demonstrating its potential to aid current detection measures. We also evaluate PhishLang against several realistic adversarial attacks, and develop six patches that make it very robust against such threats. Furthermore, we integrate PhishLang with GPT-3.5 Turbo to create explainable blocklisting - warnings that provide users with contextual information into different features that led to a website being marked as phishing. Finally, we have open-sourced the PhishLang framework, and developed a Chromium-based browser extension and URL scanner website, which implement explainable warnings for end-users.", "sections": [{"title": "1 INTRODUCTION", "content": "Phishing attacks have resulted in numerous data breaches and cases of credential theft, causing financial losses exceeding $52M and impacting over 300K users during the last year alone [6, 104]. These scams are largely successful due to sophisticated social engineering techniques [42] that enable the creation of websites imitating legitimate organizations. To counter these threats, researchers across industry and academia have focused on developing countermeasures that leverage unique phishing signals, such as URL characteristics, network patterns, heuristic features in source code, and advanced machine learning methods [74, 97]. A more recent focus has been on the visual aspects of phishing sites [13, 71], with several methods now using image-based content, such as screenshots, for phishing detection [11, 69]. Although these models demonstrate high detection rates in controlled environments, they often do not fare well against complex evasion techniques [23, 110]. Keeping these models up-to-date also poses significant challenges, as attackers continuously develop evasion tactics. Updating models to counter new threats often requires the creation of revised ground truth datasets and the identification of novel feature sets or strategies. Additionally, the inherent complexity and resource demands of the more recent deep learning models, especially those that depend on image content - limit their practicality for large-scale, real-time deployment [77, 90]. Furthermore, their reliance on opaque, black-box approaches complicates explainability [40], limiting opportunities for iterative improvement. To address these challenges, we have developed PhishLang, an efficient and transparent phishing detection framework that harnesses the capabilities of Large Language Models (LLMs) which are trained to comprehend the context of potential phishing websites. By evaluating these sites against known phishing patterns, our framework avoids reliance on static feature sets. Instead, it utilizes the advanced linguistic skills of LLMs to identify subtle patterns in the source code. Our model not only operates with significantly lower resource requirements-using up to three times less memory and seventeen times less storage than traditional deep learning-based phishing detection models-but also performs better than several ML-based models at detecting new attacks, especially those which contain evasive features. It is also highly resilient against realistic adversarial attacks. We also utilize our model in conjunction with GPT 3.5 Turbo [78] to build \"Explainable Blocklisting\" - an approach that provides users with contextual information on why a phishing website was blocked. This can not only help users understand and engage with the protection mechanism more effectively but also educate them on recognizing phishing attempts independently when protections are unavailable or fail. Finally, we open-source our framework, PhishLang, and also provide a Chromium-based browser extension that can proactively block threats from within the users' web browser, as well as an online scanning website, which can be used to scan new URLs on demand. The structure of the paper is as follows: Section 3 elaborates on our methodology for parsing source code to isolate critical tags associated with phishing functionality (Section 3.1). This technique significantly reduces the input data size for the Large Language Models (LLMs), enhancing both training and inference efficiency. We employ this method to develop our training ground truth from a vast collection of both phishing and benign websites from the open-source PhishPedia [69] dataset (Section 3.2). Following this, we describe our training process for LLMs using a \"sliding window\" technique that divides the parsed code into smaller chunks, optimizing the contextual information available to the model (Section 3.3), a technique that has been used to maximize LLM training efficiency [28]. To strike a balance between processing speed and detection accuracy, we evaluate various leading open-source language models to select the most effective one for our purpose (Section 3.4). To interpret the prediction capabilities of our model, we also perform a Local Interpretable Model-Agnostic Explanations (LIME) analysis [88], identifying the features that our model focuses on (Sec- tion 7.3). Section 4 details the development and implementation of our real-time phishing detection system, PhishLang, which operates on a continuous stream of live URLs from Certstream to identify new"}, {"title": "2 RELATED WORK", "content": "Phishing detection measures: Numerous studies have explored the various techniques employed by attackers to execute phishing attacks [14, 30, 49]. Insights derived from these studies have been subsequently utilized to develop various detection measures. Scholars have introduced mechanisms for detecting phishing websites that leverage a combination of URL-based (such as Domain reputation, SSL Certification abuse, level of domain and reputation of TLD, etc.) and source code-based heuristic features (such as DOM content, presence of form fields, encoding exploits, etc.) [74, 97] as well as machine learning approaches [91]. However, several studies [76, 102] have highlighted that these models tend to overfit the features they are trained on and exhibit a lack of resilience against evasive attacks. The common factor in all these cases was the automated models were not trained on the newer features, or the features were simply not available. For a phishing attack to be successful, it is imperative that the message is effectively conveyed to the potential victim [13]. This intuition led to the development of frameworks that study the screenshot of the website [12, 100] to identify suspicious features, comparing their appearance with that of known phishing websites [56], or identifying malicious artifacts, such as brand logos [70]. While showing good performance in a controlled research environment, in practice, these models are extremely resource-intensive with respect to processing power and storage required, as well as much slower than traditional heuristic or machine learning-based phishing detection methods [75, 90]. Such limitations severely restrict their utility in real-world anti-phishing frameworks, which must process millions of URLs daily [19, 77].\nLLMs in Content Moderation: Open-source LLMs have previously shown promise as content moderation tools in diverse areas of social computing and security. For example, fine-tuned BERT models are used to identify cyberbullying instances [24] and to discern toxic triggers on Reddit platforms [31]. Demicir [37] combined LSTM and GPT-2 techniques to spot elusive malware. Several of these papers have demonstrated how LLMs can be trained on small amounts of data, with minimal data processing and without the need for defining specific features, which in turn outperform several traditional machine learning-based methods. However, utilizing LLMs to identify online scams and malicious websites has been very limited. Existing work [52, 62] have predominantly focused on using proprietary LLMs like ChatGPT [10], Claude [8], and Perplexity [9]. These models, while powerful, possess a proprietary architecture, thus preventing potential enhancements or alterations by the broader research community. Their reliance on a cost-per-use API model [78] further impedes scalability- a critical component in the fight against harmful content [95]. Moreover, Derner at al. [38] has pointed out how attackers can adversarially exploit these commercial LLMs to evade classifications, thus significantly impacting their protective capabilities. A notable exception towards utilizing LLMs for scam detection is Su et al. [99], who have attempted to use BERT to detect malicious websites, though the analysis is limited only to the URL of the website. Thus, the current state accentuates the pressing need for an efficient open-source model tailored for phishing website detection, which is also resilient against adversarial attacks.\nExplainable warnings: Security systems often use warning dialogs to convey crucial information, particularly when blocking malicious content or changes in system configurations that could affect usability [87]. However, repetitive and generic warnings provide minimal insight into actual threats, leading to user desensitization and ignored alerts [13, 64, 108]. This issue is prevalent in antiphishing systems integrated into web browsers or installed as plugins [3, 7], which typically indicate a dangerous website without explaining why it was flagged. Despite evidence that providing contextual information about threats enhances user understanding and trust [39, 73], this approach is largely unadopted in commercial antiphishing tools. This study thus explores using LLMs to automatically generate detailed contextual warnings about phishing threats. Our approach highlights specific features of a website and explains their malicious nature. By combining features detected by our local LLM model with the advanced language synthesis of GPT-3.5T, our system generates high-quality warnings that inform users about malicious features,"}, {"title": "3 METHOD", "content": "Large Language Models excel at comprehending the semantics of natural language, enabling them to detect subtle nuances and patterns in text [93, 113]. This capability is particularly beneficial for understanding complex textual data. The flexibility of LLMs extends to the realm of source code analysis as well. Researchers have effectively utilized models like BERT to analyze source code snippets, creating embeddings that capture the contextual relationships within the code [59], which have also proven useful for applications such as code completion [33]. Building on this foundation, LLMs have been adapted for malware identification [86, 109]. By training on a large repository of source code, these models can predict the characteristics of the syntax that should appear in a malicious file versus that in a benign file, detecting threats without the need for specific feature definitions or predefined rules during the training stage. We use a similar methodology for building our phishing detection model, which is capable of anticipating the necessary content (text or syntax) in a website to classify it as phishing, both at a macro level (the overall website context) and a micro level (specific code snippets). More specifically, if any part of a website's source code - or a combination thereof-resembles that of known phishing sites, it is flagged as phishing. Conversely, if it reflects the characteristics of known benign sites, it is marked as safe. We break down the various processes in our architecture below:"}, {"title": "3.1 Parsing the source code", "content": "A large part of a website's source code consists of aesthetic features, which include visual elements like layouts, colors, and styles [54] - primarily serving to enhance the user experience but are not reliable indicators for phishing detection [16]. Both legitimate and phishing websites may include high-quality designs [11] (in some cases, they might even utilize identical templates [90]). Moreover, these aesthetic components are generally unrelated to the actionable areas of a website-such as forms, input fields, buttons, and text which users interact with, and thus where phishing tactics are typically included [29]. Thus, including them in our training data not only introduces artifacts that will not be useful for phishing detection and may impact model performance but also increases the overall complexity and the time required for training the model, as the LLM needs to create more complex embeddings to map relationships between different tokens in the dataset [46].\nThus, to identify tags that will be relevant for phishing detection, we at first manually evaluated a set of 500 phishing websites from our training dataset (See Section 3.3), co-relating the source code of each website with its rendered website (as well as it's screenshot) to identify several tags which are used to include actionable phishing objects, i.e., elements in the source code that are either involved in user interactions, execute some malicious functionality or contain deceptive text. We dedicate the proceeding text to listing the chosen tags and why they are relevant: We analyze Headings (h1, h2, h3) as they are used to grab attention and may contain misleading language on phishing sites. Paragraphs (p) contain the main textual content and are often employed to convey deceptive messages. Links (a) are critical because they can redirect users to malicious sites or mimic legitimate URLs. Lists (ul, ol, li) on phishing sites might outline deceptive instructions. Form Tags (form) are used to collect sensitive data, making them a strong indicator of phishing. We also use title tags which appear on the browser tab and attackers might insert misleading titles that closely mimic legitimate sites. For example, a phishing site might use a title such as \"Secure Banking Login\" to deceive users into thinking they are accessing their bank's official website, which might give the (phishing website) credibility. We also include Footer tags as benign websites often contain more links or information in the footer compared to phishing sites [112], since the former is trying to provide resources to enhance user experience, whereas phishing attacks may provide little to no information in the"}, {"title": "3.2 Training data", "content": "To establish a ground-truth dataset for training our classifier, we utilized Lin at el's PhishPedia dataset [69], which includes 30K phishing and 30K benign websites. PhishPedia is not only the largest publicly available phishing dataset but is also notable for its reliability, having been used in several prior literature [48, 70]. Each entry in this dataset typically includes associated metadata, such as HTML source code, screenshots, and OCR text. The phishing URLs were originally obtained from OpenPhish [79], a widely recognized antiphishing blocklist. These URLs underwent preprocessing to eliminate inactive sites and false positives, and the researchers also implemented strategies to counteract evasive tactics like cloaking and conducted manual verifications to correct any inaccuracies in target brand representations. Conversely, the benign URLs were sourced from the Alexa Rankings list [5], a (now defunct) online database that ranked websites based on popularity. For the purposes of training our model, we only required the HTML source file, such that it can be parsed into a collection of relevant segments as previously detailed in Section 3.1. Out of the 30K phishing entries, 22,419 had an HTML file, whereas the same number for benign URLs was 26K. We further analyzed this dataset beyond targeted organizations [69], to obtain the distribution of HTML tags, libraries and frameworks used, and JavaScript APIs and function calls. Table 1 illustrates the distribution of HTML tags in both our phishing and benign websites. Notably, phishing websites contain fewer tags than benign websites. This is because phishing websites primarily focus on capturing user credentials, whereas benign websites need to offer a wide range of functionalities, requiring a more extensive and complex HTML structure. To identify the technology stack utilized by the phishing and benign websites, we used Wappalyzer [32], a tool that uses various heuristics"}, {"title": "3.3 Training and inference", "content": "During the training phase, we propose a sliding window approach, where each website sample, represented as parsed HTML content, is processed using this approach. The window size, W, is set to the maximum token count that the LLM can handle. However, to ensure coherence and completeness, the window is adjusted to include only complete tags. This adjustment is crucial as it prevents the misinterpretation of partial data. Each window of content inherits the label of the entire website. Despite the possibility that some sections that are benign are incorrectly assigned the phishing label due to belonging to a phishing website a window, given that the majority of the phishing websites were shorter, with most having fewer than 900 tokens (compared to benign sites having a media of 2,415 tokens), it is unlikely for a chunk not to have significant phishing features. The model is then trained on these chunks, $w_i$, defined as:\n$w_i=content[i\\times S+1:min((i\\times S+W),T)]$\nWhere S is the step size or stride of the sliding window. It determines how much the window moves forward after each iteration, and T is the total token count of the website's content. It represents the length of the content in terms of tokens, which is used to determine the boundaries for the sliding window approach. Here i ranges from 0 to $\\lfloor(T-W)/S\\rfloor$, ensuring full coverage of the content, and each window $w_i$ is assigned the label y, corresponding to the entire website's classification (either phishing or benign). We use 70% of our ground truth data for training the model and reserve 30% for testing. The model is trained using 5-fold cross-validation, with each fold undergoing 10 epochs of training.\nThe inference process mirrors the training setup. The model analyzes the parsed content of a website by breaking it down into chunks using the same sliding window technique. Each chunk is evaluated independently for signs of phishing. If any chunk is predicted as phishing, the entire website is classified accordingly:\nWebsite Classification=\\begin{cases}\nPhishing & \\text{if } p_i = Phishing\\\\\nBenign & \\text{otherwise}\n\\end{cases}\nTo counteract evasion techniques where attackers might pad websites with non-functional elements to dilute phishing content across chunks [81], a dynamic merging strategy is employed. If all chunks are initially predicted as benign, they are progressively merged and re-evaluated. This merging starts with the first two chunks, and if the combined chunk remains benign, it is further merged with the next chunk, continuing until a definitive phishing signal is detected or all chunks have been evaluated:\nPmerge=Model(w_1+w_2+...+w_k)\nFinal Classification=\\begin{cases}\nPhishing & \\text{if } p_{merge} = Phishing\\\\\nBenign & \\text{otherwise}\n\\end{cases}\nThe sliding window method to break down the parsed website content into multiple chunks for both training and inference is essential for several reasons. Firstly, it circumvents the token limitations of the LLM models, as most of them can process only a finite number of"}, {"title": "3.4 Choosing the optimum model", "content": "To choose the language model most suitable for building our classifier, we evaluated and compared eight language models that are considered to provide good performance in binary classification tasks (the number of parameters for the models in parenthesis): DistilBERT(66M) [92], TinyBERT(15M) [57], DeBERTA-base [51], FastText (10M) [47], and GPT-2 (117M) [85], Llama (7B), Bloom and T5. Our aim was to choose the model which provides the best trade-off between performance, speed, and memory usage. We trained each of these models on the training split (70%) of our ground truth dataset and tested them on our test split (30%).\nThe performance of these models is illustrated in Table 2. We found Llama 2 to have the best performance out of all the models tested, with an F1 score of 0.96. However, its median inference time of 34s is impractical for real-world usage where a phishing detection tool would need to process hundreds of thousands of URLs every day, and it has the highest median memory usage at 4,873.47MB. While the other two LLMs, Bloom and T5, had lower median inference times of 7.49 and 12.40 seconds respectively, they also performed worse (F1 scores of 0.88 and 0.74, respectively) and had high memory requirements (7351.89MB for Bloom and 1,279.49MB for T5). GPT-2 had the lowest median inference time (1.81 seconds) but also had the worst performance among the LLMs (F1 0.68) and required 922.41MB of memory. Moreover, all four LLMs required inference over GPUs (while they can be used for inference using the CPU, the prediction times will be much slower), and consequently required a large amount of VRAM for said inference per sample. Requiring GPU to provide good inference time is not ideal in a practical setting, as most consumer-end devices where the inference needs to occur (such as portable/mobile systems) might not have GPUs.\nWhen considering the Small Language Models (SLMs), DistilBERT had the highest accuracy level at 0.94, with both precision and recall impressively scoring 0.94. While not the quickest, its prediction times were reasonably efficient, with a median of 0.85 seconds per prediction and a memory usage of 502.17MB. In comparison, DeBERTa-base delivered lower precision and recall also fell short in time efficiency, and had a higher memory usage of 1,341.39MB. FastText, though more compact with a memory usage of 201.88MB, suffered from reduced accuracy and median prediction times of 1.43 seconds. Meanwhile, TinyBERT, the smallest model tested, offered quicker results at 0.78 seconds but at the cost of lower performance metrics and required 495.15MB of memory. All SLMs could provide inference over CPUs.\nAlso, due to the unavailability of commercial LLMs such as ChatGPT and Claude as local implementations, fine-tuning and testing on these models would incur significant costs for training and, especially for inference, given the millions of URLs an anti-phishing tool needs to process daily. However, we compare ChatGPT (3.5 Turbo) and GPT 4 with our model, and other ML-based phishing detection tools in Section 4.\nThus, considering all eight language models, based on a combination of high precision and recall, fast prediction times, and efficient memory usage, DistilBERT was the most suitable choice for our framework and was thus chosen as the final language model for our PhishLang framework. Recent studies have also found that foundation models (and their derivatives like DistilBERT) perform well for binary classification tasks [53, 58]."}, {"title": "4 REAL-TIME FRAMEWORK", "content": "We implement our model as a framework that continuously identifies new phishing websites and reports them to various antiphishing entities, including browser protection tools, commercial blocklists, scanners, and hosting providers."}, {"title": "4.1 Identifying new URLs", "content": "We run our model on Certstream [25], a Certificate Transparency Log that streams all SSL-certified URLs. This platform is extensively utilized to detect new phishing URLs, as a vast majority of phishing sites now employ SSL certification [63]. Prior research has also frequently leveraged this data source [70, 90]. If a website is flagged as benign with low confidence (<0.5), the model investigates the first five links in the website's source to detect phishing attacks that might not be apparent on the landing page, such as hidden phishing elements or image-based links. We observed an average of 27 domains per second on Certstream, and despite occasional delays in processing due to network bottlenecks, the model provided predictions within a median time of 9 minutes. From September 28, 2023, to January 11, 2024, PhishLang scanned 42.7M domains (172M websites through links), flagging 25,796 as phishing (0.057%).\nWe detected 17,396 regular, 3,159 JavaScript evasion, 3,349 Clickjacking, 1,057 DOM, and 835 Text encoding attacks. Some evasions might not have been identified and could have been classified as regular attacks. To verify the accuracy of the model, two coders, experienced in computer security concepts, manually assessed 2.5k websites flagged as phishing, ensuring a representative sample of each attack type. They first checked if the websites impersonated any of the 409 brands identified as common phishing targets by OpenPhish in August 2022 [80], followed by examining if the sites solicited sensitive information or triggered suspicious downloads. Downloads with four or more detections on VirusTotal were deemed malicious,"}, {"title": "4.3 Evaluating Commercial Phishing Countermeasures", "content": "In this section, we evaluate the effectiveness of different phishing countermeasures against websites identified by PhishLang. These include two commercial browser protection tools-Google Safe Browsing and Microsoft SmartScreen-and two open-source phishing blocklists, PhishTank [4] and OpenPhish [79]. Safe Browsing is the default protection in Google Chrome, Mozilla Firefox, and Safari, while SmartScreen powers Microsoft Edge, collectively protecting nearly 95% of all internet users [98]. We assess how many phishing websites identified by PhishLang are covered by these tools and whether our reporting helps detect the initially missed websites. PhishTank and OpenPhish, community-driven blocklists, are used by several commercial antiphishing tools [2, 77].\nPhishLang checks each identified phishing URL against these tools and blocklists in real time. If a URL is not covered, it is reported, and we monitor its detection status for up to a week. Considering ethical implications, we report all 26k identified websites immediately to ensure user protection, despite our 97% accuracy rate suggesting a potential 3% false positives being reported. This trade-off is acceptable given the typically higher noise in blocklist data [77, 89, 90].\nBrowser protection tools employ heuristic and ML-based models within the browser environment to detect phishing pages [68]. Using Selenium [41], we automate opening each website in a browser and take screenshots to confirm detection by these tools. All tools initially have low coverage, with Google Safe Browsing outperforming SmartScreen. Reporting undetected URLs significantly improved detection rates, especially for evasive phishing attacks. For instance, Google Safe Browsing's detection of JS evasion attacks increased from 35% to 73%, and SmartScreen's detection of DOM-based attacks rose from 18% to 86%. Regular phishing URL detection also improved significantly. Despite these improvements, it is important to note that these tools generally maintain high detection rates and low false positives [77]. Our model thus helps close detection gaps, particularly for new and evasive attacks, and accelerates the integration of new threat intelligence.\nPhishTank and OpenPhish have different responses to reported URLs. PhishTank lists reported URLs immediately, requiring community verification before labeling them as phishing or benign, and"}, {"title": "5 ADVERSARIAL IMPACT", "content": "The effectiveness of Machine Learning (ML) and Deep Learning (DL) models can be significantly reduced by adversarial inputs designed to lower the model's confidence score, which can also lead to misclassification. These inputs exploit inherent vulnerabilities related to the training data, architectural design, or developmental assumptions of the model. For example, in the case of Machine-Learning phishing webpage detectors (ML-PWD), Panum et al. [81] showed how image-based phishing detection models are vulnerable to FSGM pixel-level exploitation, where adversarial websites are modified to appear like benign ones. AlEroud et al. [39] explores how models relying on URL-based features can be exploited by employing perturbations in the URI structure. Both image-based features and URL-based attributes are irrelevant to our model, which solely relies on a collection of tags extracted directly from the HTML source code. Consequently, the only avenue for an attacker to exploit our model is to target the specific HTML tags we analyze and alter their associated text.\nWhile previous literature has investigated how attackers can modify attributes in the \"Feature space\" [21, 34, 66, 94] (i.e., when the website is processed into feature vectors specific to those that are used by the ML-PWD for prediction), recent studies have focused more on attackers introducing perturbations in the \"problem space\u201d (i.e., where the attacker directly modifies the webpage) [11, 22, 68]. This shift is due to real-world attackers operating in the \"problem space\" [83], where the perturbations introduced are subject to certain physical constraints. If these constraints are not met, which can happen when perturbations are added in the feature space, there is an increased risk of generating adversarial samples that, while potentially reducing the model's efficiency [105], may result in samples that are physically unrealizable.\nIn the context of phishing websites, this often manifests as notable artifacts in the webpage layout, as demonstrated by Yuan [112] and Draganovic et al. [43]. These artifacts make the sites suspicious to users, reducing the attacks' overall effectiveness. Moreover, to perturb the feature space, the attacker is required to have internal access to the processing pipeline of the ML-PWD- process that can be challenging and costly [18]. Additionally, most adversarial implementations in the literature target specific ML-PWDs,"}, {"title": "5.1 Evaluating Problem-space evasions", "content": "Apruzzese [110] and Yuan et al. [111] have conceptualized evasive attacks that are relevant and generalizable for creating phishing samples capable of evading the four most popular ML-PWDs. They identified 57 features that attackers commonly use to bypass ML-PWD systems. Building on this framework, Montaruli et al. [72] developed a novel set of 16 evasive attacks focusing on the problem space of adversarial attacks. These attacks involve directly modifying the HTML content using fine-grained manipulations, allowing for changes to the HTML code of phishing webpages without compromising their maliciousness or visual appearance. This approach successfully preserves both the functionality and appearance of the websites. Some examples of these attacks include InjectIntElem, which involves injecting a specified number of (hidden) internal HTML elements (e.g., <a> tags with internal links) into the body of the webpage and ObfuscateJS, where the JavaScript code within <script> elements of a webpage is obfuscated by encoding them into Base64 and then inserts a new script to decode and execute the original script. A full description of these attacks is provided in https://tinyurl.com/vcjer5my). For brevity, we do not go into comprehensive details about which feature (based on Apruzzese [110] and Yuan et al. [111]) these attacks target.\nSince different phishing websites can be compromised by different attacks, Montaruli et al also developed a query-efficient black-box optimization algorithm based on WAF-A-MOLE [36]. This algorithm employs an iterative methodology that involves successive rounds of mutation to modify the original malicious sample, aiming to reduce the confidence score provided by ML-PWD. Attacks are carried out as Single Round (SR) or Multi Round (MR) manipulations. SR manipulations, such as UpdateHiddenDivs, UpdateHiddenButtons, UpdateHiddenInputs, and UpdateTitle, are applied once to achieve their goal of hiding or modifying specific HTML elements and do not require further changes to evade detection. In contrast, MR manipulations, including InjectIntElem and InjectExtElem, require multiple sequential applications to progressively adjust the internal-to-external link ratios and other features to effectively evade detection. The optimizer begins by initializing the best adversarial example and score with the initial phishing webpage and its corresponding score. It then sequentially applies the SR manipulations, updating the best example and score whenever a new manipulation achieves a lower score."}, {"title": "5.2 Mitigating Adversarial Attacks", "content": "To mitigate or nullify the effectiveness of these attacks", "strategies": "a) Parser Modification Here", "modifications": "Attacks InjectIntElem (A1)", "samples": "S1", "display none\" to hide elements. S3 uses the <style> HTML element to achieve the same effect as S2 but through the <style> tag. S4 places HTML elements to be hidden inside the <noscript> tag, effective only if JavaScript is enabled in the victim's web browser. To counter these attacks, we propose two parser-based modifications. Patch 1 ignores any tags with a hidden attribute or \"display": "none\" in the style attribute", "#!\" or \"#none.\u201d To counter this attack, we propose Patch 3, a parsing modification that checks if the 'action' attribute directs to an internal section or is a valid external link (using Python's request library [44": "that nullified the attack effectiveness to 0. Finally", "patches": "Patch 4.1 converts Base64 (or any other non-UTF-8 encodings) to UTF-8 before parsing, and Patch 4.2 retrains the model with adversarial samples where A7 was the primary attack. Although these patches did not completely nullify the attack, they significantly reduced the median adversarial advantage to 0.047 (from 0.378) and the false negative rate to 1.68% (from 19.01%). Attack UpdateHiddenButtons (A12) obfuscates all disabled button elements by removing the disabled attribute and inserting a new <script> element that re-applies this attribute during rendering using the setAttribute() DOM method. This approach ensures that both the rendering and original behavior of the buttons are preserved. While a potential countermeasure could involve ignoring scripts with the disabled attribute, this could break the user experience. For example, in a web form where the \"Submit\" button is initially disabled and only enabled after all required fields are filled out, ignoring such scripts could prevent proper functionality. Therefore, to address this attack, we retrained our model with adversarial samples where A12 was the primary attack (Patch 5). This reduced the median adversarial advantage to 0.40 (from 0.276) and the false negative rate to 1.84% (from 18.35%). Attack UpdateHiddenInputs (A13) evades detection of hidden and disabled input elements by changing the type attribute from \"hidden\" to \"text\" and adding the hidden attribute. We adversarially trained with samples that had this attack as the primary focus (Patch 6), which reduced the median adversarial advantage to 0.035 (from 0.322) and the"}]}