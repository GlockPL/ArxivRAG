{"title": "Hierarchical Imitation Learning of Team Behavior from Heterogeneous Demonstrations", "authors": ["Sangwon Seo", "Vaibhav Unhelkar"], "abstract": "Successful collaboration requires team members to stay aligned, especially in complex sequential tasks. Team members must dynamically coordinate which subtasks to perform and in what order. However, real-world constraints like partial observability and limited communication bandwidth often lead to suboptimal collaboration. Even among expert teams, the same task can be executed in multiple ways. To develop multi-agent systems and human-AI teams for such tasks, we are interested in data-driven learning of multimodal team behaviors. Multi-Agent Imitation Learning (MAIL) provides a promising framework for data-driven learning of team behavior from demonstrations, but existing methods struggle with heterogeneous demonstrations, as they assume that all demonstrations originate from a single team policy. Hence, in this work, we introduce DTIL: a hierarchical MAIL algorithm designed to learn multimodal team behaviors in complex sequential tasks. DTIL represents each team member with a hierarchical policy and learns these policies from heterogeneous team demonstrations in a factored manner. By employing a distribution-matching approach, DTIL mitigates compounding errors and scales effectively to long horizons and continuous state representations. Experimental results show that DTIL outperforms MAIL baselines and accurately models team behavior across a variety of collaborative scenarios.", "sections": [{"title": "1 INTRODUCTION", "content": "Imitation learning (IL) is a paradigm for training agent behaviors using demonstrations [1]. IL typically assumes that the demonstrations are generated by an expert following a single, optimal policy."}, {"title": "2 RELATED WORKS", "content": "We begin with a brief overview of related research."}, {"title": "2.1 Imitation Learning of Multimodal Behavior", "content": "Extensive research has been conducted on learning multimodal behaviors from demonstrations. In works such as [14, 22], the authors extend Generative Adversarial Imitation Learning (GAIL) to learn a policy that depends on a learned latent state. This learned latent state effectively encodes different modes of the behavior. However, these methods assume the latent states remain static during a task execution; thus, their methods are unsuitable for modeling agent behavior whose latent states can change during the tasks. Other approaches, such as [32], use Conditional Variational Autoencoders (CVAE) to capture multimodal human behavior, but the learned latent space responsible for generating multimodality lacks grounding and difficult to associate with specific subtasks.\nInformed by the Option framework [43], another line of research leverages hierarchical policies to model multimodal behavior. Hierarchical policies typically consider two levels: high-level policies that govern decision-making over extended temporal intervals, and low-level policies responsible for executing specific actions within shorter time frames [17, 19]. To learn such policies from demonstrations, various approaches have been explored; e.g., variational inference [27, 45], hierarchical behavior cloning [18, 19, 50], and hierarchical variants of GAIL [7, 17, 21, 39]. Most recently, [38] propose a factored distribution-matching approach to train hierarchical policies. While all these methods show remarkable performance in single-agent tasks, their extension to multi-agent scenarios has been rarely explored and often lacks theoretical grounds."}, {"title": "2.2 Multi-agent Imitation Learning", "content": "Learning team behavior from demonstration can be framed as a multi-agent imitation learning problem. Since [40] introduced the multi-agent variant of generative adversarial imitation learning (called MA-GAIL), several extensions have been proposed to enhance its training efficiency and scalability [5, 6, 25, 33, 47, 49]. However, these methods generally assume that the demonstrations originate from a single multi-agent policy, limiting their ability to capture diverse team behaviors. Despite the importance of accounting for multimodality when modeling team behavior, only a few approaches have incorporated latent states into MAIL. Among these, [20] model agent roles as latent variables, while [46] represent strategies as latent features. However, both methods assume static latent states and do not consider their dynamics. To address this gap, [37] propose Bayesian Team Imitation Learner (BTIL), a multi-agent extension of [45], which can learn hierarchical policies"}, {"title": "3 BACKGROUND", "content": "In this section, we present preliminaries on distribution-matching-based imitation learning and introduce the mathematical model of team behavior."}, {"title": "3.1 Imitation Learning via Distribution Matching", "content": "Using the Markov Decision Process (MDP) framework, an agent's behavior is defined by a policy \\(\\pi(a|s)\\), which represents the probability distribution of an action \\(a\\) given a state \\(s\\). The goal of imitation learning is to minimize the discrepancy (represented as a loss \\(L\\)) between the learner's policy \\(\\pi\\) and the expert's policy \\(\\pi_e\\): \\(\\min_\\pi L(\\pi, \\pi_e)\\). However, due to the inaccessibility of \\(\\pi_e\\), this objective is often ill-defined and highly challenging to solve.\nTo address this, Ho and Ermon [15] reformulate imitation learning as a problem of matching the occupancy measures of the learner and the expert. The (normalized) occupancy measure of a policy \\(\\pi\\) is defined as \\(\\rho_\\pi(s, a) = (1 - \\gamma) \\Sigma_{t=0}^\\infty \\gamma^t p(s_t = s, a_t = a | \\pi)\\), implying the stationary distribution over states \\(s\\) and actions \\(a\\) induced by \\(\\pi\\). Thanks to the one-to-one correspondence between a policy \\(\\pi(s|a)\\) and its occupancy measure \\(\\rho_\\pi(s, a)\\) [30], matching the occupancy measures is equivalent to matching the policies. This can be formalized as:\n\n\\(\\arg \\min_\\pi D_f (\\rho_\\pi(s, a) || \\rho_{\\pi_e}(s, a))\\)\n\nwhere \\(\\rho_\\pi\\) is the learner's occupancy measure, \\(\\rho_{\\pi_e}\\) is the expert's occupancy measure, and \\(D_f\\) denotes the \\(f\\)-divergence [11]. While direct access to \\(\\rho_{\\pi_e}\\) is still infeasible, it can be approximated using the empirical distribution calculated from expert demonstrations \\(D\\). Due to its performance and scalability, since its introduction, the distribution matching approach has become a mainstream technique in imitation learning, giving rise to numerous variants, including the following multi-agent and hierarchical ones.\nMulti-Agent Variants. Assuming a unique equilibrium in multi-agent behaviors, Song et al. [40] formulate an occupancy measure matching problem in multi-agent settings:\n\n\\(\\arg \\min_\\pi \\sum_{i=1}^n D_f (\\rho_{\\pi_i \\pi_{\\neg i}}(s, a_i) || \\rho_{\\pi_e}(s, a_i))\\)\n\nwhere \\(\\pi_i\\) is joint policies except the \\(i\\)-th agent's policy, \\(\\pi_e\\) denotes multi-agent expert policy at equilibrium, and \\(\\rho_{\\pi_i, \\pi_{\\neg i}}(s, a_i) = (1 - \\gamma) \\Sigma_{t=0}^\\infty \\gamma^t p(s_t = s, a_{it} = a_i | \\pi_i, \\pi_{\\neg i})\\). This objective function implies that we can iteratively minimize the objective with respect to individual policies \\(\\pi_1, \\cdots, \\pi_n\\), and the updates can be calculated similarly to the single-agent problem by considering other agents' policies as part of the environment dynamics.\nHierarchical Variants. Jing et al. [17] extend occupancy measure matching approach to hierarchical imitation learning. They model an agent behavior as an option policy \\(\\tilde{\\pi} = (\\pi_l, \\pi_H)\\), where"}, {"title": "3.2 Model of Team Behavior", "content": "We borrow a model of team behavior introduced in [35] to represent our multi-agent behavior in sequential team tasks. The model consists of a decentralized partially observable MDP (Dec-POMDP) to capture the task dynamics and an Agent Markov models (AMM) to represent agents' multimodal behavior [26, 45].\nDec-POMDP. Dec-POMDP is a probabilistic model representing the dynamics of the partially observable sequential multi-agent tasks. It is expressed as a tuple \\(M = (n, S, A_i, T, \\mu_0, X_{\\Omega_i}, {O_i}, \\gamma)\\), where \\(n\\) is the number of agents, \\(S\\) is the set of states \\(s\\), \\(A_i\\) is the set of the \\(i\\)-th agent's actions \\(a_i\\), \\(\\Omega_i\\) is the set of the \\(i\\)-th agent's observations \\(o_i\\), \\(T(s'|s, a)\\) denotes the probability of a state \\(s'\\) transitioning from a state \\(s\\) and a joint action \\(a= (a_1, \\dots, a_n)\\), \\(\\mu_0(s)\\) is an initial state distribution, and \\(\\gamma\\) is a discount factor. We define an extended action set as \\(A^+ = A_i \\cup \\{#\\}\\), where the symbol \\(\\#\\) denotes \"Not Available\". \\(O_i : S \\times A^+ \\rightarrow 2^{\\Omega_i}\\) is an observation function for the \\(i\\)-th agent, which maps a pair of state \\(s\\) and previous joint action \\(a\\) to an individual observation \\(o_i \\in \\Omega_i\\). The values of previous actions \\(a_i\\) at time \\(t=0\\) are set as \\(\\#\\). We denote capital letters without subscripts as joint spaces or joint functions, e.g., \\(A = \\times A_i\\) for a joint action space and \\(O = \\Pi_i O_i\\) for a joint observation function.\nAgent Markov Model (AMM). When faced with complex team tasks, humans typically break them down into subtasks and dynamically adjust their plan regarding which subtasks to perform and in what order. Once they decide on the next subtask, they execute the necessary actions to complete it. AMM is designed to account for this hierarchical human behavior, and thus, is equivalent to hierarchical policies of Option framework with a one-step option [17, 43]. Given a task model \\(M\\), AMM defines the behavior model of the \\(i\\)-th agent as a tuple \\((X_i, \\pi_i, \\zeta_i; M)\\), where \\(X_i\\) is the set of the possible subtasks, \\(\\pi_i (a_i|o_i, x_i)\\) denotes a subtask-driven policy, and \\(\\zeta_i (x_i|o_i, x^-\\underline{i})\\) is the probability of an agent choosing their next subtask \\(x_i\\) based on an observation \\(o_i\\) and the current subtask \\(x^-\\underline{i}\\)1. Following [17], we define the value of the previous subtask at time \\(t=0\\) as \\(\\#\\) and express the initial distribution of subtasks as \\(\\zeta_i (x_i|o_i, x^-\\underline{i}=\\#\\)). Similar to previous works [17, 37], we assume the set of possible subtasks, \\(X_i\\), is finite and given as prior knowledge. We then represent the AMM for the \\(i\\)-th agent simply as \\((\\pi_i, \\zeta_i)\\), omitting the non-learnable components \\(X_i\\) and \\(M\\)."}, {"title": "4 PROBLEM FORMULATION", "content": "While Seo et al. [35] emphasize the need for modeling team behavior under partial observability and propose a corresponding mathematical model, few multi-agent imitation learning methods have been developed to address such complex teamwork models. To our knowledge, BTIL is the only approach to learning multi-model team behavior from demonstrations [37]. However, BTIL does not account for partial observability, and its applicability is limited to small, discrete domains, as both high- and low-level policies are constrained to categorical distributions. Thus, a practical method for learning team behavior models that addresses multimodality, partial observability, and scalability is still lacking. To derive such a method, we first formulate the problem of multi-agent imitation learning from heterogeneous demonstrations."}, {"title": "4.1 Formalizing Hierarchical Multi-Agent Distribution Matching", "content": "Inspired by the recent success of distribution matching-based imitation learning, we aim to apply this method to learn the team behavior model. Similar to the hierarchical variants for fully observable single-agent scenarios in Sec. 3.1, we define oaxx-occupancy measure for the \\(i\\)-th agent given a task model \\(M\\) and joint agent models \\(N_{1:n}\\) as:\n\n\\(\\rho_{N_i, N_{-i}} (o_i, a_i, x_i, x_i^-\\underline{ }) = (1 - \\gamma) \\Sigma_{t=0}^\\infty \\gamma^t p(o_{it}=o_i, a_{it}=a_i, x_{it} =x_i, x_{it-1}=x_i^-\\underline{ } | N_{1:n}, M)\\)\n\nThe notation \\(\\rho_{N_i, N_{-i}}\\), borrowed from MA-GAIL [40], represents the occupancy measure induced by the agent \\(i\\)'s policy \\(N_i\\) and other agents' policy \\(N_{-i}\\). Unless ambiguous, we simply denote \\(\\rho_i = \\rho_{N_i, N_{-i}}\\).\nBy combining this occupancy measure with the multi-agent variant of distribution matching introduced in Sec. 3.1, we can formulate the distribution-matching problem for team behavior with \\(n\\) agents as follows:\n\n\\(\\arg \\min_{N_{1:n}} \\sum_{i=1}^n D_f (\\rho_i (o_i, a_i, x_i, x_i^-\\underline{ }) || \\rho_E (o_i, a_i, x_i, x_i^-\\underline{ }))\\)\n\nwhere \\(\\rho_E\\) denotes the oaxx-occupancy measure of the expert team model \\((\\pi_e, \\zeta_e)\\). In Sec. 5, we provide theoretical justification for using Eq. 3 as the imitation learning objective. While this extension seems natural, the theoretical results in the existing literature are insufficient to guarantee that occupancy measure matching is equivalent to policy matching in partially observable multi-agent scenarios.\nAdditionally, informed by IDIL [38], we aim to adopt a factored approach to minimize the objective above. This factored approach enables us to leverage existing non-adversarial imitation learning methods, such as IQLEARN [10], which demonstrate more stable training compared to generative adversarial approaches. However, since the theoretical foundations for factored distribution matching are also developed under the assumption of full observability, further theoretical analysis is necessary to ensure its applicability in partially observable multi-agent settings. We provide this analysis in Sec. 5."}, {"title": "4.2 Problem Statement", "content": "Since we cannot know which subtasks each member has in mind at the time of task execution, demonstrations contain only observations and actions. We define the set of \\(d\\) demonstrations as \\(D = {\\tau^m}_{m=1}^d\\), where \\(\\tau = (o, a)_{0:h}\\) is a trajectory of a team's task execution. We denote an individual trajectory of the \\(i\\)-th agent and the set of them as \\(\\tau_i = (o_i, a_i)_{0:h}\\) and \\(D_i = {\\tau_{m,i}}_{d=1}\\), respectively, adding a subscript \\(i\\). The sequence of expert's subtasks corresponding to the \\(m\\)-th demonstration is defined as \\(X^m = (x^m_{0:h})\\). Since the labels of the subtasks are challenging to collect in practice, only a small portion of them (e.g., for \\(l (\\leq d)\\) demonstrations) are optionally available. Thus, our goal is to learn agent models \\((\\pi, \\zeta\\))1:n that exhibit the behaviors of \\(n\\) team members from the following inputs: a multi-agent task model \\(M\\), the set of possible subtasks \\(X\\), heterogeneous demonstrations \\(D\\), and optionally partial labels of subtasks \\({X^m}_{m=1}^l\\)."}, {"title": "5 LEARNING MODEL OF TEAMWORK VIA FACTORED DISTRIBUTION MATCHING", "content": "As mentioned in Sec. 4.1, matching occupancy measures does not always guarantee matching the team behavior models unless a one-to-one correspondence is established between the agent model (i.e., partial observation-based hierarchical policy) \\(N_i = (\\pi_i, \\zeta_i)\\) and its oaxx-occupancy measure \\(\\rho_i\\). Therefore, we first present this one-to-one correspondence, which extends the Theorem 1 from [17] to multi-agent partially-observable settings.\nTheorem 5.1. For each agent \\(i\\), given a multi-agent task model \\(M\\) and other agents' models \\(N_{-i}\\), suppose \\(\\rho_i\\) is the oaxx-occupancy measure for a stationary agent model \\(N_i = (\\pi_i, \\zeta_i; M)\\) where\n\n\\(\\zeta_i (x|o, x^-\\underline{ }) = \\frac{\\Sigma_{a} \\rho_i (o, a, x, x^-\\underline{ })}{\\Sigma_{x,a} \\rho_i (o, a, x, x^-\\underline{ })} \\)\n\n\\(\\pi_i (a|o, x) = \\frac{\\Sigma_{x^-\\underline{ }} \\rho_i (o, a, x, x^-\\underline{ })}{\\Sigma_{a,x^-\\underline{ }} \\rho_i (o, a, x, x^-\\underline{ })} \\)\n\nThen, \\(N_i = (\\pi_i, \\zeta_i; M)\\) is the only agent model whose oaxx-occupancy measure is \\(\\rho_i\\).\nThis can be proved similarly to Theorem 1 of [17] after deriving the stationary distributions of policy \\((\\pi w/v)\\) and state transition \\(T(v'|v, w)\\), where \\(v = (o, x^-\\underline{ })\\) and \\(w = (x, a)\\). The complete proof is provided in the Appendix. This theorem implies that we can consider the imitation learning of agent models \\(N_{1:n}\\) as matching the oaxx-occupancy measures between \\(\\rho_{N_i,E_{-i}}\\) and \\(\\rho_E\\) for all \\(i\\). Here, \\(\\rho_{N_i,E_{-i}}\\) denotes the oaxx-occupancy measure induced by the \\(i\\)-th agent model \\(N_i\\) with other agents' models given as expert models \\(E_{-i} = (\\pi_e^-\\underline{i}, \\zeta_e^-\\underline{i})\\). Thus, we can factorize the occupancy measure matching of the joint team model as follows: \\(\\arg \\min_{N_{1:n}} \\sum_{i=1}^n D_f (\\rho_{N_i,E_{-i}} (\\cdot) || \\rho_E (\\cdot))\\). Due to the one-to-one correspondence, the following two problems lead to the same optimal solution, \\(N_E\\):\n\n\\(\\arg \\min_{N_{1:n}} \\sum_{i=1}^n D_f (\\rho_{N_i,E_{-i}} (\\cdot) || \\rho_E (\\cdot)) = \\arg \\min_{N_{1:n}} \\sum_{i=1}^n D_f (\\rho_{N_i,N_{-i}} (\\cdot) || \\rho_E (\\cdot)) = N_E\\).\n\nThis justifies our objective function, Eq 3, for learning the expert team behavior model via distribution matching."}, {"title": "6 DEEP TEAM IMITATION LEARNER", "content": "With the theoretical foundations established in the previous section, we now present DTIL, a practical algorithm designed to minimize Eq. 4. The distribution matching framework enables policies to be represented using deep neural networks and efficiently learns them by leveraging additional interactions with the environment. As a result, DTIL is capable of learning team behavior models even in highly complex tasks.\nAs mentioned in Sec. 3.1, the expert occupancy measure is typically estimated from expert demonstrations, i.e., \\(\\rho_E (o, a, x, x^-\\underline{ }) \\approx \\mathbb{E}_D [\\mathbb{1}(o, a, x, x^-\\underline{ })]\\). However, as our demonstration \\(D\\) does not contain the labels of subtasks, we cannot compute this empirical distribution. Thus, similar to [17], we take an expectation-maximization (EM) approach to iteratively optimize Eq. 4. Alg. 1 outlines DTIL. In line 4 (E-step), it predicts unknown expert intents from \\(D\\) using the current estimate of agent models \\((\\pi_{\\theta}, \\zeta_{\\phi})\\). In line 5, it collects online samples by interacting with the environment. Then, in line 6 (M-step), it updates agent model parameters \\((\\theta, \\phi)\\) via occupancy measure matching."}, {"title": "7 CONVERGENCE PROPERTIES", "content": "While the optimization of Eq. 4 will provide us with agent models whose occupancy measure is close to that of experts, it is not guaranteed that our practical, factored approach of iteratively minimizing each term of Eq. 4 will converge. Although IDIL provides theoretical analysis regarding the convergence of this factored distribution matching, their analysis is made under the assumption of full observability, thereby inapplicable to our setting. Thus, we provide a theoretical analysis regarding the convergence of DTIL in this section.\nWe start the analysis by defining two approximations of the expert oaxx-occupancy measure, \\(\\rho_{\\theta, \\phi}^k\\) and \\(\\rho_{\\theta, \\phi}^k\\). These approximations are computed from the estimates of the expert's oax-occupancy and oxx-occupancy measures, i.e., \\(\\rho(o, a, x)\\) and \\(\\rho(o, x, x^-\\underline{ })\\), with the estimate of expert models \\(N^k = (\\pi^k, \\zeta^k)\\):\n\n\\(\\rho_{\\theta, \\phi}^k(o_i, a_i, x_i, x_i^-\\underline{ }) = \\rho(o_i, x_i, x_i^-\\underline{ })\\pi_{\\theta}(a_i|o_i, x_i)\\)\n\n\\(\\rho_{\\theta, \\phi}^k(o_i, a_i, x_i, x_i^-\\underline{ }) = \\rho(o_i, a_i, x_i)\\pi(x_i^-\\underline{ }|o_i, a_i, x_i, N^k)\\)"}, {"title": "8 EXPERIMENTS", "content": "Through numerical experiments, we now assess DTIL's performance against MAIL baselines across multiple domains."}, {"title": "8.1 Experimental Setup", "content": "8.1.1 Domains. We evaluate DTIL across multiple domains with varying complexity, including the Multi-Jobs-n suite, Movers, Flood, and the SMACv2 suite. The key characteristics of our experimental domains are presented in Table 1. Our domains include both continuous and discrete observation and action spaces, with varying numbers of agents (2-5) who are either subtask-agnostic or subtask-driven. Please refer to Figs. 3-1 for illustrations of Multi-Jobs-n domains. Remaining domains are illustrated in Fig. 2.\nThe Multi-Jobs-n simulates the motivating example introduced in Fig. 1 in continuous observation and action spaces. Movers and Flood are collaborative team tasks in partially observable environments introduced by [35], considering only discrete states and actions. These domains are designed to admit multiple near-optimal strategies, allowing agents to exhibit multimodal behaviors. We create synthetic agents exhibiting hierarchical behavior and generate 50 and 100 demonstrations for training and testing, respectively, for each domain. SMACv2 is a challenging benchmark for multi-agent reinforcement learning [9]. We consider two domains in this suite: Protoss-5v5 and Terran-5v5, where a team of five agents is tasked with defeating five enemies. We obtain a multi-agent policy via MAPPO [48] and generate 50 trajectories per domain for training. In all domains, team members must make decentralized decisions in partially observable environments. For more details, please refer to the Appendix.\nBaselines. We compare our approach with Behavior Cloning (BC), MA-GAIL (MG) [40], INDEPENDENT-IQL (IIQL), and MA-OPTIONGAIL (MOG). BC is a supervised learning approach to learning policies, which serves as a fundamental baseline for imitation learning [23]."}, {"title": "8.2 Results", "content": "8.2.1 DTIL achieves expert-level task performance. Table 2 shows the task rewards averaged over three trials for Multi-Jobs-n (MJ-n), Movers, and Flood. We observe that IQLEARN-based approaches, IIQL and DTIL, generally perform better than approaches based on generative adversarial imitation learning. Between MG and MOG, MG performed better, likely because MG additionally utilizes other agents' information during training. In contrast, MOG and DTIL take only individual observation-action trajectories and do not utilize any other information that might break the partial observability condition even during the training phase.\nWhile DTIL outperformed IIQL in Multi-Jobs-2, it performed on par in other domains. We believe this is due to the domains being too simple, allowing subtask-agnostic approaches to extract one optimal solution from demonstrations. In more complex domains, we could observe an improvement in task performance due to the hierarchical structure of our agent model. As shown in Table 3, DTIL achieved the highest task reward and win rate in both the SMACv2 domains: Protoss-5v5 and Terran-5v5. We want to highlight that even though IIQL often achieves high task rewards, it can neither learn multimodal behavior nor utilize semi-supervision. On the other hand, DTIL can improve its performance by augmenting subtask labels. As shown in Table 2, DTIL achieved a task reward similar to the expert task reward in all domains with 20% supervision.\n8.2.2 DTIL accurately learns multimodal team behavior. As mentioned in Sec. 4, our goal is to learn the different team behaviors generated by expert teams rather than a unimodal team policy. Additionally, in human-AI teaming applications, an AI agent must accurately interpret its human teammate's high-level plan. To achieve this, it is essential to learn a model that exhibits hierarchical behavior aligned with expert team members. Table. 4 presents the accuracy of subtask inference computed with 20%-supervision models. Note that without any supervision, we cannot associate the learned latent values with the actual subtasks. In all cases, DTIL outperforms MOG and the random baselines.2\n8.2.3 DTIL outperforms BTIL in more complex tasks. As demonstrated in Table 2, DTIL outperformed BTIL in terms of task reward."}, {"title": "9 CONCLUSION", "content": "This work introduces DTIL, an algorithm for learning generative models of team behavior from heterogeneous demonstrations. Experiments show that DTIL outperforms state-of-the-art multi-agent imitation learning baselines and captures expert team behavior across six diverse teamwork domains. Additionally, DTIL can generate a wide range of expert team behaviors. DTIL also motivates future research directions. First, DTIL assumes a known, finite set of subtasks, though real-world subtasks may be difficult to define a prior or represent as scalars. Future MAIL methods should explore more expressive hierarchical representations. Second, by enabling generative models of team behavior, DTIL can enable novel human-AI teaming applications, such as AI-enabled team coaching [34, 36] and end-user programming of multi-agent systems [2, 41]. We invite developers of these and other impactful applications to utilize DTIL and make additional details available at http://tiny.cc/dtil-appendix"}, {"title": "ACKNOWLEDGMENTS", "content": "This research was supported in part by NSF award #2205454, ARO CA# W911NF-20-2-0214, and Rice University funds."}]}