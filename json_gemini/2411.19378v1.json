{"title": "LIBRA: LEVERAGING TEMPORAL IMAGES FOR BIOMEDICAL RADIOLOGY ANALYSIS", "authors": ["Xi Zhang", "Zaiqiao Meng", "Jake Lever", "Edmond S. L. Ho"], "abstract": "Radiology report generation (RRG) is a challenging task, as it requires a thorough understanding of medical images, integration of multiple temporal inputs, and accurate report generation. Effective interpretation of medical images, such as chest X-rays (CXRs), demands sophisticated visual-language reasoning to map visual findings to structured reports. Recent studies have shown that multimodal large language models (MLLMs) can acquire multimodal capabilities by aligning with pre-trained vision encoders. However, current approaches predominantly focus on single-image analysis or utilise rule-based symbolic processing to handle multiple images, thereby overlooking the essential temporal information derived from comparing current images with prior ones. To overcome this critical limitation, we introduce Libra, a temporal-aware MLLM tailored for CXR report generation using temporal images. Libra integrates a radiology-specific image encoder with a MLLM and utilises a novel Temporal Alignment Connector to capture and synthesise temporal information of images across different time points with unprecedented precision. Extensive experiments show that Libra achieves new state-of-the-art performance among the same parameter scale MLLMs for RRG tasks on the MIMIC-CXR. Specifically, Libra improves the RadCliQ metric by 12.9% and makes substantial gains across all lexical metrics compared to previous models\u00b9.", "sections": [{"title": "1 INTRODUCTION", "content": "Radiology reports are a key component of biomedical radiology analysis, designed to enhance clarity and efficiency in medical communication. These structured reports are typically divided into distinct sections, such as Findings, Impression, Indication, Technique, Comparison, and History (Ganeshan et al., 2018). Serving as the primary medium for radiologists to convey their findings and conclusions from imaging studies like chest X-rays (CXRs), radiology reports play a crucial role in guiding diagnostic and therapeutic decisions for various diseases (Najjar, 2023). However, generating these reports manually is a complex and time-consuming task. Automating radiology report generation (RRG) presents a valuable opportunity to boost radiologist productivity, improve communication, and alleviate burnout (Zhang et al., 2020b). Despite its potential, RRG is a challenging task due to the intricate nature of medical imaging and the need for precise and accurate documentation.\nAlthough the advancements in paradigm models like LLaVA (Liu et al., 2023) and InstructBLIP (Dai et al., 2023), they still exhibit a significant performance gap when applied to RRG tasks. In particular, their effectiveness diminishes in biomedical contexts due to the substantial differences between biomedical and general image-text pairs (Tu et al., 2023; Saab et al., 2024). Consequently, these models often perform at a surface level, akin to a layperson-level understanding, when dealing with specialised medical imaging tasks. Recent advancements have explored continued pre-training of general-purpose foundation models for medical tasks, but these models still fall short in addressing the complexities of medical image analysis (Li et al., 2023a; Chaves et al., 2024; Park et al., 2024)."}, {"title": null, "content": "Harnessing the full potential of multimodal large language models (MLLMs) for understanding and reasoning over multimodal data in the biomedical domain poses a significant challenge, primarily due to the granularity and specificity required in medical image analysis (Wang et al., 2023a). Preliminary attempts to apply MLLMs to RRG have largely concentrated on generating reports from a single image, often neglecting the crucial temporal relationships between images taken at different time points (Zhang et al., 2024b). The MIMIC-CXR Database (Johnson et al., 2019a) reveals that 67% of patients underwent at least two studies at different time intervals, underscoring the necessity of incorporating temporal context in clinical practice. This highlights a critical gap in existing models, which are not equipped to handle the dynamic progression of medical conditions reflected in sequential imaging studies.\nIn standard clinical practice, radiologists rely heavily on comparing current imaging results with previous studies to assess temporal changes. However, many models inadvertently introduce spurious references to prior examinations that do not exist during inference, leading to inaccurate reports. Recent advancements, such as MedVersa (Zhou et al., 2024) and MAIRA-2 (Bannur et al., 2024), can process multiple images simultaneously. These models insert visual tokens from different images at specific points within the textual input, hinging on the LLM to interpret reference information. Additionally, LLaVA-Rad (Chaves et al., 2024) utilised GPT-4V (OpenAI et al., 2024) for radiology report extraction and augmentation, eliminating hallucinations associated with prior image references in the dataset. Despite these efforts, existing models still lack integrated mechanisms for perceiving temporal information within their architectures, which restricts the model's ability to achieve the temporal awareness required for comprehensive reporting. Meanwhile, existing MLLMs typically utilise visual embeddings from the last or penultimate layer of the image encoder (Chen et al., 2023a; Zhang et al., 2024a), which can only capture the global characteristics of the image. However, RRG tasks require not only identifying findings but also capturing subtle details such as severity, extent, and the progression of findings (Sloan et al., 2024). Such fine-grained details are difficult to represent using embeddings from any single layer of the image encoder (Jiang et al., 2024). Therefore, for the high-granularity demands of RRG tasks, relying solely on representation from a single layer limits the quality of report generation. To tackle these limitations, we aim to enhance the temporal awareness of MLLMs for RRG task by tackling two main challenges:\n\u2022 It is non-trivial to design effective structures in MLLMs to handle prior study citations across various time points in the RRG task.\n\u2022 The scarcity of effective connectors in MLLMs capable of handling the high-granularity requirements of downstream tasks.\nTo overcome these challenges, we propose Libra (Leveraging Temporal Images for Biomedical Radiology Analysis), a novel framework designed to incorporate temporal change information into the RRG task. Libra employs a pre-trained visual transformer, RAD-DINO (P\u00e9rez-Garc\u00eda et al., 2024), as the image encoder to generate robust image features, which are then refined using a new adapter specifically designed for the temporal awareness, before being fed into the medical large language model (LLM), Meditron (Chen et al., 2023b). Through a two-stage training strategy, Libra demonstrates the potential of MLLMs for specialised radiology applications. Our modular approach integrates state-of-the-art open-source pre-trained models for image and text while using a temporal-aware adapter to align these modalities with the text embedding space. Extensive experiments show that Libra substantially outperforms existing MLLMs in RRG tasks, demonstrating superior cross-modal understanding and reasoning capabilities. Specifically, our contributions include:\n\u2022 We present Libra, the temporal-aware MLLM capable of capturing temporal changes and overcoming the challenge of handling prior study citations, setting a new state-of-the-art performance in RRG tasks on the MIMIC-CXR dataset among MLLMs of the same parameter scale.\n\u2022 We designed the Temporal Alignment Connector (TAC) with two components: the Layerwise Feature Extractor (LFE), which extracts high-granularity image feature embedding from the encoder, and the Temporal Fusion Module (TFM), which integrates temporal references from prior studies. This enhances Libra's ability to capture and utilise temporal information in the RRG task."}, {"title": "2 LIBRA: BEING AWARE OF TEMPORAL CHANGES IN CHEST X-RAYS REPORT GENERATION", "content": null}, {"title": "2.1 MODEL ARCHITECTURE", "content": "Libra model follows the common architecture of MLLMs, such as LLaVA, which includes an image encoder, a text decoder (i.e., an LLM pre-trained based on text-only data) and an adapter module for mapping visual representations into text space. In this work, we utilise a frozen biomedical image encoder, RAD-DINO (P\u00e9rez-Garc\u00eda et al., 2024), a visual transformer extensively pre-trained on medical scans using the DINOv2 image-only self-supervised learning approach (Oquab et al., 2024). The LLM is deployed by Meditron-7B (Chen et al., 2023b), which builds on Llama-2 (Touvron et al., 2023) and further pre-trained on a specialised medical corpus. However, for the adapter module, we specifically designed a Temporal Alignment Connector (TAC) to capture and integrate temporal information across two images from different time points, as shown in Figure 1. The TAC bridges the image encoder and LLM. It consists of two components: the Layerwise Feature Extractor (LFE), which extracts high-granularity image representations, and the Temporal Fusion Module (TFM), which integrates temporal references from prior studies. This design enables Libra to effectively manage temporal data and enhances its ability to generate accurate and coherent radiology reports."}, {"title": "2.2 TEMPORAL ALIGNMENT CONNECTOR (TAC): NEW ADAPTER DESIGN", "content": "To address the challenges of integrating temporal information and aligning high-granularity image features in RRG tasks, we developed the TAC module. It bridges the gap between visual features extracted from multiple temporal snapshots of a patient's CXRs and the language model. The TAC consists of two sub-modules: the LFE, which is to extract the layerwise representation of images, and the TFM, which is to capture the temporal-aware representation between two images."}, {"title": "2.2.1 LAYERWISE FEATURE EXTRACTOR (LFE)", "content": "To leverage abundant feature information in a pre-trained image encoder, we extract all image patch token features from each hidden layer for a given input image. Specifically, RAD-DINO has 12 hidden layers and processes 518 \u00d7 518 images into 14 \u00d7 14 patches, generating 1,369 patch token"}, {"title": null, "content": "sequences per hidden layer for each image. We do not use the [CLS] token. Following this, we obtain patch embeddings of the same dimension from each layer, denoted as $E_{img} \\in \\mathbb{R}^{N \\times D_{img}}$, where $N$ is the number of patch tokens and $D_{img}$ is the embedding dimension of the image encoder. Then, we combine them as $E_{img} = \\{E_{img}^i\\}_{i=1}^n$, where $n$ is the number of hidden layers. Drawing from VGG (Simonyan & Zisserman, 2015), we employ a progressive compression strategy to reduce the layer dimension, ensuring that the LFE captures the most relevant features. Initially, we utilise Squeeze-and-Excitation (SE) Networks (Hu et al., 2019), which construct informative features by integrating both spatial and channel-wise information within local receptive fields at each layer. The SE block is applied to obtain calibrated feature representations, using GELU (Hendrycks & Gimpel, 2023) as the activation function. Next, we employ a specialised pointwise convolution module to align the feature spaces across different layers, using a depthwise 2D convolution with filters and stride of 1, without bias. The dimensions are gradually reduced to better align with the text in the LLM. The compressed features are represented as $A_{img} \\sim Conv2d(SE(E_{img}^{k}))$, where $k$ is the original layer number and $j$ is the layer number after compression. Following the size-reduction pattern of convolutional layers in VGG, the image representations are compressed according to $\\{k,j\\} \\in \\{12,6,3,1\\}$. Through three stages of progressive compression, we obtain the final representation of image patch features. We define the function $LFE(.)$ to project image features into the same dimension:\n$A_{img} = LFE(E_{img}) \\rightarrow A_{img} = Conv2d(SE(A'')) \\in \\mathbb{R}^{1 \\times N \\times D_{img}$,\nwhere $A'' = Conv2d(SE(A'))$, $A' = Conv2d^2(SE^2(E_{img}))$. The LFE independently pro-cesses each image to produce a unified representation for temporal alignment, as (a) in Figure 1."}, {"title": "2.2.2 TEMPORAL FUSION MODULE (TFM)", "content": "The fusion module is inspired by the transformer decoder. It utilises prior images as auxiliary information to weight the current image, generating a refined, temporal-aware representation suitable for the LLM's latent space. The TFM uses an attention mechanism to dynamically learn relationships between image pairs, enabling the model to adapt to various temporal changes.\nPrior Image Prefix Bias: The dataset contains samples with and without a prior image. The patch token features of the current and previous images processed by the LFE are denoted as $A_{curr}^{img}$ and $A_{prior}^{img}$. If no true prior image exists, we set $A_{prior}^{img} = A_{curr}^{img}$, effectively using the current image as a dummy prior. To differentiate this case, we introduce a trainable bias $b_{prior}$. Following the attention mechanism's scaling techniques for adjusting hidden space degrees of freedom with a chi-square distribution (Vaswani, 2017; Wang et al., 2018a), we apply an exponent of $\\sqrt{d}$ to the cosine similarity, where $d$ is the hidden dimension of the LLM, to obtain a weighted bias addition to $A_{prior}^{img}$:\n$A'_{prior} = A_{prior}^{img} + b_{prior} \\left(\\frac{Cos(A_{curr}^{img}, A_{prior}^{img})+1}{2}\\right)^{\\sqrt{d}} A_{img}$,\nThis nonlinear scaling amplifies higher similarity values, effectively modulating the influence of prior image features. When no true prior image is available, the high similarity score ensures that the effect of the dummy prior is adequately represented. This adjustment prevents samples with a dummy prior image from undergoing redundant rounds of parallel multi-head self-attention during subsequent propagation through the transformer blocks, as shown in Figure 1.\nTransformer Block: Our transformer block includes a multi-head cross-attention sub-layer, multi-head self-attention sub-layers, and two multi-layer perceptron (MLP) sub-layers. As shown in Figure 1 (b). The paired $(A_{curr}^{img}, A_{prior}^{img})$ are processed with layer normalization and residual connections:\n$T^{self}_{curr} = LayerNorm(A_{curr}^{img} + SelfAttn(A_{curr}^{img}; A_{curr}^{img})),$\n$T^{self}_{prior} = LayerNorm(A_{prior}^{img} + SelfAttn(A_{prior}^{img}; A_{prior}^{img})),$\n$T^{attn} = LayerNorm(T^{self}_{curr} + CorssAttn(T^{self}_{curr}; T^{self}_{prior})),$\n$T^{o} = LayerNorm(A_{img} + MLP_{attn}(T^{attn})),$\nwhere $MLP_{attn}$ is a simple neural network composed of two fully connected layers with GELU as the activation function. After that, the features are processed through $MLP_{final}$, a straightforward"}, {"title": null, "content": "neural network consisting of four fully connected layers with the same activation function, but with hidden dimensions matching those of the LLM. We define our fusion module function as $TFM(\\cdot)$.\n$Z_{img} =TFM(A_{curr}^{img}, A_{prior}^{img}) \\Leftrightarrow Z_{img} = MLP_{final}(T^{attn}),$\nSubsequently, $Z_{img} \\in \\mathbb{R}^{N \\times d}$ becomes the input sequence that LLM can comprehend, where $N$ is the number of patch tokens and $d$ is the hidden dimension of LLM. The final output is a refined feature representation that encapsulates the temporal evolution of the patient's condition, making it suitable for the language model to generate accurate and contextually aware radiology reports."}, {"title": "2.3 PROMPT DESIGN", "content": "To enhance Libra's ability to perceive temporal changes and integrate medical information in the RRG task, we designed a comprehensive prompting strategy comprising a system prompt and detailed clinical prompts, as shown in Figure 1. The system prompt is crafted to enable the LLM to recognise temporal variations: \u201cThe assistant specialised in comparing Chest X-ray images, identifying differences, and noting temporal changes.\u201d Following this, additional sections of the report, such as {Indication}, {History}, {Comparison}, and {Technique}, are incorporated into the clinical instructions (e.g., in Appx. B.4). The clinical prompt to Libra is: \u201cProvide a detailed description of the findings in the radiology image. Following clinical context: {...}.\u201d If no clinical instructions are available, the prompt defaults to: \u201cProvide a detailed description of the findings in the radiology image.\u201d After tokenizing and embedding the prompt and the answer, we insert the image patch tokens at the specified location, typically between the system prompt and the clinical prompts."}, {"title": "2.4 TEMPORAL-AWARE TRAINING", "content": "In this study, we focus on frontal-view images, either posterior-anterior (PA) or anterior-posterior (AP), and the Findings sections of radiology reports, as they contain the most direct clinical observations. We adopt a two-stage training strategy inspired by recent advancements in MLLM fine-tuning techniques (McKinzie et al., 2024). This proposed approach aims to progressively teach the model essential skills, including visual feature alignment and temporal information extraction. Libra's training process involves two stages: temporal feature alignment and downstream task fine-tuning.\nIn the first stage, the visual encoder and LLM weights are frozen, and the TAC is trained with Findings and Impression generation, along with CXR-related VQA tasks to extract high-quality image representations and capture temporal changes. In the second stage, we apply Low-Rank Adaptation (LoRA) (Hu et al., 2021) to fine-tune the pre-trained LLM on the Findings section generation task, keeping the visual encoder and connector weights frozen. Unlike traditional full fine-tuning, LoRA achieves comparable performance with significantly lower training costs. The detailed training configuration, including learning rate schedules and model parameters, is provided in Appx. B.1."}, {"title": "3 EXPERIMENTS", "content": null}, {"title": "3.1 TASK AND DATASET", "content": "Task Description We focus on generating the Findings section of radiology reports for a set of frontal CXRs to ensure fair comparison with previous work. The Findings section contains descriptions by radiologists of both normal and abnormal findings. Typically, these CXRs are accompanied by additional sections such as Indication and Technique, which provide the rationale for the study, including clinical history or specific requests from the referring physician. While not directly related to diagnostic interpretation, these sections serve as routine records and assist the model in better understanding temporal changes between images across different periods. Therefore, we incorporate clinical instructions about the current image as prompts to guide Libra to complete the RRG task.\nThe most common CXR is frontal views, either PA or AP. While other views, such as lateral images, are occasionally used for diagnostic purposes, they mainly serve as supplementary tools to aid in interpreting anatomical structures more comprehensively (Islam et al., 2023). Therefore, the frontal view remains the standard perspective for clinical interpretation of CXRs, and our study is consistent with prior work in RRG tasks, including studies by Chaves et al. (2024) and Hyland et al. (2024). For both the current and prior images, we exclusively use a single frontal view.\nDataset Description To train Libra, we utilise the MIMIC-CXR dataset (Johnson et al., 2019b) and its derivative datasets, Medical-Diff-VQA (Hu et al., 2023) and MIMIC-Ext-MIMIC-CXR-VQA (Bae et al., 2023), as shown in the Table 1. All datasets are split according to the official labels to ensure no data leakage. Detailed descriptions of datasets can be found in the Appx. B.2."}, {"title": "3.2 EVALUATION METRICS", "content": "We evaluate the generated reports using lexical and radiology-specific metrics, following established protocols.. For lexical metrics, we use ROUGE-L (Lin, 2004), BLEU-{1,4} (Papineni et al., 2002), METEOR (Banerjee & Lavie, 2005) and BERT (Devlin et al., 2019). For radiology-specific metrics, we use RadGraph-F1 (Jain et al., 2021), RGER (Delbrouck et al., 2022a), F1-CheXpert (Irvin et al., 2019a), CheXbert vector similarity (Yu et al., 2022), as well as RadCliQ version 0 Yu et al. (2022).\nAdditionally, we introduce a novel temporal entity F1 score (called $F1_{temp}$) to assess the model's ability to capture temporal information. Clinical metrics typically emphasise the accuracy of medical findings, prioritising the detection of clinically relevant entities. In contrast, the temporal entity score specifically measures the accuracy of entities related to progression over time described in the report. Detailed descriptions of all metrics and an illustrative $F1_{temp}$ analysis are provided in Appx. B.3.\nTemporal Entity F1 Following the work of Bannur et al. (2023), we set a reward list that includes common radiology-related keywords associated with temporal changes. Temporal entities are then extracted from both the ground truth ($E_{gt}$) and the generated reports ($E_{gr}$). Importantly, no stemming or lemmatization is applied during token processing to preserve the precise description of temporal changes. After extraction, we compute precision ($P_{temp}$) and recall ($R_{temp}$), which are subsequently used to calculate the temporal entity score, defined as the harmonic mean of precision and recall (Van Rijsbergen, 1974), also known as the F1 score.\n$P_{temp} = \\frac{|E_{gr} \\cap E_{gt}| + \\epsilon}{|E_{gt}| + \\epsilon}; R_{temp} = \\frac{|E_{gr} \\cap E_{gt}| + \\epsilon}{|E_{gr}| + \\epsilon}; F1_{temp} = (1+\\beta^2) \\cdot \\frac{P_{temp} \\cdot R_{temp}}{\\beta^2 \\cdot P_{temp} + R_{temp}}$"}, {"title": "3.3 MAIN RESULT", "content": "Although MIMIC-CXR provides an \"official\" test split, strict comparisons with prior studies remain challenging due to differences in test set inclusion criteria and pre-processing steps. For instance, Yu et al. (2022) and Jeong et al. (2023) included only one image per study, resulting in a test set of 1,597 samples, while Tanida et al. (2023) followed the Chest ImaGenome split (Wu et al., 2021b). Such variations in test set distributions can significantly impact the reported results (Park et al., 2024). To ensure fairness, we use a widely adopted test set focused on frontal-view CXRs, aligned with previous studies such as MAIRA-1 (Hyland et al., 2024) and LLaVA-Rad (Chaves et al., 2024) 4.\nAdditionally, the latest concurrent work M4CXR (Park et al., 2024) employs multi-turn chain-of-thought (Wei et al., 2023) prompting to generate reports, which differs from our task setup. Furthermore, we do not compare with the recent work MAIRA-2, as it is a MLLM specifically designed for grounded radiology report generation task, which incorporates lateral views and prior study reports for each subject within the input prompt. Bannur et al. (2024) emphasises a positive transfer between this task and general report generation, which is also beyond the scope of our study 5.\nTaking these considerations into account, we compared our model with state-of-the-art models, including LLaVA-Med (Li et al., 2023a), CheXagent (Chen et al., 2024), GPT-4V (OpenAI et al., 2024), Med-PaLM (Tu et al., 2023), LLaVA-Rad and MAIRA-1. The results are shown in Table 2. Since many of these models are not publicly available, we present their evaluation results as reported in the original sources."}, {"title": "4 ABLATION STUDY", "content": "To thoroughly assess the impact of various optimisation components, we conducted a series of ablation experiments, evaluating different modules and dataset expansions within Libra. All ablation models were tested on the MIMIC-CXR official test split for the Findings section generation task, using identical hyperparameter settings during training and inference."}, {"title": "Q1: Does incorporating temporal information bring positive effects to Libra in RRG tasks?", "content": "Temporal information is embedded in paired images and referenced in the corresponding radiology reports, reflecting changes over time through references to previous symptoms and descriptions of their progression. As indicated in Table 1, 86% of the test data includes true prior images. To assess whether Libra can effectively perceive and utilise temporal information during inference, we conducted separate evaluations using either true prior images or only dummy prior images. When true prior images were unavailable, the model treated the current image as a dummy prior image. As shown in Table 3, Libra demonstrated substantial enhancements across all metrics when true prior images were used as references. Specifically, the improvement in clinical scores exceeds that of lexical scores, indicating that temporal information is crucial for generating high-quality medical reports, beyond merely improving linguistic fluency. Notably, the F1temp score shows the most significant impact, with a difference of 7.41%. This highlights Libra's capability to effectively leverage temporal changes provided by true prior images, thereby enhancing the quality of the generated Findings section and overall performance in the RRG task."}, {"title": "Q2: Does the Temporal Alignment Connector effectively improve model performance?", "content": "To evaluate the impact of TAC on Libra's performance in the RRG task, we initialised a model with the RAD-DINO (P\u00e9rez-Garc\u00eda et al., 2024) image encoder, TAC, and Meditron (Chen et al., 2023b) as the LLM. A baseline experiment (Libra-1) was conducted by fine-tuning only the TAC for the Findings generation task. As shown in Table 4, we performed ablation studies by progressively removing different TAC components, including TFM, LFE, the Prior Image Prefix Bias (PIPB), and the entire TAC. Removing TFM restricted the model to processing only the current image, using a"}, {"title": null, "content": "configuration similar to LLaVA (Liu et al., 2023) but with a four-layer MLP to align the image representation with the LLM's hidden dimensions. Without LFE, the model follows the LLaVA setup, utilising the penultimate layer of the image encoder. The results indicate that removing any TAC submodule leads to a decline in all metrics compared to Libra-1. Removing TFM caused a notable drop in the F1temp score (\u2193>2%), emphasising its role in capturing temporal information. Removing LFE especially decreases RadGraph-related scores, highlighting its importance in extracting detailed image features. The removal of PIPB impacted clinical scores more than lexical scores. Finally, removing the entire TAC results in substantial declines across all metrics, underscoring its critical role in integrating image details and temporal information6."}, {"title": "Q3: Are additional Impression and VQA datasets needed during the feature alignment?", "content": "To evaluate the impact of dataset expansion during the first stage on Libra's performance, we trained a model (Libra-f) using only the Findings data in the first stage, while Libra incorporated additional Impression and VQA data for alignment, in Table 5. After the first stage, Libra showed superior performance in lexical metrics compared to Libra-f, but a slight decline in clinical metrics. This can be attributed to the inclusion of VQA tasks, which encouraged the model to focus on more fine-grained and grounded information. While VQA emphasises the detailed description of the single symptom, the Findings section generation task requires a comprehensive overview of the CXR, encompassing multiple normal and abnormal findings. This shift in focus also impacted the F1temp score, as each finding typically involves its own temporal changes. Additionally, the reduction in the number of generated findings entities led to a decrease in the number of identified temporal entities. In the second stage, fine-tuned on the Findings dataset, Libra achieved the best results across all metrics, indicating that incorporating additional datasets in the first stage enhances Libra's understanding and reasoning of CXRs."}, {"title": "5 PERFORMANCE ANALYSIS", "content": "We used CXRs from the official test split to generate Findings sections and perform a qualitative analysis of Libra. Figure 2 illustrates a case where no true prior image was available for reference. For example, while the ground truth report only mentioned the presence of \u201csternal wires,\u201d Libra not only identified their presence but also provided details on the specific type. This demonstrates Libra can deliver more precise clinical information by not only recognizing conditions but also distinguishing the specific types of surgical interventions."}, {"title": "6 CONCLUSION", "content": "In this study, we presented Libra, a temporal-aware multimodal large language model specifically designed for chest X-ray report generation tasks. Trained exclusively on the open-access MIMIC-CXR dataset (Johnson et al., 2019b), Libra utilises a two-stage training framework that leverages a radiology-specific pre-trained image encoder and language model, connected through a specially designed Temporal Alignment Connector to bridge visual and textual modalities. By identifying observations from current CXR and referencing prior scan, Libra generates radiology reports with substantial improvements across all metrics compared to existing models of the same parameter scale. Qualitative analysis in the case study highlights Libra's ability to effectively utilise the temporal relationships between current and prior images, addressing the challenge of hallucinations related to prior study references in RRG tasks. Libra sets a new paradigm for MLLMs in multimodal medical AI research. In the future, we aim to further enhance Libra's clinical applicability and accuracy, paving the way for the development of general-purpose AI models in the medical domain. For a detailed discussion of Libra's limitations and future work, please refer to Appx. A.3."}, {"title": "A APPENDIX A", "content": null}, {"title": "A.1 RELATED WORK", "content": "Radiology Report Generation Several efforts have been made to develop models tailored for radiology report generation (RRG), driven by the need to address the long-tail distribution of observations in chest X-rays and provide fine-grained descriptions of findings. This task has gained prominence as an important objective for the designed system (Wang et al., 2018b).\nSuch systems inherently require a language generation component. Initially, recurrent neural networks were used (Liu et al., 2019), but these have recently been replaced by Transformer architectures (Miura et al., 2021b; Chen et al., 2022), including large language models (LLMs) such as PaLM (Chowdhery et al., 2022) and Vicuna-7B (Chiang et al., 2023).\nTo ensure clinical accuracy in generated text, some studies have moved beyond standard language modelling losses, instead employing reinforcement learning (RL) to optimise for rewards that prioritise \u201cclinically relevant\u201d features, such as the inclusion of specific findings (Liu et al., 2019; Irvin et al., 2019b) or maintaining logical consistency (Miura et al., 2021a; Delbrouck et al., 2022a). However, these methods often depend on external models like CheXbert (Smit et al., 2020) or Rad-Graph (Jain et al., 2021) to extract clinical entities, adding complexity to the optimisation process.\nWith the advancement of LLMs, an increasing number of studies have demonstrated that plain auto-regressive language modelling can achieve substantial results. However, we acknowledge that the benefits of more complex training objectives or RL-based approaches may be complementary. Consequently, research on leveraging temporal information in RRG tasks can be broadly categorised into two types: LLM-based methods and non-LLM-based methods.\nnon-LLM-based Model These architectures are typically transformer encoder-decoder models and their variants. They are often composed of multiple modules that require separate training. Meanwhile, they handle single and double image inputs by symbolically differentiating tasks and employing two distinct neural network architectures: one designed for single-image inputs and another for double-image inputs.\nSerra et al. (2023) utilises symbolic alignment in its Longitudinal Projection Module and employs a separately trained BERT-based (Devlin et al., 2019) text generator. RECAP (Hou et al., 2023a) implements a transformer encoder-decoder with symbolic task differentiation and two-stage training, beginning with classification tasks followed by report generation. TiBiX (Sanjeev et al., 2024) adopts a transformer model with causal attention layers and incorporates learnable padding tokens to handle cases without prior images. BioViL-T (Bannur et al., 2023) is a self-supervised vision-language training framework that features a CNN\u2013Transformer hybrid multi-image encoder trained jointly with a BERT-based text model.\nOn one hand, the difference in model parameter sizes, and on the other, as LLM-based models generally outperform other types of models in the RRG task, papers on non-LLM-based models or those using small language models typically do not compare their methods with LLM-based approaches. At the same time, we conducted comparisons and discussions to reaffirm this point, as detailed in Appx. D.2.\nLLM-based Model Current LLM-based models have achieved notable success in the RRG task, largely benefiting from visual instruction tuning (Liu et al., 2023). Structurally, these models (Li et al., 2023a; Chaves et al., 2024; Hyland et al., 2024; Zhou et al., 2024; Park et al., 2024) typically consist of an image encoder and an adapter that bridges the connection to the LLM. These adapters often rely on single-layer hidden representations (e.g., the last or penultimate layer) from pre-trained image encoders, limiting their ability to simultaneously integrate information from multiple images.\nIn end-to-end training, LLM-based models handle multiple image inputs by concatenating them with textual prompts to form a composite input for the LLM. For example, the input format is \u201c<Current Image Placeholder> + <Prior Imgae Placeholder> + <Prompt>\u201d.\nAlthough our model adopts an end-to-end training paradigm, it does not rely on the vanilla approach to handle multiple images. Instead, we designed a novel adapter called the Temporal Alignment"}, {"title": "A.2 RESEARCH OBJECTIVES", "content": "Temporal Information Temporal change is a crucial factor in understanding disease progression, especially in radiology, where paired images and corresponding radiology reports capture changes over time. This temporal context is often reflected in comparisons between current and prior scans, documenting symptom evolution or newly identified findings.\nThe relativity of position within the timeline is a key determinant of temporal information. This relative positioning does not alter the established description of well-established facts about symptoms already present in the current image. For instance, if the prior image is recent, descriptions of changes will be minimal, whereas an older prior image may highlight more pronounced changes.\nResearch Object Our study focuses on the chest X-"}]}