{"title": "TransBox: $\\mathcal{EL}^{++}$-closed Ontology Embedding", "authors": ["Hui Yang", "Jiaoyan Chen", "Uli Sattler"], "abstract": "OWL (Web Ontology Language) ontologies, which are able to represent both relational and type facts as standard knowledge graphs and complex domain knowledge in Description Logic (DL) axioms, are widely adopted in domains such as healthcare and bioinformatics. Inspired by the success of knowledge graph embeddings, embedding OWL ontologies has gained significant attention in recent years. Current methods primarily focus on learning embeddings for atomic concepts and roles, enabling the evaluation based on normalized axioms through specially designed score functions. However, they often neglect the embedding of complex concepts, making it difficult to infer with more intricate axioms. This limitation reduces their effectiveness in advanced reasoning tasks, such as Ontology Learning and ontology-mediated Query Answering. In this paper, we propose $\\mathcal{EL}^{++}$-closed ontology embeddings that are able to represent any logical expressions in DL $\\mathcal{EL}^{++}$ via composition. Furthermore, we develop TransBox, an effective $\\mathcal{EL}^{++}$-closed ontology embedding method that can handle many-to-one, one-to-many and many-to-many relations. Our extensive experiments demonstrate that TransBox often achieves state-of-the-art performance across various real-world datasets for predicting complex axioms.", "sections": [{"title": "1 Introduction", "content": "Ontologiesstructured according to the Web Ontology Language (OWL) standards developed by the W3C [14] are extensively used across various domains such as the Semantic Web [16], healthcare [13], finance [4], and biology [6]. In recent years, OWL ontology embeddings, which are vector-based knowledge presentations, have gained significant attention due to the increasing need for more effective methods to predict or infer missing knowledge as well as for more wider ontology application especially in combination with machine learning [11].\nUnlike Knowledge Graphs [18], which focus on individual entities (a.k.a. instances), ontologies emphasize concepts that represent groups of individuals, allowing for richer semantic interpretations. These interpretations are typically formalized through Description Logics (DLs) [3], which provide a logical foundation for reasoning within ontologies.\nIn ontologies, knowledge is often represented and leveraged through the intricate composition of atomic concepts and roles. For example, defining a disease typically requires the integration of various components, such as its symptoms and the affected locations in the body (Formally, $Disease = \\exists hasSymptom.Symptom \\sqcup occursIn.BodyLocation \\sqcap$...). Similarly, when utilizing this information for diagnosis, one must consider a wide array of patient data (see Section 6.2.4 for a more concrete example). Embedding methods must effectively capture these complex constructs, which are formed by logical operators (e.g., conjunction $\\sqcap$, existential quantification $\\exists r$), to accurately model intricate knowledge and enhance practical applicability in real-world scenarios. This capability is essential for a variety of applications, such as Ontology Learning [23], ontology-mediated query answering [5], and tasks like updating ontologies to incorporate newly emerging complex concepts from external resources, such as the latest clinical guidelines or research findings [12].\nIn this work, we propose $\\mathcal{EL}^{++}$-closed ontology embeddings that guarantee all logical operations in $\\mathcal{EL}^{++}$ (i.e., $\\sqcap$, $\\exists r$, and role composition) are effectively captured. In other words, $\\mathcal{EL}^{++}$-closed embedding could generate the embedding of any complex $\\mathcal{EL}^{++}$ concept by composing the embeddings of atomic concepts, and thus could be applied in many ontology reasoning tasks beyond the standard subsumption predictions over atomic concepts. However, we show that most existing methods, whether based on language models (LMs) or geometric models, are either not $\\mathcal{EL}^{++}$-closed or have other theoretical limitations.\nFirstly, LM-based approaches [9, 10, 28] rely on textual information, such as concept descriptions, to predict logical relationships. However, these approaches obscure the reasoning process within large neural networks and are unable to guarantee adherence to any formal logical constraint, including $\\mathcal{EL}^{++}$-closeness.\nSecondly, geometric model-based methods also face challenges related to $\\mathcal{EL}^{++}$-closure or theoretical limitations. For example, methods that embed concepts as balls, such as ELEM [22] and EMEM++ [24], struggle to handle conjunctions ($\\sqcap$) because the intersection of two balls is generally not a ball.\nOn the other hand, methods that embed concepts as boxes, such as BoxEL [31], ELBE [26], and Box$^2$EL [17], generally maintain closure under $\\sqcap$, except Box$^2$EL, which introduces bump vectors that are not defined for conjunctions like $A \\sqcap B$. Despite being more effective with conjunctions, box embedding methods face several significant challenges, as outlined below:"}, {"title": "2 Preliminaries and Related Work", "content": "Ontologies use sets of statements (axioms) about concepts (unary predicates) and roles (binary predicates) for knowledge representation and reasoning. We focus on $\\mathcal{EL}^{++}$-ontologies which keep a good balance between expressivity and reasoning efficiency with wide application [2]. Let $N_C = \\{A, B,...\\}$, $N_R = \\{r,t,...\\}$, and $N_I=\\{a, b, ...\\}$ be pair-wise disjoint sets of concept names (also called atomic concepts) and role names, and individual names, respectively. $\\mathcal{EL}^{++}$-concepts are recursively defined from atomic concepts, roles and individuals as\n$\\top | \\bot | A | C\\sqcap D | \\exists r.C | \\{a\\}$ \nand an $\\mathcal{EL}^{++}$-ontology is a finite set of TBox axioms of the form\n$C \\sqsubseteq D, r \\sqsubseteq t, r_1 \\circ r_2 \\sqsubseteq t$\nand ABox axioms $A(a)$ or $r(a, b)$. Note $C, D$ are (possibly complex) $\\mathcal{EL}^{++}$-concepts, $A$ is a concept name, $r_1, r_2, t$ are role names, and $a, b$ are individual names. The following is an example of axioms of a toy family ontology (see Figure 5 for the complete version)."}, {"title": "2.2 Ontology Embeddings", "content": "There are two primary kinds of ontology embedding approaches: those based on Language Models (LMs) and those based on geometric models. LM-based approaches, including those based on traditional non-contextual word embedding models and those based on Transformer [30] with contextual embeddings, such as OPA2Vec [28], OWL2Vec [10], and BERTSub [9], rely on textual data (e.g., concept descriptions) and predict logical informations by LMs. However, these methods loosely capture the underlying conceptual structure and often lack the interpretability needed for human understanding. In this work, we focus on geometric model-based approaches, which offer more intuitive representations.\nGeometric model-based methods use different geometric objects for constructing the geometric models of ontologies. For example, cones [15, 33] and fuzzy sets [29] have been used for $\\mathcal{ALC}$-ontologies. For $\\mathcal{EL}$-family ontologies, most methods using either boxes (Box2EL [17], BoxEL [31], ELBE [26]) or balls (ELEM [22], EMEM++ [24]). Among these, box-based methods have gained popularity due to their closure under intersection, which aligns well with the intersection of concepts in Description Logic. In contrast, ball-based embeddings are not closed under intersection, as the intersection of balls does not typically form a ball.\nHowever, these existing methods only consider embedding atomic concepts but neglect the embeddings of complex concepts, which prevents them from evaluating axioms beyond normalized ones. This limitation restricts their applicability to tasks involving complex axioms. In this work, we propose the $\\mathcal{EL}^{++}$-closed ontology embedding, which is able to generate embeddings of complex concepts from atomic ones, allowing for both training and evaluation across a wider variety of ontologies. Our analysis reveals that a large part of the existing geometric model-based approaches are not $\\mathcal{EL}^{++}$-closed, while the remaining methods cannot model many-to-many relationships, demonstrating suboptimal performance when tested on real-world datasets."}, {"title": "3 $\\mathcal{EL}^{++}$-Closed Embeddings", "content": "Before introducing our ontology embedding method, we first present the concept of $\\mathcal{EL}^{++}$-closed embedding. Given an ontology $\\mathcal{O}$, the goal of ontology embedding is to create a geometric model of the ontology $\\mathcal{O}$ that \"faithfully\" represents the meaning of concepts, roles, and individuals in $\\mathcal{O}$. In this model, each element in $N_C$, $N_R$, and $N_I$ occurring in $\\mathcal{O}$ is mapped to a geometric object in an embedding space $S$ (e.g., linear space $\\mathbb{R}^n$). For clarity, we define:\n*   $\\mathcal{E}_C$: the set of all geometric objects, such as balls or boxes, that are considered as subsets of $S$ and serve as candidates for concept embedding.\n*   $\\mathcal{E}_R$: the set of all geometric objects that are considered as subsets of $S \\times S$ and serve as candidates for role embedding.\nTo align with the machine learning framework, the elements of $\\mathcal{E}_C$ and $\\mathcal{E}_R$ should have numerical representations by vectors or specific values. We omit the collection for individuals because individuals are often embedded as points in the space $S$, and therefore the collection is equivalent to the space itself."}, {"title": "3_$\\mathcal{EL}^{++}$-Closed Embeddings", "content": "EXAMPLE 2. Different embedding methods define their ontology embedding spaces in distinct ways. For example:\n*   For ELEM and ELEM++, $\\mathcal{E}_C$ consists of all n-dimensional balls represented by their centers in $\\mathbb{R}^n$ and radii in $\\mathbb{R}$, and $\\mathcal{E}_R$ consists of subsets defined by vector-based translations: $E_{v_r} = \\{(x, x+v_r) | x \\in \\mathbb{R}^n\\}$, where $v_r \\in \\mathbb{R}^n$.\n*   For Box$^2$EL, when all the bump vectors are set to zero vector, $\\mathcal{E}_C$ consists of n-dimensional boxes represented by their lower left and upper right corners in $\\mathbb{R}^n$ or by their centers and offsets both in $\\mathbb{R}^n$, and $\\mathcal{E}_R$ consists of products of boxes, $Head(r) \\times Tail(r) \\subseteq \\mathbb{R}^n\\times\\mathbb{R}^n$.\n*   For ELBE, $\\mathcal{E}_C$ also consists of boxes as in Box$^2$EL, and $\\mathcal{E}_R$ is defined similarly to the ELEM case.\n*   For BoxEL, $\\mathcal{E}_C$ consists of boxes as in Box$^2$EL and ELBE, and $\\mathcal{E}_R$ consists of subsets defined by affine transformations: $E_{(k_r, b_r)} = \\{(x, k_r \\cdot x + b_r) | x \\in \\mathbb{R}^n\\}$, where $k_r, b_r \\in \\mathbb{R}^n$.\nThe $\\mathcal{EL}^{++}$-closed embeddings refer to embedding methods where the assigned spaces $\\mathcal{E}_C$ (for concepts) and $\\mathcal{E}_R$ (for roles) are closed under the $\\mathcal{EL}^{++}$-semantics defined by Equations 2, 16, and 4. The formal definition is provided in Definition 3.1, where each of the three conditions directly corresponds to these equations.\nDefinition 3.1 ($\\mathcal{E}_{\\mathcal{EL}^{++}}$-Closed Embeddings). An ontology embedding method over a space $S$, with embedding candidate sets $\\mathcal{E}_C$ for concepts and $\\mathcal{E}_R$ for roles, is $\\mathcal{EL}^{++}$ -closed if $\\mathcal{E}_C$ includes the empty set $\\emptyset$, the whole space $S$, all singletons $\\{x\\}$ with $x \\in S$, and satisfies the following closure properties:\n(1) Conjunction closure: $S \\cap S' \\in \\mathcal{E}_C$ for all $S, S' \\in \\mathcal{E}_C$;\n(2) Existential quantification closure: $\\exists ES \\in \\mathcal{E}_C$, where $\\exists ES = \\{x | \\exists y \\in S : (x, y) \\in E\\}$ for any $E \\in \\mathcal{E}_R$ and $S \\in \\mathcal{E}_C$;\n(3) Role composition closure: $E \\circ E' \\in \\mathcal{E}_R$ for all $E, E' \\in \\mathcal{E}_R$, where $E \\circ E' = \\{(x, z) | \\exists y \\in S : (x, y) \\in E \\text{ and } (y, z) \\in E'\\}$.\nWe require that $\\mathcal{E}_C$ includes $\\emptyset$, $S$, and $\\{x\\}$ to capture the semantics of $\\bot$, $\\top$, and $\\{a\\}$, respectively. It is important to note that this closure property is not specific to any particular ontology; rather, it is an inherent characteristic of the ontology embedding method itself."}, {"title": "4 Method: TransBox", "content": "We now introduce TransBox, our method for constructing geometric models of a given $\\mathcal{EL}^{++}$ ontology $\\mathcal{O}$. This section is structured as follows. First, we present the basic framework of TransBox in Section 4.1. In Section 4.2, we demonstrate that TransBox is EL++-closed. Sections 4.3 and 4.4 introduce two enhancements that improve the learned embeddings and handling of box intersections. The training procedure for TransBox is outlined in Section 4.5.\nIn TransBox, we embed each atomic concept $A \\in N_C$ to a box $Box(A)$, which is defined as an axis-aligned hyperrectangle in the n-dimensional linear space $\\mathbb{R}^n$. As in previous works [17, 26, 31], each box $Box(A)$ is represented by a center $c(A) = (c_1,..., c_n) \\in \\mathbb{R}^n$ and offset $o(A) = (o_1,...,o_n) \\in \\mathbb{R}_{\\ge 0}^n$. Formally, $Box(A) \\subseteq \\mathbb{R}^n$ is the area defined by:\n$Box(A) = \\{x \\in \\mathbb{R}^n | c(A) - o(A) \\le x \\le c(A) + o(A)\\}$,\nwhere $\\le$ is the element-wise comparison. Each individual $a \\in N_I$ is embedded as a points $x_a \\in \\mathbb{R}^n$.\nEach role $r \\in N_R$ is also associated with a box $Box(r) \\subseteq \\mathbb{R}^n$, with the semantics defined as follows: For each pair of embedded instances $x_a, x_b \\in \\mathbb{R}^n$, we have $r(a, b)$ is true if $x_a - x_b \\in Box(r)$. Formally, we embed each $r$ to an area in $E_{Box(r)} \\subseteq \\mathbb{R}^n \\times \\mathbb{R}^n$ defined by:\n$E_{Box(r)} = \\{(x, y) | x, y \\in \\mathbb{R}^n, x - y \\in Box(r)\\}$."}, {"title": "4.2 $\\mathcal{EL}^{++}$-closeness", "content": "In the TransBox framework, the candidate space for representing concepts $\\mathcal{E}_C^2$ comprises all boxes in $\\mathbb{R}^n$, and the candidate space for representing roles $\\mathcal{E}_R$ is defined as a subset $E_{Box(r)} \\subseteq \\mathbb{R}^n \\times \\mathbb{R}^n$, as specified in Equation (6). To demonstrate that TransBox is $\\mathcal{EL}^{++}$-closed, it suffices to verify the three conditions outlined in Definition 3.1 for any boxes $Box(A), Box(B) \\in \\mathcal{E}_C$ and subsets $E_{Box(r)}, E_{Box(t)} \\in \\mathcal{E}_R$ as follows:\n(1) Closed under conjunction: $Box(A) \\cap Box(B)$ is always a box, and thus belongs to $\\mathcal{E}_C$. Therefore, the first condition of Definition 3.1 holds. We denote by $Box(A \\sqcap B) := Box(A) \\cap Box(B)$;\n(2) Closed under existential qualification: The second condition of Definition 3.1 holds according to the following proposition:\nPROPOSITION 4.1. Let $S = Box(B)$ and $E = E_{Box(r)}$, and let\n$\\exists E S = \\{x | \\exists y \\in \\mathbb{R}^n : (x, y) \\in E\\}$.\nThen $\\exists ES$ is a box with center $c(r) + c(B)$ and offset $o(r) + o(B)$, and thus, we have $\\exists ES \\in \\mathcal{E}_C$.\nIn the remainder of this paper, we will denote $Box(\\exists r.B) := E_{Box(r)} \\circ Box(B)$, given that $r$ and $B$ are embedded as $Box(r)$ and $Box(B)$, respectively.\n(3) Closed under role composition: The third condition of Definition 3.1 holds based on the following proposition:"}, {"title": "4.3 Semantic Enhancement", "content": "Semantic enhancement refers to replacing the ontology $\\mathcal{O}$ with a \u201cstronger\u201d version, $\\mathcal{O}_{stg}$, during the training process. We consider $\\mathcal{O}_{stg}$ stronger than $\\mathcal{O}$ if it derives all axioms in $\\mathcal{O}$ (i.e., $\\mathcal{O}_{stg} \\models a$ for any $a \\in \\mathcal{O}$). This enhancement can lead to better results by imposing stronger constraints (an example is provided in Section 6.1).\nTo realize the semantic enhancements, we introduce a new logical operator, $\\exists_{all}$, which denotes individuals related by a role to every individual in a concept. Therefore, $Box(\\exists_{all}B)$, as illustrated in Figure 3, is formally defined by:\n$Box(\\exists_{all}B) = \\{x \\in \\mathbb{R}^n | \\forall y \\in Box(B) : x - y \\in Box(r)\\}$.\nPROPOSITION 4.3. $Box(\\exists_{all}B)$ is a box with center $c(r) + c(B)$ and offset $max\\{0, o(r) - o(B)\\}$, and we have $Box(\\exists_{all}B) \\subseteq Box(\\exists r.B)$.\nBased on Proposition 4.3, we perform a semantic enhancement on a given ontology $\\mathcal{O}$ by replacing any axiom $a \\in \\mathcal{O}$ of the form $C \\subseteq D$ with $C \\subseteq D_{stg}$, where $D_{stg}$ is obtained by substituting each occurrence of $\\exists r$ with $\\exists_{all}$. Such an operation is guarantee to be an enhancement by the following result:\nPROPOSITION 4.4. Let $\\mathcal{O}_{stg}$ be the collection of all axioms $C \\subseteq D_{stg}$ obtained as described above. Then, for any $a \\in \\mathcal{O}$, we have $\\mathcal{O}_{stg} \\models a$.\nIt is important to note that only TransBox can apply the above enhancement, as it embeds roles as a set of translations represented by boxes. In contrast, methods like ELBE and BoxEL would collapse into TransE with this enhancement. This occurs because ELBE and BoxEL embed roles as a single translation, meaning $Box(r)$ is reduced to a single point, which would similarly reduce $Box(A)$ to a single point under the same enhancement."}, {"title": "4.4 Enhancing Box Intersections", "content": "The standard intersection of boxes tends to become empty as the space grows exponentially sparser with increasing dimensions. Consequently, the likelihood of finding intersecting boxes decreases"}, {"title": "4.5 Training", "content": "Distance and inclusion loss of Boxes. As shown in [22, 26], the distance of two boxes can be described by the following equation:\n$d(Box, Box') = |c_1 - c_2| - o_1 - o_2$.\nNote that $d(Box, Box') \\in \\mathbb{R}^n$ is a vector that captures the minimal difference between two points between Box and Box', and a negative $d(Box, Box')$ indicates that Box and Box' have non-empty intersection.\nThe inclusion loss that determines whether one box is included in another is defined as follows. We assign each box on $(\\mathbb{R} \\cup \\{0\\})^n$ a binary mask $m = (m_1, ..., m_n)$, where $m_j = 1$ if $I_i \\ne 0$, and $m_j = 0$ otherwise. Using this mask, we extend the standard inclusion loss to boxes in $(\\mathbb{R} \\cup \\{0\\})^n$ as:\n$\\mathcal{L}_{\\subseteq}(Box, Box') := ||o(Box) * m * (1 - m')|| + \\\\  || max\\{0, (d(Box, Box') + 2o(Box)) * m * m' - \\gamma\\}||$\nwhere (1) The first term encourages the offset of Box to shrink to zero in dimensions where Box' is empty (i.e., where $m'_j = 0$); (2) The second term represents the standard inclusion loss [17], but is restricted to the dimensions where both Box and Box are non-empty (i.e., $m_jm'_j = 1$ in those dimensions). Note that when the mask vectors have a value of 1 in all dimensions, the above inclusion loss becomes equivalent to the standard one over $\\mathbb{R}^n$.\nAxiom Loss. The model is trained by the following loss function for two kinds of axioms with semantic enhancement applied:\n(1) General concept inclusion axioms $C \\subseteq D$: We define the loss as the inclusion loss between $Box(C)$ and $Box(D)$:\n$\\mathcal{L}(C \\subseteq D) = \\mathcal{L}_{\\subseteq}(Box(C), Box(D))$.\n(2) Role inclusion axioms $r_1 \\circ ... \\circ r_n \\subseteq t$ ($n = 1, 2$):\n$\\mathcal{L}(r_1 \\circ ... \\circ r_n \\subseteq t) = \\mathcal{L}_{\\subseteq}(Box(r_1 \\circ ... \\circ r_n), Box(t))$."}, {"title": "5 Soundness", "content": "Let $\\mathcal{O}$ be an $\\mathcal{EL}^{++}$ ontology. We can extend any TransBox embedding of $\\mathcal{O}$ to an interpretation $I_g$ following the $\\mathcal{EL}^{++}$ semantics introduced in Section 2.1, where the atomic concepts in $\\mathcal{O}$ are represented by $Box(A)$, and the roles in $\\mathcal{O}$ by $E_{Box(r)}$, as specified in Equation (6).\nBy the following result, we show that Transbox is sound in the sense that such an interpretation $I_g$ is a geometric model of $\\mathcal{O}$ when the loss for any axiom in $\\mathcal{O}$ is zero.\nTHEOREM 5.1 (SOUNDNESS). If for every axiom $a \\in \\mathcal{O}$, the loss defined by Transbox is 0 (i.e., $\\mathcal{L}(a) = 0$), then $I_g$ is a geometric model of $\\mathcal{O}$.\nPROOF. Since Transbox is closed under $\\mathcal{EL}^{++}$, for any (complex) $\\mathcal{EL}^{++}$-concept $C$, we have $Box(C) = C^{I_g}$ by definition. Now, consider any axiom $a$ of the form $C \\subseteq D$. By our definition, $\\mathcal{L}(C \\subseteq D) = 0$ if and only if $Box(C) \\subseteq Box(D)$. This implies that $C^{I_g} = Box(C) \\subseteq Box(D) = D^{I_g}$. Similarly, for role composition axioms of the form $r_1 \\circ r_2 \\subseteq t$, we have $(r_1 \\circ r_2)^{I_g} \\subseteq t^{I_g}$ whenever $\\mathcal{L}(r_1 \\circ r_2 \\subseteq t) = 0$.\nThus, we conclude that $I_g$ is indeed a geometric model of $\\mathcal{O}$."}, {"title": "6 Evaluation Results", "content": "To evaluate and demonstrate the expressiveness of the TransBox embedding method, we use a simple family ontology and compare the embedding results with ELBE [26]. For better illustration, we train all embeddings in a 2-dimensional space, and add visualization loss below into the final loss functions to avoid overly small boxes as in Box2EL [17].\n$\\mathcal{L}_V = \\frac{1}{n|N_C|} \\sum_{A \\in N_C} \\sum_{i=1}^n max\\{0, 0.2 - o(Box(A))_{i}\\}$.\nAs shown in Figure 4a, the ELBE embeddings fail to capture the disjointness between Father and Mother. This is because, in ELBE, roles are embedded as translations defined by a single vector. Consequently, the embedded boxes of Father and Mother must be close to the translated box $Box(Child)+v_{hasParent}$ due to the axioms Child $$\\exists hasParent. Mother and Child $$\\exists hasParent.Father.\nIn contrast, TransBox embeddings (Figures 4b and 4c) resolve this issue by utilizing multi-transition representations within $Box(r)$.\nThis approach allows TransBox to correctly learn distinct embeddings for each concept. Additionally, applying the semantic enhancements described in Section 4.3 further improves the embeddings of roles. Specifically, in Figure 4c, we observe that the embedding $Box(hasParent) \\approx -Box(hasChild)$ well captures the fact that the role hasParent is the inverse of hasChild."}, {"title": "6.2 Axiom Prediction", "content": "Benchmark. Following with prior research [17, 31], we utilize three normalized biomedical ontologies: GALEN [27], Gene Ontology (GO) [1], and Anatomy (Uberon) [25] in our study. Training, validation, and testing employ the established 80/10/10 partition as in [17] for predicting normalized axioms. For predicting complex axioms, however, we adopt a distinct test set generated via"}, {"title": "7 Conclusion and Future Work", "content": "In this work, we introduced $\\mathcal{EL}^{++}$-closed ontology embeddings that can generate embeddings for complex concepts from embeddings of atomic ones, enabling their use in more various tasks. Additionally, we proposed a novel $\\mathcal{EL}^{++}$-closed embedding method, TransBox, which achieves state-of-the-art performance in predicting complex axioms for three real-world ontologies.\nIn future work, we plan to extend the closed embeddings to more expressive Description Logics, such as ALC. We are also interested in integrating geometric models with language models, incorporating the textual information of concepts and roles in $\\mathcal{EL}^{++}$-closed embeddings for more accurate complex axiom prediction."}, {"title": "A Proofs", "content": "PROOF. (of Proposition 4.1) By definition, a point $x \\in \\exists ES$ if and only if there exist $y \\in Box(B)$ and $z \\in Box(r)$ such that\n$x - y = z$.\nSince $y \\in Box(B)$, we can express $y$ as $y = c(B)+u_1$, where $u_1 \\in \\mathbb{R}^n$ and $|u_1| \\le o(B)$. Here, $|u_1|$ means the vector obtained by taking the absolute value of each component of $u_1$. Similarly, $z \\in Box(r)$ can be written as $z = c(r) + u_2$, where $u_2 \\in \\mathbb{R}^n$ and $|u_2| \\le o(r)$. Substituting these into the equation $x - y = z$, we get:\n$x - (c(B) + u_1) = c(r) + u_2$.\nThus, we have:\n$x = (c(B) + c(r)) + (u_1 + u_2)$.\nLet $u = u_1 +u_2$, then u is any point in $\\mathbb{R}^n$ such that $|u| \\le o(B)+o(r)$ by construction. Therefore, the set $\\exists ES$ of all possible $x$ of the above form is exactly a box with center $c(B) + c(r)$ and offset $o(B) + o(r)$.\nPROOF. (of Proposition 4.2) It suffices to show that $(x, y) \\in Box(r)$ and $(y, z) \\in Box(t)$ if and only if $x - z \\in Box(r \\circ t)$, which is demonstrated as follows.\nSince $x - y \\in Box(r)$, we can express $x - y = c(r) + u_1$, where $u_1 \\in \\mathbb{R}^n$ and $|u_1| \\le o(r)$. Similarly, since $y - z \\in Box(t)$, we write $y - z = c(t) + u_2$, where $u_2 \\in \\mathbb{R}^n$ and $|u_2| \\le o(t)$.\nTherefore, we have: $x-z = (c(r)+c(t))+(u_1+u_2)$. Let $w = u_1+u_2$, then w is a point in $\\mathbb{R}^n$ such that $|w| \\le o(r) +o(t)$. By construction, the set of all possible $x - z$ is exactly the box $Box(r \\circ t)$, with center $c(r) + c(t)$ and offset $o(r) + o(t)$.\nPROOF. (of Proposition 4.3) For any $y \\in Box(B)$, it can be written as $c(B) + u_1$, where $|u_1| \\le o(B)$. Assume $x \\in Box(\\exists_{all}B)$, then $x - y = x - c(B) - u_1 \\in Box(r)$ for any $|u_1| \\le o(B)$. That is:\n$|x - c(B) - u_1 - c(r)| \\le o(r), \\forall u_1 \\le o(B)$.\nRewriting $x = c(B)+c(r)+u$, we have $|u-u_1| \\le o(r), \\forall|u_1| \\le o(B)$. It follows that this holds if and only if $|u| \\le max\\{0, o(r) - o(B)\\}$.\nIn conclusion, $x \\in Box(\\exists_{all}B)$ if and only if $x = c(B) + c(r) + u$ for some $|u| \\le max\\{0, o(r) - o(B)\\}$. This proves the proposition.\nPROOF. (of Proposition 4.4) For any interpretation I of O, the following holds:\n(1) $A^I \\subseteq A^I$ for any atomic concept A;\n(2) If $D^I \\subseteq D^I$, then $(\\exists_{all}D)^I \\subseteq (\\exists r.D)^I$ for any role r;\n(3) If $D_i^I \\subseteq D_i^I$ for i = 1, 2, then $D_1^I \\cap D_2^I \\subseteq D_1 \\cap D_2$.\nBy induction, we conclude that $D_{stg} \\subseteq D$. Therefore, for any C \\subseteq D \\in O, we have $\\{C \\subseteq D_{stg}\\} = C \\subseteq D, and thus O' \\models C \\subseteq D$."}]}