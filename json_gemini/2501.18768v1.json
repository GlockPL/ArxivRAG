{"title": "Diversity By Design: Leveraging Distribution Matching for Offline Model-Based Optimization", "authors": ["Michael S. Yao", "James C. Gee", "Osbert Bastani"], "abstract": "The goal of offline model-based optimization (MBO) is to propose new designs that maximize a reward function given only an offline dataset. However, an important desiderata is to also propose a diverse set of final candidates that capture many optimal and near-optimal design configurations. We propose Diversity in Adversarial Model-based Optimization (DynAMO) as a novel method to introduce design diversity as an explicit objective into any MBO problem. Our key insight is to formulate diversity as a distribution matching problem where the distribution of generated designs captures the inherent diversity contained within the offline dataset. Extensive experiments spanning multiple scientific domains show that DynAMO can be used with common optimization methods to significantly improve the diversity of proposed designs while still discovering high-quality candidates.", "sections": [{"title": "1. Introduction", "content": "Discovering designs that optimize certain desirable properties is a ubiquitous task that spans a wide range of scientific and engineering domains. For example, we might seek to design a drug with the most potent therapeutic efficacy (Brown et al., 2019; Kong et al., 2023; Du et al., 2024); build a robot that is most capable of navigating complex environments (Ahn et al., 2020; Trabucco et al., 2021; Wang et al., 2023); or engineer a material with a certain desirable property (Stanev et al., 2018; Pogue et al., 2023; Gashmard et al., 2024; Ma et al., 2024). However, experimentally validating every proposed design can be expensive, time-intensive, or even impossible in many applications. These limitations can preclude the use of conventional 'online' optimization methods for such generative design tasks.\nAn alternative approach is to instead discover design candidates in the offline setting, where we assume that no newly proposed designs can be experimentally evaluated during the course of the optimization process. Instead, we only have access to a static dataset of previously observed designs and their corresponding reward values. The objective then is to propose a (small) set of candidate designs to ultimately evaluate experimentally, with the hope that using the information available in the offline dataset will yield desirable designs in the real-world.\nMultiple prior works have proposed a variety of offline optimization algorithms (Yu et al., 2021; Trabucco et al., 2021; Fu & Levine, 2021; Chen et al., 2022; Mashkaria et al., 2023;"}, {"title": "2. Background and Preliminaries", "content": "Offline Model-Based Optimization. In generative design problems, we seek to learn a generative policy $\\pi^*$ over a space of policies $\\Pi$ such that the distribution $q_{\\pi^*}(x)$ :"}, {"title": "3. Distribution Matching for Generative Offline Optimization", "content": "3.1. Motivating Limitation of Na\u00efve MBO\nPrior work from Mullis et al. (2019); Jain et al. (2022); Kim et al. (2023) have shown that an important challenge in offline optimization as in (3) is that of reward hacking: learned generative policies can exploit a small region of the design space, resulting in a low diversity of proposed designs. For example, consider the following lemma:"}, {"title": "3.2. An Alternative MBO Problem Formulation", "content": "To reward generative policies in proposing diverse designs, we modify the original MBO objective in (3) according to"}, {"title": "3.3. Adversarial Source Critic as a Constraint", "content": "Separately, to address the problem of forward surrogate model overestimation of candidate design fitness according to $r_{\\theta}(x)$, we constrain the optimization problem to ensure that expected source critic scores over $q_{\\pi}(x)$ and $p_{\\tau}(x)$ differ by no more than a constant $W_0 \\in \\mathbb{R}_+$, similar to the approach to offline MBO used by Yao et al. (2024). That is,"}, {"title": "3.4. Constrained Optimization via Lagrangian Duality", "content": "Our problem in (7) is ostensibly challenging to solve: both the objective J(\u03c0) and the constraint imposed by the source critic can be arbitrarily non-convex, making traditional constrained optimization techniques intractable in solving the optimization problem out-of-the-box. In this section, we derive an explicit solution to (7) to make the problem tractably solvable using any standard optimization algorithm.\nRecall from Lagrangian duality that solving (7) is equivalent to the min-max problem"}, {"title": "3.5. Overall Algorithm", "content": "To summarize, our work aims to solve two separate but related problems in offline MBO in (3): traditional model-based optimization approaches can yield candidate designs that are [1] of low diversity; and [2] not optimal due to exploiting out-of-distribution errors of the forward surrogate $r_{\\theta}(x)$. We introduce a KL-divergence-based distribution matching objective\u2014with input hyperparameters \u03c4 and \u03b2\u2014to solve the diversity problem; and build off prior work (Yao et al., 2024) to constrain the search space using source critic feedback to solve the out-of-distribution evaluation problem. We then show that there exists a provable, explicit solution to our modified offline MBO problem (i.e., Lemma 3.4 and (8)). In contrast with prior work imposing specific constraints on the forward model (Trabucco et al., 2021; Yu et al., 2021) or design space (Yao et al., 2024), or requiring the use of model-free optimization methods (Krishnamoorthy et al., 2023; Mashkaria et al., 2023), our approach only modifies the MBO objective and is therefore both optimizer- and task- agnostic. We refer to our method as Diversity in Adversarial Model-based Optimization (DynAMO)."}, {"title": "4. Experimental Evaluation", "content": "Datasets and Offline Optimization Tasks. We evaluate DynAMO on a set of six real-world offline MBO tasks spanning multiple scientific domains and both discrete and continuous search spaces. Five of the tasks are from Design-Bench, a publicly available set of offline optimization benchmarking tasks from Trabucco et al. (2022): (1) TFBind8 aims to maximize the transcription factor binding efficiency of a short DNA sequence (Barrera et al., 2016); (2) UTR the gene expression from a 5' UTR DNA sequence (Sample et al.,"}, {"title": "5. Results", "content": "Main Results. DynAMO consistently proposes the most diverse set of designs and achieves an Optimality Gap as high as 74.2 (DynAMO-BO-qUCB) and an average Rank as low as 1.2 compared to baseline methods (Table 1). We find that DynAMO offers the largest improvements in diversity for first-order methods, although also improves upon the evolutionary algorithms and Bayesian optimization methods evaluated. This makes sense, as both Grad. and Adam are only local optimizers that often end up exploring a much smaller region of the design space (without using DynAMO) compared to gradient-free methods. For example, DynAMO-Grad. (resp., DynAMO-CMA-ES; resp., DynAMO-BO-qEI) achieves a Pairwise Diversity Optimality Gap of 35.7 (resp., 55.2; resp., 74.2); in contrast, no other baseline method achieves a diversity score greater than -6.9 (resp., 16.8; resp., 51.4) within the same optimizer class.\nThese results do not come at the cost of the quality of designs; for example, for all 3 optimizers where DynAMO scores an average Rank of 1.2 (i.e., Grad., Adam, and CoSyNE backbone optimizers), DynAMO is also within the top 2 methods in proposing high-quality designs according to both Rank and Optimality Gap. In fact, DynAMO proposes the best designs for 5 out of the 6 backbone optimizers according to the Best@128 Optimality Gap. These results suggest that DynAMO can be used to improve both the quality and diversity of designs in a variety of experimental settings for both discrete and continuous search spaces.\nAblation Studies. DynAMO consists of two important but separate algorithmic components: (1) a KL-divergence-based distribution matching objective; and (2) a constraint dependent on an adversarial source critic. We show both components are important for DynAMO to generate both diverse and high-quality designs (Appendix E.2). DynAMO also takes as input two important hyperparameters-\u03b2 and \u03c4-as introduced in (5). We empirically ablate the values of these hyperparameters in Appendix E.3. Additional results and discussion are included in Appendix E."}, {"title": "6. Related Work", "content": "Model-free offline optimization. In our work, we specifically look at model-based optimization methods that explicitly optimize against a forward surrogate model $r_{\\theta}(x)$ that acts as a proxy for the hidden oracle function r(x). However, related work have also proposed offline optimization methods that do not require access to a model $r_{\\theta}(x)$ and instead impose constraints on the backbone optimization method\u2014we refer to such work as model-free offline optimization. For example, Mashkaria et al. (2023) frame generative design tasks as a \u2018next-sample' prediction problem and learn a transformer to roll out sample predictions; and"}, {"title": "7. Discussion and Conclusion", "content": "We introduce DynAMO, a novel task- and optimizer- agnos- tic approach to MBO that improves the diversity of proposed designs in offline optimization tasks. By framing diversity as a distribution-matching problem, we show how DynAMO can enable generative policies to sample both high-quality and diverse sets of designs. Our experiments reveal that DynAMO significantly improves the diversity of proposed designs while also discovering high-quality candidates.\nLimitations and Future Work. In our work, we focus on evaluating DynAMO and baseline methods in a one-shot, batched oracle evaluation setting-future work might explore how to extend our method to the active learning setting. Separately, recent domain-specific foundation models (Lin et al., 2023; Ohana et al., 2024; Nguyen et al., 2024; Zeni et al., 2025) may also give rise to more sophisticated and accurate forward surrogate models $r_{\\theta}(x)$ that can be leveraged with DynAMO and other MBO methods in future work.\nImpact Statement. Offline generative methods such as DynAMO have significant potential to help design more effective drugs; engineer new materials with desirable prop- erties; and solve other scientific tasks. However, like any real-world algorithm, these methods can also be misused to create potentially harmful or dangerous designs. Careful oversight by domain experts and researchers is essential to ensure our contributions are used for social good."}]}