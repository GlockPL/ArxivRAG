{"title": "Multi-Target Cross-Lingual Summarization: a novel task and a\nlanguage-neutral approach", "authors": ["Diogo Pernes", "Gon\u00e7alo M. Correia", "Afonso Mendes"], "abstract": "Cross-lingual summarization aims to bridge\nlanguage barriers by summarizing documents\nin different languages. However, ensuring se-\nmantic coherence across languages is an over-\nlooked challenge and can be critical in several\ncontexts. To fill this gap, we introduce multi-\ntarget cross-lingual summarization as the task\nof summarizing a document into multiple tar-\nget languages while ensuring that the produced\nsummaries are semantically similar. We pro-\npose a principled re-ranking approach to this\nproblem and a multi-criteria evaluation proto-\ncol to assess semantic coherence across target\nlanguages, marking a first step that will hope-\nfully stimulate further research on this problem.", "sections": [{"title": "Introduction", "content": "Cross-lingual summarization refers to the task of\nproducing a summary in a different language than\nthe original document and has the potential to break\nlanguage barriers by helping people to effectively\ncapture the essence of documents written in for-\neign languages (Wang et al., 2022). This is a very\nchallenging task, as it combines the difficulties\nof monolingual summarization, such as factual in-\nconsistencies with respect to the source document\n(Maynez et al., 2020), with those of machine trans-\nlation, such as translation of idiomatic expressions\nand cultural references (Fadaee et al., 2018).\nThe availability of large pre-trained multilin-\ngual transformers (Liu et al., 2020; Xue et al.,\n2021), followed by the widespread development\nand adoption of decoder-only language models\n(Radford et al., 2018; Touvron et al., 2023; Jiang\net al., 2023; Team et al., 2024) has enabled a sin-\ngle model to perform cross-lingual summarization\nfrom multiple source languages to multiple target\nlanguages (many-to-many summarization, M2MS).\nDespite the increasing emphasis on this many-to-\nmany paradigm, ensuring semantic coherence in\nsummaries across different target languages has\nnot been a primary focus of state-of-the-art meth-\nods, nor has it been systematically evaluated. Clearly,\nif information is not conveyed coherently across\nlanguages, the trustworthiness of the system is com-\npromised. Users cannot rely on the summaries to be\naccurate and unbiased, regardless of the language\nin which they consume the content. In addition,\nin legal or regulatory contexts, ensuring that infor-\nmation is presented coherently across languages\ncan be critical. This helps meet regulatory require-\nments and ensures that information is transmitted\ncoherently across language boundaries.\nTo fill this gap, we introduce a novel variant of\ncross-lingual summarization, which we call multi-\ntarget cross-lingual summarization (MTXLS),\nwhere we specifically address the challenge of pro-\nmoting semantic coherence across target languages.\nThis framework represents an important step to-\nwards more comprehensive cross-lingual summa-\nrization techniques and evaluation. Our main con-\ntributions in this work are summarized as follows:\nFirst, we introduce MTXLS formally as a novel\ntask (Section 3), motivated by the need of produc-\ning summaries coherently for multiple target lan-\nguages. Second, we present a re-ranking-based ap-\nproach to address this problem (Section 4), where\nthe re-ranking phase selects a set of summaries that\nexhibit superior semantic coherence across target\nlanguages compared to treating each cross-lingual\nsummarization task independently. Notably, our\napproach circumvents the need for a pivot language.\nThis language-neutral strategy ensures a more ro-\nbust and unbiased multilingual summarization pro-\ncess. Finally, we propose and conduct a multi-\ncriteria evaluation protocol that goes beyond the\nsimple evaluation of the similarity between gener-"}, {"title": "Related Work", "content": "Research in cross-lingual summarization has re-\ncently gained traction, in part due to the increased\navailability of large datasets for this task (Ladhak\net al., 2020; Perez-Beltrachini and Lapata, 2021;\nUrlana et al., 2023). Among these, CrossSum\n(Bhattacharjee et al., 2023) stands out as the most\nresourceful. This news dataset contains document-\nsummary pairs for 45 different languages and more\nthan 1,500 language directions, and it was built by\nautomatically pairing the data from the multilin-\ngual dataset XL-Sum (Hasan et al., 2021), which\nconsists of news articles from BBC.\nEarlier cross-lingual summarization models op-\nerated on a per-language-pair basis (Zhu et al.,\n2019; Cao et al., 2020; Bai et al., 2021; Liang et al.,"}, {"title": "Quality Estimation for Machine\nTranslation", "content": "In machine translation (MT), quality estimation\nmethods aim to predict translation quality without\naccess to gold standard outputs (Specia et al., 2013,\n2018). Our focus is on using sentence-level MT\nquality estimation to evaluate semantic coherence\nin the generated summaries across target languages,\nby taking two system-generated summaries for dif-\nferent languages and evaluating how well one trans-\nlates the other.\nQuality estimation methods for MT can be per-\nformed at various levels: word-level, where binary\nlabels (OK or BAD) are assigned to each machine-\ntranslated word, and sentence- or document-level,\nwhere a score is generated as an estimate of the\nquality of the whole translated sentence or doc-\nment. Many quality estimation methods pro-\nduce both word-level and sentence-level scores\n(Wang et al., 2018; Kepler et al., 2019a,b; Lee,\n2020). A sentence-level quality estimation method\ncan arise from training multilingual sentence en-\ncoders like LASER (Artetxe and Schwenk, 2019)\nor SONAR (Duquenne et al., 2023). These models\nalign representations of translated sentences, allow-\ning embedding similarity metrics in the common\nspace to serve as quality estimation metrics for\nMT. BLASER (Chen et al., 2023a), an automatic\ntext-free metric for evaluating speech translation,"}, {"title": "Multi-Target Cross-Lingual\nSummarization", "content": "This section formalizes the task of MTXLS. Let\nx\u2208 X represent a document in the source lan-\nguage o, and let T = {t1, t2,...,t} denote a set\nof N target languages. Without loss of general-\nity, we assume that o \u2208 \u03a4. The primary goal of\nMTXLS is to generate a set of N summaries, de-\nnoted as S = {Yt1, Yt2,\u00b7\u00b7\u00b7, Ytv }, where there is a\nsummary yt, \u2208 Y for each language in T.\nIt is evident that this task can be seen as a com-\nbination of a monolingual summarization task in\nlanguage o and N \u2013 1 cross-lingual summarization\ntasks from o to each target language t \u2208 T \\ {0}.\nWhile these tasks could be approached indepen-\ndently, we impose a constraint: all N summaries\nshould convey identical information regardless of\nthe language. This constraint ensures the alignment\nof information across different languages, thus pro-\nmoting coherence in the resulting set of summaries."}, {"title": "Summarize-and-Translate", "content": "Consider a scenario where a summarization model\nis available for generating summaries from lan-\nguage o to a pivot language \u03c0. Additionally, there\nare models for translating from \u03c0 to each language\nin T. Common statistical approaches to these\ntasks involve modeling the summarization distribu-\ntion p(\u03a5\u03c0 | \u03a7\u03bf, \u03c0) and the translation distributions\np(Yt | Y, t) for each t \u2208 T.\nTo enforce the desired coherence constraint\nacross target languages, a simple strategy is to as-\nsume that the target summaries are conditionally\nindependent of the source document given the pivot"}, {"title": "Methodology", "content": "We now relax the conditional independence as-\nsumption made previously by explicitly condition-\ning yt on xo, as shown in Figure 1b. Notably, this\napproach does not involve decoding yt after y\u3160,\nbut rather allows the two processes to run in par-\nallel, and explicitly promotes semantic similarity\nbetween y and each yt, as required to satisfy our\nconstraint. We now have:\np(Yt | xo, t, \u03c0) = Ey |x\u03bf, \u03c0P(Yt | X\u03bf, \u03a5\u03c0,t).\nLet us impose that:\n$\\displaystyle P(Y_t \\mid x_o, Y_\\pi, t) = \\frac{1}{Z} \\phi(Y_t, Y_\\pi) q(Y_t \\mid x_o,t),$"}, {"title": "Beyond Summarize-and-Translate", "content": "We now relax the conditional independence as-\nsumption made previously by explicitly condition-\ning yt on xo, as shown in Figure 1b. Notably, this\napproach does not involve decoding yt after y\u03c0,\nbut rather allows the two processes to run in par-\nallel, and explicitly promotes semantic similarity\nbetween y\u03c0 and each yt, as required to satisfy our\nconstraint. We now have:\n$\\displaystyle p(Y_t \\mid x_o, t, \\pi) = E_{Y_\\pi \\mid x_o, \\pi}[P(Y_t \\mid X_o, Y_\\pi,t)].$\nLet us impose that:\n$\\displaystyle P(Y_t \\mid x_o, Y_\\pi, t) = \\frac{1}{Z} \\phi(Y_t, Y_\\pi) q(Y_t \\mid x_o,t),$"}, {"title": "A Language-Neutral Formulation", "content": "Despite not using translation to obtain summaries\nfor the target languages, the approach we have de-\nscribed in Section 4.1 still relies in a pivot language.\nHowever, following the same formulation, we can\ncircumvent this issue by defining a joint distribu-\ntion for the summaries in all the target languages:\n$\\displaystyle p(S \\mid x_o, T) \\propto \\varphi(S) \\prod_{i=1}^N q(Y_{t_i} \\mid x_o, t_i),$\nwhere\n$\\displaystyle\\varphi(S) = \\frac{1}{\\binom{N}{2}} \\sum_{i,j: i < j} \\phi(Y_{t_i}, Y_{t_j}),$\nmeasures the semantic similarity of the set of sum-\nmaries S by averaging all the pairwise similari-\nties between each pair of summaries in S. This\nmodel is represented graphically in Figure 1c. Note\nthat the formulation in Section 4.1 is a partic-\nular case of this one where S = {yt, Y} and\np(S | xo, T) = p(Yt | xo,t,\u03c0)q(Y\u3160 | \u03a7\u03bf, \u03c0)."}, {"title": "Summary Sampling", "content": "Our primary goal is now to conceive a method that\nallows us to sample summaries from:\n$\\displaystyle p(S \\mid x_o, T) = \\frac{\\varphi(S) \\prod_{i=1}^N q(Y_{t_i} \\mid x_o,t_i)}{Z'},$\nWe demonstrate we can achieve this goal through\nrejection sampling, which works as follows. Given\na distribution f(x) from which we aim to sam-\nple and a proposal distribution g(x) satisfying\n$\\displaystyle \\frac{f(x)}{g(x)} \\le M,$\nwe start by generating a sample\nx from g and a sample u uniformly in [0, 1]. Sub-\nsequently, we accept x if $\\displaystyle \\frac{f(x)}{Mg(x)} \\ge u$ and reject it\notherwise."}, {"title": "A Mode-Seeking Heuristic", "content": "The procedure presented in Section 4.3 offers a sys-\ntematic means to sample sets of summaries from\nthe distribution p(S | xo,T). However, in many\npractical scenarios, the objective is to obtain a sin-\ngle set of high-quality summaries, i.e. a set with\nhigh probability under this distribution. This goal\nmotivates the approach we present here.\nLet us assume we can generate k candidate sum-\nmaries for each target language using diverse beam\nsearch (Vijayakumar et al., 2018) or a sampling\nalgorithm. In this setup, there are $k^N$ different sets\nof summaries resulting from the different combi-\nnations of selecting a candidate from each target\nlanguage. Among these sets, we wish to choose the\nset S* that maximizes (S), in order to achieve\nour goal of having a maximally semantically coher-\nent set of summaries. Interestingly, this criterion\ncorresponds to choosing the set S* with maximum\nprobability of being accepted in the rejection sam-\npling procedure described in Section 4.3.\nHowever, finding S* among the $k^N$ candidate\nsets is an instance of the generalized maximum\nclique problem, which is NP-hard (Feremans et al.,\n2003), and therefore we must resort to a heuris-\ntic search. For this purpose, we introduce a ran-\ndom permutation o of the target languages T, e.g.\n\u03c3(\u03a4) = (tv,tv-1,...,t1), and define the proxy\nsimilarity function as follows:\n$\\displaystyle \\hat{\\varphi}(S; \\sigma) = \\frac{1}{N-1} \\sum_{i=1}^{N-1} \\phi(Y_{\\sigma(T)_i}; Y_{\\sigma(T)_{i+1}}).$"}, {"title": "Choice of $\\phi$", "content": "So far, we have presented our methodology in a for-\nmal manner, but have not yet provided specifics on\nimplementing a function \u03c6 capable of measuring\nthe semantic similarity between two summaries in\ndifferent languages. In practice, any quality estima-\ntion model for MT (Section 2.2) could be used. In\nour experiments, we leverage the cosine similarity\nof SONAR embeddings (Duquenne et al., 2023) as\nthe similarity metric, reserving BLASER 2.0 (Chen"}, {"title": "Main Results", "content": "In this section, we present results on MTXLS con-\nsidering all the seven languages mentioned in Sec-\ntion 5.1 as targets. To perform this task, we took\neach of the seven languages as the source in turn\nand discarded the clusters that lacked a document\nin the source language. Then, we iterated through\nthe remaining clusters taking the document in the\nsource language as the input for summarization and\nwe generated summaries for all the languages in\nthe cluster, including the source language, using\neach of the methods mentioned in Section 5.2.\nThe results are in Table 2 and are presented\nper language pair. Due to space limitations, we\npresent detailed results only for English (en) and\nChinese (zh), and show the averages for the re-\nmaining source and target languages (rest). An\nextended version of this table, including detailed\nresults for more languages, confidence intervals,\nand the accuracy of each approach on following the\ntarget language is shown in Appendix D.1. When\nthe source and target languages are the same, S&T\nand PivotRR reduce to M2MS because we use the\nsource language as the pivot. Consequently, the\nresults of these three methods for ROUGE-2 and\nBLASER 2.0 (R) coincide for en\u2192en and zh\u2192zh.\nWe begin by discussing the results of Mistral\n7B, as these deserve special attention. Interestingly,\nthe model always performs worst in terms of simi-"}, {"title": "Effect of Varying the Number of\nCandidates", "content": "In this experiment, we investigate how the perfor-\nmance of our methods changes as we vary the num-\nber of candidates for re-ranking, using English as\nthe source language. To vary the number of candi-\ndates generated by beam search multinomial sam-\npling, we kept the number of beams per output\nsequence constant and equal to 5 and varied the\nnumber of output sequences. The results are in\nFigure 2, where we show the averages and standard\ndeviations across the seven target languages.\nInterestingly, increasing the number of candi-\ndates does not affect the similarity between the\nselected summaries and their respective references,"}, {"title": "Conclusion", "content": "This work introduces multi-target cross-lingual\nsummarization to address the challenge of achiev-\ning coherent summaries across multiple target lan-\nguages. We propose two re-ranking approaches\ntailored to this task, which improve semantic coher-\nence across languages compared to conventional\nbeam search decoding, while still preserving simi-\nlarity to the reference summaries. In particular, one\nof these methods eliminates the need for a pivot lan-\nguage, thus treating all languages equally and elim-\ninating potential biases arising from pivot language\nselection. Furthermore, we extended the evalua-\ntion framework for cross-lingual summarization by\nincluding the assessment of semantic coherence\nacross different target languages."}, {"title": "Dataset Clustering and Analysis", "content": "As mentioned in Section 5.1, the original CrossSum\ndataset presents documents in one language paired\nwith summaries in another language, a format that\ndoes not serve our multi-target setting. Therefore,"}, {"title": "LLM Prompt", "content": "The following prompt was used on the experiments\nwith Mistral 7B:\nFor the <source_lang> news article\nfrom BBC written below, provide a\nsummary in <target_lang_1>, a summary in\n<target_lang_2>, and a summary in\n<target_lang_N>. All summaries should be\none or two sentences long and follow the\nstyle of BBC. All summaries must contain\nthe same information. Present the answer\nin the format of a JSON object where the\nkeys are the language codes and the values\nare the summaries.\nText:\n<source_document>"}]}