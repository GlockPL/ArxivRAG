{"title": "A Sensitivity Analysis of Cellular Automata and Heterogeneous Topology Networks: Partially-Local Cellular Automata and Homogeneous Homogeneous Random Boolean Networks", "authors": ["Tom Eivind Glover", "Ruben Jahren", "Francesco Martinuzzi", "Pedro Gon\u00e7alves Lind", "Stefano Nichele"], "abstract": "Elementary Cellular Automata (ECA) are a well-studied computational universe that is, despite its simple configurations, capable of impressive computational variety. Harvesting this computation in a useful way has historically shown itself to be difficult, but if combined with reservoir computing (RC), this becomes much more feasible. Furthermore, RC and ECA enable energy-efficient AI, making the combination a promising concept for Edge AI. In this work, we contrast ECA to substrates of Partially-Local CA (PLCA) and Homogeneous Homogeneous Random Boolean Networks (HHRBN). They are, in comparison, the topological heterogeneous counterparts of ECA. This represents a step from ECA towards more biological-plausible substrates. We analyse these substrates by testing on an RC benchmark (5-bit memory), using Temporal Derrida plots to estimate the sensitivity and assess the defect collapse rate. We find that, counterintuitively, disordered topology does not necessarily mean disordered computation. There are countering computational \"forces\" of topology imperfections leading to a higher collapse rate (order) and yet, if accounted for, an increased sensitivity to the initial condition. These observations together suggest a shrinking critical range.", "sections": [{"title": "1 INTRODUCTION", "content": "Standard Artificial Intelligence (AI) approaches rely on high-performance computing such as with cloud or cluster computing. However, these are very energy-intensive resources, and many popular models are energy-intensive in training [90] and inference [56]. Conversely, biological intelligence has made highly energy-effective solutions, e.g. the brain. Despite operating under conditions such as increased decentralisation, asynchronisation, and slower signal propagation, biological intelligence has achieved highly energy-efficient solutions, with the human brain operating at approximately 25-30 watts [34, 96]. These observations indicate that there is still much to learn and gain from studying biological intelligence.\nIn this work, we focus on unconventional computational models, such as Cellular Automata (CA) or Random Boolean Networks (RBN), which utilise Boolean logic between local cells (nodes). This reliance on Boolean logic enables easy hardware implementation, as the operations can be implemented in circuitry or an FPGA, allowing energy-efficient inference of the model. It is possible to create an abstract pathway from CA to Biological Neural Networks (BNN); one example can be seen in Figure 1, and this pathway would require many steps. CA is a special case of RBN where the neighbour connections are entirely regular, and every cell has the same activation function. Viewed from the other direction, RBN is a CA with random neighbourhood and random rules per cell (node). RBN is a well-known simple biological model of the Gene regulatory network [44, 69], which is an intelligence- and computational space used for solving, among other things, morphological problems. Though there are multiple discrete steps between CA and RBN, as can be seen in Figure 1, we limit ourselves to exploring substrates between CA and Homogeneous Homogeneous RBN (HHRBN); these are the substrates that have the same rule (Homogeneous) in every cell. Essentially, we compare homogeneous topology networks to heterogeneous topology networks.\nNote that many of the modern directions of these CA and RBN computational models are moving into the continuous domain, such as for CA, the continuous models of Lenia [13] and Neural CA [71] are having much success modelling biological processes and biological like behaviours. Similarly, Random Boolean Networks (RBN) are moving into the continuous domain, such as continuous RBN [95] or stochastic RBN [19, 85], and they are argued to be more biologically plausible models. Although we want to encourage these explorations and essential directions, relying on the continuous domain currently means either running on specialised hardware or taking an energy efficiency loss, as floating point calculations are much more costly than integer or, even further, binary calculations. The future is still being determined, but when it comes to specialised hardware like neuromorphic chips [76], they are not expected to replace traditional hardware. Unless there is a significant breakthrough, specialised hardware like neuromorphic chips or quantum computing will likely exist alongside and in cooperation with conventional computational systems. Therefore, the energy-efficient binary models are still worth developing.\nIn this work, we stay within the binary domain for energy efficiency. We explore and compare three binary models for computation, namely CA and models intermediate to RBN (PLCA, HHRBN). We do this mainly to understand them as computational models better. Firstly, We combine them with Reservoir Computing (RC), an energy-efficient \"substrate-independent\" training method. The combination can potentially be an energy-efficient model in training and inference. We run these models on a simple 5-bit memory benchmark and find that ECA generally performs better. We follow up by measuring the sensitivity (a necessary but not sufficient condition of chaos) of the networks using a Temporal Derrida plot. We find that, in general, HHRBN and PLCA slightly increase the sensitivity. Yet, by analysing the defect collapse rate, we find that the substrates as a whole also skew towards ordered behaviour as there are mitigating circumstances like how the random topology leads to imperfect connectivity that leads to a stronger attractor. This signifies that we observe a shrinking critical range in PLCA and HHRBN. Though HHRBN are disordered in the topology, the behaviour of the network from these effects is not as disordered as it might be natural to assume. This means counter-intuitively that regular connections can more reliably reach a higher level of disorder."}, {"title": "2 BACKGROUND AND RELATED WORK", "content": "In this section, we will detail the background, related work, and theory relevant to this paper. This paper connects many fields, uses several substrates, and relies on several empirical and theoretical methods. This has made this section necessarily extensive to provide a comprehensive overview. In general this sections begins by explaining the different substrates, followed by more theoretical overview of said substrates as well as relevant concepts. Finally, the relevant related work is presented."}, {"title": "2.1 Cellular Automata", "content": "Cellular Automata (CA) are a simple model consisting of a grid of cells possessing a limited set of k discrete states placed on a uniformly connected grid, typically in 1 or 2 dimensions. The cell state changes iteratively, depending on the state of the neighbours. Which neighbour state combination results in which next state is determined by a lookup table, typically called the Transition Table (TT). CA was first used to study self-replication by John von Neumann in 1940 but published in 1966 [96]. It can be considered an idealised system for parallel and decentralised computation [68].\nElementary Cellular automata (ECA) is a subset of CA in 1-dimension, binary states (S = 2) and 3 neighbours (K = 3) (left, right and centre). Therefore, ECA only has $S^{S^K} = 2^{2^3} = 256$ possible rules, and the whole set of these is often named the rule-space. It is a convention to name individual rules in a rule-space after the output states of the TT Binary(01011010) = Decimal(90). CA is deterministic, and the rule, together with the initial condition, leads the CA into a set of subsequent states called the trajectory. An example of rule 90 can be seen in Figure 2. Rule 110 has even been shown to be computationally universal [14], but one can question whether that is a useful definition of computation for a parallel and distributed computational substrate [35].\nBeyond ECA are many other types of CA, such as 2-dimensional CA, where instead of configuring the cells in a 1-dimensional line, they are now configured as a 2D surface. In 2D CA, the most typical neighbourhood scheme is one of two configurations in Figure 3.\nThe Rule space of 2D CA is quite large, especially with a Moore neighbourhood. This space have $2^{2^9} = 1.32 * 10^{154}$ different Rules. It is too large to search exhaustively, but explorations into 2D CA are often limited to totalistic or outer-totalistic rules. Totalistic rules mean the rule does not distinguish which neighbours are in which state, but rather \"counts\" the number of neighbours with a specific state. 2D CA, with a Moore neighbourhoods, has only ten states to differentiate 0,1,...9 alive neighbours. Only $2^{10} = 1024$ totalistic rules exist in this rule-space. Outer-totalistic does the same but differs on the central cell. This means if the central cell is \"dead\" there are 9 (0,1,...8) different totalistic states the outer neighbours can be in and likewise, if the central cell is \"alive\". This means there are 2 * 9 = 18 states for outer-totalistic rules to differentiate. Therefore, there are $2^{18} = 262144$ different rules in this rule-space, though this can be somewhat reduced with symmetry equivalence classes [28].\nThe most famous version of 2D CA is Game of Life (GoL) [24], Figure 4. GoL is an outer-totalistic rule, and it works in the following manner: at each CA step, the next state changes depending on the following rules\n\u2022 Any living cell dies if it has two or fewer living neighbours.\n\u2022 Any Living cell persists if it has two or three living neighbours.\n\u2022 Any living cell dies if it has more than three live neighbours.\n\u2022 Any dead cell becomes alive if it has exactly three living neighbours.\nIn [81], it was also demonstrated that Game of Life is Turing complete. GoL can be expressed in a more general form, which is the convention for the outer-totalistic 2D CA. GoL would be in the form of B3/S23, where the Birth \"B\" component expresses the sum of neighbours needed to come alive, and the Survives \"S\" component expresses the sum of neighbours needed to stay alive. If a sum falls outside this value, the cell will die or remain dead.\nThe 2D outer-totalistic binary CA rule space is often called the \"life-like\" [50] rules-space, and beyond GoL, there are several other rules that are said to have \"life-like\" properties. In [20], the goal was to identify rules that could support similar \"life-like\" structures that can be constructed in GoL. In addition to identifying new ones, this article also provides an overview of many previously studied life-like rules that support structures such as replicators, oscillators, and spaceships."}, {"title": "2.2 Random Boolean Networks", "content": "The RBN is similar to a CA yet has two key differences. Firstly, in the RBN, the grid neighbour connections are not regular but randomly set up. Secondly, every node (cell) typically has a random TT, often called an Activation function or Boolean function. This type of RBN is also sometimes called Classical RBN (CRBN) [25]. The number of direct neighbours can be random, semi-random or constant. The latter is called homogeneous RBN [25], an example is given in Figure 5.\nAs with CAs, several extensions exist beyond the original RBN, such as Continuous RBN [95] or stochastic RBN [19, 85]. While Kauffman first developed the CRBN to model gene regulatory networks, these more modern extensions to the RBN model better emulate the biological activity of development [19, 95]. However, the CRBN were discovered early on [45] to contain a limited number of stable states, or attractors, from which the system would settle down to following a random initialisation. The basin of attraction reduces numerous initial states to a few stable cycles or fixed points.\nRBN can be defined as the following. A set of N nodes connected randomly to K number of other nodes, the specific connections for a given node can be denoted by $K_N$. The nodes can be in one of the two binary states, and every N has the a random activation function $f_a$ (TT), out of $2^{2^K}$ possible rule setups."}, {"title": "2.3 RBN Classification", "content": "ECA is often partitioned and classified into several different categories or traits. In [63], a good overview of many common or well-known ones can be found.\nSimilarly, RBN can be classified by their behaviour, i.e. ordered, complex or chaotic [25, 47]. Depending on the value of N and K, the behaviour might differ, and one alternative name for RBN is the NK model. In [46], Kauffmann added another parameter P, which can organise the rule-space. The rule has a given P parameter value based on the number of neighbourhood combinations resulting in a 1 or a 0. In later work [47], the larger distribution dominates, meaning $P \u2265 0.5$. Figure 5 has $P = 0.5$. One can use this parameter to control the behaviour. P close to 1 would likely result in ordered behaviour, and P close to 0.5 would likely result in chaotic behaviour. In between these, a critical (complex) $P_c$ behaviour might be found in the phase transition between order and chaos. This point or border is often also called the edge of chaos. The work is reminiscent of CA work in [49], which we will introduce later. The P is the same as A is in Langton's work, and in this work, we will use the A notation for both.\nAnother way to categorise RBN and CA is to look at the basin of attraction. [105, 106, 107, 109] did extensive work in both RBN and CA and their basin of attractions. What opened up this possibility was a method that could calculate backwards from a state. Take a cell in a state and consider what possible local neighbourhood configurations would result in this state. These are the possible previous states (preimage) for the neighbourhood. Finally, this can be applied to all the cells and limit the possibilities between cells by constraining satisfaction. The possible preimages often collapse to very few, making it possible to calculate the basin of attraction quickly."}, {"title": "2.4 Intermediate Substrates", "content": "A system can be in a range of possible states that would be somewhere between CA and RBN. This paper will discuss a substrate with homogeneous rules but random neighbour wiring. This is what we define as HHRBN. HHRBN to distinguish it from what is in [25] called HRBN. It is called HHRBN rather than non-local CA because the substrate seems to behave more like RBN than CA, and the equivalence in this substrate is more applicable in RBN than CA.\nIn [51], Li worked with systems where all cells had the same activation function (TT), but the neighbour connections were in various configurations. In this work Li classified the different connection schemes as between non-local (random) and partially-local (central self-reference) as well as non-distinct and distinct input/output (uniform number of outputs). Li then classifies the rule-space for these substrates using mean field approximation and shows they are very neatly classified, particularly non-local CA (HHRBN).\nMuch earlier [97, 98, 99] studied a system that Li would classify as partially-local CA.\nHHRBN has additional commonly used names beyond non-local CA [51], such as Graph CA [31, 61] or (Cellular) Automata Networks [6]\nIn [105], Wuensche examined substrates between CA and RBN, including non-local CA and other disordered CA. He defines disordered CA as a superset of CA, which includes non-local CA and mixed rule CA. Furthermore, Wuensche calculates these networks' basin of attraction fields and demonstrates how rewiring the network can train or modify the basin of attraction. Mixed rule CA is also known as Non-uniform CA [6, 11] or hybrid CA [6]."}, {"title": "2.5 Minimum Equivalent (ME)", "content": "In ECA, RBN and everything in between, we find equivalence classes that effectively reduce the number of unique rules for a given substrate. The List of rules that make up the minimum set of unique rules is called the Minimum equivalent (ME). These rules can be used as a smaller replacement for the entire computational space of a specific substrate.\nECA consists of $2^{2^3} = 256$ rules, but due to symmetries and other properties, there are only 88 rules that are considered unique. The reason is that all excluded rules can be transformed into one of the 88 unique rules by one of the following trivial methods."}, {"title": "2.6 Definition of chaotic behaviour in nonlinear systems", "content": "To this day, the definition of chaotic behaviour is not mathematically univocal [48, 93]. Still, we will work with a well-accepted definition of chaotic function. The definition is as follows (see. [17, subsection 1.8]): a function or map $f: V \u2192 V$, on a vector space V, is chaotic if it satisfies the following conditions:\n\u2022 f has a sensitivity to initial conditions,\n\u2022 f must be topologically transitive,\n\u2022 has dense periodic orbits (periodic points are dense in V).\nIn essence, a function or map is unpredictable as it is sensitive to initial condition, indecomposable (can not be decomposed into two or more subsystems) as it is topological transitive, and yet has an element of regularity as it has regular periodic orbits that are dense. Devaney also notes [17] that while there are stronger definitions of chaotic function, the one above is a good definition in that it is generally easy to verify and applies to a larger number of important examples.\nIndeed, if a system is topologically transitive and has dense periodic orbits, then it also has sensitivity to initial condition [5]. This means one should view sensitivity to initial condition as a necessary condition for f to be chaotic, whereas topological transitivity and the existence of dense periodic orbits together are sufficient conditions. This implication means it is possible to drop the first condition of Devaney's definition. We will, however, keep it as part of the definition of chaotic function since it is easy to check in practice.\nAs mentioned above, the definition of chaotic behaviour usually applies to continuous variables. However, it can be adapted to systems whose time-evolution is intrinsically discrete, as is the case of CAs and RBNs. Chaotic behaviour can be considered in eight different situations, taking all possible combinations of continuous and discrete time, continuous and discrete space, and continuous and discrete state variables. We will consider the most common cases of such combinations, aiming to introduce the concept of chaotic behaviour in CA and RBN.\nThe definition of chaotic function when space, time, and states are continuous variables is given above, as this is the common situation when defining chaotic behaviour. The prototypical example of a chaotic system in such conditions is a turbulent fluid, described by the so-called Navier-Stokes equations [80], which are partial differential equations, having therefore continuous time and space variables as well as continuous state functions (velocity of the fluid).\nThe most well-known example of a chaotic system is the Lorenz system. This system comprehends a set of three ordinary differential equations to model forced dissipative hydrodynamic flow [55]. It is defined by $\\dot{x} = \u2212\u03c3x + \u03c3y$, $\\dot{y} = -xz + rx - y$ and $\\dot{z} = xy - bz$. For the choice of parameter values \u03c3 = 10, b = 8/3,r = 28 [18], one gets an orbit resembling a butterfly when plotting the x against z. Due to its odd features, this orbit was classified as a \"strange attractor\", a stable orbit showing chaotic behaviour. One such feature is that arbitrarily small difference in the initial condition leads to large deviations in the following orbits over time. In other words, the Lorenz system shows sensitivity to initial conditions.\nNote that this system does not have a space dimension - in our classification with time, space and state, the phase space, accounts for the state dimension. It is rather a low-dimensional example of chaos [42, subsection 3.2.2], not an example of spatiotemporal chaos.\nYet, the same definition can be used when space is discrete. Indeed, a spatially extended (continuous) system can be discretized into a mesh or grid of points, each governed by a set of differential equations where both dependent and independent variables - state variable and time, respectively - are continuous. This would be the case of a (discrete) set of coupled Lorenz systems distributed in space, each described by continuous states and time.\nAnother famous example of chaos but with discrete time is the logistic map $x_{t+1} = ax_t(1 - x_t)$, used as a simple mathematical model to population dynamics [18, 66]. Yet, this simple equation is capable of producing chaotic dynamics. From 3 < a < 3.57, the population starts to oscillate between more and more values until it reaches the parameter region of about 3.57 < a < 4 where the oscillation explodes into the infinite (no longer oscillating). For this region, small initial values for the population yield large variations over time (sensitive to initial condition) [69, Chapter 2]. To note that this is not true for all regions of 3.57 < a < 4, e.g. at $a = 1+ \\sqrt{8}$ there is a period-3 cycle.\nSimilarly to the Lorenz system, the logistic map does not have an explicit space, but space can be introduced by considering again a discrete set of coupled maps for a so-called coupled map lattice [42, subsection 3.2.2].\nThis is the case of chaotic behaviour observed in CAs and RBNs, which have discrete state space (discrete number of accessible states, e.g., a binary set of \"0\" and \"1\"), evolve iteratively (discrete time), and are composed of a discrete set of (spatially localised) nodes or cells. We, therefore, call them fully discrete systems.\nIn a strict sense, taking the definition of chaotic function introduced above, this case can not show chaotic behaviour. To understand this, we can consider the concept of dense periodic orbits. The definition of dense can be stated as follows:\nLet A be a subset of a topological space X. Then A is said to be dense in X if:\n$\\forall x \\in X, \\forall \\epsilon > 0, \\exists a \\in A : d(x, \u03b1) < \\epsilon$,\nwhere d(x, a) is the Euclidean distance between x and a. This means that in order to be dense, it must be possible that elements of the two sets X and A are arbitrarily close. This feature can not occur in intrinsically discrete systems, as distances are always multiples of an elementary (the smallest positive) distance between two distinct instants, states or spatially distributed nodes. We shall continue this discussion in subsection 5.6. In the rest of this paper, we will assume an \"analogue\" of chaotic behaviour based on the comparison of the number of accessible global states (set of individual states) and the periodicity of the overall trajectories. More precisely, in the particular case of having a CA with N cells, each one taking values {0, 1} (binary states), and then there are $2^N$ accessible configurations (global states). Being a finite number of accessible configurations, chaotic orbits can not occur strictly, as defined above: all orbits will be periodic with a period not larger than $2^N$. In this context, we define \"chaotic\" behaviour as the one typical of rules for which the periodicity scales geometrically with the size of the system. In particular, a CA of size N is considered to show \"chaotic\" behaviour if, when doubling its size to 2N, the periodicity of its orbits increases quadratically, from $2^N$ to $2^{2N}$. Henceforth, behaviours observed in ECA are called \"chaotic\" with the quotation marks to more clearly separate it as an analogy of chaos in other systems. However, these quotation marks are often omitted.\nPerhaps the most famous example of \"chaos\" in systems with discrete time, space and states is the ECA Rule 30. Despite being a simple substrate (ECA) configuration, it can produce very complex and pseudo-random behaviour. In Mathematica, the central column of rule 30 is used for the pRNG [104, p. 317].\nWork has also been done for a subset of CA called linear CA, in [12] it is claimed that for linear CA over Zm that ergodicity is equivalent to topological transitivity and that dense periodic orbits (regularity) are equivalent to surjectivity.\nIn [58] Martinez worked on a method to determine if a linear CA over Zm has a behaviour that is equicontinutity, sensitive to initial condition, strong transitivity or is positive expansive. They take a hierarchical view of the definitions of chaos, from positively expansive to strong transitive to transitive to sensitive, being the weakest.\nBeyond a scientific interest in the computational aspect of chaos, it has some very useful applications. Like rule 30, many other chaotic systems, such as the logistic map [69, Chapter 2], can be used as a pseudorandom number generator (PRNG).\nTo understand why, we begin with Shannon's concepts of confusion and diffusion [87]. To mitigate simple statistical analysis, a cipher must have properties of confusion and diffusion. Confusion, as in the input and the output, should have a very complicated relation. Diffusion means that the input should affect the whole output. These concepts are also important for pRNG. Let's view this in terms of chaotic behaviour; a system sensitive to initial conditions would mean that small changes to the initial condition would lead to large changes in the output. Therefore, one can not use a similar output to predict the input, enhancing confusion. Further, a topologically transitive system (cannot be decoupled) ensures that one cannot decouple the solution, leading over time to an effect on every part of the system, enhancing diffusion. Finally, a dense periodic orbit means the system can return to close but not the same configurations, ensuring a rich set of values that are close in output but distant in input, enhancing confusion and diffusion.\nTypically, when testing systems for pRNG, one does many statistical tests such as the ones in [7]. Note that one of the tests in this article is non-linearity, which would imply that the exploration of linear CA as chaotic systems would unlikely result in a good pRNG. Furthermore, theoretical reasoning states that highest capacity computation lies on the edge of chaos (see subsection 2.7). Assuming this is true, identifying chaotic behaviour is important for identifying useful computational substrates."}, {"title": "2.7 Identifying the edge of chaos and with a parameter", "content": "The parameter space of a complex system often has a phase transition between order and disorder; this phase transition region is often called \"Edge of Chaos\" It is theorised that this region commonly contains the highest capacity for computation defined as transformation, manipulation and storage of information.\nLangton [49] explored this theory in 1-dimensional multi-state CA with enlarged neighbourhoods and found that the CA rule-space forms a phase transition between order and chaos when organised over a \u03bb (Lambda) parameter. The A parameter starts by defining a state as the quiescent state. To generate a Transition Table (TT) with a given A value, one allocates to each TT entry a random number a uniformly distributed between 0 and 1 and attributes the quiescent state to all entries with \u03b1 < \u03bb and a non-quiescent state to the other. Using this method, Langton generated different candidate rules in several regions of the rule-space over the A parameter. He showed that the rules-space organises into a phase transition between order and chaos and that strong candidates for computation are more likely to be found there. Notably, this lambda method does not seem to work in the ECA rule space, as mentioned in [49] and previous work."}, {"title": "2.8 Reservoir Computing (RC)", "content": "Reservoir Computing (RC) is a substrate-independent framework for computing. RC is independent because it works on many different substrates, but to be clear, different substrates would, of course, have different capabilities. The RC framework consists of 3 parts: the input, the untrained reservoir and the output.\nThe input part encodes some information into the untrained reservoir and typically into higher dimensions. The untrained reservoir typically expands, modifies or changes the information, but could, in the context of the framework, be considered a black box as seen in Figure 8. The output part is typically linear, does dimensional reduction, and extracts useful features.\nThe RC concept originated in echo state networks (ESN) using recurrent neural networks as a substrate [37] and in liquid state machines (LSM) using a spiking neural network for a substrate [57]. Since then, both ESM and LSM and a host of other substrates have been put under the umbrella term of RC. Due to RC substrate-independent nature, many different substrates have been explored and/or compared [92]. Some explore different topology configurations as in [23], where a deep layered sub-reservoirs were analysed instead of the typical one big reservoir. RC is also a very popular method with physical reservoirs [92], as an extreme example in [21] it was demonstrated that RC can use the surface waves on a bucket of water as a reservoir and they successfully solved speech recognition and xor tasks using this substrate. One interesting substrate is real biological neural networks (BNN), specifically disassociated neurons that self-organise over a microelectronic array [2].\nThere is also evidence that reservoir computing is a useful trick for computation (one of many) used in biology. [74] shows that a linear classifier can extract information about the short-term past stimulus (images, xor) from the primary visual cortex of an anaesthetised cat. Additionally, there is some evidence of RC in other biological and computational processes. In [15], the ESN (RC) model was used to simulate an example of a known genetic regulation network (GRN) process and performed satisfactorily. Similarly, in [41], the LSM (RC) model was used."}, {"title": "2.9 Reservoir Properties", "content": "An important property in ESN is the Echo State Property (ESP). Given some input signal, the reservoir must asymptotically remove the initial condition information to have this property. In [37], it is shown that for a reservoir with specified conditions, it violates the ESP if the spectral radius of the weight matrix is larger than 1, and it was empirically observed that for Spectral radius below 1, the ESP is given. Note that in [36] Jaeger warns that this does not mean that ESP is granted for any system with a spectral radius of below 1 (asymptotically stable). It is not a necessary nor a sufficient condition.\nSimilar to the ESP is the concept of the fading memory property. It states that an input/output system is said to have fading memory when the outputs associated with inputs that are close in the recent past are close, even when those inputs may be very different in the distant past. [32, 92].\n[57] determines two conditions for real-time computation on perturbations. The separation property (SP) is a necessary condition, and the approximation property (AP) is a sufficient condition. SP refers to the separation between trajectories based on differences in perturbations. AP refers to the capabilities of the readout mechanism."}, {"title": "2.10 Reservoir Computing with CA (ReCA)", "content": "The first study that introduced CA as a substrate in reservoir computing is [110]. This study investigated Game of Life and several ECA rules as reservoir substrates and tested on a 5-bit and 20-bit memory benchmark. In addition, it presents a theoretical comparison of CA vs ESN, using the metric of the number of operations needed to solve the benchmark, which documents a clear advantage of using CA.\nAs an ECA reservoir only relies on simple discrete binary interactions between cells (see [83] for details), it affords a hardware-friendly substrate implementation. The problem (perhaps ironically) becomes how to implement the readout layer in hardware. In [70], ReCA using ECA with a max-pooling and softmax strategy was implemented on a Field Programmable Gate Array (FPGA). In [75], a CA was implemented on Complementary metal-oxide-semiconductor (CMOS) combined with a custom hardware SVM implemented in resistive random-access memory (ReRAM). In [53], a synthesised hardware implementation of ReCA using ECA with a max-pooling and ensemble bloom filter classifier. Showing impressive results compared to \"state-of-the-art\" in terms of energy efficiency, memory usage and area(number of gates) usage, but with comparably poor accuracy [70].\nOther works have also studied ReCA using the 5-bit memory benchmark. [73] changed the structure of the CA to a deep-layered architecture and compared it to a single layer, which resulted in noticeable performance improvements. Additionally, in [72] the authors organised the CA substrate as consisting of two regions of different ECA rules. Different combinations of rules were explored, and some showed great promise. In [59], an exploration was conducted of different cell history selection methods for the classification model on the 5-bit memory task, a temporal order task and arithmetic and logic operation tasks. In [4], CA rules with multiple states and larger neighbourhoods were evolved and then tested on the 5-bit memory benchmark. In [94] ECA and asynchronous ECA is tested and compared on the 5-bit memory benchmark, mainly in the context of the distractor period.\nIn [60], it was pointed out that the benchmark has no train test split. They modified the benchmark by training on just a few (2 or 3) of the 32 possible input streams, and some of the rules with more ordered behaviour could still solve this version of the benchmark.\nIn [29], the full ECA set was tested using key parameters of number of bits (Nb), redundancies (R) and Grid size. [30] extended this work to include more parameters such as Iterations (I) and Distractor Period (Dp). This paper also explained many of the unexpected results in the previous study, but perhaps as important, it similarly to [60] pointed out some weaknesses in the 5-bit memory benchmark.\nReCA is also used on other benchmarks than the 5-bit memory benchmark. [26, 53, 70] implemented ReCA in hardware and tested using MNIST. An additional example is [67], where the authors solved tasks of sine and square wave classification, non-linear channel equalization, Santa Fe Laser Data and iris classification.\nIn [43] an method for Rule Selection for ReCA was presented. Limiting the search-space to only Linear rules that obey a list of specific mathematical properties (see paper for details), the paper demonstrates the method selects for rules in the high performance (95-80 percentile) bracket on several time-series prediction benchmarks compared to the full Linear CA space of same neighbourhood and number of states."}, {"title": "2.11 Reservoir computing with Random Boolean Networks (ReRBN)", "content": "In [88, 89], ReRBN was explored on temporal parity and temporal density (temporal majority task). For the tasks and parameters explored, it was found that the heterogeneous RBN (different in-degree RBN) reservoir worked best at a critical connectivity K = 2 (in-degree of 2). In contrast, [8] found that for homogeneous RBN, criticality was instead found at K = 3. [9] extended this work, exploring different reservoir properties such as perturbation percentage, the relationship with attractor and performance and comparing a subset reading from a larger reservoir to a subset equal reservoir.\nIn [10], the relationship between N and K was also studied with a balance b between excitatory and inhibitory nodes. They find that K is the most important of the control parameters, as it affords simpler fine tuning of the other parameters."}, {"title": "2.12 Reservoir computing with Intermediate substrates", "content": "RC explorations Between CA and RBN substrates are less common. This paper reports and extends on work done in a master thesis [39], where Life-like CA, ECA, PLCA and HHRBN were explored using the 5-bit memory benchmark.\nIn another master thesis [40], Reservoir computing with cellular automata networks where explored on a simple text classification task. The study explores and compares different ways to construct the network and how that affects performance. The cellular automata networks described include fixed predecessors (in-degree); from the description, it seems they explored PLCA, confirmed by the lack of the same score for rules 204 and 170. Yet, we can not directly compare it with the work in this paper, as the study constructs the transitions rule differently."}, {"title": "2.13 5-bit Memory Benchmark", "content": "The 5-bit memory benchmark traces its root to the short long-term memory task introduced in [33]. Although often cited as the source [4, 72, 73, 110], none of the benchmarks in [33] are the 5-bit memory benchmark, but some of them are very similar in intention. The earliest source where the 5-bit memory benchmark is recognisable is in [62], but named \"noiseless memorisation\u201d, corroborated with the clearer and more detailed explanation of the benchmark in [91, p. 47] and in [38].\nThe 5-bit memory benchmark's goal is to test whether a system is capable of memorising a 5-bit and reproducing it at a later stage. Table 6 shows an example of the memory task. The benchmark has 4 input channels where only a single channel can be active at the same time. The first two input channels are dedicated to the 5-bits. The bits are fed into the system sequentially over 5 steps. One can view the first input channel as the \"pure\" 5-bits and the second as the reversed 5-bits. The 3rd input channel is dedicated to constantly feeding input into the system during the distractor period and the output stage. The 4th input channel is dedicated to the cue signal, signalling that the output is to be given. The benchmark has 3 output channels where one and only one should be active simultaneously. Note that some earlier examples have 4 output channels but one is dropped as it is never intended to give output. The first two are dedicated to the original 5-bits inserted into the system and should sequentially output them following the cue signal, the final output channel should give a signal in all other cases. Due to this output's nature, one can abstract and view the task as a temporal classification problem.\nIn this paper, we often call it the x-bit memory benchmark, as we have varied the number of bits to be memorised. Also, note that the 20-bit memory benchmark mentioned in some of the previous sources is not the same as the 5-bit memory benchmark but with 20 bits to memorise. The 20-bit memory benchmark uses 7 input channels, 5 for the input and a bit length of 10."}, {"title": "2.14 Small-world", "content": "In [100], they explored graphs varying on p value where p = 1 meant random connectivity and p = 0 is regular connectivity. They demonstrated that small worldliness was achieved with a relatively low p-value. However, in relation to this work, they have a larger neighbour degree. Additionally, we work with fixed in-degree networks (all cells have 3 neighbours). For these reasons, we might not see the same level of small-worldness in our topologies, but naturally, in contrast to ECA, some is expected in PLCA and HHRBN."}, {"title": "2.15 Derrida Plots and the Derrida Coefficient", "content": "Derrida Plots is named after the author of its origin in [16]. It is a tool primarily used to identify the behaviour of a particular RBN (critical, chaotic or ordered (frozen)). Derrida originally used it to compare a classical RBN (quenched) and an \"annealed\" RBN, in which every connection and activation function is randomly reassigned after each iteration. To construct a Derrida plot, one compares different initial conditions on the same system. Start with a random initial condition, flip 1-bit for one of the initial conditions, then evolve both the RBN one step, and calculate the $D_h$. Then do the same but for two flipped bits in the original initial condition, and so on until N bits. One plots the $D_h$ as a function of the number of flips in the initial condition. Typically, the hamming distance increases linearly with the number of flips until it saturates. If the linear increase has a slope larger than one, the behaviour is considered to be chaotic; if the slope is below one, the behaviour is ordered (frozen); and if it is exactly one, the behaviour is critical (\"complex\u201d) [22][108, p. 246].\nThis behaviour classification can be formalised with the Derrida coefficient, $D_c$, given the angle \u03b8 of the initial slope, namely $D_c = log_2 (tan \u03b8)$. For $\u03b8 > \u03c0/4$, $D_c$ is positive and negative for $\u03b8 < \u03c0/4$ [108, p. 250].\nIn [3] the Derrida coefficient of ECA was mapped together with the Generative morphological diversity \u03bc, to classify ECA on a spectrum of autistic, schizophrenic, and creative personality."}, {"title": "3 METHODOLOGY AND EXPERIMENTAL SETUP", "content": "This section will detail the specific methods and experimental setup used in this paper. We begin with the experimental methods by documenting the x-bit memory benchmark details used for the 2D life-like (CA, HHRBN) and the 1D (CA, PLCA, HHRBN). Then, we will explain the details of the Temporal Derrida Plots (TDP) used to analyse the sensitivity. Then, we will give details on how we measured the rate of defect collapse (collapse rate). We continue with the network analysis method of the longest simple cycle and how it is estimated. Finally, we will document the source code and the dependencies with which the code was built."}, {"title": "3.1 x-bit memory benchmark", "content": "The 2D life-like experiments use the same setup as [64]. It uses a parameter of R, which in this specific experiment is the grid size, and in terms of full grid size, it is R x R. The Iterations I represent the number of iterations between encoding steps and the number of steps fed into the classifying model, chose to be a ridge regression model. The projection ratio $P_r$ is the ratio of cells that the input is encoded into, set to $P = 0.6$.\nFor the 1D substrates 5-bit memory benchmarks, the experimental setup and the default parameters are the same in [30]. Redundancy R = 4 is the number of connected \"sub-reservoirs\" with individual mapped input. Note how R in the life like experiment and in the 1D experiment signifies different things. The Iterations I = 2 represent the number of iterations between encoding steps and the number of iterations the classifying model had access to. The sub reservoir grid size $L_d = 40$, meaning the total number of cells(nodes) where $L_d * R = 160$. An example demonstrating R, $L_d$ and I can be seen in Figure 10. The classifying model, in this case, was an SVM with a linear kernel."}, {"title": "3.2 Temporal Derrida Plots (TDP)", "content": "In this work, we introduce a variant of Derrida plots: Instead of introducing a new defect at each step (see subsection 2.15), we follow the development of one or a few defects starting at t = 0 and see how $D_h$ changes throughout time (i.e. as a function of iterations). In this way, one follows how $D_h$ diverges or converges to a specific value over multiple iterations. Henceforth, we call these plots \"temporal Derrida plots\u201d (TDP). Derrida plots retrieve approximately the Lyapunov exponent in state space, whereas the temporal Derrida plot retrieves the Lyapunov exponent in time.\nIf we take the simple example of the rules 204 and 170, a simple inspection of these rules would tell you they are very ordered in their behaviour, simply propagating the initial condition. Yet, using the original method [16] and as described in [108] (cf. [3, Appendix A]), Derrida plots for rule 204 and 170 yields $D_c = 0$, meaning that they follow the 45 angle line. This interpretation is that rules 204 and 170 are complex/ critical in the Derrida Plot method. This is not the case with our TDP. Therefore, we argue that the Derrida plot method's weaknesses in ECA substrates are solved with our variant of TDP. Furthermore, if one is identifying chaotic features for the purpose of harvesting the chaos for something directly useful, e.g. a Random Number Generator (as they are being used for [104, p. 317]). Then, it would be more beneficial to know the development through the substrate over time, as a single step would not be enough to diffuse the seed value. In contrast, a strength of using the Derrida plot rather than the TDP is that it allows one to sample a larger number of initial states of the state space.\nIn addition to the $D_h$, we plot the Damerau-Levenshtein distance (Dal). This can catch deceptive different-looking configurations like the aether in rule 110; this effect has a marked impact later in the results section with Figure 14b.\nFurthermore, we run these TDP beyond N steps as we also want to see where the substrates settle. Which we argue tells us something of how \"chaotic\" the substrate truly is; a substrate that is \"chaotic\" to the idea of using it as an RNG should not have any preference for 1 or 0 (balanced), and where the substrate settles tells us this experimentally. If a system is \"ergodic\" to the sense that it covers the entire state space, then it should find every state equally likely and settle at a $D_h$ of half the grid size (half-max distance).\nWe run this for five configurations, 1, 5 and 9-bit changed in the centre, and 5 and 9-bits changed randomly. The randomly placed defects are coded such that there are no collisions in placements, as introducing a defect in the same place twice would cancel each other out. Note that there is, in effect, little difference between centre and random in PLCA and HHRBN; the random defects were introduced to better compare between the substrates and kept throughout the experiments for PLCA and HHRBN for consistency. All the TDP experiments use grid size of 100 cells ($N = 100$)."}, {"title": "3.3 Defect collapse", "content": "We explore whether the systems tend to collapse into the same attractor after a defect is introduced. This can happen in CA because the attractor basin is large enough to encompass the defect. However, in PLCA and HHRBN, this can also occur due to the neighbourhood itself, e.g. if you encode the information into a node that is not the in-node of any other cell, the information can not propagate anywhere. We inspect the substrate in two ways: via the Defect plots when all collapsed defects have been excluded from the data in Subsection 4.3, and we look at the statistic of collapsing in Subsection 4.4."}, {"title": "3.4 Longest Simple Cycle", "content": "The difference between CA, PLCA, and HHRBN is essentially that of the topology. All the topologies can be reduced to graphs, and therefore, it is natural to apply some graph theory, yet the graph theory sub-field is broad, and the scope of this paper is already large. Therefore, we limit ourselves to finding the longest simple cycle in the topology. The longest simple cycle is the longest cycle without any repeating node (except the first and last). We picked this metric because it indicates how much information can be encoded into the network, as any oscillating pattern in the substrate would be limited by the longest simple cycle. Note that the longest simple cycle in CA would be equal to the number of cells (N) due to its regular neighbourhood configuration. It is Therefore, it is not necessary to run this analysis on CA."}, {"title": "3.5 Source code and Dependencies", "content": "The source code for the project can be found at [27]. The code relies primarily on Evodynamics [79] to run the ECA, PLCA and HHRBN and on scikit-learn [77] for the classification models. A more detailed list of dependencies can be found in [27]."}, {"title": "4 RESULTS", "content": "In this section, we present the results of this paper in chronological order. Starting with the 5 bit memory benchmark experiments, then the TDP and collapse rate results, followed by the network analyse and finally a extended 3 and 4 bit memory benchmark results are presented."}, {"title": "4.1 Life-like 5-bit memory benchmark", "content": "We begin with a smaller experiment between 2D outer totalistic CA (life-like CA) and HHRBN (note that the concept of 2D breaks down in HHRBN). There are $2^{18} = 262144$ different rules in the \"life-like\" rule space(see subsection 2.1). Therefore, an exhaustive search was not practical. A subset of interesting behaving rules were selected from [64, 65, 78]. The results can be found in Table 7, and we see here that many of the rules perform well in the CA case but not in the HHRBN case, except for B368/S12578. We can quite clearly see from Figure 11a and 11b that the behaviour of said rule changes. It is important to point out that there is a bias as these rules have been selected for their behaviour in a CA context. Therefore one can not conclude about the greater scope of ECA, and HHRBN reservoirs, we can at least say that the topology changes the behaviour. The CA results are better overall than in [64, 65], though not of a different scale, this might be explained by the difference in hyper-parameter or other implementation detail of the ridge regression model as one was implemented in Julia and the other in Python, we therefore still consider it a successful replication of the previous study."}, {"title": "4.2 ECA 5-bit memory benchmark", "content": "A similar exploration of the full ME set of ECA rules for ECA, PLCA, and HHRBN was also conducted. The bias of selecting rules is removed by exploring the entire rule space. The rules have different ME sets [28], but the ECA ME set is a super-set of the HHRBN ME set. Therefore, we use the ECA ME set by default. In Figure: 12, we see these results. There is a clear trend that general performance goes down from CA to PLCA to HHRBN. In the perfect run metric, only three rules scored any perfect run for HHRBN (108, 170, 204). We see a similar trend in the weighted average metric. Note that these rules (108, 170, 204) are all ordered in behaviour and that rules 170 and 204 are equivalent in the HHRBN ME set, meaning in effect, only 2 behaviours of the 46 unique HHRBN managed to solve the task. More details and results can be found in [39]."}, {"title": "4.3 Temporal Derrida Plots", "content": "In this subsection, we will present the TDP, all plots except Figure 17a have the collapsed runs removed; the tally of the collapsed runs can be found later in Table 8, 9 and 10. They are separated because the collapse can greatly impact the temporal Derrida plots. In short, we still see this impact by removing them and displaying them alone, but we can compare sensitivity independent of collapse rate.\nWe begin with an example of the ideal \"chaotic\" ECA rule 30 in Figure 13a. In ECA, we see as we expect, the randomly placed defects to be quicker to permute the substrate. The central defects naturally take longer to permute the substrate as they are limited by the CA's \"speed of light\" (in CA, information can only flow to direct neighbouring cells at every step, creating a speed limit for information often called the \"speed of light\"). For PLCA and HHRBN, much is the same, except the defects permute the substrates faster. The CA \"speed of light\" does not apply similarly to a random topology of PLCA and HHRBN; the random topology creates a certain level of \"small worldness\". We will discuss this further in subsection 2.14 The defect configuration for many substrates settles towards the same $D_h$ of 50 (half-max distance), the normal average distance two random binary vectors would have between each other. That they all settle at the same distance is also a test of sensitivity, as the differences between configurations have expanded to the maximum probabilistic difference.\nFor rule 90, seen in Figure 13b, it's clear that the ECA behaviour is very different. The distance might look erratic at first but is quite regular, and it is due to rule 90s additive behaviour. In rule 90 and all the other additive ECA, the defects permute in a way that is invariant to the initial condition of the different cells see [30, subsubsections: 2.1.2-3 and 5.5.5-6 ] for more detail. The PLCA and HHRBN behave much closer to rule 30 and \"chaotic\" behaviour.\nHowever, reflecting on this, we hypothesise that this is due to every run having a random topology rather than the regular Rule 90 behaviour somehow becoming \"chaotic\u201d in PLCA and HHRBN. We also see that the trajectories settle at roughly the same place but with a slightly higher amplitude in rule 90 than in rule 30, indicating a more significant variance."}, {"title": "4.4 Collapse rate", "content": "We will begin with the collapse rate for the selection of rules in the previous section; these are found in Table 8, 9 and 10.\nWe see that for these rules, there is almost no collapse in ECA, but for PLCA, 1-bit defects regularly collapse except for Rule 54. In contrast, rules 170 and 184 have a large number of collapses. In the case of rule 170, we will explain, in subsection 4.5, how the rule dependency is one reason for the high collapse rate. For the HHRBN, much is the same compared to PLCA, except for rule 54, which has a higher collapse rate for 1-bit defects. In Figure 17b, we see the TDP for the full ECA ME results with and without collapsed runs included. Firstly, we can see that the additive rules, such as rule 90 and its frequencies, impact the data such that it is still visible in the total data consisting of all the 88 ME rules. We can also separate the different defect initialisations in inclusive and exclusive collapsed runs. Similarly, in PLCA, we see a separation between the defect sizes but not the defect types (random, central); this is as expected as they are functionally the same in PLCA and HHRBN substrates. In HHRBN, excluding collapsed runs, we see something that looks mainly like sensitive behaviour, though it does not settle at a roughly half-max distance. This might be due to how the ME set is derived because of the complement transformation many of the later rules in the rule-set are excluded, and the later rules have on average a higher \u03bb, in fact, the average \u03bb = 0.386. See Table 5 for why that is. Additionally, we observed that a 1-bit defect has a slightly higher settling distance. Looking at many individual rules, a few rules, such as rule 140, have this effect of a significantly higher average for the 1-bit difference."}, {"title": "4.5 Network topology, longest simple cycle", "content": "We can see from examples such as Figure 13a that the original ECA \"speed of light\" is still there in a sense also for PLCA and HHRBN, though vastly different. Defects can still only affect neighbouring cells, but the pathways through the network that the defects can propagate are more small-world than ECA. This is unsurprising as they are randomly generated in contrast to ECA. We want to create a sense of how this affects the computations. We do this by finding a network's longest simple cycle. The longest simple cycle indicates the theoretical memory size that can be encoded into the network and the substrate's ability to retain a memory in the cycle.\nWe begin by considering a 1-dependency rule, such as rule 170. A ECA rule that depends only on 1 neighbour means by definition that 2 of the neighbours do not affect the computation. This effectively reduces the in-degree from 3 to just 1. Though our collapse results do not show a clear trend based on this dependency, we still argue that it must affect the computation because the network properties of a 1-in-degree network are majorly different from a 3-in-degree network. The following evidence may persuade the reader of this conclusion.\nWe take the first topology generated for rule 170 from the data in Figure 12 and only keep the nodes that compute (the third in node neighbour). The network can be seen in Figure 18, and 19. We see that there are 4 isolated subgraphs in this topology and very few and very short cycles. 63% of the networks generated had at least one isolated subgraph. On average, the networks had 2.26 isolated subgraphs. Note, that this means the example is skewed towards a more affected network, it still demonstrates well the issues with the networks.\nWe made many networks with 1-in-degree for N (3 160) nodes and found the longest simple cycle in these networks; see Figure 20. The computing cost of running this for 1-in-degree is trivial. Therefore, we run this 10000 per N. This data shows that the trend is asymptotic and that for N = 100, the average longest simple cycle is about 8 and 10 for N = 160. For 2-in-degree networks, finding the values for 100 and 160 nodes is vastly different; the number of cycles in this graph grows quickly, and the times to compute them were exponential. Therefore, we only computed for 3 to 50 nodes. Assuming this is asymptotic in the same manner as for 1-in-degree, we can fit a line to the data and know that the true values for the longest simple cycle are below that line. This can be seen in Figure 21. From this estimate, the longest simple cycle should be below 70 for N = 100 and below 110 for N = 160. This is substantially larger than 1-in-degree, but still significantly below ECA, which the longest simple cycle will always equal the number of cells(nodes). For 3-in-degree, see Figure 22. It is still below the max, as for N = 100, it must be below 95 and for N = 160, it must be below 150. Note that this is a theoretical max, and assuming the data is asymptotic, the true value must be below this value. To be clear, even if they are not asymptotic and simply linear, we still expect this to have some effect on the computation. Also, note that we could have fitted these values to an asymptotic curve but opted not to. This is because the asymptotic degree would greatly affect where the curve ends up even with minor changes, and we do not know what it is for 2 and 3 degrees. Therefore, we opted for a theoretical max with linear fitting of the form y = mx + b."}, {"title": "4.6 Sensitivity in the x-bit memory benchmark", "content": "As was established in [30], the 5-bit memory benchmark can be solved with a simple random vector given sufficient dimensions. Therefore, it would be interesting to see if, as we compare CA, PLCA and HHRBN, as the tasks become easier, more and more rules can solve this issue and whether that would be true for HHRBN than for PLCA and PLCA compared to ECA. We expect HHRBN and PLCA to be more sensitive, and we expect them to perform better on the 3 and 4-bit memory benchmarks. We see from Figure 12, 23 and 24 that yes, more rules are capable of solving the easier tasks, but it is not entirely clear that a more significant portion of the rule-space of HHRBN or PLCA is finding the task trivial. If we view this through the average performance across the rule space, as seen in Table 18, there does seem to be a degree of this behaviour, but there are clear exceptions such as 3-bit W.avg.. The results corroborate the previous presented topology effects for the longest simple cycle and collapse rate in PLCA and HHRBN."}, {"title": "5 DISCUSSION", "content": "This section focuses on discussing the results and, how they relate to each other, and how they relate to the field in a larger context. We will also give a short account of how we would approach the problem of a more natural definition of fully discrete chaos."}, {"title": "5.1 A complicated relationship between disordered topology and its computation", "content": "We see through the collapse rate that in random topologies, the collapse rate significantly increases in PLCA and HHRBN, meaning random topology leads instead to more orderly computation. In contrast, if we look at results that don't collapse, we see a significant increase in sensitivity; we, in other words, have conflicting computational \"forces\". If we look at our results that would be affected by both \"forces\", we still see a slight trend towards disorder. However, if we consider individual rules such as rule 30, we would argue that as the collapse rate goes from 0% to 33% (PLCA) and 21% (HHRBN), this rule, on average, becomes more orderly. Therefore, this relationship is complicated."}, {"title": "5.2 Implications for RBN and RBN reservoirs", "content": "Our HHRBN has the same topology as Classical RBN, so implications beyond HHRBN can be derived. We demonstrated how different fixed in-degree networks should have a lower than max longest simple cycle. As RBN has random rules per node, the average dependency of RBN is slightly less than three. Therefore, the degree of RBN is also, in practice, less than three. This means that when applied to RBN, the results of the 3-in-degree random networks are, in practice, overestimates. It is likely already overestimated as we fit a line to a (hypothesised) asymptotic function. Therefore, the RBN's longest simple cycle in practice should be even smaller. We also see a significant collapse rate even in the most \"chaotic\" rule (rule 30). The natural assumption would be to see similar effects when applied to RBN. Therefore, we see many effects that reduce the sensitivity or disorderly nature of the computation that we would also hypothesise to apply RBN."}, {"title": "5.3 Weaknesses of this study", "content": "Though this work is quite extensive, the work has some limitations. Due to computational constraints, we limit ourselves to CA grid sizes of a specific size, and we know that CA can exhibit significant behaviour changes on different grid sizes [30]."}, {"title": "5.4 Size of the Edge of Chaos", "content": "In [54, 86] several examples where introducing more heterogeneity extended the critical area. As we introduce topological heterogeneity to our networks, it would be interesting to consider if we are observing the same phenomenon of an increasing critical range. We begin by considering Wolfram's well-known ECA classification [63, 101]; there are only 11/88 rules in the \"chaotic\" class. We might also argue that the additive rules 60, 90 and 150 should not be considered \"chaotic\", making the space even smaller. Also, consider that there are only 4 complex rules, indicating that the ECA rule space is skewed towards ordered behaviour. As we see a small indication that introducing heterogeneity moves the space towards more sensitive \"chaotic\" behaviour, we should also expect to see more rules exhibit critical behaviour. Nevertheless, the same evidence indicates the opposite of an extended criticality: a shrinking criticality. If we go by the collapse rate, we can say that the region of ordered behaviour has extended. If we control for the collapse rate, the remaining runs behave closer to sensitive \"chaotic\" behaviour; this indicates a shrunk critical range. If we go by the 5-bit memory benchmark, we again see signs of a shrunk criticality as very few runs managed to solve the benchmark. If we go by individual rules, one can argue that rules 170 and 184 behaved closer to critical, though the collapse rate is highly significant for these rules. As our experiments were not specifically designed to identify the size of the critical area, we can not definitively conclude that our findings apply universally. A thorough examination of all individual rules might yield a different conclusion. It would not be surprising if heterogeneity could extend criticality. Still, the study of complexity often reveals that the relationship between critical behaviour, substrates, and hyper-parameters is intricate and complex. This rather indicates a rich field worthy of much study."}, {"title": "5.5 Implications for ReCA, ReRBN and intermediates", "content": "We can consider some implications for reservoir computing with CA, PLCA, and HHRBN reservoirs from our results. If one considers the tool in practical terms, the ECA substrate seems superior as its regular topology lends reliability to the implementation, but also the localised neighbourhood affords easier implementation into FPGA, as the other substrates would naturally create more issues with the transfer of information due to the placement of neighbours.\nIt is common practice in RC to have redundant mappings for encoding the input. As we observe a higher collapse rate in PLCA and HHRBN, reservoir usage of these substrates will benefit more from higher input redundancy than the ECA. Alternatively, one could more carefully select encoding placement into the network as there are likely to be nodes that afford better distribution of the perturbation than others."}, {"title": "5.6 A \"discrete\" version of chaos", "content": "In this paper, we pointed out that the definition of chaos breaks down when applied to fully discrete systems, i.e., systems intrinsically discrete in space and time and with a discrete number of accessible states. The sensitivity on initial conditions required in the definition of chaotic function has a natural analogy for discrete systems, but what about the concept of dense periodic orbits and topological transitivity?\nHere, we propose a definition of the meaning of a dense set of trajectories of a discrete system, with some additional mathematical formalism. The set of accessible trajectories of a discrete system is dense in the full phase space of possible configurations if, for each configuration in phase space, the minimal Hamming distance, $\\min(D_h)$, to an accessible trajectory converges to zero not slower than the size N of the system:\n$\\lim_{N\u2192\u221e} N \\min(D_h) = 1$.\nIn practice, this definition can be read as\n$\\min(D_h) \\sim \\frac{1}{N}$,\nso graphically, plotting minimum Hamming distances as a function of the inverse of the system's size should hold a line with a unitary slope. If we consider the orbit, the natural analogy is the attractor in binary systems, A cyclic trajectory that the deterministic discrete system must eventually converge to. Therefore, we conclude that a natural analogy for a dense periodic orbit is a long attractor that periodically expands and contracts the $D_h$ between previous states without finding the exact previous state. Expanding and contracting are important because a long attractor does not necessarily mean chaos. Consider the example of a binary vector that does iterative counting upwards by 1. This would have full coverage over the state space and the longest possible attractor, but it should not be considered chaotic behaviour.\nAs for the topological transitivity, it is similarly defined in continuous space [17, subsection 1.8]: an iterative map $f: J \u2192 J$ is said to be topological transitivity if, for any pair of non-empty sets U, V C J, there exists k \u2265 0 such that $f^k(U) \u2229 V \u2260 \u00d8$. In other words, it still implies that a system can not be decomposed into two subsystems. It also means that it must be possible from a given state to transition to any other state in the system. This condition is again reflected in the attractor, where there should be only one possible attractor in the system. The presence of two separate attractors would imply that the system can be decomposed, contradicting the notion of topological transitivity."}, {"title": "6 CONCLUSION", "content": "This work investigated computational differences between ECA, PLCA, and HHRBN. It explores what happens with the simplest computational universe when introducing topological heterogeneity. We investigated using a simple 5-bit memory benchmark, sensitivity metric and collapse rate of the different substrates. We see how, in PLCA and HHRBN, performance on the 5-bit memory benchmark is substantially worse. That collapse rate increases substantially, which counterintuitively means that a more disordered topology can sometimes mean more ordered computation. In general, we see a weak sign of increased sensitivity, and if the collapse rate is controlled for, we see a strong sign of increased sensitivity. This indicates that we are observing a shrinking critical range. We see evidence consistent with the previous observations when we make the 5-bit memory benchmark easier by solving a 4 and 3-bit memory benchmark. Our results conclude that ECA is, at least with current hardware, the better reservoir for edge AI. We also try to address the issue of \"chaos\" in a fully discrete system and attempt to define a condition for the natural analogy."}, {"title": "7 FUTURE WORK", "content": "Many future work projects can naturally extend this work. As we identified in the intro (Figure 1), there are many steps between ECA and BBN, which can be explored. Additionally, there are different paths between ECA and BNN, so other orders of steps could be explored. For example, would we get the same results if we introduce other forms of heterogeneity, such as mixed-rule CA? Furthermore, our networks have a random topology beyond what is typically true in most biological systems. It would be interesting to see how the regular topology (ECA) and the irregular topology (HHRBN) perform when compared to evolved networks such as the connectome of a C. elegans. Alternatively, if we explore networks with increasing locality of connections, this might even be a suitable control parameter for reservoir quality."}]}