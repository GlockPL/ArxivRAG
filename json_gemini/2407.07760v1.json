{"title": "Learning Spatial-Semantic Features for Robust Video Object Segmentation", "authors": ["Xin Li", "Deshui Miao", "Zhenyu He", "Yaowei Wang", "Huchuan Lu", "Ming-Hsuan Yang"], "abstract": "Tracking and segmenting multiple similar objects with complex or separate parts in long-term videos is inherently challenging due to the ambiguity of target parts and identity confusion caused by occlusion, background clutter, and long-term variations. In this paper, we propose a robust video object segmentation framework equipped with spatial-semantic features and discriminative object queries to address the above issues. Specifically, we construct a spatial-semantic network comprising a semantic embedding block and spatial dependencies modeling block to associate the pretrained ViT features [18, 34] with global semantic features and local spatial features, providing a comprehensive target representation. In addition, we develop a masked cross-attention module to generate object queries that focus on the most discriminative parts of target objects during query propagation, alleviating noise accumulation and ensuring effective long-term query propagation. The experimental results show that the proposed method set a new state-of-the-art performance on multiple datasets, including DAVIS2017 test (89.1%), YoutubeVOS 2019 (88.5%), MOSE (75.1%), LVOS test (73.0%), and LVOS val (75.1%), which demonstrate the effectiveness and generalization capacity of the proposed method. We will make all source code and trained models publicly available.", "sections": [{"title": "1 Introduction", "content": "Video Object Segmentation (VOS) aims to track and segment target objects that are specified in the initial frame with mask annotations in a video sequence [47, 36, 40, 50]. This practice holds significant promise across various applications, particularly as the prevalence of video content surges in domains like autonomous driving, augmented reality, and interactive video editing [35, 2]. The main challenges faced by VOS include drastic target appearance change, occlusion, and identity confusion caused by similar objects and background clutter, which becomes more difficult, particularly in long-term videos.\nVOS methods [33, 11] typically perform video object segmentation by comparing the test frame with past frames. Specifically, they first use an association model to generate the correlated features of the test sample and target templates pixel-wise and then predict target masks based on the correlated features. The pixel-wise correlated features [44] contribute to accurate mask predictions. To account for targets that appear differently over time, some methods [50, 10] utilize a memory module to store these changing appearances. In addition, several recent methods [42, 14] introduce object queries to help distinguish different target objects for alleviating identity confusion.\nDespite the good performance on short-term test videos with simple-structured objects, existing methods do not work well in the following challenging scenarios. First, when dealing with target objects with multiple complex or separate parts caused by occlusion, background clutter, and shape complexity, existing methods often generate incomplete prediction masks. This is because the pixel-wise correlation adopted by existing methods mainly focuses on detailed spatial information at the pixel level and neglects the semantic information at the instance level. Second, although object query improves the ID association accuracy, it does not work well in sequences with dramatic target appearance changes. Existing methods [42, 14] update target queries relying on the entire predicted sample, which can introduce noise and errors and further lead to tracking failures, where targets are missed or their identities are swapped.\nTo address the aforementioned issues, we propose a VOS framework to learn spatial-semantic features and generate discriminative object queries for robust VOS. To be specific, we construct a Spatial-Semantic Block comprising a semantic embedding module and a spatial dependencies modeling module to efficiently leverage the semantic information and local details of the pre-trained ViTs for VOS without training all the parameters of the ViT backbone. In addition, we develop a discriminative query to focus more on the representative target parts for more reliable target representation and query update, which can alleviate noise accumulation in query propagation and ensure robustness in long-term videos. We evaluate the proposed method extensively on five benchmarks, including DAVIS 2017 [36], Youtube VOS2018 [47], Youtube VOS2019, LVOS [22], and MOSE [17]. The experimental results show that our approach set new state-of-the-art performance on all these datasets including DAVIS2017 test (89.1%), YoutubeVOS 2019 (88.5%), MOSE (75.1%), and LVOS test (73.0%). In addition, the results achieved by only using the DAVIS and Youtube datasets for training show that our method generalizes well to different datasets, as shown in Figure 1.\nIn this work, we make the following contributions:\n\u2022 We propose a spatial-semantic network block to incorporate semantic information with spatial information for Video Object Segmentation. We use the cls token from a pre-trained ViT backbone to extract global semantic information for interaction with current features. In addition, we design a spatial dependencies modeling module to leverage the features of the visual transformer better. The proposed spatial-semantic block enables robust VOS performance in handling the target objects with multiple complex or separate parts.\n\u2022 We develop a discriminative query mechanism to capture the representative region of the target for better target representation learning and updating. This achieves significant improvement in VOS datasets, especially in long-term VOS datasets.\n\u2022 We conduct extensive experiments on five diverse datasets to demonstrate the effectiveness of the proposed method and comprehensive ablation studies to evaluate the contribution of each proposed component. We set a new state-of-the-art performance on all these datasets."}, {"title": "2 Related Work", "content": "Video Object Segmentation. Video Object Segmentation (VOS) aims to identify and separate objects within a video sequence from the background and from each other based on one annotated target frame, which is also known as semi-supervised VOS. Early VOS methods [4, 46, 41, 23, 31] use test-time learning to adapt pre-trained segmentation models for separating the specified targets online. Test-time learning of a deep model is time-consuming and significantly reduces the run speed. To avoid online learning, propagation-based methods [19, 16, 39] model the temporal correlations between adjacent frames with offline shift attention learning to propagate target masks from the previous frame to the current. Despite the promising performance, these methods suffer from error accumulation caused by inaccurate predictions. To achieve fast and efficient inference, matching-based methods [7, 24, 3, 49] identify targets by comparing the target template to the test image and predict target masks based on matching features. For instance, OML [7] and VideoMatch [24] conduct pixel matching between the nearest frames for mask prediction. Some methods [33, 50, 51, 10] further improve the matching-based VOS framework by introducing target memory to enrich target templates and developing advanced matching strategies to generate more comprehensive correlated features. For example, SimVOS [44] and JointFormer [32] use the ViT blocks [18] to model features and patch correspondence for object modeling jointly. In addition, some recent methods [42, 14] introduce object queries to enrich target representation for better ID identification.\nDespite the rich target features and sufficient interactions, existing methods lack modeling of object-level semantic information, so they do not work well on dealing with objects with complex structures and long-term deformation. Unlike existing methods, the proposed method learns features and associates templates with test frames at both pixel and object levels achieved through a novel object-aware backbone and a pixel-and-query level correlation component, leading to robust VOS performance on complex and long-term videos.\nQuery Transformer Networks. DETR [5] utilizes queries to represent target regions for object detection tasks. In the segmentation domain, some methods [9, 6] introduce masked cross-attention to use queries to represent targets for segmentation tasks. Motivated by the above methods, ISVOS [42] utilizes additional queries generated by Mask2Former [9] to represent instance information for target modeling, enhancing target representation in matching-based VOS methods. To achieve end-to-end training, Cutie [14] learns object-level information and facilitates interaction between object and pixel information, improving segmentation accuracy and efficiency. However, the query-based VOS methods suffer from query drift due to ineffective propagation. These methods model the entire target sample in the query, incorporating redundant information and degrading saliencies of the target object. Instead, the proposed method generates discriminative queries for target representation and query propagation, which is more robust to variations and preserves discriminative information.\nAdapters for Dense Tasks. Vit-Adapter [8] first addresses the problem of utilizing pre-trained models to boost detection and segmentation performance and proposes a dense adapter to exchange information with ViT blocks. To enhance spatial hierarchical features, ViT-CoMer [45] introduces a multi-receptive field feature pyramid module to provide multi-scale spatial features. However, those two methods require full fine-tuning for the best performance, which is inefficient, especially in the video domain. Meanwhile, they focus on spatial information enhancement while lacking semantic feature interaction between ViT blocks and multi-scale features.\nTo better suit VOS tasks, our method features several design distinctions compared to existing adapters. First, we fuse the feature from ViT to multi-scale features and do not need extra modules to transmit information to ViT blocks, which is more efficient. Second, the proposed fusion block associates semantic priors and local dense features. Third, the proposed method freezes the ViT blocks during training and does not need full fine-tuning."}, {"title": "3 Proposed Algorithm", "content": "Our method aims to learn comprehensive target features that contain semantic, spatial, and discriminative information for video object segmentation. This helps us cope with complex target appearance variations and ID confusion between target objects with similar appearances in long-term videos.\nTo this end, we propose a spatial-semantic feature learning network that first embeds semantic information from a trained ViT network with multi-scale features from a trainable CNN network."}, {"title": "3.1 Overall Framework", "content": "Figure 2(a) shows the overall framework of the proposed method, consisting of two core modules: a spatial-semantic feature generation module and a pixel-and-query dual-level target association module. Given a test frame $X_t \\in \\mathbb{R}^{3\\times H\\times W}$, the proposed method predicts the mask of all specified objects within it based on the initial frame $X_{init} \\in \\mathbb{R}^{3\\times H\\times W}$ and its corresponding annotation mask $M_{init} \\in \\mathbb{R}^{H\\times W}$. First, the feature generation module takes the test frame $X_t$ as input and generates spatial-semantic features. Then, the target association module associates the spatial-semantic features with the reference samples using pixel- and query-wise correlation to generate the correlated features. Finally, a decoder predicts the mask based on these correlated features.\nTo better represent video targets that vary over frames, we develop a dual-level memory module consisting of the base features generated by the feature generation module and target queries that capture more global information. Both types of memory are updated every few frames using the online predicted masks, enabling adaptive target representations for effective target association."}, {"title": "3.2 Spatial-Semantic Feature Learning", "content": "Figure 2(b) shows the spatial-semantic network, which consists of a semantic embedding module that incorporates semantic information from a trained ViT model into multi-scale features and a spatial dependency modeling module that learns spatial dependencies based on these incorporated features.\nSemantic embedding. As the VOS task is designed for generic objects and no class labels are given, it is challenging to learn semantic representations directly from the VOS dataset during training. Fortunately, the CLS token in a trained ViT model aggregates semantic information from the entire image and provides a rich, global representation of the image content. Thus, we incorporate the CLS token with the multi-scale features generated from CNN networks to obtain detailed semantic features at different scales. To be specific, we use a cross-attention operation to perform the corporation of these two features by taking the semantic feature (CLS token $F_i^s$) concatenated with global token representation ($F_i^g$) as the Key and Value, and the spatial-temporal features ($F_{ipsem}^{i-1}$) from the (i - 1)-th block as the Query to produce the semantic-aware features $F_{sem}^i$ as:\n$F_{sem} = f_{softmax}\\left(\\frac{(W^q(F_i^s, F_i^g))(W^k F_{ipsem}^{i-1})^T}{\\sqrt{d}}\\right)(W^v F_{ipsem}^{i-1}),$ (1)\nwhere $f_{softmax}$ is the SoftMax operation, and $W^q$, $W^k$, and $W^v$ are the projection matrices for Query, Key, and Value.\nSpatial dependency modeling. We note that it is important to learn spatial dependencies between semantic features for the VOS model to understand the relationships between different object parts, which helps handle objects with complex structures or separate parts. Given the features of the input image patches generated by the ViT backbone model $F_{vit}^i$, we adopt the Multi-scale Deformable Cross-Attention operation [5] to capture the spatial information $F_{spa}^i$:\n$F_{spa}^i = f_a(\\sum_j F_{vit}^i f_{gs} (F_{sem}^i, \\Delta p)) \\cdot f_{gs} (F_{sem}^i, \\Delta p),$ (2)\nwhere $\\Delta p$ indicates the deformable offsets generated based on the query features $F_{vit}^i$, $f_{gs}$ performs grid sampling on $F_{sem}^i$ using the offsets $\\Delta p$, $f_a$ denotes the attention weights generated based on the query features $F_{vit}^i$ and the grid sampled features, and $\\sum_j$ is the summation over all sampled points.\nTo enable an effective long-range propagation through the cascade blocks, we use the residual connection [21] followed by a convolution-feed-forward to generate the final spatial-semantic feature of the current block:\n$F_{spsem}^i = F_{spa}^i + f_{ConFFN} (F_{spa}^i + F_{spsem}^{i-1}).$ (3)\nThe spatial-semantic block leverages the strengths of both a pre-trained ViT network for provid- ing semantic information and a multi-scale CNN network for rich spatial information, enabling comprehensive spatial-semantic feature learning for robust VOS."}, {"title": "3.3 Discriminative Query Propagation", "content": "We note that updating the target query memory directly with entire object patches generated based on online predicted masks is ineffective as the predicted masks often cover background noise, reducing target distinctiveness and leading to accumulating errors over time. To propagate target queries effectively across frames, we update the target queries with the most distinctive feature of the target object.\nDiscriminative feature generation. We select the discriminative feature of a target object by comparing the target query $Q \\in \\mathbb{R}^{N\\times C}$ with every channel activation in the correlated feature map of the target and taking the most similar one. Formally, given the feature map $F \\in \\mathbb{R}^{H\\times W\\times C}$, the salient channel activation feature of a target is computed as:\n$Q_s = F[ arg \\underset{i\\in(1,H*W)}{max} (F^QT \\odot M)],$ (4)\nwhere M represents a binary mask of the target, and F[i] denotes the i-th channel activation of the feature map F. We use the strategy in Cutie [14] to generate the target mask M, which uses the predicted segmentation results in the previous frame.\nDiscriminative query update. Based on the discriminative target feature $Q_s$ generated from a new target sample, we can update target queries by dynamically calculating the relationship between the salient query and salient pixel features in an additive manner, which can be formulated as:\n$Q_{out} = (\\alpha \\frac{A}{\\|A\\|^2} + Q_s)W_{out} + Q,$ (5)\nwhere \u03b1 is a learnable parameter to scale the matrix, and A denotes the correspondence between salient query and salient pixel features, which is computed as.\n$A = Q \\odot Q_s.$ (6)"}, {"title": "4 Experiments", "content": "4.1 Experimental Settings\nPretrained Backbone & Baseline. We conduct different pre-trained weights obtained using various pre-training methods, including MAE [20], DINOV2 [34], and Depth Anything [48]. To validate the effectiveness of our proposed discriminative query propagation, we first choose ResNet50 as the backbone for a baseline model. The multi-scale features from the backbone are utilized in the Decoder for fine prediction. Soft aggregation operation is used to merge the predicted masks in the multi-target scenario.\nTraining. Our training settings are similar to Cutie [14]. Note that, apart from ResNet pre-training, we do not pre-train other models on static images and directly trained them on video datasets (YoutubeVOS [47] and DAVIS [36]). To enhance the performance of our model, we utilize the MEGA dataset constructed by Cutie, which includes the YouTubeVOS [47], DAVIS [36], OVIS [37], MOSE [17], and BURST [1] datasets. We sample eight frames to train the model, and three are randomly selected to train the matching process. For each sequence, we randomly choose at most three targets for training. Point supervision in loss is adopted to reduce the memory requirements.\nFor optimization, AdamW [25] optimizer is performed with a learning rate of 5e-5 and a weight decay of 0.5. We train the model for 125K with batch size 16 on the video dataset, and 195k on the MEGA dataset. For specific parameter settings, please refer to our supplementary materials. All our models are trained on 8 x NVIDIA V100 GPUs and tested on an NVIDIA V100 GPU.\nInference. Our feature and query memory are updated every 5th frame during the testing phase. For longer sequences, we employ a long-term fusion strategy [10] for updating. To enhance storage quality, we skip frames without targets and do not store them. The test input size contains two scales: 480 for fair comparison and 600/720 for further improvement."}, {"title": "4.2 Ablation Study", "content": "Effect of different components. Table 1 provides a comprehensive analysis of the components of our method. The first two rows describe current methods upon which we build improvements. Cutie enhances performance by embedding a query transformer within the model.\n+ Discriminative Query, which performs the discriminative query generation by adaptively refining target queries with the most representative features in the query transformer. The performance gains compared to Cutie demonstrate the advantages of target modeling of discriminative query for VOS, especially in long-term benchmarks.\n+ ViT, which directly utilizing ViTDet [29] to incorporate pre-trained model features at multiple scales. The performance denotes that directly employing ViT to generate multi-scale features is minimally beneficial for VOS tasks.\n+ Spatial, which applies spatial dependencies modeling in learning multi-scale detail features. It is clear that the proposed spatial dependencies modeling significantly boosts model performance in"}, {"title": "4.3 Evaluations with State-Of-The-Art Methods", "content": "DAVIS 2017 [36] is a benchmark that offers densely annotated, high-quality, full-resolution videos with multiple objects of interest. Table 3 shows that our method achieves favorable performance (86.7%) against state-of-the-art only trained on YoutubeVOS and DAVIS. The gains are primarily due to the injection of semantic information, which helps handle sequences with object occlusion. With more training data and improved inference size, our method performs best on the DAVIS test set (89.1%).\nYouTubeVOS 2018 [47] has 3471 videos with 65 categories for training and videos for validation. In the validation, there are 26 categories that the model has not seen in its training, enabling us to evaluate its generalization ability for 350 class-agnostic targets. Table 3 illustrates that our method achieves better performance on seen and unseen categories than previous state-of-the-art models. The performance gain is attributed to spatial-semantic feature representation and representative target correlation.\nYouTubeVOS 2019 is an extension of YouTube-VOS 2018, featuring a greater number of masked targets and including more challenging sequences in its val set. In Table 3, our approach exhibits competitive performance relative to the leading state-of-the-art methods. Remarkably, our model achieves the highest performance (87.5%) without relying on pre-training. This underscores the effectiveness of our method in leveraging inherent semantic-spatial information and discriminative query representation.\nMOSE [17] contains 2149 video clips and 5200 objects from 36 categories under more complex scenes. Compared to existing state-of-the-art methods, our model gains great improvement in this dataset. The benefits primarily stem from the discriminative query capturing more representative target information. Additionally, the feature extraction module provides matching with more semantic and spatially informative features.\nLVOS [22] is a new long-term dataset to validate the robustness of VOS methods when facing these challenges. It contains videos 20 times longer than those in the existing VOS datasets, typically featuring a single target per video. In this benchmark, our method achieves a performance gain (66.5% compared to 56.2%). This improvement is attributed to the superior quality of memorized features and queries, which enhance the robustness of our approach in long-term video scenarios. Our method demonstrates enhanced stability and effectiveness in long-duration contexts by incorporating more discriminative queries that capture crucial target information into the object memory.\nResults with larger test size. Following Cutie, we notice that the performance of our method can be further improved by changing hyperparameters, such as test input scales, memory interval, memorize or not, and the size of the memory bank during inference. As is shown in Table 3, owing to the robust spatial-semantic feature learning and discriminative query propagation, our method achieves state-of-the-art performance on most benchmarks (e.g. 75.1% on MOSE [17] val set). Besides, our method is only trained with video data, without any pre-train, but still achieves better performance.\nVisualized Results. We visualize some challenging sequences, including scenarios involving part-to-whole changing and faint objects, as well as those where targets closely resemble the background. In Figure 3(a), our approach shows favorable segmentation performance against the state-of-the-art methods. For instance, our model accurately segments targets and provides detailed results in a video that includes gorillas, necks, and a riding man. This is attributed to our proposed discriminative query propagation, which enables our model to capture more detailed target information by focusing on the most representative features, thereby facilitating precise segmentation in these challenging scenarios Additionally, incorporating semantic features in our model ensures the model understands"}, {"title": "5 Limitations", "content": "Our method enhances target representation by introducing spatial-semantic features and discriminative query propagation, achieving significant improvements in numerous challenging scenarios. However, in the scenes with object parts as the target (some sequences in the VOT challenge, shown in the appendix), the proposed model finds it difficult to represent a part of the target and often extends the predicted mask to the whole target. For example, when the target is the head of a person, our model may gradually shift to segmenting the entire body. This tendency arises because our method tends to interpret the target as the entire object seen during the training phase. A potential direction to improve this issue could be letting the model learn more part-aware semantic features, which also needs a new benchmark."}, {"title": "6 Conclusion", "content": "In this paper, we propose a spatial-semantic network to learn comprehensive representations to improve the robustness of video object segmentation in handling objects with complex structures or separate parts. In addition, we develop a discriminative query propagation module that selects representative features in target objects for query update, enabling more reliable query propagation in long-term videos. Our approach achieves favorable performance compared with the state-of-the-art methods on numerous datasets. Both quantitative and qualitative results demonstrate the effectiveness of the proposed method in tracking and segmenting video objects in long-term and complex scenes."}, {"title": "A Appendix", "content": "The appendix is organized as follows:\n1. We provide a detailed implementation of training and inference.\n2. We provide more experiments with different training settings and test hyperparameters.\n3. More visualization and failure cases are shown.\nA.1 Implement Details.\nA.1.1 Multi-Object Processing\nIn this section, we provide additional implementation details for completeness. Our training and inference code will be released to ensure reproducibility.\nFollowing the previous work [10, 11], we extend our methods to the multi-object setting. The predicted mask of each frame undergoes channel separation for different targets. The separated masks for each target are then concatenated with the RGB image and the aggregated mask of other targets for feature extraction. The extracted features are fused with RGB features to obtain pixel-level value features, which are then stored in memory. It's important to note that the pixel-level features stored in memory include both the key (RGB feature) and the value.\nA.1.2 Training Details\nAs mentioned in the main paper, we train our network only on video-level datasets, which is different from other methods [14, 10, 51]. The backbone weights are initialized from MAE pretraining, similar to the approaches used in SimVOS [44] and JointFormer [32]. We conduct spatial-semantic fusion every three ViT layers and five discriminative query transformer blocks for query propagation. We implement our network using PyTorch and train the model on 8 NVIDIA V100s, employing automatic mixed precision (AMP) during training.\nTwo different training datasets are used for better improvements. The \"DAVIS&YTVOS\" settings combine the training sets of YouTubeVOS-2019 and DAVIS-2017, with the DAVIS-2017 dataset expanded five times to increase the data ratio. The \"MEGA\" set contains four different training sets, including DAVIS-2017, YouYubeVOS-2019, MOSE, and BURST datasets, with the DAVIS- 2017 dataset expanded five times to increase the data ratio. To sample a training sequence, we first randomly select a \"seed\" frame from all the frames and then randomly select 7 other frames from the same video. We re-sample if any two consecutive frames have a temporal distance greater than the max-skip. Following the previous work [10, 14], the max-skip is set to [5, 10, 15, 5] after [0%, 10%, 30%, 80%] of the training iterations, respectively. To avoid sampling frames without targets, we use the data statistics provided by Cutie [14]."}, {"title": "A.1.3 Inference Details", "content": "During the inference phase, the model memorizes the feature every 5th frame like [10, 14]. The pixel-level memory always stores the first frame and its mask. The top-K filter is used to augment the memory-reading in pixel memory. For object-level memory, we use the streaming average to accumulate the discriminative query representation like the operation in [14].\nA.2 Additional Quantitative Results\nIn this section, we provide more additional ablation studies and sota comparisons with different settings. In the main paper, we provide ablation studies for different components. To provide a broader comparison, we conduct tests under various combinations of training data and testing settings. The experimental results in Table A2 detail the impact of different components of our model on the performance, facilitating a better understanding of the effectiveness of our model.\nTo compare with more methods on MOSE, we follow the training strategy from MOSE [17], and replace the YouTubeVOS with MOSE in the training process. Table A3 shows the results of the variants of our method. All the variants achieve great improvement in terms of I&F, which further validates the effectiveness of the proposed spatial-semantic feature learning and discriminative query representation."}, {"title": "A.3 Additional Qualitative Comparison", "content": "We present a more qualitative comparison of most benchmarks in Figure A.1 and Figure A.2.\nIn Figure A.1, we categorize the scenarios into three main types: overall-to-part segmentation (man on bike and deer), small object segmentation (small monkey), blurry object segmentation (cat toy), and segmentation under varying lighting conditions (man-bike and golf). In these three cases, our method demonstrates precise segmentation compared to other methods. The semantic embedding allows our model to consider semantic information when parsing objects, spatial dependence modeling enhances the understanding of spatial positions and details of the object, and the discriminative query accurately represents the target. The integration of these three components enables the model to achieve excellent segmentation and tracking performance in complex and challenging scenarios.\nIn Figure A.2, we select more sequence results on MOSE, primarily divided into two categories: positional changes of similar targets and rapid movement of targets. In both cases, our method outperforms Cutie. This is attributed to our proposed spatial-semantic feature representation and discriminative query expression, which enable the model to distinguish appearance-similar targets and consistently track the target.\nFigure A.3 compares sequences that require a semantic understanding of the target. Without the semantic embedding module, the method suffers from segmenting parts of the target. The addition of the semantic embedding module enables the model to fully understand the semantic information of the target, allowing for precise segmentation and tracking in these scenarios.\nFailure cases are shown in Figure A.4, in which our method tends to segment the whole instance rather than the part of them. The challenging sequences need the model to segment and track the head of the man, flamingo, and kangaroo. Our model interprets semantics as the entire instance during tracking and segmentation, such as a person, bird, or kangaroo, resulting in segmentation outcomes gradually favoring the whole instance."}]}