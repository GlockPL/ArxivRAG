{"title": "Synthesizing 3D Abstractions by Inverting Procedural Buildings with Transformers", "authors": ["Maximilian Dax", "Jordi Berbel", "Jan Stria", "Leonidas Guibas", "Urs Bergmann"], "abstract": "We generate abstractions of buildings, reflecting the essential aspects of their geometry and structure, by learning to invert procedural models. We first build a dataset of abstract procedural building models paired with simulated point clouds and then learn the inverse mapping through a transformer. Given a point cloud, the trained transformer then infers the corresponding abstracted building in terms of a programmatic language description. This approach leverages expressive procedural models developed for gaming and animation, and thereby retains desirable properties such as efficient rendering of the inferred abstractions and strong priors for regularity and symmetry. Our approach achieves good reconstruction accuracy in terms of geometry and structure, as well as structurally consistent inpainting.", "sections": [{"title": "1. Introduction", "content": "Abstract visual representations aim to capture the key geometric and structural properties of an object. Abstractions are useful in many applications as they facilitate perceptual comparisons and understanding. For buildings, use cases for abstract representations range from 3D mapping for navigation to the generation of synthetic environments for training of deep learning agents. However, inferring abstractions based on sensor data at a desired level of detail is a challenging problem. Traditional approaches are largely based on geometric simplification and optimization, while learning approaches face a lack of training data.\nWe here propose a model to infer abstractions from point cloud data. While manually labeled large-scale datasets for this task are costly and thus unavailable, there is an abundance of high-quality 3D building simulators. Specifically, the gaming and animation industries have developed models to sample synthetic environments and render corresponding 3D representations at various levels of complexity-the inverse direction of what we aim to achieve (Fig. 1). Our framework inverts such 3D models with a transformer model [47] which takes point clouds as input and predicts corresponding abstractions (Sec. 2). Training is fully supervised, based on a dataset of procedural buildings paired with corresponding point cloud simulations. We develop various technical components tailored to the generation of abstractions. This includes the design of a programmatic language to efficiently represent abstractions, its combination with a technique to guarantee transformer outputs consistent with the structure imposed by this language, and an encoder-decoder architecture for the inference model.\nOur approach achieves accurate in-distribution geometric and structural reconstruction, and structurally consistent inference for incomplete inputs (Sec. 3). We find that the main limitations are attributed to constraints of the procedural model, not the inference framework itself. While this can be partially mitigated by data augmentation, our results suggest that procedural model advancements will be crucial for real-world applicability. Based on our analysis, we suggest to make procedural models more flexible to better suit inversion (Sec. 4)."}, {"title": "2. Method", "content": "We train an inference model that takes a building point cloud x as input and infers a parametric description \u03b8, an abstraction, of the corresponding building. When training the model, we aim to combine structural knowledge (e.g. a building is made of storeys/floors, style and position of windows, etc.) and geometric fit.\nWe formalize these notions in a Bayesian framework, and represent structural knowledge in the prior distribution p(\u03b8). We further represent the geometric fit in terms of the distribution over point clouds x conditional on abstractions -the likelihood p(x|\u03b8). With these definitions, the Bayesian posterior p(\u03b8|x) \u221d p(\u03b8)p(x|\u03b8) represents the distribution over abstractions \u03b8 for a given point cloud x. We solve this Bayesian inference problem by training an inference model q(\u03b8|x) to approximate the posterior p(\u03b8|x). An abstraction for a point cloud x can then be inferred by sampling from the trained model \u03b8 ~ q(\u03b8|x).\nBelow, we define p(\u03b8) in terms of a procedural building model and p(x|\u03b8) in terms of a renderer (Sec. 2.1), introduce the training objective for optimization of q(\u03b8|x) (Sec. 2.2) and describe the architecture of q(\u03b8|x) (Sec. 2.3).\nPrior. We employ a procedural building model adapted from the Unreal Engine 5 City Samples [1] demo. This model samples abstract buildings \u03b8 by placing a collection of building assets (e.g., facade segments) according to a set of rules (appendix Fig. 4), and hence serves as our implicit prior distribution over buildings p(\u03b8). Both, assets and rules, have been carefully designed to capture styles of buildings of San Francisco, Chicago and New York. In total there are 911 unique assets, and a typical building is composed of around 10 \u2013 40 different asset types. We represent abstractions in a custom data format based on Protocol Buffers [46] (Fig. 5), which we designed to facilitate representation of regularities and significantly minimize redundancy, thereby simplifying inference. It is chosen as a hierarchical format, starting from macroscopic structure (height, footprints), and then goes into more detail (storeys, facade description etc). Importantly, identical structures can refer to the same description \u2013 e.g. different storeys can refer to the same facade. For more details, see appendix A.2.\nLikelihood. The likelihood p(x|\u03b8) models the distribution over point clouds consistent with a given abstraction \u03b8. We represent the likelihood implicitly: for a given abstraction \u03b8 we sample x ~ p(x|\u03b8) by composing surface points from all assets of a building into a joint point cloud, filtering interior points, and adding Gaussian noise with mean 0 and variance \u03c3\u00b2 to each point. As log-likelihood maximization with Gaussian kernels corresponds to mean squared error minimization, this likelihood approximates a geometric distance between point cloud and inferred geometry.\nDataset. We generate a dataset with 341721 pairs of abstractions and point clouds, hence samples from the joint distribution (\u03b8, x) ~ p(\u03b8, x) (see Fig. 1a for examples). We augment the asset colors in the dataset by adding uncorrelated Gaussian noise with a standard deviation of 0.15 in the HSV color system to half of the dataset.\nWe train the network q(\u03b8|x) using simulation-based inference [6] ideas (see appendix A.1 for details) with the loss\nL = \\mathbb{E}_{x~p(x|\u03b8), \u03b8~p(\u03b8)} [-log q(\u03b8|x)]. (1)\nacross the dataset of simulated buildings from Sec. 2.1. This objective formally corresponds to minimization of the Kullback-Leibler divergence D_{KL}(p(\u03b8|x) || q(\u03b8|x)). With sufficient training data and network capacity, the inference network q(\u03b8|x) will therefore become an accurate estimator of p(\u03b8|x) [33]. After training, the inference network thus inverts the forward model (i.e., the procedural model and point cloud renderer), and an abstraction for a point cloud x can be generated by sampling \u03b8 ~ q(\u03b8|x)."}, {"title": "2.3. Model Architecture", "content": "We now define the density estimator q(\u03b8|x) which maps a point cloud x to a conditional distribution over abstractions \u03b8. We use a transformer-based encoder-decoder model.\nTokenization of Protocol Buffers. To obtain a transformer-compatible data format for the abstraction programs \u03b8 we convert these to sequences of tokens. We follow [11], which offers a method to bijectively map any Protocal Buffer to a token sequence. Importantly, at inference time, this allows to mask invalid tokens, ensuring syntactically valid Protocol Buffers. For details see appendix A.2.\nEncoder. We subdivide the point cloud into cubes of length 7 meters, resulting in n\u1d65 smaller point clouds {x\u2081, ..., x\u2099\u1d65}. Each voxel point cloud is embedded into a feature space z\u1d62 = g(x\u1d62) \u2208 \u211d\u2075\u00b9\u00b2, where g is a PointCloudTransformer [17] (see Tab. 3 for hyperparameters) and i includes point coordinates and color information. We provide the voxel positions by adding sinusoidal positional embeddings [29] to the PointCloudTransformer's output.\nDecoder. We employ a standard transformer decoder [47] (see Tab. 3 for hyperparameters). The decoder is conditioned on the embeddings z\u1d62 via cross-attention, which enables spatially global information integration.\nTraining. We train the encoder-decoder model end-to-end for 10\u2075 steps, with batch size 16, employing AdamW [24] with \u03b2\u2081 = 0.9, \u03b2\u2082 = 0.98 and a learning rate of 5 \u00b7 10\u207b\u2074 after a linear warm up for 10\u00b3 steps. We apply dropout on the voxel embeddings with rates varying between 0 and 0.8, to force the model to infer missing information based on regularity in the data. We regularize with additive Gaussian noise on the point cloud with standard deviations \u03c3 between 0 m and 0.5 m."}, {"title": "3. Results", "content": "We first evaluate our trained model on a hold-out subset of 17086 point clouds from the simulated dataset. For each point cloud, we compare the inferred abstraction to the corresponding ground truth, both in terms of structural properties and geometric fit (Fig. 2). Structural high-level variables, such has the predicted number of storeys of the buildings, the number of facades, and asset precision and recall, are inferred correctly with high accuracies of above 95%. As color/materials are augmented for some assets of a building, the intersection over union (IoU) of the predicted and the ground truth modifications is calculated (94.2%), and the Euclidean distance in HSV color space is evaluated on the intersecting assets. The deviation between inferred building geometries and the input point cloud is around 10 cm (in the absence of point cloud noise). We conclude that our model achieves excellent in-distribution reconstruction.\nWe next test properties of the model under point cloud modifications. Specifically, we drop large blocks of the point clouds (a) at random or (b) systematically; and we (c) split the point clouds and move both parts away from each other (Fig. 3). These manipulations lead to missing information, and (c) additionally could lead to out-of-distribution buildings. We find that the inference model successfully reconstructs missing information by leveraging regularity and symmetry priors from the procedural model. We further observe that inferred buildings for (c) are not simply stretched versions of the originals, but contain additional asset instances (e.g., more windows). While all buildings are visually neat, we observe occasional visual artefacts and slight inaccuracies of asset placements."}, {"title": "4. Discussion", "content": "Our framework casts abstraction into an inverse problem, with the forward direction defined by a procedural model and a point cloud renderer. The inference network is trained to invert this forward direction, thereby converting point cloud data to the procedural model space. In this space, buildings are composed of assets, which can directly serve as abstract representations: they allow for high-level editing and rendering at different levels of detail.\nLimitations. As a central limitation, our approach is constrained to the scope of the forward direction, hence is limited to buildings that are captured by the procedural model. While procedural models generate diverse buildings, they usually are not comprehensive, because for simulation of realistic environments it is not necessary to capture all buildings within the targeted distribution. Therefore, the procedural model used here is limited to only a subset of buildings. Application to real buildings further requires the point clouds to be consistent with real point cloud measurements (e.g., with photogrammetry, lidar). We designed our renderer to capture some of the main features of realistic point clouds (e.g., only rendering points from the surface, added noise), but more realistic settings require further extension (e.g., local variation of noise levels). Additionally, it may be necessary to explicitly account for domain shifts between simulated and real data within our model [5, 49].\nStrengths. Our simulation-based approach has various advantages compared to other methods. First, it enables training without human annotations with large amounts of synthetic data. Second, regularity priors implemented in the procedural model are captured by the trained inference model. Buildings are highly regular objects, and such priors enable inference of missing information. Third, our approach provides an inductive bias to prioritize structure over geometric accuracy, as it is defined on the abstract descriptions (as opposed to a geometric reconstruction loss). Indeed, we observed that inferred buildings sometimes did not optimally capture the input point cloud, but always looked structurally self-consistent. This is desirable, as structure is typically more important than rigorous geometric accuracy for abstractions. Fourth, the native representation of procedural models is optimized for extremely efficient rendering, which is inherited by our framework. Fifth, our approach enables human-in-the-loop interaction between procedural modeling and its inversion. This could help to quantitatively assess procedural models at scale, validating how well they capture real buildings and identifying improvements.\nOptimizing procedural models for inversion. Designing procedural models with inversion in mind likely improves the scope and performance of our framework. E.g., our experiments suggest a need for more flexible asset selection and placement. This could alleviate inconsistencies between asset placement and augmented point clouds (e.g., Fig. 3, split manipulation). Parametric assets, e.g. variable window sizes, would further enhance flexibility, which however also weakens the regularity prior. Such modifications to procedural models will be crucial for future applicability of our framework to real data."}]}