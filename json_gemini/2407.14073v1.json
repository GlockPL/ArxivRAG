{"title": "LoAS: Fully Temporal-Parallel Datatflow for Dual-Sparse Spiking Neural Networks", "authors": ["Ruokai Yin", "Youngeun Kim", "Di Wu", "Priyadarshini Panda"], "abstract": "Spiking Neural Networks (SNNs) have gained significant research attention in the last decade due to their potential to drive resource-constrained edge devices. Though existing SNN accelerators offer high efficiency in processing sparse spikes with dense weights, opportunities are less explored in SNNs with sparse weights, i.e., dual-sparsity. In this work, we study the acceleration of dual-sparse SNNs, focusing on their core operation, sparse-matrix-sparse-matrix multiplication (spMspM). We observe that naively running a dual-sparse SNN on existing spMspM accelerators designed for dual-sparse Artificial Neural Networks (ANNs) exhibits sub-optimal efficiency. The main challenge is that processing timesteps, a natural property of SNNs, introduces an extra loop to ANN spMspM, leading to longer latency and more memory traffic. To address the problem, we propose a fully temporal-parallel (FTP) dataflow, which minimizes both data movement across timesteps and the end-to-end latency of dual-sparse SNNs. To maximize the efficiency of FTP dataflow, we propose an FTP-friendly spike compression mechanism that efficiently compresses single-bit spikes and ensures contiguous memory access. We further propose an FTP-friendly inner-join circuit that can lower the cost of the expensive prefix-sum circuits with almost no throughput penalty. All the above techniques for FTP dataflow are encapsulated in LoAS, a Low-latency inference Accelerator for dual-sparse SNNs. With FTP dataflow, compression, and inner-join, running dual-sparse SNN workloads on LoAS demonstrates significant speedup (up to 8.51x) and energy reduction (up to 3.68\u00d7) compared to running it on prior dual-sparse accelerators.", "sections": [{"title": "I. INTRODUCTION", "content": "Spiking Neural Networks (SNNs) have attracted considerable interest as potential energy-efficient substitutes for Artificial Neural Networks (ANNs) [5], [11], [43]. Inspired by the biological neuron, SNNs leverage highly sparse unary-coded ({0,1}) spikes to compute and communicate information [54]. Thus, running SNNs on hardware significantly reduces computation and data movement, making it suitable for edge computing. Therefore, SNNs have been widely used in computer vision tasks, such as image classification [46], [56], optical flow estimation [28], semantic segmentation [21], and object detection [20].\nOpportunity. As the need for edge devices with limited memory capacity increases, recent research on SNNs highlights the significance of dual-sparse (both spikes and weights are sparse), which can be achieved by neural pruning techniques [5], [23]. Pruning the weight connections of SNNs has been explored during both training [4], [49] and inference [38]."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "A. Preliminary of SNNs\n1) Leaky-Integrate-and-Fire Neuron: The Leaky-Integrate-and-Fire (LIF) neuron is a classical neuron model [8] and widely adopted by prior SNN works [23], [24], [63], [65], thanks to its bio-plausibility and high accuracy. In this work, we focus on accelerating the workloads of dual-sparse SNNs that use LIF neurons.\nDuring inference, each layer has an input spike tensor $A \\in \\{0,1\\}^{M \\times K \\times T}$ where $U \\in \\{0,1\\}$ and a weight matrix defined as $B \\in \\mathbb{Z}^{K \\times N}$. Here T is the number of total timesteps; M, N, and K are the spatial dimensions of the input and weight matrix. The behavior of an SNN layer can be described below:\nStep 1: Sparse Matrix Multiplication Sparse matrix multiplication across all timesteps is performed to obtain the full output matrix $O \\in \\mathbb{Z}^{M \\times N \\times T}$, which will be sent to LIF neurons.\n$O_{m,n}[t_i] = \\sum_{k=0}^{K} A_{m,k}[t_i]B_{k,n},$ (1)"}, {"title": "", "content": "where the $t_i$ is the current timestep. With dual-sparsity, sparse matrix multiplication becomes spMspM.\nStep 2: LIF firing LIF neurons take the snapshot of O at timestep $t_i$ and generate a snapshot of the output spike tensor $C \\in \\{0,1\\}^{M \\times N \\times T}$ for current timestep $t_i$:\n$C_{m,n}[t_i] =\\begin{cases}1 & X_{m,n}[t_i] > \\theta_{th} \\\\ 0 & else,\\end{cases}$ (2)\nwhere $X_{m,n}[t_i] = O_{m,n}[t_i] + U_{m,n}[t_{i-1}].$\nHere, $U[t_{i-1}]$ is the membrane potential that carries over the temporal information from previous timestep $t_{i-1}$, and $\\theta_{th}$ is the firing threshold, a pre-defined scalar value.\nStep 3: Membrane Potential Update After the output spikes are generated, we update the membrane potential that will carry residual information to the next timestep according to the equation below.2\n$U_{m,n}[t_i] = \\tau X_{m,n}[t_i] (1-C_{m,n}[t_i]),$ (3)\nwhere $\\tau \\in (0, 1)$ is the leaky factor. From the above equations, we observe that to generate the output spike matrix C for timestep $t_i$, we need to know the information from the previous timestep $U[t_{i-1}]$. This brings temporal dependency between output spike matrices across timesteps. The behavior of a LIF neuron can be found in Figure 2.\n2) Spike Encoding and SNN Training: One key step in leveraging SNNs in conventional machine learning tasks is encoding the input source data (e.g., image pixels or text embeddings) into spike trains across multiple timesteps. The input spike trains are then sequentially sent to the SNN for processing. Recent SNN works adopt direct encoding (a special case of rate encoding) to achieve high accuracy on conventional computer vision tasks in very few timesteps ($ \\leq 4$) [?], [23], [25], [57], [65]. In direct encoding, the source data, instead of being directly converted into spike trains, first goes through one ANN layer. The output from the ANN layer is then converted into spike trains. We will focus on accelerating direct-coded dual-sparse SNNs in this work. The SNNs are trained using backpropagation-through-time (BPTT) [53] with surrogate gradient [37] to achieve very close performance to ANNs on many complex tasks [57], [65].\nB. Distinctive Features and Challenge of SNNs\nSeveral distinctive features make SNNs favorable for low-power edge deployment, but they also come with challenges.\nFeature 1: Unary Activation One of the most distinctive features of SNNs is their unary spike activation. More specifically, the SNNs leverage single-bit non-weighted activation to propagate information through layers. The primary benefit of the unary activation is the simplified low-power arithmetic"}, {"title": "", "content": "units that they require. As shown in Figure 2, compared to the multiply-accumulate (MAC) of ANNs, SNN only requires simple bitwise-AND and accumulate (AC) operations during inference time.3 Without the expensive multipliers [16], the computations for SNNs require extremely low power and area.\nFeature 2: Sparse Spike Activity The second feature of SNNs is their highly sparse spike-firing activity. In ANNs, upon completion, MAC results go through the ReLU unit, which filters out non-positive outputs. Different from ANNs, AC results in SNNs go through the Leaky-Integrate-and-Fire (LIF) unit, which only fires (generates an output of 1) when the input is greater than a pre-set threshold. As a result, the output sparsity in SNNs is usually much higher (~ 90%) [60], [61], [63], [65] than that of ANNs (~ 50%) [41], [45]. More sparse outputs apparently lead to more computation and memory saving under the context of spMspM acceleration.\nChallenge: Repeated Timesteps Despite the aforementioned hardware-friendly features, one main challenge of deploying SNNs on hardware is their intrinsic repeated timesteps. A timestep is the minimum unit of time in SNNs, thus discrete.4 In one timestep, each neuron needs to complete the AC operations for all inputs, fire a spike if necessary, and update its membrane potential (will be discussed shortly). The SNN needs to run across multiple timesteps to capture the temporal dynamics from the input data, as shown in Figure 2. Running multiple timesteps increases latency and fails to be energy efficient, diluting the advantage of low-power circuits unless we have a specialized architecture design [36].\nC. spMspM Dataflows in SNNs\nThere are various ways to map spMspM onto hardware, each with unique efficiency [31], [34]. Three different spM-spM dataflows have been proposed in existing dual-sparse ANN accelerators: Inner-product (IP) [15], [18], [19], [42], Outer-product (OP) [9], [39], [41], [64], and Gustavson's (Gust) [51], [62]. In Figure 3, we illustrate these three dataflows in SNNs for two input matrices A and B, and an output matrix C. We also formulate their abstract loop nests on the right-hand side. As we discussed in Section II-B, it is impossible not to consider the multiple timesteps for spMspM operations in SNNs.\nInside the black box in Figure 3, the dataflow is for one timestep, thus identical to ANN dataflow. Outside the black box, multiple input matrices A (blurred) represent the input spike matrices across different timesteps, which need to be processed. Meanwhile, multiple output spike matrices C that have temporal dependency between each other are also generated. Specifically, to accommodate the timesteps in SNNs, we need to consider one more loop dimension (t dimension) in the original triple-nested for-loop. The t dimension (annotated in the blue box) brings temporal dependency to each output"}, {"title": "", "content": "pixel in SNNs. For example, to process the SNN using IP dataflow as shown in Figure 3, we first calculate the output cell at (0,0) position for timestep 0 (C[0,0,0]), then instead of moving to the position (0,1), we move on to process the output cell at (0,0) for timestep 1 (C[0,0,1]). Since the output cell C[0,0,1] is temporal dependent on the result of the output cell C[0,0,0], we cannot process C[0,0,1] before C[0,0,0].\nD. ANN spMspM Hardware for dual-sparse SNNs\nWe review existing ANN spMspM accelerators to understand why naively running dual-sparse SNNs on these accelerators is sub-optimal.\nInner-join Design: For the IP dataflow, prior accelerators usually adopt the inner-join-based design [9], [15]. In such designs, non-zero values in rows of matrix A and columns of matrix B are compressed using bitmask representation (a bit string that has 1's for positions with non-zero values and O's otherwise). An inner-join unit scans two bitmasks on the fly to determine if there's a matched position (both multiplicands are non-zero) and then sends the matched pairs to the compute units. Running dual-sparse SNNs on an inner-join-based design does not require the extra bit-masks for the input spike matrix A (the unary spike train itself can be viewed as a bit-mask). However, as shown in Figure 4, the timesteps will impose multiple extra rounds of running the expensive inner-join units (e.g., occupying roughly 46% of the system-level power [15]), thus incurring high energy cost. Moreover, since the spike trains are used as bit-masks, all the spikes, no matter 1 or 0, are necessary to be fetched from off-chip DRAM. This brings no memory traffic saving on the sparse spike matrix \u0410.\nMerger-based Design: Unlike IP dataflow designs that exhibit full output matrix (C) reuse, OP and Gust dataflow designs focus on the reuse of input matrix A and B. In OP, each column of A and each row of B will only be transversed"}, {"title": "E. Dataflow Architecture for SNNs", "content": "SpinalFlow: Temporal Sequential Design. SpinalFlow [36] is the first SNN-tailored accelerator for extracting the efficiency from the single-bit activation and the extremely sparse spike activity. The authors identified the challenge of sequentially processing the entire SNN network through timesteps. To overcome the challenge, SpinalFlow proceeds all timesteps for one layer and then proceeds to the next layer, as shown in Figure 1. SpinalFlow dispatches LIF neurons across different processing elements (PEs) and parallelizes the computation. Within each layer, the timesteps are processed sequentially, as shown in Figure 1. Spinalflow is optimized exclusively for the temporal-coded SNNs that potentially lag in terms of accuracy performance compared to rate-coded SNNs [29]. In this work, we focus on accelerating spMspM for general rate-coded SNNs that yield competitive accuracy as ANNs in various tasks.\nPTB: Partially Temporal Parallel. While SpinalFlow's design is tailored to the temporal-coded SNNs, PTB [29] proposes a general architecture design for the rate-coded SNN. By leveraging the high data-reuse pattern across different PEs in the systolic array architecture [27], PTB breaks the processing of all timesteps into multiple time-windows (each consists of several contiguous timesteps) and run these time-windows in parallel, as shown in Figure 1. PTB parallelly maps multiple time-windows across different columns of the systolic array. The computation of different LIF neurons is also parallelized across the rows of the systolic array. We illustrate this hardware mapping strategy in Figure 6 with details. Though PTB tries to parallelize the processing of timesteps, the parallelization is on the granularity of the time-window. Inside each time-window (column of PEs), the timesteps are still processed sequentially. Consequently, we categorize PTB as a partially temporal parallel design. One unique aspect of LoAS from PTB is that LoAS places the temporal dimension in the inner-most loop, enabling all optimizations.\nPrior SNN accelerators with LIF neurons process timesteps in a sequential or partially parallel manner. In this way, as we discussed in (Section II-C & II-D), it is very challenging for those existing SNN designs to have good performance on spMspM SNN acceleration. Thus, we need a spMspM-friendly strategy to process timesteps."}, {"title": "III. FULLY TEMPORAL PARALLEL DATAFLOW", "content": "We propose a fully temporal-parallel dataflow (FTP) that targets reducing the negative effects of repeatedly processing the timesteps on spMspM accelerators (Section II-D). The proposed FTP is formulated in Algorithm 1.\nAn SNN-friendly spMspM dataflow should satisfy three goals: (1) avoid as much data refetch as possible across the timesteps; (2) generate as few partial sums as possible on the temporal dimension (timesteps); (3) reduce the latency as much as possible on the temporal dimension to reduce the extra cost of sparsity handling units.\nOur first observation is that for all three spMspM dataflows (Section II-C), unless placing the temporal dimension (t-dim) at the innermost loop, it will bring at least T times more data refetch to the dimensions below, compared to the original dataflow. For example, in OP, if t-dim is placed between m and n, T times more access to B's rows is required. If t-dim is placed between k and m, T times more access to A's columns and B's rows is required. Depending on the on-chip buffer capacity, repeated memory access might lead to"}, {"title": "IV. LOAS", "content": "An overview of LoAS is shown in Figure 7. LoAS consists of multiple temporal parallel processing elements (TPPEs) and parallel Leaky-Integrate-Fire units (P-LIFs) that are tailored to run the FTP dataflow; a scheduler that distributes workloads across TPPEs; and a compressor that compresses the output\nspikes from P-LIFs and writes them back to the on-chip memory. An on-chip SRAM is equipped to capture data reuse.\nA. Spikes Compression\nWe first discuss how sparse input spikes (matrix A) across timesteps are compressed in LoAS. Efficiently compressing matrix A in SNNs necessitates solving two challenges:\nHow to maximize the compression ratio of 1-bit spikes? Assume that the input spike matrix A has a size of 128\u00d7128 for each timestep. Then for either CSR or CSC, we need to use two 7-bit coordinates to compress each 1-bit non-zero spike.5 Furthermore, SNNs naturally run for multiple timesteps, which means that for the same coordinate, different spike values may occur at different timesteps (e.g., 0 for T=1&3, and 1 for T=2&4). To faithfully capture all the non-zero spikes, we need separate coordinate values for each timestep.\nHow to maintain contiguous memory access of non-zero spikes across timesteps? The FTP dataflow we proposed in Section III requires spatial unrolling of the input spike matrix A across all timesteps beneath the k dimension. Consequently, a dis-contiguous memory layout of A along the t dimension will cause fragmented memory access at all levels of memory hierarchies, leading to higher data movement costs.\nTo better illustrate these two points, we provide an example in Figure 8. Envisioning that the input spikes sent to the system have the pre-synaptic neuron a0,0 (first element of row-0 in matrix A) firing a spike at t0 and t2. As shown in step 1, to represent this pre-synaptic neuron behavior, a single-bit 1 needs to be stored at row-0, column-0 of matrix A for both timestep 0 and 2 into the memory, shown in the box of 'unpacked real data.' Then, for each non-zero spike in row-0 of matrix A for each timestep, if we need to use a coordinate value (e.g., 4-bit for CSR) to record its position. We then need 2 \u00d7 4 = 8 bits to compress 2 bits (2 spikes). The compression efficiency in this case is only 25%. Furthermore, memory access to spikes across different timesteps is discontinuous (sequentially access different rows of A). We propose the following spikes compression format for LoAS to solve these"}, {"title": "", "content": "two challenges. In our method, as shown in step 2, we pack all the spikes (both 0 and 1) across all timesteps into one continuous data block in the system for each pre-synaptic neuron. In the example of Figure 8, we store a 4-bit value 1010 at the first position of row-0 of matrix A for a0,0 and 0111 at the fourth position for a0,3. Since neurons a0,1 and a0,2 do not spike at any timestep, their packed value would be 0000 (shown in the box of 'packed real data'). We define these neurons as silent neurons. With this strategy, only the non-silent neurons will be treated as non-zero values and stored in the memory for matrix A, as shown in step 3. In our example, we end up using 4 bits to compress 5 bits. The compression efficiency in this case is 125%.\nTo accommodate our FTP dataflow, we compress the input spike matrix A in a row-wise manner and use the bitmask format [9], [15], [42] to represent the coordinates of the non-zero values. The bitmask format uses a 1-bit coordinate value for each position in the row. In our example, the bitmask is 1001 since the first and the fourth elements in the row are non-zero. The second and third elements are silent neurons, so we do not store them in the memory (represented by a 0 in bitmask). Following the bitmask, a pointer is stored to provide the starting location of the non-zero values of the row. We call this compressed row: a fiber [34], [62].\nThe key to our compression method is the ratio of silent neurons in the SNN. Fortunately, empirical studies have shown that SNNs have a significant fraction of silent neurons (60%~70%, as shown in Table II). We further use a similar bitmask-based technique to compress weights in a column-wise manner. Each compressed weight column is also called a fiber.\nB. Temporal Parallel Processing Elements\nThe fundamental building blocks of LoAS's compute engine are Temporal Parallel Processing Elements (TPPEs) and Parallel Leaky-Integrate-Fire units (P-LIFs), which we describe next. Figure 7 also details the design of TPPE. Each TPPE produces the full sum for one output neuron across all timesteps (Line 5 in Algorithm. 1). Before the computation starts, the bitmask (bm-B) of a fiber from weight matrix B (fiber-B) and its non-zero data are read from SRAM and broadcasted into the small bitmask buffers (128 bits in our design) inside each TPPE. The bitmask (bm-A) of fiber from input spike matrix A (fiber-A) is also fetched and sent to the TPPEs. Each TPPE will hold the bitmask for a distinct fiber along the row of A. After the data are loaded, an inner-join operation [9], [15], [18] is performed between the two bitmasks. Depending upon the inner-join result, the matched non-zero data of fiber-A will be fetched from the global cache and sent to the pseudo-accumulator (soon be discussed) to perform the accumulation (AC) operation. After the TPPE completes the full computation of one output neuron, it will send the result to the P-LIF unit to generate output spikes for all timesteps in one shot."}, {"title": "C. Inner-join Unit", "content": "The inner-join operation has been extensively studied by prior works [9], [15], [18] for spMspM acceleration in ANNs. The inner-join mechanism with prefix-sum circuit has been efficiently implemented with the bitmask representation [15]. In [15], a logical-AND operation is first applied to two bitmasks to get the AND-result, which represents the location where both data are nonzero. The AND-result is then sent to a priority encoder to convert the matched positions into integer values. The matched positios are sent to two separate prefix-sum circuits to get the number of 1s in front of the matched position for each bitmask. This gets the offsets for each non-zero data in the memory.\nDuring the above process, the use of two fast prefix-sum circuits is an expensive operation (taking more than 45% power and area in [15]).7 To reduce the overhead brought by the prefix-sum circuits, we propose an FTP-friendly inner-join unit that is detailed in Figure 9.\nWe first observe that in ANNs, the MAC operation requires both inputs to be explicitly known at computation time. Therefore, we need two fast prefix-sum circuits to match the processing speed between two inputs. However, this is not the case with SNNs. In SNNs, we only have two cases for the input (1 or 0), meaning we either accumulate or discard the weight. This provides the opportunity to have an imbalanced processing speed for two inputs at the prefix-sum stage.\nIn our design, instead of using two fast prefix-sum circuits as in ANNs, we have one fast and one laggy prefix-sum circuit, as shown in Figure 9. Recall that our compression method only fetches the non-silent neurons (that fire at least once across timesteps) from DRAM for A. Thus, as soon as we find a matched position in AND-result, we are confident that the corresponding non-zero value in fiber-B will be accumulated at least once (at least one timestep). Therefore, we can begin accumulating the non-zero value in fiber-B without knowing the exact spike information from fiber-A. In this way, we can ensure the throughput of consuming fiber-B is always high regardless of the processing speed of fiber-A.\nIn our efficient inner-join unit, each time the fast prefix-sum circuit generates an offset, the corresponding non-zero value of fiber-B will be directly sent to a pseudo-accumulator for accumulation. This mechanism opportunistically presumes"}, {"title": "D. Other Units", "content": "After the computation of the pseudo-accumulator completes, its accumulation results are duplicated and sent to each correction accumulator. The correction value inside each accumulator will be subtracted from the pseudo accumulation results for each timestep. Finally, we send the corrected results to the P-LIF units to generate the output spikes. As shown inside the purple box in Figure 7, we spatially unroll the LIF operations so that the output spikes for all timesteps will be generated at once.\nLoAS uses a unified global buffer for holding compressed fiber-A and fiber-B with their bitmask representations. We adopt a FiberCache design [62]. A unified shared cache exhibits better utilization compared to separate ones. Each line in the global cache consists of two parts. The first part is the bitmask representation of a fiber, followed by a pointer. The second part is the non-zero values of that fiber. If the line manages to hold all the non-zero values, the pointer will be a NULL pointer. Otherwise, it will point to the location of the line where the rest of the data are held. Each PE will take responsibility for generating one output neuron. Therefore, we use a highly banked global cache to ensure multiple PEs can access their data concurrently. Inside each bank, we fetch as many chunks as possible for one fiber in matrix A and hold them as long as possible to maximally have the data reuse of A. This can be achieved by adopting a replacement policy for the global cache as in [31], [62]. Only one compressed row fiber of matrix B is fetched into the global cache and broadcasted to all TPPES. We follow a compression unit as [15], where an inverted prefix-sum circuit is used to compress the output spikes and generate their bitmask representations. Similar to the observation in [15], this compression step does need to be performed fast. Therefore, we equip an inverted laggy prefix-sum circuit to perform the compression. The scheduler will be responsible for casting the data to each TPPE through a simple swizzle-switch-based crossbar [47]."}, {"title": "V. EXPERIMENTAL METHODOLOGY", "content": "Software Configuration: For the dual-sparse SNNs, we train and compress the AlexNet [26], VGG16 [50], and ResNet19 [17]. We use the open-source toolchains for lottery-ticket-hypothesis (LTH)-based SNN pruning [13], [22]. We set the default timesteps T to 4 across all experiments. We use 15 rounds of LTH searching, and all SNNs are trained towards convergence with similar accuracy as state-of-the-art dense baselines [22]. We further select representative layers from each network to provide single-layer insights. The summary of the workloads is in Table II. We further use a simple yet effective preprocessing technique: zeroing out all presynaptic neurons that have a low firing activity to further improve the number of silent neurons. We take the trained SNN and mask the neurons with only one output spike throughout all timesteps. We find that with a very small number of fine-tuning (<5 epochs), the accuracy can be fully recovered, as shown in"}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "A. Hardware Evaluation\nOverall Performances: Figure 12 compares the performance between three dual-sparse SNN accelerator baselines (SparTen-SNN, GOSPA-SNN, and Gamma-SNN) and LoAS (with and without fine-tuned preprocessing) on three SNNs (speedup w.r.t the cycle numbers of the SparTen-SNN).\nThe first observation is that LoAS significantly outperforms the other three accelerator baselines in all cases, obtaining average speed-ups of 6.79\u00d7 (vs. SparTen-SNN), 5.99\u00d7 (vs. GoSPA-SNN), and 3.25\u00d7 (vs. Gamma-SNN). This is due to LoAS leverages FTP dataflow. The FTP dataflow completely unleashes LoAS from the intra-PE latency penalty of sequentially running the timesteps. It also enables LoAS to invoke less on-chip and off-chip data communications across timesteps. The second observation is that LoAS's performance gain is highly correlated with the sparsity of matrix A. This relationship is expected since our workloads are extremely sparse on matrix B; thus, the overall computation is matrix-A-bounded. Consequentially, the performance of two baselines suffers more from sequentially running timesteps through matrix A with less sparsity. However, LoAS will not get this sequentially running penalty. As a result, LoAS achieves from 4.08x speedup (vs. SparTen-SNN) on VGG16 (highest matrix A sparsity) to 8.51\u00d7 speedup (vs. SparTen-SNN) on ResNet19 (lowest matrix A sparsity). Finally, we observe that with the help of pre-processing (removing the neurons that only spike one time), LoAS further improves the performance by 20% on average. This is because the pre-processing technique helps to increase the density of silent neurons (Section IV-A), which"}, {"title": "", "content": "Detailed Analysis: We next explain the performance gains of LoAS. Owing to the FTP dataflow, LoAS has much less on-chip and off-chip memory traffic than the two baselines. As shown in Figure 13, compared to SparTen-SNN (IP), LoAS has 3.93\u00d7(3.70\u00d7), 3.57\u00d7(2.22\u00d7), and 4.07\u00d7(2.24\u00d7) less on-chip SRAM (off-chip DRAM) access on Alexnet, VGG16, and ResNet19, respectively. This behavior is expected since IP dataflow design like SparTen is known for having poor input data reuse. This inefficient input data reuse pattern is"}, {"title": "VII. RELATED WORK", "content": "Except for the prior SNN dense accelerator works we discussed in Section II-E, there also exists prior works that try to leverage the sparsity in SNNs. In [3], a neuron filter unit is leveraged to only fetch the weight if there is a 1-spike. However, dual-sparsity (both spike and weight sparsity) is not considered. In [2], the dual-sparsity of SNN is considered to skip the unmatched computation. However, the weights and spikes are fetched in a dense format without any compression from the off-chip memory, thus failing to save data movement costs. In this work, LoAS leverages the dual-sparsity in SNNS from both computation and data movement.\nAs we discussed, PTB processes the timesteps in a partially parallel manner. Even if one re-configures the PTB to run all timesteps in parallel (time-window=1), it still differs from LoAS in the loop ordering. In PTB's loop ordering, t-dim is placed between m-dim and n-dim, while LoAS places the t-dim in the inner-most loop. As discussed in Section III, LoAS's loop ordering brings more efficiency in spMspM operation. Moreover, PTB targets accelerating workloads with time-series data from DVS sensors [30], where the timestep is usually large (> 100). On our workloads, where the timesteps are small (< 8), PTB experiences low hardware utilization.\nIn [32], processing timesteps in parallel is also studied. However, they target the temporal-coded SNN workloads, and the loop ordering is not discussed. Finally, as discussed in Section II-E, Stellar [33] is another work that also tries to process timesteps in parallel. However, it targets the non-LIF, FS-coded SNNs and does not support the dual-sparsity."}, {"title": "VIII. CONCLUSION", "content": "In this work, we observe that naively running dual-sparse SNNs on existing spMspM accelerators exhibits sub-optimal efficiency due to the latency and memory traffic penalty brought by processing timesteps. To improve the efficiency, we propose a fully temporal-parallel dataflow (FTP), which avoids the above problems. To maximize the benefits of FTP, we propose FTP-friendly spike compression and inner-join mechanism. We also build LoAS, a novel architecture that exemplifies the FTP dataflow. With the help of both FTP-friendly compression and inner-join, LoAS demonstrates significant speedup (up to 8.51\u00d7) and energy reduction (up to 3.68x) compared to prior dual-sparse accelerator baselines."}]}