{"title": "LoAS: Fully Temporal-Parallel Datatflow for Dual-Sparse Spiking Neural Networks", "authors": ["Ruokai Yin", "Youngeun Kim", "Di Wu", "Priyadarshini Panda"], "abstract": "Spiking Neural Networks (SNNs) have gained significant research attention in the last decade due to their potential to drive resource-constrained edge devices. Though existing SNN accelerators offer high efficiency in processing sparse spikes with dense weights, opportunities are less explored in SNNs with sparse weights, i.e., dual-sparsity. In this work, we study the acceleration of dual-sparse SNNs, focusing on their core operation, sparse-matrix-sparse-matrix multiplication (spMspM). We observe that naively running a dual-sparse SNN on existing spMspM accelerators designed for dual-sparse Artificial Neural Networks (ANNs) exhibits sub-optimal efficiency. The main challenge is that processing timesteps, a natural property of SNNs, introduces an extra loop to ANN spMspM, leading to longer latency and more memory traffic. To address the problem, we propose a fully temporal-parallel (FTP) dataflow, which minimizes both data movement across timesteps and the end-to-end latency of dual-sparse SNNs. To maximize the efficiency of FTP dataflow, we propose an FTP-friendly spike compression mechanism that efficiently compresses single-bit spikes and ensures contiguous memory access. We further propose an FTP-friendly inner-join circuit that can lower the cost of the expensive prefix-sum circuits with almost no throughput penalty. All the above techniques for FTP dataflow are encapsulated in LoAS, a Low-latency inference Accelerator for dual-sparse SNNs. With FTP dataflow, compression, and inner-join, running dual-sparse SNN workloads on LoAS demonstrates significant speedup (up to 8.51x) and energy reduction (up to 3.68\u00d7) compared to running it on prior dual-sparse accelerators.", "sections": [{"title": "I. INTRODUCTION", "content": "Spiking Neural Networks (SNNs) have attracted consid- erable interest as potential energy-efficient substitutes for Artificial Neural Networks (ANNs) [5], [11], [43]. Inspired by the biological neuron, SNNs leverage highly sparse unary- coded ({0,1}) spikes to compute and communicate infor- mation [54]. Thus, running SNNs on hardware significantly reduces computation and data movement, making it suitable for edge computing. Therefore, SNNs have been widely used in computer vision tasks, such as image classification [46], [56], optical flow estimation [28], semantic segmentation [21], and object detection [20].\nOpportunity. As the need for edge devices with limited memory capacity increases, recent research on SNNs high- lights the significance of dual-sparse (both spikes and weights are sparse), which can be achieved by neural pruning tech- niques [5], [23]. Pruning the weight connections of SNNs has been explored during both training [4], [49] and inference [38]."}, {"title": "II. BACKGROUND AND MOTIVATION", "content": "A. Preliminary of SNNs\n1) Leaky-Integrate-and-Fire Neuron: The Leaky-Integrate- and-Fire (LIF) neuron is a classical neuron model [8] and widely adopted by prior SNN works [23], [24], [63], [65], thanks to its bio-plausibility and high accuracy. In this work, we focus on accelerating the workloads of dual-sparse SNNs that use LIF neurons.\nDuring inference, each layer has an input spike tensor $A \\in [UM\\times K\\times T_where U \\in \\{0,1\\}$ and a weight matrix defined as $B\\in ZK\\times N$. Here T is the number of total timesteps; M, N, and K are the spatial dimensions of the input and weight matrix. The behavior of an SNN layer can be described below:\nStep 1: Sparse Matrix Multiplication Sparse matrix multi- plication across all timesteps is performed to obtain the full output matrix $O \\in ZMXN\\times T$, which will be sent to LIF neurons.\n$O_{m,n}[t_i] = \\sum_{k=0}^{K} A_{m,k}[i]B_{k,n},$ (1)\nwhere the $t_i$ is the current timestep. With dual-sparsity, sparse matrix multiplication becomes spMspM.\nStep 2: LIF firing LIF neurons take the snapshot of O at timestep $t_i$ and generate a snapshot of the output spike tensor $C\\in [UM\\times N\\times T$ for current timestep t\u2081:\n$C_{m,n}[t_i] =\\begin{cases}\n1 & X_{m,n}[t_i] > U_{th} \\\\\n0 & else,\\end{cases}$ (2)\nwhere\n$X_{m,n}[t_i] = O_{m,n}[t_i] + U_{m,n}[t_{i-1}].$\nHere, $U[t_{i-1}]$ is the membrane potential that carries over the temporal information from previous timestep $t_{i-1}$, and $U_{th}$ is the firing threshold, a pre-defined scalar value.\nStep 3: Membrane Potential Update After the output spikes are generated, we update the membrane potential that will carry residual information to the next timestep according to the equation below.\n$U_{m,n}[t_i] = \\tau X_{m,n}[t_i] (1-C_{m,n}[t_i]),$ (3)\nwhere $\\tau\\in (0, 1)$ is the leaky factor. From the above equations, we observe that to generate the output spike matrix C for timestep t\u017c, we need to know the information from the previous timestep $U[t_{i-1}]$. This brings temporal dependency between output spike matrices across timesteps. The behavior of a LIF neuron can be found in Figure 2.\n2) Spike Encoding and SNN Training: One key step in leveraging SNNs in conventional machine learning tasks is encoding the input source data (e.g., image pixels or text embeddings) into spike trains across multiple timesteps. The input spike trains are then sequentially sent to the SNN for processing. Recent SNN works adopt direct encoding (a special case of rate encoding) to achieve high accuracy on conventional computer vision tasks in very few timesteps (\u22644) [?], [23], [25], [57], [65]. In direct encoding, the source data, instead of being directly converted into spike trains, first goes through one ANN layer. The output from the ANN layer is then converted into spike trains. We will focus on accelerating direct-coded dual-sparse SNNs in this work. The SNNs are trained using backpropagation-through-time (BPTT) [53] with surrogate gradient [37] to achieve very close performance to ANNs on many complex tasks [57], [65].\nB. Distinctive Features and Challenge of SNNs\nSeveral distinctive features make SNNs favorable for low- power edge deployment, but they also come with challenges.\nFeature 1: Unary Activation One of the most distinctive features of SNNs is their unary spike activation. More specif- ically, the SNNs leverage single-bit non-weighted activation to propagate information through layers. The primary benefit of the unary activation is the simplified low-power arithmetic\nFeature 2: Sparse Spike Activity The second feature of SNNs is their highly sparse spike-firing activity. In ANNs, upon completion, MAC results go through the ReLU unit, which filters out non-positive outputs. Different from ANNs, AC results in SNNs go through the Leaky-Integrate-and-Fire (LIF) unit, which only fires (generates an output of 1) when the input is greater than a pre-set threshold. As a result, the output sparsity in SNNs is usually much higher (~ 90%) [60], [61], [63], [65] than that of ANNs (~ 50%) [41], [45]. More sparse outputs apparently lead to more computation and memory saving under the context of spMspM acceleration.\nChallenge: Repeated Timesteps Despite the aforementioned hardware-friendly features, one main challenge of deploying SNNs on hardware is their intrinsic repeated timesteps. A timestep is the minimum unit of time in SNNs, thus discrete.\nIn one timestep, each neuron needs to complete the AC operations for all inputs, fire a spike if necessary, and update its membrane potential (will be discussed shortly). The SNN needs to run across multiple timesteps to capture the temporal dynamics from the input data, as shown in Figure 2. Running multiple timesteps increases latency and fails to be energy efficient, diluting the advantage of low-power circuits unless we have a specialized architecture design [36].\nC. spMspM Dataflows in SNNs\nThere are various ways to map spMspM onto hardware, each with unique efficiency [31], [34]. Three different spM- spM dataflows have been proposed in existing dual-sparse ANN accelerators: Inner-product (IP) [15], [18], [19], [42], Outer-product (OP) [9], [39], [41], [64], and Gustavson's (Gust) [51], [62]. In Figure 3, we illustrate these three dataflows in SNNs for two input matrices A and B, and an output matrix C. We also formulate their abstract loop nests on the right-hand side. As we discussed in Section II-B, it is impossible not to consider the multiple timesteps for spMspM operations in SNNs.\nInside the black box in Figure 3, the dataflow is for one timestep, thus identical to ANN dataflow. Outside the black box, multiple input matrices A (blurred) represent the input spike matrices across different timesteps, which need to be processed. Meanwhile, multiple output spike matrices C that have temporal dependency between each other are also gener- ated. Specifically, to accommodate the timesteps in SNNs, we need to consider one more loop dimension (t dimension) in the original triple-nested for-loop. The t dimension (annotated in the blue box) brings temporal dependency to each output"}, {"title": "III. FULLY TEMPORAL PARALLEL DATAFLOW", "content": "We propose a fully temporal-parallel dataflow (FTP) that targets reducing the negative effects of repeatedly processing the timesteps on spMspM accelerators (Section II-D). The proposed FTP is formulated in Algorithm 1.\nAn SNN-friendly spMspM dataflow should satisfy three goals: (1) avoid as much data refetch as possible across the timesteps; (2) generate as few partial sums as possible on the temporal dimension (timesteps); (3) reduce the latency as much as possible on the temporal dimension to reduce the extra cost of sparsity handling units.\nOur first observation is that for all three spMspM dataflows (Section II-C), unless placing the temporal dimension (t-dim) at the innermost loop, it will bring at least T times more data refetch to the dimensions below, compared to the original dataflow. For example, in OP, if t-dim is placed between m and n, T times more access to B's rows is required. If t- dim is placed between k and m, T times more access to A's columns and B's rows is required. Depending on the on- chip buffer capacity, repeated memory access might lead to"}, {"title": "IV. LOAS", "content": "An overview of LoAS is shown in Figure 7. LoAS consists of multiple temporal parallel processing elements (TPPEs) and parallel Leaky-Integrate-Fire units (P-LIFs) that are tailored to run the FTP dataflow; a scheduler that distributes workloads across TPPEs; and a compressor that compresses the output\nA. Spikes Compression\nWe first discuss how sparse input spikes (matrix A) across timesteps are compressed in LoAS. Efficiently compressing matrix A in SNNs necessitates solving two challenges:\nHow to maximize the compression ratio of 1-bit spikes? Assume that the input spike matrix A has a size of 128\u00d7128 for each timestep. Then for either CSR or CSC, we need to use two 7-bit coordinates to compress each 1-bit non-zero spike.\nFurthermore, SNNs naturally run for multiple timesteps, which means that for the same coordinate, different spike values may occur at different timesteps (e.g., 0 for T=1&3, and 1 for T=2&4). To faithfully capture all the non-zero spikes, we need separate coordinate values for each timestep.\nHow to maintain contiguous memory access of non-zero spikes across timesteps? The FTP dataflow we proposed in Section III requires spatial unrolling of the input spike matrix A across all timesteps beneath the k dimension. Consequently, a dis-contiguous memory layout of A along the t dimension will cause fragmented memory access at all levels of memory hierarchies, leading to higher data movement costs.\nB. Temporal Parallel Processing Elements\nThe fundamental building blocks of LoAS's compute en- gine are Temporal Parallel Processing Elements (TPPEs) and Parallel Leaky-Integrate-Fire units (P-LIFs), which we de- scribe next. Figure 7 also details the design of TPPE. Each TPPE produces the full sum for one output neuron across all timesteps (Line 5 in Algorithm. 1). Before the computation starts, the bitmask (bm-B) of a fiber from weight matrix B (fiber-B) and its non-zero data are read from SRAM and broadcasted into the small bitmask buffers (128 bits in our design) inside each TPPE. The bitmask (bm-A) of fiber from input spike matrix A (fiber-A) is also fetched and sent to the TPPEs. Each TPPE will hold the bitmask for a distinct fiber along the row of A. After the data are loaded, an inner-join operation [9], [15], [18] is performed between the two bitmasks. Depending upon the inner-join result, the matched non-zero data of fiber-A will be fetched from the global cache and sent to the pseudo-accumulator (soon be discussed) to perform the accumulation (AC) operation. After the TPPE completes the full computation of one output neuron, it will send the result to the P-LIF unit to generate output spikes for all timesteps in one shot.\nC. Inner-join Unit\nThe inner-join operation has been extensively studied by prior works [9], [15], [18] for spMspM acceleration in ANNs. The inner-join mechanism with prefix-sum circuit has been efficiently implemented with the bitmask representation [15]. In [15], a logical-AND operation is first applied to two bitmasks to get the AND-result, which represents the location where both data are nonzero. The AND-result is then sent to a priority encoder to convert the matched positions into integer values. The matched positios are sent to two separate prefix- sum circuits to get the number of 1s in front of the matched position for each bitmask. This gets the offsets for each non- zero data in the memory.\nD. Other Units\nAfter the computation of the pseudo-accumulator com- pletes, its accumulation results are duplicated and sent to each correction accumulator. The correction value inside each accumulator will be subtracted from the pseudo accumulation results for each timestep. Finally, we send the corrected results to the P-LIF units to generate the output spikes. As shown inside the purple box in Figure 7, we spatially unroll the LIF operations so that the output spikes for all timesteps will be generated at once."}, {"title": "V. EXPERIMENTAL METHODOLOGY", "content": "Software Configuration: For the dual-sparse SNNs, we train and compress the AlexNet [26], VGG16 [50], and ResNet19 [17]. We use the open-source toolchains for lottery- ticket-hypothesis (LTH)-based SNN pruning [13], [22]. We set the default timesteps T to 4 across all experiments. We use 15 rounds of LTH searching, and all SNNs are trained towards convergence with similar accuracy as state-of-the-art dense baselines [22]. We further select representative layers from each network to provide single-layer insights. The summary of the workloads is in Table II. We further use a simple yet effective preprocessing technique: zeroing out all presynaptic neurons that have a low firing activity to further improve the number of silent neurons. We take the trained SNN and mask the neurons with only one output spike throughout all timesteps. We find that with a very small number of fine-tuning (<5 epochs), the accuracy can be fully recovered, as shown in"}, {"title": "VI. EXPERIMENTAL RESULTS", "content": "A. Hardware Evaluation\nOverall Performances: Figure 12 compares the perfor- mance between three dual-sparse SNN accelerator baselines (SparTen-SNN, GOSPA-SNN, and Gamma-SNN) and LoAS (with and without fine-tuned preprocessing) on three SNNs (speedup w.r.t the cycle numbers of the SparTen-SNN).\nThe first observation is that LoAS significantly outperforms the other three accelerator baselines in all cases, obtaining average speed-ups of 6.79\u00d7 (vs. SparTen-SNN), 5.99\u00d7 (vs. GoSPA-SNN), and 3.25\u00d7 (vs. Gamma-SNN). This is due to LoAS leverages FTP dataflow. The FTP dataflow com- pletely unleashes LoAS from the intra-PE latency penalty of sequentially running the timesteps. It also enables LoAS to invoke less on-chip and off-chip data communications across timesteps. The second observation is that LoAS's performance gain is highly correlated with the sparsity of matrix A. This relationship is expected since our workloads are extremely sparse on matrix B; thus, the overall computation is matrix-A- bounded. Consequentially, the performance of two baselines suffers more from sequentially running timesteps through matrix A with less sparsity. However, LoAS will not get this sequentially running penalty. As a result, LoAS achieves from 4.08x speedup (vs. SparTen-SNN) on VGG16 (highest matrix A sparsity) to 8.51\u00d7 speedup (vs. SparTen-SNN) on ResNet19 (lowest matrix A sparsity). Finally, we observe that with the help of pre-processing (removing the neurons that only spike one time), LoAS further improves the performance by 20% on average. This is because the pre-processing technique helps to increase the density of silent neurons (Section IV-A), which"}, {"title": "VII. RELATED WORK", "content": "Except for the prior SNN dense accelerator works we discussed in Section II-E, there also exists prior works that try to leverage the sparsity in SNNs. In [3], a neuron filter unit is leveraged to only fetch the weight if there is a 1-spike. However, dual-sparsity (both spike and weight sparsity) is not considered. In [2], the dual-sparsity of SNN is considered to skip the unmatched computation. However, the weights and spikes are fetched in a dense format without any compression from the off-chip memory, thus failing to save data movement costs. In this work, LoAS leverages the dual-sparsity in SNNS from both computation and data movement.\nAs we discussed, PTB processes the timesteps in a partially parallel manner. Even if one re-configures the PTB to run all timesteps in parallel (time-window=1), it still differs from LoAS in the loop ordering. In PTB's loop ordering, t-dim is placed between m-dim and n-dim, while LoAS places the t-dim in the inner-most loop. As discussed in Section III, LoAS's loop ordering brings more efficiency in spMspM operation. Moreover, PTB targets accelerating workloads with time-series data from DVS sensors [30], where the timestep is usually large (> 100). On our workloads, where the timesteps are small (< 8), PTB experiences low hardware utilization.\nIn [32], processing timesteps in parallel is also studied. However, they target the temporal-coded SNN workloads, and the loop ordering is not discussed. Finally, as discussed in Section II-E, Stellar [33] is another work that also tries to process timesteps in parallel. However, it targets the non-LIF, FS-coded SNNs and does not support the dual-sparsity."}, {"title": "VIII. CONCLUSION", "content": "In this work, we observe that naively running dual-sparse SNNs on existing spMspM accelerators exhibits sub-optimal efficiency due to the latency and memory traffic penalty brought by processing timesteps. To improve the efficiency, we propose a fully temporal-parallel dataflow (FTP), which avoids the above problems. To maximize the benefits of FTP, we propose FTP-friendly spike compression and inner- join mechanism. We also build LoAS, a novel architecture that exemplifies the FTP dataflow. With the help of both FTP-friendly compression and inner-join, LoAS demonstrates significant speedup (up to 8.51\u00d7) and energy reduction (up to 3.68x) compared to prior dual-sparse accelerator baselines."}]}