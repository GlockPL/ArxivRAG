{"title": "ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024", "authors": ["Ruibo Fu", "Rui Liu", "Chunyu Qiang", "Yingming Gao", "Yi Lu", "Tao Wang", "Ya Li", "Zhengqi Wen", "Chen Zhang", "Hui Bu", "Yukun Liu", "Shuchen Shi", "Xin Qi", "Guanjun Li"], "abstract": "The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024) is part of the ISCSLP 2024 Competitions and Challenges track [1]. While current text-to-speech (TTS) technology can generate high-quality audio, its ability to convey complex emotions and controlled detail content remains limited. This constraint leads to a discrepancy between the generated audio and human subjective perception in practical applications like companion robots for children and marketing bots. The core issue lies in the inconsistency between high-quality audio generation and the ultimate human subjective experience. Therefore, this challenge aims to enhance the persuasiveness and acceptability of synthesized audio, focusing on human alignment convincing and inspirational audio generation.\nIndex Terms: Text-to-Speech (TTS), Audio Generation, ISCSLP 2024, ICAGC 2024, Convincing Audio", "sections": [{"title": "1. Introduction", "content": "The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024) is a pioneering event in the realm of synthetic audio, set to be a highlight within the ISCSLP 2024 Competitions and Challenges track. Despite the significant advancements in text-to-speech (TTS) technology, current systems often fall short in conveying intricate emotions and controlled, detailed content, which is crucial for applications such as companion robots for children and marketing bots. This gap leads to a noticeable discrepancy between the generated audio and human subjective perception, particularly in real-world applications.\nPrevious audio synthesis competitions have laid the groundwork for advancing TTS technology, pushing the boundaries of naturalness and intelligibility. The LIMMITS 24 Challenge [2], for instance, provided participants with 80 hours of TTS data across multiple languages, allowing for extensive voice cloning and multilingual synthesis experiments. Similarly, the Blizzard Challenge 2023 [3] continued its tradition of evaluating corpus-based speech synthesis systems, focusing on tasks such as building synthetic voices from provided speech data and performing extensive listening tests to evaluate their performance.\nICAGC 2024 aims to bridge this gap by challenging participants to enhance the persuasiveness and acceptability of synthesized audio. The focus is on generating audio that is not only high in quality but also capable of inspiring and resonating with listeners on a deeper emotional level. The challenge is divided into two tracks: Inspirational Emotion Modulation and Convincing Background Audio and Speech Fusion Generation. Track 1, Inspirational Emotion Modulation, requires participants to clone the voice of a target speaker and modulate it"}, {"title": "2. Challenge Task", "content": null}, {"title": "2.1. Track 1: Inspirational Emotion Modulation", "content": "Objective: Utilize given audio from a target speaker to\nclone their voice and modulate it to express proper con-vincing emotions for different theme (including novel\nchapters, ancient Chinese poems, etc.), which can inspire\nand resonate with the listener.\nMaterials Provided: The organizers will provide partici-pants with ten target speakers, each with a single theme\nsentence.\nTask: Competitors are required to clone the voice of\nthe target speakers and modulate the cloned voice to ex-press the several specified themes. For the testing set, ten\nparagraph-level texts will be provided. Participants must\ngenerate each text in the test set with the cloned voice,\nmodulating proper inspirational emotion for each theme,\nand submit the corresponding audio files."}, {"title": "2.2. Track 2: Convincing Background Audio and Speech\nFusion Generation", "content": "Objective: On the basis of the synthesized voices from\nTrack 1, use background audio generation technology\n(TTA) to add realistic background sounds to make the\naudio more lifelike and convincing to real-life scenarios.\nTask: Teams are tasked with submitting audio for each\ntext in the test set, adding background sounds they be-"}, {"title": "3. Reference Audio Information", "content": "The provided dataset includes audio files and their corresponding transcriptions. Each audio file follows the naming convention \"itemID-speakerID-sentenceID.wav\", where \"itemID\" denotes the index of a complete literary work or a segment of it, \"speakerID\" indicates the specific speaker who produced the audio, and \"sentenceID\" represents the sentence index within the selected literary work.\nThe dataset covers a range of literary genres such as ancient poems, modern poems, novels, storytelling, and fairy tales. Some speakers have only one item, while others may have multiple items spanning various literary genres. To clone a specific speaker's timbre, all of their audio samples can be utilized. It is important to note that the speaking styles can vary significantly across genres, even for the same speaker. Thus, the combination of \"itemID-speakerID\" effectively determines the speaker's timbre and speaking style in an audio file.\nThe transcription file, \"reference.csv\", contains pairs of audio files and their corresponding text content, listed line by line. During the testing phase, another file, \u201ctest.csv\", will be provided. Similar to \"reference.csv\", \"test.csv\" uses \"itemID\" to represent a complete literary work or part of it. However, in this context, an identical \"itemID\" appearing in both \"reference.csv\" and \"test.csv\" refers to a speaking style rather than the same literary work. The synthesis process involves generating text content based on the speaker index and the logical identifier \"itemID\".\""}, {"title": "4. Evaluation Metrics", "content": "Evaluating audio generation systems is a complex task that requires considering various aspects. The following are the detailed definitions and evaluation methods for the proposed metrics in the Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024):"}, {"title": "4.1. Speaker Similarity", "content": "Speaker similarity measures how closely the synthesized audio resembles the characteristics of a target speaker's speech, where the goal is to generate audio that sounds like the target speaker's timbre.\nSubjective: Native speakers listen to pairs of synthesized\nand target speaker utterances and score their speaker\nsimilarity on a rating scale.\nObjective: Extract speaker embeddings from the synthe-sized and target speaker's speech, and compute distance\nmetrics between them. Techniques like Resemblyzer[4]\nand WavLM[5] will serve as objective assessment tools\nfor evaluating speaker similarity."}, {"title": "4.2. Emotion Inspiration Degree", "content": "This metric evaluates the ability of the synthesized speech to effectively convey the intended emotions in a way that resonates with and inspires the listener on a cognitive level. The goal is to generate synthesized speech that not only expresses the desired emotion accurately, but also has the power to stir corresponding feelings and thoughts within the listener, leaving them inspired, moved, or motivated.\nSubjective: Native speakers evaluate the inspirational\nimpact and cognitive effect of the synthesized speech\nsamples conveying different emotions. They will rate the\ndegree to which the audio instilled the intended emotion\nwithin them and influenced their mindset or outlook."}, {"title": "4.3. Audio and Speech Convincing Matching Degree", "content": "Audio enhancement techniques, such as generating sound effects, background sounds, and vocal fillers, aim to improve the overall quality and convincingness of the synthesized speech. This metric assesses the effectiveness of these techniques in enhancing the subjective experience of the listener.\nSubjective: Native speakers evaluate the generated non-speech audio elements, such as sound effects, background sounds, and vocal fillers, in the synthesized speech samples. They will evaluate how convincingly these audio elements blend with and complement the synthesized speech, contributing to an authentic, realistic experience as if occurring in a true-to-life setting or scenario."}, {"title": "4.4. Overall Evaluation", "content": "The overall evaluation combines the individual metrics to provide a comprehensive assessment of the system's performance, considering trade-offs and interactions between different aspects, such as speaker similarity, emotion inspiration, and audio-speech convincing matching.\nSubjective: Native speakers provide holistic judgments\non the overall quality of the synthesized speech.\nObjective: Combine scores from individual metrics with\nweights of 0.3 for speaker similarity, 0.3 for emotion in-spiration, and 0.4 for audio-speech convincing matching\nto obtain an overall system score."}, {"title": "5. Challenge Rules", "content": null}, {"title": "5.1. Training Dataset", "content": "You are allowed to use external data in any way you\nwish. Only voice cloning reference speaker audio data\nwill be provided by organizers.\nEach participant MUST provide detailed explanations\nregarding the data sources and scale used, as well as\nother relevant details."}, {"title": "5.2. Submitted Synthetic Speech Samples", "content": "Synthetic speech may be submitted at any standard sam-pling rate (but always at 16 bits per sample). Waveforms\nwill not be downsampled for the listening test.\nAny examples that you submit for evaluation will be re-tained by the organisers for future use.\nYou must include in your submission of the test sen-tences a statement of whether you give the organisers\npermission to publically distribute your waveforms and\nthe corresponding listening test results in anonymised\nform. (We strongly encourage you to agree to this rule.)"}, {"title": "5.3. Paper", "content": "Each participant will be expected to submit a five-page\npaper (using the ISCSLP 2024 template) describing their\nentry for review."}, {"title": "5.4. Listening Test", "content": "Participants who wish to submit multiple systems (e.g.,\nan individual entry and a joint system) should contact\nthe organisers in advance to agree with this. We will\ntry to accommodate all reasonable requests, provided the\nlistening test remains manageable."}, {"title": "5.5. Notice", "content": "This is a challenge, which is designed to answer scien-tific questions, and not a competition. Therefore, we rely\non your honesty in preparing your entry.\nIf you are in any doubt about how to apply these rules,\nplease contact the organizers immediately."}, {"title": "6. Registration", "content": "If you are registering as a team, it is sufficient for one team member to fill the registration Google form, they will be con-tacted for all communications.\nhttps://forms.gle/vgpWTZMK8noacC3GA"}, {"title": "7. Challenge Submission", "content": null}, {"title": "7.1. File Naming and Submission Format", "content": "1. File Naming Convention:\nThe naming format for an audio file is \"type(s/p)-itemID-speakerID-sentenceID.wav\".\ntype: Indicates whether the file is a sentence (s) or\nparagraph (p).\nitemID: Index of a complete literary work or a part\nof it.\nspeakerID: Index of a specific speaker who pro-duced that audio file.\nsentenceID: Sentence index of the selected liter-ary work.\n2. Submission Requirements\nThe text to be synthesized is included in the attach-ment. Please ensure that you use this text for your\naudio synthesis.\nAudio files need to be synthesized separately for\nsentences and paragraphs.\nThe generated audio style must be consistent with\nthe given reference audio. Specifically, the style\nof the generated audio should match the reference\naudio with the same itemID and speakerID.\nEach file should be synthesized according to the\nprovided text and named accordingly using the for-mat \"type-itemID-speakerID-sentenceID\" (e.g., s-1-1-1.wav/p-1-1-1.wav)."}, {"title": "7.2. Submission Procedure", "content": "Here are the steps that every participating team has to follow.\n1. Each team should complete the registration-form. The\nfollowing information is mandatory:\npreferred team name the organisers may adjust\nthis so that all teams have meaningful, unique\nnames\naffiliation - the name of your University and lab,\nor your Company\nthe name of the main contact person this must\nbe exactly one person who is responsible for all\ncommunication with the organiser\ncontact details: main contact person's email ad-dress, postal address, phone number\n2. After receiving the confirmation email from the orga-nizer, teams would receive the sample data by the given\ndate. It should be noted that the provided data should\nbe used only for this challenge. The data cannot be dis-tributed to the web without the permission of the orga-nizers.\n3. During the testing phase, we will send an email to the\nparticipating teams with 10 texts to be synthesized (in-cluding novel chapters, ancient Chinese poems, etc.).\nEach participating team is asked to generate the corre-sponding audio files using their best models.\n4. Each team has to compress those 10 generated audio files\ninto a zip file name after the team, and then send it to the\nofficial email: icagc2024@iscslp2024.com"}, {"title": "8. Challenge Ranks and Rewards", "content": "Ranks: Track 1 and Track 2 are ranked separately.\nCertificate: Top 5 ranked teams from each Track will\nreceive certificates.\nSouvenir: Top 3 ranked teams from each Track will gain\nexquisite gifts.\nPaper: Taking into account the challenge ranks and sub-mitted papers comprehensively, 5 teams will be invited\nto present their work at ISCSLP 2024.\nInternship or referral opportunities"}, {"title": "9. Important Dates", "content": "May 15, 2024: Registration for the challenge opens\nMay 25, 2024: Release voice cloning reference speaker\ndata\nJune 20, 2024: Release text test data\nJune 30, 2024: Submission deadline\nJuly 10, 2024: Announcement of winners in all tracks"}]}