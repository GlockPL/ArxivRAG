{"title": "ICAGC 2024: Inspirational and Convincing Audio Generation Challenge 2024", "authors": ["Ruibo Fu", "Rui Liu", "Chunyu Qiang", "Yingming Gao", "Yi Lu", "Tao Wang", "Ya Li", "Zhengqi Wen", "Chen Zhang", "Hui Bu", "Yukun Liu", "Shuchen Shi", "Xin Qi", "Guanjun Li"], "abstract": "The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024) is part of the ISCSLP 2024 Competitions and Challenges track [1]. While current text-to-speech (TTS) technology can generate high-quality audio, its ability to convey complex emotions and controlled detail content remains limited. This constraint leads to a discrepancy between the generated audio and human subjective perception in practical applications like companion robots for children and marketing bots. The core issue lies in the inconsistency between high-quality audio generation and the ultimate human subjective experience. Therefore, this challenge aims to enhance the persuasiveness and acceptability of synthesized audio, focusing on human alignment convincing and inspirational audio generation. Index Terms: Text-to-Speech (TTS), Audio Generation, ISCSLP 2024, ICAGC 2024, Convincing Audio", "sections": [{"title": "1. Introduction", "content": "The Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024) is a pioneering event in the realm of synthetic audio, set to be a highlight within the ISCSLP 2024 Competitions and Challenges track. Despite the significant advancements in text-to-speech (TTS) technology, current systems often fall short in conveying intricate emotions and controlled, detailed content, which is crucial for applications such as companion robots for children and marketing bots. This gap leads to a noticeable discrepancy between the generated audio and human subjective perception, particularly in real-world applications.\nPrevious audio synthesis competitions have laid the groundwork for advancing TTS technology, pushing the boundaries of naturalness and intelligibility. The LIMMITS 24 Challenge [2], for instance, provided participants with 80 hours of TTS data across multiple languages, allowing for extensive voice cloning and multilingual synthesis experiments. Similarly, the Blizzard Challenge 2023 [3] continued its tradition of evaluating corpus-based speech synthesis systems, focusing on tasks such as building synthetic voices from provided speech data and performing extensive listening tests to evaluate their performance.\nICAGC 2024 aims to bridge this gap by challenging participants to enhance the persuasiveness and acceptability of synthesized audio. The focus is on generating audio that is not only high in quality but also capable of inspiring and resonating with listeners on a deeper emotional level. The challenge is divided into two tracks: Inspirational Emotion Modulation and Convincing Background Audio and Speech Fusion Generation. Track 1, Inspirational Emotion Modulation, requires participants to clone the voice of a target speaker and modulate it"}, {"title": "2. Challenge Task", "content": ""}, {"title": "2.1. Track 1: Inspirational Emotion Modulation", "content": "Objective: Utilize given audio from a target speaker to\nclone their voice and modulate it to express proper convincing emotions for different theme (including novel chapters, ancient Chinese poems, etc.), which can inspire and resonate with the listener.\nMaterials Provided: The organizers will provide participants with ten target speakers, each with a single theme sentence.\nTask: Competitors are required to clone the voice of the target speakers and modulate the cloned voice to express the several specified themes. For the testing set, ten paragraph-level texts will be provided. Participants must generate each text in the test set with the cloned voice, modulating proper inspirational emotion for each theme, and submit the corresponding audio files."}, {"title": "2.2. Track 2: Convincing Background Audio and Speech Fusion Generation", "content": "Objective: On the basis of the synthesized voices from\nTrack 1, use background audio generation technology\n(TTA) to add realistic background sounds to make the audio more lifelike and convincing to real-life scenarios.\nTask: Teams are tasked with submitting audio for each text in the test set, adding background sounds they be-"}, {"title": "3. Reference Audio Information", "content": "The provided dataset includes audio files and their corresponding transcriptions. Each audio file follows the naming convention \u201citemID-speakerID-sentenceID.wav\u201d, where \u201citemID\u201d denotes the index of a complete literary work or a segment of it, \u201cspeakerID\u201d indicates the specific speaker who produced the audio, and \u201csentenceID\u201d represents the sentence index within the selected literary work.\nThe dataset covers a range of literary genres such as ancient poems, modern poems, novels, storytelling, and fairy tales. Some speakers have only one item, while others may have multiple items spanning various literary genres. To clone a specific speaker's timbre, all of their audio samples can be utilized. It is important to note that the speaking styles can vary significantly across genres, even for the same speaker. Thus, the combination of \u201citemID-speakerID\u201d effectively determines the speaker's timbre and speaking style in an audio file.\nThe transcription file, \u201creference.csv\u201d, contains pairs of audio files and their corresponding text content, listed line by line. During the testing phase, another file, \u201ctest.csv\u201d, will be provided. Similar to \"reference.csv\", \"test.csv\" uses \u201citemID\u201d to represent a complete literary work or part of it. However, in this context, an identical \u201citemID\u201d appearing in both \"reference.csv\" and \"test.csv\" refers to a speaking style rather than the same literary work. The synthesis process involves generating text content based on the speaker index and the logical identifier \u201citemID\u201d."}, {"title": "4. Evaluation Metrics", "content": "Evaluating audio generation systems is a complex task that requires considering various aspects. The following are the detailed definitions and evaluation methods for the proposed metrics in the Inspirational and Convincing Audio Generation Challenge 2024 (ICAGC 2024):"}, {"title": "4.1. Speaker Similarity", "content": "Speaker similarity measures how closely the synthesized audio resembles the characteristics of a target speaker's speech, where the goal is to generate audio that sounds like the target speaker's timbre.\nSubjective: Native speakers listen to pairs of synthesized and target speaker utterances and score their speaker similarity on a rating scale.\nObjective: Extract speaker embeddings from the synthesized and target speaker's speech, and compute distance metrics between them. Techniques like Resemblyzer[4] and WavLM[5] will serve as objective assessment tools for evaluating speaker similarity."}, {"title": "4.2. Emotion Inspiration Degree", "content": "This metric evaluates the ability of the synthesized speech to effectively convey the intended emotions in a way that resonates with and inspires the listener on a cognitive level. The goal is to generate synthesized speech that not only expresses the desired emotion accurately, but also has the power to stir corresponding feelings and thoughts within the listener, leaving them inspired, moved, or motivated."}, {"title": "4.3. Audio and Speech Convincing Matching Degree", "content": "Audio enhancement techniques, such as generating sound effects, background sounds, and vocal fillers, aim to improve the overall quality and convincingness of the synthesized speech. This metric assesses the effectiveness of these techniques in enhancing the subjective experience of the listener.\nSubjective: Native speakers evaluate the generated non-speech audio elements, such as sound effects, background sounds, and vocal fillers, in the synthesized speech samples. They will evaluate how convincingly these audio elements blend with and complement the synthesized speech, contributing to an authentic, realistic experience as if occurring in a true-to-life setting or scenario."}, {"title": "4.4. Overall Evaluation", "content": "The overall evaluation combines the individual metrics to provide a comprehensive assessment of the system's performance, considering trade-offs and interactions between different aspects, such as speaker similarity, emotion inspiration, and audio-speech convincing matching.\nSubjective: Native speakers provide holistic judgments on the overall quality of the synthesized speech.\nObjective: Combine scores from individual metrics with weights of 0.3 for speaker similarity, 0.3 for emotion inspiration, and 0.4 for audio-speech convincing matching to obtain an overall system score."}, {"title": "5. Challenge Rules", "content": ""}, {"title": "5.1. Training Dataset", "content": "You are allowed to use external data in any way you wish. Only voice cloning reference speaker audio data will be provided by organizers.\nEach participant MUST provide detailed explanations regarding the data sources and scale used, as well as other relevant details."}, {"title": "5.2. Submitted Synthetic Speech Samples", "content": "Synthetic speech may be submitted at any standard sampling rate (but always at 16 bits per sample). Waveforms will not be downsampled for the listening test.\nAny examples that you submit for evaluation will be retained by the organisers for future use.\nYou must include in your submission of the test sentences a statement of whether you give the organisers permission to publically distribute your waveforms and the corresponding listening test results in anonymised form. (We strongly encourage you to agree to this rule.)"}, {"title": "5.3. Paper", "content": "Each participant will be expected to submit a five-page paper (using the ISCSLP 2024 template) describing their entry for review."}, {"title": "5.4. Listening Test", "content": "Participants who wish to submit multiple systems (e.g.,\nan individual entry and a joint system) should contact the organisers in advance to agree with this. We will try to accommodate all reasonable requests, provided the listening test remains manageable."}, {"title": "5.5. Notice", "content": "This is a challenge, which is designed to answer scientific questions, and not a competition. Therefore, we rely on your honesty in preparing your entry.\nIf you are in any doubt about how to apply these rules,\nplease contact the organizers immediately."}, {"title": "6. Registration", "content": "If you are registering as a team, it is sufficient for one team member to fill the registration Google form, they will be contacted for all communications."}, {"title": "7. Challenge Submission", "content": ""}, {"title": "7.1. File Naming and Submission Format", "content": "1. File Naming Convention:\nThe naming format for an audio file is \"type(s/p)-itemID-speakerID-sentenceID.wav\".\ntype: Indicates whether the file is a sentence (s) or paragraph (p).\nitemID: Index of a complete literary work or a part of it.\nspeakerID: Index of a specific speaker who produced that audio file.\nsentenceID: Sentence index of the selected literary work.\n2. Submission Requirements\nThe text to be synthesized is included in the attachment. Please ensure that you use this text for your audio synthesis.\nAudio files need to be synthesized separately for sentences and paragraphs.\nThe generated audio style must be consistent with the given reference audio. Specifically, the style of the generated audio should match the reference audio with the same itemID and speakerID.\nEach file should be synthesized according to the provided text and named accordingly using the format \"type-itemID-speakerID-sentenceID\" (e.g., s-1-1-1.wav/p-1-1-1.wav)."}, {"title": "7.2. Submission Procedure", "content": "Here are the steps that every participating team has to follow.\n1. Each team should complete the registration-form. The\nfollowing information is mandatory:\npreferred team name the organisers may adjust\nthis so that all teams have meaningful, unique\nnames\naffiliation - the name of your University and lab,\nor your Company\nthe name of the main contact person this must\nbe exactly one person who is responsible for all\ncommunication with the organiser\ncontact details: main contact person's email address, postal address, phone number\n2. After receiving the confirmation email from the orga-\nnizer, teams would receive the sample data by the given\ndate. It should be noted that the provided data should\nbe used only for this challenge. The data cannot be dis-\ntributed to the web without the permission of the orga-\nnizers.\n3. During the testing phase, we will send an email to the\nparticipating teams with 10 texts to be synthesized (in-\ncluding novel chapters, ancient Chinese poems, etc.).\nEach participating team is asked to generate the corre-\nsponding audio files using their best models.\n4. Each team has to compress those 10 generated audio files\ninto a zip file name after the team, and then send it to the\nofficial email: icagc2024@iscslp2024.com"}, {"title": "8. Challenge Ranks and Rewards", "content": "Ranks: Track 1 and Track 2 are ranked separately.\nCertificate: Top 5 ranked teams from each Track will receive certificates.\nSouvenir: Top 3 ranked teams from each Track will gain exquisite gifts.\nPaper: Taking into account the challenge ranks and submitted papers comprehensively, 5 teams will be invited to present their work at ISCSLP 2024.\nInternship or referral opportunities"}, {"title": "9. Important Dates", "content": "May 15, 2024: Registration for the challenge opens\nMay 25, 2024: Release voice cloning reference speaker data\nJune 20, 2024: Release text test data\nJune 30, 2024: Submission deadline\nJuly 10, 2024: Announcement of winners in all tracks"}]}