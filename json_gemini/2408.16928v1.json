{"title": "ACE-2005-PT: Corpus for Event Extraction in Portuguese", "authors": ["Lu\u00eds Filipe Cunha", "Ricardo Campos", "Purifica\u00e7\u00e3o Silvano", "Al\u00edpio Jorge"], "abstract": "Event extraction is an NLP task that commonly involves identifying the central word (trigger) for an event and its associated arguments in text. ACE-2005 is widely recognised as the standard corpus in this field. While other corpora, like PropBank, primarily focus on annotating predicate-argument structure, ACE-2005 provides comprehensive information about the overall event structure and semantics. However, its limited language coverage restricts its usability. This paper introduces ACE-2005-PT, a corpus created by translating ACE-2005 into Portuguese, with European and Brazilian variants. To speed up the process of obtaining ACE-2005-PT, we rely on automatic translators. This, however, poses some challenges related to automatically identifying the correct alignments between multi-word annotations in the original text and in the corresponding translated sentence. To achieve this, we developed an alignment pipeline that incorporates several alignment techniques: lemmatization, fuzzy matching, synonym matching, multiple translations and a BERT-based word aligner. To measure the alignment effectiveness, a subset of annotations from the ACE-2005-PT corpus was manually aligned by a linguist expert. This subset was then compared against our pipeline results which achieved exact and relaxed match scores of 70.55% and 87.55% respectively. As a result, we successfully generated a Portuguese version of the ACE-2005 corpus, which has been accepted for publication by LDC.", "sections": [{"title": "1 INTRODUCTION", "content": "Event extraction is a crucial Information Extraction task, that has witnessed a growing interest among researchers in recent years [14]. However, this task can pose significant challenges due to its inherent complexity and the diverse ways in which events can be expressed in natural language [13]. Despite these challenges, several works [2, 16\u201318] have demonstrated significant progress in this field, achieving promising results.\nOne of the most widely used corpora in event extraction research is the ACE-2005 (Automatic Content Extraction) corpus [9], which has become a gold standard in the field [7]. This corpus provides a comprehensive collection of annotated events, offering a valuable benchmark for evaluating the effectiveness of event extraction systems. Unfortunately, it is only available in English, Chinese and Arabic, but not in other languages, such as Portuguese.\nIn order to bridge this gap and promote event extraction for the Portuguese language, we introduce ACE-2005-PT (Automatic Content Extraction for Portuguese), an automated translation of the ACE-2005 corpus that has been partially verified by a linguist expert. To generate this corpus, we developed a translation and annotation alignment pipeline that combines machine translation and several text alignment techniques such as fuzzy string matching, lemmatization, synonym matching, multiple translations and a word aligner based on Transformers [21]. By leveraging this pipeline, we have successfully translated the ACE-2005 corpus into Portuguese, including two variants: European Portuguese and Brazilian Portuguese. Then, a subset of ACE-2005-PT annotations"}, {"title": "2 ACE-2005 CORPUS", "content": "The ACE-2005 corpus is composed of several textual documents from various sources, such as newswires, online journals, broadcast transcripts, discussion forums and conversational telephone speech. Each document is provided with annotations of events, which consist of event triggers and their corresponding event arguments [1]. Event triggers represent the terms that indicate the occurrence of an event and play a crucial role in event extraction. Each event trigger is associated with a specific event type resulting in 33 event types in the ACE-2005 corpus. Event arguments describe entities, temporal expressions or values that serve as participants or attributes of the events. Each event argument is associated with a semantic role that represents its relationship within the event, such as the agent that performs an action and the time or location of the event. For each event occurrence, ACE-2005 annotates event arguments by linking them to their respective event trigger.\nConsider the following sentence, which illustrates the process of extracting an event trigger and its corresponding arguments.\nMarie Curie was born in Warsaw on November 7, 1867.\nIn this example, the word \"born\" corresponds to a trigger with type Life: Be-Born. Regarding the event arguments, we have \"Marie Curie\", \"Warsaw\" and \"November 7, 1867\" with roles Person, Place and Time respectively."}, {"title": "3 ACE-2005 TRANSLATION TO PORTUGUESE", "content": "In this section, we introduce the translation process and annotation alignment pipeline used to generate the ACE-2005-PT corpus and present statistics associated with this methodology. Following that, we introduce the methods used to evaluate our pipeline effectiveness and the obtained results."}, {"title": "3.1 Translation and Annotation Alignment", "content": "The initial step in the process of producing the ACE-2005 corpus consisted of performing automatic translation of the documents. Specifically, we used Google Translator\u00b2 for Brazilian Portuguese, while for European Portuguese, we relied on DeepL translator 3, as\nGoogle Translator does not support translation into this language variant.\nAfter translation, we have an original text with annotated terms and we want to transfer these annotations to the translated text, in a process called alignment (of annotations). In the simplest case, we translate the annotated term and its translation becomes annotated in the translated text. However, such a direct approach works only half of the time. For instance, in the sentence \"The soldiers were ordered to fire their weapons\", ACE-2005 states that the trigger \"fire\" should be annotated. However, this sentence is translated to \"Os soldados receberam ordens para disparar as suas armas\" where the word \"fire\" is translated to \"inc\u00eandio\" (fire as a noun) in isolation and to \"disparar\" (fire as a verb) in context.\nWe employed a commonly used pre-processing approach\u00b9 of the ACE-2005 corpus, which performed sentence tokenization on the documents and assigned each event annotation to its respective sentence. Subsequently, we automatically translated each source (src) sentence ssrc, along with its corresponding triggers tsrc and arguments asrc. In case a mismatch occurs, these translations (trans) result in annotations atrans and ttrans that are not contained in the sentences Strans. For these cases, we applied an alignment procedure (Algorithm 1) to identify the correct span offsets of atrans and ttrans within strans. For the sake of simplicity, we only describe the event argument alignment process as the trigger alignment procedure was similar. The alignment pipeline is composed of four components: lemmatization, multiple translations, a BERT-based word aligner and fuzzy string similarity."}, {"title": "3.2 Statistics by pipeline component", "content": "Using the alignment pipeline, we produced ACE-2005-PT, featuring 16,260 sentences and 14,886 annotations (triggers and arguments), mirroring the original corpus. In this section, we focus, for the sake of simplicity, on the European corpus, as the results were similar in the Brazilian variant. Upon translation, we were able to find 2,721 triggers (51.9%) and 5,127 arguments (53.1%) in their respective translated sentences by using substring matching between the annotations and the translated text. This indicates that approximately half of the ACE-2005-PT annotations required alignment.\nTable 1 provides the number of alignments performed by each component of our alignment pipeline for each data split. Notably, the pipeline works sequentially, meaning that annotations aligned by earlier methods are not addressed again by subsequent pipeline elements. To select the best component order in the pipeline we experimented with all the permutations between the components and calculated the corresponding alignment results using a manually aligned corpus that is further introduced in Section 3.3. The"}, {"title": "3.3 Evaluation", "content": "To measure the effectiveness of the alignment pipeline, manual alignments were conducted on the entire ACE-2005-PT test set, which includes 1,310 annotations (triggers and arguments). These alignments were performed by an expert linguist to ensure high-quality annotations, following the same annotation guidelines of the original ACE-2005 corpus [1]. These annotations were then compared with the automatic alignment performed by our pipeline. During this comparison, we used two different metrics: the exact match between the pipeline-generated alignments and the manual ones (strict), and the F1 score between the tokens found by the pipeline and the ones that were manually aligned (relaxed). Resorting to both evaluation metrics allow us to more accurately assess the model's effectiveness."}, {"title": "3.4 Results", "content": "Table 2 shows the alignment validation results of our pipeline when compared with the manually annotated test split of ACE-2005-PT.\nIn this table, we can analyse the relaxed and exact match between the pipeline-generated alignments and the gold annotations performed by the linguistic expert. The results are presented for each alignment method, along with the overall performance of the pipeline. The pipeline achieved a Relaxed score of 87.72% and an exact score of 70.55%. As expected, the flexible match obtained a higher result value than the exact match. Looking at the overall performance, there is a 17.22% difference between both metrics, which shows that, despite our pipeline getting most of the alignments in the correct form, it still presents difficulties in reproducing the exact alignments performed by an expert.\nLooking at the annotation types (triggers and arguments), our pipeline demonstrates more difficulties when aligning arguments (85.16% relaxed and 60.76% exact) compared to triggers (93.11% relaxed and 91.23% exact). This is expected since a trigger is usually composed of one word, while arguments may contain several words, making it harder to identify the correct alignment. Another observation to be made is that in the case of trigger alignment, the results obtained for both relaxed and exact scores are similar (93.11% relaxed and 91.23% exact). On the other hand, one can observe, in the argument alignments, a higher difference between the relaxed and the exact score, with a score of 85.15% and 60.76% respectively. Again, this indicates that aligning arguments is a harder task for our pipeline.\nFinally, looking at the aligning modules individually, one can observe that the string match of the translated annotation with the translated text is the method that yielded the highest evaluation scores, for both triggers and arguments. Both the Lemmatization (Lemma) and the Multiple Word Translation (MTrans) methods revealed alignment scores above 90% on the trigger alignment. As for the argument alignment, the Fuzzy string match and MTrans were the ones that achieved better alignment scores. The limitations of each alignment technique are further explored in Section 4.\nThe quality of these alignments can further be assessed by observing the Event Extraction task results of models that were trained using the ACE-2005-PT corpus. In Cunha et. al (2023) [6] this corpus was used to train Question Answering models to perform Event Extraction for Portuguese. The model trained with ACE-2005-PT is available for direct scrutiny online.\nTable 3 presents F1-scores for trigger extraction and argument extraction on both the ACE-2005 corpus in English (original) and our Portuguese version presented in this paper, ACE-2005-PT.\nSince the Portuguese model consists of a QA model it should primarily be compared with the BERT_QA_Arg model in Table 3 due to the similarities in their model architectures. Looking at the table, one can observe that the results for the Portuguese model (64% F1-score for Trigger Extraction and 46% F1-score for Argument Extraction) are lower when compared to the results of the English models. However, since the language used to train and evaluate these models is different, a direct comparison might be inaccurate due to divergences in language vocabulary, such as idiomatic expressions, and other cultural differences between languages."}, {"title": "4 ERROR ANALYSIS AND LIMITATIONS", "content": "Firstly, it is important to acknowledge that automatic translation is prone to translation errors which can lead to inaccuracies and a loss of the intended meaning in the translated texts.\nRegarding annotation alignments, some errors were found. For instance, in non-null subject languages like English, it is obligatory to include the subject in sentence construction, while in Portuguese, the subject can be implicit [3]. Consider the next sentence as an example:\n\"We have been under heavy fire in the Middle East\"\nIn this example, the word \"fire\" is marked as an event trigger of type Conflict: Attack, and \"We\" is an event argument with the role Target. However, in the Portuguese translation, \"We have\" was translated to \"Temos\" (the conjugated verb \"have\" in the first person plural). As a result, the subject became implicit, omitting the argument \"We\". During the validation process on the test split of the ACE-2005-PT corpus, we detected 41 errors in the alignment of arguments related to this phenomenon. These errors account for 11.71% of the total argument alignment errors.\nAnother identified issue consists of our pipeline's inability to accurately identify the sentences' syntax structure. According to the ACE-2005 annotation guidelines [1], when annotating a noun phrase (NP) with a determiner, the latter should be included in the argument. For instance, in the previous example, the NP \"the Middle East\", which is an event argument with the role Place, comprises the determiner \"the\". In this case, the NP is part of a prepositional phrase (PP) headed by the preposition \"in\", but the preposition is not part of the argument. In the context of the Portuguese language, in examples like this one, the preposition is contracted with the determiner, that is, in the PP \"no M\u00e9dio Oriente\", we observe the contraction of \"em\" (\"in\") + \"o\" (\"the\"). The event argument in Portuguese is \"o M\u00e9dio Oriente\" and not \"no M\u00e9dio Oriente\". While manually annotating the test set of the ACE-2005 Portuguese version, our expert followed ACE-2005 guidelines, which meant separating the preposition from the determiner. However, this requirement poses a challenge for automatic alignment.\nWe detected 242 instances where our pipeline failed to correctly align the determiner, accounting for 69.14% of the errors in argument alignment. This is one of the main reasons for the discrepancy between the relaxed (87.77%) and Exact (70.55%) scores. By using the exact match, these cases are considered completely misaligned. On the other hand, by using the relaxed match, despite being penalised for omitting the determiner, the pipeline still receives credit for identifying the remaining correctly aligned tokens.\nWe also conducted an error analysis on the pipeline text alignment components. For instance, character-level similarity matching disregards the order of characters and lacks semantic understanding. In the previous example, the argument annotation \"We\" was translated to \"N\u00f3s\" in isolation. By relying on character similarity matching, \"N\u00f3s\" was mistakenly aligned with the word \"no\" (the) due to their shared characters, resulting in an incorrect alignment. This issue becomes more prevalent when dealing with shorter strings, thus, this method was not used to align triggers which are usually composed of one single word. On the other hand, the lemmatization method also introduces ambiguity, as multiple words with different meanings can map to the same lemma. This ambiguity may result in incorrect alignments, where annotations are aligned with incorrect spans in the translated text due to multiple possible lemma matches.\nIn Algorithm 3, we determine the translated annotation offsets using the maximum and minimum indexes of the previously computed alignments. However, in case of alignment errors, the calculated minimum or maximum values can become disproportionately large or small, resulting in absurdly large annotation spans. To address this issue, we implemented a safeguard by comparing the lengths of the aligned annotation and the source annotation. If a significant discrepancy is detected, the alignment candidate is discarded."}, {"title": "5 CONCLUSION", "content": "In this work, we developed a translation and alignment pipeline specifically designed to automatically translate the ACE-2005 corpus into both European and Brazilian Portuguese. To assess its effectiveness, we compared the pipeline results against manually aligned annotations performed by a linguistic expert, achieving a relaxed score of 87.77% and an exact match of 70.55%. While our primary focus was the Portuguese language, our pipeline can be easily adapted to accommodate other languages and corpora. This work expanded the usability of the ACE-2005 corpus beyond its original three languages by creating a Portuguese version, which has been accepted for publication by the LDC.\nIn the future, our pipeline could translate ACE-2005 into other languages and extend to other corpora such as the Entities, Relations and Events corpus (ERE) [5], enhancing NLP tasks in various languages and tasks."}]}