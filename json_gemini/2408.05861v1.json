{"title": "Leveraging Knowledge Graph-Based Human-Like Memory Systems to Solve\nPartially Observable Markov Decision Processes", "authors": ["Taewoon Kim", "Vincent Fran\u00e7ois-Lavet", "Michael Cochez"], "abstract": "Humans observe only part of their environment at any mo-\nment but can still make complex, long-term decisions thanks\nto our long-term memory system. To test how an AI can learn\nand utilize its long-term memory system, we have developed\na partially observable Markov decision processes (POMDP)\nenvironment, where the agent has to answer questions while\nnavigating a maze. The environment is completely knowledge\ngraph (KG) based, where the hidden states are dynamic KGs.\nA KG is both human- and machine-readable, making it easy\nto see what the agents remember and forget. We train and\ncompare agents with different memory systems, to shed light\non how human brains work when it comes to managing its\nown memory systems. By repurposing the given learning ob-\njective as learning a memory management policy, we were\nable to capture the most likely belief state, which is not only\ninterpretable but also reusable. Our code is open-sourced at\nhttps://github.com/humemai/agent-room-env-v2-lstm.", "sections": [{"title": "1 Introduction", "content": "The cognitive science theory suggests that humans have a\nlong-term memory system (Tulving 1983, 1985; Tulving and\nThomson 1973). This allows humans to solve various tasks,\nalthough they can only observe the world partially. For ex-\nample, when trying to navigate through a familiar city, hu-\nmans rely on stored memories of the layout and landmarks.\nThis memory system enables the recall of past experiences,\naiding in decision-making and problem-solving even when\nimmediate sensory input is limited. Similarly, when answer-\ning a question, humans draw on long-term memory to re-\ntrieve relevant facts, concepts, and previous knowledge.\nAlthough there have been many studies, we still lack a\ncomprehensive understanding of how our long-term mem-\nory system is managed. Research has identified mechanisms\nfor encoding, storing, and retrieving memories, but the pre-\ncise regulation and optimization of these processes remain\nunclear (Artuso and Palladino 2019; Squire 2009).\nUnderstanding memory management is not only a fun-\ndamental question in cognitive science but also has signif-\nicant implications for artificial intelligence (AI). By study-\ning how humans efficiently encode, store, and retrieve infor-\nmation, we can develop more sophisticated AI systems that\nmimic these processes. AI agents that can manage long-term\nmemory effectively are potentially better equipped to handle\ncomplex tasks that require recalling past information, adapt-\ning to new situations, and making informed decisions based\non incomplete data. This motivation drives our research to\ncreate a controlled environment where we can systemati-\ncally explore these memory management strategies in AI, ul-\ntimately aiming to bridge the gap between human cognitive\nprocesses and AI (Hassabis et al. 2017; Lake et al. 2017).\nThe contributions of this paper are as follows.\n\u2022 KG-based Environment Design: We design and re-\nlease a KG-based environment, compatible with Gymna-\nsium (Towers et al. 2023), where an agent has to answer\nquestions while navigating a maze. This environment is\nhighly configurable and offers various levels of complex-\nity.\n\u2022 Comparative Analysis of Memory Systems: We com-\npare agents with different memory systems and discuss\nparallels with how human brains work when it comes to\nmanaging its own memory systems.\n\u2022 Optimal Memory Management Policy: We show that\nthe best agent is the one that repurposes the given learn-\ning objectives as learning a memory management pol-\nicy, allowing itself to capture the most likely belief state,\nwhich is not only interpretable but also reusable."}, {"title": "2 Background", "content": "2.1 A Machine With Human-Like Memory\nSystems\nRecent advances in cognitive science and AI have inspired\nthe development of machines with memory systems inspired\nby human memory. Kim et al. (Kim et al. 2022, 2023) in-\ntroduced an agent model that explicitly incorporates both\nsemantic and episodic memory systems, demonstrating that\nsuch an agent outperforms those with only one type of mem-\nory system. Their environment is designed to challenge the\nagent's ability to encode, store, and retrieve memories effec-\ntively.\nBoth episodic and semantic memories are part of a human\nlong-term memory system. Episodic memory pertains to the\nrecollection of specific events and experiences that are tied\nto particular times and places. It involves personal memo-\nries that include contextual details about when and where\nthey occurred. Semantic memory, on the other hand, deals\nwith general world knowledge and facts that are not linked\nto specific occurrences. For example, knowing that Paris is\nthe capital of France is a semantic memory, whereas recall-\ning a visit to the Eiffel Tower involves episodic memory.\nTo computationally model episodic and semantic memo-\nries, Kim et al. (Kim et al. 2022, 2023) add relation qualifier\nkey-value pairs to RDF (Resource Description Framework)\ntriples to make them RDF quadruples (Lanthaler, Wood,\nand Cyganiak 2014; Hartig et al. 2024). For example, given\na memory triple (Agent, atLocation, Library),\nthey would turn this into something like (Agent,\natLocation, Library, {timestamp: 42}), and\n(Agent, atLocation, Library, {strength:\n2}), to turn them into episodic and semantic memories,\nrespectively.\nKGs are advantageous because they are both human- and\nmachine-readable, facilitating the structured encoding, stor-\ning, and retrieving of information."}, {"title": "2.2 Reinforcement Learning (RL) in Partially\nObservable Markov decision process\n(POMDP)", "content": "RL is useful when supervision of every action is not feasible,\nsuch as in memory management. This is often formalized as\na Markov Decision Process (MDP) or a Partially Observable\nMDP (POMDP), which account for partial observability.\nWe consider an agent interacting with its environment\nover discrete time steps. The underlying dynamics of\nthe environment is modeled as an MDP (Bellman 1957),\nbut the agent can only partially observe it, making it a\nPOMDP (\u00c5str\u00f6m 1965; Fran\u00e7ois-Lavet et al. 2019). It is de-\nfined by: (i) a state space S that is discrete or continuous;\n(ii) an action space A that is discrete or continuous; (iii) the\nenvironment's transition function $T : S \\times A \\rightarrow P(S)$; (iv)\nthe environment's reward function $R : S \\times A \\rightarrow R$ that is\neither deterministic or stochastic; (v) an observation space\n\u03a9 that is discrete or continuous; (vi) the observation model\n$O : S \\times A \\rightarrow P(\\Omega)$; and (vii) a discount factor \u03b3\u2208 [0,1).\nIn our environment, S, A, and \u03a9, are discrete, T and O are\nstochastic, and R is deterministic.\nThe environment starts in an initial state distribution. At\ntime step t, the agent maintains a belief state bt, which is\na probability distribution over the state space S. The agent\nthen chooses an action $a_t \\in A$ based on its current belief\nstate bt according to a policy $\u03c0 : B \\rightarrow P(A)$, where B de-\nnotes the belief space. After taking action $a_t \\sim \u03c0(a_t | b_t)$,\nthe agent observes an observation $O_{t+1} \\in \\Omega$. Using the ob-\nservation $O_{t+1}$ and the action at, the agent updates its belief\nstate to $b_{t+1}$.\nThe agent's objective is to choose a policy \u3160 that maxi-\nmizes the cumulative discounted rewards:\n$\\underset{\\pi}{\\text{max}}E_{\\pi} [\\sum_{t=0}^{\\infty} \\gamma^{t} r_{t}]$\nwhere $r_t = R(s_t, a_t), a_t \\sim \\pi(a_t | b_t), s_{t+1} \\sim T(s_t, a_t)$,\nand $o_{t+1} \\sim O(o_{t+1} | s_{t+1}, a_t)$. The belief state $b_{t+1}(s_{t+1})$\nis updated using the current belief state bt, the action at,\nand the observation $o_{t+1}$, along with the observation model\nO and the transition model T. The update rule is given by:\n$b_{t+1}(s_{t+1}) = \\frac{O(o_{t+1} | s_{t+1}, a_t) \\sum_{s_t \\in S}T(s_{t+1} | s_t, a_t)b_t(s_t)}{\\sum_{s'\\in s} O(o_{t+1} | s', a_t) \\sum_{s_t \\in S}T(s' | s_t, a_t)b_t(s_t)}$\nUpdating the belief state can be computationally expen-\nsive due to several factors. The belief state represents a prob-\nability distribution over the entire state space, which can be\nvast. Updating it requires summing over all possible states\nand applying transition and observation probabilities, mak-\ning the process highly intensive.\nA simple way to mitigate this computational expense\nis to use the history of the past 7 observations $h^{(\u03c4)} = (o_{t-\u03c4+1}, ..., o_t)$ as the estimated state \u015dt, with which we\nlearn the policy $\u03c0(a_t | \u015d_t)$. This alleviates the need to main-\ntain or update the belief state bt."}, {"title": "3 Methodology", "content": "3.1 The Rooms Environment\nThe Rooms Environment provides a simulation for evaluat-\ning Al agents with long-term memory systems. Unlike the\nprevious work, this environment consists of multiple inter-\nconnected rooms, each containing various types of objects.\nStatic objects, such as beds and tables, remain fixed in their\nlocations and do not move. Independent objects, like Bob\nand Alice, move between rooms according to probabilis-\ntic rules that dictate their movements. Dependent objects,\nwhich cannot move by themselves, may be transported by\nindependent objects with a certain probability that depends\non the current state of the room. Finally, the agent represents\nthe RL agent that navigates between rooms based on its ex-\nploration policy. This configuration allows for the study of\nhow agents manage and utilize long-term memory to make\ndecisions in a dynamic and partially observable environ-\nment.\nIn this environment, hidden states at time t, st, are repre-\nsented as dynamic KGs that capture the entire rooms and the\nlocations of all objects within them. Each room and its con-\ntents form a sub-graph of this overall KG. Since the agent\ncan only be present in one room at a time, it can only observe\nthe objects within that particular room. This partial observ-\nability requires the agent to utilize its long-term memory.\nA dynamic KG is defined as a directed graph $G =\n(V, R, T, E)$. Here, V represents the set of entities, R the\nset of relations, T the set of (discrete) timestamps, and\n$E \u2286 V\u00d7R\u00d7V\u00d7T$ the set of edges. Each edge in & is a tuple\n(vi, rk, vj, t) indicating that a relation rk exists from entity\nvi to entity vj at time t. This structure enables the graph to\ndynamically reflect the changes within V and & over time,\nunder the governance of temporal rules.\nFigure 1 shows an example hidden state st and partial ob-\nservation ot. In this example, there are 32 rooms, i.e., Office\nroom, Kitchen, ..., and 25 objects, i.e., Coat, Towel,\nAgent. Therefore, the number of all hidden states is |S| =\n$N_{rooms}^{(N_{objects} - N_{static objects})} = 32^{25-8} \u2248 3.8 \u00d7 10^{25}$, since every ob-\nject is located in one of Nrooms and the static objects do not\nmove. The number of possible sub-graphs, i.e., partial ob-\nservation, that the agent can observe is $2^{(N_{objects} \u2013 N_{static objects}) \u00d7\nN_{rooms}} = 2^{25-4} \u2248 6.7 \u00d7 10^7$, since the agent can either see\nor do not see an object in the room. Although the environ-\nment might look simple, we have a huge state space, which\ncan be too much for typical table-based RL methods.\nIn this example, the partial observation (sub-\ngraph of the hidden dynamic KG) is { (Closet,\nnorth, Nursery), (Closet, east, Wall),\n(Closet, south, Wall), (Closet, west,\nSauna), (Agent, atLocation, Closet) }.\nThen the hidden state is a much bigger KG that contains\nthis sub-graph.\nThe exploration policy, $\u03c0_{explore}(a_{explore} | o_t)$, allows the\nagent to navigate through the room by taking one of five pos-\nsible actions: North, East, South, West, or Stay. As the agent\nexplores, the environment intermittently poses questions\nabout the object locations. Consequently, the agent must not\nonly learn an effective exploration policy, \u03c0explore, but also\ndevelop a robust question-answering function, $\u03c0_{answer} =\nf_{qa}(o_t)$. The question-answering function also serves as\nthe reward function: the agent receives a reward of +1 for\neach correctly answered question and 0 otherwise. Although\nstraightforward, using ot for the exploration and question\nanswering is not enough. Therefore, the agent should take\nadvantage of its previous partial observations to form mem-\nory.\nTo maximize its rewards, the exploration policy should\nguide the agent to spend more time in \"informative\" rooms,\nwhere the likelihood of encountering valuable information\nis higher. This is crucial because objects do not spend equal\namounts of time in each room, meaning that not all sub-\ngraphs are equally informative. Therefore, the agent needs\nto utilize its long-term memory to maintain and retrieve in-\nformation about the maze layout and object locations, which\nis essential for both exploration and answering questions.\nIn this work, \u201cquestion answering\" is approached as a\nform of KG reasoning, specifically restricted to answering\none-hop queries. These queries involve predicting the tail in\na given RDF triple (s, r, ?), where s is the subject, r is the re-\nlation, and the goal is to predict the unknown tail. This type\nof reasoning involves traversing the graph from a known en-\ntity using a specified relation.\nWhen the environment randomly samples objects (RDF\ntriples) to generate questions, not all objects have an equal\nprobability of being sampled. This bias in sampling af-\nfects both the exploration policy and the question-answering\nfunction. The agent must account for this sampling bias by\nfocusing its exploration on areas with a higher likelihood of\nbeing queried.\nThe environment randomly samples a triple from its\ncurrent hidden state and turns it into a question. In our\nrunning example, for instance, (Phone, atLocation,\nKitchen) and (Alice, atLocation, Library)\ncan be selected and turned into (Phone, atLocation,\n?) and (Alice, atLocation, ?), respectively.\nThe Rooms environment is highly configurable, allowing\nfor a wide range of experimental setups. Researchers can\nadjust various parameters such as the frequency of questions"}, {"title": "3.2 Agents and Their Learning Objectives", "content": "Baseline Agent The baseline agent utilizes the history of\nthe past 7 observations, denoted as $h^{(\u03c4)} = (o_{t-\u03c4+1}, ..., o_t)$,\nto represent the current estimated state \u015dt. This approach is\nthe most straightforward method for handling a POMDP. For\nthe question-answering function $f_{qa}$, the agent employs a\nsimple strategy: using the most recent relevant memory to\nanswer the question. This strategy assumes that the most re-\ncent observation contains the most accurate and relevant in-\nformation for responding to queries about the environment.\nIn this setup, solving the POMDP involves optimizing the\nexploration policy $\u03c0_{explore}$, where the reward function the\nquestion-answering function fqa. Thus, the \u201ccurrent state\"\nfor $\u03c0_{explore}$ is $h^{(\u03c4)}$, and the \u201cnext state\u201d is the updated $h^{(\u03c4)}$\nwhere the oldest observation is removed, and the new obser-\nvation is added to the sequence.\nHumemAI Agent The agent with human-like memory\nsystems (hereinafter HumemAI) introduces a function, $f_{mm}$,\nknown as the memory management function. This agent fea-\ntures a memory system Mt, which includes both short-term\nand long-term memory components, $M_{short}$ and $M_{long}$, re-\nspectively. The long-term memory system is further divided\ninto episodic ($M_{episodic}$) and semantic ($M_{semantic}$) memory.\nThe function fmm first transfers the current par-\ntial observation to the short-term memory system by\nadding the relation qualifier current_time along with its\nvalue. For example, (Agent, atLocation, Home\nOffice) att = 42 becomes (Agent, atLocation,\nHome Office, {current_time: 42}). The func-\ntion then determines whether this short-term memory\nshould be (1) moved to the episodic part of the long-\nterm memory, (2) moved to the semantic part of the\nlong-term memory, or (3) forgotten. If option (1) is se-\nlected, the short-term memory is stored in the episodic\nmemory as (Agent, atLocation, Home Office,\n{timestamp: 42}). If option (2) is chosen, it is\nstored as (Agent, atLocation, Home Office,\n{strength: 1}).\nThe \"strength\" term indicates the generalizability of this\nmemory in the semantic memory system, with higher val-\nues representing repeated additions of the same memory.\nWe also introduce an exponentially decaying factor for each\nsemantic memory to ensure that all memories gradually\nweaken over time, allowing for continuous adaptation. The\ndecaying of semantic memory is well studied in cognitive\nscience (Rogers and Friedman 2008; Milberg et al. 1999;\nSilveri et al. 2018; Catrical\u00e0 et al. 2015).\nIn essence, the long-term memory system is up-\ndated at each step according to the equation $M_{long}^{t} =\nf_{mm}(M_{long}^{t-1}, M_{short}^{t}, a_{mm})$. The agent begins with an empty"}, {"title": "3.3 The Learning Algorithm", "content": "Given the discrete nature of the action spaces for both the\nbaseline and the HumemAI agents, since all of our policies\nhave discrete action spaces, we opt for a value-based RL al-\ngorithm (Watkins and Dayan 1992), i.e., DDQN (Hasselt,\nGuez, and Silver 2016) instead of the vanilla DQN (Mnih\net al. 2013), to avoid the overestimation of state-action val-\nues. On top of that, we decouple value and advantage in the\ndeep Q-networks (DQN), i.e., dueling network (Wang et al.\n2016), to generalize learning across actions. A stochastic\ngradient descent algorithm, Adam (Kingma and Ba 2015),\nwas used to minimize the temporal difference (TD):\n$\\mathcal{L}(\\theta) = E_{(s,a,r,s')\\sim D} [(y - Q(s, a; \\theta))^2]$\nwhere\n$y=r+ \u03b3\\underset{a'}{max} Q(s', arg max Q(s', a'; \u03b8); \\theta^-)$\nIn the above equations, L(\u03b8) represents the TD loss func-\ntion. (s, a, r, s') are the state, action, reward, and next state,\nrespectively, sampled from the replay buffer D. Q(s, a; \u03b8) is\nthe Q-value predicted by the current Q-network parameter-\nized by \u03b8. y is the target value computed using the reward\nr and the discounted maximum Q-value of the next state s'.\ny is the discount factor. \u03b8 represents the parameters of the\ntarget Q-network, which are periodically updated with \u03b8.\nBy minimizing this loss, we aim to bring the predicted\nQ-values closer to the target Q-values, thus improving the\npolicy's performance over time.\nThe DQN receives as input either $M_t$ for HumemAI agent\nor $h^{(\u03c4)}$ for the baseline agent, and outputs the Q-values of\nthe actions. This deep neural network involves KG embed-\ndings (KGEs), LSTMs (Hochreiter and Schmidhuber 1997),\nand MLPs (Sanger and Baljekar 1958). Algorithm 1 shows\nhow a forward pass is made to compute the Q-values for the\nHumemAI agent.\nThe algorithm to compute the Q-values for the HumemAI\nagent is for one policy only. Since this agent learns two\npolicies, i.e., $\u03c0_{mm}$ and $\u03c0_{explore}$, the algorithm is repeated\ntwice. All the weights are randomly initialized except one:\n$\u0398_{LSTM}$ is initialized with $\u0398_{LSTM}^{\u03c0_{explore}}$ because we want to leverage\nthe memory management knowledge already learned by the\nagent.\nAs for $\u03c0_{mm}$, out of $N_{observations}$ observations within a given\nroom, only one of them gets stored in the short-term memory\nsystem at a time. This is because each quadruple (observa-\ntion) requires an action. In the end, all $N_{observations}$ of them\nwill be moved to the episodic, semantic, or be forgotten, but\nthey are done sequentially, not all at once."}, {"title": "4 Experiments", "content": "We evaluate the baselines with varying sizes of $h^{(\u03c4)}$. Addi-\ntionally, for the HumemAI agent, we create two extra vari-\nants for ablation: one utilizing only an episodic memory sys-\ntem and the other utilizing only a semantic memory sys-\ntem. This allows us to analyze the individual contributions\nof each memory type to the overall performance.\nWe designed the training setup to ensure fair compari-\nson between all agents. The HumemAI agent, trained in two\nphases, has about 85,000 learnable parameters per phase\nand undergoes 100 training episodes. The decaying fac-\ntor semantic memory was set to 0.9. For parity, baseline\nagents have 170,000 learnable parameters and 200 training\nepisodes. All other hyperparameters remain constant across\nagents. We select the best-performing model from training\nand runs in a separate test environment with a greedy pol-\nicy. We ran in total of 5 runs to report average performance.\nTable 1 shows the overall results.\nTable 1 shows the superior performance of the HumemAI\nagent, especially those with both episodic and semantic\nmemory systems. The two-phase training approach proves\neffective, with Phase 1 optimizing memory management\nand Phase 2 leveraging this to improve exploration. The\nHumemAI agents consistently outperform baselines across\nmemory capacities. Interestingly, longer observation histo-\nries for baseline agents don't necessarily improve perfor-\nmance, as seen with the capacity 192 baseline performing\nworse than the HumemAI agents with capacity 12. This sug-\ngests that longer histories may include irrelevant informa-\ntion. Training efficiency is also notable, with the HumemAI\nagent (capacity 24) requiring 40 minutes in total, while a\nbaseline agent (capacity 48) took about 70 minutes on a\nsingle-CPU machine (AMD Ryzen 9 7950X 16-Core Pro-\ncessor). The exact details of the hyperparameters can be\nfound in the supplementary material.\nFigure 4 illustrates the attention weights of $\u03c0_{mm}$ and\n$\u03c0_{explore}$ over time for the HumemAI agent with capacity 24.\nIn 4(a), we observe a decrease in the agent's focus on short-\nterm memory as the agent moves between the rooms (ob-\nserves different sub-graphs), while attention to long-term\nmemory increases. This shift is logical, given that the long-\nterm memory starts empty at the beginning of an episode.\nNotably, for both $\u03c0_{mm}$ and $\u03c0_{explore}$, the agent shows a prefer-\nence for the episodic component of long-term memory. This\npreference suggests that episodic memories play a crucial\nrole in both memory management and exploration strategies.\nFigure 5 illustrates the agent's memory systems at the\nfinal time step of an episode. It essentially captures infor-\nmation about the hidden belief state. The agent predomi-\nnantly uses episodic memory for atLocation relations\n(7 in episodic vs. 4 in semantic), suggesting a preference\nfor timestamped object locations. This strategy likely aids\nin answering questions by distinguishing between old and\nnew memories. Conversely, map layout information is stored\nmore in semantic memory (8 semantic vs. 5 episodic), re-\nflecting its factual and static nature. Notably, the agent never\nstores wall-related memories in episodic memory, possi-\nbly because the environment never queries wall locations.\nThis memory distribution demonstrates the agent's ability to\nadapt its storage strategy based on the nature of the informa-\ntion and its relevance to the task at hand."}, {"title": "5 Related Work", "content": "Our work stands at the intersection of cognitive science,\nKG, and RL, and while there has been significant research\nin each of these areas individually, there is relatively little\nwork that combines them to address the challenges of par-\ntially observable environments. Notably, the term \"episodic\nmemory\" is often used in RL to refer to state-action pairs\nexperienced by the agent, similar to the replay buffer used\nin DQN (Mnih et al. 2013). However, this usage diverges\nfrom our approach, which adheres closely to the cognitive\nscience definition of episodic memory introduced by Tulv-\ning et al. (Tulving 1983, 1985; Tulving and Thomson 1973),\nemphasizing its recency property.\nThe cognitive architectures like ACT-R (Anderson 2007)\nand Soar (Laird 2012) offer comprehensive frameworks for\nunderstanding human memory systems and their applica-\ntions in AI agents, yet they fall short in providing compu-\ntational methods for training RL agents with these memory\nsystems. Extensions of these frameworks, such as the work\nby Gorski and Laird (Gorski and Laird 2011) and Li (Li\n2020), focus on learning episodic memory retrieval within\nthese architectures, though they do not leverage KGs or so-\nphisticated RL techniques.\nMemory-based RL approaches, such as those by Ming\net al. (Meng, Gorbet, and Kuli\u0107 2021) and Lin et al. (Lin\net al. 2018), add memory components to address partial ob-\nservability and sample efficiency, respectively. Lin et al. use\nepisodic memory to regularize DQN but differs from our\nmethod by not leveraging KGs for interpretability and long-\nterm memory management.\nIn a similar vein, Han et al. (Han et al. 2019) in-\ntegrate RL with memory systems to improve question-\nanswering performance. However, their memory system\nlacks the structured, interpretable nature of KGs, relying\ninstead on numeric embeddings. Demir (Demir 2023) in-\ntroduces memory-modifying actions and intrinsic motiva-\ntion to guide memory retention in partially observable envi-\nronments, focusing on event sequences without employing\nKGs.\nYoung et al. (Young, Sutton, and Yang 2018) and Hu\net al. (Hu et al. 2021) propose methods for maintaining\nepisodic memory through reservoir sampling and general-\nizable episodic memory representations, respectively. These\napproaches aim to improve RL performance by conditioning\npolicy networks on past experiences, but they do not address\nthe structured nature of KGs or the human-like episodic-\nsemantic memory systems utilized in our work.\nIncorporating KGs into RL has primarily focused on KG\nreasoning (Wan et al. 2020; Das et al. 2018; Xiong, Hoang,\nand Wang 2017; Wang et al. 2022; Park et al. 2022), where\nthe goal is to learn to reason about the relationships between\nentities. These works typically use current triples or concate-\nnated visited triples as state representations, differing from\nour approach that constructs and updates a dynamic KG as\npart of the agent's state representation. The actions in KG\nreasoning involve selecting outgoing edges from the current\nnode, whereas our actions include memory management and\nnavigation based on the KG state.\nSeveral works use KGs as prior knowledge to enhance RL\nperformance. H\u00f6pner et al. (Hopner, Tiddi, and van Hoof\n2022) improve sample efficiency by leveraging KGs, while\nPiplai et al. (Piplai et al. 2020) guide exploration using KGs.\nChen et al. (Chen et al. 2022) employ RL to learn rule min-\ning within KGs. These approaches highlight the benefits of\nintegrating KGs with RL but do not focus on the human-like\nmemory systems or the partially observable environments\nthat our work addresses."}, {"title": "6 Conclusion", "content": "In this paper, we introduced an approach to solving\nPOMDPs by leveraging KG-based human-like memory sys-\ntems. We developed the Rooms Environment with vary-\ning levels of complexity, where agents navigate and an-\nswer questions based on partial observations. We compared\na baseline agent with a simple memory system to our agent,\nwhich incorporates episodic and semantic memory systems.\nResults show that the HumemAI agent outperforms the base-\nline, effectively capturing information about the belief state\nand improving decision-making capabilities.\nThere is room for improvement in our paper. Potential ar-\neas for enhancement include jointly learning memory man-\nagement and exploration policies, which could yield better"}]}