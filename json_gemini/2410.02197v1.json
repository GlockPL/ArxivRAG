{"title": "General Preference Modeling with Preference Representations for Aligning Language Models", "authors": ["Yifan Zhang", "Ge Zhang", "Yue Wu", "Kangping Xu", "Quanquan Gu"], "abstract": "Modeling human preferences is crucial for aligning foundation models with human values. Traditional reward modeling methods, such as the Bradley-Terry (BT) reward model, fall short in expressiveness, particularly in addressing intransitive preferences. Although supervised pair preference models (PairPM) can express general preferences, their implementation is highly ad-hoc and cannot guarantee a consistent preference probability of compared pairs. Additionally, they impose high computational costs due to their quadratic query complexity when comparing multiple responses. In this paper, we introduce preference representation learning, an approach that embeds responses into a latent space to capture intricate preference structures efficiently, achieving linear query complexity. Additionally, we propose preference score-based General Preference Optimization (GPO), which generalizes reward-based reinforcement learning from human feedback. Experimental results show that our General Preference representation model (GPM) outperforms the BT reward model on the RewardBench benchmark with a margin of up to 5.6% and effectively models cyclic preferences where any BT reward model behaves like a random guess. Furthermore, evaluations on downstream tasks such as AlpacaEval2.0 and MT-Bench, following the language model post-training with GPO and our general preference model, reveal substantial performance improvements with margins up to 9.3%. These findings indicate that our method may enhance the alignment of foundation models with nuanced human values. The code is available at https://github.com/general-preference/general-preference-model.", "sections": [{"title": "Introduction", "content": "Modeling human preferences is a cornerstone in developing foundation models that interact seamlessly with users. In natural language modeling and reinforcement learning, aligning models with human intent and values has led to significant advancements, including improved text generation and enhanced decision-making policies (Ouyang et al., 2022; Christiano et al., 2017). Traditional approaches often rely on reward modeling, wherein a reward function is learned to guide the optimization of policies. While effective in certain contexts, these methods face challenges in expressiveness and computational efficiency, particularly when addressing complex or intransitive human preferences (Tversky, 1969; Munos et al., 2023)."}, {"title": "Related Work", "content": "Reward-Based Reinforcement Learning from Human Feedback (RLHF). The earlier approaches to modeling human preference for language model alignment usually learn a reward model from a preference dataset. The human preference is assumed to follow the Bradley-Terry (BT) model (Bradley & Terry, 1952) or the Thurstone model (Thurstone, 2017). LLM policies then are fine-tuned to maximize these scalar reward signals for better alignment (Christiano et al., 2017; Ziegler et al., 2019; Ouyang et al., 2022). Later, the direct preference optimization (DPO) methods are proposed by Rafailov et al. (2024) to only implicitly learn a reward model represented by an LLM. The human preference is still assumed to follow the Bradley-Terry model. However, the reliance on scalar rewards imposes a total ordering on preferences, which may not reflect the intransitive or stochastic nature of human judgments (Tversky, 1969; Agranov & Ortoleva, 2017).\nPreference-Based Reinforcement Learning from Human Feedback. Recently, there emerged a line of works that directly estimates the preference probability without imposing a reward-based preference model or any transitivity assumptions (Lou et al., 2022; Wu et al., 2023; Wang et al., 2023) either for preference-based RL or in the context of RLHF. Efforts have been made to optimize policies directly from pair-wise preference comparisons, thereby mitigating the limitations of scalar reward functions (Munos et al., 2023; Swamy et al., 2024; Rosset et al., 2024; Wu et al., 2024b).\nIntransitivity in Game Theory. The symmetric zero-sum game and its intransitivity have also been frequently studied in the context of game theory. Balduzzi et al. (2018) was motivated by evaluation among different agents, showing that any symmetric zero-sum game can be decomposed into a \u201ctransitive\" game and a \"cyclic\" game, and proposed Nash averaging for better agent/task evaluation. Balduzzi et al. (2019) generalized the results from matrix games to functional-form games and propose new algorithms to construct diverse populations of effective agents. Czarnecki et al. (2020) investigated the geometrical properties of real-world games (e.g., Tic-Tac-Toe, Go, StarCraft II) and proposed that real-world games have a \"spinning top\" geometry, with a strong transitive dimension and gradually diminishing non-transitive cyclic dimensions. Very recently, Bertrand et al. (2023) examined the limitations of the Elo rating system and proposed an alternative \"disc decomposition\" method that can better handle both transitive and cyclic game dynamics.\nRepresentation Learning and Embedding. Representation learning and embedding techniques have successfully captured relational structures across various domains (Mikolov et al., 2013; Chen et al., 2020; Radford et al., 2021), yet their application in preference modeling and RLHF remains limited. Our work introduces preference representation learning, an approach that enhances expressiveness while maintaining computational efficiency, bridging the gap left by traditional approaches."}, {"title": "Background", "content": "In this section, we present preliminaries on reward modeling, preference modeling, and reinforcement learning from human feedback (RLHF) for language model alignment. We consider an autoregressive language model that generates responses to the given prompts. Let $x = [x_1, x_2, ...]$ denote a prompt, a sequence of tokens. The language model $\u03c0$ generates a response $y = [y_1, y_2, ..., y_n]$ based on the conditional probability distribution: $\u03c0(y | x) = \\prod_{i=1}^n \u03c0(y_i | x, y_{<i})$, where $y_{<i}$ represents the sequence of tokens generated before position $i$. In this paper, we assume a general-preference oracle. Given two responses $y$ and $y'$ to the same prompt $x$, the oracle provides the feedback indicating which response is preferred.\n$\\mathbb{P}(y > y' | x) := \\mathbb{E} [o (y > y' | x)] .$"}, {"title": "Reward-Based Reinforcement Learning from Human Feedback", "content": "The most prevalent approach to aligning language models with human preferences is to consider a scalar reward function $r(y; x)$ that assigns a numerical score to each response. The preference between two responses is then determined solely by the reward scores for the two responses. For example, the Bradley-Terry (BT) model (Bradley & Terry, 1952) is a widely used method for modeling pairwise preferences in this context. However, the BT model can not capture intransitive (e.g. cyclic) preferences effectively (Bertrand et al., 2023). Under the BT model, the probability that response y is preferred over y' is given by:\n$\\mathbb{P}(y > y' | x) = \u03c3(r(y; x) \u2013 r(y'; x)),$\nwhere $\u03c3(z) = 1/(1 + e^{-z})$ is the logistic (sigmoid) function.\nIn practice, the reward function $r(y; x)$ is learned by maximizing the likelihood of the observed preference data. Once the reward function is established, policy optimization techniques, such as Proximal Policy Optimization (PPO) (Schulman et al., 2017), can be applied to adjust the language model to generate responses that maximize expected rewards. The optimization problem can be formulated as:\n$\\max_\u03b8 \\mathbb{E}_{x\u223cX, y\u223c\u03c0_\u03b8(\u22c5|x)} [r(y; x)] \u2212 \u03b2\\mathbb{E}_{x\u223cX} [KL (\u03c0_\u03b8(\u22c5 | x) || \u03c0_{ref}(\u22c5 | x))],$   (3.1)\nwhere $\u03b8$ are the parameters of the policy $\u03c0_\u03b8$, $\u03c0_{ref}$ is a reference policy (often the pre-trained or supervised-fine-tuned language model), \u03b2 is a scaling parameter that controls the strength of regularization, and KL denotes the Kullback-Leibler divergence."}, {"title": "General Preference Modeling", "content": "We consider the scenario where given a prompt x, a set of responses {$y_i$} is generated, and human preferences over these responses are represented as pairwise probabilities $\\mathbb{P}(y_i > y_j | x) \u2208 (0,1)$, indicating the likelihood that response $y_i$ is preferred over $y_j$ given the prompt x.\nTo model these preferences, we define a (pairwise) preference score function:\n$s(y_i > y_j | x) := \\log \\frac{\\mathbb{P}(y_i > y_j | x)}{1 - \\mathbb{P}(y_i > y_j | x)},$  (3.2)\nwhich represents the log-odds of $y_i$ being preferred over $y_j$. This score function allows us to express the preference probability as:\n$\\mathbb{P}(y_i > y_j | x) = \u03c3 (s(y_i > y_j | x)),$\t  (3.3)\nwhere $\u03c3(z) = 1/(1 + e^{-z})$ is the logistic function. One can see that the BT model is a special case: $s(y_i > y_j | x) = r(y_i; x) \u2013 r(y_j; x)."}, {"title": "Supervised Pair Preference Models", "content": "Existing approaches often involve concatenating the prompt and responses with a template and training an LLM-based sequential classifier in a supervised learning manner. For example, Jiang et al. (2023) simply concatenate the three segments (x, y1, y2) sequentially and form a single input sequence with special tokens as separators:"}, {"title": "General Preference Modeling with Preference Representations", "content": "In this section, we propose a general preference representation learning framework that can model human preferences efficiently and expressively. Each response is embedded as a vector in a latent space, and the preferences are modeled through interactions between these representations (embeddings) using a skew-symmetric operator. We first define preference representations, which serve as the foundation for modeling the relationships between responses."}, {"title": "Preference Representations", "content": "Given a prompt $x$, we assign to each response $y$ a preference representation vector $v_{y|x} \u2208 \u211d^{2k}$. These representations are designed to capture the features relevant to human preferences beyond what can be represented by scalar rewards."}, {"title": "Skew-symmetric Preference Operator", "content": "To model the directional nature of preferences, we introduce the skew-symmetric preference operator, which ensures that the model respects the skew-symmetry (anti-symmetry) in preference modeling."}, {"title": "Skew-symmetric Preference Operator", "content": "To capture the directional nature of preferences, we define a skew-symmetric (anti-symmetric) preference operator $R_\\rangle \u2208 \u211d^{2k\u00d72k}$. Specifically, $R_\\rangle$ is a block-diagonal matrix consisting of k skew-symmetric blocks of the form (for more discussion, please see Appendix A):\n$R_l = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\\\ \\end{bmatrix}, l = 1, ..., k.$ (4.1)\nAn example of $R_\\rangle$ for $k = 2$ is:\n$R_\\rangle = \\begin{bmatrix} 0 & -1 & 0 & 0 \\\\ 1 & 0 & 0 & 0 \\\\ 0 & 0 & 0 & -1 \\\\ 0 & 0 & 1 & 0 \\\\ \\end{bmatrix}$"}, {"title": "Preference Score", "content": "Finally, we define the preference score, which quantifies the degree to which one response is preferred over another. This score is calculated based on the interaction between the preference representations, mediated by the skew-symmetric operator."}, {"title": "Preference Score", "content": "The preference score between two responses $y_i$ and $y_j$ using preference representations is defined as:\n$s(y_i > y_j | x) = <R_\\rangle v_{y_i|x}, v_{y_j|x}>,$  (4.2)\nwhere $(\u00b7, \u00b7)$ denotes the inner product in $\u211d^{2k}$. This score captures the anti-symmetric relationship between responses induced by human preferences.\nWe model the preference probability using the logistic function as defined in Equation (3.3). Our general preference representation model (GPM) exhibits two desirable properties:"}, {"title": "Skew-symmetry", "content": "The preference score function is skew-symmetric, satisfying:\n$s(y_i > y_j | x) = \u2212s(y_j > y_i | x).$\nThis reflects the fact that the preference relation is naturally skew-symmetric: if $y_i$ is preferred over $y_j$ with probability $p_{i,j}$, then $y_j$ is preferred over $y_i$ with probability $1 \u2212 p_{i,j}$.\nSpecifically,\n$s(y > y | x) = <R_\\rangle v_{y|x}, v_{y|x}> = 0.$\nThis means that a response is neither superior nor inferior to itself."}, {"title": "Magnitude preserving", "content": "The skew-symmetric preference operator does not change the representation vector's magnitude, which makes this operation stable for training and inference.\n$<R_\\rangle v_{y|x}, R_\\rangle v_{y|x}> =  <v_{y|x}, v_{y|x}>.$"}, {"title": "Relation to Bradley-Terry Model", "content": "If we set $k = 1$, $v_y = [r(y | x), c]$, where c is a constant and $c \u2260 0$ (e.g., $c = 1$), and $R_\\rangle = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\\\ \\end{bmatrix}$, then the preference score reduces to:\n$s(y_i > y_j | x) = c(r(y_i | x) \u2013 r(y_j | x)),$\nand the preference probability becomes:\n$\\mathbb{P}(y_i > y_j | x) = \u03c3[c(r(y_i | x) \u2013 r(y_j | x))],$\nwhich is exactly the Bradley-Terry (BT) model as a disk game (Balduzzi et al., 2019)."}, {"title": "Expressiveness of the Model", "content": "Our general preference representation model is fully expressive for any real skew-symmetric preference matrix (see Appendix A.1 for complex representations interpretation). Specifically, we establish the following theorem (similar results have been proved in Balduzzi et al. (2018)):"}, {"title": "Efficient Preference Optimization with General Preference", "content": "To address the potential intransitive human preference, the preference-based LLM alignment algorithms (Munos et al., 2023; Azar et al., 2023; Wu et al., 2024b; Rosset et al., 2024) have been proposed to directly work on the preference pairs instead of assuming a reward function.\nGiven a preference oracle $\\mathbb{P}(y > y' | x)$. The objective is to find a policy $\u03c0$ that performs well against another competing policy $\u03c0'$ in terms of these preference probabilities. For example, Azar et al. (2023) consider competing with another fixed policy $\u03bc$ (X denotes the distribution over prompts):\n$\\max_\u03c0 \\mathbb{E}_{x\u223cX} [\\mathbb{E}_{y\u223c\u03c0(\u22c5|x), y'\u223c\u03bc(\u22c5|x)} [\\mathbb{P} (y > y' | x)] \u2013 \u03b2KL(\u03c0||\u03c0_{ref})],$   (5.1)\nOther works (Munos et al., 2023; Wu et al., 2024b; Rosset et al., 2024) consider solving the two-player constant-sum game:\n$\\max_\u03c0 \\min_{\u03c0'} \\mathbb{E}_{x\u223cX} [\\mathbb{E}_{y\u223c\u03c0(\u22c5|x), y'\u223c\u03c0'(\u22c5|x)} [\\mathbb{P} (y > y' | x)]] .$, (5.2)\nTo simplify notation, we define the winning probability of a policy $\u03c0$ over another policy $\u03c0'$ as:\n$\\mathbb{P} (\u03c0 > \u03c0' | x) = \\mathbb{E}_{y\u223c\u03c0(\u22c5|x), y'\u223c\u03c0'(\u22c5|x)} [\\mathbb{P} (y > y' | x)] .$, (5.3)\nThe optimization problem then becomes:\n$\\max_\u03c0 \\min_{\u03c0'} \\mathbb{E}_{x\u223cX} [\\mathbb{P} (\u03c0 > \u03c0' | x)] .$, (5.4)\nThe von Neumann winner is a concept drawn from social choice theory (Sen, 1986) and has been studied in preference-based RL (Owen, 2013; Dud\u00edk et al., 2015). It is the Nash equilibrium of the two-player symmetric game (Equation 5.4). It represents a mixed strategy\u2014a probability distribution over possible responses\u2014that performs optimally in the worst-case scenario against any opponent.\nMore formally speaking, a distribution $\u03c0^*$ is called a von Neumann winner if it satisfies:\n$\\min_{\u03c0'\u2208\u0394} \\mathbb{E}_{x\u223cX} [\\mathbb{P} (\u03c0^* > \u03c0' | x)] \u2265 1/2.$\nThis condition ensures that, on average, the von Neumann winner $\u03c0^*$ is at least as likely to be preferred than any other policy $\u03c0'$. The von Neumann winner always exists due to the symmetric nature of the two-player game (Equation 5.4)."}, {"title": "Efficient Policy Optimization", "content": "To align language models with the general preference, previous works (Wu et al., 2024b; Rosset et al., 2024; Liu et al., 2024; Swamy et al., 2024) have proposed iterative training frameworks that include 1) sampling from the current LLMs multiple responses $y_1, y_2,..., y_K$; 2) using a general preference model to label the preference among the K responses; 3) aggregating the preferences into different learning objectives.\nA key advantage of our model is its computational efficiency. The previous general preference models require O(K2) inference-time compute to evaluate all pairwise preferences among K responses, as each pair is represented by a different concatenated sequence from (x, yi, yj) to predict $\\mathbb{P}(y_i > y_j|x)$. In contrast, computing the preference representation for K responses requires only O(K) forward passes / inference-time compute: we first calculate the representation vi for each yi, and then use them to calculate the preference probability between any two responses using formula $s(y_i > y_j) = (R_\\rangle v_i, v_j)$. This way, our model is as efficient as a reward model while being way more expressive."}, {"title": "General Preference Optimization (GPO)", "content": "Policy Optimization with Preference Score. Once we have a general preference model that outputs the preference score $s(y_i > y_j|x)$ at hand, we aim to find a policy $\u03c0$ that performs well against an opponent policy $\u03bc$ in terms of expected preference scores. The optimization problem is formulated as:\n$\\max_\u03b8 \\mathbb{E}_x [\\mathbb{E}_{y\u223c\u03c0_\u03b8(x), y'\u223c\u03bc(\u22c5|x)} [s(y > y' | x)]] \u2013 \u03b2\\mathbb{E}_x [KL (\u03c0_\u03b8(\u22c5 | x)||\u03c0_{ref}(\u22c5 | x))],$ (5.5)\nwhere $\u03c0_{ref}$ is a reference policy (e.g., the initial language model), $\u03bc$ is the opponent policy (usually the same as $\u03c0_{ref}$), and $\u03b2 > 0$ is a regularization parameter controlling the divergence from the reference policy. We would like to point out that this formulation is different from the many previous works (Wu et al., 2024b; Swamy et al., 2024; Rosset et al., 2024; Munos et al., 2023; Azar et al., 2023) as they consider maximizing the win rate $\\mathbb{P}(y > y'|x)$, while our formulation is to maximize $s(y > y'|x) = \\log(\\frac{P(y>y'|x)}{1-P(y>y'|x)})$. Note that $\\mathbb{P}(y > y'|x)$ only varies between 0 and 1, while $s(y > y'|x)$, similar to the reward $r(y; x)$ in RLHF or DPO, can take arbitrary values. The flexibility in its value range might benefit fine-tuning.\nGeneral Preference Optimization. We consider the SPPO loss used by Wu et al. (2024b) for iterative preference optimization, except that we use preference score instead of preference probability in the loss form. SPPO used K responses for each prompt x and calculated the empirical win rate of each response yk. Instead, we calculate $\u015d(y_i > \u03bc | x)$ to estimate the empirical win rate over the distribution \u03bc as below:\n$\u015d (y_i > \u03bc | x) = \\frac{1}{K} \u2211_{k=1}^K s (y_i > y_k | x), \u2200 i \u2208 [K],$,  (5.6)\nAt each iteration t, GPO has the following learning objective:\n$\u03c0_{\u03b8_{t+1}} = arg \\min_{\u03c0_{\u03b8}} \\mathbb{E}_{x\u223cX, y\u223c\u03c0_{\u03b8_t}} \\Big[ \\log(\\frac{\u03c0_\u03b8(y | x)}{\u03c0_{\u03b8_t}(y | x)}) \\Big( \\frac{1}{\u03b2} \u015d(y > \u03c0_{\u03b8_t} | x) - log \\frac{\u03c0_\u03b8(y | x)}{\u03c0_{\u03b8_t}(y | x)} \\Big) \\Big]$,   (5.7)"}, {"title": "Experiments", "content": "We conducted extensive experiments to evaluate the effectiveness of the proposed General Preference representation model (GPM) in comparison to traditional reward-based models, particularly focusing on its ability to model cyclic preferences and improve language model alignment. Our experiments are designed to address the following key questions:\n\u2022 Q1: Can the GPM effectively capture and model cyclic and intransitive preferences, where traditional models like the Bradley-Terry (BT) reward model struggle?\n\u2022 Q2: How does the GPM perform on standard preference modeling benchmarks (RewardBench) compared to the BT model?\n\u2022 Q3: How does using the GPM for downstream policy optimization impact language model performance on real-world tasks compared to reward-based approaches?"}, {"title": "Cyclic Preference Modeling", "content": "To address Q1, we evaluate the ability of the GPM to capture intransitive, cyclic preferences that traditional transitive models (like the BT model) struggle to represent.\nCyclic Preference Dataset. We constructed a dataset by inducing cyclic preferences from the Ultrafeedback dataset Cui et al. (2024). The dataset includes responses evaluated across four key metrics: instruction following, honesty, truthfulness, and helpfulness. We created preference cycles such as: instruction following > honesty > truthfulness > helpfulness > instruction following, ensuring the presence of intransitive cycles. We further generated four sub-datasets by omitting one metric from each cycle, resulting in datasets of varying complexity with 216 to 363 instances.\nTraining and Evaluation. We trained the GPM using the Gemma-2B-it language model as the base and evaluated the models based on their ability to predict the human-provided preferences in these datasets. Since cyclic preferences are inherently intransitive, we measure accuracy as the percentage of correctly predicted human preferences, where higher scores indicate better handling of non-transitive preferences."}, {"title": "Experiments on RewardBench", "content": "To address Q2, we compare the GP representation model and the BT reward model on the RewardBench benchmark (Lambert et al., 2024), which covers diverse preference modeling tasks, including Chat, Chat-Hard, Safety, and Reasoning.\nDatasets and Experimental Setup. We train both the BT and GPMs using the Skywork Reward Data Collection (Liu & Zeng, 2024), which contains 80,000 pairwise preference examples from tasks in various domains. We evaluate both models on RewardBench, using two different base models: Gemma-2B-it (Team et al., 2024) (2B parameters) and Llama-3.1-8B-Instruct (Dubey et al., 2024) (8B parameters), which are well-suited for instruction-following tasks (see Appendices A.2 and B.2 for the implementation details and experimental setup).\nResults and Analysis. The GPM consistently outperforms the BT model for both base models on RewardBench, with notable improvements in tasks involving complex reasoning (e.g., Chat-Hard and Reasoning). These results highlight the superior expressiveness of the GPM in preference modeling.\nAblation Studies. We further conducted ablation studies to assess the impact of varying the representation (embedding) dimension in the GPM."}, {"title": "Downstream Performance on Aligning Language Models with Human Preferences", "content": "To address Q3, we investigate the effectiveness of the GPM in language model for alignment using Self-Play Policy Optimization (SPPO) (Wu et al., 2024b) and our proposed General Preference Optimization (GPO).\nExperimental Setup. We fine-tuned language models using SPPO and GPO, integrating preference scores provided by our GP representation model (GPM). We evaluated the models on AlpacaEval 2.0 (Dubois et al., 2024) and MT-Bench (Zheng et al., 2023), two widely used benchmarks for evaluating LLM alignment.\nResults and Analysis. The evaluation results on the benchmarks are as follows. For AlpacaEval 2.0, we compared the generated responses of the aligned models with those of GPT-4-turbo. To avoid the preference bias when using GPT-4-turbo as the evaluator, we also used DeepSeek-V2 (DeepSeek-AI, 2024) and GPT-40-mini as the evaluators besides GPT-4-turbo itself. Notice that the Length Controlled (LC) Win Rate results are using a generalized linear model fitted using default evaluator GPT-4-turbo, so it does not apply to other evaluators."}, {"title": "Conclusion", "content": "This work introduced preference representation learning, a framework for modeling human preferences that can capture complex, intransitive structures like cyclic preferences. The proposed General Preference representation model (GPM) achieves linear complexity while maintaining the ability to model intricate preference relationships. It consistently outperforms traditional models like Bradley-Terry and supervised preference models across various benchmarks, including cyclic preference datasets and real-world tasks from RewardBench. Additionally, incorporating preference scores from GPM into policy optimization methods, such as SPPO and the newly introduced General Preference Optimization (GPO), led to significant performance improvements in downstream tasks that require alignment with intricate human preferences, as demonstrated in benchmarks like AlpacaEval 2.0 and MT-Bench."}, {"title": "Limitations", "content": "Despite the current interesting experimental results, further exploration is still needed. The choice of representation (embedding) dimensions and model architecture can influence the expressiveness and computational efficiency of GPM. An overly expressive model such as a high embedding dimension can impact performance, particularly in tasks prone to overfitting. Future works should examine the expressiveness of preference representations in a wider range of real-world datasets."}, {"title": "Complex Representations Interpretation", "content": "Our model can also be interpreted using complex representations. By representing the representations as complex vectors $v_y \u2208 \u2102^k$, we can express the preference score as:\n$s(y_i > y_j | x) = Im <<v_y_i, v_y_j>>,$", "where": "Im(.) denotes the imaginary part, and <<.,.>> is the Hermitian inner product. This formulation captures cyclic and intransitive preferences through the angular relationships between complex presentations."}, {"title": "Expressiveness of Complex Preference Representations", "content": "Let $P \u2208 \u211d^{k\u00d7k}$ be a real skew-symmetric matrix (i.e., $P = \u2212P^T$). Then, there exist complex vectors {$v_i$}$_i=1^k$, $v_i$ \u2208 $\u2102^k$ such that:\n$P_{ij} = Im <<v_i, v_j>>, \u2200 i, j."}, {"title": "Implementing General Preference Representation Model", "content": "When the preference score matrix $P$ has an even dimension, i.e., $P \u2208 \u211d^{2k\u00d72k}$, we have a more interesting interpretation based on spectral decomposition."}, {"title": "Expressiveness of Preference Representation Model", "content": "Let $P \u2208 \u211d^{2k\u00d72k}$ be a real skew-symmetric matrix (i.e., $P = \u2212P^T$). Then there exist representations (embeddings) {$V_i$}$_i=1^{2k}$ $\u2208 \u211d^{2k}$ and a block-diagonal skew-symmetric matrix $R_\\rangle \u2208 \u211d^{2k\u00d72k}$, with $R_\\rangle$ consisting of k blocks of the form:\n$R_l = \\begin{bmatrix} 0 & -1 \\\\ 1 & 0 \\\\ \\end{bmatrix}, l = 1, ..., k.$"}, {"title": "Additional Experimental Results", "content": "Ablations on Scale Gate and Embedding head. We investigate the effects of scale gates and embedding head dimensions, with and without L2 normalization, on model performance. As shown in Table 7, for Gemma-2B-it models, incorporating a scale gate generally enhances GPM performance across various embedding dimensions. L2 normalization on the embedding head output consistently improves models with scale gates. Interestingly, Gemma-2B-it-based models without L2 normalization or scale gates outperform those with L2 normalization but no scale gates. A plausible explanation for this phenomenon is that removing L2 normalization introduces additional degrees of freedom, particularly beneficial for models with smaller parameter spaces and high-dimensional embedding layers. This increased flexibility may allow the model to better utilize its limited parametric capacity, potentially leading to enhanced expressiveness and task-specific adaptability.\nFor larger models, such as those based on Llama3.1-8B-Instruct, the impact of scale gates becomes less pronounced. This diminished effect may be attributed to the inherently stronger representational capacity of the 8B parameter model, which can likely capture complex patterns more effectively without additional architectural modifications.\nThese observations suggest a nuanced relationship between model size, normalization techniques, and architectural enhancements like scale gates, highlighting the importance of considering these factors in model design and optimization.\nMore Results on Evaluating Language Model Alignment. We further conduct a rigorous evaluation of our downstream task-specific models using various benchmarks. Alpaca Eval 2.0 evaluation results are listed in Table 4 and Table 5, using GPT-4-turbo and Deepseek-V2 as evaluators respectively. For LM-Harness, we chose Arc-Challenge, TruthfulQA, WinoGrande, GSM8k, HellaSwag, and MMLU as the evaluation tasks, and used the default rule-based evaluator of lm-evaluation-harness for accuracy calculation. These tasks are the same as those evaluated by Open LLM Leaderboard v1 (Beeching et al., 2023), which no longer provides service. Notice that the evaluator of LM-Harness is not the same as Open LLM Leaderboard v1, so we only compare different models within our results and do not compare them with the Open LLM Leaderboard v1. The results are showed in Tables 8 and 9. To facilitate direct comparison with current state-of-the-art models, we adhere to the evaluation protocol established by the Open LLM Leaderboard v1. Our models are evaluated locally using this standardized framework."}, {"title": "Implementation Details", "content": "Details on Training Setup. Our experiments on RewardBench and Cyclic Preference Dataset were implemented using the HuggingFace Transformers library (Wolf et al., 2020) and the OpenRLHF framework (Hu et al., 2024). For reward model training on Skywork Reward Data Collection, we employed the following settings:\n\u2022 Gemma-2B-it: Trained with a learning rate of 1 \u00d7 10\u22125.\n\u2022 Llama-3.1-8B-Instruct: Trained with a learning rate of 2 \u00d7 10-6.\n\u2022 Training Configuration: Both models were trained for two epochs with a global batch size of 32. We used a cosine learning rate scheduler with a warm-up ratio of 0.03. Input sequences were truncated to a maximum length of 2048 tokens.\n\u2022 Hyperparameters: For the Bradley-Terry (BT) model, the temperature parameter $\u03b2$ was set to 1, following standard practice (Rafailov et al., 2024). For our General Preference (GP) model, we set $\u03b2 = 0.1$, determined via hyperparameter tuning on a validation set.\n\u2022 Hardware: All experiments were conducted on machines equipped with NVIDIA A800 80GB GPUs, utilizing 8 GPUs per experiment.\nFor cyclic preference experiments, the training settings are as follows, except for the parameters specified below; all other experimental parameters remain consistent with experiments on RewardBench:\n\u2022 Gemma-2B-it: Trained with a learning rate of 1 \u00d7 10\u20136.\n\u2022 Training Configuration: Models were trained for 50 epochs with a global batch size of 1.\n\u2022 Hardware: Experiments were conducted on machines equipped with NVIDIA A800 80GB GPUs, utilizing a single GPU per experiment.\nDetails on Evaluation Dataset RewardBench. RewardBench is divided into four core sections:\n\u2022 Chat: Evaluates the ability to differentiate between thorough and correct responses in open-ended conversations, using data from AlpacaEval (Li et al., 2023) and MT Bench (Zheng et al., 2023).\n\u2022 Chat-Hard: Tests the handling of trick questions and subtle instruction differences, using adversarial examples from MT Bench and LLMBar (Zeng et al., 2024).\n\u2022 Safety: Assesses the capacity to refuse harmful content appropriately, using data from XSTest (R\u00f6ttger et al., 2024), Do-Not-Answer (Wang et al., 2024), and a custom AI2 dataset.\n\u2022 Reasoning: Measures code generation and reasoning abilities, with prompts from HumanEval-Pack (Muennighoff et al., 2023) and PRM800k (Lightman et al., 2023)."}, {"title": "More on General Preference Optimization", "content": "Connection to Policy Gradient. Applying policy gradient theorem on Equation (5."}]}