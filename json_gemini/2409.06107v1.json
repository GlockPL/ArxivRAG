{"title": "Doppelg\u00e4nger's Watch\nA Split Objective Approach to Large Language Models", "authors": ["Shervin Ghasemlou", "Seungwhan Moon", "Ashish Katiyar", "Babak Damavandi", "Aparajita Saraf", "Mangesh Pujari", "Pinar Donmez", "Anuj Kumar"], "abstract": "In this paper, we investigate the problem of \"generation supervision\" in large\nlanguage models, and present a novel bicameral architecture to separate supervision\nsignals from their core capability, helpfulness. Doppelg\u00e4nger, a new module\nparallel to the underlying language model, supervises the generation of each token,\nand learns to concurrently predict the supervision score(s) of the sequences up to\nand including each token. In this work, we present the theoretical findings, and\nleave the report on experimental results to a forthcoming publication.", "sections": [{"title": "Introduction", "content": "Large Language Models (LLMs) have become indispensable in a variety of applications, where\nensuring their response quality has become a paramount concern. While helpfulness is perceived\nas the core capability of language models, other supervisory signals like sentiment analysis (Xing,\n2024; Zhan et al., 2024), factual correctness (Gunjal and Durrett, 2024), bias (Chen et al., 2024),\nand compliance (Weidinger et al., 2023; Shelby et al., 2023; Bianchi et al., 2023) are among the\nobjectives that language models can be evaluated for. However, achieving high performance on these\nobjectives - which we refer to as supervision signal in the rest of the paper - without compromising\nthe acquired helpfulness is an emerging critical concern. The challenge lies in achieving a balance\nbetween these different objectives and helpfulness, as enhancements in one of these aspects could\noften lead to compromises in the other (Bai et al., 2022).\nIn this paper, we propose an expansion of the Transformer architecture by Vaswani et al. (2017) into\na bicameral one. We introduce a parallel component, termed the Doppelg\u00e4nger, which supervises the\ngeneration process of the tokens, concurrently evaluating the supervision score of input queries and\ngenerated responses. The architecture is agnostic to what supervisory objective it is trained for.\nContrary to most recent approaches (Touvron et al., 2023; Ge et al., 2023; OpenAI, 2024; Jiang\net al., 2023; Gemini, 2024), where enhancing such supervisory signals is a fine-tuning step post\nthe pretraining of the base model, our approach distinctly separates optimizing for these objective\nfrom helpfulness - keeping the LLM's helpfulness intact. This separation, as demonstrated through\na mathematical proof in Section 4 offers several advantages over previous methods. Our approach\ndoes not necessitate large-scale training as the language component of this architecture remains static\nduring Doppelg\u00e4nger's training \u2013 hence providing the Doppelg\u00e4nger with its understanding of the"}, {"title": "Previous work", "content": "Responses generated by an LLMs unchecked for supervisory signals can have significant negative\nimpact on their overall performance(Feng et al., 2023; Barman et al., 2024; Qu et al., 2023; Li et al.,\n2023; Shokri et al., 2017). The extension to multiple modalities, such as vision, audio, and inertial\nmodalities increases such impact due to potential for mismatches between modalities (Gou et al.,\n2024; Moon et al., 2023). Setting methods like self evaluation (Ren et al., 2023), adding external\nclassifiers (Kim et al., 2023; Moon et al., 2023), and prompt optimization (Wu et al., 2024; Diao et al.,\n2023) aside, improving other supervisory signals usually is done in a similar fashion to fine tuning\nfor helpfulness. Among these approaches are supervised instruction fine-tuning(SFT) (Ouyang et al.,\n2022), reinforcement learning from human feedback (RLHF) (Dai et al., 2023). While both methods\nhave shown promise, these methods do not address the inverse relation between the objectives of\nhelpfulness and other supervisory signals as reported by Bai et al. (2022).\nModifications to the architectures of language models with the aim of improving different objectives\nindependently, is an understudied field. LoRA (Hu et al., 2021) allows fine-tuning without retraining\nthe entire model, however, training LoRA for other objective changes the behavior of the model,\nresulting in deteriorated helpfulness. Adding a classification head to the model (Arora et al., 2022)\nhelps with redirecting token generation to more desired outputs, but requires retraining the underlying\nlanguage model for other objectives, which similarly, causes performance regression(Bai et al., 2022).\nTo the best of our knowledge, ours is the first study to separate the optimization of supervisory signals\nfrom helpfulness, while keeping the generative capabilities of the language model untouched."}, {"title": "Architecture", "content": "In this section, we present our new bicameral architecture, which consists of the language component,\nfor which here we use a decoder only transformer, and the Doppelg\u00e4nger component, which is similar\nin architecture and gets input from the language component at the end of each attention module, to\nsupervise the inputs and generated outputs."}, {"title": "A bicameral architecture", "content": "As Figure 2 Right illustrates, the architecture consists of two main components, the language\ncomponent and the Doppelg\u00e4nger. The attention modules are sequentially connected, and the\noutputs from the last layer of the language component are normalized and fed into a linear language\nhead for sampling and token generation. Similarly, the outputs from the last layer of the supervision\ncomponent(Doppelg\u00e4nger) are normalized and fed into a linear classification head for score prediction.\nThe idea is, as we explain in more details in Section 4, to use a pretrained model for the language\ncomponent that is not fine tuned for the aspect that the Doppelg\u00e4nger component is to supervise,\ne.g, factual correctness. We keep the language component untouched \u2013 the language component\nwill stay frozen during the training time. The architecture keeps the auto-regressive behaviour of\nthe underlying language model, where each generated token is appended to the concatenation of the\ninput and previously generated sequence of tokens, to be used for the generation of the next token.\nThe Doppelg\u00e4nger component, is an almost identical component. For simplicity, we call the attention\nmodules of the Doppelg\u00e4nger component the shadow modules. There is one major difference\nbetween the two components: while each module in the language component accepts input from the\nprevious attention module only, the inputs to the shadow modules are a linear transformation of the\nconcatenation of the outputs from the previous attention module and the outputs from the previous\nshadow module. The exception is the first module in both components which as input receive the\nembeddings of the input tokens. Since Doppelg\u00e4nger probes into the language component through\nthese connections, it will be able to utilize its language understanding in predicting the supervision\nscores. Depending on the supervision task, this could mean Doppelg\u00e4nger can have a much smaller\nnumber of parameters by avoiding replicating the language capabilities of the language component."}, {"title": "Multi Objective Optimization in Language Models", "content": "In this section, we provide a set of definitions, examples, and a lemma along its proof, to establish the\ntheoretical foundations of the proposed architecture."}, {"title": "Definitions", "content": "Definition 4.1 (Language Function) Let T be an embedding space of tokens, and S a Cartesian\nproduct of a set of n Cartesian spaces Si such that $S = \\prod_{i=1}^{n} S_i$. We define a Language function L\nas:\n$L:T^* \\rightarrow S$\nsuch that for any given input sequence $(t_1, t_2, ..., t_m) \\in T^*$, where m is the length of the sequence,\nthe output is a set ${s_i|s_i \\in S_i, 0 < i < n}$.\nDefinition 4.2 (Extended Language Function) We define a language function $L : T^* \\rightarrow S$ with n\noutputs as an extended language function, denoted by $L_{\\theta_1,\\theta_2,\u2026\u2026,\\theta_n}$, where each $\\theta_i, 0 < i < n$ is a set\nof independent parameters that solely are used in determining the value of $s_i E S$.\nLet's provide a few examples.\nExample 4.1 Most Language models are sequence to sequence models, where $n = 1$ and $S = T^*$,\nwhere the model takes in sequences, and outputs sequences.\nExample 4.2 For the bicameral architecture introduced in section 3, we have $n = 2$ and $S = T^* \\times R$,\nwhere the model takes in a sequence, and outputs both another sequence, and a score.\nIn this example, the parameters in the language head only contribute to the generation of the output\nsequence, and the ones in the Doppelg\u00e4nger solely contribute to predicting the score. The rest of the\nparameters are common between the two. Now let's define reward functions.\nDefinition 4.3 (Reward Function) A reward function $R_s$, maps any value from the range of a\nlanguage function to a real number r:\n$R_s: S\\rightarrow R$\nDefinition 4.4 (Composite Reward Function) A function that is the result of mapping some input\nreward functions $R_1, R_2, ... R_k$, under some map M into a single reward function is called a\nComposite Rewards Function, denoted by CR:\n$CR = M(R_1, R_2, ... R_k), M : R^* \\rightarrow R$\nNote that this definition poses no restriction on how the reward functions are composed.\nUsing the definitions above, now we can present the following lemma."}, {"title": "Conclusion and future work", "content": "This research introduces an innovative bicameral architecture for large language models designed to\noversee the generation process while maintaining helpfulness. The proposed method incorporates a\nparallel component named Doppelg\u00e4nger, which is designed to monitor the generation process and\nassess it for some supervisory score. Theoretical results suggest that this approach is expected to\nperform at least as well as, if not better than, existing methods currently in use in the field. Detailed\nexperimental results will be presented in a forthcoming paper."}, {"title": "Appendix", "content": "A.1 Split Objective Supremacy: Proof and Theoretical Implications\nIn this section we first provide a proof for the lemma presented in Section 4. We also further explain\nand provide context for this lemma and the ideas shaping it.\nProof A.1 Given s = Le(t) we have\n$CR(L_o(t)) = CR(R_{s_1}(s_{s_1}), R_{s_2}(s_{s_2}),..., R_{s_n}(s_{s_n}))$\nwhere $s_{s_i}$ is the image of s on $S_i$. Assume 0 is in an optimum state with respect to CR, which implies\nCR is the Pareto front for all of its composing reward functions $R_{s_i}(s_{s_i})$ \u2013 so no $R_{s_i}(s_{s_i})$ can be\noptimized further without making at least one other $R_{s_j}(s_{s_j}),j \\neq i$ worse. Given that, another\nextended language function $L_{\\theta_1,\\theta_2,...,\\theta_n} : T^* \\rightarrow S_1 \\times S_2 \\times . . . \\times S_n$ can be built where each $\\theta_i$ is\noptimized with respect to $R_{s_i}$ independently, where an optimum state of $\\theta_i$ is at least as optimum as\nthe state of 0 with respect to $R_{s_i}$. They can be equally optimum when either $\\theta_i = 0$, or, when 0 not\nonly is optimum with respect to CR, but also it is optimum with respect to $R_{s_i}$. Therefor, for each\n$R_{s_i}, 0 < i < n$, given $(s_1, s_2, ..., s_n) = L_{\\theta_1,\\theta_2,...,\\theta_m}(t)$, we have:\n$R_{s_i}(s_{s_i}) \\leq R_{s_i}(s_i)$\nConsequently, since CR is monotonic with respect to the composing reward functions, we can infer:\n$CR(L_o(t)) \\leq CR(L_{\\theta_1,\\theta_2,...,\\theta_n}(t))$\nIf 0 is not in an optimum state with respect to CR, then, a fortiori, the argument holds.\nA common practice for optimizing LLMs for different objectives has been to assign each a reward\nfunction which could involve human evaluators. The language model usually has only one kind of\noutput generated, a sequence of tokens, where any metric, including helpfulness, should be inferred\nfrom that output \u2013 which in Lemma 4.1 we denote such single outputs by s. The projections of it on\neach $S_i$ subspace for each objective, is $s_{s_i}$. The model is optimized with respect to some combination\nof the reward functions, which we model by composite reward functions, and can take place through\npretraining, fine-tuning, or other approaches described in Section 2. The composite reward function\ndescribed in Section 4 is defined such that it is agnostic to how the reward functions are combined\nor in what order applied. As long as the composite reward function is monotonic in respect to the\ncomposing reward functions the lemma holds. The monotonicity assumption, is not a limiting one as\nin real world applications, usually better helpfulness, better factual correctness or better other metrics,\nmean a better language model.\nLemma 4.1 states that we can almost always build a model that has a different set of parameters,\neach yielding a separate output, per each objective(i.e., reward function) that would yield a higher\ncomposite reward in comparison to single output alternatives, using the exact same reward functions\nand composite reward function. The only exception, mathematically speaking, is when the objective\nfunctions are all separable and the set of parameters in the single output model, 0, is in an optimum\nstate for all the objective functions, in which case the two language models would perform the same\nwith respect to given composite reward functions. In such a model, all parameter sets $\\theta_i$ should be\nindependent from the others, and should be optimized with respect to their corresponding reward\nfunctions.\nWhile the bicameral architecture presented in Section 3 is designed based on the same idea, it should\nbe pointed out that this specific architecture is not necessarily the best architecture that can achieve\nsplit objective supremacy."}, {"title": "Discussion", "content": "This work introduces a novel bicameral architecture aimed at enhancing the supervision of language\nmodels without altering their inherent language capabilities. The core idea is to replace the traditional\nfine-tuning process with a supervision mechanism that operates in parallel to the generative component.\nThis allows the model to predict supervision scores for both inputs and generated tokens concurrently\nduring generation.\nThe current work is also centered around the idea that large language models, pre-trained on large cor-\npus of data are inherently helpful. The architecture leverages this idea to preserve the helpfulness and\navoid the adverse effects reported in other studies Bai et al. (2022), by introducing the Doppelg\u00e4nger\ncomponent. This component is designed to be trained to supervise the original, \"untouched\" language\nmodel, enabling it to generate content freely while still being under supervision. The supervision can\nbe learned through labeled training data, which helps the Doppelg\u00e4nger component to effectively\nscore or classify both inputs and outputs. The probes from the language component to the Doppel-\ng\u00e4nger, also enable it to predict or classify at the same time as token generation. Without such probes,\nthe Doppelg\u00e4nger would be able to predict only up to the token before the one that is being gener-\nated, and therefor, always a token behind. In contrast to approaches like generator classifiers(Arora\net al., 2022), where the language component remains frozen and only late fusion classification heads\nare used, our bicameral approach does not compromise the language capabilities. Unfreezing the\nlanguage component or integrating a LoRAHu et al. (2021) component could potentially enhance\nsupervision but at the cost of altering the language capabilities. The proposed architecture, however,\nis expected learn and predict supervisory signals without modifying the language component, as\nsupported by our mathematical results.\nA significant limitation of this study is the absence of experimental results, which are currently being\nconducted and will be detailed in a forthcoming paper. Future work could also expand this architecture\nto include bidirectional interactions between the language and Doppelg\u00e4nger components. This\nwould be particularly advantageous in real-time user interactions, allowing the system to dynamically\nadjust and redirect the responses based on supervisory scores."}]}