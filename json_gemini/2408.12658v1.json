{"title": "HIERARCHICAL GENERATIVE MODELING OF MELODIC VOCAL CONTOURS IN HINDUSTANI CLASSICAL MUSIC", "authors": ["Nithya Shikarpur", "Krishna Maneesha Dendukuri", "Yusong Wu", "Antoine Caillon", "Cheng-Zhi Anna Huang"], "abstract": "Hindustani music is a performance-driven oral tradition that exhibits the rendition of rich melodic patterns. In this paper, we focus on generative modeling of singers' vocal melodies extracted from audio recordings, as the voice is musically prominent within the tradition. Prior generative work in Hindustani music models melodies as coarse discrete symbols which fails to capture the rich expressive melodic intricacies of singing. Thus, we propose to use a finely quantized pitch contour, as an intermediate representation for hierarchical audio modeling. We propose GaMaDHaNi, a modular two-level hierarchy, consisting of a generative model on pitch contours, and a pitch contour to audio synthesis model. We compare our approach to non-hierarchical audio models and hierarchical models that use a self-supervised intermediate representation, through a listening test and qualitative analysis. We also evaluate audio model's ability to faithfully represent the pitch contour input using Pearson correlation coefficient. By using pitch contours as an intermediate representation, we show that our model may be better equipped to listen and respond to musicians in a human-AI collaborative setting by highlighting two potential interaction use cases (1) primed generation, and (2) coarse pitch conditioning.", "sections": [{"title": "1. INTRODUCTION", "content": "Hindustani music is a performance-driven music tradition that has a high level of melodic intricacy [1]. Despite the recent advances in generative modeling for music [2, 3], this genre remains difficult to model for several reasons including (1) a lack of a readily available and widely accepted abstract representation reflecting the genre faithfully (like Western symbolic notation), (2) as a niche musical form, the scarcity of available datasets restricts the ability to model the raw waveform directly.\nSymbolic notation is a well-defined discrete representation of music including lead sheet, MIDI, piano roll, text, and markup language. Musical notation used in Hindustani pedagogy uses a similar discrete representation by highlighting the prominent notes which fails to faithfully capture the fine melodic intricacies connecting these notes as seen in Fig. 1. Previous work on generative modeling for Hindustani music has side-stepped the lack of well-defined abstract representations with two methods: (1) using musical notation from textbooks or music theory [4\u20136], (2) leveraging MIDI extracted from audio [7, 8]. However, both methods ignore the rich melodic ornamentation present in this music. Computational analyses for the genre have addressed the difficulty in data representation by using the fundamental frequency contour, hereby referred to as 'pitch', as an intermediate representation for several melodic tasks including music style classification [9], motif discovery and matching [10\u201312] and raga recognition [13\u201315]. With evidence that pitch faithfully represents the melody for computational tasks, we are motivated to incorporate it in the context of generative modeling.\nIn this work, we present GaMaDHaNi\u00b9 (Generative Modular Design of Hierarchical Networks), a modular hierarchical generative model for Hindustani singing. We employ a two-level hierarchy of data representation in"}, {"title": "2. RELATED WORK", "content": "2.1 Music Representations in Indian Art Music\nPast work on melody-based computational tasks for Indian Art Music include music style classification [9], motif discovery and matching [10\u201312], and raga recognition [14\u201316]. Previous work shows that fine quantization outperforms coarse quantization in pitch contours for tasks including raga recognition [16, 17] and motif matching [11]. Thus motivated by their ability to capture melodic information we use finely quantized pitch as an intermediate representation. Additionally, for Carnatic music, previous work on compact representations for Gamakas (type of note ornamentation) [18], and non-uniform pitch quantization schemes that can preserve raga-characteristics [19, 20] present forms of representation that are more condensed than the pitch contour while being adequately detailed which could be an interesting inclusion for future work.\n2.2 Generative Modeling for Hindustani Music\nHindustani music is an improvised form of music where melodic movements are guided by a melodic framework (raga) [1]. Past work on the generation of this music is of two types: rule-based and data-driven models. AI-Raga [4] is a rule-based AI system developed to generate musical notation of compositions and improvisations that adhere to raga grammar based on an elaborate set of rules termed 'generative theory of music' [21]. Another work develops a Finite State Machine (FSM) to generate improvisations based on raga-specific melodic movements situated in theory [5]. An initial attempt at data-driven models learned from the musical notation of alaps, i.e. slow improvisation, in textbooks using bigrams in an FSM [6]. RMMM [7] explores the use of LSTM [22] and transformer-based [23] architectures to generate MIDI extracted from a corpus of Hindustani music. Other work also proposes generating MIDI with GANs [8, 24]. All models discussed in this section approach modeling data as solfege notation. While doing so, one gives up on the transitory melodic regions between notes of the melody, which is inherent to Hindustani music. AI-Raga [4] partially addresses this by using domain-informed tuning systems, and a simulation of transitory glides between notes. We propose to address this problem by incorporating a fine pitch data representation. Additionally, in contrast to previous work, we propose to generate audio waveform rather than symbolic data.\n2.3 Hierarchical Audio Generation\nWithin the domain of music generation, hierarchical learning offers two distinct advantages: enhanced learning abilities on data-constrained tasks and multi-level controllabil-ity. MIDI-DDSP [25] takes advantage of the hierarchy in"}, {"title": "3. METHOD", "content": "In this work, we seek a generative model for Hindustani vocal music by learning the joint distribution of amplitude mel-spectrograms s and pitch f following\n$$p(s, f) = p(s|f)p_\\theta(f),$$\nwhere $p_\\phi$ and $p_\\theta$ are parameterized with neural networks called Spectrogram and Pitch Generators respectively. The generated spectrogram is converted to audio using a vocoder. Pitch conditioning f to $p_\\theta$ is taken from our dataset for training and sampled from $p_\\theta$ for inference.\n3.1 Pitch Generator\nWe study the modeling of vocal pitch as the primary component in our hierarchical generation pipeline. Vocal pitch f are represented as integer-valued sequences sampled at 100Hz, with 90% of the values ranging from 86Hz to 899Hz, quantized with a fine resolution of 10 cents. To model such sequences, we investigate two distinct methods. The first employs an autoregressive, language-like model to predict the discrete pitch sequence, whereas the second leverages recent advancements in diffusion-based modeling for iterative generation of the entire sequence.\n3.1.1 Discrete autoregressive model\nWe use a vanilla decoder-only transformer, to autoregressively predict the next token of a pitch sequence. In this"}, {"title": "task, the pitch values f are considered to be discrete tokens in a vocabulary V, each mapped to an embedding vector of size d through an embedding matrix $E \\in \\mathbb{R}^{|V|\\times d}$. The model is trained with cross-entropy loss.", "content": "3.1.2 Continuous diffusion model\nWe use a simple yet effective diffusion variant, Iterative \u03b1-Deblending (IADB) [35] as the training objective of our model that generates finely quantized pitch f. IADB defines a simplified diffusion process that is a linear interpolation between noise $x_0 \\sim X_0 = \\mathcal{N}(0, I)$ and data $x_1 \\sim X_1 = X_{data}$:\n$$x_\\alpha = (1 - \\alpha) x_0 + \\alpha x_1.$$\nWe leverage a deterministic iterative deblending process proposed in [35] to sample a data point $x_1 \\sim X_1$ from noise $x_0 \\sim X_0$. With the total number of iterations in the process as T, and given a time step $t \\in \\{0, 1, 2, ..., T\\}$, we define the blending parameter $a_t = \\frac{t}{T}$ and an \u03b1-blended point $x_{a_t}$. Thus, the iterative deblending is defined as:\n$$x_{a_{t+1}} = (1 - a_{t+1}) \\hat{x}_0 + a_{t+1} \\hat{x}_1,$$\nwhere $(\\hat{x}_0, \\hat{x}_1) = \\mathbb{E}(X_0, X_1 | x_{a_t}, a_t)$ is the expected value of the posterior samples given $x_{a_t}, a_t$. Heitz et. al. [35] show that using expected posteriors $\\hat{x}_0, \\hat{x}_1$ in the deblending process (Eq. 3) instead of $x_0, x_1$ converges to the same point, while making the sampling process deterministic.\nTaking the derivative of $x_a$, with respect to the blending parameter $a_t$, the training objective becomes,\n$$D_\\theta(x_{a_t} | a_t) \\approx \\frac{d x_{a_t}}{d a_t} = (\\hat{x}_1 - \\hat{x}_0),$$\nTaking a trained model $D_\\theta$, we perform an iterative sampling procedure to generate outputs:\n$$x_{a_{t+1}} = x_{a_t} + (a_{t+1} - a_t) D_\\theta(x_{a_t}, a_t),$$\n3.2 Spectrogram Generator\nOn the next level of the hierarchy, we train a model to generate a spectrogram conditioned on pitch, which is then converted to an audio signal using a vocoder. This method uses IADB as described in Sec. 3.1.2, while additionally conditioned on singer and pitch. Each singer ID is embedded as a discrete vector, and the processed pitch is time-downsampled to match the spectrogram's time axis. Both conditioning signals are concatenated as additional channels to the mel-spectrogram input. Thus given a conditioning signal c, the training objective $D_\\theta(x_{a_t} | a_t, c)$ is similar to Eq. 4 but is additionally conditioned on c.\nThe singer and pitch values are conditioned using classifier-free guidance (CFG) [36]. Given a conditioning strength w, CFG is implemented such that $D_\\theta(x_{a_t} | c)$ is used during the iterative sampling, defined as,\n$$D(x_{a_t} | a_t, c) = (1-w) D_\\theta(x_{a_t} | a_t) + w D_\\theta(x_{a_t} | a_t, c)$$"}, {"title": "4. EXPERIMENTS", "content": "In this paper, we consider the Spectrogram Generator as a tool to convert melodic ideas from the Pitch Generator into perceivable audio. As a result, we evaluate both the Generators with a focus on quality of pitch generation and the spectrogram's fidelity in representing that pitch.\nThrough our experiments, we aim to motivate our choices for (1) a hierarchical approach to generation, (2) the use of pitch as an intermediate representation, through listening tests. We also qualitatively evaluate the overall melodic quality of generations. Additionally, we assess the Spectrogram Generator by testing pitch adherence: the ability of the model to reliably reproduce the pitch conditioning through quantitative and qualitative analyses. We leave evaluation of other aspects of the Spectrogram Generator such as audio quality, singer adherence to future work. Readers are encouraged to listen to relevant supplementary audio samples on our project website while going through this and the following sections.\n4.1 Dataset\nWe use a combination of the Saraga and Hindustani Raga Recognition datasets [37, 38]. Audio files in the combined dataset contain audio of vocal performances including the tanpura, i.e. a drone, along with the melodic and rhythmic accompaniment across 56 unique singers. It spans about 120 hours across 362 audio files, where the files range from 88 seconds (s) to 1.2 hours with a median duration of 20 minutes. The dataset is randomly split into training and validation sets at a 90:10 ratio. Furthermore, each audio file is split into 60 s segments resulting in 7174 and 719 segments in the training and validation sets respectively. Due to different inductive biases in the models used, they all have different receptive fields and are thus trained on sequences with lengths varying from 8.2 s-12 s, randomly sampled from the 60 s segments during training.\nThe vocals are isolated using 2-stem source separation with HT Demucs [39] and further, the pitch is extracted using CREPE [40] and is sampled at 100 Hz. We algorithmically reduce the number of pitch detection errors using a loudness-based pitch filtering approach; using a sliding window to calculate area under the loudness curve, we retain only corresponding pitch values exceeding an empirically set threshold. We normalize the pitch to a logarithmic scale such that an arbitrarily chosen frequency, 440Hz is 0 on this scale, and quantize it into 10-cent bins. Additionally, during training, the pitch is transposed by a random multiple of 10 cents within a range of [-400, 400] cents.\nArtifacts in the dataset Our source separation model, HT Demucs [39], allows some leakage from other instruments including mainly the sarangi (stringed melodic accompaniment) and the tabla (rhythmic accompaniment) as artifacts in the vocal stem due to the out of distribution nature of Hindustani music data for the model. These 'leaked' sounds are generated in our models too (both our proposed model and the baselines established). Additionally, instances of speech are found in some generated samples as it is present in our dataset. The Carnatic FTA-Net [41], presents a domain-informed model trained to extract pitch contours from Carnatic vocal audio. Owing to the similarities between Carnatic and Hindustani music, an interesting direction for future work would be to adopt their methodologies in our data processing pipeline.\n4.2 Model Architectures\nBelow we present model specific architectures and data preprocessing for the Pitch Generators (Autoregressive and Diffusion) and the Spectrogram Generator.\nPitch Generator (Discrete Autoregressive) This model was trained on 12s (1200 token) sequences. The quantized pitch f is converted into a sequence of discrete embedding vectors e, using an embedding space $E \\in \\mathbb{R}^{|V|\\times d}$ where effective vocabulary size is |V| = 796 and embedding dimension is d = 512. The model is a decoder-only transformer [23] with 8 layers, with each layer having an output dimension of 512. AliBi positional method [42] is used to encode the position of tokens in the sequence. A cosine learning schedule with linear warm-up is used. Samples are generated with a temperature of 0.99 and using top k sampling with k=40.\nPitch Generator (Continuous Diffusion) This model was trained on 10.24s (1024 elements) sequences. The quantized pitch contour is limited to a range of 400 integers. This distribution is converted into a continuous Gaussian using the quantile function which maps a variable's probability distribution to another probability distribution. This model is implemented as a U-Net with three down-sampling and upsampling layers each with a stride of 4, 2 and 2 respectively. Each layer is made of four 1-D convolution layers with weight normalization [43] and Mish non-linearity [44]. The bottleneck involves 4 attention layers with 8 heads each.\nSpectrogram Generator This model is trained on 8.2s (512 elements) of mel-spectrogram sequences. The relevant pitch conditioning is linearly interpolated and down-sampled to match the sequence length of the spectral data. The spectral data is produced with 192 mels and a hop size of 256 (0.016 s) given 16 kHz audio and is converted to a continuous Gaussian distribution using the quantile transform function as well. Apart from additional channels for singer and pitch conditioning, the architecture is the same as that used by the Pitch Generator (Continuous Diffusion) (Sec 4.2). For simplicity, spectrograms are converted to audio using the Griffin-Lim algorithm [45]. Future work could harness the power of recent developments in neural vocoders including HiFi-GAN [46].\n4.2.1 Conditioning signals\nIn addition to pitch, the Spectrogram Generator utilizes singer conditioning to help maintain the consistency of the voice in generated audio as seen in the supplementary audio samples. Each singer is assigned a unique ID and mapped to an embedding vector of size $d_{singer} = 128$. Conditioning was implemented with CFG as discussed in Sec. 3.2 with a strength of w = 3 for pitch and singer conditioning. This value was determined based on empirical"}, {"title": "studies as an optimal balance between fidelity to pitch and minimizing artifacts due to incorrect pitch extraction.", "content": "4.3 Baseline Models\nThrough our baseline models, we aim to motivate two major architectural choices: (1) hierarchy in the model and (2) an intermediate pitch representation. These models thus include a non-hierarchical baseline, a hierarchical baseline with a self-supervised intermediate representation (hierarchical Encodec baseline), and the ground truth.\nNon-hierarchical Baseline In this baseline, we highlight a naive approach of modeling audio directly with no hierarchy. We train a diffusion model with the IADB objective directly on processed audio mel-spectrograms. The model architecture is similar to other diffusion models used in this paper (Sec. 4.2) and was trained on the same dataset as our model with sequences of length 8.2s.\nHierarchical Encodec Baseline We train a hierarchical autoregressive baseline on a self-supervised intermediate representation, Encodec [26]. Through this model, we aim to compare the effect of self-supervised and pitch intermediate representations. To this end, we train MSPrior [47, 48], a decoder-only transformer adapted for real-time use, on Encodec tokens [26] extracted using the 24 kHz Encodec model with a target bandwidth of 3 kbps (4 channels per token). This model was trained on only the Hindustani Raga Recognition Dataset (which constitutes about 1/3th of our dataset) with a sequence length of 900 (12 s). We use a temperature of 0.99 for sampling.\nGround Truth To set the gold standard of melodic quality, we use ground truth pitch for comparison. As the listening test focuses on evaluating the Pitch Generator, we standardize audio quality across all models (except the hierarchical Encodec baseline which already generates waveform) by synthesizing the ground truth pitch with our Spectrogram Generator. We use five singers (3 low and 2 high voice range) with reasonable representation in the dataset as singer conditioning. Depending on the range of the generated pitch, we randomly select from the appropriate set of singers to generate audio for the contour.\n4.4 Human Evaluation on Melodic Quality\nTo evaluate the musical quality and characteristics of generated samples, we conduct a listening study and offer qualitative observations supported by audio examples in our supplementary material.\nListening study We compare five systems: non-hierarchical baseline, hierarchical Encodec baseline, autoregressive and diffusion variants of our method, and ground truth. Participants were presented with 8.2 s audio samples, from two random systems and asked to rate which one is more musically interesting, on a 5-point Likert scale. We recruited 15 participants who are trained in Hindustani or Carnatic music. Although Carnatic music is stylistically different from Hindustani music, the two share the context of raga and tala giving participants enough context to evaluate samples for this study. Participants' primary instruments were the voice or other melodic instruments including"}, {"title": "4.5 Pitch Adherence in Spectrogram Generator", "content": "Although the Spectrogram Generator loss lacks an explicit term for pitch adherence, we evaluate it by calculating the"}, {"title": "5. INTERACTION USE CASES", "content": "We show two interactive use cases of GaMaDHaNi: (1) continuing an input melodic sequence or 'prime', and (2) guiding generation with coarse solfege-like notation.\n5.1 Primed Generation\nWe investigate using our model for melodic sequence continuation. To this end, we input a four-second pitch sequence from our dataset termed \u2018prime' into our Pitch Generator, and ask the model to continue the sequence. The model can generate realistic-sounding continuations with"}, {"title": "5.2 Coarse Pitch Conditioning", "content": "To explore further possibilities for interaction, we evaluate the model's ability to adhere to solfege-like conditioning given to the Pitch Generator. To this end, a \u2018coarse pitch' signal is inferred by calculating a moving average of the pitch with a window size of 1s and a hop size of 0.01s. The Pearson correlation coefficient between the input and generated coarse pitch is 0.97, and between the ground truth and generated pitch is 0.79. Both values are averaged over 64 random samples from the validation set. Thus solfege input, once converted into a similar coarse pitch signal, can be used to guide the model's generation as seen in Fig. 5, where the model renders a solfege-based descending scale into realistic-sounding audio. Although simple, this is an interesting avenue for interactive generation that we plan to explore in the future."}, {"title": "6. CONCLUSION", "content": "We present a modular hierarchical system to generate melodically rich Hindustani vocal audio using a relatively small dataset. Our model has comparable or better performance than established baselines while including an interpretable intermediate pitch representation. We present interesting forms of interaction including primed generations and coarse pitch conditioning that could be developed further to achieve interactive human-machine music making.\nThere are interesting future directions such as the use of tonic, raga and rhythmic aspects as conditioning for generation. Additionally, the Spectrogram Generator could adopt more advanced vocoders and conditioning signals such as loudness and phoneme features for better results."}, {"title": "7. ETHICS STATEMENT", "content": "This work, to our knowledge, is the first model trained to explicitly generate Hindustani vocal music and thus we find it important to emphasize that this work is intended to foster human-AI collaboration, creating a more accessible environment for creative exploration and is by no means intended to replace music teachers or musicians. While we acknowledge the ethical concerns involved in modeling singing voices, we include singer conditioning in our approach with the sole intention of maintaining voice consistency in the generated samples. Additionally, we note that this work utilizes datasets contributed by artists or institutes holding distribution rights to ensure responsible use with informed consent. These datasets were released with appropriate permissions to process audio recordings for research purposes. However, despite our current model's limited scope, future enhancements may pose a risk of mimicking the identities of existing singers, necessitating the establishment of protective guidelines for artists."}]}