{"title": "HIERARCHICAL GENERATIVE MODELING OF MELODIC VOCAL\nCONTOURS IN HINDUSTANI CLASSICAL MUSIC", "authors": ["Nithya Shikarpur", "Krishna Maneesha Dendukuri", "Yusong Wu", "Antoine Caillon", "Cheng-Zhi Anna Huang"], "abstract": "Hindustani music is a performance-driven oral tradition\nthat exhibits the rendition of rich melodic patterns. In this\npaper, we focus on generative modeling of singers' vocal\nmelodies extracted from audio recordings, as the voice is\nmusically prominent within the tradition. Prior generative\nwork in Hindustani music models melodies as coarse dis-\ncrete symbols which fails to capture the rich expressive\nmelodic intricacies of singing. Thus, we propose to use\na finely quantized pitch contour, as an intermediate rep-\nresentation for hierarchical audio modeling. We propose\nGaMaDHaNi, a modular two-level hierarchy, consisting\nof a generative model on pitch contours, and a pitch con-\ntour to audio synthesis model. We compare our approach\nto non-hierarchical audio models and hierarchical mod-\nels that use a self-supervised intermediate representation,\nthrough a listening test and qualitative analysis. We also\nevaluate audio model's ability to faithfully represent the\npitch contour input using Pearson correlation coefficient.\nBy using pitch contours as an intermediate representation,\nwe show that our model may be better equipped to listen\nand respond to musicians in a human-AI collaborative set-\nting by highlighting two potential interaction use cases (1)\nprimed generation, and (2) coarse pitch conditioning.", "sections": [{"title": "1. INTRODUCTION", "content": "Hindustani music is a performance-driven music tradition\nthat has a high level of melodic intricacy [1]. Despite the\nrecent advances in generative modeling for music [2, 3],\nthis genre remains difficult to model for several reasons\nincluding (1) a lack of a readily available and widely ac-\ncepted abstract representation reflecting the genre faith-\nfully (like Western symbolic notation), (2) as a niche mu-\nsical form, the scarcity of available datasets restricts the\nability to model the raw waveform directly.\nSymbolic notation is a well-defined discrete representa-\ntion of music including lead sheet, MIDI, piano roll, text,\nand markup language. Musical notation used in Hindustani\npedagogy uses a similar discrete representation by high-\nlighting the prominent notes which fails to faithfully cap-\nture the fine melodic intricacies connecting these notes as\nseen in Fig. 1. Previous work on generative modeling\nfor Hindustani music has side-stepped the lack of well-\ndefined abstract representations with two methods: (1) us-\ning musical notation from textbooks or music theory [4-6],\n(2) leveraging MIDI extracted from audio [7, 8]. How-\never, both methods ignore the rich melodic ornamentation\npresent in this music. Computational analyses for the genre\nhave addressed the difficulty in data representation by us-\ning the fundamental frequency contour, hereby referred\nto as 'pitch', as an intermediate representation for sev-\neral melodic tasks including music style classification [9],\nmotif discovery and matching [10-12] and raga recogni-\ntion [13-15]. With evidence that pitch faithfully represents\nthe melody for computational tasks, we are motivated to\nincorporate it in the context of generative modeling.\nIn this work, we present GaMaDHaNi\u00b9 (Generative\nModular Design of Hierarchical Networks), a modular hi-\nerarchical generative model for Hindustani singing. We\nemploy a two-level hierarchy of data representation in-"}, {"title": "2. RELATED WORK", "content": "2.1 Music Representations in Indian Art Music\nPast work on melody-based computational tasks for In-\ndian Art Music include music style classification [9], mo-\ntif discovery and matching [10-12], and raga recognition\n14-16]. Previous work shows that fine quantization out-\nperforms coarse quantization in pitch contours for tasks in-\ncluding raga recognition [16, 17] and motif matching [11].\nThus motivated by their ability to capture melodic infor-\nmation we use finely quantized pitch as an intermediate\nrepresentation. Additionally, for Carnatic music, previ-\nous work on compact representations for Gamakas (type of\nnote ornamentation) [18], and non-uniform pitch quantiza-\ntion schemes that can preserve raga-characteristics [19,20]\npresent forms of representation that are more condensed\nthan the pitch contour while being adequately detailed\nwhich could be an interesting inclusion for future work.\n2.2 Generative Modeling for Hindustani Music\nHindustani music is an improvised form of music where\nmelodic movements are guided by a melodic framework\n(raga) [1]. Past work on the generation of this music is of\ntwo types: rule-based and data-driven models. AI-Raga [4]\nis a rule-based AI system developed to generate musical\nnotation of compositions and improvisations that adhere to\nraga grammar based on an elaborate set of rules termed\n'generative theory of music' [21]. Another work develops\na Finite State Machine (FSM) to generate improvisations\nbased on raga-specific melodic movements situated in the-\nory [5]. An initial attempt at data-driven models learned\nfrom the musical notation of alaps, i.e. slow improvisation,\nin textbooks using bigrams in an FSM [6]. RMMM [7] ex-\nplores the use of LSTM [22] and transformer-based [23]\narchitectures to generate MIDI extracted from a corpus of\nHindustani music. Other work also proposes generating\nMIDI with GANs [8,24]. All models discussed in this sec-\ntion approach modeling data as solfege notation. While\ndoing so, one gives up on the transitory melodic regions\nbetween notes of the melody, which is inherent to Hindus-\ntani music. AI-Raga [4] partially addresses this by using\ndomain-informed tuning systems, and a simulation of tran-\nsitory glides between notes. We propose to address this\nproblem by incorporating a fine pitch data representation.\nAdditionally, in contrast to previous work, we propose to\ngenerate audio waveform rather than symbolic data.\n2.3 Hierarchical Audio Generation\nWithin the domain of music generation, hierarchical learn-\ning offers two distinct advantages: enhanced learning abil-\nities on data-constrained tasks and multi-level controllabil-"}, {"title": "3. METHOD", "content": "In this work, we seek a generative model for Hindustani\nvocal music by learning the joint distribution of amplitude\nmel-spectrograms s and pitch f following\n$$p(s, f) = p(s|f)p_\\theta(f),$$\nwhere $p_\\phi$ and $p_\\theta$ are parameterized with neural networks\ncalled Spectrogram and Pitch Generators respectively.\nThe generated spectrogram is converted to audio using a\nvocoder. Pitch conditioning f to $p_\\theta$ is taken from our\ndataset for training and sampled from $p_\\theta$ for inference.\n3.1 Pitch Generator\nWe study the modeling of vocal pitch as the primary com-\nponent in our hierarchical generation pipeline. Vocal pitch\nf are represented as integer-valued sequences sampled at\n100Hz, with 90% of the values ranging from 86Hz to\n899Hz, quantized with a fine resolution of 10 cents. To\nmodel such sequences, we investigate two distinct meth-\nods. The first employs an autoregressive, language-like\nmodel to predict the discrete pitch sequence, whereas the\nsecond leverages recent advancements in diffusion-based\nmodeling for iterative generation of the entire sequence.\n3.1.1 Discrete autoregressive model\nWe use a vanilla decoder-only transformer, to autoregres-\nsively predict the next token of a pitch sequence. In this\ntask, the pitch values f are considered to be discrete tokens\nin a vocabulary V, each mapped to an embedding vector\nof size d through an embedding matrix $E \\in \\mathbb{R}^{|V| \\times d}$. The\nmodel is trained with cross-entropy loss.\n3.1.2 Continuous diffusion model\nWe use a simple yet effective diffusion variant, Iterative\na-Deblending (IADB) [35] as the training objective of our\nmodel that generates finely quantized pitch f. IADB de-\nfines a simplified diffusion process that is a linear inter-\npolation between noise $x_0 \\sim X_0 = \\mathcal{N}(0, 1)$ and data\n$x_1 \\sim X_1 = X_{\\text{data}}$:\n$$x_\\alpha = (1 - \\alpha)x_0 + \\alpha x_1.$$\nWe leverage a deterministic iterative deblending pro-\ncess proposed in [35] to sample a data point $x_1 \\sim X_1$ from\nnoise $x_0 \\sim X_0$. With the total number of iterations in the\nprocess as T, and given a time step $t \\in \\{0, 1, 2, ..., T\\}$,\nwe define the blending parameter $a_t = \\frac{t}{T}$ and an $\\alpha$-\nblended point $x_{a_t}$. Thus, the iterative deblending is de-\nfined as:\n$$x_{a_{t+1}} = (1 - a_{t+1}) \\hat{x}_0 + a_{t+1}\\hat{x}_1,$$", "type": "equation"}, {"title": null, "content": "where $(\\hat{x}_0, \\hat{x}_1) = \\mathbb{E}(X_0, X_1 | x_{a_t}, a_t)$ is the expected value\nof the posterior samples given $x_{a_t}, a_t$. Heitz et. al. [35]\nshow that using expected posteriors $\\hat{x}_0, \\hat{x}_1$ in the deblend-\ning process (Eq. 3) instead of $x_0, x_1$ converges to the same\npoint, while making the sampling process deterministic.\nTaking the derivative of $x_{a_t}$ with respect to the blending\nparameter $a_t$, the training objective becomes,\n$$D_\\theta(x_{a_t}, a_t) \\approx \\frac{dx_{a_t}}{da_t} = (\\hat{x}_1 - \\hat{x}_0),$$\nTaking a trained model $D_\\theta$, we perform an iterative sam-\npling procedure to generate outputs:\n$$x_{a_{t+1}} = x_{a_t} + (a_{t+1} - a_t) D_\\theta(x_{a_t}, a_t),$$\n3.2 Spectrogram Generator\nOn the next level of the hierarchy, we train a model to gen-\nerate a spectrogram conditioned on pitch, which is then\nconverted to an audio signal using a vocoder. This method\nuses IADB as described in Sec. 3.1.2, while additionally\nconditioned on singer and pitch. Each singer ID is embed-\nded as a discrete vector, and the processed pitch is time-\ndownsampled to match the spectrogram's time axis. Both\nconditioning signals are concatenated as additional chan-\nnels to the mel-spectrogram input. Thus given a condition-\ning signal c, the training objective $D_\\theta(x_{a_t}|a_t, c)$ is similar\nto Eq. 4 but is additionally conditioned on c.\nThe singer and pitch values are conditioned using\nclassifier-free guidance (CFG) [36]. Given a conditioning\nstrength w, CFG is implemented such that $D(x_{a_t}|C)$ is\nused during the iterative sampling, defined as,\n$$D(x_{a_t}|a_t, c) = (1-w)D_\\theta(x_{a_t}|a_t)+WD_\\phi(x_{a_t}|a_t, c)$$"}, {"title": "4. EXPERIMENTS", "content": "In this paper, we consider the Spectrogram Generator as\na tool to convert melodic ideas from the Pitch Generator\ninto perceivable audio. As a result, we evaluate both the\nGenerators with a focus on quality of pitch generation and\nthe spectrogram's fidelity in representing that pitch.\nThrough our experiments, we aim to motivate our\nchoices for (1) a hierarchical approach to generation, (2)\nthe use of pitch as an intermediate representation, through\nlistening tests. We also qualitatively evaluate the overall\nmelodic quality of generations. Additionally, we assess\nthe Spectrogram Generator by testing pitch adherence: the\nability of the model to reliably reproduce the pitch condi-\ntioning through quantitative and qualitative analyses. We\nleave evaluation of other aspects of the Spectrogram Gen-\nerator such as audio quality, singer adherence to future\nwork. Readers are encouraged to listen to relevant supple-\nmentary audio samples on our project website while going\nthrough this and the following sections.\n4.1 Dataset\nWe use a combination of the Saraga and Hindustani Raga\nRecognition datasets [37,38]. Audio files in the combined\ndataset contain audio of vocal performances including the\ntanpura, i.e. a drone, along with the melodic and rhythmic\naccompaniment across 56 unique singers. It spans about\n120 hours across 362 audio files, where the files range from\n88 seconds (s) to 1.2 hours with a median duration of 20\nminutes. The dataset is randomly split into training and\nvalidation sets at a 90:10 ratio. Furthermore, each audio\nfile is split into 60 s segments resulting in 7174 and 719\nsegments in the training and validation sets respectively.\nDue to different inductive biases in the models used, they\nall have different receptive fields and are thus trained on\nsequences with lengths varying from 8.2 s-12 s, randomly\nsampled from the 60 s segments during training.\nThe vocals are isolated using 2-stem source separation\nwith HT Demucs [39] and further, the pitch is extracted\nusing CREPE [40] and is sampled at 100 Hz. We algorith-\nmically reduce the number of pitch detection errors using\na loudness-based pitch filtering approach; using a sliding\nwindow to calculate area under the loudness curve, we re-\ntain only corresponding pitch values exceeding an empiri-\ncally set threshold. We normalize the pitch to a logarithmic\nscale such that an arbitrarily chosen frequency, 440Hz is 0\non this scale, and quantize it into 10-cent bins. Addition-\nally, during training, the pitch is transposed by a random\nmultiple of 10 cents within a range of [-400, 400] cents.\nArtifacts in the dataset Our source separation model,\nHT Demucs [39], allows some leakage from other instru-\nments including mainly the sarangi (stringed melodic ac-\ncompaniment) and the tabla (rhythmic accompaniment) as\nartifacts in the vocal stem due to the out of distribution\nnature of Hindustani music data for the model. These\n'leaked' sounds are generated in our models too (both our\nproposed model and the baselines established). Addition-\nally, instances of speech are found in some generated sam-\nples as it is present in our dataset. The Carnatic FTA-Net"}, {"title": "4.2 Model Architectures", "content": "Below we present model specific architectures and data\npreprocessing for the Pitch Generators (Autoregressive and\nDiffusion) and the Spectrogram Generator.\nPitch Generator (Discrete Autoregressive) This\nmodel was trained on 12s (1200 token) sequences. The\nquantized pitch f is converted into a sequence of discrete\nembedding vectors e, using an embedding space $E \\in\n\\mathbb{R}^{|V| \\times d}$ where effective vocabulary size is |V| = 796 and\nembedding dimension is d = 512. The model is a decoder-\nonly transformer [23] with 8 layers, with each layer having\nan output dimension of 512. AliBi positional method [42]\nis used to encode the position of tokens in the sequence.\nA cosine learning schedule with linear warm-up is used.\nSamples are generated with a temperature of 0.99 and us-\ning top k sampling with k=40.\nPitch Generator (Continuous Diffusion) This model\nwas trained on 10.24s (1024 elements) sequences. The\nquantized pitch contour is limited to a range of 400 inte-\ngers. This distribution is converted into a continuous Gaus-\nsian using the quantile function which maps a variable's\nprobability distribution to another probability distribution.\nThis model is implemented as a U-Net with three down-\nsampling and upsampling layers each with a stride of 4, 2\nand 2 respectively. Each layer is made of four 1-D con-\nvolution layers with weight normalization [43] and Mish\nnon-linearity [44]. The bottleneck involves 4 attention lay-\ners with 8 heads each.\nSpectrogram Generator This model is trained on 8.2s\n(512 elements) of mel-spectrogram sequences. The rele-\nvant pitch conditioning is linearly interpolated and down-\nsampled to match the sequence length of the spectral data.\nThe spectral data is produced with 192 mels and a hop size\nof 256 (0.016 s) given 16 kHz audio and is converted to a\ncontinuous Gaussian distribution using the quantile trans-\nform function as well. Apart from additional channels for\nsinger and pitch conditioning, the architecture is the same\nas that used by the Pitch Generator (Continuous Diffusion)\n(Sec 4.2). For simplicity, spectrograms are converted to\naudio using the Griffin-Lim algorithm [45]. Future work\ncould harness the power of recent developments in neural\nvocoders including HiFi-GAN [46]."}, {"title": "4.2.1 Conditioning signals", "content": "In addition to pitch, the Spectrogram Generator utilizes\nsinger conditioning to help maintain the consistency of the\nvoice in generated audio as seen in the supplementary au-\ndio samples. Each singer is assigned a unique ID and\nmapped to an embedding vector of size $d_{\\text{singer}} = 128$.\nConditioning was implemented with CFG as discussed in\nSec. 3.2 with a strength of w = 3 for pitch and singer con-\nditioning. This value was determined based on empirical"}, {"title": "4.3 Baseline Models", "content": "Through our baseline models, we aim to motivate two ma-\njor architectural choices: (1) hierarchy in the model and (2)\nan intermediate pitch representation. These models thus\ninclude a non-hierarchical baseline, a hierarchical baseline\nwith a self-supervised intermediate representation (hierar-\nchical Encodec baseline), and the ground truth.\nNon-hierarchical Baseline In this baseline, we high-\nlight a naive approach of modeling audio directly with no\nhierarchy. We train a diffusion model with the IADB ob-\njective directly on processed audio mel-spectrograms. The\nmodel architecture is similar to other diffusion models used\nin this paper (Sec. 4.2) and was trained on the same dataset\nas our model with sequences of length 8.2s.\nHierarchical Encodec Baseline We train a hierarchi-\ncal autoregressive baseline on a self-supervised intermedi-\nate representation, Encodec [26]. Through this model, we\naim to compare the effect of self-supervised and pitch in-\ntermediate representations. To this end, we train MSPrior\n[47, 48], a decoder-only transformer adapted for real-time\nuse, on Encodec tokens [26] extracted using the 24 kHz\nEncodec model with a target bandwidth of 3 kbps (4 chan-\nnels per token). This model was trained on only the Hin-\ndustani Raga Recognition Dataset (which constitutes about\n1/5th of our dataset) with a sequence length of 900 (12 s).\nWe use a temperature of 0.99 for sampling.\nGround Truth To set the gold standard of melodic\nquality, we use ground truth pitch for comparison. As\nthe listening test focuses on evaluating the Pitch Genera-\ntor, we standardize audio quality across all models (except\nthe hierarchical Encodec baseline which already generates\nwaveform) by synthesizing the ground truth pitch with our\nSpectrogram Generator. We use five singers (3 low and\n2 high voice range) with reasonable representation in the\ndataset as singer conditioning. Depending on the range of\nthe generated pitch, we randomly select from the appropri-\nate set of singers to generate audio for the contour."}, {"title": "4.4 Human Evaluation on Melodic Quality", "content": "To evaluate the musical quality and characteristics of gen-\nerated samples, we conduct a listening study and offer\nqualitative observations supported by audio examples in\nour supplementary material.\nListening study We compare five systems: non-hierarchical baseline, hierarchical Encodec baseline, au-\ntoregressive and diffusion variants of our method, and\nground truth. Participants were presented with 8.2 s audio\nsamples, from two random systems and asked to rate which\none is more musically interesting, on a 5-point Likert scale.\nWe recruited 15 participants who are trained in Hindustani\nor Carnatic music. Although Carnatic music is stylistically\ndifferent from Hindustani music, the two share the context\nof raga and tala giving participants enough context to eval-\nuate samples for this study. Participants' primary instru-\nments were the voice or other melodic instruments includ-"}, {"title": "Results", "content": "Fig. 3 shows the number of wins in each\nsystem. We ran a Kruskal-Wallis H test and confirmed\nthat there are statistically significant pairs among the\ncombinations. According to a post-hoc analysis using\nthe Wilcoxon signed-rank test with Bonferroni correction\n(with p < 0.05/10), we find that our hierarchical model\nwith an autoregressive Pitch Generator outperforms the\nnon-hierarchical baseline. Given the small sample size,\nwe also compare all systems against each other by aggre-\ngating ratings and considering them as independent sam-\nples. Using the Independent (Mann-Whitney U) test with\nBonferroni correction, we find that both our models, dis-\ncrete autoregressive and continuous diffusion outperform\nthe non-hierarchical baseline significantly. Through these\nexperiments, we establish that our model outperforms the\nnon-hierarchical baseline."}, {"title": "Diversity in Generation", "content": "Participants did not prefer our\nmethods significantly more than the hierarchical Encodec\nbaseline. This baseline tends to hold a single note or move\nthrough a few stable notes without much dynamism. This\nunderstandably was preferred by participants as vilambit\nalap or slower improvisation, a common way to establish a\nraga in Hindustani music, involves the use of such long and\nstable notes. With only 8.2 s duration audio samples, the\nlisteners do not have enough time to notice the lack of dy-\nnamic movement. In contrast, our proposed methods can\nrender both slow and fast movements, resulting in more\nvariety as seen in generated samples. We hypothesize that\nthis could be due to the different intermediate representa-\ntions of both models, i.e. due to the importance of intricate\nmelodic movements, a model trained to explicitly generate\nfine pitch would be able to capture melodic complexity."}, {"title": "Consistency of vocal timbre", "content": "We note that generations\nfrom the hierarchical model, which includes singer condi-\ntioning, display more consistency in the timbre of voice;\nthe baseline models sometimes abruptly switch vocal tim-\nbre in the middle of generation."}, {"title": "4.5 Pitch Adherence in Spectrogram Generator", "content": "Although the Spectrogram Generator loss lacks an explicit\nterm for pitch adherence, we evaluate it by calculating the"}, {"title": "5. INTERACTION USE CASES", "content": "We show two interactive use cases of GaMaDHaNi: (1)\ncontinuing an input melodic sequence or 'prime', and (2)\nguiding generation with coarse solfege-like notation.\n5.1 Primed Generation\nWe investigate using our model for melodic sequence con-\ntinuation. To this end, we input a four-second pitch se-\nquence from our dataset termed \u2018prime' into our Pitch Gen-\nerator, and ask the model to continue the sequence. The\nmodel can generate realistic-sounding continuations with"}, {"title": "5.2 Coarse Pitch Conditioning", "content": "To explore further possibilities for interaction, we evaluate\nthe model's ability to adhere to solfege-like conditioning\ngiven to the Pitch Generator. To this end, a \u2018coarse pitch'\nsignal is inferred by calculating a moving average of the\npitch with a window size of 1s and a hop size of 0.01s. The\nPearson correlation coefficient between the input and gen-\nerated coarse pitch is 0.97, and between the ground truth\nand generated pitch is 0.79. Both values are averaged over\n64 random samples from the validation set. Thus solfege\ninput, once converted into a similar coarse pitch signal, can\nbe used to guide the model's generation as seen in Fig. 5,\nwhere the model renders a solfege-based descending scale\ninto realistic-sounding audio. Although simple, this is an\ninteresting avenue for interactive generation that we plan\nto explore in the future."}, {"title": "6. CONCLUSION", "content": "We present a modular hierarchical system to generate\nmelodically rich Hindustani vocal audio using a relatively\nsmall dataset. Our model has comparable or better perfor-\nmance than established baselines while including an inter-\npretable intermediate pitch representation. We present in-\nteresting forms of interaction including primed generations\nand coarse pitch conditioning that could be developed fur-\nther to achieve interactive human-machine music making.\nThere are interesting future directions such as the use of\ntonic, raga and rhythmic aspects as conditioning for gen-\neration. Additionally, the Spectrogram Generator could\nadopt more advanced vocoders and conditioning signals\nsuch as loudness and phoneme features for better results."}, {"title": "7. ETHICS STATEMENT", "content": "This work, to our knowledge, is the first model trained\nto explicitly generate Hindustani vocal music and thus we\nfind it important to emphasize that this work is intended to\nfoster human-AI collaboration, creating a more accessible\nenvironment for creative exploration and is by no means\nintended to replace music teachers or musicians. While\nwe acknowledge the ethical concerns involved in modeling\nsinging voices, we include singer conditioning in our ap-\nproach with the sole intention of maintaining voice consis-\ntency in the generated samples. Additionally, we note that\nthis work utilizes datasets contributed by artists or insti-\ntutes holding distribution rights to ensure responsible use\nwith informed consent. These datasets were released with\nappropriate permissions to process audio recordings for re-\nsearch purposes. However, despite our current model's\nlimited scope, future enhancements may pose a risk of\nmimicking the identities of existing singers, necessitating\nthe establishment of protective guidelines for artists."}]}