{"title": "Grounding Language in Multi-Perspective Referential Communication", "authors": ["Zineng Tang", "Lingjun Mao", "Alane Suhr"], "abstract": "We introduce a task and dataset for referring expression generation and comprehension in multi-agent embodied environments. In this task, two agents in a shared scene must take into account one another's visual perspective, which may be different from their own, to both produce and understand references to objects in a scene and the spatial relations between them. We collect a dataset of 2,970 human-written referring expressions, each paired with human comprehension judgments, and evaluate the performance of automated models as speakers and listeners paired with human partners, finding that model performance in both reference generation and comprehension lags behind that of pairs of human agents. Finally, we experiment training an open-weight speaker model with evidence of communicative success when paired with a listener, resulting in an improvement from 58.9 to 69.3% in communicative success and even outperforming the strongest proprietary model.", "sections": [{"title": "Introduction", "content": "Language agents embodied in situated interactions alongside human users must be able to reason jointly about the space they occupy, the language they encounter, and their human partner's perception. For example, consider a home assistant robot that is assisting its human user in finding their lost keys. This system must take into account its previous and current observations of the space, as well as estimate what the user's current perspective is like in the shared environment. If the system generates a description of the keys' location that the user clearly and unambiguously understands, they have achieved communicative success. Both the speaker and listener must take into account the physical relationship between objects, their own view of the environment, and an estimate of the other person's perspective in the environment.\nWe study human-human and human-agent referential communication in photorealistic 3D environments, introducing a platform that supports generating task instances with varying levels of difficulty. In contrast to most prior work on referring expression generation and comprehension, we focus on the setting where both agents are physically embodied in a scene but with different perspectives of the scene. We collect a dataset of 2,970 human-written referring expressions grounded in 1,485 generated scenes. We evaluate several recent vision-and-language models on the tasks of referring expression generation and comprehension."}, {"title": "Task and Environment", "content": "We study the task of embodied referential communication, where two agents coordinate their attention in a shared scene using referring expressions. To this end, we design a platform that for generating photorealistic 3D scenes that support this task at varying levels of difficulty."}, {"title": "Embodied Referential Communication", "content": "We use a reference game, where a speaker describes a target referent, and a listener attempts to identify the target using the speaker's description. In our task, two agents are physically embodied in the same shared 3D scene, but with different perspectives, and thus different observations of the scene. Each scene includes candidate referent objects, one of which is a target object that the speaker needs to communicate to the listener. Communicative success is achieved if the listener is able to identify the speaker's intended target.\nFormally, let O be the set of possible agent observations, each represented as a 2D image; R be the set of candidate referents in an scene, and X be the set of possible referring expressions. A speaker model $p_s : O \\times R^N \\times \\{1...N\\} \\rightarrow \\triangle^*$ maps from an observation of the shared scene, a set of referents, and the index of the target referent $r_t$ to a distribution over possible referring expressions. A listener model $p_l : O \\times R^N \\times X \\rightarrow \\triangle^{\\{1...N\\}}$"}, {"title": "Scene Generation", "content": "Formally, we denote a scene S = (e, ps, pi, R, t) as an environment e \u2208 E populated with two agents ps and \u03c1\u03b9 and N referents R, as well as the index of the target referent rt. To generate a scene, we first sample a base environment, then place the two agents, then the candidate referents. Finally, we render each agent's observation of the scene."}, {"title": "Controlled Difficulty", "content": "We implement two ways to control the difficulty of referential communication via scene generation: by manipulating the relative orientation of speaker and listener, and by adversarially placing referents.\nSpeaker-listener orientation. The relative orientation of the speaker ps and listener p\u012b is the absolute difference $\u03c8' = min(|\u03c8_s \u2013 \u03c8_l|, 360\u00b0 \u2013 |\u03c8_s \u2013 \u03c8_l|)$ of their horizontal rotations (yaw). We experiment with the influence of \u03c8' on interaction dynamics. When \u03c8' is close to 0\u00b0, the two agents are facing the same direction, and their observations are likely to be similar to one another. When \u03c8' is close to 180\u00b0, the agents are facing each other and thus have completely different views of the same scene.\nAdversarial placement of referents. We design a referent placement policy model $R : C^* \\times O_S \\times P_S\\times P_L \\rightarrow \\triangle^{R^N \\times\\{1...N\\}}$, which takes as input a set of empty coordinates C, the speaker's observation prior to referent placement, and both agent poses. It generates a distribution over referent locations prior to the physics simulation, and over referent indices representing the target. The policy model is implemented as a vision transformer and is trained to maximize the communicative failure rate between two fixed agent models, ps and pl, by optimizing\n$max \\mathbb{E}_{(R',t')\\sim R(.)} [1 - Success(p_s, p_l, o_s, o_l, R', t')]$,\nwhere os and ol are the agents' observations after referents R are placed. During scene generation, we use the trained policy to sample initial positions of referents, then apply gravitational physics to find the resting position of each referent."}, {"title": "Experimental Setup", "content": "We use our scene generation platform to evaluate embodied, multi-perspective referential communication with pairs of agents including humans and automated models."}, {"title": "Data", "content": "We generate a set of 27,504 scenes for training and evaluating automated agents. We recruit crowd-workers to participate in the task both as listeners and speakers, collecting a dataset of 2,970 human-written referring expressions paired with human listener selections in 1,485 of these scenes.\nScene generation. We use ScanNet++ (non-commercial license), which contains 450 high-quality 3D indoor environments, as the basis of our task instances. We generate scenes using both forms of controlled difficulty. First, we train our adversarial referent placement policy, implemented as ViT-s/16 using GPT-40 as both a speaker and listener in 27,600 generated scenes comprising 60 samples per base environment. To generate our final dataset of scenes, we first sample 300 agent placements for each relative angle in {0, . . ., 180} distributed uniformly across the 450 base environments. For each of these agent placements, we sample two referent placements, resulting in two complete scenes: one where referent locations are randomly sampled, and another where referents are placed using the adversarial referent placement policy."}, {"title": "Evaluated Models", "content": "We experiment with four instruction-tuned vision-language models. Two of these models are designed for more general use: GPT-40, a proprietary model developed by OpenAI that supports real-time joint processing of audio, vision, and text; and LLaVA-1.5, a large open-weight instruction-tuned multimodal model. We also experiment with two instruction-tuned open-weight models designed specifically to refer to regions of and ground references in images at any granularity: Ferret and Groma. Ferret employs a hybrid region representation that combines discrete coordinates and continuous features to represent regions in an image, while Groma utilizes a localized visual tokenization mechanism, where an image is decomposed into regions of interest and encoded into region tokens. We use these models as listeners only as preliminary experiments showed poor performance on reference generation.\nWe also experiment with modular vision-language reasoning systems, which decompose the problems of language understanding and perception by first mapping language to some executable code, which is then executed on an image. In this work, we use ViperGPT, using GPT-4 to generate intermediate Python programs. We use ViperGPT as a listener agent only.\nFor both speaker models, we provide as input the speaker's observation os and a prompt to describe the location of the blue sphere. For listeners, we provide as input a referring expression x and the listener's observation ol, as well as a list of each candidate referent's bounding box, and prompt the model to select the bounding box corresponding to the described target. We sample from all models using a temperature of 0."}, {"title": "Results", "content": "We experiment with four configurations of agent dyads, combining humans and automated speakers and listeners. Table 1 includes results for the 1,485 validation scenes we use for collecting human-human data, split across scenes with random and adversarial referent placement.\nHuman speakers and listeners. Using the referring expressions collected in Section 3.1, we find that human-human pairs achieve an average communicative success rate of 87.6.\nHuman speakers, automated listeners. We evaluate model performance in comprehending human-written referring expressions. For each human-written referring expression in our collected dataset, we select the most likely referent according to the model. We observe substantially lower accuracy in referent selection compared to human listeners. Ferret, which was designed for fine-grained vision-and-language processing, outperforms the other models at an average selection accuracy of 69.2, but still lags far behind human performance.\nAutomated speakers, human listeners. We acquire a single referring expression from each instruction-tuned model for each evaluation scene. For each referring expression, we acquire three human listener selections and compare the majority class referent to the intended target. Both GPT-40 and LLaVA-1.5 are significantly less successful in describing target referents when compared to human speakers; GPT-40's references lead to correct human listener selection in 64.9% of scenes, while the LLaVA-1.5 speaker is successful for 55.7%.\nAutomated speakers and listeners. We evaluate settings where both agents are automated systems. Using the referring expressions acquired from both speaker agents, we use all five listener models to perform referent selection. In nearly all cases, performance with pairs of automated listeners is lower than dyads containing at least one human. However, both Ferret and Groma perform on par with human listeners on referring expressions generated by both GPT-40 and LLaVA-1.5, for both random and adversarial referent configurations. In fact, both models actually outperform human listeners for referring expressions generated by LLaVA-1.5 for random referent configurations."}, {"title": "Adversarial Referent Placement", "content": "Our adversarial referent placement policy was trained to minimize communicative success between a GPT-40 speaker and listener. Table 1 shows that scenes generated with this policy indeed reduce rates of communicative success in this setting by 3.9%, a statistically significant difference confirmed by a paired t-test (p < 0.05). The learned policy also reduces the success rate for nearly all other combinations of agents, including for human-human pairs, where we see rates of communicative success drops from 91.6 to 85.1 when adversarially placing candidate referents."}, {"title": "Language Analysis", "content": "We manually annotate 200 randomly-sampled referring expressions written by crowdworkers and GPT-40 with respect to referential strategies used by the speaker. Then, to scale to all validation data, we use GPT-40 to categorize referential strategy given in-context examples selected from these 200 examples. We consider four core referential strategies: reference to other candidate referents (e.g., in front of the other two red balls), reference to fixed objects in the scene (in front of the kitchen entryway), and reference to the listener (on your left) or speaker's perspective (closest to me).\nOverall, our analysis shows that, compared to humans, automated models are more likely to refer to the target's relative position among objects in the scene, and much less frequently refer to its position with respect to the listener's view. This policy is detrimental to model performance: LLaVA especially fails to accurately refer to other objects in the scene when describing the target, with only 61.2% of such references resulting in communicative success.\nWe also analyze the influence of view similarity between both agents on referential strategies and communicative success"}, {"title": "Learning from Communicative Success", "content": "We propose to further train our speaker model from learning signals acquired during referential communication. The basic premise that motivates this approach is that empirical observations of language interpretation provides evidence of utterance meaning, regardless of speaker intent. For instance, if the listener selects a different referent than the intended target, this indicates the speaker's referring expression describes (or at the very least, better describes) the chosen referent, even if the generated expression fails to describe the intended referent. In contrast to prior work that proposes methods that learn from communicative success (or failure), we additionally explore the use of preference-based learning signals that explicitly pair the intended and chosen targets in case of communicative failure."}, {"title": "Related Work", "content": "The meanings of relative spatial terms are highly dependent on the situated environment: the items participating in the relation and their intrinsic parts and affordances; the relative perspectives of participants in an embodied scene; and within-interaction conventions formed during multi-turn embodied dialogue, among other factors. In this work, we focus on the influence of relative perspective between multiple on the use of spatial language.\nProduction and comprehension of referring expressions has been studied in human-human dialogue and in interactions between human and automated language users. However, most work has focused on disembodied referential communication, where agents tasked with communicating about sets of stimuli, or where agents are not physically situated within an environment. The problem of situated language grounding in multi-agent settings reflects an increasingly popular real-world scenario of embodied agents. In studies where interaction participants are both embodied with different visual perspectives on the same scene, they must either be literally physically embodied in a single scene, or are placed in synthetic environments.\nA small number of existing works have trained language-generation models using evidence of communicative success in interaction with another agent. For example, train an instruction-generating agent by observing humans follow generated instructions, and  use signals from reference games with automated listeners to improve a speaker model. Our work takes inspiration from the latter to improve our speaker model using referent selections from an automated listener; however, we explore a preference-based objective that explicitly pairs the intended and empirically chosen referents."}, {"title": "Conclusion", "content": "We study multi-agent referential communication in situated interactions. In this setting, a speaker and a listener are both embodied in a shared scene, but are placed in different locations, with different views of the scene. We design a platform that supports generation of photorealistic 3D scenes, with control for difficulty of the referential task. We evaluate both humans and automated agents as speakers and listeners in this task. While human-human dyads are successful at coordinating on a referent around 88.4% of the time, automated models fall far behind when used both as speakers and as listeners. However, we can substantially improve the performance of an open-weight speaker model by training it with evidence of communicative success in referential communication with both automated and human listeners. Our findings suggest that despite the increasing relevance of multi-agent situated interactions between humans and automated agents, there is significant headroom for applying models that jointly process language and visual perception in this setting. However, they also show the promise of training such agents in interaction with people."}, {"title": "Limitations", "content": "Our task currently focuses on single-shot reference, where a speaker creates a single referring expression, and the listener cannot ask for clarification or engage in interactive reference resolution. Evaluating how models participate in an interactive version of our task is a compelling direction for future work. Additionally, while our experiments are currently conducted exclusively in English, the language of space and motion has enormous variation across language communities. Core spatial concepts studied in English, like on or in, do not have universally uniform meanings, with different languages dividing the conceptual space of spatial language in vastly different ways. Future work should explore how spatial concepts and referential strategies vary across movement and non-static environment, multi-turn conversations, language features, and more complex scenarios. Finally, our experiments on learning from communicative success perform only a single round of speaker deployment and training. Future work could perform further rounds of speaker deployment and listener judgments, and analyze dynamics of language change in a continual learning setting."}, {"title": "A Data", "content": "A.1 Scene Generation\nAgent placement. We impose three constraints on agent placement to help a more efficient scene generation pipeline:\n\u2022 Maximum distance between the agents: Let dmax be the maximum allowed distance between the speaker and the listener. Denoting the positions of the speaker and listener as ps and pl, respectively, we require that |ps - pi| \u2264 dmax. We use dmax = 10.\n\u2022 Field of view overlap: Let Fovs and Fovl be the fields of view of the speaker and listener, respectively. We require that the intersection of their fields of view is non-empty, i.e., Fovs \u2229 Fovl \u2260 0.\n\u2022 Relative viewing angle: Let \u03c8s and \u03c8l be the horizontal viewing angles of the speaker and listener, respectively, relative to a common reference direction. The relative viewing angle between the agents is given by \u03c8' = min(|\u03c8s-\u03c8l|, 360\u00b0-|\u03c8s-\u03c8l|). We can place the agents with a pre-set relative viewing angle by satisfying Co \u2264 |\u03c8' - \u03c8l| \u2264 C1, where Co, C1 is the viewing angle difference bounds we set.\nReferent placement. We impose three constraints on referents placement so they don't stack, become obstructed, or float in the air to meet real world physics standards:\n\u2022 Visibility constraint: Let Viss and Visl be the sets of points visible from the speaker's and listener's cameras, respectively. For each referent ri, we require that ri \u2208 Viss \u2229 Visl.\n\u2022 Physically-based placement: Let X, Y, Z be the sets of valid x, y, and z coordinates within the environment bounds. For each referent ri, we randomly sample coordinates (xi, yi, zi) \u2208 X \u00d7 Y \u00d7 Z and drop the referent using gravitational physical simulation until it comes to rest on a solid horizontal surface.\n\u2022 Minimum distance: Let dmin be the minimum required distance between any two referents. For all pairs of referents ri and rj, where i \u2260 j, we enforce |ri-rj| \u2265 dmin. We use dmin = 0.3.\nScene rendering. Our environment supports rendering observations at different resolutions; e.g., we use H = 720 and W = 1280 for HD resolution. For environment generation, we use Quadro RTX 6000 for graphics rendering for a single process. We parallelize data generation with Habitat-Sim with 4 Quadro RTX 6000.\nScene rejection sampling. We use GPT-4v to discard low quality images rendering during the dataset generation. We use the following prompt:\nA.2 Adversarial Referent Placement\nFor each training iteration, the vision transformer (ViT-s/16) takes as input the speaker view, and the available object placement locations and speaker and listener locations processed as (x, y, z) coordinates flattened into a normalized array. The model is trained to output the hard location from the input object placement locations as a single-choice pipeline.\nA.3 Crowdsourcing\nFor speakers and listeners we prompt the user to follow a description and a tutorial. When annotating, they still have access to the tutorial."}, {"title": "B Experiments", "content": "B.1 Experimental Setup\nWe prompt the instruction-tuned vision and language models to output speaker and listener text. Except for the model-specific architecture input formatting. We use the following prompts:\nSpeaker Prompt:\nListener Prompt:"}, {"title": "Influence of Speaker Visibility", "content": "In 26% of generated scenes, the speaker is visible to the listener agent. We find that for human speakers, the visibility of the speaker significantly (though only slightly) increases communicative success, while the difference is not significant for GPT-40 based speakers."}, {"title": "Error Example", "content": "We analyze the frequency of several common communication errors in collaborative tasks involving both human and automated speakers interacting with human listeners, with varying degrees of task difficulty. Out-of-context reference is when speaker reference context that is not in listener's view;. Perspective misalignment is when speaker reference its own perspective which will change drammatically when switched to listener's perspective. Ambiguity is that speaker expression can resolve to different meanings according to views. Relative position error is when the speaker expression describes wrong relative position like 'to the left of'. Expression error is simply wrong expression. Misunderstanding"}, {"title": "View Overlap Analysis", "content": "We perform analysis on speaker and listener view overlap, which is calculated by the percentage of objects area seen by speaker and listener. We use logistic regression on individual data points with likelihood ratio test (LRT) both p-values<0.001. And we calculate accuracy over 0.02 interval of buckets on the overlap percentage for the scatter plot and Chi-Square test with p-value<0.05. Higher overlap usually means speaker and listener have close view pose and position. We can see from the plot that for both adversarial and random placements, as the view overlap increases, the performance is better."}, {"title": "AI Assistants Usage", "content": "When conducting this research, we use AI to enhance our coding efficiency and quality. We use ChatGPT and Claude.ai to assist in writing"}]}