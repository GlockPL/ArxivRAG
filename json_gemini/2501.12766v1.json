{"title": "NExtLong: Toward Effective Long-Context Training without Long Documents", "authors": ["Chaochen Gao", "Xing Wu", "Zijia Lin", "Debing Zhang", "Songlin Hu"], "abstract": "Large language models (LLMs) with extended context windows have made significant strides yet remain a challenge due to the scarcity of long documents. Existing methods tend to synthesize long-context data 1 but lack a clear mechanism to reinforce the long-range dependency modeling. To address this limitation, we propose NExtLong, a novel framework for synthesizing long-context data through Negative document Extension. NExtLong decomposes a document into multiple meta-chunks and extends the context by interleaving hard negative distractors retrieved from pretraining corpora. This approach compels the model to discriminate long-range dependent context from distracting content, enhancing its ability to model long-range dependencies. Extensive experiments demonstrate that NExtLong achieves significant performance improvements on the HELMET and RULER benchmarks compared to existing long-context synthesis approaches and leading models, which are trained on non-synthetic long documents. These findings highlight NExtLong's ability to reduce reliance on non-synthetic long documents, making it an effective framework for developing advanced long-context LLMs. Our code is available in https://github.com/caskcsg/longcontext/tree/main/NExtLong.", "sections": [{"title": "Introduction", "content": "Large language models (LLMs) have garnered significant attention due to their powerful and versatile capabilities. Recently, the context length of LLMs has been rapidly extended (Peng et al., 2023; \u0391\u0399 et al., 2024; Yang et al., 2024). For example, the Llama series models increase the context length from 4k in Llama 2 (Touvron et al., 2023b) to 128K in Llama 3.1 (Meta, 2024). The increased context\nIn this work, we define \u201clong-context data\" as synthetic long-form datasets, and \"long documents\" as non-synthetic long documents that meet the target training length.\nwindow enables LLM to unlock more challenging tasks, such as Document Summary (Wu et al., 2023b), Longbook QA (Caciularu et al., 2023) and Code Planning (Bairi et al., 2023). To model long-range dependencies, mainstream methods (Fu et al., 2024; Gao et al., 2024b) typically continue training existing LLMs pre-trained on a 4K or 8K context length with long documents that reach the target length, e.g., 128K. However, the scarcity of high-quality long documents in most domains remains a significant challenge, particularly as the target context length continues to increase (Gao et al., 2024a).\nTo address the challenge of scarcity of long documents, existing approaches synthesize long-context data by concatenating shorter documents. Similarity-based methods, such as KNN (Guu et al., 2020; Levine et al., 2021), aggregate the top-k se-"}, {"title": "Related Work", "content": "Unlocking LLMs' ability to process long-context tasks. Train-free methods bypass parameter updates for long-context handling. LM-Infinite (Han et al., 2023) employs a A-shaped attention mask with a distance limit for length generalization. StreamingLLM (Xiao et al., 2023) mitigates the \"attention sink\" phenomenon by balancing attention scores. Self-Extend (Jin et al., 2024b) introduces group-wise attention to map unseen relative positions, while DCA (An et al., 2024) uses token-wise attention and memory-efficient mechanisms for effective context extension. Train-based methods enhance performance through continued training. Chen et al. (2023b) extend RoPE-based (Su et al., 2021) LLMs via positional interpolation, and PoSE"}, {"title": "Method", "content": "long-context tasks.\nThis section introduces our proposed method, NExtLong, which comprises two stages: Negative Document Extension and Long-Range Dependence Modeling. NExtLong aims to enhance long-context modeling by synthesizing extended-length documents. An overview of the approach is shown in Figure 2, and the corresponding pseudocode is presented in Appendix C.2.\nNegative Document Extension\nThe Negative Document Extension stage consists of two steps: document chunking and hard negative mining.\nDocument Chunking\nWe sample a document from the training dataset as a meta-document r and divide it into sequential meta-chunks. We define the documents to be expanded as meta-documents. The meta-document is divided into several chunks according to a certain chunking granularity. These chunks are defined as meta-chunks. To ensure sentence integrity, we define a maximum length s as the chunking granularity. The chunking process follows a two-step approach:\nSplitted by newline: The meta-document r is first splitted into paragraphs based on newline characters (\\n), preserving the coherence of each paragraph.\nForm chunks: These paragraphs are concatenated sequentially to form meta-chunks mi until the cumulative length reaches the maximum length s. If adding another paragraph exceeds this threshold, the current group is finalized as a complete meta-chunk, and the process continues with the remaining text. The effect of chunking granularity s is analyzed in Section 5.5.\nIn this way, the meta-document r is divided into p meta-chunks:\nr^{chunk} \u2192{m1, m2,..., mp}\nwhere $i$ is the index of iteration\nThe number of meta-chunks p depends on the length of the meta-document r and the chunking granularity s.\nHard Negative Mining\nTo obtain distracting texts as hard negatives for each meta-chunk, we build a FAISS index from the pretraining dataset, which undergoes extensive deduplication. Unlike methods that treat entire documents as indivisible units, we also divide each document in the pretraining dataset into smaller chunks based on the same granularity s. This chunking enables more fine-grained and efficient content retrieval during the extension process. Formally, each document di in the pretraining corpus is divided into q chunks:\ndi^{chunk}={Ci1, Ci2, ..., Ciq}\nEach chunk is indexed individually for precise and efficient retrieval. We compute the embedding vector ei for each chunk and insert it into the FAISS index:\n{Ci1,..., Ciq}\\xrightarrow{Project}\\xrightarrow{Project} {ei1,..., eig} \u2192 FAISS\nAfter building the FAISS index, we retrieve the top-k most similar chunks as hard negatives nij for each meta-chunk mi. These hard negatives are then concatenated with the meta-chunk to form an extended chunk li:\nli = [Mi, Ni\u2081, Ni\u2082,..., Nik]\nWe conduct ablation experiments on the position of meta-chunks (Appendix A.1), confirming that placing the meta-chunk before the hard negatives yields better performance. The number of hard negatives, i.e., k, depends on the length of the meta-document, chunking granularity s, and target context length. Details for calculating k are provided in Appendix C.1.\nFinally, we synthesize a long document t by concatenating the extended chunks:\nt = [l1, l2, ..., lp]\nLong-Range Dependence Modeling\nIn alignment with the pretraining stage, we employ next token prediction (NTP) loss (Radford, 2018) to extend the context length of the base model. The loss function is defined as:\nLoss = -\\sum_{t=1}^{T}log P(x_{t+1}|x_1, x_2,...,x_t)\nThe key distinction of NExtLong lies in the differentiation of tokens during training. The tokens"}, {"title": "Experiments", "content": "In this section, we evaluate the effectiveness of NExtLong by comparing it with other data synthesis methods (Section 4.2) and state-of-the-art (SOTA) models (Section 4.3).\nExperimental Setups\nDatasets We select two commonly used pretraining datasets composed entirely of short documents (Refer to Appendix A.2 for document length distribution): Cosmopedia v2 (Ben Allal et al., 2024) and FineWeb-Edu (Lozhkov et al., 2024). Both datasets are used for the main experiments, and we also provide ablation studies on their selection in Appendix A.3. Various methods, including NExtLong and baseline approaches, are employed to synthesize target-length samples concatenated from these short documents. The datasets are described as follows:\n\u2022 Cosmopedia v2: An advanced version of the largest synthetic dataset for pretraining, comprising over 39 million generated samples from textbooks, blog posts, and stories.\n\u2022 Fine Web-Edu: Consists of 1.3 trillion tokens of educational web pages filtered from the Fine Web dataset.\nEvaluation Recent long-context evaluations have focused on a 128K context length (Zhang et al., 2024c; Hsieh et al., 2024; Yen et al., 2024b), leading to the creation of various evaluation datasets. Accordingly, we set the target context length to 128K for comprehensive evaluation. We evaluate the models using the HELMET (Yen et al., 2024b) and RULER (Hsieh et al., 2024) benchmarks. The evaluation spans five task types from the HELMET benchmark: synthetic recall, retrieval-augmented generation (RAG), many-shot in-context learning (ICL), passage re-ranking, and long-document QA, covering a total of 17 sub-tasks. Detailed descriptions of the HELMET benchmarks can be found in Appendix B.3. Additionally, the RULER benchmark includes 13 synthesis sub-tasks.\nComparison with Other Data Synthesis Methods\nWe first compare NExtLong with previous long-context data synthesis methods on the 128K context length setting.\nExperimental Settings for Extending Context Length to 128K. We fine-tune the Meta-Llama-3-8B-base (Meta, 2024) model using a batch size of 4M tokens for 1000 steps with the open-source framework GPT-NeoX2. The RoPE frequency base is increased from 500,000 in Meta-Llama-3-8B-base to 200,000,000. The same training configuration is applied to all methods for a fair comparison. Further details are available in Appendix B.1.\nBaseline Methods We compare NExtLong with several methods that synthesized 32,000 128K-length samples (approximately 4 billion training tokens) from short documents:\n\u2022 Standard Method: Randomly samples and concatenates short documents (Ouyang et al., 2022; Le Scao et al., 2023; Touvron et al., 2023a).\n\u2022 KNN (Guu et al., 2020; Levine et al., 2021): Pairs each document with the top k most similar retrieved documents.\n\u2022 ICLM (Shi et al., 2023b): Uses a traveling salesman algorithm to reduce redundancy and improve diversity."}, {"title": "NextLong Enhances Long-Range Dependency Modeling", "content": "To assess the improvement in long-range dependency modeling achieved by NExtLong's negative document extension, we conduct a probing experiment using the Longbook QA dataset (Zhang et al., 2024b), which features long-range dependencies up to 128K in length. In this experiment, we use the normalized attention weights assigned to the first third of the context, when predicting the last token, as a metric for evaluating the model's long-dependency modeling ability.\nAs shown in Figure 5, we observe a positive correlation between this long-dependency metric and the model's performance on LongQA. Complementarily, as discussed in Appendix A.4, NExtLong reduces the model's dependence on proximal text (the last third context). These findings demonstrate that models trained with NExtLong's negative document extension exhibit enhanced long-dependency modeling capabilities, resulting in significantly improved long-context performance."}, {"title": "NextLong Performs Strongly After Supervised Finetuning.", "content": "To evaluate how NExtLong performs after supervised fine-tuning, we follow the approach in ProLong (Gao et al., 2024b) and fine-tune our base model using the UltraChat (Ding et al., 2023) short-context SFT dataset. We test the model on the recently proposed LongBench v2 benchmark (Bai et al., 2024). As shown in Table 3, NExtLong outperforms ProLong overall, especially on the Long metric. We also compare NExtLong with other SOTA models in Appendix A.6. The results demonstrate that Llama-3-8B-NExtLong-512K-Base performs strongly as a base model. With the same SFT dataset, the improved long-context base model enables the training of a superior fine-tuned model."}, {"title": "The Importance of Hard Negatives for Achieving Better Results", "content": "To evaluate the impact of hard negatives on performance, we design 5 document retrieval strategies. For each meta-chunk, we retrieve 512 documents from the Faiss index and select k from these documents using the following strategies:\nSelf-Repeat: Repeat meta-chunks without including retrieved documents."}, {"title": "NExtLong Shows No Significant Performance Degradation on Short Text.", "content": "To verify how well NExtLong maintains model performance on short text tasks, following Quest (Gao et al., 2024a), we select 7 widely-used short-text datasets: HellaSwag (Hel.) (Zellers et al., 2019),"}, {"title": "The Impact of Chunking Granularity s", "content": "We perform an ablation study on chunking granularity s using values of 512, 1024, 2048, 8192, and 32768. The results, shown in Figure 7, indicate that the model performs best with a granularity of 2048. While a granularity of 1024 yields optimal performance for 128K context length, it underperforms in the 8k and 16k ranges compared to 2048. We conclude that too small a granularity disrupts semantic integrity, while too large introduces redundant information, negatively impacting the hard negative mining stage. A moderate granularity offers the best balance for performance."}, {"title": "Conclusion and Future Works", "content": "This paper introduces NExtLong, a framework that improves long-range dependency modeling in LLMs through negative document extension. By dividing a document into meta-chunks and inserting hard negative distractors, NExtLong increases learning difficulty, encouraging the LLMs to better model long-range dependencies over extended contexts. Experimental results show that NExtLong outperforms existing methods on HELMET and RULER benchmarks, achieving notable performance gains.\nIn the future, we plan to explore more effective negative chunk mining strategies, such as generative approaches to creating more diverse and harder distractors, further enhancing the model's ability to learn fine-grained long-range dependencies."}, {"title": "A More Ablations", "content": "Placing Meta-Chunk at Different Positions\nWe explore three different strategies for combining meta-chunk and hard negatives, which are represented by the following descriptions:\nHead: Placing the meta-chunk at the beginning of the retrieved hard negatives.\nTail: Placing the meta-chunk at the end of the retrieved hard negatives.\nRandom: Randomly inserting the meta-chunk within the retrieved hard negatives.\nDocument Length Distribution of Cosmopedia V2 and FineWebEdu\nWe analyze the document length distribution of two datasets, Cosmopedia V2 and FineWebEdu, by sampling 8 million documents from each dataset and encoding them using the Meta-Llama-3-8B tokenizer. Document lengths are categorized into two ranges: [0, 8192] and > 8192. Table 6 shows that the majority of documents in both datasets are relatively short (under 8K). We apply the NExt-Long algorithm to extend the document length to 128K and 512K, achieving approximately a 64-fold increase compared to the original."}, {"title": "Dataset Ablation Study", "content": "We compared three different dataset selection strategies: (1) using FineWeb-Edu alone for long-context data synthesis, (2) using Cosmopedia v2 alone for long-context data synthesis, and (3) combining both datasets for long-context data synthesis. The results are shown in Table 7. The findings indicate that the combined strategy achieved the best performance, highlighting that a diverse dataset significantly enhances data synthesis."}, {"title": "NExtLong Reduces Dependence on Proximal Text", "content": "Complementary with Section 5.1, we investigated the dependence of different models on proximal text (the last third of the text). As shown in Figure 8, NExtLong demonstrates a lower degree of dependence on proximal text. This shift in attention toward long-range text contributes to improving the model's performance."}, {"title": "The Result on Needle-in-a-Haystack Benchmark", "content": "Following previous works (Gao et al., 2024a; Zhang et al., 2024a; Liu et al., 2024), we evaluate the Llama-3-8B-NExtLong-512K-Base model on the widely used Needle-in-a-Haystack task. As"}, {"title": "NExtLong Outperforms Current SOTA Models", "content": "To evaluate the performance of instruct supervised NExtLong against state-of-the-art (SOTA) models, we compare it with several leading instruct models, including GLM-4-9B (GLM et al., 2024), Qwen2.5-7B (Yang et al., 2024), Llama3.1-8B (Meta, 2024), and GPT-40-mini. The comparison is based on their performance on the LongBench v2 benchmark (Bai et al., 2024), a comprehensive suite designed to evaluate long-context understanding. Different from Section 5.2, we fine-tune the Llama-3-8B-NExtLong-512K-Base model with the public Magpie-Llama-3.3-Pro-1M-v0.1 (Xu et al., 2024) dataset to achieve better performance."}, {"title": "Experiment Details", "content": "Training Llama-3-8B-NExtLong-128K detailed setup\nWe use the parameters listed in Table 9 to train the 128K model. For other data synthesis methods, we only modify the training dataset while keeping all other training parameters unchanged.\nTraining Llama-3-8B-NExtLong-512K-Base detailed setup\nWe use the parameters listed in Table 10 to train the 512K model. The training samples are sourced from the NExtLong-512K and NExtLong-64k datasets in a ratio of 1:2."}, {"title": "Pseudocode of NExtLong", "content": "We present the complete process of constructing the NExtLong dataset using pseudocode in Algorithm 1."}]}