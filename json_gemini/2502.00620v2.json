{"title": "Representations Shape Weak-to-Strong Generalization: Theoretical Insights and Empirical Predictions", "authors": ["Yihao Xue", "Jiping Li", "Baharan Mirzasoleiman"], "abstract": "Weak-to-Strong Generalization (W2SG), where a weak model supervises a stronger one, serves as an important analogy for understanding how humans might guide superhuman intelligence in the future. Promising empirical results revealed that a strong model can surpass its weak supervisor. While recent work has offered theoretical insights into this phenomenon, a clear understanding of the interactions between weak and strong models that drive W2SG remains elusive. We investigate W2SG through a theoretical lens and show that it can be characterized using kernels derived from the principal components of weak and strong models' internal representations. These kernels can be used to define a space that, at a high level, captures what the weak model is unable to learn but is learnable by the strong model. The projection of labels onto this space quantifies how much the strong model falls short of its full potential due to weak supervision. This characterization also provides insights into how certain errors in weak supervision can be corrected by the strong model, regardless of overfitting. Our theory has significant practical implications, providing a representation-based metric that predicts W2SG performance trends without requiring labels, as shown in experiments on molecular predictions with transformers and 5 NLP tasks involving 52 LLMs.", "sections": [{"title": "1. Introduction", "content": "As AI systems become increasingly capable of performing complex tasks beyond human comprehension, humans will inevitably serve as \u201cweak supervisors\u201d in aligning advanced AI. To investigate this fundamental problem, Burns et al. (2023) propose an analogy that can be empirically explored today: can a weak model effectively supervise a stronger one? This framework, known as Weak-to-Strong Generalization (W2SG), involves leveraging a weak model, finetuned on a specific task, to supervise the finetuning of a stronger model. In this analogy, the finetuning task represents concepts tied to human values or skills, the finetuned weak model represents humans-limited in capability but aligned with human values, and the strong model represents superhuman intelligence\u2013powerful but initially unaligned. Promising results from (Burns et al., 2023) show that the strong model can significantly outperform its weak supervisor. For instance, a GPT-4 model supervised by a fine-tuned GPT-2-level model achieves nearly 20% better performance than the weak supervisor on NLP tasks.\nAt first glance, this phenomenon seems counterintuitive. After all, the strong model is explicitly trained to fit the weak supervision. Yet, it goes beyond mere imitation and generalizes better. It is important to understand which intrinsic properties of the weak and strong models enable W2SG.\nEfforts have been made toward a theoretical understanding of W2SG. Charikar et al. (2024) demonstrates that the disagreement between finetuned weak and strong models correlates with performance gains in W2SG. However, their analysis assumes high-quality representations in the strong model and does not address the role of the weak model's representations. The analysis of (Lang et al., 2024; Shin et al., 2024) assumes a generalized version of an adversarially robust strong model, where W2SG arises solely from underfitting weak supervision. This framework excludes important scenarios such as benign overfitting, where W2SG occurs despite overfitting. Wu & Sahai (2024) particularly studied benign overfitting and examined the impact of number of weakly labeled data points. However, we still lack an overarching explanation that captures the interaction between weak and strong models in enabling W2SG, as well as how it determines which weak supervision errors are corrected in general scenarios. The challenge lies in characterizing the abstract concepts including the knowledge embedded in the weak and strong models, their utilization, and their respective roles in W2SG. Striving for results"}, {"title": "2. Related Work", "content": "There have been many recent works that theoretically explore W2SG. Somerstep et al. (2024) adopt a transfer learning perspective, focusing on improving W2SG through in-context learning rather than explaining how W2SG emerges. Lang et al. (2024); Shin et al. (2024) analyze W2SG by considering a generalized version of adversarially robust models, showing that certain errors in weak supervision can be corrected by leveraging the good neighborhood structure in the data. However, their argument attributes error correction solely to underfitting-i.e., avoiding fitting mislabeled finetuning data. This overlooks an important scenario recently discussed in (Wu & Sahai, 2024), known as benign overfitting, where the strong model overfits mislabeled finetuning data but still achieves accurate test-time predictions. Benign overfitting is particularly relevant in practice, as large neural networks often have the capacity to overfit while still generalizing effectively (Zhang et al., 2021). Closer to our setting, Charikar et al. (2024) formalized W2SG using a representation-based perspective. Their work demonstrates that performance gain in W2SG correlates with the disagreement between the finetuned weak and strong models, assuming high-quality representations for the strong model. While insightful, it does not characterize the role of the weak model's representations, leaving the exact conditions for effective W2SG unclear.\nCompared to (Lang et al., 2024), we analyze W2SG in a more realistic setting where error correction can result from either underfitting or overfitting, allowing for a full spectrum of behaviors. While benign overfitting is not our primary focus, we discuss it as a special case in Sec. 4 due to its importance and offer new insights. Compared to (Charikar et al., 2024), we explicitly links W2SG performance to the interaction between the weak and strong models' representations, providing"}, {"title": "3. W2SG from a Representation Perspective", "content": "We first formalize finetuning from a representation-based perspective, then introduce the properties of the representations considered, and finally present our main theory."}, {"title": "3.1. A representation-based perspective", "content": "The knowledge a model acquires through pretraining enables it to interpret inputs, extract relevant information, and organize it into meaningful intermediate states. This can be formalized as a \u201crepresentation function\", h, which transforms data into structured representations. Finetuning leverages this knowledge to produce the desired output, which we formalize as learning a new function f on the fixed h. The entire model is thus represented as the composition foh. For simplicity, we consider the outputs of h as vectors, and focus on the case where f is a linear functions. This is practically relevant because: (1) Training a linear task head on fixed representations is common with large foundation models, e.g., using embedding LLMs (Muennighoff et al., 2022), linear probing on intermediate activations (Zou et al., 2023; Nanda et al., 2023; Marks & Tegmark, 2023). (2) fine-tuning of LLMs largely operates in the NTK regime (Jacot et al., 2018), where training dynamics are captured by a linear model on representations derived from model gradients (Malladi et al., 2023). (3) Our experiments in Sec. 5 show that insights from analyzing linear functions generalize to the complex non-linear setting of finetuning entire LLMs from pretrained weights."}, {"title": "3.2. Preliminaries", "content": "Notations. We sometimes abbreviate a matrix $A \\in \\mathbb{R}^{l \\times m}$ as $[A_{i,j}]_{1 < i < l, 1 < j < m}$ when each element $A_{i,j}$ can be expressed as a generic term in terms of its indices. $\\lambda_{\\min, \\neq 0}(A)$ denotes the smallest nonzero eigenvalue of matrix $A$.\nData. Let $\\mathcal{D}$ denote the distribution of the finetuning task's data, defined over the input-label pairs $(x, y) \\in \\mathcal{X} \\times \\mathcal{Y}$, where $\\mathcal{Y} = \\mathbb{R}$. In W2SG, we have two splits of data sampled from $\\mathcal{D}$. The first subset, $\\mathcal{D} = \\{(X_i, Y_i)\\}_{i=1}^{\\tilde{n}}$, consists of $\\tilde{n}$ i.i.d. samples and is used for finetuning the weak model. The second subset, $\\tilde{\\mathcal{D}} = \\{(x_i, \\hat{y}_i)\\}_{i=1}^{\\hat{n}}$ with $\\hat{n}$ i.i.d. samples is used for finetuning the strong model. Note that the weak model's outputs will be used as labels in place of the actual $\\hat{y}_i$'s. In our notation, quantities associated with the two splits are marked by the diacritical symbols $\\tilde{}$, and $\\hat{}$, respectively.\nModels. We denote the weak and strong models' representation functions as $h_w$ and $h_s$, respectively. The finetuned weak model is represented as $f_w \\circ h_w$, with\n$f_w = \\arg \\min_{f \\in \\mathcal{F}_w} {\\frac{1}{\\tilde{n}} \\sum_{i=1}^{\\tilde{n}} (f(h_w(x_i)) - Y_i)^2 + \\beta_w R(f)},$\nwhere $R(\u00b7)$ represents $l_2$ regularization.\nThe W2S model, which refers to the strong model finetuned with weak supervision, is represented as $f_{w2s} \\circ h_s$, with\n$f_{w2s} = \\arg \\min_{f \\in \\mathcal{F}_s} {\\frac{1}{\\hat{n}} \\sum_{i=1}^{\\hat{n}} (f(h_s(\\hat{x}_i)) - f_w(h_w(\\hat{x}_i)))^2 + \\beta_s R(f)}.$\nAdditionally, as a reference, we define the strong ceiling model as the strong model finetuned with the ground truth labels. It is represented as $f_{sc} \\circ h_s$ with\n$f_{sc} = \\arg \\min_{f \\in \\mathcal{F}_s} {\\frac{1}{\\hat{n}} \\sum_{i=1}^{\\hat{n}} (f(h_s(\\hat{x}_i)) - \\hat{y}_i)^2 + \\beta_s R(f)}.$\nEvaluation. At test time, given any labeling function $g : \\mathcal{X} \\rightarrow \\mathcal{Y}$, we define its test error as the loss on the population: $\\text{Err}(g) = \\mathbb{E}_{(x,y) \\sim \\mathcal{D}}[(g(x) - y)^2]$. We then introduce the shorthand notations: the weak model's test error $\\text{Err}_w = \\text{Err}(f_w \\circ h_w)$, the W2S model's test error $\\text{Err}_{w2s} = \\text{Err}(f_{w2s} \\circ h_s)$, and the strong ceiling model's test error $\\text{Err}_{sc} = \\text{Err}(f_{sc} \\circ h_s)$. $\\text{Err}_{w2s}$ measures the performance achieved through W2SG, while $\\text{Err}_{sc}$ serves as the upper limit."}, {"title": "3.3. Assumption: representations with a well-concentrated principal part and a manageable non-principal part", "content": "We first define two basic concepts, kernel and covariance, before introducing a general assumption on representations.\nDefinition 3.1 (Kernel Matrix). Given $h : \\mathcal{X} \\rightarrow \\mathbb{R}^d$, we define the kernel matrix on the finetuning dataset $\\mathcal{D}$ as $K(h) = [h(x_i)^T h(x_j)]_{1 < i,j < \\tilde{n}}$, a $\\tilde{n} \\times \\tilde{n}$ matrix where each element represents the inner product between a pair of representations. $K(h)$ is defined on $\\tilde{\\mathcal{D}}$ in the same manner.\nDefinition 3.2 (Population/Empirical Covariance Matrices). Given $h : \\mathcal{X} \\rightarrow \\mathbb{R}^d$, we define the population covariance over distribution $\\mathcal{D}$ as $\\Sigma(h) := \\mathbb{E}_{x \\sim \\mathcal{D}} [h(x)h(x)^T]$. The empirical version on $\\tilde{\\mathcal{D}}$ is defined as $\\hat{\\Sigma}(h) := \\frac{1}{\\tilde{n}} \\sum_{i=1}^{\\tilde{n}} h(x_i)h(x_i)^T$. $\\tilde{\\Sigma}(h)$ is defined on $\\hat{\\mathcal{D}}$ in the same manner.\nGiven a representation function and a reasonable sample size, certain components in the representations should concentrate well, meaning they adequately reflect the population distribution. These components are pivotal to the model's generalization. In our analysis, we focus on cases where the remainder\u2014the less-well-concentrated components-satisfies certain conditions, ensuring their impact remains theoretically tractable. The decomposition of representations into these two parts is formalized as follows.\nDefinition 3.3 (($\\delta, \\gamma, \\tilde{\\gamma}$)-decomposability). Given $\\mathcal{D}, \\tilde{\\mathcal{D}}, \\hat{\\mathcal{D}}$, and a representation function $h : \\mathcal{X} \\rightarrow \\mathbb{R}$, we say that the representations of h are ($d, \\gamma, \\tilde{\\gamma}$)-decomposable w.r.t. a subspace $V$ (of $\\mathbb{R}$), for some $d=O(1), \\gamma = O(1)$, and $\\tilde{\\gamma} = O(1)$, if there exists a subset of eigenvectors of $\\Sigma(h)$ corresponding to non-zero eigenvalues such that the following holds. Let $\\Pi_V$ denote the orthogonal projections onto $V$ and $V^{\\perp}$, respectively. Define $\\rho = \\lambda_{\\min,\\neq 0}(\\Sigma(\\Pi_V h))$ and $\\tilde{\\gamma} = \\min(\\lambda_{\\min},\\tilde{\\lambda})$. With high probability of $1 - o(1)$:\na. Boundedness. A basic condition that ensures reasonable magnitudes of representations and labels: $|\\Sigma(h)||_{op} = O(1)$, $|\\hat{\\Sigma}(h)||_{op} = O(1)$, $|\\tilde{\\Sigma}(h)||_{op} = O(1)$, $E[y^2] = O(1)$, $\\lambda_{\\min}(\\hat{\\Sigma}) = O(1)$ and $\\lambda_{\\min}(\\tilde{\\Sigma}) = O(1)$.\nb. Concentration on V. Representations are well-concentrated in the subspace V, both in terms of their covariance and their correlation with labels: $|\\hat{\\Sigma}(\\Pi_V h) - \\Sigma(\\Pi_V h)||_{op} = o(\\gamma^2 + \\delta^2 + \\rho^2)$, $|\\tilde{\\Sigma}(\\Pi_V h) - \\Sigma(\\Pi_V h)||_{op} = o(\\gamma^2 + \\delta^2 + \\rho^2)$, $|\\Pi_V h(x_i)\\hat{y} - \\mathbb{E}[\\Pi_V h(x)y]|| = o(\\gamma + \\delta + \\rho)$ and $|\\Pi_V h(x_i)\\tilde{y}_i - \\mathbb{E}[\\Pi_V h(x)y]|| = o(\\gamma + \\delta + \\rho)$.\nc. Kernel-wise $\\delta$-isotropy on $V^{\\perp}$. The kernels constructed using only the components in $V^{\\perp}$ exhibit certain uniformity in all orientations, with the extent of uniformity controlled by $\\delta$: ||K(\\Pi_{V^{\\perp}}h) - I||_{op} = o(\\gamma^2 + \\delta^2)$, and ||K(\\Pi_{V^{\\perp}}h) - I||_{op} = o(\\gamma^2 + \\delta^2)$.\nd. Small cross-sample inner-product on $V^{\\perp}$. $|[(\\Pi_{V^{\\perp}} h(x_i))(\\Pi_{V^{\\perp}} h(x_j))]_{1 < i < \\tilde{n}, 1 < j < \\hat{n}}||_{op} = o(\\gamma + \\delta)$, which holds when representations on $V^{\\perp}$ are nearly orthogonal across samples or have small magnitudes.\ne. Diminishing population covariance on $V^{\\perp}$. The representations on $V^{\\perp}$ have small magnitude in the population: $|\\Sigma(\\Pi_{V^{\\perp}} h)||_{op} = o(\\gamma + \\delta)$.\nRemark. To provide a clearer understanding of Kernel-wise $\\delta$-isotropy on $V^{\\perp}$, consider the following: If d is very small (e.g., $\\delta = 0$), the kernel on $\\mathcal{D}$ is nearly identical to $\\hat{I}$, meaning it does not exhibit any specific patterns that differentiate between data points. In contrast, with a larger $\\delta$ (e.g., $\\delta \\gg \\gamma$), this requirement is much more relaxed\u2014the kernel no"}, {"title": "3.4. Principal representations shape PredGap", "content": "Intuition. One implication of Def. 3.3 is that only what is learned through the principal representations will be reflected at test time. Thus, the weak model's mistakes primarily stem from its inability to generate certain outputs using its principal representations. For the same reason, among these mistakes, only those expressible through the strong model's principal"}, {"title": "4. A Case Study on Benign Overfitting", "content": "Our theory can be applied to study and provide new insights into benign overfitting, an intriguing special case of W2SG, where the W2S model appears to mimic the weak supervision during finetuning, yet generalizes better at test time."}, {"title": "4.1. A general condition", "content": "Benign overfitting has been studied in the general machine learning context to understand deep neural networks' generalization (Bartlett et al., 2020; Wang et al., 2021; Frei et al., 2022; Mallinar et al., 2022). Recently, (Wu & Sahai, 2024) theoretically characterized benign overfitting in W2SG for a specific data distribution. Here, we aim to derive broader insights from a representation perspective. We consider the scenario where the strong model's representations are highly expressive, enabling near-perfect overfitting of arbitrary labelings on the finetuning data, mirroring the behavior of very large neural networks in practice (Zhang et al., 2021). This occurs when $\\delta_s = o(\\gamma_s)$ (Lem. B.4), yielding a highly isotropic non-principal kernel. Meanwhile, since generalization depends solely on the principal representations by Thm. 3.8, a small $|\\Pi_{P_s}(I - P_w)\\hat{y}||^2$ suffices for good W2SG performance, regardless of the extent of overfitting. In this way, we connect benign overfitting to the general relationship between the weak and strong models' representations:\nTheorem 4.1 (A general condition for benign overfitting 1). In addition to Assumption 3.7, suppose that (1) $\\delta_s = o(\\gamma_s)$ and $\\delta_s < \\beta_s = o(\\tilde{\\lambda}_s)$, (2) w.h.p., the strong ceiling model achieves nearly perfect performance, i.e., $\\text{Err}_{sc} = o(1)$, (3) w.h.p., $|\\Pi_{P_s}(I - P_w)\\hat{y}||^2 = \\text{Err}_w - \\Delta$ with $\\Delta = \\Theta(1)$. Then, w.h.p., the W2S model achieves an almost zero ($o(1)$) training error on $\\mathcal{D}$, but generalizes better than the weak model: $\\text{Err}_{w2s} < \\text{Err}_w - \\Delta + o(1)$. See proof in Appx. B.3.1.\n1Thm 4.1 can be extended to cases where the strong ceiling is not perfect, but we omit this for brevity."}, {"title": "4.2. Instantiation of Theorem 4.1 on a toy example", "content": "We present a concrete example of the scenario in Theorem 4.1 to demonstrate the realizability of the conditions. While more complex examples could be constructed, we focus on a simple one to succinctly illustrate the core ideas.\nExample 4.2. The label is a Gaussian: $y \\sim \\mathcal{N}(0, 1)$. Given $(x, y)$, the weak model's representation is $h_w(x) = [\\sqrt{\\eta}y + \\sqrt{1 - \\eta}\\zeta, \\xi_w]^T$, where $\\eta \\in (0, 1)$ is some constant, $\\zeta \\sim \\mathcal{N}(0, 1)$, and $\\xi_w \\sim \\mathcal{N}(0, \\delta_w I)$ are both independently drawn. The strong model's representation is $h_s(x) = [y, \\xi_s]^T$, where $\\xi_s \\sim \\mathcal{N}(0, \\delta_s I)$ independently. The scalings satisfy $\\tilde{n} = \\Theta(n) = w(1)$, $d = w(\\tilde{n}^2)$, and $\\sigma^2 = o(n)$ but $\\neq 0$. Additionally, $\\beta_s = o(\\lambda_s)$ and $\\beta_w = o(\\lambda_w)$.\nHere, the weak model's first coordinate carries a signal about the label $y$, but corrupted by noise $\\zeta$, with $\\eta$ controlling the signal strength (i.e., with SNR $\\frac{\\eta}{1 - \\eta}$). The strong model's first coordinate carries a perfect signal about $y$. The remaining coordinates in both models are high-dimensional random noise. Both models' representations are special cases of Example 3.5 and are therefore ($o(\\eta), \\frac{1}{\\tilde{n}}, \\frac{1}{\\tilde{n}}$) decomposable.\nCorollary 4.3. Benign overfitting occurs in Example 4.2. Specifically, w.h.p., (1) The weak model's errors on both $\\mathcal{D}$ and the population are $(1 - \\eta) \\pm o(1)$. (2) The W2S model overfits the weak model's outputs on $\\mathcal{D}$, achieving a training loss of o(1). (3) However, compared to the weak model, the W2S model achieves a smaller test error: $\\text{Err}_{w2s} = (1 - \\eta)^2 \\pm o(1)$.\nFor instance, if $\\eta = 0.6$, then $\\text{Err}_w \\approx 0.4$, while $\\text{Err}_{w2s} \\approx 0.16$, despite nearly perfect overfitting on $\\tilde{\\mathcal{D}}$."}, {"title": "4.3. A closer look at error propagation", "content": "We provide a rough derivation of the W2S error (with details in Appx. B.3.2), illustrating which errors are replicated and which are corrected (overfitted but benignly) by the W2S model, and how representations determine this.\nThe principal representations for both models are simply at their first coordinates. Thus, the spans of their principal kernels are one-dimensional. Let $\\hat{\\zeta} \\in \\mathbb{R}^n$ denote the vector collecting the $\\zeta$ values on $\\tilde{\\mathcal{D}}$, i.e., $\\hat{\\zeta} = [\\hat{\\zeta}_1, ..., \\hat{\\zeta}_\\tilde{n}]^T$. Similarly, define $\\hat{y} = [\\hat{y}_1,..., \\hat{y}_\\tilde{n}]^T$. We can approximate the projection matrices as: P_w \\approx qq^T and P_s \\approx \\hat{y}\\hat{y}^T, where $q = \\sqrt{\\eta}\\hat{y} + \\sqrt{1 - \\eta}\\hat{\\zeta}$. Note that vectors $\\hat{y}$ and $\\hat{\\zeta}$ are almost orthogonal as the corresponding random variables are uncorrelated: $q^T\\hat{y} = \\frac{1}{\\tilde{n}} \\sum_i \\hat{y}_i\\hat{\\zeta}_i \\approx \\mathbb{E}[y\\zeta] = 0$. Let $e_w$ be the vector whose i-th element is the weak model's error on data point $(\\hat{x}_i, \\hat{y}_i)$. By Lemma A.12, we can approximate $e_w$ as:\n$\\text{Err}_{w2s} \\approx \\text{PredGap}$. By Thm 3.8, PredGap$\\approx||P_s e_w||^2$. Then,\nTherefore, only errors within the span of the strong model's principal kernel are overfitted harmfully, while overfitting elsewhere remains benign."}, {"title": "5. Predicting W2SG Without Labels", "content": "Leveraging Thm. 3.8, we derive a representation-based metric that can predict W2SG performance without labels in experiments across various settings. Notably, this metric strongly correlates with W2SG performance even when we finetune entire LLMs-a scenario significantly more complex than what we analyze in theory."}, {"title": "5.1. A label-agnostic metric for W2SG", "content": "We start with upper-bounding the RHS of Thm. 3.8.\nCorollary 5.1 (Upper Bound 1). Define $C = \\frac{1}{\\tilde{n}} \\sum_{i=1}^{\\tilde{n}} \\hat{y}_i^2$. Following Theorem 3.8, directly applying the submultiplicative property of the norm yields the following upper bound:\n$\\text{PredGap} \\leq C ||P_s (I - P_w)||_{op}^2 + o(1)$,\nCorollary 5.2 (Upper Bound 2). Following Theorem 3.8, we can also obtain an upper bound that involves $\\text{Err}_{sc}$ as long as $|\\mathbb{E}[y^2] - \\frac{1}{\\tilde{n}} \\sum_{i=1}^{\\tilde{n}} \\hat{y}_i^2| = o(1)$ (see proof in Appendix B.4) :\n$\\text{PredGap} \\leq (\\sqrt{C} ||P_s (I - P_w) P_s||_{op} + \\sqrt{\\text{Err}_{sc}})^2 + o(1)$.\nIn both upper bounds, $C$ represents the variance of the labels on $\\mathcal{D}$, which can be treated as a constant given a fixed dataset. Therefore, $\\text{PredGap}$ is governed by the norm $||P_s (I - P_w)||_{op}$ or $||P_s (I - P_w) P_s||_{op}$. Comparing the two bounds, the one in Corollary 5.2 is tighter particularly when $\\text{Err}_{sc}$ is small 2 . This follows from $||P_s (I - P_w) P_s||_{op} \\leq ||P_s (I - P_w)||_{op}$. However, in our experiments, both are similarly indicative of W2SG performance.\nNow that $\\text{PredGap}$ can be bounded in terms of the above label-agnostic metrics, and $\\text{PredGap}$ is indicative of the error $\\text{Err}_{w2s}$ as discussed at the end of Sec. 3.2, we turn our focus to examining the following relationship in real models\n$\\text{Err}_{w2s} \\sim ||P_s (I - P_w)||_{op}$ (or $||P_s (I - P_w) P_s||_{op}$)\nto evaluate whether the metrics offer practical insights. Specifically, we consider the three setups summarized in Table 1, with their details discussed in the corresponding subsections. In each setup, we fix the strong model and vary the weak model to obtain different $\\text{Err}_{w2s}$ and $||P_s (I - P_w)||_{op}$ (or $||P_s (I - P_w) P_s||_{op}$) pairs and study their relationship."}, {"title": "5.2. Empirical measure of Pw and Ps", "content": "Before proceeding, let's address an important question: how can we compute $P_w$ and $P_s$ for real models? In some cases, representations are not fixed during fine-tuning, making $h$ difficult to define. Additionally, determining the principal representation, $\\Pi_V h$, is challenging because the exact V depends on the population, which is unknown in practice. To tackle this, we design heuristics to approximate P as follows\n$P \\approx \\frac{1}{\\tilde{n}} K(\\Pi h) (\\frac{1}{\\tilde{n}} K(\\Pi h) + \\beta_{eff} I)^{-1} $\nWe explain the key components below.\nh: extracting representations. We consider two ways of defining the representations, depending on the setup. (1) Last layer embeddings. In Exps. I and II, the definition of representation is self-evident, as finetuning is simply training a task head on the embeddings produced by the base model 3. (2) Activation maps. 4 In Exp. III, we finetune the entire LLM from pretrained weights, so we don't have fixed representations as in the theoretical setting. To address this, we adopt a simple heuristic: we treat the layer-wise normalized vectorized activation maps of the pre-trained LLM, which encode"}, {"title": "5.3. Experimental setups", "content": "Exp. I: Molecular prediction. Our first setting follows (Charikar et al., 2024). We use the GuacaMol (Brown et al., 2019) dataset for pretraining both the strong and weak models. For finetuning, we consider three regression datasets\u2014ESOL, Free-Solv, and Lipop-from the MoleculeNet (Wu et al., 2018) benchmark, curated by ChemBench (Charleshen, 2020), which involve predicting molecular physical properties. The strong model is MolBERT (Fabian et al., 2020), a BERT (Devlin, 2018) pretrained for 100 epochs on GuacaMol. We use smaller transformers pretrained on GuacaMol as weak models. These weak models have 2 layers and 2 attention heads. We vary the hidden size across 64, 128, 256, and vary the number of pretraining epochs from 1 to 50, resulting in 150 weak models. During finetuning, we extract last-layer embeddings and perform linear re-"}, {"title": "5.4. Results", "content": "Strong correlation between Errw2s and $||P_s(I \u2013 P_w)||_{op}$ across various settings. For each of the weak models, we perform the W2SG procedure to obtain the resulting W2S model. We then measure Errw2s and $||P_s(I - P_w)||_{op}$ and plot the results in Figures 2, 3 and 4. Across all the setups, we observe a strong correlation between the two quantities, with high Spearman's correlation values displayed at the top of the figures. The results are highly similar for $||P_s(I - P_w)P_s||_{op}$, as shown in Appx. D.3. Therefore, we only focus on discussing $||P_s(I \u2013 P_w)||_{op}$ in the main paper. Notably, the correlation between Errw2s and $||P_s(I \u2013 P_w)||_{op}$ extends beyond the theoretical setting, covering the following variations: (1) Loss function and evaluation metric. While Thm. 3.8 is based on linear regression with MSE loss, Exps. II and III demonstrate that the correlation also holds for classification tasks using CE finetuning loss, with Errw2s measured as classification error. (2) The form of finetuning. Thm. 3.8 assumes that finetuning involves training a function on fixed representations. However,"}, {"title": "6. Discussion", "content": "Using activation maps as representations in Exp. III is a simple heuristic that yields promising results. However, more principled methods for defining and extracting representations from LLMs, such as those through NTK (Malladi et al., 2023) or representation engineering (Zou et al., 2023), could be explored. Future research could leverage these approaches to improve results and uncover new applications. For instance, (Zou et al., 2023) introduces a method for extracting specific concept directions in representations, such as honesty and power-seeking. This could enable computing our metric based on topic-specific representations, allowing predictions of W2SG for general tasks within specific topical domains."}, {"title": "7. Conclusion", "content": "In this work, we show that W2SG can be characterized using kernels derived from the principal components of weak and strong models' representations. The theory is applicable to a wide range of representation distributions, provides insights into how models' internal structures influence error correction and the conditions for benign overfitting. Additionally, it offers a label-free metric for predicting W2SG performance, validated through experiments on diverse datasets and LLMs.\nWe see positive societal impacts in our work as it advances the understanding of Weak-to-Strong Generalization, a crucial problem for aligning superhuman AI in the future. Our results could enhance transparency in AI systems' behavior through analysis of their internal structures and contribute to the broader goal of improving AI safety and reliability."}, {"title": "A. Main Analysis", "content": "In this section, we provide a thorough analysis of the errors associated with the weak model, the W2S model, and the strong ceiling model. Some of these results are used to prove our main conclusion, Theorem 3.8, while others are applied in subsequent analyses."}, {"title": "A.1. Notations and additional notes", "content": "Symbol definitions. We introduce the following notations. The symbol r represents a representation", "representation": "n$\\tilde{R"}, ["r_1 r_2 ... r_{\\tilde{n}}"], "and $\\hat{R} := [\\hat{r}_1 \\hat{r}_2 ... \\hat{r}_{\\hat{n}}"], "samples": "nFor the covariance matrices", "clutter": "n$\\Sigma"}