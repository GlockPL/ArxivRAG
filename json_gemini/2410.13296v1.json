{"title": "Fairness-Enhancing Ensemble Classification in\nWater Distribution Networks", "authors": ["Janine Strotherm", "Barbara Hammer"], "abstract": "As relevant examples such as the future criminal detection software [1]\nshow, fairness of AI-based and social domain affecting decision support tools\nconstitutes an important area of research. In this contribution, we investigate\nthe applications of AI to socioeconomically relevant infrastructures such as\nthose of water distribution networks (WDNs), where fairness issues have yet to\ngain a foothold. To establish the notion of fairness in this domain, we propose\nan appropriate definition of protected groups and group fairness in WDNS as\nan extension of existing definitions. We demonstrate that typical methods\nfor the detection of leakages in WDNs are unfair in this sense. Further, we\nthus propose a remedy to increase the fairness which can be applied even to\nnon-differentiable ensemble classification methods as used in this context.", "sections": [{"title": "0 Introduction", "content": "Due to the increasing usage of AI-based decision making systems in socially relevant\nfields of application the question of fair decision making gained importance in recent\nyears (cf. [1], [6]). Fairness is hereby related to the several (protected) groups or\nindividuals which are affected by the algorithmic decision making and characterized\nby sensitive features such as gender or ethnicity. Most algorithms on which these\ntools are based on rely on data which can be biased with respect to the questions\nof fairness without intention, resulting in skewed models. Also the algorithm itself\ncan discriminate protected groups or individuals without explicitly aiming to do so\ndue to an undesirable algorithmic bias (cf. [10, 12]).\nSeveral definitions of fairness have been discussed (cf. [3, 4, 5, 10, 12, 16]).\nFrom a legal perspective, one distinguishes between disparate treatment and dis-\nparate impact (cf. [3]). While disparate treatment occurs whenever a group or an\nindividual is intentionally treated differently because of their membership in a pro-\ntected class, disparate impact is a consequence of indirect discrimination happening\ndespite \"seemingly neutral policy\" (cf. [12]). From a scientific viewpoint, the variety"}, {"title": "1 Fairness in Machine Learning", "content": "Let (\u03a9, F, P) be a probability space of interest, consisting of the sample space \u03a9 \u2260 0,\na o-field F and a probability measure P. Moreover, let \u0176 : \u03a9 \u2192 Y be some binary\nclassifier, i.e., \u0423 = {0,1}, being trained to model some true labels Y : \u03a9 \u2192 \u03a5.\nUsually, \u00dd can be written as some model f : X \u2192 {0,1}, applied to the features\nX : \u03a9 \u2192 X, i.e., \u0176 = f(X) holds. In recent years, the interest towards the\nquestion of such classifier \u00dd being fair with respect to some additional, sensitive\nfeature S : \u03a9 \u2192 S has risen. Mostly, S = {0,1} gives binary information about the\nmembership or non-membership of a protected class, such as some certain gender or"}, {"title": "2 Leakage Detection in Water Distribution Net-\nworks", "content": "A key challenge in the domain of WDNs is to detect leakages. In this task, \u03a9\ncorresponds to possible states of a WDN, given by time-dependant demands of the\nend users of the D nodes in the network. We assume that among those, d nodes\nare provided with sensors (usually, D \u226b d), which deliver pressure measurements\np(t) \u2208 Rd for different times t \u2208 R and which can be used for the task at hand. As\nwe usually measure pressure values within fixed time intervals d \u2208 [R+, we introduce\nthe notation t\u2081 := to +id, where to is some fixed reference point with respect to time."}, {"title": "2.1 Methodology", "content": "There are several methodologies that make use of pressure measurements to ap-\nproach the problem of leakage detection using ML, i.e., by training a classifier\n\u0176 \u2208 {0,1} = y that predicts the true state of the WDN Y\u2208 Y with respect\nto the question whether a leak is active (1) or not (0). One standard approach\ncomes in two steps: In first instance, so called virtual sensors are trained, i.e., re-\ngression models being able to predict the pressure at a given node j\u2208 {1, ...,d}\nand some time ti \u2208 R, based on measured pressure at the remaining nodes \u0135 \u2260 j\nand over some discrete time interval of size T\u2084 + 1 \u2208 N. Subsequently, these virtual\nsensors are used to compute pressure residuals of measured and predicted pressure\nto train an ensemble classifier that is able to predict whether a leakage is present in\nthe WDN at the time of the used residual (cf. [9])."}, {"title": "2.1.1 Virtual Sensors", "content": "The virtual sensors f; : Rdr \u2192 R for each node j \u2208 {1, ..., d} and d\u2084 := d-1 are linear\nregression models trained on leakage free training data D'; = {(p+j(ti), Pj (ti)) \u2208\nRdr \u00d7 R | i = 0, ..., n}. More precisely, y(t\u2081) = 0 \u2208 Y holds for all realisations\ni = 0, ..., n, of Y and the inputs are given by the rolling means P\u2260j(ti) := (T, +\n1)-1 \u03a3\u03c4\u03bf P\u2260j (ti \u2013 1d) at all nodes except the node j, which is the only preprocessing\nrequired for the training pipeline (cf. [2])."}, {"title": "2.1.2 Ensemble Leakage Detection", "content": "Standard leakage detection methods rely on the residuals rj(ti) := |pj(ti)-f(P\u2260j(ti))| \u2208\nR+ we obtain from the true pressure measurements p(ti) \u2208 Rd and the virtual sensor\npredictions f; (p+j(ti)) \u2208 R for each sensor node j \u2208 {1, ...d} and (possibly unseen)\ntimes t\u2081 \u2208 R (cf. [9]).\nA simple detection method performing good on standard benchmarks is the\nthreshold-based ensemble classification introduced by [2]: Without any further train-\ning, we can define a classifier ff : R+ \u2192 Y by\nf(rj(ti)) = f(rj (ti), 0j) := 1 {rj (ti) > 0;}\nfor each sensor node j \u2208 {1, ...d} and a node-dependant hyperparameter \u03b8; \u2208 R+.\nWe easily obtain an ensemble classifier fc : X \u2192 Y, called the H-method, with"}, {"title": "2.1.3 Evaluation", "content": "We evaluate the H-method in terms of general performance, measured by accuracy\n(ACC), and in terms of fairness, measured by disparate impact as well as equal\nopportunity score DI and EO, respectively (cf. eq. (1.1) and (1.2)) in section 2.3,\nafter introducing the application domain and data set."}, {"title": "2.2 Application Domain and Data Set", "content": "One key contribution of this work is to introduce the notion of fairness in the ap-\nplication domain of WDNs. The WDN considered is Hanoi (cf. [13]) displayed in\nfigure 2.1. It consists of 32 nodes and 34 links.\nTo evaluate the H-method presented in section 2.1 on Hanoi, we generate pressure\nmeasurements with a time window of d = 10min. using the atmn toolbox (cf. [15]).\nThe pressure is simulated at the sensor nodes displayed in figure 2.1 and for different\nleakage scenarios, which differ in the leakage location and size. As the WDN is\nrelatively small, we are able to simulate a leakage at each node in the network and\nfor three different diameters d\u2208 {5, 10, 15}cm. For the preprocessing according to\nsection 2.1, we choose T\u2081 = 2 such as [2] do."}, {"title": "3 Fairness-Enhancing Leakage Detection in Wa-\nter Distribution Networks", "content": "Motivated by the result that the standard leakage detection method presented in\nsection 2.1 does not satisfy the notions of fairness, as another main contribution of\nthis work, we modify this H-method to enhance fairness as introduced in section 1.\nGiven virtual sensors f; for all j = 1, ..., d and resulting residuals r(t\u2081) \u2208 X = Rdc+\n(cf. section 2.1) as well as labels y(tz) \u2208 Y = {0,1} for times t\u2081 \u2208 R, we can turn the\nchoice of the hyperparameter \u0398 := (0j)j=1,...,d \u2208 X of the ensemble classifier fe (cf.\neq. (2.3)) into an optimization problem (OP) with corresponding function space\nH := {f\u00a3 : X \u2192 Y, r \u2192 f(r, \u0398) | \u0398 \u2208 X}. In the following section, we therefore\npresent different, in contrast to the H-method optimization-based, methods that\naim at optimizing the parameter \u0398\u2208 X in order to obtain an optimal ensemble\nclassifier f(., \u0398opt.) \u2208 H. These methods on the one hand are further baselines,\nwhere treating the modelling problem as an OP enables us to optimize the result of\nthe H-method itself without fairness considerations. On the other hand, we consider\nfairness-enhancing methods, where the parameter \u0398\u2208 X needs to be optimized such\nthat the resulting ensemble classifier is as accurate and fair on the given training\ndata as possible."}, {"title": "3.1 Methodology", "content": "The following methods define training algorithms based on labeled training data\nD = {(r(ti), y(ti)) \u2208 X \u00d7 Y | i = 1, ..., n}\u00b2 for an ne > nr, which also holds data"}, {"title": "3.1.1 Optimizing Loss with Fairness Constraints", "content": "In general, a learning problem can be phrased as an OP, where the objective is to\nminimize some suitable loss function L: X \u2192 R with respect to the parameter\n\u0398\u2208 X, i.e.,\n$\\left\\{\\min _{\\theta \\in \\mathcal{X}} L(\\Theta).\\right.$\n\nThe advantage of redefining the choice of hyperparameters \u0398 (H-method) as an OP\nis that we can now extend this OP by fairness constraints, which can be given by\nside constraints Ck : X \u2192 R of the underlying OP:\n$\\left\\{\\begin{array}{ll}\\min _{\\Theta \\in \\mathcal{X}} & L(\\Theta), \\\\\\text { s.t. } & C_{k}(\\Theta) \\geq 0 \\forall k=1, \\ldots, K .\\end{array}\\right.$\n\nChoice of Loss Functions In view of the notions of fairness (cf. section 1), an intuitive\nand by means of linearity easily to differentiate loss function is given by the difference\nof the false positive rate (FPR) and the TPR, i.e., L\u2081(\u0398) := \u2013 TPR(\u0398) + FPR(\u0398).\nAnother classical evaluation score which we can use as a loss function is the accuracy\nL2(\u0398) := \u2212 ACC(\u04e8).\nChoice of Fairness Constraints In terms of fairness constraints, [16] introduce\nthe covariance between a single binary sensitive feature and the signed distance of\na feature vector and the decision boundary of a convex margin-based classifier as a\nproxy for fairness measurements. We adapt this idea to our setting by considering\nthe covariance of each sensitive feature and replacing the signed distance by the\nprediction of the ensemble classifier \u0176 = f(X, \u0398). Using that \u0177(ti) = f(r(ti), \u0398)\nholds for all realisations i = 1, ..., nc, for all sensitive features Sk for k = 1, ..., K,\nthe empirical covariance is given by\n$\n\\operatorname{Covemp.}\\left(S_{k}, \\hat{Y}\\right)=\\frac{1}{N_{c}} \\sum_{i=1}^{n_{c}}\\left(S_{k}\\left(t_{i}\\right)-\\bar{S}_{k}\\right) f\\left(r\\left(t_{i}\\right), \\Theta\\right).\n$\nRemark 3.1. The usage of the (empirical) covariance as a proxy for fairness is based\non the idea that fairness of a machine learning model \u00dd can be interpreted as the\nassumption of \u0176 being independent of the sensitive feature S (cf. [10]), or in our case,\neach of the sensitive features Sk for k = 1, ..., K. As independence of two random\nvariables implies their covariance being equal to zero, the latter can be interpreted\nas a necessary condition for fairness. Therefore, we consider the covariance of each\nsensitive feature Sk for k = 1, ..., K_and the prediction of the ensemble classifier\n\u0176 = f(X, \u0398), which by linearity is given by\nCov(Sk, Y) = E((Sk \u2013 E(Sk)) \u00b7 (\u0176 \u2013 E(\u00dd)) = E((Sk \u2013 E(Sk)) \u00b7 \u0176).\n\nHowever, as the probability measure P(Sk, \u0176)-\u00b9 on Y \u00d7 Y is unknown, we replace it\nby its empirical approximation=1 (sk(ti),y(t)) and obtain the empirical covari-\nance (3.6). As in practise, an exact value of zero will rarely be achieved, enforcing\nthe empirical covariance to be small is a reasonable fairness proxy."}, {"title": "3.1.2 Optimizing Fairness with Accuracy Constraints", "content": "Instead of optimizing some loss function L under some fairness side constraints, [16]\nsuggest to optimize a fairness proxy under loss constraints. They use the covariance\nas a proxy while constraining the training loss by some percentage of the optimal\nloss obtained when training without fairness considerations. As a variation, we use\nthe disparate impact score DI directly as a loss function and the accuracy ACC for\nthe constraint. The resulting DI+ACC-method is therefore given by\n$\\left\\{\\begin{array}{ll}\\min _{\\Theta \\in \\mathcal{X}} & -D I(\\Theta), \\\\\\text { s.t. } & A C C(\\Theta)>(1-\\lambda) A C C_{o p t .} .\\end{array}\\right.$\n\nThe hyperparameter \u03bb\u2208 [0,1] hereby regulates how much the obtained accuracy\nACC(O) is allowed to differ from the optimal accuracy ACCopt. received in the\nACC-method (cf. table 3.1).\nIn contrast to the methods proposed in section 3.1.1, we like to test the OP (3.8)\nas a non-differentiable OP, which therefore requires a non-gradient-based optimiza-\ntion technique."}, {"title": "3.1.3 Evaluation", "content": "We evaluate all presented methods, i.e., the standard H-method (section 2.1), the\noptimization-based baselines T-F-PR- and ACC-method as well as the fairness-\nenhancing T-F-PR+F-, ACC+F- (cf. section 3.1.1 and table 3.1) and DI+ACC-\nmethod (cf. section 3.1.2), as we did in section 2.1.3."}, {"title": "3.2 Experimental Results and Analysis", "content": "Based on the pressure measurements in the Hanoi WDN as introduced in section\n2.2 and the resulting residuals, we test all six methods introduced in section 2.1 (H-\nmethod) and section 3.1 (T-F-PR-, ACC-, T-F-PR+F, ACC+F, DI+ACC-method,\nalso see Evaluation in section 3.1.3) per diameter d in practise."}, {"title": "3.2.1 Setup", "content": "H-method We use the H-method presented in section 2.1 and tested in section 2.3\nas a baseline. Subsequently, we use the hyperparameter found here as an initial\nparameter \u0398\u03bf \u2208 X for the remaining optimization-based methods.\nGradient-Based Methods While the T-F-PR- and the ACC-method are used as\nanother baseline, the remaining methods are fairness-enhancing methods. The mag-\nnitude of fairness can be regulated by a hyperparameter: The T-F-PR+F-and\nACC+F-method ensure fairness by bounding the empirical covariance of each sensi-\ntive feature and the models approximated prediction (cf. eq. (3.6) and (3.7)) by the\nhyperparameter c\u2208 [0,\u221e). In addition, for all these methods, we choose b = 100\nand T = 0.8.\nNon-Gradient-Based Method In contrast, the DI+ACC-method regulates fairness\nby different choices of the hyperparameter \u03bb \u2208 [0, 1] that controls how much loss in\naccuracy is allowed while increasing fairness."}, {"title": "4 Conclusion", "content": "In this work, we introduced the notion of fairness in an application domain of\nhigh social and ethical relevance, namely in the field of water distribution networks\n(WDNs). This required the extension of fairness definitions for a single binary sen-\nsitive feature to single non-binary or multiple, possibly even non-binary, sensitive\nfeatures. We then investigated on the fairness issue in the area of leakage detection\nwithin WDNs. We showed that standard approaches are not fair in the context of\ndifferent groups related to the locality within the network. As a remedy, we pre-\nsented methods that increase fairness of the ensemble classification model with re-\nspect to the introduced fairness notion while satisfying the legal notions of disparate\ntreatment and disparate impact simultaneously. We empirically demonstrated that\nfairness and overall performance of the model are interdependent and the use of\nhyperparameters provides the ability to trade off fairness and overall performance.\nHowever, this trade off lies in the responsibility of the policy maker.\nTo allow more fine-grained steps between improving fairness and decreasing over-\nall performance in the presented covariance-based approaches, next steps would be\nto swap loss function and constraint to achieve similar results as in the approach\nwith accuracy constraint. Moreover, the notion of fairness within the water domain\nis still in its beginning and extensions to more complex WDNs as well as more\npowerful ML algorithms is essential."}]}