{"title": "DIAR: DIFFUSION-MODEL-GUIDED IMPLICIT Q-LEARNING WITH ADAPTIVE REVALUATION", "authors": ["Jaehyun Park", "Yunho Kim", "Sejin Kim", "Byung-Jun Lee", "Sundong Kim"], "abstract": "We propose a novel offline reinforcement learning (offline RL) approach, introducing the Diffusion-model-guided Implicit Q-learning with Adaptive Revaluation (DIAR) framework. We address two key challenges in offline RL: out-of-distribution samples and long-horizon problems. We leverage diffusion models to learn state-action sequence distributions and incorporate value functions for more balanced and adaptive decision-making. DIAR introduces an Adaptive Revaluation mechanism that dynamically adjusts decision lengths by comparing current and future state values, enabling flexible long-term decision-making. Furthermore, we address Q-value overestimation by combining Q-network learning with a value function guided by a diffusion model. The diffusion model generates diverse latent trajectories, enhancing policy robustness and generalization. As demonstrated in tasks like Maze2D, AntMaze, and Kitchen, DIAR consistently outperforms state-of-the-art algorithms in long-horizon, sparse-reward environments.", "sections": [{"title": "INTRODUCTION", "content": "Offline reinforcement learning (offline RL) is a type of reinforcement learning where the agent learns a policy from pre-collected datasets, rather than collecting data through direct interactions with the environment (Fujimoto et al., 2019). Since offline RL does not involve learning in the real environment, it avoids safety-related issues. Additionally, offline RL can efficiently leverage the collected data, making it particularly useful when data collection is expensive or time-consuming. However, offline RL relies on the given dataset, so the learned policy may be inefficient or misdirected if the data is poor quality or biased. Furthermore, a distributional shift may arise during the process of learning from offline data (Levine et al., 2020), leading to degraded performance in the true environment.\nTo overcome the limitations of offline RL, existing research have been made to address these issues by leveraging diffusion models, a type of generative model (Janner et al., 2022). Incorporating diffusion models allows for learning the overall distribution of the state and action spaces, allowing decisions to be made based on this knowledge. Methods such as Diffuser (Janner et al., 2022) and Decision Diffuser (DD) (Ajay et al., 2023) use diffusion models to predict decisions not autoregressively one step at a time, but instead by inferring the entire decision for the length of the"}, {"title": "RELATED WORK", "content": ""}, {"title": "OFFLINE REINFORCEMENT LEARNING", "content": "Offline reinforcement learning (offline RL), also referred to as batch reinforcement learning, has gained significant attention in recent years due to its potential in learning effective policies from pre-collected datasets without further interaction with the environment. This paradigm is particularly useful in real-world applications where exploration can be costly or dangerous, such as healthcare, robotics (Kalashnikov et al., 2018), and autonomous driving.\nOne of the primary challenges in offline RL is the issue of out-of-distribution actions (Kumar et al., 2019), where a learned policy selects actions not well represented in the offline dataset. To address this, several works have introduced behavior regularization techniques that constrain the policy to remain close to the behavior policy seen in the offline data. Among these, Conservative Q-learning (CQL) introduces a conservative Q-function that underestimates the value of out-of-distribution actions, reducing the likelihood of the learned policy selecting potentially harmful actions (Kumar"}, {"title": "DIFFUSION-BASED PLANNING IN OFFLINE RL", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) have shown remarkable performance in fields such as image inpainting (Lugmayr et al., 2022) and image generation (Ramesh et al., 2022; Saharia et al., 2022). Recent research has extended the application of diffusion models beyond image domains to address classical trajectory optimization challenges in offline RL. One prominent model, Diffuser (Janner et al., 2022), directly learns trajectory distributions and generates tailored trajectories based on situational demands. By prioritizing trajectory accuracy over single-step precision, Diffuser mitigates compounding errors and adapts to novel tasks or goals unseen during training. Additionally, Decision Diffuser (DD) was introduced, which predicts the next state using a state diffusion model and leverages inverse dynamics for decision-making (Ajay et al., 2023). Furthermore, a method called Latent Diffusion-Constrained Q-learning (LDCQ) has been proposed, which combines latent diffusion models with Q-learning to reduce extrapolation errors (Venkatraman et al., 2024). Emerging methods also focus on learning interpretable skills from visual and language inputs and applying conditional planning via diffusion models (Liang et al., 2024). Approaches that generate goal-divergent trajectories using Gaussian noise and facilitate reverse training through denoising processes have also been explored (Jain & Ravanbakhsh, 2023)."}, {"title": "PRELIMINARY: LATENT DIFFUSION REINFORCEMENT LEARNING", "content": "To train the Q-network, a diffusion model that has trained based on latent representations is required. The first step is to learn how to represent an action-state sequence of length H as a latent vector using B-Variational Autoencoder (\u03b2-VAE) (Pertsch et al., 2021). The second step is to train the diffusion model using the latent vectors generated by the encoder of the B-VAE. This allows the diffusion model to learn the latent space corresponding to the action-state sequence. Subsequently, the Q-network is trained using the latent vectors generated by the diffusion model.\nLatent representation by B-VAE The B-VAE plays three key roles in the initial stage of our model training. First, the encoder q0E (Z St:t+H, at:t+H) must effectively represent the action-state sequence St:t+H, at:t+H_from the dataset D into a latent vector z. Second, the distribution of z generated by the B-VAE must be conditioned by the state prior p\u04e9. (z|st). This is learned by minimizing the KL-divergence between the latent vector generated by the encoder and the one generated by the state prior. The formation of the latent vector is controlled by adjusting the \u1e9e value, which determines the weight of KL-divergence. Lastly, the policy decoder \u03c0\u04e9\u266d(at|st, z) of the B-VAE must be able to accurately decode actions when given the current state and latent vector as inputs. These three objectives are combined to train the B-VAE by maximizing the evidence lower bound (ELBO) (Kingma & Welling, 2014) as shown in Eq. 1.\n$$L(\\theta) = E_D [E_{q_{\\theta_E}} [\\sum_{i=t}^{t+H-1} log \\pi_{\\theta_D}(a_i| s_i, z)] \u2013 \\beta D_{KL}(q_{\\theta_E}(z|s_{t:t+H}, a_{t:t+H}) || P_{\\theta_S}(z|s_t))]$$\nTraining latent vector with a diffusion model The latent diffusion model (LDM) effectively learns latent representations, focusing on the latent space instead of the original data samples (Rombach et al., 2022). The model minimizes a loss function that predict the initial latent zt generated by the VAE encoder q4, rather than noise as in traditional diffusion models. H-length trajectory segments St:t+H, at:t+Hare sampled from dataset D and paired with initial states and latent variables (st, zt). The focus lies on modeling the prior p(z st) to capture the distribution of latent z"}, {"title": "PROPOSED METHOD", "content": "Using diffusion models to address long-horizon tasks typically involves training over the full trajectory length (Janner et al., 2022). This approach differs from autoregressive methods that focus on selecting the best action at each step, as it learns the entire action sequence over the horizon. This allows the model to learn long sequences of decisions at once and generate a large number of actions in a single pass. However, predicting decisions over the entire horizon may not always lead to the optimal outcome, as it does not select the best action at each step.\nAdditionally, there is a well-known problem of overestimating the Q-value when training a Q-network (Hasselt et al., 2016; 2018; Fu et al., 2019; Kumar et al., 2019; Agarwal et al., 2020). This occurs when certain actions, appearing intermittently, are assigned a high Q(s, a) value. In these cases, the state may not actually hold high value, but the Q-value becomes \"lucky\" and inflated. Therefore, it is essential to ensure that the Q-network does not overestimate and can correctly assess the value based on the current state.\nTo resolve both of these issues, we propose Diffusion-model-guided Implicit Q-learning with Adaptive Revaluation (DIAR), introducing a value function to assess the value of each situation. Unlike the Q-network, which learns the value of both state and action, the state-value function learns only the value of the state. By introducing constraints from the value function, we can train a more balanced Q-network and, during the decision-making phase, make more optimal predictions with the help of the value function."}, {"title": "DIFFUSION-MODEL-GUIDED Q-LEARNING FRAMEWORK", "content": "The value-network Vn with parameter \u03b7 evaluates the value of the current state st, and the Q-network Q with parameter & evaluates the value of the current state st and action at. Additionally, by"}, {"title": "ADAPTIVE REVALUATION IN POLICY EXECUTION", "content": "DIAR method reforms a decision if the value of the current state is higher than the value of the state after making a decision over the horizon length H. We refer to this process as Adaptive Revaluation. Using the value-network Vn, if the current state's value V(st) is greater than V(st+H), the value after making a decision for H steps, the method generates a new latent vector z\u0142 from the current state st and continues the decision-making process. When predicting over the horizon length, there may be cases where taking a different action midway through the horizon would be more optimal. In such cases, the value-network Vn checks this, and if the condition is met, a new latent vector is generated.\nAdaptive Revaluation uses the difference in value to examine whether the agent's predicted decision is optimal. Since the current state st can be obtained directly from the environment, it is easy to compute the value V(st) of the current state st. Whether the current trajectory is optimal can be determined using a state decoder fe(St+H|St, Zt). By inputting the current state st and latent vector"}, {"title": "THEORETICAL ANALYSIS OF DIAR", "content": "In this section, we prove that in the case of sparse rewards, when the current timestep t, if the value V(st) of the current state st is higher than the value V(st+H) of the future state st+H, there is a more ideal trajectory than the current trajectory. An ideal trajectory is defined as one where, for all states at timestep k, the discount factor 0 < \u03b3 < 1 ensures that V(sk) \u2264 V(Sk+1). This means that for an agent performing actions toward a goal, the value of each state in the trajectory increases monotonically.\nNow, consider an assumption about an ideal trajectory: for any timesteps i, j with i < j, we assume that V(si) > V(sj) for si and sj from the dataset D. Furthermore, since the state sj is not the goal and we are in a sparse reward setting, \u2200r(si, ai) = 0. If we write the Bellman equation for the value function, it results in Eq. 8.\n$$V(Si) = E_{(si,ai,si+1)\u223cD} [r(s_i, a) +\\gamma V(s_{i+1})]$$\nEq. 8 represents the value function V(si) when there is a difference of one timestep. The value function V(si) can be computed using the reward received from the action taken in the current state si and the value of the next state si+1. Therefore, by iterating Eq. 8 to express the timesteps from i to j, we obtain Eq. 9.\n$$V(Si) = E_{(sa:)\u223cD} [\\sum_{t=i}^{j-1} \\gamma^{t-i}r(s_t, a_t) + \\gamma^{j-i}V (s_j)]$$\nSince the current environment is sparse in rewards, no reward is given if the goal is not reached. Therefore, in Eq. 9, all reward r(st, at) terms are zero. By substituting the reward as zero and reorganizing Eq. 9, we can derive Eq. 10.\n$$V(si) = E_{(sijai:)\u223cD} [\\gamma^{j-i}V(sj)]$$\nSince the magnitude of \u03b3 is 0 < y \u2264 1, the term \u03b3i\u2212\u00bfV(sj) is always less than or equal to V (sj). This contradicts the initial assumption, indicating that the assumption is incorrect. Therefore, for any ideal trajectory, all value functions V(si) must follow a monotonically increasing function. In other words, if the trajectory predicted by the agent is an ideal trajectory, the value V(sj) after making a decision over the horizon H must always be greater than the current value V(si). If the current value V(si) is greater than the future value V(sj), then this trajectory is not an ideal trajectory. Consequently, generating a new latent vector z\u2081 from the current state si to search for an optimal decision is a better approach."}, {"title": "EXPERIMENTS", "content": "We compare the performance of our model with other models under various conditions and environments. We focus on goal-based tasks in environments with long-horizons and sparse rewards. For offline RL, we use the Maze2D, AntMaze, and Kitchen datasets to test the strengths of our model in long-horizon sparse reward settings (Fu et al., 2020). These environments feature very long trajectories in their datasets, and rewards are only given upon reaching the goal, making them highly suitable for evaluating our model. We also compare the performance improvements achieved when using Adaptive Revaluation, analyzing whether it allows for reconsideration of decisions when incorrect ones are made and enables the generation of the correct trajectory. Furthermore, to ensure more accurate performance measurements, all scores are averaged over 100 runs and repeated 5 times, with the mean and standard deviation reported."}, {"title": "PERFORMANCE ON OFFLINE RL BENCHMARKS", "content": "In this section, we compare the performance of our model in offline RL. To evaluate our model, we compare it against various state-of-the-art models. These include behavior cloning (BC), which imitates the dataset, and offline RL methods based on Q-learning, such as IQL (Kostrikov et al., 2022) and IDQL (Hansen-Estruch et al., 2023). We also compare our model with DT (Chen et al., 2021), which uses the transformer architecture employed in LLMs, and methods that use diffusion models, such as Diffuser (Janner et al., 2022), DD (Ajay et al., 2023), and LDCQ (Venkatraman et al., 2024). Through these comparisons with various algorithms, we conduct a quantitative performance evaluation of our model.\nDatasets like Maze2D and AntMaze require the agent to learn how to navigate from a random starting point to a random location. Simply mimicking the dataset is insufficient for achieving good performance. The agent must learn what constitutes a good decision and how to make the best judgments throughout the trajectory. Additionally, the ability to stitch together multiple paths through trajectory combinations is essential. In particular, the AntMaze dataset involves a complex state space and requires learning and understanding high-dimensional policies. We observed that our method DIAR, consistently demonstrated strong performance in these challenging tasks, where"}, {"title": "IMPACT OF ADAPTIVE REVALUATION", "content": "In this section, we analyze the impact of Adaptive Revaluation. We directly compare the cases where Adaptive Revaluation is used and not used in our model. The test is conducted on long-horizon sparse reward tasks, where rewards are sparse. For overall training, an expectile value of t = 0.9 was used, with H = 30 for Maze2D and H = 20 for AntMaze and Kitchen. Other training settings were generally the same, and detailed configurations can be found in the Appendix A."}, {"title": "COMPARISON WITH SKILL LATENT MODELS", "content": "We further compare our model with other reinforcement learning methods that use skill latents. For the D4RL tasks, we selected methods that use generative models to learn skills and make decisions based on them. As performance baselines, we chose the VAE-based methods OPAL\u00b9 (Ajay et al., 2021) and PLAS (Zhou et al., 2020), as well as Flow2Control (Yang et al., 2023), which utilizes normalizing flows. The performance comparison is shown in Table 3."}, {"title": "CONCLUSION", "content": "In this study, we proposed Diffusion-model-guided Implicit Q-learning with Adaptive Revaluation (DIAR), which leverages diffusion models to improve abstraction capabilities and train more adaptive agents in offline RL. First, we introduced an Adaptive Revaluation algorithm based on the value function, which allows for long-horizon predictions while enabling the agent to flexibly revise its decisions to discover more optimal ones. Second, we propose an Diffusion-model-guided Implicit Q-learning. Offline RL faces the limitation of difficulty in evaluating out-of-distribution state-action pairs, as it learns from a fixed dataset. By leveraging the diffusion model, a generative model, we balance the learning of the value function and Q-function to cover a broader range of cases. By combining these two methods, we achieved state-of-the-art performance in long-horizon sparse reward tasks such as Maze2D, AntMaze, and Kitchen. Our approach is particularly strong in long-horizon sparse reward situations, where it is challenging to assess the current value. Additionally, a key advantage of our method is that it performs well without requiring extensive hyper-parameter tuning for each task. We believe that the latent diffusion model holds significant strengths in offline RL and has high potential for applications in various fields such as robotics."}, {"title": "EXPERIMENTS DETAILS", "content": "DIAR consists of three main components: the B-VAE for learning latent skills, the latent diffusion model for learning distributions through latent vectors, and the Q-function, which learns the value of state-latent vector pairs and selects the best latent. These three models are trained sequentially, and when learning the same task, the earlier models can be reused. Detailed model settings and hyperparameters are discussed in the next section. For more detailed code implementation and process, you can refer directly to the code on GitHub."}, {"title": "B-VARIATIONAL AUTOENCODER", "content": "The B-VAE consists of an encoder, policy decoder, state prior, and state decoder. The encoder uses two stacked bidirectional GRUs. The output of the GRU is used to compute the mean and standard deviation. Each GRU output is passed through an MLP to calculate the mean and standard deviation, which are then used to compute the latent vector. This latent vector is used by the state prior, state decoder, and policy decoder. The policy decoder takes the latent vector and the current state as input to predict the current action. The state decoder takes the latent vector and the current state to predict the future state. Lastly, the state prior learns the distribution of the latent vector for the current state, ensuring that the latent vector generated by the encoder is trained similarly through KL divergence.\nIn Maze2D, H = 30 is used; in AntMaze and Kitchen, H = 20 is used. The diffusion model for the diffusion prior used in B-VAE training employs a transformer architecture. This model differs from the latent diffusion model discussed in the next section, and they are trained independently. Training the B-VAE for too many epochs can lead to overfitting of the latent vector, which can negatively impact the next stage."}, {"title": "LATENT DIFFUSION MODEL", "content": "The generative model plays the role of learning the distribution of the latent vector for the current state. The current state and latent vector are concatenated and then re-encoded for use. The architecture of the diffusion model follows a U-Net structure, where the dimensionality decreases and then increases, with each block consisting of residual blocks. Unlike the traditional approach of predicting noise e, the diffusion model is trained to directly predict the latent vector z. This process is constrained by Min-SNR-\u03b3. Overall, the diffusion model operates similarly to the DDPM method."}, {"title": "Q-LEARNING", "content": "In our approach, we utilize both a Q-network and a Value network. The Q-network follows the DDQN method, employing two networks that learn slowly according to the update ratio. The Value network uses a single network. Both the Q-network and the Value network are structured with repeated MLP layers. The Q-network encodes the state into a 256-dimensional vector and the latent vector into a 128-dimensional vector. These two vectors are concatenated and passed through additional MLP layers to compute the final Q-value. The Value network only encodes the state into a 256-dimensional vector, which is then used to compute the value. Between the linear layers, GELU activation functions and LayerNorm are applied. In this way, both the Q-network and Value network are implicitly trained under the guidance of the diffusion model."}, {"title": "DIAR POLICY EXECUTION DETAILS", "content": "We provide a detailed explanation of how DIAR performs policy execution. It primarily selects the latent with the highest Q-value. However, if the current state value V (st+h) is higher than the future state value V(ss+H), it triggers another search for a new latent. DIAR repeats this process until it either reaches the goal or the maximum step T is reached."}, {"title": "TRAINING PROCESS FOR B-VAE", "content": "This section details the process by which the B-VAE is trained. The B-VAE consists of four models: the skill latent encoder, policy decoder, state decoder, and state prior. These four components are trained simultaneously. Additionally, a diffusion prior is trained alongside to guide the B-VAE in generating appropriate latent vectors. The detailed process can be found in Algorithm 3."}, {"title": "TRAINING PROCESS FOR LATENT DIFFUSION MODEL", "content": "This section also provides an in-depth explanation of how the latent diffusion model is trained. The goal of the latent diffusion model is to learn the distribution of latent vectors generated by the B-VAE. The latent diffusion model is trained by first converting the offline dataset into latent vectors using the encoder of the B-VAE, and then learning from these latent vectors. The detailed process can be found in Algorithm 4."}, {"title": "DIFFUSION PROBABILISTIC MODELS", "content": "Diffusion models (Sohl-Dickstein et al., 2015; Ho et al., 2020) function as latent variable generative models, formally expressed through the equation po(xo) := \u222b po(x0:T), dx1:T. Here, x1,..., XT denote the sequence of latent variables, integral to the model's capacity to assimilate and recreate the intricate distributions characteristic of high-dimensional data types like images and audio. In these models, the forward process q(xt|xt\u22121) methodically introduces Gaussian noise into the data, adhering to a predetermined variance schedule delineated by \u1e9e1, ..., \u03b2\u03c4. This step-by-step addition of noise outlines the approximate posterior q(x1:T|20) within a structured mathematical formulation, which is specified as follows:\n$$q(x_{1:T}|x_0) := \\prod_{t=1}^{T}q(x_t|x_{t-1}), q(x_t|x_{t-1}) := N(x_t; \\sqrt{1 \u2013 \\beta_t}x_{t-1}, \\beta_\u03b5 I)$$\nThe iterative denoising process, also known as the reverse process, enables sample generation from Gaussian noised data, denoted as p(x\u0442) = N(x+; 0, I). This process is modeled using a Markov chain, where each step involves generating the sample of the subsequent stage from the sample of the previous stage based on conditional probabilities. The joint distribution of the model, po (x0:T), can be represented as follows:\n$$P_{\\theta}(x_{0:T}) := p(x_T) \\prod_{t=1}^{T}Po(x_{t-1}|x_t), Po(x_{t-1}|x_t) := N(x_{t-1}; \u03bc_o(x_t, t), \u03a3_o(x_t, t))$$\nIn the Diffusion Probabilistic Model, training is conducted via a reverse process that meticulously reconstructs the original data from noise. This methodological framework allows the Diffusion model to exhibit considerable flexibility and potent performance capabilities. Recent studies have further demonstrated that applying the diffusion process within a latent space created by an autoencoder enhances fidelity and diversity in tasks such as image inpainting and class-conditional image synthesis. This advancement underscores the effectiveness of latent space methodologies in refining the capabilities of diffusion models for complex generative tasks (Rombach et al., 2022). In light of this, the application of conditions and guidance to the latent space enable diffusion models to function effectively and to exhibit strong generalization capabilities."}, {"title": "QUALITATIVE DEMONSTRATION THROUGH MAZE2D RESULTS", "content": "Following the main section, we report more results in the Maze2D environments. We qualitatively demonstrate that DIAR consistently generates favorable trajectories."}]}