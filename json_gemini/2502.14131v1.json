{"title": "Gradients can train reward models: An Empirical Risk Minimization Approach for Offline Inverse RL and Dynamic Discrete Choice Model", "authors": ["ENOCH \u0397. \u039aANG", "HEMA YOGANARASIMHAN", "LALIT JAIN"], "abstract": "We study the problem of estimating Dynamic Discrete Choice (DDC) models, also known as offline Maximum Entropy-Regularized Inverse Reinforcement Learning (offline MaxEnt-IRL) in machine learning. The objective is to recover reward or Q functions that govern agent behavior from offline behavior data. In this paper, we propose a globally convergent gradient-based method for solving these problems without the restrictive assumption of linearly parameterized rewards. The novelty of our approach lies in introducing the Empirical Risk Minimization (ERM) based IRL/DDC framework, which circumvents the need for explicit state transition probability estimation in the Bellman equation. Furthermore, our method is compatible with non-parametric estimation techniques such as neural networks. Therefore, the proposed method has the potential to be scaled to high-dimensional, infinite state spaces. A key theoretical insight underlying our approach is that the Bellman residual satisfies the Polyak-\u0141ojasiewicz (PL) condition a property that, while weaker than strong convexity, is sufficient to ensure fast global convergence guarantees. Through a series of synthetic experiments, we demonstrate that our approach consistently outperforms benchmark methods and state-of-the-art alternatives.", "sections": [{"title": "1 Introduction", "content": "Learning from previously collected datasets has become an essential paradigm in sequential decision- making problems where exploration during interactions with the environment is infeasible (e.g., self-driving cars, medical applications) or leveraging large-scale offline data is preferable (e.g., social science, recommendation systems, and industrial automation) [Levine et al., 2020]. However, in such cases, defining a reward function (a flow utility function) that accurately captures the underlying decision-making process is often challenging due to the unobservable/sparse rewards [Zolna et al., 2020] and complexity of real-world environments [Foster et al., 2021]. To circumvent these limita- tions, learning from expert demonstrations has gained prominence, motivating approaches such as Imitation Learning (IL) and offline Inverse Reinforcement Learning (offline IRL) or equivalently, Dynamic Discrete Choice (DDC) model estimation2.\nWhile IL directly learns a policy by mimicking expert actions, it is susceptible to distribution shift, i.e., when the testing environment (reward, transition function) is different from the training environment. On the other hand, offline IRL aims to infer the underlying reward function that best explains expert behavior. Given this reward function, a new policy can be trained after a change in the environment's transition dynamics (e.g., modifications in recommendation systems) or in the reward function (e.g., marketing interventions). This capability enables offline IRL to be employed in counterfactual simulations, such as evaluating the effects of different policy decisions without direct experimentation. However, an imprecise reward function can lead to suboptimal policy learning and unreliable counterfactual analyses, ultimately undermining its practical utility. As a result, offline IRL's key metric becomes the precision of reward inference.\nWhile the precise reward function estimation objective has been studied in recent offline IRL literature, theoretically guaranteed existing methods have been limited to explicitly learning a transition model (e.g., Zeng et al. [2023]). However, if relearning the transition function is required every time it changes, the premise of IRL for counterfactual simulations may be undermined. The Dynamic Discrete Choice (DDC) literature in econometrics has separately explored the problem towards the goal of precise reward estimation [Aguirregabiria and Mira, 2007, Hotz and Miller, 1993, Rust, 1994, Su and Judd, 2012]. However, existing methodologies with theoretical precision guarantees suffer from the curse of dimensionality [Geng et al., 2023]: computational complexity exponentially grows as state dimension increases. Most importantly, in both IRL and DDC literature, theoretical guarantees of precise reward estimation have been limited to linear reward structures [Zeng et al., 2023] or monotone value function structure [Feng et al., 2020]. This motivates us to ask the following question:\nCan we propose a scalable gradient-based method to infer rewards (or Q* function) while provably ensuring global optimality with no assumption on reward structure/transition function knowledge?\nOur contributions. In this paper, we propose an Empirical Risk Minimization (ERM)\u2013based gradient- based method for IRL/DDC as an inverse Q-learning method. This method provably finds the true Q* function (up to statistical error, which diminishes at an $O(1/\\sqrt{N})$ rate with N samples) with $O(1/T)$ rate of convergence, where T is the number of gradient iterations. In addition, the true reward function can be computed from estimated Q* with no extra statistical or computational cost given the estimated Q* function. In developing this method, we make the following technical contributions:\n\u2022 We propose an empirical risk minimization (ERM) problem formulation, which we refer to as ERM-IRL in the IRL literature and ERM-DDC in the DDC literature, reflecting the shared problem. This formulation allows us to circumvent the need for explicit transition function estimation."}, {"title": "2 Related works", "content": "The equivalence between Dynamic Discrete Choice (DDC) models and entropy-regularized Inverse Reinforcement Learning (MaxEnt-IRL) was first identified by Ermon et al. [2015]. We provide a detailed exposition and the proof of equivalence in Section C.\n2.1 Dynamic discrete choice model estimation\nIn the econometrics literature, stochastic decision-making behaviors are usually considered to come from the random utility model [McFadden, 2001], which often assumes that the effect of unobserved covariates appear in the form of additive and conditionally independent randomness in agent utilities Rust [1994]. The seminal paper by Rust [Rust, 1987] pioneered this literature, demonstrating that a DDC model can be solved by solving a maximum likelihood estimation problem that runs above iterative dynamic programming. As discussed in the introduction, this method suffers computational intractability in terms of number of state dimensions.\nHotz and Miller [1993] introduced a method which is often called the two-step method conditional choice probability (CCP) method, where the CCPs and transition probabilities estimation step is followed by the reward estimation step. The reward estimation step avoids dynamic programming by combining simulation with the insight that differences in value function values can be directly inferred from data without solving Bellman equations. However, simulation methods are in principle trajectory-based numerical integration methods which also suffer scalability issues. Fortunately,"}, {"title": "3 Problem set-up and backgrounds", "content": "3.1 Offline Inverse Reinforcement Learning (Offline IRL) problem?\nConsider a single-agent Markov Decision Process (MDP) defined as a tuple (S, A, P, vo, r, \u03b2) where S denotes the state space and A denotes a finite action space, $P \\in \\mathbb{R}^{S\\times A \\times S}$ is a Markovian transition kernel, vo \u2208 As is the initial state distribution over S, r \u2208 RS\u00d7A is a reward function and \u03b2 \u2208 (0, 1) a discount factor. Given a stationary Markov policy $\u03c0 \\in \\Delta^{A}$, an agent starts from initial state so and takes an action ah \u2208 A at state sh \u2208 S according to ah ~ \u03c0 (\u00b7 | sh) at each period h. Given an initial state so, we define the distribution of state-action sequences for policy \u03c0 over the sample space (S \u00d7 A)\u221e = {(S0, A0, S1, A1, . . .) : Sh \u2208 S, ah \u2208 A, h \u2208 N} as P\u03c0. We also use Er to denote the expectation with respect to P\u03c0. Following existing literature [Fu et al., 2017, Geng et al., 2020, Ho and Ermon, 2016], we consider the entropy-regularized optimal policy, which is defined as\n$\u03c0^{*} := \\underset{\u03c0 \\in \\Delta^{A}}{argmax} \\mathbb{E}_{\u03c0}[\\sum_{h=0}^{\\infty} \u03b2^{h} (r(s_{h}, a_{h}) + \u03bbH(\u03c0(\\cdot | s_{h})))]$\nwhere H denotes the Shannon entropy and A is the regularization coefficient. Throughout, we make the following assumption on agents' decisions.\nASSUMPTION 3.1. When interacting with the MDP (S, A, P, vo, r, \u03b2), each agent follows the entropy-regularized optimal stationary policy \u03c0*.\nThroughout the paper, we use \u03bb = 1, the setting which is equivalent to dynamic discrete choice (DDC) model with mean zero T1EV distribution (Appendix C.5); all the results of this paper easily generalize to other values of \u03bb. Given \u03c0*, we define the value function V* as:\n$V^{*}(s) := \\mathbb{E}_{\u03c0^{*}}[\\sum_{h=0}^{\\infty}\u03b2^{h} r(s_{h}, a_{h}) | s_{0} = s]$\nSimilarly, we define the Q* function as follows:\n$Q^{*}(s, a) := r (s, a) + \u03b2 \\cdot \\mathbb{E}_{s'~P(s,a)} [V^{*}(s') | s, a]$\nGiven state s and policy \u03c0*, let q = [q1 ...q|A|] denote the probability distribution over the action space A, such that:\n$q_{a} = \\frac{exp (Q^{*}(s, a))}{\\sum_{a' \\in A} exp (Q^{*}(s, a'))}$\nfor a \u2208 A\n(1)\nThen, according to Assumption 3.1, the value function V* must satisfy the recursive relationship defined by the Bellman equation as follows:\n$V^{*}(s) = \\underset{q\\in \\Delta A}{max} {\\mathbb{E}_{a~q}[r(s, a) + \u03b2\\mathbb{E}_{s'~P(s,a)} [V^{*}(s') | s, a]] + H(q)}$.\nFurther, we can show that (see Appendix C.3):\n$V^{*}(s) = ln \\sum_{a \\in A} exp (Q^{*}(s, a))$\n$\\pi^{*}(a | s) = \\frac{exp (Q^{*}(s, a))}{\\sum_{a' \\in A} exp (Q^{*}(s, a'))}$ for a \u2208 A\nThroughout, we define a function Vo as\n$V_{0}(s) := ln \\sum_{a \\in A} exp (Q(s, a)))$"}, {"title": "4 ERM-IRL (ERM-DDC) framework", "content": "4.1 Identification through ERM\nWe now propose a one-shot Empirical Risk Minimization framework (ERM-IRL/ERM-DDC) to solve the IRL problem stated in Definition 3.1. We recast the IRL problem as an ERM problem as follows:\n$\\underset{Q \\in Q}{min} \\mathbb{E}_{(s,a)~\u03c0^{*}, \u03bd_{0}}[L_{NLL}(Q)(s, a) + \\mathbb{1}_{a=a_{s}} L_{BE} (Q) (s, a)]$,\n(5)\nwhere the first term, $L_{NLL}(Q)(s, a) := \u2013 log (p_{Q}(a | s))$, denotes the Negative Log-Likelihood (NLL) of Q with $p_{Q}(a | s) := \\frac{e^{Q(s,a)}}{\\sum_{\\tilde{a} \\in A}e^{Q(s,\\tilde{a})}}$, the second term, $L_{BE}(s, a)$, is the squared Bellman error, is from Definition 3.2, and as is defined in Assumption 3.2. We can thus write Equation (5) as follows:\n$\\underset{Q \\in Q}{min}\\mathbb{E}_{(s,a)~\u03c0^{*}, \u03bd_{0}}[\u2212log (p_{Q}(a | s)) + \\mathbb{1}_{a=a_{s}} (TQ(s, a) \u2013 Q(s, a))^{2}]$\nRemark. The joint minimization of the NLL term and BE term is the key novelty in our approach. Prior work on the IRL and DDC literature [Hotz and Miller, 1993, Zeng et al., 2023] typically minimizes the log-likelihood of the observed choice probabilities (the NLL term), given observed or estimated state transition probabilities. The standard solution is to first estimate/assume state transition probabilities, then obtain estimates of future value functions, plug them into the choice probability, and then minimize NLL term. In contrast, our recast problem avoids the estimation of state-transition probabilities and instead jointly minimizes the NLL term along with the Bellman-error term. This is particularly helpful in large-state spaces since the estimation of state-transition probabilities can be infeasible/costly in such settings. In Theorem 4.1, we show that the solution to our recast problem in Equation (5) identifies the reward function.\nTHEOREM 4.1 (IDENTIFICATION THROUGH ERM).\nThe solution to the ERM problem (Equation (5)) with any \u03bb > 0 uniquely identifies Q* up to s \u2208 \u015c and a \u2208 A, i.e., finds Q that satisfies $Q(s, a) = Q^{*}(s, a)$ for s \u2208 \u015c and a \u2208 A. Furthermore, we can uniquely identify r up to s \u2208 \u015c and a \u2208 A by $r(s, a) = Q(s, a) \u2013 \u03b2\\cdot \\mathbb{E}_{s'~P(s,a)} [V]$.\nEssentially, Theorem 4.1 ensures that solving Equation (5) gives the exact r and Q* up to \u015c and thus provides the solution to the IRL problem defined in Definition 3.1. See Appendix B.3 for the proof.\n4.2 Empirical challenge and the minimax resolution\nWhile the idea of ERM-IRL framework \u2013 minimizing Equation (5) \u2013 is straightforward, its main challenge comes from the difficulty of empirically approximating $L_{BE}(Q)(s, a) = (TQ(s, a) \u2013 Q(s, a))^{2}$ and its gradient. As discussed in Section 3.3, TQ is not available unless we know the transition function.\nAs a result, we have to rely on an estimate of T. A natural choice, common in TD-methods, is $T_{Q} (s, a, s') = r(s, a) + \u03b2 \\cdot V_{Q}(s')$ which is computable given Q and data D. Thus, a natural proxy objective to minimize is:\n$\\mathbb{E}_{s'~P(s,a)} [L_{TD} (Q) (s, a, s')] := \\mathbb{E}_{s'~P(s,a)} [(T_{Q} (s, a, s') \u2013 Q(s, a))^{2}]$\nTemporal Difference (TD) methods typically use stochastic approximation to obtain an estimate of this proxy objective [Adusumilli and Eckardt, 2019, Tesauro et al., 1995]. However, the issue with TD methods is that minimizing the proxy objective will not minimize the Bellman error in general (see Appendix B.1 for details), because of the extra variance term, as shown below.\n$\\mathbb{E}_{s'~P(s,a)} [L_{TD} (Q) (s, a, s')] = L_{BE}(Q)(s, a) + \\mathbb{E}_{s'~P(s,a)} [(TQ(s, a) \u2013 T_{Q} (s, a, s'))^{2}]$\nAs defined, T is a one-step estimator, and the second term in the above equation does not vanish even in infinite data regimes. So, simply using the TD approach to approximate squared Bellman error provides a biased estimate. Intuitively, this problem happens because expectation and square are not exchangeable, i.e., $\\mathbb{E}_{s'~P(s,a)} [\\delta_{Q} (s, a, s') | s, a]^{2} \\neq \\mathbb{E}_{s'~P(s,a)} [\\delta_{Q} (s, a, s')^{2} | s, a]$. To remove this problematic square term, we employ an approach often referred to as the \u201cBi-Conjugate Trick\u201d"}, {"title": "5 GLADIUS: Algorithm for ERM-IRL (ERM-DDC)", "content": "Denote N := |D| where D is a finite dataset. We define the empirical ERM-IRL objective, the finite-sample approximation of the ERM-IRL objective defined as equation (8), as follows:\n$\\underset{Q \\in Q}{min} \\underset{\\zeta \\in \\mathbb{R}^{S\\times A}}{max} \\frac{1}{N}\\sum_{(s,a,s') \\in D} [-log (p_{Q}(a | s)) + \\mathbb{1}_{a=a_{s}} \\{(T_{Q} (s, a, s') \u2013 Q(s, a))^{2} \u2013 \u03b2^{2} (V_{Q} (s') \u2013 \u03b6(s, a))^{2}\\}]$\n$= \\underset{Q \\in Q}{min} \\frac{1}{N} \\sum_{(s,a,s') \\in D} [-log (p_{Q}(a | s))\n+ \\mathbb{1}_{a=a_{s}} \\{(T_{Q} (s, a, s') \u2013 Q(s, a))^{2} \u2013 \u03b2^{2} \\underset{\\zeta \\in \\mathbb{R}^{S\\times A}}{min} \\sum_{(s,a,s') \\in D}(V_{Q} (s') - \u03b6(s, a))^{2}\\}]$\n(9)\nAlgorithm 1 solves the equation (9) through an alternating gradient ascent descent algorithm we call Gradient-based Learning with Ascent-Descent for Inverse Utility learning from Samples (GLADIUS). Given the function class Q of value functions, let $Q_{\u03981} \\in Q$ and $\u03b6_{\u03982} \\in \\mathbb{R}^{S\\times A}$ denote the functional representation of Q and \u03b6. Our goal is to learn the parameters $\u0398^{*} = {\u0398_{1}, \u0398_{2}}$, that together characterize Q and \u00a7. Each iteration in the GLADIUS algorithm consists of the following two steps:\n(1) Gradient Ascent: Update $\u0398_{2}$ based on the current value of $Q_{\u0398_{1}}$.\n(2) Gradient Descent: Update $\u0398_{1}$ based on the current value of $\u0398_{2}$.\nAfter a fixed number of gradient steps of $Q_{\u0398_{1}}$ and $\u0398_{2}$ (which we can denote as $Q$ and \u00a7), we can compute the reward prediction r as $r(s, a) = Q(s, a) \u2013 \u03b2\u011d(s, a)$ due to Theorem 4.3."}, {"title": "6 Theory and analysis of Algorithm", "content": "As discussed in the previous section, Equation (9) represents a mini-max optimization problem. Such problems are known to be globally solvable by a simple gradient ascent-descent algorithm when it is a concave-convex mini-max problem. However, the challenge is that Equation (8) is not a concave-convex mini-max problem. Given Q, it determines \u011f that serves as the Bayes-optimal estimator for $\\mathbb{E}_{s'~P(s,a)} [V_{Q} (s') | s, a]$. This implies that -$\\mathbb{E}_{s'~P(s,a)} [(V_{\\theta 2} (s') -\\zeta)^{2} | s, a]$ is strongly\nconcave in \u03b6. On the other hand, given such an optimal \u03b6, $L_{BE}(Q)(s, a)$ term is not convex in Q [Bas-Serrano et al., 2021]. The key result in this section is proving that both $L_{BE}(Q)(s, a)$ and $L_{NLL} (Q) (s, a) = [- log (p_{Q}(a | s))]$ satisfies the Polyak-\u0141ojasiewicz (PL) condition under certain assumptions, which is enough for Algorithm 1 to converge to global optima.\n6.1 Polyak-\u0141ojasiewicz (PL) in terms of Q\nThe Polyak-\u0141ojasiewicz (PL) condition prevents the gradient from vanishing prematurely, keeping optimization progress steady. That is, it nearly possesses the smooth, fast convergence behavior of strongly convex functions. Throughout, we use $||Q||_{L^{2} (\u03c0^{*},\u03bd_{0})} := (\\mathbb{E}_{(s,a)~\u03c0^{*},\u03bd_{0}} [Q(s, a)^{2}])^{\\frac{1}{2}}$ for Q \u2208 Q.\nDEFINITION 6.1 (POLYAK-\u0141OJASIEWICZ (PL) CONDITION WITH RESPECT TO $L^{2}$ NORM). A function f : Q \u2192 R is is said to satisfy the Polyak-\u0141ojasiewicz (PL) condition with respect to $L^{2}$ norm with measure \u03bc if f has a nonempty solution set and a finite minimal value f (Q*) for Q* \u2208 Q, and there exists some c > 0 such that $||\u2207_{Q}f(Q)||_{L^{2}(\u00b5)} \u2265 c(f(Q)\u2212 f(Q^{*}))$, \u2200x \u2208 X.\nRemark. Note, in this definition, we are identifying Q as a subset of $\\mathbb{R}^{S\\times A}$ hence the derivative is defined appropriately.\nTo prove PL, we need the following Lemmas which describes the behavior of $L_{NLL}(Q)$ and $L_{BE}(Q)$.\nLEMMA 6.1. $L_{NLL}(Q) := \\mathbb{E}_{(s,a)~\u03c0^{*}, \u03bd_{0}} [-log (p_{Q}(a | s))]$ is convex and Lipschitz smooth in Q in terms of $L^{2} (\u03c0^{*}, \u03bd_{0})$ norm.\nLemma 6.2. $L_{BE}(Q) := \\mathbb{E}_{(s,a)~\u03c0^{*}, \u03c9} [L_{BE}(Q)(s, a)]$ is of $C^{2}$ and Lipschitz smooth in Q in terms of $L^{2} (\u03c0^{*}, \u03bd_{0})$ norm.\nGiven Lemma 6.1 and 6.2, the following Theorems that $L_{NLL}(Q)$ and $L_{BE}(Q)$ satisfies PL condition.\nTHEOREM 6.3. For given s \u2208 S and a \u2208 A, $L_{BE}(Q)(s, a)$ satisfies PL condition with respect to Q. Furthermore, $L_{BE}(Q)$ satisfies the PL condition with respect to Q in terms of $L^{2}(\u03c0^{*}, \u03bd_{0})$.\nTHEOREM 6.4. For given s \u2208 S and a \u2208 A, $L_{NLL}(Q)(s, a)$ satisfies PL condition with respect to Q. Furthermore, $L_{NLL}(Q)$ satisfies the PL condition with respect to Q in terms of $L^{2} (\u03c0^{*}, \u03bd_{0})$.\nIn general, the sum of two PL functions is not necessarily PL. However, according to the following Lemma 6.5, our problem is a special case where such property holds,\nLEMMA 6.5. $L_{NLL}(Q) + \\mathbb{1}_{a=a_{s}} L_{BE}(Q)$ satisfies PL in terms of Q in terms of $L^{2} (\u03c0^{*}, \u03bd_{0})$.\nWe remark that this result by itself, establishes the PL condition in the tabular setting with finite states and actions where the Q function is parametrized as a vector/matrix in $\\mathbb{R}^{S\\times A}$. In the next section, we extend this to a more general hypothesis class.\n6.2 Polyak-\u0141ojasiewicz (PL) in terms of \u04e9\nWe now extend previous section to cases where the underlying state and action spaces are potentially featurized, i.e. S = $\\mathbb{R}^{dim(S)}$ and A = $\\mathbb{R}^{dim(A)}$. (In this case, $L^{2} (\u03c0^{*}, \u03bd_{0})$ norm is reduced to the (weighted) euclidean norm with dimension dim(S) + dim(A).) When dim(S) + dim(A) is very large, it is often preferable/necessary to approximate Q* using a set of parametrized functions $Q = {Q_{\u03b8} : \\mathbb{R}^{dim(S)+dim(A)} \\rightarrow \\mathbb{R} | \u03b8\\in\u0398 \\subseteq \\mathbb{R}^{d}, Q_{\u03b8} \\in F }$, where F denotes a class of functions such as linear, polynomial or deep neural network function class that is parametrized by 0. In this case, we make the following assumption often called the realizability assumption.\nASSUMPTION 6.1 (REALIZABILITY). Q contains an optimal function Q*, meaning there exists $\u03b8^{*} \\in \u0398$ such that $Q_{\u03b8^{*}} = Q^{*}$.\nUnder this parametrization, the ERM-IRL problem (the equation (5)) becomes\n$\\underset{\u03b8\\in\\mathbb{R}^{d}}{min} \\mathbb{E}_{(s,a)~\u03c0^{*}, \u03bd_{0}}[L_{NLL}(Q_{\u03b8})(s, a) + \\mathbb{1}_{a=a_{s}} L_{BE}(Q_{\u03b8})(s, a)]$\nOur next question is whether our previous result \u2013 showing that the equation (5) being PL in terms of Q - can ensure that this new equation is also PL in terms of 0, which is defined as follows.\nDEFINITION 6.2 (POLYAK-\u0141OJASIEWICZ (PL) CONDITION WITH RESPECT TO l2 NORM). Given $\\Theta \\subseteq \\mathbb{R}^{d}$, a function h : \u2299 \u2192 R is is said to satisfy the Polyak-\u0141ojasiewicz (PL) condition with respect to l2 norm if h has a nonempty solution set and a finite minimal value h(0*) for 0* \u2208 \u0398 \u2286 Rd, and there exists some c > 0 such that $||\u2207h(0)||_{2} \u2265 c(h(0)\u2212 h(0^{*}))$, \u2200\u03b8 \u2208 \u0398.\nIn this paper, we restrict our attention to the function class Q which satisfies the Assumption 6.2.\nASSUMPTION 6.2. For $Q_{\u03b8} \\in Q$,\n1. $Q_{\u03b8}(s, a)$ is continuously differentiable with respect to 0, meaning its Jacobian\n$DQ_{\u03b8} := \\frac{dQ_{\u03b8}(s, a)}{\u2202\u03b8} \\in \\mathbb{R}^{(dim(S)+dim(A))\\times d}$\nexists and is well-defined.\n2. There exists a constant m > 0 such that for all \u03b8 \u2208 \u0398,\n$\u03c3_{min} (DQ_{\u03b8}) \u2265 m$\nwhere $\u03c3_{min} (DQ_{\u03b8})$ is the smallest singular value of $DQ_{\u03b8}$.\nThe two lemmas show that Assumption 6.2 is easy to satisfy by popular function classes such as linear and the neural network function class.\nLEMMA 6.6. Let $Q_{\u03b8}(s, a) = \u03b8^{T}\u03c6(s, a)$, where the known feature mapping $\u03c6 : S \u00d7 A\u2194 \\mathbb{R}^{d}$, satisfies $||\u03c6(s, a)||$ \u2264 B almost surely with respect to (s, a) ~ $(\u03c0^{*}, \u03bd_{0})$ for some B > 0. Then dataset size |D| \u2265 Cd implies that Assumption 6.2 holds with probability at least 1 \u2013 $e^{-C|D|}$.\nLemma 6.7 ([PENNINGTON ET AL., 2017]). Let $Q_{\u03b8}$ be a deep nonlinear neural network composed of smooth activation functions (e.g., sigmoid) and linear layers parameterized by 0. When initialized using orthogonal weight initialization, $Q_{\u03b8}$ satisfies Assumption 6.2.\nThe following Theorem 6.8 shows that satisfying Assumption 6.1 and 6.2 is enough to achieve PL condition in terms of 0. That is, linear, polynomial, neural network-parametrization satisfies PL. This also subsumes the previous result in the tabular case with d = S \u00d7 A, the states encoded as standard basis vectors 0 = {$Q(s,a)$}s,a\u2208SXA.\nTHEOREM 6.8. Suppose that Assumption 6.1 and 6.2 are satisfied for O. Then the loss function $L_{NLL} (Q_{\u03b8}) + \\mathbb{1}_{a=a_{s}} L_{BE}(Q_{\u03b8})$ satisfies the Polyak\u2013\u0141ojasiewicz (PL) condition in terms of 0.\n6.3 Global convergence of GLADIUS\nDenote N := |D| where D is a finite dataset. We define $Q_{\u03b8^{*}}$ as the solution to the empirical ERM-IRL objective (equation (9)), i.e.,\n$Q_{\u03b8^{*}} \\underset{Q \\in Q}{argmin} \\underset{\\zeta \\in \\mathbb{R}^{S\\times A}}{max} \\frac{1}{N} \\sum_{(s,a,s') \\in D} [\u2212log (p_{Q}(a | s)) + \\mathbb{1}_{a=a_{s}} \\{(T_{Q} (s, a, s') \u2013 Q(s, a))^{2} \u2013 \u03b2^{2} ((V_{Q} (s') \u2013 \u03b6(s, a))^{2})\\}]$"}, {"title": "7 Simulation experiments", "content": "We now present results from simulation experiments, in which we compare the performance of our approach against a series of benchmark algorithms.\nIn the main text, we use the high-dimensional version of the canonical bus engine replacement problem ([Rust, 1994]) as the setting for our experiments. This setting has been extensively used as the standard benchmark for the reward learning problem in the DDC literature in economics [Aguirregabiria and Mira, 2002, Arcidiacono and Ellickson, 2011, Arcidiacono and Miller, 2011, Barzegary and Yoganarasimhan, 2022, Chernozhukov et al., 2022, Chiong et al., 2016, Geng et al., 2023, Hotz and Miller, 1993, Kasahara and Shimotsu, 2009, Norets, 2009, Reich, 2018, Su and Judd, 2012, Yang, 2024].\n7.1 Experimental Setup\nThe bus engine replacement problem [Rust, 1987] is a simple regenerative optimal stopping problem. In this setting, the manager of a bus company operates many identical buses. As a bus accumulates mileage, its per-period maintenance cost increases. The manager can replace the engine in any period (which then becomes as good, and this replacement decision re-sets the mileage to one). However, the replacement decision comes with a high fixed cost. Each period, the manager makes a dynamic trade- off between either replacing the engine or continuing with maintenance. We observe the manager's decisions for a fixed set of buses, i.e., a series of states, decisions, and state transitions. Our goal is to\nlearn the manager's reward function from these observed trajectories under the assumption that he made these decisions optimally.\nDataset. There are N independent and identical buses (trajectories) indexed by j, each of which has 100 periods over which we observe them, i.e., h \u2208 {1 . . . 100}. Each bus's trajectory starts with an initial mileage of 1. The only reward-relevant state variable at period h is the mileage of bus Xjh \u2208 {1, 2, . . . 20}.\nDecisions and rewards. There are two possible decisions at each period, replacement or continua- tion, denoted by djh = {0, 1}. djh = 1 denotes replacement, and there is a fixed cost 0\u2081 of replacement. Replacement resets the mileage to 1, i.e., the engine is as good as new. djh = 0 denotes maintenance, and the cost of maintaining the engine depends on the mileage as follows: 00xjh. Intuitively, the manager can pay a high fixed cost 0\u2081 for replacing an engine in this period but doing so reduces future maintenance costs since the mileage is reset to 1. In all our experiments, we set \u03b8\u2081 = 1 (maintenance cost) and 0\u2081 = 5 (replacement cost). Additionally, we set the discount factor to \u03b2 = 0.95.\nState transitions at each period. If the manager chooses maintenance, the mileage advances by 1, 2, 3, or 4 with a 1/4 probability each. If the manager chooses to replace the engine, then the mileage is reset to 1. That is, $P({X_{j(h+1)} = X_{jh} + k} | d_{jh} = 0) = 1/4, k \u2208 {1,2,3,4}$ and $P{x_{j(h+1)} = 1 | d_{jh} = 1} = 1$. When the bus reaches the maximum mileage of 20, we assume that mileage remains at 20 even if the manager continues to choose maintenance.\nHigh-dimensional setup. In some simulations, we consider a high-dimensional version of the prob- lem, where we now modify the basic set-up described above to include a set of K high-dimensional state variables, similar to Geng et al. [2023]. Assume that we have access to an additional set of K state variables {$s^{k}_{jh}$}$_{k=1}^{k}$ where each $s^{k}_{jh}$ is an i.i.d random draw from {-10, -9, . . ., 9, 10}.\nWe vary K from 2 to 100 in our empirical experiments to test the sensitivity of our approach to the dimensionality of the problem. Further, we assume that these high-dimensional state variables $s^{k}$ S do not affect the reward function or the mileage transition probabilities. However, the researcher does not know this. So, they are included in the state space, and ideally, our algorithm should be able to infer that these state variables do not affect rewards and/or value function and recover the true reward function.\nTraing/testing split. Throughout, we keep 80% of the trajectories in any experiment for train- ing/learning the reward function, and the remaining 20% is used for evaluation/testing.\nFunctional form. For all non-parametric estimation methods (including ours), we used a multi-layer perception (MLP) with two hidden layers and 10 perceptrons for each hidden layer for the estimation of Q-function. For parametric-oracle methods, we use the reward functions' true parametric form (as described earlier).\n7.2 Benchmark Algorithms\nWe compare our algorithm against a series of standard, or state-of-art benchmark algorithms in the DDC and IRL settings.\nRust (Oracle) Rust is an oracle-like fixed point iteration baseline that uses the nested fixed point algorithm [Rust, 1987]. It assumes the knowledge of: (1) linear parametrization of rewards by 0\u2081 and 02 as described above, and (2) the exact transition probabilities.\nML-IRL (Oracle) ML-IRL from Zeng et al. [2023] is the state-of-art offline IRL algorithm that minimizes negative log-likelihood of choice (i.e., the first term in Equation (5)). This method requires a separate estimation of transition probabilities, which is challenging in high-dimensional settings. So, we make the same Oracle assumptions as we did for Rust (Oracle), i.e., assume that transition probabilities are known. Additionally, to further improve this method, we leverage the finite dependence property of the problem [Arcidiacono and Miller, 2011], which helps avoid roll-outs."}, {"title": "8 Conclusion", "content": "In this paper, we propose a provably globally convergent empirical risk minimization framework that combines non-parametric estimation methods (e.g., machine learning methods) with IRL/DDC models. This method's convergence to global optima stems from our new theoretical finding that the Bellman error (i.e."}]}