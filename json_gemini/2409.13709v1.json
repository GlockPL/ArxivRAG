{"title": "Column Vocabulary Association (CVA): semantic interpretation of dataless tables", "authors": ["Margherita Martorana", "Xueli Pan", "Benno Kruit", "Tobias Kuhn", "Jacco van Ossenbruggen"], "abstract": "Traditional Semantic Table Interpretation (STI) methods rely primarily on the underlying table data to create semantic annotations. This year's SemTab challenge introduced the \"Metadata to KG\" track, which focuses on performing STI by using only metadata information, without access to the underlying data. In response to this new challenge, we introduce a new term: Column Vocabulary Association (CVA). This term refers to the task of semantic annotation of column headers solely based on metadata information. In this study, we evaluate the performance of various methods in executing the CVA task, including a Large Language Models (LLMs) and Retrieval Augmented Generation (RAG) approach, as well as a more traditional similarity approach with SemanticBERT. Our methodology uses a zero-shot setting, with no pretraining or examples passed to the Large Language Models (LLMs), as we aim to avoid a domain-specific setting. We investigate a total of 7 different LLMs, of which three commercial GPT models (i.e. gpt-3.5-turbo-0.125, gpt-40 and gpt-4-turbo) and four open source models (i.e. llama3-80b, 1lama3-7b, gemma-7b and mixtral-8x7b). We integrate this models with RAG systems, and we explore how variations in temperature settings affect performances. Moreover, we continue our investigation by performing the CVA task utilizing SemanticBERT, analyzing how various metadata information influence its performance. Initial findings indicate that LLMs generally perform well at temperatures below 1.0, achieving an accuracy of 100% in certain cases. Nevertheless, our investigation also reveal that the nature of the data significantly influences CVA task outcomes. In fact, in cases where the input data and glossary are related (for example by being created by the same organizations) traditional methods appear to surpass the performance of LLMs.", "sections": [{"title": "1. Introduction", "content": "Tabular data is the most common format used for data storage and sharing[1]. However, tabular data often lacks semantic annotations and can contain inaccurate or missing information. Semantic Table Interpretation (STI) aims to find semantic annotations for table cells and columns, as well as column relationships, using existing Knowledge Graphs (KGs). Semantic annotations are particularly important when used to enrich and augment metadata. In fact, several studies[2, 3, 4] have shown that high-quality metadata supports data Findability, Accessibility, Interoperability and Reusability (FAIR Guiding Principles) [5]. Rich metadata plays a critical role when dealing with confidential data, as the underlying data is not commonly openly and freely accessible. Enhancing the FAIRness for this type of data has gained more attention in recent years, and previous work has suggested that high-quality and rich metadata improves the discovery and reuse of these resources[6]. Nevertheless, the automatic enrichment of metadata when only the metadata is available is a challenging task because much of the contextual information is missing, and the underlying data cannot be used to help find the most appropriate annotations. Large Language Models (LLMs) could be a helpful solution in this by leveraging their training data as background knowledge. Moreover, Retrieval Augmented Generation (RAG) systems could further integrate external knowledge - for example from knowledge graphs, controlled vocabularies and glossaries - to enhance the LLMs flexibility across different domains. In this year SemTab challenge, participants in the \"Metadata to KG\" track aim to annotate tables using only table metadata (e.g. column and table names) without accessing the underlying data. This approach tests the ability to enrich metadata effectively under similar conditions imposed by restricted access data. To guide our investigation we have formulated the following research questions:\n\u2022\n\u2022\n\u2022\n\u2022\nHow do traditional semantic similarity methods compare to newer methods using Large Language Models (LLMs) in the semantic annotation of table metadata when the underlying data is not available?\nHow does the temperature setting of LLMs impact their performance in this task?\nHow do different combinations of metadata information in traditional methods affect their performance?\nHow does the nature of the input data and glossary influence the results?\nIn the pages that follow, we further describe the importance of metadata, especially in settings where the underlying data is not available. We also introduce the term \u201cColumn Vocabulary Association\", before discussing our main methodology and results."}, {"title": "1.1. Dataless tables", "content": "Recently we have seen more and more solutions for sharing data that is considered confidential or with restricted access. For example, there have been multiple developments of online Open Government Data (OGD) portals- e.g. Central Bureau for Statistics Netherlands (CBS)\u00b9, U.S. Government's Open Data\u00b2 and the Canada's Open Government Portal\u00b3 portals aimed at enhancing innovation and research, by allowing users to investigate population data. Through OGD portals, the general public and researchers are able to use this data in a variety of fields, such as journalism, software development and research [7]. The data in these portals is usually"}, {"title": "1.2. Column Vocabulary Association (CVA)", "content": "In the domain of Semantic Table Interpretation (STI), there are some well known challenged, including the Column Type Annotation (CTA), the Column Entity Annotation (CEA), and the Column Property Annotation (CPA) tasks. The CTA task involves identifying the semantic type (e.g. dates or geographical locations) of each column in the table. The CEA task, instead, involves linking each cell to an entity in a knowledge graph: for example, the cell containing the string \"New York\u201d to be linked to the WikiData entity for New York City (Q60). CPA requires the identification of relationships between columns of a table: for example, recognising that the columns with headers \"Mayor's Name\u201d and \u201cCity\u201d are related to each other by the property eg:isMayorOf. In this work we introduce a new term: the Column Vocabulary Association (CVA) task. This task differs significantly from the previous ones because it does not rely on any information from the underlying data within the table. Instead, it aims to associate column headers with entries in controlled vocabularies purely based on semantic similarities. The distinction between the word association and annotation is also important in this context. Annotation typically refers to the labelling of data with tags or categories. In contrast, with the term association we refer more on the conceptual linkage between the textual information in a column header, and an external knowledge repository. This approach emphasizes understanding and leveraging the semantic meaning of the column headers themselves, without using any underlying data. By focusing on semantic similarities, we aim to create a method for interpreting and integrating restricted access datasets, to facilitate metadata enrichment and data discovery."}, {"title": "2. SemTab Challenge", "content": "Since its inception in 2019, the Semantic Web Challenge on Tabular Data to Knowledge Graph Matching, known as SemTab, has become a competitive event focused on benchmarking systems and approaches that support and enhance Semantic Table Interpretation (STI). The SemTab challenge typically consists of two main tracks: the \u201cAccuracy Track\u201d and the \u201cDataset Track\". The Accuracy Track places participants in a specific context with a predefined set of input data. Participants submit solutions for table annotation tasks, including Column Type Annotation (CTA), Column Entity Annotation (CEA), and Column Property Annotation (CPA). Previous editions of the SemTab challenge have featured interesting solutions. For example, MTab uses a voting algorithm combines with probability models to improve annotations[10]. Similarly, TorchicTab enables the annotation of tables with diverse structures through the use of an external knowledge graph[11]. Another example is SemTex, which utilizes a hybrid annotation approach that improves performances, by analyzing relationships between entities in knowledge graphs and integrating them with gradient boosting analysis[12]. The Dataset Track, on the other hand, involves the submission of new datasets and bench- marks for evaluating various tasks related to the SemTab challenge and tabular data. In previous years, participants have produced versatile datasets applicable across multiple domains. One such example is the ToughTables Dataset, which consists of high-quality manually-curated tables with non-obviously linkable cells, such as those with ambiguous names, typos, and misspelled entity names [13]. There have also been domain-specific datasets. For example, the BiodivTab benchmark includes 50 datasets based on real-world biodiversity research data, en- hanced by manual annotation [14]. Another example is the TSOTSATable Dataset, that contains annotations of tables made using the FoodOn Ontology[15], Open Research Knowledge Graph (ORKG)[16], and Wikidata[17]. This year, the SemTab challenge introduced a new track: the \u201cMetadata to KG\u201d track. Par- ticipants of this task were asked to map table metadata to KGs without having access to the underlying data. This presents a unique challenge due to the limited available context, making traditional STI methods less applicable, as they typically rely on actual data for annotation. To better define this metadata-only task, we introduce the term Column Vocabulary Association (CVA). As further described in section 1.2,CVA involves annotating columns using solely KGs table metadata, without utilizing the underlying data. This approach is particularly relevant in scenarios where the data is confidential and cannot be accessed. The Metadata to KG track was divided into two rounds. Although the end goal remained the same (i.e. to annotate column headers with the appropriate term from KGs), each round features different input data and KGs. Below, we provide an explanatory summary of the two rounds."}, {"title": "2.1. Metadata to KG - Round 1", "content": "Round 1 of the \"Metadata to KG\" track required participants to map a set of table metadata to DBpedia properties. Participants were provided with tables metadata and DBpedia properties files in both JSONL and OWL formats, all of which were accessible in the following GitHub repository. The tables metadata file included information about 141 columns derived from different tables. For each column, the provided information included the column ID, column label, table ID, table name, and a list of the other column labels within the same table. The DBpedia properties file contained 2,881 properties. For each DBpedia property, the information included the property ID (the actual URI of the property in DBpedia), the property label, and the description. Below, we report examples of a table metadata entry (Listing 1) and of DBpedia property (Listing 2)."}, {"title": "2.2. Metadata to KG - Round 2", "content": "Round 2 of the \"Metadata to KG\" track introduced a level of complexity by using a collection of custom vocabularies for the mapping task. Participants were again provided with tables metadata and custom vocabularies files in JSONL and OWL formats, accessible in the following GitHub repository5. In this round, the tables metadata file contained 1181 entries (one entry corresponds to one column) from various datasets, with each column having the same information as in round 1 included: column ID, column label, table ID, table name, and the other columns labels. The custom vocabularies file consisted of 1192 entries, where each entry had, again, the same information as in round 1: ID (in this case not a URI, but a minted ID), label and description. The tables metadata included a very diverse set of topics: including but not limited to: COVID-19 clinical trials, Indian movies ratings and Saudia Arabia stock exchange data. As in Round 1,"}, {"title": "3. Methods", "content": "In this section, we outline our methodology, which considers the CVA task as a textual informa- tion retrieval challenge. Given that most of the table metadata (including label, table name, and table columns) and glossary information (label and description) are described in text, the goal is to retrieve the most similar glossary entries from the glossary file based on the table metadata. Our approach involves two main methods: one utilizing LLMs (both open and commercial) and another employing a traditional semantic similarity method using SentenceBERT. We experimented with three GPT models (gpt-3.5-turbo-0125 - gpt-40 - gpt-4-turbo), two LLama models (11ama3-70b - 11ama3-8b), a Gemma model (gemma-7b) and a Mixtral model (mixtral-8x7b). Additionally, we varied the temperature settings (0.5, 0.75, 1.0, 1.25, and 1.5) for the LLMs to examine the impact of creativity on task performance. All our tests were conducted in a complete zero-shot setting, with no pretraining of the models and no examples provided through assistant instructions or user prompts. This approach is a key feature of our method. We chose this strategy because we aim to develop a method that is not domain-specific. Pretraining models with specific examples from particular mappings and vocabularies could bias the reported accuracy towards that specific domain. Instead, we aim to propose a method that can be applied more in general settings, regardless of the data domain or vocabulary."}, {"title": "3.1. CVA with LLMs", "content": "Prompt Engineering Through trial and error, we developed effective prompts, both user queries and assistant instructions. We found that repeating some information from the assistant instructions within the prompt resulted in more precise results by ensuring the models only used the data we provided, thus minimizing hallucinations. Both the prompt and instructions specified to return the 5 most similar glossary entries for each metadata. Below, we show the instructions given to the assistants and the query template for the user prompt used in Round 1 of the SemTab Challenge. The instructions and prompts for Round 2, which are quite similar, can be found on the GitHub page."}, {"title": "Model-Temperature Selection", "content": "We ran the queries three times for each LLM and temperature combination, then evaluated the preliminary performance using an evaluation script and groundtruth provided by the organisers. Based on these results, we selected the best-performing LLM-temperature combination to compute results on the full dataset."}, {"title": "CVA on Full Metadata Set", "content": "In the first round, we input the complete glossary JSON file containing the DBpedia properties into the vector. We then processed 25 metadata entries at a time when using the OpenAI API, while for the open-source LLMs, each metadata entry was added individually. This approach was taken to minimize the cost of running some of the more expensive OpenAI models (40 and 4turbo). In the second round, given the larger size of the glossary, we split it into smaller, topic-based glossaries. We created 75 smaller glossary files and divided the full metadata set into 75 corresponding files. Each metadata file was then processed one at a time against the vector containing the 75 glossary files."}, {"title": "3.2. Semantic similarity using SentenceBERT", "content": "Our second method involved computing the semantic similarity between table metadata and glossary entries using SentenceBERT[18]. First, we generated a vector representation for each metadata and glossary entry. Next, we calculated the cosine similarity between the embedding of each table metadata and the glossary entries to identify the top 5 glossary entries with the highest cosine similarity scores. The initial steps of this method posed two challenges: 1) determining which table metadata and glossary information to use for generating the vectors, and 2) deciding how to vectorize the textual information - whether to concatenate all textual content before vectorization or to vectorize each part separately and then sum the vectors to form the final embedding. To address these questions, we experimented with different combinations of textual information, computed the cosine similarity, and evaluated the results against the groundtruth using the evaluation script provided by the organizers. Based on these findings, we selected the best performing combinations to use across the full metadata set."}, {"title": "3.3. Evaluation", "content": "The evaluation was conducted using a script provided by the track organizers. This script computed the accuracy of the generated mappings for a sample metadata file and a sample ground truth. It calculated two metrics: hit@1 and hit@5. To reiterate, users are supposed to generate the 5 most relevant mapping between the table metadata and the glossary, sorted from the most relevant. Hit@1 checks if the first mapping (thus the one considered to be the most relevant) is correct, while hit@5 checks if the correct mapping is among the top five results. Participants were then asked to generate the mappings for the entire table metadata file and submit them to the organizers. The organizers can then run the evaluation script again using the complete ground truth, which has not yet been shared with the participants."}, {"title": "4. Results", "content": "In the following sections, we present the preliminary challenge's results. These results show the accuracy scores obtained from the evaluation script on the sample metadata and the sample groundtruth provided. At this point, we are not aware on how our methods performed for the full set of metadata file, as the complete groundtruth has not yet been provided to participants."}, {"title": "4.1. CVA with LLMs", "content": "Here we present the results from our initial analysis using different LLMs and temperature settings. We employed three models from OpenAI and four open-source models, testing them at five different temperatures, as detailed in the table below1. The table shows the average accuracy results for each model-temperature combination, evaluated using the evaluation script with the sample metadata and sample ground truth. Each query was run three times per model-temperature combination, and accuracy results were then averages. The numbers in bold correspond to the best-performing model-temperature combinations. gpt-40 outperformed other models in both Rounds 1 and 2, specifically at temperatures 0.5, 0.75, and 1.0. We observed that the LLMs did not perform very well in Round 2. In the discussion section5 we explore possible reasons for this outcome. Based on these preliminary results, for Round 1, we used gpt-40 at temperatures 0.5, 0.75, and 1.0 on the full metadata file for final analysis. For Round 2, we used gpt-40 only at temperatures 0.5 and 0.75."}, {"title": "4.2. CVA with Sentence BERT", "content": "Below we show the results from our initial analysis with SentenceBERT. Table 2 includes the possible combinations of information from the table metadata and the glossary, and the accuracy results for both Round 1 and 2, which we obtained by running the evaluation script against the ground truth for the sample metadata file. We used these results to find the best performing combinations, which were then applied to the full metadata file. In Round 1, we did not have a single combination that performed best for both hit@1 and hit@5. The best hit@1 (0.56) is obtained when we use the column label and/or table name to represent the table metadata embedding, and use the property label for the DBpedia property embedding. The best hit@5 (0.67) is obtained when we use the sum of the vectors of the column label and the table name as the table metadata embedding, and use the vector of the property label as the DBpedia property embedding. For Round 2, the best hit@1 and hit@5 are both obtained when we use the sum of the vectors of the column label and the table name as the table metadata embedding, and encode the vocabulary description as the vocabulary embedding. Based on this results, we did perform SentenceBERT on the full data for Round 1. For Round 2, instead, we sent the results from SentenceBERT for final analysis using the setting with the sum of the vectors of the column label and the table name as the table metadata embeddings."}, {"title": "5. Discussion", "content": "During our analysis, we came across several interesting points that we would like to report in this section. Firstly, regarding the LLM prompt engineering, we found that repeating some of the same sentences in both the assistant instructions and user prompts improved the LLM's ability to follow instructions more precisely and avoid hallucinations. Specifically, this approach helped prevent the addition of entries not included in the glossaries. Additionally, we observed notable differences in the type of data included between the Round 1 and Round 2 of the challenge. In round 1, there was no clear link between the table metadata file and the glossary. The glossary consisted of DBpedia properties, while the table metadata appeared to be a random collection from various data sources. In round 2, instead, there was a very clear connection between the table metadata and the glossary. It seemed that the table metadata was designed to match the glossary or vice versa, likely because the data was collected from institutions/organizations/repositories that build their own glossaries to describe their data. These differences had an effect in the performances of the methods proposed in this work. LLMs, particularly gpt-40, performed much better in round 1, where it was important to leverage the LLM's background knowledge to find the most relevant mappings. In round 2, however, the semantic similarities method was sufficient and sometimes even outperformed LLMs. This was due to the high degree of semantic similarity between the column headers and the glossary, as both probably originated from the same institution and were intentionally made to be similar. Lastly, we want to highlight differences related to the temperature settings we used for the LLMs. We found that temperatures below 1 performed better than higher temperatures. Temperature regulates the LLM's creativity, where 0 represents no creativity and 2 indicates full creativity. In many cases, and particularly with gpt-4-turbo and gemma-7b, a temperature of 1.5 resulted in no outputs or error messages. This suggests that lower temperatures may lead to better performance for these types of tasks, although further investigation is needed."}, {"title": "6. Conclusion", "content": "In this study, we investigated different methods for mapping column headers to glossaries when the underlying data is unavailable. Our approach operates in a zero-shot setting, meaning we do not pretrain models or provide examples of correct mappings. This allows us to evaluate model performance across different domains more broadly. We introduce the concept of \u201cColumn Vocabulary Association\u201d (CVA) and distinguish it from other STI tasks. Additionally, we analyze how different temperatures of LLMs and the types of input data and glossaries impact the performance in the CVA task. Our findings suggest that LLMs perform well when there are no clear connections between the data and glossaries used for mapping (as observed in Round 1), leveraging their extensive background knowledge. Instead, for CVA tasks where the input metadata and glossaries are closely related (as in Round 2), traditional semantic similarity methods (e.g. SentenceBERT) may perform better than LLMs. Preliminary results indicate that open source models still lag behind commercial ones, with less accurate performance compared to the GPT models. In future work we aim to further evaluate LLM performance by calculating additional metrics introduced in [19] to assess the consistency of the LLM performances."}]}