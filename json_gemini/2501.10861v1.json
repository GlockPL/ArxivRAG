{"title": "Dynamic Continual Learning: Harnessing Parameter Uncertainty for Improved Network Adaptation", "authors": ["Christopher F. Angelini", "Nidhal C. Bouaynaya"], "abstract": "When fine-tuning Deep Neural Networks (DNNs) to new data, DNNs are prone to overwriting network parameters required for task-specific functionality on previously learned tasks, resulting in a loss of performance on those tasks. We propose using parameter-based uncertainty to determine which parameters are relevant to a network's learned function and regularize training to prevent change in these important parameters. We approach this regularization in two ways: (1), we constrain critical parameters from significant changes by associating more critical parameters with lower learning rates, thereby limiting alterations in those parameters; (2), important parameters are restricted from change by imposing a higher regularization weighting, causing parameters to revert to their states prior to the learning of subsequent tasks. We leverage a Bayesian Moment Propagation framework which learns network parameters concurrently with their associated uncertainties while allowing each parameter to contribute uncertainty to the network's predictive distribution, avoiding the pitfalls of existing sampling-based methods. The proposed approach is evaluated for common sequential benchmark datasets and compared to existing published approaches from the Continual Learning community. Ultimately, we show improved Continual Learning performance for Average Test Accuracy and Backward Transfer metrics compared to sampling-based methods and other non-uncertainty-based approaches.", "sections": [{"title": "I. INTRODUCTION", "content": "The term \"Narrow AI\" is gaining traction for describing Artificial Intelligence (AI) and Machine Learning (ML) systems that cannot adapt to information after deployment. In standard training schemes, Deep Neural Networks (DNNs) assume the collection of observations a network is trained on will accurately describe the environment in which it is deployed. In the real world, DNNs are regularly subject to out-of-distribution (OOD) data, various types of noise, shifting distributions of conceptual objectives, and may require adaptation to new data after the initial training period [1]. These characteristics of real-world data undermine assumptions of data consistency made during training and require DNNs to have the ability to adapt and expand upon previously learned data representations.\nUnlike biological learning systems, which can adapt and consolidate learned information at will, standard machine learning systems restrict network performance to the most recently trained task [2], [3]. Fine-tuning networks to new information generally results in a partial or complete loss of performance on previously trained tasks, known as Catastrophic Forgetting or Catastrophic Interference [4], [5]. This phenomenon arises from overwriting and replacing network parameters during the training process for a new task. As a result, a sub-discipline of machine learning called Continual Learning (CL), or Lifelong Learning, has emerged, focusing on mitigating Catastrophic Interference in DNNs over a sequence of tasks.\nCurrent approaches to CL can be categorized into four main categories: Regularization, Dynamic Architectures, Rehearsal, and Dual Memory Systems [3]. Regularization-based approaches restrict parameter change when training subsequent tasks attempting to preserve previous network representations [2], [6]\u2013[8]. Dynamic Architecture approaches add neural resources to expand on previous representations [9], [10]. Rehearsal approaches preserve encompassing data samples that describe most information learned from previous tasks, supplementing training new tasks with these selected data samples [11]\u2013[13]. Dual Memory Systems consolidate information from short-term, temporary to long-term, permanent memory to enable rapid adaptation while preserving previously encountered information [14]\u2013[17].\nMethods described in this paper will focus on regularization-based approaches to avoid undesirable characteristics of other approaches, such as storing previous data, adding neural resources for new tasks, or complex network and optimization structures. At the limit, allowing for such characteristics would create an ever-growing storage or computational resource requirement unsuitable for many real-world applications. Regularization-based approaches focus on constraining the optimization process when learning a new task to avoid significant changes to parameters learned in previous tasks. Most regularization approaches rely on identifying important parameters within the network and adjusting regularization according to the relevance of each parameter to a previous task. This contrasts with applying a uniform factor across all network parameters."}, {"title": "II. MOMENT PROPAGATION FRAMEWORK", "content": "MP leverages principles from VI to approximate the variational posterior of a network by imposing an approximating distribution, also known as variational distribution, $q_\\theta(\\Omega)$, over the network parameters. The variational distribution is then optimized by minimizing the Kullback\u2013Leibler (KL) divergence between the variational distribution and the true posterior distribution, $p(\\Omega|D)$, where $D = \\{X^{(i)},y^{(i)}\\}_{i=1}^N$, represents the training data with $i^{th}$ input $X^{(i)}$ and corresponding label $y^{(i)}$. However, given that minimizing the KL divergence between the variational distribution and the true posterior is intractable due to the log evidence, an equivalent form must be considered. The log evidence can be isolated as it does not depend on the variational distribution nor will it affect the optimization. A tractable and equivalent objective called the Evidence Lower Bound (ELBO) can be maximized in place of the original KL divergence between the variational distribution and the true posterior distribution, shown in Equation (1).\n$\\text{ELBO} = E_{q_\\theta(\\Omega)}[\\ln p(D|\\Omega)] - \\text{KL}_{q_\\theta(\\Omega)}[q_\\theta(\\Omega)||p(\\Omega)]$\nWhen maximizing the ELBO it's components, the network log-likelihood, $E_{q_\\theta(\\Omega)}[\\ln p(D|\\Omega)]$, is maximized and the KL divergence between the network prior and the variational distribution, $\\text{KL}_{q_\\theta(\\Omega)}[q_\\theta(\\Omega)||p(\\Omega)]$, is minimized. Existing approaches to VI for DNNs [20] rely on approximating the network's log-likelihood by creating variations in the predictive distribution through Monte Carlo sampling of the approximated variational posterior. A distribution is then fit over these predictions to approximate the predictive distribution, allowing for the estimation of the network log-likelihood and the resulting gradient update of the network parameters. However, this process requires performing inference for each requested sample, increasing the time and computation requirements as the number of samples increase producing a trade-off between network efficacy and efficiency.\nThis trade-off can be circumvented by allowing parameters to contribute their learned uncertainty to features as they are transmitted through the network, thereby accumulating uncertainty in the network's predictive distribution. We introduced a framework that propagates the first two moments of the predictive distribution through the non-linear layers of the network using a first-order Taylor Series approximation for variance, called Moment Propagation (MP) [18]. This method involves learning the mean and variance of network parameters in an online fashion while enabling these parameters to influence the predictive mean and covariance. MP facilitates the propagation of an analytical expression for both the mean and covariance moments, allowing the predictive distribution to be determined without the need for sampling. Consequently, the stochasticity typically associated with the sampling process is removed from the predictive distribution estimation permitting the uncertainty in the predictive distribution to be solely based on uncertainty contributed by the network parameters. The propagation of uncertainty through this method provides an effective, deterministic measure for both mean and covariance, yielding a consistent and repeatable assessment of predictive uncertainty that is directly differentiable. Overall, MP enhances estimates of both posterior and predictive distributions with unbiased, directly differentiable evaluations of the network log-likelihood.\nIn the following sections, MP is derived for a convolutional neural network with $L$ layers. To streamline notation, the reference to the layer $l$ is excluded from the representation for the $l^{th}$ layer. The derivations for various network layers are presented while assuming the following:\n\u2022 Without loss of generality, the input feature to the network at layer $l = 0$, convolutional or linear, is treated as deterministic.\n\u2022 The $j^{th}$ network parameter $w_j$ follows a Normal distribution $w_j \\sim N(\\mu_{\\omega_j}, \\sigma_{\\omega_j})$.\n\u2022 The network parameters are independent of each other and the input."}, {"title": "III. WEIGHT UNCERTAINTY", "content": "After training, parameters essential for a task are expected to exhibit reduced uncertainty. Conversely, parameters unessential for the task are expected to gravitate towards the sparsity-inducing prior, resulting in increased uncertainty. To measure a parameter's relative importance to a DNN's function, two forms of uncertainty are considered: the variance of each parameter, $\\sigma_{\\omega_i}^2$, and the Signal-to-Noise Ratio (SNR) of each parameter, $\\text{SNR}_{w_i} = |\\mu_{\\omega_i}| / \\sigma_{\\omega_i}^2$. Parameter importance is inversely proportional to a parameter's variance and proportional to a parameter's SNR. MP's ability to self-determine important parameters is demonstrated by observing a cumulative distribution function (CDF) of both parameter SNR and variance from a trained two 800-node hidden layer fully connected network. The CDFs for parameter uncertainty are then correlated to the network's ability to prune parameters based on each parameter's learned uncertainty.\nAlthough parameters aligning with the prior may appear equivalent, it doesn't necessarily imply insignificance to the network's functional approximation. To demonstrate parameter importance to a learned network function, parameters are ordered and pruned according to their SNR and variance at various percentages of the total network parameters. The subsequent impact on network performance is then assessed.\nDuring pruning, all network parameters are ordered based on importance, regardless of layer, as there is no guarantee that important parameters will be evenly distributed across all layers. This approach allows for more efficient pruning without compromising performance. The performance of the pruned network, considering SNR and variance-based pruning, is compared against random pruning (the lower performance bound) and pruning based on the smallest absolute value. Additionally, the performance of the MP framework is compared to Bayes-by-Backprop (BBB) and standard deterministic networks, both featuring the same two hidden layer architecture. For MP and BBB frameworks, smallest absolute value-based pruning is performed on parameters using the mean of the parameter's approximating distribution.\nBased on the pruning performance of the MP framework, trained MP networks can accurately discern important parameters via learned parameter uncertainty. This inherent ability for self-determination of relevant parameters is a crucial tool for CL. Important parameters can be appropriately regularized to avoid changes and prevent catastrophic interference, ensuring the retention of knowledge from previous tasks. In contrast, unimportant parameters, which have demonstrated minimal contribution to network predictions, can be used for learning subsequent information, making MP beneficial in Continual Learning scenarios."}, {"title": "IV. BAYESIAN CONTINUAL LEARNING", "content": "Continual Learning (CL) can be divided into three sub-categories: Task Incremental Learning, Domain Incremental Learning, and Class Incremental Learning, where the premise of learning and retaining information over multiple training periods remains the same, but how context is provided changes in each scenario [23]. Methods described in this paper will focus on Task Incremental Learning, in which the context $c$ for the input $X$ changes over time and is provided during training and inference periods. The output space $y$ is separated for each input context, producing the mapping $f : X \\times C \\rightarrow y$ [23]. This additional task context is only leveraged for the multi-headed network architecture in which task information is required to choose the appropriate output head and is not used to select which network parameters are active."}, {"title": "A. Learning Rate Adaptation", "content": "Initially explored by Ebrahimi et al. [24], Learning Rate Adaptation (LRA) leverages learned parameter uncertainty to adapt the learning rates of individual parameters according to their relevance to the network's functional approximation. Lower learning rates for important network parameters ideally prevents catastrophic interference in these parameters by restricting change while allowing unimportant parameters to learn new information freely. Our approach to Learning Rate Adaption replaces the Bayes-by-Backprop (BBB) framework with the Moment Propagation framework. Instead of using a BBB parameter's latent sampling distribution to determine parameter importance, parameter importance is determined by a random variable that directly contributes to the network's predictive distribution.\nLearning rates for each parameter are determined by mapping parameter importance from all network parameters to a user-defined range of learning rates. First, parameter importance is determined according to the parameter's variance, $1/\\sigma_{\\omega_i}^2$, or SNR, $|\\mu_{\\omega_i}| / \\sigma_{\\omega_i}^2$. The resulting importance values are remapped across the whole network, excluding the classification head, instead of across each layer given there is no guarantee that parameter importance will be even distributed across all layers. The remapping function is shown in Equation (7), where $a_{t+1}$ represents the vector of all parameter learning rates for the next task, $l_t$ represents the vector of all parameter importance values from task $t$. The min and max user-defined range of learning rate values are defined as $a_{min}$ and $a_{max}$. Learning rates for each parameter's mean and variance are updated synchronously because the approximating distribution for each parameter is treated as a single parameter. Ultimately, the mean and variance of the most important parameters will receive the lowest user-defined learning rate, restricting any change in the parameter. The mean and variance of the most uncertain parameters will receive the highest, allowing these parameters to learn freely.\n$a_{t+1} \\leftarrow \\frac{( (l_t - \\min(l_t)) (a_{\\max} - a_{\\min}) )}{(\\max(l_t) - \\min(l_t))} + a_{\\min}$"}, {"title": "B. Per-Parameter Bayesian Inference", "content": "Our second approach, Per-Parameter Bayesian Inference (PPBI), leverages the same concept of leveraging parameter uncertainty but instead changes the weighting of the KL regularization term within the ELBO. Drawing inspiration from Ebrahimi et al. [24], Ahn et al. [25], and Nguyen et al. [26], this framework performs approximate Bayesian Inference on a per-parameter basis guided by parameter uncertainty without any changes to the regularization term. PPBI applies a similar methodology to Learning Rate Adaption; however, for each new task, a KL regularization term weighting value is applied according to each parameter's importance. Before training a task the network prior is replaced with the previous tasks learned posterior. To control the explicit regularization of every parameter, regularization weights are adjusted based on each parameter's importance by mapping parameter importance values to a user-defined range of regularization weighting values. For this technique, the most important parameters will map to the highest user-defined weighting, heavily restricting change from the previous tasks posterior (the prior), while the least important parameters will map to the lowest user-defined weighting, allowing those parameters to change easily to maximize the network log-likelihood for the current task. For PPBI, the remapping function is shown in Equation (8).\n$\\Gamma_{t+1} \\leftarrow \\frac{( (l_t - \\min(l_t)) (\\Gamma_{\\min} - \\Gamma_{\\max}) )}{(\\max(l_t) - \\min(l_t))} + \\Gamma_{\\max}$"}, {"title": "C. Experimental Setup", "content": "1) Datasets and Networks: Our CL methodologies are evaluated for eight different CL benchmark datasets of increasing difficulty: Two Split MNIST, Five Split MNIST, Permuted MNIST, Two Split CIFAR10, Five Split CIFAR10, Mixed CIFAR10-CIFAR100, and a sequence of eight datasets. Split MNIST and CIFAR10 datasets consist of each base dataset separated into two tasks of five classes and five tasks of 2 classes, for two split and five split, respectively. The Permuted MNIST dataset consists of ten different pixel level permutations applied to the entire base MNIST dataset, resulting in ten tasks of ten classes each. Mixed CIFAR10-CIFAR100 combines the base CIFAR10 and CIFAR100 benchmark datasets and alternates between tasks of two classes from CIFAR10 and twenty classes from CIFAR100, for ten tasks. Finally, a sequence of eight datasets is evaluated, consisting of MNIST, CIFAR10, CIFAR100, NotMNIST, SVHN, Traffic Signs, FaceScrub, and FashionMNIST where each task is a new dataset.\nFor all sequential approaches, 15% of the training set for each task is reserved for validation, while the test set is exclusively used for testing after task training is complete. The order of each dataset is randomized for each epoch, including for joint training. All datasets are normalized to the mean and the standard deviation of the full dataset before splitting. For split CIFAR10 and mixed CIFAR10/CIFAR100, a random crop with a padding of 4 pixels and a random horizontal flip are added to assist with generalization. These transforms are added to existing approaches for a fair comparison. Transform was not applied to the sequence of eight datasets, as it adversely affected performance, but datasets are padded to a image size of 32x32 pixel, and input data consisting of one channel are replicated across two additional channels for a consistent input image size of 32x32x3.\nA two 800-node hidden layer fully connected network architecture is used across all approaches to compare continual learning performance for two-split, five-split, and permuted MNIST datasets. An AlexNet Convolutional Neural Network architecture is used for two-split and five-split CIFAR10, mixed CIFAR10-CIFAR100, and the sequence of eight datasets. We recollect all results for these architectures and datasets, slightly improving on some previously reported results due to differences in architectures used.\n2) Hyperparameters: A grid search is performed for hyperparameters to maximize the performance across all tasks. Given that the variance of each parameter directly contributes to the predictive distribution, the initialization of these values has a significant impact on overall performance. Thus, parameter variance initialization is searched between $\\sigma^2 \\leftarrow [-10, -18]$.\nRegularization toward the prior also has a significant impact on performance. The initial KL weighting is searched between $\\Gamma_0 \\leftarrow [1e-3, 1e-8]$ and is initially applied to all parameters regardless of approach. Values higher than 1e-3 typically cause too much regularization toward the prior, resulting in poor performance due to the network's inability to learn a sufficiently complex representation for the task. For LRA, the maximum learning rate for the mapping to the user-defined range is searched between $a_{\\max} \\leftarrow [1e-3, 1e-5]$. For KL Weight Adaptation, the maximum KL weighting for the mapping to the user-defined range is searched between $\\Gamma_{\\max} \\leftarrow [1e-2, 1e-7]$. The minimum learning rate and KL weighting are both set to 1e-12 and are not tuned. The number of epochs and batch size were fixed at 250 and 500, respectively. A large batch size was chosen because more performance was retained in deterministic baseline tests with Fine Tuning and enabled more efficient use of computational resources."}, {"title": "D. Performance Measurement", "content": "Performance is gauged through the Average Test Classification Accuracy (ACC) and Backward Transfer (BWT). Average Test Classification Accuracy is an average of all test accuracies on individual tasks after training all tasks. Backward Transfer indicates how much learning new information has affected performance on previous tasks. Backward Transfer values less than zero indicate catastrophic forgetting, while values greater than zero indicate improved performance on previous tasks after training on new information [24]. These metrics are shown in Equation 14.\n$\\text{BWT} = \\frac{1}{t-1} \\sum_{i=1}^{t-1} R_{t,i} - R_{i,i}$\n$\\text{ACC} = \\frac{1}{t} \\sum_{i=1}^{t} R_{i,t}$"}, {"title": "V. RESULTS AND DISCUSSION", "content": "Our CL methodologies are compared to parameter uncertainty-based methods: Uncertainty-Based Continual Learning (UCL) [25] and Uncertainty-Guided Continual Learning (UCB) [24]. We also compare our methods to long-standing standards for continual learning: Elastic Weight Consolidation (EWC) [2], Memory Aware Synapses (MAS) [7], Synaptic Intelligence (SI) [6], and Hard Attention to Task (HAT) [8]. These approaches to catastrophic interference mitigation are compared to baselines using Moment Propagation Framework: Fine Tuning (FT), where no efforts are made to mitigate catastrophic interference, Feature Freezing (FF), where all but the classification head is frozen after training the first task, and Joint Training (JT), where tasks are trained sequentially, but jointly with previous tasks. Fine Tuning represents the lower bound on performance, indicating performance was not gained over sequential training, while Joint Training represents the theoretical upper bound on performance.\nOur MP-based LRA and PPBI methodologies outperform their sampling-based predecessors, UCB and UCL, respectively. This performance improvement can be attributed to better measures of parameter uncertainty resulting from the MP framework. In UCB, performance improved as the number of samples of the predictive distribution increases [24]. Approximating the predictive distribution via the propagated moments improves measures of the network log-likelihood. The directly differentiable network log-likelihood then improves gradient updates of the network parameter, improving the measure of parameter uncertainty and, thus, parameter importance. Similarly, UCL only leverages one sample of the predictive distribution to estimate the network likelihood [25] but outperforms UCB on more complex benchmark datasets by heavily relying on changes made to the KL regularization term in the ELBO to further restrict parameter change. Additionally, our methods perform on par with HAT's Average Test Accuracy despite not having a propagation mask to select which features are used for which tasks.\nDespite improved performance over previous approaches, results for LRA and PPBI only show a marginal performance improvement over hyperparameter tuning the FF method. This implies that freezing a MP DNN after training the first task and learning a new classification layer can provide reasonable Task Incremental Learning performance. This trend differed slightly for the mixed CIFAR10 CIFAR100 dataset, where the ACC for FF was lower than uncertainty-based methods, indicating the network did not gain enough information from the training of the first task, two classes of CIFAR10, to sufficiently adapt to CIFAR100 based tasks with only learning the classification layer."}, {"title": "A. Split and Permuted MNIST", "content": "PPBI slightly outperforms LRA for split and permuted MNIST benchmark sets. For two-split MNIST, PPBI achieves an Average Test Accuracy of 99.40% and a Backward Transfer of -0.05% with a Variance-based uncertainty metric. For five-split MNIST, PPBI achieves an Average Test Accuracy of 99.84% and a Backward Transfer of -0.04% with an SNR-based uncertainty metric. For permuted MNIST with ten tasks, PPBI achieves an Average Test Accuracy of 98.19% and a Backward Transfer of -0.25% with a variance-based uncertainty metric. The restrictive nature of Bayesian Inference minimizing the KL divergence between the variational posterior and prior may have helped in this scenario, given that all tasks are distributed. Thus, restricting change from one task to the next may have had less impact than more advanced tasks."}, {"title": "B. Split CIFAR10 and Mixed CIFAR10/100", "content": "Conversely, for Split CIFAR10 and Mixed CIFAR10/CIFAR100, LRA sightly outperforms PPBI. For two-split CIFAR10, LRA achieves an Average Test Accuracy of 90.83% and a Backward Transfer of -0.30% with a Variance-based uncertainty metric. For five-split CIFAR10, LRA achieves an Average Test Accuracy of 92.17% and a Backward Transfer of -0.28% with a Variance-based uncertainty metric. For mixed CIFAR10 CIFAR100, LRA achieves an Average Test Accuracy of 77.16% and a Backward Transfer of -0.29% with a Variance-based uncertainty metric. We attribute these results to the same characteristics of Per-parameter Bayesian Inference, which benefited Split and Permuted MNIST tasks. However, in the case of CIFAR10 and CIFAR100 datasets separating classes into tasks may require parameters to diverge from the previous tasks posterior. Thus, PPBI may cause unwanted regularization to the network prior. Learning Rate Adaptation avoids this problem by restricting all changes in important parameters while allowing unimportant parameters to move away from the network posterior to the previous task."}, {"title": "C. Sequence of Eight Datasets", "content": "For the Sequence of Eight Datasets PPBI and LRA, perform similarly. LRA provides the maximum Average Test Accuracy at 77.40% with a Backward Transfer of -0.64% with a variance-based uncertainty metric. PPBI and LRA significantly outperform all other methods apart from HAT."}, {"title": "VI. CONCLUSION", "content": "In this work, two Continual Learning methodologies are presented that leverage learned parameter uncertainty derived from a Moment Propagation framework to regularize training of new tasks to prevent Catastrophic Forgetting. These methods are detailed by deriving the custom layers for a basic Convolutional Neural Network from the principles of Variational Inference. The concept of parameter importance through learned uncertainty from the Moment Propagation framework's is demonstrated and applied to Continual Learning through two methodologies to Learning Rate Adaptation and Per-parameter Bayesian Inference. While these approaches leverage learned parameter importance in different ways, both mitigate catastrophic forgetting through regulariz the learning of network parameters. These methods were evaluated on multiple sequential benchmark datasets, and performance was compared to other similar previously published approaches. Ultimately, Learning Rate Adaptation and Per-parameter Bayesian Inference outperform or yield comparable results to existing approaches through improved measures of parameter uncertainty."}]}