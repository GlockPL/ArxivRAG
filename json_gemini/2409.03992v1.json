{"title": "Confidential Computing on nVIDIA H100 GPU: A Performance Benchmark Study", "authors": ["Jianwei Zhu", "Hang Yin", "Shunfan Zhou"], "abstract": "This report evaluates the performance impact of enabling Trusted Execution Environments (TEE) on NVIDIA H100 GPUs for large language model (LLM) inference tasks. We benchmark the overhead introduced by TEE mode across various models and token lengths, focusing on the bottleneck caused by CPU-GPU data transfers via PCIe. Our results show that while there is minimal computational overhead within the GPU, the overall performance penalty is primarily due to data transfer. For most typical LLM queries, the overhead remains below 5%, with larger models and longer sequences experiencing near-zero overhead.", "sections": [{"title": "1 Introduction", "content": "Trusted Execution Environments (TEEs), are increasingly important in machine learning and AI due to rising security requirements across both enterprise and decentralized applications. The introduction of TEE-enabled GPUs, such as the NVIDIA H100, adds an extra layer of protection for sensitive data but may affect performance. Understanding these trade-offs, especially for large-scale machine learning tasks, is crucial for adopting TEE in high-performance AI applications.\nThis report quantifies the performance overhead of enabling TEE on the NVIDIA H100 GPU during LLM inference tasks, examining where the overhead arises and under what conditions it can be minimized."}, {"title": "2 Background", "content": null}, {"title": "2.1 Trusted Execution Environment", "content": "A TEE is a hardware-based security feature that isolates computations, preventing unauthorized access and tampering even from the operating system or the physical hardware owner. As the core technology to enable Confidential Computing, TEEs create secure enclaves where sensitive data and code are processed with encryption, ensuring confidentiality and integrity even if the broader system is compromised. Traditionally implemented in CPUs, TEE technology was extended to GPUs by NVIDIA in 2023, enabling tamper-proof and confidentiality-preserving computation inside the GPU with minimal performance penalty."}, {"title": "2.2 nVIDIA H100 GPU", "content": "The NVIDIA H100 GPU represents a significant milestone as the first GPU to support TEE. In TEE mode, the H100 operates in an isolated and secure environment where data transfers between the CPU and GPU are encrypted. This is facilitated by \"bounce buffers,\" which protect all inputs and outputs during transit between the CPU's encrypted memory and the GPU's internal memory."}, {"title": "2.3 Performance Impact", "content": "Enabling TEE on the NVIDIA H100 GPU introduces performance overheads primarily due to additional encryption and decryption during secure data transfer. While the GPU's internal computation remains unaffected, the main bottleneck lies in the CPU-GPU IO, particularly when data is exchanged via PCIe. This impact varies with the size of the data transfer. The following sections present experimental results quantifying these effects across various use cases.\nWith the TEE-enabled NVIDIA H100 GPU, it becomes crucial to quantify performance trade-offs during practical use cases. In the next section, we outline the methodology used to assess the performance impact of TEE during LLM inference tasks."}, {"title": "3 Methodology", "content": "To evaluate the performance overhead, we conducted experiments comparing inference throughput and latency with TEE mode enabled and disabled, under different models, input and output lengths, and batch size setups. Our primary focus was to reveal the performance penalty in real-world large language model (LLM) inference tasks."}, {"title": "3.1 Metrics", "content": "The primary metrics are evaluated following typical evaluation frameworks [AAK+24]:\n\u2022 TTFT (Time To First Token): The time from request arrival to the generation of the first output token. It includes scheduling delay and prompt processing. Lower TTFT is essential for real-time applications, while higher TTFT is tolerable in batch processing.\n\u2022 ITL (Inter Token Latency): The time between generating each token during decoding. This directly affects the perceived model speed. A rate of around 6 tokens per second is necessary for smooth user experience, assuming average reading speed.\n\u2022 TPS (Token per Second): The average rate of token generation during decoding. It's calculated as the total decoding time divided by the number of tokens generated.\n\u2022 Latency: The total execution time per request, including scheduling, prompt processing, and token generation. Lower normalized latency improves system throughput, especially under high query loads.\n\u2022 QPS (Query per Second): The maximum load a system can handle while meeting latency targets. Higher QPS reduces serving costs and is a key measure of system capacity."}, {"title": "3.2 Test Scenarios", "content": "Experiments were structured to explore TEE mode impact under diverse conditions:\n\u2022 TEE mode ON vs. TEE mode OFF: Tests were performed with TEE mode alternately enabled and disabled on the H100 GPU, allowing for a direct comparison of performance.\n\u2022 Sequence Lengths: We tested various token lengths by sampling the ShareGPT Dataset [ano] to simulate different LLM inference tasks.\n\u2022 Batch Size: We tested both the fixed batch sizes (1, 4, and 16) and dynamic batch size determined by VLLM [KLZ+23] to simulate the performance for serving real-time requests and batch requests."}, {"title": "3.3 Experimental Setup", "content": null}, {"title": "3.3.1 Infrastructure", "content": "We set up the experiments with the following hardware:\n\u2022 GPU: NVIDIA H100 NVL (94GB, 3.9TB/s bandwidth)\n\u2022 CPU: AMD EPYC 9V84 96-Core Processor with SEV-SNP\n\u2022 Memory: 314 GB\n\u2022 Driver versions:\n CUDA 12.5 (driver version 555.42.02)\n Kernel driver version 550.90.07"}, {"title": "3.3.2 Application", "content": "The experiments utilized the Benchmark suites of vLLM v0.5.4 (rev: 4db5176) [KLZ+23]."}, {"title": "3.3.3 Models", "content": "Three LLMs were used for inference:\n\u2022 Meta-Llama-3.1-8B-Instruct\n\u2022 Phi-3-14B-128k-Instruct\n\u2022 Meta-Llama-3.1-70B-Instruct with 4-bit bitsandbytes quantization to fit into a single H100 GPU"}, {"title": "4 Results", "content": "Conclusion 1: The average overhead is less than 7%. We quantified the overhead by measuring the throughput with TEE mode enabled versus disabled, across varying input sizes and model configurations as shown in Table 1.\nConclusion 2: The overhead reduces toward zero as the model size grows. As shown in Table 1, the smallest model (LLama-3.1-8B) has the highest overhead. The medium size model (Phi3-14B-128k) has roughly 2/3 of the overhead compared with the smaller one. The largest model (Llama-3.1-70B) has a trivial overhead close to zero.\nConclusion 3: The latency is the main factor contributing to the overhead of the TEE model. Table 2 shows the overhead introduced to the latency measured by TTFT and ITL. TTFT has a higher overhead compared with ITL, indicating the bottleneck is likely introduced by the IO instead of the computation happening inside the TEE. Nevertheless, the overhead becomes trivial when hosting heavy computation models like Llama-3.1-70B.\nConclusion 4: The overhead reduces as the token size grows. As shown in Figure 1, the throughput overhead reduces when the sequence length grows, measured by the total input and output token count. The detailed throughput metrics across various sequence length can be found at Table 3.\nConclusion 5: TEE can reach a typical throughput\nOur experiments revealed that, with medium-sized inputs, the NVIDIA H100 GPU achieves 130 TPS for Llama3-8B, while the larger Phi-3-14B model reaches approximately 6 TPS. These results"}, {"title": "5 Conclusion", "content": "Our results show that as input size grows, the efficiency of TEE mode increases significantly. When computation time within the GPU dominates overall processing time, the IO overhead introduced by TEE mode diminishes, allowing efficiency to approach nearly 99%.\nEfficiency growth is more pronounced in larger models, such as Phi3-14B-128k and Llama-3.1-70B, due to their greater computational demands, which result in longer GPU processing times. Consequently, the IO overhead becomes increasingly trivial as model size increases.\nThe total token size (sum of input and output token size) significantly influences the throughput overhead. Larger total token counts lead to higher efficiencies, as they enhance the ratio of computation time to IO time.\nThese findings underscore the scalability of TEE mode in handling large-scale LLM inference tasks, particularly as input sizes and model complexities grow. The minimal overhead in high-computation scenarios validates its applicability in secure, high-performance AI workloads."}]}