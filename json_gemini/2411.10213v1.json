{"title": "An Empirical Study on LLM-based Agents for Automated Bug Fixing", "authors": ["XIANGXIN MENG", "ZEXIONG MA", "PENGFEI GAO", "CHAO PENG"], "abstract": "Large language models (LLMs) and LLM-based Agents have been applied to fix bugs automatically, demonstrating the capability in addressing software defects by engaging in development environment interaction, iterative validation and code modification. However, systematic analysis of these agent and non-agent systems remain limited, particularly regarding performance variations among top-performing ones. In this paper, we examine seven proprietary and open-source systems on the SWE-bench Lite benchmark for automated bug fixing. We first assess each system's overall performance, noting instances solvable by all or none of these sytems, and explore why some instances are uniquely solved by specific system types. We also compare fault localization accuracy at file and line levels and evaluate bug reproduction capabilities, identifying instances solvable only through dynamic reproduction. Through analysis, we concluded that further optimization is needed in both the LLM itself and the design of Agentic flow to improve the effectiveness of the Agent in bug fixing.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) [60] are advanced machine learning models trained on vast amounts of textual data, capable of understanding and generating human-like text. LLM-based Agents [48] are systems that utilize large language models to interact with the environment and accomplish specific tasks. Recently, LLM-based Agents have demonstrated significant influence in automated bug fixing in code repositories [14, 52, 59]. Thanks to the powerful natural language processing capabilities of LLMs, these Agents can efficiently understand and analyze source code and its associated natural language descriptions, such as user-submitted issue descriptions and code comments. Additionally, through dynamic interaction with local environments (e.g., via terminal), LLM-based Agents can retrieve useful information from the code repository, perform code editing and execution, and iterate and validate repair results, thereby improving the accuracy and efficiency of bug fixes. This combination of LLM and environmental feedback has made automated bug fixing more efficient"}, {"title": "2 Background", "content": "In this section, we first introduce SWE-bench Lite and then we introduce the leading LLM-based bug fixing systems."}, {"title": "2.1 SWE-bench Lite", "content": "SWE-Bench [13] is a comprehensive benchmark designed to evaluate LLMs on complex real-world software engineering tasks sourced from GitHub issues and corresponding pull requests across 12 popular Python repositories. This benchmark addresses the limitations of existing coding benchmarks such as HumanEval [6] by presenting tasks that require models to understand and coordinate changes across large codebases involving multiple functions and files. The benchmark includes 2,294 task instances and emphasizes the need for models to interact with execution environments and handle long contexts, showcasing the challenges that real-world software engineering problems pose to current LLMs. Their evaluations reveal that even the best-performing models at the time of publication, such as Claude 2, achieve a success rate of only 1.96%, highlighting significant room for improvement.\nAs the computational demands and high difficulty of SWE-bench which comprises 2,294 issue-commit pairs across 12 Python repositories, the authors of SWE-bench introduces SWE-bench Lite 2, which includes 300 more manageable and self-contained instances focused on functional bug fixes, covering 11 of the original 12 repositories. It retains the diversity of SWE-bench but is easier to evaluate. The selection criteria is shown below:\n(1) Removed instances with images, external links, commit SHAs, or references.\n(2) Excluded problem statements under 40 words.\n(3) Excluded instances editing more than one file or with more than three edit hunks.\n(4) Excluded instances creating/removing files or with error message checks.\n(5) Sampled final 300 test and 23 development instances from remaining ones."}, {"title": "2.2 Leading LLM-based Bug Fixing Systems", "content": "LLM-based Bug Fixing Systems are systems built on Large Language Models (LLMs) that can automatically edit code repositories to fix bugs based on issue reports. Bug fixing is a highly resource-intensive task in software development, requiring developers to reproduce the bugs reported in issue reports, precisely locate defective code snippets within large code repositories, understand the cause of errors, and implement fixes. Automating bug fixing has long attracted widespread attention in both academia and industry. Given the strong logical reasoning and coding capabilities demonstrated by LLMs, numerous works have explored the development of automated bug fixing tools based on LLMs. In this paper, we study seven leading LLM-based Bug Fixing Systems (four commercial systems [1, 2, 26, 28] and three open source systems [35, 49, 59]), comparing their differences in system design and performance in automated bug fixing, analyzing the shortcomings and limitations of existing systems, and providing direction for future work in building adaptive, high-reliability automated bug fixing systems."}, {"title": "3 Study Design", "content": "In this section, we first introduce the research questions and then introduce the data collection."}, {"title": "3.1 Research Questions", "content": "RQ1. Effectiveness of Systems: How does the LLM-based Agent currently perform in automatic bug fixing in code repositories?\nMotivation: In the SWE-bench Lite leaderboard, the solution rates of various systems vary significantly, and there are substantial differences in the instances that each system can and cannot solve. This discrepancy is usually due to the quality of issue description and the design of the systems themselves. When an issue description is of sufficiently high quality, we expect an LLM-based Agent to be able to resolve it. Therefore, it is necessary to analyze why certain instances with high-quality issue description are not successfully fixed by the Agent, while some instances with low-quality issue description are resolved. Additionally, there are significant differences in the implementation of Agent-based and non-Agent systems, and it is worth investigating the differences in their resolution capabilities.\nApproach: We will analyze the differences in the instances solved by various systems, showing how many instances are resolved by all systems and how many instances are not solved by any system. Then, based on the criteria proposed by Agentless[] for evaluating issue descriptions, we will score the quality of the issue descriptions for each instance, where higher scores indicate higher-quality issue descriptions. Subsequently, we will investigate why many high-scoring issues cannot be resolved by any system, while some low-scoring issues can be resolved by all tools. Additionally, we will examine the characteristics of instances that can be resolved by all Agent systems but not by non-Agent systems, as well as those instances that can be resolved by all non-Agent systems but not by Agent systems.\nRQ2. Effectiveness of FL: How do different systems perform in Fault Localization and what are the reasons for their differences?\nMotivation: Fault localization is a crucial step in bug fixing, as the more accurately the fault is localized, the higher the probability of successfully fixing the bug. Therefore, we need to investigate the differences in fault localization effectiveness among different systems.\nApproach: Based on the ground truth, we will compile statistics on the proportion of successfully localized faulty files and the proportion of successfully localized faulty lines for each system in each SWE-bench Lite instance.\nRQ3. Effectiveness of Reproduction: How Bug Reproduction in Different Systems Affects Bug Fixing Performance?\nMotivation: Bug reproduction is an important step in bug fixing and an essential part of dynamic debugging. Its role is reflected in two aspects. First, the error messages from running the bug reproduction script can be used for fault localization. Second, the bug reproduction script can be used to validate the final generated patch. The higher the quality of the bug reproduction script, the more accurate information it can provide to the Agent, increasing the probability of successfully fixing the bug. Therefore, we need to investigate the impact of bug reproduction on bug fixing.\nApproach: We will compile statistics on the adoption rate of the reproduction scripts generated by each system, providing a comparison of the impact of bug reproduction on bug fixing. Additionally, we will analyze the cases that can only be solved with the involvement of reproduction scripts, ans the cases that bug reproduction negatively impact bug fixing."}, {"title": "3.2 Data Collection", "content": "In RQ1, we design a scoring system based on the five metrics and corresponding candidate values provided by Agentless, allowing us to evaluate the quality of different issue sets across multiple dimensions. In RQ2, we conduct a reverse analysis of the patches generated by different tools, thereby offering an unbiased evaluation of each tool's performance in fault localization.In RQ3, to determine the use of reproduction by different systems from their trajectories, we first identify Agentless, RepoGraph+Agentless, and Gru as systems that do not support reproduction, based on keyword matching for \"reproduce\" and manual analysis. Then, for the remaining four systems, we utilize different heuristic rules to identify the construction of reproduction scripts."}, {"title": "4 Analysis & Results", "content": "We will sequentially present the analysis results and insights for RQ1 to RQ3."}, {"title": "4.1 RQ1: Effectiveness of Systems", "content": "We analyzed the versions of cases that each of the seven tools can solve individually, as well as the differences between the cases that each tool can resolve, as shown in Figure 1-(a). The histogram at the top of the figure shows the number of cases each tool can resolve in SWE-bench Lite. Specifically, ranked from lowest to highest, these are Agentless, Agentless+RepoGraph, AutoCodeRover, Alibaba Lingma Agent, Gru, Honeycomb, and MarsCode Agent, resolving 82, 89, 92, 99, 107, 115, and 118 cases, respectively. MarsCode Agent performs the best, achieving a 43.9% performance improvement over Agentless and addressing 39.3% of the total 300 cases in SWE-bench Lite. Compared to the popular APR benchmark Defects4J over the past decade, SWE-bench Lite introduces stricter usage protocols, prohibiting participants from leveraging dynamic evaluation results generated by closely related failing test cases as feedback information for filtering patches. This test case set can only be utilized as a quality standard once the patch generation process has concluded. In this context, many error localization methods based on dynamic test execution information\u2014such as spectrum-based and mutation-based error localization methods\u2014cannot be used, adding further complexity to problem detection and resolution. This strict protocol undoubtedly aligns more closely with real-world development scenarios, where repair tools must rely almost solely on issues raised by users and the current state of the code repository to devise solutions. Against this backdrop, MarsCode's ability to address 39.3% of cases underscores its advanced capabilities and utility in real-world development environments.\nIn Figure 1-(a), the histogram beneath the tool names (referred to as Part-I) presents the case versions that different tool combinations can address. Each row indicates the number of case versions solvable by the tools marked with black dots but not by those marked with gray dots. For instance, in the first row, only MarsCode is marked with a black dot, while the other six tools are marked with gray dots, indicating that MarsCode can solve a unique set of 9 cases that none of the other six tools can handle. Similarly, the seventh row demonstrates that MarsCode and Honeycomb together can solve 5 cases that the other five tools cannot address. The final row shows that all seven tools collectively can solve 36 cases. The following sections will analyze the statistical findings presented in this figure from several perspectives.\nAnalysis of Case Solvability. Among the 300 cases in SWE-bench Lite, 168 cases are solvable by at least one of the seven tools (representing the sum of all values in Part-II of Figure 1-(a)), while 132 cases remain unsolved by any tool (not displayed in the figure and constituting the complement of the 168 solvable cases). Furthermore, 36 cases can be solved by all seven tools (represented by the last row in Part-II). We hypothesize that the issue descriptions for these 36 universally solvable cases are generally of higher quality, whereas the 132 cases that none of the tools can resolve likely exhibit lower-quality issue descriptions. To validate this hypothesis, we conducted a significance analysis of issue quality differences using five metrics provided by the issue quality analysis report"}, {"title": "5 Discussion", "content": null}, {"title": "5.1 Large Language Model", "content": "From the LLM perspective, it is necessary to further enhance the model's reasoning ability so that it can accurately identify information related to the bug within the issue, thereby reducing the interference of noise. Additionally, for multiple potential repair locations, the model should utilize its reasoning capability to select the location most relevant to the issue."}, {"title": "5.2 Agentic Flow", "content": "From the Agentic flow perspective, agents should especially focus on the quality of the issue and pay attention to multiple suspicious locations in the stack trace. The Agentic flow design should include mechanisms to check the completeness of patches and consider the global impact of the fixes. During the use of the model, mechanisms should be established to either avoid the randomness of the model's output or make full use of the diversity in the model's output.\nIn fault localization, the accuracy of line-level localization is more important than file-level, as the discovery space at the line level is larger, necessitating finer-grained localization results. During the reproduction process, it is crucial to strengthen the determination of the correctness of the reproduction, as an incorrect reproduction can lead to the failure of the entire solving process."}, {"title": "6 Threats to Validity", "content": "Fail-to-Pass Tests: SWE-bench uses Fail-to-Pass (F2P) tests to verify the correctness of generated patches. However, F2P tests may not be comprehensive, allowing a patch to pass F2P and be deemed correct without fully addressing the user's issue. This is a common problem in the field of APR as well as in LLM evaluation based on unit tests [25]. In this context, we assume that a patch is correct as long as it passes the F2P test cases. We also call for contributions from the academic community to improve the test cases in the SWE-bench evaluation dataset to make the evaluation results more reliable.\nUncertainty of LLM: The output of LLMs is stochastic, leading to a probabilistic nature for whether an instance is solved. In this work, we directly analyzed the patches and trajectories submitted by various systems, assuming that the results submitted to SWE-bench represent the best performance of the Agent. Furthermore, conducting multiple experiments for each system to eliminate stochasticity is impractical in terms of both cost and accessibility."}, {"title": "7 Related Work", "content": "In this section, we discuss basic concepts of large language models and their application on software engineering tasks, especially for fault localization and automated program repair. We also discuss recent advances in LLM-based agents for software engineering."}, {"title": "7.1 Large Language Models", "content": "Large language models (LLMs) are highly advanced pre-trained language models. These models undergo initial unsupervised training on vast amounts of corpus, followed by fine-tuning for specific tasks to enhance performance. In natural language processing (NLP), LLMs have been extensively applied to various tasks such as machine translation [42, 56], text summarization [58], and classification [30].\nLanguage models are classified into three categories based on their architecture: encoder-only models [9], decoder-only models [34], and encoder-decoder models [40]. Most existing LLMs for code utilize the transformer architecture's encoders, known for their exceptional learning capabilities and scalability. Regardless of their architecture, most models can be fine-tuned with task-specific data to enhance performance [23].\nLarge language models (LLMs) have become a promising choice for various software engineering tasks due to their impressive performance in both code generation and understanding [53]. Researchers and developers have applied LLMs to several software engineering tasks, such as program synthesis [25, 41, 43, 44, 62], code translation [54, 55], program repair [12, 24, 50], fault detection and localization [8, 37], incident analysis [5, 7], code summarization [10] and testing [38]. For example, Codex [6], StarCoder [27], and DeepSeek-Coder [61] are notable code-specific LLMs developed through extensive training on large datasets of open-source code snippets. Additionally, instruction-following code-specific LLMs such as DeepSeek-Coder-Instruct [61] and Magicoder [46] have been created using instruction-tuning methods to enhance their utility in coding tasks."}, {"title": "7.2 Fault Localization", "content": "Fault localization (FL) [47] techniques aim to discover and analyze the location and causes of faults, which can be categorized into dynamic and static approaches. Dynamic FL techniques, such as spectrum-based fault localization (SBFL) [3, 4] and mutation-based fault localization (MBFL) [36], analyze the dynamic execution information of a program to determine fault locations, though they are resource-intensive. Static FL techniques [29] determine fault locations through semantic or syntactic analysis at the bug report or source code level, offering fast detection with low resource consumption. Advanced FL techniques, such as multiple fault localization (MFL) and combined dynamic and static methods, have emerged to guide APR tools in finding and fixing more errors [15, 32, 51]."}, {"title": "7.3 Automated Program Repair", "content": "Automated program repair (APR) [19] has attracted significant attention over the past decade. APR techniques aim to generate patches for buggy programs to pass given test suites. These techniques can be categorized into search-based [20, 31], semantics-based [16, 17, 33], and pattern/learning-based approaches [21, 22, 57]. Search-based APR techniques like GenProg [18] use predefined code mutation operators to generate patches, while semantics-based APR techniques generate patches by solving repair constraints based on test suite specifications. Learning-based APR techniques, such as those utilizing deep learning models, train on large code repositories to predict correct patches. Recent work has shown the use of LLMs for APR, often focusing on constructing APR-specific prompts to guide LLMs in generating patches for buggy program statements [50]."}, {"title": "7.4 Agents for Software Development", "content": "The emergence and popularity of agent-based frameworks have led to the development of agent-based approaches for solving software engineering tasks. Devin and its open-source counterpart OpenDevin [45] are among the first end-to-end LLM agent-based frameworks. These frameworks use agents for planning based on user requirements and enable agents to iteratively perform tasks using tools like file editors, terminals, and web search engines. SWE-agent [52], for example, designs a custom agent-computer interface (ACI) allowing LLM agents to interact with the repository environment through actions such as reading, editing files, and running bash commands. AutoCodeRover [59] provides LLM agents with specific APIs to effectively identify locations needing modification to resolve issues. Numerous other agent-based approaches have been developed, both in open-source and commercial products."}, {"title": "8 Conclusion", "content": "In this paper, we analyzed the top 4 commercial systems and the top 3 open-source systems on SWE-bench Lite. We conducted detailed analyses of the performance of LLM-based Agents in automatic bug fixing for code repositories, the performance of different systems in Fault Localization, and their performance in Reproduction. The analysis results indicate that to further enhance the capabilities of LLM-based Agents in bug fixing, future efforts should focus on improving the reasoning ability of LLMs. Additionally, attention should be given to the Agentic flow design, considering the quality of issues, stack traces, and the correctness of reproductions."}, {"title": "Data availability", "content": "All raw data comes from the official SWE-bench experiment repository: https://github.com/swe-bench/experiments/"}]}