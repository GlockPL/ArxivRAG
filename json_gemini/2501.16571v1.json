{"title": "Efficient Object Detection of Marine Debris using\nPruned YOLO Model", "authors": ["Abi Aryazaa", "Novanto Yudistira", "Tibyani"], "abstract": "Marine debris remains a persistent and crucial issue that demands attention, as\nthe substances present in the waste pose significant harm to marine life.\nIngredients such as microplastics, polychlorinated biphenyls, and pesticides can\npoison and damage the habitats of organisms living in proximity. However,\nsolutions involving human labor, such as diving, are becoming increasingly\nineffective due to the limitations of humans in underwater environments. To\naddress this challenge, technology involving autonomous underwater vehicles\ncontinues to be developed for effective sea garbage collection. In this\ndevelopment process, the selection of object detection architecture in\nautonomous underwater vehicles plays a critical role. For the creation of robots\ncapable of handling marine debris, a one-stage detector type architecture is\nhighly recommended due to its necessity for real-time detection. This research\nfocuses on utilizing the You Only Look Once model version 4 (YOLOv4)\narchitecture for the object detection of marine debris. The dataset utilized in this\nresearch comprises 7683 images of marine debris collected in Trash-ICRA 19\ndataset with 480x320 pixels. Furthermore, various modifications, such as using\npretrained model, training from scratch, disable mosaic augmentation, enable\nmosaic augmentation, freezing the backbone only layer, freezing the backbone\nand neck layer, YOLOv4-tiny, and adding channel pruning of YOLOv4, are\nimplemented each other and compared to find the most impactful for enhance\nthe efficiency of the architecture. Numerous studies have demonstrated that the\napplication of channel pruning can improve detection speed without significantly\nsacrificing accuracy. The dataset employed in this research is the trash-ICRA 19\ndataset, featuring images of objects in seawater. This dataset emphasizes images\nof plastic waste objects in water, incorporating several other classes. Through\nthe application of channel pruning to YOLOv4 trained on the underwater object\nimage dataset, the frame rate per second value from the base YOLOv4 is from\n15.19FPS increases to 19.4 FPS, accompanied by only 1.2% reduction in mean\nAverage Precision from 97.6% to 96.4%.", "sections": [{"title": "1. Introduction", "content": "Environmental pollution has always been a serious problem worldwide, and\none contributing factor stems from waste-the remnants or traces of everything\ndiscarded by humans[1]. In 2022, the generation of waste in Indonesia reached\n35 million tons, with 23.5 million tons being managed, and 11.6 million tons left\nunmanaged[2]. Unmanaged waste can occur for several reasons, such as few\nlandfills, and limited waste processing capacity. As a result, unmanaged waste is\ndispersed in various places, such as buried in the ground or flowing into the sea.\nAll waste disposed of or abandoned in marine environments or the Great Lakes,\nwhether intentionally or not, is termed marine debris[3]. Marine debris\nencompasses nonbiodegradable solids, manufactured or processed products,\noriginating from diverse sources like fishing activities, beach tourism, industrial\nwaste, and improper garbage disposal on land. Classifications of marine debris\ninclude plastic, metal, glass, rubber, and organic materials. Approximately 14\nbillion tons of waste find their way into the ocean each year[4], with plastic\nwaste dominating as the most prevalent type, constituting around 60-80 percent\nof total marine debris[5].\nCurrently, robots and artificial intelligence play crucial roles in various re-\nsearch endeavors. In underwater research, robots are essential for observing\nenvironments that are challenging for humans to reach, such as areas with\nlimited water depth, leak detection in underwater pipelines, and more.\nAutonomous Underwater Vehicles (AUVs) equipped with sensors like side-scan\nsonar, cameras, echosounders, Acoustic Doppler Current Profilers (ADCP), and\nConductivity, Temperature, and Depth (CTD) sensors prove highly useful for\nwater observation[6]. AUVs, being unmanned, are automatically controlled by a\ncomputer[7].\nOne of the focal points in artificial intelligence is computer vision, which\ninvolves developing algorithms capable of understanding and extracting\ninformation from images and videos, mirroring the way humans use vision to\nperceive information[8]. Computer vision encompasses technologies like digital\nimage processing, pattern recognition, and data analysis techniques such as\nmachine learning and deep learning[9]. Object detection, a case study in image\nprocessing and computer vision, aims to identify the presence and position of\nspecific objects in images or videos. Object detection has evolved rapidly since\nthe introduction of the Region-based Convolutional Neural Networks (R-CNN)\nmode in 2014[10]. Numerous models have since been created to address tasks\nrelated to object recognition and localization, categorizing object detection into\ntwo types: two-stage detectors and one-stage detectors. Two-stage detectors\nperform detections through two stages: region proposal and classification. While\nthis makes detection more reliable, it increases computational costs due to\ncomplexity[11]. Conversely, one-stage detectors use a single feed-forward"}, {"title": "2. Related Works", "content": "Fulton et al., 2019 [28], conducted a study entitled 'Robotic Detection of\nMarine Litter Using Deep Visual Detection Models,' which served as the\nreference for the dataset used in this study. The YOLOv2 method was employed\nwith a mean average precision (mAP) value of 47.9%, achieving an frames per\nsecond (FPS) of 205 using GTX 1080Ti.\nYang et al., 2021 [20], conducted a study entitled 'Research on Underwater\nObject Recognition based on YOLOv3' on the same dataset, utilizing the YOLOv3\nmethod. They achieved a mAP of 76.1%, recall of 75.6%, with a frame rate of 20\nFPS. The study also included a comparison between the YOLOv3 method and\nFaster R-CNN[46].\nTian et al., 2022 [27], conducted a study entitled 'A modified YOLOv4\ndetection method for a vision-based underwater garbage cleaning robot' on the\nsame topic, focusing on marine garbage using the YOLOv4 method. The research\naimed to detect objects by comparing several methods, such as 4SP- YOLOv4,\n4S-YOLOv4, YOLOv4, YOLOv3, Faster R-CNN, and SSD. The 4S-YOLOv4 method\nobtained the best mAP value at 95.5%. This method, a modified YOLOv4 version,\nsuccessfully increased the accuracy by introducing a 4-scale YOLOv4. Meanwhile,\nthe best FPS was achieved by the 4SP-YOLOv4 method by trimming the 4-scale\nYOLOv4 version, reducing parameters, and optimizing models.\nMajchrowska et al., 2021 [29], conducted a study entitled 'Waste detection\nin Pomerania: a non-profit project for detecting waste in the environment,'\nfocusing on waste detection. The research involved various datasets, training,\nand dataset combination. The Trash-ICRA 19 dataset was one of the tested\ndatasets, with evaluation results in the study showing a mAP value of only 7.3%\nwith the EfficientDet-D2 model using the EfficientNet-B2 backbone. According to\nthe researchers, poor image quality resulted in mAP values below 10%.\nFurthermore, the dataset was combined with other datasets and trained,\nachieving a final accuracy of 73.02% using Weighted Sampler to produce a more\nstable evaluation of each class.\nLiao Juang, 2023 [21], conducted a study entitled 'Automatic Marine Debris"}, {"title": "3. Methodology", "content": "The YOLO method presents a single-stage approach to object detection,\nemploying an innovative technique. Object detection categorically falls into two\ntypes: single-stage detectors and two-stage detectors. In the case of single-stage\ndetectors, the detection process concentrates on a sole evaluation network\nwithout traversing through the proposal region. YOLO stands out by utilizing a\nsingle evaluation network, resulting in faster detection speeds compared to\nother object detection methods[19]. However, this approach does affect the\nconfidence level of the detection results, which tends to be lower than that of\nthe two-stage detector.\nThe process initiates by converting the input image into a 3D matrix.\nSubsequent to the training process, each cell within a grid predicts N possible\nbounding boxes and confidence values[33]. Furthermore, bounding boxes with\nconfidence values falling below the threshold are eliminated, retaining only\nthose with high confidence levels. The bounding boxes are then arranged based\non their confidence value and subjected to the non-maximum suppression\n(NMS) process. This process scrutinizes similar and adjacent bounding box\nlayouts to acquire a singular bounding box representing an object[34]. The NMS\ncalculation employs the intersection over union (loU) metric. Specifically, the loU\nbetween bounding box A and bounding box B is calculated, and if the result\nexceeds the confidence threshold, bounding box B is removed.\nThe development of detection objects continues to evolve over time until\nnow, and for the evolution of detection objects until 2020 can be seen in Figure\n2."}, {"title": "3.1. YOLO", "content": "YOLOv4 represents an advancement of YOLO, offering the flexibility to choose\nits backbone. By selecting CSPDarknet53 as the backbone, YOLOv4 achieves\nsuperior mAP and FPS compared to CSPResNeXt-50 and EfficientNet- B0[35].\nCSPDarknet53 is an enhanced iteration of Darknet-53, the original backbone for\nYOLOV3. This enhancement involves the incorporation of CSPNet (Cross Stage\nPartial Network) into the residual block and a transition in the activation layer to\ninclude mish activation and Leaky-ReLU activation[36]. Mish, as an activation\nfunction, possesses self-adjusting and non-monotonic properties, resulting in\nsmoother output compared to the ReLU activation function used in darknet-53[37].\nWithin the neck layer, YOLOv4 incorporates SPP (Spatial Pyramid Pooling) and\nPANet (Path Aggregation Network). SPP integrates a max-pooling layer with three\npooling sizes: 5\u00d75, 9\u00d79, and 13\u00d71[38]. While originally designed for instance\nsegmentation, PANet, as employed in YOLOv4, conducts upsampling and\ndownsampling using both low-level and high-level feature maps[39].\nThe YOLOv4 head mirrors the YOLOv3 head, featuring three outputs. The first\noutput is tailored for detecting small objects, the second output for medium-sized\nobjects, and the third output for large objects[40].\nThe constituent elements of YOLOv4 operate distinctly. In the backbone,\nCSPDarknet53 undertakes feature extraction on the image, while the neck\nincorporates SPP, executing max pooling with four different pool sizes (5\u00d75,\n9\u00d79,13\u00d713, 1\u00d71)[38]. Post-SPP, another neck component, PANet, enhances the\nfeature map's quality by performing upsampling and downsampling using\nboth low-level and high-level feature maps. The head section executes the object\ndetection process across three different scales: the first YOLO head for small\nobjects, the second for medium-sized objects, and the third for large objects[40]."}, {"title": "3.2. Loss Function", "content": "In the architecture of YOLOv4, the selection of a loss function plays a pivotal\nrole in optimizing the model's performance. YOLOv4 incorporates four types of\nloss functions to efficiently train the network. These encompass the CloU loss\n[41], object confidence loss, no-object confidence loss,\nand classification loss [40], each addressing distinct aspects of the detection process.\nThe CloU loss, or Complete Intersection over Union loss, is a distinctive addition in\nYOLOv4 and is employed to refine the calculation of Intersection over Union (IoU) scores.\nThe formula for CloU loss includes the integration of specific parameters such as the\ncoordinates (x, y) and the dimensions (width and height) of bounding boxes. This\nrefinement in the loss formula signifies a nuanced approach to assessing the agreement\nbetween predicted and ground truth bounding boxes, thereby contributing to more\naccurate and precise object localization[41].\n$L_{CIOU} (Complete \\ IoU \\ loss) = 1 - IoU + \\frac{\\rho^2(b,b^{gt})}{c^2} + \\alpha v$  (1)\nWhere ($\\rho$) denotes the Euclidian distance. (b) and ($b^{gt}$) denote the central point of\nthe predicted box and ground truth box. c is the length of the shortest enclosing box that\ncovering the two boxes. ($\\alpha$) is a positive trade-off parameter, and (v) measures the\nconsistency of aspect ratio. The equation ($\\alpha$) can be defined as\n$\\alpha (positive \\ tradeoff \\ parameter) = \\frac{v}{(1 \u2013 IoU) + v}$  (2)\nThe equation v can be defined as\n$v(consistency \\ of \\ aspect \\ ratio) = \\frac{4}{\\pi^2} (arctan \\frac{w^{gt}}{h^{gt}} - arctan \\frac{w}{h})^2$  (3)\nWhere w denotes the width, h denotes the height, and $gt$ denotes the ground truth.\nThe object confidence loss and no object confidence loss use binary cross entropy that\ncalculates the confidence prediction and confidence ground truth. Confidence ground truth\nbecomes 1 if in that box there is an object but becomes 0 if in that box there is no object.\nThe equation of object confidence loss can be defined as\n$Obj = \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\lambda_{ij}^{obj}[C_i log \\widehat{C_i} + (1 - C_i)log (1 \u2013 \\widehat{C_i})]$  (4)\nWhere $s^2$ is defined as the total number of grid cells, and since the image's aspect\nratio is 1:1, the formulas is s x s or $s^2$. B is defined as the total number of bounding boxes. i\nis defined as specific cell location of the grid, j is defined as the bounding box accessed within\nthe cell. $I_{i,j}^{noobj}$ is a function, defined as 1 if the cell i inside the j bounding box contain\nobjects, and 0 otherwise. The equation of no-object confidence loss can be defined as\n$Noobj = \\sum_{i=0}^{S^2} \\sum_{j=0}^{B} \\lambda_{i,j}^{noobj}[C_i log \\widehat{C_i} + (1 \u2212 C_i)log (1 \u2013 \\widehat{C_i})]$  (5)\nWhere the variable is remaining the same except the $I_{i,j}^{noobj}$ defined as 1 if the cell i\ninside the j bounding box is not contain objects, and 0 otherwise. $\\widehat{C_i}$ is defined as predicted\nconfidence, $C_i$ is defined as actual confidence Classification loss uses binary cross entropy to\ncalculate the class prediction that has implemented one-hot encoding. The equation of\nclassification can be defined as\n$class = \\sum_{i=0}^{S^2} \\sum_{CE classes}^{} \\lambda_{i,j}^{obj}[p_i log \\widehat{p_i} + (1 \u2212 p_i)log (1 \u2013 \\widehat{p_i})]$  (6)\n$\\widehat{p_i}$ is defined as predicted classification, $p_i$ is defined as actual classification."}, {"title": "3.3. Channel Pruning", "content": "After several years of deep learning development, encompassing architectures such\nas AlexNet[42], VGG[43], GoogLeNet[44], U-Net[45], MaskRCNN[46], and others, the\narchitecture of neural networks has grown larger. This growth has implications for the\ncomputational load, making it increasingly challenging to deploy these architectures on low-\nlevel hardware. Pruning emerges as a solution to manage this significant computational load\nand reduce it. Pruning can be applied at various levels, commencing with weights and\nextending to layers[47]. It is evident that pruning at the most granular level, such as weights,\nwill yield the most flexible pruning. Nevertheless, it is essential to ensure adequate\npreparation, including the provision of software and hardware accelerators. At larger levels,\nsuch as layers, pruning is less flexible but is the most straightforward to perform. Channel\npruning, as one level of pruning, focuses on reducing a model's size by pruning its channels.\nIn recent years, a diverse array of models has been developed, featuring an ever-increasing\nnumber of parameters and convolutions, which translates to a substantial computational\ncost for training. Pruning aims to eliminate unimportant weights, thus reducing the overall\ncomputational burden."}, {"title": "4. Experiments", "content": "The research conducted training using six distinct schemes, each designed to\nexplore and optimize the YOLOv4 architecture for marine debris detection. The\nbreakdown of these schemes is as follows:\nScheme 1: YOLOv4 from Scratch"}, {"title": "4.1. Scheme 1", "content": "Training YOLOv4 model without leveraging pre-existing weights or\nknowledge, starting the learning process from scratch."}, {"title": "Scheme 2: Pretrained YOLOv4", "content": "Utilizing pre-existing weights and knowledge by training YOLOv4 on a\npretrained model, enhancing the model's ability to generalize and learn\npatterns effectively."}, {"title": "Scheme 3: YOLOv4 with freezing backbone layer", "content": "Training YOLOv4 with a frozen backbone layer, allows the model to fine-\ntune specific aspects without altering the foundational features learned\nduring the initial training."}, {"title": "Scheme 4: YOLOv4 with freezing backbone+neck layer", "content": "Extending the freezing approach to both the backbone and neck layers,\naiming to selectively refine higher-level features in the model while\npreserving lower-level representations."}, {"title": "Scheme 5: YOLOv4-tiny from Scratch", "content": "Employing a smaller variant, YOLOv4-tiny, and training it from scratch,\nexploring the trade-off between model complexity and detection\nperformance."}, {"title": "Scheme 6: Pruned-YOLOv4", "content": "Implementing channel pruning techniques on YOLOv4 to create a pruned\nversion, seeking to enhance efficiency without significantly compromising\ndetection accuracy.\nEach scheme represents a unique approach to training the YOLOv4\narchitecture, allowing for a comprehensive analysis of different strategies and\ntheir impact on marine debris detection performance."}, {"title": "4.2. Scheme 2", "content": "Scheme 2 is a model training scheme using YOLOv4 with pre-training from\nthe COCO[50] dataset. In Scheme 2, a comparison of the pre-trained YOLOv4\nmodel without mosaic augmentation and with mosaic augmentation is\nconducted. The training progress for each step is illustrated in Figure 7. The\ncorresponding loss values are depicted in Figure 8, and the mAP results are\npresented in Table 2."}, {"title": "4.3. Scheme 3", "content": "Scheme 3 is a model training scheme that is similar to scheme 2, but it\nincludes a frozen layer on the backbone only. The step of each training can be\nseen at Figure 9. The loss values are described in Figure 10, with the mAP results\ndescribed in Table 3"}, {"title": "4.4. Scheme 4", "content": "Scheme 4 is a model training approach similar to scheme 3, utilizing a frozen\nlayer, but it adds the frozen layer to the neck. The steps of each training iteration\ncan be observed in Figure 11. The corresponding loss values are depicted in\nFigure 12, and the mAP results are detailed in Table 3."}, {"title": "4.5. Scheme 5", "content": "Scheme 5 is a model trained using YOLOv4-tiny from Scratch. It compares the\nYOLOv4-tiny model with and without mosaic augmentation. The step of each\ntraining can be seen at Figure 13. Figure 14 describes the loss values, and Table\n5 shows the mAP results."}, {"title": "4.6. Scheme 6", "content": "Scheme 6 involves channel pruning of the fine-tuned model in scheme 2 with\nmosaic augmentation. The model is pruned by 20% and 50%. The step of each\ntraining can be seen at Figure 15. Figure 16 shows the corresponding loss values,\nwhile Table 6 presents the mAP results."}, {"title": "4.7. Analysis", "content": "Table 7 presents a model comparison based on the best mAP and FPS\nobtained by testing all schemes 1-5. The highest mAP value, 97.6%, was achieved\nwhen testing with YOLOv4 Pretrained. Interestingly, the use of mosaic\naugmentation resulted in a decrease in mAP values for most models, with\nYOLOv4 from Scratch and YOLOv4 Pretrained experiencing a decrease of around\n0.2%, and YOLOv4 Tiny experiencing a decrease of 1%. Notably, only the YOLOv4\nfreeze layer showed an increase in mAP. The mAP increase in YOLOv4 Pretrained\nFreeze Backbone+Neck reached 2.8%, while in YOLOv4 Pretrained Freeze\nBackbone, it only reached 0.2%.\nBased on mAP, the best results are achieved by using pretrained models and\nthen fine-tuning them. The highest mAP is obtained both with and without\nmosaic augmentation. The model with the lowest mAP is YOLOv4 pretrained\nfreeze backbone+neck, as weight updates are only performed on the head layer.\nIn terms of FPS, YOLOv4 achieves the best FPS with YOLOv4 Tiny with Mosaic\nAugmentation, with a value of 31.65. The Backbone+Neck model had the lowest\nFPS. The number of parameters or complexity of the model also affects the FPS\nvalue. When comparing YOLOv4 with YOLOv4 Freeze Backbone+Neck, the\nmodel's mAP value is directly proportional to its FPS. When comparing YOLOv4-\ntiny with YOLOv4, the number of parameters is inversely proportional to FPS."}, {"title": "5. Conclusion", "content": "Based on the test results of the six schemes, it can be concluded that an\nefficient YOLOv4 model for marine debris object detection can be designed by\nperforming channel pruning on the trained model with the mAP resulting 96.4%.\nThis method increases the FPS of the model while only decreasing the mAP by\napproximately 1.2% from the base model 97.6% to 96.4%.\nThe YOLOv4 base model underwent several changes in the six test schemes,\nincluding the use of pretraining, freezing layers, and channel pruning. Based on\nthe tests conducted, the model that achieved the highest mAP is pretrained\nYOLOv4 model with score of 97.6% while for the highest FPS is YOLOv4-tiny with\nthe score 31.65FPS which is developed by focusing for low GPU user. This is show\nthat the proposed model, YOLOv4 50% pruning model proven not the best in\neach accuracy and speed. However, the proposed model is the most efficiency\nmodel because it has an almost similar with the best mAP model with gap 1.2%\nlower only which is higher than YOLOv4-tiny and offer a notable increase in\nspeed which is higher than pretrained YOLOv4 model.\nFrom the comparation with previous research, it shows that GPU\nperformance is one of the aspects that affect the FPS result. However, since the\nproposed research use GTX 1050 Mobile that lower than the compared research,\nthe efficient YOLOv4 using 50% pruning was able to successfully overcome the\ndetection object that utilize high GPU performance proved by competitive result\nwith the gap only 1.1FPS and 0.8FPS lower than the compared research. The\nresult also shows that with some improvement, the base YOLOv4 model can be\nused for real-time detection for marine debris cases even with low GPU, while\nmaintaining a near-identical mAP value as the original YOLOv4\nTo improve this research in the future, several suggestions can be\nconsidered, such as conducting tests by replacing the backbone and neck layers\nof YOLOv4 with CSPResNext50 or EfficientNet-B3 on the backbone, and FPN or\nSFAM on the neck to observe variations. Perform pruning experiments with layer\npruning in addition to channel pruning."}]}