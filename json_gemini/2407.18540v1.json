{"title": "A Universal Prompting Strategy for Extracting\nProcess Model Information from Natural\nLanguage Text using Large Language Models", "authors": ["Julian Neuberger", "Lars Ackermann", "Han van der Aa", "Stefan Jablonski"], "abstract": "Over the past decade, extensive research efforts have been\ndedicated to the extraction of information from textual process descrip-\ntions. Despite the remarkable progress witnessed in natural language\nprocessing (NLP), information extraction within the Business Process\nManagement domain remains predominantly reliant on rule-based sys-\ntems and machine learning methodologies. Data scarcity has so far pre-\nvented the successful application of deep learning techniques. However,\nthe rapid progress in generative large language models (LLMs) makes it\npossible to solve many NLP tasks with very high quality without the need\nfor extensive data. Therefore, we systematically investigate the potential\nof LLMs for extracting information from textual process descriptions,\ntargeting the detection of process elements such as activities and actors,\nand relations between them. Using a heuristic algorithm, we demonstrate\nthe suitability of the extracted information for process model generation.\nBased on a novel prompting strategy, we show that LLMs are able to\noutperform state-of-the-art machine learning approaches with absolute\nperformance improvements of up to 8% F1 score across three different\ndatasets. We evaluate our prompting strategy on eight different LLMs,\nshowing it is universally applicable, while also analyzing the impact of\ncertain prompt parts on extraction quality. The number of example texts,\nthe specificity of definitions, and the rigour of format instructions are\nidentified as key for improving the accuracy of extracted information.\nOur code, prompts, and data are publicly available.", "sections": [{"title": "1 Introduction", "content": "In the field of Business Process Management (BPM), process models are estab-lished tools for designing, implementing, enacting, and analyzing enterprise pro-"}, {"title": "2 Task Descriptions and Challenges", "content": "In this section, we describe the three main (sub)tasks of process information\nextraction (Section 2.1), before highlighting a range of challenges associated\nwith such extraction and with the use of LLMs for it (Sections 2.2-2.3)."}, {"title": "2.1 Task descriptions", "content": "Our work focuses on three established subtasks of (process) information extrac-\ntion from text: Mention Detection (MD), Entity Resolution (ER), and Relation\nExtraction (RE) [2, 7, 23, 26].\nMention Detection (MD) is concerned with finding and extracting text frag-\nments that contain process relevant information, such as activities (or actions),\nprocess-relevant objects or data (i.e., business objects), or involved persons and\ndepartments (i.e., actors). For instance, in Figure 1, the upper example shows\nmentions of data, actions, and actor, whereas the lower one focuses on activities.\nThis definition is similar to Named Entity Recognition (NER), though we also\nextract spans not covered by the traditional definition of NER, e.g., activities.\nEntity Resolution (ER) aims to recognize when different mentions refer to\nthe same process entity. For example, in Figure 1, successful ER would identify\nthat the word it in \"it is examined\" corresponds to the claim mentioned in\nthe previous phrase. Another common example is using ER to recognize that\nthe same actor (across mentions) performs different steps. ER is a super-set of\nco-reference resolution and anaphora resolution [29] and is a crucial step when\ndealing with process-related texts, which frequently involve repeated mentions\nacross sentences or even paragraphs [23].\nRelation Extraction (RE) is the task of detecting and classifying relations\nbetween mentions. Relations are usually directed and have one (unary relation),\nor two (binary relations) arguments. For instance, the upper example in Figure 1\nshows three kinds of relations: uses signals which data objects are used by an ac-\ntivity, performer captures which actor performed an activity, and flow captures\na sequential relation between two activities. RE is crucial when it comes to infor-\nmation extraction in our context, given that processes inherently involve process\nsteps (i.e., activities) that are connected to each other through relations. Note\nthat we regard constraint extraction [2, 26] (CE), which relates to declarative\nprocess modeling, as an RE problem: constraints have one or two arguments,\nare directed, and carry type information (e.g., Succession, Init)."}, {"title": "2.2 Challenges of Process Information Extraction from Text", "content": "Information extraction, a common task in natural language processing (NLP),\nfaces general challenges, which are also well-known in BPM literature [1,15], and\noften central elements of interest in the design of rule-based and learning-based\nsystems alike [2,23]. Simply using LLMs for process information extraction solves\nsome of these challenges and justifies the investigation of their applicability.\nIn the context of process-related texts Linguistic Variance means that the\nsame behavior or process characteristics can be described in a variety of ways\nsuch as, for instance, active and passive voice. Context Cues are a challenge in\nthat single words can fundamentally alter the meaning of a process description\n(e.g., \u201cfirst, a claim is created\u201d and inverted semantics in \u201cfinally, a claim is cre-\nated\". Processes are typically described in sequential form, although they usually\ncontain branches (e.g. XOR decision branches). This results in Long-distance"}, {"title": "2.3 Challenges of Process Information Extraction Using LLMs", "content": "Using LLMs for process information extraction from texts helps with linguistic\nchallenges, but adds itself several additional challenges. We discuss these here\nand reference them later in Section 6.3 to show how we can deal with them.\n(C1) Limited output control. Input and output consist of plain text.\nGiven that the input for inference is raw text, it inherently suits LLMs for our\ntasks (cf. Section 2.1). However, as the expected output should adhere to a\nspecific schema, it becomes necessary to instruct the LLM to conform to this\nschema. Moreover, this principle necessitates a robust output parser, as LLMs\ntend to exhibit variability in their output, which presently cannot be entirely\neradicated. Having only limited control over generated output is especially prob-\nlematic for the BPM domain, where definitions for relevant information often\noverlap, e.g., actions (just predicate) versus activities (predicate and object).\n(C2) Input presentation dependencies. Although LLMs provide an in-\nterface for natural language input, the quantity, form and level of detail must be\ncarefully matched to the task at hand. The LLM faces the challenge of determin-\ning the importance of the input components. Furthermore, while LLMs emulate\nhuman reasoning, the interpretation of inputs may diverge significantly from\nthat of human beings, thereby rendering prompt optimization a trial-and-error\nprocess. This challenge is further aggravated by some elements of process models,\nthat are complex to explain concisely, e.g., parallel and exclusive workflows.\n(C3) Black-box. Deep learning methods generally suffer from challenges\nconcerning explainability of predictions [34], which is also true for LLMs. Con-\ntrary to classical learning methods, such as decision trees, LLMs offer no fail safe\nmechanism to validate extraction rules. This is problematic for business process\ninformation extraction in particular, since recent work focuses on \"human-in-the-loop\" systems [30], where the human must be able to follow system decisions.\n(C4) Data unawareness. In contrast to generative AI models trained on\ntask-specific data, an LLM is usually not aware of the particular dataset it is\ntasked to process. Thus, using LLMs to process a particular dataset requires to"}, {"title": "3 Related Work", "content": "Related approaches are divided into rule-based, machine learning (ML)-based,\ndeep learning-based, and LLM-based process information extraction. An ap-\nproach is considered related if it solves at least one of the tasks in Section 2.1.\nRule-based approaches. Rule-based approaches leverage linguistic fea-\ntures to extract information from natural language process descriptions through\nexplicitly coded mapping rules. For instance, Friedrich et al.'s seminal work [16]\nemploys syntax features and word information from a lexical database to iden-\ntify patterns at both sentence and document levels for BPMN model creation.\nOther approaches like those in [26, 28] adopt similar techniques for automatic\ntext annotation, employing regular expressions for syntactic dependency trees,\nand part-of-speech tags, showcasing superior performance on novel datasets. Ad-\nditionally, [2] presents a rule-based technique, currently leading in extracting\ndeclarative process models from raw text using syntax parsing and word-level\nfeatures. Similar advancements are seen in [14] and [9], the latter integrating\nML-based entity MD with subsequent rule-based RE. Furthermore, [21] focuses\non extracting Dynamic Condition Response (DCR) graphs. Recent studies sug-\ngest that while rule-based approaches can be tailored to specific tasks and data\nsets, they can hardly deal with ambiguity and linguistic variance. [8,23].\nML-based approaches. [23] presents a ML extraction pipeline based on [9]\nand is used as a baseline for our comparative evaluation (Section 6). The deep-\nlearning approach presented in [25] classifies text fragments analyzing the input\ntext on several levels of granularity. However, extracting these fragments is not\npart of the approach, which simplifies the task of MD to mention classification,\ni.e., locating the information to extract is omitted. Though the work presented in\n[6] overcomes this limitation and outperforms the approach, it does not support\nRE. In general, techniques of this paradigm either struggle to deal with linguistic\nvariability and ambiguity, or they require vast amounts of training data, making\nthem particularly unfeasible for small datasets (see Section 2.2).\nLLM-based approaches. Bellan et al. [8] utilize pre-trained LLMs to cope\nwith data scarcity, yet their approach exhibits three primary weaknesses: (i) It is"}, {"title": "4 LLM-based Process Model Extraction from Text", "content": "To generate process models from natural language texts, we first extract process\ninformation using LLMs and a novel prompting strategy (Section 4.1). Next, we\ndescribe a baseline algorithm for generating a process model (Section 4.2)."}, {"title": "4.1 Process Information Extraction with LLMs", "content": "Extracting process information with LLMs requires a prompt design that ad-\ndresses the challenges mentioned in Section 2.3. Thus, a prompt structure con-\nsisting of the three modules Context, Task Description and Restrictions is de-\nscribed below at the example of the Mention Detection task. However, the\nprompting strategies for the remaining tasks (Section 2.1) are analogous.\nHigh-Level Prompt Structure. LLMs take freely formulated text as input,\nwhich is called the prompt. To this end we base our prompts on an ablation\nstudy (Section 6.2), which is used to identify beneficial and detrimental prompt\ncomponents. To do this, we first need a modular prompt design so that we\ncan specifically remove individual components in the study to examine their\nbenefits and ultimately to only keep the advantageous components. Adhering\nto the best practices outlined in [31], our initial prompt design is structured\ninto three modules (see Figure 2): (A) a context description framing the process\ninformation extraction task on a high level, (B) a detailed task description, and\n(C) constraints that further restrict the context and the output format, and\ncontains disambiguation hints. To design potentially relevant components for all\nthree modules we rely on general design patterns [11, 22, 31, 33]. Therefore, in\nthe next subsection, the three modules are specified and discussed in terms of\nhow they address the LLM-specific challenges outlined in Section 2.3.\nContext and Task Description. In module (A) we use the persona design\npattern [33] to control the language style of generation results. We assign it the\nrole of a process modeling expert. This is followed by the context manager design"}, {"title": "4.2 Process Model Generation a Proof of Concept", "content": "Based on the format instructions contained in the prompt design (Section 4.1),\ninformation can be extracted automatically through a parser. This information\ncan then be automatically converted into a process model in BPMN language,\nwhich was chosen because it is a widely used industry standard.\nOur conversion algorithm works in three major stages. First, in the Con-\nsolidation phase, we assign the conditions to the mentions of their respective\ndecisions. Next, all mentions of gateways are merged, if they refer to the same\ndecision point. Finally, for all activities that are not assigned an executing actor,\nwe find the closest actor mention in the text left of it. In the second stage, the\nVertex stage, we create process elements for all mentions, e.g., Tasks, Data Ob-\njects, Swimlanes, etc. The final Linking stage connects related elements, e.g.,\nsuccessive tasks with Sequence Flows, if they are located in the same Swimlane,\nor Message Flows otherwise. We also create Data Associations between Data\nObjects and Tasks, adding the label of the Data Object to the label of the Task,\nfor labels like \u201cregister the claim\u201d.\nFigure 3 shows the resulting BPMN model after applying this algorithm on\nthe information contained in document doc-3.3 of the (PET) dataset we later\nuse in our experiments. Note, that the layouting is adjusted manually, as well\nas the wrong assignment of Task sent back a claim to the Swimlane of Actor A\nclaims officer. These issues illustrate why synthesizing a model from extracted\nprocess information is still challenging, which warrants further research beyond\nthe scope of this paper."}, {"title": "5 Experiment Setup", "content": "In this section we define the experiments we run to evaluate the usefulness of\nLLMs for process information extraction 6. It covers an overview of the datasets\nwe use, including the respective baselines, and a definition of metrics we apply.\nWe use three well-known datasets for evaluating our prompts. One of these\n(PET) represents the current state of the art, both in terms of size, as well as\nthe process information techniques developed for it. The other datasets feature\ndifferent characteristics, making them relevant for validation experiments. This\nlets us gain insights into the robustness of an LLM as a process information\nextractor, as well as how it behaves when applied to other process modeling\nlanguages. We call the best approaches for extracting the information from these\ndatasets baselines, and use them in Section 6 as comparisons with various LLMs.\nPET [9]: This is the largest dataset currently available. It contains 45 docu-\nments with annotations for information especially useful for creating process\nmodels in BPMN. These include 7 types of mentions such as activities, actors,\ndata objects, but also 6 relation types. These cover the behavioural process per-\nspective (Flow), data perspective (uses, and organizational perspective (actor,\nperformer). Additionally, this dataset features relations that span multiple sen-\ntences. It therefore tests the ability of approaches to reason across wider spans\nof text. We use an extended version of this dataset, which includes data for\nthe ER task, as presented in [23]. The currently best approach for extracting\ninformation is using conditional random fields for MD, a pre-trained neural co-"}, {"title": "6 Results", "content": "Table 1 shows the results we observed when running the experiments as described\nin Section 5 with an optimized prompt, that follows the recommendations we\nfound in our study of best practices (Section 6.2). All results use GPT-40, the\nlatest version of OpenAI's GPT with temperature = 0. top_p is unchanged, as\nper OpenAI's recommendation, when using temperature based sampling. 7\nFor the reference dataset PET, our experiments show that GPT-40 is capable\nof an absolute F\u2081 score improvement of 5% for MD, 22% for ER, and 17% for\nRE. Remarkably, for RE, GPT-40 is able to match and outperform the machine\nlearnt baseline, which was trained on 36 manually annotated documents [23],"}, {"title": "6.1 Model Comparison", "content": "We originally developed our prompts for GPT-4 version GPT-4-0125-preview, to\nassess how well our prompting strategy generalizes to other models we prompted\na total of eight models for the MD and RE tasks on PET. We selected models\nfollowing AlpacaEval 8, which is designed for testing the instruction following\ncapabilities of LMMs [13]. At the time of writing model YI Large Preview was\nnot publicly accessible and could not be considered in our comparison, even\nthough it ranked third on Alpaca Eval."}, {"title": "6.2 Ablation Study", "content": "We conduct an ablation study to assess the usefulness of the best practices\npresented in Section 4.1 and to measure the impact of the prompt's main com-\nponents. This study is run on the reference dataset (PET), as it is the largest\none and used by recent publications [8,9,23]. To obtain a baseline for the tasks\nof MD and RE, we use a prompt that implements the best practices as shown\nin Figure 2 and run it on the GPT-4-0125-preview model. We then purposefully\nremove specific components from this prompt, namely the format examples, the\ncontext manager, the persona, the definition of mention and relation types (meta\nlanguage), the instruction to think in several steps (chain of thought), any dis-\nambiguation hints, and the instruction to generate explanations (reflection) and\na fact check list about the process. Additionally, we also use a prompt with very\nshort descriptions of relations and types (balancing brevity and specificity). We\nrun each prompt in the zero-shot setting and record the observed F\u2081 score, as\nwell as the parsing errors that occurred.\nTable 3 provides detailed results. Removing examples has a significant neg-\native effect (-0.22 for MD and -0.07 for RE), mainly rooted in the number of\nparsing errors that are made (919 for MD), as well as directionality of relations"}, {"title": "6.3 Lessons Learned", "content": "Using LLMs for extracting process relevant information brings with it a cate-\ngory of challenges, which we already discussed in Section 2.3. Solving these is\nparamount for successful application of LLMs. In this section we discuss how we\napproached these challenges and what lessons we learned.\n(C1) Limited output control. The expected output format, as well as the\nform of extracted information, can mainly be influenced by the prompt compo-\nnents Meta Language and Format Examples. Adding these results in significantly\nimproved F\u2081 scores, (+0.22 and +0.05 respectively for MD on PET). These im-\nprovements are explained by less parsing errors (919 less for MD on PET), and"}, {"title": "7 Conclusion", "content": "Summary. This paper presents an extensive study on the usefulness of LLMs\nfor the extraction of process information from natural language text. We col-\nlected linguistic challenges and discuss how LLMs are uniquely fit for solving\nthem. We also discussed challenges that arise through the use of LLMs and\nshow how other communities propose to deal with these (or similar) concerns\nthrough prompt engineering. We present experimental results on three process\ninformation extraction datasets, which at least match the current state of the\nart on these datasets and in most cases improve it by as much as 8% in the F\u2081\nmetric. This shows the suitability of LLMs as a method for extracting business\nprocess relevant information from natural language process descriptions. To flesh\nout this notion, we analyze how well our prompting strategy can be applied to\ndifferent LLMs without changing them, showing their universal nature. We make\nall our code, prompts, and LLM answers available, to support further research.\nLimitations. A limitation of our work is that the list of prompt components\nwe present may not be exhaustive, and they may have interactions that our\nablation study does not capture. Additionally, some models suffer from halluci-\nnations, especially Qwen1.5 and Llama 3, which hallucinate non-existent entity\nand relation types \u2013 20 and 37 instances in the worst cases respectively. However,\nthe severity of this problem diminishes in the few-shot setting (0 and 4 instances\nrespectively in the worst case). We plan on analyzing how our prompts could\nbe enhanced to improve instruction following for these models. Lastly, the cur-\nrent pricing models prohibit large-scale application of the most capable model to\nprocess information extraction. Alternatives (e.g., Llama) avoid this issue, but\nrequire more labeled examples to reach comparable levels of performance.\nFuture Work. In future work we aim to use LLMs as tools to support labeling\nof new data. Current datasets are limited in origin, i.e., they usually describe\nprocesses from municipalities or small service providers. We plan to analyze"}]}