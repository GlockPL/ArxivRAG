{"title": "UniMEL: A Unified Framework for Multimodal Entity Linking with Large Language Models", "authors": ["Qi Liu", "Yongyi He", "Defu Lian", "Zhi Zheng", "Tong Xu", "Che Liu", "Enhong Chen"], "abstract": "Multimodal Entity Linking (MEL) is a crucial task that aims at linking ambiguous mentions within multimodal contexts to the referent entities in a multimodal knowledge base, such as Wikipedia. Existing methods focus heavily on using complex mechanisms and extensive model tuning methods to model the multimodal interaction on specific datasets. However, these methods overcomplicate the MEL task and overlook the visual semantic information, which makes them costly and hard to scale. Moreover, these methods cannot solve the issues like textual ambiguity, redundancy, and noisy images, which severely degrade their performance. Fortunately, the advent of Large Language Models (LLMs) with robust capabilities in text understanding and reasoning, particularly Multimodal Large Language Models (MLLMs) that can process multimodal inputs, provides new insights into addressing this challenge. However, how to design a universally applicable LLMs-based MEL approach remains a pressing challenge. To this end, we propose UniMEL, a unified framework which establishes a new paradigm to process multimodal entity linking tasks using LLMs. In this framework, we employ LLMs to augment the representation of mentions and entities individually by integrating textual and visual information and refining textual information. Subsequently, we employ the embedding-based method for retrieving and re-ranking candidate entities. Then, with only ~0.26% of the model parameters fine-tuned, LLMs can make the final selection from the candidate entities. Extensive experiments on three public benchmark datasets demonstrate that our solution achieves state-of-the-art performance, and ablation studies verify the effectiveness of all modules. Our code is available at https://anonymous.4open.science/r/UniMEL/.", "sections": [{"title": "1 INTRODUCTION", "content": "Entity Linking is a basic task in Knowledge Graph (KG) domains [13], which aims at linking mentions (i.e., segments of text referring to specific entities) of a document to entities in a knowledge base (KB). It is widely used in many Natural Language Processing (NLP) downstream applications, such as question answering [16, 42, 69], recommendation systems [25] and so on. Recently, the development and widespread use of social media and the Internet have further propelled textual and visual multimodality to become an important medium for data tasks. Meanwhile, the quality of online information is increasingly uneven, with many mentions being ambiguous, and their context being coarse. Therefore, in many cases, relying solely on textual modality to disambiguate the mentions proves inadequate. [23, 48]. However, integrating textual and visual modality often facilitates a more precise and effortless disambiguation. Therefore, Multimodal Entity Linking, linking mentions along with textual and visual modality to a multimodal knowledge base (MMKB) [12, 23], becomes essential. For example, as shown in Figure 1, the mention \"United States\" could be challenging to distinguish, as it may refer to various entities such as a country name, a sport team, or a liner. However, considering textual and visual information simultaneously, it becomes easier and clearer to accurately link the mention \"United States\" to the entity \"United States national wheelchair rugby team\".\nSo far, many deep learning-based methods have been developed to fuse visual information with textual contexts when linking the multimodal mentions to entities. While these methods have made inspiring results in MEL tasks by fusing mention text and image to obtain mention representation [59], applying cross-attention mechanisms [63], and encoding the image to extract features [57, 59], these deep learning-based methods of MEL tasks still have several challenges as follows:\n\u2022 Redundant entity descriptions. Generally, the description of the entity is usually too long, leading to a hard focus on valid information in the process of disambiguation. In this case, it is necessary to pay more attention to entities and mentions related parts.\n\u2022 Lack of important semantic information in mentions. Mentions differ from entities in that they lie in the contextual completeness of the feature information they contain. As shown in Figure 1, the mention textual context is a truncated piece extracted directly from documents, lacking pivotal semantic information and sufficient evidence for linking the mention to a specific entity effectively. How to utilize images from mentions to supplement their lacking semantic information becomes essential.\nFortunately, Recent advances in Large language models (LLMs), such as GPT series models [3, 10, 53, 54] and LLaMA series models [2, 53, 60, 61], have considerably enhanced numerous NLP tasks [27, 51, 55]. Pretrained on massive corpora, LLMs have the potential to generate instruction-following text via in-context learning [10] or fine-tuning [49] in response to user prompts. Although LLMs have demonstrated surprise on many data tasks, they inherently have a limited capacity to access visual information. Concurrently, Large Vision Models (LVMs) have the capability for comprehensive visual interactions, but commonly lag in reasoning. In light of this complementarity, Multimodal Large Language Models (MLLMs) arise at an opportune time. The emergence of recent models such as GPT-4V [1], LLaVA [40], Qwen-VL [8], and text-to-image generation models like DALL-E-3 [9], has marked significant progress in the MLLMs field. In addition, MLLMs have wide research scenarios, including image caption, vision question answering, etc. Based on the generative capabilities of LLMs, the previous methods let LLMs play different roles, such as an answer generator [57] and memory controller [75], which has made inspiring results in the MEL task. However, utilizing LLMs for the MEL task is still untrivial due to the following reasons:\n\u2022 Combining the visual context with the textual context effectively is challenging. The images and textual content of the mentions usually correspond and complement each other, each providing additional information for mentions. Therefore, the fusion of images and text is quite important for entity linking.\n\u2022 LLMs lack domain-specific knowledge. Although LLMs demonstrate powerful general capabilities, they do not directly excel in specific domain tasks (e.g., MEL tasks).\nTo tackle these issues above, we introduce a unified framework for the MEL task with Large Language Models (UniMEL). Drawing inspiration from question-answering systems, mentions and entity candidates are concatenated to form queries inputted into the LLMs. Specifically, we aim to enhance the query, which consists of mentions and entity candidates, through the following ways: (1) For mentions, the image and the contextual information associated with mentions are processed as input to MLLMs in order to extract deeper semantic relationships between the image and its context. This approach ensures that the integrity of the original image is maintained (i.e., without cropping or encoding), thereby fully leveraging the unaltered raw data. Considering the extensive corpus utilized for pre-training MLLMs, this method has the potential to enrich the concrete information pertaining to the mention. (2) For entities, the overdetailed and redundant descriptions significantly challenge the MEL task. Short and precise new descriptions could be obtained through the utilization of LLMs' summarization capabilities. (3) For narrowing down the candidate set, the embedding model is employed to retrieve and rerank the primitive candidate set. Subsequently, the top-K candidates are selected and concatenated with mentions to generate a multi-choice query. (4) For augmenting the capabilities of LLMs in MEL tasks, we finetune only a few parameters of the LLMs as a selector to choose the referent entity of the mention.\nOverall, the contributions of our work can be summarized as follows:\n\u2022 In this work, we propose UniMEL, a framework to deal with the MEL task with LLMs and MLLMs, which could adequately fuse image and context of multimodal mentions and generate new concise descriptions of entities. To the best of our knowledge, it is the first work to introduce MLLMs-based methods in the MEL task.\n\u2022 We propose a unified prompt template set, specifically crafted for the Multimodal Entity Linking (MEL) task, to effectively bridge the semantic gap between modality content. And we finetune the LLM in the training set of the MEL dataset, to supply domain-specific knowledge.\n\u2022 We devise an ingenious solution to supplement the mention and refine the entity information. Specifically, we provide the description of the mentions for the three publicly available datasets and present a summary of the descriptions of the entities.\n\u2022 UniMEL achieves state-of-the-art (SOTA) results on three public multimodal entity linking datasets (22.3% Top-1 accuracy gains on Richpedia, 21.3% Top-1 accuracy gains on WikiMEL, 41.7% Top-1 accuracy gains on Wikidiverse), exhibiting high performance."}, {"title": "2 RELATED WORK", "content": "Existing methods of entity linking can be divided into two steps:\ncandidate selection and candidate reranking. For the first step, they\ncommonly used coarse-grained filtering for generation candidates,\ne.g., TF-IDF [6] and word2vec [47]. For the second step, they em-\nployed LSTM [28] or BERT [32] for text encoders to measure the\nsimilarity, such as dot product [24] and cosine similarity [26], and\nsome research leveraged CNN [22], cross-encoder and bi-and cross-\nencoder to capture the correlation between mention context and\nentity description, like BLINK [67]. By contrast, De Cao et al. [17]\nused an autoregressive entity retrieval method by using BART [33]\narchitecture to generate the unique name for each entity. Although\nthese methods have demonstrated significant effectiveness in text-\nonly entity linking, accurately linking entities in short and coarse\ntexts remains challenging. This has motivated the exploration of\nMultimodal Entity Linking."}, {"title": "2.1 Entity Linking", "content": "Existing methods of entity linking can be divided into two steps:\ncandidate selection and candidate reranking. For the first step, they\ncommonly used coarse-grained filtering for generation candidates,\ne.g., TF-IDF [6] and word2vec [47]. For the second step, they em-\nployed LSTM [28] or BERT [32] for text encoders to measure the\nsimilarity, such as dot product [24] and cosine similarity [26], and\nsome research leveraged CNN [22], cross-encoder and bi-and cross-\nencoder to capture the correlation between mention context and\nentity description, like BLINK [67]. By contrast, De Cao et al. [17]\nused an autoregressive entity retrieval method by using BART [33]\narchitecture to generate the unique name for each entity. Although\nthese methods have demonstrated significant effectiveness in text-\nonly entity linking, accurately linking entities in short and coarse\ntexts remains challenging. This has motivated the exploration of\nMultimodal Entity Linking."}, {"title": "2.2 Multimodal Entity Linking", "content": "To tackle the challenges above, MEL is essential. With the impor-\ntance and abundance of multimodal information, it is vital to fuse\nvisual and textual information during multimodal entity linking.\nHow to reasonably fuse visual and textual information becomes\nserious. Moon et al. [48] built a deep zero-shot multimodal net-\nwork for MEL. Adjali et al. [4] proposed a novel method to build\nannotated datasets for evaluating methods on the MEL tasks and\nreleased a new MEL dataset, the Twitter MEL dataset. Zhou et al. [78] released three MEL datasets and proposed a MEL dataset construction approach. Gan et al. [23] modeled the alignment of textual and visual mentions as a bipartite graph matching problem. They also published a new MEL dataset, i.e., M3EL. Wang et al. [63] extracted the hierarchical features of text and visual co-attention through the multimodal co-attention mechanism. To resolve the limited contextual topics and entity types, Wang et al. [66] presented Wikidiverse, a high-quality human-annotated MEL dataset from Wikinews, including diversified contextual topics and entity types. To model visual and textual information at the knowledge level, Zhang et al. [20] proposed an Interactive Learning Network to fully use the multimodal information. Xing et al. [68] exploited the fine-grained and dynamic alignment relations between entity and mention. Luo et al. [46] proposed a novel to understand the comprehensive expression of abbreviated textual context and implicit visual indication. Shi et al. [57] applied LLMs to the MEL task. Song et al. [59] refined the queries with multimodal data."}, {"title": "2.3 LLMs-based Data Tasks", "content": "Large Language Models (LLMs) are pre-trained on the massive corpus, which has demonstrated outstanding capabilities in many data tasks, such as text summarization and domain-specific question answering. For summarization tasks, LLMs can transfer a long text to a brief text [58]. Fine-tuning LLMs with domain-specific data enhances their performance on domain-specific tasks, such as Chat-Law [14], HuatuoGPT [72] and InstructGPT [49]. However, LLMs are susceptible to hallucination, often generating inaccurate and erroneous information. To mitigate these problems, Lewis et al. [34] introduced the Retrieval-Augmented Generation (RAG) model, and then RAG enhances LLMs by retrieving relevant document chunks from the external knowledge base. The development of multimodal large language models (MLLMs), which extend traditional LLMs to the multimodal domain, is increasingly gaining interest. Similarly, MLLMs are pretrained on extensive vision and language data, which show promising ability in multimodal tasks, such as multimodal information summarization [11], image caption [35, 38, 41, 64], and visual question answering [15]. Like LLMs, These MLLMs exhibit an inclination to generate hallucinations. Existing methods mitigate hallucination from four aspects: (1) Data, e.g., LRV-Instruction [37], (2) Model, e.g., LLaVA to LLaVA-1.6 [39], (3) Training, e.g., RLHF-V [71], HA-DPO [77] and (4) Inference, e.g., GCD [18]."}, {"title": "3 PRELIMINARY", "content": "Multimodal Large Language Model. Compared with text-only LLM, MLLM can process not only text inputs but also other modalities of input data (e.g., image), providing a richer and more comprehensive multimodal understanding and reasoning capability. Multimodal Knowledge Base. A multimodal knowledge base (MMKB) is composed of an entity set. And each entity has different modal information. In the MMKB, the static attributes (i.e., occupation, name) of each entity are encapsulated within the text descriptions. Conversely, the images of entities in the MMKB tend to exhibit a broader range of dynamic attributes (i.e., clothes color) compared to textual descriptions. If dynamic attributes are employed as descriptors for textual entities, it commonly results in a misleading focus on these attributes when recognizing the entity. As presented in Table 1, we provide a specific example for an entity. Compared to the descriptions from raw text, the image descriptions do not produce more static attributes; instead, they generate additional noise information, where \"red text\" indicates noise and \"blue text\" signifies valid attribute information. Incidentally, the pertinent information output by MLLMs is already presented in the textual description. Generally, the entity descriptions in MMKB are very long and redundant, which is not conducive to representing the entity effectively. Ambiguous Mentions. The mentions are the segments of text referring to specific entities, but their textual contexts are truncated pieces extracted directly from documents, which may lack pivotal semantic information, leading to ambiguities and insufficient evidence for linking the mention to a specific entity effectively. For example, the surname \"Trump\", when mentioned in various contexts, can refer to different individuals. Secondly, the images of mentions may contain not only specific mentions but also other irrelevant and potentially distracting noise information."}, {"title": "4 METHODOLOGY", "content": "We introduce our UniMEL, which is a unified framework to process multimodal entity linking tasks using LLMs. As shown in Figure 2, UniMEL takes the multimodal mention context and entity information as input and gives the top-K candidate entity names. In the Section 4.1, we first present the problem formulation. Subsequently, we provide the details of our UniMEL, which has four modules: LLMs-based Entity Augmentation (Section 4.2), MLLMs-based Mention Augmentation (Section 4.3), Retrieval Augmentation (Section 4.4), Multi-choice Selection (Section 4.5)."}, {"title": "4.1 Problem Formulation", "content": "Multimodal Entity Linking is aiming at mapping mentions with multimodal contexts to its corresponding entity in a knowledge base. Formally, we define an entity set of the multimodal knowledge base with N entities as $\\& = {E_i}_{i=1}^N$. And each entity $E_i$ is represented by a triple $(en_i, ed_i, ev_i)$, including its entity name $en_i$, entity description $ed_i$ and entity image $ev_i$. The mention set is defined as $M = {M_j}_{j=1}^D$, where each mention $M_j$ is represented by a triple $(mn_j, mc_j, mv_j)$, comprising the mention name $mn_j$, textual context $mc_j$, visual context $mv_j$. The referent entity of $M_j$ in knowledge base $\\& $ is predicted through:\n$e^* (M_j) = arg max_{E_i\\in \\& } sim(M_j, E_i),$\\n(1)\nwhere sim() is the similarity between the mention and the entity with multimodal information."}, {"title": "4.2 LLMs-based Entity Augmentation", "content": "To address the problem of excessive length and redundant information in entity descriptions, we employ LLMs to summarize the descriptions effectively. LLMs have been pretrained on a massive corpus that includes diverse world knowledge, and many works [50, 74] have demonstrated that they possess the capability of them to generate high-quality summaries for long texts. First, we need to provide the LLM with the entity name and its original description. Secondly, we design a specific instruction to emphasize the conciseness and content requirements of the summaries to be generated. Then, following the given instruction, the LLM will generate a new, concise, high-quality, and information-rich summary of the entity description, thereby augmenting entity information.\nAs shown in Figure 2b, for each entity $E_i$, we provide LLMs with the entity name $en_i$ and description $ed_i$, subsequently tasking them to generate a summarized description for the entity, represented as:\n$es_i = LLM(C(en_i;ed_i)),$\n(2)\nwhere C(X; Y) denotes the function that concatenates X and Y. We apply the designed and universal prompt template $T_{ea}$ to do this summarization.\nPrompt $T_{ea}$:\nPlease summarize the main content of the given entity in one sentence, including entity name and description.\nEntity name: Asia-Pacific Economic Cooperation\nEntity description: The Asia-Pacific Economic Cooperation (APEC; AY-pek) is an inter-governmental forum for 21 member economies...\nResponse:\nAPEC is an inter-governmental forum of 21 member economies in the Pacific Rim that promotes free trade and economic cooperation..."}, {"title": "4.3 MLLMs-based Mention Augmentation", "content": "Due to the powerful visual comprehension and instruction following capabilities of MLLMs, we employ them to augment the descriptive information of mentions accompanied by images. We input the image containing the specific mention into the MLLM and provide it with the mention name and textual context. Firstly, we concatenate the mention name with its context to form a coherent semantic unit, ensuring that the connection is both meaningful and logically consistent. Secondly, as MLLMs are adept at providing detailed and comprehensive descriptions of images, we design a specific task instruction to guide MLLMs to focus on the mention itself, preventing the generation of irrelevant environmental (e.g., \"an outdoor setting with greenery\") or detailed descriptions (e.g., \"wearing a dark suit with a white shirt\"). To let the MLLM produce well-formatted mention descriptions, we leverage its in-context learning capabilities, and construct a designed mention description as the demonstration example, specifying the output format for the MLLM. Then, the MLLM utilizes visual and textual information to generate a description for the mention, thereby augmenting its information.\nSpecifically, as shown in Figure 2a, for each mention $M_j$, we provide the MLLM with three inputs: the mention image $mv_j$, the mention name $mn_j$, and the mention textual context $mc_j$. Then, we drive the MLLM to generate a high-quality textual description for $M_j$, which can be represented as:\n$md_j = MLLM(C(mv_j;mn_j;mc_j)),$\n(3)\nby a designed prompt template $T_{ma}$.\nPrompt $T_{ma}$:\nTarget entity name: APEC.\nImage Description: APEC Leaders wave for the media dressed in Driza-Bones in Sydney...\nOnly generate a description of the target entity, not a description of the image.\nAnswer follow the format: \"The APEC refer to...\"\nResponse:\nThe APEC is a regional economic forum comprising 21 member economies in the Asia-Pacific region. It was established in 1989 to foster economic cooperation..."}, {"title": "4.4 Retrieval Augmentation", "content": "Before selecting the entity that best matches the mention, it is generally necessary to narrow down the candidate set while ensuring its accuracy as much as possible. Thus, by focusing on a smaller, more precise set of candidates, the model can better learn the fine-grained distinctions between the mentions to be linked and different entities, thereby improving its performance.\nWith augmented information for entities and mentions, the first step is to concatenate the entity name $en_i$ and new description $es_i$, represented as:\n$er_i = C(en_i; es_i).$\\n(4)\nSubsequently, we obtain the embedding representation of $er_i$ using a pretrained embedding model, represented as:\n$eemb_i = Embed(er_i).$\\n(5)\nBy applying the above two steps to all entities in the knowledge base $\\&$, we generate a vectorized knowledge base $V_{\\&}$.\nThen, for the mention $M_j$, we concatenate the mention name $mn_j$ with its textual context $mc_j$ and description $md_j$, represented as:"}, {"title": "as:", "content": "$mr_j = C(mn_j; mc_j; md_j),$\n(6)\nSimilarly, we derive the embedding representation of $mr_j$ using the pretrained embedding model, represented as:\n$memb_j = Embed(mr_j).$\n(7)\nAs shown in Figure 2c, we compute the cosine similarity between the mention embedding $memb_j$ and each entity embedding $eemb_i$ in a vectorized knowledge base $V_{\\&}$, retrieving the K entities with the highest similarity scores. These K entities will subsequently serve the next module, which utilizes an LLM for entity selection. Some works [29, 45, 76] indicate that in tasks involving multiple-choice selections by LLMs, the order in which optional objects are presented can influence the results. Thus, we rerank these K entities based on their similarity scores in descending order to obtain the set of candidate entities $S_c(M_j)$."}, {"title": "4.5 Multi-choice Selection", "content": "LLMs exhibit strong general applicability, furthermore, fine-tuning them on domain-specific tasks and data could enhance their capa-bilities in handling domain-specific tasks. Supported by the high-quality data and small-scaled candidates from the previous steps, we design a prompt template for the LLM instruction tuning. Formally, an LLM instruction can be formulated as a triplet: (I, T, R), where I signifies the instruction guiding the model, T represents the text input provided, and R denotes the ground-truth response serving as the target output during the fine-tuning process. The LLM could predict an answer given the instruction and text input:\n$P = f(I, T|\\theta),$\n(8)\nwhere P is the predicted answer of the LLM, and $\\theta$ are the parameters of the model. In detail, we concatenate the mention $M_j$ with the entities in its candidate set $S_c(M_j)$, obtaining the text input T:\n$T = C(M_j; C(S_c(M_j))),$\n(9)\nwhere $C(S_c(M_j))$ denotes the concatenation of all entities in the candidate set. Additionally, we require the fine-tuned LLMs to possess a stable and manageable output format. We design an example of the desired format as a demonstration to better tune the formatting of LLMs outputs. We drive the LLM to select the entity that best matches the given mention by the following prompt template $T_{ms}$."}, {"title": "5 EXPERIMENTS", "content": "In this section, we conducted comprehensive experiments on three public MEL datasets to evaluate our proposed UniMEL. Moreover, we present extensive analyses to provide a more profound understanding of our framework."}, {"title": "5.1 Experimental Setup", "content": "Datasets. We begin with a review of datasets introduced in prior works. The first MEL dataset is SnapCaptionsKB, built by Moon et al. [48]. It is composed of 12K user-generated image and textual caption pairs. Adjali et al. [4] proposed a Twitter-MEL dataset, collected between January and April 2019 using Twitter official API. Zhou et al. [78] released three new MEL datasets: Weibo-MEL, Wikidata-MEL and Richpedia-MEL. Subsequently, Wang et al. [63] constructed a new version of Twitter-MEL, WikiMEL and RichpediaMEL. Gan et al. [23] built the dataset M3EL from the Internet Movie Data Base (IMDb), The Movie Database (TMDb), and Wikipedia. Later, Wang et al. [66] released a high-quality MEL dataset WIKIDiverse.\nFollowing previous works [46, 57, 59, 65, 68], we evaluated the performance of UniMEL on three MEL datasets: Wikidiverse, WikiMEL, and Richpedia [62]:\n\u2022 Wikidiverse is conducted from the Wikinews and covers different topics and 13 entity types (e.g., Person, Country, Organization, Event, Music, etc.), which is based on the KB of Wikipedia with about 16M entities in total.\n\u2022 WikiMEL contains over 22K multimodal samples extracted from Wikipedia and Wikidata. In contrast to Wikidiverse, the majority of entity types in WikiMEL are Person.\n\u2022 Richpedia collects the Wikidata index of entities in a large-scale multimodal knowledge graph Richpedia, and is obtained the multimodal information from Wikipedia.\nWe used Wikidata as our knowledge base and removed the mentions that we could not find the corresponding entities in Wikidata. We maintained the same dataset split consistent with previous work [59]. In Wikidiverse, 80%, 10%, 10% are divided into training set, validation set and test set. In WikiMEL and Richpedia, 70%, 10%, 20% are divided into training set, validation set and test set. The statistics of the datasets are summarized in Table 2."}, {"title": "5.1.1", "content": "Baselines. We compared our method with recent state-of-the-art methods, which are divided into two groups: (1) text-only methods and (2) visual-text fusion methods. Specifically, we considered:\n\u2022 ARNN [21] (text-only) captures textual features by a duo of Attention-RNN.\n\u2022 BERT [19] (text-only) is used to encode the mention context and entity description and then calculate the relevance score.\n\u2022 BLINK [67] (text-only) integrates bi-encoder for candidate generation and cross-encoder for final entity disambiguation.\n\u2022 HieCoAtt [44] (visual-text) proposes a co-attention mechanism and and constructs co-attention maps at different levels.\n\u2022 DZMNED [48] (visual-text) takes a multimodal attention mechanism to fuse visual, textual and character-level features of mention.\n\u2022 JMEL [5] (visual-text) uses fully connected layers to project the visual and textual features into an implicit joint space.\n\u2022 MEL-HI [73] (visual-text) designs a two-stage image and text correlation mechanism to eliminate irrelevant images, and introduces an attention mechanism to capture feature in the mention and entity.\n\u2022 CLIP [52] (visual-text) employs two Transformer-based encoders to capture the relationships and semantics between visual and textual information.\n\u2022 GHMFC [63] (visual-text) uses gated hierarchical multimodal fusion and contrastive training to learn cross-modality information at a fine-grained level.\n\u2022 MMEL [70] (visual-text) proposes a joint learning framework to learn the features of contexts and entity candidates together.\n\u2022 DRIN [68] (visual-text) models four different types of alignment between a mention and entity and builds a dynamic GCN to select the corresponding alignment relations for different input samples.\n\u2022 DWE [59] (visual-text) views each mention as a query and enhances query by refined multimodal information. in addition, it enriches the semantics of entity representation by Wikipedia."}, {"title": "5.1.2", "content": "Evaluation Metric. Following previous works [59, 68], we use the Top-k accuracy metric for evaluation:\n$Top-k = \\frac{1}{|M|} \\sum_{M_j\\in M} \\sum_{E_i \\in S_c(M_j)} \\frac{I\\{S(M_j, E^*)<S(M_j, E_i)\\}<k}{(11)}$\nwhere M represents the set of mentions in the dataset, $S_c(M_j)$ denotes the set of candidate entities to be linked with mention $M_j$, and $E^*$ is the ground truth entity which should b linked with $M_j$. I is the indicator function, and S is the similarity computation function."}, {"title": "5.1.3", "content": "Candidate Retrieval. For a fair comparison, we employed the same method as recent works [59, 63, 68] for selecting the Top-100 candidate entities for each mention in a coarse-grained manner. Specifically, in Richpedia and Wikimel, we calculated the similarity between entity names with mention names by fuzzy string matching to select 100 candidate entities. In Wikidiverse, since the dataset provides 10 similar candidate entities, we only need to supplement additional 90 candidate entities using fuzzy string matching in the same way."}, {"title": "5.1.4", "content": "Implementation Details. Our UniMEL framework is implemented with PyTorch on NVIDIA RTX A6000. We employed LLaMA-3-8B [2] and LLaVA-1.6 [39] as our default LLM and MLLM, with the temperature set to 0 and other parameters remaining at their default settings, unless otherwise stated. For the embedding model, we employed SFR-Embedding-Mistral [56]. We adopted cosine similarity to calculate the relevance between the embeddings of the mention information and the entity information. During the model fine-tuning, we sampled 10,000 mentions from the Wikidiverse training set and constructed the fine-tuning dataset according to our framework procedure. Subsequently, we fine-tuned LLaMA-3-8B using the LORA [30] method, setting the rank to 8 and alpha to 32. We leveraged the AdamW optimizer [43] with a batch size of 1, a learning rate of 1e-4 and a warmup ratio of 0.03."}, {"title": "5.1.5", "content": "Main Results\nTable 3 presents the performances of our proposed UniMEL method against several competitive approaches on Wikidiverse, WikiMEL and Richpedia datasets. We report Top-1, Top-5, Top-10 and Top-20 accuracy for all three datasets. Overall, based on these results, we can draw some observations and conclusions as follows:\nFirst, our UniMEL achieves the best performance compared to existing SOTA methods on three datasets, suggesting its effectiveness. Text-only methods, such as BERT, demonstrated impressive performance, and could even compete with some visual-text methods like DZMNED and JMEL. It indicates the enduring importance of textual information as a fundamental modality for MEL, and the effective utilization of this modality is crucial in MEL tasks. It is observed that BLINK marginally underperforms compared to BERT on WikiMEL and Richpedia but performs better on Wikidiverse. They both extract the representations of mentions and entities through text encoders. ARNN also achieves comparable results comparable to those of BERT by capturing text features with Attention-RNN. Moreover, compared with the SOTA visual-text methods, the text-only methods still have a gap in performance. It is difficult to deal with vague and low-quality mention information using only textual information.\nFurthermore, each visual-text method is noteworthy, showing diverse results. Their differences lie in the methods of extracting visual representations and integrating them with textual information. Among the first five visual-text methods shown in Table 3, GHMFC appears more competitive. This could be attributed to its fine-grained learning of cross-modal information. It suggests that shallow modality interactions and simplistic multimodal fusion may not enhance the performance of the MEL task. Subsequently, CLIP, pre-trained on a large-scale image-text corpus, yields notable results. It is worth noting that DWE+ outperforms all other baselines on three datasets, coming second only to our proposed UniMEL. It benefits from its extensive utilization of visual information, complemented by the employment of Wikipedia for semantic augmentation in entity representation. Overall, on Richpedia, WikiMEL and Wikidiverse, our UniMEL obtains the Top-1 accuracy of 94.8%, 94.1% and 92.9%, respectively, improving 22.3%, 21.3% and 41.7% over the previous methods."}, {"title": "5.2", "content": "In addition, we compared our UniMEL with the existing SOTA LLMs-based method, GEMEL [57], on Top-1 accuracy. GEMEL is the first work to introduce generative methods based on LLMs in the MEL task. On WikiMEL and Wikidiverse, we aligned with GEMEL by setting the number of candidate entities to 16. As shown in Table 4, our UniMEL continues to demonstrate superiority, outperforming GEMEL by 11.6% on WikiMEL and 6.7% on Wikidiverse. Meanwhile, we compared our UniMEL with the GCN-based method, DRIN, on Wikidiverse. Following the DRIN setup, we expressly set the number of candidate entities on Wikidiverse to 10 and tested for Top-1, Top-3, and Top-5 accuracy. As shown in Table 5, it is evident that UniMEL outperforms DRIN by 42.7% on Top-1 accuracy. It can be observed that the smaller the number of candidate entities, the easier it is to select the link entity."}, {"title": "5.3 Ablation Study", "content": "To delve into the effectiveness of all the modules in our framework", "variants": 1, "Selection": "when disabling the multi-choice selection module, we simply considered the Top-1 entity from the reranking results in the embedding retrieval as the answer. Although embedding retrieval yields impressive results, using multi-choice selection improves the accuracy of Uni"}]}