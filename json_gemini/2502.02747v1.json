{"title": "PatchPilot: A Stable and Cost-Efficient Agentic Patching Framework", "authors": ["Hongwei Li", "Yuheng Tang", "Shiqi Wang", "Wenbo Guo"], "abstract": "Recent research builds various patching agents that combine large language models (LLMs) with non-ML tools and achieve promising results on the state-of-the-art (SOTA) software patching benchmark, SWE-Bench. Based on how to determine the patching workflows, existing patching agents can be categorized as agent-based planning methods, which rely on LLMs for planning, and human-based planning methods, which follow a pre-defined workflow. At a high level, agent-based planning methods achieve high patching performance but with a high cost and limited stability. Human-based planning methods, on the other hand, are more stable and efficient but have key workflow limitations that compromise their patching performance. In this paper, we propose PatchPilot, an agentic patcher that strikes a balance between patching efficacy, stability, and cost-efficiency. PatchPilot proposes a novel human-based planning workflow with five components: reproduction, localization, generation, validation, and refinement (where refinement is unique to PatchPilot). We introduce novel and customized designs to each component to optimize their effectiveness and efficiency. Through extensive experiments on the SWE-Bench benchmarks, PatchPilot shows a superior performance than existing open-source methods while maintaining low cost (less than 1$ per instance) and ensuring higher stability. We also conduct a detailed ablation study to validate the key designs in each component.", "sections": [{"title": "1. Introduction", "content": "Automatic patching of issues and vulnerabilities has long been a challenging task in software engineering and security (Jiang et al., 2021; Le Goues et al., 2021; Monperrus, 2018; Gazzola et al., 2018). Before the emergence of generative AI, automated code generation primarily relied on program synthesis (Feng et al., 2018; Huang et al., 2019), which requires human-written specifications and cannot be applied to complex programs due to the constraints of SMT solvers. With the recent success of LLMs in various generative tasks (Peng et al., 2023; Lian et al., 2023; Ghosal et al., 2023; Huang et al., 2024), particularly in code generation (Zhu et al., 2024; Anthropic, 2024; Achiam et al., 2023; Team et al., 2023; Roziere et al., 2023), researchers recently started exploring their applications in automatically fixing software vulnerabilities. They build LLM-based agents that automatically analyze and fix issues in real-world codebases (Wang et al., 2024b; Liu et al., 2024; Ruan et al., 2024; Zhang et al., 2024; Yang et al., 2024a).\nTechnically speaking, existing patching agents consist of three main components: localization, generation, and validation. The localization component identifies the code snippets responsible for the issue that need to be fixed. The generation component produces patch candidates, while the validation component selects the final patch from candidates. There are two ways to schedule these components: agent-based planning(Yang et al., 2024a; moatless, 2024; Zhang et al., 2024; IBM, 2024; Liu et al., 2024; CodeR, 2024; Pedregosa et al., 2011; Ma et al., 2024; Wang et al., 2024b; Amazon, 2024), which utilizes LLMs to dynamically determine the patching workflow for different issues; and human-based planning(Xia et al., 2024; Ouyang et al., 2024) that follows a fixed, predefined workflow for all issues, as specified by humans. Although achieving high patching performance, agent-based planning methods suffer a high cost and are not stable, which significantly limits their applicability in the real world. In contrast, existing human-based planning methods are more stable and cost-efficient but have limited patching performance due to limitations in their planning workflows.\nIn this paper, we present PatchPilot, a novel patching framework that balances the patching efficacy, stability, and cost-efficiency. At a high level, PatchPilot designs a human-based planning workflow composed of five components: reproduction, localization, generation, validation, and refinement. Given an issue as input, PatchPilot first reproduces the issue and retrieves related testing cases and finds the root cause (code snippets causing the issue) through localization. Its generation and validation components then generate patch candidates and validate whether they fix the issues"}, {"title": "2. Existing Patching Agent and Limitations", "content": "At a high level, existing patching agents mainly have three components: localization, generation, and validation. The localization component pinpoints the code snippets that cause the issue and need to be fixed (denoted as \"root cause\"), the generation produces patch candidates, and the validation tries to find a final patch in the candidates. Although they have similar components, based on planning strategies, existing patching agents can be categorized into agent-based planning and human-based planning. Agent-based planning leverages LLMs to determine the patching workflow (i.e., deciding when and which components to call), which can be different from different issues. On the contrary, human-based planning follows a fixed workflow for all issues pre-specified by humans.\nAgent-based planning. Most existing patching agents follow agent-based planning. However, most of them are closed-source: Marscode Agent (Liu et al., 2024), Composio SWE-Kit (Composio, 2024), CodeR (CodeR, 2024), Lingma (Ma et al., 2024), Amazon Q (Amazon, 2024), IBM Research SWE-1.0 (IBM, 2024), devlo (devlo, 2024), Gru (gru, 2024), and Globant Code Fixer Agent (Globant, 2024). Here, we focus on the open-source approaches.\nA notable early method is SWE-Agent (Yang et al., 2024a), which has only localization and generation and leverages an LLM planner to drive the patching process. To assist the planner in calling functions within each component, SWE-Agent provides an Agent-Computer Interface (ACI), which grants LLMs the ability to execute bash commands and handle file operations (e.g., file_open and func_edit). Follow-up works improve SWE-Agent by either improving its current components (AutoCodeRover (Zhang et al., 2024)) or incorporating additional components (Moatless (moatless, 2024; Antoniades et al., 2024) and SpecRover (Ruan et al., 2024)). Notably, Moatless and SpecRover add a validation component. This component first lets LLM generate an input that can trigger the issue (denoted as \"Proof-of-Concept (PoC)\") and then runs the PoC against the generated patches to decide if they fix the issue.\nSo far, the SOTA open-source tool in this category is Open-Hands (Wang et al., 2024b), which is inspired by the Code-Act Agent (Wang et al., 2024a). OpenHands has three components: localization, generation, validation. Its validation follows a similar idea as SpecRover, i.e., reproducing and executing PoC to decide if the issue is fixed. Similar to the SWE-agent, OpenHands also designs an ACI for the agent.\nLimitations. Agent-based planning approaches inherently suffer from two critical limitations. First, as probabilistic models, LLMs intrinsically have randomness. The randomness is aggregated and amplified when the model is making all critical decisions during the patching. This will significantly jeopardize the stability and reliability of the patching agents, hindering their real-world usage. Second, to reduce randomness, existing approaches conduct multiple samples and trials, and ensemble them to obtain the LLMs' decisions."}, {"title": "3. Methodology of PatchPilot", "content": "Recall from Section 2 that we discussed the advantages and disadvantages of human-\nAs specified in Section 3, each component has its own technical challenges, and we introduce specified designs to address them. Specifically, first, we not only reproduce the issue but also find related benign tests, which later are critical for determining whether the generated patches break normal functionalities during the validation. Second, we design our localization to provide not only the root cause but also the related context that is necessary for patching, and design additional tools for localization to retrieve necessary information from the codebase. Third, for generation, we break it down into patch planning, which produces a multi-step patching plan, and patch generation, which generates patches following the plan. This design is inspired by the recent emergence of inference-phase reasoning (Wei et al., 2022; Yao et al., 2024; Yang et al., 2024b). Having a detailed plan can explicitly prompt the LLM to think deeper and reason about the issue and give more comprehensive patching solutions, which is more effective than directly prompting the model to generate the whole patch.\nThrough extensive evaluations, we first show that PatchPilot outperforms all SOTA open-source methods on the SWE-Bench-Lite and SWE-Bench-Verified benchmark (Jimenez et al., 2023). Besides, we show that PatchPilot achieves the lowest cost among both top open-source and closed-source methods, validating its balance in patching accuracy and cost-efficiency. Second, we demonstrate that PatchPilot is more stable than the SOTA agent-based planning method OpenHands (Wang et al., 2024b), validating the advantage of human-based planning in terms of stability. Finally, we validate the key designs discussed above through a detailed ablation study and demonstrate that PatchPilot is compatible with multiple SOTA LLMs. Although PatchPilot is not on top of the SWE-Bench, to the best of our knowledge, it achieves the best balance between patching performance, stability, and cost-efficiency. These are critical for practicality, making PatchPilot a promising candidate for deployment in real-world scenarios. We will open-source our implementation, agents, and logs on SWE-Bench."}, {"title": "3.1. Technical Overview", "content": "Problem definition. Given a buggy code repository written in Python, denoted as $\\mathcal{R}$, which contains a set of functionalities $\\mathcal{F} = f_1, f_2,..., f_n$ written in different files. The repository may have one or more issues, where each issue $\\mathcal{B}_i$ has an issue description written in text, denoted as $D_i$. The issue $\\mathcal{B}_i$ affects a subset of functionalities, denoted as $\\mathcal{F}_{B_i} \\subseteq \\mathcal{F}$. A successful patch, denoted as $p$, should fix all functionalities in $\\mathcal{F}_{B_i}$ while preserving the behaviors of the unaffected functionalities $\\mathcal{F}_{S_i} = \\mathcal{F} \\setminus \\mathcal{F}_{B_i}$. Our main goals are twofold. First, we aim to resolve as many issues as possible across different issues and diverse repositories. Second, we also aim to maximize the stability and reduce the cost of our patching framework. We believe stability and cost-efficiency are critical for real-world applications of a patching tool. An unstable tool that produces only one correct patch across multiple runs significantly hinders its applicability for critical bugs. Furthermore, if the tool is too costly to use, it limits its usage by ordinary users.\nRationale behind PatchPilot. Recall from Section 2 that we discussed the advantages and disadvantages of human-"}, {"title": "3.2. Reproduction and validation", "content": "Reproduction. We introduce three improvements over existing work (Xia et al., 2024). First, reproduction in existing patching agents directly provides an LLM with $\\mathcal{R}$ and $D_i$ and prompts it to generate a PoC. However, $D_i$ often includes only short code snippets related to the issue without specifying necessary dependencies and configurations (e.g., the issue descriptions of Django typically do not have environment setups). Without such information, the generated PoCs often fail to run successfully. To address this challenge, we propose a self-reflection-based PoC reproduction, which is similar to the Reflexion mechanism designed for language agents (Shinn et al., 2024). During the process, we let LLM iteratively generate and refine the generated PoC for certain iterations. We carefully construct our prompts to guide the LLM focus on checking and correcting 1) whether any key dependencies and configurations are missing; and 2) whether the PoC actually reproduces the target issue. If the reproduction fails to generate a valid PoC within the maximum iterations, we proceed without a PoC. Second, different from existing works that only use the generated PoCs, we extract a more complete set of information based on PoCs. This includes files covered by running the PoC, stack traces and outputs. As we will discuss later, this extra information helps localization and refinement. Third, we utilize LLM to identify three functionality test files from $\\mathcal{R}$ that are most relevant to the target issue (each file may contain multiple testing cases). These functionality tests enable the validation component to decide if the patch candidates preserve the functionalities of $\\mathcal{F}_S$, an important metric for a successful patch. More details about additional information retrieved based on PoCs are discussed in Appendix A.\nValidation. The simple validation strategy utilized in existing works (Tao et al., 2024; Globant, 2024; Ma et al., 2024) is just to feed the patch candidates and the related information to an LLM and let it select the most qualified one. A more advanced strategy (Liu et al., 2024; Wang et al., 2024b; Arora et al., 2024) is to run the generated PoC and let an LLM decide whether the patches fix the issue based on their outputs. As mentioned above, ensuring the correctness of the original functionalities is as important as fixing the issue. As such, we include the functionality tests recovered by our reproduction in the validation. Specifically, we first run our PoC on the patch candidates and use an LLM as a judge for its evaluation. Since no assertions are available for bug fixing, this serves as the only feasible solution. We then also run the functionality tests and decide whether they pass based on their given assertions. Finally, we rank the patches based on the tests they pass. As specified in Appendix A, we prioritize patches that pass PoC tests over functionality tests during ranking."}, {"title": "3.3. Localization", "content": "Key challenges. Some existing localization directly query an LLM to identify the root cause at a line level (Yang et al., 2024a; Arora et al., 2024; Zhang et al., 2024). Although they provide the LLM with tools to retrieve information from the codebase and allow it to refine its results, it is still difficult for LLMs to directly perform localizations at the line level. Besides, most agent-based tools incur high costs because they need to maintain the LLM agent's context history during localization. Agentless designs a hierarchical workflow, which first identifies the issue-related files, then the functions, and lastly the lines. This method gradually zooms into and makes the task easier at the line level as it filters out the majority of the non-related functions in the earlier steps. At each step, Agentless lets the LLM make decisions only based on the issue description. This approach has three critical limitations. First, the information in issue descriptions is diverse and not all of them have useful information for localization. For example, some descriptions only specify error messages and PoC-related information that is not helpful for localization. Second, this method lacks a direct mechanism for retrieving details directly from the codebase. Third, in most cases, the localization returns only the root cause it is confident about as a few lines of code. While this information is accurate, it is often insufficient for writing a correct patch due to the lack of necessary context.\nOur design. We follow the three-step procedure in Agentless given it is more stable and efficient than letting LLM directly do line level localization. First, to address the limitation of inconsistent issue descriptions, we provide the LLM with the PoC code and information after running it (i.e., files it covered, stack trace, and running outputs). This enables the LLM to access more comprehensive information, such as key functions or classes invoked in the PoC and the stack trace, which is particularly useful for cases where only the code to reproduce the issue is provided in the issue description. For example, the files covered by PoC can help filter out some files irrelevant to the target issue, reducing the search space, especially for codebases with many files. Second, to enable the LLM to extract and leverage more information from the codebase, we add a set of tools to the localization component. These tools allow the LLM to search for class definitions and function definitions, or perform fuzzy string matching to locate and return relevant files. These tools provide precise search capabilities and can handle both class/function level information and line level details. Appendix A has more details on the tools we integrate. Third, as shown in Figure 1, we add a review step that lets an LLM retrieve code snippets related to the current root cause. As mentioned above, localization oftentimes returns overly precise root causes that fail to include necessary context or even do not fully cover all root causes. Identifying more contexts is important to generate correct and complete patches. Note that we still constrain the maximum length of the final root cause to make sure not to overwhelm the generation with excessive context."}, {"title": "3.4. Patch Generation", "content": "Key challenge. Most existing patch generation components simply stack the related information and feed them to LLM for patch generation. Such a simple solution has two critical challenges. First, LLMs typically give incomplete patches. This is because fixing an issue often requires modifications across multiple locations or involves multiple steps, making it difficult to generate a complete patch in one shot. In addition, the incomplete root causes also lead to this issue. Second, being able to generate diverse patches is also crucial to increasing the likelihood of finding a successful patch within certain trials. Moreover, we find that simply increasing the temperature still results in similar patches. We need other strategies to increase patch diversity, enabling the agent to search for more potential solutions.\nOur design. First, as shown in Figure 1, rather than directly generating the patch, PatchPilot breaks down the generation process into planning and generation. The planning phase first queries the LLM to generate a patch plan with multiple steps. The generation phase then generates the patch following the plan. After finishing each step in the plan, we also include a lightweight in-generation validation with lint and syntax checks, and reconduct this step if the check fails. This design is motivated by the Chain-of-thoughts prompting strategy (Wei et al., 2022). That is, having a plan explicitly forces the LLM to break down the patch generation into multiple steps. This helps the model to better reason about the patch task, encouraging it to provide more complete patches. Besides the in-generation validation can identify and fix errors at an early stage, improving the patch efficiency. Second, to enhance the diversity of the generated patch candidates, we design three types of prompts for plan generation. These prompts explicitly guide the LLM to produce patching plans with different focuses: a comprehensive and extensive patch designed to prevent similar issues, a minimal patch with the smallest possible modifications, or a standard patch without any specific instructions. Appendix D contains more details on the prompts that we use. As demonstrated Figure 1, we will generate N plans following the pre-specified prompts and thus produce N patch candidates in each batch."}, {"title": "3.5. Patch Refinement", "content": "Recall that refinement is a unique component in PatchPilot that existing works do not have. The motivation for adding this component is to better leverage the validation feedback and the current partially correct patches. As shown in Section 4.3, refining existing parties based on validation results is more effective and efficient than re-generating patches from scratch. More specifically, as demonstrated in Figure 1, PatchPilot focuses on refining the top-ranked patch in the current batch. It feeds the current batch and its validation result back to the generation component and asks it to generate a new batch of patches. The generation still follows the planning and generation workflow. Here, when generating the plans, we design the prompt to guide the"}, {"title": "4. Evaluation", "content": "We evaluate PatchPilot from the following aspects: First, we perform a large-scale comparison of PatchPilot with both SOTA open-source and closed-source methods on the SWE-Bench-Lite and SWE-Bench-Verified patching benchmark (Jimenez et al., 2023), showcasing PatchPilot's ability to balance patching accuracy and cost-efficiency. Second, we conduct a stability analysis on PatchPilot and OpenHands, demonstrating PatchPilot's human-based planning is more stable than the SOTA agent-based planning. Third, we conduct an ablation study to quantify the contribution of each component to PatchPilot's overall performance. Finally, we show PatchPilot's compatibility and performance on different models, including GPT-40 (OpenAI, 2024a), Claude-3.5-Sonnet (Anthropic, 2023), and a reasoning model 03-mini (OpenAI, 2024b). We failed to integrate DeepSeek-r1 (DeepSeek, 2025) due to the problems with their APIs (See Appendix C.3)."}, {"title": "4.1. PatchPilot vs. Baselines on SWE-Bench", "content": "Setup and design. We utilize the SWE-Bench (Jimenez et al., 2023) benchmark, where each instance corresponds to an issue in a GitHub repository written in Python. Specifically, we consider two subsets: SWE-Bench-Lite (SWE-Bench, 2023a), consisting of 300 instances, and SWE-Bench-Verified (SWE-Bench, 2023b), comprising 500 instances that have been verified by humans to be resolvable.\nWe mainly compare PatchPilot with three SOTA open-source methods: two agent-based planning methods Open-"}, {"title": "4.2. PatchPilot vs OpenHands in Stability", "content": "Setup and design. We compare the stability of PatchPilot and OpenHands, the SOTA open-source agent-based planning tool. We find 102 common instances resolved by PatchPilot and OpenHands in the SWE-Bench-Lite benchmark and randomly select a subset of 45. We run PatchPilot and OpenHands on these instances three times with GPT-40 model and different Python random seeds. We report and compare their resolved rate and total cost in each run.\nResults. Figure 2 shows the resolved rate and costs of PatchPilot and OpenHands across three runs. As shown in the figure, PatchPilot consistently resolved more instances, achieving 30, 32, and 35 resolved instances in the three runs, with a standard deviation of 2.52. In comparison, OpenHands resolved only 15, 20, and 21 instances, with a higher standard deviation of 3.21. The lower standard deviation of PatchPilot demonstrates its stability, which further validates our discussion about human-based planning vs. agent-based planning in Section 3.1. Additionally, PatchPilot demonstrated a clear advantage in terms of cost"}, {"title": "4.3. Ablation Studies", "content": "Setup and design. We conduct a detailed ablation study to investigate the efficacy of key designs in PatchPilot. We use the full SWE-Bench-Lite benchmark and the Claude-3.5-Sonnet model for all variations of our method. Specifically, we consider the following four variations: Base Local+Gen: We combine simple localization without providing the LLM with tools or a review step, along with simple generation without the two-phase design (Figure 1). We choose the final patch by majority voting. Our Local+Gen: We combine PatchPilot's localization and generation components together with majority voting for final patch selection. Comparing with can assess the effectiveness of our proposed techniques for localization and generation. Our Local+Gen+PoC: We further add our validation component to but with only the PoC tests (the validation strategy employed by most existing tools). Comparing with can assess the effectiveness of having PoC validation instead of simple majority voting. Our Local+Gen+Val: We add the full validation component, comparing with can assess the efficacy of having functionality tests in validation. Finally, comparing with PatchPilot can assess the importance of having an additional refinement component.\nResults. Figure 3 shows the resolved rates across different variations and our final method. By incrementally building upon the core functionalities of PatchPilot, we evaluate the contributions of individual components to the overall patching performance.\nLocalization and generation. First, we can observe that"}, {"title": "4.4. PatchPilot on Different Models", "content": "Setup and design. To demonstrate the compatibility of PatchPilot to different LLMs, we conduct an experiment that integrates PatchPilot with three SOTA LLMs: two general models GPT-40 and Claude-3.5-Sonnet, and one reasoning model: 03-mini. We select a subset of 100 instances from the SWE-Bench-Lite benchmark; all these 100 instances have been successfully resolved by at least one method ranked Top-10 on the SWE-Bench leaderboard. We run PatchPilot with the selected models on these instances and report the final resolved rate. We keep all other components the same and only change the model to show the impacts of the different models.\nResults. The resolved rate of PatchPilot with different models are: GPT-40: 19.00%; Claude-3.5-Sonnet: 39.00%, and 03-mini: 43.00%. 03-mini achieves the highest resolved rate, indicating having inference-phase reasoning capabilities is helpful not only for general math and coding tasks but also for the specialized patching task. Note that although we cannot directly compare with the results reported from official reports (anthropic, 2024; OpenAI, 2024c; Under,"}, {"title": "5. Discussion", "content": "Resolved rates vs. stability and cost. As a human-based planning method, PatchPilot achieves a well-balanced trade-off between resolved rates, stability, and cost-efficiency. We believe that emphasizing stability and cost-efficiency is essential for a patching agent to be practical in real-world applications. Although PatchPilot is not on the top of the leaderboard, it has been shown to be a stable and affordable method, confirming its practicality. Furthermore, PatchPilot outperforms all open-source methods on the leaderboard, establishing reasonably good performance.\nStatic analysis vs. LLMs. We tried multiple static program analysis approaches in different components which were not effective and cannot outperform LLMs. First, we added function summaries for the functions in the root cause to the generation component. It improves the performance of PatchPilot with GPT-40. However, it is not helpful when using more advanced models (03-mini and Claude-3.5-Sonnet), indicating that advanced models may be able to infer function behaviors. In validation, we tried to apply rule-based criteria to the PoC outputs to decide whether an issue is fixed. This is worse than using LLM as a judge, given that many issues are \u201clogical bugs\" that do not cause crashes. LLMs can better understand the issue and make decisions based on PoC outputs. We also tried to use CodeQL to infer patch-related locations that needed to be changed together with the current patch, but it failed due to CodeQL's limited performance.\nComplex prompting strategy. We tried Tree of Thoughts (ToT) (Yao et al., 2024) in generation, i.e., generating multiple candidates for each step of a plan. While significantly increasing costs, this approach does not improve the resolved rate given the LLMs cannot generate candidates with enough diversity for specific patching steps."}, {"title": "6. Conclusion and Future Works", "content": "We present PatchPilot, a stable and cost-efficient patching agent driven by a human-based planning workflow. We design five important components and each element has its"}, {"title": "A. Additional Technical Details", "content": "Tools provided in localization. For file-level localization, we provide LLM with three tools, which are search_func_def, search_class_def, and search_string. search_func_def and search_class_def take the function or class name as input and output the file containing the corresponding function or class definition based on exact matching. search_string takes a string as input and returns the file that contains this string the most times. If no file is found, we perform a fuzzy match with a decreasing similarity threshold until a match is found or timeout. This is because the string searched by the LLM, often an error message in $D_i$, is typically a formatted string in the code and may not exactly match the one in $D_i$. We chose to provide these three tools because they could cover most of the information provided in the issue descriptions.\nNote that the use of these tools is predefined and incorporated into the file-level prompt at the start of the localization process, rather than being determined dynamically by the LLM. This ensures that our approach remains firmly rooted in human-based planning.\nAdditional PoC information. We also attempt to identify the issue-introducing commit by finding the first historical commit where the PoC triggers the issue and its preceding commit. This is achieved by using a binary search to find the latest commit whose PoC output matches the current commit. Then, we prompt LLM to analyze the PoC output to determine if the preceding commit produces expected results or triggers unrelated errors. If it is the former case, we consider we have found the issue-introducing commit. We extract the code difference between the issue-introducing commit and its previous commit. This code difference provides valuable insights into how the bug was introduced, which is highly beneficial for both localization and generation. In Section 4.1, PatchPilot successfully identified the issue-introducing commit in 18 out of 300 instances on the SWE-Bench-Lite benchmark, among which 13 were resolved, achieving a resolved rate of 72.22%. In the SWE-Bench-Verified benchmark, the issue-introducing commit was found in 39 instances, with 24 successfully resolved, achieving a resolved rate of 61.53%. These high resolved rates further demonstrate the effectiveness of the issue-introducing commit.\nRanking criteria. We design our ranking criteria to prioritize the PoC test over benign functionality tests. The rank of a patch p is defined as\n$RK_p = \\mathbb{1}(PoC\\_failed) + \\frac{numfailed\\_func\\_test}{numexecuted\\_func\\_test}$,\nwhere $RK_p$ is the rank of the patch p, $\\mathbb{1}(PoC\\_failed)$ is an indicator function determined by whether the PoC fails, $numfailed\\_func\\_test$ is the number of failed functionality tests, and $numexecuted\\_func\\_test$ is the number of executed functionality tests. We rank the patches based on the reverse order of $RK_p$ (i.e., the lower the $RK_p$, the higher the ranking). If multiple patches have the same rank, we leverage an LLM to determine which one is the best."}, {"title": "B. Implementation", "content": "Reproduction. For reproduction, we set the iteration limit as 7 for the PoC generation. We generate at most one PoC for each instance. If a PoC is successfully obtained, we leverage the Python coverage package to get the files that are covered during the execution of the PoC. We provide LLM with $\\mathcal{R}$'s directory tree structure and the issue description $D_i$, directly prompting LLM to retrieve three existing test files as benign functionality tests.\nLocalization. In localization, we perform a hierarchical workflow, including file level, class and function level and line level localization, followed by a review step. At the file level, PatchPilot constructs a tree-structured repository representation, filtered based on the PoC coverage, retaining only the files executed during PoC execution. The issue description, the repository representation, and the set of tools described in Appendix A are then provided to the LLM, prompting it to return the top 5 files most relevant to the issue. To achieve self-consistency (Ahmed & Devanbu, 2023), we perform file-level localization 4 times and perform majority voting to get the top 5 files. These files are inputs of the class and function-level localization. At the class and function level, the LLM is provided with the signatures and comments of classes and functions extracted from the retrieved files and is prompted to identify functions and classes likely related to the issue. The number of functions and classes returned by LLM is not limited. At the line level, we provide the complete source code of the identified classes and functions to the LLM and prompt it to localize to specific lines. The class and function-level localization and the line-level localization are both performed 4 times, with the results of each step merged. The localized lines, along with the surrounding code within a \u00b115-line range, are considered the root cause that will be provided to generation. At the end, we prompt LLM to perform review. If the root cause is less than 150 lines of code, we prompt LLM to retrieve more code"}, {"title": "C. Additional Experiments and Results", "content": "Table 3 and Table 2 are about the current results on the SWE-Bench Lite/Verified, which show that PatchPilot has the highest performance among open-source tools and is competitive with closed-source tools in terms of resolved rate and cost.\nC.2. More Analysis on Stablility Test.\nNext, we go deep to analyze the resolved instances in each run in Section 4.2. Figure 4 provides a Venn diagram comparison of the inter-"}, {"title": "C.3. Failed Attempts with DeepSeek-r1.", "content": "During our experiments with four platforms offering DeepSeek-r1 model services (DeepSeek (DeepSeek, 2025), together.ai (together.ai, 2024), firework.ai (fireworks.ai, 2024), and deepinfra (deepinfra, 2024)), we encountered multiple barriers that prevented us from completing the necessary tests. Specifically, DeepSeek's own platform was affected by a DDoS attack, causing repeated service disruptions. Meanwhile, both together.ai and firework.ai exhibited severe response delays that rendered our experiments infeasible. In addition, deepinfra restricted the permissible context length for DeepSeek-r1 to 16k tokens, falling short of our PatchPilot pipeline's requirements. Consequently, despite our attempts, we"}, {"title": "C.4. Case Study", "content": ""}, {"title": "Successful Cases.", "content": "Listing 1-4 shows an example issue that was resolved by PatchPilot but rarely resolved by other methods in Table 3. Among the top 10 methods", "details": "In Django", "choices.\nFindings": "In our study of processing this issue", "s two core features are critical to resolving the issue": 1}]}