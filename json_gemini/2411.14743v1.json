{"title": "FOCUS: Knowledge-enhanced Adaptive Visual Compression for Few-shot Whole Slide Image Classification", "authors": ["Zhengrui Guo", "Conghao Xiong", "Jiabo Ma", "Qichen Sun", "Lishuang Feng", "Jinzhuo Wang", "Hao Chen"], "abstract": "Few-shot learning presents a critical solution for cancer diagnosis in computational pathology (CPath), addressing fundamental limitations in data availability, particularly the scarcity of expert annotations and patient privacy constraints. A key challenge in this paradigm stems from the inherent disparity between the limited training set of whole slide images (WSIs) and the enormous number of contained patches, where a significant portion of these patches lacks diagnostically relevant information, potentially diluting the model's ability to learn and focus on critical diagnostic features. While recent works attempt to address this by incorporating additional knowledge, several crucial gaps hinder further progress: (1) despite the emergence of powerful pathology foundation models (FMs), their potential remains largely untapped, with most approaches limiting their use to basic feature extraction; (2) current language guidance mechanisms attempt to align text prompts with vast numbers of WSI patches all at once, struggling to leverage rich pathological semantic information. To this end, we introduce the knowledge-enhanced adaptive visual compression framework, dubbed FOCUS, which uniquely combines pathology FMs with language prior knowledge to enable a focused analysis of diagnostically relevant regions by prioritizing discriminative WSI patches. Our approach implements a progressive three-stage compression strategy: we first leverage FMs for global visual redundancy elimination, and integrate compressed features with language prompts for semantic relevance assessment, then perform neighbor-aware visual token filtering while preserving spatial coherence. Extensive experiments on pathological datasets spanning breast, lung, and ovarian cancers demonstrate its superior performance in few-shot pathology diagnosis. Code will be made available at https://github.com/dddavid4real/FOCUS.", "sections": [{"title": "1. Introduction", "content": "Traditional pathology has long relied on microscopic examination of tissue specimens for disease diagnosis, particularly in cancer detection [38]. The digitization of glass slides into whole slide images (WSIs) has transformed this field, enabling computational analysis of these high-resolution images [29]. This has given rise to computational pathology (CPath), where computational methods are introduced to automatically analyze WSIs for disease diagnosis [10]. However, the ultra-high resolution nature of WSIs (often exceeding 100,000 \u00d7 100, 000 pixels) and the fact that diagnostic patterns may be scattered across different tissue regions present unique challenges, naturally leading to the development of multi-instance learning (MIL), a weakly-supervised learning paradigm [5, 25, 32, 44, 47, 49, 50, 52].\n\nMIL for slide-level diagnosis and prognosis typically follows three steps: first, the tissue regions (foreground) of WSI are segmented into smaller patches; second, these patches are encoded into feature representations using pre-trained feature extractors; finally, an aggregation mechanism combines these features to generate slide-level representations for analysis [20]. While MIL framework has been proven highly effective in the field of CPath [5-7, 9, 13, 16, 18, 19, 22-26, 32, 35, 40, 42, 47-49, 51], training current approaches typically demands large annotated datasets that are labor-intensive and time-consuming to obtain, particularly for rare pathological conditions [15]. These limitations underscore the pressing need for a learning mechanism that can effectively learn from limited examples while maintaining the advantages of traditional MIL approaches, potentially revolutionizing how CPath models are deployed in resource-constrained settings [36, 46]. Meanwhile, although MIL methods are designed to handle the inherent sparsity of diagnostic information in WSIs, where only a small fraction of patches contains relevant features, the few-shot setting introduces additional challenges. With limited training examples, models must learn to identify and leverage discriminative patterns from a drastically reduced set of WSIs, each containing thousands of patches with varying degrees of diagnostic relevance. To this end, the concept of few-shot weakly-supervised learning (FSWL) is first proposed by Qu et al. [30], aiming at addressing these challenges via a data-efficient manner. Build-"}, {"title": "2. Related Works", "content": "Multiple instance learning in CPath. Due to the gigapixel size of WSIs and GPU memory constraints [2], the MIL framework has become the predominant approach for WSI analysis. In recent years, MIL-based methods have demonstrated remarkable success in weakly-supervised WSI diagnosis and prognosis [5, 9, 13, 16, 18, 22-26, 32, 34, 40, 42, 47-49, 51]. MIL methods typically comprise two key components: a feature extractor and an aggregator. The feature extractor (usually pre-trained) projects WSI patches into a latent space to obtain their representations, then the aggregator combines these patch-level representations to create a slide-level representation for diagnosis or prognosis. Traditional approaches rely on simple, non-parametric aggregation methods such as max and mean pooling operators [5]. Subsequent research has focused on developing more sophisticated aggregation algorithms to better identify and capture critical diagnostic patterns distributed across thousands of patches. Ilse et al. [18] introduce ABMIL, an"}, {"title": "3. Methodology", "content": "This work introduces the knowledge-enhanced adaptive visual compression framework for few-shot whole slide image analysis, dubbed FOCUS, as illustrated in Figure 1. FOCUS comprises three key components: the knowledge-enhanced adaptive visual token compression module, the sequential visual token compression module, and the cross-modal aggregation module. These components work in concert to achieve adaptive visual token compression by leveraging pathology FMs and domain-specific prior knowledge. The overall workflow of FOCUS is further elaborated by pseudo-code (see Algorithm 1 in Supplementary Material)."}, {"title": "3.1.1 WSI Bag Construction", "content": "Formally, let \\( X \\in \\mathbb{R}^{H\\times W\\times 3} \\) be a whole slide image (i.e., a bag). Due to the gigapixel resolution of WSIs, direct processing is computationally infeasible. Following standard practice, we first divide the foreground of \\( X \\) into \\( N \\) non-overlapping patches \\( X = \\{X_1, X_2, ..., X_N\\} \\) where each \\( X_i \\in \\mathbb{R}^{h\\times w\\times 3} \\). Each WSI \\( X \\) is assigned with a slide-level label \\( Y \\in \\{0,1,..., S\\} \\), where \\( S \\) is the number of classes. The label for each patch (instance) inside the bag is unavailable. These patches are projected through a pre-trained model \\( f(.) \\) (pathology foundation models in our implementation) to obtain a bag of patch-level representations:\n\\[ B = \\{f(x_i) : X_i \\in X\\} = \\{b_i\\}_{i=1}^{N}, \\]\nwhere \\( b_i \\in \\mathbb{R}^d \\) and \\( d \\) is the feature dimension. Unlike multi-scale approaches, our framework operates on a single WSI scale (high-resolution scale in our implementation), simplifying the input requirements. Note that in the"}, {"title": "3.2. Knowledge-enhanced Adaptive Compression", "content": "As shown in Figure 1(a), the knowledge-enhanced adaptive visual token compression module leverages both foundation models and pathology domain knowledge to conduct visual token compression and identify diagnostically relevant patches. This module implements a two-stage selection strategy: global redundancy removal through coarse-grained filtering via pathology FM features, followed by language-guided visual token prioritization."}, {"title": "3.2.1 Global Redundancy Removal via Pathology FMS", "content": "Given a bag of patch representations \\( B = \\{b_i\\}_{i=1}^{N} \\) extracted by the pathology foundation model (CONCH [27]"}, {"title": "3.3. Sequential Visual Token Compression", "content": "As illustrated in Figure 1(b), we introduce a sequential visual token compression module that processes tokens in sequential order to eliminate local redundancies while maintaining relative spatial structure. This module implements a multi-stage compression strategy with progressively increasing similarity thresholds. Given a sequence of selected tokens \\( B = \\{b_1, b_2, ..., b_k\\} \\), at each stage \\( i \\), we compute the cosine similarity between consecutive tokens:\n\\[ S_{j,j+1} = \\frac{b_j \\cdot b_{j+1}}{||b_j||_2 \\cdot ||b_{j+1}||_2}, \\quad j \\in \\{1, ..., k - 1\\} \\] \nThe compression follows a threshold-based rule with increasing thresholds \\( \\theta_i = \\theta_{base} + i\\Delta \\) to generate binary masks:\n\\[ \\text{mask}_j = \\begin{cases} 1, & \\text{if } \\min(s_{j-1,j}, s_{j,j+1}) < \\theta_i \\\\ 0, & \\text{otherwise} \\end{cases} \\]\nwhere \\( \\Delta \\) controls the increase in filtering intensity across stages. The sequence is progressively compressed as \\( B_{i+1} = \\{b_j \\in B_i : \\text{mask}_j = 1\\} \\), with the final compressed sequence \\( B_c \\) containing tokens that maintain distinctiveness through all compression stages."}, {"title": "3.4. Cross-modal Aggregation for Prediction", "content": "Following the compression stages, we employ a cross-modal aggregation module, illustrated in Figure 1(c), to combine the compressed visual tokens with pathological semantic information for final prediction. Given the compressed sequence \\( B_c \\in \\mathbb{R}^{N_c\\times d} \\) and text embeddings \\( T = [T_L; T_P] \\), we first compute text-guided aggregation through a multi-head attention mechanism, then the outputs from all heads are concatenated and normalized:\n\\[ \\text{Head}_i = \\text{softmax}\\left(\\frac{QW_q (KW_k)^T}{\\sqrt{d_k}}\\right)VW_v, \\]\n\\[ O = \\text{LayerNorm}(\\text{Concat}(\\text{Head}_1, ..., \\text{Head}_h) W_o), \\]\nwhere \\( Q = TW_q \\), \\( K = B_cW_k \\), \\( V = B_cW_v \\) are query, key, and value projections, respectively, and \\( d_k \\) is the dimension per head. The final prediction is obtained through a classification head:\n\\[ P(Y | B_c, T) = \\text{softmax}(W_o O + \\beta_c), \\]\nwhere \\( W_o \\) and \\( \\beta_c \\) are learnable parameters."}, {"title": "3.5. Training Strategy", "content": "The model is trained end-to-end using cross-entropy loss to minimize the negative log-likelihood of the ground truth labels:\n\\[ \\mathcal{L} = \\log P(Y | B_c, T). \\]"}, {"title": "4. Experiments and Results", "content": null}, {"title": "4.1. Experimental Settings", "content": "Datasets. To demonstrate the effectiveness of our proposed framework FOCUS, we conduct extensive experiments on three publicly available pathology datasets, i.e., TCGA-NSCLC\u00b9 [37], CAMELYON2,3 [3, 4], and UBC-OCEAN4, covering lung, breast, and ovarian cancers. TCGA-NSCLC comprises two lung cancer subtypes: lung adenocarcinoma (LUAD, 541 slides) and lung squamous cell carcinoma"}, {"title": "4.2. FSWL Results", "content": "To evaluate the effectiveness of FOCUS, we compare our framework against SOTA methods in general pathology image analysis approaches including TransMIL [32], DS-MIL [21], DTFD-MIL [49], and WiKG-MIL [23], as well as SOTA methods in few-shot pathology image analysis like TOP-MIL [30] and ViLa-MIL [33]. Additionally, we establish strong baselines using foundation model features with Max Pooling, Mean Pooling, and Attentive Pooling as linear probing methods. CONCH [27] is applied to extract visual features for all methods. We compare FOCUS with these methods across three datasets under three few-shot settings (4-shot, 8-shot, and 16-shot), as shown in Table 1."}, {"title": "4.3. Ablation Studies", "content": null}, {"title": "4.3.1 Effects of Key Modules in FOCUS", "content": "To evaluate the contribution of each module within FOCUS to the overall performance, we conduct an ablation study on the UBC-OCEAN dataset using 5 variants under 4-, 8-, and 16-shot settings. The experimental results, measured by Balanced ACC with ten-fold cross-validation, are reported in Table 2."}, {"title": "4.3.2 Effects of Pathology FM Encoders", "content": "To evaluate the effect of using different pathology FM as feature extractors for FOCUS, we employ 5 pathology FMs, i.e., UNI [8], GPFM [28], Virchow [39], PLIP [17], and CONCH [27] for comparison. Figure 2 shows the implications of different FMs as feature extractors across various few-shot settings, measured by Balanced ACC via ten-fold cross-validation. CONCH consistently outperforms other FMs, achieving the highest Balanced ACC across all settings (70.4%, 77.3%, and 86.4% for 4-shot, 8-shot, and 16-shot, respectively). This can be attributed to the fact that CONCH is pre-trained with large-scale pathological image-caption pairs, potentially generating more aligned visual and textual features. Interestingly, despite being a vision-language model, PLIP yields the lowest performance among both multi-modal and uni-modal FMs. Besides, all FMs show substantial improvement as the number of shots increases, with the most significant gains observed in 4- and 8-shot settings. The performance gap between FMs narrows under 16-shots, with UNI (84.3%), GPFM (83.2%), and Virchow (83.2%) achieving comparable results."}, {"title": "4.3.3 Effects of LLMs for Prompt Generation", "content": "We also explore the effects of employing different LLMs for pathology knowledge query answering and the candidates include ChatGPT3.5-Turbo, ChatGPT-40 [1], OpenAI-01-mini, Claude-3.5-Sonnet, and Llama3.1-405B [11]. Each LLM is prompted with identical instructions (detailed in Supplementary Material) to generate the visual descriptions used in FOCUS. We conduct this ablation study on the UBC-OCEAN dataset under 16-shots, with the results shown in Figure 3. The experimental results demonstrate the strong capability of LLMs in generating effective pathology description prompts, with all variants achieving accuracies above 83%. Among the evaluated models, Claude3.5-Sonnet achieved the highest Balanced ACC of 86.4%, followed by ChatGPT3.5-Turbo (84.8%) and OpenAI-01-mini (84.6%). These observations suggest that LLMs with enhanced language modeling capabilities and domain-specific knowledge can potentially further boost the performance of FOCUS."}, {"title": "5. Conclusion", "content": "This work presents FOCUS, a knowledge-enhanced adaptive visual compression multi-instance learning framework for few-shot WSI analysis. We propose a three-stage progressive and adaptive visual compression strategy that leverages pathology foundation models and domain-specific prompts to achieve visual redundancy removal and informative region prioritization. FOCUS operates without requiring burdensome inputs: it first conducts global redundancy removal through coarse-grained filtering based on pathology foundation models, integrates language descriptions for guided visual token compression by computing semantic relevance, then performs neighbor-aware visual token compression via fine-grained filtering, and finally a cross-modal aggregation module is applied to generate slide-level representation. Extensive experiments against SOTA methods on three pathology datasets across multiple cancer types demonstrate the effectiveness of FOCUS, showcasing strong few-shot weakly-supervised learning capability for WSI analysis and pushing the boundaries of CPath in data-scarce scenarios."}]}