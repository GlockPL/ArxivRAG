{"title": "A Performance Analysis of You Only Look Once Models for Deployment on Constrained Computational Edge Devices in Drone Applications", "authors": ["Lucas Rey", "Ana M. Bernardos", "Andrzej D. Dobrzycki", "David Carrami\u00f1ana", "Luca Bergesio", "Juan A. Besada", "Jos\u00e9 Ram\u00f3n Casar"], "abstract": "Advancements in embedded systems and Artificial Intelligence (AI) have enhanced the capabilities of Unmanned Aircraft Vehicles (UAVs) in computer vision. However, the integration of AI techniques o-nboard drones is constrained by their processing capabilities. In this sense, this study evaluates the deployment of object detection models (YOLOv8n and YOLOv8s) on both resource-constrained edge devices and cloud environments. The objective is to carry out a comparative performance analysis using a representa-tive real-time UAV image processing pipeline. Specifically, the NVIDIA Jetson Orin Nano, Orin NX, and Raspberry Pi 5 (RPI5) devices have been tested to measure their detection accuracy, inference speed, and energy consumption, and the effects of post-training quanti-zation (PTQ). The results show that YOLOv8n surpasses YOLOv8s in its inference speed, achieving 52 FPS on the Jetson Orin NX and 65 fps with INT8 quantization. Conversely, the RPI5 failed to satisfy the real-time processing needs in spite of its suitability for low-energy consumption applications. An analysis of both the cloud-based and edge-based end-to-end processing times showed that increased communication latencies hindered real-time applications, revealing trade-offs between edge (low latency) and cloud processing (quick processing). Overall, these findings contribute to providing recommendations and optimization strategies for the deployment of AI models on UAVs.", "sections": [{"title": "1. Introduction", "content": "In recent years, the integration of unmanned aerial vehicles (UAVs), commonly re-ferred to as drones, with machine learning techniques has significantly advanced the field ofaerial robotics. Machine learning algorithms, particularly deep learning approaches, enable drone-based systems to process large volumes of data collected from onboard sensors. The incorporation of computer vision algorithms into drone technologies has enhanced their autonomous capabilities, allowing drones to detect and track objects with high accuracy. Additionally, these advancements support collaborative data fusion, where information from multiple drones and sensors is combined to improve situational awareness and in-form decision-making processes. Such capabilities contribute to the effective management and coordination of drone operations in complex environments [1,2], opening the door to possible applications such as vision-driven swarming [3]."}, {"title": "2. The State of the Art", "content": "This section is divided into two subsections. The first Section 2.1, Deploying Deep Learning on Energy-Efficient Devices, evaluates the performance and optimization tech-niques for deep learning models deployed on edge hardware, with a focus on energy efficiency and real-time processing. The second subsection, Section 2.2, Quantization Techniques in Object Detection Models, reviews strategies to optimize the computational and memory demands in object detection algorithms, analyzing their feasibility for edge devices in constrained environments."}, {"title": "2.1. Deploying Deep Learning on Energy-Efficient Devices", "content": "In recent years, significant research has focused on optimizing and evaluating the performance of deep learning models on edge devices, particularly for object detection tasks in resource-constrained environments. Various studies have benchmarked the hardware configurations to identify the optimal settings for achieving a balance between the inference speed, energy consumption, and real-time processing requirements. For instance, Baller et al. [9] evaluated a range of edge devices, including the NVIDIA Jetson Nano, and identified the optimal configurations in terms of the inference speed and energy consumption across various deep learning frameworks and classification models. Such evaluations are crucial for understanding the limitations and potential of hardware in real-time scenarios.\nBuilding on hardware-specific optimizations, Li et al. introduced [11], an optimization framework for CNN inference on edge devices for application in an IoT context. This frame-work combines offline pruning and quantization techniques with runtime optimizations, such as multiplication reduction, data layout adjustments, and data parallelization. The experimental results showed performance improvements of 1.96\u00d7 in high-end devices and 1.73\u00d7 in low-end devices in improving their edge device efficiency using advanced model compression techniques. Similarly, Ferraz et al. [12] benchmarked CNN inference on edge devices, such as the NVIDIA Jetson AGX Xavier and the Movidius Neural Stick, noting considerable improvements in the inference time and power efficiency.\nRecent studies also emphasize the use of distributed and cooperative processing to enhance edge device performance. Hou et al. [13] introduced DistrEdge, a method de-signed to enhance CNN inference across distributed edge devices by employing deep reinforcement learning. This approach distributes inference tasks among multiple devices, optimizing the computational load and significantly improving the overall inference speed."}, {"title": "2.2. Quantization Techniques in Object Detection Models", "content": "Quantization techniques are valuable for optimizing deep learning models for deploy-ment in edge devices, improving the inference speed and energy efficiency, especially in real-time applications such as autonomous drone navigation and object detection. Common techniques include post-training quantization (PTQ), Quantization-Aware Training (QAT), and mixed-precision quantization (MPQ), all of which enhance the energy efficiency in reducing the computational complexity while maintaining the model's performance.\nPTQ is a technique that reduces the precision of a trained model to formats such as 8-bit integers by converting the weights and activations from floating-point representations. One of its primary advantages is that it reduces the size of the model and increases the inference speed without requiring retraining, preserving the original training process. For instance, Przewlocka-Rus et al. [22] demonstrated that PTQ applied to object detection models on the NVIDIA Jetson Nano decreased the inference times by up to 35% with a less than 2% degradation in accuracy. Similarly, Jiang et al. [21] evaluated various PTQ approaches, including affine, logarithmic, and dynamic quantization, and found that these methods effectively minimized the size of models while maintaining high accuracy with minimal losses.\nQAT integrates quantization into the training process, allowing the model to learn to minimize quantization errors. This method typically results in higher accuracy than PTQ, especially for complex models and tasks. Gupta and Asthana [24] applied QAT to object detection models such as YOLO, achieving a 1.8% improvement in mAP compared to PTQ, with reductions in the model size and inference time.\nMPQ uses different precisions for various parts of the model, balancing trade-offs between size, speed, and accuracy. Critical layers use higher precision, while less critical layers use lower precision. Park et al. [25] explored mixed-precision quantization in object tracking models, achieving energy consumption reductions of up to 45% and inference time improvements of 30% on edge devices like the Jetson Nano and Raspberry Pi. Furthermore, Al-Hamid and Kim [23] proposed a Unified Scaling-Based Pure-Integer Quantization (USPIQ) method that reduced the on-chip memory by 75% with only a 0.61% loss in the mAP while achieving a 2.84x speedup in the inference time compared to traditional methods.\nThese quantization approaches improve the energy and space efficiency in edge device deployments, making the implementation of sophisticated object detection and tracking models viable. However, the effects of these techniques on the latest YOLO models and their integration into real-time drone applications are still being researched. Studies on deploying YOLO models in different edge-cloud or embedded edge implementations are limited [21-23,25].\nDespite significant advances in the research on object detection at the edge, several gaps remain. Multiple studies have assessed edge devices in isolation without results on the latency in the interaction between the hardware and software processes.\nTo address these gaps, this study focuses on YOLOv8 models applied to edge en-vironments, emphasizing their accuracy, inference speed, and energy consumption. By incorporating quantization techniques, this research offers new insights into the poten-tial of quantization approaches to optimize YOLOv8 for resource-limited edge scenarios. The findings deliver practical recommendations for selecting the configurations in power-"}, {"title": "3. Methodology", "content": "In this study, three edge computing devices are used to evaluate the performance of YOLOv8 models and their quantized versions: the Raspberry Pi 5, the Jetson Orin Nano, and the Jetson Orin NX. These devices were chosen due to their different hardware capabilities, allowing for an evaluation of object detection tasks under various processing constraints.\nThese devices will be used to evaluate the performance of YOLOv8 models and their quantized versions under uniform experimental conditions. The Raspberry Pi 5, a CPU-centric device, was selected to assess the capabilities of a modern single-board computer without a dedicated GPU in object detection tasks. In contrast, the Jetson Orin Nano and Jetson Orin NX, which are GPU-powered platforms, were chosen for their high-performance processing capabilities. Together, these devices represent a range of edge computing architectures, enabling an evaluation of the inference speed, accuracy, and energy consumption across diverse hardware configurations.\nIn order to carry out these evaluations, it is necessary to first have an object detection model. To this end, Section 3.1 describes the process of creating such a model, including the generation of a dedicated dataset in 3.1.1, model selection and training in 3.1.2, and model optimization in 3.1.3. From the model, two types of experiments are developed:\nExperiments that deploy the model in isolation on each of the previous hardware plat-forms. These experiments aim to evaluate the inference speed and power consumption, for which the results are detailed in Section 4.\nExperiments that integrate the object detection model into a typical drone image pro-cessing pipeline. As described in Section 5, these experiments seek to compare the ca-pabilities of edge devices against a cloud deployment under a representative scenario.\nIn these experiments, a set of metrics are used to evaluate the performance of the models. These metrics are described in Section 3.2."}, {"title": "3.1. Object Detection Model Selection and Training", "content": "To train the evaluated object detection models, a dataset of annotated images was created. This dataset was collected within the indoor testbed described by the authors"}, {"title": "3.1.2. Selection and Training of Object Detection Models", "content": "For the selection of the model for this study, it was necessary to evaluate different ver-sions of the YOLO architecture in terms of their performance and efficiency. YOLOv9 was considered a potential candidate; however, YOLOv8 demonstrated superior optimization for real-time performance and latency reduction. Moreover, YOLOv8 was elected as the most stable version during the experimental advance of this research."}, {"title": "3.1.3. Optimization Techniques for Edge Deployment", "content": "To optimize the original models (small and nano) for edge devices, several techniques were employed, including reducing the precision to FP16 and applying INT8 quantization.\nFP16, or Half-Precision Floating Point, reduces the bit precision of floating-point numbers from 32 bits (FP32) to 16 bits. This precision reduction, often implemented using tools such as TensorRT during model export, enhances the performance by decreasing the computational load and memory usage. Despite reducing the precision, FP16 offers inference speed improvements without significantly impacting the accuracy.\nINT8 quantization optimizes models by converting weights and activations from floating-point precision into 8-bit integers. This process, often applied through post-training quantization (PTQ), significantly reduces the computational demand and improves the energy efficiency and inference speed, making it suitable for resource-constrained edge devices. However, unlike FP16, which typically preserves the model accuracy, INT8"}, {"title": "3.2. The Evaluation Strategy and Metrics", "content": "As previously introduced, the experiments for measuring the performance of the YOLO models were performed in two stages to measure the set of metrics described in this section,. Initially, in a controlled and isolated setting, we measured the latency, throughput, and energy efficiency to balance the inference speed and accuracy against edge device constraints. The experiments carried out, whose results are described in Section 4.1, included the following:\nInference Time Measurement : This experiment assessed the computational efficiency of each model in isolation by measuring the time to perform inferences on 5000 images at 640 \u00d7 640 resolution. The average inference time was recorded, and the MIS (FPS) metric was derived to indicate the processing capacity.\nContinuous Inference Evaluation (Inferences Per Minute, IPM): In this experiment, each device performed inferences continuously over 60 s, accounting for all of the latency factors, including model loading, network overhead, and processing delays. The IPM metric provided insight into the system's throughput under real-world conditions.\nEnergy Consumption Analysis: Real-time power usage was monitored using tegrastats for Jetson devices and vcgencmd pmic_read_adc for the Raspberry Pi 5. Power data, averaged over the duration, were used to calculate the energy con-sumption (W-s) and energy per inference (J/inference), indicating the energy efficiency necessary for battery-powered deployments.\nSecondly, trials were performed in real-world scenarios to assess the performance under actual conditions. This systematic testing provides a comprehensive grasp of the abilities of the model, from isolated evaluations to complete operational contexts. In the latter case, the following test was performed for the results discussed in Section 5:\nThroughput and Latency Assessment: The throughput times were measured in both edge and cloud environments using a timestamp-based approach involving a video server, a processing agent, and a prediction client. Timestamps captured the communication times and processing delays, with the Round-Trip Time (RTT)"}, {"title": "4. Performance Evaluation of Quantized YOLO over Resource-Constrained Devices", "content": "This section provides a comprehensive evaluation of the performance of quantized YOLOv8 models on different resource-constrained devices, emphasizing the effects of the quantization techniques and hardware constraints on the model's efficiency and accuracy. It showcases the distinct quantization formats (FP32, FP16, and INT8) applied to various devices like the Jetson Orin Nano, Jetson Orin NX, and Raspberry Pi 5, along with their associated production deployment. In particular, FP32 represents the default training format used for this type of model, which is sometimes also treated as the \u201coriginal\u201d model in the following sections. This visual workflow sup-ports comprehension of the process and identifies the specific models examined in the experiments."}, {"title": "4.1. The Inference Performance", "content": "Figure 4 illustrates the Mean Inference Speed (MIS) for the YOLO models on the Orin Nano, Orin NX, and Raspberry Pi devices, comparing the performance with TensorRT (TRT) on the Orin devices and NCNN on the Raspberry Pi.\nOn the Orin Nano, the YOLOv8n_INT8 model excelled with an average iteration time of 23.16 ms, followed by YOLOv8n_FP16 at 26.70 ms and YOLOv8s_INT8 at 28.25 ms. The YOLOv8s_FP32 model had the longest iteration time of 42.97 ms, illustrating the advantages of INT8 quantization in terms of the time performance. The Orin NX showed an even better performance, with YOLOv8n_INT8 achieving the shortest iteration time of 15.16"}, {"title": "4.2. Degradation Analysis of Model Accuracy", "content": "This section compares the YOLOv8s and YOLOv8n models across various edge devices to assess the impact of quantization on both their inference speed and accuracy."}, {"title": "4.3. Energy Consumption Analysis", "content": "The mean average energy consumption was measured over 5000 images,"}, {"title": "5. Performance Evaluation Using a Realistic Testbed", "content": "This section aims to assess the performance of different hardware devices, specifi-cally the Orin Nano, Orin NX, and Raspberry Pi 5, using an object detector in a realistic deployment scenario that mirrors real-world drone operations. In this sense, this section takes advantage of the platform presented by the authors in [26] which considers the core elements (e.g., telemetry, video streaming, and data processing) of a real-time drone system, where the interplay of computation and communications affects the latency and system throughput. As depicted in Figure 7, the testbed platform allows live drone images to be processed both in an edge device and in a cloud environment. This enables perfor-mance benchmarking of edge algorithms against a centralized cloud, providing a detailed comparison.\nThe deployment architecture consists of three main components: a drone ground control station (GCS) that gathers video in real time from a drone and broadcasts it using the WebSocket protocol, an edge computing device to perform object detection on video streams using TensorRT optimization, and a cloud processing instance (accessible through the internet) that also allows for the deployment of the object detection model. The interaction between these entities is illustrated in Figure 8.\nThe workflow begins with the transmission of video from the drone to its associated ground control station. Then, using WebSocket communication for low-latency and reliable data transfer, the video frames can be forwarded either to an edge computing device, which performs the inference locally (at the edge), or to a cloud instance that performs inference remotely (in the cloud). AWS SageMaker is employed for cloud detection, allowing for a systematic comparison of the latency and processing times between the edge and cloud setups. In either case, the inferences and annotated images are sent back to the ground control station.\nWithin this framework, two tests were performed. In Section 5.1, different edge devices are compared in the real, representative testbed. Then, in Section 5.2, a test is performed between edge and cloud processing. The latter evaluation allows for an analysis of the trade-offs between local and centralized processing in terms of the computational power and communication delays. In both cases, by timing each phase (i.e., video transmission, object detection, and prediction distribution), it is possible to identify latency issues and potential bottlenecks."}, {"title": "5.1. Edge Deployment", "content": "Table 5 provides a summary of the RTT times, RTT deviations, processing times, and throughput (FPS) for the different models and quantization configurations.\nRegarding the RTT (Round-Trip Time), the Jetson Orin Nano demonstrated its best performance in terms of the RTT with the YOLOv8s INT8 model, achieving a minimum RTT of 44.27 ms. The RTT increased to 73.18 ms when using the YOLOv8s FP32 model. For"}, {"title": "5.2. Edge vs. Cloud Comparison", "content": "In order to assess the trade-offs between computational power and latency in edge and cloud environments, we conducted a series of experiments comparing the two setups using the YOLOv8s model in its FP16 configuration on the Orin Nano. The primary metrics evaluated were the Round-Trip Time (RTT), model processing latency, and communication latency, offering a comprehensive understanding of how both environments handle real-time data processing and communication delays.\nFor our cloud-based deployment, we selected a cloud instance located in London (eu-west-2) using the ml.g4dn.xlarge configuration, which is powered by an NVIDIA T4 GPU. To handle the model inferences efficiently, we deployed the model on a Triton Inference Server within the SageMaker environment. Triton, known for its high performance and flexibility in supporting multiple AI frameworks and backends, provided a robust platform for managing the inference requests for our YOLOv8s model. The most decisive factor in choosing the NVIDIA Triton Inference Server was its support for TensorRT optimization, which allowed us to conduct a fair comparison under consistent conditions. Additionally, by utilizing Triton, we could take advantage of advanced features such as dynamic batching, model ensemble, and TensorRT acceleration, thereby maximizing the inference speed and fully leveraging the computational power of the cloud instance.\nThe model was deployed and invoked directly through the SageMaker SDK API's invoke endpoint, which facilitated direct HTTP-based invocation. However, it is important to note that the invoke endpoint does not support WebSocket communication, meaning that all inference requests are handled over HTTP. This limitation may introduce additional latency in scenarios where persistent, real-time communication channels are desired. The London (eu-west-2) location helped us explore the impact of geographical data transmission delays from Madrid, providing insights into latency management for future applications.\nTable 6 presents a comparative investigation of the latency metrics between the edge deployments on the Orin Nano and the cloud deployments using the YOLOv8s model in FP16 format. The edge RTT latency shows a mean of 35.09 ms, which is within the feasible limits for real-time applications. By contrast, the cloud deployment shows a much higher mean RTT of 348.21 ms due to the increased communication distance to remote GPU servers. This difference is reflected in the communication latency, where the edge setup"}, {"title": "6. Discussion", "content": "An evaluation of various quantization techniques and their impact on inference la-tency has confirmed INT8 as the fastest configuration, achieving notable reductions in the processing times across all evaluated devices. For instance, on the Orin Nano, YOLOv8s with INT8 achieves 41.20 FPS compared to 27.00 FPS with FP32, representing a significant improvement in speed. However, this increase comes at the cost of reduced accuracy, with the mAP50-95 dropping from 0.8608 in FP32 to 0.7968 in INT8\u2014a reduction of approxi-mately 6.4 percentage points. In contrast, FP16 emerges as a balanced alternative, offering slight improvements in accuracy while reducing the latency. For example, YOLOv8s on the Orin Nano achieves 37.90 FPS in FP16 with an accuracy of 0.8622 mAP50-95, providing an optimal balance for scenarios requiring both speed and precision. This quantization ap-proach also enables intermediate models, such as YOLOv8s, to outperform smaller versions like YOLOv8n_FP32 on more advanced devices such as the Orin NX, where YOLOv8s_FP16 achieves 48.24 FPS with a mAP50-95 of 0.8620, compared to YOLOv8n_FP32\u2032s 35.65 FPS and 0.8420 mAP50-95. Furthermore, the application of quantization techniques has been ob-served to contribute to marginal enhancements in the model's generalization, possibly due to the reduced computational complexity, enabling more efficient image processing. This is particularly relevant for tasks involving real-time object detection, where the computational resources are constrained. Conversely, while the Raspberry Pi 5 demonstrates acceptable accuracy metrics, such as a mAP50-95 of 0.8620 for YOLOv8s in FP32, its inference latency limits its applicability. The device achieves only 7.32 FPS and 217 inferences per minute in this configuration, significantly reducing the real-time capabilities. In addition to op-timizing performance, quantization techniques have a significant impact on the energy efficiency of employing devices. The results demonstrate that INT8 is the most efficient configuration in terms of energy consumption, achieving a notable reduction without a significant loss of accuracy. In this context, the Orin NX, despite its higher overall power demand, demonstrates a better performance in the EPI metric (J/inference). This makes it the preferred choice for real-time, inference-intensive applications, especially in the FP16 and INT8 configurations. The Raspberry Pi 5 has the lowest total power consumption; however, its low performance in terms of Inferences Per Minute (IPM) restricts its appli-cability in environments where a real-time response is required. This analysis highlights the importance of selecting optimized hardware devices, such as the Orin models, for edge architectures that demand both high performance and sustainable energy efficiency, particularly in long-running applications where controlled power consumption is essential."}, {"title": "7. Conclusions", "content": "This research provides practical tools and guidelines for integrating YOLOv8 models into autonomous systems while also evaluating their performance on computationally constrained drone computational devices. The findings show that the YOLOv8n (nano) and YOLOv8s (small) models can achieve efficient operation on devices such as the Raspberry Pi 5, Orin NX, and Jetson Orin Nano. However, achieving the optimal performance in real-time applications requires careful consideration of the trade-offs between the detection precision and inference speed, especially when utilizing INT8 quantization, which accelerates the processing but may slightly reduce the accuracy."}]}