{"title": "A minimalistic representation model for head direction system", "authors": ["Minglu Zhao", "Dehong Xu", "Deqian Kong", "Wen-Hao Zhang", "Ying Nian Wu"], "abstract": "We present a minimalistic representation model for the head direction (HD) system, aiming to learn a high-dimensional representation of head direction that captures essential properties of HD cells. Our model is a representation of rotation group U(1), and we study both the fully connected version and convolutional version. We demonstrate the emergence of Gaussian-like tuning profiles and a 2D circle geometry in both versions of the model. We also demonstrate that the learned model is capable of accurate path integration.", "sections": [{"title": "1. Introduction", "content": "Spatial navigation is a fundamental cognitive function shared across many species, from insects to humans. A critical component of this navigational system is the perception of direction, which allows animals to maintain a consistent representation of their orientation in the environment. In mammals, this perception of direction is primarily mediated by the head direction (HD) system, a network of neurons that collectively encode the animal's current head orientation relative to its environment (Taube et al., 1990a).\nHD cells, discovered in the rat's dorsal presubiculum (Rank, 1984; Taube et al., 1990b), exhibit a remarkable property: they fire maximally when the animal's head faces a specific direction in the horizontal plane, regardless of location or ongoing behavior. Each cell has a preferred direction, with firing rates decreasing as the head turns away, typically following a Gaussian-like tuning curve (Blair et al., 1997). Distributed across interconnected brain regions (Taube, 2007), these cells form a neural \u201ccompass\" maintaining consistent directional representation (Cullen, 2019). Intriguingly, the HD system maintains direction representation even without external sensory cues a phenomenon known as path integration (McNaughton et al., 2006). This suggests that the HD system functions as a neural integrator updating based on self-motion cues. Theoretical and computational models have"}, {"title": "2. Model and Learning", "content": ""}, {"title": "2.1. General Framework", "content": "We represent head direction $x \\in [0,2\\pi)$ in a continuous d-dimensional vector $v(x) \\in \\mathbb{R}^d$, which is regarded as responses of putative HD cells and subjects to three constraints:\n(1) Transformation rule: $v(x + dx) = F(v(x), dx)$, where $F$ is a function describing changes in the representation $v(x)$ from a change $d\u00e6$ in direction. The set of transformations {F(\u00b7, dx),\u2200dx} and the set of representations {v(x),\u2200x \u2208 [0,2\u03c0)} together form a representation of the rotation symmetry group U(1), so that $F(v(x),0) = v(x)$, and $F(v(x), dx_1 + dx_2) = F(F(v(x), dx_1), dx_2)$. Here the addition in $x + dx$ is mod 2\u03c0.\n(2) Nonnegativity constraint: $v(x) \\geq 0$, reflecting neurons' nonnegative firing rates.\n(3) Unit norm constraint: $|v(x)|^2 = \\sum_{i=1}^d V_i(x)^2 = 1$ corresponds to a constant total activity of neurons regardless of direction x (to be one without loss of generality). This implies the direction x is only represented by spatial patterns of neuronal responses rather than summed responses, which has been widely used in neural coding (Pouget et al., 2003; Dayan and Abbott, 2005).\nThe transformation rule defines a recurrent neural network $u_t = F(U_{t-1}, dx_t)$ that enables path integration."}, {"title": "2.2. Model for local motion", "content": "For local motion dx, the first order Taylor expansion gives us\n$v(x + dx) = F(v(x),dx) = F(v(x),0) + F'(v(x),0)dx = v(x) + f(v(x))dx$,\nwhere $f(v(x)) = F'(v(x), 0)$ is the derivative of $F(v(x), dx)$ with respect to dx evaluated at dx = 0. This first-order Taylor expansion corresponds to the Lie algebra of the Lie group"}, {"title": "2.2.1. FULLY CONNECTED VERSION", "content": "In the fully connected version, we model local changes in direction as:\n$v(x + dx) = v(x) + Bv(x)dx$\nwhere $B \\in \\mathbb{R}^{d \\times d}$ is a learnable matrix, and $dx \\in [-b, b]$ for a small b > 0. This formulation allows for complex interactions between all dimensions of the representation, capturing potential long-range dependencies in the neural code."}, {"title": "2.2.2. TOPOGRAPHICAL CONVOLUTIONAL VERSION", "content": "In the topographical convolutional version, we place the neurons $v_i$ on a ring, and we model local changes as $v(x + dx) = v(x) + B * v(x)dx$, where B is a learnable convolutional operator, * denotes the 1D convolution operation with periodic boundary condition, and $dx \\in [-b, b]$ for a small b > 0. The convolutional nature of B is expressed as: $(B * v(x))_i = \\sum_{j=-k}^k B_jv_{(i+j) \\mod d}(x)$ where $B_j$ are learnable weights of the convolutional kernel, and k is the kernel size."}, {"title": "2.3. Learning Method", "content": "Our model learns two sets of parameters:\n(1) V: the representations v(x) for all $x \\in {\\frac{k}{n}, k = 0, ..., n \u2212 1}$, where n is the number of grid points. We denote these v(x) collectively as V. For a general continuous x, we express v(x) as a linear interpolation between the two nearest grid points.\n(2) B: the update matrix or kernel B.\nWe define a one-step loss function to train these parameters by minimizing the prediction error of local changes:\n$L(V, B) = \\mathbb{E}_{x,dx} [|v(x + dx) \u2013 F(v(x), dx)|^2]$\nThis loss function focuses on the accuracy of single-step updates, eliminating the need for backpropagation through time, which significantly simplifies the learning process and reduces computational complexity.\nThe above loss function can be minimized by projected gradient descent, i.e., after a gradient descent step or a step of Adam optimizer (Kingma and Ba, 2014), we set all negative elements in each v(x) to 0, and then normalize each v(x) to have norm 1. Expectation $\\mathbb{E}_{x,dx}$ can be approximated by uniformly sampling \u00e6 from [0, 2\u03c0) and d\u00e6 from interval [-b, b]. The detailed training algorithm and details on linear interpolation can be found in Appendix A."}, {"title": "3. Experiments and Results", "content": "We conduct a series of experiments to evaluate the performance and properties of our model across various configurations. We explore dimensions $d \\in {10, 20, 50, 100}$ and local range $b \\in {\\frac{m}{n^2}, m = 2, 5, 10, 20}$ for both the fully connected and convolutional versions of the model. Here we fix n = 100 in all experiments."}, {"title": "3.1. Ring Structure in PCA Plot", "content": "We apply Principal Component Analysis (PCA) to the learned representations v(x) across all directions as in Figure 1(a). The PCA plot of the first two principal components reveal a clear ring structure. This emergent property demonstrates that our model has learned a continuous, circular representation of head direction, mirroring both the topology of the actual direction space and the attractor dynamics observed in biological HD systems.\nThe ring structure is consistent across both model versions, tested dimensions, and local ranges. This result suggests that our high-dimensional representation effectively captures the underlying one-dimensional nature of head direction while providing computational advantages, validating our model's ability to capture essential features of biological head direction systems despite its minimalistic design."}, {"title": "3.2. Gaussian-like Tuning Profiles", "content": "After training, we observe that individual dimensions of the learned representation v(x) exhibit Gaussian-like tuning profiles as in Figure 1(b). Each dimension (or \u201ccell\u201d) in our model responds maximally to a particular head direction and shows a smooth decrease in activity for directions further from its preferred direction. This behavior closely resembles the tuning curves observed in biological HD cells (e.g., McNaughton et al. (2006))."}, {"title": "3.3. Path integration", "content": "We evaluate our model's capability for path integration, a crucial function of biological head direction systems. Path integration involves updating the direction estimate based on a sequence of incremental changes. Despite being trained with a one-step loss function, our model demonstrates remarkable accuracy in multi-step path integration tasks. We test the model's ability to accurately track directional changes over 50 steps and recover the final direction. Our experiments show that the model performs path integration with high accuracy. Detailed procedure and results are provided in Appendix B."}, {"title": "4. Conclusion", "content": "We present a minimalistic representation model for the head direction system that captures essential features of biological HD systems while maintaining computational efficiency. Our model demonstrates that key properties of HD cells, such as Gaussian-like tuning and a ring structure, can emerge from a simple learning framework based on representing and updating directions in a high-dimensional space."}, {"title": "Appendix A. Training Details", "content": ""}, {"title": "A.1. Learning Algorithm", "content": "We use Adam optimizer(Kingma and Ba, 2014) to minimize the loss function. The algorithm proceeds as follows:"}, {"title": "A.2. Continuous Representation and Linear Interpolation", "content": "To achieve a continuous representation, we define v(x) at discrete points $x_k = \\frac{k}{n}2\\pi$ for k = 0, 1, ..., n \u2013 1, and use linear interpolation for intermediate values:\n$v(x) = (1 \u2212 w)v(x_{\\lfloor k \\rfloor}) + wv(x_{\\lceil k \\rceil})$\nwhere $k = \\frac{x}{2\\pi}n$, $w = k \u2212 \\lfloor k \\rfloor$, and $\\lfloor.\\rfloor$, $\\lceil.\\rceil$ denote floor and ceiling functions respectively."}, {"title": "A.3. Second-Order Fully Connected Version", "content": "For larger local motion range b (specifically, $b = \\frac{20}{n}2\\pi$ in our experiments), we employ a second-order model to capture higher-order dynamics:\n$v(x + dx) = v(x) + Bv(x)dx + Cv(x)d^2x$\nwhere $C\\in \\mathbb{R}^{d \\times d}$ is another learnable matrix. This second order term allows the model to better account for changes over larger directional steps."}, {"title": "A.4. Model parameters", "content": "In our training process, we use n = 100 discrete directions. The model was trained for 200,000 epochs with a batch size of 256, using an Adam optimizer (Kingma and Ba, 2014) with an initial learning rate of 4e-5. A learning rate scheduler (ReduceLROnPlateau) is employed with a factor of 0.8 and patience of 5000 epochs to adapt the learning rate during training. For the convolutional model, we use a kernel size of 3. An example training loss curve can be found in Figure 2."}, {"title": "Appendix B. Path Integration", "content": "Path integration is the process of updating a direction estimate based on a sequence of incremental changes. In the context of our direction representation model, we use path integration to track changes in direction over time and recover the final direction (Gao et al., 2021; Xu et al., 2023, 2024)."}, {"title": "B.1. Experiment procedure", "content": "Let $x_o \\in [0, 2\\pi)$ be the initial direction, and (dx1, dx2,...,dxn) be a sequence of directional shifts. Given our direction representation function $v(x) \\in \\mathbb{R}^d$ and the update function F(v,dx), we track the changes in the direction representation as follows: vo = \u03c5(\u03c7\u03bf), vt = F(vt\u22121, dxt), t = 1, ..., n, where vt represents the direction representation after t steps."}, {"title": "B.2. Recovering the Final Direction", "content": "After obtaining the final representation Un, we recover the corresponding direction Xn by maximizing the inner product between un and v(x) over all possible x:\n$Xn = \\underset{x \\in [0,2\\pi)}{arg \\, max} \\, (Un, v(x))$\nThis maximization leverages the property that v(x) should be most similar to un when x is close to the true final direction."}, {"title": "B.3. Evaluation Metrics", "content": "Path integration is evaluated using two local range scenarios: $b = \\frac{2\\pi}{n}$ radians, and $b = \\frac{m}{n^2}2\\pi$ radians, where m is the multiple of the basic angular step size used during training."}, {"title": "Appendix C. Additional Results", "content": "In Figure 1(b), we sampled 4 neurons to show the tuning curves. Here we attach the full tuning curves with d = 100. We observe that all neurons representations in v(x) exhibit Gaussian-like tuning profiles."}]}