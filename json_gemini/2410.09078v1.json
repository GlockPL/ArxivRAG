{"title": "Knowledge-Augmented Reasoning for EUAIA Compliance and Adversarial Robustness of LLMs", "authors": ["Tomas Bueno Momcilovica", "Dian Balta", "Beat Buesser", "Giulio Zizzo", "Mark Purcell"], "abstract": "The EU AI Act (EUAIA) introduces requirements for AI systems which intersect with the processes required to establish adversarial robustness. However, given the ambiguous language of regulation and the dynamicity of adversarial attacks, developers of systems with highly complex models such as LLMs may find their effort to be duplicated without the assurance of having achieved either compliance or robustness. This paper presents a functional architecture that focuses on bridging the two properties, by introducing components with clear reference to their source. Taking the detection layer recommended by the literature, and the reporting layer required by the law, we aim to support developers and auditors with a reasoning layer based on knowledge augmentation (rules, assurance cases, contextual mappings). Our findings demonstrate a novel direction for ensuring LLMs deployed in the EU are both compliant and adversarially robust, which underpin trustworthiness.", "sections": [{"title": "1 Introduction", "content": "The European Union (EU) bases trustworthiness of artificial intelligence systems (AIS) on three properties: lawful, ethical, and robust [7]. The EU AI Act (EUAIA, [3]) is an upcoming regulation that sets obligations on the lawful design and implementation of AIS in the EU. Its content outlines the high-level requirements for improving the auditability of the AIS, whose generic descriptions are interepretable across contexts.\nHowever, for properties such as adversarial robustness, providers of AIS with large language model-based (LLM) components are facing a difficult and highly dynamic challenge whose boundaries are not yet known. Providers in the EU who would like to ensure both compliance and robustness, are doubly burdened. First there is the need to constantly readapt their defenses against novel adversarial attacks [5], and second is the overhead for correctly interpreting \"compliant robustness\" with auditable evidence.\nThis paper presents a novel approach of knowledge augmentation for aligning adversarial robustness of LLMs with EUAIA compliance. By integrating detection, reasoning and reporting layers alongside the layer for interacting with users, we propose a comprehensive functional architecture as a reference for ensuring the AIS is dynamically protected and auditable. The research provides a framework for combining robustness and compliance activities while retaining the provenance to the requirements."}, {"title": "2 Requirements", "content": "The EUAIA places AI systems that are deployed in particular products or domains under categories of risk, where high-risk AI systems play a central role [3]. The regulation which has been adopted in 2024 places duties on stakeholders at design and runtime. Before standards are expected in the following years, these duties provide a basis for safety- and security-oriented requirements.\nGeneral-purpose AI models (GPAI, Art. 3, [3]) such as LLMs are not inherently high-risk. However, their broad capabilities and wide attack surface have over time crystallized similar requirements with respect to adversarial robustness. In examples provided by an ever-increasing body of work (cf. [16]), adversaries of an LLM can include third parties with malicious intentions, curious users who test the boundaries, and even completely benign users whose prompts elicit harmful or otherwise unintended output.\nBased on an analysis of requirements in Table 1, EUAIA compliance and adversarial robustness are complementary properties, despite the difference in details. On the one hand, the requirements that are derived from the regulation (cf. [2] for expanded list) provide a generic description of stakeholders (R0), risk management (R3) and cybersecurity measures, and the need for human oversight (R10) and reporting (R12). On the other hand, the state-of-the-art literature introduces specific roles (R1), the detection of automated (R4), semi-automated (R5), and manual attacks (R15), and sustained coverage of these threats (R7). However, aside from the direct references to the term in EUAIA (R6, R12), the two sources emphasize different facets of a larger system - i.e., components for assuring the quality of"}, {"title": "3 Functional Architecture and Workflow", "sections": [{"title": "3.1 Architecture", "content": "The functional architecture depicted in Figure 1 is composed of a cyclical workflow linking four stakeholders and four layers of components. Stakeholders and components are connected with arrows denoting action IDs (A#), as described in Table 2, whereby each non-functional element of the architecture has a corresponding requirement ID (R#) identified in Table 1.\nThe stakeholders include users, LLM developers, AIS developers and auditors, who represent the various roles involved in the design and implementation of AIS, with the corresponding EUAIA-defined role in parentheses. Auditors and users are external temporary roles, whereby a user can be benign, curious or malicious. Developers are internal and lasting roles, whose responsibilities depend on the access to the internal workings of an LLM and the system deploying it.\nThe layers involve the interaction layer which fulfils the functional requirements of an AIS, and detection, reasoning and reporting layers which fulfill the quality requirements underlying robustness. In other words, the first layer is enough to establish a fully working AIS, without special consideration for other properties. Interaction has a simple structure inspired by practice [11], containing the user-facing application (i.e., the interface between the user and the AIS) and the LLM."}, {"title": "3.2 Workflow", "content": "Detection is based on input and output classification, following the current paradigm of dealing with adversarial attacks [1]. Input detectors have thresholds based on some combination of single and n-pairs of metrics. Metrics denote ways of measuring particular properties of input prompts, examples including perplexity (pp; i.e., the extent to which the model is \"surprised\" by a prompt), context length (cl) and character set size (cs). Output detectors attempt to detect unexpected LLM results which may be results of undetected attacks. They can be implemented similar as for inputs, but also using flags for harmful keywords to provide an early warning to the developer.\nReasoning serves as the middleware between other layers by de-coupling the logic from detection, interaction and reporting activities. The layer provides a set of rules derived using deductive or inductive reasoning, which are behind decisions to classify an input as an attack, an output as an incident, or detector performance as a trigger for change. The library with assurance cases is a set of graphs connecting claims about satisfied requirements relating to compliance and robustness, with the evidence from chosen strategies. Given the adaptability of LLMs and the context-specific properties, context-aware mappings provide the needed metadata to separate the rules and assurance case elements to what they are appropriate. In addition, these mappings enable the variables in reports to be linked with actual values.\nFinally, the reporting layer is primarily based on the EUAIA need for human oversight. Instructions for use and technical documentation are factsheets for users and auditors respectively. However, given the relevance of figures and test results to the monitoring of adversarial robustness, these components are useful to developers for AIS debugging and improvement as well. In addition, assessments based on counterfactuals and anomaly data allow the developers to monitor detectors with respect to needed changes. Incident reports are triggered by an event of a potentially successful attack; although primarily an EUAIA requirement for mandatory auditing of serious incidents, less critical but problematic anomalies provide an opportunity to developers to perform forensic analyses.\nThe Figure 2 depicts three main cyclic processes. The primary cycle is the simplest: a user enters a prompt into the application (A1), which is then forwarded to the input detectors (A2). The detectors' results are provided as input (A3) to a rule that classifies the prompt as safe or unsafe, passing on the prompt or the warning respectively to the LLM (A4). The LLM then generates instead elicits a warning to the user (A4, A5, A6). Relying only on this cycle would be a naive approach to handling adversarial attacks, whereby the developer would expect the detectors to perform well over time and prompts.\nThe secondary cycle introduces the required auditability for EUAIA compliance. The information about the deployed detectors is structured in assurance cases, which feed into the documentation (A7). This documentation provides an interface to the user to understand the model and its limitations before use (A0), and an interface to the auditor (A8) to establish a clear picture about the AIS.\nThis cycle also provides the basis for required dynamicity for adversarial robustness. Assurance cases are intended to provide the logic needed to evaluate detector performance. Given a number of prompts or some other triggering rule (A9), prompts would be processed through non-deployed detector combinations. This would provide the basis for counterfactually assessing the sustained robustness of the detectors (A10). This evaluation is initially be the responsibility of the AIS developer (A11), whose understanding of the context-sensitive performance and coverage would be needed to re-configure the detectors (A12).\n Figure cycles allow the automated monitoring of incidents system-atically. An evaluation of the LLM output (A5), whether in real-time or delayed intervals, allows some successful attacks (i.e., incidents; A13) to be automatically detected. Here is context-specific information necessary to operationalize ambiguous EUAIA language: which risks or anomalies are \"reasonably foreseeable\" (A14) and worth exploring; which incidents are \"serious\" enough (A17) to demand contact with the auditor (A18); and when is a given risk management procedure not \"suitable\" anymore (A12). This cycle also proposes providing relevant information to the LLM developer, who may not be associated with the AIS directly, but nonetheless benefits from adversarially retraining the LLM, thereby making it more secure in the AIS as well."}]}, {"title": "4 Discussion and Conclusion", "content": "This paper introduces a knowledge-augmented framework designed to align the adversarial robustness of large language models with the EU AI Act compliance. By using a combination of detection, reasoning and reporting layers, we address the critical need for compliance and robustness in AI systems.\nThe functional architecture is meant as a reference for implementing physical components. For example, our early prototype implements simple detectors in Python, including n-pair detectors based on logistic regression classifiers pretrained on Hugging Face jailbreak data [8]. The reasoner is based on a combination of an assurance case and an ontology, stored in the graph database, where evaluations are performed through queries. Additionally, graphical visualizations and textual data is generated for Jupyter embeddings to provide clear and informative reporting. The interaction layer uses the streamlit package to provide a user-facing application, while GPT-2, accessed via the Hugging Face package, serves as the foundational LLM.\nOur findings highlight a promising direction for developing resilient AI technologies capable of withstanding adversarial attacks while meeting regulatory standards. Future work will focus on the following aspects: (1) defining new detectors and combinations thereof, such as classifiers trained on larger samples of malicious and benign prompts; (2) expanding the reasoning based on the wider context, including computer language tasks (e.g., code translation); (3) evaluating the components of the architecture with respect to helping developers assure robustness and auditors determine compliance of the LLM-based systems."}]}