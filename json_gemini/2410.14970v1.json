{"title": "Taming the Long Tail in Human Mobility Prediction", "authors": ["Xiaohang Xu", "Renhe Jiang", "Chuang Yang", "Zipei Fan", "Kaoru Sezaki"], "abstract": "With the popularity of location-based services, human mobility prediction plays a key role in enhancing personalized navigation, optimizing recommendation systems, and facilitating urban mobility and planning. This involves predicting a user's next POI (point-of-interest) visit using their past visit history. However, the uneven distribution of visitations over time and space, namely the long-tail problem in spatial distribution, makes it difficult for AI models to predict those POIs that are less visited by humans. In light of this issue, we propose the Long-Tail Adjusted Next POI Prediction (LoTNext) framework for mobility prediction, combining a Long-Tailed Graph Adjustment module to reduce the impact of the long-tailed nodes in the user-POI interaction graph and a novel Long-Tailed Loss Adjustment module to adjust loss by logit score and sample weight adjustment strategy. Also, we employ the auxiliary prediction task to enhance generalization and accuracy. Our experiments with two real-world trajectory datasets demonstrate that LoTNext significantly surpasses existing state-of-the-art works.", "sections": [{"title": "1 Introduction", "content": "Human mobility prediction is essential in various applications, aiming to forecast the next Point of Interest (POI) a user may visit based on their historical location data, preferences, and patterns [10, 8, 53, 33]. By predicting user movements, it supports urban planning, traffic management, and environmental protection, and provides intelligent personalized Location-Based Social Networking (LBSN) services [7, 37], enhancing people's life quality.\nThe growth of POI prediction tasks is closely linked to the development of LBSN platforms, where users frequently share their itineraries and reviews, leading to a substantial accumulation of geographical visitation data. However, data collection faces challenges due to network and privacy constraints on mobile devices and the requirement for user authorization to record check-ins. This often results in data being sparse and biased towards popular locations, exhibiting a severe long-tail effect. Currently, these methods fall into two primary categories: Sequence-based and Graph-based models.\n\u2022 Sequence-based models treat users' trajectories as independent visitation sequences. Existing methods include Recurrent Neural Networks (RNNs) [6, 12, 11], Long Short Term Memory (LSTM) [2, 20, 14, 13] and Gated Recurrent Unit (GRU) [4, 5] for modeling the rich spatial-temporal information implied in the visitation sequence.\n\u2022 Graph-based models focus on building models and data structures to capture trend information in the data to enhance the prediction performance, such as the movement trends among all users [45, 39, 34, 35, 41], geographic adjacency [27, 22, 26], and category transition between POIs [49, 48]. This helps in modeling complex global visitation preferences and the semantic context of locations."}, {"title": "2 Related Work", "content": "Next POI Prediction. Most current works on the next POI prediction treat trajectories as time series, further incorporating spatial-temporal contexts into models to enrich the semantics of POIs. The pioneering ST-RNN [18] introduces spatial-temporal intervals to RNN for context awareness. DeepMove [6] integrates LSTM with attention mechanisms to consider both the short-term and long-term preferences of users comprehensively, and LSTPM [29] enhances spatial context integration. The Flashback model [43] tackles user sparsity by mining similar contexts in historical data. However, due to the limited capability of RNN in modeling long sequences, researchers have explored using graphs for improvements. GETNext [45], based on the Transformer architecture, combines global mobility patterns graph with various spatial-temporal contexts to fully utilize information among similar user trajectories for improving prediction performance. Graph-Flashback [27] considers constructing a knowledge graph to improve POI representation and integrates it with sequence recommendation models. SNPM [46] builds a POI similarity graph to aggregate similar POIs and enhance POI representation results. However, all these studies overlook the significant impact of the long-tail problem on the next POI prediction."}, {"title": "3 Problem Definition", "content": "Given a user set $U = {u_1, u_2, ..., u_{|U|}}$ and a POI set $P = {p_1,p_2, \u2026,p_{|P|}}$, with $|U|$ and $|P|$ indicating the number of users and POIs respectively, we denote the POI check-in as a triplet $(u, p, t)$, which means a user $u$ visits POI $p$ at time $t$. Each POI $p$ is a triplet $p = (lat, lon, freq)$, representing its latitude, longitude, and visit frequency. We proceed to outline our problem definition as follows.\nDefinition 1 (User Next POI Prediction) Given a user check-in sequence denoted as $Q_u= ((p_1,t_1), (p_2,t_2), .., (p_n, t_n))$, our goal is to predict a list of top POIs that the user $u$ is likely to visit next, which can be taken as a typical sequence classification task over $|P|$ POI candidates. In particular, our work focuses on how to accurately predict the \u201cless visited\" POIs belonging to the long-tailed interval."}, {"title": "4 Methodology", "content": "In this section, we introduce the details of the LoTNext framework, as shown in Figure 2, which consists of the preliminary POI prediction model, Long-Tailed Graph Adjustment module, and Long-Tailed Loss Adjustment module."}, {"title": "4.1 Preliminary Model", "content": "We first construct a preliminary end-to-end model, which is designed for precise next POI prediction.\nTrajectory Embedding Layer. For the embedding generation, we initialize embeddings for POIs $E^P \u2208 R^{|P|\u00d7d_p}$, timestamps $E^T \u2208 R^{|T|\u00d7d_t}$, and users $E^U \u2208 R^{|U|\u00d7d_u}$, where $d_p$, $d_t$ and $d_u$ are the corresponding embedding dimension and $|T|$ is the number of the time slots. In our case, considering each hour slot over a week, there are 168 time slots in total. During the sequence processing phase, we select embedding from $E^P$ and $E^T$ based on the POI and time indices in the input sequence $Q_u$ to construct an embedding sequence $X \u2208 [][R^{n\u00d7(d_p+d_t)}$, as $X = [(E^P_1, E^P_2, ..., E^P_n)||(E^T_1, E^T_2, ..., E^T_n)]$, where n is the sequence length and $||$ denotes the concatenation operation.\nTransformer Encoder. Transformer [32] architecture in modeling trajectories has been demonstrated in multiple studies [39, 45, 40], we adopt its encoder block to encode spatial-temporal contexts within trajectories, focusing on capturing long-distance dependencies through multi-layer stacking. To maintain positional information in the sequence, we incorporate a learnable positional embedding $E_{pos} \u2208 R^{nxd_p}$ with the raw embedding sequence $X$ to form the Transformer input $\\tilde{X} = X + E_{pos}$. Next, we define the Transformer encoder block as follows:\n$Z = LayerNorm(\\tilde{X} + Multi-Head Attention(\\tilde{X})), $\n$Z = LayerNorm(Z) + FFN(Z)),$ (1)\nwhere FFN is a fully connected layer and the Multi-Head Attention can be described as:\n$Multi-Head(\\tilde{X}) = [head_1||head_2||...||head_h]W^O,$\n$head_i = Softmax\\left(\\frac{\\tilde{X}W^Q(\\tilde{X}W^K)^T}{\\sqrt{d_k}}\\right)\\tilde{X}W^V,$ (2)\nwhere $\\tilde{X}$ is the input of the i-th head, $W^O$, $W^Q$, $W^K$, and $W^V$ are the learnable weights matrix, h is the number of the head, and $\\sqrt{d_k}$ is the scaling factor.\nSpatial Contextual Attention Layer. Inspired by Flashback [43], we introduce the Spatial Contextual Attention Layer to analyze the relationship between spatial proximity and user interactions. It assigns dynamic weights to POIs in a sequence, taking into account both the order and geographical distances, focusing on POIs most influential to future movements. The spatial weight $w_k$ for each POI $p_k$ in the sequence $(p_1, p_2, \u2026\u2026\u2026, p_k, \u2026\u2026\u2026, p_n)$, considering its spatial distance to all previous POIs $p_1~p_k$, is:\n$w_k = \\frac{\\sum_{j=1}^{k}(e^{-\\beta(\\Delta(p_j,p_k))}+\\epsilon)}{\\sum_{i=1}^{n}\\sum_{j=1}^{k}(e^{-\\beta(\\Delta(p_j,p_k))}+\\epsilon)},$ (3)\nwhere $\\Delta(p_j, p_k)$ is the haversine distance between $p_j$ and $p_k$, $\\beta$ is the distance decay weight, and $\\epsilon$ is a small constant to prevent division by zero. Considering $Z_k$, an element from the Transformer output sequence $Z = (Z_1, Z_2, ..., Z_k, ..., Z_n)$. The refined output $\\tilde{z_k}$ is obtained by applying spatial weight to the cumulative previous outputs, defined as follows:\n$\\tilde{z_k} = \\frac{\\sum_{j=1}^{k}w_jz_j}{\\sum_{j=1}^{k}w_j},$ (4)\nPrediction Layer. To provide personalized predictive outcomes and ensure accurate representation even for users with fewer check-ins, we further introduce user embeddings $E^U$ and fuse it with refined output $\\tilde{Z'}$ to form the input $O = [\\tilde{Z'}||E^U]$ for the final fully connected layer $L = OW + b^P$, where $W \u2208 R^{(d_p+d_u)\u00d7|P|}$ is the weight matrix of the fully connected layer, $b \u2208 R^{|P|}$ is the bias, and $L\u2208 R^{n\u00d7|P|}$ is the logit scores for n steps of POI prediction. As the POI prediction is essentially a sequence classification task, we adopt the standard cross-entropy loss $L_{CE}$ as follows:\n$L_{CE} = -\\frac{1}{N}\\sum_{k=1}^N \\sum_{i=1}^{|P|} y_i^klog(\\frac{exp(l_i^k)}{\\sum_{j=1}^{P} exp(l_j^k)}),$ (5)\nwhere $l_i^k \u2208 R^1$ represents the logit score of the k-th sample for the i-th POI candidate in P, and $y_i^k$ is the ground-truth indicator on POI label i for the k-th sample. In our implementation, we mix the n steps of prediction and B samples in one batch together as N = n \u00d7 B samples in total."}, {"title": "4.2 Long-Tailed Graph Adjustment", "content": "In the next POI prediction task, we model user-POI interactions via a User-POI Interaction Graph $G^{In} = (V^{In}, A^{In})$, where $V^{In} = [E^U ||E^P] \u2208 R^{(|U|+|P|)\u00d7d}$ is the input node feature matrix of the $G^{In}$, $d = d_p = d_u$, and $A^{In} \u2208 R^{|U|\u00d7|P|}$ is the adjacent matrix. It's a bipartite graph where user U and POI P nodes connect through edges symbolizing interaction frequencies or preferences. Graph Neural Networks (GNNs) [38] can leverage graphs to learn complex node representations, but performance hinges on graph quality. However, the $G^{In}$ often has long-tailed distributions\u2014most interactions are limited to few nodes with high visit frequency, which affects the quality of node embeddings and model efficacy. To tackle the long-tail problem in $G^{In}$, we propose a denoising layer to prune and reduce sparse interactions caused by the distribution. This layer evaluates edge importance, retaining only beneficial edges for learning. Initially, an attention layer weights edges according to user-POI embedding interactions, processed by a multilayer perceptron (MLP) to obtain attention scores:\n$A_{ij} = \u03c3(W^B. LeakyReLU(W^A[E^U||E_i^P] + b^A) +b^B).$ (6)\nHere, \u03c3 denotes the sigmoid function, ensuring that the attention scores $A_{ij}$ lie in the (0, 1) interval, $E^U$ and $E_i^P$ means the embedding of user and POI, $W$ represents the trainable weight matrix, and $b$ represents the bias. Based on the attention score $A_{ij}$, the denoising process applies a thresholding operation to filter out edges with scores below a threshold \u03b4, effectively reducing noise and focusing on high-quality interactions. This process aims to derive the denoised graph $\\hat{G}^{In} = (V^{In}, \\hat{A}^{In})$ can be formalized as:\n$\\hat{A}_{ij}^{In} = A_{ij}1[A_{ij} \u2265 \u03b4],$ (7)\nwhere $\\hat{A}^{In}$ denotes the refined edge and $1[\u00b7]$ is the indicator function. The threshold \u03b4 controls the sparsity of the graph, only edges with weights signifying a strong user-POI relationship are retained in $\\hat{G}^{In}$. It is worth noting that when all edges fall below \u03b4, the edge with the highest attention score is retained to prevent isolated nodes in the graph. The model then leverages the Graph Convolutional Network (GCN) [16] layer to learn the node embedding $E^{In}$ of the $\\hat{G}^{In}$, as follows:\n$E^{In} = LeakyReLU ((D^{In})^{-\\frac{1}{2}}\\hat{A}^{In} (D^{In})^{-\\frac{1}{2}}V^{In}W^{In}),$ (8)\nwhere $D^{In}$ is the degree matrix of the $\\hat{A}^{In}$, and $W^{In}$ is the graph convolution weight. It is noted that here we perform a slicing operation $E^{In} = E^{In} [|P| :] $ to select the node embedding representing the POI of $G^{In}$. Beyond merely focusing on direct interactions between users and POIs, we further extend our exploration to utilize all users' check-in data to uncover global mobility patterns among POIs. We build a user-independent directed Global Transition Graph $G^{Tr} = (V^{Tr}, A^{Tr})$, where $V^{Tr} \u2208 R^{|P|\u00d7d_p} and A^{Tr} \u2208 R^{|P|\u00d7|P|}$. Here, $V^{Tr}$ is equal to $E^P$, and $A^{Tr}$ stores the visit frequency between two different POIs. It is important to note that we do not perform a denoising process on the $G^{Tr}$, as it accurately reflects the mobility patterns of all users, containing a wealth of global transition information. Similarly, we employ GCN refer to Equation (8) to learn the node embedding $E^{Tr}$ of the $G^{Tr}$. Finally, we perform mean pooling to combine the two node embeddings $E^{In}$ and $E^{Tr}$, which yields the denoised POI embedding $\\tilde{E^P}= (E^{In} + E^{Tr})$ that incorporate comprehensive user mobility patterns from interaction and transition graphs. To introduce denoised embedding in our model, we refine our input embedding sequence X construction process as $\\tilde{X} = [(\\tilde{E^P}, \\tilde{E^P},..., \\tilde{E^P_{p_n}})||(E^T_1, E^T_2,..., E^T_n)]$."}, {"title": "4.3 Long-Tailed Loss Adjustment", "content": "Logit Score Adjustment. Traditional classification models often mechanically employ the softmax function for outputting predictions, which may lead to an oversight of the potential discrepancies in the posterior distributions between training and testing data. To improve model discrimination, logit adjustment has been explored, which originates in the domain of face recognition [28, 52], It involves modifying the model's output layer (i.e., logits) to encourage the generation of more compact intra-class representations while increasing the distance between classes, thereby augmenting the model's capability to handle long-tailed data.\nTo address the long-tail problem in human next POI prediction tasks, we propose the Logit Score Adjustment module. It adjusts the logits by a factor that is inversely correlated with the frequency of occurrence of each label, effectively dampening the influence of frequently occurring labels and amplifying that of rarer ones. The adjustment factor $\u03b1_i$ for label i with frequency $freq_i$ is given by:\n$\u03b1_i = \u03c4[1 - \\frac{log(freq_i + \\epsilon)}{log(freq_{max} + \\epsilon)}],$ (9)\nwhere $freq_{max}$ is the maximum label frequency observed in the dataset, \u03c4 is the logit adjustment weight and \u03f5 is a small constant to stabilize the logarithm operation. We can adjust final logits $l_i \u2208 R^1$ based on the logits $l_i \u2208 R^1$ as $\\hat{l_i} = l_i + \u03b1_i$.\nSample Weight Adjustment. Based on the Equation (5), for the standard cross-entropy loss, we can find due to the nature of the softmax function, which normalizes the logits the into probabilities, the model can become biased toward head classes. This imbalance means that the model's updates are predominantly driven by the head classes, as the loss from incorrectly classified examples in long-tailed classes contributes insignificantly to the overall loss. Even marginal improvements in the predictions for these long-tailed classes may contribute insignificantly to the overall loss. Therefore, it is necessary to reweight long-tailed samples, like with Focal Loss [17], which reduces the weights of well-classified samples to better focus on minority classes, but it does not explicitly consider the imbalance degree between classes in the long-tailed distribution. Unlike Focal Loss, we propose a novel Long-Tail Adjusted (LTA) loss to adaptively re-weight long-tailed samples. Specifically, for the final prediction layer, we have the hidden inputs of N samples $O = (o^1, o^2, ..., o^N)$ and the weights $W = (w^1, w^2, ..., w^{|P|})$ for |P| candidates, where $o^k \u2208 R^{(d_u+d_p)}$ is from the k-th sample. The true class label for the k-th sample is denoted by $y_k$. We can take $w^{y_k} \u2208 R^{(d_u+d_p)}$ as the class \u201ccenter\u201d for the class to which the k-th sample truly belongs. Then we assess the impact posed by the k-th sample to the overall prediction through the cosine similarity between $o^k$ and $w^{y_k}$ as follows:\n$cos(o^k, w^{y_k}) = \\frac{o^k. w^{y_k}}{|| o^k |||| w^{y_k} ||}$ (10)\nBased on these cosine similarities, we compute the adjusted vector magnitude $\u03c2^k$ for each sample as:\n$\u03c2^k = \\begin{cases}\n1, & cos(o^k, w^{y_k}) \u2264 0, \\\\\n1-cos(o^k, w^{y_k}), & cos(o^k, w^{y_k}) > 0.\n\\end{cases}$ (11)\nThen we determine the geometric mean of the vector magnitude to serve as a baseline magnitude \u03be. The traditional definition of the geometric mean of the vector magnitudes is the N-th root of their product \u03be = $\u03c2_1 \u03c2_2 . . . \u03c2_N$. However, it can be problematic in practice due to numerical underflow or overflow when dealing with very small or very large values. To mitigate this issue, we utilize logarithm to turn the product into a sum, making the calculation more numerically stable, as follows:\n$\u03be = exp(\\frac{\\sum_{i=1}^{N}log(\u03c2^i)}{N})$ (12)\nWe calculate adaptive weights $\u03d5^k$ for each sample using the deviation of vector magnitude from the geometric mean:\n$\u03d5^k = \\begin{cases}\n1, & \u03c2^k \u2013 \u03be \u2264 0, \\\\\n1 + \\frac{\u03c2^k \u2013 \u03be}{\u03be}, & \u03c2^k \u2013 \u03be > 0.\n\\end{cases}$ (13)\nFinally, the overall Long-Tail Adjusted loss $L_{LTA}$ can be formulated as:\n$L_{LTA} = -\\frac{1}{N}\\sum_{k=1}^N \u03d5^k \\sum_{i=1}^{|P|} y_i^k log(\\frac{exp(\\hat{l^k_i})}{\\sum_{j=1}^{|P|} exp(\\hat{l^k_j})}).$ (14)\nBy combining the Logit Score Adjustment and the Sample Weight Adjustment, we present a nuanced approach to recalibrating the model's focus across the spectrum of label frequencies. It ensures that each sample contributes to the model's learning process in proportion to its significance, as dictated by the distributional characteristics of the dataset and the discriminative capacity of the model."}, {"title": "4.4 Model Optimization", "content": "Building upon our Long-Tailed Loss Adjustment module, we further embrace auxiliary prediction tasks to optimize LoTNext. To incorporate these tasks, we define a joint loss function that combines three distinct loss components: the standard cross-entropy loss ($L_{CE}$), the Long-Tail Adjusted Loss ($L_{LTA}$), and the Mean Squared Error loss for auxiliary time prediction ($L_{Aux}$). Each component serves a critical role: $L_{CE}$ ensures the fidelity of the next POI prediction, $L_{LTA}$ addresses the long-tailed data imbalance through adaptive weighting, and $L_{Aux}$ measures the accuracy of the timing predictions, an auxiliary task that supports the model by providing it with temporal context, thereby improving prediction accuracy and robustness, which can be denoted as:\n$L_{Aux} = \\frac{1}{N}\\sum_{k=1}^{N} ||t_k - \\hat{t_k}||^2,$ (15)\nwhere $t_k$ is the forecasted time slot of k-th candidate POI and $\\hat{t_k}$ is the ground truth time slot. The overall loss function is constructed as a weighted sum of these components, with the weights \u03bb being learnable parameters, as follows:\n$L_{Joint} = \u03bb_1L_{CE} + \u03bb_2L_{LTA} + \u03bb_3L_{Aux}.$ (16)"}, {"title": "5 Experiments", "content": "Datasets & Baselines. We evaluate our LoTNext on two publicly available real-world LBSN datasets: Gowalla and Foursquare2 Each user check-in record includes the User ID, POI ID, latitude, longitude, and timestamp. To focus solely on the impact of long-tailed POIs and ensure the dataset's quality, we filter out inactive users with fewer than 100 check-ins. We then split each user's check-in records according to temporal order, using the first 80% for training and the remaining 20% for testing. To batch training, we uniformly segment the length of each input trajectory (e.g., 20). The specific statistical results are shown in Table 1, where we additionally calculated the percentage of POIs with a frequency smaller than 200 times and smaller than 100 times out of the total number of POIs. For instance, defining long-tailed POIs as those with a frequency of less than 100 times, approximately 98.38% of POIs could be considered long-tailed POIs. Considering both Table 1 and Table 2, the reason why the model performs about 20% points better on Foursquare compared to Gowalla is due to the more severe long-tail effect on the Gowalla dataset, along with a sparser density of the dataset.\nTo demonstrate the performance of the LoTNext, we implement the following 10 state-of-the-art methods as the comparison baselines:\n\u2022 ST-RNN [18] extends the RNN by introducing the spatial and temporal transition matrices.\n\u2022 DeepMove [6] considers long-term and short-term interests of users by attention mechanism.\n\u2022 LBSN2Vec [42] introduces the hypergraph and calculates the similarity of users and time embeddings to rank POIs.\n\u2022 LightGCN [9] simplifies the structure of Graph Convolutional Network (GCN) to learn user preferences for POIs.\n\u2022 LSTPM [29] proposes geo-nonlocal LSTM to further extend DeepMove structure.\n\u2022 Flashback [43] searches the most similar hidden states in historical information based on the current context information and updates the model.\n\u2022 STAN [23] explores the influence between non-adjacent check-in records in trajectory sequences through the attention mechanism.\n\u2022 GETNext [45] introduces the global mobility patterns of all users into the Transformer architecture to improve model prediction effects.\n\u2022 Graph-Flashback [27] combines Spatial-Temporal Knowledge Graph with the sequential model to enrich the representation of each POI.\n\u2022 SNPM [46] learns the general characteristics of POIs by constructing a POI similarity graph and aggregating similar POIs.\nMetrics. To evaluate the model performance, we utilize two of the most common metrics for the next POI prediction: Accuracy@k(Acc@k) and Mean Reciprocal Rank (MRR). Acc@keffectively measures whether the true label is present within the top-k predicted results. Here, we consider k=1, 5, and 10 to comprehensively assess the model's performance. MRR directly quantifies the average rank of the correct label among all predictions when the correct label is not within the top-k predictions, with higher values indicating better average prediction performance by the model.\nSettings. We implement LoTNext using PyTorch 1.13.1 on a Linux server equipped with 384GB RAM, 10-core Intel(R) Xeon(R) Silver 4210R CPU @ 2.40GHz, and Nvidia RTX 3090 GPUs. The embedding dimensions for POIs and users are set to 10, and the time embedding dimension is set to 6. For the Transformer architecture, we incorporate two multi-head attention mechanisms and 2 encoder blocks. For the spatial decay rate \u03b2, we follow the settings of Flashback [43].\nOverall Performance. Table 2 shows the predictive performance of all baseline methods and LoTNext on two datasets. Based on Table 2, we can draw the following conclusions:\n\u2022 On both public datasets, LoTNext outperforms all other state-of-the-art baseline methods across all metrics. Compared to the most recent and best-performing baseline method, SNPM, LoTNext achieves more significant improvements in Acc@1. These results indicate that LoTNext is better at predicting long-tailed POIs that are less popular but highly relevant to specific users.\n\u2022 Utilizing graphs to model all user mobility patterns, thereby improving POI embeddings, representing as SNPM, Graph-Flashback, and GETNext, significantly outperform sequential methods represented by LSTPM and DeepMove, which rely solely on an individual user's short-term and long-term interests to predict the user's next location. However, the raw User-POI Interaction Graph has a large number of long-tailed nodes with a degree of 1 or very small. LoTNext, through its long-tailed graph adjustment module, effectively filters these long-tailed nodes, thereby enhancing the model's predictive performance.\nPerformance on Long-Tailed Samples. To evaluate whether our model achieves accuracy improvement on long-tailed samples, we define samples with a frequency less than 100 on Gowalla dataset"}, {"title": "A.1 Notations", "content": "The notations used in our paper are summarized as follows."}, {"title": "A.2 Computational Cost", "content": "In this section, we explore the computational cost of LoTNext. We selected three sequence-based and three graph-based baselines to demonstrate the computational efficiency of our approach. Table 5 lists the inference time for each deep learning model during the testing phase (running one training/testing instance, i.e., test time divided by batch size). We ensured that all models were executed on the same RTX 3090 GPU. Surprisingly, due to batch training, the graph-based methods generally run significantly faster than the sequence-based methods. DeepMove is the fastest among the sequence-based methods, as it only considers calculating attention using historical trajectories. Compared to DeepMove, LSTPM further introduces a geographical relationship adjacency matrix to enrich the spatial context, making it slightly slower than DeepMove. STAN employs a dual-layer attention architecture, with one attention layer aggregating spatiotemporal correlations within user trajectories and the other selecting the most likely next POI based on weighted check-ins, resulting in the longest inference time for STAN."}, {"title": "A.3 Hyperparameter Analysis", "content": "We conduct hyperparameter sensitivity experiments on the Long-Tailed Graph Adjustment module's threshold \u03b4 and the weight 7 of the logit adjustment module to identify the optimal parameter values on Gowalla and Foursquare datasets. We first experiment with a range of thresholds \u03b4 from 0.1 to 0.9 in increments of 0.2, which controls the sensitivity of the model to the long-tailed distribution by filtering less significant edges in the graph. The results, shown in Figure 6(a) for Gowalla and Figure 6(c) for Foursquare, indicate that Acc@1 and MRR remain stable across different values, with the optimal threshold identified as d = 0.5. Next, we vary the logit adjustment weight 7 from 1 to 2 in increments of 0.2 to test the model's performance in balancing class imbalances. Figure 6(b) and Figure 6(d) reveal that T = 1.2 yields the best results on both datasets, suggesting a moderate adjustment weight helps generalize better without overly amplifying rare classes. These consistent findings across both datasets underscore the robustness of d = 0.5 and r = 1.2, highlighting the importance of hyperparameter tuning in improving model accuracy and ranking metrics for better prediction of user behavior in diverse datasets."}, {"title": "A.4 Model Training Pseudo-code", "content": "Algorithm 1 shows the pseudo-code of the LoTNext training process. In our experiments, all training instances are processed through mini-batches."}]}