{"title": "Structure learning with Temporal Gaussian Mixture for model-based Reinforcement Learning", "authors": ["Th\u00e9ophile Champion", "Marek Grze\u015b", "Howard Bowman"], "abstract": "Model-based reinforcement learning refers to a set of approaches capable of sample-efficient decision making, which create an explicit model of the environment. This model can subsequently be used for learning optimal policies. In this paper, we propose a temporal Gaussian Mixture Model composed of a perception model and a transition model. The perception model extracts discrete (latent) states from continuous observations using a variational Gaussian mixture likelihood. Importantly, our model constantly monitors the collected data searching for new Gaussian components, i.e., the perception model performs a form of structure learning (Smith et al., 2020; Friston et al., 2018; Neacsu et al., 2022) as it learns the number of Gaussian components in the mixture. Additionally, the transition model learns the temporal transition between consecutive time steps by taking advantage of the Dirichlet-categorical conjugacy. Both the perception and transition models are able to forget part of the data points, while integrating the information they provide within the prior, which ensure fast variational inference. Finally, decision making is performed with a variant of Q-learning which is able to learn Q-values from beliefs over states. Empirically, we have demonstrated the model's ability to learn the structure of several mazes: the model discovered the number of states and the transition probabilities between these states. Moreover, using its learned Q-values, the agent was able to successfully navigate from the starting position to the maze's exit.", "sections": [{"title": "1. Introduction", "content": "Model-based reinforcement learning is a theory describing how an agent should interact with its environment. More precisely, the agent maintains a model of its environment, which is composed of observations, states, and actions. When new observations become available, the agent needs to estimate the most likely states of the environment. This process is generally referred to as perception or inference, and can be implemented by minimizing the variational free energy, also known as the negative evidence lower bound in machine learning (Blei et al., 2017).\nVariational inference is a form of approximate inference, where the true posterior is approximated by the variational distribution. Specifically, inference is generally rendered tractable by restricting the approximate posterior to a class of distributions, where each distribution corresponds to a different set of parameter values. Inference then refers to the optimization of these parameters such that the variational free energy is minimized.\nAn important family of distributions frequently used in variational inference is the exponential family (Holland and Leinhardt, 1981), which contains most of the well-known distributions. For example, the Gaussian, Bernoulli, and Dirichlet distributions are all members of this family. Importantly, all the distributions within the exponential family can be mapped to the same functional form, which can be used to derive modular inference algorithms such as variational message passing (Winn and Bishop, 2005).\nIn these previous approaches, the generative model's structure is known in advance, i.e., the number of latent states is known by the agent. In practice, this means that the model needs to be designed by a domain expert and is not learned by the agent. However, for some applications, the model may not be available, or experts may only have a partial understanding of the problem. In this case, learning the model becomes essential. Recent work has focused on problems where observations are discrete (Smith et al., 2020; Friston et al., 2018; Neacsu et al., 2022). In contrast, this paper addresses the case where observations are continuous.\nThroughout this paper, we assume that the data is distributed according to a mixture of multivariate Gaussian distributions. Then, each component of the mixture is associated with a latent state, and we aim to learn how these states change over time. The key challenge is to identify the correct number of components. This requires the model to add and remove components when necessary. While the Gaussian Mixture Model (GMM) is able to prune superfluous components, we also need a way to increase the number of components as new clusters of data points are discovered. To this end, our agent constantly monitors the data for new clusters of data points, and new Gaussian components are added to the mixtures when such clusters are identified.\nWhile learning the model structure and inferring the current state using variational inference is useful, it does not prescribe which actions should be taken. Thus, the next step consists of comparing the quality of different policies, where in reinforcement learning, the quality of a policy is the discounted sum of rewards. Ideally, we would like to compare all possible policies, and select the policy with the highest quality. Unfortunately, the number of valid policies grows exponentially with the time horizon of planning. Specifically, if the agent can choose one of A actions, for T time steps, then the number of possible policies is: $A^T$.\nThis exponential explosion renders an exhaustive search intractable. Instead, algorithms such as Monte Carlo tree search (Browne et al., 2012) can be used to efficiently explore the space of valid policies (Champion et al., 2022a,b,c,d). An alternative approach is the Q-learning algorithm (Mnih et al., 2015), where instead of searching the space of policies using a search tree, the agent learns the value of performing each action in each state. However, Q-learning requires states to be observable. Therefore, we propose a variant of the Q-learning algorithm which can learn from beliefs over states.\nIn section 2, we present the variational Gaussian mixture model, which is used to learn the number of components and perform inference of the latent variables. Section 3 explains how the temporal transition can be learned by taking advantage of a categorical-Dirichlet model. Then, in Section 4, we describe an optimization which allows the agent to forget part of the data points by integrating the information they provided into the prior. By forgetting part of the data, the agent reduces the amount of memory required, while speeding up the inference process. Next, in Section 5, we describe how Q-learning can be adapted to work with beliefs over states. Finally, in Section 6 we empirically validate our approach, before summarizing and concluding the paper in Section 7."}, {"title": "2. Perception Model", "content": "In this section, we discuss the theory and implementation details of our perception model. Specifically, we describe how mean shift clustering can be used to initialize the parameters of a variational Gaussian mixture (VGM) model. Then, we describe the VGM generative model, variational distribution and update equations."}, {"title": "2.1 Mean Shift Clustering", "content": "Given a dataset X = {x1, ..., XN}, the mean shift algorithm (Carreira-Perpin\u00e1n, 2015) aims to find clusters of data points. While the mean shift algorithm does not require the number of clusters to be known, the user must specify the bandwidth parameter 06, which defines the window's radius (see below). For each data point an, the mean shift algorithm initializes a spherical window of radius \u03b8 centred at an, then the window's centroid mn is set to the mean of the data points within the window, i.e.,\n$m_n = \\frac{1}{|W_{\\theta_b}|} \\sum_{x_n \\in W_{\\theta_b}} x_n$\nwhere Wer is the set of data point indices within the window of radius \u03b8\u0463, and |Wer | corresponds to the number of data points in this window. The above update equation is iterated until convergence of the window's centroid position, at which point the window is centred on a part of the space with high density. The points whose window's centroid land close to each other are then grouped within the same cluster. This procedure will output K clusters, we let \u00eenk be one if the n-th data point is associated to the k-th cluster, and zero otherwise. Figure 1 illustrates the mean shift algorithm."}, {"title": "2.2 Variational Gaussian mixture", "content": "While the mean-shift algorithm can identify clusters of data points, this algorithm does not scale well with the number of data points, i.e., for each additional data point Equation (1) must be iterated until convergence. Thus, we need to find a way to discard data points without losing the information they contain. As will become apparent later, this can be achieved using a variational Gaussian mixture (VGM) model (Bishop and Nasrabadi, 2006). In the following sub-sections, we discuss the generative model, variational distribution, variational free energy and update equations of the variational Gaussian mixture."}, {"title": "2.2.1 GENERATIVE MODEL", "content": "The generative model assumes that each data point \u00e6n is sampled from one of K Gaussian components. For each data point, znk is a binary random variable equal to one if the k-th component is responsible for the n-th data point, and zero otherwise. We denote by zn the one-of-K (binary) random vector (i.e., a vector containing binary random variables) describing which component is responsible for the n-th data point, and let Z = {1, ..., zv} be the set of random vectors associated with all the data points.\nAdditionally, each Gaussian component is parameterized by its mean vector \u03bc\u03b5 and precision matrix Ak. For conciseness, the set of all mean vectors and precision matrices are denoted by \u03bc and A, respectively. Using the aforementioned notation, the probability of the data (i.e., X), given the latent variables (i.e., Z) and the component parameters (i.e., \u03bc and A) is:\n$P(X|Z, \\mu, \\Lambda) = \\prod_{n=1}^{N} \\prod_{k=1}^{K} N(x_n; \\mu_k, \\Lambda_k^{-1})^{z_{nk}}.$\nMoreover, the prior over latent variables is defined as a categorical distribution parameterized by the mixing coefficients D, where Dk is the fraction of the data for which the k-th component is responsible, i.e., the larger Dk is, the more prevalent the k-th component will be. More formally:\n$P(Z|D) = \\prod_{n=1}^{N} Cat(z_n; D).$\nFinally, the VGM model takes advantage of conjugate priors by defining the prior over D, \u03bc and A as a Dirichlet, Gaussian and Wishart distributions, respectively. More precisely:\n$P(\\mu|A) = \\prod_{k=1}^{K} N(\\mu_k; m_k, (\\beta_k \\Lambda_k)^{-1}),$\n$P(A) = \\prod_{k=1}^{K} W(\\Lambda_k; W_k, v_k),$\n$P(D) = Dir(D; d),$\nwhere d is a vector containing concentration parameters of the prior over D, Wk and vk are the scale matrix and degrees of freedom of the Wishart distribution over Ak, mk is the mean vector of the prior over \u03bck, and finally, \u1e9ek is a parameter scaling the values of the precision matrix Ak. Putting together all the factors defined in Equations (2) to (6), we see that the generative model factorizes as follows:\n$P(X, Z, \\mu, \\Lambda, D) = P(Z|D)P(D)P(\\mu|\\Lambda)P(\\Lambda)P(X|Z, \\mu, \\Lambda),$\nwhich corresponds to the graphical model of Figure 2. Note, the VGM is parameterized by: d, Wk, Uk, \u1e9ek, and mk, but how should we choose these parameters? Experimentally, we set the parameters dk and \u1e9ek to two times the number of states, i.e., 2K. Additionally, we initialized vk to 2K+O-0.99, where O is the dimensionality of the observations, i.e., xn \u2208 RO. Moreover, mk was set to the mean of the data points associated to the k-th component by the mean shift algorithm. Finally, according to Appendix B of (Bishop and Nasrabadi, 2006), the expected precision matrix under a Wishart distribution P(A) is Wkuk, thus we let:\n$W_k = \\frac{A_k}{v_k}$"}, {"title": "2.2.2 EMPIRICAL PRIOR AND VARIATIONAL DISTRIBUTION", "content": "Now that the generative model has been laid out, we would like to take into account the information provided by the available data points. As mentioned previously, we want to forget part of the data points as time passes, thus, we assume that the dataset X = {x1,...,xN} has been split into the data points to forget X' = {x\u2081 | i \u2208 N'} and data points to keep X\" = {xj|j \u2208 N\"}, where N' and N\" are the indices of the data points to forget and keep, respectively.\nFor now, we assume that N' and N\" are given, and the process of constructing N' and N\" will be detailed in Section 4. Since we want to forget the data points indexed by X', we compute an empirical prior based on these data points alone. The empirical prior is a posterior distribution conditioned on the data points indexed by X'. Critically, because of conjugacy, the empirical prior has the same functional form as the prior, and it can be used as a prior when additional data points will become available.\nAs computing the exact posterior is intractable, we turn to the variational inference framework (Fox and Roberts, 2012). More precisely, we approximate the true posterior using a structured mean-field approximation:\n$P(Z', \\mu, \\Lambda, D|X') \\approx E(Z', \\mu, \\Lambda, D) = E(Z')E(D) \\prod_{k=1}^{K} E(\\mu_k|\\Lambda_k)E(\\Lambda_k),$\nwhere Z' are the latent variables corresponding to the data points to forget, and to ensure conjugacy, the individual factors are defined as follows:\n$E(\\mu,\\Lambda) = \\prod_{k=1}^{K} N(\\mu_k; \\bar{m}_k, (\\beta_k\\bar{\\Lambda}_k)^{-1}),$\n$E(A) = \\prod_{k=1}^{K} W(\\Lambda_k; \\bar{W}_k, \\bar{v}_k),$\n$E(Z') = \\prod_{n \\in N'} Cat(z_n; \\bar{\\pi}_{n.}),$\n$E(D) = Dir(D; \\bar{d}).$\nNote, the empirical prior is parameterized by: \u00eenk, d, Wk, Uk, Bk and mk. Similarly, we want to compute posterior beliefs using all the available data X. Once again, we approximate the true posterior using a structured mean-field approximation:\n$P(Z, \\mu, \\Lambda, D|X) \\approx Q(Z, D, \\mu, A) \\equiv Q(Z)Q(D) \\prod_{k=1}^{K} Q(\\mu_k|\\Lambda_k)Q(\\Lambda_k)$\nwhere to ensure conjugacy, the individual factors are defined as follows:\n$Q(\\mu,\\Lambda) = \\prod_{k=1}^{K} N(\\mu_k; \\hat{m}_k, (\\beta_k\\hat{\\Lambda}_k)^{-1}),$\n$Q(A) = \\prod_{k=1}^{K} W(\\Lambda_k; \\hat{W}_k, \\hat{v}_k),$\n$Q(Z) = \\prod_{n=1}^{N} Cat(z_n; \\hat{\\pi}_{n.}),$\n$Q(D) = Dir(D; \\hat{d}).$\nNote, the variational distribution is parameterized by: \u00eenk, d, \u0174k, \u00fbk, \u03b2\u03ba and mk. Note, the posterior parameters can be identified by the hat over each symbol, while the empirical prior parameters have a bar on top of each symbol. Also, the responsibilities \u00eenk are identical in the empirical prior and posterior distribution, because they cannot be used in the generative model. Instead, d will replace d, which will update the prior over D and indirectly the prior over states P(Z|D). Importantly, the inference process will rely on iteratively updating the parameters of the empirical prior and variational distribution until convergence of the variational free energy."}, {"title": "2.2.3 VARIATIONAL FREE ENERGY", "content": "The goal of variational inference is to make the variational distribution as close as possible to the true posterior. Mathematically speaking, we want:\n$Q^*(Z, D, \\mu, \\Lambda) = arg \\min_{Q(Z,D,\\mu,\\Lambda)} D_{KL} [Q(Z, D, \\mu, \\Lambda)|| P(Z, D, \\mu, \\Lambda|X)] .$\nHowever, we do not know the true posterior, therefore, Bayes theorem is used to re-arrange the above expression as follows:\n$Q^*(Z, D, \\mu, \\Lambda) = arg \\min_{Q(Z,D,\\mu,A)} D_{KL} [Q(Z, D, \\mu, \\Lambda)|| P(X, Z, D, \\mu, \\Lambda)] + ln P(X)$\n$= arg \\min_{Q(Z,D,\\mu,A)} D_{KL} [Q(Z, D, \\mu, \\Lambda)|| P(X, Z, D, \\mu, \\Lambda)],$\nwhere the In P(X) term can be dropped, since its is a constant w.r.t. Q(Z, D, \u03bc, \u039b). The variational free energy is defined as the KL-divergence between the variational distribution and the generative model, and can be re-arranged as follow using Equation (7) and (14):\n$F = D_{KL} [ Q(Z, D, \\mu, \\Lambda)|| P(X, Z, D, \\mu, \\Lambda)]$\n$= E_{Q(D)} [ln Q(D)] + E_{Q(\\mu,A)} [lnQ(\\mu|A)] + E_{Q(x)} [lnQ(A)] + E_{Q(z)} [lnQ(Z)]$\n$\u2013 E_{Q(D)} [ln P(D)] \u2013 E_{Q(\\mu,A)}[ln P(\\mu|\\Lambda)] \u2013 E_{Q(x)} [ln P(A)] \u2013 E_{Q(z,D)}[ln P(Z|D)]$\n$\u2013 E_{(Z,\\mu,\\Lambda)} [ln P(X|Z, \\mu, \\Lambda)],$\nwhere the expectations are computed as described in Appendix B."}, {"title": "2.2.4 UPDATE EQUATIONS", "content": "Let Y = {Z, D, \u03bc, \u039b} be the set of all latent variables, and Y; be an arbitrary random variable. As explained by Winn and Bishop (2005) and Champion et al. (2021), the variational free energy can be minimized by iteratively applying the following update equation for each latent variable Yj:\n$ln Q^* (Y_j) = E_{i\\neq j}[ln P(X, Y)] + c,$\nwhere c is a constant, and the expectation is over all factors of Q(\u2022) but Q(Y). As shown in Appendix C, the update equations for the variational posterior of the VGM can be obtained from (23). Additionally, as explained in Appendix D, these equations can be re-arranged to provide update equations for both the empirical prior and variational distribution parameters. Interestingly, the final set of equations suggest that the empirical prior parameters should be updated as if we wanted to compute the posterior distribution given the data points to forget X', and the prior parameters. Similarly, the variational posterior parameters need to be updated as if we wanted to compute the posterior distribution given the data points to keep X\", and the empirical prior parameters. More precisely, the empirical prior parameters dk, \u016ak, and Bk should be updated as follows:\n$v_k = v_k + N_k^f$\n$\\beta_k = \\beta_k + N_k^f$\n$\\bar{d}_k = d_k + N_k^f,$\nwhere Nk is the number of data points to forget attributed to the k-th component:\n$N_k^f = \\sum_{n \\in N'} \\bar{\\pi}_{nk}.$\nSimilarly, the posterior parameters dk, \u00fbk, and \u1e9ek are obtained by simply adding N\" to the associated empirical prior parameters dk, \u016ak, and \u1e9ek:\n$\\hat{v}_k = v_k + N_k^r,$\n$\\hat{\\beta}_k = \\beta_k + N_k^r,$\n$\\hat{d}_k = d_k + N_k^r$\""}, {"title": "3. Transition Model", "content": "In this section, we explain how to learn the transition mapping between consecutive time steps using a categorical-Dirichlet model. When combined with the perception model from the last section, the resulting model is a temporal Gaussian mixture (TGM) that is able to learn the environment's structure. That is, a model which can learn the number of states and the transition probabilities between states given the action taken."}, {"title": "3.1 Temporal Gaussian mixture", "content": "While the variational Gaussian mixture (from Section 2.2) is able to learn a static model of data, it cannot model time series. A potential solution would be to duplicate the Gaussian mixture likelihood at each time step, and to define the transition probability between any two consecutive time steps, c.f., Figure 5(a). Unfortunately, as depicted in Figure 6(a), adding such transition mapping interferes with the VGM's ability to learn one component for each cluster of data points. Put simply, we empirically found that learning the likelihood first, and using the posterior over latent variables to learn the transition mapping, works better than learning the likelihood and transition mappings jointly.\nTherefore, we use a second generative model for the transition mapping. See Figure 5(b) for an illustration of the model's structure. Note, the parameters of the variational distribution over Zo and Z\u2081 are computed by iterating over the update equations presented in Section 2.2.4, and these parameters remain fixed during the inference process of the transition model.\nMore precisely, when the agent acts in the environment, it records the observations made X, and the actions performed A. Then, for each observation\u00b9 xn \u2208 X, the VGM is used to compute the optimal variational distribution over the corresponding latent variable zn. However, the transition model relies on two observed variables Xo and X1, i.e., it does not depend on X. The trick is to consider any two consecutive observations Xn, Xn+1 \u2208 X, as observations for Xo and X1, respectively. Consequently, the parameters of the variational distribution over Zn, Zn+1 \u2208 Z can be used to define"}, {"title": "3.1.1 GENERATIVE MODEL", "content": "We now focus on the transition model definition. Like in Section 2.2.1, the prior over D is a Dirichlet distribution, and the prior over Zo is a Categorical distribution parameterized by D:\n$P(D) = Dir(D; d),$\n$P(Z_0|D) = \\prod_{n=1}^{M} Cat(z_n^0|D),$\nwhere M is the number of data points available to train the transition model. Perhaps unsurprisingly, the transition mapping is modelled as a Categorical distribution parameterized by a 3d-tensor B, where the prior over B is a Dirichlet distribution:\n$P(B) = Dir(B;b),$\n$P(Z_1|Z_0, A_0, B) = \\prod_{n=1}^{M} Cat(z_n^1|z_n^0, a_n, B).$\nFinally, merging the above four equations, leads to the full generative model for the transition model:\n$P(Z_{0:1}, B, D|A_0) = P(Z_0|D)P(D)P(Z_1|Z_0, A_0, B)P(B),$\nwhich corresponds to the Bayesian network depicted in Figure 5(b)."}, {"title": "3.1.2 EMPIRICAL PRIOR, VARIATIONAL DISTRIBUTION AND UPDATE EQUATIONS", "content": "Since we want to forget the data points indexed by M' and keep the data points indexed by M\", we introduced an empirical prior like in Section 2.2.2. Once inference has been performed, the parameters of the empirical prior will be used as prior parameters, effectively integrating the information provided by the data points to forget into the prior distribution. After this replacement occurs, the data points indexed by M' can be safely discarded. More precisely, the empirical prior is defined as follows:\n$P(Z_{0:1}, B, D|A_0) \\approx E(Z_{0:1}, B, D) \\approx E(B)E(D)E(Z_0)E(Z_1),$\nwhere the individual factors are defined as follows:\n$E(Z_0) = \\prod_{n \\in M'} Cat(z_n; \\bar{\\pi}_n.)$\n$E(B) = Dir(B;\\bar{b}),$\n$E(D) = Dir(D; \\bar{d}).$\nAdditionally, we approximate the full posterior by a variational distribution as follows:\n$P(Z_{0:1}, B, D|A_0) \\approx Q(Z_{0:1}, B, D) \\equiv Q(B)Q(D)Q(Z_0)Q(Z_1),$"}, {"title": "4. To forget or not to forget", "content": "In the previous sections, we described the perception and transition models. Importantly, we explained that each model is provided with two sets of data points, i.e., the data points that can be forgotten and the data points that need to be kept in memory. The forgettable data points are used to compute empirical priors, i.e., posterior distributions taking into account only the data points to forget. Importantly, empirical priors have the same functional form as the prior distributions and can therefore be used as prior beliefs when the inference process is over, effectively integrating the information provided by the forgettable data points into the prior. We now focus on how to decide which data points to forget and keep."}, {"title": "4.1 Plasticity vs stability dilemma", "content": "When learning, the brain needs to be plastic by allowing changes in synaptic connectivity, as well as in the strength of existing synaptic connections. However, these changes should ideally preserve the previously learned concepts to ensure some form of stability over time, and avoid forgetting useful information. This is the plasticity versus stability dilemma (Mermillod et al., 2013).\nOur model is faced with a similar challenge, as in general, new components are progressively discovered as the agent explores the environment. These new components need to be learned without forgetting older components. For example, imagine a mouse moving through a maze and observing a noisy estimate of its position. As new maze cells are explored, new positions are sampled and new components will become visible. However, if the number of samples for a new component is too small, this component may be confused with nearby cells, until enough data points are available to differentiate between the cells."}, {"title": "4.2 Flexible and fixed components", "content": "In this section, we discuss how our model addresses the plasticity vs stability dilemma. More precisely, we assume that the Gaussian components can be either flexible (plastic) or fixed (stable). All components are initialized flexibly, and they become fixed if they persist over long periods of time. Figure 8 illustrates how components transform into fixed components.\nMathematically, every 100 training iterations, the variational Gaussian mixture identifies a set of Gaussian components Gt, where t \u2208 {100, 200, ...}. Given two Gaussian components with distributions \u039d(\u03a7; \u03bc1, \u039b\u2081) \u2208 Gt and N(\u03a7; \u03bc2, A2) \u2208 Gt+100, we say that N(\u03a7; \u03bc1, \u039b\u2081) at time t becomes \u039d(\u03a7; \u03bc2, A2) at time t + 100, if:\n$D_{KL} [N(X; \\mu_1, \\Lambda_1)||N(X; \\mu_2, \\Lambda_2)] < \\theta_{kl},$\nwhere Okl is the threshold under which two Gaussian components are considered identical, and in our simulation \u03b8ki was set to 0.5. In other words, a Gaussian component N(\u03a7; \u03bc1, \u039b1) \u2208 Gt persists over time if there exists another component \u039d(\u03a7; \u03bc2, \u039b2) \u2208 Gt+100 for which the KL-divergence is smaller than a predetermined threshold. If a Gaussian component persists more than Ocounts times in a row, the component becomes fixed. In our simulation, we chose @counts = 4."}, {"title": "4.3 Which data points should be forgotten?", "content": "Figure 9 illustrates the data available for a trial composed of seven actions and eight observations. For each observation, the variational Gaussian mixture can be used to compute the associated re-sponsibilities. An observation xt can be forgotten if the previous Xt\u22121, current xt, and next Xt+1 observations are all associated to a fixed component. Of course, if the observation is at the beginning of a trial, then the previous observation can be ignored. Similarly, if the observation is at the end of a trial, then the next observation can be ignored."}, {"title": "5. Planning and decision making", "content": "In this section, we explain how Q-learning (Sutton and Barto, 2018) can be adapted to work with a problem where there is uncertainty about the current states. First, we introduce the standard Q-learning algorithm, then we adapt this algorithm to work with beliefs over states."}, {"title": "5.1 Q-learning", "content": "The Q-learning algorithm aims to solve the reinforcement learning problem, where at each time step, an agent takes an action at based on the current state st. Then, the environment produces a next state st+1 and the reward rt+1 obtained by taking action at in state st. The agent's goal is to maximize the discounted sum of future rewards:\n$G_t = \\sum_{\\tau=t+1}^{T} \\gamma^{\\tau-t-1} r_\\tau,$\nwhere \u03b3\u2208 [0,1] is the discount factor, and the time horizon T must be larger than t + 1. In reinforcement learning, a policy \u03c0defines the probability of taking each action in each state, i.e., \u03c0(\u03b1\u03c2). The expected value of a state s under a policy \u03c0is denoted v\u03c0(s), i.e.,\n$v_\\pi(s) = E_\\pi[G_t | S_t = s] = E_\\pi \\Bigg[\\sum_{\\tau=t+1}^{T} \\gamma^{\\tau-t-1} r_\\tau \\Big| S_t = s\\Bigg],$\nwhere \u03c5\u03c0(s) are sometimes called the state values. Importantly, different policies \u03c0lead to different state values, and there exists at least one policy \u03c0* with the highest state values:\n$v^*(s) = max_\\pi v_\\pi(s),$\nwhere v*(s) are the optimal state values obtained by following one of the optimal policies \u03c0*. Similarly, the expected value of taking action a in state s, and then follow policy \u03c0is denoted q\u03c0(a, s), i.e.,\n$q_\\pi(a, s) = E_\\pi[G_t | a_t = a, S_t = s] = E_\\pi \\Bigg[\\sum_{\\tau=t+1}^{T} \\gamma^{\\tau-t-1} r_\\tau \\Big| a_t = a, S_t = s\\Bigg],$"}, {"title": "5.2 Beliefs based Q-learning", "content": "While Q-learning assumes that states are observable, the discrete states available to our planner are the latent variables Z, over which the agent has posterior beliefs. The question then becomes:"}, {"title": "6. Experiments", "content": "In this section, we validate our approach on a maze solving task. The agent is able to move \"up\", \"down\", \"left\" and \"right\", but if the action selected would lead the agent to bump into a wall, then"}, {"title": "7. Conclusion", "content": "In this paper, we tackled the problem of learning the model structure from data, when the observations are continuous. More specifically, we aimed at identifying the number of clusters present in the data, and each cluster was associated to a latent state. As more data is gathered by the agent, new clusters are discovered, and the model needs to increase its number of states. In contrast, if the agent currently believes that there are more states, than the actual number of clusters in the data, then the number of states needs to decrease.\nWe proposed a variational Gaussian mixture model, in which the Gaussian mixture model is able to remove clusters not supported by the data, and the agent is constantly monitoring the data searching for new clusters. When new clusters are identified, the corresponding components are added to the mixture. These two mechanics allow the model to learn the number of components. While this is happening, the agent also learns the parameters of the individual components through variational inference.\nHowever, inferring the current state is only part of the story, and the agent also needs to learn how different actions impact the temporal transition between states. This is done by taking advantage of a categorical-Dirichlet model, which effectively keeps track of the number of times the agent moved from a state to another when performing a specific action.\nOnce the agent can perform inference and predict the consequences of its action, the last requirement is to perform decision making and planning. We proposed a variant of the Q-learning algorithm, which accommodates stochastic states, i.e., having beliefs over states instead of observable states. This new algorithm enabled the agent to solve several mazes, by reaching the exit position and eating the cheese.\nExperimentally, our approach was able to solve several mazes, but still has limitations. For example, the approach is susceptible to unlearning existing components, where several components fuse together into a single component. Additionally, since e-greedy is used to trade-off exploration and exploitation, the agent tends to struggle to explore remote parts of the maze. These limitations suggest that more research is required to increase the stability of the Gaussian components. Additionally, the agent would benefit from improving the exploration strategy, for example, the agent should probably focus on discovering rarely explored parts of the maze instead of randomly selecting action. Also, the environments being modelled were somewhat idealised, e.g., the organism follows a rather stereotyped trajectory, passing from one cluster to another.\nFurthermore, one could investigate alternative approaches for learning the number of components such as the infinite Gaussian mixture model (Rasmussen, 1999). Another interesting direction of"}]}