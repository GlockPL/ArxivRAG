{"title": "HARDMATH: A BENCHMARK DATASET FOR CHALLENGING PROBLEMS IN APPLIED MATHEMATICS", "authors": ["Jingxuan Fan", "Sarah Martinson", "Erik Y. Wang", "Kaylie Hausknecht", "Jonah Brenner", "Danxian Liu", "Nianli Peng", "Corey Wang", "Michael P. Brenner"], "abstract": "Advanced applied mathematics problems are underrepresented in existing Large Language Model (LLM) benchmark datasets. To address this, we introduce HARDMATH, a dataset inspired by a graduate course on asymptotic methods, featuring challenging applied mathematics problems that require analytical approximation techniques. These problems demand a combination of mathematical reasoning, computational tools, and subjective judgment, making them difficult for LLMs. Our framework auto-generates a large number of problems with solutions validated against numerical ground truths. We evaluate both open- and closed-source LLMs on HARDMATH-MINI, a sub-sampled test set of 366 problems, as well as on 40 word problems formulated in applied science contexts. Even leading closed-source models like GPT-4 achieve only 43.8% overall accuracy with few-shot Chain-of-Thought prompting, and all models demonstrate significantly lower performance compared to results on existing mathematics benchmark datasets. We additionally conduct a detailed error analysis to gain insights into the failure cases of LLMs. These results demonstrate limitations of current LLM performance on advanced graduate-level applied math problems and underscore the importance of datasets like HARDMATH to advance mathematical abilities of LLMs.", "sections": [{"title": "1 Introduction", "content": "Many mathematical equations that arise in practical scientific and engineering problems cannot be solved analytically. Traditional mathematics courses tend to focus on equations with exact, analytical solutions, teaching only a limited set of techniques for solving them. Similarly, the mathematical reasoning datasets used to benchmark large language models (LLMs) are predominantly restricted to problems of this nature. However, many real-world mathematics problems involve integrals, ordinary differential equations (ODEs), and partial differential equations (PDEs) that do not have closed-form solutions and must be approached with a different set of techniques. While numerical solutions offer valuable insights, they often fail to provide intuition behind solutions behavior. A key approach in applied mathematics involves finding approximate analytical solutions to complex problems using asymptotic and applied analysis techniques\u2014methods that are largely underrepresented in existing LLM benchmark datasets. To address this gap, we introduce HARDMATH, a dataset specifically designed to focus on asymptotic reasoning in mathematics. This dataset captures a fundamentally different type of mathematical reasoning compared to other benchmarks and can be useful for evaluating LLMs' abilities to make research-relevant approximations.\nHARDMATH consists of 1,466 problems inspired by a graduate-level course on asymptotic methods. These problems cover algebraic equations, ODEs, and integrals commonly encountered in real-world scientific and engineering contexts,"}, {"title": "2 Related work", "content": "2.1 Mathematical datasets\nLLMs have shown promising capabilities in mathematics. However, evaluating and expanding the full extent of these abilities requires diverse datasets with problems that go beyond basic arithmetic or elementary word problems. Existing benchmarks often focus on these simpler domains, with a gap in addressing graduate-level applied mathematics problems that demand a deeper understanding and diverse, multi-modal analytical skills. Most mathematics datasets for evaluating or training LLMs contain samples that either present the problem directly or within a constructed narrative context. Notable examples of these datasets include MATH (12,500 high school competition-style problems) [Hendrycks et al., 2021], GSM8K (8,500 multistep grade-school problems) [Cobbe et al., 2021], MATHQA (37,000 GRE/GMAT-level multiple-choice problems) [Amini et al., 2019], and ODYSSEY-MATH (387 hand-curated problems across various difficulty levels) [Netmind.AI, 2024]. While these existing datasets are valuable for assessing LLM math performance in certain areas, most are limited in scope and complexity.\nRecent efforts target more advanced problems that are most often manually-sourced. Relevant works include JEEBENCH [Arora et al., 2023] and a subset of the MATHBENCH dataset [Liu et al., 2024], both of which cover some college-level topics including simple ODEs and multivariable calculus. More advanced-level problems are presented in GHOSTS, which contains a GRAD-TEXT subset\u2014a collection of 130 exercises from graduate-level mathematics textbooks in functional analysis, topology, and probability theory [Frieder et al., 2024]\u2014and in ARB, which features a small set of university-level formal mathematics problems from prior qualifying examinations in the mathematics departments at Harvard University and the University of California, Berkeley [Sawada et al., 2023]. However, these datasets are limited by their size and scalability; datasets created by scraping textbooks or similar resources are generally quite small and difficult to broaden easily. Most of these challenging datasets also focus on abstract, formal mathematics and exclude other forms of mathematical reasoning. Finally, textbook problems are often protected by copyright, which can complicate their public use.\nExisting datasets (summarized in Table 1) thus lack the scale and specific focus needed to evaluate LLMs on advanced mathematical problems that may be highly useful for scientific research. HARDMATH aims to address these limitations by offering a large collection of challenging applied mathematics problems inspired by a graduate-level course on asymptotic methods. It emphasizes problems that require diverse mathematical approaches, numerical calculations, and subjective judgment, mirroring the complexity of problems faced by researchers in a variety of domains. Code for auto-generating the problems in HARDMATH can be used to generate any number of additional problems, which is a unique and powerful feature for scaling LLM benchmarking and model developments like novel prompting techniques or fine-tuning. A key area of interest in current LLM research is developing models that can effectively use external tools. The problems in our dataset are unique because they involve approximate solutions that cannot be formalized"}, {"title": "2.2 Recent interest in advanced mathematical reasoning", "content": "As LLMs continue to improve, there has been growing interest in developing more challenging benchmarks, especially in mathematics. A notable example is the recent open challenge, Humanity's Last Exam, which aims to create the world's most difficult public AI benchmark, requesting questions that \"only exceptional individuals can answer correctly,\" do not involve \"straightforward calculation/computation,\" and are written by individuals with PhD-level academic training [Hendrycks and Wang, 2024]. Similarly, frontier models have been advancing quickly, and many are explicitly focused on quantitative and scientific reasoning, such as OpenAI's recent o1 series. In line with our motivation for developing HARDMATH to better track the progress of LLMs, OpenAI argues that \"recent frontier models do so well on MATH and GSM8K that these benchmarks are no longer effective at differentiating models\" [OpenAI, 2024]."}, {"title": "3 Datasets", "content": "3.1 HARDMATH design choices\nHere, we detail the HARDMATH dataset, which contains problems on polynomial nondimensionalization, polynomial root-finding, ODEs, integrals, and word problems that contextualize each of these. HARDMATH contains four problem classes with seven distinct problem types, as well as 40 handwritten word problems contextualizing the problem types."}, {"title": "3.2 Dataset generation and verification", "content": "The data generation code uses SymPy [Meurer et al., 2017], a library for symbolic mathematics, and SciPy, a library for scientific computing [Virtanen et al., 2020], to implement the mathematical procedures required for obtaining approximate, analytical solutions. Problems are generated by combining randomly selected coefficients, functional forms, and initial conditions uniquely defined for each problem (described in Appendix A)\u2014no duplicate problems are included. Solutions are generated by navigating through a set of possible cases during the algorithmic problem-solving strategy. Each mathematical step is embedded in explanatory text so HARDMATH solutions match the style and rigor of traditional problem set solutions. The main results for all problems are included in boxed environments in the solution explanations to distinguish them from the rest of the text. This follows the formatting convention used in other mathematics datasets designed for LLM benchmarking, such as the MATH dataset [Hendrycks et al., 2021].\nFor each problem type, the dataset includes: 1) ATEX-formatted problem statements with prompts, 2) ATEX-formatted solution steps and final analytical answer(s), 3) demonstration of the accuracy of the analytical results by comparing with numerical solutions, and 4) metadata descriptors of the problem and solution types. For every problem type, we select evaluation points in each solution regime and calculate the relative error between the analytical solution and the numerical solution at these points. Problems were included in HARDMATH only if their approximate solutions had less than 10% error from the numerically calculated ground-truths. For the polynomial root correction problems, we also confirm that the corrections improve the original approximation. While manually verifying each solution step-by-step is impractical for a dataset of this size, our validation process ensures a high level of confidence in the accuracy of the solutions provided."}, {"title": "3.3 Problem types", "content": "3.3.1 Nondimensionalization of polynomials\nNondimensionalization is a technique to simplify equations by reducing the number of parameters [Evans, 1972]. In HARDMATH, the first type of polynomial used for nondimensionalization demonstration contains symbolic coefficients and is of the form\n$a_1x^{n_1} + a_2x^{n_2} + a_3, n_1 > n_2 > 0$.\nNondimensionalization converts this to the form $\\epsilon y^{n_1} + y^{n_2} + 1$. The second type contains numerical coefficients and is of the form\n$\\pm a_1x^{n_1} + a_2x^{n_2} \\pm a_3, n_1 > n_2$\nwhich can be simplified to $\\epsilon y^{n_1} \\pm y^{n_2} \\pm 1$ given a specific numerical value of $\\epsilon$.\nFor all problem types described in this section, details regarding the parameters used to generate problems and the mathematical solution techniques are provided in Appendix A.\n3.3.2 Polynomial root-finding\nExact formulas exist for quadratic, cubic, and quartic equations, but deriving them for quintic or higher-order poly-nomials is not possible [Stewart, 2015]. HARDMATH includes approximate root-finding examples for higher order polynomials of the form $\\epsilon x^1 + x^2 \\pm 1$ (example in Appendix A.1.2). The goal is to solve for roots in terms of $\\epsilon$ using the method of dominant balance for small and large positive $\\epsilon$ regimes."}, {"title": "3.3.3 Polynomial root correction terms", "content": "The use of two-term dominant balances\u2014such as in the previous problem type-neglects terms and introduces an error. We can calculate a correction term $\\delta$ to reduce this error. Suppose the true roots $x^*$ of a polynomial are given by $x^* (\\epsilon) = x(\\epsilon) + \\delta$, where x is our approximation to the root and $\\delta$ is the error term. Plugging the roots $x^* (\\epsilon) = x(\\epsilon) + \\delta$ into the polynomial allows one to use a Taylor expansion of $\\delta$ around to solve for the correction $\\delta$. Appendix A.1.3 shows a full worked solution."}, {"title": "3.3.4 Nonlinear ordinary differential equations", "content": "We generate nonlinear third-order ODEs for which there do no exist exact analytical solutions and provide approximate formulae for small and large x regimes, where the small x regime is near x = 0 and the large x regime typically involves the solution diverging (example in Appendix A.1.4). The method is robust for higher-order problems, but for simplicity HARDMATH includes only third-order ODEs."}, {"title": "3.3.5 Traditional integrals", "content": "We consider integrals of the form $I(\\epsilon) = \\int_0^1 e^{\\epsilon + P(x)} dx$, where P(x) is an arbitrary polynomial. HARDMATH provides approximations of each integral in three regimes: small, intermediate, and large $\\epsilon$. A full example is in Appendix A.1.5."}, {"title": "3.3.6 Laplace integrals", "content": "We consider integrals of the form $I(x) = \\int_0^\\infty g(t)e^{\\pm xf(t)}dt$, which can be approximated using Laplace's Method when x is very large because the integral's value is dominated by the region around $t_0$ [Bender and Orszag, 2013]. Depending on where the minimum is, the approximation is either\n$I(x) \\approx g(t_0)e^{\\pm xf(t_0)}\\sqrt{\\frac{2\\pi}{x|f''(t_0)|}}$, or $I(x) \\approx \\frac{g(t_0)e^{\\pm xf(t_0)}}{x\\sqrt{f''(t_0)}}$"}, {"title": "3.4 Word problems in context", "content": "One motivation for creating HARDMATH is to help LLMs recognize and solve problems where approximation techniques are needed. To evaluate how LLMs perform on such problems in realistic scenarios, we develop a smaller dataset of 40 manually-generated word problems (example in Box 2). Although this dataset is smaller than our hand-verified evaluation set, it is large enough to evaluate the effect of additional context in the problem statement on LLM accuracy."}, {"title": "4 Evaluation", "content": "4.1 Evaluation protocols\nWe conduct evaluations of various LLMs on HARDMATH-MINI, a carefully curated subset of 366 problems that matches the statistical composition of HARDMATH (Fig. 1). This smaller dataset is designed to optimize computational resources while retaining a sufficient number of questions to ensure consistent and reliable testing outcomes, thus maintaining the integrity of our evaluation. The evaluation focuses on four distinct problem types: 1) Nondim includes nondimensionalization in symbolic and numerical form; 2) Roots includes polynomial root-finding; 3) ODEs includes nonlinear ODEs; and 4) Integrals includes traditional and Laplace integrals. The input prompt for each problem contains the essential problem setup and a detailed description of the question. Additionally, hints specific to each problem type are provided to guide the format of the answer. When few-shot prompting is used, it adds a fixed set of paired problem-solution examples from the corresponding problem types. Example prompts can be found in Appendix A.2.1, Table 6.\nWe evaluate model-generated responses by scoring them for accuracy using a combined protocol of automatic final answer assessment and procedural LLM-based grading. The automatic assessment follows methodology from Hendrycks et al. [2021], where models are prompted to enclose their final answers using the ATEX\\boxed{} command (Table 6). Evaluation then compares the model's output within the \\boxed{} command to the dataset solution. To handle different mathematical expression formats, we implement both SymPy-based equivalence checks and numerical evaluations.\nIn addition to the standard automatic assessment of final answers, we develop a novel procedural grading approach leveraging LLMs, tailored to the unique evaluation challenges of our dataset: 1) Some problem types require complex, multi-step solution procedures (e.g. determining critical point in Laplace integral approximation) where a single cut-off criterion at the final answer cannot capture the full spectrum of model performance. Thus, grading intermediate steps in the solution procedure is necessary for comprehensive assessment. 2) HARDMATH targets the models' ability to make human-like abstraction and approximation judgments. Some problem types allow a narrow range of solutions rather than a single exact one, as long as the reasoning is self-consistent and the final result falls within certain threshold to numerical ground truth.\nInspired by LLMs' ability to generate consistent ratings for response content and style [Hackl et al., 2023], we employ GPT-40 as a procedural grader. The model is prompted with a ground truth answer key and grading rubrics adapted from course grading guidelines for each problem type (example grading prompts in Appendix A.2.2 Table 4). We manually verify a subset of grading responses and found that LLM-based grading is closely aligned with human grading. Average score adjustment for each model and problem type is summarized in Appendix A.2.3 Table 5. We implement this procedural grading alongside automatic answer assessment for the problem types Roots, ODEs, and Integrals."}, {"title": "4.2 Model choice", "content": "We compare the performance of several closed- and open-source models on HARDMATH in zero- and few-shot settings with the Chain-of-Thought (CoT) [Wei et al., 2023] prompting. Closed-source LLMs include GPT-3.5 [Radford et al., 2018, 2019, Ouyang et al., 2022], GPT-4 [Achiam et al., 2023] and o1-mini [OpenAI, 2024a], open-source LLMs include Llama3 [AI, 2024] and CodeLlama [Meta, 2023]. We believe this subset of models to be representative of current LLM capabilities. We provide the prompts and hyper-parameters for LLMs evaluations in Appendix A.2.4 Table 6."}, {"title": "4.3 Quantitative results", "content": "We present the accuracy of the models and prompting settings for each problem type and the combined evaluation set (Table 2, Figure 4). Few-shot CoT prompting significantly boosts performance for all models, with o1-mini and GPT-4 showing the greatest improvement, consistent with [Wei et al., 2023] (Figure 4a). Interestingly, although the 01-mini official prompting guide recommends simple prompting over CoT [OpenAI, 2024b], we observe fairly large performance increase for all problem types at 5 shot CoT compared to 0 shot. Performance increase with prompting behavior also shows problem type-specific patterns: Figure 5 demonstrates that performance saturates quickly for harder problem types like ODEs. The varying performance increases among different problem types may be due to different error modes in model answers, which we discuss in the following section. It's notable that o1-mini, though with much smaller parameter size, shows considerably better performance at all tested shot levels, confirming its optimized ability for STEM reasoning [OpenAI, 2024a].\nAmong closed-source models, 01-mini with 5-shot CoT prompting achieves the highest overall accuracy of 62.3%. GPT-4 at 5-shot CoT scores only 43.8%. Among open-source models, Llama3-8b with 5-shot CoT prompting achieves the highest overall accuracy of 20.2%. We discuss the performance of these representative models-01-mini, GPT-4 and Llama3-on HARDMATH-MINI in comparison with established datasets, including GSM-8K [Cobbe et al., 2021], MATH [Hendrycks et al., 2021], and more advanced mathematics datasets like GHOSTS [Frieder et al., 2024].\nLlama3-8b achieves a test accuracy of 30.0% on the MATH dataset with 4-shot CoT and 79.6% on the GSM-8K dataset with 8-shot CoT prompting [AI, 2024]. Testing Llama3-8b on HARDMATH-MINI results in an overall accuracy of 20.2% with 5-shot CoT prompting. GPT-4 (gpt-4-turbo-2024-04-09) is reported to achieve 72.2% accuracy on the MATH dataset with 0-shot CoT prompting [OpenAI, 2024c] and 92.0% on the GSM-8K dataset with 5-shot CoT prompting [Achiam et al., 2023]. On theMINIGHOSTS dataset, which also covers graduate-level mathematics, GPT-4 reaches an average score of 4.15 out of 5. We test GPT-4 on our HARDMATH-MINI dataset and obtained an overall accuracy of 43.8% with 5-shot CoT prompting.\nFinally, we include results on OpenAI's new o1-mini, which is reported to achieve 90.0% accuracy on MATH-500 with 0-shot CoT [OpenAI, 2024a]. Testing 01-mini on HARDMATH-MINI reveals a significant performance increase compared to results on other models on some (e.g. Nondim) but not all problem types. Overall accuracy with 5 shot CoT reaches 62.3%, substantially lower compared to performance on existing mathematics benchmarks. This indicates that the HARDMATH benchmark consists of problems that are still challenging and unfamiliar to even the most performant LLMs developed specifically for advanced reasoning."}, {"title": "4.3.1 Extensions to word problems", "content": "To assess LLM's ability to solve similar applied math problems in real science and engineering contexts, we also test GPT-4 (best performing model with a stable version) on a set of 40 hand-crafted word problems that included a mixture of Nondim, Roots, ODEs, and Integrals. We avoided additional prompt engineering, omitting the problem-specific hints listed in Table 3. This evaluation resulted in an overall accuracy of 28.1%. We plan to leverage the automated generation method as a basis to expand the number of word problems for future work."}, {"title": "4.4 Fine-grained results", "content": "In addition to reporting the summarized test accuracy, we study the detailed breakdown of model responses at different correctness levels and summarize specific error modes of LLMs solving these challenging applied mathematics questions. This analysis helps us compare performance nuances and understand reasoning paths by model, prompting technique and question type.\nWe first break down model performance by percentage of correct, partial and incorrect responses (Figure 2). This analysis reveals how few-shot prompting enhances model performance across varying problem types but through different strategies. Overall quantitative results already show that ODEs are comparatively harder for all models while Nondim problems appear to be the easiest (Figure 4b). For hard problems like ODEs, full correctness is rare. Correctness level analysis shows that models tend to increase partial credit responses with CoT prompting, as they struggle to solve the problems entirely but manage to partially address them\u2014in this case, starting with the easier small x regime solutions. In contrast, for simpler problems like Roots, advanced models like 01-mini and GPT-4 get more fully correct responses with increasing CoT shot number, demonstrating the models' ability to understand the approximation reasoning procedure fully (Fig. 2).\nSecond, we summarize the error modes of partial and incorrect responses to better understand the model's reasoning pitfalls. Specifically, we want to dissect how CoT changes model performance on the level of detailed errors. Figure 3 uses GPT-4's responses at 0 vs. 5 shots on problem type Roots as an analysis example. This illustrates how 5 shot CoT"}, {"title": "5 Conclusion", "content": "We introduce HARDMATH, a new dataset covering several problem types from an advanced applied mathematics course that can be used to benchmark LLMs' mathematical capabilities and perform model developments, including fine-tuning. This dataset consists of 1060 examples, and we additionally include 366 verified examples in HARDMATH-MINI and 40 verified 'problems in context' that we use to evaluate various leading LLMs. HARDMATH is unique in several ways. First, there do not exist large-scale mathematical datasets covering problems of similar difficulty from applied mathematics. Second, HARDMATH's problems and solutions are algorithmically generated, meaning that one could produce datasets of arbitrary size using our framework. This feature of HARDMATH is especially unique, since most existing mathematical datasets require manual problem-setting or curation from other sources (many of which are not publicly accessible).\nOur evaluation highlights that while few-shot CoT prompting significantly improves model performance, especially for models like o1-mini and GPT-4, the overall accuracy on HARDMATH-MINI problems remains much lower compared to other existing benchmarks. This suggests that our dataset poses unique and challenging tasks that go beyond the boundaries of current LLM capabilities, particularly in approximation-oriented mathematical reasoning. These findings emphasize the need for further improvement in LLMs to address hard math problems.\nOur evaluation results use HARDMATH-MINI as a comprehensive test set; however, future work will fine-tune LLMs on the larger HARDMATH to improve performance. Additionally, while we have evaluated several frontier models, we plan to extend our evaluations to even more LLMs as they become available. This expanded evaluation should provide more detailed insights into performance disparities across different models, further advancing our understanding of LLMs' capabilities in handling complex mathematical reasoning."}, {"title": "A Appendix", "content": "A.1 Implementation and method details for data generation\nThe following subsections detail the process used to generate the problems and solutions for each problem type.\nA.1.1 Nondimensionalization of polynomials\nThe first nondimensionalization sub-type is generalized by varying the integer values for the degrees $n_1$ and $n_2$ within the range 0 < $n_2$ < $n_1$ < 10, while keeping $a_1$, $a_2$, $a_3$ > 0 symbolic. Solutions to these problems express the dimensionless parameter $\\epsilon$ in terms of these three coefficients."}, {"title": "A.1.2 Polynomial root-finding", "content": "As with the nondimensionalization problems, degrees in the polynomial are randomly generated with maximum order ten and 0 < $n_2$ < $n_1$. See a full problem and solution below."}, {"title": "A.1.3 Polynomial root correction terms", "content": "The true roots $x^*$ of a polynomial are given by $x^* (\\epsilon) = x(\\epsilon) + \\delta$, where x is our existing approximation to the root as found in Appendix A.3 and $\\delta$ is the error term. This requires us to solve\n$\\epsilon(x + \\delta)^{n_1} \\pm (x + \\delta)^{n_2} + 1 = 0$\nfor $\\delta$ by equating coefficients of $\\epsilon$ terms of the same order, as detailed in the worked solution below."}, {"title": "A.1.4 ODES", "content": "We generate third-order ordinary differential equations of the form\ny''' = $f_1(x)(y'')^a + f_2(x)(y')^b + f_3(x)y^c + f_4(x)$,"}, {"title": "A.1.5 Integrals", "content": "The polynomial P(x) is randomly generated to consist of up to ten terms, where each term is a power function of x with an integer power randomly sampled from 1 and 20 and an integer coefficient sampled from 1 to 10. The integration bound a \u2208 [0, 100] is also randomly selected. This form ensures that the integral does not oscillate.\nThe height is approximated as the maximum value of the integrand, which is, and the width can be estimated as the distance over which the integrand decreases from its maximum value by a factor of 2, which implies that the width x obeys the equation\n$\\frac{1}{\\epsilon + P(x)} = \\frac{1}{2\\epsilon} \\Rightarrow P(x) = \\epsilon$.\nIn the regime of small $\\epsilon$, the term with the smallest degree and $\\epsilon$ are the dominant terms, and in the regime of intermediate $\\epsilon$, the term with the largest degree and $\\epsilon$ are dominant. There exists one more solution regime when the width of the integral exceeds the limits of integration, or when $\\epsilon$ is \"very large.\" In this case, the integral is approximated by $L/\\epsilon$, where L is the integration range."}, {"title": "A.1.6 Laplace integrals", "content": "Laplace integrals of the form $I(x) = \\int_a^b g(t)e^{\\pm xf(t)}dt$ assume that f(t) > 0, is never a constant, and has an absolute minimum at a point $t_0$ either in the interior of or on the bounds of the interval [a, b].\nThe set of possible Laplace integrals I(x) in our dataset are parameterized by four parameters: the bounds [a, b], g(t), f(t), and the sign in front of x. To generate the dataset, the bounds for each problem were randomly sampled from the [-1, -0.9, . . . 0.9, 1], and the sign was uniformly sampled from {-1,1}. The functions f(t) and g(t) were generated by randomly selecting a linear combination of polynomials up to fifth order and basic trigonometric functions.\nOur solution uses SymPy under the hood to find the minima of f(t) (or the dual annealing algorithm if SymPy fails to return the minima)."}, {"title": "A.2 Evaluation setup", "content": "A.2.1 Prompts for response generation"}]}