{"title": "Fair In-Context Learning via Latent Concept Variables", "authors": ["Karuna Bhaila", "Minh-Hao Van", "Kennedy Edemacu", "Chen Zhao", "Feng Chen", "Xintao Wu"], "abstract": "The emerging in-context learning (ICL) ability of large language models (LLMs) has prompted their use for predictive tasks in various domains with different types of data facilitated by serialization methods. However, with increasing applications in high-stakes domains, it has been shown that LLMs can inherit social bias and discrimination from their pre-training data. In this work, we investigate this inherent bias in LLMs during in-context learning with tabular data. We focus on an optimal demonstration selection approach that utilizes latent concept variables for resource-efficient task adaptation. We design data augmentation strategies that reduce correlation between predictive outcomes and sensitive variables helping to promote fairness during latent concept learning. We utilize the learned concept and select demonstrations from a training dataset to obtain fair predictions during inference while maintaining model utility. The latent concept variable is learned using a smaller internal LLM and the selected demonstrations can be used for inference with larger external LLMs. We empirically verify that the fair latent variable approach improves fairness results on tabular datasets compared to multiple heuristic demonstration selection methods. Code and data are available at https://github.com/karuna-bhaila/fairicl.", "sections": [{"title": "1 Introduction", "content": "Large language models (LLMs) have demonstrated immense capabilities in performing a wide range of natural language processing (NLP) tasks. A factor contributing to widespread LLM usage is their in-context learning (ICL) (Brown et al., 2020) ability which allows adaptation to downstream tasks without costly training or fine-tuning. Provided with a small number of demonstration examples, ICL equips LLMs with the ability to infer task-specific context and perform inference with impressive utility. Recent research has also explored the applicability of LLMs on tabular data through serialization methods that facilitate ICL by transforming the data into natural language formats (Hegselmann et al., 2023). With the increasing integration of LLM inference in domains such as healthcare (Wu et al., 2023), finance (Li et al., 2023a), and the legal sector (Sun, 2023) with various data formats, it has become crucial to scrutinize their use from a trustworthiness perspective.\nLLMs have been shown to exhibit discriminatory behavior in their outputs due to stereotypes and prejudices inherent in pre-training data (Abid et al., 2021; Basta et al., 2019). When used for decisive tasks, LLMs may mirror social inequalities and biases from the real world leading to harmful consequences. Furthermore, in the ICL setting with tabular data classification, recent research has empirically verified the presence of bias in LLM outputs. Liu et al. (2023) investigated unfairness in ICL with tabular data by flipping the labels of in-context demonstration examples and observed bias reduction but with significant trade-offs in model utility. Li et al. (2024) similarly implemented multiple heuristic methods for demonstration selection based on sensitive attribute and label distribution in the demonstrations. Hu et al. (2024) discovered that increasing the representation of minority groups and underrepresented labels in demonstrations helps to improve fairness at some cost to utility. They further developed a strategy that uses clustering to extract representative samples and selects demonstrations from the extracted samples based on their performance on a validation set.\nIn this work, we similarly explore optimal demonstration selection for ICL to promote fairness in LLM predictions but we utilize the latent concept variable mechanism (Wang et al., 2024) to achieve fair in-context learning. Wang et al. (2024) formulate ICL via a Bayesian perspective and theorize that inference with a finite number of demonstrations selected using the latent concept variable approximates the optimal Bayes predictor."}, {"title": "2 Related Work", "content": "The latent concept is learned from an observed set of task-specific training data with a small LLM and used to obtain demonstrations that can be generalized to larger LLMs for improving downstream task performance.\nMotivated by the influence of latent concepts on model performance, we formulate a fair demonstration selection approach for in-context learning, dubbed as FairICL. As the latent concepts are learned from an observed set of task-specific examples, we conjecture that the training data distribution may affect the quality of the learned latent concepts and ultimately the model predictions from both accuracy and fairness perspectives. Therefore, in FairICL, we incorporate an effective data augmentation technique that promotes decorrelation between the sensitive attributes and the outcome variables by randomizing the relationship between them. This augmentation allows us to obtain a fairer representation of the task-specific data used to learn the fair latent concept variable while preserving relevant information among non-sensitive attributes and the label. We then utilize the learned concepts to select demonstrations from the observed training examples such that the probability of observing the learned latent variable is maximized when conditioned on the corresponding example. The selected demonstrations are used to perform in-context learning with external LLMs larger than the one used for learning. We empirically validate our FairICL on real-world tabular datasets known to represent social biases and demonstrate that FairICL can effectively achieve fairness goals while maintaining predictive utility. We compare the performance of FairICL with multiple heuristic approaches and conduct a comprehensive analysis of the influence of different hyperparameters. Our empirical results show that FairICL can generalize demonstration selection to external LLMs and outperform baseline methods."}, {"title": "Fairness in LLMs", "content": "As LLM integration into decision-making systems continues to rise, it becomes essential for them to be evaluated from a fairness perspective. Multiple works have highlighted discriminatory behavior in LLM outputs originating from societal biases contained in pre-training data (Abid et al., 2021; Wang et al., 2023) or under-representation of minority population (Gallegos et al., 2023). For instance, Huang et al. (2021) analyzed implicit gender-based stereotypes in LLM outputs via commonsense inference. Wang et al. (2023) evaluated the influence of normal and adversarial prompts on bias in GPT models. Abid et al. (2021) demonstrated unfairness in LLM outputs w.r.t religious groups. Following these works, the study of LLM fairness has also extended to tabular data inference with pre-trained language models (Li et al., 2023b; Liu et al., 2023; Chhikara et al., 2024; Hu et al., 2024; Atwood et al., 2024a). These works focus on LLM inference with in-context learning and formulate ways to select demonstration examples while prompting fairness or ensuring representation for minority groups. Li et al. (2023b) evaluated multiple heuristic methods of selecting demonstrations and a guardrail technique instructing LLM to be fair in its decision-making. (Liu et al., 2023) implemented label-flipping for demonstration examples and achieved fair predictions with significant utility loss. Chhikara et al. (2024) evaluated LLM's familiarity with commonly known fairness notions and utilized a similarity-based demonstration selection approach. Hu et al. (2024) aimed to increase minority group representation in demonstrations and selected demonstrations based on corresponding validation set performance. Atwood et al. (2024a) explored remediation techniques for fairness and compared prompt-based techniques with inprocessing and postprocessing methods. Similar to some earlier works, we aim to address bias in LLM predictions in tabular data by selecting demonstration examples that promote fairness. However, we utilize the latent concept variable model and present a framework to learn fair representations of the latent concept. The demonstrations selected by the latent concept are used for ICL to obtain fair outcomes while maintaining predictive utility."}, {"title": "3 Preliminaries", "content": ""}, {"title": "3.1 In-Context Learning", "content": "The in-context learning (Brown et al., 2020) ability of LLMs has prompted multiple research works that investigate how LLMs can learn from demonstration examples for certain tasks without being explicitly trained for those tasks. Let us denote a pre-trained LLM as M with parameters W. Let $D = \\{(x_i, Y_i)\\}_{i=1}^{n}$ denote a tabular dataset observed for an arbitrary task where $x_i \\in X$ represents attributes of the i-th instance and $Y_i \\in Y$ its corresponding outcome. Assume $a_i \\in A$ de-"}, {"title": "3.2 Latent Concept Learning", "content": "notes its sensitive attribute. For in-context learning, the LLM is provided with k examples from D as demonstrations or context to guide the model in structuring its response for a test example x. Conditioned on a task description inst, a set of sampled demonstrations $\\{(X_1,Y_1),\\ldots, (x_k, Y_k)\\}$, and a test query x, the prediction output $\\hat{y}$ from M can be formally formulated as\n$\\hat{y} \\leftarrow M(\\text{inst}, g(X_1,Y_1),\\ldots,g(x_k, Y_k), g(x)),$ (1)\nwhere $g(x_k, Y_k)$ denotes a prompt function (e.g., a template) that transforms the k-th demonstration example into natural language text. To simplify, we omit the task description and prompt function thereafter and represent the output probability as\n$P_M(y|(x_1,y_1),\\dots, (x_k, Y_k), x; W).$ (2)\nICL performance has been found to be significantly influenced by demonstration examples and their ordering (Liu et al., 2021; Rubin et al., 2021; Su et al., 2022; Lu et al., 2021; Ma et al., 2024). Consequently, recent works explore effective demonstration selection based on similarity to query input (Liu et al., 2021; Rubin et al., 2021; Su et al., 2022), entropy of predicted labels (Lu et al., 2021), and low predictive bias (Ma et al., 2024).\nAn essential research question in in-context learning is effective demonstration selection to enable optimal downstream performance. Towards this objective, Xie et al. (2021) put forth an interpretation of ICL based on latent concept variables concluding that pre-trained models learn latent concepts during next-token prediction training and infer a shared latent concept among demonstrations used for inference. They show that under the assumptions of the hidden Markovian data generation process, discrete latent concept tokens, and an approximately infinite number of demonstrations, in-context learning is an optimal predictor. Similarly, Wang et al. (2024) studied latent concepts in LLMs but under a more general assumption of continuous latent concepts and that the data generation process is governed by an underlying causal mechanism given as $X \\rightarrow Y \\leftarrow \\theta$ or $Y \\rightarrow X \\leftarrow \\theta$ where X represents the input, Y the output, and $\\theta$ denotes the latent concept variable. Under these assumptions, in-context learning can become an optimal predictor with a finite number of demonstrations chosen using the latent concept variable $\\theta$. To find the optimal value of $\\theta$ when considering the $X \\rightarrow Y \\leftarrow \\theta$ direction, Wang et al. (2024) formulated a latent concept variable framework for learning task-specific concept tokens that capture sufficient information for next-token prediction by minimizing a loss jointly conditioned on X and the learned $\\theta$ as\n$l(x, y; \\theta) = -\\log P_M(y|\\theta, x),$ (3)\nwhere $\\theta$ represents the learned latent concept variable, x denotes an input token sequence and y the discrete target variable. In practice, $\\theta$ is optimized by adding new tokens to M's vocabulary with corresponding embedding vectors which we refer to as $W_\\theta$. During training, $W_\\theta$ is updated using the loss defined above. The learned $\\theta$ is then used to select k most suitable demonstrations based on the likelihood of observing the concept tokens when conditioned on the demonstration pairs formulated as $P_M(\\theta|(x_i, Y_i),\\ldots, (x_k, Y_k))$. Assuming independence among the sampled demonstrations, the top-ranked examples are obtained based on latent concept likelihood for individual examples,\n$\\underset{(x_i,Y_i) \\in D}{argmax} P_M(\\theta|x_i, Y_i).$ (4)\nThe selected demonstrations are used to perform in-context learning and are further generalizable for inference with LLMs larger than the ones used to learn $\\theta$."}, {"title": "4 Fair Latent Concept Learning", "content": "LLMs have been shown to replicate social bias and prejudice likely present in their pre-training corpus. Providing LLMs with biased examples as demonstrations during ICL may further corroborate the prediction bias, potentially leading to discriminatory outcomes in classification tasks. However, filtering pre-training data and re-training/fine-turing LLMs on unbiased data is often practically infeasible due to resource constraints. Moreover, removing discrimination from pre-training data may not entirely address the unfairness resulting from biased demonstrations during inference. Here, we focus on the demonstration selection process itself which can guide LLM predictions by providing task-specific contextual information. Researchers have empirically shown that varying demonstrations can affect the bias and fairness outcomes of"}, {"title": "4.1 Problem Setup", "content": "In the latent concept variable model, demonstrations are selected based on the likelihood of observing latent concept variable $\\theta$ (Wang et al., 2024). Generally, concept variable captures format and task information and can help improve in-context learning performance. However, the quality of the learned latent concept variable highly depends on the observed data D. We hypothesize that using a biased dataset D to learn the latent concept can lead to selecting demonstrations that favor the majority group. For instance, consider a dataset containing a comparatively higher number of positive/advantaged class instances for the majority group reflecting real societal bias. The latent concept variable may associate the positive outcome with the majority class as this biased prediction can lead to better prediction accuracy owing to imbalanced label distributions. Consequently, demonstrations selected using the latent concept variable can reinforce the bias originating from the dataset. In the following, we propose FairICL, a fair latent concept learning framework with data augmentation to mitigate unfairness in ICL predictive outcomes arising from demonstration selection. An overview of the method is presented in Fig. 1."}, {"title": "4.2 Constructing Augmented Training Data", "content": "To ensure fair predictive outcomes, we consider the correlation between the sensitive attribute a and the outcome variable y in the dataset D used to learn the latent concept variable $\\theta$. We conjecture that learning latent concept variables from an unbiased dataset can prevent $\\theta$ from incorporating bias into the task-specific contextual information that improves ICL performance. To this end, we design and implement a data pre-processing strategy on D aimed at de-correlating the sensitive attribute and the label. Assuming we obtain a dataset $\\hat{D}$ that preserves task-relevant information from D and not the biased correlation between a and y, we then construct an augmented training dataset $\\mathbb{D}$ from both D and $\\hat{D}$ to promote fairness while learning task-specific contextual information in a fair representation of latent concepts $\\theta_f$. Note that our focus is on LLM classification with ICL on tabular data which is the most commonly used data representation in fairness literature.\nTo de-correlate the outcome variable from the sensitive attribute, we introduce synthetically generated examples sampled from the distribution of D but with any influence of a on y removed. Naively, we can achieve this de-correlation in the generated examples by sampling each attribute in x and the label y independently. However, this could result in highly noisy data samples that obfuscate useful information about the relationship between non-sensitive attributes and the outcome thus negatively affecting the prediction accuracy. Here, we propose to generate samples based on an attribute hierarchy-based sampling process that simultaneously promotes sensitive attribute and label de-correlation and preserves task-relevant information.\nFor hierarchical attribute sampling, we define an ordering for all non-sensitive attributes. We construct a synthetic sample based on the attribute"}, {"title": "4.3 Learning Fair Latent Concept Variable", "content": "ordering as follows: we first randomly sample a label from a uniform distribution and obtain a subset of D conditioned on the sampled label value. We then sample the first non-sensitive attribute in the ordered list uniformly from the values occurring in the subset. We further constrain the subset to include only the sampled value of the first non-sensitive attribute and sample the second non-sensitive attribute uniformly and so on. To populate the sensitive attribute value, we randomly sample it from a uniform distribution independent of the label and any non-sensitive attributes. Furthermore, if D contains any proxy-sensitive attributes that may allude to the sensitive attribute, we condition its sampling on the sensitive attribute attribute value to promote complete de-correlation. In this manner, we generate $\\hat{D} = \\{(x_i, Y_i)\\}_{i=1}^{\\hat{n}}$ as an unbiased representation of D.\nWe then construct our augmented training dataset which contains n + \u00f1 instances and each augmented instance contains q demonstration examples from D and one query sample from either D or $\\hat{D}$ to facilitate in-context learning. Formally each instance takes the form $((X_1,Y_1),\\ldots, (x_q, y_q), x, y)$ which we denote as $(\\mathbb{x}, y)$ thereafter. We also denote this formatted training dataset containing augmented samples as $\\mathbb{D} = \\{\\mathbb{x}_i, Y_i\\}_{i=1}^{n+\\hat{n}}$ and the process is shown in lines 1-5 of Algorithm 1. The following discusses how we learn the fair latent concept variable from $\\mathbb{D}$.\nWe learn the latent concept variable by implementing prompt tuning to optimize a set of new token embeddings that is prepended to each training input token sequence (Wang et al., 2024). More importantly, we utilize the augmented dataset $\\mathbb{D}$ to construct input sequences for learning $\\theta_f$ to promote improvements in fairness and utility simultaneously. Directly optimizing $\\theta_f$ as a sequence of words is not efficient due to the discrete nature of text space. Typically, large language models (LLMs) process inputs as sequences of tokens, which are subsequently transformed into embeddings. Therefore, we optimize the fair latent concept in the LLM M's embedding space, where $\\theta_f$ is represented as a sequence of c learnable tokens, each associated with an embedding vector. We denote the subset of weights in W corresponding to $\\theta_f$ as $W_{\\theta_f}$. During training, we prepend $\\theta_f$ to the input sequences and learn $W_{\\theta_f}$ by minimizing the"}, {"title": "4.4 Demonstration Example Selection with $\\theta_f$ Likelihood", "content": "likelihood as $P_M(\\theta_f|x_i, Y_i)$.\nThe learned fair latent concept $\\theta_f$ is then used to select top-ranking examples from $\\mathbb{D}$ which will be provided as context to a larger external LLM during inference via ICL. This demonstration selection follows the rationale that training examples that maximize the likelihood of predicting the trained task-specific latent concept variable are optimal demonstrations for the corresponding task objective (Wang et al., 2024). For each training example $(x_i, Y_i) \\in \\mathbb{D}$, the likelihood of $\\theta_f$ is expressed using the probability distribution shown as\n$P_M(\\theta_f|x_i, Y_i).$ (6)\nIn our implementation, we obtain this likelihood as the probability of observing the trained $\\theta_f$ when postfixed to a sample $(x_i, Y_i)$. In Algorithm 1, these steps are outlined in lines 15-17. Subsequently, training examples are sorted based on their computed likelihood values (line 18). We then select the top m examples that maximize the likelihood of $\\theta_f$ and form the demonstration candidate set (line 19). We subsample this candidate set to allow each test query to be paired with varying demonstrations during testing. Finally, we randomly select k demonstration examples from the candidate set for each test instance, combining these to construct the final prompt for inference with an external LLM."}, {"title": "5 Experimental Evaluation", "content": ""}, {"title": "5.1 Datasets", "content": "We evaluate the effectiveness of fair demonstration selection with FairICL using a benchmark fair machine learning dataset. The Adult Income dataset (Becker and Kohavi, 1996) contains 1994 U.S. census records and the task is to predict whether an individual has an annual income greater than 50,000 US dollars based on demographic and economic attributes that characterize each entry. Following previous work (Liu et al., 2023), we use a subset of 10 attributes from the dataset named in Fig. 2. We also subsample a training dataset of 30,000 records after preprocessing. We perform serialization on the tabular Adult dataset similar to Hegselmann et al. (2023); Carey et al. (2024),"}, {"title": "5.2 Fairness Metrics", "content": "Here, we briefly describe two fairness notions used to determine LLM's bias w.r.t the majority and minority groups represented by sensitive attribute a when predicting a binary outcome variable y.\nStatistical Parity Statistical parity (Dwork et al., 2012) requires the predictions to be independent of the sensitive attribute and can be evaluated as\n$\\Delta SP = P(\\hat{y}|s = 0) - P(\\hat{y}|s = 1).$ (7)\nEqual Opportunity Equal opportunity (Hardt et al., 2016) requires that for members of majority and minority groups, the probability of being assigned a positive outcome is the same. We evaluate equal opportunity using group-based TPRs as\n$\\Delta EO =P(\\hat{y} = 1|y = 1, s = 0) - P(\\hat{y} = 1|y = 1, s = 1).$ (8)"}, {"title": "5.3 Baselines", "content": "We demonstrate the effectiveness of FairICL by comparing its performance against several baselines that implement different demonstration selection approaches briefly described as follows.\n\u2022 Random refers to standard in-context learning where k training examples are randomly sampled as demonstrations for each test instance (Brown et al., 2020).\n\u2022 Balanced implements in-context learning with equal representation for each sensitive attribute and class label combination in the demonstrations (Li et al., 2023b).\n\u2022 Instruction is used to evaluate an LLM for fair and unbiased decisions based on manual prompting-based guidance with a balanced demonstration set (Li et al., 2023b; Atwood et al., 2024b).\n\u2022 Removal omits the sensitive attribute from the demonstrations of Balanced (Li et al., 2023b).\nAs we serialize tabular data, we further replace gendered pronouns with gender-neutral ones in the training data.\n\u2022 Counterfactual is another heuristic technique and constructs demonstrations using k/2 examples from the majority (minority) group and the remaining examples by flipping the sensitive attribute of the previously sampled examples (Li et al., 2023b).\n\u2022 LatentConcept is the latent concept variable-based approach from Wang et al. (2024) where the latent concept variable is learned using the training dataset and then used to select top-k demonstrations."}, {"title": "5.4 Experimental Setup", "content": "In the FairICL framework we use LLaMA-2-7B (Touvron et al., 2023) as the internal LLM for fair latent concept learning and LLaMA-2-13B (Touvron et al., 2023) as the external LLMs for inference. We fix the learning rate at 0.0001 for all experiments and optimize the concept token embeddings over 5 epochs. For main experiments, we fix the number of added tokens c at 10, and the number of demonstrations during training q at 2. We randomly sample k = 4 demonstrations from a top-ranked candidate set of M = 100 training examples for each test query. We report performance as an average of 5 runs with standard deviations for different test splits. We also evaluate the learned fair latent concepts with LLaMA-2-7B as the external LLM. To analyze the effectiveness of latent concept learning with an augmented dataset, we conduct an ablation study where the augmented samples are created via complete random sampling"}, {"title": "5.5 Results", "content": "Model performance and comparison with baselines We report the results from inference with LLaMA-2-7B and LLaMA-2-13B in Table 2 for the Adult income dataset. Firstly, we observe the baseline performances denoted by Random where k demonstrations are randomly selected from the training set D. LLaMA-2-13B has increased utility compared to LLaMA-2-7B undoubtedly due to the model's complexity. However, the fairness metrics ASP and \u0394\u0395\u039f are larger indicating a significant presence of bias in the outputs generated by LLaMA-2-13B. With the LatentConcept method which optimizes demonstration selection for utility, model performance is improved but the bias is further amplified for both 7B and 13B models. These results motivate our study of bias in LLMs specifically for tabular classification and methods that can promote fairness in a resource-efficient manner.\nIn Table 2, we observe that FairICL can noticeably improve statistical parity and equal opportunity measure for LLaMA-2-7B compared to the Random and LatentConcept methods while achieving comparable performance. Similarly, FairICL significantly reduces unfairness for LLaMA-2-13B with minimal loss of utility. Note that, the latent concept variable is learned using the smaller LLaMA-2-7b as the internal model and the selected demonstrations are utilized to construct inference prompts for LLaMA-2-13b. This shows that FairICL can generalize the fair demonstration selection process to larger LLMs thus making the method resource-efficient. Since the external LLMs are used only for few-shot inference, FairICL also enables generalization to black-box LLMs.\nWe also evaluate the effectiveness of FairICL compared to multiple fair demonstration selection baselines. As discussed in Section 5.3, these methods have been formulated to address the LLM fairness issue via heuristic approaches. For LLaMA-2-7B, the fair baselines reduce unfairness compared to the LatentConcept method but incur a significant loss in performance. Compared to Random, only the Balanced approach shows a notable reduction in SP and EO metrics. FairICL, however, achieves the best fairness results without negatively affecting utility. For LLaMA-2-13B, the baselines mostly maintain performance but do not achieve"}, {"title": "FairICL performance over training epochs", "content": "fair outcomes. In contrast, FairICL shows a large decline in fairness metrics with similar or even improved accuracy and f1-scores compared to non-fair baselines. These results demonstrate that the de-correlation of the sensitive attribute and the outcome in FairICL helps learn fair latent concepts, resulting in demonstration selection that promotes fair predictions.\nWe analyze the performance of FairICL as latent concept learning progresses over the training epochs and present the results in Fig. 4 for LLaMA-2-13B. To obtain these results, we fix the parameters q at 2, c at 10, and kat 4. We observe that the accuracy and F1 experience a small decline after the first epoch but remain fairly stable thereafter. The SP and EO metrics on the other hand have a decreasing trend as the latent concepts are further optimized. This indicates that FairICL effectively allows the concept tokens to learn fairness-promoting context from the augmented examples and utility-preserving information from the original training samples. This ultimately leads to a demonstration selection process that improves both fairness and performance in LLMs."}, {"title": "Ablation Study", "content": "In this section, we investigate the role of data augmentation and latent concept learning in FairICL through an ablation study. We implement two variations of FairICL, FairICL-LC and FairICL-R. FairICL-LC directly evaluates the fair latent concepts as we prepend the learned concept variable to the prompt for test instances along with randomly sampled k demonstrations. FairICL-R adopts a completely random sampling mechanism for all attributes to create the augmented dataset and follows an inference procedure similar to FairICL. In other words, the generated dataset"}, {"title": "Number of Demonstrations", "content": "does not preserve the useful correlation between the non-sensitive attributes and outcomes like in FairICL. We report ablation results in Table 3 for LLaMA-2-7B since FairICL-LC can be evaluated only for the internal LLM whose vocabulary contains additional tokens corresponding to the latent concept variable. FairICL-LC achieves the best accuracy and F1 score indicating that the latent concept learns information relevant to the task. Also, the low fairness metrics imply that training with the augmented dataset prompts the latent concept to favor fair predictions. FairICL-R achieves almost ideal fairness metrics but does not maintain model accuracy as the randomly generated dataset removes even the useful correlation between non-sensitive attributes and labels. FairICL, however, preserves relevant information in D prompting fair latent concept learning thus achieving fair and accurate predictive results.\nWe investigate the influence of the number of demonstrations during latent concept learning, denoted by q, and during inference, denoted by k, on the overall performance of FairICL. Firts, we vary q among {0, 2, 4} and include the results in Fig. 5 for LLaMA-2-13B while keeping the other parameters fixed at c = 10 and k = 4. From Fig. 5, we observe that the accuracy and F1-scores remain fairly unchanged across different values of q. However, the SP and EO values are noticeably higher at q = 4 with the best metrics observed at q = 2. Since the q demonstrations"}, {"title": "Size of Augmented data", "content": "for training are obtained from the original dataset containing biased examples, training prompts constructed with more biased samples negatively affect the fairness during inference. On the other hand, having fewer demonstrations does not affect model utility as the augmented samples preserve useful correlations from the original dataset.\nWe then vary k among {2, 4, 6, 8} for LLaMA-2-13B with fixed q = 2 and c = 10, and include the results in Fig. 6. Here, the fairness metrics demonstrate a sharper decline as the number of demonstrations during inference increases. We also observe a slight decrease in the utility metrics as the number of demonstrations during inference increases most likely due to the trade-off between utility and fairness. Since the k demonstrations are obtained from the top 100 training examples ranked by the fair latent concept variable, having a larger k allows the inference prompt to strongly guide the LLM towards fairer predictions.\nIn this section, we conduct a sensitivity analysis of \u00f1, the size of D relative to n, the size of D, to evaluate the influence of the augmented dataset on FairICL's performance. We vary \u00f1 as {0, 25, 50, 100}% of its original size of 30,000 generated samples. We fix the other parameters q at 2, c at 10, and k at 10 to perform latent concept learning and obtain results for LLaMA-2-13B shown in Fig. 7. Note that the n = 0% setting corresponds to the LatentConcept baseline method in Table 2. From the results, we notice that the accuracy and F1-scores are generally unchanged the more augmented examples are included in the training prompt. This indicates that the data augmentation process in FairICL does not negatively affect an LLM's predictive performance. We further observe significant drops in the fairness metrics as the size of D used for latent concept learning is increased. This demonstrates the positive impact of the data augmentation strategy in FairICL."}, {"title": "6 Conclusion", "content": "We have investigated the issue of fairness in large language models during in-context learning for classification on tabular datasets. We focused on a latent concept learning framework that optimizes the demonstration selection process for improved model utility via latent concept variables learned"}]}