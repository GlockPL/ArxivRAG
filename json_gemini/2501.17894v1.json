{"title": "Progress in Artificial Intelligence and its Determinants", "authors": ["Michael R. Douglas", "Sergiy Verstyuk"], "abstract": "We study long-run progress in artificial intelligence in a quantitative way. Many measures, including traditional ones such as patents and publications, machine learning benchmarks, and a new Aggregate State of the Art in ML (or ASOTA) Index we have constructed from these, show exponential growth at roughly constant rates over long periods. Production of patents and publications doubles every ten years, by contrast with the growth of computing resources driven by Moore's Law, roughly a doubling every two years. We argue that the input of AI researchers is also crucial and its contribution can be objectively estimated. Consequently, we give a simple argument that explains the 5:1 relation between these two rates. We then discuss the application of this argument to different output measures and compare our analyses with predictions based on machine learning scaling laws proposed in existing literature. Our quantitative framework facilitates understanding, predicting, and modulating the development of these important technologies.", "sections": [{"title": "Introduction", "content": "The rapid advance of artificial intelligence (AI) and machine learning (ML) is taking even the experts by surprise. Given its massive costs and potential impact on so many human activities, it is important to understand the factors which control this progress. Much discussion of this topic asserts that computational resources are the dominant factor, and their exponential growth (Moore's First Law, [48]) is the primary driver of this progress (e.g., [62]). Is this the only relevant factor? Can one formalize this relation and make it quantitative? What can we say about the future?\nWe begin by bringing together a variety of input measures, taking into account computational hardware resources as well as human intellectual work. We then standardize and compare a variety of output measures, starting from the traditional publications and patents. Additionally, popular ML benchmarks provide objective measures for specific ML models, but do not individually capture the overall growth of the field. To address this we construct a new, exhaustive index, which we call the Aggregate State of the Art in ML (ASOTA) Index, and validate it by comparison with the other output measures. The ASOTA Index is defined in terms of ML benchmarks in a way which respects their basic properties, but unlike individual benchmarks can be continued indefinitely into the future to include yet undiscovered advances.\nTo facilitate understanding, prediction and rational resource allocation, we then develop and contrast two models of the relationship between inputs and outputs: an approach centered on the concept of a production function such as [17] that may be understood as a mechanism which combines inputs like computational resources or AI developers' time and produces outputs such as new ML models, and a framework based on the ML scaling laws developed in [20, 32, 39] and many other works that describes an empirical \u201cblack-box\u201d relationship between compute and ML model performance measures. We quantitatively confirm the belief that Moore's Law is the dominant factor driving progress in AI, but that its role is much more nuanced than traditionally assumed and is better captured by the former model. In particular, this highlights the contribution of human intelligence to pushing the AI frontier further.\nFor previous work on the productivity and costs of computing, see [50, 57]; a related discussion of the dynamics and drivers of technological progress can be found in [12] and [25]."}, {"title": "Measures of inputs and outputs", "content": "Moore's First Law is generally stated as \u201cthe number of transistors in an integrated circuit doubles about every two years.\u201d While this is only approximate, and it leaves out factors such as the speed of computation (which also increased), this rough rate of exponential growth also holds for FLOP/sec per dollar [69]. We define the stock of available computational resources by multiplying the prices FLOP/sec/$ by the quantity of actual monetary investment in computing in a given time period (one year). Accumulating these investments over time and properly accounting for their depreciation, we obtain a time series $K_t$ of total computational capital plotted in Figure 1. It can not be emphasized enough that the spectacular growth in $K_t$ is almost entirely driven by the exponential decline in the price of FLOP/sec, given much more modest dynamics in investments and the high depreciation rate.\nTo obtain the computational resources devoted to AI development one would need to further multiply this figure by the fraction of total computational resources put into AI research activity, denoted $\\Phi_{AI}$. Here we take the fraction $\\Phi_{AI}$ to be constant, an assumption we critically examine later.\nTo quantify progress in AI, one can look at traditional measures of research output, such as numbers of published papers and numbers of patents. One can also look at the performance of state of the art models on standard benchmarks, such as computer chess $Y_{Elo}$ [63], language modeling $Y_{LM}$ [64] or image classification $Y_{IC}$ [23]. However, the list of relevant benchmarks changes with time (e.g., see [54]). To deal with this we have defined novel ML performance measures that combine many different benchmark performance figures following a systematic augmentation (see [14, 8, 9, 15, 2, 3, 27, 28, 16, 5, 51, 6]), including the themes of Artificial General Intelligence, singularity and existential risk [13, 51, 37, 41]."}, {"title": "A model of research productivity", "content": "We start from the basic economic approach to productivity and growth (e.g., see [60, 44, 1, 11]; as well as [56, 4]), which focuses on studying exactly this type of relation. In economic terms, computational resources are a form of capital, i.e., physical means of production (that is, durable goods which are used for producing other goods). If information and information services are goods, then machines which process information surely qualify. However, up to the present day, capital does not produce anything by itself: it must be employed by labor. Indeed, AI researchers are an essential part of the discussion, contributing their effort and cognitive, intellectual resources to the activity.\nThe simplest quantity expressing this input factor is the number of people who contribute to AI research. Thus, we measure $L_t$ as the number of people employed in occupations relevant to AI research (see Supplement for details). As motivation, one can argue in the spirit of [59] that by and large, the rate of scientific idea creation can be understood mechanistically as the number of scientists multiplied by a constant discovery rate. While this abstracts away details of organization and heterogeneity of the labor force, this is justified along the lines of [19]. In modern economics, labor is often viewed as a form of human capital, which captures the fact that different persons can have very different levels of productivity. Much like capital"}, {"title": "Differences across output measures and over time", "content": "The relation Eq. (1) with $\u03b1 = 0.20$ holds to a reasonable approximation for all of the output measures and over all time. However, a closer examination suggests that there may be significant heterogeneity in the data. While the output measures available for the full timespan ($Y_{papers}$ and $Y_{patents}$) double every 10 years, the measures available for shorter periods grow considerably"}, {"title": "Machine learning scaling laws", "content": "Let us turn to an ML-based approach to our questions. The computer science community is interested in understanding the relation between resources and performance of ML models, which turns out to follow scaling laws. Generally, this relation is studied for particular tasks, for which the usage of compute can be precisely defined and varied in controlled experiments.\nIn modern AI, the operation with the largest compute requirement is the training of a model. Consider a large language model (LLM). Its basic task is to continue a text; in other words given a sequence of words, it must predict the word most likely to follow the given sequence. An LLM is trained to do this by going through an entire corpus of text and for each word, slightly varying the LLM parameters to increase the probability of correctly predicting it. This suggests, and it is indeed the case, that the compute required to train a standard ML model is $C \\sim D\\cdot P\\cdot T$, where $C$ is measured in FLOPs, $D$ is the size of the dataset, $P$ is the size of the model (usually the number of parameters), and $T$ is an order-one factor counting the number of passes over the training data and other particulars. This relationship holds for a very wide range of models and tasks, and since the right-hand-side terms in the relationship are independent of $C$ and $Y$, they can be treated as parameters exogenously controlled in experiments (within the limits of available resources).\nScaling laws of the form $Y = C^{a'}$ have been proposed as general properties of ML systems [20, 32, 39]; they are supported by both the empirical evidence (via training a series of ML models of the same form using different compute C and measuring the performance Y, e.g. [39]) and theoretical arguments (see [10, 46]). However, while the proposed form of this scaling law"}, {"title": "Comparison of the two frameworks", "content": "The parameters $\u03b1 \\sim 0.2$ of the economic model and $a' \\sim 0.1\\pm0.05$ of the ML scaling laws both govern the relation between input compute and output performance, and have similar numerical values, suggesting that they can be directly compared. But before we do this, let us explain the differences between the frameworks.\nFirst, the two frameworks describe different relations between inputs and outputs. Our economic framework is concerned with the overall development of AI knowledge, including new techniques and ever improving models, as a function of the total computational resources and labor employed. By contrast, an ML scaling law pertains to a specific model applied to a specific task. Since the former involves many instances of the latter, selected and combined by AI researchers, and different models are not truly exchangeable, it is not clear whether the two frameworks should give the same value of the exponent.\nSecond, the measurement approach in the economics framework ascribes all the output share beyond labor to capital, while it can be argued that there are additional productive factors relevant for AI progress, such as energy. Thus, our estimate 0.2 could be viewed as an upper bound on a. Third, in the economics framework the method of calculating this parameter deals with a significantly broader definition of industry related to AI technologies than is the case for specific ML applications considered above. Fourth, there is meaningful variation over time of the inferred a in the economic applications (see studies cited in the Supplement) and in the ML framework across different studies/applications (cited above). Given all this, it is noteworthy that the two approaches produce remarkably similar measurements, $\u03b1 \\sim 0.2$ versus $a' \\sim 0.1$ (cf. the economy-wide value of 0.45)."}, {"title": "Conclusions", "content": "Our main results are twofold. First, we provide a dataset of measures of AI research output, including a new Aggregate State of the Art in ML (or ASOTA) index, which can be continued into the future and provide a solid foundation for research in this area. Second, we show that the Cobb-Douglas production function with the standard \u201ccapital\u201d input factor replaced by a computational resources factor, with an output elasticity of $\u03b1 = 0.2$, fits these output measures to reasonable accuracy over the span of five decades. The extreme simplicity of this \u201cminimal economic model\" and the absence of free parameters (recall that $\u03b1 = 0.2$ was obtained from another, entirely different data source) is remarkable.\nThe similarity of the \u201cmacro\" minimal economic model to the \u201c\u201cmicro\u201d machine learning scaling laws, both in form and in the values of the scaling exponents $\u03b1 \\sim a'$, is a further evidence for its validity. This also holds out hope for the development of structural models which relate the two levels of explanation.\nOur model can be seen as a quantification of the idea that Moore's Law has been the primary driver of progress in AI (and computer science more generally) [62]. Many authors have argued that Moore's Law has slowed down in recent years ([57, 70, 26], but also see [50, 29]), and this is consistent with the data on FLOP/sec prices and our estimate of computational capital in Figure 1. To complement these calculations, an important open question is to better estimate the compute resources actually devoted to AI research (or equivalently the fraction $\\Phi_{AI}$). There are reasons to think that this grew substantially over the period 2012 to the present, perhaps explaining the differences between output measures visible in Figure 3. Since $\\Phi_{AI} \\leq 1$, such growth cannot compensate for a slowdown in Moore's Law forever.\nCompared to previous work, we feel the most underappreciated point highlighted by the minimal economic model is the importance of the labor input factor. Its high elasticity $1 \u2212 \u03b1 = 0.8$ means that increases in labor (or human capital) translate almost fully into research output. This signals a larger need for highly skilled researchers, especially as Moore's Law slows down. This will help address the problem of jobs lost to AI and create new well paid positions.\nOne can ask whether there are other important input factors. For example, it might be that At in Eq. (1), the \u201cresearch productivity,\u201d is growing exponentially. Moreover, it is widely expected that the application of AI will drive improvements in productivity in many industries. Taking this logic further, we need to also understand the effect of AI progress on AI research productivity itself."}, {"title": "Additional Details on Methods", "content": "Let us repeat Eq. (1) from the main text:\n$Y_t = A_tK_t^\u03b1L_t^{1-a}$.\nOur model posits that the stock of the two inputs K and L are combined to produce the flow of output Y (similarly to how economists model the production of goods or the provision of services given the corresponding factors of production).\nNext, we discuss the sources of data and methods to estimate quantities of interest necessary for utilizing the Cobb-Douglas production function. Output elasticity parameters \u03b1 and (1 \u2013 \u03b1)\n(assuming Constant Returns to Scale, which is a standard approach in economic literature) are sourced from U.S. BEA statistics (more on which below); the available amounts of computational and cognitive/intellectual resources are also taken form U.S. official statistics (see below); while the measures of performance improvements come from various publicly available datasets (see below).\nCapital stock is calculated as investment flows in terms of FLOP/sec accumulated over time and accounting for depreciation (broadly following the so-called \u201cperpetual-inventory method\"). Formally,\n$K_t := (1 \u2212 \u03b4_t)K_{t\u22121} + (1 \u2212 0.5\u03b4_t)I_t$,\nwhere $I_t$ is investments in terms of FLOP/sec made available at year t, $\u03b4_t$ is the depreciation rate at t, and $K_t$ is the total amount of capital available in the economy in terms of FLOP/sec. It is measured as Investment in Private Fixed Assets for Computers and peripheral equipment sourced from BEA divided by Computing hardware costs in USD per FLOP/sec from Wikipedia (after log-linear interpolation), with prices deflated appropriately by GDP Deflator from the Federal Reserve Bank of St. Louis database. Depreciation rate is the implicit depreciation rate calculated using the formula given above with the same $I_t$ as there but with $K_t$ being Historical-Cost Net"}, {"title": "ML performance Indices", "content": "We use different models' performance results on all ML benchmark tasks and all datasets available from Papers With Code database. It starts in 1998, with first metrics improvements in 2004, and the number of available task-dataset combinations reaching 50 in 2009; currently it contains 8858 valid task-dataset combinations (with 1106 of them having at least 10 model performance entries).\nThen we construct several daily-frequency aggregate meta-measures. Below, for a performance measure $X_{it}$ for task-dataset combination i at date t, a rate of improvement $Z_{it}$ is calculated as $Z_{it} := (X_{it}/X_{i,t\u22121}) \u2212 1$ when the metrics is of the accuracy type, and $Z_{it} := 1 \u2212 (X_{it}/X_{i,t\u22121})$\nwhen the metrics is of the loss type, where $X_{it} := max_{\u03c4\u2208{1,...,t}} X_{i\u03c4}$. Specifically, we calculate:\n1. Number of Metrics included as of period t: it is defined as $N_t := \u2211_i 1(X_{it} > 0)$, i.e.,\ntask-dataset combinations with at least 1 entry by a given date;\n2. Equal-Weighted Index: define $\u2206^{EW} := \u2211_i Z_{it}/\u2211_i 1(Z_{it} > 0)$, average rate of improvement on\na given date that is weighted equally, and the index is a cumulative product of (1 + $\u2206^{EW}$)\nover t;\n3. Activity-Weighted Index: define $\u2206^{AW} := \u2211_i Z_{it} \u2211_{\u03c4=1}^t 1(X_{i\u03c4} > 0)/\u2211_i 1(Z_{it} > 0) \u2211_{\u03c4=1}^t 1(X_{i\u03c4} > 0)$,\naverage rate of improvement on a given date that is weighted by the number of performance\nentries so far, and the index is a cumulative product of (1 + $\u2206^{AW}$) over t;\n4. Equal-Weighted Expanding Index: define\n$\u2206^{EWE} := (\u2211_i Z_{it}/\u2211_i 1(Z_{it} > 0)) \u00d7 (\u2211_i 1(Z_{it} > 0)/\u2211_i 1(X_{i\u03c4} > 0))$, average rate of improvement\non a given date that is weighted equally multiplied by the number of improvements over\nthe number of metrics included so far, and the index is a cumulative product of (1 + $\u2206^{EWE}$)\nover t;\n5. Activity-Weighted Expanding Index: define\n$\u2206^{AWE} := (\u2211_i Z_{it} \u2211_{\u03c4=1}^t 1(X_{i\u03c4} > 0)/\u2211_i 1(Z_{it} > 0) \u2211_{\u03c4=1}^{\u03c4'} 1(X_{i\u03c4} > 0)) \u00d7 (\u2211_i 1(Z_{it} > 0)/\u2211_i 1(X_{i\u03c4} > 0))$,\naverage rate of improvement on a given date that is weighted by the number of perfor-\nmance entries so far multiplied by the number of improvements over the number of metrics\nincluded so far, and the index is a cumulative product of (1 + $\u2206^{AWE}$) over t;\n6. Equal-Weighted Renewing Index: define\n$\u2206^{EWR} := (\u2211_i Z_{it}/\u2211_i 1(Z_{it} > 0)) \u00d7 (\u2211_i 1(Z_{it} > 0)/\u2211_i (1(X_{i\u03c4} > 0) \u2212 1(X_{i,\u03c4\u2212365} > 0)))$, average rate\nof improvement on a given date that is weighted equally multiplied by the number of\nimprovements over the number of metrics included during the last year, and the index is a\ncumulative product of (1 + $\u2206^{EWR}$) over t;\n7. Activity-Weighted Renewing Index: define\n$\u2206^{AWR} := (\u2211_i Z_{it} \u2211_{\u03c4=1}^t 1(X_{i\u03c4} > 0)/\u2211_i 1(Z_{it} > 0) \u2211_{\u03c4'=1}^{\u03c4} 1(X_{i\u03c4'} > 0))\u00d7(\u2211_i 1(Z_{it} > 0)/\u2211_i (1(X_{i\u03c4} > 0) \u2212 1(X_{i,\u03c4\u2212365} > 0)))$,\naverage rate of improvement on a given date that is weighted by the number of performance\nentries so far multiplied by the number of improvements over the number of metrics in-\ncluded during the last year, and the index is a cumulative product of (1 + $\u2206^{AWR}$) over\nt.\nThe motivations for these measures are the following ones. The first measure, Number of\nMetrics, merely tracks the number of task-dataset combinations available at a given period. The\nfollowing two measures quantify the rate of performance improvement in available task-dataset"}, {"title": "Additional Text", "content": "Cobb-Douglas specification satisfies the mathematical properties of what is known as neoclassical production function: constant returns to scale, positive and diminishing returns to private inputs, Inada conditions on behavior at the extremes. Jones [36] shows how Cobb-Douglas production function can be derived from microeconomic foundations presuming that techniques for combining capital and labor to produce output are drawn from Pareto distributions (whose shape parameters define the production function's exponent a). This production function has some theoretically restrictive assumptions such as constant output elasticity parameters as well as empirical challenges about measurement of its inputs and identification of its parameters; but because of its analytical convenience, theoretically appealing features such as admitting a balanced growth path and a constant positive capital share with technological change not necessarily being labor augmenting as well as satisfactory empirical performance, it is the workhorse specification for long-term highly-aggregated economic analysis and forecasting (e.g., see [35] for a concise theoretical discussion and [67] for an overview of the empirical aspects).\nFrom Figure S5, we can see that the amount of labor in the AI-related sector was growing at a faster rate than in the economy overall, but the growth rate has substantially slowed down since 2000.\nIn our study, all wages are deflated by the CPI from the Federal Reserve Bank of St. Louis database. In Figure S6 one can see the time series of wages in the AI-related occupations and in the wider economy. Assuming competitive labor markets, the difference in wages is the premium to human capital in the latter sector. Surprisingly, it is remarkably stable."}]}