{"title": "ADVANCEMENTS IN VISUAL LANGUAGE MODELS\nFOR REMOTE SENSING: DATASETS, CAPABILITIES, AND\nENHANCEMENT TECHNIQUES", "authors": ["Lijie Tao", "Haokui Zhang", "Haizhao Jing", "Yu Liu", "Kelu Yao", "Chao Li", "Xizhe Xue"], "abstract": "Recently, the remarkable success of ChatGPT has sparked a renewed wave of interest in artificial\nintelligence (AI), and the advancements in visual language models (VLMs) have pushed this enthusi-\nasm to new heights. Differring from previous AI approaches that generally formulated different tasks\nas discriminative models, VLMs frame tasks as generative models and align language with visual\ninformation, enabling the handling of more challenging problems. The remote sensing (RS) field, a\nhighly practical domain, has also embraced this new trend and introduced several VLM-based RS\nmethods that have demonstrated promising performance and enormous potential. In this paper, we\nfirst review the fundamental theories related to VLM, then summarize the datasets constructed for\nVLMs in remote sensing and the various tasks they addressed. Finally, we categorize the improvement\nmethods into three main parts according to the core components of VLMs and provide a detailed\nintroduction and comparison of these methods.", "sections": [{"title": "1 Introduction", "content": "Remote sensing technology can help people obtain various observation data from a distance, and it is widely applied in\nfields such as disaster monitoring [1], urban planning [2], and agricultural management [3], surface observation [4],\nenvironmental monitoring [5], etc. Consequently, remote sensing image processing technology has been a key topic of\nfocus for researchers both domestically and internationally. Over the past decade, with the development of artificial\nintelligence technology, significant breakthroughs have also been made in remote sensing image processing techniques.\nSpecifically, various 3D convolutional neural networks (CNNs) [6] have been specifically designed for hyperspectral\nand multispectral image classification, as well as spectral reconstruction. U-Net [7] is utilized for semantic segmentation\nand land cover mapping in aerial imagery. A pyramid network has been proposed for multi-scale object detection\nin SAR images. The YOLO framework [8] is applied for small target detection in infrared remote sensing images.\nBesides, AI techniques are also applied in fields such as denoising, fusion, enhancement, and compression of remote\nsensing images, achieving remarkable results. In summary, AI techniques have significantly advanced the processing of\nremote sensing images, elevating the capabilities in these fields. However, most of these methods primarily focus on\nimage processing, and the models they employ are discriminative models. There are inherent limitations to this type of"}, {"title": "2 Foundation Models", "content": "technology, such as the inability to incorporate some human common sense, and the trained models can only perform a\nsingle vision task.\nRecently, due to advancements in data availability, computational power, and the introduction of the transformer\nmodel [9], natural language processing (NLP) has made significant breakthroughs. By feeding tens of billions of text\ntokens and scaling the model to tens of billions of parameters, ChatGPT [10] achieved amazing performance in natural\nlanguage understanding, language translation, text generation, question answering, etc. The impressive success of\nChatGPT [10] has reignited enthusiasm for AI. Larger and more advanced models continue to emerge, showcasing\nunprecedented advancements in language understanding through their extensive knowledge and sophisticated reasoning\nabilities, demonstrating human-like thinking. Specifically, Google released the encoder-only model BERT [11], which\npushed the GLUE [12] benchmark to 80.4% and achieved an accuracy of 86.7% on MultiNLI [13] and set a new record\nof 93.2 for the F1 score on the SQUAD v1.1 [14] question-answering test, surpassing human performance by 2.0 points.\nMeta released LLaMA [15], which contains a series of models for 7 billion parameters to 65 billion parameters and it\ncan beat GPT-3 [16] with 1/10 parameters. The development of large language models reveals new possibilities for\nthe advancement of artificial intelligence technology, particularly in generative AI. Compared to discriminative AI,\ngenerative AI models approach problem-solving in a more flexible and open manner, allowing for the modeling of a\nbroader range of issues. The development of LLMs also offers new insights for addressing the challenges faced by\ndiscriminative models in the computer vision field.\nThe introduction of models such as CLIP [17] has effectively bridged the modality gap between images and language,\ncreating opportunities for innovative integration. CLIP [17], with its innovative multimodal encoder-decoder structure\nutilizing BERT [11] as a text encoder and Vision Transformer (ViT) [18] / ResNet [19] as an image encoder, exemplifies\nthe synergy between images and language vocabulary, leading to improved accuracy in classification and detection\ntasks through a deep understanding of the relationship between image content and textual vocabulary . GLIP [20]\nfurther enhances this consistency between language sentences and image content. Furthermore, the emergence of\nVLM has fundamentally addressed numerous constraints associated with traditional discriminative models in remote\nsensing image processing, thereby catalyzing the transition to AI 2.0. Generally speaking, VLM is a type of model\ndesigned to integrate and understand both visual and textual information, enabling tasks such as image captioning,\nvisual question answering, and cross-modal retrieval by leveraging the relationships between images and language.\nCompared to previous discriminative models, VLM offers greater flexibility in modeling various AI tasks, allowing it\nto handle multiple tasks within a single framework. Additionally, its multimodal perceptual ability brings it closer to\nhuman perception. In the past two years, both LLMs and VLMs have developed rapidly, with various stronger models\nbeing introduced, such as LLaMA [15], LLaVA [21], and GPT-4 [22], among others. The performance of related tasks\nhas also been continually improved.\nSimilar to previous discriminative AI technologies, which have significantly improved the field of remote sensing data\nprocessing, VLM\u2014one of the core technologies in AI 2.0\u2014also brings new opportunities for this domain. VLMs, by\nleveraging their multimodal capabilities, have demonstrated significant progress across various tasks within remote\nsensing, including geophysical classification [23, 24], object detection [25, 26], and scene understanding [27, 28]. By\nintroducing LLaVA-1.5 [21] into the remote sensing field, Kuckreja et al. proposed GeoChat [25], which not only\nanswers image-level queries but also accepts region inputs for region-specific dialogue [25]. Deng et al. designed\nChangeChat, which utilizes multimodal instruction tuning to handle complex queries such as change captioning,\ncategory-specific quantification, and change localization [29]. Inspired by GPT-4 [22], Hu et al. developed a high-\nquality Remote Sensing Image Captioning dataset (RSICap) to facilitate the advancement of large VLMs in the remote\nsensing domain [30]. However, overall, the application of VLM technology in the field of remote sensing is still in its\nearly stages. Current VLM-based remote sensing data processing technologies have already demonstrated significant\npotential, but there are still many issues that need to be addressed."}, {"title": "Contribution", "content": "In this paper, we present a comprehensive review of VLM-based remote sensing applications, offering a\nthorough delineation of the principal methodologies and the most recent advancements in VLMs, particularly within\nthe context of remote sensing. By providing an in-depth introduction to the most representative and influential works in\nthis domain, the survey enables researchers to quickly acquire a nuanced understanding of the developmental trajectory\nof these methodologies, facilitating the classification and evaluation of different approaches. Furthermore, the survey\nextends its contribution by presenting a detailed classification of enhancement techniques, focusing on key components\nsuch as vision encoders, text encoders, and the alignment between vision and language. This classification not only\nsystematically illustrates the research progress made within this specialized field but also serves as a valuable reference\npoint, supporting and inspiring further innovative research endeavors by scholars aiming to explore and advance the\nboundaries of VLM applications in remote sensing."}, {"title": "Survey Pipeline", "content": "This survey is organised as follows. The section 2 presents an convice introduction to foundation\nmodels, including Transformer, Vision Transformer and the famious VLM-LLaVA. The section 3 outline the datasets"}, {"title": "2.1 Transformer", "content": "The emergence of the Transformer has changed the paradigm in the NLP field and laid the foundation for the subsequent\ndevelopment of LLMs. As shown in Figure 1, the Transformer consists of an encoder and a decoder, each comprising\nan attention layer (also known as the token mixer layer) and a Feed Forward layer (FFN, sometimes referred to as the\nchannel mixer layer). The attention layer allows for the exchange of information among all input tokens, while the Feed\nForward layer facilitates information exchange across channels.The FFN layer consists of two linear layers. The width\nof the first linear layer is set to 4\u00d7 (the scale factor) that of the input tokens, while the second linear layer matches the\nwidth of the input tokens. The attention mechanism is formulated as follows:\n$Attention(Q, K, V) = Softmax(\\frac{QKT}{\\sqrt{dk}})V$ (1)"}, {"title": "2.2 Vision Transformer", "content": "Inspired by the success and popular of transformer used in NLP, the Vision Transformer (ViT) model was proposed\nlater and tailored for computer vision field. As shown in Figure 2, ViT is composed of three components, that is Linear\nProjection of Flattened Patches(Embedding layer), Transformer Encoder and MLP Head. In summary, to enable NLP\nmodels to process images, the images are serialized by cropping them into uniform-sized patches. These patches are\nthen transformed into features using a projection layer. To ensure the model is sensitive to the positional information of\nthe image patches, position embeddings are added to the corresponding patch embeddings. Currently, there are four\ntypes of position embedding methods, including 1) absolute position embedding; 2) relative position embedding (RPE)\n[31]; 3) conditional position embedding (CPE) [32]; 4) rotary position embedding (RoPE) [33]. The ViT adopts the\nfirst one.\nCompared to convolutional networks, ViT has a significant advantage in scaling up ability. Its performance on image\nprocessing does not saturate quickly as the model size increases, allowing for better utilization of larger models and\nenhanced capabilities in handling more complex tasks. This characteristic has sparked a surge of research interest\nin the ViT architecture, leading to the continuous emergence of various improved versions of ViT models, such as\nSwinTransformer [34], Deit [35], etc. So far, the model paradigms in both the NLP and CV fields have been unified,\nwhich has laid the foundation for the emergence of VLM."}, {"title": "2.3 Vision Language Model", "content": "The introduction of the Transformer has significantly advanced language processing, while the emergence of ViT has\nunified the processing paradigms in NLP and CV. CLIP [17] further broke down the modal barriers between language\nand images. Building on these foundations, VLMs have been proposed by merging ViT and LLMs. Figure 3 shows the\narchitecture of LLaVA [21], a very representive VLM model. From Figure 3, we can see that the core idea of LLaVA is"}, {"title": "3 Datasets", "content": "Whether it is the previous discriminative models or the current generative models, they all fall under data-driven models.\nTherefore, VLM-based remote sensing image processing methods also require corresponding training data as support.\nFigure 5 compares the datasets used in previous discriminative models with those used in current VLMs. From the\nfigure, we can observe that the most significant improvement is that VLM datasets include both image and language\nannotations.\nIn the remote sensing field, existing VLM datasets can be broadly categorized into three types based on their production\napproaches:\n\u2022 Manual Datasets, which are completely manually annotated by humans based on specific task requirements.\n\u2022 Combining datasets, which are build by combing several existing datasets and adding part new language\nannotations.\n\u2022 Automatically annotated datasets. This type of data construction involves minimal human participation and\nrelies on various multimodal models for filtering and annotating image data.\nIn this section, we will delve into the construction methods and applicable task domains of existing remote sensing\nVLMs datasets, offering insights into how these datasets support extensive applications and in-depth research within the\nfield. The statistical infromation of current existing datasets are summarized in Table 1"}, {"title": "3.1 Manual Datasets", "content": "Manually curated datasets, though limited in size, are typically of high quality and are specifically designed for tasks.\nThere are three high-quality datasets made by hand, which are meticulously crafted to address specific challenges within\nthe remote sensing domain.\nHallusionBench [37], the first benchmark designed to examine visual illusion and knowledge hallucination of large\nvisual language models(LVLMs) and analyze the potential failure modes based on each hand-crafted example pair,\nconsists of 455 visual-question control pairs, including 346 different figures and a total of 1129 questions on diverse\ntopics. RSICap [30], RSIEval comprises five experts annotated captions with rich and high-quality information, thus"}, {"title": "3.2 Combining Datasets", "content": "As shown in Table 1, some datasets are built from scratch, whose sources are usually from Sentinel-2, Gaofen, Google\nEarth Engine and so on. But most datasets created by merging existing remote sensing datasets, which facilitate multi-\ntask models by integrating various data types from traditional RS domain-specific datasets, greatly eases labour-intensive\ncompared to manual production and starting from scratch. The most utilized datasets are AID [60], NWPU-RESISC45\n[61], UCM [62] for Scene Classification, FAIR1M [63], DIOR [64] for Object Detection, iSAID [65] for Semantic\nSegmentation, LEVIR-CD [66] for Change Detection, more details about those datasets could be found in section 4.\nBesides, the well-known benchmark dataset Million-AID [67] is frequently included."}, {"title": "3.3 Automatically Annoteted Datasets", "content": "The explosive development of VLMs and LLMs has provided efficient methods for constructing large-scale remote-\nsensing visual language datasets. This category, which is rapidly becoming the mainstream, leverages the power of\nVLMs and LLMs to construct large-scale, high-quality datasets.\nRS5M [26] employs the general multimodal model BLIP2 [68] to generate image captions and then uses CLIP [17]\nto select the top 5 highest-scoring captions. SkyScript [27] matches Google Images with the OpenStreetMap (OSM)\ndatabase and selects relevant image attribute values from OSM to concatenate them with CLIP [17] as image captions.\nLHRS-Align [54] uses a similar approach to extract multiple attribute values from OSM and then summarizes them\nusing Vicuna-13B [69] to produce long and fluid captions. GeoChat [25] specifically provide system instructions as"}, {"title": "3.4 Summary", "content": "prompts that ask Vicuna [69] to generate multi-round question and answer pairs. GeoReasoner [55] obtaines image-text\ndata pairs for geolocation from the communities of the two games, utilizes BERT [11] to clean and filter text that lacked\ngeolocation-specific information, and then uses CLIP [17] to align the image and text to construct a highly locatable\nStreet View dataset. HqDC-1.4M [46] utilized the \u201cgemini-1.0-pro-vision\" [70] to generate descriptions for images\nfrom multiple public RS datasets, thereby obtaining a dataset of image-text pairs to serve as the pretraining data for\nRSVLMs. ChatEarthNet [56] carefully designed prompts that embed semantic information from the land cover maps to\nmake ChatGPT-3.5 and ChatGPT-4v achieve the best results on satellite imagery for the caption generation. VRSBench\n[57] did a similar job and spent thousands of hours manually verifying the generated annotations. FIT-RS [58] used the\nefficient TinyLLaVA-3.1B [71] to swiftly generate concise background descriptions for numerous RSIs, then combined\nthem as the prompt to obtain detailed and fluent sentences using GPT-4/GPT-3.5. Examples, which utilize advanced\nmodels like BLIP2[68] and CLIP [17] to generate detailed image-text pairs, supporting complex applications such as\ndisaster monitoring, urban planning, and environmental analysis.\nThe existing three types of datasets each have their own advantages and disadvantages, as detailed below:\n\u2022 Manually annotated datasets generally have higher quality and are most closely aligned with specific problems.\nBut, these datasets typically consisting of only a few hundred or thousand images, are limited in scale compared\nto combined datasets and VLM datasets, which can reach tens of millions of images. The size of a dataset is\ncrucial for model training, as larger and richer datasets yield more accurate and versatile models. Manually\nannotated datasets can be used to fine-tune large models, but they struggle to support training large models\nfrom scratch.\n\u2022 Combining datasets. By merging existing datasets, large-scale datasets can be constructed at a low cost. This\ntype of data can easily reach millions of entries. However, datasets created in this manner typically have\nlower annotation quality compared to manually annotated datasets and may not fully align with specific tasks.\nNonetheless, they can be utilized for model pre-training."}, {"title": "4 Capabilities", "content": "In various tasks within the field of remote sensing, VLMs have demonstrated powerful capabilities. These capabilities\ncan be broadly classified into two categories: pure visual tasks and visual language tasks. Pure visual tasks focus on\nanalyzing remote sensing images to extract meaningful information about the Earth's surface. Visual language tasks,\nwhich combine natural language processing with visual data analysis, have opened new avenues for remote sensing\napplications. The zero-shot and Few-shot tasks are SC, OD, SS and IR, which are described in more detail below."}, {"title": "4.1 Pure visual", "content": "\u2022 Scene Classification(SC) RS scene classification entails the classification of satellite or aerial images into\ndistinct land-cover or land-use categories with the objective of deriving valuable insights about the Earth's\nsurface. It offers valuable information regarding the spatial distribution and temporal changes in various\nland-cover categories, such as forests, agricultural fields, water bodies, urban areas, and natural landscapes.\nThe three most commonly used datasets in this field are AID [60],NWPU-RESISC45 [61] and UCM [62],\nwhich contain 10,000, 31,500, 2,100 images, and 30,45,21 categories, respectively.\n\u2022 Object Detection(OD) RSOD aims to determine whether or not objects of interest exist in a given RSI and to\nreturn the category and position of each predicted object. In this domain, the most widely used datasets are\nDOTA [72] and DIOR [64]. DOTA [72] contains 2806 aerial images which are annotated by experts in aerial\nimage interpretation, with respect to 15 common object categories. DIOR [64] contains 23463 images and\n192472 instances, covering 20 object classes, much more details than DOTA [72]."}, {"title": "4.2 Combine with natural language", "content": "\u2022 Semantic Segmentation(SS) Semantic Segmentation is a computer vision task in which the goal is to categorize\neach pixel in an image into a class or object. For RS images, it plays an irreplaceable role in disaster assessment,\ncrop yield estimation, and land change monitoring. The most widely used datasets are ISPRS Vaihingen and\nPotsdam, which contain 33 true orthophoto (TOP) images and 38 image tiles with a spatial resolution of 5 cm,\nrespectively. Another widely used dataset iSAID [65] contains 2806 aerial images, which mainly collected\nfrom Google Earth.\n\u2022 Change Detection(CD) Change detection in remote sensing refers to the process of identifying differences in\nthe structure of objects and phenomena on Earth surface by analysing two or more images taken at different\ntimes. It plays an important role in urban expansion, deforestation, and damage assessment. LEVIR-CD\n[66], AICD [73] and the Google Data Set focus on changes of buildings. Other types of changes such as\nroads, groundwork are commonly included in CD datasets.\n\u2022 Object Counting(OC) RSOC aims to automatically estimate the number of object instances in a RSI. It plays an\nimportant role in many areas, such as urban planning, disaster response and assessment, environment control\nand mapping. RemoteCount [24] is a manual dataset for object counting in remote sensing imagery consisted\nof 947 image-text pairs and 13 categories, which are mainly selected from the validation set of the DOTA\ndataset.\n\u2022 Image Retrieval(IR) IR from RS aims at retrieving the interested RS images from the massive RS image\nrepositories. With the launch of more and more Earth observation satellites and the emergence of large-scale\nremote sensing datasets, RS image retrieval can prepare auxiliary data or narrow the search space for a large\nnumber of RS image processing tasks.\n\u2022 Visual Question Answer(VQA) VQA is a task that seeks to provide answers to free-form and open-ended\nquestions about a given image. As the questions can be unconstrained, a VQA model applied to remote sensing\ndata could serve as a generic solution to classical problems but also very specific tasks involving relations\nbetween objects of different nature. VQA was first proposed by Anto et al. [74] in 2015, and then made it\ngreatly developed in the RS domain by RSVQA [59] in 2020. It contributed a RSVQA framework and two\nVQA datasets, RSVQA-LR and RSVQA-HR, which wildly utilized in further works.\n\u2022 Image Captioning(IC) IC aims to generate natural language descriptions that summarizes the content of an\nimage. It requires representing the semantic relationships among objects and generating an exact and descriptive\nsentence, so that it's more complex and challenge than image detection, classification, and segmentation tasks.\nIn this domain, the commonly used datasets are Syndney-Captions [75], RSICD [76], NWPU-Captions [77],\nand UCM-Captions [75]. Recently, CapERA [78] provids UAV video with diverse textual descriptions to\nadvance visual-language-understanding tasks.\n\u2022 Visual Grounding(VG) RSVG aims to localize the referred objects in remote sensing images with the guidance\nof natural language. It was first introduced in [79] in 2022. In 2023, Yang [42] et al. not only build the new\nlarge-scale benchmark of RSVG based on detection in DIOR dataset, termed RSVGD, but design a novel\ntransformer-based MGVLF module to address the problems of scale variation and cluttered background of RS\nimages.\n\u2022 Remote Sensing Image Change Captioning(RSICC) A new task aiming at generating human-like language\ndescriptions for the land cover changes in multitemporal RS images. It is a combine of IC and CD task,\noffering important application prospects in damage assessment, environmental protection, and land planning.\nChenyang et al. [80] firstly introduced the CC task into the RS domain in 2022 and proposed a large-scale\ndataset LEVIR-CC containing 10077 pairs of bitemporal RS images and 50385 sentences describing the\ndifferences between the images. In 2023, they propose a pure Transformer-based model [81] to further improve\nthe performance of the RSICC task.\n\u2022 Referring Remote Sensing Image Segmentation(RRSIS) RRSIS is to provide a pixel-level mask of desired\nobjects based on the content of given remote sensing images and natural language expressions. It was firstly\nproposed by Zhenghang et al. [43] in 2024, who created a new dataset, called RefSegRS, for this task. In\nthe same year, Sihan et al. [48] curated an expansive dataset comprising 17,402 image-caption-mask triplets,\ncalled RRSIS-D, and propose a series of models to meet the ubiquitous rotational phenomena in RRSIS."}, {"title": "5 Recent Advances in Visual Language Models for Remote Sensing", "content": "The invention of Visual Language Models (VLMs), has greatly changed the remote sensing field which mainly focus\non geospatial analysis. With the development and usage of various sensors, conventional vision models can not\nhandle such complex data type, while VLMs technique could enhance the accuracy and efficiency of remote sensing\nanalysis. Specifically, the combination of visual and linguistic data within VLMs allow more subtle and sophisticated\nCurrently, there are mainly two types of models that integrate vision and language: one is based on contrastive learning\nstructures, such as the CLIP series, and the other is based on large language models that fuse visual features, like\nLLaVA. As shown in Figure 7. Accordingly, the current improvements can also be categorized into two major types:\nenhancements within the contrastive learning framework and improvements within the conversational framework."}, {"title": "5.1 Advancements in Contrastive Vision-Language Models", "content": "Contrastive VLMs primarily consist of an image encoder and a text encoder, with the core challenge being the alignment\nof features extracted by both encoders. The image encoder, utilizing a convolutional neural network (CNN) or Vision\nTransformer (ViT), encodes images into feature vectors. Concurrently, the text encoder processes textual descriptions\ninto corresponding feature vectors. The model is trained to maximize the similarity between positive image-text pairs\nwhile minimizing the similarity between negative pairs. CLIP is a pioneering model in vision-language pre-training,\nemploying InfoNCE loss to align text and image features in a shared space. Currently, in the field of remote sensing,\ncontrastive VLMs focus on transferring the image-text alignment capabilities established in RGB images and text to\nremote sensing images and text, which are then utilized to address various downstream tasks, such as change detection\nand remote sensing image caption,\nBy converting annotations from 10 object detection datasets, 4 remote sensing semantic segmentation datasets, and 3\nremote sensing image-text datasets using box-to-caption and mask-to-box conversions, Liu et al. created a combined\nimage-caption dataset and fine-tuned the CLIP model on this newly constructed dataset through continual learning [24].\nSimilar to [24], GeoRSCLIP builds a remote sensing dataset consisting of 5 million image-text pairs by filtering existing\nimage-text pair datasets and generating captions for image-only datasets. It then fine-tunes CLIP on this proposed\ndataset [26]. CRSR [82] incorporates a Transformer Mapper network, utilizing an attention mechanism and learnable\nsemantic predictive queries to provide a comprehensive representation of image features. This approach effectively"}, {"title": "5.2 Advancements in Conversational Vision-Language Models", "content": "Currently, conversational VLMs are particularly noteworthy for their ability to address complex remote sensing tasks\nby transforming intricate visual features into a format that LLMs can comprehend. By integrating advanced image\nencoders that adeptly capture subtle variations in remote sensing imagery with text encoders optimized for efficiently\nprocessing language instructions, these models demonstrate remarkable performance, particularly in few-shot learning\nscenarios. The alignment layer is pivotal in establishing connections between remote sensing imagery and LLMs.\nTechniques such as utilizing MLPs for feature projection and incorporating learnable query embeddings for task-specific\nimage feature extraction have proven effective in elevating the overall performance of VLMs.\nLeveraging the existing API of pre-trained LLMs, conversational VLMs extract features from image data using a visual\nencoder. These features are then transformed into a format that LLMs can interpret, enabling the completion of various\nvisual language tasks. To enhance task performance, researchers meticulously designed each component, which is\ndescribed in detail below. Figure 8 clearly shows the model details of conversational VLMs.\nImage Encoder Table 3 presents the visual encoders commonly used in current VLM implementations. Most studies\nutilizes the standard CLIP image encoder [17], but RSGPT [30] and SkyEyeGPT [44] have explored other variants,\nEVA-CLIP encoder [88], which is trained with improved training techniques.\nAdditionally, EarthGPT[45] adopt the vision transformer (ViT) as the image encoder to extract multi-layer features,\nwhich is beneficial to capture subtle differences in RS images and provides a more comprehensive understanding of\nimages. Then the CNN backbone is designed as the image encoder to integrate multi-scale local details into visual\nrepresentation. GeoChat [25], in addition to using CLIP-ViT(L-14) as the visual backbone network, interpolates the\npositional encoding in the transformer-based CLIP model with high-resolution 512x512 to understand details presented\nin remote sensing imagery (e.g., small objects and object details).\nText Encoder Training large language models (LLMs) from scratch is highly costly. Therefore, conversational visual\nlanguage models (VLMs) utilize pre-trained LLMs as text encoders. These LLMs process visual elements extracted\nby the visual encoder and converted by middleware, along with the researcher's questions or language instructions, to\ngenerate the desired results. Currently, the academic community is focusing on the LLaMA [21] and Vicuna [69] series,\nwhich serve as text encoders for most conversational LLMs. Table 3 provides a summary of these models. Common\napproaches include directly using LLMs or efficiently fine-tuning them with Low-Rank Adaptation (LoRA) [91]. Liu\net al. [81] designed a complex image classifier and proposed a multi-prompt learning strategy to generate multiple\nlearnable prompt embeddings as prefixes for LLMs to enhance their utilization for the RSICC task. RS-LLaVA [89]\nand RSGPT [30] demonstrated the capabilities of the proposed architecture using two different LLMs (vicuna-7B and\nvicuna-13B). The results indicated that fine-tuning larger language models leads to higher performance but at the cost\nof longer training times and higher computational resource requirements.\nVision-Language Alighment In the architecture of conversational Vision-Language Models (VLMs), the middle\nconnection layer plays a crucial role. It acts as a bridge between remote sensing images and Large Language Models\n(LLMs), projecting image information into a space that LLMs can comprehend, enabling them to understand complex\nremote sensing (RS) tasks. A common approach to bridge the modal gap is to use Multi-Layer Perceptrons (MLPs),\nwhich employ single linear layer [44, 45] or double layers [46, 58, 89] to project visual tokens and align feature"}, {"title": "5.3 Others", "content": "In addition to the aforementioned approaches, as shown in Table 4, several other advancements have been made to\naddress domain-specific tasks in remote sensing. Txt2Img [95] employs modern Hopfield layers for hierarchical\nprototype learning of text and image embeddings, achieving notable performance in generating text descriptions for\nremote sensing images. CPSeg [96] enhances image segmentation by integrating a novel \"chain of thought\" process\nthat leverages textual information associated with the image. SHRNet [97] utilizes a hash-based architecture to extract\nmulti-scale image features, while QRN improves alignment efficiency and enhances the visual-spatial reasoning\nability of remote sensing visual question answering (VQA) systems. MGeo [98], a multi-modal geographic language\nmodel, introduces the geographic environment (GC) as a new modality, effectively extracting multi-modal correlations\nfor accurate query-POI matching. GeoCLIP [99], with its independently designed Location Encoder, is the first to\ntackle worldwide geo-localization via an image-to-GPS retrieval approach by explicitly aligning image features with\ncorresponding GPS locations. SpectralGPT [100], based on the MAE[101] architecture, provides a foundational\nmodel for the spectral data domain, supporting various downstream tasks. TEMO [102] enhances the word embedding\ncomponent of VLMs, capturing long-term dependencies from description sentences of varying lengths, generating text\nmodal features for each category, and employing a masking strategy to prevent overfitting. This method is particularly\neffective for few-shot object detection (FSOD) in text-modal knowledge-oriented remote sensing imagery."}, {"title": "5.4 Performance Comparison", "content": "In this section, we present a comparative analysis of various Visual-Language Model (VLM) pre-training methods,\nfocusing on their performance across common datasets. It is important to note that we have prioritized performance"}, {"title": "6 Conclusions and Future Work", "content": "This review provides a systematic overview of Vision-Language Models (VLM) in the field of remote sensing. It\nintroduces milestone works in the development of VLM itself", "directions": "n\u2022 Addressing regression problems. Currently, the mainstream approach in VLM involves using a tokenizer\nto vectorize text, then aligning visual information with the text word vectors before feeding them into the\nLLM model. This approach results in the model being insensitive to numerical values. For example, the\nnumber 100 may be tokenized as '1', '00', or it could be split into '10', '0', or '100'. This leads to suboptimal\nperformance when the current framework is applied to certain regression tasks. Future research on how to\nintegrate regression problems into the current framework is a topic worth exploring.\n\u2022 Aligning with the structural characteristics of remote sensing images. Current VLM efforts in the remote\nsensing field largely follow the overarching framework established in the computer vision domain, without\nfully leveraging the unique characteristics of remote sensing images. For instance, remote sensing images\ncome in various types, such as SAR images, HSI, and radar images, each with distinct features. Designing\nfeature extractors that cater to these data characteristics is essential for maximizing the advantages of the\nimages. However, in the current remote sensing VLM work, visual encoders primarily rely on pre-trained\nfeature extractors based on RGB images, which do not fully align with remote sensing images. Additionally,\nthere is a lack of research considering the fusion of different modalities of remote sensing images to enhance\nthe performance of VLM.\n\u2022 Multimodal output. Existing VLM models in the remote sensing field typically output only text. While\nthey can handle traditional tasks like image classification, they struggle with dense prediction tasks such\nas segmentation, where text output is insufficient. Therefore, researching multimodal outputs-allowing\nVLM models to also output images or even videos is a highly"}]}