{"title": "Norm Growth and Stability Challenges in Localized Sequential Knowledge Editing", "authors": ["Akshat Gupta", "Christine Fang", "Atahan Ozdemir", "Maochuan Lu", "Ahmed Alaa", "Thomas Hartvigsen", "Gopala Anumanchipalli"], "abstract": "This study investigates the impact of localized updates to large language models (LLMs), specifically in the context of knowledge editing - a task aimed at incorporating or modifying specific facts without altering broader model capabilities. We first show that across different post-training interventions like continuous pre-training, full fine-tuning and LORA-based fine-tuning, the Frobenius norm of the updated matrices always increases. This increasing norm is especially detrimental for localized knowledge editing, where only a subset of matrices are updated in a model. We reveal a consistent phenomenon across various editing techniques, including fine-tuning, hypernetwork-based approaches, and locate-and-edit methods: the norm of the updated matrix invariably increases with successive updates. Such growth disrupts model balance, particularly when isolated matrices are updated while the rest of the model remains static, leading to potential instability and degradation of downstream performance. Upon deeper investigations of the intermediate activation vectors, we find that the norm of internal activations decreases and is accompanied by shifts in the subspaces occupied by these activations, which shows that these activation vectors now occupy completely different regions in the representation space compared to the unedited model. With our paper, we highlight the technical challenges with continuous and localized sequential knowledge editing and their implications for maintaining model stability and utility.", "sections": [{"title": "Introduction", "content": "Recent advances in model interpretability have led to methods that perform localized updates to large language models by intervening at very specific locations (Meng et al. 2022a,b; Hernandez, Li, and Andreas 2023). A popular domain in which such methods are employed is knowledge editing (Yao et al. 2023) - a task where singular facts are added or updated inside of a model in a data and compute efficient manner. This paper starts with a simple yet powerful observation that sequential knowledge updates made to a model always leads to an increase in the norm of the matrix being updated. We first ask the question - is the increase in norm with continous updates specific to localized knowledge editing methods or is this a general phenomenon?"}, {"title": null, "content": "Our experiments with numerous post-training interventions, including continual pretraining, full fine-tuning and LORA-based fine-tuning, present a very surprising result - the norm of the weight matrices being updated always increases for all these post-training interventions. While there has been dispersed work showing norm growth (Merrill et al. 2020), to the best of our knowledge, our study is the first to emprically evaluate this comprehensively for large number of important post-training interventions.\nWe next study the presence and implications of this phenomenon when performing localized updates. To do so we study the task of knowledge editing where localized updates are very common. Knowledge editing methods usually update specific parts of the model, for example the MLP sub-modules of certain layers, to add or update new information. This allows data and compute efficient updates to be made to a model. We perform continuous sequential knowledge edits to a model using various parameter modifying knowledge editing methods along with localized fine-tuning, and find that for all these scenarios, the norm of the edited matrix always increases with the number of updates. While the increasing norm may not be concering in general, it is especially detrimental for performing localized updates. This is because disproportionate and contionuous growth in the norm of one or few layers of a model, while the rest of the model remains frozen, will compromise the balance and stability of the entire system, eventually leading to a breaking point as observed in prior work (Gupta, Rao, and Anumanchipalli 2024; Gupta, Baskaran, and Anumanchipalli 2024). This disprortionate growth is shown in Figure 3.\nWe further analyze the effects of performing continous localized knowledge updates to a model by studying how the hidden activations of the model changes. We find that contrary to the increasing norm of edited matrix, the norm of the activation vectors generated after the edited layers continuously decreases. We also show that these activation begin to occupy different regions in space when compared to the original. A follow-up to our work by Gupta et al. (2025) resolve the problems of disproportionate norm growth by using regularization methods and propose a more robust knowledge editing method called ENCORE.\nTo summarize, we make the following contributions in this paper:"}, {"title": null, "content": "1. We show that the frobenius norm of the updated weight matrices always increases during post training interventions.\n2. The norm of edited matrix increases disproportionately for localized knowledge updates, leading to model collapse.\n3. This collapse is accompanied by a change in the norm and orientations of the resultant hidden activations, showing that the activations of the edited models now lie in a different region of representation space."}, {"title": "Norm Growth During Post-Training Interventions", "content": "We focus on the following common interventions that are applied on a model after the pretraining step - continual pre-training, full fine-tuning, LORA based fine-tuning (Hu et al. 2021). We discuss our experiment settings for each of the following below:\n1. Continued pretraining (CPT) - We consider continued pretraining as as separate case from full fine-tuning although in both cases all the weights of the model are updated using a next-token prediction loss. We define continued pretraining as a process where the foundational knowledge of a model is extended by training on a large domain-specific corpora. In our experiments, we present the results for performing CPT on 20 billion tokens for Python programming (Li et al. 2023a) on Llama-2 (7B) (Touvron et al. 2023).\n2. Full Fine-Tuning (FFT) - We define full fine-tuning as task specific next-token prediction training of a model to optimize the model's parameters on a particular task. We present the results for fine-tuning Llama-2 (7B) model on 110k question answer pairs for programming (Wei et al. 2024).\n3. LORA based full fine-tuning (LFFT) - Here we use LORA (Hu et al. 2021) to fine-tune all the model weights in the same setting as FFT.\nFor each of the above, we present post-training intervention results using the checkpoints provided in the study by Biderman et al. (2024). In each of the above cases, the model update equation can be written as:\n$$W_{new} = W_{old} + \\Delta W$$    (1)\nThus, the norm of the new weight matrix does not necessarily have to decrease and can lie in the range as shown below using the triangle inequality:\n$$||W_{old}||_F - |\\Delta W|_F| < |W_{new}|_F < |W_{old}|_F + |\\Delta W|_F$$   (2)\nwhich means that after each update, the norm of the matrix being updated can both decrease or increase. Yet we find that the norm of the updated matrix in each of these interventions always increases. This can be seen in Figure 1. The frobenius norm of all three MLP and attention matrices"}, {"title": "Localized Updates during Knowledge Editing", "content": "Knowledge editing is defined as the task of making data and compute efficient knowledge updates to large language model without compromising their general ability (Yao et al. 2023; Kolbeinsson et al. 2024). In this paper, we focus on parameter-modifying knowledge editing methods, where knowledge is updated by changing the weights of the model (Meng et al. 2022a,b; Gupta, Sajnani, and Anumanchipalli 2024). The compute efficient component of knowledge editing comes from the fact that usually only one or a few layers of a model are updated when incorporating new knowledge. In this paper we focus on four popular knowledge editing methods - ROME (Meng et al. 2022a), MEMIT (Meng et al. 2022b), MEND (Mitchell et al. 2021) and PMET (Li et al. 2023b). We perform sequential edits using these methods on GPT2-XL (1.5B) (Radford et al. 2019) and GPT-J (6B) (Wang and Komatsuzaki 2021) for 2000 edits and analyze the different properties of the updated matrices. The list of layers updated for the different model editing algorithms can be seen in Table 1."}, {"title": "Effect on Norm of Internal Activations", "content": "To understand further effects of the increase norm of the edited matrix, we look at its effect on the corresponding activations that are output from that layer. To do so, we send 1 million tokens of wikipedia articles through both GPT2-XL and GPT-J, and study the norm and orientations of the internal activations of the model before and after editing. Specifically, we are looking at the residual stream vectors after each layers inside an LLM. The norm of the activation before and after editing can be seen in Figure 4. In this figure, we present the norm of the activation vectors for GPT2-XL when editing the model using ROME, comparing the unedited model with the model after 100, 500 and 2000 edits. We see that the activation norms remains the same until layer 17 for all cases, but slowly starts to decrease as we edit the model more. After 2000 edits, the norm of the activation vectors at layer 40 is much almost have that of the original enedited model, showing that the decrease in norm compounds as the activation vectors pass through the model. In contrast to the increasing norm of the edited weight matrices, the average norm of the activations decreases post editing. While in this paper we do not analyze the implications of this, it is studied in more detail in Gupta et al. (2025). They show that the norms of activation vectors generated from the edited matrices increase while the norms of activations from the subsequent layers decreases, resulting in an increased importance of vectors generated from those layers."}, {"title": "Effect on Orientation of Internal Activations", "content": "We further analyze the orientations of the activation vectors. To do so, we create classifiers between the residual stream vectors of layers \"i\" versus \u201cj\u201d. Each classifier is a simple binary logistic regression classifier. A classifier $C_{ij}$ is a binary classifier between two groups of vectors - residual stream vectors between layer i and residual stream vectors between layer j. The classifier is trained with 800k training vectors and tested on 200k test vectors, equally divided into the two classes. This classifiers are trained only on the activations of the un-edited model. The accuracy of the classifier can be seen in Figure 5 (a). We see that the simple binary classifiers reach almost a 100% accuracy when classifying the activation vectors from different layers. This shows that the activation vectors of different layers are linearly separable from each other. When a classifier is trained with activation vectors from the same layers, which forms the diagonal line in Figure 5 (a), we see that the classification accuracy is 50% or at random. This shows that the activation vectors between the same layers are not linearly separable, stengthening the linear seprability claim.\nWe save the classifiers trained on the un-edited model and use them to clasify the activation vectors of the edited model. Figure 5 shows this for ROME, where we analyze the output for 200k activation vectors when passed through a model sequentially edited for 100, 500 and 2000 edits. The cell $i-j$ corresponds to the case where classifier $C_{i,j}$, trained on the activations coming from the $i^{th}$ and $j^{th}$ layer of the un-edited model, are used to classifiy the activations of the $j^{th}$ layer of the edited model. We want to re-iterate to the reader that the graphs in Figure 5 are not supposed to be symmetrical by definition. We will give an example below. For cell 3-5, we use the classifier $C_{3,5}$ trained between activations of layer 3 and layer 5 of the un-edited model, but use it to classify the activations of layer 5 at test time. For cell 5-3, we use the same classifier $C_{3,5}$, but this time we use it to classify the activations of layer 3.\nFor cell $i - j$, if the orientations in space of the activation vectors of the edited models remains the same as that of the unedited model, the classifier $C_{i,j}$ would assign it the class $j$. But we see in Figure 5 that this is not the case. The test time accuracy of the classifier begins to get worse for activation vectors right after the edited layer. This shows that the activation vectors are not only changing in norms but also the regions they occupy in space. This is so much so that the activations coming out of layer j of the edited model after 2000 edits now lie in a region for layer i, leading to an incorrect classification accuracy for classifier $C_{i,j}$.\nWith this study, we show that the increasing norm not only changes the norm of the internal activations of the model, but also the orientations in space in which these vectors lie. We observe that post-editing, the internal activations now lie in a very different region in space. The region is so different that a linear classifier trained to identify the activation vectors of a specific layer is now unable to identify the output activations from the specific layer."}, {"title": "Conclusion", "content": "This study highlights a critical challenge in the domain of localized updates for large language models: the persistent increase in the norm of updated matrices during sequential knowledge editing. While this phenomenon appears universal across post-training intervention methods, its implications are particularly pronounced in localized knowledge editing, where only specific parts of the model are modified. The resulting imbalance leads to downstream performance degradation, as evidenced by changes in both the norm and orientation of internal activations. Our findings emphasize the need for innovative strategies to address these challenges, paving the way for more robust and sustainable approaches to localized knowledge editing in LLMs. This work serves as a foundational step towards understanding and mitigating the inherent limitations of current techniques, with the ultimate goal of enabling dynamic and scalable updates to pre-trained models. A follow-up work by Gupta et al. (2025) overcomes these highlighted limitations by using appropriate regularizations with knowledge editing, allowing long-term sequential knowledge editing."}]}