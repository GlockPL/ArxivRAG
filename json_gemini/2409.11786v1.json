{"title": "Efficient Low-Resolution Face Recognition via Bridge Distillation", "authors": ["Shiming Ge", "Shengwei Zhao", "Chenyu Li", "Yu Zhang", "Jia Li"], "abstract": "Face recognition in the wild is now advancing towards light-weight models, fast inference speed and resolution-adapted capability. In this paper, we propose a bridge distillation approach to turn a complex face model pretrained on private high-resolution faces into a light-weight one for low-resolution face recognition. In our approach, such a cross-dataset resolution-adapted knowledge transfer problem is solved via two-step distillation. In the first step, we conduct cross-dataset distillation to transfer the prior knowledge from private high-resolution faces to public high-resolution faces and generate compact and discriminative features. In the second step, the resolution-adapted distillation is conducted to further transfer the prior knowledge to synthetic low-resolution faces via multi-task learning. By learning low-resolution face representations and mimicking the adapted high-resolution knowledge, a light-weight student model can be constructed with high efficiency and promising accuracy in recognizing low-resolution faces. Experimental results show that the student model performs impressively in recognizing low-resolution faces with only 0.21M parameters and 0.057MB memory. Meanwhile, its speed reaches up to 14,705, 934 and 763 faces per second on GPU, CPU and mobile phone, respectively.", "sections": [{"title": "I. INTRODUCTION", "content": "ALTHOUGH face recognition techniques become nearly mature for several real-world applications, they still have difficulties in handling low-resolution faces and being deployed on low-end devices [1]. These demands are very important for tasks like video surveillance and automatic driving. In general, most well-known face recognition models [2]\u2013[6] are trained from massive high-resolution faces by using sophisticated architectures that contain huge parameters, making them uneconomical to deploy. Moreover, these models may be not suitable to directly apply on low-resolution scenarios due to the different distribution between high-resolution training faces (sometimes from private datasets) and low-resolution ones. An important reason is that the high-resolution facial details will be missing during the degeneration of the resolution, which the existing models largely depend on. An alternative way is to train a new model on massive low-resolution faces in target scenarios (e.g., surveillance faces in the wild). However, collecting and labeling such faces is very time and labor consuming. Moreover, directly training on low-resolution faces usually suffer from unsatisfactory accuracy [7], since the reduction of image resolutions may lose some valuable knowledge which can be provided from some pretrained models [8]. Therefore, it is necessary to fully exploit the knowledge from massive high-resolution faces and pretrained models for facilitating low-resolution face recognition. To recognize low-resolution faces, some feasible ideas based on hallucination or embedding are proposed to exploit high-resolution knowledge.\nThe \"hallucination\" idea is based on the fact that a person who is familiar with a high-resolution face can recognize the low-resolution counterpart. Several existing approaches propose to hallucinate the high-resolution faces before recog-nition by explicitly reconstructing details, such as e.g., with super-resolution [9]\u2013[12]. Among them, the mapping between high-resolution to low-resolution faces is modeled by some carefully designed parametric functions (e.g. nonlinear La-grangian [13], SVD [9] and sparse representation [10]). During inference, parametric coefficients that best fit the given low-resolution faces are computed and adopted to recover the missing details to make recognition easier. These approaches generally can achieve good recognition accuracy, while the additional reconstruction often brings in computational burden and slows down the recognition speed.\nDifferent from the hallucination-based approaches, the \u201cem-bedding\" idea applies an implicit scheme to directly project low-resolution faces into an embedding space that mimics high-resolution behaviors. In this way, only the high-resolution features are encoded into the model without explicit recon-struction. For example, Biswas et al. [14] embedded low-resolution facial images into an Euclidean space, in which image distances well approximate the dissimilarities among high-resolution images. Ren et al. [15] projected face images of different resolutions into a unified space for coupled match-ing. A shared characteristic of such embedding models is to encode informative high-resolution details into low-resolution features via cross-resolution analysis. They are generally model-specific and not designed to mimic an existing high-resolution model whose pre-learned knowledge is not fully adopted. However, the pre-learned knowledge often contains rich high-resolution details and can guide low-resolution face recognition if being properly adapted and transferred.\nKnowledge distillation is an efficient way to transfer knowl-edge via the teacher-student framework [16]\u2013[21]. In the framework, the teacher is usually a strong yet complex model that performs well on its private dataset, while a much simpler student is learned to mimic the teacher's behavior, leading to maximal performance preservation and speed improvement. The key of knowledge distillation is the trade-off between speed and performance, and such a technique provides an opportunity to convert many complex models into simple ones for practical deployment. However, most of existing distillation approaches assume that teacher and student training are restricted on the same dataset or the same resolution, which is not suitable in many real scenarios where a well-pretrained teacher on an existing dataset would like to be reused to supervise model training on a new dataset. In [1], Ge et al.proposed a selective knowledge distillation ap-proach to transfer the most informative knowledge from pre-trained high-resolution teacher to a lightweight low-resolution student by solving a sparse graph problem, which actually performed cross-resolution distillation so that the accuracy of low-resolution face recognition can be improved. However, the selected knowledge may not be optimal in adapting on the training faces.\nIn summary, transferring the knowledge from high-resolution to low-resolution models is helpful and can avoid the computationally-intensive reconstruction. Thus, a learning framework to help recognize low-resolution faces should be able to effectively transfer informative high-resolution knowl-edge in a principle manner. That is to say, It need to actually solve two subproblems: what knowledge should be trans-ferred from the high-resolution models and how to perform such transfer. In this way, the challenges in low-resolution face recognition and knowledge distillation are simultaneously addressed with a single framework.\nInspired by that, as shown in Fig. 1, we propose a novel bridge distillation approach that can convert existing high-resolution models pretrained on their private datasets into a much simpler one for low-resolution face recognition on target dataset. In our approach, public high-resolution faces and their resolution-degraded versions are used as a bridge to compress a complex high-resolution teacher model to a much simpler low-resolution student model via two step distillation. The first cross-dataset distillation adapts the pretrained knowledge from private to public high-resolution faces. It learns a feature map-ping that preserves both the discriminative capability on public high-resolution faces as well as the detailed face patterns encoded in the original private knowledge. Then, the second resolution-adapted distillation learns a student model in a multi-task fashion to jointly mimic the adapted high-resolution knowledge and recognize the public low-resolution faces, which are synthesized to simulate the probable distribution of the target low-resolution faces. In this way, the student model only needs to be aware of the high-resolution details that are still discriminative on low-resolution faces regardless of the others, resulting into compact knowledge transfer.\nThe contributions are summarized as follows: 1) we propose a bridge distillation framework that is able to convert high-resolution face models to much simpler low-resolution ones with greatly reduced computational and memory cost as well as minimal performance drop; 2) we propose cross-dataset distillation, which adapts the pre-learned knowledge from pri-vate to public high-resolution faces that preserves the compact and discriminative high-resolution details; 3) comprehensive experiments are conducted and show that the student models achieve comparable accuracy with the state-of-the-art high-resolution face models, but with extremely low memory cost and fast inference speed."}, {"title": "II. RELATED WORKS", "content": "In this section, we first briefly review the development of low-resolution face recognition models, then introduce knowledge distillation directions which are tightly correlated with the proposed approach in greater details."}, {"title": "A. Low-Resolution Face Recognition", "content": "Recently, deep learning approaches have motivated many strong face recognition models, e.g. [5], [6], [22]\u2013[24]. How-ever, the performance of these models may drop sharply if applied to low-resolution faces. An important reason is that the high-resolution facial details will be missing during the degeneration of the resolution, which the existing models largely depend on. To address this problem, several recent works propose to adopt two ideas to handle low-resolution recognition: hallucination and embedding, which reconstructs the high-resolution details explicitly or implicitly during in-ference.\nIn the hallucination category, high-resolution facial details are explicitly inferred and then utilized in the recognition process. For example, Kolouri et al. [13] proposed to fit the low-resolution faces to a non-linear Lagrangian model, which explicitly considers high-resolution facial appearance. Jian et al. [9] and Yang et al. [10] instead adopted SVD and sparse representation to jointly performing face hallucination and recognition. Several works [25], [26] were able to gen-erate highly realistic high-resolution face images from low-resolution input. Cheng et al. [12] introduced a complement super- resolution and identity joint deep learning method with a unified end-to-end network architecture to address low-resolution face recognition. Although hallucination is a direct way to address low-resolution face recognition, it is usually computationally intensive as it introduces face reconstruction as a necessary pipeline. Li et al. [27] introduced a GAN pre-training approach and fully convolutional architecture to im-prove face re-identification and employed supervised discrim-inative learning to explore low-resolution face identification. Recent face recognition models proposed designing effective loss functions [5], [23], [28]\u2013[31] for feature learning, that can facilitate the hallucination-based approaches.\nIn contrary, the embedding category implicitly encodes high-resolution features in low-resolution computation to avoid high-resolution reconstruction. This inspires several embedding-based approaches, which project both high-resolution and low-resolution faces into a unified feature space so that they can be directly matched [32]\u2013[34]. This paradigm can be implemented by jointly transforming the features of high-resolution and low-resolution faces into a unified space using multidimensional scaling [35], joint sparse coding [36] or cross-resolution feature extraction and fusion [37], [38]. Recently, deep learning were adopted for low-resolution face recognition in [7] and [39]. However, most of these works focus on joint high- and low-resolution training, while the distillation of high-resolution models for low-resolution tasks is still an open problem.\nFrom these works, we find transferring the knowledge from high-resolution to low-resolution models is helpful and can avoid the computationally-intensive face reconstruction. Thus, we need to actually solve two subproblems: what knowledge should be transferred from the high-resolution models and how to perform such transfer. Therefore, we also briefly review knowledge distillation studies."}, {"title": "B. Knowledge Distillation and Transfer", "content": "Knowledge distillation [16] is a useful way that utilizes a strong model to supervise a weak one, so that the weaker model can be improved on the target task and carry out domain adaptation [40]. In particular, the teacher-student framework is actively studied [17], [18], [20], [41], [42], where the teacher model is usually a strong yet complex model that performs well on its private dataset. Knowledge distillation will learn a new and much simpler student model to mimic the teacher's behavior under some constraints, leading to maximal performance preservation and speed improvement. In this manner, the learned student model can recover some missing knowledge that may not be captured if it is trained independently. Among them, Luo et al. [18] proposed to distil a large teacher model to train a compact student net-work. In their approach, the most relevant neurons for face recognition were selected at the higher hidden layers for knowledge transfer. Su and Maji [43] utilized cross quality distillation to learn models for recognizing low-resolution images. Lopez-Paz et al. [19] proposed the general distillation framework to combine distillation and learning with privileged information. There are some approaches that focus on the tasks of continue or incremental learning. In [44], Rebuffi et al.introduced iCaRL, a class-incremental training strategy that allows learning strong classifiers and a data representation simultaneously by presenting a small number of classes and progressively adding new classes. In [45], Li and Hoiem proposed Learning without Forgetting method that uses only new task data to train the network while preserving the original capabilities. Typically, these continue or incremental learning approaches are different from those feature extraction and fine-tuning adaption techniques as well as multitask learning that uses original task data.\nTo sum up, the core of knowledge distillation is the trade-off between speed and performance, and such a technique provides an opportunity to convert complex models into simple ones that can be deployed in the wild. However, most of these works assume that teacher and student training are restricted on the same dataset or the same resolution. In many real scenarios, we would like to reuse a well-pretrained teacher model on an existing dataset to supervise model training on a novel dataset, which cannot be directly handled by existing approaches. Moreover, we attempt to use such a learning framework to help recognize low-resolution faces, since informative high-resolution knowledge can be effectively transferred in a principle manner. In this way, the challenges in low-resolution face recognition and knowledge distillation are simultaneously addressed with a single framework."}, {"title": "III. THE PROPOSED APPROACH", "content": "Our bridge distillation approach is an intuitional and general framework (see Fig. 2) to convert an existing high-resolution model to a simpler one, which is expected to work well for low-resolution recognition. The framework basically follows the teacher-student framework [16] but with two-step distil-lation. A complex teacher model is first trained on the high-resolution face recognition task, then distilled to a simpler student model for low-resolution scenario. The framework refers to the following notations.\nDomains. The framework involves three domains: 1) pri-vate domain refers to an external high-resolution private face dataset $I_p$ for training the teacher model, which is usually large and unnecessary to be visible to the framework, 2) public domain is the public face dataset $I_s$ and used as a bridge to learn a simple student model by adapting and mimicking the teacher's knowledge, and 3) target domain refers to the deployment scenario for recognizing unseen low-resolution faces $I_T$ (e.g., surveillance faces). Without loss of generality, we assume the face distribution in the target domain is observable but the labels are not available.\nTeacher model. In the proposed framework, the teacher model is assumed to be off-the-shelf, meaning that it is pretrained on $I_p$. In general, the pretrained teacher model encodes rich and general knowledge of high-resolution face recognition. When applied to a novel dataset (e.g., $I_T$), it is thus desirable to transfer the pre-learned high-resolution knowledge across datasets rather than retraining the model from scratch. Note that it differs from many teacher-student frameworks [16]\u2013[18], [20], [41], [42], where training is restricted on the private dataset or the same resolution or available target dataset. In this work, we assume that the teacher model $M_t$ is in the form $M_t = (F_t,S_t)$, composed by a feature extraction backend $F_t$ and a softmax classifi-cation layer $S_t$. This is a widely applied architecture which most face classification models (e.g. [5], [6]) conform. Thus, given an input face image $I$, high-level features are first computed with $f_t(I) = F_t(I; w_t)$, which are processed via $s_t(I) = S_t(f_t(I); w_t)$ to obtain classification scores. Here, $w_t = [w_t^f; w_t^s]$ denotes the model parameters. The design of $F_t$ and $I_T$ can be blind to the proposed approach.\nStudent model. The student model has much simpler design than that of the teacher. By training the student to mimic the teacher's behavior on the public dataset $I_s$, the model's complexity is largely reduced. In practical settings, our aim is to obtain an optimized student model $M_s$ that works well on the low-resolution target dataset $I_T$. Usually $I_s$, $I_T$ and $I_p$ do not share identities, i.e. $I_s \\cap I_T = \\phi$ and $I_s \\cap I_p = \\phi$. The difference in resolution also exists between private and target datasets. Thus, the problem is how to properly transfer the high-resolution knowledge well-learned from $I_p$ to facilitate low-resolution recognition on $I_T$ by using $I_s$. Such transfer needs to address the distribution and resolution differences simultaneously.\nFormulation. Given the above settings, low-recognition face recognition can be formulated as an open-set domain adaptation problem [46] in cross-resolution context, where the goal is to achieve effective knowledge transfer from the discriminative high-resolution teacher $M_t (I; w_t)$ to a simpler low-resolution student $M_s (I'; w_s)$, with huge domain shift between the private dataset $I_p$ and the target dataset $I_t$. Here, the two datasets usually do not share identities, leading to the difference in characteristics distribution. Moreover, the extra resolution differences further adds to the domain shift. Therefore, the knowledge transfer needs to address the distri-bution and resolution differences simultaneously. To this end, we introduce public source faces $I_s$ as bridging domain and propose bridge distillation to perform transfer via two step distillations: 1) cross-dataset distillation adapts the pre-learned knowledge in high-resolution faces from private domain to public domain and distils it to compact features, and 2) resolution-adapted distillation transfers the knowledge from high-resolution public faces to their low-resolution versions by taking the adapted features as supervision signals to train the student model. In this context, $I_s$ could be easily achieved from many public benchmarks, meanwhile many off-the-shelf face recognition models are available and can serve as teachers. Therefore, our approach provides an economical way to learn an efficient model for facilitating low-resolution face recognition."}, {"title": "B. Cross-dataset Distillation", "content": "Cross-dataset distillation addresses the subproblem that what knowledge should be transferred from the high-resolution models. It is used to compress and adapt the teacher's knowledge learned from $I_p$ to $I_s$ with an adaptation process which should take two considerations into account. First, it should preserve high-resolution knowledge learned from $I_p$, while selectively enhancing them to be discriminative on $I_s$ so that the correct high-resolution knowledge could be extracted from $I_s$ with the teacher. Second, the adapted features should be compact so that the student can mimic them with extremely limited resources.\nPrevious works show that directly training on low-resolution face images usually suffer from unsatisfactory recognition ac-curacy [7]. The rationale of this degeneration is that important face details gradually lose when image resolution reduces. To mitigate this problem, high-resolution face knowledge should be encoded into the trained model, so that detail reconstruction can be implicitly performed during the recognition process.\nIn our setting, the teacher model $M_t (I; w_t)$ learned on the rich high-resolution dataset $I_P$ is usually strong for rec-ognizing high-resolution faces from its own dataset. Here $I$ represents the input high-resolution face image, and $w_t$ is the concatenation of model parameters. We expect the teacher's knowledge can be transferred to the student model $M_s (I'; w_s)$, whose input is a low-resolution face image $I'$ that comes from separate datasets $I_s$ or $I_T$. However, we assume that $I_s$ also contains high-resolution faces, so that the teacher network can learn the high-resolution details. For better clearness, we use the notation $I$ and $I'$ to represent a high-resolution and low-resolution face image, respectively. Given that $I_p$ and $I_s$ can have distinct distributions, we propose to first adapt the teacher's knowledge learned from $I_p$ to $I_s$. The adaptation process should take two considerations into account. First, it should preserve the high-resolution knowledge learned from $I_p$, while selectively enhancing them to be discriminative on the public dataset $I_s$. Second, the adapted features should be compact, so that the student model can mimic them with extremely limited computational resources.\nTo implement this idea, we instantiate the adaptation func-tion as a small sub-network $F_a (f_t (I); w_a)$ that maps the high-dimensional features $f_t (I)$ into a reduced feature space. By plugging this adaptation module into the teacher model, optimal feature mapping can be learned in a data-driven fashion. Given the adapted features $f_a (I) = F_a (f_t (1); w_a)$, we assume that a simple softmax classifier $S_a (f_a (I); w_a^s)$ recognizes the faces in the public dataset $I_s$ well. Denote $M_a = (F_a, S_a)$ as the full adaptation module, where $w_a = [w_a^f; w_a^s]$ are its parameters to be learned. We propose to train the adaptation module with the following objective, which can be deemed as a variant of knowledge distillation [16], [19],\n$\\min_{W_a} C (w_a, I_s) + \\lambda D (w_a, I_s).$\n(1)\nThe proposed objective is a balanced sum of a classification loss $C$ and a distillation loss $D$, with $\\lambda$ as the balancing weight. For classification on the high-resolution public faces, we have\n$C (W_a, I_s) = \\sum_{I \\in I_s} l (M_a (f_t (I); w_a), y (I)),$\n(2)\nwhere we adopt the widely used cross-entropy classification loss $l(., .)$, and $y (I)$ is the one-hot groundtruth identity for input image 1. The distillation loss $D$ enforces the adapted features to mimic the original features\u2019 behavior on face classification. To this end, we first fine-tune the teacher\u2019s softmax layer on $I_s$, obtaining $S_t (f_t (I); w_t^*)$, where $w_t^*$ denotes the retrained layer weights. One can think of $S_t$ as a feature selector that preserves the discriminative components of the originally learned knowledge for recognizing faces in $I_s$. Then the distillation loss is given as:\n$D (W_a, I_s) = \\sum_{I \\in I_s} l (M_a (f_t (I); w_a), \\hat{s} (I)),$\n(3)\nwhere $\\hat{s} (I) = S_t (f_t (I); w_t^*) /T$ and $T$ is the temperature [16] for softening the softmax outputs. Eq.3 uses cross-entropy to approximate the softened teacher's softmax features, which ensures the adaptation process bringing in the additional teacher's knowledge learned from $I_p$. In addition to the recog-nition capacity regarding $I_s$ given by Eq.2, the knowledge of the teacher network is retained. As a result, the adapted features mimic the impact of the discriminative selection of the original high-resolution features, but with greatly reduced dimensions. After training the adaptation module $M_a$, we discard the softmax layer and only retain the adapted and reduced features $f_a (I) = F_a (f_t (I); w_a^f)$ as supervision for training the student model.\nDifference from Other Distillation Approaches. Different from classic distillation approaches [16], [19], the proposed bridge distillation approach goes through two steps of dis-tillations: the first step adapts the pretrained complex model to the public dataset, and the second step learns to mimic it with a simpler model. As the first step distils the model itself cross private and public datasets, we call the proposed algorithm cross-dataset distillation. Thus, the teacher need not to be fully trained on the public dataset. Directly retraining the teacher on the public dataset not only costs a lot of time, but may also overfit to the dataset and lose the previously learned knowledge. By contrast, the proposed adaptation approach preserves such information, and is much faster to train.\nDifference from Other Learning Approaches. Recent continue or incremental learning approaches [44], [45] focus on using new task data to train the network while preserving the knowledge learned from original task data. By contrast, our setting of cross-dataset distillation mainly performs knowl-edge adaptation rather than knowledge preservation such that the teacher capacity can be transferred to public domain, which can facilitate the knowledge alignment between high-resolution and low-resolution instances. In this way, the ca-pacity on high-resolution recognition can be preserved while the adapted knowledge into public high-resolution dataset can avoid being contaminated, leading to effective knowledge transfer. We also note that our cross-dataset distillation com-bines knowledge adaptation and knowledge transfer together, which is carried out by fine-tuning and transfer learning."}, {"title": "C. Resolution-adapted Distillation", "content": "Resolution-adapted distillation addresses the subproblem that how to perform knowledge transfer from high-resolution faces to low-resolution ones. Given the high-resolution knowledge adapted to the public dataset, we ask the student to approximate them during inference. Since the capacity of the student is weak, the feature layer that mimics the adapted knowledge should be sufficiently deep. Empiri-cally, we find that the mimicking layer is best inserted before the identity layer for softmax classification, as shown in Fig. 2.\nBased on this design, we divide $M_s$ to $M_l$ and $M_h$, i.e. the lower and the higher part, and its parameters $w_s$ to $w_l$ and $w_h$ accordingly. The lower part $M_l$ corresponds to the main feature branch till the mimicking layer, and $M_h$ consist of the rest layers. The student model is trained using the following objective\n$\\min_{W_h,W_l} C (w_s, I_s) + R (w_l, I_s),$\n(4)\nwhere face classification and feature regression are combined in a unified multi-task learning task. For the classification loss $\\hat{C}$ we still adopt the cross-entropy, but this time perform train-ing on the degraded low-resolution versions of face images from the public dataset $I_s$:\n$\\hat{C} (w_s, I_s) = \\sum_{I \\in I_s} \\sum_{I' \\in \\mathcal{D}(I)} l (M_s (I'; w_s), y (I)),$\n(5)\nwhere $\\mathcal{D}(I)$ denotes the set of images degraded from $I$. The regression loss $R$ is defined as\n$R(w_s, I_s) = \\sum_{I \\in I_s} \\sum_{I' \\in \\mathcal{D}(I)} ||M_l (I'; w_l)-F_a (f_t (I); w_a^f) ||_2^2.$\n(6)\nAfter training, the teacher model and the adaptation module are discarded. During inference, the student model takes as input a low-resolution face image and outputs its classified identity features or labels, depending on the task."}, {"title": "D. Implementation Details", "content": "The proposed framework is designed to be flexible, so that in principle the teacher model can take form of any models or their ensemble as long as they end up with a softmax layer for face classification. Note that many state-of-the-art models [2], [5], [6], [47] meet this assumption. In this work, we adopt two most recent architecture VGGFace2 [6], CosFace [5] with a 112 \u00d7 112 input resolution and a 1024 embedding feature dimension, and their ensemble in the teacher model for example. VGGFace2 is pretrained on massive high-resolution face images from the VGGFace2 dataset [6] and works at resolution 224 \u00d7 224, while CosFace is pretrained on CASIA-WebFace dataset [48] and a large-scale private dataset. No retraining or fine-tuning is performed on these datasets during evaluation. Also, we assume that their datasets are private, i.e., they cannot be accessed by the proposed approach.\nFor the student model, we design a light-weight architecture similar to the ones proposed in [49], [50]. As shown in Fig. 2, it takes as input a low-resolution face image. We train the student model using various resolutions of $p \\times p$, where $p = \\{96,64, 32, 16\\}$. The architecture has ten convolutional layers, three max pooling layers and three fully connected layers, interleaved by ReLU non-linearities. Several 1\u00d71 convolution layers are intersected between 3 \u00d7 3 ones to save storage and improve inference speed. Two skip connections are established to enhance the information flow. Global average pooling is used to make final prediction so that the architecture can handle arbitrary resolutions. With this architecture, the amount of parameters is only 0.21M, which is only 0.81% or 0.57% of the teacher's size (26M for VGGFace2 or 37M for CosFace).\nOur adaptation module takes the features before the teacher model's softmax layer as input. It has two fully connected layers with 512 and 128 units, respectively. More complex architecture can be used at the cost of additional training time. All the weights in the student model and the adaptation module are initialized via Xavier's method. During training, the student model is first pretrained on the low-resolution faces in $I_s$, then fine-tuned with the supervision adapted from the high-resolution knowledge extracted by the teacher model. Training images from $I_s$ are downsampled by different factors to simulate degeneration at various levels. Batch normaliza-tion is performed to accelerate convergence speed. We use stochastic gradient descent to train the student models. In all the experiments, the batch size and learning rate are set as 256 and 0.001, respectively."}, {"title": "IV. EXPERIMENTS", "content": "To validate the proposed approach, we conduct extensive ex-periments on three challenging public benchmarks: UMDFaces [51] dataset, LFW (Labeled Faces in the Wild) [52] dataset and UCCS (UnConstrained College Students) [53] dataset, for different evaluation tasks.\nUMDFaces serves as the public dataset $I_s$, which contains 367,888 images in 8, 419 subjects. To generate high-resolution public images, faces are normalized into 224 \u00d7 224 and 112\u00d7112 sizes for learning the VGGFace2 and CosFace adap-tation module. Low-resolution public face images are achieved by randomly perturbing the localized facial landmarks for 16 times, normalizing to various resolutions of $p \u00d7 p$ and degrading to approximate the distribution of target faces (e.g., blurring, changing illumination, etc). Among all these high-and low-resolution images, 80% of them are randomly selected for training and the rest for evaluating.\nLFW is used for target dataset, where 6, 000 pairs (including 3,000 positive and 3,000 negative pairs) are selected to evaluate various models on face verification task. To this end, the images are resized according to the model input. We extract the features from the mimicking layer and the identity layer of the student model for each input pair. Cosine similarity is computed for verification using a threshold. This experiment aims to show that better supervision from the high-resolution models can help generate better features in supervising the training of student models.\nUCCS contains 16,149 images in 1,732 subjects. It is a very difficult dataset with various levels of challenges, includ-ing blurred image, occluded appearance and bad illumination. The identities in the training and testing datasets are exclusive. It is widely used to benchmark face recognition models in unconstrained scenario. Thus, on this dataset we compare the proposed approach with the state-of-the-art models on face identification task, following the standard top-K error metric [54].\nGenerally, the high-resolution face images are available from many public datasets (e.g., UMDFaces), which fulfills the training of student models by transferring knowledge or recognition ability from high-resolution public faces to low-resolution ones synthesized according to the distribution of target faces. As a result, the trained student models could recognize low-resolution face images in the wild (even the high-resolution images are not available in this case, such as surveillance face images in UCCS). All the experiments across this paper are conducted using a NVIDIA K80 GPU, a single-core 2.6HZ Intel CPU and a mobile phone with a Qualcomm Snapdragon 821 processor implemented with TensorFlow framework [55]."}, {"title": "A. Evaluating Adaptation on UMDFaces", "content": "In the first experiment, we aim to look into the improvement brought by the proposed adaptation module via cross-dataset distillation. To this end, we evaluate four cases: 1) No cross-dataset distillation. In this case, the pretrained 2,048 or 1,024 dimensional features extracted by the VGGFace2 or CosFace teacher model are directly used to supervise the student model. 2) Direct feature compression without distillation. This setting adapts the pre-learned teacher's knowledge to the low-resolution setting with classification loss only, but excludes the distillation loss. 3) The proposed cross-dataset distillation, which equals to the formula (1) that combines both losses. 4) Mixed-dataset distillation. In this case, we assume that the private high-resolution dataset can be accessible and used for training. In our experiment, we random select the training instances in 1,000 subjects from private high-resolution dataset (VGGFace2 dataset for VGGFace2 model or CASIA-WebFace dataset for CosFace model) and combine it with UMDFaces training set as a mixed training dataset, and then train a larger 9,419-way classifier to adapt the teacher knowledge into public high-resolution dataset.\nIn the last three cases, features are reduced to 128 dimen-sions by adaptation. Fig.3 shows the recognition accuracy of these four cases on UMDFaces, indicating that feature com-pression via cross-dataset distillation has a better performance than direct compression. Moreover, performance only slightly drops from case 1 to 3, especially cross-dataset distillation on the ensemble of two teachers has a 0.15% drop against the supervised student model by VGGFace2 without cross-dataset distillation, while features are greatly reduced 128 dimensions, thus greatly compressing the redundancy and saving resource consumption. In addition, the trained classifier for mixed-dataset distillation achieves the accuracies of 91.87% and 93.27% with VGGFace2 and CosFace models, respectively. It shows that our cross-dataset distillation only has a very small drop in accuracy without ensemble, while achieving an accuracy improvement with ensemble. This implies that our approach can perform effective knowledge adaptation even not accessing the private dataset."}, {"title": "B. Evaluating Face Verification on LFW", "content": "In the second experiment, we extensively evaluate the performance of the proposed approach on face verification task under various input resolutions $p = \\{96,64, 32, 16\\}$, supervision signals $x = \\{c,s,dc,sc\\}$ and distilled teachers $T = \\{O,V,C,E\\}$. Here, the abbreviations of supervision sig-nals stand for only class-level supervision (c), cross-dataset distillation when discarding class-level supervision (s), direct distillation in addition to class-level supervision (dc) and cross-dataset distillation in addition to class-level supervision (sc), respectively. In the"}]}