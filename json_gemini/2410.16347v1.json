{"title": "Domain-Adaptive Neural Posterior Estimation for Strong Gravitational Lens Analysis", "authors": ["Paxson Swierc", "Marcos Tamargo-Arizmendi", "Aleksandra \u0106iprijanovi\u0107", "Brian D. Nord"], "abstract": "Modeling strong gravitational lenses is prohibitively expensive for modern and next-generation cosmic survey data. Neural posterior estimation (NPE), a simulation-based inference (SBI) approach, has been studied as an avenue for efficient analysis of strong lensing data. However, NPE has not been demonstrated to perform well on out-of-domain target data e.g., when trained on simulated data and then applied to real, observational data. In this work, we perform the first study of the efficacy of NPE in combination with unsupervised domain adaptation (UDA). The source domain is noiseless, and the target domain has noise mimicking modern cosmology surveys. We find that combining UDA and NPE improves the accuracy of the inference by 1-2 orders of magnitude and significantly improves the posterior coverage over an NPE model without UDA. We anticipate that this combination of approaches will help enable future applications of NPE models to real observational data.", "sections": [{"title": "Introduction and Related Work", "content": "Galaxy-scale strong gravitational lensing is a cosmic probe that provides key information about dark energy, dark matter, and galaxy evolution [4, 96, 95, 66, 49, 34, 94, 86]. Modern and future cosmic survey experiments e.g., the Dark Energy Survey (DES) [16, 30], Hyper Suprime-Cam [3, 70], the Kilo-Degree Survey (KiDS) [22, 58], the Rubin Observatory Legacy Survey of Space and Time [46], Euclid [27], JWST [83, 13], and the Nancy Grace Roman Telescope [26, 59, 103] are expected to contain 103 - 105 lensing systems [74, 87, 17]. Traditional techniques for lens modeling have relied heavily on analytic likelihood-fitting, which is computationally expensive and human-time intensive [61]. Additionally, due to simplifying assumptions in designing the likelihoods, these techniques often lack the capability of modeling non-Gaussian likelihoods and posteriors [61]. However, these techniques have advanced notably in automation and speed [73, 39, 31, 88]. Supervised deep learning-based inference techniques - including neural network regression and the recently reinvigorated simulation-based inference (SBI) [20, 38, 1, 24, 108, 35, 45] like neural posterior estimation (NPE) [75, 36, 102, 107] \u2014 have been studied in applications on a wide variety of physics and cosmology topics [21, 51, 79, 80, 8, 41, 64], including strong lensing [47]. Once these models are trained (aka, \u201camortized\u201d), these methods are very fast compared to traditional modeling methods [20]. In many areas of cosmology, including strong lensing, when there isn't enough real observational data for training deep learning-based models, realistic simulations are used [93, 71, 9, 11]. Nevertheless, these simulated data can differ significantly from real, observational data \u2014 i.e.,"}, {"title": "Methods: Lensing, Neural Posterior Estimation, Domain Adaptation", "content": "Physics of strong gravitational lensing: When light from a background object encounters a sufficiently massive lensing object on its way to an observer, the image of the background object is significantly magnified and distorted [72, 100]. This warped image is the primary observable data (see Fig. 1(b) for example images). In parametric lens modeling, one can consider > 10 parameters from the background object and the lens that could be inferred from the imaging data [61, 50, 77, 10]. In this study, we infer only five parameters related to the lens: Einstein radius \u03b8E, relative angular positions between the background object and lens (x, y), and lens eccentricity moduli (\u20ac1,1,1,2). Like all astronomical data, strong lensing images are subject to observational noise from multiple sources e.g., atmosphere, sky brightness, CCD gain, number of exposures, exposure time, CCD readout, and photon counting. These noises can add values to pixel counts or cause blurring in the images; they need to be accounted for in model building to avoid systematic bias and large error bars.\nNeural Posterior Estimation (NPE): To infer parameter posterior densities, we employ NPE [36], which uses a CNN-based embedding network to summarize images into features, which are then passed to a Masked Autoregressive Flow (MAF), a combination of an autoregressive model and a normalizing flow [75], to estimate posterior densities. MAF can estimate posterior distributions of arbitrary shape (i.e., non-Gaussian). In the standard NPE-only approach, there is a single loss function LNPE that takes the form of the negative log posterior volume [36].\nUnsupervised Domain Adaptation (UDA): In UDA methods, the source domain data have labels, and the target domain data do not have labels. Common UDA approaches include adversarial methods [91, 69, 40, 54, 48, 32] and distance-based methods [105, 28]. In distance-based methods, the loss is defined as a multi-dimensional distance between latent features from the source and target domain data. In this work, we use distance-based methods, for which the UDA loss function LUDA is the Maximum Mean Discrepancy (MMD) [37]. MMD is a method that calculates the distance between distributions: when applied to the latent feature space, it can be used as a loss function. In [84], MMD was used as a metric to quantify the NPE model misspecification. When MMD is included as a loss during training, it is intended to cause the network to align latent feature spaces for the source and target data; this leads to the extraction of domain-invariant features and enables the model to work well on both.\nCombining NPE and UDA: We combine NPE and UDA methods via their losses. The UDA loss LUDA is calculated using the source and target domain latent features (without labels) at the end of the embedding network. The NPE loss LNPE is calculated using the source data (with labels) at the end of the MAF. The total loss function LTot = LNPE + BUDA * LUDA is used with gradient descent to update all weights; BUDA is a hyperparameter weighting the MMD loss."}, {"title": "Experiments", "content": "Data: We use the deeplenstronomy [71] software, which is built on lenstronomy [9, 11], to generate simulations of galaxy-scale strong lensing images as if observed in a ground-based survey. We use a single photometric band (g), which is sufficient for producing morphological features of a lensing system. Images have a pixel scale of 0.263 \"/pixel to match that of DES [30, 2]. During"}, {"title": "Results: UDA improves NPE performance on target domain data", "content": "First, we check that the addition of UDA to NPE does not lead to a significant deterioration in model performance on source data compared to NPE alone. For all parameters, the NPE-UDA model is slightly more accurate when applied to the source data than when applied to the target data (Fig. 1(a) and Table 1(c)). Also, the NPE-UDA model has nearly the same degree of calibration on source data as on target data (Fig. 1(d)). Next, the demonstration of performance relies on comparing the NPE-only and NPE-UDA models on target data. The NPE-only model has an average residual (i.e., bias) of 0.26\" for the Einstein radius. This bias is far outside the state-of-the-art uncertainties for traditional modeling techniques, which produce uncertainties at the level of ~ 0.01\" [65] or, more generally, at ~ 1-5% [82, 89]. In contrast, for the NPE-UDA model, the accuracy improves (the average residual reduces) by approximately 88%, 88%, 99%, 98%, and 93% for all five parameters \u03b8E, \u20ac1,1, \u20ac1,2, x, y, respectively. Also, in applications to the target domain data, the parameter uncertainties for the NPE-UDA model are very well-calibrated, while those for the NPE-only model are highly overconfident (Fig. 1(d)). This reflects the NPE-only model's bias toward the source domain when applied to the target domain data. Finally, the feature spaces for the NPE-UDA model applied to source and target data are overlapping but not when the NPE-only model is applied to data from those domains (Fig. 1(b) left and right, respectively)."}, {"title": "Summary and Outlook", "content": "We show for the first time that (unsupervised) domain adaptation (UDA) enhances simulation-based inference (SBI) models when applied to unlabeled target domain data. We used neural posterior estimation (NPE) to infer five parameters of lensing systems from single-band imaging data. We compare NPE models that have UDA (NPE-UDA) to NPE models that don't have UDA (NPE-only) (\u00a72). We incurred a domain shift between the source and target domains: the source images are nearly noiseless, and the target images have the same noise characteristics as DES (\u00a73). When applied to the target domain, the NPE-UDA model is 1-2 orders of magnitude more accurate than the NPE-only model for all five lens parameters (Fig. 1(c) and Table 1(b)). Similar approaches may significantly improve the accuracy of SBI/NPE models when they are applied to real observational data."}]}