{"title": "Visual Large Language Models for Generalized and Specialized Applications", "authors": ["Yifan Li", "Zhixin Lai", "Wentao Bao", "Zhen Tan", "Anh Dao", "Kewei Sui", "Jiayi Shen", "Dong Liu", "Huan Liu", "Yu Kong"], "abstract": "Visual-language models (VLM) have emerged as a powerful tool for learning a unified embedding space for vision and language. Inspired by large language models, which have demonstrated strong reasoning and multi-task capabilities, visual large language models (VLLMs) are gaining increasing attention for building general-purpose VLMs. Despite the significant progress made in VLLMs, the related literature remains limited, particularly from a comprehensive application perspective, encompassing generalized and specialized applications across vision (image, video, depth), action, and language modalities. In this survey, we focus on the diverse applications of VLLMs, examining their using scenarios, identifying ethics consideration and challenges, and discussing future directions for their development. By synthesizing these contents, we aim to provide a comprehensive guide that will pave the way for future innovations and broader applications of VLLMs. The paper list repository is available: https://github.com/JackYFL/awesome-VLLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "Computer vision tasks are challenging and diverse, requiring machines to possess a range of capabilities such as object perception [1], [2], spatial understanding [3], temporal action interpretation [4], [5], interaction with humans [6], and the ability to handle various domains and data transformations, among others. Numerous vision-language models (VLMs) [5], [7] have been developed to enhance the generalization and reasoning abilities of vision models by leveraging large-scale vision-language data, thereby improving their capacity to address these diverse challenges.\nConventional VLMs (before 2018, fig. 1) are designed to handle limited vision tasks, such as captioning and retrieval, primarily through encoder-decoder architectures. These approaches rely on convolutional neural networks (CNNs) to extract visual representations [8], [9] or hand-crafted visual features, and generate sentences using recurrent neural networks (RNNs) [10], long short-term memory (LSTM) [11] or conditional random field (CRF) [12]-based decoder. However, constrained by the limited scale of pretrained datasets and the simplicity of their architectures, these models are only capable of addressing specific tasks with moderate performance.\nWith the advancement of deep learning techniques such as Transformer [13], research on Vision-Language Models (VLMs) gained emergence from 2018 to 2022 (see fig. 1). Inspired by breakthroughs in natural language processing, these VLMs typically follow the Pre-training, Fine-tuning, and Prediction paradigm across various tasks, leveraging foundation models like BERT [14] and CLIP [15] that are pretrained on large-scale language or vision-language datasets. These foundation models encapsulate rich prior knowledge, enabling robust zero-shot and transfer learning performance across diverse tasks. Based on language foundation models like BERT, numerous approaches [16]\u2013[18] have been developed by integrating a visual extractor with the language foundation model, replacing earlier recurrent architectures. By leveraging the prior knowledge embedded in large-scale language corpora, these methods achieve significantly better performance compared to conventional models. Building on VLM foundation models such as CLIP, many studies aim to transfer this prior knowledge to downstream tasks through techniques like prompt tuning [19], [20], visual adaptation [21], and knowledge distillation [22], [23]. Despite these advancements, most of these methods are discriminative-based, which inherently lack multi-task capabilities and strong reasoning abilities. This limitation hinders the further development and broader applications of VLMs.\nWith the rise of language generative models [24], [25], researchers focus on leveraging the prior knowledge embedded in Large Language Models (LLMs) to develop general-purpose and highly reasoning VLMs. Leveraging instruction-tuning techniques [26] in LLMs, current Visual Large Language Models (VLLMs) [27] can process versatile instructions and generate responses that align with human preferences. Specifically, these VLLMs (as shown in fig. 2) employ a vision encoder to patchy vision data, use a connector to project visual tokens into the language space, and rely on an LLM as a decoder to produce instruction-following answers. These VLLMs can be applied to generalized and specialized applications, leveraging the prior knowledge and the sequence-to-sequence architecture of LLMs. Furthermore, they also inherit the reasoning capabilities of LLMs, enabling them to tackle more complex vision tasks.\nPrevious related surveys have primarily focused on general VLMs [7], [28]\u2013[30], or specific aspects of multi-modal large language models (MLLMs), such as methodologies [31], [32], reasoning capabilities [33], interpretability [34], evaluation benchmarks [35], development trends [36]\u2013[39], and hallucination issues [40]. With the advancement of VLLMs across various tasks, their applications have become increasingly diverse, covering both generalized and specialized scenarios. However, these surveys lack a comprehensive overview of the applications, limitations, and future directions of the vision modality in MLLMs.\nIn contrast, our survey provides a holistic perspective on VLLM applications, categorizing them into three main aspects: vision-to-text, vision-to-action, and text-to-vision. Each category is further broken down into detailed subtasks, with an in-depth analysis. Additionally, we explore the social impact of VLLMs, and the challenges they face in different scenarios. Finally, based on the ethical concerns, we propose some promising directions for future development, like security & privacy, efficiency, interpretability & explaninability, complex reasoning, etc. We hope our survey will provide a fresh perspective on VLLM applications and serve as a continuous source of inspiration for others."}, {"title": "II. \u03a4\u0395\u03a7\u039f\u039d\u039f\u039c\u03a5", "content": "Image-to-text tasks are fundamentally designed for models to extract image features and translate them into human language. Traditionally, methods such as CNNs combined with RNNs are the dominant solution for this task. These models primarily focus on leveraging the spatial information of images to generate corresponding textual descriptions. However, such methods demonstrate limitations in handling the complexity and variability of real-world images. Recently, with the rise of LLMs, numerous studies have explored leveraging the power of these advanced models for image understanding tasks. In particular, when combined with a robust pretrained vision encoder, such as CLIP-based or CNN-based architectures, these models have significantly enhanced the ability to accurately and contextually interpret and describe visual content. These advancements have enabled a wide range of applications, including general domain tasks such as image captioning, visual question answering (VQA), visual dialogue, referring expression comprehension (REC), referring expression segmentation (RES), and optical character recognition (OCR), as well as applications in specific domains.\nGeneral domain application: In the category of general domain applications, image-to-text models are designed to address multiple visual understanding tasks, rather than focusing on any specific domain. Traditionally, most computer vision models were developed to solve single, discrete tasks such as classification, detection, or segmentation. However, these models were limited in providing generalist solutions capable of handling multiple vision tasks simultaneously. Recently, in the field of natural language processing, the instruction-tuning paradigm introduced in some large language models (LLMs) [24], [247] has proven to be a game-changer, ushering in a new era where LLMs can manage a diverse range of text-based tasks. This paradigm shift has also influenced the vision domain, leading to the development of instruction-tuned visual LLMs capable of effectively interpreting and describing visual content across various tasks, such as image captioning and VQA. Numerous studies [42], [44]\u2013[46], [248], [249] have pioneered the use of visual instruction tuning, achieving superior performance on these tasks and demonstrating its potential to enable more versatile and generalizable visual understanding models.\nGeneral Ability. Image captioning, VQA and visual dialogue are fundamental capabilities of most vision-language image-to-text models. Captioning involves generating descriptive textual explanations for visual data, while VQA focuses on answering specific questions. Visual dialogue further extends VQA by enabling multi-turn conversations about the visual content. Currently, nearly all visual LLMs employ multiple training stages to develop these capabilities.\nThe first stage, known as pretraining, involves training models on extensive datasets of image-text pairs, such as COCO [250], Visual Genome [251], CC3M [252], Conceptual Captions [253], and LAION400M [254], etc. The primary objective during this phase is feature alignment, which unifies image and text into a single embedding space. This alignment process ensures that the model can understand and process visual and textual information cohesively.\nDuring pretraining, connectors between the vision encoder and LLMs are trained to synchronize visual and language representations. These connectors include linear projectors [42], [45], MLPs [62], [248], Q-formers [44], [51], and Resamplers [46], [55], etc. These connectors serve as bridges aligning the visual and language spaces, working in conjunction with the pretraining objective of the image captioning task. This alignment enhances the models' efficiency and effectiveness, enabling them to excel in generating coherent textual descriptions from visual inputs.\nIn the subsequent training stage, referred as supervised fine-tuning, models are trained on high-quality image-text pair datasets curated from high-capacity closed-source resources like GPT-4V [255], or Gemini [256]. Examples include LLaVA-Instruct-158K [42], GRIT [85], ShareGPT4V [257], and LVIS-Instruct4V [258]. Additionally, some studies [259] integrate high-quality text-only dialogue datasets, including ShareGPT-80K [260], SlimOrca [261], the Alpaca dataset [262], and the Vicuna dataset [263]. These datasets help to preserve the model's ability to handle multi-turn human dialogues. This fine-tuning stage equips models with the ability to effectively follow user instructions and enhances their multimodal conversation capabilities, thereby improving their performance in visual dialogue tasks.\nDuring fine-tuning, some models [42], [55], [56] opt to freeze the vision encoder to preserve the prior knowledge of visual representations. In contrast, others [58], [259] train both the vision encoder and LLMs to enhance the model's robustness in handling complex visual input. Furthermore, additional training stages are integrated into the training pipeline of certain models. For example, MiniGPT-v2 [45], Qwen-VL [55], and DeepSeek-VL [62] incorporate extra training stages to strengthen their conversational capabilities.\nReferring Expression Segmentation (RES). Object segmentation is a fundamental task in computer vision, involving the localization of objects at the pixel level. Traditional methods have addressed this task by targeting a predefined set of objects. Recently, the open vocabulary approach has gained prominence, leveraging LLMs to segment objects based on natural language descriptions, known as referring expressions. LISA [67] introduces the novel task of Reasoning Segmentation, where models generate segmentation masks guided by complex and implicit query text instructions. This architecture integrates a VLLM with an additional vision backbone, specifically the Segment Anything Model (SAM) [264], to extract visual embeddings. Leveraging an embedding-as-mask paradigm, the model decodes the last-layer embedding of the <SEG> token from the VLLM into a segmentation mask, thereby enhancing its ability to address intricate reasoning and leverage world knowledge effectively.\nGSVA [74] builds on LISA by introducing multiple <SEG> tokens to handle multiple targets and  tokens to reject empty targets, effectively addressing the challenges of Generalized Referring Expression Segmentation (GRES). Several other works, such as Osprey [79], VisionLLMv2 [78], GLaMM [76], and GROUNDHOG [68], integrate mask or region extractors to enhance the model's ability to capture fine-grained details. This strategy significantly improves RES accuracy, highlighting the effectiveness of incorporating specialized extractors for managing complex visual details.\nUnlike LISA and other approaches that use SAM [264] as a supporting segmentation model, VistaLLM [70] and LLaFS [75] directly generate segmentation coordinates from the LLM decoders. These models simplify the segmentation process by leveraging the LLMs to directly generate precise segmentation coordinates, eliminating the need for an intermediate segmentation model.\nReferring Expression Comprehension (REC). REC is a critical computer vision task that involves identifying and localizing objects in an image based on natural language descriptions. Early works like Shikra [86] and Kosmos-2 [85] build on the architectures of LLaVA [42] and Kosmos-1 [41], using instruction-tuning datasets for grounding and referring tasks. These models represent location coordinates as text tokens, which are generated by LLMs.\nSimilarly, VisionLLM [77], Pink [90], ChatSpot [87], InfM-LLM [93], and ASMv2 [265] aim to enhance training datasets by incorporating more region-level, fine-grained information. Models such as Ferret [91], GPT4R0I [88], Ferret-v2 [92], and Lion [94] modify their architectures to better capture detailed features from input images. Specifically, Ferret and GPT4ROI leverage Spatial-Aware Visual Sampler and Region Feature Extractor modules, allowing these models to focus on and extract critical image regions, thereby improving grounding accuracy.\nSphinx [95] leverages multiple image encoders, including CLIP-ViT [15], CLIP-ConvNeXt [266], and DINOv2 [267], and introduces a novel joint mixing strategy to integrate model weights. This approach enhances multi-modal understanding and delivers superior performance in fine-grained visual tasks.\nAn alternative approach leverages external modules for precise object localization instead of directly generating bounding boxes from LLMs. For instance, DetGPT [84] extracts all objects using the GroundingDINO [268] model for the grounding task. BuboGPT [89] introduces a pipeline that combines the Recognize Anything Model (RAM) [269], GroundingDINO, and the SAM [264] to align bounding boxes with objects in the output sequence.\nFurther, Lenna [100] and VisionLLMv2 utilize hidden fea-"}, {"title": "B. Video-to-text", "content": "Video-to-text tasks deal with advanced video content understanding problems such as video captioning, video question answering, video conversation, etc. The input videos could be either short within a few seconds, or a couple of hours long. They are accessed from historically cached databases or real-time streaming platforms. Compared to image-to-text tasks, video-to-text tasks are significantly more challenging because of the big semantic gap between sparse video content and abstract language information. In the spirit of LLMs, how to embed sparse causal information from long videos, e.g., temporal activity evolution, and physical motion dynamics, into a dense and tokenizable space for learning a video LLM is an open-ended research problem. According to the application goals, we categorize existing literature in the following parts.\nGeneral-purpose Video Understanding: In the era of LLMs, textual language is becoming a unified interface for modeling various video understanding problems. Such a trend leads to many existing general-purpose video LLMs. The basic idea of building a general-purpose model is to perform large-scale model pre-training on billions or trillions of visual-text data. The pre-training stage brings generalizable knowledge for downstream video understanding tasks. For example, early work such as VideoBERT [137] stands on top of the pretrained BERT [302] to pre-train on large-scale video datasets by masked language modeling (MLM), which is found critical to both video generation and classification tasks. Instead of MLM, recent LLMs [138]\u2013[142], [303] that adopt autoregressive modeling (ARM) show much better capability, which aims to predict the next token in language space.\nThe \"next-token prediction\" has inspired recent general-purpose video LLMs. For example, LaViLa [138] treats video data as a condition and adopts GPT2 [304] and T5-large [305] as video narrator and rephrase, to fine-tune LLMs for applications including activity recognition, event retrieval, and video question answering. Vid2Seq [139] leverages large-scale narrated videos and time tokens in pre-training, leading to a generalizable model for multiple video captioning benchmarks. VideoLLaMA [140] leverages video Q-former and ImageBind to handle visual temporal change and the modality correspondence between modalities of audio, video, and text. Following similar model architecture, recent video LLMs such as LLaMA-VID [141], MiniGPT4-Video [142], ST-LLM [143], LLaVA-Next-Video [303], LLaVA-OV [306], and Aria [307] have been introduced rapidly.\nLong-form video LLMs. The common idea of existing video LLM is to use a pre-trained LLM as the decoder to generate text responses. This raises a major challenge of how to effectively encode long-form videos into visual tokens and project them into text token space for LLM to decode. Recently, LLaMA-VID [141] uses two tokens including the context token and content token to encode each frame, enabling hourly-long video understanding. LVChat [150] dynamically adjusts the number of video tokens by considering the duration of input videos. MA-LMM [308] stores the features of historical video tokens in a memory bank such that the model can process hourly-long videos in an online manner. Different from previous methods, recent LongVU [144] adopts temporal frame selection and spatial token selection to significantly reduce redundant tokens in video LLMs. Until today, how to efficiently encode long video-text context into a unified token space for video conversation is still challenging and being actively studied in research communities.\nVideo Conversation: Instead of building foundational video-to-text models for general purposes, video conversation raises a great interest because the language model serves as a conversational agent of humans. However, this scenario naturally requires multiple rounds of reasoning steps to understand the complex video content, as well as humans' ambiguous intent in a long dialog context. This leads to the recent trend of studying video-language chat applications. ChatGPT is generally regarded as a breakthrough that successfully makes LLMs such as GPT-4 [309] applicable for complex reasoning tasks. Recently, its multi-modality version GPT-4(V) [310] takes images and video into account in conversation. These agents are commercial products without public code and models released. Since LLaVA [42] is a representative visual assistant for image-to-text agent applications. Thanks to its simplicity, plenty of video assistants are developed recently, such as the Video-LLaVA [145], VideoChat [146], VideoChat2 [147], VideoChatGPT [148], Valley [149], BT-Adapter-LLaVA [151]. Note that most existing general-purpose video LLMs are technically feasible for video conversation, though they are not optimized from engineering aspects for improving the conversation experience.\nEgocentric Understanding: For video understanding tasks, most applications focus on the third-person view. However, understanding egocentric human activities is essential for plenty of real-world applications. For example, a headset with an egocentric camera sensing system could assist a worker in finishing complex mechanical repair jobs, when the system can understand the humans' egocentric visual world and interact with humans with plain language. Prior research such as [311]\u2013[316] primarily resort to vision-language models such as CLIP [317] to achieve egocentric video understanding, which are too small (less than 1 billion parameters) to handle complicated video tasks. Powered by advances in LLMs, the recent GPT4Ego [152] leverages ChatGPT and chain-of-thought to generate text prompts for egocentric videos,"}, {"title": "C. Vision-to-action", "content": "Vision-to-action tasks based on VLLMs mainly take visual modalities such as images/videos/depth/3D as the visual input conditioned with language instructions, and the VLLM will generate actions to control behaviors of vehicles, robots or software (e.g., APIs), etc. We divide such tasks into three main streams according to the application scenarios: autonomous driving (AD), embodied AI and tool management. These VLLM agents [319] will be more intelligent and general when making decisions by equipping with LLM's strong contextualizing, reasoning, and generalization ability.\nAutonomous Driving: Recent AD systems transit from rule-based to data-driven ones [320]\u2013[322], and many of these methods resort to LLMs for better perception, planning and prediction ability. The vision input for LLMs in AD can be various and adaptive, like multi-view images [166], video [172], bird's eye view (BEV) map [159] and LiDAR [162], [171], which is easy to be extended to various driving tasks.\nPerception. As mentioned in Sec. II-A, VLLMs have shown strong reasoning and zero-shot abilities in understanding tasks. This characteristic can also be extended to the AD system to improve the generalization capability in complex driving environments [323]. NuScences-QA [158] proposes a VQA-based benchmark for the AD scenario, including both images and point clouds modalities as input and the question answering pairs on the existence, counting, status, types, and relationships of the objects. DriveLM [160] introduces a graph-structured reasoning task to model the logical dependencies during the AD question and answering process. Talk2BEV [159] proposes to utilize the BEV map to perform scene understanding, visual reasoning, and free-form conversation tasks. HiLM-D [161] proposes an efficient method to localize risk objects, predict intentions, and provide suggestions. Specifically, HiLM-D designs two branches for high-resolution perception and low-resolution reasoning, respectively. Built upon OpenFlamingo [324], Dolphins [164] can process data integrating video/images, instructions, and historical control signals, and generate the understanding results of the driving scenario. Specifically, a new CoT process (Grounded Chain-of-Thought, GCoT) is proposed to enhance the reasoning ability of Dolphins by providing the spatial information of the objects. LiDAR-LLM [162] takes 3D LiDAR data as input and performs outdoor scene understanding like description, grounding and VQA based on a frozen LLM.\nPlanning. VLLMs can perceive complex driving scenarios and generate driving maneuvers or control signals for drivers based on current visual features. ADriver-I [169] introduces the interleaved vision-action pair as the inputs for autoregressively predicting the control signals of the current frame. It then employs a diffusion model to forecast future frames based on historical vision-action pairs, facilitating future action prediction. DriveVLM [166] capitalizes VLLMs for scene understanding and planning by containing a Chain-of-Thought (CoT) process, including scene description, scene analysis and hierarchical planning. DriveVLM can generate three kinds of driving plans, which are meta-actions for a short-term decision, decision description for articulating the more fine-grained driving strategy, and trajectory waypoints for depicting the vehicle's path. DiLu [168] improves the planning capability of AD by combining a reasoning and reflection module based on past driving knowledge. Specifically, a memory module is employed to store the past driving experience like decision prompts, reasoning processes, etc., which can be utilized to enhance the reasoning and planning capabilities of the LLMs. DriveGPT4 [172] can predict the low-level control signals by tuning an end-to-end VLLM based on the BDD-X dataset [325] annotated by ChatGPT. Specifically, DriveGPT4 can analyze video frames by a pretrained vision encoder, and is also trained for two stages like LLaVA [42]. SurrealDriver [170] proposes a driver agent simulation framework for urban scenarios, which can provide driving maneuvers based on real world driver experience. LMDrive [171] introduces an end-to-end driving framework, which can perceive multi-view multi-modal sensor data (camera and LiDAR) and generate control signals according to the navigation instructions based on an LLM decoder.\nPrediction. VLLMs can also be utilized to predict the trajectories and actions of vehicles and pedestrians to assist in navigation. In NuPrompt [173], Wu et al. formulate a new task by prompting to predict the described object trajectory across frames. They also propose a method PromptTrack to predict the 3D bounding box of the prompt-referred objects. BEV-InMLLM [174] integrates BEV features and multi-view video into the LLM for perception, prediction and planning tasks. Such a method can forecast the actions of the surrounding entities and predict the potential dangers.\nEmbodied AI: Embodied AI refers to the artificial intelligence systems designed to control physical embodiments and interact with the environments [326]. These systems possess the abilities in cognition, decision-making, and control [327], [328], which are commonly applied in the field of robotics. Since conventional robot agents mainly concentrate on some constricted tasks and lack common sense knowledge, there exists a growing number of works based on LLMs for learning versatile downstream task policies. Among these methods, VLLM-based agents constitute a significant portion and can be categorized into four streams [329], i.e., perception, manipulation, planning, and navigation.\nPerception. VLLM-based robots can extract semantic knowledge and understand environments from visual signals such as RGB or LiDAR. OpenEQA [175] introduces a new benchmark on embodied question answering perception tasks, such as object recognition, attribute recognition, object localization, etc. This benchmark presents two types of problems: the episodic-memory task for understanding the environment through the episodic memory, and the active one by requiring only navigation actions. AffordanceLLM, 3DVG, 3D-LLM, and PaLM-E utilize 3D data to enhance perception. In AffordanceLLM [176], the authors propose detecting the interaction point of an object (affordance grounding) via VLLM, introducing depth information to better capture object geometry and improve grounding performance. 3DVG [177] enhances object detection and classification by introducing a language-object correlation (LOC) module that fuses 3D point cloud geometry with 2D image details via VLLM. This multi-modal fusion improves fine-grained object perception and extends the model's capacity for open-vocabulary scenarios. 3D-LLM [178] proposes a framework that integrates 3D spatial information into an LLM, enabling it to process 3D point clouds and perform tasks such as 3D question answering, captioning, and navigation. Similarly, PaLM-E [183] improves embodied perception by integrating visual inputs, such as images and 3D scene representations, into the language model's embedding space using ViT and object-centric encoders. Other works utilize additional modules or agents for perception. The paper [179] introduces a multi-agent VLLM framework with specialized agents to reduce errors in object identification and coordinate refinement. REPLAN [180] employs a VLM Perceiver, enabling robots to ground actions in visual data, allowing object detection and obstacle identification.\nManipulation. To create a universal robot capable of handling various downstream tasks, a key skill is the ability to manipulate objects in its environment based on the specific requirements of each task [330]. This ability can be greatly enhanced by using VLLMs. PaLM-E [183] incorporates continuous inputs like images, states and language from an agent into a pretrained LLM (PaLM [331]) for manipulation planning. RT-X [182] introduces a large and diverse manipulation dataset \u201cOpen X-Embodiment\u201d which includes 527 skills and 1M+ real robot trajectories enabling generalized policy learning. Instruct2Act [184] proposes a training-free method by calling the APIs of existing foundation models using the LLM. Based on the expertise of foundation models and the reasoning ability of LLM, InstructAct can understand complex instructions for manipulation tasks. Roboflamingo [185] is based on an off-the-shelf VLLM, which learns the sequential historical information with a policy head and is finetuned by imitation learning on manipulation datasets. Such a decomposition design makes Roboflamingo flexible and efficient when deploying in the real world. VoxPoser [3] proposes synthesizing robot trajectories for manipulation tasks based on open-set instructions and objects. Specifically, a training-free method is proposed by utilizing the code-writing abilities of LLM, which can generate 3D value maps by calling the vision-language model. Niu et al. introduces an instruction-tuning-based method, LLARVA [186], which leverages structured prompts for various robotic learning tasks. They also demonstrate that visual traces, formed by intermediate 2D representations, are beneficial for aligning vision and action spaces. ManipLLM [187] targets at predicting the contact point of the object given the text prompt, RGB image and depth map by tuning a VLLM. Kim et. al. presents an efficient tuning VLLM OpenVLA [188] by training on a large-scale real-world manipulation dataset.\nPlanning. In embodied AI, task planning involves decomposing high-level objectives into atomic subtasks while accounting for real-world dynamics. NLMap [189] enables agents to construct open-vocabulary queryable scene representations by integrating VLMs and LLMs. It allows robots to identify relevant objects within the environment, and gen-"}, {"title": "D. Text-to-vision", "content": "The boundary between reality and artificiality is increasingly blurred with the rise of generative applications of VLLMs. These models signify a major shift from merely interpreting data to creatively producing new visual content. This section transitions to VLLMs'innovative visual generative applications in generating images, 3D models, and videos.\nText-to-image: This section focuses on how VLLMs transform text prompts into images seamlessly. Text-to-image applications include image generation, image editing, image synthesis, and visual story generation.\nImage Generation. The core of image generation is crafting visuals from text prompts to closely reflect the prompt's intention, ensuring both visual coherence and realism. VLLMs effectively bridge the linguistic and visual realms.\nOptimizing image generation prompts is crucial for synchronizing the visual and textual elements of VLLMs. GILL [219] enhances the handling of longer and more complex textual inputs for image generation by instructing LLMs to predict fixed-size visual embeddings aligned with CLIP space, thus controlling Stable Diffusion for image generation. Instead of relying on LLMs to predict fixed-size embeddings, [220] and [221] utilize LLMs to generate more detailed prompts, resulting in more accurate and rich content.\nIn addition to textual refinements, some research enhances the visual embedding capabilities of VLLMs. SEED [222] and LaVIT [223] introduce sophisticated visual tokenizers that translate non-linguistic images into sequences of discrete tokens, akin to a foreign language that LLMs can interpret. These visual tokenizers enable LLMs to simultaneously see and draw. VL-GPT [225] proposes an innovative image tokenizer-detokenizer framework, which, alongside traditional text tokenization methods, allows VL-GPT to handle interleaved image-text data seamlessly.\nBeyond textual or visual refinements, some research improves image generation by focusing on multimodal fusion. MiniGPT-5 [226] boosts multimodal generation through \"generative vokens\" as pivotal elements, bridging LLMs with Stable Diffusion for more cohesive and context-aware vision-and-language interactions. Emu [224] processes multimodal inputs through an autoregressive training process. Kosmos-G [227] aligns the output space of MLLM with CLIP using the textual modality as an anchor. CoDi-2 [228] aligns modalities with language for both encoding and generation. [229] presents the Joint Autoregressive Mixture (JAM) framework to fuse existing text and image generation models into a single, robust model capable of generating seamless multimodal outputs. CM3Leon [230] is a retrieval-augmented, token-based, decoder-only single multi-modal language model capable of generating and infilling both text and images, with notably fewer training computes compared to existing methods.\nRather than only translating prompts into embeddings, DiffusionGPT [231] utilizes LLMs for image generation model selection by constructing domain-specific Trees for various generative models and them to guide the selection of an appropriate image generation model.\nImage Editing. The task of image editing involves modifying images based on language descriptions, such as object removal, color adjustment, and adding text. The primary challenge is accurately interpreting nuanced instructions while maintaining the original image's context and aesthetics.\nThe models in the section of Image Generation, inherently equipped for image generation, seamlessly adapt to image editing tasks when provided with image inputs. This integration leverages the rich, instruction-based datasets from InstructPix2Pix, enabling more precise and contextually accurate modifications to existing images. For example, Kosmos-G [227] and CoDi-2 [228] both improve image editing capability by leveraging the data constructed by InstructPix2Pix [346]."}, {"title": "III. ETHICS CONSIDERATION, CHALLENGES, AND FUTURE WORK", "content": "A. Ethics Consideration\nImpact on Labor Markets: The application of VLLMs automates tasks across various industries. Sectors like creative design", "workers.\nBias": "VLLMs", "376": [377], "378": [379], "380": ".", "Challenges\nEfficiency": "The efficiency of VLLMs becomes a critical challenge due to their widespread deployment and the substantial computational demands they impose. The efficiency of VLLMs encompasses two key aspects: training and inference.\nTraining Efficiency. For instance, training an efficient VLLM model DeepSeek-V3 [62"}]}