{"title": "UnityGraph: Unified Learning of Spatio-temporal features for Multi-person Motion Prediction", "authors": ["Kehua Qu", "Rui Ding", "Jin Tang"], "abstract": "Multi-person motion prediction is a complex and emerging field with significant real-world applications. Current state-of-the-art methods typically adopt dual-path networks to separately modeling spatial features and temporal features. However, the uncertain compatibility of the two networks brings a challenge for spatio-temporal features fusion and violate the spatio-temporal coherence and coupling of human motions by nature. To address this issue, we propose a novel graph structure, UnityGraph, which treats spatio-temporal features as a whole, enhancing model coherence and coupling. Specifically, UnityGraph is a hypervariate graph based network. The flexibility of the hypergraph allows us to consider the observed motions as graph nodes. We then leverage hyperedges to bridge these nodes for exploring spatio-temporal features. This perspective considers spatio-temporal dynamics unitedly and reformulates multi-person motion prediction into a problem on a single graph. Leveraging the dynamic message passing based on this hypergraph, our model dynamically learns from both types of relations to generate targeted messages that reflect the relevance among nodes. Extensive experiments on several datasets demonstrates that our method achieves state-of-the-art performance, confirming its effectiveness and innovative design.", "sections": [{"title": "I. INTRODUCTION", "content": "Multi-person motion prediction aims to predict future human motion sequences for multiple individuals based on historical sequences. Single-person motion prediction methods [33], [69], [10] focus solely on modeling an individual's spatial joints relation or trajectory features in the temporal domain, neglecting the interactions between different individuals, as shown in Fig. 1 (a). In contrast, multi-person motion prediction carries more practical significance, as it is more common for multiple people to be present in one scene in the real world. At the same time, multi-person motion prediction is also more challenging because of the complicated interactions between individuals and plays a significant role in many real-world applications, such as autonomous driving [68], [40], robotics [70], [55], surveillance systems [71].\nMost current researches [41], [47] involving multi-person scenes focus on trajectory prediction, which model the agent as a single 2D point on the ground plane and exploit the temporal features and the spatial interactions among different person to make prediction, as shown in Fig. 1 (b). However, these methods are insufficient for 3D tasks such as motion prediction, which requires detailed body pose information. Adeli et al. [49] first combine scene context to model interactions between humans and objects in 3D task. Subsequently, more approaches [50], [52], [11] are being developed to capture spatio-temporal features more effectively. However, previous methods usually employ dual-path networks to learn temporal and spatial features separately, as shown in Fig. 1 (c). Given the variations and diverse action types in human motion data, this decoupled modeling strategy inherently disrupts the real world unified spatio-temporal interdependencies, making it challenging to accurately capture the cross-dependencies of spatio-temporal relationships and potentially limiting the accuracy and realism of the predictions [13], [44].\nTo address the above issue, we propose a novel graph structure named UnityGraph based on hypervariate graph to learn human motion features with the spatio-temporal consistent perspective, as shown in Fig. 1 (d). The core idea of the hypervariate graph is to construct a space-time fully-connected structure. Specifically, given the observation of scenes involving N persons over a duration of T time steps. The hypergraph can be viewed as a graph with N\u00d7T nodes that are connected by hyperedges, as shown in Fig. 3 (a). The higher-order connectivity [8], [43] of hyperedge can connect multiple nodes at the same time is utilised to express complex interactions between multiple people over the spatio-temporal domain. Such a special design formulates spatio-temporal features of individuals as node-to-node dependencies in the hypergraph. Different from previous methods that learn spatio-temporal features separately with a dual-path structure, our approach views spatio-temporal correlations as a whole. It abandons the uncertain compatibility of spatial and temporal modeling, constructs adaptive spatio-temporal dependencies. We set three types of hyperedges to learn spatio-temporal features from node-to-node dependencies: i) Short-term hyperedges connect neighboring nodes of the same individual to model short-term dynamics. ii) Long-term hyperedges connect all nodes of the same individual throughout the time series, capturing the long-term dynamics. iii) Spatial hyperedges link different nodes at the same frame to capture interaction features between different individuals. Notably, our strategy of designing hyperedges achieves more accurate predictions than treating the hypergraph as a fully connected graph. This approach ensures spatio-temporal coupling while avoiding the unacceptably large computational effort associated with full connectivity, as discussed in our extensive ablation experiments."}, {"title": "II. RELATED WORK", "content": "A. Single-Person Motion Prediction.\nFor single-person motion prediction, early methods are developed based on time series models [36], [1], [42]. Wang et al. [42] utilize pose velocities and temporal positional information to make prediction. However, it is inappropriate to consider consider human motion sequence as the time series task. Human motion involves multiple skeleton joints with spatial connection. For example, the connection between the knee and the ankle differs from that between the elbow and the shoulder. To capture spatial features, GCN-based models are widely used [44], [37], [66], [46]. Mao et al. [66] propose a classic model based on GCN which takes into account both temporal smoothness and spatial dependencies among human body joints. Recently, Transformer-based methods have gained widespread attention in the field of motion prediction [33], [69], [35], [34]. Yu et al. [69] propose a cross-transformer network to register dynamic spatio-temporal information and seize its inherent coherence, which existing decoupled methods generally seldom consider. However, these methods primarily address single-person scenarios and overlook typical real-world interactions.\nB. Multi-Person Motion Prediction.\nMulti-person scenes are practical and common. It also takes a new challenge for us to consider the sophisticated connection between different persons. Adeli et al. [49] is the first group to leverage human-to-human interaction in prediction. Recently, more methods [50], [51], [9] demonstrate the effectiveness of Transformer in this task because of its ability to capture the global interactions among entire crowds. Wang et al. [50] employ a local-range Transformer to encode the motion of an individual in the sequence and a global-range Transformer to encode the motion of multiple individuals. Both encoded motion is then sent to a Transformer-based decoder to predict future motion. However, the design of the two-stream network diminishes the coupling of spatial-temporal features. Guo et al. [9] proposes a novel cross interaction attention mechanism that exploits historical information of both persons, and learns to predict cross dependencies between the two pose sequences. This method primarily focuses on capturing human-to-human interactions and ignore the individual's joint relations, which are equally crucial for accurate prediction. Despite Transformer has strong ability, there is potential for further im-provement. Liu et al. [62] propose a multi-granularity learning module to capture different level interaction between individuals. Xu et al. [52] introduce physical constraints into relation learning, e.g., the different joints connected by bone show stronger associations and the joints belonging to the same individual are related.\nC. Hypervariate Graph Network.\nHypervariate graph networks, which allow edges to connect more than two nodes, have gained prominence for modeling complex, higher-order relationships among nodes. This ca-pability is less feasible with traditional graph networks [4], [3], [2]. These networks have been successfully applied in diverse areas including trajectory prediction [41], multivariate"}, {"title": "III. METHODOLOGY", "content": "A. Problem Formulation\nGiven a scene with N persons, each person has J skeleton joints, we define the observed sequence of the n-th person as $X_T^n = [x_1^n, x_2^n,...,x_T^n]$, where n \u2208 {1,2, ..., N}, N denotes the number of observed people, T represents the observed motion sequence length, and each $x_t^n \u2208 R^{J\u00d73}$ denotes the joints' 3-D coordinates of the n-th person at the t-th motion sequence. Our objective is to predict the future motion sequence of the n-th person, denoted as $Y_{T+1:T+P}^n = [Y_{T+1}^n, Y_{T+2}^n,..., Y_{T+P}^n]$, where P denotes the predicted motion sequence length. The ground-truth of the n-th person can be defined as $Y_{T+1:T+P}^{n*} = [Y_{T+1}^n, Y_{T+2}^n, \u2026, Y_{T+P}^n]$.\nB. Hypergraph Construction\nMany existing methods [49], [50] adopt an intuitive ap-proach to model spatial and temporal features independently. We argue that such separate feature learning is not sufficient to capture the complex spatial-temporal dependencies underlying the motion sequence. To comprehensively model these intrinsic composite features, we consider using a specially designed hypergraph for its object-oriented character, see Figure 4. Mathematically, this hypergraph can be defined as a quaternion G = {V, E1, E2, E3}, where V is a set of N\u00d7T nodes, E1, E2 and E3 are the sets of three different hyperedges.\n1) Nodes Initialization: The nodes are composed of features that derive from N\u00d7T poses. We adopt a graph attention network (GAT) as the encoder:\n$g_{t,l}^n = GAT (x_{t}^n, W_l)$ (l = 0)  (1)\nwhere W is the set of parameters of the input pose attention graph and l is the number of update layer and l = 0 meant the initial stage. Accordingly, the output is $g_{t,l}^n$, which is a motion representation attending to different joint connectivity. The node of the n-th person at the t-th frame can be denotes as $g_{t}^n \u2208 R^{J\u00d73}$. Thus, the node set can be express as v ={$g_{t}^n|t = 1, 2, ..., T; n = 1, 2, \u2026, N}.\n2) Hyperedges Initialization: To capture high-order spatio-temporal correlations among nodes, we design three types of hyperedges: short-term hyperedges, long-term hyperedges, and spatial hyperedges.\nShort-term hyperedges We design N \u00d7 (T \u2013 1) short-term hyperedges, connecting each person's t-th node to the (t+1)-th node. Formally, we define a short-term hyperedge as follows:\n$(e_1)_{t,l}^n = Aggregate (g_{t,l}^n, g_{t+1,l}^n)$ (l = 0)  (2)\nWhere Aggregate() is the mean operation. Short-term hyperedges allows the propagation of information between two neighboring nodes $g_{t,l}^n$ and $g_{t+1,l}^n$ in the temporal domain. The set of short-term hyperedges E\u2081 can be expressed as the following two steps:\n$E_1 = \\cup_{n=1}^N \\{(e_1)_{t,l}^n, t \u2208 {1,2,\u2026, T \u2013 1}\\}$ (l = 0)  (3)\nLong-term hyperedges To capture temporal features on long-term, we design N long-term hyperedges. Each hyper-edge connects T nodes of single person:\n$(e_2)_{l}^n = Aggregate (g_{1,l}^n, g_{2,l}^n,\u2026,g_{T,l}^n)$ (l = 0)  (4)\nThe set of long-term hyperedges E2 can be expressed as:\n$E_2 = \\{(e_2)_{1,l}^n, (e_2)_{2,l}^n,\u2026\u2026, (e_2)_{N,l}^n\\}$ (l = 0)  (5)\nSpatial hyperedges In addition, we define T spatial hyper-edges to capture the interactions across different persons. The spatial hyperedges are defined as follows:\n$(e_3)_{t,l} = Aggregate (g_{t,l}^1, g_{t,l}^2,\u2026,g_{t,l}^N)$ (l = 0)  (6)\nWhere spatial hyperedges focus on aggregating information across N individuals within t-th frame to obtain interaction features. The set of temporal hyperedges can be defined as:\n$E_3 = \\{(e_3)_{1,l}, (e_3)_{2,l},\u00b7\u00b7\u00b7, (e_3)_{T,l}\\}$ (l = 0)  (7)"}, {"title": "C. Dynamic Message Passing", "content": "Spatial and temporal features are different in nature and have different impacts. In order to capture their own features without deconstructing the inherent correlation and coupling between them. We customize a neural message passing method to obtain the spatio-temporal features iteratively through node-to-hyperedge and hyperedge-to-node update.\n1) Node-to-hyperedge update: To propagate short-term information through hyperedges among nodes, we deploy a multi-head attention mechanism to compute the weight between hyperedge $(e_1)_{t,l}^n$ and nodes $g_{t,l}^n$, $g_{t+1,l}^n$, as shown in Fig. 5 (a). The attention weight is computed as follows:\n$(\u03b1_1)_{t,l}^n = Softmax (\\frac{((e_1)_{t,l}^n W_{e,l}) (g_{t,l}^n W_{g,l})^T}{\\sqrt{d_{e1}}})$ (8)\nWhere $(\u03b1_1)_{t,l}^n$ denotes the attention weight for node $g_{t,l}^n$. $W_{e,l}$ and $W_{g,l}$ are learnable transformation matrices and $d_{e1}$ is the dimension of the $(e_1)_{t,l}^n W_{e,l}$.\nWith the hypergraph attention mechanism, the updated t-th spatial hyperedges $(e_1)_{t,l+1}^n$ is computed by aggregating information from the t-th node $g_{t,l}^n$ and the (t+1)-th node $g_{t+1,l}^n$. Specifically, the hyperedge representation is computed by the following equation:\n$(e_1)_{t,l+1}^n = \u03c3 [(\u03b1_1)_{t,l}^n \u00b7 g_{t,l}^n + (\u03b1_1)_{t+1,l}^n \u00b7 g_{t+1,l}^n] + (e_1)_{t,l}^n$ (9)\nWhere $(e_1)_{t,l+1}^n$ denotes the updated short-term hypredge of the (l + 1)-th layer. \u03c3 (\u00b7) is an activation function.\nContrast to the short-term, the long-term hypredge update need to use all T nodes of the n-th individual. The process can be expressed as:\n$(\u03b1_2)_{t,l}^n = Softmax (\\frac{((e_2)_{l}^n W_{e,l}) (g_{t,l}^n W_{g,l})^T}{\\sqrt{d_{e2}}})$ (10)\n$(e_2)_{l+1}^n = \u03c3 [\\sum_{t=1}^T (\u03b1_2)_{t,l}^n \u00b7 g_{t,l}^n] + (e_2)_{l}^n$ (11)\nWhere $(\u03b1_2)_{t,l}^n$ denotes the attention weight for node $g_{t,l}^n$. $(e_2)_{l+1}^n$ denotes the updated long-term hypredge of the (l+1)-th layer.\nSimilarly, we update spatial hyperedges by incorporating information from connected nodes, as shown in Fig. 6. Specifically, the updated spatial hyperedges $(e_3)_{t,l+1}$ is computed as follows:\n$(\u03b1_3)_{t,l} = Softmax (\\frac{((e_3)_{t,l} W_{e,l}) (g_{t,l}^n W_{g,l})^T}{\\sqrt{d_{e}}})$ (12)\n$(e_3)_{t,l+1} = \u03c3 [\\sum_{n=1}^N (\u03b1_3)_{t,l}^n \u00b7 g_{t,l}^n] + (e_3)_{t,l}$ (13)\nWhere $(\u03b1_3)_{t,l}^n$ denotes the attention weight of different nodes. The weight $(\u03b1_3)_{t,l}^n$ allows the model to focus on the most relevant nodes when updating the hyperedge representations. The updated spatial hyperedges $(e_3)_{t,l+1}$ obtains the features from all individuals of t-th frame and captures the interactions across different individual.\n2) Hyperedge-to-node update: In this section, we update nodes by propogating information through hyperedges, as shown in Fig. 7. Similar to the hyperedges update, we first calculate the weight between a node $g_{t,l}^n$ and the temporal and spatial hyperedges $(e_1)_{t,l}, (e_2)_{l}, (e_3)_{t,l}$, connected with it as follows:\n$(\u03b2_1)_{t,l} = Softmax (\\frac{((e_1)_{t,l} W_{e,l})^T (g_{t,l}^n W_{g,l})^T}{\\sqrt{d_{g}}})$ (14)\n$(\u03b2_2)_{l} = Softmax (\\frac{((e_2)_{l} W_{e,l})^T (g_{t,l}^n W_{g,l})^T}{\\sqrt{d_{g}}})$ (15)\n$(\u03b2_3)_{t,l} = Softmax (\\frac{((e_3)_{t,l} W_{e,l})^T (g_{t,l}^n W_{g,l})^T}{\\sqrt{d_{g}}})$ (16)\nWhere $(\u03b2_1)_{t,l}, (\u03b2_2)_{l}, (\u03b2_3)_{t,l}$ denotes the attention weight for three different hyperedges. $d_{g}$ is the dimension of the $g_{t,l}^n W_{g,l}$. The representation of the node in the (l + 1)-th layer are computed using the following equations:\n$g_{t,l+1}^n = MLP ((\u03b2_1)_{t,l}\u00b7 (e_1)_{t,l}) + MLP ((\u03b2_2)_{l}\u00b7 (e_2)_{l})+ MLP ((\u03b2_3)_{t,l}\u00b7 (e_3)_{t,l}) + g_{t,l}^n$ (17)\nWhere $g_{t,l+1}^n$ is the updated representation of node in (l+1)-th layer.\nOverall, we repeat the node-to-hyperedge and hyperedge-to-node phases for L times and obtain the updated graph nodes. Finally, we get the sequences of N individuals' motion representation by:\n$Z_{1:T}^1 = Concat (g_{1,L}^1, g_{2,L}^1,\u2026,g_{T,L}^1)$\n$Z_{1:T}^2 = Concat (g_{1,L}^2, g_{2,L}^2,\u2026,g_{T,L}^2)$\n$Z_{1:T}^N = Concat (g_{1,L}^N, g_{2,L}^N,\u2026,g_{T,L}^N)$ (18)\nD. Interactive decoding\nTo accurately predict future poses, relying solely on his-torical information is insufficient. We also consider dynamics in the future, such as others' future motions occurring during the same window of time. Formally, after updating all the historical features, we obtain motion sequences of N persons. Next, we apply a predictor to generate the set of future poses recursively, as shown in Fig. 8. We take person N as the example, mathematically the procedure at T +1 frame can be expressed as:\n$\\hat{Y}_p^N = GRU (Z_{1:T}^N, x_{T}^N)$ (p = T + 1) (19)\nWhere p denotes the p-th predicted frame and $\\hat{y}_p^N$ is the p-th predicted motion of N-th person (p \u2208 (T + 1,T + P)). The first prediction motion is matter in autoregressive decoding, we take the last observed motion $x_{T}^N$ as input to reduce uncertainty in latent space.\nThe reasoning persons' interactions $\\Omega_{p}^{N}$ are obtained by the relation reasoning module $R^2M$:\n$\\Omega_{p+1}^{N}= R^{2}M (\\bigcup_{m=1}^{N-1}  I_{(p)}^{\\left(N,m\\right)} ) (p = T+1)$\n$\\mathcal{L}_p^N = \\sum_{m=1}^{N-1} I_{(p)}^{\\left(N,m\\right)} + \\Omega_{p}^{N}$  (20)\nWhere m denotes a distinct individual from person N. $I_{(N,m)}$ is the interaction correlation between the person N person and the person m, it can be calculated as follow:\n$I_{(N,m)} = ATT (g_{T,L}^N, g_{T,L}^m)$ (21)\nWhere ATT () is the calculation of attention score. Subsequently, when p > T+1, the computation can be expressed as follows:\n$\\hat{Y}_p^N = GRU (\\hat{Y}_{p-1}^N, \\Omega_{p}^N)$ (p > T + 1) (22)\n$\\Omega_{p+1}^{N}= R^{2}M (\\bigcup_{m=1}^{N-1}  I_{(p)}^{\\left(N,m\\right)} ) (p > T + 1)$\n$\\mathcal{L}_p^N = \\sum_{m=1}^{N-1} I_{(p)}^{\\left(N,m\\right)} + \\Omega_{p}^{N}$  (23)\n$I_{(N,m)} = ATT (\\hat{Y}_{p-1}^N, \\hat{Y}_{p-1}^m)$ (24)\nFinally, we obatain the predicted motion sequences of N persons:\n$\\hat{Y}_{T+1:T+P}^1 = Concat (\\hat{Y}_{T+1}^1, \\hat{Y}_{T+2}^1,\u2026, \\hat{Y}_{T+P}^1)$\n$\\hat{Y}_{T+1:T+P}^2 = Concat (\\hat{Y}_{T+1}^2, \\hat{Y}_{T+2}^2,\u00b7\u00b7\u00b7, \\hat{Y}_{T+P}^2)$\n$\\hat{Y}_{T+1:T+P}^N = Concat (\\hat{Y}_{T+1}^N, \\hat{Y}_{T+2}^N,\u2026, \\hat{Y}_{T+P}^N)$ (25)"}, {"title": "IV. EXPERIMENT AND DISCUSSIONS", "content": "To assess the effectiveness of our method, we select it against a variety of baselines, including classical approaches such as LTD [66], TRIPOD [49], DVITA [67], and MRT [50], in addition to the single-person motion prediction method TCD [10]. We also incorporate state-of-the-art models, JRT [52] and TBIFormer [11], as baselines. LTD [66] serves as a foundational model in human motion prediction, encoding temporal within trajectory space. TRIPOD [49] models interactions between humans and objects. DVITA [67] addresses multi-person prediction by dividing it into several single-person prediction tasks. MRT [50] highlights interaction significance in multi-person prediction tasks via a multi-range Transformer. JRT [52] investigates physical and interaction relations to the human body, achieving notable results. TBIFormer [11] focuses on skeleton dynamics, converting pose into multi parts to learn dynamic body part interactions.\nB. Datasets\nWe utilize four multi-person motion prediction benchmarks in our experiments: CMU-Mocap [58], MuPoTs-3D [59], 3DPW [53], and a Mixed dataset: Mix1&Mix2. The details of these datasets are presented below.\n1) CMU-Mocap [58]: The Carnegie Mellon University Motion Capture Database (CMU-Mocap) collects data from 112 subjects. Most scenes capture the movements of one person, and only 9 scenes include the movements and interactions of two persons. We follow study [50] to combine samples from both one-person and two-person scenes to create new sequences including three individuals. We aim to predict future 3000ms (45 frames) motion using the historical 1000ms (15 frames) motion.\n2) MuPoTS-3D [59]: Multiperson Pose Test Set in 3D (MuPoTS-3D) consists of over 8000 frames collected from 20 sequences with 8 subjects. Following previous works [50], [52], we evaluate our model's performance with the same segment length as CMU-Mocap on the test set.\n3) 3DPW [53]: 3D Poses in the Wild (3DPW) dataset is a comprehensive 3D motion dataset acquired using mobile phone cameras. It consists of 60 videos and approximately 68,000 frames, including a variety of scenarios and motions. In our study, we adhere to the settings used in [32], [49], [62],\nE. Loss Function\nWe design a joint loss function to evaluate proposed net-work. The loss function includes three parts, prediction loss, reconstruction loss, and inference loss.\nThe prediction loss $L_{pre}$ and reconstruction loss $L_{rec}$ are used to measure the error between the predicted motion and the corresponding ground truth. The calculations are defined as follows:\n$L_{pre} = ||\\hat{Y}_{T+1:T+P}^n - Y_{T+1:T+P}^n||_2$ (26)\n$L_{rec} = ||X_{1:T}^n - \\hat{X}_{1:T}^n||_2$ (27)\nWhere motion sequence $\\hat{X}_{1:T}^n$ is the reconstructed output of $X_{1:T}^n$ from our network. $||\u00b7||_2$ denotes the $l_2$ norm.\nIn addition, we design the inference loss $L_{inf}$ to supervise individuals' representation reasoning:\n$L_{inf} = \\sum_{n=1}^N \\sum_{p=T+1}^{NT+P} ||r_p^n - \\hat{r}_p^n ||_2$ (28)\nWhere the ground truth $r_p^n$ is calculated by the equation.23 with the input $\\hat{Y}_{T+1:T+P}^n$.\nThe overall loss function of UnityGraph is a weighted sum of the three loss terms:\n$L = \u03bb_{pre}L_{pre} + \u03bb_{rec}L_{rec} + \u03bb_{inf} L_{inf}$ (29)\nWhere $\u03bb_{pre}$, $\u03bb_{rec}$, and $\u03bb_{inf}$ are hyperparameters that control the relative importance of each loss term.\nF. Qualitative Results\n1) Visualization of prediction result: We provide a qualitative comparison on 3DPW test set between our method and other recent methods, including MRT [50], TCD [10] and JRT [52], as shown in Fig. 9. Compared with the previous methods, our results are more natural and closer to the ground truth, particularly in the movement of the lower limbs. We also provide the visualization results on CMU-Mocap dataset to verify our method's effectiveness on scenes of 3 persons as shown in Fig. 10. We can notice that both LTD [66] and MRT [50] generate unnatural arm distortions that do not appear in our approach, as shown in the red circles. The visualization results on different datasets demonstrate the generalization and accuracy of our method.\n2) Visualization of interactive decoding: To verify the ef-fectiveness of reasoning relations during decoding. We visual-\nG. Ablation Study\nThe ablation experiments are conducted on the CMU-Mocap and Mix1 datasets, with results presented in Table III, Table IV, and Table V. We analyze the computational complexity and discuss the strategy of hyperedge construction. Additionally, we conduct an ablation study on different loss function settings.\n1) Computational complexity analysis: We evaluate the trade-off between the model's computational cost and perfor-mance, as shown in Table. III. We report the number of param-"}, {"title": "V. CONCLUSION", "content": "In this paper, we introduce a novel graph structure named UnityGraph for multi-person motion prediction. UnityGraph addresses the previous issue of maintaining consistency and coupling of spatio-temporal features, which was caused by separately modeling the spatial and temporal dimensions. Extensive experiments demonstrate that UnityGraph achieves state-of-the-art performance. In contrast to the multi-path modeling strategy adopted by most previous methods,our method offers a novel perspective with a single graph in human motion prediction. In the future, we will fine-tune UnityGraph and conduct experiments on single-person datasets like H3.6M to demonstrate its effectiveness in single-person motion prediction tasks."}]}