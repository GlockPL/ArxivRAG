{"title": "Text2SQL is Not Enough: Unifying Al and Databases with TAG", "authors": ["Asim Biswal", "Liana Patel", "Joseph E. Gonzalez", "Siddharth Jha", "Amog Kamsetty", "Shu Liu", "Carlos Guestrin", "Matei Zaharia"], "abstract": "Al systems that serve natural language questions over databases promise to unlock tremendous value. Such systems would allow users to leverage the powerful reasoning and knowledge capabilities of language models (LMs) alongside the scalable computational power of data management systems. These combined capabilities would empower users to ask arbitrary natural language questions over custom data sources. However, existing methods and benchmarks insufficiently explore this setting. Text2SQL methods focus solely on natural language questions that can be expressed in relational algebra, representing a small subset of the questions real users wish to ask. Likewise, Retrieval-Augmented Generation (RAG) considers the limited subset of queries that can be answered with point lookups to one or a few data records within the database. We propose Table-Augmented Generation (TAG), a unified and general-purpose paradigm for answering natural language questions over databases. The TAG model represents a wide range of interactions between the LM and database that have been previously unexplored and creates exciting research opportunities for leveraging the world knowledge and reasoning capabilities of LMs over data. We systematically develop benchmarks to study the TAG problem and find that standard methods answer no more than 20% of queries correctly, confirming the need for further research in this area.", "sections": [{"title": "INTRODUCTION", "content": "Language models promise to revolutionize data management by letting users ask natural language questions over data, which has led to a great deal of research in Text2SQL and Retrieval-Augmented Generation (RAG) methods. In our experience, however (including from internal workloads and customers at Databricks), users' questions often transcend the capabilities of these paradigms, demanding new research investment towards systems that combine the logical reasoning abilities of database systems with the natural language reasoning abilities of modern language models (LMs).\nIn particular, we find that real business users' questions often require sophisticated combinations of domain knowledge, world knowledge, exact computation, and semantic reasoning. Database systems clearly provide a source of domain knowledge through the up-to-date data they store, as well as exact computation at scale (which LMs are bad at).\nLMs offer to extend the existing capabilities of databases in two key ways. First, LMs possess semantic reasoning capabilities over textual data, a core element of many natural language user queries. For example, a Databricks customer survey showed users wish to ask questions like which customer reviews of product X are positive?, or why did my sales drop during this period?. These questions present complex reasoning-based tasks, such as sentiment analysis over free-text fields or summarization of trends. LMs are well-suited to these tasks, which cannot be modeled by the exact computation or relational primitives in traditional database systems.\nSecondly, the LM, using knowledge learned during model training and stored implicitly by the model's weights, can powerfully augment the user's data with world knowledge that is not captured explicitly by the database's table schema. As an example, a Databricks internal AI user asked what are the QoQ trends for the \"retail\" vertical? over a table containing attributes for account names, products and revenue. To answer this query the system must understand how the business defines QoQ (e.g., the quarter over quarter trends from the last quarter to the current quarter or this quarter last year to this quarter this year), as well as which companies are considered to be in the retail vertical. This task is well-suited to leverage the knowledge held by a pre-trained or fine-tuned LM.\nSystems that efficiently leverage databases and LMs together to serve natural language queries, in their full generality, hold potential to transform the way users understand their data. Unfortunately, these questions cannot be answered today by common methods, such as Text2SQL and RAG. While Text2SQL methods [26, 28, 31, 32] are suitable for the subset of natural language queries that have direct relational equivalents, they cannot handle the vast array of user queries that require semantic reasoning or world knowledge. For instance, the previous user query asking which customer reviews are positive may require logical row-wise LM reasoning over reviews to classify each as positive or negative. Similarly the question which asks why sales dropped entails a reasoning question that must aggregate information across many table entries.\nOn the other hand, the RAG model is limited to simple relevance-based point lookups to a few data records, followed by a single LM invocation. This model serves only the subset of queries answerable by point lookups and also fails to leverage the richer query execution capabilities of many database systems, which leaves computational tasks (e.g., counting, math and filtering) to a single invocation of the error-prone LM. In addition to being error prone and inefficient at computational tasks, LMs have also been shown to perform poorly on long-context prompts limiting their ability to reason about data at scale in the generation phase of RAG.\nWe instead propose table-augmented generation (TAG) as a unified paradigm for systems that answer natural language questions over databases. Specifically, TAG defines three key steps, as shown in Figure 1. First, the query synthesis step syn translates the user's arbitrary natural language request R to an executable database query Q. Then, the query execution step exec executes Q on the database system to efficiently compute the relevant data T. Lastly, the answer generation step gen utilizes R and T, where the LM is orchestrated, possibly in iterative or recursive patterns over the data, to generate the final natural language answer A. The TAG model is simple, but powerful: it is defined by the following three"}, {"title": "THE TAG MODEL", "content": "We now describe the TAG model, which takes a natural language request R and returns a natural language answer A grounded in the data source. We outline three main steps that TAG systems implement: query synthesis, query execution, and answer generation. We define TAG tractably as a single iteration of these steps, but one can consider extending TAG in a multi-hop fashion."}, {"title": "Query Synthesis", "content": "The syn function takes a natural language request R and generates a query Q to be executed by the database system. Given a user request, this step is responsible for (a) deducing which data is relevant to answering the request (e.g., using the table schema), and (b) performing semantic parsing to translate the user request into a query that can be executed by the database system. This query could be in any query language, but in our example we use SQL.\nFigure 1 shows an example TAG instantiation for the user query which asks, \"Summarize the reviews of the highest grossing romance movie considered a 'classic'\". Here, the data source contains information about each movie's title, revenue, genre, and an associated review. In this step, the system leverages the semantic reasoning abilities of the LM to generate a SQL query that uses attributes on movie_title, review, revenue, and genre from the data source. Note that in this example, the database API is able to execute LM UDFs within SQL queries, so this step also introduces calls to the LM for each row to identify classic films within the query."}, {"title": "Query Execution", "content": "In the query execution step, the exec function executes the query Q in the database system to obtain the table T. This step leverages the database query engine to efficiently execute the query over vast amounts of stored data. The database API can be served by a wide variety of systems, which we explore in Section 3. Some APIs may allow for LM-based operators [18-21], permitting the database engine to leverage the LM's world knowledge and reasoning capabilities within exec.\nIn the example shown in Figure 1, the database query is a selection and ranking query written in SQL, which returns a table containing relevant rows. The query performs the selection using an LM to assess which movies are classics according to their movie_title, as well as a standard filter on genre to find romance movies. The query also ranks the results based on revenue to find the highest grossing film. As the figure shows, the resulting table contains reviews for the movie \"Titanic\"."}, {"title": "Answer Generation", "content": "The answer generation step in TAG mirrors the generation step in RAG. In this step, the gen function uses the LM to generate an answer A to the user's natural language request R, using the computed data T.\nFigure 1 shows the final stage of the example TAG pipeline outputting a summary of the reviews on \"Titanic\" as the answer to the original user request. In the example, the relevant data T is encoded as a string for the model to process. The encoded table is passed to the LM along with the original user request, R. To obtain the answer, this step leverages the model's semantic reasoning capabilities over the review column to summarize the reviews."}, {"title": "TAG DESIGN SPACE", "content": "In this section, we explore the generality of the TAG model and describe the rich design space it produces, highlighting several under-studied opportunities for further research.\nQuery Types The TAG model is expressive enough to serve a broad range of natural language user queries. We consider two important query categorizations, according to (a) the level of data aggregation required to answer the query and (b) the knowledge and capabilities needed for answering the query. First, the TAG model captures both point queries, such as retrieval-based questions [3, 9, 15, 16, 25, 30] which require look-ups to one or a few rows of the database, as well as aggregation queries, such as summarization or ranking-based questions which require logical reasoning across many rows of the database. Secondly, the TAG model enables natural language queries with varying demands on the system to provide data or reasoning-based capabilities, including for tasks such as sentiment analysis and classification.\nData Model The underlying data model can take many forms. Our implementation uses relational databases to store and retrieve structured attributes for knowledge-grounding in the downstream question-answering task. Others may operate on more unstructured data (e.g., free-text, images, video, and audio) or semi-structured data, which may be stored with a variety of data models, such as key-value, graph, vector, document, or object stores.\nDatabase Execution Engine and API The underlying system used to store the data can use many possible database execution engines. Text2SQL considers the setting of an SQL-based query engine for retrieving relational data for user queries. In this setting, syn will leverage the knowledge of the data source, such as the table schema, and return a SQL query to perform the retrieval step. In another common setting, retrieval systems over vector embeddings, syn transforms the natural language query into an embedding and exec performs similarity-based retrieval over the vector store.\nWhile these two settings have been widely studied, several under-studied alternative settings present interesting opportunities for efficiently implementing TAG systems to serve a broader range of queries. For instance, recent works augment relational models with semantic operators [21], which provide a set of declarative AI-based operators (e.g., filtering, ranking, aggregating, and performing search with natural language specifiers) or LM user-defined functions [19], which provide a general-purpose LM function. Additionally, query languages like MADLib [10], Google's BigQuery ML [1], and Microsoft's Predictive SQL [24] augment SQL-based APIs with native ML-based functions. Leveraging these systems provides unique opportunities for executing optimized reasoning-based retrieval patterns. For instance, in the example shown in Figure 1, a TAG pipeline implemented with semantic operators [21] might use a sem_filter operator to filter rows based on whether they are a 'classic' during the query execution step.\nLM Generation Patterns Given the table T of relevant data, gen can be comprised from a vast array of implementation decisions to produce the final natural language answer A in response to the user request R. While Text2SQL omits the final generation step and stops short after query execution, RAG pipelines typically leverage a single LM-call generation implementation where relevant data is fed in context. In this setting, several works study sub-problems related to table encoding [8], prompt compression [5], and prompt tuning [13] to optimize the in-context learning results.\nMore recent research, such as LOTUS [21], highlights the potential of composing iterative or recursive LM generation patterns for answering queries involving reasoning-based transformations, aggregations, or rankings across multiple data rows. Early work demonstrates the rich design space presented by these LM-based algorithms and promising results on several downstream tasks."}, {"title": "EVALUATION", "content": "In this section, we introduce the first TAG benchmark and evaluate a collection of baselines, aiming to address the following questions:\n(1) How do existing methods for table question answering perform on queries requiring semantic reasoning or world knowledge?\n(2) How does a hand-written implementation of the TAG model, which divides computational and reasoning steps across DBMS and LM operations, perform on these queries?"}, {"title": "Benchmark Methodology", "content": "Existing benchmarks have explored how models perform on basic queries answerable entirely from data in the data source. We build upon prior work by modifying queries such that they require knowledge not directly available in the data source or semantic reasoning to answer. We select BIRD [17], a widely used Text2SQL benchmark on which LMs have been evaluated, for its large scale tables along with its variety of domains and query types.\nDataset Our queries span 5 domains selected from BIRD, each containing diversity in query types. We select california_schools, debit_card_specializing, formula_1, codebase_community, and european_football_2 as the DB sources for our queries.\nQueries The BIRD benchmark defines fundamental query types, including match-based, comparison, ranking, and aggregation queries. We select queries among these types from the BIRD benchmark and modify them to require either world knowledge or semantic reasoning for the model to answer. As an example of a modified query requiring world knowledge, in the california_schools DB, a modified query adds an additional clause asking for only schools in the Bay Area. This information is not in the table and requires the model's world knowledge to answer. Next, a modified query requiring LM reasoning asks for the top 3 most sarcastic comments on a particular post in the codebase_community DB. For evaluation on these queries, we rely on human-labeled ground truth. Our final benchmark consists of 80 modified queries, 40 requiring parametric knowledge and 40 requiring reasoning, with 20 of each of the 4 chosen BIRD query types.\nEvaluation metrics We measure accuracy as the percentage of exact matches as compared to the labeled correct answer for the"}, {"title": "Baselines", "content": "Text2SQL In this baseline, the LM generates SQL code which is run to obtain an answer. For a given NL query, we construct an LM prompt containing table schemas for every table in the query's domain, using the same prompt format as in the BIRD work. We evaluate this baseline executing the generated SQL code in SQLite3 and measuring the number of incorrect answers, including instances where the model fails to generate valid SQL code.\nRetrieval Augmented Generation (RAG) RAG style methods have been explored for table retrieval [6, 25], where tabular data is embedded into an index for search. For our baseline, we use row-level embeddings. A given row is serialized as \"- col: val\" for each column before being embedded into a FAISS [7] index. During query time, we perform vector similarity search to retrieve 10 relevant rows to feed in context to our model along with the NL question.\nRetrieval + LM Rank We extend the RAG baseline by utilizing an LM to assign a score between 0 and 1 for retrieved rows to rerank rows before input to the model, as is done in the STaRK work [25]. We use Llama-3.1-70B-Instruct as our reranker.\nText2SQL + LM In this baseline, our model is first asked to generate SQL to retrieve a set of relevant rows to answer a given NL query. This is an important distinction from the Text2SQL baseline, where the model is asked to directly generate SQL code that alone provides an answer to the query when executed. Similar to the RAG baseline, relevant rows are fed in context to the model once retrieved.\nHand-written TAG We also evaluate hand-written TAG pipelines, which leverage expert knowledge of the table schema rather than automatic query synthesis from the natural language request to the database query. We implement our hand-written TAG pipelines with LOTUS [21]. The LOTUS API allows programmers to declaratively specify query pipelines with standard relational operators as well as semantic operators, such as LM-based filtering, ranking, and aggregations. LOTUS also provides an optimized semantic query execution engine, which we use to implement the query execution and answer generation steps of our hand-written TAG pipelines."}, {"title": "Results", "content": "Table 1 shows the accuracy, measured by exact match, and execution time of each method. As the table shows, across the selected BIRD query types, we find that our hand-written TAG baseline consistently achieves 40% exact match accuracy or better, where all other baselines fail to exceed 20% accuracy.\nThe Text2SQL baseline performs poorly on all baselines with an execution accuracy no higher than 20% but especially poorly on ranking queries with only 10% accuracy, as many of the ranking queries require reasoning over text. The Text2SQL + LM generation baseline has similar poor performance across baselines, but does worse on match-based and comparison queries with only 10% accuracy. On these query types, several context length errors occur trying to feed in many rows to the model after the executed SQL.\nTurning our attention to the RAG baseline, we see that it fails to answer a single query correctly across all query types, highlighting its poor fit for queries in this space. Adding LM reranking allows Retrieval + LM rank to answer a query correctly among the comparison queries, however the baseline still performs worse than all others besides RAG.\nOur hand-written TAG baseline answers 55% of queries correctly overall, performing best on comparison queries with an exact match accuracy of 65%. The baseline performs consistently well with over 50% accuracy on all query types except ranking queries, due to the higher difficulty in ordering items exactly. Overall, this method gives us between a 20% to 65% accuracy improvement over the standard baselines.\nAdditionally, Table 2 highlights the weaknesses of standard methods in answering the query types discussed in Section 3. Namely, vanilla Text2SQL especially struggles on queries requiring LM reasoning with 10% exact match accuracy, due to its omission of the answer generation step. Meanwhile, the RAG baseline and Retrieval + LM Rank baseline struggle on all query types, answering only one query correctly, due to their reliance on the LM to handle all exact computation over data. In contrast, the hand-written TAG baseline achieves over 50% accuracy on both queries requiring knowledge and queries requiring reasoning, emphasizing the TAG model's versatility in the queries it encapsulates.\nNotably, along with offering superior accuracy, the hand-written TAG method offers an efficient implementation with up to 3.1x lower execution time over other baselines. The hand-written baseline takes an average of 2.94 seconds for all queries. This relatively low execution time highlights that an efficient TAG system can be designed by exploiting efficient batched inference of LMs.\nLastly, we qualitatively analyze the results of each baseline on aggregation queries. The RAG baseline is only able to provide information about some of the races, as most of the relevant races are not retrieved. On the other hand, the Text2SQL + LM baseline is not able to utilize any information from the DBMS, relying only on parametric knowledge and providing no further analysis. The hand-written baseline provides a thorough summary of all the races from 1999 to 2017 held at Sepang International Circuit. We observe a similar trend across other aggregation queries provided by the benchmark, with initial results highlighting the potential of TAG systems to successfully aggregate large amounts of data to provide informative answers. We leave quantitative analysis to future work."}, {"title": "RELATED WORK", "content": "Text2SQL Text2SQL using LMs has been extensively explored in prior work. WikiSQL [33], Spider [29], and BIRD [17] are all popular datasets for cross-domain Text2SQL. These datasets contain structured data across many domains on which the task of converting natural language queries to SQL is evaluated. However, this direction does not utilize model capabilities beyond SQL generation, keeping queries that require reasoning or knowledge beyond a static data source out of scope.\nRetrieval Augmented Generation Retrieval augmented generation (RAG) [16] enables LMs to extend beyond their parametric knowledge to large collections of text. SQUAD [22] and HotPotQA [27] focus on question-answering over single document and multiple document sources respectively. The dense table retrieval (DTR) model [11] extends RAG to tabular data, embedding tabular context to retrieve relevant cells and rows for a query. Join-aware table retrieval [6] adds a table-table similarity score term to the DTR model to improve performance on complex queries involving joined tables. In contrast to prior RAG work, the TAG model encompasses a larger field of queries users have over their data by leveraging LM capabilities in the query execution step and allowing DBMS operations for exact computation over large amounts of data.\nNL Queries over Semi-structured Data Prior work has explored the relational information between table entities and unstructured entity fields in semi-structured data sources. STaRK [25] evaluates table retrieval methodologies across semi-structured knowledge bases (SKBs), including both structural and nonstructural information. SUQL [20] addresses the task of conversational search, where an LM is used as a semantic parser to handle unstructured components user queries over hybrid data. While these works primarily focus on natural language search queries over semi-structured data, we seek to explore a broader range of queries leveraging more LM capabilities for tasks beyond search and lookup.\nAgentic Data Assistants Recent work has explored LM agents as data assistants [12]. Spider2-V [4] explores multimodal agent"}, {"title": "CONCLUSION", "content": "In this work we proposed table-augmented generation (TAG) as a unified model for answering natural language questions over databases. We developed benchmarks to study two important types of queries: those that require world knowledge, and those that require semantic reasoning capabilities. Our systematic evaluation confirms that baseline methods are unable to make meaningful traction on these tasks. However, hand-written TAG pipelines can achieve up to 65% higher accuracy, demonstrating substantial research opportunities for building TAG systems."}]}