{"title": "Text2SQL is Not Enough: Unifying Al and Databases with TAG", "authors": ["Asim Biswal", "Liana Patel", "Joseph E. Gonzalez", "Siddharth Jha", "Amog Kamsetty", "Shu Liu", "Carlos Guestrin", "Matei Zaharia"], "abstract": "Al systems that serve natural language questions over databases\npromise to unlock tremendous value. Such systems would allow\nusers to leverage the powerful reasoning and knowledge capabil-\nities of language models (LMs) alongside the scalable computa-\ntional power of data management systems. These combined ca-\npabilities would empower users to ask arbitrary natural language\nquestions over custom data sources. However, existing methods\nand benchmarks insufficiently explore this setting. Text2SQL meth-\nods focus solely on natural language questions that can be ex-\npressed in relational algebra, representing a small subset of the\nquestions real users wish to ask. Likewise, Retrieval-Augmented\nGeneration (RAG) considers the limited subset of queries that can\nbe answered with point lookups to one or a few data records within\nthe database. We propose Table-Augmented Generation (TAG), a\nunified and general-purpose paradigm for answering natural lan-\nguage questions over databases. The TAG model represents a wide\nrange of interactions between the LM and database that have been\npreviously unexplored and creates exciting research opportuni-\nties for leveraging the world knowledge and reasoning capabili-\nties of LMs over data. We systematically develop benchmarks to\nstudy the TAG problem and find that standard methods answer\nno more than 20% of queries correctly, confirming the need for\nfurther research in this area.", "sections": [{"title": "1 INTRODUCTION", "content": "Language models promise to revolutionize data management by let-\nting users ask natural language questions over data, which has led\nto a great deal of research in Text2SQL and Retrieval-Augmented\nGeneration (RAG) methods. In our experience, however (includ-\ning from internal workloads and customers at Databricks), users'\nquestions often transcend the capabilities of these paradigms, de-\nmanding new research investment towards systems that combine\nthe logical reasoning abilities of database systems with the natural\nlanguage reasoning abilities of modern language models (LMs).\nIn particular, we find that real business users' questions often\nrequire sophisticated combinations of domain knowledge, world\nknowledge, exact computation, and semantic reasoning. Database\nsystems clearly provide a source of domain knowledge through\nthe up-to-date data they store, as well as exact computation at\nscale (which LMs are bad at).\nLMs offer to extend the existing capabilities of databases in two\nkey ways. First, LMs possess semantic reasoning capabilities over\ntextual data, a core element of many natural language user queries.\nFor example, a Databricks customer survey showed users wish to\nask questions like which customer reviews of product X are positive?,\nor why did my sales drop during this period?. These questions present\ncomplex reasoning-based tasks, such as sentiment analysis over\nfree-text fields or summarization of trends. LMs are well-suited to\nthese tasks, which cannot be modeled by the exact computation or\nrelational primitives in traditional database systems.\nSecondly, the LM, using knowledge learned during model train-\ning and stored implicitly by the model's weights, can powerfully\naugment the user's data with world knowledge that is not cap-\ntured explicitly by the database's table schema. As an example, a\nDatabricks internal AI user asked what are the QoQ trends for the \"re-\ntail\" vertical? over a table containing attributes for account names,\nproducts and revenue. To answer this query the system must under-\nstand how the business defines QoQ (e.g., the quarter over quarter\ntrends from the last quarter to the current quarter or this quarter\nlast year to this quarter this year), as well as which companies are\nconsidered to be in the retail vertical. This task is well-suited to\nleverage the knowledge held by a pre-trained or fine-tuned LM.\nSystems that efficiently leverage databases and LMs together to\nserve natural language queries, in their full generality, hold poten-\ntial to transform the way users understand their data. Unfortunately,\nthese questions cannot be answered today by common methods,\nsuch as Text2SQL and RAG. While Text2SQL methods [26, 28, 31, 32]\nare suitable for the subset of natural language queries that have\ndirect relational equivalents, they cannot handle the vast array of\nuser queries that require semantic reasoning or world knowledge.\nFor instance, the previous user query asking which customer reviews\nare positive may require logical row-wise LM reasoning over re-\nviews to classify each as positive or negative. Similarly the question\nwhich asks why sales dropped entails a reasoning question that must\naggregate information across many table entries.\nOn the other hand, the RAG model is limited to simple relevance-\nbased point lookups to a few data records, followed by a single LM\ninvocation. This model serves only the subset of queries answer-\nable by point lookups and also fails to leverage the richer query\nexecution capabilities of many database systems, which leaves com-\nputational tasks (e.g., counting, math and filtering) to a single in-\nvocation of the error-prone LM. In addition to being error prone\nand inefficient at computational tasks, LMs have also been shown\nto perform poorly on long-context prompts limiting their ability to\nreason about data at scale in the generation phase of RAG.\nWe instead propose table-augmented generation (TAG) as a\nunified paradigm for systems that answer natural language ques-\ntions over databases. Specifically, TAG defines three key steps, as\nshown in Figure 1. First, the query synthesis step syn translates\nthe user's arbitrary natural language request R to an executable\ndatabase query Q. Then, the query execution step exec executes Q\non the database system to efficiently compute the relevant data T.\nLastly, the answer generation step gen utilizes R and T, where the\nLM is orchestrated, possibly in iterative or recursive patterns over\nthe data, to generate the final natural language answer A. The TAG\nmodel is simple, but powerful: it is defined by the following three"}, {"title": "2 THE TAG MODEL", "content": "We now describe the TAG model, which takes a natural language\nrequest R and returns a natural language answer A grounded in\nthe data source. We outline three main steps that TAG systems im-\nplement: query synthesis, query execution, and answer generation.\nWe define TAG tractably as a single iteration of these steps, but one\ncan consider extending TAG in a multi-hop fashion.\n2.1 Query Synthesis\nThe syn function takes a natural language request R and generates\na query Q to be executed by the database system. Given a user\nrequest, this step is responsible for (a) deducing which data is\nrelevant to answering the request (e.g., using the table schema), and\n(b) performing semantic parsing to translate the user request into\na query that can be executed by the database system. This query\ncould be in any query language, but in our example we use SQL.\nFigure 1 shows an example TAG instantiation for the user query\nwhich asks, \"Summarize the reviews of the highest grossing romance\nmovie considered a 'classic'\". Here, the data source contains infor-\nmation about each movie's title, revenue, genre, and an associated\nreview. In this step, the system leverages the semantic reasoning\nabilities of the LM to generate a SQL query that uses attributes on\nmovie_title, review, revenue, and genre from the data source.\nNote that in this example, the database API is able to execute LM\nUDFs within SQL queries, so this step also introduces calls to the\nLM for each row to identify classic films within the query.\n2.2 Query Execution\nIn the query execution step, the exec function executes the query\nQ in the database system to obtain the table T. This step leverages\nthe database query engine to efficiently execute the query over\nvast amounts of stored data. The database API can be served by a\nwide variety of systems, which we explore in Section 3. Some APIs\nmay allow for LM-based operators [18\u201321], permitting the data-\nbase engine to leverage the LM's world knowledge and reasoning\ncapabilities within exec.\nIn the example shown in Figure 1, the database query is a se-\nlection and ranking query written in SQL, which returns a table\ncontaining relevant rows. The query performs the selection us-\ning an LM to assess which movies are classics according to their\nmovie_title, as well as a standard filter on genre to find romance\nmovies. The query also ranks the results based on revenue to find\nthe highest grossing film. As the figure shows, the resulting table\ncontains reviews for the movie \"Titanic\".\n2.3 Answer Generation\nThe answer generation step in TAG mirrors the generation step\nin RAG. In this step, the gen function uses the LM to generate\nan answer A to the user's natural language request R, using the\ncomputed data T.\nFigure 1 shows the final stage of the example TAG pipeline\noutputting a summary of the reviews on \"Titanic\" as the answer\nto the original user request. In the example, the relevant data T is\nencoded as a string for the model to process. The encoded table is\npassed to the LM along with the original user request, R. To obtain\nthe answer, this step leverages the model's semantic reasoning\ncapabilities over the review column to summarize the reviews."}, {"title": "3 TAG DESIGN SPACE", "content": "In this section, we explore the generality of the TAG model and\ndescribe the rich design space it produces, highlighting several\nunder-studied opportunities for further research.\nQuery Types The TAG model is expressive enough to serve a broad\nrange of natural language user queries. We consider two important\nquery categorizations, according to (a) the level of data aggregation\nrequired to answer the query and (b) the knowledge and capabilities\nneeded for answering the query. First, the TAG model captures both\npoint queries, such as retrieval-based questions [3, 9, 15, 16, 25, 30]\nwhich require look-ups to one or a few rows of the database, as well\nas aggregation queries, such as summarization or ranking-based\nquestions which require logical reasoning across many rows of\nthe database. Secondly, the TAG model enables natural language\nqueries with varying demands on the system to provide data or\nreasoning-based capabilities, including for tasks such as sentiment\nanalysis and classification.\nData Model The underlying data model can take many forms.\nOur implementation uses relational databases to store and retrieve\nstructured attributes for knowledge-grounding in the downstream\nquestion-answering task. Others may operate on more unstructured\ndata (e.g., free-text, images, video, and audio) or semi-structured\ndata, which may be stored with a variety of data models, such as\nkey-value, graph, vector, document, or object stores.\nDatabase Execution Engine and API The underlying system\nused to store the data can use many possible database execution\nengines. Text2SQL considers the setting of an SQL-based query\nengine for retrieving relational data for user queries. In this setting,\nsyn will leverage the knowledge of the data source, such as the table\nschema, and return a SQL query to perform the retrieval step. In\nanother common setting, retrieval systems over vector embeddings,\nsyn transforms the natural language query into an embedding and\nexec performs similarity-based retrieval over the vector store.\nWhile these two settings have been widely studied, several under-\nstudied alternative settings present interesting opportunities for\nefficiently implementing TAG systems to serve a broader range\nof queries. For instance, recent works augment relational models\nwith semantic operators [21], which provide a set of declarative\nAI-based operators (e.g., filtering, ranking, aggregating, and per-\nforming search with natural language specifiers) or LM user-defined\nfunctions [19], which provide a general-purpose LM function. Ad-\nditionally, query languages like MADLib [10], Google's BigQuery\nML [1], and Microsoft's Predictive SQL [24] augment SQL-based\nAPIs with native ML-based functions. Leveraging these systems\nprovides unique opportunities for executing optimized reasoning-\nbased retrieval patterns. For instance, in the example shown in\nFigure 1, a TAG pipeline implemented with semantic operators [21]\nmight use a sem_filter operator to filter rows based on whether\nthey are a 'classic' during the query execution step.\nLM Generation Patterns Given the table T of relevant data, gen\ncan be comprised from a vast array of implementation decisions\nto produce the final natural language answer A in response to the\nuser request R. While Text2SQL omits the final generation step and\nstops short after query execution, RAG pipelines typically leverage\na single LM-call generation implementation where relevant data\nis fed in context. In this setting, several works study sub-problems\nrelated to table encoding [8], prompt compression [5], and prompt\ntuning [13] to optimize the in-context learning results.\nMore recent research, such as LOTUS [21], highlights the po-\ntential of composing iterative or recursive LM generation patterns\nfor answering queries involving reasoning-based transformations,\naggregations, or rankings across multiple data rows. Early work\ndemonstrates the rich design space presented by these LM-based\nalgorithms and promising results on several downstream tasks."}, {"title": "4 EVALUATION", "content": "In this section, we introduce the first TAG benchmark and evaluate\na collection of baselines, aiming to address the following questions:\n(1) How do existing methods for table question answering\nperform on queries requiring semantic reasoning or world\nknowledge?\n(2) How does a hand-written implementation of the TAG model,\nwhich divides computational and reasoning steps across\nDBMS and LM operations, perform on these queries?\n4.1 Benchmark Methodology\nExisting benchmarks have explored how models perform on ba-\nsic queries answerable entirely from data in the data source. We\nbuild upon prior work by modifying queries such that they require\nknowledge not directly available in the data source or semantic\nreasoning to answer. We select BIRD [17], a widely used Text2SQL\nbenchmark on which LMs have been evaluated, for its large scale\ntables along with its variety of domains and query types.\nDataset Our queries span 5 domains selected from BIRD, each\ncontaining diversity in query types. We select california_schools,\ndebit_card_specializing, formula_1, codebase_community, and euro-\npean_football_2 as the DB sources for our queries.\nQueries The BIRD benchmark defines fundamental query types, in-\ncluding match-based, comparison, ranking, and aggregation queries.\nWe select queries among these types from the BIRD benchmark\nand modify them to require either world knowledge or semantic\nreasoning for the model to answer. As an example of a modified\nquery requiring world knowledge, in the california_schools DB, a\nmodified query adds an additional clause asking for only schools\nin the Bay Area. This information is not in the table and requires\nthe model's world knowledge to answer. Next, a modified query\nrequiring LM reasoning asks for the top 3 most sarcastic comments\non a particular post in the codebase_community DB. For evaluation\non these queries, we rely on human-labeled ground truth. Our final\nbenchmark consists of 80 modified queries, 40 requiring parametric\nknowledge and 40 requiring reasoning, with 20 of each of the 4\nchosen BIRD query types.\nEvaluation metrics We measure accuracy as the percentage of\nexact matches as compared to the labeled correct answer for the"}, {"title": "4.2 Baselines", "content": "Text2SQL In this baseline, the LM generates SQL code which is\nrun to obtain an answer. For a given NL query, we construct an\nLM prompt containing table schemas for every table in the query's\ndomain, using the same prompt format as in the BIRD work. We\nevaluate this baseline executing the generated SQL code in SQLite3\nand measuring the number of incorrect answers, including instances\nwhere the model fails to generate valid SQL code.\nRetrieval Augmented Generation (RAG) RAG style methods\nhave been explored for table retrieval [6, 25], where tabular data is\nembedded into an index for search. For our baseline, we use row-\nlevel embeddings. A given row is serialized as \"- col: val\" for each\ncolumn before being embedded into a FAISS [7] index. During query\ntime, we perform vector similarity search to retrieve 10 relevant\nrows to feed in context to our model along with the NL question.\nRetrieval + LM Rank We extend the RAG baseline by utilizing an\nLM to assign a score between 0 and 1 for retrieved rows to rerank\nrows before input to the model, as is done in the STaRK work [25].\nWe use Llama-3.1-70B-Instruct as our reranker.\nText2SQL + LM In this baseline, our model is first asked to generate\nSQL to retrieve a set of relevant rows to answer a given NL query.\nThis is an important distinction from the Text2SQL baseline, where\nthe model is asked to directly generate SQL code that alone provides\nan answer to the query when executed. Similar to the RAG baseline,\nrelevant rows are fed in context to the model once retrieved.\nHand-written TAG We also evaluate hand-written TAG pipelines,\nwhich leverage expert knowledge of the table schema rather than\nautomatic query synthesis from the natural language request to\nthe database query. We implement our hand-written TAG pipelines\nwith LOTUS [21]. The LOTUS API allows programmers to declara-\ntively specify query pipelines with standard relational operators as\nwell as semantic operators, such as LM-based filtering, ranking, and\naggregations. LOTUS also provides an optimized semantic query\nexecution engine, which we use to implement the query execution\nand answer generation steps of our hand-written TAG pipelines."}, {"title": "4.3 Results", "content": "Table 1 shows the accuracy, measured by exact match, and execu-\ntion time of each method. As the table shows, across the selected\nBIRD query types, we find that our hand-written TAG baseline\nconsistently achieves 40% exact match accuracy or better, where\nall other baselines fail to exceed 20% accuracy.\nThe Text2SQL baseline performs poorly on all baselines with an\nexecution accuracy no higher than 20% but especially poorly on\nranking queries with only 10% accuracy, as many of the ranking\nqueries require reasoning over text. The Text2SQL + LM genera-\ntion baseline has similar poor performance across baselines, but\ndoes worse on match-based and comparison queries with only 10%\naccuracy. On these query types, several context length errors occur\ntrying to feed in many rows to the model after the executed SQL.\nTurning our attention to the RAG baseline, we see that it fails to\nanswer a single query correctly across all query types, highlighting\nits poor fit for queries in this space. Adding LM reranking allows\nRetrieval + LM rank to answer a query correctly among the com-\nparison queries, however the baseline still performs worse than all\nothers besides RAG.\nOur hand-written TAG baseline answers 55% of queries correctly\noverall, performing best on comparison queries with an exact match\naccuracy of 65%. The baseline performs consistently well with over\n50% accuracy on all query types except ranking queries, due to the\nhigher difficulty in ordering items exactly. Overall, this method\ngives us between a 20% to 65% accuracy improvement over the\nstandard baselines.\nAdditionally, Table 2 highlights the weaknesses of standard meth-\nods in answering the query types discussed in Section 3. Namely,\nvanilla Text2SQL especially struggles on queries requiring LM rea-\nsoning with 10% exact match accuracy, due to its omission of the\nanswer generation step. Meanwhile, the RAG baseline and Retrieval\n+ LM Rank baseline struggle on all query types, answering only one\nquery correctly, due to their reliance on the LM to handle all exact\ncomputation over data. In contrast, the hand-written TAG baseline\nachieves over 50% accuracy on both queries requiring knowledge\nand queries requiring reasoning, emphasizing the TAG model's\nversatility in the queries it encapsulates.\nNotably, along with offering superior accuracy, the hand-written\nTAG method offers an efficient implementation with up to 3.1x\nlower execution time over other baselines. The hand-written base-\nline takes an average of 2.94 seconds for all queries. This relatively\nlow execution time highlights that an efficient TAG system can be\ndesigned by exploiting efficient batched inference of LMs.\nLastly, we qualitatively analyze the results of each baseline on\naggregation queries. Figure 2 shows the results for the RAG, Naive\nTAG, and hand-written baselines on the example query \"Provide\ninformation about the races held on Sepang International Circuit.\".\nThe RAG baseline is only able to provide information about some\nof the races, as most of the relevant races are not retrieved. On the\nother hand, the Text2SQL + LM baseline is not able to utilize any\ninformation from the DBMS, relying only on parametric knowledge\nand providing no further analysis. The hand-written baseline pro-\nvides a thorough summary of all the races from 1999 to 2017 held\nat Sepang International Circuit. We observe a similar trend across\nother aggregation queries provided by the benchmark, with initial\nresults highlighting the potential of TAG systems to successfully\naggregate large amounts of data to provide informative answers.\nWe leave quantitative analysis to future work."}, {"title": "5 RELATED WORK", "content": "Text2SQL Text2SQL using LMs has been extensively explored in\nprior work. WikiSQL [33], Spider [29], and BIRD [17] are all pop-\nular datasets for cross-domain Text2SQL. These datasets contain\nstructured data across many domains on which the task of convert-\ning natural language queries to SQL is evaluated. However, this\ndirection does not utilize model capabilities beyond SQL genera-\ntion, keeping queries that require reasoning or knowledge beyond\na static data source out of scope.\nRetrieval Augmented Generation Retrieval augmented gener-\nation (RAG) [16] enables LMs to extend beyond their parametric\nknowledge to large collections of text. SQUAD [22] and HotPotQA\n[27] focus on question-answering over single document and multi-\nple document sources respectively. The dense table retrieval (DTR)\nmodel [11] extends RAG to tabular data, embedding tabular context\nto retrieve relevant cells and rows for a query. Join-aware table re-\ntrieval [6] adds a table-table similarity score term to the DTR model\nto improve performance on complex queries involving joined ta-\nbles. In contrast to prior RAG work, the TAG model encompasses\na larger field of queries users have over their data by leveraging\nLM capabilities in the query execution step and allowing DBMS\noperations for exact computation over large amounts of data.\nNL Queries over Semi-structured Data Prior work has explored\nthe relational information between table entities and unstructured\nentity fields in semi-structured data sources. STaRK [25] evaluates\ntable retrieval methodologies across semi-structured knowledge\nbases (SKBs), including both structural and nonstructural informa-\ntion. SUQL [20] addresses the task of conversational search, where\nan LM is used as a semantic parser to handle unstructured compo-\nnents user queries over hybrid data. While these works primarily\nfocus on natural language search queries over semi-structured data,\nwe seek to explore a broader range of queries leveraging more LM\ncapabilities for tasks beyond search and lookup.\nAgentic Data Assistants Recent work has explored LM agents\nas data assistants [12]. Spider2-V [4] explores multimodal agent"}, {"title": "6 CONCLUSION", "content": "In this work we proposed table-augmented generation (TAG) as\na unified model for answering natural language questions over\ndatabases. We developed benchmarks to study two important types\nof queries: those that require world knowledge, and those that re-\nquire semantic reasoning capabilities. Our systematic evaluation\nconfirms that baseline methods are unable to make meaningful\ntraction on these tasks. However, hand-written TAG pipelines can\nachieve up to 65% higher accuracy, demonstrating substantial re-\nsearch opportunities for building TAG systems."}, {"title": "7 ACKNOWLEDGEMENTS", "content": "This research was supported in past by affiliate members and sup-\nporters of the Stanford DAWN project and the Sky Computing Lab\nat Berkeley, including Accenture, AMD, Anyscale, Cisco, Google,\nIBM, Intel, Meta, Microsoft, Mohamed Bin Zayed University of Ar-\ntificial Intelligence, NVIDIA, Samsung SDS, SAP, VMware, and a\nSloan Fellowship. Any opinions, findings, and conclusions or rec-\nommendations expressed in this material are those of the authors\nand do not necessarily reflect the views of the sponsors."}, {"title": "A SAMPLE QUERIES", "content": "We detail the modifications made to BIRD queries for our benchmark. Each query is modified to require either LM knowledge or reasoning\nto answer. Sample queries are shown below.\nOriginal BIRD Query:\nWhat is the grade span offered in the school with the highest longitude?\nModified Query:\nWhat is the grade span offered in the school with the highest longitude in cities in that are part of the 'Silicon Valley' region?\nAnalysis:\nThis query is modified to require LM knowledge of which cities are within the Silicon Valley region of California, information not available\nin the data source.\nOriginal BIRD Query:\nAmong the players whose height is over 180, how many of them have a volley score of over 70?\nModified Query:\nAmong the players whose height is over 180, how many of them have a volley score of over 70 and are taller than Stephen Curry?\nAnalysis:\nThis query is modified to require LM knowledge of how tall Stephen Curry is.\nOriginal BIRD Query:\nWhat are the titles of the top 5 posts with the highest popularity?\nModified Query:\nOf the 5 posts wih highest popularity, list their titles in order of most technical to least technical.\nAnalysis:\nThis query is modified to require LM reasoning over a textual field, the post's title.\nOriginal BIRD Query:\nWrite all comments made on the post titled 'How does gentle boosting differ from AdaBoost?'\nModified Query:\nSummarize the comments made on the post titled 'How does gentle boosting differ from AdaBoost?' to answer the original question.\nAnalysis:\nThis query is modified to rely on LM reasoning over text on the textual comment fields to provide a summary."}, {"title": "BLM PROMPTS", "content": "We summarize the prompts used with instruction tuned Llama-3.1 80B for query synthesis and answer generation.\nB.1 Query Synthesis\nFor the query synthesis step, in our case a Text2SQL step, we use the same table schema encoding and LM prompt as the original BIRD\nbenchmark. An example prompt for query synthesis is shown below."}, {"title": "B.2 Answer Generation", "content": "On the Text2SQL + LM and RAG baselines, the answer generation step requires the LM to answer a user question with the provided rows in\ncontext. We utilize a separate prompt for aggregation queries, while match-based, comparison, and ranking share the same prompt. We show\nboth prompts below.\n3 You will be given a list of data points and a question. Use the data points to answer the question. Your answer must be\nvalueN]. If you are\na list of values that is evaluatable in Python. Respond in the format [value1, value2,\nunable to answer the question, respond with (). Respond with only the list of values and nothing else. If a value\nis a string, it must be enclosed in double quotes."}, {"title": "C HANDWRITTEN PIPELINES", "content": "We use the LOTUS package to construct hand-written TAG pipelines. For each query in our benchmark, a pipeline consisting of a series of\ndataframe transformations and filters along with LOTUS semantic LM operators was written in Python. Example pipelines are visible below.\n2 query = \"What is the grade span offered in the school with the highest longitude in cities in that are part of the\n3 Silicon Valley region?\""}]}