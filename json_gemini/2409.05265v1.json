{"title": "Learning Submodular Sequencing from Samples", "authors": ["Jing Yuan", "Shaojie Tang"], "abstract": "This paper addresses the problem of sequential submodular maximization: selecting and ranking items in a sequence to optimize some composite submodular function. In contrast to most of the previous works, which assume access to the utility function, we assume that we are given only a set of samples. Each sample includes a random sequence of items and its associated utility. We present an algorithm that, given polynomially many samples drawn from a two-stage uniform distribution, achieves an approximation ratio dependent on the curvature of individual submodular functions. Our results apply in a wide variety of real-world scenarios, such as ranking products in online retail platforms, where complete knowledge of the utility function is often impossible to obtain. Our algorithm gives an empirically useful solution in such contexts, thus proving that limited data can be of great use in sequencing tasks. From a technical perspective, our results extend prior work on \"optimization from samples\" by generalizing from optimizing a set function to a sequence-dependent function.", "sections": [{"title": "Introduction", "content": "Submodular optimization is one of the most important problems in machine learning, with applications in sparse reconstruction (Das and Kempe 2011), data summarization (Lin and Bilmes 2011), active learning (Golovin and Krause 2011, Tang and Yuan 2022), and viral marketing (Tang and Yuan 2020). Most of the existing work is on the problem of selecting a subset of items that maximizes some submodular function. Many real applications, however, require not only the selection of items but also their ranking in a certain order (Azar and Gamzu 2011, Tschiatschek et al. 2017, Tang and Yuan 2021).\nThis paper focuses on one such problem, termed sequential submodular maximization (Asadpour et al. 2022, Zhang et al. 2022, Tang and Yuan 2024). The problem's input consists of a ground set \u03a9 and k submodular functions, denoted as $f_1,\\ldots, f_k : 2^{\\Omega} \\rightarrow \\mathbb{R}_+$. Our objective is to select a sequence of k items, denoted as $\\pi= {\\pi_1,\\ldots,\\pi_k}$, from \u03a9, aiming to maximize $F(\\pi) \\stackrel{\\text{def}}{=} \\sum_{j\\in[k]} f_t(\\pi_{[t]})$. Here, $\\pi_{[t]} \\stackrel{\\text{def}}{=} {\\pi_1,\\ldots, \\pi_t}$ represents the first t items of \u03c0.\nNotably, each function $f_t$ takes the first t items from the ranking sequence \u03c0 as its input."}, {"title": "Preliminaries and Problem Formulation", "content": "Throughout the remainder of this paper, let $[m] = {0,1,2,...,m}$ for any positive integer m. Given a function f, let $f(i | S) = f(S\\cup{i}) - f(S)$ denote the marginal utility of an item $i\\in \\Omega$ on top of a set of items $S \\subseteq \\Omega$. We say a function f is submodular if and only if for any two sets X and Y such that $X \\subseteq Y$ and any item $i \\notin Y, f(i | X) \\geq f(i | Y)$. Moreover, we say a submodular function f has curvature $c \\in [0, 1]$ if $f (i | S) \\geq (1-c)f({i})$ for any $S\\subseteq \\Omega$ and $i \\notin S$.\nNow we are ready to introduce our research problem. Given k submodular functions $f_1,\\ldots, f_k : 2^{\\Omega} \\rightarrow \\mathbb{R}_+$, the sequential submodular maximization problem aims to find a sequence $\\pi= {\\pi_1,\\ldots,\\pi_k}$ from a ground set \u03a9 that maximizes the value of $F(\\pi)$. Here,\n$F(\\pi) \\stackrel{\\text{def}}{=} \\sum_{t\\in[k]} f_t(\\pi_{[t]}),$ \nwhere $\\pi_{[t]} \\stackrel{\\text{def}}{=} {\\pi_1,\\cdots, \\pi_t}$ represents the first t items of \u03c0. That is, each function $f_t$ takes the first t items from \u03c0 as its input. Throughout this paper, we use the notation \u03c0 to denote both a sequence of items and the set of items in that sequence.\nExisting studies on sequential submodular maximization all assume that $f_1,\\ldots, f_k$ are known in advance, however, in our setting, we do not have direct access to those functions. Instead, we rely on a dataset comprising observations $(\\pi,\\varphi(\\pi))$, where in each sample $(\\pi,\\varphi(\\pi))$, \u03c0 denotes a feasible sequence and \u03c6(\u03c0) denotes the observed utility of \u03c0. It is important to note that the observed utility of a sequence \u03c0 may be subject to randomness, rendering \u03c6(\u03c0) a realization of this stochastic variable. Take, for instance, the product sequencing example outlined in the introduction: F(\u03c0) denotes the likelihood of purchase"}, {"title": "Problem Formulation", "content": "Our objective is to compute a sequence $\\pi= {\\pi_1,\\cdots,\\pi_k}$ that maximizes the value of F(\u03c0) based on the samples drawn from a distribution D. We say this problem is \u03b3-optimizable with respect to a distribution D, if there exists an algorithm which, given polynomially many samples drawn from D, returns with high probability a sequence \u03c0 of size at most k such that $F(\\pi) \\geq \\gamma F(\\pi^*)$ where $\u03c0^*$ denotes the optimal solution of this problem.\nAs with the standard PMAC-learning framework, we fix a distribution called two-stage uniform sampling and assume that samples are drawn i.i.d. from this distribution. In particular, two-stage uniform sampling works in two stages: In the first stage, a length t is randomly selected from the set {1,\u2026, k} with uniform probability. Subsequently, a sequence of length t is randomly chosen, and its realized utility is observed. In the following, we present an approximation algorithm with respect to this distribution."}, {"title": "Algorithm Design", "content": "Our algorithm first estimates the expected marginal contribution \u2206(i, t) of each item $i \\in \\Omega$ to a uniformly random sequence of size t, that does not contain i, for every item $i \\in \\Omega$ and every size $t \\in [k \u2212 1]$. A formal definition of \u2206(i,t) is given by:\n$\\Delta(i,t) = \\mathbb{E}_{\\pi_{t+1,i}} [F(I_{t+1,i})] - \\mathbb{E}_{\\pi_{t,-i}} [F(I_{t,-i})]$ \nwhere $I_{t+1,i}$ denotes a random sequence of length t +1 with i being placed at the last slot and $I_{t,-i}$ denotes a random sequence of length t that does not contain i. Unfortunately, one can not access the value of either $\\mathbb{E}_{\\pi_{t+1,i}} [F(I_{t+1,i})]$ or $\\mathbb{E}_{\\pi_{t,-i}} [F(I_{t,-i})]$ directly. To estimate these values, we draw inspiration from a technique proposed in (Balkanski et al. 2016), estimating the value of $\\mathbb{E}_{\\pi_{t+1,i}} [F(I_{t+1,i})]$ and $\\mathbb{E}_{\\pi_{t,-i}} [F(I_{t,-i})]$ using avg($\\Phi_{t+1,i}$) and avg($\\Phi_{t,-i}$) respectively. Here, avg($\\Phi_{t+1,i}$) represents the average (observed) utility of all samples where the length is t + 1 and i is placed at the last slot, while avg($\\Phi_{t,-i}$) denotes"}, {"title": "Performance Analysis", "content": "Let \u03c0\u00ba be the sequence returned from Algorithm 1, we next analyze the approximation ratio of \u03c0\u00ba, assuming $f_t$ is a monotone submodular function with curvature c for all $t \\in {1,2,\\ldots,k}$. We first present two technical lemmas. The first lemma derives an approximation ratio for the case when $(1 -c)^2 \\geq \\alpha\\cdot \\frac{1-c}{1+c-c^2}$, while the second lemma derives an approximation ratio for the remaining cases. The final approximation ratio is the better of these values.\nAssume $f_t$ is a monotone submodular function with curvature c for all $t \\in {1,2,\\ldots,k}$, for the case when $(1 -c)^2 \\geq \\alpha\\cdot \\frac{1-c}{1+c-c^2}$, we have that, with a sufficiently large polynomial number of samples,\n$F(\\pi^{\\circ}) \\geq ((1 -c)^2 \u2013 o(1)) F(\\pi^*)$ \nwhere $\\alpha = \\frac{n-k}{n} \\cdot \\frac{n-k-1}{n-1} \\cdot \\cdot\\cdot \\frac{n-2k+1}{n-k+1}$.\nProof: According to Line 2 in Algorithm 1, when $(1 - c)^2 \\geq \\alpha\\cdot \\frac{1-c}{1+c-c^2}$, it returns $\u03c0^{\\flat}$ as \u03c0\u00ba. Here \u03c0\u00ba denotes the sequence corresponding to the optimal solution of P.1. To prove this lemma, it suffices to show that $F(\\pi^{\\flat}) \\geq ((1 - c)^2 \u2013 o(1))F(\\pi^*)$."}, {"title": "Discussions", "content": "We present two remarks: one regarding the design of our algorithm and the other addressing a potential gap in existing studies.\nFirst, our algorithm design and analysis assume a good estimation of the curvature c of each individual function. This assumption might not always hold; if c is unknown, we can adopt \u03c0\u00ba as our final solution, yielding an approximation ratio of $(1 \u2013 c)^2$, as shown in Lemma 1.\nSecond, while our study builds on the work of Balkanski et al. (2016) by extending the \"learning-from-samples\" approach from set functions to sequence functions, we identify a potential gap in their analysis. Specifically, their proof of Lemma 1 relies on the assumption that $f(R|S^*) \\geq (1 \u2013 c) f (R)$, where $S^*$ is an optimal set solution, R is a uniformly random set of size k-1 (with k being the size constraint of the final solution) and c is the curvature of function f. This assumption is, unfortunately, not generally valid; according to the definition of the curvature c, this assumption holds only if $R\\cap S^* = \\emptyset$. Our study addresses this issue by introducing the notion of \u03b1 and further extends their research to a more complex sequence function."}, {"title": "Appendix", "content": "With a sufficiently large polynomial number of samples, the estimation $\\tilde{\\Delta}(i,t)$ is $n^2$-close to $\u25b3(i,t)$ for all $i \\in \\Omega$ and $t \\in [k \u2212 1]$, with high probability, i.e., $\\tilde{\\Delta}(i,t)+\\frac{\\delta}{n^2}\\geq$ $\u25b3(i, t) \\geq \u25b3(i,t) - \\frac{\\delta}{n^2}$ where \u03b4 = $\\underset{\\pi:\\vert \\pi \\vert \\leq k}{max} \\varphi(\\pi)$ denotes the maximum realized value of any sequence with a length of at most k.\nProof: Our proof is inspired by the one presented in Balkanski et al. (2016) (Appendix A); however, we extend their analysis from set functions to sequence functions. Consider an arbitrary pair of $i \\in \\Omega$ and $t \\in [k \u2212 1]$.\nObservation 1: The probability of sampling a sequence of length t is no less than 1/k, whose value is at least 1/n. Note that the case when t = 0 is trivial because the value of an empty sequence is known to be zero. Furthermore, given that the sampled sequence has a length of t, the probability of it not containing item i is at least 1-t/n \u2265 1/n. Hence, the probability of sampling a sequence of length t without i is at least $1/n^2$.\nObservation 2: The probability of sampling a sequence of length t + 1 is no less than 1/k, where 1/k is at least 1/n. Additionally, given that the sampled sequence has a length of"}]}