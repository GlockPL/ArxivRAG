{"title": "Semantic-Geometric-Physical-Driven Robot Manipulation Skill Transfer via Skill Library and Tactile Representation", "authors": ["Mingchao Qi", "Yuanjin Li", "Xing Liu", "Zhengxiong Liu", "Panfeng Huang"], "abstract": "Deploying robots in open-world environments involves complex tasks characterized by long sequences and rich interactions, necessitating efficient transfer of robotic skills across diverse and complex scenarios. To address this challenge, we propose a skill library framework based on knowledge graphs, which endows robots with high-level skill awareness and spatial semantic understanding. The framework hierarchically organizes operational knowledge by constructing a \"task graph\" and a \"scene graph\" to represent task and scene semantic information, respectively. We introduce a \"state graph\" to facilitate interaction between high-level task planning and low-level scene information. Furthermore, we propose a hierarchical transfer framework for operational skills. At the task level, the framework integrates contextual learning and chain-of-thought prompting within a four-stage prompt paradigm, leveraging large language models' (LLMs) reasoning and generalization capabilities to achieve task-level subtask sequence transfer. At the motion level, an adaptive trajectory transfer method is developed using the A* algorithm and the skill library, enabling motion-level adaptive trajectory transfer. At the physical level, we introduce an adaptive contour extraction and posture perception method based on tactile perception. This method dynamically obtains high-precision contour and posture information from visual-tactile texture data and adjusts transferred skills, such as contact positions and postures, to ensure effectiveness in new environments. Experimental results validate the effectiveness of the proposed methods. Project website: https://github.com/MingchaoQi/skill_transfer", "sections": [{"title": "I. INTRODUCTION", "content": "Deploying robots in open-world environments involves managing complex operational scenarios, particularly when dealing with long sequences and contact-rich manipulation tasks [1], [2], [3]. The diversity of these scenarios makes it impractical to program specific responses for each situation, necessitating a high degree of skill generalization. This ensures that robots can efficiently transfer learned skills to similar new scenarios. For instance, tasks such as opening doors and drawers require fundamental skills like approaching, grasping, pulling, and releasing, despite differences in objects and motion patterns. In these contexts, robots must flexibly transfer and integrate these skills.\n\nA prevalent method for skill transfer in robotic platforms is Learning from Demonstration (LfD), which enables robots to learn strategies from human or heuristic demonstrations. For long-horizon tasks, robots must plan, execute, and sequence"}, {"title": "II. RELATED WORK", "content": "A robot skill library is a structured set of skills, each corresponding to a specific task's operational process. These libraries are built through Imitation Learning and Reinforcement Learning, aimed at enabling skill reuse and composition for improved multi-tasking. Earlier robotic systems were task-specific with limited generalization. To improve adaptability, researchers proposed generalized skill libraries using modular skills for task planning [14]. These libraries include basic and complex skills can be categorized as task-specific or general [15], [16]. Skills are often represented using behavior trees or finite state machines [17], and transfer learning allows robots to apply skills to new tasks."}, {"title": "B. Skill Transfer", "content": "Skill transfer reduces training time and sample requirements by transferring knowledge from a source to a target"}, {"title": "C. Tactile Perception", "content": "Li et al. developed a method for precise widget positioning, enabling accurate USB connector placement using advanced matching techniques [23]. Izatt et al. enhanced this by integrating a filtering algorithm that merges vision and tactile data for object pose tracking, enabling precise tool operations [24]. Both used model-based tracking with predefined geometries. Daolin Ma et al. addressed locating contact points on unknown objects by solving for constraints [25]. Yu She et al. applied thresholding and PCA to deduce cable orientation from depth maps [26]. Achu Wilson et al. combined tactile feedback with vision for cable routing [27]."}, {"title": "III. METHOD", "content": "This paper proposes a knowledge graph-based skill library framework designed to enable robots to comprehend high-level skills and spatial semantic information in scenes, facilitating the transfer of robot skills across similar scenarios. The skill library is depicted using a formalized semantic model"}, {"title": "The Construction of Operational Task Graph", "content": "A complex task can be represented as a collection of subtasks $T = T_1, T_2,..., T_n$, where each subtask $T_i$ is a triple $(S_i, V_i, O_i)$ consisting of a subject $S_i$, predicate $V_i$, and object $O_i$. The predicate $V_i$ is mapped to a sequence of robot-executable action primitives through a function $f: V \u2192 A*$, such that $f(V_i) = (A_{i1}, A_{i2}, ..., A_{ik})$, where each action primitive $A_{ij}$ corresponds to a low-level robot control command.\n\nBased on the aforementioned theory, this paper decomposes a complex long-horizon task into two distinct levels: the task level and the action level. The task level includes the complex task $T$ and a sequence of subtasks ${T_1, T_2, ...,T_n}$ derived through planning, where subtasks are considered as skills or high-level actions. The action level consists of action primitives, where a sequence of action primitives ${A_1, A_2,..., A_n}$ accomplishes a subtask. Action primitives refer to low-level, short-term tasks that can be directly executed through motion planning [31]. To represent robotic operational skills, this paper constructs a knowledge graph $G = (V, E)$ to model the hierarchical relationships among tasks, subtasks, and action primitives. In this graph, nodes $V$ represent tasks, subtasks, and action primitives, while edges $E$ illustrate the relationships between them, forming an operational \"task graph\u201d. The hierarchical node relationships include \"contain\u201d, \u201cnext step\", \"start\" and \"end\"."}, {"title": "The Construction of Operational Scene Graph", "content": "A scene graph can be represented as a directed graph $G = {O, E}$, where $O = {o_1, o_2, ..., o_n}$ denotes the set of objects in the scene. Each object $O_i = {Oi, Bi, Ai}$ consists of an object category $Oi$, a position $Bi$, and a set of attributes $Ai$. The set of directed edges $E = {E_1, E_2, ..., E_m}$ represents relationships, with each edge $E_k = {Oi,ri\u2192j, Oj}$ being an object-relationship-object triplet, indicating a relationship $r_{ij}$ between objects $O_i$ and $O_j$.\n\nFor robots to effectively manage skill transfer tasks, it is essential to consider not only their task planning and action execution strategies but also the spatial semantic information of their environment. To address this, this paper introduces the concept of a \"scene graph,\" which represents the structural composition of the environment. In this graph, each component and its configuration are modeled as subgraphs, allowing the robot to selectively focus on relevant parts while disregarding irrelevant information.\n\nThe drawer box and the main body can be represented as two objects, $O_{drawer}$ and $O_{body}$, connected by a prismatic joint, represented as the directed edge $E_k = {O_{drawer}, r'_{slider}, O_{body}}$ where $slider$ denotes the sliding relationship. The motion range of the joint, damping, friction,"}, {"title": "The Construction of Operational State Graph", "content": "The \"task graph\" and \"scene graph\" outlined earlier are mutually isolated. To resolve this issue, this paper introduces the concept of a \"state graph\", which integrates the \"task graph\" and \u201cscene graph\u201d, thus forming a comprehensive knowledge graph for the robot operation skill library. Essentially, this process involves linking the subtask sequences in the task graph with the entity relationship information in the scene graph. To achieve this connection, the concepts of \"Require\" and \"Obtain\" are introduced during the robot's subtask sequence execution. \"Require\u201d denotes the state that the robot and operating environment must meet before commencing the subtask sequence, which can be retrieved from the scene graph. \"Obtain\" refers to the state of the robot and operating environment after completing the subtask sequence, also feeding back into the scene graph for real-time updates, thereby providing information for the execution of subsequent subtask sequences."}, {"title": "Hierarchical Transfer Framework of Operational Skills", "content": "The primary design of this framework is based on the previously constructed skill library knowledge graph. It implements task-level subtask sequence transfer, motion-level trajectory adaptive transfer, and operation adaptive transfer driven by physical-level tactile perception."}, {"title": "Task-level subtask sequence transfer", "content": "Subtask sequences planned in similar task scenarios are always akin, such as in the tasks of opening doors and drawers. To enable the transfer of task-level subtask sequences across similar scenarios, it is essential for the agent to have a comprehensive understanding of the transfer tasks and scenarios. In this context, we introduce LLM, leveraging its powerful reasoning and generalization capabilities to achieve the aforementioned goals. This paper is based on the contextual learning and thought-chain prompts in prompt-based learning, constructing a new four-stage prompt word paradigm.\n\nFirst, we provide the LLM with a comprehensive description of the complex planning task and the state graph descriptions from the skill library. This includes detailed accounts of long-horizon operations tasks, their subtask sequences, and the meanings and specific components associated with \"Require\u201d and \u201cObtain\u201d in the state graphs.\n\nNext, we enable the LLM to comprehend the entire knowledge graph of the operations skill library. We input the skill library into the LLM as Neo4j-based code, which helps the model understand subtask sequences in the task graph, components of objects in the scene graph, their connections, and their relative positional relationships."}, {"title": "Motion-level adaptive trajectory transfer", "content": "The transferred subtask sequences, represented in human semantic knowledge, cannot be directly translated into action primitives executable by robots. Therefore, transferring motion-level trajectories becomes necessary. Direct trajectory transfer can face challenges, especially in tasks like opening doors and drawers, where the mechanisms differ significantly-drawers require pushing or pulling along a translational axis, whereas doors operate around a rotational axis. Deviations from the ideal trajectory by the robot can exert excessive lateral forces, potentially causing damage. Additionally, changes in obstacles along the path during door or drawer operations may lead to collisions.\n\nWe propose an adaptive trajectory transfer method leveraging the A* algorithm and a robotic operation skill library to achieve motion-level trajectory adaptation for robot skills. This method initially employs the skill library's \"scene graph\" to retrieve spatial semantic information and generate a corresponding three-dimensional discretized model. Subsequently, it identifies the trajectory's start and end points based on the new task's subtask sequence and \"state graph\" information, which is generated in an earlier phase with the assistance of a LLM. Finally, the A* algorithm autonomously plans the trajectory and avoidance tasks using task and scene information. This approach enables the robot's operational trajectory to adapt to various tasks, ensuring smooth and safe task completion in new environments."}, {"title": "Physical-level tactile perception-driven adaptive manipulation transfer", "content": "Task-level and motion-level transfers facilitate the generalization of specific tasks in new environments. However, for tasks involving intricate tactile interactions, robots must manage the variability of object attributes and the complexity of underlying contact mechanisms, which constrains the adaptability and generalization of their skills.\n\nThis paper introduces an adaptive learning mechanism that dynamically adjusts operational strategies based on real-time feedback from high-resolution visuotactile sensors such as Gelsight. The feedback encompasses information about an object's contours, textures, posture, forces, and torques. For"}, {"title": "Haptic Contour Extraction Algorithm Based on Image Texture", "content": "To accurately distinguish contours in tactile images, it is necessary to divide the contact area into three regions: texture region, contour region, and non-contact region. The contour region is characterized by sharp gradient changes; the non-contact region primarily exhibits scattered, low-gradient noise; and the texture region is determined by the surface and contact conditions of the touched object, typically displaying regular patterns with moderate gradient strength.\n\nThe algorithm first defines the texture threshold ($T_{texture}$) as the floor value of the average of non-zero gradient values in the gradient map $G$, given by the following equation:\n\n$T_{texture} = \\frac{1}{C}\\sum_{i,j:G_{i,j}\u22600} G_{ij}$        (1)\n\nNext, each pixel (i, j) in the image $I$ is smoothed by applying a Gaussian kernel $G_k$, resulting in the smoothed image $S$. Then, the gradient magnitude $G_{ij}$ and gradient direction $\u0398_{ij}$ are computed for each pixel. Non-maximum suppression is applied to retain local maximum gradient values, refining the contour points. A dual-threshold detection is then performed on these contour points using dynamically adjusted high and low thresholds ($T_{high}$ and $T_{low}$), optimized to match the texture information. Through this process, the final set of contour points $C$ is determined. Finally, the Hough Transform is used to extract lines from the contour point set and compute the line direction vectors $H$, providing the necessary data for further processing or analysis.\n\nThe haptic contour extraction algorithm based on image texture is designed to process a 320x240 resolution Gelsight image and extract contour point sets and line direction vectors using the Hough transform. The complete processing flow and pseudocode are presented in Figure 3 and Algorithm 1, respectively."}, {"title": "IV. EXPERIMENTS", "content": "In this paper, the robotic arm utilized is the KUKA LBR iiwa 14 R820. The tactile sensor employed is the Gelsight mini, with a sampling frequency of 10Hz. For low-level control, we use position control in the end-effector's"}, {"title": "Validation of the Effectiveness of the Hierarchical Skill Transfer Framework", "content": "In our experiments, we set up a drawer and a cabinet with doors containing vertically striped mugs in various positions. We aimed to transfer the skills of opening a drawer and retrieving a mug to a designated stacking location to the task of opening a cabinet door and retrieving mugs in different positions for stacking.\n\nIn this transfer scenario, the robot is required to complete the transfer of a long sub-task sequence, which includes approaching, grasping, pulling, and releasing the handles of drawers or doors, followed by approaching, picking, and moving to the designated location to accurately stack mugs. Throughout the movement process, the robot must adaptively complete the trajectory planning tasks based on the scenario information provided by the skill library. During the mug stacking phase, the robot adjusts its orientation adaptively, utilizing the tactile representation of physical-level information such as the mug's contour and orientation, to successfully complete the stacking. As illustrated in the Figure 4, the experimental results demonstrate that the robot successfully completed the aforementioned tasks using the hierarchical skill transfer framework described in this paper, confirming the framework's effectiveness in handling complex manipulative tasks with long sequences and rich contact interactions."}, {"title": "Evaluation of the Skill Transfer Framework", "content": "When addressing new operating scenarios, reinforcement learning (RL) methods are frequently employed for policy exploration. However, the training process demands extensive interaction with the environment, which is often inefficient and poses safety risks. Consequently, this paper focuses on a specific aspect of skill transfer: transferring the drawer-opening skill to the door-opening task. Experiments were conducted within the simulated environment, Robosuite.\n\nTo evaluate the efficiency of the hierarchical skill transfer method proposed in this paper, we implemented the hierarchical transfer framework for operational skills in a simulated environment and compared its performance with that of the direct policy transfer method. The direct policy transfer approach involves pre-training with the SAC reinforcement learning algorithm in a drawer-opening environment, followed by the direct transfer of the policy network learned by the SAC algorithm to an untrained door-opening task to observe the performance of the transferred policy in the new task.The design of the reward function for the drawer-opening task is as follows:\n\n$r=\\begin{cases}\n2 & \\text{success} \\\\\n0.25 \u00d7 (1 - \\text{tanh}(10 \u00d7 d)) & \\text{unsuccess}\n\\end{cases}$     (2)\n\nwhere success and unsuccess indicate whether the task of opening the drawer is completed, and $d$ represents the Euclidean distance between the robot gripper and the target position on the drawer handle.\n\nTrain the drawer-opening algorithm using the SAC algorithm from Stable Baselines3 for 1000 episodes, each with a fixed length of 1000 time steps. The training results are presented in the Figure 5. Then, transfer the trained policy network to the door-opening task and test its performance over 50 episodes, each consisting of 1000 steps. Finally, record the success rate.\n\nThe results indicate that the success rate of the direct transfer strategy is 0% within 50 episodes, while our method achieves a 100% success rate within the same timeframe. This suggests that our approach is more effective and robust when applied to various environments and tasks."}, {"title": "Verification of Adaptive Tactile Threshold Algorithm", "content": "The goal of this experiment is to evaluate the performance of a tactile-based adaptive contour extraction algorithm in"}, {"title": "V. CONCLUSION", "content": "This paper introduces a method designed to create a skill library and a hierarchical skill transfer framework,"}]}