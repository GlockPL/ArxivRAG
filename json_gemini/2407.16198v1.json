{"title": "INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model", "authors": ["Yiwei Ma", "Zhibin Wang", "Xiaoshuai Sun", "Weihuang Lin", "Qiang Zhou", "Jiayi Ji", "Rongrong Ji"], "abstract": "With advancements in data availability and computing resources, Multimodal Large Language Models (MLLMs) have showcased capabilities across various fields. However, the quadratic complexity of the vision encoder in MLLMs constrains the resolution of input images. Most current approaches mitigate this issue by cropping high-resolution images into smaller sub-images, which are then processed independently by the vision encoder. Despite capturing sufficient local details, these sub-images lack global context and fail to interact with one another. To address this limitation, we propose a novel MLLM, INF-LLaVA, designed for effective high-resolution image perception. INF-LLaVA incorporates two innovative components. First, we introduce a Dual-perspective Cropping Module (DCM), which ensures that each sub-image contains continuous details from a local perspective and comprehensive information from a global perspective. Second, we introduce Dual-perspective Enhancement Module (DEM) to enable the mutual enhancement of global and local features, allowing INF-LLaVA to effectively process high-resolution images by simultaneously capturing detailed local information and comprehensive global context. Extensive ablation studies validate the effectiveness of these components, and experiments on a diverse set of benchmarks demonstrate that INF-LLaVA outperforms existing MLLMs. Code and pretrained model are available at https://github.com/WeihuangLin/ INF-LLaVA.", "sections": [{"title": "1. Introduction", "content": "The field of multimodal large language models (MLLMs) [20, 54, 85] has achieved substantial breakthroughs, driven by monumental advancements in computer vision [25, 30, 31] and natural language processing [1, 90, 112]. These MLLMs have demonstrated exceptional efficacy across an array of complex tasks, including image captioning [19,65], visual question answering [4,95], and visual dialogue [21, 67]. This substantial progress not only highlights the transformative potential of MLLMs but also significantly extends the boundaries of understanding, reasoning, and interactive capabilities integral to the development of general artificial intelligence (AGI).\nExtensive research has highlighted the critical importance of high-resolution imagery in computer vision, particularly for tasks requiring precise image perception, such as object detection [56, 106] and segmentation [50, 110]. Similarly, enhancing resolution can significantly improve the visual acuity of Multimodal Large Language Models (MLLMs). High-resolution input images inherently provide enriched detail and intricate object relationships, which are essential for mitigating hallucination issues [7, 37, 103] and enhancing fine-grained perception tasks [33,34]. However, large language models (LLMs) necessitate careful control over the number of image tokens produced by the image encoder, as these tokens significantly affect inference speed and computational cost. Additionally, because the visual encoder in MLLMs is typically a Vision Transformer (ViT) [25], which has a computational complexity that scales quadratically with image resolution, it is crucial to limit the resolution of images fed into the ViT. As discussed, high-resolution image perception poses significant challenges for MLLMs. Thus, achieving a balance between leveraging the advantages of high-resolution inputs and managing the practical limitations of computational resources is essential for the successful deployment of MLLMs.\nTo address the challenge of efficient high-resolution image perception in MLLMs, existing methods are categorized into two main approaches: cropping-based methods and dual-encoder methods, as depicted in Fig. 1. Given that the ViT encoder [25] is pretrained on low-resolution images and considering its quadratic complexity relationship with image resolution, cropping-based methods [24, 49, 51, 96] partition a high-resolution image into several sub-images. These sub-images are independently processed by the ViT encoder to extract their visual features, as illustrated in Fig. 1(a). However, this independent cropping and encoding approach fails to adequately model the interrelationships between the sub-images. Research [10, 62, 98, 111] underscores that understanding object relationships is essential for comprehensive image interpretation. Recognizing a linear relationship between the complexity of convolutional neural networks (CNNs) [48, 57] and image resolution, some researchers [47,64] have proposed a dual-encoder approach. This method leverages a pretrained ConvNeXt [57] encoder to supplement the ViT encoder for high-resolution image perception, illustrated in Fig. 1(b). However, the dual-encoder method requires additional pretrained convolutional neural networks, necessitating extensive computational resources and large-scale datasets [79, 80], often demanding thousands of GPU hours.\nIn this paper, we introduce INF-LLaVA, a highly effective and efficient framework designed to enhance input image resolution within multimodal large language models (MLLMs), as illustrated in Fig. 1(c). The framework incorporates two innovative designs that significantly improve image resolution handling. Firstly, we propose Dual-perspective Cropping Module (DCM), a sophisticated cropping strategy that partitions high-resolution images into sub-images from both local and global perspectives. From the local perspective, sub-images maintain continuous, detailed information, capturing essential details from various regions of the original image. From the global perspective, sub-images aggregate global information, albeit with less detail, as each patch in these sub-images is cropped from the original image following a specific stride pattern. This method approach ensures that DCM surpasses previous cropping methods by preserving the integrity of both global and local information at the cropping stage. Secondly, we introduce Dual-perspective Enhancement Module (DEM) to facilitate interaction between local and global features. While a straightforward approach would involve cross-attention between these features, the quadratic increase in token number due to high-resolution images often results in out-of-memory issues. To address this, our module applies a more resource-efficient strategy: it concatenates global-perspective sub-image features back into the original image's shape based on 2D priors. These concatenated global features are then re-cropped into multiple sub-images from a local perspective. Each newly generated sub-image is matched with its corresponding local perspective sub-image, and cross-attention is performed to enrich the global features with enhanced local details. Additionally, symmetric operations are applied to local-perspective sub-images to bolster global information. Building upon DCM and DEM, we propose a new high-resolution MLLM, namely INF-LLaVA. Experimental results overwhelmingly demonstrate that these innovative designs not only enhance the handling of high-resolution images within MLLMs but also significantly optimize computational efficiency, establishing INF-LLaVA as a compelling solution to advance the field.\nIn summary, our contributions are three-fold:\n\u2022 We propose a novel Dual-perspective Cropping Module (DCM), which integrates both global and local perspectives when cropping high-resolution images into sub-images. This enhances the model's ability to capture detailed and contextual information.\n\u2022 We introduce Dual-perspective Enhancement Module (DEM), an effective and efficient module for fusing dual-perspective features, resulting in dual-enhanced features that significantly improve performance.\n\u2022 Based on these two novel modules, we develop INF-LLaVA, a powerful MLLM that outperforms existing models on multiple benchmarks, demonstrating the effectiveness of our approach."}, {"title": "2. Related Work", "content": "2.1. Large Language Models (LLMs)\nIn the early stages of natural language processing (NLP) advancements, models like GPT-2 [77] and BERT [22], pretrained on web-scale text datasets, showcased exceptional representational capabilities. These models achieved monumental success and marked a significant breakthrough in the field of NLP. Building on the effectiveness of the pre-training paradigm, researchers have further enhanced large language models (LLMs) by increasing the amount of pre-training data and scaling up model parameters. Representative works in this domain include GPT-3 [9], PaLM [18], and OPT [108], which have each set new benchmarks for performance and capability. Recent efforts have pivoted towards improving LLM responses to be more aligned with human preferences by incorporating human instructions and feedback. Notable examples include InstructGPT [75], ChatGPT [73], and GPT-4 [1], which demonstrate strong perceptual and reasoning abilities in human conversations. These models have advanced the state of conversational AI, making interactions more intuitive and human-like. Additionally, the open-source LLaMA series [71, 90, 91] represent a significant contribution to the field. To further enhance the human interaction capabilities of LLaMA, researchers have developed Alpaca [87], Vicuna [17], and MPT [88], which fine-tune the LLaMA model using additional high-quality instruction data. Recognizing the importance of aligning models with human intentions and preferences, some researchers [1, 3, 91] have incorporated Reinforcement Learning from Human Feedback (RLHF) [78,81] into the training process. This approach ensures that models not only respond accurately but also in ways that are aligned with human values and requirements, thereby significantly enhancing the user experience and reliability of AI systems.\n2.2. Multimodal Large Language Models (MLLMs)\nMultimodal Large Language Models (MLLMs) are designed to extend the capabilities of traditional large language models (LLMs) by incorporating both textual and visual understanding, thereby enhancing their ability to interpret visual information and provide contextually rich responses. MLLMs [60, 113, 115] generally comprise three core components: a vision encoder, a connector, and an LLM. The vision encoder acts as the \"eyes\" of the model, enabling it to perceive and analyze visual content. This encoder can utilize various structures, such as Vision Transformer (ViT) [25] or ConvNeXt [57], and can be pretrained using different methodologies, including self-supervised learning [11,74] or supervised learning [76]. Most MLLMs employ CLIP-ViT, which is pre-trained on extensive image-text pairs, as the vision encoder to extract visual features effectively. The connector in MLLMs is responsible for transforming these visual features into the textual domain, facilitating seamless integration with the LLM. There are three prevalent types of projectors: 1) Cross-attention-based methods: Models like Flamingo [2] and CogVLM [93] utilize cross-attention mechanisms to interweave visual and textual tokens within the LLM, effectively merging the two modalities. 2) Query-based methods: Approaches such as Blip-2 [45], Instruct-Blip [20], and Qwen-VL [6] employ learnable queries to extract visual features using transformer-like architectures. These queries are then concatenated with text tokens, and the combined tokens are fed into the LLM. 3) Projection-based methods: Techniques like LLaVA [52, 54], Mini-GPT4 [115], and DeepSeek-VL [60] leverage a linear layer or a multi-layer perceptron (MLP) to project visual tokens into the textual domain directly, subsequently feeding the mixed tokens into the LLM. The LLM, serving as the \"brain\" of the MLLM, interprets and processes the combined text and image information, delivering coherent and contextually appropriate responses. The range of LLMs available for integration is extensive, including models like LLaMA [71,90,91], Qwen [6], DeepSeek [8], and Yi [101]. Through the synergistic combination of these sophisticated components, MLLMs significantly enhance the capabilities of traditional LLMs, enabling them to seamlessly integrate and process multiple modalities.\n2.3. High-resolution MLLMs\nHigh-resolution images offer significant advantages for Multimodal Large Language Models (MLLMs) by enabling the capture of detailed object information and complex relationships between objects in images. However, directly inputting high-resolution images into the vision encoder results in prohibitive computational expenses, primarily due to the quadratic complexity associated with the Transformer architecture [92] and the substantial increase in the number of visual tokens. To mitigate this issue, existing high-resolution MLLMs can be categorized into two primary types: Cropping-based methods and Dual-Encoder methods, as illustrated in Fig. 1(a) and Fig. 1(b). Cropping-based methods [24, 51, 53, 99] partition an image into multiple non-overlapping patches and feed each patch into the vision encoder separately, thereby obtaining visual features for local regions. To ensure that each patch maintains an aspect ratio close to 1:1, LLaVA-UHD [96] introduces various patching strategies during the crop operation. Furthermore, to individually model each patch's information, Monkey [49] employs LoRA [35] to fine-tune the vision encoder for each specific patch. Despite their benefits, cropping-based methods can disrupt the global coherence of image information by segmenting a complete image into isolated sub-images. As a result, some researchers have proposed dual-encoder methods to maintain the integrity of global information. Dual-encoder methods leverage an auxiliary visual encoder to enhance high-resolution image understanding without significantly increasing the number of visual tokens. For instance, Vary [94] and Deepseek-VL [60] utilize the Segment Anything Model (SAM) [40] within a high-resolution vision encoder to better capture high-resolution information. Meanwhile, MiniGemini [47] and LLaVA-HR [64] employ ConvNeXt [57], pretrained on the massive LAION2B dataset [80], to augment the visual features extracted by the Vision Transformer (ViT). However, dual-encoder methods necessitate an additional pretrained vision encoder to process high-resolution images. Both SAM, pre-"}, {"title": "3. Preliminary", "content": "A multimodal large language model (MLLM) is an advanced AI system designed to handle and integrate both visual and textual data effectively. It typically consists of three primary components: an image encoder $F_1(\\cdot)$, a connector $F_C(\\cdot)$, and a well-pretrained large language model (LLM) $F_L(\\cdot)$. The image encoder processes the input image $I \\in \\mathbb{R}^{H\\times W\\times 3}$, where H and W represent the height and width of the image, respectively, and the three denotes the RGB color channels. This encoder extracts high-dimensional visual features from the image. The connector maps these visual features into a format that the LLM can interpret, effectively serving as a bridge between the visual and textual domains. The LLM, pretrained on a vast amount of textual data, processes the integrated visual and textual data to generate coherent and contextually relevant responses.\nThe input to the MLLM typically includes an image I and a corresponding instruction text $T_{ins} \\in \\mathbb{R}^{L}$, where L is the number of tokens in the instruction. Initially, the image I is processed by the image encoder $F_1(I)$ to extract visual features. Concurrently, the instruction $T_{ins}$ is tokenized using the tokenizer $F_T$ of the LLM to convert the text into a series of tokens. The extracted visual features are then flattened and projected into visual tokens. The connector $F_C$ converts these visual tokens into a format compatible with the LLM. Subsequently, the visual tokens and the textual tokens are concatenated along the spatial dimension and fed into the LLM $F_L$.\nThe LLM decodes the combined visual and textual tokens to generate a response token-by-token. Mathematically, this decoding process can be formulated as:\n$p(R_t | I, T_{ins}, R_{0:t-1}) = F_L\\left(R_t \\mid F_C\\left(F_1(I)\\right), F_T(T_{ins}), F_T(R_{0:t-1})\\right).$  (1)\nHere, $p(R_t | I, T_{ins}, R_{0:t-1})$ represents the probability distribution of the predicted token $R_t$ at time t, given the image, instruction text, and the previously generated tokens. $F_T(R_{0:t-1})$ denotes the tokenized form of the response generated up to token t \u2212 1, and $R_t$ is the t-th token of the generated response."}, {"title": "4. Methods", "content": "In this section, we commence by providing a comprehensive overview of the proposed INF-LLaVA framework in Sec. 4.1, highlighting its innovative structure and capabilities. Next, we delve into the specifics of the two critical components: Dual-perspective Cropping Module and Dual-perspective Enhancement Module, thoroughly examining their intricate functionalities in Sec. 4.2 and Sec. 4.3, respectively.\n4.1. Overview\nFig. 2 depicts the comprehensive pipeline of the proposed INF-LLaVA framework. Due to the pre-trained ViT-CLIP encoder's limitation in processing high-resolution images\u2014given its quadratic complexity characteristics\u2014directly feeding high-resolution images into it is computationally prohibitive. To address this challenge, we propose Dual-perspective Cropping Module (DCM) $F_{DCM}(\\cdot)$, which partitions high-resolution images into several sub-images from both global and local perspectives, using the resolution defined by the pretrained vision encoder. Mathematically, this operation can be described as:\n$[I_{loc}^1, I_{loc}^2, ..., I_{loc}^N ; I_{glo}^1, I_{glo}^2, ..., I_{glo}^N] = F_{DCM}(I),$  (2)\nwhere $[I_{loc}^1, I_{loc}^2, ..., I_{loc}^N]$ and $[I_{glo}^1, I_{glo}^2, ..., I_{glo}^N]$ represent the sub-images from local and global perspectives, respectively. Here, N denotes the number of sub-images from each perspective, and $I \\in \\mathbb{R}^{W_h\\times H_h \\times 3}$ is the high-resolution input image. Next, each local and global sub-image is separately fed into the pretrained vision encoder:\n$F_{loc}^i = F_I(I_{loc}^i),$  (3)\n$F_{glo}^i = F_I(I_{glo}^i),$  (4)\nwhere $F_{loc}^i \\in \\mathbb{R}^{w_i\\times h_i \\times d}$ and $F_{glo}^i \\in \\mathbb{R}^{w_i\\times h_i \\times d}$ are the visual features of the i-th local and global sub-images, respectively. Here, $w_i \\times h_i$ denotes the number of visual tokens per sub-image, and d is the channel dimension of the visual features. These local and global sub-image features are then recombined using 2D positional prior information to form high-resolution image features:\n$\\hat{F}_{loc} = F_{loc}(F_{loc}^1, F_{loc}^2, ..., F_{loc}^N),$  (5)\n$\\hat{F}_{glo} = F_{glo}(F_{glo}^1, F_{glo}^2,..., F_{glo}^N),$  (6)\nwhere $F_{loc}(\\cdot)$ and $F_{glo}(\\cdot)$ are the recombination functions based on local and global positional information. The resulting features $\\hat{F}_{loc} \\in \\mathbb{R}^{w_h\\times h_h \\times d}$ and $\\hat{F}_{glo} \\in \\mathbb{R}^{w_h\\times h_h \\times d}$ represent the high-resolution visual features from local and global perspectives, respectively, where $w_h \\times h_h$ is the number of visual tokens in the recombined high-resolution images. To facilitate efficient interaction between the local and global features, the proposed Dual-perspective Enhancement Module (DEM) $F_{DEM}(\\cdot)$ is employed. This module ensures a robust exchange of information between local and global features, resulting in dual-enhanced features:\n$F_{dual} = F_{pool}(F_{DEM}(\\hat{F}_{loc}, \\hat{F}_{glo})),$  (7)\nwhere $F_{pool}(\\cdot)$ is the average pooling function used to reduce the number of visual tokens, thereby accelerating training and inference speeds while minimizing computational overhead. The resulting $F_{dual} \\in \\mathbb{R}^{w_i\\times h_i \\times d}$ represents the visual features enhanced from dual perspectives. Finally, the connector $F_C(\\cdot)$ projects the dual-enhanced visual features to obtain visual tokens that align with the textual features. The instruction $T_{ins}$ is tokenized by the tokenizer $F_T(\\cdot)$, converting it into a sequence of tokens. These visual tokens and textual tokens are then concatenated along the spatial dimension and fed into the pretrained LLM to generate the response:\n$R = F_L(F_C(F_{dual}), F_T(T_{ins})),$  (8)\nwhere $R\\in \\mathbb{R}^{L_{res}}$ represents the response generated by the LLM. Here, $L_{res}$ is the length of the generated response in tokens.\n4.2. Dual-perspective Cropping Module\nDual-perspective Cropping Module (DCM) is designed to effectively partition the high-resolution image $I \\in \\mathbb{R}^{W_h\\times H_h\\times 3}$ into multiple sub-images $I_i \\in \\mathbb{R}^{W_l\\times H_l\\times 3}$, where $W_l \\times H_l$ corresponds to the resolution utilized by the vision encoder during pretraining. For instance, in the case of the CLIP-ViT-large-patch14-336 encoder, $W_l = H_l = 336$. The primary objective of DCM is to perform cropping from both local and global perspectives to capture fine-grained details and broader contextual information, respectively.\nIn the sections that follow, we will elaborate on the methodologies employed for cropping high-resolution images from both perspectives, ensuring that the integrity and essential characteristics of the original image are preserved.\n4.2.1 Local-perspective Cropping\nThe local-perspective cropping technique is designed to systematically extract smaller regions from the high-resolution image while preserving detailed and continuous visual information. This approach ensures that each sub-image retains the high-resolution details necessary for accurate analysis and representation. By strategically segmenting the high-resolution image into localized sub-images, DCM effectively maintains the intricate details of the original image.\nGiven a high-resolution image $I \\in \\mathbb{R}^{W_h\\times H_h\\times 3}$, the first step is to determine the relationship between the dimensions of the high-resolution image $(W_h, H_h)$ and the dimensions expected by the pretrained vision encoder $(W_l, H_l)$. This relationship is quantified as follows:\n$n_W = \\left\\lfloor\\frac{W_h}{W_l}\\right\\rfloor,$  (9)\n$n_H = \\left\\lfloor\\frac{H_h}{H_l}\\right\\rfloor,$  (10)\nwhere $\\left\\lfloor\\cdot\\right\\rfloor$ represents the floor function, which rounds down to the nearest integer. $n_W$ and $n_H$ correspond to the number of local sub-images along the width and height of the high-resolution image, respectively.\nThus, the high-resolution image I is divided into $n_W \\times n_H$ local sub-images $I_{loc}^i$, each sized $W_l \\times H_l$. Specifically, for each sub-image $I_{loc}^i$, where $i \\in [0, n_H \\times n_W - 1]$, the following formulas determine the row and column positions:\n$row = \\left\\lfloor\\frac{i}{n_W}\\right\\rfloor,$  (11)\n$col = i \\mod n_W,$  (12)\nwhere $\\mod$ denotes the modulo operation. The bounding box for each local sub-image $I_{loc}^i$ can be described by:\n$I_{loc}^i = I \\left[row \\cdot H_l : (row + 1) \\cdot H_l, col \\cdot W_l : (col + 1) \\cdot W_l, : \\right].$  (13)\nThis process ensures that each local sub-image $I_{loc}^i$ contains continuous and detailed visual information from the high-resolution image, thereby preserving the integrity and quality of the image for localized analysis.\nBy using the local-perspective cropping technique, DCM maintains high-fidelity representation in each sub-image, enabling robust feature extraction and analysis. This approach ensures that finer details are not lost, providing a comprehensive understanding of localized regions without compromising on resolution or detail.\n4.2.2 Global-perspective Cropping\nConversely, global-perspective cropping aims to capture broader contextual information from high-resolution images to preserve the spatial relationships between objects. This approach ensures that our model retains an understanding of both micro and macro-level details within the extracted sub-images, facilitating the integration of comprehensive global contextual information.\nGiven the high resolution $W_h \\times H_h$ of the input image and the low resolution $W_l \\times H_l$ used by the pretrained vision encoder, the number of global sub-images $n_W \\times n_H$ is computed similarly to the local perspective, as shown in Equ. (14) and (15):\n$n_W = \\left\\lfloor\\frac{W_h}{W_l}\\right\\rfloor,$  (14)\n$n_H = \\left\\lfloor\\frac{H_h}{H_l}\\right\\rfloor,$  (15)\nFor the i-th row and j-th column of sub-images, the pixel indices set $I_{ij}$ is defined as follows:\n$I_{ij}^{glo} = \\left\\{ I(x,y) \\mid \\begin{array}{c} x = j + m\\cdot n_W, \\\\ y = i + n\\cdot n_H, \\\\ 0 \\le m < \\frac{W_h}{n_W}, m \\in \\mathbb{N} \\\\ 0 \\le n < \\frac{H_h}{n_H}, n \\in \\mathbb{N} \\end{array} \\right\\},$  (16)\nwhere (x, y) corresponds to the pixel indices within the high-resolution image. $\\mathbb{N}$ represents the set of natural numbers. Thus, each pixel (u, v) in the $I_{ij}^{glo}$ sub-image can be mapped back to the high-resolution image I as:\n$I_{ij}^{glo}(u, v) = I \\left(j + u \\cdot n_W, i + v \\cdot n_H\\right).$  (17)\nThis module effectively partitions the high-resolution image into sub-images that encapsulate the global perspective by interleaving pixels from different regions. Consequently, it enables the model to maintain a coherent global context alongside the detailed local information captured by global-perspective cropping.\n4.3. Dual-perspective Enhancement Module\nIn Sec. 4.2, we meticulously detailed the process of segmenting a high-resolution image into multiple sub-images from both local and global perspectives. Building on this foundational step, we now delve into the methods for extracting sub-features from these sub-images and their subsequent integration into high-resolution local-perspective and global-perspective features, as discussed in Sec. 4.3.1. Next, we elucidate the sophisticated techniques employed for enhancing these features\u2014specifically, the"}, {"title": "4.3.1 Sub-features Combination", "content": "Upon obtaining the local-perspective sub-images $[I_{loc}^1, I_{loc}^2, ..., I_{loc}^N]$ and global-perspective sub-images $[I_{glo}^1, I_{glo}^2,..., I_{glo}^N]$ from DCM, we proceed to extract their corresponding features using Equ. (3) and Equ.(4). This extraction yields local-perspective sub-features $[F_{loc}^1, F_{loc}^2,...,F_{loc}^N]$ and global-perspective sub-features $[F_{glo}^1, F_{glo}^2,..., F_{glo}^N]$. As illustrated in Fig. 3, the sub-features from both perspectives are then systematically recombined. Specifically, the local-perspective sub-features $[F_{loc}^1, F_{loc}^2,... F_{loc}^N]$ are aggregated to construct the comprehensive local-perspective feature $\\hat{F}_{loc} \\in \\mathbb{R}^{w_h\\times h_h\\times d}$. Likewise, the global-perspective sub-features $[F_{glo}^1, F_{glo}^2, F_{glo}^N]$ are amalgamated to form the global-perspective feature $\\hat{F}_{glo} \\in \\mathbb{R}^{w_h\\times h_h\\times d}$. This meticulous combination process ensures that the local-perspective feature effectively encapsulates fine-grained details, while the global-perspective feature maintains a coherent understanding of the broader contextual information."}, {"title": "4.3.2 Global-Perspective Enhancement", "content": "After obtaining the local-perspective feature $\\hat{F}_{loc} \\in \\mathbb{R}^{w_h\\times w\\times d}$ and global-perspective feature $\\hat{F}_{glo} \\in \\mathbb{R}^{w_h\\times w\\times d}$, the next crucial step involves interacting and fusing these two feature sets to derive more robust and comprehensive features. A straightforward approach would be to leverage cross-attention between $\\hat{F}_{loc}$ and $\\hat{F}_{glo}$ to enable interaction between local and global information. However, applying cross-attention directly to such high-resolution features can lead to significant out-of-memory issues during training. To circumvent this, we propose a novel dual-perspective enhancement method that interacts with local and global features in a more efficient manner, mitigating these computational challenges.\nSpecifically, for global-perspective enhancement, as illustrated in Fig. 4, we first crop both the local-perspective and global-perspective features from a global perspective."}, {"title": "4.3.3 Local-Perspective Enhancement", "content": "Similarly, Local-Perspective Enhancement aims to refine local features by integrating global contextual information. This process ensures that the local features not only retain fine-grained details but also benefit from the broader context provided by the global features.\nFirst, the local-perspective and global-perspective features are cropped into sub-features through a local-perspective cropping operation. This is formulated as follows:\n$[G_{loc}^1, G_{loc}^2,..., G_{loc}^N] = C_{loc}(\\hat{F}_{glo}),$  (22)\n$[L_{loc}^1, L_{loc}^2,..., L_{loc}^N] = C_{loc}(\\hat{F}_{loc}),$  (23)\nwhere $C_{loc}(\\cdot)$ denotes the local-perspective cropping operation. The resulting sub-features, $[G_{loc}^1, G_{loc}^2, ..., G_{loc}^N]$ and $[L_{loc}^1, L_{loc}^2, ..., L_{loc}^N]$, represent the global and local sub-features respectively, generated from this local-perspective cropping.\nNext, we employ a cross-attention mechanism to facilitate interaction between local and global sub-features, enhancing the local features with global context. The formulation of this interaction is as follows:\n$A_{loc} = Softmax\\left(\\frac{(L_{loc}^i \\cdot W_q) \\cdot (G_{loc}^i \\cdot W_k)^T}{\\sqrt{d}}\\right),$  (24)\n$V_{loc}^i = A_{loc} \\cdot G_{loc}^i \\cdot W_v,$  (25)\nwhere $W_q, W_k, W_v \\in \\mathbb{R}^{d \\times d}$ are learnable embedding matrices used for query, key, and value transformations, respectively. The attention map $A_{loc} \\in \\mathbb{R}^{h_i w_i \\times h_i w_i}$ captures the relationships between local and global sub-features. The resulting enhanced feature $V_{loc}^i \\in \\mathbb{R}^{w_i \\times h_i \\times d}$ is the i-th local-enhanced sub-feature.\nFinally, the set of local-enhanced sub-features $[V_{loc}^1, V_{loc}^2, ..., V_{loc}^N]$ are aggregated to form the local-enhanced feature $\\hat{V}_{loc} \\in [][\\mathbb{R}^{w_h\\times h_h\\times d}$. This process ensures that the local features are enriched with complementary global contextual information, providing a more comprehensive and robust representation of the original high-resolution image."}, {"title": "4.3.4 Dual-Perspective Fusion", "content": "After obtaining the global-enhanced feature $\\hat{V}_{glo}$ and the local-enhanced feature $\\hat{V}_{loc}$, the next critical step involves fusing these features to create a comprehensive representation that leverages the strengths of both perspectives. To achieve this, we employ a concatenation-based method that effectively combines the global and local features.\nInitially, we utilize two separate embedding layers to reduce the dimensionality of the features. This dimensionality reduction step is essential to ensure that the subsequent concatenation is computationally efficient and to highlight the most salient aspects of each feature set. The embedding operation can be described as follows:\n$\\tilde{V}_{glo} = \\hat{V}_{glo}W_{glo}, \\quad \\tilde{V}_{loc} = \\hat{V}_{loc}W_{loc},$  (26)\nwhere $W_{glo}, W_{loc} \\in \\mathbb{R}^{d \\times \\tilde{d}}$ are learnable projection matrices for the global and local features, respectively. These matrices transform the original features into a lower-dimensional space, thereby emphasizing their most critical components.\nNext, we concatenate the embedded global and local features along the channel dimension to form the dual-enhanced feature. This concatenation ensures that both global contextual information and local detailed information are retained and integrated. The fusion process is formulated as follows:\n$V_{dual} = [\\tilde{V}_{glo}; \\tilde{V}_{loc}],$  (27)\nwhere $[;]$ denotes the concatenation operation along the channel dimension. As a result, the dual-enhanced feature $V_{dual} \\in [\\mathbb{R}^{w_h\\times h_h\\times d}$ encapsulates a comprehensive view of the input image, combining the strengths of both perspectives to produce a robust and informative representation."}, {"title": "5. Experiments", "content": "5.1. Evaluations\nTo rigorously assess the effectiveness", "include": "n\u2022 ScienceQA-img [63", "66": "This open-ended visual question answering dataset requires the model to utilize external knowledge sources to answer questions accurately. It challenges the model's proficiency in integrating visual content with external textual information", "43": "SEEDBench is a comprehensive benchmark tailored specifically for evaluating Multimodal Large Language Models (MLLMs). It spans 12 evaluation dimensions", "55": "MMBench is a multi-modality benchmark offering a comprehensive evaluation pipeline. It features a curated dataset and introduces the innovative CircularEval strategy using ChatGPT to enhance model prediction assessment. MMBench ensures a holistic evaluation of the model's multi-modal capabilities by providing a structured and thorough testing environment.\n\u2022 MMBench-CN [55"}]}