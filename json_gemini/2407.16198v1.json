{"title": "INF-LLaVA: Dual-perspective Perception for High-Resolution Multimodal Large Language Model", "authors": ["Yiwei Ma", "Zhibin Wang", "Xiaoshuai Sun", "Weihuang Lin", "Qiang Zhou", "Jiayi Ji", "Rongrong Ji"], "abstract": "With advancements in data availability and computing resources, Multimodal Large Language Models (MLLMs) have showcased capabilities across various fields. However, the quadratic complexity of the vision encoder in MLLMs constrains the resolution of input images. Most current approaches mitigate this issue by cropping high-resolution images into smaller sub-images, which are then processed independently by the vision encoder. Despite capturing sufficient local details, these sub-images lack global context and fail to interact with one another. To address this limitation, we propose a novel MLLM, INF-LLaVA, designed for effective high-resolution image perception. INF-LLaVA incorporates two innovative components. First, we introduce a Dual-perspective Cropping Module (DCM), which ensures that each sub-image contains continuous details from a local perspective and comprehensive information from a global perspective. Second, we introduce Dual-perspective Enhancement Module (DEM) to enable the mutual enhancement of global and local features, allowing INF-LLaVA to effectively process high-resolution images by simultaneously capturing detailed local information and comprehensive global context. Extensive ablation studies validate the effectiveness of these components, and experiments on a diverse set of benchmarks demonstrate that INF-LLaVA outperforms existing MLLMs. Code and pretrained model are available at https://github.com/WeihuangLin/ INF-LLaVA.", "sections": [{"title": "1. Introduction", "content": "The field of multimodal large language models (MLLMs) [20, 54, 85] has achieved substantial breakthroughs, driven by monumental advancements in computer vision [25, 30, 31] and natural language processing [1, 90, 112]. These MLLMs have demonstrated exceptional efficacy across an array of complex tasks, including image captioning [19, 65], visual question answering [4, 95], and visual dialogue [21, 67]. This substantial progress not only highlights the transformative potential of MLLMs but also significantly extends the boundaries of understanding, reasoning, and interactive capabilities integral to the development of general artificial intelligence (AGI).\nExtensive research has highlighted the critical importance of high-resolution imagery in computer vision, particularly for tasks requiring precise image perception, such as object detection [56, 106] and segmentation [50, 110]. Similarly, enhancing resolution can significantly improve the visual acuity of Multimodal Large Language Models (MLLMs). High-resolution input images inherently provide enriched detail and intricate object relationships, which are essential for mitigating hallucination issues [7, 37, 103] and enhancing fine-grained perception tasks [33, 34]. However, large language models (LLMs) necessitate careful control over the number of image tokens produced by the image encoder, as these tokens significantly affect inference speed and computational cost. Additionally, because the visual encoder in MLLMs is typically a Vision Transformer (ViT) [25], which has a computational complexity that scales quadratically with image resolution, it is crucial to limit the resolution of images fed into the ViT. As discussed, high-resolution image perception poses significant challenges for MLLMs. Thus, achieving a balance between leveraging the advantages of high-resolution inputs and managing the practical limitations of computational resources is essential for the successful deployment of MLLMs.\nTo address the challenge of efficient high-resolution image perception in MLLMs, existing methods are categorized into two main approaches: cropping-based methods and dual-encoder methods, as depicted in Fig. 1. Given that the ViT encoder [25] is pretrained on low-resolution images and considering its quadratic complexity relationship with image resolution, cropping-based methods [24, 49, 51, 96] partition a high-resolution image into several sub-images. These sub-images are independently processed by the ViT encoder to extract their visual features, as illustrated in Fig. 1(a). However, this independent cropping and encoding approach fails to adequately model the interrelationships between the sub-images. Research [10, 62, 98, 111] underscores that understanding object relationships is essential for comprehensive image interpretation. Recognizing a linear relationship between the complexity of convolutional neural networks (CNNs) [48, 57] and image resolution, some researchers [47, 64] have proposed a dual-encoder approach. This method leverages a pretrained ConvNeXt [57] encoder to supplement the ViT encoder for high-resolution image perception, illustrated in Fig. 1(b). However, the dual-encoder method requires additional pretrained convolutional neural networks, necessitating extensive computational resources and large-scale datasets [79, 80], often demanding thousands of GPU hours.\nIn this paper, we introduce INF-LLaVA, a highly effective and efficient framework designed to enhance input image resolution within multimodal large language models (MLLMs), as illustrated in Fig. 1(c). The framework incorporates two innovative designs that significantly improve image resolution handling. Firstly, we propose Dual-perspective Cropping Module (DCM), a sophisticated cropping strategy that partitions high-resolution images into sub-images from both local and global perspectives. From the local perspective, sub-images maintain continuous, detailed information, capturing essential details from various regions of the original image. From the global perspective, sub-images aggregate global information, albeit with less detail, as each patch in these sub-images is cropped from the original image following a specific stride pattern. This method approach ensures that DCM surpasses previous cropping methods by preserving the integrity of both global and local information at the cropping stage. Secondly, we introduce Dual-perspective Enhancement Module (DEM) to facilitate interaction between local and global features. While a straightforward approach would involve cross-attention between these features, the quadratic increase in token number due to high-resolution images often results in out-of-memory issues. To address this, our module applies a more resource-efficient strategy: it concatenates global-perspective sub-image features back into the original image's shape based on 2D priors. These concatenated global features are then re-cropped into multiple sub-images from a local perspective. Each newly generated sub-image is matched with its corresponding local perspective sub-image, and cross-attention is performed to enrich the global features with enhanced local details. Additionally, symmetric operations are applied to local-perspective sub-images to bolster global information. Building upon DCM and DEM, we propose a new high-resolution MLLM, namely INF-LLaVA. Experimental results overwhelmingly demonstrate that these innovative designs not only enhance the handling of high-resolution images within MLLMs but also significantly optimize computational efficiency, establishing INF-LLaVA as a compelling solution to advance the field.\nIn summary, our contributions are three-fold:\n\u2022 We propose a novel Dual-perspective Cropping Module (DCM), which integrates both global and local perspectives when cropping high-resolution images into sub-images. This enhances the model's ability to capture detailed and contextual information.\n\u2022 We introduce Dual-perspective Enhancement Module (DEM), an effective and efficient module for fusing dual-perspective features, resulting in dual-enhanced features that significantly improve performance.\n\u2022 Based on these two novel modules, we develop INF-LLaVA, a powerful MLLM that outperforms existing models on multiple benchmarks, demonstrating the effectiveness of our approach."}, {"title": "2. Related Work", "content": "In the early stages of natural language processing (NLP) advancements, models like GPT-2 [77] and BERT [22], pretrained on web-scale text datasets, showcased exceptional representational capabilities. These models achieved monumental success and marked a significant breakthrough in the field of NLP. Building on the effectiveness of the pre-training paradigm, researchers have further enhanced large language models (LLMs) by increasing the amount of pre-training data and scaling up model parameters. Representative works in this domain include GPT-3 [9], PaLM [18], and OPT [108], which have each set new benchmarks for performance and capability. Recent efforts have pivoted towards improving LLM responses to be more aligned with human preferences by incorporating human instructions and feedback. Notable examples include InstructGPT [75], ChatGPT [73], and GPT-4 [1], which demonstrate strong perceptual and reasoning abilities in human conversations. These models have advanced the state of conversational AI, making interactions more intuitive and human-like. Additionally, the open-source LLaMA series [71, 90, 91] represent a significant contribution to the field. To further enhance the human interaction capabilities of LLaMA, researchers have developed Alpaca [87], Vicuna [17], and MPT [88], which fine-tune the LLaMA model using additional high-quality instruction data. Recognizing the importance of aligning models with human intentions and preferences, some researchers [1, 3, 91] have incorporated Reinforcement Learning from Human Feedback (RLHF) [78, 81] into the training process. This approach ensures that models not only respond accurately but also in ways that are aligned with human values and requirements, thereby significantly enhancing the user experience and reliability of AI systems."}, {"title": "2.2. Multimodal Large Language Models (MLLMs)", "content": "Multimodal Large Language Models (MLLMs) are designed to extend the capabilities of traditional large language models (LLMs) by incorporating both textual and visual understanding, thereby enhancing their ability to interpret visual information and provide contextually rich responses. MLLMs [60, 113, 115] generally comprise three core components: a vision encoder, a connector, and an LLM. The vision encoder acts as the \"eyes\" of the model, enabling it to perceive and analyze visual content. This encoder can utilize various structures, such as Vision Transformer (ViT) [25] or ConvNeXt [57], and can be pretrained using different methodologies, including self-supervised learning [11, 74] or supervised learning [76]. Most MLLMs employ CLIP-ViT, which is pre-trained on extensive image-text pairs, as the vision encoder to extract visual features effectively. The connector in MLLMs is responsible for transforming these visual features into the textual domain, facilitating seamless integration with the LLM. There are three prevalent types of projectors: 1) Cross-attention-based methods: Models like Flamingo [2] and CogVLM [93] utilize cross-attention mechanisms to interweave visual and textual tokens within the LLM, effectively merging the two modalities. 2) Query-based methods: Approaches such as Blip-2 [45], Instruct-Blip [20], and Qwen-VL [6] employ learnable queries to extract visual features using transformer-like architectures. These queries are then concatenated with text tokens, and the combined tokens are fed into the LLM. 3) Projection-based methods: Techniques like LLaVA [52, 54], Mini-GPT4 [115], and DeepSeek-VL [60] leverage a linear layer or a multi-layer perceptron (MLP) to project visual tokens into the textual domain directly, subsequently feeding the mixed tokens into the LLM. The LLM, serving as the \"brain\" of the MLLM, interprets and processes the combined text and image information, delivering coherent and contextually appropriate responses. The range of LLMs available for integration is extensive, including models like LLaMA [71, 90, 91], Qwen [6], DeepSeek [8], and Yi [101]. Through the synergistic combination of these sophisticated components, MLLMs significantly enhance the capabilities of traditional LLMs, enabling them to seamlessly integrate and process multiple modalities."}, {"title": "2.3. High-resolution MLLMS", "content": "High-resolution images offer significant advantages for Multimodal Large Language Models (MLLMs) by enabling the capture of detailed object information and complex relationships between objects in images. However, directly inputting high-resolution images into the vision encoder results in prohibitive computational expenses, primarily due to the quadratic complexity associated with the Transformer architecture [92] and the substantial increase in the number of visual tokens. To mitigate this issue, existing high-resolution MLLMs can be categorized into two primary types: Cropping-based methods and Dual-Encoder methods, as illustrated in Fig. 1(a) and Fig. 1(b). Cropping-based methods [24, 51, 53, 99] partition an image into multiple non-overlapping patches and feed each patch into the vision encoder separately, thereby obtaining visual features for local regions. To ensure that each patch maintains an aspect ratio close to 1:1, LLaVA-UHD [96] introduces various patching strategies during the crop operation. Furthermore, to individually model each patch's information, Monkey [49] employs LoRA [35] to fine-tune the vision encoder for each specific patch. Despite their benefits, cropping-based methods can disrupt the global coherence of image information by segmenting a complete image into isolated sub-images. As a result, some researchers have proposed dual-encoder methods to maintain the integrity of global information. Dual-encoder methods leverage an auxiliary visual encoder to enhance high-resolution image understanding without significantly increasing the number of visual tokens. For instance, Vary [94] and Deepseek-VL [60] utilize the Segment Anything Model (SAM) [40] within a high-resolution vision encoder to better capture high-resolution information. Meanwhile, MiniGemini [47] and LLaVA-HR [64] employ ConvNeXt [57], pretrained on the massive LAION2B dataset [80], to augment the visual features extracted by the Vision Transformer (ViT). However, dual-encoder methods necessitate an additional pretrained vision encoder to process high-resolution images. Both SAM, pretrained on the SA-1B dataset, and ConvNeXt, pretrained on the LAION-2B dataset, require extensive computational resources, amounting to tens of thousands of GPU hours, which can be cost-prohibitive. In this paper, we introduce INF-LLaVA, a novel framework that addresses these challenges by integrating an innovative Dual-perspective Cropping Module and a new Dual-perspective Enhancement Module to enhance the preservation of high-resolution global information. Our approach ensures not only the efficiency of computational resources but also the comprehensive capture of both local and global image details, thereby advancing the capabilities of high-resolution MLLMs."}, {"title": "3. Preliminary", "content": "A multimodal large language model (MLLM) is an advanced AI system designed to handle and integrate both visual and textual data effectively. It typically consists of three primary components: an image encoder $F_I(\\cdot)$, a connector $F_C(\\cdot)$, and a well-pretrained large language model (LLM) $F_L(\\cdot)$. The image encoder processes the input image $I \\in \\mathbb{R}^{H \\times W \\times 3}$, where $H$ and $W$ represent the height and width of the image, respectively, and the three denotes the RGB color channels. This encoder extracts high-dimensional visual features from the image. The connector maps these visual features into a format that the LLM can interpret, effectively serving as a bridge between the visual and textual domains. The LLM, pretrained on a vast amount of textual data, processes the integrated visual and textual data to generate coherent and contextually relevant responses.\nThe input to the MLLM typically includes an image $I$ and a corresponding instruction text $T_{ins} \\in \\mathbb{R}^L$, where $L$ is the number of tokens in the instruction. Initially, the image $I$ is processed by the image encoder $F_I(I)$ to extract visual features. Concurrently, the instruction $T_{ins}$ is tokenized using the tokenizer $F_T$ of the LLM to convert the text into a series of tokens. The extracted visual features are then flattened and projected into visual tokens. The connector $F_C$ converts these visual tokens into a format compatible with the LLM. Subsequently, the visual tokens and the textual tokens are concatenated along the spatial dimension and fed into the LLM $F_L$.\nThe LLM decodes the combined visual and textual tokens to generate a response token-by-token. Mathematically, this decoding process can be formulated as:\n$$p(R_t | I, T_{ins}, R_{0:t-1}) = F_L (R_t | F_C(F_I(I)), F_T(T_{ins}), F_T(R_{0:t-1})).$$"}, {"title": "4. Methods", "content": "In this section, we commence by providing a comprehensive overview of the proposed INF-LLaVA framework in Sec. 4.1, highlighting its innovative structure and capabilities. Next, we delve into the specifics of the two critical components: Dual-perspective Cropping Module and Dual-perspective Enhancement Module, thoroughly examining their intricate functionalities in Sec. 4.2 and Sec. 4.3, respectively."}, {"title": "4.1. Overview", "content": "Fig. 2 depicts the comprehensive pipeline of the proposed INF-LLaVA framework. Due to the pretrained ViT-CLIP encoder's limitation in processing high-resolution images\u2014given its quadratic complexity characteristics\u2014directly feeding high-resolution images into it is computationally prohibitive. To address this challenge, we propose Dual-perspective Cropping Module (DCM) $F_{DCM}(\\cdot)$, which partitions high-resolution images into several sub-images from both global and local perspectives, using the resolution defined by the pretrained vision encoder. Mathematically, this operation can be described as:\n$$[I_{loc}^1, I_{loc}^2, ..., I_{loc}^N; I_{glo}^1, I_{glo}^2, ............, I_{glo}^N] = F_{DCM}(I),$$\nwhere $[I_{loc}^1, I_{loc}^2, ..., I_{loc}^N]$ and $[I_{glo}^1, I_{glo}^2, ............, I_{glo}^N]$ represent the sub-images from local and global perspectives, respectively. Here, $N$ denotes the number of sub-images from each perspective, and $I \\in \\mathbb{R}^{W_h \\times H_h \\times 3}$ is the high-resolution input image. Next, each local and global sub-image is separately fed into the pretrained vision encoder:\n$$F_{loc}^i = F_I(I_{loc}^i),$$\n$$F_{glo}^i = F_I(I_{glo}^i),$$\nwhere $F_{loc}^i \\in \\mathbb{R}^{w_l \\times h_l \\times d}$ and $F_{glo}^i \\in \\mathbb{R}^{w_l \\times h_l \\times d}$ are the visual features of the i-th local and global sub-images, respectively. Here, $w_l \\times h_l$ denotes the number of visual tokens per sub-image, and $d$ is the channel dimension of the visual features. These local and global sub-image features are then recombined using 2D positional prior information to form high-resolution image features:\n$$F_{loc} = F_{loc}(F_{loc}^1, F_{loc}^2, ..., F_{loc}^N),$$\n$$F_{glo} = F_{glo} (F_{glo}^1, F_{glo}^2, ............, F_{glo}^N),$$\nwhere $F_{loc}(\\cdot)$ and $F_{glo}(\\cdot)$ are the recombination functions based on local and global positional information. The resulting features $F_{loc} \\in \\mathbb{R}^{w_h \\times h_h \\times d}$ and $F_{glo} \\in \\mathbb{R}^{w_h \\times h_h \\times d}$ represent the high-resolution visual features from local and global perspectives, respectively, where $w_h \\times h_h$ is the number of visual tokens in the recombined high-resolution images. To facilitate efficient interaction between the local and global features, the proposed Dual-perspective Enhancement Module (DEM) $F_{DEM}(\\cdot)$ is employed. This module ensures a robust exchange of information between local and global features, resulting in dual-enhanced features:\n$$F^{dual} = F_{pool}(F_{DEM}(F_{loc}, F_{glo})),$$\nwhere $F_{pool}(\\cdot)$ is the average pooling function used to reduce the number of visual tokens, thereby accelerating training and inference speeds while minimizing computational overhead. The resulting $F^{dual} \\in \\mathbb{R}^{w_l \\times h_l \\times d}$ represents the visual features enhanced from dual perspectives.\nFinally, the connector $F_C(\\cdot)$ projects the dual-enhanced visual features to obtain visual tokens that align with the textual features. The instruction $T_{ins}$ is tokenized by the tokenizer $F_T(\\cdot)$, converting it into a sequence of tokens. These visual tokens and textual tokens are then concatenated along the spatial dimension and fed into the pretrained LLM to generate the response:\n$$R = F_L(F_C(F^{dual}), F_T(T_{ins})),$$\nwhere $R \\in \\mathbb{R}^{L_{res}}$ represents the response generated by the LLM. Here, $L_{res}$ is the length of the generated response in tokens."}, {"title": "4.2. Dual-perspective Cropping Module", "content": "Dual-perspective Cropping Module (DCM) is designed to effectively partition the high-resolution image $I \\in \\mathbb{R}^{W_h \\times H_h \\times 3}$ into multiple sub-images $I_i \\in \\mathbb{R}^{W_l \\times H_l \\times 3}$, where $W_l \\times H_l$ corresponds to the resolution utilized by the vision encoder during pretraining. For instance, in the case of the CLIP-ViT-large-patch14-336 encoder, $W_l = H_l = 336$. The primary objective of DCM is to perform cropping from both local and global perspectives to capture fine-grained details and broader contextual information, respectively.\nIn the sections that follow, we will elaborate on the methodologies employed for cropping high-resolution images from both perspectives, ensuring that the integrity and essential characteristics of the original image are preserved."}, {"title": "4.2.1 Local-perspective Cropping", "content": "The local-perspective cropping technique is designed to systematically extract smaller regions from the high-resolution image while preserving detailed and continuous visual information. This approach ensures that each sub-image retains the high-resolution details necessary for accurate analysis and representation. By strategically segmenting the high-resolution image into localized sub-images, DCM effectively maintains the intricate details of the original image.\nGiven a high-resolution image $I \\in \\mathbb{R}^{W_h \\times H_h \\times 3}$, the first step is to determine the relationship between the dimensions of the high-resolution image ($W_h, H_h$) and the dimensions expected by the pretrained vision encoder ($W_l, H_l$). This relationship is quantified as follows:\n$$n_w = \\lfloor \\frac{W_h}{W_l} \\rfloor,$$\n$$n_H = \\lfloor \\frac{H_h}{H_l} \\rfloor,$$\nwhere $\\lfloor\\rfloor$ represents the floor function, which rounds down to the nearest integer. $n_w$ and $n_H$ correspond to the number of local sub-images along the width and height of the high-resolution image, respectively.\nThus, the high-resolution image $I$ is divided into $n_w \\times n_H$ local sub-images $I_{loc}^i$, each sized $W_l \\times H_l$. Specifically, for each sub-image $I_{loc}^i$, where $i \\in [0, n_H \\times n_w - 1]$, the following formulas determine the row and column positions:\n$$row = \\lfloor \\frac{i}{n_w} \\rfloor$$\n$$col = i \\mod n_w,$$\nwhere $mod$ denotes the modulo operation. The bounding box for each local sub-image $I_{loc}^i$ can be described by:\n$$I_{loc}^i = I [row \\cdot H_l : (row + 1) \\cdot H_l, col \\cdot W_l : (col + 1) \\cdot W_l, : ].$$\nThis process ensures that each local sub-image $I_{loc}^i$ contains continuous and detailed visual information from the high-resolution image, thereby preserving the integrity and quality of the image for localized analysis.\nBy using the local-perspective cropping technique, DCM maintains high-fidelity representation in each sub-image, enabling robust feature extraction and analysis. This approach ensures that finer details are not lost, providing a comprehensive understanding of localized regions without compromising on resolution or detail."}, {"title": "4.2.2 Global-perspective Cropping", "content": "Conversely, global-perspective cropping aims to capture broader contextual information from high-resolution images to preserve the spatial relationships between objects. This approach ensures that our model retains an understanding of both micro and macro-level details within the extracted sub-images, facilitating the integration of comprehensive global contextual information.\nGiven the high resolution $W_h \\times H_h$ of the input image and the low resolution $W_l \\times H_l$ used by the pretrained vision encoder, the number of global sub-images $n_W \\times n_H$ is computed similarly to the local perspective, as shown in Equ. (9) and (10):\n$$n_W = \\lfloor \\frac{W_h}{W_l} \\rfloor,$$\n$$n_H = \\lfloor \\frac{H_h}{H_l} \\rfloor,$$\nFor the i-th row and j-th column of sub-images, the pixel indices set $I_{ij}$ is defined as follows:\n$$I^{glo}_{ij} = \\{ I(x,y) |\\begin{array}{c} x = j + m \\cdot n_w, \\\\ y = i + n \\cdot n_H, \\\\ 0 \\leq m < \\frac{W_h}{n_w}, m \\in \\mathbb{N} \\\\ 0 \\leq n < \\frac{H_h}{n_H}, n \\in \\mathbb{N} \\end{array} \\},$$\nwhere $(x, y)$ corresponds to the pixel indices within the high-resolution image. $\\mathbb{N}$ represents the set of natural numbers. Thus, each pixel $(u, v)$ in the $I^{glo}_{ij}$ sub-image can be mapped back to the high-resolution image $I$ as:\n$$I^{glo}_{ij}(u, v) = I (j + u \\cdot n_w, i + v \\cdot n_H).$$ This module effectively partitions the high-resolution image into sub-images that encapsulate the global perspective by interleaving pixels from different regions. Consequently, it enables the model to maintain a coherent global context alongside the detailed local information captured by global-perspective cropping."}, {"title": "4.3. Dual-perspective Enhancement Module", "content": "In Sec. 4.2, we meticulously detailed the process of segmenting a high-resolution image into multiple sub-images from both local and global perspectives. Building on this foundational step, we now delve into the methods for extracting sub-features from these sub-images and their subsequent integration into high-resolution local-perspective and global-perspective features, as discussed in Sec. 4.3.1. Next, we elucidate the sophisticated techniques employed for enhancing these features\u2014specifically, the global-perspective enhancement in Sec. 4.3.2 and the local-perspective enhancement in Sec. 4.3.3. These enhancement techniques are designed to amplify the specific details and contextual richness of their respective features, ensuring a thorough and nuanced capture of both fine-grained and broad-spectrum information. Finally, in Sec. 4.3.4, we detail the fusion process where we meticulously integrate the locally-enhanced global features with the globally-enhanced local features. This fusion results in a dual-enhanced feature, which effectively combines detailed local information with comprehensive global insights. This robust integration provides an enriched and holistic representation of the high-resolution image, significantly enhancing the model's performance in various high-level visual analysis."}, {"title": "4.3.1 Sub-features Combination", "content": "Upon obtaining the local-perspective sub-images $[I_{loc}^1, I_{loc}^2, ..., I_{loc}^N]$ and global-perspective sub-images $[I_{glo}^1, I_{glo}^2, ............, I_{glo}^N]$ from DCM, we proceed to extract their corresponding features using Equ. (3) and Equ.(4). This extraction yields local-perspective sub-features $[F_{loc}^1, F_{loc}^2, ..., F_{loc}^N]$ and global-perspective sub-features $[F_{glo}^1, F_{glo}^2, ............, F_{glo}^N]$. As illustrated in Fig. 3, the sub-features from both perspectives are then systematically recombined. Specifically, the local-perspective sub-features $[F_{loc}^1, F_{loc}^2, ..., F_{loc}^N]$ are aggregated to construct the comprehensive local-perspective feature $F_{loc} \\in \\mathbb{R}^{w_h \\times h_h \\times d}$. Likewise, the global-perspective sub-features $[F_{glo}^1, F_{glo}^2, ............, F_{glo}^N]$ are amalgamated to form the global-perspective feature $F_{glo} \\in \\mathbb{R}^{w_h \\times h_h \\times d}$. This meticulous combination process ensures that the local-perspective feature effectively encapsulates fine-grained details, while the global-perspective feature maintains a coherent understanding of the broader contextual information."}, {"title": "4.3.2 Global-Perspective Enhancement", "content": "After obtaining the local-perspective feature $F_{loc} \\in \\mathbb{R}^{w_h \\times w_h \\times d}$ and global-perspective feature $F_{glo} \\in \\mathbb{R}^{w_h \\times w_h \\times d}$, the next crucial step involves interacting and fusing these two feature sets to derive more robust and comprehensive features. A straightforward approach would be to leverage cross-attention between $F_{loc}$ and $F_{glo}$ to enable interaction between local and global information. However, applying cross-attention directly to such high-resolution features can lead to significant out-of-memory issues during training. To circumvent this, we propose a novel dual-perspective enhancement method that interacts with local and global features in a more efficient manner, mitigating these computational challenges.\nSpecifically, for global-perspective enhancement, as illustrated in Fig. 4, we first crop both the local-perspective and global-perspective features from a global perspective. This operation can be mathematically formulated as follows:\n$$[G_{glo}^1, G_{glo}^2, ..., G_{glo}^N] = C_{glo} (F_{glo}),$$\n$$[L_{loc}^1, L_{loc}^2, ..., L_{loc}^N] = C_{glo} (F_{loc}),$$\nwhere $C_{glo}(\\cdot)$ denotes the global-perspective cropping operation. $[G_{glo}^1, G_{glo}^2, ..., G_{glo}^N]$ and $[L_{loc}^1, L_{loc}^2, ..., L_{loc}^N]$ represent the global and local sub-features, respectively, resulting from this global-perspective cropping.\nTo infuse local sub-feature information into the global sub-features, we perform a cross-attention operation between corresponding local and global sub-features. This can be formulated as follows:\n$$A^{glo} = Softmax (\\frac{(G_{glo}^i \\cdot W_q) \\cdot (L_{loc}^i \\cdot W_k)^T}{\\sqrt{d}}),$$,\n$$V_{glo}^i = A^{glo} \\cdot L_{loc}^i \\cdot W_v,$$\nwhere $W_q, W_k, W_v \\in \\mathbb{R}^{d \\times d}$ are learnable embedding matrices. $A^{glo} \\in \\mathbb{R}^{h_l w_l \\times h_l w_l}$ are the attention map between local sub-feature and global sub-feature. $V_{glo}^i \\in \\mathbb{R}^{w_l \\times h_l \\times d}$ are the i-th global-enhanced sub-feature.\nFinally, the set of enhanced sub-features $[V_{glo}^1, V_{glo}^2, ............, V_{glo}^N]$ are combined to form the globally-enhanced feature $V_{glo} \\in \\mathbb{R}^{w_h \\times h_h \\times d}$. This procedure ensures that the resulting feature representation captures rich, multi-scale information by effectively integrating local details with the global context, thereby enhancing the overall robustness and expressiveness of the model."}, {"title": "4.3.3 Local-Perspective Enhancement", "content": "Similarly, Local-Perspective Enhancement aims to refine local features by integrating global contextual information. This process ensures that the local features not only retain fine-grained details but also benefit from the broader context provided by the global features.\nFirst, the local-perspective and global-perspective features are cropped into sub-features through a local-perspective cropping operation. This is formulated as follows:\n$$[G_{loc}^1, G_{loc}^2, ..., G_{loc}^N] = C_{loc}(F_{glo}),$$\n$$[L_{loc}^1, L_{loc}^2, ..., L_{loc}^N] = C_{loc}(F_{loc}),$$\nwhere $C_{loc}(\\cdot)$ denotes the local-perspective cropping operation. The resulting sub-features, $[G_{loc}^1, G_{loc}^2, ..., G_{loc}^N]$ and $[L_{loc}^1, L_{loc}^2, ..., L_{loc}^N]$, represent the global and local sub-features respectively, generated from this local-perspective cropping. Next, we employ a cross-attention mechanism to facilitate interaction between local and global sub-features, enhancing the local features with global context. The formulation of this interaction is as follows:\n$$A^{loc} = Softmax (\\frac{(L_{loc}^i \\cdot W_q) \\cdot (G_{loc}^i \\cdot W_k)^T}{\\sqrt{d}}),$$,\n$$V_{loc}^i = A^{loc} \\cdot G_{loc}^i \\cdot W_v,$$\nwhere $W_q, W_k, W_v \\in \\mathbb{R}^{d \\times d}$ are learnable embedding matrices used for query, key, and value transformations, respectively. The attention map $A^{loc} \\in \\mathbb{R}^{h_l w_l \\times h_l w_l}$ captures the relationships between local and global sub-features. The resulting enhanced feature $V_{loc}^i \\in \\mathbb{R}^{w_l \\times h_l \\times d}$ is the i-th local-enhanced sub-feature.\nFinally, the set of local-enhanced sub-features $[V_{loc}^1, V_{loc}^2, ............, V_{loc}^N]$ are aggregated to form the local-enhanced feature $V_{loc} \\in \\mathbb{R}^{w_h \\times h_h \\times d}$. This process ensures that the local features are enriched with complementary global contextual information, providing a more comprehensive and robust representation of the original high-resolution image."}, {"title": "4.3.4 Dual-Perspective Fusion", "content": "After obtaining the global-enhanced feature $V_{glo}$ and the local-enhanced feature $V_{loc}$, the next critical step involves fusing these features to create a comprehensive representation that leverages the strengths of both perspectives. To achieve this, we employ a concatenation-based method that effectively combines the global and local features.\nInitially, we utilize two separate embedding layers to reduce the dimensionality of the features. This dimensionality reduction step is essential to ensure that the subsequent concatenation is computationally efficient and to highlight the most salient aspects of each feature set. The embedding operation can be described as follows:\n$$\\tilde{V}_{glo} = V_{glo}W_{glo}, \\tilde{V}_{loc} = V_{loc}W_{loc},$$\nwhere $W_{glo}, W_{loc} \\in \\mathbb{R}^{d \\times \\frac{d}{2}}$ are learnable projection matrices for the global and local features, respectively. These matrices transform the original features into a lower-dimensional space, thereby emphasizing their most critical components.\nNext, we concatenate the embedded global and local features along the channel dimension to form the dual-enhanced feature. This concatenation ensures that both global contextual information and local detailed information are retained and integrated. The fusion process is formulated as follows:\n$$V^{dual} = [\\tilde{V}_{glo}; \\tilde{V}_{loc}],$$\nwhere $[;]$ denotes the concatenation operation along the channel dimension. As a result, the dual-enhanced feature $V^{dual} \\in \\mathbb{R}^{w_h \\times h_h \\times d}$ encapsulates a comprehensive view of the input image, combining the strengths of both perspectives to produce a robust and informative representation."}, {"title": "5. Experiments", "content": "To rigorously assess the effectiveness, robustness, and versatility of the proposed INF-LLaVA, we conduct extensive evaluations across a diverse set of vision-language benchmarks. By leveraging a broad spectrum of datasets, we ensure a comprehensive evaluation of the model's capabilities across various dimensions and contexts. These benchmarks include:\n\u2022 ScienceQA-img [63]: ScienceQA is a large-scale multimodal dataset containing 21,208 science questions from elementary and high school curricula. Each question is annotated with lectures and explanations to provide comprehensive contextual understanding, making this dataset ideal for testing the model's ability to interpret and generate accurate, context-aware responses to science-related queries.\n\u2022 OKVQA [66]: This open-ended visual question answering dataset requires the model to utilize external knowledge sources to answer questions accurately. It challenges the model's proficiency in integrating visual content with external textual information, providing a robust test of the model's ability to generate responses based on prior knowledge and visual understanding.\n\u2022 SEEDBench [43]: SEEDBench is a comprehensive benchmark tailored specifically for evaluating Multimodal Large Language Models (MLLMs). It spans 12 evaluation dimensions, including comprehension of both image and video modalities. This benchmark offers a robust framework for assessing the performance of the model across a diverse array of multimodal tasks, ensuring a holistic evaluation.\n\u2022 MMBench [55]: MMBench is a multi-modality benchmark offering a comprehensive evaluation pipeline. It features a curated dataset and introduces the innovative CircularEval strategy using ChatGPT to enhance model prediction assessment. MMBench ensures a holistic evaluation of the model's multi-modal capabilities by providing a structured and thorough testing environment.\n\u2022 MMBench-CN [55]: The Chinese version of MMBench, MMBench-CN, facilitates a direct comparison of Vision-Language Model (VLM) performance in both English and Chinese contexts with verified translations. This benchmark tests the model's multilingual and cross-cultural adaptability, making it an essential tool for evaluating the robustness of the model in diverse linguistic settings.\n\u2022 AI2D [39]: AI2D is a dataset comprising illustrative diagrams aimed at research on diagram understanding and associated question answering. This benchmark challenges the model's ability to interpret and reason about structured graphical information, testing its proficiency in understanding and generating responses based on diagrammatic data.\n\u2022 LLaVA-Bench-in-the-wild [54]: This benchmark evaluates the capabilities of large multimodal models on real-world tasks and domains. Featuring detailed images and curated questions, LLaVA-Bench-in-the-wild tests the model's performance on diverse and challenging datasets regularly encountered in practical applications, ensuring its real-world applicability.\n\u2022 MMMU [105]: MMMU is a benchmark designed to evaluate multimodal models on college-level tasks spanning multiple disciplines. It requires advanced reasoning and subject matter expertise, thereby testing the depth of the model's understanding and its ability to handle complex, cross-disciplinary questions, making it a critical benchmark for assessing advanced cognitive capabilities."}, {"title": "5.2. Implementation Details", "content": "The vision encoder used in our implementation is the CLIP-ViT-L/14, which has demonstrated strong performance in visual tasks. The large language model (LLM) employed is LLaMA3-8B, which provides robust language understanding and generation capabilities. The training of INF-LLaVA is meticulously structured into two distinct stages to ensure optimal alignment and fine-tuning of the model components.\nIn the first stage, the pretraining phase, our primary objective is to align the features extracted by the vision encoder with the word embeddings generated by the LLM. To achieve this, we freeze both the vision encoder and the LLM during the pretraining phase. This allows us to focus on training the DEM and the projector. The model is pretrained using the CC-595k dataset [54], comprising a substantial collection of aligned image-text pairs, for 1 epoch. We utilize the AdamW optimizer [59] with a learning rate of 1 \u00d7 10\u22123 and employ a cosine learning rate schedule to smoothly adjust the learning rate during training. A global batch size of 256 is used to ensure efficient utilization of computational resources.\nIn the subsequent stage, the supervised fine-tuning (SFT) phase, our goal is to refine the model's performance on downstream tasks. During this phase, we freeze the vision encoder and proceed to train the DEM, the projector, and the LLM. The model is fine-tuned using the LLaVA-656K mixture dataset [52], which contains an extensive and diverse set of annotations to support comprehensive learning. We employ a lower learning rate of 2 \u00d7 10\u22125 and a batch size of 128 to carefully fine-tune the model parameters, ensuring precise adjustments without overfitting."}, {"title": "5.3. Quantitative Analysis", "content": "As shown in Tab. 1, we conduct a comprehensive comparison of the proposed INF-LLaVA with existing state-of-the-art (SOTA) Multimodal Large Language Models (MLLMs) across 8 widely recognized benchmarks. The results clearly demonstrate the superior performance of INF-LLaVA. Specifically, INF-LLaVA achieves an impressive score of 75.71 on the ScienceQA-img benchmark and 70.35 on the MMBench benchmark. These scores represent significant improvements over other leading MLLMs. Notably, INF-LLaVA outperforms QWen-VL-Chat by a considerable margin. It is important to highlight that QWen-VL-Chat leverages a substantially larger amount of data, utilizing 5 billion examples for pretraining, 76.8 million examples for multi-task training, and 350,000 in-house examples for supervised fine-tuning. In stark contrast, INF-LLaVA achieves its superior results with only 595,000 examples for pretraining and 665,000 examples for finetuning. Additionally, as detailed in Tab. 2, we trained an enhanced version of INF-LLaVA, marked as INF-LLaVA*, using a larger dataset. Our observations indicate that when fed with more data, the performance of INF-LLaVA improves even further across most benchmarks. For instance, on the LLaVA-bench-in-the-wild benchmark, INF-LLaVA* achieves an improvement of over ten points."}, {"title": "5.4. Qualitative Analysis", "content": "To thoroughly investigate the impact of different image resolutions on the performance of INF-LLaVA, we conducted a series of experiments using various input image resolutions. As demonstrated in Fig. 5, leveraging high-resolution images enables INF-LLaVA to provide more accurate answers to complex questions, particularly those requiring fine-grained perception. For instance, in the second case of the second row in Fig. 5, the screen displays multiple team names along with their corresponding scores. Identifying the team name associated with a specific score is a challenging task. However, with an input image resolution of 1008 \u00d7 1008 pixels, INF-LLaVA accurately identifies the KDE team as the one that scored 16 points. In contrast, when using a lower resolution of 336 \u00d7 336 pixels, INF-LLaVA provides an incorrect response. Moreover, in the second case of the third row, the task involves identifying the contents of a purple box that is closed, making it difficult to ascertain what is inside. With an input resolution of 1008 \u00d7 1008 pixels, INF-LLaVA successfully recognizes the \"ringdoll\" text on the box lid and accurately infers that the box might contain a ringdoll. Conversely, using a 336 \u00d7 336 pixels resolution, INF-LLaVA incorrectly concludes that the box contains no objects, demonstrating a lack of evidence-based reasoning. These observations underscore the importance of high-resolution images in enhancing the model's ability to perceive and interpret complex visual details, thereby improving its overall accuracy and effectiveness."}, {"title": "5.4.2 Comparison with LLaVA 1.5", "content": "In Fig. 6 and Fig. 7, we present a detailed comparison between INF-LLaVA and LLaVA-1.5, both of which utilize the same training dataset. The results clearly demonstrate that INF-LLaVA outperforms LLaVA-1.5 in several critical areas. Firstly, in terms of text recognition capabilities, INF-LLaVA exhibits greater accuracy. For instance, as shown in the first case of Fig. 6, when asked, \"What is the tagline with 'Wendell Rodricks' name?\", INF-LLaVA accurately identifies the tagline, whereas LLaVA-1.5 provides an incorrect response. This highlights INF-LLaVA's superior ability to discern and interpret textual information within high-resolution images. Secondly, regarding the issue of hallucination, INF-LLaVA demonstrates a clear advantage. When uncertain about an answer, INF-LLaVA candidly indicates its uncertainty, whereas LLaVA-1.5 tends to fabricate responses, leading to hallucination problems. For example, in the last case of Fig. 6, when asked to provide Chinese text, LLaVA-1.5 incorrectly responds with English text, whereas INF-LLaVA acknowledges its inability to answer, thereby avoiding erroneous information. Thirdly, in terms of counting accuracy, INF-LLaVA benefits from the enhanced details provided by high-resolution images, enabling more precise perception and reasoning. For instance, as depicted in the first case of Fig. 7, INF-LLaVA correctly identifies the number of birds in the image, while LLaVA-1.5 fails to provide an accurate count. This demonstrates INF-LLaVA's superior capabilities in tasks requiring detailed visual analysis. These comparisons consistently highlight the superior performance of INF-LLaVA across various complex tasks, emphasizing its advanced capabilities in text recognition, reducing hallucination issues, and enhancing counting accuracy."}, {"title": "5.5. Ablation Studies", "content": "We conduct a series of experiments to evaluate the impact of different input image resolutions on the performance of INF-LLaVA, as detailed in Tab. 3. Initially, we evaluate the model using single-resolution inputs and observe that higher-resolution images generally yield better results compared to lower-resolution images (e.g., 336 \u00d7 336). For instance, on the POPE benchmark, INF-LLaVA achieves scores of 85.86, 87.32, 87.17, and 88.12 for resolutions of 336 \u00d7 336, 672 \u00d7 672, 1008 \u00d7 1008, and 1344 \u00d7 1344, respectively. This trend underscores the benefit of high-resolution images in capturing finer details, which enhances the model's performance. However, it is noteworthy that the performance improvement is not strictly linear with increasing resolution. The optimal resolution varies across different benchmarks, potentially due to variations in image content and the inherent difficulty levels of the benchmarks. Inspired by previous works [24, 49], we explore the synergy of integrating both low-resolution and high-resolution images to capture comprehensive global and local information. When combining low-resolution and high-resolution features, we perform an element-wise addition of the two feature sets. As demonstrated in Tab. 3, this dual-resolution approach enables INF-LLaVA to achieve its best performance on benchmarks such as SEEDBench and MMVet when using 336\u00d7336 and 1008\u00d71008 resolutions concurrently. The superior results obtained with this dual-resolution strategy highlight its effectiveness in balancing detailed visualization with global contextual understanding. Consequently, we adopt this resolution setting as the default configuration in subsequent ablation experiments.\nTo further investigate the impact of varying image resolutions, we conducted a comprehensive set of experiments testing INF-LLaVA across multiple benchmarks with different image resolutions. As illustrated in Fig. 8, we observe a general trend where performance improves with increased resolution on most benchmarks, such as POPE [46], Infographic VQA [68], SEEDBench [43], MM-Vet [104], OKVQA [66], and LLaVA-in-the-wild [54]. This trend indicates that higher resolutions enable INF-LLaVA to capture more detailed visual information, thereby enhancing performance. However, it is important to note that this upward trend does not uniformly apply across all benchmarks. For instance, on GQA [36], AI2D [39], and TextCaps [83], the optimal resolution is 1008\u00d71008. On the MMMU [105] and DOCVQA [69] benchmarks, the best performance is achieved at a resolution of 672 \u00d7 672. This variability suggests that increasing resolution does not necessarily lead to continuous performance improvement for all tasks. In certain cases, higher resolutions may introduce redundancies or distortions due to image enlargement, which can negatively affect model performance."}, {"title": "5.5.2 Variants of Dual-perspective Enhancement Module", "content": "As illustrated in Fig. 4, DEM integrates two crucial enhancement operations: Global-Perspective Enhancement and Local-Perspective Enhancement. In this section, we conduct a series of ablation experiments to rigorously evaluate the effectiveness of these two enhancements."}, {"title": "5.5.4 Module Ablation", "content": "In this section, we conduct a thorough ablation study to examine the effectiveness of the proposed modules. As detailed in Tab. 6, we first evaluate the performance when using only one type of cropping method. The first two lines show that using global-perspective cropping alone (w/DCM global) consistently outperforms local-perspective cropping alone (w/ DCM local). This result suggests that global-perspective cropping, which maintains continuous visual information, is beneficial for model performance. In contrast, local-perspective cropping may inadvertently segment complete objects into several sub-images, complicating the recognition and understanding processes. Next, we investigate the impact of combining both cropping features through element-wise addition of the features (w/ DCM) instead of DEM. As evident from the third line of Tab. 6, this approach results in even lower performance than using only one type of cropping. We hypothesize that this degradation in performance arises due to the differing information densities of the tokens produced by each cropping method, making direct element-wise addition a sub-optimal fusion strategy. Finally, we consider the complete DEM module, which integrates both cropping methods in a more sophisticated manner. The last row of Tab. 6 demonstrates that DEM achieves the highest performance across all benchmarks. This clearly indicates that the combined use of local- and global-perspective enhancements, when managed effectively, significantly improves model performance. These findings underscore the effectiveness of the proposed DCM and DEM modules and highlight the importance of thoughtful feature integration strategies in enhancing model capabilities."}, {"title": "6. Conclusion", "content": "In this paper, we proposed INF-LLaVA, a novel MLLM designed for high-resolution image perception and reasoning. INF-LLaVA leverages two innovative modules: Dual-perspective Cropping Module (DCM), which crops high-resolution images into sub-images from both local and global perspectives, and Dual-perspective Enhancement Module (DEM), which efficiently fuses these features to obtain dual-enhanced features. Extensive experiments demonstrate that these modules significantly enhance INF-LLaVA's ability to understand high-resolution images, resulting in outstanding performance across various benchmarks. This work establishes a new state-of-the-art in vision-language tasks."}]}