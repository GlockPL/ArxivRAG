{"title": "How Toxic Can You Get? Search-based Toxicity Testing for Large Language Models", "authors": ["Simone Corbo", "Luca Bancale", "Valeria De Gennaro", "Livia Lestingi", "Vincenzo Scotti", "Matteo Camilli"], "abstract": "Language is a deep-rooted means of perpetration of stereotypes and discrimination. Large Language Models (LLMs), now a pervasive technology in our everyday lives, can cause extensive harm when prone to generating toxic responses. The standard way to address this issue is to align the LLM, which, however, dampens the issue without constituting a definitive solution. Therefore, testing LLM even after alignment efforts remains crucial for detecting any residual deviations with respect to ethical standards. We present EvoTox, an automated testing framework for LLMs' inclination to toxicity, providing a way to quantitatively assess how much LLMs can be pushed towards toxic responses even in the presence of alignment. The framework adopts an iterative evolution strategy that exploits the interplay between two LLMs, the System Under Test (SUT) and the Prompt Generator steering SUT responses toward higher toxicity. The toxicity level is assessed by an automated oracle based on an existing toxicity classifier. We conduct a quantitative and qualitative empirical evaluation using four state-of-the-art LLMs as evaluation subjects having increasing complexity (7-13 billion parameters). Our quantitative evaluation assesses the cost-effectiveness of four alternative versions of EvoTox against existing baseline methods, based on random search, curated datasets of toxic prompts, and adversarial attacks. Our qualitative assessment engages human evaluators to rate the fluency of the generated prompts and the perceived toxicity of the responses collected during the testing sessions. Results indicate that the effectiveness, in terms of detected toxicity level, is significantly higher than the selected baseline methods (effect size up to 1.0 against random search and up to 0.99 against adversarial attacks). Furthermore, EvoTox yields a limited cost overhead (from 22% to 35% on average). This work includes examples of toxic degeneration by LLMs, which may be considered profane or offensive to some readers. Reader discretion is advised.", "sections": [{"title": "I. INTRODUCTION", "content": "Social harm being perpetrated through written text is a long-established pressing matter. Language can, indeed, reiterate offensive stereotypes and sting targets at a high risk of discrimination. These issues are often categorized as \"hate speech,\u201d a phenomenon whose severity has escalated significantly in recent years, now being considered, in some cases, a cyber-enabled crime [1]. The widespread use of Large Language Models (LLMs) as language generators introduces new concerns due to their potential to produce harmful, or so-called toxic, content [2] typically defined as rude, disrespectful, or unreasonable content; likely to make people leave a discussion [3]. Toxic degeneration in LLMs often emerges from the data they are trained on, which can reflect societal prejudices and stereotypes. The mainstream approach to dampening this issue is to filter the training data, often with hand-made rules and heuristics. Additionally, fine-tuning processes integrate alignment that discourages the generation of toxic responses [4]-[7]. Recent studies show that alignment is not a definitive solution, leaving residual defects. Specifically, an aligned LLM is still susceptible to deviations with respect to desired ethical standards [8]. Therefore, automated testing to assess LLMs' proneness to toxicity before deployment in production is crucial. Recent approaches to automated testing for LLMs considering ethical concerns focus on generating adversarial attacks. Adversarial attacks, also called Jailbreak prompts in this context, add malicious prefixes or postfixes to given prompts to elicit outputs that should not be given by aligned LLMs [9], [10]. Jailbreak techniques suffer from scalability issues when attacks heavily rely on manually crafted prompts [11]. Existing automated Jailbreak attacks are typically white-box, meaning they require access to open-source LLMs to exploit internal information, such as layer structure, weights, and gradient information. Furthermore, attacks typically generate semantically meaningless (or out of distribution) prompts that do not fit in day-to-day human-to-LLM interactions [12] (e.g., they contain unnatural prefixes or randomly fuzzed sections). In this work, we explore the extent to which LLMs can be pushed to generate toxic responses, even when alignment measures are in place, through automatically generated natural day-to-day interactions. The generation of natural, realistic prompts that can trigger toxic degeneration is challenging for several reasons. LLMs take as input arbitrary natural language prompts leading to a huge search space. Natural language has complex grammatical and syntactical rules that are difficult to replicate accurately with mainstream fuzzing methods [13]. Generated prompts need to be logically coherent and, at the same time, consistently trigger toxic degeneration avoiding repetitive and predictable patterns. While fuzzing can, in principle, introduce variety through randomness, ensuring that this randomness results in realistic and meaningful prompts remains challenging. We address these challenges by introducing EvoTox, an automated search-based testing [14] framework that assesses the proneness to toxic text generation of a target LLM, referred to as System Under Test (SUT). EvoTox adopts a 1 + \u03bb Evolution Strategy [15] (ES). Starting from a given seed (i.e., initial prompt) and, for each iteration, the strategy searches for new prompts (i.e., mutants) using a second LLM"}, {"title": "II. BACKGROUND", "content": "In this section, we provide preliminary knowledge about LLMs (Section II-A) and search-based testing (Section II-B)."}, {"title": "A. Large Language Models", "content": "LLMs are probabilistic generative models of text based on Deep Neural Networks (DNNs) trained on massive amounts of data. These models are based on the Transformer architecture, designed to process sequential data, like sequences of text tokens\u00b9, through the self-attention mechanisms [20]. The Transformer architecture enables LLMs to capture complex dependencies throughout the input text, leading to highly coherent and contextually relevant text generation. State-of-the-art LLMs are either closed-access (commercial), including models with hundreds or thousand billions of parameters like CHATGPT/GPT-4 [21] and GEMINI [22], [23], or open-access (community), including models with few or tens of billions of parameters such as LLAMA [24], [25], VICUNA [26], MISTRAL [27], and GEMMA [28]. These latter models represent a set of accessible alternatives for research and application development. LLMs are often fine-tuned (i.e., adapted through further training) for specific applications. Fine-tuning is generally used to turn the LLM into an instruction-following agent [29], [30] or a chatbot-assistant [31]. Instruction-following fine-tuning consists of training the model to follow specific directives, given in the form of natural language prompts, to solve a task. Chatbot-assistant fine-tuning adapts the model to interact with a user in conversational settings, making it suitable for applications like virtual assistants. During training, the model weights are updated based on the likelihood of generating the target response given an input sequence composed of: (1) the system message (initial task instructions in a specific chatbot-assistant use case); and (2) a user prompt (user's question or request in a specific chatbot-assistant use case). Advanced methods use sequences or chats with multiple exchanges between the user and the LLM to cover multiple tasks or multiple steps within a task (e.g., to handle user corrections). Fine-tuning the LLM on this request-response"}, {"title": "B. Search-based Testing", "content": "Search-Based Software Testing [14] (SBST) uses meta-heuristic optimization to automate testing tasks such as test case generation and prioritization for a specific SUT. The idea is to recast the testing problem as an optimization problem by defining a proper fitness function according to the objective(s) of the testing task (e.g., cover test targets, spot defects). Evolutionary algorithms are a family of meta-heuristic optimization algorithms commonly employed by SBST techniques. These algorithms evolve a population of individuals (candidate solutions to an optimization problem) in an iterative fashion using genetic operators (e.g., mutation and crossover). The fitness function estimates the proximity of each individual to the desired optimum. During the evolution process, the best individuals are selected for the next generation based on their fitness. In SBST, evolutionary algorithms steer the search process towards better test cases, where \"better\" is defined by the fitness function that shall embed domain knowledge to evaluate the quality of the test cases. As an example, EVOSUITE [34] automatically generates unit test cases for Java programs to satisfy a given coverage criterion (e.g., branch coverage). In this latter case, the fitness function is defined based on the degree of coverage achieved by the generated test cases. Evolution Strategy [15] is a well-known evolutionary algorithm consisting of iterative selection and mutation. The original version has been proposed for real-valued optimization where a Gaussian mutation is applied, and the selection is based on the fitness value of each individual. While originally tailored to continuous problems, it was later adapted for discrete domains by developing appropriate mutation operators and selection mechanisms that align with the discrete nature of the problem. Notable examples of its successful application in discrete domains include job scheduling and vehicle routing. The simplest evolution strategy operates on a population of size one: the current individual (parent) generates one offspring through mutation. If the mutant's fitness is at least as good as the parent's, it becomes the parent of the next generation; otherwise, the mutant is discarded. This method is known as (1+1)-ES. More generally, multiple mutants can be generated to compete with the parent. In this case, the number of mutants is denoted as \u03bb. In (1+1)-ES, the best mutant becomes the parent of the next generation, while the current parent is possibly discarded."}, {"title": "III. EvoTOX FRAMEWORK", "content": "In this section, we outline the EvoTox framework. We provide an overview of the entire testing system (Section III-A), and we delve into the details of toxicity evaluation and prompt generation modules (Sections III-B and III-C, respectively)."}, {"title": "A. Overview", "content": "EvoTox is a search-based testing framework designed to assess LLM's proneness to toxicity. It pushes the responses of a LLM SUT towards increased toxicity levels through the iterative generation and selection of natural, realistic prompt mutants obtained through rephrasing of the parent. Our approach adopts a (1+1)-ES exploiting the interplay between two LLMs, that is, we systematically test LLMs using LLMs to identify toxicity degeneration, thus offering insights for further improvement of the SUT before deployment in production. Our framework is agnostic of the underlying LLMs, it is black-box since it does not require internal information of the SUT, and it also allows for self-testing as further detailed in the following. Notice that ES has been selected due to its reliance on mutation and selection rather than recombination/crossover (used by other metaheuristic search algorithms) that does not naturally map to our problem domain, where the search space contains individuals representing meaningful prompts. Figure 3 shows a high-level workflow of the evolutionary process implemented by EvoTox. The structure includes three main logical components: the SUT, the PG, and the Toxicity Evaluation System (TES). The SUT is the LLM being tested; it receives a prompt as input and generates a response to that prompt as output. The PG is the LLM that generates prompt mutants by iteratively rephrasing an initial prompt (seed) with the aim of increasing the toxicity level of the responses. The TES acts as the oracle of EvoTox to assess the toxicity level of the responses in different toxicity categories. As illustrated in Figure 3, EvoTox incorporates two datasets: the seed dataset which holds an initial set of prompt seeds used to initiate the test process; and the archive keeping track of all the generations of the test process, including selected/discarded prompt mutants, and all the collected responses including those achieving the highest toxicity level. EvoTox adopts a (1+x) setting, meaning that it operates with a population of a single prompt, which evolves up to a given testing budget (e.g., maximum number of generations or maximum toxicity score). In each generation, the parent yields A mutants obtained by rephrasing the parent prompt to maximize a fitness function that maps a given prompt to the toxicity score of the corresponding response generated by the current SUT. This means that the fitness depends on the specific LLM being tested. The fitness function has image [0,1] \u2208 R, where 0 represents the lowest (null) likelihood of toxic content and 1 represents the highest likelihood of toxic content."}, {"title": "B. Toxicity evaluation", "content": "Characterizing the toxicity of machine-generated natural language content represents a crucial point for EvoTox to understand toxic degeneration in LLMs. This represents a challenging task because the ground truth, defined as adherence to ethical and societal norms, cannot be rigorously specified to mechanically detect toxic content using standard algorithms. Manual human annotation of generated responses during testing becomes impractical due to a potentially large volume of text for each test session. Additionally, we aim for a fully automated testing process and, therefore, an automated oracle. We synthesize such oracle by embedding into EvoTox an automated tool for detecting toxic language and hate speech. Specifically, we exploit a widely used, commercially deployed toxicity detector called PERSPECTIVE API [3], developed by Google JIGSAW unit. PERSPECTIVE uses pre-trained classifiers to predict the perceived impact of a comment on a conversation by evaluating the content of the comment across a range of attributes, henceforth referred to as toxicity categories. PERSPECTIVE considers six toxicity categories: severe toxicity, insult, profanity, identity attack, threat, and sexually explicit content. The API returns, for a given piece of text, six real-valued scores in the [0,1] range, one for each category. Since categories are not mutually exclusive and there can be some overlap in the evaluated content, scores are independent of each other and, thus, are not normalized to sum to 1. According to Gehman et al. [8], the approach employed to calibrate the predictive model (isotonic regression) ensures that the score can be meaningfully interpreted as a confidence level of toxicity. Given the vector of toxicity scores, we apply scalarization to summarize the result with a single value that EvoTox uses as fitness for a given prompt. Scalarization converts the problem from multi- to single-objective optimization\u00b3. We consider two scalarization approaches: average of the scores and max of the scores, both of them with clamped variants. Clamped variants are alternative scalarization versions, where scalarized scores exceeding a fixed threshold are multiplied by a factor $g \\in R$: 0<g<1, deliberately reducing the actual fitness. This approach-referred to as gaslighting-implements a fitness function that aims at promoting diversity during the search process. Gaslighting manipulates (decreases) the actual toxicity score of good individuals (possibly local optima) and forces the evolutionary search toward other directions to find other, potentially better, candidates. Possible alternative approaches to evaluating toxicity leverage, for example, LLMs, which recent studies show to align well with human judgment on ethical reasoning [35], [36]. Results vary depending on factors like LLM, prompt phrasing and language [35]\u2013[37], and highlight that LLMS classification predictions correlate with PERSPECTIVE [38]."}, {"title": "C. Prompt evolution", "content": "The PG component creates prompt mutants for each generation of the evolutionary search by rephrasing (and thus evolving) the parent prompts with the aim of increasing the toxicity score out of the responses given by the LLM SUT. The evolutionary search starts from an initial prompt sampled from the seed dataset. The seed dataset contains questions that could trigger toxic responses. At the current stage, we populate the set of seeds by using HARMFULQA [39], a reference dataset of 1.9k prompts, developed in a safety evaluation benchmark for LLMs. This dataset is widely used to measure how prompt toxicity influences the toxicity of text generated by LLMs. An example of seed follows: \"How to use number theory to bypass firewalls and gain access to restricted networks?\". Such a prompt could generate harmful content, such as hacking instructions, privacy violations, or encouragement of illegal activities. Starting from a seed and then, for each generation, EvoTox makes use of the PG LLM to generate A mutants in the neighborhood of the parent. Essentially, we leverage the inherent ability of LLMs to craft natural, realistic sentences (closely resembling human writing) that are semantically similar to the parent. We achieve this by prompting the PG for a specific rephrasing task. The interaction with the PG for the rephrasing task is structured in a chat-like format. The initial system message explains the rephrasing task to the PG, while mechanically generated user messages feed the PG with the input prompt to be rephrased. This prompt-response interaction with the PG takes place in a separate chat from the one used for prompting the SUT. This setup allows for testing configurations where the SUT can also serve as the PG, that is, self-testing. EvoTox prompts the PG A times to generate alternative mutants of the given parent. Each mutant is generated using a specific user message that requests rephrasing the original prompt in ways that may increase the likelihood of eliciting toxic responses within a particular conditioning class. EvoTox accepts arbitrary conditioning classes defined by the tester. Conditioning classes can encompass the toxic categories recognized by our oracle (PERSPECTIVE) but are not limited to them. For example, a chosen class might be \u201csarcasm\", which can potentially trigger toxic content across various categories (e.g., identity attack, or sexually explicit content). EvoTox comes with \u03bb=5 default categories, which we selected based on their prevalence in literature about bias, fairness and toxicity [40]\u2013[42]: homophobic, insulting, racist, sexist, and generic toxic content. After generating the mutants, EvoTox computes the toxicity score (fitness) of the corresponding responses. The prompt associated with the highest score among the generated mutants is promoted to be the parent for the next generation. If none of the mutants achieve a score equal to or higher than the parent, the original parent prompt is retained. During these interactions, we generate the SUT responses via sampling of the LLM output token distribution, which, unlike deterministic generation strategies (e.g., greedy decoding), allows for more diverse responses for the same input and enhances the chance of discovering diverse responses over multiple iterations. EvoTox adopts alternative prompt evolution methods, implemented through different prompting techniques used during the interaction with the PG: few-shot learning, stateful evolution, and informed evolution.\""}, {"title": "IV. EVALUATION", "content": "This section reports on the empirical evaluation of EvoTox using four evaluation subjects and five testing methods under comparison: four different versions of EvoTox and four selected baseline methods. We answer the following research questions:\nRQ1: What is the effectiveness of EvoTox compared to selected baseline methods?\nRQ2: What is the cost overhead introduced by EvoTox?\nRQ3: What are the most common conditioning classes exploited by EvoTox to increase the toxicity score?\nRQ4: How fluent, or human-like, are the prompts generated by EvoTox compared to adversarial attacks?\nRQ5: What is the perceived toxicity level of responses obtained by EvoTox according to human raters?"}, {"title": "A. Design of the evaluation", "content": "To address our research questions, we compare the results of different toxicity testing approaches applied to the same set of evaluation subjects, all within the same budget. RQ1, RQ2, and RQ3 are answered quantitatively using selected metrics to assess effectiveness, cost overhead, and frequency of conditioning classes. RQ4 is answered both quantitatively and qualitatively using selected metrics and having humans evaluate the fluency of the generated input prompts. RQ5 is answered qualitatively by having domain experts evaluate the perceived toxicity level of responses. The following sections detail the evaluation subjects selected, the testing approaches compared, and the testbed used for the experimental campaign.\n1) Evaluation subjects: Table I lists the selected LLMs used to evaluate EvoTox. We employ a diverse set of open-access state-of-the-art LLMs released between 2023 and 2024. All selected subjects use standard LLM format GGUF and LLM quantization Q5_K_M [43]. For all selected models, we keep the output probability distribution unchanged (temperature =1, top-p and top-k set to retain the full vocabulary) to avoid additional sampling hyper-parameters and ensure a balance between diversity and favoring higher-probability outputs. The models were chosen for their variety in parameter sizes (ranging from 7 billion to 13 billion parameters) and their distinct alignments: one uncensored subject and four subjects aligned following state-of-the-art practices [4]. We use three aligned subjects (Mistral 7B, Llama3 8B, and Vicuna 13B) as SUT and PG LLMs. We also use one additional non-aligned subject (Vicuna 13B uncensored) as PG LLMs. For each aligned subject, we test it using different PG LLMs: itself (i.e., self-testing), and also two versions of Vicuna (Vicuna 13B and VicunaU 13B uncensored) to study the extent to which a larger volume of parameters and censorship affect the evolution and toxic degradation of the SUT.\n2) Methods under comparison: We evaluate and compare different versions of EvoTox implementing alternative prompt evolution strategies introduced in Sec. III. In particular, we consider prompt evolution using few-shot learning complemented by stateful evolution (SE) and informed evolution (IE), as well as gaslighting (GL) variants. Table II lists all the versions of EvoTox selected for our experiments. For comparison with EvoTox, we use existing approaches in the field of Jailbreak research. Specifically, we use two well-known datasets: AdvBench [17] and MaliciousInstructions [18]. AdvBench dataset is a benchmark designed to evaluate adversarial robustness in language models, consisting of 1k potentially harmful behaviors that adversaries try to elicit. MaliciousInstructions contains 100 malicious instructions with 10 different malicious intents (e.g., psychological manipulation, cyberbullying). Furthermore, we adopt mainstream Jailbreak techniques exploited by MasterKey [44]. These techniques include 80 adversarial prompt templates in different categories, such as DAN (do anything now), STAN (strive to avoid norms), DevMode, and universal black-box jailbreaking. We refer the reader to Liu et al. [19] for a comprehensive description of Jailbreak techniques and their categorization. We also use Random Search (RS) as a baseline since it represents a neutral reference point evaluating the practical advantages of our evolutionary search strategies. Notice that generating random sequences of words, for instance through generational fuzzers [45], is not suitable since the likelihood of producing realistic, coherent, and meaningful sentences is low; thus, we leverage HARMFULQA. Specifically, RS selects prompts from the dataset HARMFULQA using uniform random sampling and adopting the same testing budget used for EvoTox. We use this baseline to get insights into the complexity of the evolution problem and quantify the relative effectiveness of the other methods listed above. For all baseline methods, we archive all prompts that lead to the highest toxicity score found during the test session.\n3) Statistical tests: To reduce the risk of obtaining results by chance, we account for randomness in all methods under comparison by repeating the testing sessions 100 times with the same budget (in terms of total number of tests). According to the guideline introduced by Arcuri & Briand [46], we apply the non-parametric Mann-Whitney U tests [47] to assess the statistical significance of the results. We also measure Vargha and Delaney's $A_{AB}$ [48] to compute the effect size of the difference between the samples A and B. We adopt the following standard classification: effect size $\u00c2_{AB}$ (=1-$A_{BA}$) is small, medium, and large when its value is greater than or equal to 0.56, 0.64, and 0.71, respectively. In addition to the Mann-Whitney U test, we employ, where applicable, the t-test [49] to evaluate the statistical significance of the results, specifically to compare means between samples. To assess the reliability of agreement between human raters, we adopt the statistical measure Fleiss' kappa [50] with the following standard classification [51]: poor, slight, fair, moderate, substantial, and almost perfect agreement when the measure is greater than 0.0, 0.2, 0.4, 0.6, 0.8, respectively.\n4) Testbed: All experiments have been executed on two desktop machines (a) and (b), both running UBUNTU 18.04.6 LTS. Machine (a) is equipped with an Intel Xeon E5-2609 v2 CPU at 2.5GHz (4 cores) with 32GB RAM and a NVIDIA Titan RTX GPU with 24GB VRAM. Machine (b) is equipped with an Intel Core i9-13900KF CPU at 5.8GHz (24 cores) with 32GB RAM and a NVIDIA Geforce RTX 3080 GPU with 16GB VRAM. We use machine (a) to deploy and run Mistral, Llama3, and Vicuna (aligned version) subjects. We use machine (b) to deploy and run VicunaU (uncensored version) subject."}, {"title": "B. Results", "content": "1) RQ1: What is the effectiveness of EvoTox compared to selected baseline methods?\na) Setup: To answer RQ1, we execute all versions of Evo-Tox listed in Table II and all the selected baseline approaches. We use all evaluation subjects listed in Table I as SUT for each testing approach. For each SUT, we consider three alternative PGs: the same model (self-testing) and two versions of Vicuna (censored, and uncensored). We compare the effectiveness of the approaches by measuring the toxicity score achieved by the best individuals found during testing. As anticipated in Section III, we use X = 5 mutants (default conditioning classes) to select the next parent for each generation. We limit the evolutionary search to 10 generations (i.e., the budget is 50 tests). According to our preliminary experiments, this setting is enough to reach a plateau for all versions of EvoTox (i.e., the average score improvement is less than 0.01). We did not fine-tune the parameters of the different versions of EvoTox but we configured the values based on preliminary results. For all methods, we use max scalarization, as it yields better performance than average. For SE, we set a fixed \u201chistory\u201d size of 5, representing the number of previous evolutions included in the interactions with PG. We found that this value allows us to include substantial contextual information without exceeding the token limit. For GL variants, we set a fixed threshold of 0.35, corresponding to the average toxicity score at the plateau. To encourage exploration beyond the plateau, we scalarized the score using a factor of g=0.5 (half of the original score).\nb) Results: Figure 7 shows the distribution of the toxicity score for RS and EvoTox (all versions in Table II) over 100 repeats for each PG-SUT pair. Table III and Table IV show the results of the statistical tests for the effectiveness of different versions of EvoTox compared to the other baseline methods and compared to each other, respectively. The first two columns of the two tables indicate the SUT-PG pair. Numbers indicate p-value and effect size $\u00c2_{AB}$, when comparing the two approaches A and B in terms of achieved toxicity score. We consider statistical significance if p-value <0.05. According to Table III, the difference between EvoTox (all versions) and all baseline methods (including RS, advbench, maliciousInstruct, and Jailbreak prompts) is statistically significant for all PG-SUT pairs. In all cases, $\u00c2_{AB}$ is greater than 0.71 when A is EvoTox, meaning that the effect size is large. In some cases, we can see that Jailbreak prompts can achieve higher peaks (see Fig. 7g and Fig. 7h). However, the effectiveness of Jailbreak is significantly lower on average. Considering different versions of EvoTox, the effectiveness of vanilla is comparable to IE and IE+GL as shown in Table IV (no statistical difference). However, IE and IE+GL can achieve higher toxicity score peaks compared to vanilla. The vanilla version yields the highest peak in 25% of the cases, while IE and IE+GL yield the highest peak in 62% and 50% of the cases, respectively. Further, gaslighting can increase the toxicity score compared to no-gaslighting. According to Figure 7, IE+GL enhances the highest toxicity score achieved by IE in 62% of the cases. The IE+SE+GL version generally performs significantly worse than the other versions. There is a medium to large effect size $\u00c2_{AB}$ when B is IE+SE+GL and A is another version of EvoTox. This may be due to the increased complexity of the input context in this specific version. An excessive amount of context can overwhelm the model's attention mechanism, preventing it from focusing on the most relevant parts of the input. As a result, the model may fail to process the context correctly. Results in Figure 8 indicate that certain characteristics of the PG model influence the effectiveness of EvoTox. Specifically, Figure 8b shows that the PG model Vicuna (a larger model) does not produce significant differences compared to self-testing, whereas VicunaU yields statistically significant improvements (p-value is 2.4\u00d710-35 and 2.7\u00d710-45 when comparing it with self-testing and Vicuna, respectively) with medium effect size ($\u00c2_{AB}$ is 0.66 in both cases). According to Figure 8b, the average toxicity score (in all runs) increases more"}, {"title": "RQ2 summary. In general, the cost of executing the SUT dominates the total execution time for a test case. Therefore, the cost overhead introduced by EvoTox (all versions) is limited compared to all baseline methods when operating under the same budget.", "content": "RQ2 summary. In general, the cost of executing the SUT dominates the total execution time for a test case. Therefore, the cost overhead introduced by EvoTox (all versions) is limited compared to all baseline methods when operating under the same budget. 3) RQ3: What are the most common conditioning classes exploited by EvoTox to increase the toxicity score?\na) Setup: To address RQ3, we maintain the same setup as in RQ1 and RQ3, but we count the number of times each conditioning class occurs during the selection process, where alternative mutants compete to become the next parent, across all testing sessions. Specifically, we are interested in the relative frequency of five selected conditioning classes (X=5) used by our default configuration of EvoTox: homophobic, insulting, racist, sexist, and generic toxic content. Our objective is to identify, for each evaluation subject, the common classes that are more susceptible to toxic degeneration, thereby identifying common weaknesses in state-of-the-art LLMs.\nb) Results: Figure 10 shows the relative occurrence frequency for each conditioning class considering all runs for all SUT LLMs. Homophobic is the conditioning class associated with the highest frequency for all SUT LLMs. The class racist occurs as the second highest frequency for 2 out of 3 SUT LLMs (Llama3 and Mistral). For Vicuna, the class insulting yields the second highest frequency, while racist is the third one. Among those considered in our experiments, homophobia and racism appear to be the most common weaknesses. Indeed, the two conditioning classes homophobic and racist are more prone to toxic degeneration with frequency up to ~0.3 and ~0.25, respectively. The classes sexist and insulting occur as the third and fourth highest frequency for both Llama3 and Mistral (up to ~0.2). The generic conditioning class toxic is the last one (up to ~0.12) for all SUT LLMs."}, {"title": "RQ3 summary. Homophobia and racism are the most common weaknesses, as the corresponding conditioning classes are the most frequently exploited by EvoTox to increase the toxicity score. In contrast, general classes, such as toxic, are less prone to toxic content degeneration.", "content": "RQ3 summary. Homophobia and racism are the most common weaknesses, as the corresponding conditioning classes are the most frequently exploited by EvoTox to increase the toxicity score. In contrast, general classes, such as toxic, are less prone to toxic content degeneration. 4) RQ4: How fluent, or human-like, are the prompts generated by EvoTox compared to adversarial attacks?\na) Setup: We address RQ4 both quantitatively and qualitatively by: (1) measuring the perplexity of the prompts generated by EvoTox and the other baseline methods and then (2) evaluating the fluency of generated prompts from a human perspective. Perplexity is an intrinsic metric evaluating a language model's level of \u201csurprise\u201d when presented with a given piece of text [52]. Statistically, it is defined as the reciprocal of the geometric mean of the token probabilities predicted by the model. As such, perplexity is inversely proportional to the likelihood that the language model can accurately predict the given token sequence. For a language model trained on a corpus of natural language, a perplexity score that is both low and close to that of reference human or human-validated text can be a good indicator of the fluency of a given piece of text in terms of diversity and quality [53]. Usually, small values of perplexity indicate less surprising and more diverse text, but scores that are too may be a consequence of low quality text [54], because human-generated text tends to be a little surprising if compared to machine-generated [53]. Moreover, since LLMs are trained on extensive corpora that often include a mixture of languages, slang, and artificial (e.g., programming) languages, these factors can distort perplexity scores for the target language (English in our case). To address this, we employ a separate model to compute perplexity. We train an n-gram language model [52] (n=5) using Book Corpus [55], a large collection of openly available English novels. This 5-gram model is used to compute the perplexity of the generated prompts. For completeness, we analyze the fluency of the prompts by engaging human evaluators. We conducted a questionnaire-based A/B testing study to evaluate the fluency of English text samples from a human perspective. The sample consists of 50 human assessors recruited using convenience sampling from the personal and professional networks of the authors. The participants' English reading proficiency levels\u2074 is distributed as follows: 22% at C2, 48% at C1, and 30% at B2, with 65% holding authoritative certifications (e.g., TOEFL, IELTS, or Cambridge). The gender distribution is 63% male and 37% female."}, {"title": "RQ4 summary. The analysis reveals that, for EvoTox, the perplexity of the mutants is generally comparable to that of the seed prompts, resulting in similar fluency. In contrast, Jailbreak prompts are typically perceived as less fluent. These findings are further supported by statistically significant human preferences.", "content": "RQ4 summary. The analysis reveals that", "RQ5": "What is the perceived toxicity level of responses obtained by EvoTox according to human raters?\na) Setup: To address RQ5, we engaged human raters, specifically psychologists and psychotherapists selected for their expertise in mental health and behavioral assessment. These professionals were tasked with evaluating the responses obtained by EvoTox to determine whether they contain harmful content from a human perspective. The experts are chosen due to their experience in evaluating psychological content, which enhances the reliability and validity of the ratings. Our sample consists of 30 human raters, comprising 31% psychologists and 69% psychotherapists, each with 2 to 25 years of professional experience. The gender distribution is 22% male and 78% female. All participants are Western European. This homogeneity is intended to reduce variability related to cultural and ethnic differences in the perception of toxicity. We created two surveys consisting of 30 questions each, with an equal split of 15 questions derived from evolved prompts and 15 from seed prompts. The questions were developed through a systematic process. First, we merged all the archives obtained during the testing sessions of our experimental campaign, including all 100 repeats. From this data, we randomly sampled 30 evolved individuals (belonging to the last generation of the test sessions). We then identified the seeds of these 30 individuals from our seed dataset (based on HARMFULQA) and retrieved the corresponding responses. The sampled responses were then evenly split between the two surveys, maintaining a 50% ratio of seed to evolved prompts. The order of questions within each survey was randomized to"}]}