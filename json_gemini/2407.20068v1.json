{"title": "UNLEASH THE POWER OF ELLIPSIS: ACCURACY-ENHANCED\nSPARSE VECTOR TECHNIQUE WITH EXPONENTIAL NOISE", "authors": ["Yuhan Liu", "Sheng Wang", "Yixuan Liu", "Feifei Li", "Hong Chen"], "abstract": "The Sparse Vector Technique (SVT) is one of the most fundamental tools in differential privacy (DP).\nIt works as a backbone for adaptive data analysis by answering a sequence of queries on a given\ndataset, and gleaning useful information in a privacy-preserving manner. Unlike the typical private\nquery releases that directly publicize the noisy query results, SVT is less informative\u2014it keeps the\nnoisy query results to itself and only reveals a binary bit for each query, indicating whether the\nquery result surpasses a predefined threshold. To provide a rigorous DP guarantee for SVT, prior\nworks in the literature adopt a conservative privacy analysis by assuming the direct disclosure of\nnoisy query results as in typical private query releases. This approach, however, hinders SVT from\nachieving higher query accuracy due to an overestimation of the privacy risks, which further leads to\nan excessive noise injection using the Laplacian or Gaussian noise for perturbation. Motivated by this,\nwe provide a new privacy analysis for SVT by considering its less informative nature. Our analysis\nresults not only broaden the range of applicable noise types for perturbation in SVT, but also identify\nthe exponential noise as optimal among all evaluated noises (which, however, is usually deemed non-\napplicable in prior works). The main challenge in applying exponential noise to SVT is mitigating the\nsub-optimal performance due to the bias introduced by noise distributions. To address this, we develop\na utility-oriented optimal threshold correction method and an appending strategy, which enhances\nthe performance of SVT by increasing the precision and recall, respectively. The effectiveness of\nour proposed methods is substantiated both theoretically and empirically, demonstrating significant\nimprovements up to 50% across evaluated metrics.", "sections": [{"title": "Introduction", "content": "The sparse vector technique (SVT) Dwork et al. [2009, 2014] is one of the most fundamental algorithmic tools in DP,\nserving as a backbone for adaptive data analysis in many applications, such as feature selection Bassily et al. [2018],\nstream data analysis Hasidim et al. [2020], and top-c selection Lyu et al. [2017]. At a high level, SVT answers a\nsequence of queries on a given dataset and extracts useful information in a privacy-preserving manner. Unlike typical\nprivate query releases that directly reveal the noisy query results, SVT discloses less information\u2014it outputs only a\nbinary bit for each query, indicating whether the query result surpasses a predefined threshold (Cf. Figure 1). The major\nadvantage of SVT over the typical private query releases is that only the positive queries (those whose results are above\nthe predefined threshold) consume privacy, thus potentially allowing an infinite number of queries on a single dataset.\nDespite the widespread usage of SVT, it still suffers from a low query accuracy Zhu and Wang [2020] due to a\nconservative privacy analysis, resulting in a sub-optimal noise selection. Specifically, prior works in the literature"}, {"title": "Preliminaries", "content": "In this section, we first recall the definition (Cf. Definition 1), mechanisms (Cf. Equation 1), and properties of the\ndifferential privacy (i.e., post-processing (Cf. Proposition 1) and composition (Cf. Theorem 1)). Then, we proceed to\ndescribe the sparse vector technique algorithm (Cf. Algorithm 1) and some of its predominant variants."}, {"title": "Differential Privacy", "content": "The differential privacy (DP) was first introduced by Dwork et al. Dwork [2006] and recently became a de facto standard\nfor privacy protection. Roughly speaking, DP ensures that the likelihood of any specific output from a random algorithm\nvaries little with sensitive neighboring inputs. The formal definition is as follows:\nDefinition 1 (Differential Privacy Dwork [2006]). A randomized algorithm $\\mathcal{M}: \\mathcal{D} \\rightarrow \\mathbb{R}$ satisfies $(\\epsilon, \\delta)$-differential\nprivacy if for any two neighboring datasets $\\mathcal{D}$ and $\\mathcal{D}'$, and any output $o \\subseteq \\mathbb{O}$:\n$\\Pr[\\mathcal{M}(\\mathcal{D}) \\in o] \\le e^{\\epsilon} \\cdot \\Pr[\\mathcal{M}(\\mathcal{D}') \\in o] + \\delta$.\nWhen $\\delta=0$, we say that $\\mathcal{M}$ satisfies pure DP (denoted by $\\epsilon$-DP). Otherwise, it guarantees an approximate DP.\nOne of the typical methods of achieving DP is through the additive-noise mechanism, which corrupts and releases query\nresults with additive noise randomly drawn from certain distributions Geng et al. [2019]. That is, given a dataset $\\mathcal{D}$, to\nguarantee DP, a randomized algorithm $\\mathcal{M}$ releases:\n$\\mathcal{M}(\\mathcal{D}) = q(\\mathcal{D}) + X$, (1)\nwherein $q(\\mathcal{D})$ is the result of a query carried on $\\mathcal{D}$ and $X$ is the additive noise drawn from a probability distribution $\\mathcal{N}$.\nThe noise scale is calibrated to the sensitivity of $q(\\mathcal{D})$ defined in the following:\nDefinition 2 ($l_p$-sensitivity). For a real-valued query $q: \\mathcal{D} \\rightarrow \\mathbb{R}$, the $l_p$-sensitivity of $q$ is defined as:\n$\\Delta_p = \\max_{\\mathcal{D},\\mathcal{D}'\\in\\mathcal{D}} ||q(\\mathcal{D}) - q(\\mathcal{D}')||_p$,\nwhere $||\\cdot||_p$ denotes the $l_p$ norm and $\\mathcal{D}, \\mathcal{D}'$ is a pair of neighboring datasets that differ by one element.\nThe Laplace mechanism is one of the most classic additive-noise mechanisms that achieves $\\epsilon$-DP by letting $X$ follow a\nLaplace distribution centered at 0 with a noise scale $b = \\frac{\\Delta_1}{\\epsilon}$, denoted as $X \\sim Lap(\\frac{\\Delta_1}{\\epsilon})$, where $\\Delta_1$ is the sensitivity of\nthe query. Meanwhile, the Gaussian mechanism mechanism typically guarantees $(\\epsilon, \\delta)$-DP by drawing the noise $X$\nfrom a normal distribution $\\mathcal{N}(0, \\sigma^2)$, where $\\sigma$ is proportional to $\\frac{\\Delta_2}{\\epsilon}$.\nDP provides a rigorous privacy guarantee mathematically and is straightforward to achieve. Additionally, there are\nother two properties that contribute to the wide-ranging applications of DP. Firstly, it is immune to post-processing,\nwhich is formally described as follows:\nProposition 1 (Post-processingDwork et al. [2014]). Let $\\mathcal{M}: \\mathcal{D} \\rightarrow \\mathbb{R}$ be a randomized algorithm that is $(\\epsilon, \\delta)$\ndifferentially private. Let $f: \\mathbb{R} \\rightarrow \\mathbb{R}$ be an arbitrary randomized mapping. Then $f \\circ \\mathcal{M}$ is $(\\epsilon, \\delta)$-differentially private.\nSecondly, different differentially private building blocks can be elegantly combined for designing more sophisticated\nalgorithms, as described by the following theorem:\nTheorem 1 (Sequential CompositionDwork [2006]). Let $\\mathcal{M}_i: \\mathcal{N}^{\\vert\\mathbb{X}\\vert} \\rightarrow \\mathbb{R}_i$ be an $(\\epsilon_i, \\delta_i)$-differential privacy algorithm\nfor $i \\in [k]$. If $\\mathcal{M}_{[k]}: \\mathcal{N}^{\\vert\\mathbb{X}\\vert} \\rightarrow \\Pi_{i=1}^k \\mathbb{R}_i$ is defined to be $\\mathcal{M}_{[k]}(x) = (\\mathcal{M}_1(x), ..., \\mathcal{M}_k(x))$, then $\\mathcal{M}_{[k]}$ is $(\\Sigma_{i=1}^k \\epsilon_i, \\Sigma_{i=1}^k \\delta_i)$-\ndifferentially private."}, {"title": "Sparse Vector Technique", "content": "Algorithm 1: SVT works as follows. Given a sequence of queries $q_i(\\mathcal{D})$ on a dataset $\\mathcal{D}$ and their corresponding\npredefined thresholds $T_i$, SVT first perturbs them with random noise drawn from noise distribution $\\mathcal{N}_2$ and $\\mathcal{N}_1$ (Line 4,\nLine 5, Line 1, and Line 6), respectively. Then, it compares each noisy query result $\\tilde{q}_i(\\mathcal{D})$ with the noisy threshold\n$\\tilde{T}_i$ (Line 7). If $\\tilde{q}_i(\\mathcal{D})$ is no smaller than $\\tilde{q}_i(\\mathcal{D})$, T is output as a positive indicator (Line 9). Otherwise, $\\bot$ is output as a\nnegative indicator (Line 14). The algorithm halts either when the number of positive outcomes reaches its maximum\nvalue $c$, or when the total number of queries exceeds its maximum value $k_{max}$ (Line 12).\nDifferent variants in the literature set this indicator to different values. For instance, Dwork et al. Dwork et al. [2009]\nset RESAMPLE to True, whereas Lyu et al. Lyu et al. [2017] argue that the overall privacy cost is significantly reduced\nby setting it to False, therefore significantly boosting the performance of SVT. Furthermore, Zhu et al. Zhu and Wang\n[2020] alternate True and False periodically for certain applications.\nIn this work, we provide a privacy analysis of our proposed method under both True and False settings (Cf. Theorem 3).\nAs the empirical evaluation results for both settings demonstrate similar trends and RESAMPLE=True yields better\nperformance in general Lyu et al. [2017], we only present the results of RESAMPLE set to True in Section 5."}, {"title": "Private Comparison", "content": "As mentioned above, to provide a rigorous DP guarantee, certain types of noise are required to ensure that for any noisy\nthreshold $\\tilde{T}_i$,\n$\\frac{\\Pr[\\tilde{q}_i(\\mathcal{D}) \\geq \\tilde{T}_i]}{\\Pr[\\tilde{q}_i(\\mathcal{D}') \\geq \\tilde{T}_i]} < e^{\\epsilon}$ (2)\nholds with high probability."}, {"title": "Advantage of SVT Over Typical Private Releases", "content": "One of the most unique properties of SVT, as well as its variants, is that only the positive outcomes incur privacy costs.\nSpecifically, the overall privacy budget $\\epsilon$ is independent of the value of $k_{max}$ depends instead on the noise scale (i.e.,\nthe variance of the noise) of each noisy query and the number of positive outcomes $n_c$.\nExample. Consider a scenario where a data analyst wants to identify the top-c most popular movies (Cf. Figure 1).\nEach movie is rated by a group of individuals who have watched it. Directly releasing the score of each movie (i.e.,\n$q_i(\\mathcal{D})$ in Figure 1) may reveal sensitive information about whether an individual has watched a particular movie, thus\nnecessitating the use of DP to protect individual privacy. Typical private query releases perturb and release the score of\nall candidate movies, which incurs a privacy budget proportional to the number of all candidates $n$. In contrast, using\nSVT to approximate the top-c movies by only outputting the indices of first c films whose scores exceed a predefined\nthreshold $T$ results in an overall privacy cost proportional to $c$ rather than $n$. When $c \\ll n$, this approach significantly\nreduces the privacy cost.\nNote that Figure 1 uses MEAN(D) as an example of query $q_i(\\mathcal{D})$. In practice, $q_i(\\mathcal{D})$ can represent any queries with\nreal-valued answers, such as SUM(D), COUNT(D), MAX(D). Furthermore, while we use top-c selection to demonstrate\nthe effectiveness of our proposed method, SVT can be applied to many other scenarios where queries need to be\nevaluated against specific criteria (predefined threshold) in a privacy-preserving manner, such as feature selection Bassily\net al. [2018], steaming data analysis Hasidim et al. [2020]."}, {"title": "Utility Metric", "content": "The utility of SVT is measured by a specialized utility metric, namely $(\\alpha, \\beta)$-accuracy Dwork et al. [2014].\nDefinition 3 ($\\alpha, \\beta)$-accuracy). An algorithm which outputs a stream of answers $a_1, ..., a_k \\in \\{T, \\bot\\}^*$ in response to a\nstream of $k$ queries $q_1, ..., q_k$ is ($\\alpha, \\beta)$-accurate with respect to a threshold $T$ if except with probability at most $\\beta$, the\nalgorithm does not halt before $q_k$, and for all $a_i = T$:\n$q_i(\\mathcal{D}) \\geq T - \\alpha$ (3)\nand for all $a_i = \\bot$:\n$q_i(\\mathcal{D}) < T + \\alpha$.\nDefinition 3 specifies that given an error tolerance parameter $\\alpha$, SVT achieves a success probability of at least 1 - $\\beta$.\nParticularly, queries with results falling within the interval $[T - \\alpha, T + \\alpha]$ are allowed to be misclassified. For instance,"}, {"title": "Privacy Analysis Revisit", "content": "As discussed in Section 2.2.1, it has been observed that previous privacy proofs tend to overestimate the privacy risk in\nSVT by bounding the privacy over all possible subsets in the output space of query results. The underlying assumption\nis that disclosing a binary bit is as risky as directly revealing the query result itself. However, this may not hold true:\nintuitively, a binary bit leaks less information than the complete query result and, therefore, should pose a lower risk.\nMotivated by this, we revisit the privacy analysis of SVT, taking into account its less informative nature. We provide a\nnew analysis result in Theorem 2. Informally speaking, our results indicate that for query perturbation, SVT poses a less\nstringent constraint on eligible noise distributions compared to typical private query releases, thereby accommodating a\nbroader range of noise distributions.\nTheorem 2 (Privacy of SVT). Algorithm 1 satisfies differential privacy if for any real numbers $b_1, b_2$, there are two\npositive real numbers $k_1$ and $k_2$ such that the following inequalities hold:\n$\\vert \\ln(f_1(x)) - \\ln(f_1(x+b_1))\\vert \\le k_1\\vert b_1\\vert$, (4)\n$\\vert \\ln(1 - F_2(x)) - \\ln(1 - F_2(x + b_2))\\vert \\le k_2\\vert b_2\\vert$, (5)\nwhere $f_1(\\cdot)$ and $F_2(\\cdot)$ are the probability density function of $\\mathcal{N}_1$ and the cumulative function of the $\\mathcal{N}_2$, respectively.\nWe defer the proof of Theorem 2 to Appendix A and provide only the key takeaways here.\nTakeaways from Theorem 2. First, Theorem 2 broadens the range of noise distributions for query perturbation.\nSpecifically, as indicated by Equation 11, distributions such as the exponential and Gumbel distribution are now eligible\nfor query perturbation (i.e., $\\mathcal{N}_2$ in Algorithm 1), whereas these types of noise were previously considered unsuitable\nfor SVT. Second, although Theorem 2 relaxes the constrains on noise distributions for query result perturbation, the\neligible noise distributions for threshold perturbation (i.e., $\\mathcal{N}_1$ in Algorithm 1) remain unchanged compared to the\nprevious work. This is because, despite the query results not being directly released, the predefined threshold is public\ninformation. To prevent negative queries from consuming privacy, the true threshold value, which is compared with the\nnoisy query result, must also be obscured. Hence, the same noise distributions, such as Laplacian or Gaussian, are used\nin traditional private query releases.\nTo clarify, we summarize some of the most commonly used noise distributions for both threshold perturbation and\nquery result perturbation in Table 1."}, {"title": "Construction", "content": "Based on the two crucial takeaways discussed in Section 3, we propose an enhanced SVT algorithm with improved\nquery accuracy, referred to as SVT-Exp. This algorithm is summarized in Algorithm 2, with key changes in the\nalgorithm design highlighted by underlines. Additionally, the main steps of Algorithm 2 are illustrated in Figure 2 for\nbetter understanding."}, {"title": "Overview", "content": "In summary, our proposed enhanced SVT algorithm introduces three key improvements:\nThe algorithm uses exponential noise for query perturbation (Line 6). This choice is based on the fact that\nthe cumulative probability function of exponential noise tightly satisfies Equation 11 in Theorem 2. Meanwhile, it"}, {"title": "Query Perturbation with Exponential Noise", "content": "In this section, we justify the use of exponential noise in Algorithm 2 for the following two reasons:"}, {"title": "Optimal Threshold Correction", "content": "In this section, we elaborate on our optimal threshold correction by: (1) providing the motivations necessitating our\ndesign (Cf. Section 4.3.1); (2) detailing our correction methodology, which incorporates derivation of the success\nprobability of SVT and computation of the optimal correction term (Cf. Section 4.3.2); and (3) presenting a numerical\ncomputation framework that generalizes our correction method to broader applications (Cf. Section 4.3.3)."}, {"title": "Motivations.", "content": "Though the exponential noise demonstrates properties that can theoretically boost query accuracy, applying it to SVT is\nchallenging due to the introduced bias. Specifically, the expectation of the difference between a noisy query and its\ncorresponding noisy threshold is written as follows:\n$\\mathbb{E} (\\tilde{q}_i(\\mathcal{D}) - T_i) = q_i(\\mathcal{D}) - T_i + \\frac{2c\\Delta}{\\epsilon_2}$"}, {"title": "Methodology.", "content": "Motivated by this, we have developed a new correction method that focuses on improving the precision of the output\nbinary vector rather than correcting each individual query result. At a high level, inspired by the definition of $(\\alpha, \\beta)$-\naccuracy (Cf. Definition 3), we compute the success probability of the SVT algorithm by multiplying the probability of\neach query being corrected classified. We then derive an optimal threshold correction term by maximizing this success\nprobability. In essence, our optimal correction method increases the threshold value to enhance the precision of SVT,\nwhich is further elucidated in the analysis presented in Figure 5.\nSuccess probability of SVT. We begin by considering a simple case where $c=1$. Assume there are $k$ true negative\nqueries before the true positive query. Given a fixed error tolerance parameter $\\alpha$, the success probability of Algorithm 2\nwith a threshold correction term $r$ is defined as follows:\n$p(r) = \\prod_{i=1}^k \\Pr [\\tilde{q}_i(\\mathcal{D}) - \\alpha < \\tilde{T}_i + r] - \\prod_{i=1}^k \\Pr [\\tilde{q}_i(\\mathcal{D}) + \\alpha \\geq \\tilde{T}_i + r],$ (7)\nwhere $\\tilde{q}_{i/j}(\\mathcal{D}) = q_{i/j}(\\mathcal{D}) + Exp(\\frac{\\epsilon_2}{2c\\Delta})$, and $\\tilde{T}_{i/j} = T_{i/j} + Lap(b)$. Note that here we abuse the notion Exp $(\\cdot)$ and Lap $(\\cdot)$\nto denote the random variables drawn from exponential distribution and Laplace distribution, respectively.\nTakeaways from Equation 7. First, Equation 7 is directly related to the $(\\alpha, \\beta)$-accuracy. Specifically, for a fixed $\\alpha$,\nthe value of $\\beta$ with a threshold correction term $r$ is given by $1 - p(r)$. Second, by maximizing $p(r)$, we maximize the\nquery accuracy of SVT, which in turn improves empirical performance. Third, Equation 7 can be easily extended to\nscenarios where $c>1$ by:  dividing all queries into subroutines, each containing exactly one positive query; and  multiplying the success probability of each subroutine.\nHowever, Equation 7 cannot be directly adopted as it is data-dependent, which could lead to privacy leakage. To address\nthis issue, we adopt a worst-case assumption by setting $q_{i/j}(\\mathcal{D}) = T_{i/j}$. Thus, the success probability $p(r)$ can be\nexpressed as:\n$p(r) = (\\Gamma (r + \\alpha))^k \\cdot (1 - \\Gamma (r - \\alpha)),$ (8)\nwhere $\\Gamma (\\cdot)$ is the cumulative distribution function (CDF) of the random variable $Z = X - Y$. Here $X$ and $Y$ are\nrandom variables drawn from distribution Exp $(\\frac{\\epsilon_2}{2c\\Delta})$ and Lap $(b)$, respectively. The shape of the probability density\nfunction (PDF) of $Z$ is illustrated in Figure 5(a), and the explicit expression of $p (r)$ is detailed in Appendix D.\nOptimal correction term. After demonstrating that the maximum value of $p(r)$ exists (Cf. Appendix E), the optimal\nthreshold correction is obtained by:\n$r^{op} = arg \\max p(r).$ (9)\nTo offer a better understanding, we perform a detailed analysis of Equation 8 and Equation 9 (Cf. Figure 5), and the\nkey findings are summarized as follows:\nFirst, as shown in Figure 5(b), an increasing number of negative queries ($k$) leads to a larger threshold correction\nterm ($r^{op}$). As $k$ grows, SVT is more likely to halt before the last true positive query, which is referred to as 'early\nhalting'. Consequently, a larger correction term is necessary to mitigate early halting, thereby enhancing the precision\nof the SVT algorithm."}, {"title": "Numerical Computation Framework.", "content": "To improve the scalability of the optimal threshold correction method, we further propose a numerical computation\nframework, which is summarized in Algorithm 3 and Algorithm 4. Instead of deriving the analytical formula for $\\Gamma(\\cdot)$\nin Equation 8, we first discretize the noise distributions $\\mathcal{N}_1$ and $\\mathcal{N}_2$ used in Algorithm 2. We then convolve these\ndiscretized distributions using the Fast Fourier Transform (FFT) to obtain the numerical representation of $\\Gamma$. The\noptimal threshold correction term, $\\tilde{r}^{op}$, is determined by maximizing $p(r)$, which is calculated based on this numerical\n$\\Gamma(\\cdot)$. Hereinafter, we use the combination of Laplace and exponential distributions as an example to illustrate the\nconcrete steps of our framework. First, to convert the continuous noise distributions into discretized sequences, we\ndefine a boundary B within which most of the probability mass (e.g., $1-\\epsilon$ as indicated in Line 1 of Algorithm 3) is\nconcentrated. Next, in Algorithm 4, we discretize the event space within B into $u$ chunks, with each chunk having a"}, {"title": "Appending Strategy", "content": "As discussed earlier, our threshold correction method enhances the performance of SVT by increasing the value of\nthe threshold, which in turn improves the precision of the algorithm. However, a higher threshold can also result in\na reduced recall. This occurs because true positive queries with relatively small results are less likely to exceed the\nadjusted threshold.\nTo further enhance recall, we propose an appending strategy. As shown in Figure 2 Step  and Algorithm 2 from\nLine 18 to Line 19, each query identified as negative by the algorithm is appended to the end of the query queue for\nanother round of querying. In settings where there is an infinite number of queries (e.g., streaming data analysis, where\nnew data and queries continuously arrive), these queries may be randomly inserted back into the queue.\nThe rationale behind this strategy is twofold. First, as stated in Theorem 3, re-querying the outputs identified as\nnegative incurs no additional privacy cost. Second, conducting multiple rounds of querying increases the likelihood\nof correctly identifying true positive queries that were misclassified initially. Simultaneously, it also enlarges the\nprobability gap between positive queries correctly identified as positive and negative queries mistakenly classified as\npositive. Concretely, for each query $q_i(\\mathcal{D})$, the probability of it identified as positive in each round is\n$p_i = \\frac{1}{1 + \\Pr[q_i(\\mathcal{D}) + Exp(\\frac{\\epsilon_2}{2c\\Delta}) \\geq T_i + r + Lap(b)]}$"}, {"title": "Evaluation", "content": "Before presenting our main results in Section 5.5, we detail the used datasets in Section 5.1, the adopted utility metrics\nin Section 5.2, the compared baselines in Section 5.3, and the selected parameters for our experiments in Section 5.4."}, {"title": "Datasets and Implementations", "content": "This work uses six different datasets: three real-world and three synthetic. Detailed information is provided in Table 2.\nFor the real-world datasets and the T40I10D100K dataset, each item's score is based on its frequency across records.\nSpecifically, for each item $I_i$ in the dataset, the score $s_i = \\sum_{j=1}^n \\mathbb{1}(I_i = 1)$, where $I_i$ is an indicator of whether the record\nRj contains item $I_i$, and n is the total number of records. For the remaining synthetic datasets, scores are assigned\ndirectly. In the Binary dataset, each positive query is assigned a score of 1,000, while negative queries receive a score of\n0. In the Zipf dataset, each item's score is proportional to $\\frac{1}{i}$, with the score calculated as $s_i = \\frac{1}{i^{1.0}} \\times 10,000$. We use m\nto denote the total number of items. Additionally, Figure 6 plots the item scores for each dataset for further insights.\nAll experiments are conducted on a laptop (6-core Intel Core i7 CPU at 2.2 GHz with 16-GB RAM)."}, {"title": "Queries and Utility Metric", "content": "The effectiveness of our proposed method is general but is validated here within the top-c selection problem, a common\napplication for SVT Lyu et al. [2017], Zhu and Wang [2020]. In this scenario, we use SVT to approximately query\nitems with the top-c highest scores in each dataset.\nSpecifically, we first shuffle the items randomly, then query each item's score (i.e., $q_i(D) = s_i$) and compare it to the\nthreshold T. If $q_i(D) > T$, the corresponding query index i is output. The algorithm halts either when c indices\nis output or the number of queries reaches a maximum limit $k_{max}$. The choice of T is crucial for query accuracy, but\ndetermining an appropriate threshold is beyond this work's scope. Various methods for threshold determination are\ndiscussed in the literature Lee and Clifton [2014], Carvalho et al. [2020]."}, {"title": "Utility Metrics", "content": "We evaluate SVT's performance on the top-c selection problem using two primary metrics: F1-score Fawcett [2006]\nand normalized cumulative rank (NCR). The F1-score is computed as:\n$F1 = \\frac{2 \\cdot precision \\cdot recall}{precision + recall} = \\frac{2TP}{2TP + FP + FN}$,\nwhere TP is the number of true positive queries, FP is the number of false positive queries, and FN is the number\nof false negative queries. However, the F1-score is more suited for unordered settings, where missing a top result\nincurs the same penalty as missing a lower-ranked result Lyu et al. [2017]. To address this, we also use the Normalized\nCumulative Rank (NCR) Lyu et al. [2017]. In NCR, each query $q_i$ is assigned a rank score defined as follows: the top\nquery (i.e., top-1) receives a score of c, the next receives c \u2013 1, and so on Lyu et al. [2017]. Queries below the threshold\nreceive a score of 0. The total score for positive outcomes is then normalized to the range [0, 1] by dividing by $\\frac{c(c+1)}{2}$,\nthe maximum possible score. Since the results for the F1-score demonstrate similar trends as NCR, detailed F1-score\nresults are provided in Appendix H."}, {"title": "Baselines", "content": "Table 3 lists all the methods compared in this work. We evaluate our method, which uses exponential noise for query\nperturbation, against SVT-Lap Lyu et al. [2017], SVT-Gau Zhu and Wang [2020], and SVT-Gum. While SVT-Lap\nand SVT-Gau are two of the most frequently used SVT variants in the literature, SVT-Gum is a new variant proposed\nin this work. SVT-Gum meets the constraints in Theorem 2 but has a slightly larger variance compared to SVT-Exp.\nMore details about SVT-Gum are provided in Appendix F. Additionally, to provide more insights, we also compare\nour method with the 'upper bound' of query accuracy, which is obtained by directly ranking the noisy query results\nperturbed with random noise drawn from Exp $(\\frac{\\epsilon_2}{2c\\Delta})$. Regarding threshold correction, we compare SVT-Exp with\noptimal threshold correction to SVT-Exp with no threshold correction and to SVT-Exp with mean correction (i.e., the\nna\u00efve solution in Section 4.3.1)."}, {"title": "Parameter Selection", "content": "This work involves several privacy parameters: the failure rate $\\delta$, the overall privacy budget $\\epsilon$, the privacy budget for\nthreshold perturbation (i.e., $\\epsilon_1$), and the privacy budget for query perturbation (i.e., $\\epsilon_2$). For SVT-Gau, $\\delta$ is set to"}, {"title": "Evaluation Results", "content": "Hereinafter, we demonstrate the effectiveness of our proposed methods through experiments on six datasets, with an\nin-depth analysis of the evaluation results. Our main results, presented in Figure 7, show a significant advantage of\nAlgorithm 2 over other baselines. The correctness and effectiveness of our optimal threshold correction method are\nshown in Table 5 and Figure 7. Finally, the effectiveness of our appending strategy and the trade-off it yields between\nefficiency and query accuracy is illustrated in Figure 8."}, {"title": "Effectiveness of Algorithm 2.", "content": "First, as illustrated in Figure 7, our proposed method (i.e., SVT-Exp (optimal correction), shown with red-circle line)\nsignificantly outperforms others baselines by up to 50% on NCR across all tested privacy regions and datasets. Moreover,\ncompared to other baselines, the performance of our proposed method closely matches the empirical SVT-Exp upper\nbound, further demonstrating its effectiveness. Second, our method shows a particular larger advantage when the gaps\nbetween predefined threshold and query results are relatively large (Cf. Figure 7(b) and Figure 6). This is because\nour threshold correction better filters out true positive queries far above the threshold, while our appending strategy\neffectively distinguishes smaller true positives from true negatives far below the threshold. Third, all compared variants,\nincluding ours, demonstrate higher NCR on datasets where the gaps between the threshold and query results are\nlarge even under smaller privacy budget (e.g., $\\epsilon = 0.05$ on Kosarak), as these datasets are generally more robust to\nnoise. Fourth, the NCR of other baselines decreases from SVT-Lap, SVT-Gumbel, to SVT-Gau, which aligns with our\ntheoretical analysis in Figure 3 and Figure 4."}, {"title": "Effectiveness of the Optimal Threshold Correction", "content": "We compare our SVT-Exp (optimal correction) with SVT-Exp (mean correction) and SVT-Exp (no correction) in\nFigure 7, while Table 5 presents the values of optimal correction term $r^{op}$ with different values of $\\epsilon$. First, as shown in\nTable 5, the optimal term is usually larger than the mean of the injected noise, aligning with our analysis in Section 4.3.2.\nSecond, SVT-Exp without correction demonstrates relatively low NCR even compared to, e.g., SVT-Lap, highlighting\nthe need for a threshold correction method when using exponential noise in SVT. Third, SVT-Exp with our optimal\ncorrection terms drastically outperforms SVT-Exp with mean correction, which slightly outperforms the other baselines.\nThis demonstrates the effectiveness of both exponential noise and optimal threshold correction methods."}, {"title": "Effectiveness of the Appending Strategy.", "content": "To demonstrate the effectiveness of our appending strategy, we compare the performance of all our baselines at the same\noverall privacy budget consumption with varying numbers of traverse (i.e., the number of times a query with the noisy"}, {"title": "Related Work", "content": "Our work focuses on enhancing one of the most fundamental DP algorithms initially introduced by Dwork et al. Dwork\net al. [2009, 2014", "2010": "namely the sparse vector technique (SVT). Benefiting from its key\nfeature where only positive outcomes consume privacy, SVT and its variants Kaplan et al. [2021", "2014": "Chen et al. [2015"}, {"2014": "Shokri and Shmatikov [2015"}]}