{"title": "UNLEASH THE POWER OF ELLIPSIS: ACCURACY-ENHANCED SPARSE VECTOR TECHNIQUE WITH EXPONENTIAL NOISE", "authors": ["Yuhan Liu", "Sheng Wang", "Yixuan Liu", "Feifei Li", "Hong Chen"], "abstract": "The Sparse Vector Technique (SVT) is one of the most fundamental tools in differential privacy (DP). It works as a backbone for adaptive data analysis by answering a sequence of queries on a given dataset, and gleaning useful information in a privacy-preserving manner. Unlike the typical private query releases that directly publicize the noisy query results, SVT is less informative-it keeps the noisy query results to itself and only reveals a binary bit for each query, indicating whether the query result surpasses a predefined threshold. To provide a rigorous DP guarantee for SVT, prior works in the literature adopt a conservative privacy analysis by assuming the direct disclosure of noisy query results as in typical private query releases. This approach, however, hinders SVT from achieving higher query accuracy due to an overestimation of the privacy risks, which further leads to an excessive noise injection using the Laplacian or Gaussian noise for perturbation. Motivated by this, we provide a new privacy analysis for SVT by considering its less informative nature. Our analysis results not only broaden the range of applicable noise types for perturbation in SVT, but also identify the exponential noise as optimal among all evaluated noises (which, however, is usually deemed non-applicable in prior works). The main challenge in applying exponential noise to SVT is mitigating the sub-optimal performance due to the bias introduced by noise distributions. To address this, we develop a utility-oriented optimal threshold correction method and an appending strategy, which enhances the performance of SVT by increasing the precision and recall, respectively. The effectiveness of our proposed methods is substantiated both theoretically and empirically, demonstrating significant improvements up to 50% across evaluated metrics.", "sections": [{"title": "1 Introduction", "content": "The sparse vector technique (SVT) Dwork et al. [2009, 2014] is one of the most fundamental algorithmic tools in DP, serving as a backbone for adaptive data analysis in many applications, such as feature selection Bassily et al. [2018], stream data analysis Hasidim et al. [2020], and top-c selection Lyu et al. [2017]. At a high level, SVT answers a sequence of queries on a given dataset and extracts useful information in a privacy-preserving manner. Unlike typical private query releases that directly reveal the noisy query results, SVT discloses less information\u2014it outputs only a binary bit for each query, indicating whether the query result surpasses a predefined threshold (Cf. Figure 1). The major advantage of SVT over the typical private query releases is that only the positive queries (those whose results are above the predefined threshold) consume privacy, thus potentially allowing an infinite number of queries on a single dataset.\nDespite the widespread usage of SVT, it still suffers from a low query accuracy Zhu and Wang [2020] due to a conservative privacy analysis, resulting in a sub-optimal noise selection. Specifically, prior works in the literature"}, {"title": "2 Preliminaries", "content": "In this section, we first recall the definition (Cf. Definition 1), mechanisms (Cf. Equation 1), and properties of the differential privacy (i.e., post-processing (Cf. Proposition 1) and composition (Cf. Theorem 1)). Then, we proceed to describe the sparse vector technique algorithm (Cf. Algorithm 1) and some of its predominant variants."}, {"title": "2.1 Differential Privacy", "content": "The differential privacy (DP) was first introduced by Dwork et al. Dwork [2006] and recently became a de facto standard for privacy protection. Roughly speaking, DP ensures that the likelihood of any specific output from a random algorithm varies little with sensitive neighboring inputs. The formal definition is as follows:\nDefinition 1 (Differential Privacy Dwork [2006]). A randomized algorithm $M: \\mathcal{D} \\rightarrow \\mathbb{R}$ satisfies $(\\epsilon, \\delta)$-differential privacy if for any two neighboring datasets $\\mathcal{D}$ and $\\mathcal{D}^{\\prime}$, and any output $o \\subseteq \\mathbb{O}$:\n$\\Pr[M(\\mathcal{D}) \\in o] \\leq e^{\\epsilon} \\cdot \\Pr[M(\\mathcal{D}^{\\prime}) \\in o] + \\delta$.\nWhen $\\delta = 0$, we say that $M$ satisfies pure DP (denoted by $\\epsilon$-DP). Otherwise, it guarantees an approximate DP.\nOne of the typical methods of achieving DP is through the additive-noise mechanism, which corrupts and releases query results with additive noise randomly drawn from certain distributions Geng et al. [2019]. That is, given a dataset $\\mathcal{D}$, to guarantee DP, a randomized algorithm M releases:\n$M(D) = q(D) + X, \\qquad \\qquad (1)$\nwherein $q(D)$ is the result of a query carried on $D$ and $X$ is the additive noise drawn from a probability distribution N. The noise scale is calibrated to the sensitivity of $q(D)$ defined in the following:\nDefinition 2 ($\\ell_p$-sensitivity). For a real-valued query $q : \\mathcal{D} \\rightarrow \\mathbb{R}$, the $\\ell_p$-sensitivity of $q$ is defined as:\n$\\Delta_p = \\max_{\\mathcal{D},\\mathcal{D}^{\\prime}\\in \\mathcal{D}} ||q(\\mathcal{D}) - q(\\mathcal{D}^{\\prime})||_p,$\nwhere $||\\cdot||_p$ denotes the $\\ell_p$ norm and $\\mathcal{D}, \\mathcal{D}^{\\prime}$ is a pair of neighboring datasets that differ by one element.\nThe Laplace mechanism is one of the most classic additive-noise mechanisms that achieves $\\epsilon$-DP by letting $X$ follow a Laplace distribution centered at 0 with a noise scale $b = \\frac{\\Delta_1}{\\epsilon}$, denoted as $X \\sim Lap(\\frac{\\Delta_1}{\\epsilon})$, where $\\Delta_1$ is the sensitivity of the query. Meanwhile, the Gaussian mechanism mechanism typically guarantees $(\\epsilon, \\delta)$-DP by drawing the noise $X$ from a normal distribution $N(0, \\sigma^2)$, where $\\sigma$ is proportional to $\\frac{\\$\\Delta_2}{\\epsilon}$.\nDP provides a rigorous privacy guarantee mathematically and is straightforward to achieve. Additionally, there are other two properties that contribute to the wide-ranging applications of DP. Firstly, it is immune to post-processing, which is formally described as follows:\nProposition 1 (Post-processingDwork et al. [2014]). Let $M : \\mathcal{D} \\rightarrow \\mathbb{R}$ be a randomized algorithm that is $(\\epsilon, \\delta)$-differentially private. Let $f : \\mathbb{R} \\rightarrow \\mathbb{R}$ be an arbitrary randomized mapping. Then $f \\circ M$ is $(\\epsilon, \\delta)$-differentially private.\nSecondly, different differentially private building blocks can be elegantly combined for designing more sophisticated algorithms, as described by the following theorem:\nTheorem 1 (Sequential CompositionDwork [2006]). Let $M_i : \\mathcal{N}^{|\\times|} \\rightarrow \\mathbb{R}_i$ be an $(\\epsilon_i, \\delta_i)$-differential privacy algorithm for $i \\in [k]$. If $M_{[k]} : \\mathcal{N}^{|\\times|} \\rightarrow \\prod_{i=1}^{k} \\mathbb{R}_i$ is defined to be $M_{[k]}(x) = (M_1(x), ..., M_k(x))$, then $M_{[k]}$ is $(\\sum_{i=1}^{k} \\epsilon_i, \\sum_{i=1}^{k} \\delta_i)$-differentially private."}, {"title": "2.2 Sparse Vector Technique", "content": "Algorithm 1: SVT. Privately indicates if query results are above thresholds.\nIn principle, SVT works as follows. Given a sequence of queries $q_i(D)$ on a dataset $D$ and their corresponding predefined thresholds $T_i$, SVT first perturbs them with random noise drawn from noise distribution $N_2$ and $N_1$ (Line 4, Line 5, Line 1, and Line 6), respectively. Then, it compares each noisy query result $\\hat{q}_i(D)$ with the noisy threshold $\\hat{T}_i$ (Line 7). If $\\hat{q}_i(D)$ is no smaller than $\\hat{q}_i(D)$, $\\top$ is output as a positive indicator (Line 9). Otherwise, $\\perp$ is output as a negative indicator (Line 14). The algorithm halts either when the number of positive outcomes reaches its maximum value $c$, or when the total number of queries exceeds its maximum value $k_{max}$ (Line 12).\nDifferent variants in the literature set this indicator to different values. For instance, Dwork et al. Dwork et al. [2009] set RESAMPLE to True, whereas Lyu et al. Lyu et al. [2017] argue that the overall privacy cost is significantly reduced by setting it to False, therefore significantly boosting the performance of SVT. Furthermore, Zhu et al. Zhu and Wang [2020] alternate True and False periodically for certain applications.\nIn this work, we provide a privacy analysis of our proposed method under both True and False settings (Cf. Theorem 3). As the empirical evaluation results for both settings demonstrate similar trends and RESAMPLE=True yields better performance in general Lyu et al. [2017], we only present the results of RESAMPLE set to True in Section 5."}, {"title": "2.2.1 Private Comparison", "content": "As mentioned above, to provide a rigorous DP guarantee, certain types of noise are required to ensure that for any noisy threshold $T_i$,\n$\\frac{\\Pr[\\hat{q}_i(D) \\geq \\hat{T}_i]}{\\Pr[\\hat{q}_i(D^{\\prime}) \\geq \\hat{T}_i]} < e^{\\epsilon}\\qquad (2)$\nholds with high probability."}, {"title": "2.2.2 Advantage of SVT Over Typical Private Releases", "content": "One of the most unique properties of SVT, as well as its variants, is that only the positive outcomes incur privacy costs. Specifically, the overall privacy budget $\\epsilon$ is independent of the value of $k_{max}$ depends instead on the noise scale (i.e., the variance of the noise) of each noisy query and the number of positive outcomes $n_c$.\nExample. Consider a scenario where a data analyst wants to identify the top-c most popular movies (Cf. Figure 1). Each movie is rated by a group of individuals who have watched it. Directly releasing the score of each movie (i.e., $q_i(D)$ in Figure 1) may reveal sensitive information about whether an individual has watched a particular movie, thus necessitating the use of DP to protect individual privacy. Typical private query releases perturb and release the score of all candidate movies, which incurs a privacy budget proportional to the number of all candidates $n$. In contrast, using SVT to approximate the top-c movies by only outputting the indices of first c films whose scores exceed a predefined threshold $T$ results in an overall privacy cost proportional to $c$ rather than $n$. When $c \\ll n$, this approach significantly reduces the privacy cost.\nNote that Figure 1 uses MEAN(D) as an example of query $q_i(D)$. In practice, $q_i(D)$ can represent any queries with real-valued answers, such as SUM(D), COUNT(D), MAX(D). Furthermore, while we use top-c selection to demonstrate the effectiveness of our proposed method, SVT can be applied to many other scenarios where queries need to be evaluated against specific criteria (predefined threshold) in a privacy-preserving manner, such as feature selection Bassily et al. [2018], steaming data analysis Hasidim et al. [2020]."}, {"title": "2.2.3 Utility Metric", "content": "The utility of SVT is measured by a specialized utility metric, namely $(\\alpha, \\beta)$-accuracy Dwork et al. [2014].\nDefinition 3 (($\\alpha, \\beta$)-accuracy). An algorithm which outputs a stream of answers $a_1, ..., a_k \\in {\\top, \\perp}^k$ in response to a stream of $k$ queries $q_1, ..., q_k$ is ($\\alpha, \\beta$)-accurate with respect to a threshold $T$ if except with probability at most $\\beta$, the algorithm does not halt before $q_k$, and for all $a_i = \\top$:\n$q_i(D) \\geq T - \\alpha$,\nand for all $a_i = \\perp$:\n$q_i(D) < T + \\alpha$.\nDefinition 3 specifies that given an error tolerance parameter $\\alpha$, SVT achieves a success probability of at least $1 - \\beta$. Particularly, queries with results falling within the interval $[T - \\alpha, T + \\alpha]$ are allowed to be misclassified. For instance,"}, {"title": "3 Privacy Analysis Revisit", "content": "As discussed in Section 2.2.1, it has been observed that previous privacy proofs tend to overestimate the privacy risk in SVT by bounding the privacy over all possible subsets in the output space of query results. The underlying assumption is that disclosing a binary bit is as risky as directly revealing the query result itself. However, this may not hold true: intuitively, a binary bit leaks less information than the complete query result and, therefore, should pose a lower risk.\nMotivated by this, we revisit the privacy analysis of SVT, taking into account its less informative nature. We provide a new analysis result in Theorem 2. Informally speaking, our results indicate that for query perturbation, SVT poses a less stringent constraint on eligible noise distributions compared to typical private query releases, thereby accommodating a broader range of noise distributions.\nTheorem 2 (Privacy of SVT). Algorithm 1 satisfies differential privacy if for any real numbers $b_1, b_2$, there are two positive real numbers $k_1$ and $k_2$ such that the following inequalities hold:\n$|\\ln(f_1(x)) - \\ln(f_1(x+b_1))| \\leq k_1|b_1|, \\qquad \\qquad (4)$\n$|\\ln(1 - F_2(x)) - \\ln(1 - F_2(x + b_2))| \\leq k_2|b_2|, \\qquad \\qquad (5)$\nwhere $f_1(\\cdot)$ and $F_2(\\cdot)$ are the probability density function of $N_1$ and the cumulative function of the $N_2$, respectively.\nWe defer the proof of Theorem 2 to Appendix A and provide only the key takeaways here.\nTakeaways from Theorem 2. First, Theorem 2 broadens the range of noise distributions for query perturbation. Specifically, as indicated by Equation 11, distributions such as the exponential and Gumbel distribution are now eligible for query perturbation (i.e., $N_2$ in Algorithm 1), whereas these types of noise were previously considered unsuitable for SVT. Second, although Theorem 2 relaxes the constrains on noise distributions for query result perturbation, the eligible noise distributions for threshold perturbation (i.e., $N_1$ in Algorithm 1) remain unchanged compared to the previous work. This is because, despite the query results not being directly released, the predefined threshold is public information. To prevent negative queries from consuming privacy, the true threshold value, which is compared with the noisy query result, must also be obscured. Hence, the same noise distributions, such as Laplacian or Gaussian, are used in traditional private query releases.\nTo clarify, we summarize some of the most commonly used noise distributions for both threshold perturbation and query result perturbation in Table 1."}, {"title": "4 Construction", "content": "Based on the two crucial takeaways discussed in Section 3, we propose an enhanced SVT algorithm with improved query accuracy, referred to as SVT-Exp. This algorithm is summarized in Algorithm 2, with key changes in the algorithm design highlighted by underlines. Additionally, the main steps of Algorithm 2 are illustrated in Figure 2 for better understanding."}, {"title": "4.1 Overview", "content": "In summary, our proposed enhanced SVT algorithm introduces three key improvements:\n\u2460: The algorithm uses exponential noise for query perturbation (Line 6). This choice is based on the fact that the cumulative probability function of exponential noise tightly satisfies Equation 11 in Theorem 2. Meanwhile, it"}, {"title": "4.2 Query Perturbation with Exponential Noise", "content": "In this section, we justify the use of exponential noise in Algorithm 2 for the following two reasons:"}, {"title": "4.3 Optimal Threshold Correction", "content": "In this section, we elaborate on our optimal threshold correction by: (1) providing the motivations necessitating our design (Cf. Section 4.3.1); (2) detailing our correction methodology, which incorporates derivation of the success probability of SVT and computation of the optimal correction term (Cf. Section 4.3.2); and (3) presenting a numerical computation framework that generalizes our correction method to broader applications (Cf. Section 4.3.3)."}, {"title": "4.3.1 Motivations.", "content": "Though the exponential noise demonstrates properties that can theoretically boost query accuracy, applying it to SVT is challenging due to the introduced bias. Specifically, the expectation of the difference between a noisy query and its corresponding noisy threshold is written as follows:\n$\\mathbb{E} (\\hat{q}_i(D) - \\hat{T}_i) = q_i(D) - T_i + \\frac{2c\\Delta}{\\epsilon_2}$"}, {"title": "5 Conclusion", "content": "This work aims to enhance the query accuracy of the SVT algorithm by utilizing exponential noise. We begin with revisiting the privacy analysis of SVT algorithms and expanding the range of noise options for query perturbation by considering its less informative nature. Our analysis identifies exponential noise as the most effective, both theoretically and empirically, among the considered noise distributions. Additionally, we develop a generic optimal threshold correction method and an appending strategy to ensure both a high query precision and recall for SVT with marginal computation cost. The effectiveness of these methods is thoroughly validated through comprehensive experiments on both real-world and synthetic datasets."}, {"title": "A Proof of Theorem 2", "content": "Before starting our proof, we first define two different query settings for the SVT algorithm: the monotonic query setting and the non-monotonic query setting (or monotonic setting and non-monotonic setting in short).\nFor the monotonic setting, when the dataset D changes into its neighboring dataset D', all the query results change in the same direction. That is to say, we have either $\\forall i\\in[n] q_i(D) \\geq q_i(D^{\\prime})$ or $\\forall i\\in[n] q_i(D) \\leq q_i(D^{\\prime})$, where n is number of different queries in total. By contrast, for the non-monotonic setting, the above statement is not necessary to be true: the query results can change in different directions when D changes into D'.\nIn the following, we first show our privacy analysis for the non-monotonic setting. We then elaborate on how to extend it to monotonic setting.\nTo begin with, we restate Theorem 2 as follows:\nTheorem 5 (Restate of Theorem 2). Algorithm 1 satisfies differential privacy if for any real numbers $b_1, b_2$, there are two positive real numbers $k_1$ and $k_2$ such that the following inequalities hold:\n$| \\ln(f_1(x)) - \\ln(f_1(x + b_1))| \\leq k_1|b_1|, \\qquad \\qquad (10)$\n$| \\ln(1 - F_2(x)) - \\ln(1 - F_2(x + b_2))| \\leq k_2|b_2|, \\qquad \\qquad (11)$\nwhere $f_1(\\cdot)$ and $F_2(\\cdot)$ are the probability density function of $N_1$ and the cumulative function of the $N_2$, respectively.\nNow, we provide our proof under the non-monotonic settings."}, {"title": "B Proof of Theorem 3", "content": "Same as in Appendix A, we provide our proof of Theorem 3 first under the non-monotonic setting, then demonstrate how we can adapt Algorithm 2 as well as the privacy proof to the monotonic setting.\nTheorem 6 (Restate of Theorem 3). Let c denote the number of positive outcomes output by Algorithm 2. Algorithm 2 satisfies $(\\epsilon_1+\\epsilon_2)$-differential privacy when RESAMPLE is set to False, and $(c\\epsilon_1, \\epsilon_2)$-differential privacy when RESAMPLE is set to True, where $\\epsilon_1$ and $\\epsilon_2$ are the privacy budgets for threshold and query perturbation, respectively."}, {"title": "C Proof of Theorem 4", "content": "In this section, we first restate Theorem 4 in the following, then prove it in three steps. First, we formalize the $(\\alpha, \\beta)$-accuracy in terms of Algorithm 2 in a simple case where only one positive query is allowed to output (i.e., c = 1), $\\Delta$ = 1, and $\\epsilon_1 = \\epsilon_2 = \\frac{\\epsilon}{2}$ as in Dwork et al. [2014]. Then, we provide the utility guarantee for the case where the threshold correction term r is set to 0. After that, we show that our optimal threshold correction term derived Equation 9 yields a accuracy no smaller than r = 0. Also note that our proof is under the non-monotonic query setting, and is trivial to extend it to the monotonic query setting.\nTheorem 7 (Restate of Theorem 4). Given any k records such that $|{i < k : d_i \\geq t - \\alpha}| = 0$ (i.e., the record above and closest to the threshold is the last one), the Algorithm 2 at least $(\\frac{4(\\ln k + \\ln \\frac{1}{\\beta})}{\\epsilon}, \\beta)$-accurate.\nBased on the definition of ($\\alpha, \\beta$)-accuracy (Cf. Definition 3), we need to compute $\\beta$ given any $\\alpha$ based on the following equation:\n$\\beta = 1 - \\prod_{i=1}^{k} \\Pr [\\hat{q}_i(D) - \\alpha < \\hat{T}_i +r] \\cdot \\Pr [q_i(D) + \\alpha \\geq \\hat{T}_i + r]\\qquad (30)$\n$= 1 - \\rho(r),$\nwhere r is the threshold correction parameter and $\\rho(r)$ is defined in Equation 7.\nWe first show that when r = 0, it holds that\n$\\beta \\leq 2 \\exp\\left(\\frac{\\ln k - \\alpha \\epsilon}{4}\\right)\\qquad (31)$"}]}