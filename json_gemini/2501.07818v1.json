{"title": "A Multi-Encoder Frozen-Decoder Approach for Fine-Tuning Large Language Models", "authors": ["Kaustubh D. Dhole"], "abstract": "Among parameter-efficient fine-tuning methods, freezing has emerged as a popular strategy for speeding up training, reducing catastrophic forgetting, and improving downstream performance. We investigate the impact of freezing the decoder in a multi-task setup comprising diverse natural language tasks, aiming to reduce deployment overhead and enhance portability to novel tasks. Our experiments, conducted by fine-tuning both individual and multi-task setups on the AlexaTM model, reveal that freezing decoders is highly effective for tasks with natural language outputs and mitigates catastrophic forgetting in multilingual tasks. However, we find that pairing frozen decoders with a larger model can effectively maintain or even enhance performance in structured and QA tasks, making it a viable strategy for a broader range of task types.", "sections": [{"title": "Introduction", "content": "Language models utilized in multi-task setups have long been shown to have improved performance across a variety of natural language tasks (Collobert and Weston, 2008). While training multiple tasks together lets individual tasks benefit from each others' data, a single model trained on multiple tasks offers lesser control of individual capabilities. This parameter-sharing paradigm is unarguably less computationally efficient as it necessitates retraining the complete model on the introduction of a new task.\nOn the other hand, fine-tuning individual task-specific models offers increased control and modularity at the cost of lesser training data. Our work attempts to further exploit this latter paradigm in a virtual assistant experimentation platform to check for performance benefits via partial fine-tuning. We explore partial fine-tuning in the context of task-specific encoder-decoder models.\nAmong partial and parameter-efficient finetuning methods (Han et al., 2024), freezing has been a popular strategy adopted to train faster, reduce catastrophic forgetting, and increase downstream performance. We seek to investigate the benefits and drawbacks of specifically freezing the parameters of the decoder while fine-tuning individual task-specific encoders (Figure 1).\nThis delineation of training of the encoder and the decoder permits us to look at both of them separately. Freezing the parameters of the decoder has shown to be effective for improving downstream performance on machine translation (Cooper Stickland et al., 2021). If a sub-module of the network is well pre-trained and exposed to data from languages or domains similar to the fine-tuning task, freezing it might not necessarily restrict its learning capability and may even enhance it. Eg. Cooper Stickland et al. (2021) observe that fine-tuning mBART (Liu et al., 2020) with a frozen decoder performs better than mBART with all trainable layers, especially if the translating output language is English. When the translating source language is English, freezing the encoder helps gain performance.\nIn general, freezing reduces the overall number of parameters to train improving training efficiency. Particularly in the case of mBART, freezing the decoder can help almost double the number of tokens"}, {"title": "Related Work", "content": "Parameter freezing has always been popular albeit with different objectives eg. Dropout (Hinton et al., 2012) & Stochastic Depth (dropping of ResNet blocks while training) (Huang et al., 2016). In encoder-decoder models, freezing has been explored a bit for machine translation. Thompson et al. (2018); Zoph et al. (2016) explore different ways to freeze parameters of RNNs. Cooper Stickland et al. (2021) find that freezing encoder or decoder parameters improve performance on distantly related, language directions. Brock et al. (2017) presents a technique, FreezeOut by progressively freezing hidden layers of popular computer vision models. Lee et al. (2019) perform experiments on freezing the final layers of BERT (Kenton and Toutanova, 2019) & ROBERTa (Liu et al., 2019) and observe that 90% of their performance can be retained by only fine-tuning 25% of the final layers and that freezing a few layers increases performance in a few tasks like SST-2 (Socher et al., 2013). He et al. (2021) show that parameter efficient adapter based fine-tuning of BERT & ROBERTa outperforms traditional fine-tuning on low-resource and cross-lingual tasks and mitigates catastrophic forgetting."}, {"title": "Experiments", "content": "We perform our experiments on two specific encoder-decoder models, a.k.a. Alexa Teacher Model (AlexaTM) released by Soltan et al. (2022); FitzGerald et al. (2022a).\nModels\nAlexaTM: In all our experiments, we use the large version of AlexaTM which is pre-trained similar to mBART Large (Liu et al., 2020) with 12-layer encoders and 12-layer decoders. This 511 million parameter model has been exposed to spoken and written formats of Arabic, English, French, German, Hindi, Italian, Japanese, Marathi, Portuguese, Spanish, Tamil and Telugu.\nAlexaTM 2B: Since we would like to address possible performance drops, we perform the same experiments with the larger version (2 billion parameters) of the same model. We keep the decoder here frozen too.\nTasks\nWe perform our experiments on multiple NLP datasets spanning tasks of diverse types.\nXSUM (Narayan et al., 2018) A real-world, large-scale summarization dataset collected from BBC to generate one-sentence news summaries.\nMTOP (Li et al., 2021) A task-oriented semantic parsing dataset covering 6 languages and 11 domains.\nSQUAD v2 (Rajpurkar et al., 2018) A large scale reading comprehension dataset with unanswerable queries.\nMASSIVE (Bastianelli et al., 2020; FitzGerald et al., 2022b) About 1M labeled utterances spanning 51 languages for intent classification and slot tagging.\nXNLI (Conneau et al., 2018) Cross-lingual natural language inference in 15 languages\nCommonGen (Lin et al., 2020) Generating a sentence using a given set of concept words that describe everyday life scenarios.\nWebNLG (Zhou and Lampouras, 2020) RDF-to-Text Generation.\nTraining & Evaluation\nEach dataset is sourced from HuggingFace (Wolf et al., 2020) or the corresponding publicly available repository and is used to fine-tune 3 instances"}, {"title": "Results & Analysis", "content": "Table 2 displays the results on NLG tasks. 3 scores are computed \u2013 the ROGUE scores (Lin, 2004), the BLEU (Papineni et al., 2002) metric and the NIST metric (Doddington, 2002). The ROGUE metrics measure the recall of words and n-grams off the reference summaries and BLEU measures the precision. The NIST scores are just like BLEU scores but they give more emphasis to rare words and phrases.\nFor XSUM, freezing the decoder of AlexaTM has just close to a 2% decrease in the ROUGE scores, and performance is improved with the larger frozen decoder. When fine-tuned on WebNLG, freezing results in a 2% drop in performance, and the performance is retained with the larger frozen decoder. So, this is interesting since freezing improves the performance in the single task setting. It might not be surprising considering that a task like WebNLG has properties similar to the pre-training data format. Besides, mixing the tasks helps WebNLG drastically but not XSUM. Interestingly, XSUM's data is beneficial for XSUM as well as WEBNLG. But it is also likely that XSUM's double share of training data might be a reason.\nFor WebNLG, the mixed model setup was better than the single model setup, except for the large frozen decoder setting which showed the best performance. The 2B model seemingly benefited more from the large pre-training data it was exposed to, as compared to cross-task data during mix training. The cross-task data is indeed useful but if the decoder is well-pre-trained the task doesn't need cross-task data. Freezing greatly benefits tasks that have properties similar to the pre-training data format. The performance of WebNLG, remains almost the same on usage of a frozen decoder unlike MTOP since WebNLG has natural language target sequences.\nIt is in the case of MTOP where we notice the biggest drop in performance on using a frozen decoder of the same size. One could perform extensive hyperparameter tuning eg. evaluate with more beams, early stopping, etc. but with num_beams=3, we see almost a 14% drop on using a frozen decoder. This doesn't look so surprising if we look at the format of the output sequences which are unlike spoken or written text i.e. they radically differ from the format of the pre-training data. Intriguingly, with a larger version of the decoder, freezing makes up for quite a bit of the performance loss as well as a 2% increase. And in the mixed setting, MTOP performs poorly possibly due to the domination of other text output datasets.\nIn XNLI, mixing the tasks benefits the model in both the frozen as well as the trainable setting. Freezing the parameters improves performance. However, we also noticed that by increasing the number of beams from 3 to 5, 10 and 20, the gap between these setups tends to decrease.\nQuestion Answering over SQUAD is best with the frozen version of the larger decoder. Mixing the tasks hurts performance for both trainable as well as frozen settings.\nFor MASSIVE, we first train only on the English intents dataset of MASSIVE and then evaluate all languages. While the MASSIVE dataset has intents for 59 languages, we report performance only for those languages on which AlexaTM was pre-trained. These are the most interesting results in this set of experiments. We see extremely substantial increases in performances on freezing the decoder for every language except for English the language on which the model was fine-tuned where there is an increase of 4%. On freezing, there is an 8% increase for Spanish, 9% for Arabic, and 10% for Italian. This strongly suggests how catastrophic forgetting can be really damaging to a model when trained on a specific task or language's data (English in this case) and how powerful freezing can be as a remedy. So, decoder freezing seems to be a powerful remedy to not only improve absolute performance but also avoid catastrophic forgetting. More importantly, with a larger frozen decoder, we get a substantial boost in performance across all languages.\nWe also perform a separate set of experiments to let intents and slots be jointly trained. Instead of training two separate parameters, we make the model predict the single flat semantic parse which contains the intent as well as the slots. We notice that joint training helps performance for intent classification. Even in this case, frozen decoders help address catastrophic forgetting and improve performance across all languages."}, {"title": "Conclusion", "content": "Our results show that freezing decoders can lead to significant efficiency gains and mitigate catastrophic forgetting, especially in multilingual and classification tasks. Freezing is particularly effective for tasks with natural language target sequences, such as WebNLG and CommonGen, where performance remains comparable or improves slightly. For structured tasks like MTOP, freezing leads to performance degradation unless paired with a larger decoder.\nFreezing decoders in multilingual tasks, such as MASSIVE, yields substantial performance improvements by preventing catastrophic forgetting."}, {"title": "Future Work", "content": "A vital evaluation from the perspective of multitask design would be to also perform a parallel evaluation with freezing encoders and other parameters of the whole network partially. Future work would explore the same. Performances on mix tasks can be highly variable being dictated by the dataset proportion and the choice of other datasets. It would be interesting to explore the downstream effects of different taxonomies (Fifty et al., 2021) of a larger number of tasks (Srivastava et al., 2023), as well as resort to data augmentation (Dhole et al., 2023; Mille et al., 2021) for up-sizing smaller datasets."}, {"title": "Ethical Considerations", "content": "Large language models (LLMs) should always be examined from a sociotechnical perspective (Dhole, 2023). Appropriate guardrails must be employed, as deployment contexts may differ from the taskspecific evaluations that we conducted. Thorough testing across diverse scenarios is crucial to ensure robustness, mitigate biases, and prevent potential harm or unintended consequences."}]}