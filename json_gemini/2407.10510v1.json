{"title": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction", "authors": ["Xingzhi Zhou", "Xin Dong", "Chunhao Li", "Yuning Bai", "Yulong Xu", "Ka Chun Cheung", "Simon Seell", "Xinpeng Song", "Runshun Zhang", "Xuezhong Zhou", "Nevin L. Zhang"], "abstract": "Traditional Chinese medicine (TCM) relies on specific combinations of herbs in prescriptions to treat symptoms and signs, a practice that spans thousands of years. Predicting TCM prescriptions presents a fascinating technical challenge with practical implications. However, this task faces limitations due to the scarcity of high-quality clinical datasets and the intricate relationship between symptoms and herbs. To address these issues, we introduce DigestDS, a new dataset containing practical medical records from experienced experts in digestive system diseases. We also propose a method, TCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs) through supervised fine-tuning on DigestDS. Additionally, we enhance computational efficiency using a low-rank adaptation technique. TCM-FTP also incorporates data augmentation by permuting herbs within prescriptions, capitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves an F1-score of 0.8031, surpassing previous methods significantly. Furthermore, it demonstrates remarkable accuracy in dosage prediction, achieving a normalized mean square error of 0.0604. In contrast, LLMs without fine-tuning perform poorly. Although LLMs have shown capabilities on a wide range of tasks, this work illustrates the importance of fine-tuning for TCM prescription prediction, and we have proposed an effective way to do that.", "sections": [{"title": "I. INTRODUCTION", "content": "Traditional Chinese Medicine (TCM) has been an indispensable part of healthcare for the Chinese population for thousands of years. TCM employs a wide range of practices, including herbal medicine, acupuncture, cupping therapy, and tuina massage [1]. Herbal medicine is the primary treatment modality of TCM. It has been shown to effectively treat the novel coronavirus (COVID-19), resulting in improved cure rates and reduced mortality [2], [3].\nHerbal prescriptions require doctors to assess patient symptoms using the four diagnostic methods: observation (wang), listening and smelling (wen), questioning (wen), and pulse-taking (qie), guided by the principle of li-fa-fang-yao [4]. Training a doctor involves extensive experience and continuous practical feedback, resulting in a prolonged training period. This contributes to a shortage of TCM doctors and limited TCM resources. Developing an effective prescription predictor is an approach to alleviate challenges such as doctor shortages and the need for prolonged training periods.\nPrescription prediction in TCM involves designing a computational system capable of predicting the appropriate prescription based on given symptoms. This system models the complex relationships between symptoms and herbs [5]. Language generation techniques show particular promise in TCM prescription prediction, treating prescription generation as a"}, {"title": "II. BACKGROUND AND SIGNIFICANCE", "content": "We now formally define the prescription prediction problem. Given a prescription dataset Ptrain, an element in Ptrain consists of a symptom description s and its corresponding prescription {hi, Wi}i\u2208[k]. Here, h denotes the herb name, w represents the herb dosage, and [k] signifies a list ranging from 1 to k. Our objective is to train a model M such that M(s) reproduce {hi, wi}i\u2208[k] accurately. For alignment with the language generation task, we concatenate {hi, wi}i\u2208[k] using a comma separator to form a single sentence. Concrete examples are presented in Fig. 1."}, {"title": "B. TCM Herbal Prescription Prediction", "content": "Research on TCM prescription prediction mainly falls into three categories: topic model-based, graph model-based, and language model-based approaches. Topic model-based approaches treat relationships between symptoms and herbs as that of documents and topics [16]\u2013[18]. These approaches rely on statistical relationships, lacking a semantic understanding of symptoms. Graph-based approaches construct a medical knowledge graph to model the relationships between symptoms and herbs [14], [19], [20]. However, these approaches also lack consecutive semantic information regarding symptoms. Language model-based approaches are a more promising way to model the complicated relationships between symptoms and herbs.\nLanguage model-based prescription prediction models take patient symptom descriptions as input and generate herbal prescriptions sequentially. TCM Translator [21] uses transformer architectures to distill context vectors from symptoms and LSTM [22] as the decoder. AttentiveHerb [7] employs a seq2seq [6] model with dual attention mechanisms to distinguish primary from secondary symptoms and map herb-symptom interactions using clinical data. Herb-Know [8] utilizes herb descriptions to model associations with symptoms and evaluates whether herb effects align with symptom descriptions. TCMBERT [9] integrates transfer learning by initially training an ALBERT [23] on TCM-related documents and fine-tuning an LSTM-based seq2seq model for context vector extraction. RoKEPG [10] incorporates additional herb knowledge for fine-tuning prescription prediction models."}, {"title": "C. Large Language Models in TCM", "content": "LLMs have made significant strides in various NLP tasks. Notable examples like ChatGPT and GPT-4.0 [24] have attracted considerable attention, although specifics about their architectures, parameters, and training strategies from OpenAI remain undisclosed. The emergence of open-sourced LLMs has sparked widespread interest among researchers. Prominent open-sourced models include LLaMA [25], Bloom [26], and ChatGLM [27], among others. While LLMs excel in standard NLP tasks, they often struggle with specialized domains requiring specific knowledge, such as medicine or finance. Supervised fine-tuning has become a standard approach to enhance LLMs for these domains by incorporating specialized knowledge.\nIn the context of TCM, several innovative models have been proposed. Bentsao [28] utilizes supervised fine-tuning on LLaMA, integrating structured and unstructured knowledge from CMeKG [29]. Huatuo [30] leverages both original and distilled data from ChatGPT, incorporating a Reinforcement Learning from Artificial Intelligence Feedback (RLAIF) mechanism and employing Proximal Policy Optimization (PPO) during fine-tuning. Zhongjing [31] applies continual pretraining to inject domain knowledge and uses supervised fine-tuning on a Chinese multi-turn medical dialogue dataset, complemented by Reinforcement Learning from Human Feedback (RLHF). ShenNong [32] builds on LLaMA with LoRA-based fine-tuning on an instructional dataset derived from ChatGPT and a traditional Chinese medicine knowledge graph, benefiting from a large-scale dataset of over 110,000 instructions."}, {"title": "D. Parameter Efficient Fine-Tuning", "content": "Parameter Efficient Fine-Tuning (PEFT) utilizes a small amount of parameters to fine-tune a large language model effectively. Assuming there is a pretrained model fo(yx), PEFT seeks to adjust a limited number of parameters, \u2206\u03b8, such that |0| > |\u2206\u03b8|. In contrast to conventional fine-tuning, which updates all parameters, denoted as |\u03b8| = |\u2206\u03b8|, and requires significant computational resources, PEFT selectively updates a limited number of learnable parameters to achieve results comparable to complete fine-tuning.\nPEFT encompasses three primary methodologies: adapter [33], p-tuning [34]\u2013[36], and LoRA [15]. Adapter tuning [33] introduces PEFT by appending additional learnable modules, named adapters, to every layer of the pretrained model, exclusively fine-tuning these adapters while maintaining the original parameters. In contrast, p-tuning leveraged prompt engineering techniques, fine-tuning learnable vectors as new tokens and inserting these tokens into the multi-head self-attention layer (MSA) [34]\u2013[36]. In this work, we employ LoRA [15] to efficiently fine-tune LLMs on the target dataset."}, {"title": "III. MATERIALS AND METHODS", "content": "In this study, as illustrated in Fig. 1, we gather a high-quality prescription dataset DigestDS and propose a PEFT approach for prescription prediction TCM-FTP. DigestDS is comprised of practical medical records collected from specialists in digestive system disorders. TCM-FTP utilizes supervised finetuning to train the model on DigestDS, incorporating LoRA technique and an effective data augmentation technique, which involves permuting the herbs in the prescriptions."}, {"title": "A. Datasets", "content": "1) Data collection: We collect outpatient medical record data generated by specialists in TCM hospital 1 1 over a span from 2008 to 2022. The prescriptions specifically focus on digestion system disorders.\n2) Data processing: Initially, we remove the incomplete data items and erase any privacy information. Subsequently, we exclude irrelevant information, retaining only essential information for prescription prediction. Specifically, we keep the chief complaint, medical history, and tongue-coating details"}, {"title": "B. TCM-FTP", "content": "To better model the intricate relationships between symptoms and herbs, we propose TCM-FTP, which employs a pre-trained LLM coupled with an auto-regressive mechanism for TCM prescription prediction. For efficient fine-tuning of the LLM, we employ the LoRA technique [15], which optimizes the LLM with limited parameters and computational resources. To enhance the dataset, we permute the herbs in prescriptions to leverage the order-agnostic property inherent in TCM prescriptions.\nGiven a prescription dataset Ptrain with {s, {hi, Wi}i\u2208[k]}, our primary goal is to use a language model to maximize the estimation of conditional probability:\n$\\max_{\\theta} E_{s,\\{h_i,W_i\\}_{i\\in[k]}\\sim P_{train}} \\sum_{i}^{k} log P((h_i, w_i)|h_{<i}, W_{<i}; s; \\theta),$ (1)\nwhere s is the symptom description, k is the number of herbs in the prescription, and (hi, wi) represents the i-th (herb, dosage) pair with i\u2208 {1,2,...,k}. h<i and w<i represent a set of herbs and a set of weights with index less than i, respectively. \u03b8 represents our model parameters.\nTo leverage the auto-regressive mechanism in LLMs, we concatenate the herbs and dosages into one string. We denote concatenated string CAT({hi, wi}i\u2208[k]) as y. Consequently, our objective function becomes:\n$\\max_{\\theta} E_{s,\\{h_i,W_i\\}_{i\\in[k]}\\sim P_{train}} \\sum_{j=1}^{|y|} log P(y_j|\\{y_{<j}\\}; s; \\theta),$ (2)\ny=CAT({hi,wi}i\u2208[k])\nwhere {y<j} denotes the set of the tokens from index 1 to index j \u2212 1 in y, and y represents the number of tokens in y.\n1) Low-rank Adaptation: We employ the LoRA [15] for PEFT to conserve computational resources. Beginning with a pre-trained LLM fo, we encounter two primary types of parametric functions within our model framework: linear and embedding functions. These functions are mathematically described as:\nFlinear(x) = Wl\u00b7x, (3)\nFemb(x) = EMB (x; We), (4)\nwhere EMB(x; \u00b7) represents the embedding operator that selects the x-th column from the specified matrix, and Wl and We denotes the parameters for the linear and embedding functions, respectively. For fine-tuning, we introduce updates via low-rank matrices A and B, leading to modifications in the original functions:\nFlinear(x)' = Flinear(x) + Al\u00b7 Bl\u00b7x, (5)\nFemb(x)' = Femb(x) + Ae \u00b7 EMB (x; Be), (6)\nwhere Al, Bl, Ae, and Be are the learnable parameters associated with the linear and embedding functions. With the updates to the low-rank matrices represented by A\u03b8, our goal is to maximize the expected conditional probability:\n$\\max_{\\Delta\\theta} E_{s,\\{h_i,W_i\\}_{i\\in[k]}\\sim P_{train}} \\sum_{j=1}^{|y|} log P(y_j|\\{y_{<j} \\}; s; \\theta + \\Delta\\theta).$ (7)\ny=CAT({hi,wi}i\u2208[k]\n2) Order-Agnostic Property: Recognizing the order-agnostic characteristic of herbs in TCM prescriptions, we implement data augmentation by permuting the herbs in the prescriptions. Given a prescription sample {s, {hi, Wi}i\u2208[k]}, we define the permuted herb sequence as CAT({hi, wi}i\u2208[k]). The sequence resulting from the permutation, ({hi, Wi}i\u2208[k]), is represented as {hri, Wri}i\u2208[k], where ri denotes the indices after shuffling. After the herb permutation, our final goal becomes:\n$\\max_{\\Delta\\theta} \\sum_{t=1}^{K} E_{s,\\{h_i,W_i\\}_{i\\in[k]}\\sim P_{train}} \\sum_{j=1}^{|y|} log P(y_j|\\{y_{<j} \\}; s; \\theta + \\Delta\\theta),$ (8)\ny=CAT({hi, wi}i\u2208[k])\nwhere K is the number of permutation times. t refers to different permutation index since permutation function is nondeterministic at each run."}, {"title": "C. Baselines", "content": "To demonstrate the performance of our model, we compare it with the following baselines:\n(1) Topic Models and Multi-label Classification Models:\n(2) TCM Prescription Prediction Models:\n(3) Pre-trained Language Models:"}, {"title": "D. Experimental setup", "content": "We implement the fine-tuning process with the Transformers package from Hugging Face\u00b2. The number of training epochs is set to 10. A cosine scheduler is adopted, with the rank of LoRA being 16 and the alpha of LoRA at 32. We consider the number of herb permutations, K, as 20 and 50, respectively. The default learning rate is set at 1e-3. We leverage the gradient accumulation technique to increase the batch size, with 8 accumulation steps and a batch size of 16. We employ two foundation models Chinese-Alpaca-Plus-7B 3 and ShenNong 4. Chinese-Alpaca-Plus-7B is a variant of LLaMA [25] with continual pre-training and supervised finetuning on Chinese corpus, denoted as LLaMA+ for simplicity. ShenNong is a further refinement of LLaMA+, fine-tuned with TCM instruction datasets. We use an 8-V100 GPU machine for fine-tuning and the running time for TCM-FTP (K = 50) is 146 hours. During the inference stage, We employ top-k and top-p combinations for decoding, setting the top-k to 50 and the top-p to 0.7, with a default temperature of 0.95."}, {"title": "E. Evaluation Metrics", "content": "For the proposed TCM-FTP, we evaluate it from both quantitative and qualitative perspectives, including the following evaluation metrics.\n\u2022 Normalized mean square error (NMSE): To tackle the problem above, we design this metric to evaluate the accuracy of predicted dosage by normalizing the squared differences using the original weights. For a given prescription p and a predicted prescription and p\u02c6, we suppose they are composed by a set of pairs of a herb and a dosage, p = {(h,w)}, where h, w refer to herb and dosage respectively, and p\u02c6 = {(h',w')} indicates the generated result from model. The calculation of NMSE is as follows,\n$\\textrm{NMSE} = \\frac{1}{Z} \\sum_{(h,w)\\in p, (h',w') \\in \\hat{p}} \\mathbb{1}[h = h'](\\frac{(w-w')}{w})^2$, (9)\nwhere Z is the number of correctly predicted herbs, and 1[\u00b7] is the indicator function. For evaluation, the average dosage of each herb from the training data is used as a baseline for dosage predictions. For herbs unseen in the training data, the dosage is predicted as the average dosage of all known herbs. This approach is referred to as NMSEbase in the NMSE calculations.\nQualitative Evaluation. Existing quantitative evaluation metrics assess model quality based solely on sample labels, neglecting the compatibility and symptom-specific effectiveness of prescriptions. To comprehensively evaluate our model, we engaged five TCM experts to conduct an expert qualitative evaluation (EQE) of selected prescriptions generated by our model. Each doctor independently assessed the prescriptions for herbal effectiveness in treating symptoms (SHE) and herbal compatibility (HC), assigning scores on a scale of 0 to 5. Higher scores indicate greater effectiveness or compatibility."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "The overall comparison results between TCM-FTP and the baselines are presented in Table II. The proposed TCM-FTP outperforms all baseline models in the herb prediction task on DigestDS, achieving an F1-score of 0.8016 using LLaMA+ as the foundation model and 0.8031 using ShenNong. This highlights the superior capability of TCM-FTP in modeling the intricate relationships between symptoms and herbs in prescriptions. Unlike previous approaches, TCM-FTP also includes herb dosage prediction, which is crucial in TCM due to the significant impact of dosage combinations. As shown in"}, {"title": "B. Parameter Analysis", "content": "The number of permutation The herb permutation introduces the order-agnostic property to enhance the prediction performance. We present the results of TCM-FTP with varying numbers of herb permutations K in Table III. As K increases from 0 to 50, the f1-score improves gradually from 0.4885 to 0.8031, and the NMSE decreases from 0.1683 to 0.0604. This suggests the importance of the order-agnostic property as an effective inductive bias for prescription prediction.\nLearning rates We validate the impact of learning rates by conducting experiments with K=10 and varying learning rates, as shown in Table IV. Higher learning rates correlate with improved precision, recall, and F1-score. This suggests that lower learning rates lead to underfitting, likely due to significant disparities between the pre-trained model parameters and the target parameters. Our objective is to fine-tune a task-specific generator without retaining the broad language capabilities of large language models (LLMs), necessitating"}, {"title": "C. Expert Qualitative Evaluation", "content": "Since conducting expert qualitative evaluation (EQE) is time-consuming and labor-intensive for doctors, this study randomly selected 20 data points from the test set for evaluation. As shown in Table V, the TCM-FTP model outperformed other baseline models across all metrics, achieving an average SHE score of 4.08 (standard deviation 0.5628) and an average HC score of 4.03 (standard deviation 0.5404). This indicates that TCM-FTP can generate effective prescriptions that adhere to TCM compatibility principles. In contrast, GPT-3.5 and GPT-4.0 [24] scored below 3 for both SHE and HC, with total scores of 5.75 and 5.89, respectively, significantly lower than TCM-FTP and TCMPR [43]. This demonstrates that GPT-3.5 and GPT-4.0 perform poorly in generating prescriptions that adhere to TCM principles. TCMPR performed relatively well, with SHE and HC scores of 3.56 (standard deviation 0.6715) and 3.54 (standard deviation 0.6578), respectively, but still fell short of TCM-FTP. Overall, TCM-FTP excelled in prescription generation, receiving higher approval from doctors and showing less score variability, indicating better stability and robustness."}, {"title": "D. Case Study", "content": "In order to visually showcase the predictive performance and capabilities of our TCM-FTP, we obtain the predicted herb results formed by each model on the test set and had them evaluated by TCM experts."}, {"title": "V. CONCLUSION", "content": "To deal with the lack of high-quality datasets and to improve the performance in Traditional Chinese Medicine (TCM) prescription predictions, we build a TCM prescription dataset DigestDS from clinical records and propose TCM-FTP to fine-tune large language models to predict herbs with the corresponding dosages. We collect DigestDS from practical clinical records by focusing on digestion disorder diseases. TCM-FTP employs a low-rank adaptation for computational and storage efficiency and adapts a data augmentation by randomly permutating the order of herbs in prescriptions. The experimental results reveal the remarkable effectiveness of TCM-FTP, surpassing previous methods by large margins in precision, recall, and F1-score. Additionally, our method achieved the best results in NMSE, effectively forming accurate herb and dosage predictions. In future work, we will continue to incorporate domain knowledge into model construction to further enhance performance, aiming to develop a practically usable prescription prediction model."}]}