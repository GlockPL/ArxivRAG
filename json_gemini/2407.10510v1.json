{"title": "TCM-FTP: Fine-Tuning Large Language Models for Herbal Prescription Prediction", "authors": ["Xingzhi Zhou", "Xin Dong", "Chunhao Li", "Yuning Bai", "Yulong Xu", "Ka Chun Cheung", "Simon Seell", "Xinpeng Song", "Runshun Zhang", "Xuezhong Zhou", "Nevin L. Zhang"], "abstract": "Traditional Chinese medicine (TCM) relies on specific combinations of herbs in prescriptions to treat symptoms and signs, a practice that spans thousands of years. Predicting TCM prescriptions presents a fascinating technical challenge with practical implications. However, this task faces limitations due to the scarcity of high-quality clinical datasets and the intricate relationship between symptoms and herbs. To address these issues, we introduce DigestDS, a new dataset containing practical medical records from experienced experts in digestive system diseases. We also propose a method, TCM-FTP (TCM Fine-Tuning Pre-trained), to leverage pre-trained large language models (LLMs) through supervised fine-tuning on DigestDS. Additionally, we enhance computational efficiency using a low-rank adaptation technique. TCM-FTP also incorporates data augmentation by permuting herbs within prescriptions, capitalizing on their order-agnostic properties. Impressively, TCM-FTP achieves an F1-score of 0.8031, surpassing previous methods significantly. Furthermore, it demonstrates remarkable accuracy in dosage prediction, achieving a normalized mean square error of 0.0604. In contrast, LLMs without fine-tuning perform poorly. Although LLMs have shown capabilities on a wide range of tasks, this work illustrates the importance of fine-tuning for TCM prescription prediction, and we have proposed an effective way to do that.", "sections": [{"title": "I. INTRODUCTION", "content": "Traditional Chinese Medicine (TCM) has been an indispensable part of healthcare for the Chinese population for thousands of years. TCM employs a wide range of practices, including herbal medicine, acupuncture, cupping therapy, and tuina massage [1]. Herbal medicine is the primary treatment modality of TCM. It has been shown to effectively treat the novel coronavirus (COVID-19), resulting in improved cure rates and reduced mortality [2], [3].\nHerbal prescriptions require doctors to assess patient symptoms using the four diagnostic methods: observation (wang), listening and smelling (wen), questioning (wen), and pulse-taking (qie), guided by the principle of li-fa-fang-yao [4]. Training a doctor involves extensive experience and continuous practical feedback, resulting in a prolonged training period. This contributes to a shortage of TCM doctors and limited TCM resources. Developing an effective prescription predictor is an approach to alleviate challenges such as doctor shortages and the need for prolonged training periods.\nPrescription prediction in TCM involves designing a computational system capable of predicting the appropriate prescription based on given symptoms. This system models the complex relationships between symptoms and herbs [5]. Language generation techniques show particular promise in TCM prescription prediction, treating prescription generation as a"}, {"title": "II. BACKGROUND AND SIGNIFICANCE", "content": "A. Problem Definition\nWe now formally define the prescription prediction problem. Given a prescription dataset $P_{train}$, an element in $P_{train}$ consists of a symptom description s and its corresponding prescription $\\{h_i, w_i\\}_{i \\in [k]}$. Here, h denotes the herb name, w represents the herb dosage, and [k] signifies a list ranging from 1 to k. Our objective is to train a model M such that M(s) reproduce $\\{h_i, w_i\\}_{i \\in [k]}$ accurately. For alignment with the language generation task, we concatenate $\\{h_i, w_i\\}_{i \\in [k]}$ using a comma separator to form a single sentence. Concrete examples are presented in Fig. 1.\nB. TCM Herbal Prescription Prediction\nResearch on TCM prescription prediction mainly falls into three categories: topic model-based, graph model-based, and language model-based approaches. Topic model-based approaches treat relationships between symptoms and herbs as that of documents and topics [16]\u2013[18]. These approaches rely on statistical relationships, lacking a semantic understanding of symptoms. Graph-based approaches construct a medical knowledge graph to model the relationships between symptoms and herbs [14], [19], [20]. However, these approaches also lack consecutive semantic information regarding symptoms. Language model-based approaches are a more promising way to model the complicated relationships between symptoms and herbs.\nLanguage model-based prescription prediction models take patient symptom descriptions as input and generate herbal prescriptions sequentially. TCM Translator [21] uses transformer architectures to distill context vectors from symptoms and LSTM [22] as the decoder. AttentiveHerb [7] employs a seq2seq [6] model with dual attention mechanisms to distinguish primary from secondary symptoms and map herb-symptom interactions using clinical data. Herb-Know [8] utilizes herb descriptions to model associations with symptoms and evaluates whether herb effects align with symptom descriptions. TCMBERT [9] integrates transfer learning by initially training an ALBERT [23] on TCM-related documents and fine-tuning an LSTM-based seq2seq model for context vector extraction. RoKEPG [10] incorporates additional herb knowledge for fine-tuning prescription prediction models."}, {"title": "C. Large Language Models in TCM", "content": "LLMs have made significant strides in various NLP tasks. Notable examples like ChatGPT and GPT-4.0 [24] have attracted considerable attention, although specifics about their architectures, parameters, and training strategies from OpenAI remain undisclosed. The emergence of open-sourced LLMs has sparked widespread interest among researchers. Prominent open-sourced models include LLaMA [25], Bloom [26], and ChatGLM [27], among others. While LLMs excel in standard NLP tasks, they often struggle with specialized domains requiring specific knowledge, such as medicine or finance. Supervised fine-tuning has become a standard approach to enhance LLMs for these domains by incorporating specialized knowledge.\nIn the context of TCM, several innovative models have been proposed. Bentsao [28] utilizes supervised fine-tuning on LLaMA, integrating structured and unstructured knowledge from CMeKG [29]. Huatuo [30] leverages both original and distilled data from ChatGPT, incorporating a Reinforcement Learning from Artificial Intelligence Feedback (RLAIF) mechanism and employing Proximal Policy Optimization (PPO) during fine-tuning. Zhongjing [31] applies continual pretraining to inject domain knowledge and uses supervised fine-tuning on a Chinese multi-turn medical dialogue dataset, complemented by Reinforcement Learning from Human Feedback (RLHF). ShenNong [32] builds on LLaMA with LoRA-based fine-tuning on an instructional dataset derived from ChatGPT and a traditional Chinese medicine knowledge graph, benefiting from a large-scale dataset of over 110,000 instructions."}, {"title": "D. Parameter Efficient Fine-Tuning", "content": "Parameter Efficient Fine-Tuning (PEFT) utilizes a small amount of parameters to fine-tune a large language model effectively. Assuming there is a pretrained model $f_{\\theta}(y|x)$, PEFT seeks to adjust a limited number of parameters, $\\Delta\\theta$, such that $|\\theta| > |\\Delta\\theta|$. In contrast to conventional fine-tuning, which"}, {"title": "III. MATERIALS AND METHODS", "content": "In this study, as illustrated in Fig. 1, we gather a high-quality prescription dataset DigestDS and propose a PEFT approach for prescription prediction TCM-FTP. DigestDS is comprised of practical medical records collected from specialists in digestive system disorders. TCM-FTP utilizes supervised finetuning to train the model on DigestDS, incorporating LoRA technique and an effective data augmentation technique, which involves permuting the herbs in the prescriptions.\nA. Datasets\n1) Data collection: We collect outpatient medical record data generated by specialists in TCM hospital over a span from 2008 to 2022. The prescriptions specifically focus on digestion system disorders.\n2) Data processing: Initially, we remove the incomplete data items and erase any privacy information. Subsequently, we exclude irrelevant information, retaining only essential information for prescription prediction. Specifically, we keep the chief complaint, medical history, and tongue-coating details"}, {"title": "B. TCM-FTP", "content": "To better model the intricate relationships between symptoms and herbs, we propose TCM-FTP, which employs a pre-trained LLM coupled with an auto-regressive mechanism for TCM prescription prediction. For efficient fine-tuning of the LLM, we employ the LoRA technique [15], which optimizes the LLM with limited parameters and computational resources. To enhance the dataset, we permute the herbs in prescriptions to leverage the order-agnostic property inherent in TCM prescriptions.\nGiven a prescription dataset $P_{train}$ with $\\{s, \\{h_i, w_i\\}_{i \\in [k]}\\}$, our primary goal is to use a language model to maximize the estimation of conditional probability:\n$\\max_{\\theta} E_{s,\\{h_i,W_i\\}_{i\\in[k]}~P_{train}}  \\sum_{i}^{k} log P((h_i, w_i)|h_{<i}, W_{<i}; s; \\theta),$ (1)\nwhere s is the symptom description, k is the number of herbs in the prescription, and $(h_i, w_i)$ represents the i-th (herb, dosage) pair with $i \\in \\{1,2,...,k\\}$. $h_{<i}$ and $w_{<i}$ represent a set of herbs and a set of weights with index less than i, respectively. $\\theta$ represents our model parameters.\nTo leverage the auto-regressive mechanism in LLMs, we concatenate the herbs and dosages into one string. We denote"}, {"title": "C. Baselines", "content": "To demonstrate the performance of our model, we compare it with the following baselines:\n(1) Topic Models and Multi-label Classification Models:\n(2) TCM Prescription Prediction Models:\n(3) Pre-trained Language Models:"}, {"title": "D. Experimental setup", "content": "We implement the fine-tuning process with the Transformers package from Hugging Face. The number of training epochs is set to 10. A cosine scheduler is adopted, with the rank of LoRA being 16 and the alpha of LoRA at 32. We consider the number of herb permutations, K, as 20 and 50, respectively. The default learning rate is set at le-3. We leverage the gradient accumulation technique to increase the batch size, with 8 accumulation steps and a batch size of 16. We employ two foundation models Chinese-Alpaca-Plus-7B and ShenNong . Chinese-Alpaca-Plus-7B is a variant of LLaMA [25] with continual pre-training and supervised finetuning on Chinese corpus, denoted as LLaMA+ for simplicity. ShenNong is a further refinement of LLaMA+, fine-tuned with TCM instruction datasets. We use an 8-V100 GPU machine for fine-tuning and the running time for TCM-FTP (K = 50) is 146 hours. During the inference stage, We employ top-k and top-p combinations for decoding, setting the top-k to 50 and the top-p to 0.7, with a default temperature of 0.95."}, {"title": "E. Evaluation Metrics", "content": "For the proposed TCM-FTP, we evaluate it from both quantitative and qualitative perspectives, including the following evaluation metrics.\nQuantitative Evaluation. For evaluation metrics, we use precision, recall, and F1-score as herb prediction metrics and designed NMSE for herb dosage evaluation.\nQualitative Evaluation. Existing quantitative evaluation metrics assess model quality based solely on sample labels, neglecting the compatibility and symptom-specific effectiveness of prescriptions. To comprehensively evaluate our model, we engaged five TCM experts to conduct an expert qualitative evaluation (EQE) of selected prescriptions generated by our model. Each doctor independently assessed the prescriptions for herbal effectiveness in treating symptoms (SHE) and herbal compatibility (HC), assigning scores on a scale of 0 to 5. Higher scores indicate greater effectiveness or compatibility."}, {"title": "IV. RESULTS AND DISCUSSION", "content": "A. Overall Results\nThe overall comparison results between TCM-FTP and the baselines are presented in Table II. The proposed TCM-FTP outperforms all baseline models in the herb prediction task on DigestDS, achieving an F1-score of 0.8016 using LLaMA+ as the foundation model and 0.8031 using ShenNong. This highlights the superior capability of TCM-FTP in modeling the intricate relationships between symptoms and herbs in prescriptions. Unlike previous approaches, TCM-FTP also includes herb dosage prediction, which is crucial in TCM due to the significant impact of dosage combinations. As shown in"}, {"title": "V. CONCLUSION", "content": "To deal with the lack of high-quality datasets and to improve the performance in Traditional Chinese Medicine (TCM) prescription predictions, we build a TCM prescription dataset DigestDS from clinical records and propose TCM-FTP to fine-tune large language models to predict herbs with the corresponding dosages. We collect DigestDS from practical clinical records by focusing on digestion disorder diseases. TCM-FTP employs a low-rank adaptation for computational and storage efficiency and adapts a data augmentation by randomly permutating the order of herbs in prescriptions. The experimental results reveal the remarkable effectiveness of TCM-FTP, surpassing previous methods by large margins in precision, recall, and F1-score. Additionally, our method achieved the best results in NMSE, effectively forming accurate herb and dosage predictions. In future work, we will continue to incorporate domain knowledge into model construction to further enhance performance, aiming to develop a practically usable prescription prediction model."}]}