{"title": "Table as Thought: Exploring Structured Thoughts in LLM Reasoning", "authors": ["Zhenjie Sun", "Naihao Deng", "Haofei Yu", "Jiaxuan You"], "abstract": "Large language models' reasoning abilities benefit from methods that organize their thought processes, such as chain-of-thought prompting, which employs a sequential structure to guide the reasoning process step-by-step. However, existing approaches focus primarily on organizing the sequence of thoughts, leaving structure in individual thought steps underexplored. To address this gap, we propose Table as Thought, a framework inspired by cognitive neuroscience theories on human thought. Table as Thought organizes reasoning within a tabular schema, where rows represent sequential thought steps and columns capture critical constraints and contextual information to enhance reasoning. The reasoning process iteratively populates the table until self-verification ensures completeness and correctness. Our experiments show that Table as Thought excels in planning tasks and demonstrates a strong potential for enhancing LLM performance in mathematical reasoning compared to unstructured thought baselines. This work provides a novel exploration of refining thought representation within LLMs, paving the way for advancements in reasoning and AI cognition.", "sections": [{"title": "1 Introduction", "content": "Recent advancements in reasoning have demonstrated that the reasoning capabilities of large language models (LLMs) can be enhanced by introducing structure into the reasoning process (Wei et al., 2023; Yao et al., 2023; Besta et al., 2024). For instance, the chain-of-thought approach organizes textual reasoning in a step-by-step manner using a linear chain structure (Wei et al., 2023). Building on this, following works have shown that incorporating more complex organizational structures further improves reasoning performance (Besta et al., 2024; Yao et al., 2023). However, these approaches structure reasoning only at the level of connections between distinct reasoning steps (inter-thought level) and leave the content of individual steps (thought level) unstructured. This raises the critical question: Can LLMs' reasoning abilities be further enhanced by introducing structure within individual thoughts?\nTo address this question, we draw inspiration from cognitive neuroscience theories of human thought. Neuroscientists have found that humans think in a structured way, with the brain's organization facilitating sequential and goal-oriented reasoning. Christoff and Gabrieli (2000) provided early evidence that the prefrontal cortex supports structured reasoning through a rostrocaudal hierarchy, enabling the processing of increasingly abstract concepts and complex goal-directed behavior. Later, Friston (2005)'s predictive coding framework demonstrated how structured cognition emerges from the brain's ability to build hierarchical models, combining experiences with current input to predict results. More recently, Jeff Hawkins (Hawkins, 2021) argued that humans think in a structured manner, with the neocortex organizing knowledge in certain structures, and thinking arises from neurons activating sequential locations in these frames. Building on these insights, we propose investigating whether similarly structured representations can be incorporated into LLMs to enhance their reasoning and planning capabilities.\nIn this work, we adopt a simple yet effective structural format a tabular schema-to approximate the structured nature of human thinking processes. In our approach, the schema of a table serves as a framework for organizing and navigating knowledge. Inspired by the sequential processes described in neuroscience-where neurons activate specific patterns step by step (Hawkins, 2021)-we model these processes as the sequential population of rows in a table, moving across columns according to a predefined schema. A single table can encapsulate one or more such structured thought processes, providing a coherent container for organizing and connecting thinking steps and associated information. Tables not only represent step-by-step processes for achieving specific goals but also serve as robust frameworks for planning tasks. Moreover, utilizing tables as structured representations enables schema design that ensures organization and data integrity, thereby facilitating efficient verification and analysis.\nThe contributions of our paper are as follows:\n\u2022 Motivated by insights from cognitive neuroscience regarding the structured nature of human thinking, we propose a novel framework, Table as Thought, that injects structure at the thought level. To the best of our knowledge, this is the first exploration and demonstration of integrating structured representations directly into the reasoning process of large language models.\n\u2022 We demonstrate the advantages of Table as Thought in tasks requiring planning and mathematical reasoning, highlighting its potential to enhance performance on tasks that demand sequential and goal-oriented thought processes.\n\u2022 We provide a detailed and comprehensive analysis of Table as Thought, offering insights into its functionality and strengths, and comparing the benefits of structured versus unstructured thought representations. We believe these findings can inspire future research into the nature and representation of thought processes in artificial intelligence and computational linguistics."}, {"title": "2 Related Work", "content": "Structures in LLM Reasoning Recent advancements in large language models (LLMs) have increasingly focused on integrating structured processes to enhance reasoning capabilities. Chain-of-Thought prompting (Wei et al., 2023) introduces a step-by-step framework that organizes thoughts in a sequential manner, enabling more coherent reasoning. Building on this, Tree of Thoughts (Yao et al., 2023) and Graph of Thoughts (Besta et al., 2024) employ hierarchical and networked structures to further enhance problem-solving, leveraging branching and interconnected paths. Moreover, self-consistency (Wang et al., 2023) improves reliability by sampling multiple reasoning paths and selecting the most consistent outcome, thereby addressing variability in generated responses.\nWhile these methods excel at organizing reasoning at a macro level\u2014such as through chaining, branching, or aggregating thought paths-they do not address the internal structure of individual thoughts. Our work is distinct in that it introduces structure directly at the thought level, refining the granularity of reasoning processes in LLMs. By focusing on the internal organization of individual reasoning steps, we provide a novel perspective on enhancing the depth and precision of structured reasoning in LLMs.\nRepresentations of Tables in LLM Inference Tables have traditionally played a significant role in LLMs for tasks involving the understanding and processing of tabular data, such as knowledge retrieval (Cong et al., 2024), question answering over structured data (Yin et al., 2020; Zhang et al., 2024b), and tabular reasoning (Herzig et al., 2020; Deng et al., 2024). In these tasks, tables are leveraged only as input for interpretation and manipulation.\nThe Chain-of-Table framework (Wang et al., 2024) extends the application of tables by employing them as proxies for intermediate thoughts in reasoning tasks involving tabular data. In this framework, LLMs iteratively update a table, forming a dynamic reasoning chain where the table evolves based on intermediate results. While this approach has proven effective on tabular-specific datasets, it remains inherently tied to tasks where tabular data is part of the input or reasoning context.\nIn contrast, our work redefines the role of tables by utilizing them as a universal framework for structuring and representing the internal thought processes of LLMs in non-table-specific tasks, such as planning and mathematical reasoning. Unlike prior approaches that depend on pre-existing tabular inputs, we employ tables as dynamic containers to organize and manipulate thoughts step by step. This approach enables structured reasoning even in tasks where no tabular data is initially present, bridging the gap between unstructured text-based reasoning and structured problem-solving paradigms. By generalizing the utility of tables beyond table-specific reasoning tasks, our work marks a significant departure from previous methods and demonstrates the versatility of this novel framework."}, {"title": "3 Table as Thought", "content": "We present the design of the Table as Thought framework, which introduces a novel approach to reasoning in large language models by leveraging tables as structured representations of thoughts.\nTable as Thought. Table as Thought employs a table as a container to represent one or more structured thoughts. These tables, referred to as \"reasoning tables\", encapsulate thoughts and provide a transparent representation of the reasoning process. A reasoning table T is initialized with an original table schema S, which is defined by the LLM for a given query Q. Structured thoughts \u0398 are then generated based on S, with each thought corresponding to a row in the reasoning table T. The table T is subsequently populated and updated according to these structured thoughts \u0398.\nThe overall reasoning workflow using the reasoning table is illustrated in Figure 1 and formalized in Algorithm 1.\nSchema Development Module The Schema Development Module dynamically adapts table schemas to accommodate various queries across different reasoning tasks. For constraint-planning tasks, where the primary objective is to satisfy constraints, we prompt LLMs to identify the constraints explicitly before designing the schema. This ensures that both explicit and implicit constraints are addressed in the reasoning process. For mathematical reasoning tasks, the schema is tailored to reflect the logical progression of the reasoning steps, enabling systematic organization of critical information.\nThe headers in the table schemas are designed to represent essential reasoning steps and key information pertinent to the task. These headers act as anchors for organizing and verifying intermediate and final reasoning outputs.\nFor example, consider the travel planning query:\nI plan to travel alone, and my\nplanned budget for the trip is\naround $1,100.\nIn this case, a key constraint is that the total cost should not exceed $1,400. To address this constraint, the schema must include a header such as Cost, with the type Number, ensuring that the relevant information is captured and evaluated against the budgetary constraint.\nFor a mathematical reasoning task, such as a question from the GSM8K dataset:\nA robe takes 2 bolts of blue fiber\nand half that much white fiber.\nHow many bolts in total does it\ntake?"}, {"title": "Here, the reasoning process requires consideration\nof the quantities of blue and white fibers. The\nschema should therefore include keys such as Blue\nFiber and White Fiber, ensuring that all relevant\nelements are systematically tracked and calculated.", "content": "Reasoning Verification Module The inclusion\nof this module stems from our findings during ex-\nperiments that current LLMs sometimes fail to gen-\nerate the complete reasoning process with struc-\ntured thoughts to solve a query. However, this\nmodule is designed not only to verify the complete-\nness of the reasoning process, but also to ensure its\ncorrectness.\nFor constraint reasoning tasks, the module guar-\nantees that all necessary information required to\nmeet the constraints defined in the schema is cap-\ntured and satisfied. Specifically, it verifies whether\nthe constraints identified during the schema devel-\nopment phase are adhered to. Constraint check-\ning is generally performed internally by the LLM\nthrough reflective reasoning on the generated ta-\nble, with constraints explicitly listed for verifica-\ntion. The structured nature of thoughts in Table as\nThought introduces an additional capability: Auto-\nCheck Constraints, which are constraints set for\nexternal verification, performed entirely by the sys-\ntem to ensure that the table adheres to the defined\nconstraints. By leveraging the structured repre-\nsentations of Table as Thought, Auto-Check Con-\nstraints facilitate the systematic validation of inter-\nmediate steps and final outputs without relying on\nthe LLM.\nFor mathematical reasoning tasks, the module\nevaluates the correctness of the reasoning process\nby ensuring that the table reflects an accurate and\nlogical reasoning path toward solving the problem.\nThis involves checking whether the intermediate\nand final outputs align with the expected reasoning\nsteps outlined in the schema.\nTable Construction Module The Table Con-\nstruction Module iteratively generates structured\nthoughts and constructs the reasoning table by in-\ncorporating the schema and feedback from the rea-\nsoning verification module. This process involves\ndynamically adding new thoughts to the table, mod-\nifying existing entries, or removing entries that do\nnot align with the schema or query requirements.\nThe iterative process terminates under one of the\nfollowing conditions:\n1. The reasoning table is verified as complete and correct by the reasoning verification module.\n2. The maximum number of iterations, which is 10 in all our experiments, is reached."}, {"title": "4 Experiments", "content": "4.1 Tasks and Language Models\nFor all tasks, we adopt the original evaluation methods to ensure consistency and comparability.\nConstraint Planning Tasks. The goal of constraint planning tasks is to generate plans that satisfy both explicit and implicit constraints. We evaluate our approach on two datasets, each presenting different levels of complexity in the expected plans. The TravelPlanner dataset (Xie et al., 2024) requires LLMs to generate detailed travel plans that adhere to explicit constraints provided in the query, such as budget limitations, as well as implicit constraints derived from common sense. The expected travel plans are highly complex, encompassing multi-day agendas that include transportation, accommodations, and daily attractions. Due to the exceptionally long context required for this task, which results in substantial token costs, we conduct experiments exclusively with GPT-4-o-mini. The calendar scheduling task from the NaturalPlan benchmark (Zheng et al., 2024) focuses on generating single-object plans. In this task, LLMs must determine an appropriate meeting time based on explicit constraints, such as the company's working hours and the unavailable time slots of each participant.\nMath reasoning tasks. We evaluate LLMs using GSM-8K and MATH500 to assess structured mathematical reasoning. GSM-8K (Cobbe et al., 2021) contains 8,000 grade-school-level word problems, testing multi-step reasoning and numerical precision. MATH500 (Lightman et al., 2023) features 500 advanced problems from the MATH dataset (Hendrycks et al., 2021), covering algebra, calculus, and geometry. It challenges models with tasks requiring symbolic manipulation and deep mathematical understanding. These datasets help evaluate our approach across diverse scenarios, from simple arithmetic to complex problems.\nLanguage Models. The schema design and table construction modules in Table as Thought require LLMs capable of generating complex, structured outputs that conform to intricate schemas. This"}, {"title": "capability is natively supported by OpenAI's Struc-\ntured Outputs Mode, which allows for precise align-\nment with defined schema requirements. Conse-\nquently, our experiments are conducted exclusively\non OpenAI's GPT-4-o-mini and GPT-4-0-2024-08-\n06 models (OpenAI et al., 2024). Expanding the\nevaluation to include open-source models with sim-\nilar capabilities remains an area for future work.", "content": "4.2 Text Thought Baselines\nDirect Prompting. Direct Prompting involves solving queries by directly generating an answer from the input, without prompting for any intermediate reasoning steps.\nCoT Prompting. Chain-of-Thought (CoT) Prompting organizes reasoning as a sequential chain of thoughts, thereby injecting structure into the reasoning process.\nText as Thought. This approach differs from Table as Thought only in its use of unstructured representations for thoughts. Text as Thought employs text as the medium for reasoning. This method extends CoT prompting by iteratively updating the reasoning process based on reflection. Each iteration involves generating intermediate reasoning steps, reflecting on their correctness, and refining the reasoning path as needed. The streamlined process is formalized in Algorithm 2."}, {"title": "4.3 Variations of Table as Thought", "content": "To fully explore and understand the boundaries of Table as Thought, we introduce two variations to the TravelPlanner task. These variations include Table as Thought with auto check constraint, which adds complexity to schema design, and Table as Thought with given schema, which simplifies the task by providing a predefined schema.\nTable as Thought with Auto-Check Constraint. This variation builds on the vanilla Table as Thought by requiring the LLM to add additional constraints during schema design to ensure data integrity and reflect the constraints present in the query. For instance, if a TravelPlanner query includes budget constraints, the LLM is expected to design a schema with headers like Cost and enforce a rule ensuring that the sum of the column does not exceed the specified budget. By introducing this variation, we aim to explore the boundaries of LLMs in designing complex reasoning structures and handling intricate schema requirements.\nTable as Thought with Given Schema. In this variation, the LLM is provided with a predefined schema, as shown in Table 7, rather than designing the schema independently. The given schema is derived from the evaluation pipeline of the TravelPlanner task (Xie et al., 2024), where answers are processed into tables following this schema before evaluation. This variation serves as a comparative baseline to assess the effectiveness and adaptability of schemas designed by LLMs compared to fixed, predefined schemas."}, {"title": "5 Results", "content": "5.1 Calendar Scheduling Task\nTable as Thought achieves the highest performance among all prompting methods on the Calendar Scheduling Task, as shown in Table 2. On GPT-40, Table as Thought improves performance by 10.8% over Direct Prompting and achieves a 5.4% improvement compared to the Text as Thought baseline. This highlights the advantage of using tables as structured representations for planning over unstructured text-based representations. A similar trend is observed with GPT-40-mini, where Table as Thought outperforms other methods, suggesting the robustness of table-based reasoning for simpler constraint reasoning tasks like calendar scheduling.\nFor GPT-40, the improvement from Direct Prompting to CoT Prompting is minimal (0.5%), indicating that chain-like reasoning structures may already be embedded in the model's reasoning process. However, incorporating self-verification through Text as Thought yields a 4.9% improvement. Importantly, transitioning from unstructured thoughts to structured tables results in a more substantial performance boost (5.4%), underscoring the benefits of structured representations in reasoning tasks."}, {"title": "5.2 TravelPlanner Task", "content": "Table 1 shows that Table as Thought with a given schema achieves the best performance across most metrics in the TravelPlanner task, underscoring the potential of structured thoughts. The significant improvement from vanilla Table as Thought to Table as Thought with a given schema highlights that current LLMs struggle to design effective table schemas for achieving complex objectives. This limitation will be analyzed in more detail in the next section.\nThe results reveal an important trend: on a challenging task like TravelPlanner, which demands reasoning toward a complex objective, introducing increasingly sophisticated structures into the reasoning process can lead to performance degradation. Specifically, methods that incorporate additional complexity-such as chain-of-thought (CoT) prompting, self-reflection in Text as Thought, and rule-constrained structured thoughts in Table as Thought with Auto-Check constraint-tend to perform worse compared to simpler approaches. The exception is Table as Thought with a given schema, which avoids this degradation by relieving the LLM of the need to design its own schema, allowing it to focus solely on reasoning within a predefined structure."}, {"title": "5.3 Math Reasoning Tasks", "content": "Table 3 highlights a general trend in the MATH500 and GSM8K tasks: introducing additional complexity into the reasoning process often leads to a performance drop, particularly for less capable models like GPT-40-mini. For instance, on MATH500, the performance of both GPT-40 and GPT-40-mini decreases as the reasoning structures become more sophisticated, from Direct Prompting to Text as Thought to Table as Thought. This effect is especially pronounced for GPT-4o-mini, where the performance of Table as Thought falls to 47.8%, compared to 65.4% with Direct Prompting. A similar trend is observed on GSM8K, where the addition of more structured reasoning methods results in marginal performance degradation. These results suggest that LLMs may already be overfitted to math reasoning tasks, as noted in recent studies (Mirzadeh et al., 2024; Zhang et al., 2024a).\nDespite this general trend, Table as Thought demonstrates its potential to improve performance by successfully solving questions that text-thought-based methods fail to address, particularly with more capable models like GPT-40. Table 4 provides a detailed breakdown of the percentage of questions that Table as Thought solves, which were missed by other methods. On MATH500, Table as Thought resolves approximately 20% of such questions, while on GSM8K, this figure exceeds 30%. These findings underscore the utility of structured reasoning in identifying alternative pathways to solutions that text-based reasoning methods may overlook."}, {"title": "6 Analysis", "content": "6.1 Effect of Schema Design on Reasoning Structures\nSchema design plays a pivotal role in structuring the reasoning paths of Calendar Scheduling tasks. Different schemas determine the granularity of the reasoning process, which in turn affects model performance.\nTable 5 shows that in the one-row schema, the reasoning process is concise: the LLM identifies all available time slots for participants in a single step and selects a suitable meeting time. This schema produces a single-row table, encapsulating the reasoning process in a compact form. In contrast, the multi-row schema divides the process into finer-grained steps. The LLM first extracts unavailable and preferred time slots for each participant. It then computes available time slots before aggregating this information to finalize the meeting time. This approach results in a table with multiple rows, each representing an intermediate reasoning step, and provides a more detailed reasoning path.\nTable 6 shows that schema complexity impacts performance differently for advanced and less capable models. For GPT-40, the multi-row schema outperforms the one-row schema, achieving 80.28% accuracy compared to 72.93%. This suggests that the finer-grained reasoning path introduced by the multi-row schema aligns well with GPT-40's stronger table reasoning capabilities. By explicitly structuring intermediate steps, the multi-row schema allows GPT-40 to better manage constraints and ensure reasoning correctness. On the contrary, GPT-40-mini performs better with the simpler one-row schema (45.05% vs. 43.46% for the multi-row schema). This indicates that the increased complexity of the multi-row schema exceeds the model's table reasoning and verification abilities, leading to performance degradation."}, {"title": "6.2 LLM Struggles to Design Effective\nSchema for Complex Planning", "content": "Unlike Calendar Scheduling, which focuses on selecting a single time slot, TravelPlanner involves generating a comprehensive travel itinerary, significantly increasing the complexity of the planning task. Our findings indicate that tasking the LLM with designing a table schema results in a notable performance drop compared to using direct prompting with a pre-defined schema. This suggests that LLMs currently lack the capability to independently design effective table schemas for complex planning tasks.\nAlthough the provided schema is not perfect omitting some critical columns, such as \"cost\" for budget constraints\u2014it is generally more effective than most LLM-designed schemas. For instance, as shown in Table 7, the LLM-developed schema and the given schema are structurally similar. However, a key difference is the use of \"Dining Options\" in the LLM-designed schema, as opposed to separating dining into specific categories like \"breakfast,\" \"lunch,\" and \"dinner.\" In practice, this simplification often leads the LLM to allocate only a single meal per day, which contradicts common-sense expectations for travel planning."}, {"title": "6.3 Ablation Study", "content": "We conducted an ablation study using GPT-40-mini on the Calendar Scheduling task to evaluate the individual contributions of schema design and reasoning verification. Table 8 shows that when reasoning verification is removed, accuracy drops from 42.3% to 38.5% (\u2193 3.8%). This indicates that without explicitly verifying constraints, the LLM may overlook key restrictions in the query, leading to false positives during self-checking. The absence of schema design leads to a larger performance drop, from 42.3% to 36.2% (\u2193 6.1%), and further to 32.7% (9.6%) when both schema design and reasoning verification are removed. This highlights the critical role of schema design in structuring the reasoning process. Table 9 shows that without a schema, the LLM tends to create tables with fewer columns, omitting key information necessary for constraint checking. While the table without schema design contains basic headers such as Participant and Selected Meeting Time, the schema-designed table includes additional headers like Conflict Check, Work Hours Start/End, and Notes/Comments. These additional columns capture critical reasoning steps and constraints, enabling more effective verification and selection of a valid meeting time."}, {"title": "7 Conclusion", "content": "We proposed Table as Thought, a novel framework that introduces structured reasoning at the thought level. The framework centers on the design and utilization of table schemas, where the LLM is tasked with constructing a schema and generating structured thoughts based on it. Our results demonstrate that Table as Thought excels in constraint planning tasks, showcasing its ability to manage complex constraints effectively. Moreover, the framework exhibits significant potential for further improving performance in math reasoning tasks, particularly in addressing unsolved problems through structured reasoning.\nAdditionally, we conducted detailed analyses of the results, exploring the interplay between schema design, reasoning complexity, and model capabilities. These insights pave the way for future research into the nature and representation of thought processes, offering a promising direction for the development of more robust reasoning frameworks in LLMs."}, {"title": "Limitations", "content": "Our proposed methods are currently supported only by models capable of generating structured data with complex schemas. This limitation restricts our experiments to a small set of closed-source models, such as those provided by OpenAI. Consequently, the generalizability of our findings to open-source LLMs remains unexplored. Future work should investigate approaches for adapting Table as Thought to a broader range of models, including those with limited native support for structured data generation."}, {"title": "Ethical Statement", "content": "This research was conducted using publicly available datasets (e.g., GSM-8K, MATH500, TravelPlanner) in compliance with their terms of use, ensuring no personally identifiable information (PII) was processed. While our proposed framework, Table as Thought, aims to enhance structured reasoning in LLMs, we acknowledge the potential risks of misuse in harmful applications, such as deceptive planning or adversarial reasoning. To mitigate this, we advocate for responsible deployment with appropriate safeguards."}, {"title": "A Completion Rate Analysis of Table as\nThought on Closed and Open Source\nLLMS", "content": "We investigate the completion rate of Table as\nThought when using both closed-source and open-\nsource LLMs in calendar scheduling tasks. De-\nspite advancements in open-source models such as\nLLaMA 3.1-8B and LLaMA 3.1-70B, these mod-\nels encounter significant challenges in adhering to\nthe tool schema required for the schema design and\ntable construction modules. Specifically, they fail\nto generate the expected outputs due to the com-\nplexity of the tool schema, leading to errors in the\nintermediate pipeline.\nIn a set of 100 test examples, we observe that\nTable as Thought fails 100% and 63% of the time\nwhen using LLaMA 3.1-8B and LLaMA 3.1-70B,\nrespectively. These high failure rates make experi-\nments with open-source models infeasible because:\n1. High Computational Cost: The need for re-\npeated retries significantly increases computa-\ntional overhead.\n2. Result Bias: Retrying may introduce bias,\nas incorrect reasoning processes may be ver-\nified as correct, prematurely terminating the\npipeline and artificially increasing the success\nrate.\nIn contrast, closed-source models such as GPT-\n4 variants demonstrate much higher success rates.\nThe results are summarized in Table 10."}, {"title": "B Example of Table as Thought on TraverPlanner Task", "content": "B.1 Table as Thought w/ Given Schema\nQuery: Please draw up a 3-day travel itinerary for one person, beginning in Oakland and heading\nto Tucson from March 15th to March 17th, 2022, with a budget of $1,400.\nRaesoning table:\nB.2 Table as Thought w/ Self-designed Schema\nQuery: Please draw up a 3-day travel itinerary for one person, beginning in Oakland and heading\nto Tucson from March 15th to March 17th, 2022, with a budget of $1,400.\nRaesoning table:"}, {"title": "C Example of Table as Thought on Calendar Scheduling Task", "content": "C.1 Table as Thought w/ One Row Schema\nQuery: You need to schedule a meeting for Roy, Kathryn and Amy for half an hour between the\nwork hours of 9:00 to 17:00 on Monday.\nHere are the existing schedules for everyone during the day: Roy has blocked their calendar on\nMonday during 9:00 to 9:30, 10:00 to 10:30, 11:00 to 11:30, 12:30 to 13:00; Kathryn has meetings\non Monday during 9:30 to 10:00, 16:30 to 17:00; Amy has blocked their calendar on Monday\nduring 9:00 to 14:30, 15:00 to 16:00, 16:30 to 17:00;\nAmy would rather not meet on Monday after 15:30. Find a time that works for everyone's schedule\nand constraints.\nRaesoning table:\nC.2 Table as Thought w/ Multi Row Schema\nQuery: You need to schedule a meeting for Kathryn, Charlotte and Lauren for half an hour\nbetween the work hours of 9:00 to 17:00 on Monday.\nHere are the existing schedules for everyone during the day: Kathryn has blocked their calendar\non Monday during 9:00 to 9:30, 10:30 to 11:00, 11:30 to 12:00, 13:30 to 14:30, 16:30 to 17:00;\nCharlotte has blocked their calendar on Monday during 12:00 to 12:30, 16:00 to 16:30; Lauren has\nblocked their calendar on Monday during 9:00 to 10:00, 12:00 to 12:30, 13:30 to 14:30, 15:00 to\n16:00, 16:30 to 17:00;\nCharlotte do not want to meet on Monday after 13:30. Find a time that works for everyone's\nschedule and constraints.\nRaesoning table:"}, {"title": "D Comparison of Table as Thought and Direct Prompting on GSM-8k", "content": "D.1 Example where Direct Prompting succeceds while Table as Thought Fails\nQuery: Stephen placed an online order for groceries. His final bill came to $40.00. Because this\nwas through a delivery vendor, they tacked on a 25% fee to his final total and charged him $3.00\nin delivery fees. Stephen also added a $4.00 tip. After the extra fees, what was the final price of\nStephen's groceries?\nRaesoning table:\nDirect Prompting response:\nTo find the final price of Stephen's groceries after all the extra fees, we need to calculate each\ncomponent and add them together.\n1. Original Bill: Stephen's original grocery bill was $40.00.\n2. Vendor Fee: The vendor adds a 25% fee to the original bill. To calculate this fee:\nVendor Fee = 0.25 \\times 40.00 = 10.00\n3. Delivery Fee: The delivery fee is a flat $3.00.\n4. Tip: Stephen added a $4.00 tip.\nNow, we add all these amounts together to find the final price:\nFinal Price = Original Bill + Vendor Fee + Delivery Fee + Tip\nFinal Price = 40.00 + 10.00 +3.00 +4.00 = 57.00\nTherefore, the final price of Stephen's groceries, after all the extra fees, is $57.00.\nD.2 Example where Table as Thought succeceds while Direct Prompting Fails\nQuery: Jamal's phone can hold 6 times more photographs than can Brittany's phone. The\nmaximum number of photographs that Brittany's phone can hold is 50 times more than the number\nof birds in Jamal's photograph of the ducks at the zoo. If Jamal's phone can hold 1800 photographs,\nhow many ducks can be seen in Jamal's photograph of ducks at the zoo?\nRaesoning table:\nDirect Prompting response:\nTo solve the problem, we need to determine the number of ducks in Jamal's photograph.\nFirst, let's define the variables:\n\u2022 Let B be the maximum number of photographs Brittany's phone can hold.\n\u2022 Let D be the number of ducks in Jamal's photograph.\nAccording to the problem, Jamal's phone can hold 6 times more photographs than Brittany's phone.\nTherefore, we have the equation:\nJ = 6B\nwhere J is the number of photographs Jamal's phone can hold. We know from the problem that\nJ = 1800. Substituting this into the equation gives:\n1800 = 6B\nTo find B, divide both sides by 6:\nB = \\frac{1800}{6} = 300\nNext, the problem states that Brittany's phone can hold 50 more photographs than the number of\nducks in Jamal's photograph. This gives us the equation:\nB = D +50\nWe already found that B = 300. Substituting this into the equation gives:\n300 = D+50\nTo find D, subtract 50 from both sides:\nD = 300- 50 = 250\nThus, the number of ducks in Jamal's photograph is 250"}]}