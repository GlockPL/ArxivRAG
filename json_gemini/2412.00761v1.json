{"title": "Learning to Forget using Hypernetworks", "authors": ["Jose Miguel Lara Rangel", "Stefan Schoepf", "Jack Foster", "David Krueger", "Usman Anwar"], "abstract": "Machine unlearning is gaining increasing attention as a way to remove adversarial data poisoning attacks from already trained models and to comply with privacy and AI regulations. The objective is to unlearn the effect of undesired data from a trained model while maintaining performance on the remaining data. This paper introduces HyperForget, a novel machine unlearning framework that leverages hypernetworks \u2013 neural networks that generate parameters for other networks \u2013 to dynamically sample models that lack knowledge of targeted data while preserving essential capabilities. Leveraging diffusion models, we implement two Diffusion HyperForget Networks and used them to sample unlearned models in Proof-of- Concept experiments. The unlearned models obtained zero accuracy on the forget set, while preserving good accuracy on the retain sets, highlighting the potential of HyperForget for dynamic targeted data removal and a promising direction for developing adaptive machine unlearning algorithms.", "sections": [{"title": "1 Introduction", "content": "The need for machine learning (ML) models to forget specific points of their training data has become an essential requirement due to increasing security, ethical, and regulatory concerns in AI. Data forgetting is a critical defense mechanism against adversarial attacks that manipulate models to change their behavior or extract training data information from them [16, 41, 5, 39, 31, 4, 34]. Additionally, regulations like the GDPR with its Right-to-be-Forgotten (RTBF) or the EU AI Act enhance individuals' data privacy rights, allowing them to request the deletion of their data [51, 44, 11, 57]. As ML models capture information of their training data in their parameters, aligning them with ethical and regulatory standards requires not only to delete stored data but also to remove its influence on the parameters, which directly impacts the model's capabilities [2, 13].\nMachine unlearning (MU) focuses on developing algorithms capable of efficiently and effectively removing the influence of specific data on an ML model, while maintaining unrelated knowledge or capabilities unaffected [1, 28, 2]. Ideally, an unlearned model should behave identically to a model that was never trained on the data being unlearned in the first place. Thus, for randomly initialized models, exact unlearning is achieved when the distribution of unlearned models is identical to the distribution of models trained on the dataset D excluding the forget set $D_f \\subseteq D$, either by equating their distribution of parameters or outputs [3, 14, 35, 24, 49]. The gold standard for exact unlearning is retraining the model from scratch without the forget set, which is costly in time and resources as it requires full access to the training dataset and must be done for each forgetting request.\nConsequently, research has focused on the development of approximate unlearning methods to mitigate the retraining drawbacks [18, 8, 28, 35, 43]. An ideal unlearning algorithm should be consistent with the retrained model outputs, preserve as much performance as possible on the retain set $D_r = D \\backslash D_f$, be faster than retraining, provide guarantees of effective removal of $D_f$ influence, be lightweight, scalable, and avoid recovering unlearned data in strict compliance scenarios. Achieving"}, {"title": "2 Related work", "content": "Hypernetworks. A neural network $G(C; \\theta_G)$ with learnable parameters $\\theta_G$ and context input $C$ is a hypernetwork if its output are parameters for another network $F(X; \\theta_F)$ that solves a task $T$ with associated data $D = {X, Y}$, i.e. $\\theta_F = G(C;\\theta_G)$. Therefore, in this framework $\\theta_G$ are optimized to use $G$ to sample parameters $\\theta_F$ that can be used by $F$ to process $X$ and predict $Y^*$ for task $T$ [6, 20].\nThis approach can reduce the parameter search space and offers adaptability as, unlike conventional parameters that remain fixed after training, it can be conditioned to sample parameters for multiple related tasks [20, 30, 36]. It has shown promise in continual learning, transfer learning, weight pruning, and uncertainty-aware modeling [53, 6, 52, 25, 23]. However, challenges limit broader adoption, such as scalability, lack of understanding of representational capacity, learning dynamics, generalization, and ensuring parameters are valid and not merely memorized or interpolated [6, 26, 25].\nDiffusion Hypernetworks. Similar to diffusion models [46, 22], neural networks training with SGD transforms randomly initialized parameters (noise) into a structured set that forms a specific dis- tribution. This inspired the use of diffusion as hypernetworks, what we call Diffusion Hypernetworks (DiHy). While this is still an emerging area of research, early approaches like G.pt [37] show that diffusion models have significant potential to act as hypernetworks. G.pt acts as a learned optimizer. By using a diffusion transformer (DiT) and DDPM [38, 22] it takes current parameters (potentially random noise) with their associated performance and target performance, and update them to obtain the desired performance, indirectly solving optimization problems traditionally handled by SGD. G.pt has demonstrated broader generative properties than other approaches, although it is less precise in achieving target metrics, requires big data sets constructed by saving checkpoints from multiple main network training runs, involves considerable training overhead, and shows limited capabilities to extrapolate to performances beyond its training data [54, 37]. However, despite these limitations, we show that its ability to learn a conditional generative model over weights of the main network is useful towards MU. We replicate some experiments related to G.pt (originally reported in [37]) and list some observations that might be of independent interest. See Appendix A.6.\nFurthermore, [10] also used DiT and DDPM to generate unconditioned parameters, [54] improved computational efficiency by incorporating an autoencoder and U-Net for learning on a parameters latent space. Both works used datasets with only optimized parameters, effectively introducing a soft bias for high-performing parameters generation, but limiting capabilities for more diverse parameter distributions. Recently, [27] extended [37] ideas to text-to-model generation, where a customized classifier for a subset of classes of a large dataset can be sampled by prompting text."}, {"title": "3 Proposed method", "content": "We propose HyperForget \u2013 a hypernetwork based framework for unlearning in which a hypernetwork is trained to learn a distribution over model parameters. The hypernetwork is then used to sample model parameters that solve a forgetting task U. Thus, the resulting model, a HyperForget Network (HyFo), can generate parameters that simultaneously yield high-performance on the retain set $D_r$ and low-performance on the forget set $D_f$ associated to task U. Indeed, it reconstructs the knowledge and capabilities for $D_r$, while effectively forgetting $D_f$, achieving this efficiently in a single forward pass, eliminating gradient calculations typically required by other unlearning methods, and allowing for dynamic unlearning adaptation in scenarios with frequent and varying forget requests.\nIn this work, we use DiT as the hypernetwork. We call the resulting model Diffusion HyperForget Network (DiHyFo). Integrating diffusion models into the HyperForget framework provides a struc- tured approach to gradual unlearning and allows to control the level of data influence removal. We explored two options to construct a DiHyFo for class-level unlearning.\nConsider a classification task $T$ with a dataset $D$ formed by examples $x_i \\in X$ with associated labels $y_i \\in \\{1, ..., m\\}$, $m \\in N$, drawn from an unknown distribution $P$. A model $F$ trained to solve $T$ is requested to forget a subset of classes, i.e., $D_f$ contains data from specific class labels. For this task, we employ a DiHyFo conditioned on class losses to generate parameters that obtain high performance in $D_r$ and low performance in $D_f$."}, {"title": "4 Experimental Setup", "content": "Checkpoints datasets. To reduce the complexity of the task, we consider a simplified scenario with $m$ classes, out of which $r$ classes are considered as 'pivot' classes. For pivot classes, we only consider high-performing parameters. For the remaining $m - r$ class losses, DiHyFo models are trained on parameters with varied values of losses. In general, any data that a model designer is certain would not need to be forgotten later can act as a pivot, simplifying the learning problem. The"}, {"title": "5 Results and Discussion", "content": "Generative performance. We measure the capabilities of each model to generate parameters with the desired loss by computing the prompt alignment [37] and correlation between the target loss and the actual loss obtained with the sampled parameters. Both these metrics show initial fluctuations but tend to increase and stabilize trough training. As depicted in Figure 2, although the observed losses generally align, it is challenging for the model to precisely match the target losses, particularly for higher loss values (see Appendix A.2 for additional results). Similarly, the observed losses for pivots track the target losses, successfully retaining performance but struggling with high losses and jumps between levels, as expected with the dataset structure. As we focus on low-loss regions for pivots, these deviations are less relevant for our evaluations.\nLearning bidirectional loss movements at a class level is a challenging task, leading to less precise prompt alignment than in other tasks [37]. However, this is not a major issue as unlearning typically focuses on high and low-performing parameters rather than covering the entire loss range. While"}, {"title": "6 Limitations and Future Work", "content": "In this work, we have provided proof-of-concept experiments for a hypernetworks based approach to unlearning. While our results indicate the potential of this approach, many opportunities for future work exist. The current formulation of HyperForget focuses on full-class unlearning, its extension and potential applicability for other unlearning tasks have yet to be explored. A key limitation of this approach is that the generative model retains the knowledge of forget sets making this approach unsuitable for strict unlearning-for-privacy applications. Furthermore, it is not clear from our experiments whether the proposed approach is scalable or not. The use of diffusion models adds significant computational overhead, and it inherits concerns from DiHy models, such as limited generalization [37, 54]. Additionally, although generated parameters generally approximate target losses, it is difficult for the model to precisely match the target loss.\nFuture works could consider evaluating our framework on other datasets, e.g., CIFAR-10 or mini- ImageNet, as well as improving the scalability of our approach. Future research could explore alternative architectures, improve checkpoint saving strategies, optimize the process and compo- nents, or explore other unlearning tasks. Improvements may come from leveraging model zoos, hypernetworks for architecture-agnostic parameter generation, and learning on latent spaces. Both hypernetworks and MU are in an early stage, and we hope this work inspires future research to expand these areas, contributing to more adaptive, secure, and ethical AI solutions."}, {"title": "A Appendix / supplemental material", "content": "This appendix provides additional information on the technical details and methods behind the proposed hypernetwork-based approach to MU and DiHyFo models. It includes explanations on data generation, experimental results, model definitions, and model evaluations.\nThe appendix is organized as follows:\n\u2022 Section A.1 Additional unlearning evaluation results: This section includes further results and analysis of unlearning capabilities.\n\u2022 Section A.2 Additional training results: We provide supplementary training results, dis- cussing model performance, stability, and consistency across experiments to support the findings of the main study.\n\u2022 Section A.3 Additional hypernetworks, HyperForget, and DiHyFo details: This section provides details on hypernetworks and how they are used for HyperForget and DiHyFo models.\n\u2022 Section A.4 Additional details on evaluation metrics: This section outlines the metrics and evaluation methods used to assess the generative performance and unlearning effectiveness of the models.\n\u2022 Section A.5 Additional details on datasets generation: This section describes the method- ologies for dataset generation and processing, including parameter checkpoint collection and strategies for class loss tracking across different configurations.\n\u2022 Section A.6 Some experimental observations on G.pt: This section presents observations and insights from experiments conducted with the G.pt model, examining performance metrics and prompt alignment under various test conditions."}, {"title": "A.1 Additional unlearning evaluation results", "content": "To apply the DiHyFo models for unlearning, we sample multiple parameters prompting different high losses values for different forget sets simultaneously with low losses for the corresponding retain sets. The generated parameters are used to load an instance of the main network, which is then evaluated on a test set. We save the sampled model that obtains the lowest average accuracy on the forget set while obtaining the highest possible average accuracy on the retain set (best unlearned model). We use accuracy for the selection and evaluation as it is a more interpretable performance measure than loss.\nWe sampled models that forget classes {2} and {2,3} for MNIST-4, and {2}, {2,3,4}, and {0,1,2,3,4} for MNIST. Figures 3 and 4 exemplify this sampling and selection process. Tables 3 and 4 present the evaluation metrics on MNIST-4, showing a close behavior of the sampled unlearned models to the retrained models.\nTaking as reference the predictions of the retrained model, we also compare the output spaces using the confusion matrix between the sampled models and the corresponding retrained model on the retain and complete test set. Figures 5, 6, 7, and 8 present the results for DiHyFo-1 and DiHyFo-2 on MNIST-4 and MNIST. The forget set is not included in this comparison as it mainly contains matrices with zeros across all entries. This serves as a visual confirmation of commented findings using the evaluation metrics, obtaining zeroes in the forget set classes.\nOverall, the evaluations suggest that the presented DiHyFo models accomplish some relevant desired conditions for an unlearning algorithm [35, 58, 14, 8, 48, 19, 56, 32, 17, 21, 2, 45]. They were consistent, timely at sampling unlearned models, provided guarantees and verification of unlearning, but were model intrinsic, with potential recoverability of knowledge, and had issues with scalability."}, {"title": "A.2 Additional training results", "content": "Learning curves for both models decrease until convergence and show a similar behavior during training and testing, Figures 9 and 10.\nMost experiments presented positive prompt alignment scores, indicating that the generated parame- ters obtain class losses that closely align with the desired class losses. However, there are cases in which even when the prompt alignment increases and stabilizes it does not achieve to surpass zero, contrasting with what is shown in the correlation and direct comparison plots. As illustrated in Table 5, in several cases this behavior is due to some generated models having high negative scores that pull the average down, suggesting that the performance of the models as a group is quite varied, with most models performing good but some models performing quite poorly in terms of generating parameters for the desired losses.\nOn the other hand, most models show high correlation between observed and desired losses, even for models with a low prompt alignment. This suggests that while the models may not always match the exact value of the losses, they generally align with the appropriate loss direction.\nResults in Table 5 were computed using the sampled models with DiHyFo-2 on MNIST-4 during the last epoch, and can be contrasted with the corresponding direct comparison plot. The general loss direction alignment does not always translate into accurate loss matching. Considering the extreme negative values as outliers or setting a lower bound of 0 on the prompt alignment makes it resemble more closely the correlation results. Indeed, the models are capturing the direction of the relationship well but might struggle with the precision of their predictions."}, {"title": "A.3 Additional hypernetworks, HyperForget, and DiHyFo details", "content": "Consider a dataset $D = \\{X, Y\\}$ associated with a task $T$. In the classical deep learning framework the learnable parameters $\\theta_F$ of a neural network $F(X;\\theta_F)$ are obtained by solving the optimization problem in Equation 1 [6].\nThe searching for the optimal configuration is performed within large search spaces formed by millions of potential parameters. The input samples $x \\in X$ are passed through the layers of $F$ to obtain predictions $y^* \\in Y^*$, later compared with the true labels $y \\in Y$ using a loss function $L(Y, Y^*)$, which is optimized by updating parameters until convergence.\n$$\\min_{\\theta_F} F(X;\\theta_F)$$\nInstead of directly optimizing the parameters of the main network, the hypernetwork framework, depicted in Figure 19, uses a separate network to learn to generate the parameters for the main network. Both networks are usually trained in an end-to-end differentiable manner [6, 20].\nDefinition A.1 Hypernetwork. A neural network $G(C; \\theta_G)$ is called a hypernetwork with learnable parameters $\\theta_G$ and context input $C$ if its output are parameters for a second neural network $F(X;\\theta_F)$ that solves a task $T$ with associated data $D = \\{X, Y\\}$, i.e. $\\theta_F = G(C;\\theta_G)$.\nThe context input $C$ for the hypernetwork contains information about the structure of the parameters of the main network that enables learning to generate its parameters.\nDuring the forward pass at training time, the parameters $\\theta$ are generated by passing $C$ through $G$, and serve as input to $F$, which then process $x \\in X$ and obtains predictions $y^* \\in Y^*$. The loss $L(Y, Y^*)$ is then computed and during the backward pass, the error is back-propagated through $G$ with the gradients of $L$ computed with respect to $\\theta_G$. Consequently, $\\theta_G$ are optimized to generate the $\\theta_F$ that best solves task $T$. This introduces the optimization problem in Equation 2 [6].\nAt test time, new parameters $\\theta_F$ can be sampled from the optimized hypernetwork and be used to make predictions with $F(X; \\theta_F)$ on the test data.\n$$\\min_{\\theta_G} F(X; G(C;\\theta_G))$$\nFor a classification task $T$ with training data $D = \\{X,Y\\}$, solved with a learning algorithm $F(X;\\theta_F)$, the associated unlearning task with forget set $D_f \\subseteq D$ and retain set $D_r = D\\backslash D_f$ is solved by constructing an unlearned model $U(D, D_f, F)$ that is expected to perform equivalently or similarly to a model that has been trained without the forget set, $F(X \\subseteq D\\backslash D_f; \\theta_F)$. To forget specific subsets of classes, the associated machine unlearning objective can be described by Equation"}, {"title": "A.4 Additional details on evaluation metrics", "content": "Each model must be evaluated on two key aspects. First, their generative properties as DiHy need to be assessed. This involves verifying whether the models can generate appropriate parameters for the main network, enabling it to approximate targeted performance levels on the classification task for each class.\nBoth DiHyFo models use MSE as training metric, and we analyze the corresponding learning curves to understand the models' learning behavior and convergence, and identify issues like overfitting or underfitting.\nDuring evaluation, the model is prompted with a value close to the best loss found in the training dataset. It is noted in [37] that using a value slightly above or below the best loss can sometimes yield better results for certain tasks. To evaluate whether the generated parameters effectively approximate prompted loss values, we use the prompt alignment metric defined by [37] as the $R^2$ score between the obtained loss and the target loss in Equation 6, averaged over the batch size.\n$$R^2 = 1 - \\frac{\\sum_{i=1}^n (C_i - \\hat{C_i})^2}{\\sum_{i=1}^n (C_i - \\eta)^2 + \\epsilon}$$\nwhere $\\epsilon$ is a small constant to avoid division by zero, and $\\eta$ is the mean of the targets:\n$$\\eta = \\frac{1}{n} \\sum_{i=1}^n C_i$$\nA score close to 1 indicates that the obtained loss closely aligns with the target loss. Negative values suggest worse alignment than the mean of the target values. We compute this score separately for each class to assess the model's ability to control losses across different classes simultaneously. And it is computed over 20 regularly-sampled prompts and averaged over multiple neural networks sampled with each DiHyFo, using randomly-initialized input parameters.\nThis metric is effective for evaluating conditional generative tasks, such as those in the learning-to- learn framework ([37]), as it measures how well the model aligns the observed losses with the target losses in terms of both direction and magnitude. However, our training datasets exhibit bidirectional loss movements, which can affect how the model learns to align the losses. In our experiments, we found that observed losses generally aligned correctly with the direction of the prompted losses, but not always in magnitude, and sometimes some generated parameters have drastic undesired performances that significantly drop the prompt alignment in comparison with the common behavior of the generated parameters. Thus, we compute the Pearson correlation between observed and target losses to assess if the model is accurately tracking the target, providing a complementary metric to the prompt alignment score.\nAdditionally, we directly compare the observed versus target losses by plotting them along the identity line (indicating perfect alignment) and include a lower performance bound\u2014often set to the median or average of losses from checkpoint collection\u2014as a reference for high performance."}, {"title": "A.5 Additional details on datasets generation", "content": "Checkpoints of parameters and associated class losses are collected during multiple training runs of an MLP on MNIST. Since a simple MLP achieves good results on MNIST early in training, we"}, {"title": "A.6 Some experimental observations on G.pt", "content": "Our experiments suggest that, while G.pt can learn to generate parameters using loss, prediction error, or accuracy, it is generally easier to learn with loss. Losses values typically span a narrower range"}]}