{"title": "A Reliable Common-Sense Reasoning Socialbot\nBuilt Using LLMs and Goal-Directed ASP.", "authors": ["ZENG, YANK\u0391\u0399\u00b9", "RAJASHEKHARAN, ABHIRAMON\u00b9", "BASU, KINJAL2", "WANG, HUADUO\u00b9", "ARIAS, JOAQU\u00cdN\u00b3", "GUPTA, GOPAL\u00b9"], "abstract": "The development of large language models (LLMs), such as GPT, has enabled the construction of several\nsocialbots, like ChatGPT, that are receiving a lot of attention for their ability to simulate a human conver-\nsation. However, the conversation is not guided by a goal and is hard to control. In addition, because LLMs\nrely more on pattern recognition than deductive reasoning, they can give confusing answers and have diffi-\nculty integrating multiple topics into a cohesive response. These limitations often lead the LLM to deviate\nfrom the main topic to keep the conversation interesting. We propose AutoCompanion, a socialbot that uses\nan LLM model to translate natural language into predicates (and vice versa) and employs commonsense\nreasoning based on Answer Set Programming (ASP) to hold a social conversation with a human. In par-\nticular, we rely on s(CASP), a goal-directed implementation of ASP as the backend. This paper presents\nthe framework design and how an LLM is used to parse user messages and generate a response from the\ns(CASP) engine output. To validate our proposal, we describe (real) conversations in which the chatbot's\ngoal is to keep the user entertained by talking about movies and books, and s(CASP) ensures (i) correct-\nness of answers, (ii) coherence (and precision) during the conversation\u2014which it dynamically regulates to\nachieve its specific purpose\u2014and (iii) no deviation from the main topic.", "sections": [{"title": "1 Introduction", "content": "Conversational agents are designed to understand dialogs and generate meaningful responses to\ncommunicate with humans. After the popularity of ChatGPT, with its surprising performance\nand powerful conversational ability, commercial Large Language Models (LLMs) for general\nNLP tasks such as GPT-4 by Achiam et al. (2023), Gemini from Anil et al. (2023), etc., sprung\nup and brought the generative AI as a solution to the public view. These LLMs work quite well\nin content generation tasks, but their deficiency in knowledge-and-reasoning-oriented tasks are\nwell-established by now, according to Nezhurina et al. (2024). These models themselves cannot\ntell whether the text they generate is based on facts or made-up stories, and they cannot always\nfollow the given data and rules strictly and sometimes even modify the data at will, also called\nhallucination. The reasoning that these LLMs appear to perform is also at a very shallow level.\nThese are serious flaws that make the LLMs unsuitable for fact-based conversations such as\nproviding correct information to a user. The reason for this flaw is that LLMs generate text that\nis purely based on a pattern-matching mechanism, and consequently have no understanding of\nthe meaning of sentences and thus a safe and sound way of reasoning is not guaranteed, as stated\nby Nezhurina et al. (2024). In contrast, humans understand the meaning of sentences and then\nuse their reasoning capabilities to check for consistency and take further action. Thus, to make\nthe machine-generated response reliable and consistent, our socialbot needs to follow a similar\napproach.\nFollowing the above insights, in this paper, we report on developing an elaborate conversa-\ntional socialbot that can understand human dialog and respond properly according to human\nexpectations. The goal of this socialbot is to be able to engage in multiple rounds of social con-\nversations with users about their interests, understand the context of what the user is saying like a\nhuman, and be able to initiate new topics or make relevant recommendations at the right time to\ncontrol the pace of the chat. We currently focus our topics of interest on discussions of movies,\nbooks, and related people, and use the STAR framework by Rajasekharan et al. (2023), which has\nbeen successfully applied in the task-oriented chatbot domain (e.g., AutoConcierge from Zeng\net al. (2024)), to build a social conversational bot called AutoCompanion.\u00b9 AutoCompanion uses\nan LLM to interact with the user and to translate between natural language dialogs and knowl-\nedge represented in predicates. It further uses an ASP system for reasoning over this knowledge.\nAfter the user's input is parsed into predicates by the LLM, the ASP reasoner uses reasoning\n(and a bit of randomness) to decide whether to continue with the current topic of conversation,\nmove on to a related topic, or recommend books or movies that are currently on sale based on the\nuser's preferences. Questions asked by the user are also answered. Subsequently, the instructions\nfor the next step from the reasoner are passed on to another LLM in the form of an open-ended\nfree-for-all on that topic, and the generated sentence is provided to the user as a reply. We believe\nthat the use of LLM should be controlled to avoid its misuse in fact-based domains and that the\nbest way to utilize LLM is to use it only as an interface for parsing and presenting knowledge.\nOur AutoCompanion outperforms LLM-only socialbots in reliability, scalability, controllability,\ncreativity, and many other aspects.\nOur contribution in this paper are as follows. First, We developed a general socialbot frame-\nwork that allows the bot to control the conversation flow and react reliably according to the user\ninput, attributed to leveraging both the LLM and s(CASP), as described in Section 3. Section 3.1\nshows the predicates we extract from natural language sentences, Section 3.2 explains how the\nreasoner works to add control and consistency to the system, and Section 3.3 provides the de-\ntails to the response generation stage. Second, We built the AutoCompanion based on the above\nframework with knowldge about books and movies, as described in Section 4. Section 5 displays\na short conversation between our AutoCompanion and the user. Finally, this paper also discusses\nthe usage and performance (in Section 4.3 and 7) of LLMs on the similar tasks."}, {"title": "2 Background and Related Work", "content": "This section introduces s(CASP), the reasoner used to control the conversations. It also presents\nstate-of-the-art Chatbots and discusses their limitations."}, {"title": "2.1 Answer Set Programming and s(CASP)", "content": "Answer Set Programming (ASP), according to Brewka et al. (2011) and Gelfond and Kahl\n(2014), is a logic programming paradigm suited for knowledge representation and reasoning\nthat facilitates commonsense reasoning. The s(CASP) system proposed by Arias et al. (2018), is\nan answer set programming system that supports predicates, constraints over non-ground vari-\nables, uninterpreted functions, and, most importantly, a top-down, query-driven execution strat-\negy. These features make it possible to return answers with non-ground variables (possibly in-\ncluding constraints among them) and compute partial models by returning only the fragment of\na stable model that is necessary to support the answer.\nComplex commonsense knowledge can be represented in ASP and the s(CASP) query-driven\npredicate ASP system can be used for querying it, as shown in the work done by Chen et al.\n(2016) and Xu et al. (2023). Commonsense knowledge can be emulated using (i) default rules,\n(ii) integrity constraints, and (iii) multiple possible worlds, according to Gelfond and Kahl (2014)\nand Gupta (2022). Default rules are used for jumping to a conclusion in the absence of exceptions,\ne.g., a bird normally flies, unless it's a penguin.\n```latex\n1\nflies(X) :- bird(X), not abnormal_bird(X).\n2\nabnormal_bird (X) :- penguin(X).\n```\nIntegrity constraints allow us to express impossible situations and invariants. For example, a\nperson cannot be dead and alive at the same time.\n```latex\n1\nfalse :- person(X), dead(X), alive(X).\n```\nFinally, multiple possible worlds allow us to construct alternative universes that may have some\nparts common but other parts inconsistent. For example, the cartoon world of children's books\nhas a lot in common with the real world (e.g., birds can fly in both worlds), yet in the former\nbirds can talk like humans but in the latter they cannot.\nA large number of commonsense reasoning applications have already been developed using\nASP and the s(CASP) system: Sartor et al. (2022); Chen et al. (2016); Xu et al. (2023). Justifi-\ncation for each response can also be given as the s(CASP) system can generate justifications for\nsuccessful queries as proof trees as shown by Arias et al. (2020)."}, {"title": "2.2 State-of-the-art Chatbots and Their Limitations", "content": "Conversational agents (chatbots) have been an active area of research for a long time. Rule-based\nor finite-state-based systems, like Eliza by Weizenbaum (1966), Chat-80 by Warren and Pereira\n(1982), and PARRY by Colby et al. (1971), encode the mapping of user commands to ontology\nusing rules and state transitions. The Conversational Knowledge Template (CKT) approach pro-\nposed by Basu et al. (2021b) enables the system to control the dialog flow and change topics.\nHere, the main challenge is parsing the dialogs and correctly extracting the knowledge conveyed\nin the dialogs.\nUntil recently, transformer-based Large Language Models, pre-trained on an enormous quan-\ntity of well-annotated data, have been applied to general NLP tasks. With the advent of Large\nLanguage Models, the paradigm changed from pre-training and fine-tuning (Casola et al. (2022))\nto teaching a language model any arbitrary task using just a few demonstrations, called in-context\nlearning, a method of prompt engineering. Brown et al. (2020) introduced an LLM called GPT-3\ncontaining approximately 175 billion parameters that have been trained on a massive corpus of\nfiltered online text, on which the well-known ChatGPT is based. GPT-3 and its successor GPT-4\ncan perform competitively on several tasks such as question-answering, semantic parsing, and\nmachine translation, as noted by Achiam et al. (2023). However, such LLMs lack the ability of\nmathematical reasoning and find it hard to overcome the hallucination brought from the training\ndata, as shown by Floridi and Chiriatti (2020); Wei et al. (2022); Nezhurina et al. (2024).\nRetrieval Augmented Generation (RAG) by Lewis et al. (2020) is proposed and widely used\nto mitigate the deficiencies mentioned above by retrieving the relevant materials using similar-\nity matching of content embedded as vectors by a transformer-based model. Recent efforts by\nKulkarni et al. (2024); Quidwai and Lagana (2024), are trying to leverage RAG for building\nchatbots, but none of them engages an explicit reasoning system.\nFinally, Our AutoCompanion is an extension of our previous work developing NLU systems\nbased on commonsense reasoning (Basu et al. (2021b); Li et al. (2021); Zeng et al. (2024)).\nOur group has been dedicated to building socialbots, specifically addressing the Amazon (2021)\nAlexa Socialbot Challenge for years. GPT-4 with in-context learning as a semantic parser leads\nto a significant advantage over our previous socialbots and helped this framework succeed, as we\nreport in this paper."}, {"title": "3 Socialbot Design", "content": "The AutoCompanion is designed to model the human thought process during conversations.\nWhen humans hear a sentence, they parse it to extract its meaning and represent the meaning in\ntheir minds as knowledge. Humans will check for consistency and correctness of this knowledge\nusing additional (commonsense) knowledge, that also resides in their mind, to find any gaps in the\ncaptured knowledge wrt their goal. If human reasoning reveals gaps in this knowledge, they may\nask further questions to cover this gap. Once the knowledge is (reasonably) complete, humans\nwill draw a conclusion. This conclusion\u2014that also represents knowledge\u2014will be translated into\na natural language response. Therefore, similar to humans, when AutoCompanion receives a user\nsentence, it will also process it using three stages: First, it will parse the natural language input to\nknowledge, which is done by LLMs reliably. Next, it will check the consistency and correctness\nof the input knowledge and use the knowledge extracted from the dialog, in conjunction with\ncommonsense knowledge it already possesses, to reason out the next action. The ASP reasoning\nsystem is used to implement this capability. Finally, the knowledge representing the next step\n(conclusion) is converted into a natural language response by another invocation of the LLM and\ncommunicated.\nThis mechanism is embodied and known as the STAR framework by Rajasekharan et al.\n(2023), which leverages the advantages of LLMs and ASP systems by combining them sys-\ntematically. It parses the semantics of the text sentences to generate the predicates using LLMs\nsuch as GPT 3.5 or GPT 4. After that, it sends the predicates to the ASP system to get reliable an-\nswers through reasoning. This framework was previously applied to task-oriented chatbots such\nas AutoConcierge by Zeng et al. (2024) and has proved to work well. Indeed, LLMs do not parse\nthe sentences to knowledge with 100% accuracy, but LLMs have proven to be the most effective\nsemantic parsers by far. Prior to LLMs, Basu et al. (2021a) used parsers such as the Stanford\nCoreNLP parser and the spaCy parser, and those methods were largely ineffective in extracting\nsemantics. Besides, the wrong predicate generated by LLMs can often be checked by additional\nlogic written in ASP.\nUnlike task-oriented chatbots, socialbots are not set to complete a task. Hence, they can chat\nwithout a goal, until whenever the user likes to end the conversation. This requires AutoCom-"}, {"title": "3.1 LLM Parser and Ontology", "content": "AutoCompanion leverages LLMs such as the GPT series to parse the natural language sentences\nto knowledge. The prompts are given to LLMs including the knowledge ontology, which is rep-\nresented in the predicates used in the reasoning part, and several examples covering the use cases.\nTopic. The topic describes a subject to talk about in a social conversation. A socialbot may cover\nmany different topics when chatting with the users, such as movies, books, sports, pets, etc.,\nwhile a certain movie, book, sport, or pet discussed is called the topic instance. AutoCompanion\nwill mainly cover movies, books, and relevant people.\nProperty. A property indicates an aspect of the topic instance to be potentially discussed. The\nlist of properties is fixed for each topic and covers all possible directions that can be used.\nTheme. A theme is a set of predicates centralized on one property of a topic instance. It typ-\nically contains one talk predicate: \u201ctalk(Topic, Instance_name, Property).\u201d, one attitude predi-\ncate: \"attitude(positive/negative/ask/argue).\", and one content predicate: \u201ccontent(Property, De-\ntailed_content).\u201d. If the user asks a question, the predicate question: \u201cquestion(What_is_asked)\u201d\nis also captured. For instance, when the user compliments the scene of the movie Titanic where\nthe giant ship is sinking, the predicates extracted from the user would be:\n```latex\n1\ntalk(movie, 'Titanic', scene). content(scene, sinking). attitude (positive).\n```\nPreference. The user preference is also extracted to be used later for recommendation. The\npreference is presented by the prefer predicate: \u201cprefer(Topic, Property, Preference).\u201d.\nIn the parsing stage, the prompts are given to extract all themes and preferences a user input\nis about, where different themes and preferences are separated by \u2018###'. LLMs also extract the\nexception predicates such as 'quit' and 'irrelevant' at this stage. The prompt is given following\nthe below template (only part of the prompts are shown due to space limitation):\nNow you are an expert about books and movies. Now you should use your\nprofession to extract predicates from sentence input...\nThe predicates are below:\ntalk(Topic, Name, Property) %% The topic can be movie or book or people, ...\nExamples:\nSentence 1 -> theme 1 %%% theme 2 ...\nInput sentence ->"}, {"title": "3.2 Topic Control", "content": "We leverage an ASP reasoner to guide and switch between the topics. Predicates from the LLM\nparser are separated into themes and preferences that are recorded and updated, and one theme\nis randomly picked to respond with. The remaining themes will be used in later rounds where\nnothing is captured from the user's input. Then, the ASP reasoner chooses whether to remain in\nthe same topic instance or switch to a relevant one by the below strategy: if the instance is first\nmentioned, the reasoner maintains it for one more round; otherwise, a random function is applied\nto decide whether to continue the current instance, the current property, or start a relevant one.\nThe maintaining and shifting can be well described by two separate mechanisms, CKT and RCC.\nConversational Knowledge Template (CKT) by Basu et al. (2021b) is used when the ASP\nreasoner continues the current topic instance. It maintains a state with a list of properties that can\nbe discussed for each topic instance. Once the reasoner chooses to go deeper into one instance\nusing other properties, the CKT will be triggered to pick up one property that has not been\ndiscussed yet for this topic instance. On the other hand, the CKT can also choose to remain\non the current property, which will lead the LLM response generator (see Section 3.3) to tell\nsomething new about this property (e.g., depict another plot, etc.).\nCKT also controls AutoCompanion's attitude about an instance and its properties. Four kinds\nof attitudes are used in AutoCompanion: positive, negative, ask, and acknowledge. For a new"}, {"title": "3.3 Response Generation", "content": "Due to the lack of detailed information about each movie (e.g. for plots, lines, etc.), book (e.g. for\nplots, characters, etc.), and person (e.g. for skills, appearance, etc.), our AutoCompanion leaves\nthe generation of these specifics to LLM, controlling only the topic and property for each round.\nWe assume that the LLMs are experts in common topics such as famous movies and books, which\nmakes it possible for us to leverage this advantage of LLMs in constructing AutoCompanion.\nOnce a comprehensive knowledge base is built for these topics, control over the specific content\nlevel can also be exerted.\nWhile leveraging the LLMs for content generation, a proper prompt makes the output closer\nto the expected performance. In our designed prompt, we first describe the conversation situation\nand the LLM's role and define the response tone and length. Then we provide the LLM with\nsome examples and ask it to generate the content following the format of the examples. The\nexamples always start with a question with the template:\nWhat are the most interesting [property] for [topic] (instance]? [attitude] ->\nAnd LLM would generate an answer for that. For example, If we ask about the value expressed\nin Batman Begin:\nWhat are the most interesting value expressed for Batman Begins? positive ->\nThe LLM would generate:\nI feel like Batman Begins nails it with the whole inner battle vibe. It's not\njust about taking down bad guys; it's Bruce Wayne wrestling with his own demons,\nand that's what makes it hit home. Do you have the same feeling as me?\nIf the LLM cannot find any specific content that supports the given attitude, we allow the LLM\nto oppose its attitude toward the content, and this will be captured by the topic controller.\nThe function of the \u201cattitude\" predicate is to help LLMs generate some alternative perspec-\ntives other than the mainstream ones. During the AutoCompanion construction, we found out\nthat LLMs seldom create a view that strays too much from the common views of the public.\nHowever, for an ordinary human, not all of their opinions may match the mainstream. Therefore,\nrandomness should be added to the attitude to help LLMs generate \u201csurprising\u201d responses.\nIf the RCC mechanism is invoked, the response generator will also create cohesive sentences\nthat naturally mention the next topic instance to discuss. The template for a cohesive sentence is:\n\"Because you mentioned [source_topic_instance], it makes me think of the\n[next_topic] [next_topic_instance), since [relation_of_them].\"\nSimilarly, if the reasoner answers the questions from the user, one of the following templates will\nbe used according to the condition:\""}, {"title": "4 Implementation", "content": "AutoCompanion is constructed in Python with the s(CASP) goal-directed ASP Reasoning En-\ngine (Arias et al. (2018)). GPT-4 is chosen as the LLM in both the LLM Parser and Response\nGeneration stages."}, {"title": "4.1 Data Collection", "content": "We deployed our AutoCompanion to discuss movies, books, and the people involved.\nMovies: The movie data is from IMDb dataset, where we collected the information of release\nyear, runtime, rating, countries involved, languages, genres, top 10 casts with their characters,\ndirectors, writers, editors, composers, producers, cinematographers, and the plot summary. The\nabove information is then marked as properties for answering users' questions and linking the\nmovies to related people. Other than those above, the properties of the movie topic include plot\nepisodes, scenes, lines, costumes, awards, music, values expressed, characterization, cinematog-\nraphy, technique, actor performance, adaption, and social impact, which are out of the knowledge\nbase but are likely to be mentioned by the users. We store the database locally with a selection\nof 500 top-rated movies, and the 500 most popular movies since 2010, resulting in 931 movies.\nBooks: They were collected from the Users-Books-Dataset, including 528 books with informa-\ntion about their series, author, rating, language, genres, awards, setting places, characters, and\nplot description. These are also recorded as properties along with their writing style, symbolism,\nemotional impact, and social background.\nPeople: The data of the people associated with the movies are also extracted from the IMDb\ndatabase and stored locally. The people topic contains properties of birth year, death year, pro-\nfession, and representative work, whereas the skills, awards, appearance, and personal life re-\nquire an LLM to generate. We extracted all people linked to the selected movies, resulting in a\nknowledge base of 5625 data. The authors in the book's knowledge base are also matched to the\ncorresponding movie people.\nIdentifying in-theater movies and bestsellers in bookstores is also required for recommenda-\ntion mode. We leveraged IMDb's website for movie in-theater by providing location information,\nand chose USA Today's Best-Selling Booklist as our best-selling book data and extract further\ndetails from Open Library. The knowledge from the recommendation knowledge base is used"}, {"title": "4.2 ASP Coding", "content": "Below are several coding examples of the ASP implementation of the reasoning part of our\nAutoCompanion. In this part, we will elaborate on how the CKT and topic switching work in our\ns(CASP) program.\nCKT implementation. The CKT component chooses what property to discuss under a topic.\nThe code for CKT is shown below:\n```latex\n1\nnext_attr(Topic, Name, Attr) :- continue_attr, talk(Topic, Name, Attr),\n2\nresponse_attr(Topic, Attr).\n3\nnext_attr(Topic, Name, Attr) :- response_attr(Topic, Attr),\n4\nnot neg_next_attr(Topic, Name, Attr).\n5\nneg_next_attr(Topic, Name, Attr) :- not next_attr(Topic, Name, Attr).\n6\nneg_next_attr(Topic, Name, Attr) :- hist(I, Topic, Name, Attr, Attitude, From).\n```\nThe first rule indicates to continue the current property if continue_attr, which is evoked by\na random function, is true. The second and third rule forms an even loop, making it possible to\nchoose any available property. The fourth rule defines that the discussed property should not be\nselected.\nTopic switching. When a topic continues for several rounds, the reasoner may switch to a related\ntopic. This is implemented by the codes below.\n```latex\n1\nnext_topic(Topic, Name, Attr, 'None', 'None') :-\n2\nround(I), hist(I, Topic, Name, Attr, Attitude, 'user'),\n3\nnot discussed_before(Topic, Name, Attr).\n4\nnext_topic(Topic, Name, Attr, 'None', 'None') :-\n5\ncontinue_topic, talk (Topic, Name, Attr1),\n6\nnext_attr(Topic, Name, Attr).\n7\nnext_topic(Topic, Name, Attr, Source, Relation) :-\n8\nlen_rcc(I), random(1, I, N), rcc(N, Topic, Name, Source, Relation),\n9\nnext_attr(Topic, Name, Attr).\n```\nThe first rule sets out that a new topic should continue for at least one round. The second rule\nindicates to continue the current topic if continue_topic is true. The third rule describes how to\nswitch the topic to another linked one by a random RCC."}, {"title": "4.3 Leveraging LLMs", "content": "Besides parsing and response generation, LLMs are also used for other purposes.\nProperty enumeration. Before constructing the AutoCompanion, a detailed list of properties is\nneeded for each topic. Although the property list has been equipped with information extracted\nfrom the knowledge base, more properties are required to cover as many themes as people might\nbe interested in while chatting. Therefore, an LLM is used to seek a comprehensive list of prop-\nerties. We used GPT-4 with the following prompt to list the properties.\n\"When people talk about their interests in [topic] in a social chat, what\naspects will they discuss? Please make each point as much different as possible\nfrom others. Summarize each point in a short name.\""}, {"title": "5 Conversation Samples", "content": "Below we display one conversation between the AutoCompanion and a human user, which\ndemonstrates how the AutoCompanion directs and switches between topics.\nBot: Hello! It's delightful coming across another cinema enthusiast. Seen any\nnoteworthy films lately? Or maybe you've been immersed in a captivating\nread? My enthusiasm seems to oscillate between both constantly!\nUser: Me too! I just saw Inception. It is a great idea to take action on one's\ndream! Dreams in the dreams! What a fabulous idea!\nThemes:\n```latex\n1\ntalk(movie, Inception, plot episode). content (plot episode, actions in dreams).\nattitude(positive).\n```\nNext:\n```latex\n1\ntalk(movie, Inception, plot episode). attitude(positive).\n```\nBot: Absolutely! The concept of a dream within a dream in Inception is simply\nastonishing. Who could forget that sensational zero-gravity hotel scene?\nThe representation of physics and action was remarkable, wasn't it?\nUser: Yeah! And those people are kicked to wake up from their level of dreams one\nafter another. very impressive and surprising, right?\nThemes:\n```latex\n1\ntalk(movie, Inception, plot episode). content (plot episode, waking up one after\nanother). attitude(positive).\n```\nNext:\n```latex\n1\ntalk(movie, The Wolf of Wall Street, plot episode). attitude (positive).\n```\nBot: Totally! Your mention of 'Inception' brings to mind 'The Wolf of Wall\nStreet', another DiCaprio masterpiece. The scene in which Jordan Belfort\ngalvanizes his firm and denounces stepping down is incredibly grand! Quite\nthe adrenaline surge, right?\nUser: Sorry I haven't seen this movie before. But I did see another of his work,\nCatch Me If You Can. I think DiCaprio's acting there quite matches the\ncharacter's traits, right? It's really a good story. I like that.\nThemes:\n```latex\n1\ntalk(person, Leonardo DiCaprio, filmography). content (filmography, Catch Me If You\nCan). attitude(positive).\n```\n```latex\n2\ntalk(movie, Catch Me If You Can, actor performance). content(actor performance, acting\nof DiCaprio matches the traits). attitude(positive).\n```\n```latex\n3\ntalk(movie, Catch Me If You Can, plot episode). attitude(positive).\n```\nNext:\n```latex\n1\ntalk(movie, Catch Me If You Can, plot episode). attitude(positive).\n```\nBot: Couldn't agree more, buddy the scene with Frank as a Pan Am pilot?\nUnbelievably good. It's astounding to think of a kid being able to pull\noff a scam that big. What scene did you enjoy the most?\nUser: Right, that's amazing! But almost everybody trusts his make-up identity\nwithout a second thought. It's kind of ridiculous, but if things like this\nhappened around us, it would be terrible, right?\nThemes:\n```latex\n1\ntalk(movie, Catch Me If You Can, characterization). content (characterization, everybody\ntrusts Frank's make-up identity). attitude(negative).\n```\n```latex\n2\ntalk(movie, Catch Me If You Can, social impact). content (social impact, terrible if\nhappened in real life). attitude(positive).\n```\nNext:\n```latex\n1\ntalk(movie, Catch Me If You Can, social impact). attitude (positive).\n```\nBot: Absolutely, buddy! Imagine having tricksters like Frank amidst us\nentertaining in films but not in reality! The movie, Catch Me If You Can,\nprompts us not to be rebels but to demand transparency, wouldn't you agree?\nThere's a certain allure to the con artist lifestyle, don't you find it\nintriguing?\nUser: So according to you, this is a very educational movie, right? Anyway I\nreally like its story, it's fun and exciting.\nThemes:\n```latex\n1\ntalk(movie, Catch Me If You Can, value expressed). content (value expressed,\neducational). attitude(positive).\n```\n```latex\n2\ntalk(movie, Catch Me If You Can, plot episode). content(plot episode, fun and\nexciting). attitude(positive).\n```\nNext:\n```latex\n1\ntalk(movie, Don't Look Up, plot episode). attitude (positive).\n```\nBot: I totally understand, it's an entertaining narrative full of excitement,\nwith underlying themes of trust and integrity. Your mention of Catch\nMe If You Can brings to mind another film featuring Leonardo DiCaprio,\nnamely Don't Look Up. Well, personal opinion aside, the sequence with the\nmeteorite approaching Earth was particularly gripping. Would you like to\nshare your thoughts on it?\nUser: Ah I don't quite like that movie. It thinks it satirizes a lot of things,\nbut there is nothing fresh or original. It is neither spicy nor funny, and\nits reflection on the political situation at that time is a bit deliberate.\nThemes:\n```latex\n1\ntalk(movie, Don't Look Up, plot episode). content (plot episode, 'nothing fresh or\noriginal, neither spicy nor funny, the reflection of the political situation is\ndeliberate'). attitude (negative).\n```\nNext:\n```latex\n1\ntalk(person, Jennifer Lawrence, filmography). attitude (negative).\n```\nBot: Absolutely, my friend. It seemed excessively satirical to the point of\nunreality, draining the plot of its potency. When you bring up 'Don't\nLook Up', I can't help but think of Jennifer Lawrence she was part\nof its cast. In my opinion, the movie 'House at the End of the Street'\ndidn't quite showcase her acting abilities. A talent like hers seemed\nunderutilized, don't you agree?\nUser: I think it's the role that limits her, although in fact her character is\nactually one of the few bright spots in this movie.\nThemes:\n```latex\n1\ntalk(person, Jennifer Lawrence, acting skill). content(acting skill, limited by role in\nHouse at the End of the Street). attitude(negative).\n```\n```latex\n2\ntalk(movie, House at the End of the Street, actor performance). content(actor\nperformance, Jennifer Lawrence is one of the few bright spots). attitude(positive).\n```\nNext:\n```latex\n1\ntalk(movie, House at the End of the Street,actor performance). attitude (positive).\n```\nBot: Absolutely, I concur with your point. Jennifer Lawrence's skills exceeded\nher role. Her performance was indeed the bright spot in the film. The\nfear and struggle she represented was so real. Do you have any insights\nregarding the performance of other actors?\nUser: The male lead is quite handsome, and the ending is really powerful, adding\na lot of color to the movie.\nThemes:\n```latex\n1\ntalk(movie, House at the End of the Street, actor performance). content (actor\nperformance, male lead is handsome). attitude (positive).\n```\n```latex\n2\ntalk(movie, House at the End of the Street, plot episode). content (plot episode,\npowerful ending). attitude(positive).\n```\nNext:\n```latex\n1\ntalk(movie, House at the End of the Street, plot episode). attitude(positive).\n```\nBot: Indeed, the finale, oh my, totally transformed the game! It added an\nexciting edge to the film! Positively, the unexpected revelation where the\n'sister' unfolds as the captive girl is flabbergasting. I was completely\ncaught off guard. Did this astonish you to the same degree as it did me?\nUser: Yeah! But simply astonished. Nothing else. I'm happy to talk with you, but\nI need to go now. See you next time!\nThemes:\n```latex\n1\ntalk(movie, House at the```json\n End of the Street, emotion impact). content(emotion impact,\njust astunished). attitude(negative).\n```\n```latex\n2\nquit.\n```\nNext:\n```latex\n1\nquit.\n```\nBot: Sure thing! I had a great time talking about movies and books with you. I\nhope we can carry on with our conversation next time. Have a great day!"}, {"title": "6 Evaluation", "content": "Next, we evaluate AutoCompanion's execution efficiency and quality of dialog generation."}, {"title": "7 Discussion", "content": "In this section, we discuss the features of AutoCompanion and the proper ways to use LLMs.\nWhy is AutoCompanion better than the LLM-only socialbots? Compared with LLM-only\nsocialbot, our AutoCompanion has the following advantages: (i) Our AutoCompanion is based\non the knowledge base, not pattern matching alone, which allows retrieval of relevant information\nabout any topic instance through a query, avoiding factual errors. When we only use LLM to build\nsocialbots, since the knowledge is not explicitly stored in a specific library, but is compressed\nand stored using neural networks and weights, this will inevitably lead to confusion between\nconcepts, and hallucinations will emerge. For instance, when asking GPT-4 the below question:\nIs Currie Graham in the movie Hitchcock?\nIt would reply:\nYes, Currie Graham is in the movie \"Hitchcock.\" He plays the role of Bertie,\none of the characters involved in the production of \"Psycho.\"\nHowever, according to IMDb, Currie Graham acts as a PR called Flack in this movie and thus\ncan be corrected through AutoCompanion. There are other advantages of AutoCompanion:\n\u2022 The use of an information database enhances the scalability of AutoCompanion. To update\nan LLM, we need to either load extremely long relevant texts at any time or train the LLM\nfor a long time. But if a database supplements our information, an update to the knowledge\nbase is all that is needed.\n\u2022 ASP reasoner's ability to control LLM is also a major advantage. LLMs usually require to\ndescribe the instructions in as much detail as possible to complete the task better (change\ntopic, recommend, etc.). In the socialbot task for LLMs, this is reflected in the limitation\nof conversation topics, the choice of strategies for talking about a certain topic for several\nrounds and then jumping to another topic, the choice of recommendation timing for the lat-\nest popular topic instances, etc. However, these instructions usually require a large amount\nof token space, which will gradually be ignored as the conversation length increases. On\nthe other hand, we don't record the specific content of the conversation but the state is\nsaved in the summary.\n\u2022 The ASP reasoner can specify some less common, out-of-the-box-thinking for an LLM.\nEven with a certain degree of randomness, it is difficult for LLMs to truly generate some\nopinions that are far away from most people's thinking. Yet, even if a movie has excellent\nreviews, there remains a small group of people who don't like it for various reasons. How-\never, when using an LLM directly, it almost always caters to the mainstream views of the\npublic and lacks its \u201cown unique insights\u201d. This can also be avoided by cleverly using an\nASP reasoner.\nIs it proper to let LLM generate the fact-based content for use by AutoCompanion? It\nis well known that LLMs are good at generating coherent text. However, the accuracy of the\ngenerated content is not guaranteed. If we have certain requirements on the correctness of the\ngenerated content, relying only on an LLM may not be a good choice. Nonetheless, with the\nincrease in training materials, the reliability of LLM has also improved to a certain extent. In\nsuch a case then, would it be better to rely only on the knowledge in the knowledge base, and\nallow LLM to only paraphrase? Not necessarily, because this requires an extremely detailed and\ncomplete knowledge base, which we may not have, such as storing all the information about\neach movie, including all plots, shots, lines, costumes, sets, and other information, as well as\nopinions about them, and this is almost impossible. For our AutoCompanion, since almost all\nmovies/books/people are well-known to the public, GPT-4 has also been exposed to relevant\nmaterials and is familiar with relevant information, so it is feasible for GPT-4 to generate content\ndirectly, but it also sacrifices some accuracy. And, since enough constraints are imposed and the\nsentences are short, we have not observed hallucinated responses in our reasonably extensive\nexperiments. In our future work, the knowledge base will be expanded to cover most movies and\nbooks, but if so, a detailed knowledge base is required.\nWhy does LLM always generate answers lacking novelty? When we used GPT-4 to build and\nuse an LLM-only socialbot, we found that the views generated by GPT-4 were often old and\nboring, not only wrt the property of a certain topic instance, but also wrt the association of one\ninstance to another. For example, when discussing fantasy literature, GPT-4 will always mention\n\u201cMiddle-Earth\u201d or the \u201cHarry Potter\u201d series first and express its appreciation for them. As another\nexample, when we mention the movie Inception or other acclaimed movies, GPT-4 always speaks\nhighly of it. Even when we ask it to talk about its shortcomings about a certain property, it\nsometimes replies, \u201cI can't think of any shortcomings.\u201d Work of Gallegos et al. (2023) suggests\nthat this is related to the bias and fairness of LLM to a certain extent. Since the data injected into\nLLM is based on statistics, LLM's belief in a certain point of view is constantly strengthened\nduring the training. Especially when a small number of samples are carefully labeled but used\nrepeatedly, those data that deviate from the general public opinion are directly eliminated. At\nthe same time, training to align reinforcement learning with users also weakens the creativity\nof LLM to a certain extent. This essentially reflects the \u201cdata organizer\u201d attribute of LLM, that\nis, the current LLM is only trained for data collection and organization, and is rarely trained for\ninnovative work \u2013 and innovation is difficult to gain from training. Franceschelli and Musolesi\n(2023) also pointed out that although LLM can produce texts of a certain value, revolutionary or\neye-catching ideas are rarely found."}, {"title": "8 Conclusion and Future Work", "content": "This paper presents an approach to building a socialbot that utilizes LLM to translate between\nnatural language and predicates while employing an ASP reasoner as the backend. We used\nthis framework to develop the AutoCompanion socialbot to chat about movies and book-related\ntopics. AutoCompanion outperforms LLM-only socialbots in reliability, scalability, ease of use,\nand creativity. We believe this organic combination of LLM and ASP is one of the most effective\nways to realize the control of LLM-based socialbots.\nOur future work includes building a more complete knowledge base that allows socialbots\nto query most of the topic knowledge rather than generating it using an LLM; it also includes\nfield testing and evaluation of socialbots. We expect to make our AutoCompanion Socialbot\navailable on online platforms shortly. Feedback from users will be one of the criteria to measure\nthe performance of our AutoCompanion. Additionally, continuing to explore how to better utilize\nLLMs to generate unstructured knowledge and ensure its reliability is also part of our future\nwork. We are also planning to add multi-modality, like importing movie clips and voice I/O to\nour AutoCompanion."}]}