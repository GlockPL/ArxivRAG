{"title": "ZOBNN: Zero-Overhead Dependable Design of Binary Neural Networks with Deliberately Quantized Parameters", "authors": ["Behnam Ghavami", "Mohammad Shahidzadeh", "Lesley Shannon", "Steve Wilton"], "abstract": "Low-precision weights and activations in deep neural networks (DNNs) outperform their full-precision counterparts in terms of hardware efficiency. When implemented with low-precision operations, specifically in the extreme case where network parameters are binarized (i.e. BNNs), the two most frequently mentioned benefits of quantization are reduced memory consumption and a faster inference process. In this paper, we introduce a third advantage of very low-precision neural networks: improved fault-tolerance attribute. We investigate the impact of memory faults on state-of-the-art binary neural networks (BNNs) through comprehensive analysis. Despite the inclusion of floating-point parameters in BNN architectures to improve accuracy, our findings reveal that BNNs are highly sensitive to deviations in these parameters caused by memory faults. In light of this crucial finding, we propose a technique to improve BNN dependability by restricting the range of float parameters through a novel deliberately uniform quantization. The introduced quantization technique results in a reduction in the proportion of floating-point parameters utilized in the BNN, without incurring any additional computational overheads during the inference stage. The extensive experimental fault simulation on the proposed BNN architecture (i.e. ZOBNN) reveal a remarkable 5X enhancement in robustness compared to conventional floating-point DNN. Notably, this improvement is achieved without incurring any computational overhead. Crucially, this enhancement comes without computational overhead. ZOBNN excels in critical edge applications characterized by limited computational resources, prioritizing both dependability and real-time performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) have become increasingly important in a variety of application domains, including video analytics, autonomous vehicle systems, and medical devices, due to recent advances that improve their performance and energy efficiency. However, practical reliability considerations make the widespread real-world deployment of DNN-based systems difficult, particularly in many safety-critical domains [1]. Recent research has shown that edge systems based on DNNs are susceptible to transient hardware faults resulting from particle strikes or voltage droops and can significantly impact the reliability of safety-critical DNN-based autonomous vehicles [2]. For example, a single soft error may result in the misclassification of objects in the field of self-driving cars, which can cause the car to take inappropriate actions [3]. This issue may worsen with the technology node scaling of hardware accelerators for autonomous machine computing. Therefore, there is a need for designing and implementing dependable DNN systems.\nOn the other hand, dependability in safety-critical real-time DNN-based embedded systems faces a unique challenge due to the requirements of low power, high throughput, and \"low latency\" where traditional fault-tolerant techniques, which include redundant-based hardware/software/data, can result in high overheads in hardware cost, energy, and performance, making them impractical for real-time edge DNN systems.\nFor instance, an autonomous vehicle must process driving data within a few milliseconds for real-world deployment and deploying traditional protection techniques may lead to delayed response times, potentially leading to reaction-time-based accidents. Given the resource constraints inherent in DNN-based edge applications and the critical need for real-time responsiveness, there exists a compelling imperative to develop a customized error mitigation technique. This prompts the central research question: Is it feasible to design highly fault-tolerant DNN models for safety-critical applications in embedded edge systems without incurring additional run-time and resource overhead?\nIn this paper, we present the advantages of quantization as an enhanced cost-free fault-tolerance feature for DNNs. Quantization is defined as the reduction of precision used to represent neural network parameters, typically from m bits to n bits (where m < n). Our observation reveals that fault-free DNN neurons values typically fall within a limited range, a small fraction of the total range offered by data representation. Contrastingly, memory faults can lead to severe inference output corruptions when neurons values manifest significantly higher than the expected range. Based on this insight, we propose restricting the range of DNN parameters through quantization as a means of fault propagation prevention without incurring overheads. Specifically, we endorse binarization, the extreme case of quantization, as a promising solution to enhance DNN reliability. However, the adoption of binarization presents a primary challenge in maintaining the high accuracy of a DNN. Addressing this concern, contemporary state-of-the-art (SOTA) Binary Neural Networks (BNNs) demonstrate reasonable accuracy via strate-"}, {"title": "II. RELATED WORK", "content": "gically incorporate some network components with floating-point data representation [4], striking a delicate balance between the hardware efficiency gains of binarization and the accuracy demands of complex tasks. However, this inclusion of floating-point data representation could directly impact BNN reliability, as uncovered through our comprehensive fault characterization. Building upon this crucial observation, we propose a novel \"selectively quantization\" technique to confine the range of these float parameters, thereby enhancing network dependability. Importantly, this proposed quantization technique introduces no additional operations or modifications to the network, mitigating the risk of extra computational burden. The experimental results emphasize a notable 5X improvement, on average, in reliability achieved by the proposed BNN model, named as ZOBNN, compared to conventional DNNs. This advancement is showcased with up to 32.32% and 24.81% accuracy improvement on SOTA BNNs, Dorefanet[5] and FracBNN[4], at a fault rate of le-4. Impressively, ZOBNN attains nearly identical accuracy compared to the baseline fault-free BNN models.\nIn summary, the main contributions of this paper are as follows:\n\u2022 Our investigation undertakes a thorough exploration into the implications of memory faults on DNNs, with a specific focus on illuminating the inherent dependability advantages of quantization. We meticulously elucidate the merits of binarization, the zenith of quantization, as an intricately designed advanced fault-tolerance feature tailored to fortify the resilience of DNNs.\n\u2022 We introduce a novel quantization method poised to elevate the robustness of binary networks, specifically by directing attention to the overhead-aware quantization of floating-point parameters.\n\u2022 The effectiveness of the proposed approach is substantiated through fault simulations conducted on SOTA BNNs.\nII. RELATED WORK\nModifying the neural network architecture has been intro- duced to increase its error resilience, which can be done either during training or outside training. Dias et al. [6] proposed a resilience optimization procedure that involves changing the architecture of the DNN to reduce the impact of faults. Schorn et al. [7] introduced a resilience enhancement technique that replicates critical layers and features to achieve a more homo- geneous resilience distribution within the network. An error correction technique based on the complementary robust layer (redundancy mechanism) and activation function modification (removal mechanism) has been recently introduced in [8]. Hong et al. [9] introduced an efficient approach to mitigate memory faults in DNNs by modifying their architectures using Tanh [10] as the activation function. To mitigate errors, based on the analysis and the observations from [11], Hoang et al. [12] presented a new version of the ReLU activation function to squash high-intensity activation values to zero, which is called Clip-Act. Ranger [13] used value restriction in different DNN layers as a way to rectify the faulty outputs caused by transient faults. Recently, Ghavami et al. [14] introduced a fine-grained activation function to improve the error resilience of DNNs. Research ([15]) emphasizes the critical need to detect errors in CNN execution to prevent their propagation to fully connected layers. They propose redesigning maxpool layers for improved error detection and potential correction. Libabno et al. ([16]) conducted neutron beam experiments to assess the impact of quantization processes and data precision reduction on the failure rate of neural networks deployed on FPGAs. Their results show that adopting an 8-bit integer design increases fault-free executions by over six times compared to a 32-bit floating-point implementation, highlighting the advantages of lower precision formats in FPGA environments. Investigating the reliability of CNNs, [17] analyzed the effects of reduced bit widths and different data types (floating-point and fixed-point) on network parameters to find the optimal balance for CNN reliability. Additionally, [18] demonstrates that implementing fault-aware training significantly improves the resilience of Quantized Neural Networks to soft errors when deployed on FPGAs. Finally, Cavagnero et al. ([19]) introduce a zero-overhead fault-tolerant solution focusing on DNN redesign and retraining, which enhances DNN reliability against transient faults by up to one order of magnitude."}, {"title": "III. PRELIMINARIES AND DEFINITIONS", "content": "A. Deep Neural Networks\nDNNs are a type of machine learning model that is designed to predict an output as accurately as possible given an input X. DNNs are composed of multiple layers, where each layer receives the output of the previous layer as its input. The input data is sequentially passed through each layer, with each layer performing a specific computation on the input and passing the result to the next layer.\nThe individual layers of a DNN F\u2081 are made up of neurons, which perform mathematical operations on the input data X. Each neuron is connected to the neurons in the previous layer \\( F_{i-1} \\) via a set of weights \\( \\theta_{i-1} \\), which are determined during the training phase of the DNN. The weights determine the strength and direction of the connections between the neurons. The computations performed by each layer of a DNN are typically non-linear, which allows the network to model complex relationships between the input and output data. The non-linearities in the layers are often introduced using activation functions, which transform the output of each neuron in a non-linear way. The final layer of a DNN produces the output prediction for the input data. Equation 1 illustrates the DNN computation.\n\\[\nDNN(X) = (F_O^N\\circ \\theta^N \\circ F_O^{N-1} \\circ ... F_O^1)(X)\n\\]\nThe DNN architecture is typically composed of a sequence of two types of layers, designed to capture the hierarchical structure of the input data:\n\u2022 The first-type consists of convolution and linear layers. Each neuron in these layers \\( N^i \\) is computed as a sum of"}, {"title": "IV. MOTIVATION: RELIABILITY ANALYSIS OF CONVENTIONAL BNNS", "content": "\u2022 the product of the neurons in the previous layer \\( N^{i-1} \\) to their corresponding weights Wk.\n\\[\nN^{(i,k)} = \\sum_{j=1}^{N-1}(W^{(k,j)})(N^{(i-1,j)})\n\\]\n\u2022 The second-type of layer which consists of batchnor- malization layers, activation functions, and others, were developed to increase the accuracy of DNNs. In DNNs, there is always at least one of the \"second-type\" layers in between the two \"first-type\" layers.\nTherefore, the comprehensive DNN architecture can be visualized as below, with 'S' denoting a second-type and 'L' indicating a first-type layer:\n\\[\nDNN(X) = (L^O\\theta^N \\circ S^{N-1} ... L^O\\theta^2 \\circ S_1 \\circ S_2 \\circ L^O\\theta^1 )(X)\n\\]\nB. Integer Only Quantization of Neural Networks\nInteger-only quantization [20] plays a pivotal role in deep learning, especially for deploying models on resource- constrained hardware platforms. It involves reducing numeri- cal precision to integers, allowing for efficient implementation of neural network operations using only integer arithmetic. This technique relies on an affine mapping of integers q to real numbers r, expressed as r = \u2206(q - Z), where \u2206 represents the scale factor and Z denotes the zero-point.\nUtilizing this quantization scheme, the quantized represen- tation of a neuron k in the ith layer can be expressed as \\( N^{(i,k)} = \\Delta_3(q^{i,k} -Z_3) \\), while that of neuron j in the (i-1)th layer is \\( N^{(i-1,j)} = \\Delta_2(q^{i-1,j} \u2013 Z_2) \\), reflecting varying scaling factors across layers. Furthermore, the jth weight of neuron k can be quantized as \\( W^{(k,j)} = \\Delta_3(q_1^{(k,j)} \u2013 Z_1) \\). Employing this technique, neuron computation proceeds as follows:\n\\[\nq_3^{(i,k)} = Z_3 + M \\sum_{j=1}^{q_1} (q_1^{(k,j)} \u2013 Z_1)(q_2^{i-1,j} \u2013 Z_2)\n\\]\nwhere M, defined as \\( M := \\frac{\\Delta_1 \\times \\Delta_2}{\\Delta_3} \\), first dequantizes the results from the previous layer by scaling them with \u03941 \u00d7 \u03942, then quantizes them again by applying A3.\nC. Binary Neural Networks\nBNNs are the extreme case of quantization where the bit-width is one. In BNN, the intermediate L-type layer's parameters coupled with their inputs are passed through a binarization function B. This allows the BNNs to run on more efficient hardware with XNOR and Pop-Count operations. While BNNs offer several advantages over DNNs, including a low memory footprint and power efficiency due to the use of binary values for weights and activations, they also suffer from a major disadvantage: a significant loss in accuracy that occurs after the binarization process. To mitigate this loss, existing BNN models[5][4] often use floating-point parameters in the first and the last L-type layers and in all S-type layers as described in equation 3.\n\\[\nBNN(X) = (L_O^N\\theta^N \\circ S_N^{-1} \\circ B \\circ O^{N-2} \\circ ... S_2 \\circ L_O^1 )(X)\n\\]\nBy using a mix of binary and floating-point representations, they can strike a balance between the memory efficiency of binary values and the higher precision of floating-point values, while still providing significant improvements in resource efficiency over traditional DNNs.\nIV. MOTIVATION: RELIABILITY ANALYSIS OF\nCONVENTIONAL BNNS\nDespite the numerous advantages offered by BNNs, their resilience to memory faults has received little attention. As a result, there is a pressing need to better understand the vulnerability of BNNs and their effective safeguards to these types of faults. Figure 1 illustrates the impact of memory bit-flip faults on a floating-point DNN, its corresponding quantized version, and a BNN, at varying memory bit-flip probabilities (fault rates). In the DNN scenario, the accuracy drops in the le-6 fault rate and becomes a random output in the 1e-4 value. However, in the BNN case, the accuracy remains nearly the same in 1e-5 and gets a 50% deviation from the baseline accuracy in the extreme case of le-4 fault rate. Our observation reveals fault-free neurons usually have values within a narrow range, significantly smaller than the total range in floating-point representation. We can state that the memory faults will propagate to the DNN output only if the affected neuron values exceed the expected range, leading to substantial output changes. Consequently, \"quantization\" can then limit computation parameter ranges, serving as a built-in mechanism to \"fault propagation prevention\".\nAdditionally, to comprehend the underlying vulnerability in BNN due to memory faults, we conducted an in-depth fault analysis of individual layers on a BNN [4]. Figure 2 illustrates these comparative results. Among the layers, L-type layers which are convolution layers exhibit the least significant accuracy drop, since all the parameters in the convolution layers are binarized as described in Section III-C. However, this is not the case for the remaining S-type layers and the final L-type layer, as their parameters are stored in floating- point format. The remaining layers' accuracy starts deviating from the baseline in a lower fault rate. This aligns with the observation that soft errors can result in substantial corruption of inference outputs in floating-point's DNNs, especially when these errors manifest as values exceeding the expected range. Binary values, constrained to a limited range, and bit-flips have no opportunity to manifest as large value deviation. It is noteworthy that even after binarizing all Convolution layers, their memory footprint still dominates compared to other layers while exhibiting the smallest drop."}, {"title": "V. PROPOSED SELECTIVE QUANTIZATION OF BNNS", "content": "V. PROPOSED SELECTIVE QUANTIZATION OF BNNS\nTo enhance the reliability of BNNs while addressing com- putation overhead, we introduce a novel selctive quantization method for the S-type layers, integrated with the first and last L-type layers within a BNN. This preserves baseline accuracy without adding extra computational overhead.\n\n\nEquation 4 exemplifies a conventional quantization ap- proach involves three components for each layer. Firstly, a quantization function Q(0) is applied to the parameters, facilitating the transition from floating-point to integer rep- resentation. Subsequently, an additional quantization layer Q is incorporated into the input, enabling the entire layer to operate seamlessly in quantized computation form. Lastly, a dequantization layer D restores the quantized result to its original form.\n\\[\nDNN(X) = (D\\circ L \\circ Q(\\theta^N) \\circ Q\\circ S^{N-1} ... Q\\circ D\\circ S(\\theta^2) \\circ L \\circ Q(\\theta^1) \\circ Q)(X)\n\\]\nHowever, this approaches introduce supplementary extra parameters and computational load. Additionally, storing the parameters of the quantization and dequantization layers in memory poses a potential source of failure, impacting the re- liability of the entire network. To alleviate these concerns and eliminate the need for the quantization (Q) and dequantization (D) layers throughout the network, several modifications are proposed. Primarily, we established a reciprocal relationship between Q and D, ensuring Q(D(x)) = x. This adjustment allows for the removal of consecutive Q and D functions, as demonstrated in Equation 5.\n\\[\nDNN(X) = (D\\circ L \\circ Q(\\theta^N) \\circ S^{N-1} ... L \\circ B \\circ O^{N-2} ... S(\\theta^2) \\circ L \\circ Q(\\theta^1) \\circ Q)(X)\n\\]\nExecuting this operation allows for the elimination of the majority of quantization and dequantization functions. Nonetheless, certain corner cases persist. Specifically, some S-type layers receive input after L-type layers, introducing a single Q-type layer before them. For instance, layer SN-1 exemplifies such a scenario. To eliminate the quantization function in these layers, it is imperative that every S-type layer can seamlessly process both quantized and dequantized inputs. Subsequent Q and D functions are linked with the input and output sections, which we would discuss separately in Sections V-C and V-B. Finally, Section V-D delves into the BOD components and the methods employed to discard them. These operations culminate in the final BNN represented by Equation 6, devoid of any additional parameters that might compromise its robustness.\n\\[\nDNN(X) = (L\\circ(\\theta^N) \\circ O\\circ S^{N-1} \\circ B \\circ O^{N-2} ... S(\\theta^2) \\circ L \\circ \\theta^1)(X)\n\\]\nA. Dequantization and Quantization Layers\nThe following equation illustrates the proposed quantization and dequantization layers of ZOBNN. The quantization layer transforms the real floating-point number xr into its quantized integer counterpart xq using the parameter \u2206. The parameter A is determined by the equation \\( \\frac{max(\\theta)}{2^{#bits-1}-1} \\). This signifies that we utilize the maximum integer, 2#bits-1 - 1, to represent the largest parameter in the model, denoted as max(\u03b8). Consequently, the parameters are quantized within the range of -max(\u03b8) to max(\u03b8).\n\\[\nQ(x) = round(x_r/\\Delta) = x_q\n\\]\nNext, the dequantization function in Equation 8 should transfer the quantized number to integer format. Also, it should be such that Q(D(x)) = x. Equation 9 illustrates this reciprocal relation between Q and D.\n\\[\nD(x_q) \u2248 \\Delta \\times (x_q) = X_r\n\\]\n\\[\nQ(D(x_q)) \u2248 round(\\frac{\\Delta \\times (x_q)}{\\Delta}) = x_q\n\\]\nB. Input Layer\nFollowing the application of common quantization to the input layer, the input is processed through the Q layer, de- noted as L1Q(01) oQoxr. By omitting the Q layer in this equation, the layer receives its input directly in the xr format. The subsequent equation elucidates the quantization method employed for this layer.\n\\[\nconv(X) = X * W = X * W \\times \\Delta = \\Delta \\times X_r * W_q\nconv(\\Delta \\times X_q) = \\Delta \\times \\gamma\nconv(X_q) = \\gamma\n\\]\nC. Output Layer\nThe output layer in the majority of BNNs typically remains unbinarized. As a result, this layer will undergo standard quantization. In the standard quantization, the output layer is succeeded by a D-type layer. The removal of this D-type layer proves to be straightforward in neural networks dedicated to detection tasks, as it is succeeded by an argmax function. Notably, Argmax(D(x)) = Argmax() = Argmax(x), thereby automatically eliminating the necessity for the dequantization function."}, {"title": "VI. EXPERIMENTAL FAULT SIMULATION SETUP", "content": "D. Binarized Layer\nThe sign layer B serves as the antecedent function for the binarized layers in the network. Following standard quanti- zation, the input to this function is derived from a D layer, denoted as BD. If the dequantization layer is excluded, the input to the sign layer becomes xq. The subsequent equation depicts this layer when receiving an input of xq. The sole modification required for this function involves dividing the original constants by \u2206, yielding identical results for an input of xr.\n\\[\nsign(X) = \\frac{clamp(X,1,-1) + 1}{delta}\n\\]\n\\[\nsign(\\Delta \\times X) = \\frac{clamp(X_q\\Delta, 1, -1) + 1}{delta}\n\\]\n\\[\nsign(X_q) = \\frac{clamp(X_q, 1, -1) + 1}{\\frac{delta}{\\Delta}}\n\\]\n\\[\nsign(X_q) = \\gamma\n\\]\nE. Other Layers via SOTA BNN\nAll the S-type layers should work with both xq and Xr inputs and give their output in xq format. Here we show the quantization of S-type layers for two common layers, batch- norm and rprelu, in SOTA BNNS.\n1) batch-norm: The batch normalization layer is a ubiqui- tous component in BNN architecture, comprising four param- eters available for quantization. When dealing with quantized input, all four parameters are quantized, whereas in the case of real input, only W and B undergo quantization. This approach ensures a consistently quantized output regardless of the input nature. Equation 12 illustrates the quantization of this function for an input xq, and equation 13 does so for an input Xr.\n\\[\nbatchnorm(X) = W \\times \\frac{X - \\mu}{\\sigma} + B\n\\]\n\\[\n= \\Delta \\times W \\times \\frac{\\Delta X_q - \\Delta \\mu_q}{\\Delta \\sigma_q} + \\Delta B_q\n\\]\n\\[\n= \\Delta \\times W \\times \\frac{X_q - \\mu_q}{\\sigma_q} + B_q\n\\]\n\\[\nbathcnomr(\\Delta \\times X_q) = \\Delta \\times \\gamma\n\\]\n\\[\nbatchnorm(X_q) = \\gamma\n\\]\n\\[\nbatchnorm(X) = W \\times \\frac{X - \\mu}{\\sigma} + B\n\\]\n\\[\n= \\Delta \\times W \\times \\frac{ X_r - \\mu_r}{\\sigma_r} + \\Delta B_q\n\\]\n\\[\n= \\Delta \\times W \\times \\frac{X_r - \\mu_r}{\\sigma_r} + B_q\n\\]\n\\[\nbathcnomr(X) = \\Delta \\times \\gamma\n\\]\n2) rprelu: The rprelu layer functions as the activation function in SOTA BNNs, comprising three parameters, two of which are biases, and a parameter W which equals 1 when x > -B1 and is constant otherwise. The following two equations illustrate the quantization of these functions given inputs of xq and xr. In the first scenario, the parameters B1 and B2 are quantized, while in the second scenario, it's the W and B2. This quantization ensures a quantized output in both cases.\n\\[\nrprelu(X) = W \\times (X + B_1) + B_2\n\\]\n\\[\n= (\\Delta = W \\times \\Delta \\times X_q + \\Delta \\times B_{1q}) + \\Delta \\times B_{2q}\n\\]\n\\[\n= \\Delta \\times W \\times (X_q + \\frac{\\Delta}{\\Delta} \\times B_{1q}) + \\frac{\\Delta}{\\Delta} \\times B_{2q}\n\\]\n\\[\nrprelu(\\Delta \\times X_q) = \\Delta \\times \\gamma\n\\]\n\\[\nrprelu(X_q) = \\gamma\n\\]\n\\[\nrprelu(X) = W \\times (X + B_1) + B_2\n\\]\n\\[\n= \\Delta \\times W \\times (X_r + B_{1r}) + \\Delta \\times B_{2q}\n\\]\n\\[\n= \\Delta \\times W \\times (X_r + \\frac{\\Delta}{\\Delta} \\times B_{1r}) + \\frac{\\Delta}{\\Delta} \\times B_{2q}\n\\]\n\\[\nrprelu(X) = \\Delta \\times \\gamma\n\\]\nVI. EXPERIMENTAL FAULT SIMULATION SETUP\nA. Fault Injection Framework\nTo evaluate the error tolerance of a DNN, intentional simu- lated faults are introduced at various levels of the abstraction. Employing software-level fault simulation proves to be the optimal approach due to its speed and reasonably accurate re- sults, especially considering the typically prolonged simulation time for large networks. In this study, a dynamic software-level fault injection tool on top of PyTorch framework is developed throughout the BNN inference process. Fault models are implemented as native tensor operations to minimize any potential performance impact. This involves flipping every bit in the memory with a probability denoted as P, referred to as the \"fault rate\u201d in our investigation. Subsequently, to ensure robustness, this fault study is repeated 500 times, and the average across all iterations is calculated.\nB. Fault Rate Value\nTo maintain the generality of our investigation across differ- ent memory hardware and environmental setups, we analyze the impact of faults within the fault rate range of le-6 to 1e-3, aligning with recent experiments. Reagen et al. [21] observed SRAM fault rates of approximately le-5 at a voltage reduction to 0.7 volts. Nguyen et al. [22] delved into DRAM intricacies, revealing fault rate fluctuations from 0 to 2e-3 at a 1s refresh rate within the temperature range of 25\u00b0C to 90\u00b0C. They also demonstrated fault rate variations from le-7 to 2e-3 at 60\u00b0C by adjusting the refresh rate from 1s to 10s.\nC. BNN Models, Dataset and Inference Setup\nThe fault study was carried out within FracBNN[4] and Dorefanet[5]--the state-of-the-art BNNs. Our investigation focused on two well-established image classification datasets, CIFAR-10 and ImageNet. FracBNN, based on the ResNet architecture, demonstrates a baseline accuracy of 89.1% on"}, {"title": "VII. EXPERIMENTAL RESULTS", "content": "VII. EXPERIMENTAL RESULTS\nA. Dependability Investigation\nFigure 3 presents a dependability comparison among the the baseline BNN, an equivalent floating-point network matched with the binary counterpart, and proposed 16-bit quantized ZOBNN under various fault rates. In the case of DorefaNet, ZOBNN achieves remarkable dependability results, maintain- ing accuracy above 78% even with a fault rate of 1e-4, while the baseline BNN accuracy drops below 48%. For FracBNN, the baseline BNN accuracy reaches 54% at the 4e-5 fault rate and declines to 34% at the le-4 fault rate. However, ZOBNN attains 78% accuracy at a fault rate of 4e-5 and exceeds 58% in the extreme case of a le-4 fault rate. Notably, the floating-point counterpart consistently generates random outputs in all scenarios. This highlights ZOBNN as presenting a remarkable 5X enhancement in robustness, on average, compared to conventional floating-point DNNS.\nFigure 4 depicts a comparative analysis of the accuracy dis- tribution between the base Fracbnn and the proposed ZOBNN. While the median remains consistent in both instances, our approach exhibits a significantly narrower range of error. This tightens the fault margin, resulting in a notably higher average accuracy.\nB. Runtime and Memory Overhead\nTable I illustrates the impact of ZOBNN on the memory footprint of FracBNN. ZOBNN enables the network to achieve quantization up to 8 bits in CIFAR-10 mode, incurring a 20.1% memory reduction. Furthermore, ZOBNN achieves 12- bit quantization on the ImageNet dataset, resulting in a 34% memory reduction."}, {"title": "VIII. CONCLUSION", "content": "VIII. CONCLUSION\nIn the dynamic realm of edge deep learning applica- tions, reliability is paramount. This paper delves into fault injection experiments, unveiling the inherent robustness of BNNs against memory faults. A novel deliberately quanti- zation method takes center stage, fortifying BNN reliability without burdensome computational costs. Experimental results highlight the transformative power of proposed ZOBNN as it significantly elevates BNN reliability while simultaneously streamlining memory footprint. This breakthrough opens doors for DNN deployment in the harshest embedded environments, where fault rates run high and resources are constrained."}]}