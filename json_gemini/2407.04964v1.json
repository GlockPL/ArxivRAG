{"title": "ZOBNN: Zero-Overhead Dependable Design of Binary Neural\nNetworks with Deliberately Quantized Parameters", "authors": ["Behnam Ghavami", "Mohammad Shahidzadeh", "Lesley Shannon", "Steve Wilton"], "abstract": "Abstract\u2014Low-precision weights and activations in deep neu-\nral networks (DNNs) outperform their full-precision counterparts\nin terms of hardware efficiency. When implemented with low-\nprecision operations, specifically in the extreme case where\nnetwork parameters are binarized (i.e. BNNs), the two most fre-\nquently mentioned benefits of quantization are reduced memory\nconsumption and a faster inference process. In this paper, we in-\ntroduce a third advantage of very low-precision neural networks:\nimproved fault-tolerance attribute. We investigate the impact of\nmemory faults on state-of-the-art binary neural networks (BNNs)\nthrough comprehensive analysis. Despite the inclusion of floating-\npoint parameters in BNN architectures to improve accuracy, our\nfindings reveal that BNNs are highly sensitive to deviations in\nthese parameters caused by memory faults. In light of this crucial\nfinding, we propose a technique to improve BNN dependability\nby restricting the range of float parameters through a novel\ndeliberately uniform quantization. The introduced quantization\ntechnique results in a reduction in the proportion of floating-\npoint parameters utilized in the BNN, without incurring any\nadditional computational overheads during the inference stage.\nThe extensive experimental fault simulation on the proposed\nBNN architecture (i.e. ZOBNN) reveal a remarkable 5X en-\nhancement in robustness compared to conventional floating-point\nDNN. Notably, this improvement is achieved without incurring\nany computational overhead. Crucially, this enhancement comes\nwithout computational overhead. ZOBNN excels in critical edge\napplications characterized by limited computational resources,\nprioritizing both dependability and real-time performance.", "sections": [{"title": "I. INTRODUCTION", "content": "Deep neural networks (DNNs) have become increasingly\nimportant in a variety of application domains, including video\nanalytics, autonomous vehicle systems, and medical devices,\ndue to recent advances that improve their performance and\nenergy efficiency. However, practical reliability considera-\ntions make the widespread real-world deployment of DNN-\nbased systems difficult, particularly in many safety-critical\ndomains [1]. Recent research has shown that edge systems\nbased on DNNs are susceptible to transient hardware faults\nresulting from particle strikes or voltage droops and can\nsignificantly impact the reliability of safety-critical DNN-\nbased autonomous vehicles [2]. For example, a single soft error\nmay result in the misclassification of objects in the field of\nself-driving cars, which can cause the car to take inappropriate\nactions [3]. This issue may worsen with the technology node\nscaling of hardware accelerators for autonomous machine\ncomputing. Therefore, there is a need for designing and\nimplementing dependable DNN systems.\nOn the other hand, dependability in safety-critical real-time\nDNN-based embedded systems faces a unique challenge due\nto the requirements of low power, high throughput, and \"low\nlatency\" where traditional fault-tolerant techniques, which\ninclude redundant-based hardware/software/data, can result in\nhigh overheads in hardware cost, energy, and performance,\nmaking them impractical for real-time edge DNN systems.\nFor instance, an autonomous vehicle must process driving\ndata within a few milliseconds for real-world deployment\nand deploying traditional protection techniques may lead to\ndelayed response times, potentially leading to reaction-time-\nbased accidents. Given the resource constraints inherent in\nDNN-based edge applications and the critical need for real-\ntime responsiveness, there exists a compelling imperative to\ndevelop a customized error mitigation technique. This prompts\nthe central research question: Is it feasible to design highly\nfault-tolerant DNN models for safety-critical applications in\nembedded edge systems without incurring additional run-time\nand resource overhead?\nIn this paper, we present the advantages of quantization\nas an enhanced cost-free fault-tolerance feature for DNNs.\nQuantization is defined as the reduction of precision used\nto represent neural network parameters, typically from m\nbits to n bits (where m < n). Our observation reveals\nthat fault-free DNN neurons values typically fall within a\nlimited range, a small fraction of the total range offered by\ndata representation. Contrastingly, memory faults can lead\nto severe inference output corruptions when neurons values\nmanifest significantly higher than the expected range. Based\non this insight, we propose restricting the range of DNN\nparameters through quantization as a means of fault prop-\nagation prevention without incurring overheads. Specifically,\nwe endorse binarization, the extreme case of quantization, as\na promising solution to enhance DNN reliability. However,\nthe adoption of binarization presents a primary challenge in\nmaintaining the high accuracy of a DNN. Addressing this\nconcern, contemporary state-of-the-art (SOTA) Binary Neural\nNetworks (BNNs) demonstrate reasonable accuracy via strate-"}, {"title": "II. RELATED WORK", "content": "Modifying the neural network architecture has been intro-\nduced to increase its error resilience, which can be done either\nduring training or outside training. Dias et al. [6] proposed a\nresilience optimization procedure that involves changing the\narchitecture of the DNN to reduce the impact of faults. Schorn\net al. [7] introduced a resilience enhancement technique that\nreplicates critical layers and features to achieve a more homo-\ngeneous resilience distribution within the network. An error\ncorrection technique based on the complementary robust layer\n(redundancy mechanism) and activation function modification\n(removal mechanism) has been recently introduced in [8].\nHong et al. [9] introduced an efficient approach to mitigate\nmemory faults in DNNs by modifying their architectures using\nTanh [10] as the activation function. To mitigate errors, based\non the analysis and the observations from [11], Hoang et al.\n[12] presented a new version of the ReLU activation function\nto squash high-intensity activation values to zero, which is\ncalled Clip-Act. Ranger [13] used value restriction in different\nDNN layers as a way to rectify the faulty outputs caused by\ntransient faults. Recently, Ghavami et al. [14] introduced a\nfine-grained activation function to improve the error resilience\nof DNNs. Research ([15]) emphasizes the critical need to\ndetect errors in CNN execution to prevent their propagation\nto fully connected layers. They propose redesigning maxpool\nlayers for improved error detection and potential correction.\nLibabno et al. ([16]) conducted neutron beam experiments to\nassess the impact of quantization processes and data precision\nreduction on the failure rate of neural networks deployed on\nFPGAs. Their results show that adopting an 8-bit integer de-\nsign increases fault-free executions by over six times compared\nto a 32-bit floating-point implementation, highlighting the\nadvantages of lower precision formats in FPGA environments.\nInvestigating the reliability of CNNs, [17] analyzed the effects\nof reduced bit widths and different data types (floating-point\nand fixed-point) on network parameters to find the optimal\nbalance for CNN reliability. Additionally, [18] demonstrates\nthat implementing fault-aware training significantly improves\nthe resilience of Quantized Neural Networks to soft errors\nwhen deployed on FPGAs. Finally, Cavagnero et al. ([19])\nintroduce a zero-overhead fault-tolerant solution focusing on\nDNN redesign and retraining, which enhances DNN reliability\nagainst transient faults by up to one order of magnitude."}, {"title": "III. PRELIMINARIES AND DEFINITIONS", "content": "A. Deep Neural Networks\nDNNs are a type of machine learning model that is designed\nto predict an output as accurately as possible given an input\nX. DNNs are composed of multiple layers, where each layer\nreceives the output of the previous layer as its input. The input\ndata is sequentially passed through each layer, with each layer\nperforming a specific computation on the input and passing the\nresult to the next layer.\nThe individual layers of a DNN $F_i$ are made up of neurons,\nwhich perform mathematical operations on the input data X.\nEach neuron is connected to the neurons in the previous\nlayer $F_{i-1}$ via a set of weights $\\Theta_{i-1}$, which are determined\nduring the training phase of the DNN. The weights determine\nthe strength and direction of the connections between the\nneurons. The computations performed by each layer of a\nDNN are typically non-linear, which allows the network to\nmodel complex relationships between the input and output\ndata. The non-linearities in the layers are often introduced\nusing activation functions, which transform the output of each\nneuron in a non-linear way. The final layer of a DNN produces\nthe output prediction for the input data. Equation 1 illustrates\nthe DNN computation.\n$DNN(X) = (F_N \\circ \\Theta_N \\circ F_{N-1} \\circ \\Theta_{N-1}... \\circ F_1 \\circ \\Theta_1)(X)$                                                                                                                                                                                                          (1)\nThe DNN architecture is typically composed of a sequence\nof two types of layers, designed to capture the hierarchical\nstructure of the input data:\n\u2022\nThe first-type consists of convolution and linear layers.\nEach neuron in these layers $N^i$ is computed as a sum of"}, {"title": "IV. MOTIVATION: RELIABILITY ANALYSIS OF\nCONVENTIONAL BNNS", "content": "Despite the numerous advantages offered by BNNs, their\nresilience to memory faults has received little attention. As\na result, there is a pressing need to better understand the\nvulnerability of BNNs and their effective safeguards to these\ntypes of faults. Figure 1 illustrates the impact of memory\nbit-flip faults on a floating-point DNN, its corresponding\nquantized version, and a BNN, at varying memory bit-flip\nprobabilities (fault rates). In the DNN scenario, the accuracy\ndrops in the 1e-6 fault rate and becomes a random output\nin the 1e-4 value. However, in the BNN case, the accuracy\nremains nearly the same in 1e-5 and gets a 50% deviation\nfrom the baseline accuracy in the extreme case of 1e-4 fault\nrate. Our observation reveals fault-free neurons usually have\nvalues within a narrow range, significantly smaller than the\ntotal range in floating-point representation. We can state that\nthe memory faults will propagate to the DNN output only if\nthe affected neuron values exceed the expected range, leading\nto substantial output changes. Consequently, \"quantization\"\ncan then limit computation parameter ranges, serving as a\nbuilt-in mechanism to \"fault propagation prevention\".\nAdditionally, to comprehend the underlying vulnerability\nin BNN due to memory faults, we conducted an in-depth\nfault analysis of individual layers on a BNN [4]. Figure 2\nillustrates these comparative results. Among the layers, L-type\nlayers which are convolution layers exhibit the least significant\naccuracy drop, since all the parameters in the convolution\nlayers are binarized as described in Section III-C. However,\nthis is not the case for the remaining S-type layers and the\nfinal L-type layer, as their parameters are stored in floating-\npoint format. The remaining layers' accuracy starts deviating\nfrom the baseline in a lower fault rate. This aligns with the\nobservation that soft errors can result in substantial corruption\nof inference outputs in floating-point's DNNs, especially when\nthese errors manifest as values exceeding the expected range.\nBinary values, constrained to a limited range, and bit-flips\nhave no opportunity to manifest as large value deviation.\nIt is noteworthy that even after binarizing all Convolution\nlayers, their memory footprint still dominates compared to\nother layers while exhibiting the smallest drop."}, {"title": "V. PROPOSED SELECTIVE QUANTIZATION OF BNNS", "content": "To enhance the reliability of BNNs while addressing com-\nputation overhead, we introduce a novel selctive quantization\nmethod for the S-type layers, integrated with the first and last\nL-type layers within a BNN. This preserves baseline accuracy\nwithout adding extra computational overhead."}, {"title": "A. Dequantization and Quantization Layers", "content": "The following equation illustrates the proposed quantization\nand dequantization layers of ZOBNN. The quantization layer\ntransforms the real floating-point number $x_r$ into its quantized\ninteger counterpart $x_q$ using the parameter $\\Delta$. The parameter\n$\\Delta$ is determined by the equation $\\frac{max(\\Theta)}{2^{\\# bits-1}-1}$. This signifies that\nwe utilize the maximum integer, $2^{\\# bits-1}-1$, to represent\nthe largest parameter in the model, denoted as $max(\\Theta)$.\nConsequently, the parameters are quantized within the range\nof $-max(\\Theta)$ to $max(\\Theta)$.\n$Q(x_r) = round(x_r/\\Delta) = x_q$ (7)\nNext, the dequantization function in Equation 8 should\ntransfer the quantized number to integer format. Also, it should\nbe such that $Q(D(x)) = x$. Equation 9 illustrates this reciprocal\nrelation between Q and D.\n$D(x_q) \\approx \\Delta \\times (x_q) = X_r$ (8)\n$Q(D(x_q)) \\approx round(\\frac{\\Delta \\times (x_q)}{\\Delta}) = X_q$ (9)"}, {"title": "B. Input Layer", "content": "Following the application of common quantization to the\ninput layer, the input is processed through the Q layer, de-\nnoted as $L_1 \\circ Q(\\Theta_1) \\circ Q \\circ x_r$. By omitting the Q layer in this\nequation, the layer receives its input directly in the $x_r$ format.\nThe subsequent equation elucidates the quantization method\nemployed for this layer.\n$conv(X) = X * W = X * W \\times \\Delta = \\Delta \\times X_r * W_q$\n$conv(\\Delta \\times X_q) = \\Delta \\times \\gamma$\n$conv(X_q) = \\gamma$ (10)"}, {"title": "C. Output Layer", "content": "The output layer in the majority of BNNs typically remains\nunbinarized. As a result, this layer will undergo standard\nquantization. In the standard quantization, the output layer is\nsucceeded by a D-type layer. The removal of this D-type layer\nproves to be straightforward in neural networks dedicated to\ndetection tasks, as it is succeeded by an argmax function. No-\ntably, $Argmax(D(x)) = Argmax(\\frac{x}{\\Delta}) = Argmax(x)$, thereby\nautomatically eliminating the necessity for the dequantization\nfunction."}, {"title": "D. Binarized Layer", "content": "The sign layer B serves as the antecedent function for the\nbinarized layers in the network. Following standard quanti-\nzation, the input to this function is derived from a D layer,\ndenoted as $B \\circ D$. If the dequantization layer is excluded, the\ninput to the sign layer becomes $x_q$. The subsequent equation\ndepicts this layer when receiving an input of $x_q$. The sole\nmodification required for this function involves dividing the\noriginal constants by $\\Delta$, yielding identical results for an input\nof $x_r$.\n$\\frac{clamp(X, 1, -1) +1}{delta}$\n$sign(\\Delta \\times X) = \\frac{clamp(X_q\\Delta, 1, -1) + 1}{delta}$\n$sign(X_q) = \\frac{clamp(X_q, \\frac{1}{\\Delta}, -\\frac{1}{\\Delta}) + \\frac{1}{\\Delta}}{delta}$\n$sign(X_q) = \\gamma$ (11)"}, {"title": "E. Other Layers via SOTA BNN", "content": "All the S-type layers should work with both $x_q$ and $X_r$\ninputs and give their output in $x_q$ format. Here we show the\nquantization of S-type layers for two common layers, batch-\nnorm and rprelu, in SOTA BNNS.\n1) batch-norm: The batch normalization layer is a ubiqui-\ntous component in BNN architecture, comprising four param-\neters available for quantization. When dealing with quantized\ninput, all four parameters are quantized, whereas in the case of\nreal input, only W and B undergo quantization. This approach\nensures a consistently quantized output regardless of the input\nnature. Equation 12 illustrates the quantization of this function\nfor an input $x_q$, and equation 13 does so for an input $X_r$.\n$batchnorm(X) = W \\times \\frac{X - \\mu}{\\sigma} + B$\n$\\frac{\\Delta \\times X_q - \\Delta \\times \\mu_q}{\\Delta \\times \\sigma_q} + \\Delta \\times B_q$\n= $\\Delta \\times W \\times \\frac{X_q - \\mu_q}{\\sigma_q} + B_q$\n$\\gamma$\n$\\frac{X_r - \\mu_r}{\\sigma_r} + B$\n$\\frac{X_r - \\mu_r}{\\sigma_r} + \\Delta \\times B_q$\n$\\frac{X_r - \\mu_r}{\\sigma_r} + B_q$\n$\\frac{X - \\mu}{\\sigma}+ B1) + B2$\n$\\times (X + B1) + B2$\n$\\times 1) + B2$ (12)\n(13)\nWhen the input $x_q$ is more than -B1 and is constant otherwise. The following two\nequations illustrate the quantization of these functions given\ninputs of $x_q$ and $x_r$. In the first scenario, the parameters B1\nand B2 are quantized, while in the second scenario, it's the\nW and B2. This quantization ensures a quantized output in\nboth cases.\n$rprelu(X) = W \\times (X + B1) + B2$ (14)\n(15)"}, {"title": "VI. EXPERIMENTAL FAULT SIMULATION SETUP", "content": "To evaluate the error tolerance of a DNN, intentional simu-\nlated faults are introduced at various levels of the abstraction.\nEmploying software-level fault simulation proves to be the\noptimal approach due to its speed and reasonably accurate re-\nsults, especially considering the typically prolonged simulation\ntime for large networks. In this study, a dynamic software-level\nfault injection tool on top of PyTorch framework is developed\nthroughout the BNN inference process. Fault models are\nimplemented as native tensor operations to minimize any\npotential performance impact. This involves flipping every bit\nin the memory with a probability denoted as P, referred to as\nthe \"fault rate\u201d in our investigation. Subsequently, to ensure\nrobustness, this fault study is repeated 500 times, and the\naverage across all iterations is calculated.\nB. Fault Rate Value\nTo maintain the generality of our investigation across differ-\nent memory hardware and environmental setups, we analyze\nthe impact of faults within the fault rate range of 1e-6 to 1e-3,\naligning with recent experiments. Reagen et al. [21] observed\nSRAM fault rates of approximately 1e-5 at a voltage reduction\nto 0.7 volts. Nguyen et al. [22] delved into DRAM intricacies,\nrevealing fault rate fluctuations from 0 to 2e-3 at a 1s refresh\nrate within the temperature range of 25\u00b0C to 90\u00b0C. They also\ndemonstrated fault rate variations from 1e-7 to 2e-3 at 60\u00b0C\nby adjusting the refresh rate from 1s to 10s.\nThe fault study was carried out within FracBNN[4] and\nDorefanet[5]--the state-of-the-art BNNs. Our investigation\nfocused on two well-established image classification datasets,\nCIFAR-10 and ImageNet. FracBNN, based on the ResNet\narchitecture, demonstrates a baseline accuracy of 89.1% on"}, {"title": "VII. EXPERIMENTAL RESULTS", "content": "Figure 3 presents a dependability comparison among the the\nbaseline BNN, an equivalent floating-point network matched\nwith the binary counterpart, and proposed 16-bit quantized\nZOBNN under various fault rates. In the case of DorefaNet,\nZOBNN achieves remarkable dependability results, maintain-\ning accuracy above 78% even with a fault rate of 1e-4, while\nthe baseline BNN accuracy drops below 48%. For FracBNN,\nthe baseline BNN accuracy reaches 54% at the 4e-5 fault\nrate and declines to 34% at the 1e-4 fault rate. However,\nZOBNN attains 78% accuracy at a fault rate of 4e-5 and\nexceeds 58% in the extreme case of a 1e-4 fault rate. Notably,\nthe floating-point counterpart consistently generates random\noutputs in all scenarios. This highlights ZOBNN as presenting\na remarkable 5X enhancement in robustness, on average,\ncompared to conventional floating-point DNNS.\nFigure 4 depicts a comparative analysis of the accuracy dis-\ntribution between the base Fracbnn and the proposed ZOBNN.\nWhile the median remains consistent in both instances, our\napproach exhibits a significantly narrower range of error. This\ntightens the fault margin, resulting in a notably higher average\naccuracy.\nTable I illustrates the impact of ZOBNN on the memory\nfootprint of FracBNN. ZOBNN enables the network to achieve\nquantization up to 8 bits in CIFAR-10 mode, incurring a\n20.1% memory reduction. Furthermore, ZOBNN achieves 12-\nbit quantization on the ImageNet dataset, resulting in a 34%\nmemory reduction."}, {"title": "VIII. CONCLUSION", "content": "In the dynamic realm of edge deep learning applica-\ntions, reliability is paramount. This paper delves into fault\ninjection experiments, unveiling the inherent robustness of\nBNNs against memory faults. A novel deliberately quanti-\nzation method takes center stage, fortifying BNN reliability\nwithout burdensome computational costs. Experimental results\nhighlight the transformative power of proposed ZOBNN as\nit significantly elevates BNN reliability while simultaneously\nstreamlining memory footprint. This breakthrough opens doors\nfor DNN deployment in the harshest embedded environments,\nwhere fault rates run high and resources are constrained."}]}