{"title": "Aggressive Post-Training Compression on Extremely Large Language Models", "authors": ["Zining Zhang", "Yao Chen", "Bingsheng He", "Zhenjie Zhang"], "abstract": "The increasing size and complexity of Large Language Models (LLMs) pose chal-lenges for their deployment on personal computers and mobile devices. Aggressive post-training model compression is necessary to reduce the models' size, but it often results in significant accuracy loss. To address this challenge, we propose a novel network pruning technology that utilizes over 0.7 sparsity and less than 8 bits of quantization. Our approach enables the compression of prevailing LLMS within a couple of hours while maintaining a relatively small accuracy loss. In experimental evaluations, our method demonstrates effectiveness and potential for practical deployment. By making LLMs available on domestic devices, our work can facilitate a new era of natural language processing applications with wide-ranging impacts.", "sections": [{"title": "Introduction", "content": "The emergence of large language models (LLMs) has been a significant breakthrough in the field of natural language processing (NLP) due to their remarkable capability in enhancing daily work productivity, as exemplified by ChatGPT OpenAI [2023]. Nevertheless, the successful deployment of these models necessitates the compression of vast amounts of human knowledge into neural network models containing hundreds of billions of parameters. As a result, such LLMs can only be accommodated on a limited range of platforms, and deployments on lower-end devices, such as personal computers, require aggressive parameter compression. Despite a plethora of proposed neural network compression methods in the literature that have demonstrated promising results on relatively small models Li and Louri [2021], Nagel et al. [2020], Hubara et al. [2021a,b], Kwon et al. [2022], applying them to LLMs demands tens of thousands of GPU hours, rendering them impractical.\nConsequently, there is a need for further research to develop novel and efficient post-training com-pression methods that cater to the unique requirements of LLMs. Such methods should also prioritize retaining the integrity of the model's performance while reducing its size and complexity. The com-pression of deep neural networks has been a crucial area of research in recent years. However, scaling up these models to handle massive amounts of data poses significant challenges. In this context, OPTQ Frantar et al. [2023] represents a ground-breaking advancement, as it offers a professional and formal solution for quantizing the massive OPT-175B Zhang et al. [2022] or BLOOM-176B Scao et al. [2022] while maintaining high accuracy. By utilizing the traditional Optimal Brain Surgeon (OBS) Hassibi et al. [1993] method, OPTQ can update remaining weights while simultaneously quan-tizing the weight with the least information, thereby significantly reducing computational overheads. To further improve the efficiency of this process for large models, OPTQ pre-computes the inverse of"}, {"title": "Background and Related Work", "content": ""}, {"title": "Post-training Model Compression", "content": "The process of post-training quantization and pruning typically involves the use of a small calibration dataset to adjust quantization parameters or uncompressed weights Hubara et al. [2021b], Kwon et al. [2022]. Typically, compression is conducted layer-by-layer with the goal of minimizing the optimization objective:\n$\nargmin_{W_c} ||W_eX_e \u2013 W_cX_e||^2\n$\n(1)\nHere, $W_e$ and $W_c$ denote the original weight and compressed weight in layer $l$, respectively, and $X_e$ represents the input to layer $l$. Based on this objective, various methods have been developed. AdaRound Nagel et al. [2020] replaces the traditional round-to-nearest quantization method by optimizing with a regularization term, while BRECQ Li et al. [2021] and Kwon et al. [2022] utilizes Fisher information for layer quantization or pruning. AdaPrune Li and Louri [2021] prunes model weights based on their magnitudes and compensates for errors by updating unpruned weights using gradient descent."}, {"title": "Hessian-based Weight Update", "content": "Similar to Adaprune, Optimal Brain Compression (OBC) Frantar and Alistarh [2022] compensates for uncompressed weights during compression. However, instead of using Stochastic Gradient Descent (SGD) Ruder [2016], OBC employs the Hessian matrix to directly calculate weight updates. This approach is more efficient than SGD and leads to a higher update frequency.\nLet us consider the weight matrix of the current layer, denoted as $W \u2208 R^{d_{row}\u00d7d_{col}}$, where $d_{row}$ and $d_{col}$ are the dimensions of the weight matrix, and $X \u2208 R^{d_{col}\u00d7N}$ is the input with $N$ calibration samples. To simplify notation, we omit the subscript $l$. We apply a row-wise pruning error modified from Equation 1: $||WX \u2013 \\hat{W}X||^2$. We then calculate the Hessian matrix of the layer output over $W_{i,:}$, which is represented as $H = 2XX^T$. When focusing on only one row $w = W_{i,:}$, the error for this specific row becomes $E = ||wX-(w+\u03b4w)X||^2$. Applying Taylor series expansion gives us:\n$\n\\frac{\\delta E}{\\delta w} = \\frac{1}{2} \\frac{\\partial E}{\\partial w}^T \u00b7 \u03b4w + \\frac{1}{2} \u03b4w^T \u00b7 H \u00b7 \u03b4w + O(||\u03b4w||^3)\n$\n(2)\nAssuming the network is sufficiently trained (i.e. $\\frac{\\partial E}{\\partial w} = 0$) and ignoring the 3rd-order term, the weight updating objective becomes\n$\nmin_{\u03b4w} {\u03b4w^T H \u03b4w | e_p^T \u03b4w + w_p = 0}\n$\n(3)"}, {"title": "Sequentially-Pruning-All Assumption", "content": "In this subsection, we provide an illustration of the assumption of the sequentially-pruning-all using Figure 2. The figure highlights the differences in weight updates between Hessian-based quantization and pruning. The left side of the figure displays yellow blocks representing quantized/pruned weights from previous steps and blue blocks representing yet uncompressed weights. On the right side of the figure, the upper triangular matrix $H_{\\theta}$ is the precomputed Cholesky decomposition of $H^{-1}$, where the $i$th row is derived from $H^{-(1:i\u22121)}$ (The subscript representing the columns $(1 : i \u2212 1)$ are already updated in $H^{-1}$). The sketched blocks indicate currently processed weights or the $H_{\\theta}$ vector used for"}, {"title": "Sparsity Scheduler", "content": "As discussed in Section 3.1, $H_{\\theta}$ provides an estimate of the weight update terms, and its rows are also involved in the loss calculation in Equation 6. Motivated by these observations, we propose to estimate the mean square error (MSE) for the entire layer after pruning, which can then be used to allocate different sparsity values to different layers. Specifically, we define the MSE estimation for layer $l$ as:\n$\nL_l = \\frac{1}{d_{row}d_{col}} \\sum_{1\\<i\\<d_{row}} \\sum_{1\\<p\\<d_{col}} \\frac{1}{2} \\frac{W_p^2}{[H]^{-1}_{pp}}\n$\n(11)\nAt first glance, this estimation may appear counterintuitive since it averages all the scores instead of taking the sum of estimated losses for the whole layer. This is because we allocate sparsities for different layers based on their expected loss. If we were to use the sum of the loss to represent a layer, some layers with high average losses but small numbers of elements might be allocated with high sparsity values, leading to a higher probability of pruning elements with high losses. It is worth mentioning that we opted to compute the average of scores for each layer instead of globally ranking all scores for all weights. This decision arose due to memory constraints, as the scores for all weights easily exceed the available memory.\nDuring our experimentation, we observed that the scores of different layers follow a exponential distribution, as illustrated in Figure 4. To assign sparsities appropriately, we apply the k-means algorithm Hartigan and Wong [1979] to partition the layers into $k$ groups based on log $L_e$, i.e.:\n$\nargmin_{\\mu_1...,\\mu_k} \\sum_{i=1}^T \\sum_{j=1}^k z_{ij} (|| log L_{e_i} \u2013 log \\mu_j ||^2)\n$\n(12)"}, {"title": "Experiments", "content": ""}, {"title": "Settings", "content": "To perform pruning on large language models (LLMs), we employ the open-source models BLOOM-176B Scao et al. [2022] and OPT Zhang et al. [2022] (OPT-125M, OPT-6.7B, OPT-30B, and OPT-66B). We conduct experiments on a single Nvidia A100 GPU, fixing the sparsity setting to 0.7 during the testing of SparseGPT. Our layerwise sparsity scheduler is restricted to output only final sparsities greater than 0.7. When applying quantizations, we quantize the weights to 4 bits. To calibrate our method, we select 128 sequences with 2048 tokens from C4 Raffel et al. [2020], while we use WikiText2 Merity et al. [2016] as the validation dataset and report perplexity as the metric. For comparison with baselines, we compare our method against SparseGPT, which is currently the state-of-the-art extremely-large LLM pruning technique, and a naive layer-wise sparsity scheduler based on the sequential order of layers."}, {"title": "Sparsity vs. Perplexity", "content": "In our experiments, we conduct sparsity scheduling on several models including OPT-125M, OPT-6.7B, OPT-30B, OPT-66B, and BLOOM-176B. The time taken by our scheduler is no longer than that of the pruning process. For instance, the sparsity scheduling for OPT-66B took 1.3 hours while the pruning process took 1.5 hours; Scheduling for BLOOM-176B took 3.4 hours and pruning took 4.5 hours. shows the perplexity scores of various language models under \u2265 0.7 sparsity settings. Our method is to achieve no less than 70% overall sparsity (the overall sparsity depends on the score distribution but is close to 0.7) using the sparsity scheduler introduced in Section 3.2. We use SparseGPT with 0.7 sparsity as the baseline. Both methods were evaluated with and without quantization to 4 bits (4b). The results show that our method outperformed SparseGPT in all cases in terms of perplexity, except for OPT-6.7B. Additionally, the quantization to 4 bits did not significantly impact the performance of either method.\nThe cause of our method's inferior performance in OPT-6.7B is illustrated in Figure 4. While most other models exhibit a significant flat region across a large proportion of layers, OPT-6.7B has a more linearly varying loss among ranked layers. This implies that allocating an aggressive range of sparsities ([0.6, 0.8]) is inadequate for all models. Therefore, we instead employ a narrower range of sparsities [0.65, 0.72], which produces the results displayed in However, as of now, there is no suitable quantitative metric to determine the optimal sparsity range, and it remains a topic for future research."}, {"title": "Comparing with Naive Layer-order Sparsity Scheduler", "content": "As described in Section 2.3, various techniques exist for layer-wise sparsity scheduling in neural network pruning. However, these approaches require extensive computations and are not viable for extremely large LLMs. In addition to comparing our method with SparseGPT, which applies uniform sparsity to all layers, we also employed a naive implementation of layer-wise sparsity scheduling. The intuition was that the errors of earlier layers accumulate towards the final output, so we assigned smaller sparsities to earlier layers and larger ones to those closer to the output. Specifically, we linearly applied the same sparsity range as our scheduler ([0.6, 0.8]) to all the layers sequentially. We performed this experiment on OPT-30B and presented the results in , where 'Layer-order' refers to the naive sparsity scheduler, which significantly harmed perplexity. To investigate the cause, we plotted our estimation scores based on the layer order in Figure 5. The graph showed that despite error accumulation across the layers, loss pruning presents an increasing behavior with layer orders. This indicates that higher layers incur more losses due to pruning. We suspect that the information contained in LLM layers becomes more critical as we approach the final output, thereby validating the effectiveness of our score-based sparsity scheduler."}, {"title": "Conclusion", "content": "In this paper, we proposed a novel score-based sparsity scheduler for pruning large language models (LLMs) that outperform existing techniques. Our method leverages the information provided by previous weight updates to estimate the expectation of weight updating terms under all possible pruning masks, allowing us to choose an optimal sparsity level for each layer. We demonstrated the effectiveness of our approach by comparing it with SparseGPT, the current state-of-the-art LLM"}]}