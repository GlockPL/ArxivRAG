{"title": "Personalized and Sequential Text-to-Image Generation", "authors": ["Ofir Nabati", "Guy Tennenholtz", "ChihWei Hsu", "Moonkyung Ryu", "Deepak Ramachandran", "Yinlam Chow", "Xiang Li", "Craig Boutilier"], "abstract": "We address the problem of personalized, interactive text-to-image (T2I) generation, designing a reinforcement learning (RL) agent which iteratively improves a set of generated images for a user through a sequence of prompt expansions. Using human raters, we create a novel dataset of sequential preferences, which we leverage, together with large-scale open-source (non-sequential) datasets. We construct user-preference and user-choice models using an EM strategy and identify varying user preference types. We then leverage a large multimodal language model (LMM) and a value-based RL approach to suggest a personalized and diverse slate of prompt expansions to the user. Our Personalized And Sequential Text-to-image Agent (PASTA) extends T2I models with personalized multi-turn capabilities, fostering collaborative co-creation and addressing uncertainty or underspecification in a user's intent. We evaluate PASTA using human raters, showing significant improvement compared to baseline methods. We also release our sequential rater dataset and simulated user-rater interactions to support future research in personalized, multi-turn T2I generation.", "sections": [{"title": "1. Introduction", "content": "Advances in text-to-image (T2I) generation, fueled by powerful diffusion models [9, 14, 27, 41, 42, 59, 60, 62], have unlocked unprecedented possibilities for image generation, as users can readily translate textual descriptions into stunning visuals. That said, capturing precise user intent remains a major challenge [27, 58]. As such, single-turn T2I generation may fail to encapsulate a user's nuanced and evolving image conception. This is especially true for complex or abstract concepts, where a user's initial prompt may not suffice in achieving a desired visual representation.\nAddressing this challenge requires moving beyond the paradigm of one-shot T2I generation towards a more iterative and interactive process [29, 54, 56], e.g., a collaborative setting where an assistive agent interacts with a user, guiding them through a series of refinements of their initial prompt. This interactive setup could allow for a more nuanced elicitation of the user's intent, gradually shaping the generated images towards a desired outcome.\nTo this end, we introduce a Personalized And Sequential Text-to-image Agent (PASTA), which learns from user preference feedback to guide the user through a sequence of prompt expansions, iteratively refining the generated image. This sequential approach (see Figure 1) allows users to articulate their vision more completely by gradually reducing uncertainty or underspec-ification in their original prompt. Our framework leverages the power of large multimodal language models (LMMs) [2, 49] and reinforcement learning (RL) [12, 48] to facilitate personalized co-creation.\nOur approach for PASTA involves a multi-stage data collection and training process. We first collect multi-turn interaction data from human raters with a baseline LMM. Using this sequential data, as well as large-scale, open source (single-turn) preference data, we train a user simulator. Particularly, we employ an EM-strategy to train user preference and choice models, which capture implicit user preference types in the data. We then construct a large-scale dataset, which consists of trajectories of interactions between a simulated user and the LMM\u00b9. Finally, we leverage this augmented data, encompassing both human and simulated interactions, to train PASTA our value-based RL agent, which presents a sequence of diverse, personalized"}, {"title": "2. Background and Problem Setup", "content": "We begin with a short background on diffusion models, LMMs, and RL. We then formulate our problem setup."}, {"title": "2.1. Background", "content": "Diffusion models [9, 14, 18, 19, 27, 41, 42, 59, 60, 62] have become a prominent approach for T2I generation. These models add Gaussian noise to an image until it becomes pure noise, and are then trained to reverse this process, iteratively removing noise to reconstruct the image. This denoising process is guided by a prompt, enabling the generation of images that semantically align with the textual description.\nLarge Multimodal Models (LMMs) [2, 49] use Transformer models [52], and are trained to predict the next token in a sequence, where tokens can represent textual or visual information. These models learn joint representations of text and images, making them useful for prompt engineering in interactive T2I systems, as they allow us to modify generated outputs based on previously generated responses.\nReinforcement Learning (RL) [12, 48] frameworks train agents to make a sequence of decisions in an environment to maximize cumulative reward. Value-based RL methods learn a value function that estimates the expected cumulative reward for taking a particular action in a given state. In our work, an RL agent learns to select actions (prompt expansions) that lead to higher user satisfaction."}, {"title": "2.2. Problem Formulation", "content": "We consider an interactive, sequential T2I decision problem in which a user engages with an agent, visualized in Fig. 1. At time t = 0, the user issues an initial prompt (e.g., \"A white cat\") intended to capture a target image. At each turn t > 1 the agent proposes L prompt expansions, which are fed to a T2I model. The T2I model then generates M images for each prompt."}, {"title": "2.3. RL Formulation", "content": "Our problem can be formulated as a latent contextual MDP [16, 26], where U is the latent context space. The state space consists of the user-agent interaction history, with the state (history) $h_t$ at time t given by:\n$h_t = (p_0, {\\{p_{\\ell,1}\\}_{{\\ell}=1}^{L}, {\\{I_{m,\\ell,1}\\}_{m=1}^{M}\\}_{{\\ell}=1}^{L}, c_1,..., {\\{p_{\\ell,t}\\}_{{\\ell}=1}^{L}, {\\{I_{m,\\ell,t}\\}_{m=1}^{M}\\}_{{\\ell}=1}^{L}, c_t})$,\nthe action space is any selection of L prompts, transitions are induced by the user choice model C and the T2I model G, and reward is given by a user utility function R. See Appendix A for a full description of this latent MDP.\nA stochastic policy $\\pi : H \\rightarrow A_{PL}$ maps interaction histories to a distribution over slates of L prompts. The value of a policy $\\pi$ is its expected cumulative sum of rewards over users and initial prompts, i.e., $v^{\\pi} = E_{u\\sim v_0} [\\sum_{t=1}^{H} r_t | u, \\pi]$. An optimal policy $\\pi^* \\in arg\\max_{\\pi} v^{\\pi}$ maximizes the value. The state-action value function for any $h \\in H$ and $P \\in P^L$ is $q_t(h, P) = E_{u\\sim v_0} [\\sum_{t'=t}^{H} r_{t'} | u, h_t = h, P_t = P, \\pi]$."}, {"title": "3. PASTA: Personalized And Sequential Text-to-image Agent", "content": "We solve the sequential prompt expansion problem using RL. Our Personalized And Sequential Text-to-image Agent (PASTA) engages with a user to personalize the prompt and maximize (latent) user utility. The user's type $u \\in U$ is unknown to the agent and must be inferred during the interaction. This framework is related to meta-RL [11, 13, 57, 64], where each episode (or meta-episode) samples a latent MDP from a predefined problem distribution. In our setting, the agent must adapt within H steps to the unknown MDP, and optimize the reward accordingly. This requires balancing exploration and exploitation: the agent must take actions that, on the one hand, provide images that reflect (its estimate of) the user's preferences, and on the other, improving its estimate of those preferences by exploring other types of expansions/images. This can be viewed as an implicit form of preference elicitation [3, 7, 23, 32, 43]."}, {"title": "3.1. Candidate Action Generator And Selector", "content": "The state space of user interaction histories serves as sufficient statistic (i.e., belief over users' types [1]). To solve our problem effectively, each interaction with a user should provide the agent sufficient information about the user (e.g., through value of information [4]). This requires the action space be rich and diverse enough to enable information gain.\nA straightforward approach to action space design uses an LMM to construct action candidates (in our case, prompt expansion candidates). Specifically, we use an LMM to process the current interaction history and generate a broad set of $L_c > L$ candidate prompts from which a slate of L prompts can be chosen. Generating a large set of candidates has been shown to introduce diversity and induce exploration [51]. Instead of relying on a given LMM to directly output L prompts, we encourage diversity by generating $L_c$ prompts, and then selecting the L-subset our agent deems optimal. This, in turn, expands the effective action space our agent can leverage during training."}, {"title": "3.2. Value-Based Candidate Selector", "content": "We use a state-action value function to define our selector policy, $\\pi_s$. Given candidates $\\{p_i\\}_{i=1}^{L_c}$ and a state-action value model $q(h, P)$ parameterized by $\\phi$, we define the selector policy by\n$\\pi_{s,\\phi}(h) \\in arg\\max_{P \\in P_L({\\{p_i\\}})} q(h, P),$\\nwhere $P_L(X) = {\\{P \\in X : |P| = L\\}}$ is the set of all possible slates of size L. Enumerating $P_L({\\{p_i\\}})$ is, of course, computationally expensive when $L_c$ is large (a detriment to both training time and inference efficiency). Inspired by Ie et al. [21], we decompose the value function into a (weighted) average of prompt-values $f_{\\phi} : H \\times P \\rightarrow \\mathbb{R}$. Specifically, we compute the value of each prompt, and estimate the value of a slate by:\n$q_{\\phi}(h, P) = \\frac{1}{L} \\sum_{p \\in P} f_{\\phi}(h, p).$\\nThis approximation reduces the exponential complexity of finding the best slate to $O(L_c \\log L_c)$ (i.e., by sorting the prompt values over the candidate set). See Fig. 2 for a schematic of the PASTA policy and value function."}, {"title": "3.3. Algorithmic Details", "content": "PASTA first prompts an LMM $\\pi_c$ (i.e., candidate generator) to generate $L_c$ candidates ${\\{p_i\\}}_{i=1}^{L_c} \\sim \\pi_c(h)$. It then uses its candidate selector $\\pi_s$ to select a slate P of prompts according to Equation (1).\nWe train PASTA using implicit Q-Learning (IQL) [25], which estimates the TD error with the Bellman optimality operator. IQL employs a soft estimate with a value function trained to approximate the high expectile without assessing state-actions"}, {"title": "4. PASTA Dataset", "content": "Training PASTA relies on the availability of sequential user-agent interaction data. While single-turn T2I preference data is readily available [24, 27, 38, 58], sequential datasets are not. Hence, we collect human-rater data for our sequential setup, and further enrich it with simulated data. Below, we describe our data creation process. All of our datasets are open-sourced here: https://www.kaggle.com/datasets/googleai/pasta-data."}, {"title": "4.1. Human Rater Sequential Data", "content": "We use human raters to gather sequential user preferences data for personalized T2I generation. Participants are tasked with interacting with an LMM agent for five turns. Throughout our rater study we use a Gemini 1.5 Flash Model [49] as our base LMM, which acts as an agent. At each turn, the system presents 16 images, arranged in four columns (L = 4, M = 4), each representing a different prompt expansion derived from the user's initial prompt and prior interactions. Raters are shown only the generated images, not the prompt expansions themselves.\nAt session start, raters are instructed to provide an initial prompt of at most 12 words, encapsulating a specific visual concept. They are encouraged to provide descriptive prompts that avoid generic terms (e.g., \u201can ancient Egyptian temple with hieroglyphs'instead of \u201ca temple\u201d). At each turn, raters then select the column of images preferred most (see the UI used in Appendix B); they are instructed to select a column based on the quality of the best image in that column w.r.t. their original intent. Raters may optionally provide a free-text critique (up to 12 words) to guide subsequent prompt expansions, though most raters did not use this facility.\nAfter five turns, raters enter an evaluation phase where they answer questions about each turn, including: (1) re-confirmation of their preferred columns; (2) quantifying whether the image slate in the current turn is better than the previous; and (3) a free-text explanation for their selection. This process yields a dataset comprising sequential interaction trajectories with user preference feedback and provides valuable insight into the dynamics of multi-turn, personalized T2I generation. See Appendix B for a comprehensive description of the rater study."}, {"title": "4.2. Simulated Sequential Data", "content": "Simulated data can drastically improve performance of RL agents [15, 22, 28, 33, 61]. For this reason, we enrich the rater data above with additional simulated agent-user interaction data. To do so, we develop a user model that encodes distinct user types that reflect a range of preferences. This model has two components: (1) A user choice model which predicts the image column a user selects; and (2) a user utility model which predicts a user's satisfaction with an image slate. We outline details of the user model in Section 5.\nWe begin by sampling a user and initial prompt from a joint prior. We use a randomized exploration policy for the agent, encouraging diverse interactions to distinguish preferences across user types. At each turn in a simulated trajectory, the agent proposes a slate of prompt expansions, and the user (via the choice model) selects their preferred prompt. The user utility model is then used to assign a satisfaction score for the images generated using the selected prompt. This process is repeated for five turns (see Appendix C for further details and analysis of our data generation process). Augmenting our human-rater data with this simulated data allows for more robust training of PASTA, as we show later in Section 6."}, {"title": "5. User Model", "content": "The data generation process described in Section 4.2 requires a user simulator consisting of both user choice and utility models. We leverage our sequential human-rater data (Section 4.1), together with large-scale open-source single-turn T2I preference data [24, 38, 58], to build these models."}, {"title": "5.1. A Parametric User Model", "content": "To train our user utility and choice models, we employ a parameterized score model $s_{\\theta} : U \\times I \\times P \\leftrightarrow [0, 1]$ to serve as the backbone of our user simulator. We assume a discrete set of K (unknown) user types, i.e., U = {1, 2, . . ., K}.\nExploiting Assumptions 1 and 2, we define the utility of a user type k over an arbitrary image slate ${\\{I_{m,\\ell,t}\\}_{m=1}^{M}}$ that associates to a prompt p using the following model:\n$R_{k,\\theta}^{\\ell,t} := R_{\\theta} ({\\{I_{m,\\ell,t}\\}_{m=1}^{M}} | u=k, p) = Agg(s_{\\theta}(k, p, I_{1,\\ell,t}), ..., s_{\\theta}(k, p, I_{M,\\ell,t})),$\nwhere Agg is an aggregator (e.g., average, max, Softmax sampler) operator. User choice probabilities are given by\n$C_{\\theta}(k, p, {\\{I_{m,1,t}\\}_{m=1}^{M},..., {\\{I_{m,L,t}\\}_{m=1}^{M}}) = Softmax(\\tau_{R_{1,t}}^{k,\\theta},...,\\tau_{R_{L,t}}^{k,\\theta}),$\nwhere $\\tau_{\\ell} = \\frac{T_{\\ell}}{\\sqrt{\\sum_j I(I^{\\ell}_{1,t}>I^{\\ell}_{j,t})}}$ is a parameterized temperature constant which depends on the scores of different image columns. Such a temperature parameterization ensures our user choice model satisfies Assumption 2, while allowing for greater flexibility in modeling.\nModel Architecture. To balance model capacity and computational efficiency, our score function adopts pre-trained CLIP [39] text and image encoders, followed by user encoders (a head for each user type). The user encoder transforms CLIP embeddings into a user-type-specific representation that captures individual preferences for images and prompts. The final score of an image-prompt pair is the inner product of the (user-type-specific) image embedding and corresponding text embedding, multiplied by a learned temperature parameter (see a schematic of our arch. in Fig. 2)."}, {"title": "5.2. User Model Training Data", "content": "To train our user model we use two types of data: (1) the sequential human-rater data collected by the procedure described in Section 4.1, and (2) a large-scale, open-source, single-turn dataset of user preferences over pairs of images. As we will demonstrate in Section 6, including single-turn data for training substantially improves model quality, suggesting that user utilities oftentimes remain stationary (for which the single-turn data already captures most of the basic user-image preferences). While single-turn data alone is insufficient to train the multi-item user choice model in Equation (4), it remains valuable. We leverage it to simplify the training process by focusing on the direct relationship between single-turn data and user utilities. However, the user choice model itself requires the nuances captured in sequential data, so we fine-tune our choice model $C_{\\theta}$ with sequential data. For a more detailed explanation of how our user choice and utility models can be effectively trained with both sequential and single-turn data, please refer to Appendix D."}, {"title": "5.3. User Model Training Method", "content": "We train our user model by maximizing likelihood of user preferences and choices in the datasets. Our parametric model is optimized to induce scores that are used to predict preferences and user choices. Of course, in our rater and open-source datasets, user types are unknown-only preference labels are provided. As discussed above, we assume that a reasonably small set of K user types suffices to explain preference diversity across the data-generating population. Let $D = {\\{x_i, y_i\\}}_{i=1}^{N}$ be a dataset in which each example, in its most general form, comprises a prompt and set of images $x_i = (p_i, {\\{I_{i,m,\\ell}\\}_{m=1}^{M},{\\ell}=1}^{L})$, and labels $y_i$ reflecting an annotator's preference. Details of the data types, their modeling and loss functions are given in Appendix D.\nInspired by the work of Chidambaram et al. [8], we fit the score function $s_{\\theta}$ using an expectation-maximization (EM) [34] objective. Specifically, we learn a posterior over user types, and alternate between user posterior estimation and likelihood maximization. In the E-step, a posterior $\\gamma_i(k) \\eqsim p_{\\theta_t} (z_i = k|x_i, y_i)$ over user types is calculated for each sample $(x_i, y_i) \\in D$. Following the work of Chidambaram et al. [8], the posterior is calculated by\n$\\gamma_i(k) = \\frac{\\eta_k \\prod_{j=1}^{M} \\sigma_{\\theta_t} (x_{i,j}, y_{i,j}, k)}{\\sum_{\\ell=1}^{K} \\eta_{\\ell} \\prod_{j=1}^{M} \\sigma_{\\theta_t} (x_{i,j}, y_{i,j}, \\ell)},$\nwhere $\\sigma_{\\theta}(x, y, k) \\eqsim p_{\\theta}(y|x, z = k)$ defines the likelihood, and $\\eta_k$ is the non-parametric prior. The M-step involves updating the prior via solving the log-likelihood maximizing problem over the posterior model:\n$\\eta_k = E_{i\\sim D} [\\gamma_i(k)],$\n$\\theta_{t+1} \\in arg\\max_{\\theta} E_{i\\sim D, k\\sim\\gamma_i} [\\log \\sigma_{\\theta}(x_i, y_i, k)],$\nDue to computational challenges of managing large datasets, we employ a mini-batch version of this ap-proach in which we sample a batch B from dataset D at each training step to estimate the log-likelihood loss:\n$\\mathcal{L}(\\theta) = -E_{i\\sim B, k\\sim\\gamma_i} [\\log \\sigma_{\\theta}(x_i, y_i, k)]$. See Appendix D.1 for a detailed description of the mini-batch EM algorithm."}, {"title": "6. Experiments", "content": "To assess the effectiveness of our approach, we conduct two empirical evaluations. First, we evaluate the quality of our user model and the simulated data it generated. This involves analyzing the model's ability to accurately capture user preferences and generate realistic interaction data. Second, we study the performance of PASTA, our multi-turn T2I agent, trained using our rater and simulated datasets. We perform a comprehensive analysis at each stage to assess the quality of the user model, the accuracy of the simulated data, and the overall performance of the T2I agent."}, {"title": "6.1. Setup", "content": "The slate size, number of images per prompt, and problem horizon are fixed L = 4, M = 4 and H = 5 in all experiments. We use Stable Diffusion XL [37] as our T2I model and multimodal Gemini 1.5 Flash [49] as our (prompt) candidate generator LMM, with $L_c = 25$. Our learned value function is fine-tuned from Gemma 2B [50]. Our experiments use a sparse reward, with rewards provided only in the final round, as our primary interest lies in the end result of interactive refinement process."}, {"title": "6.2. User Model Evaluation", "content": "We train the utility model using large-scale, single-turn, open-source datasets, as outlined in Sec. 5.2. We use the combination of the following datasets: HPS V2 [58], Pick-a-Pic [24], and Simulacra Aesthetic Captions [38]. During training, we keep the weights of the CLIP models frozen, training only the user image and text encoders. We train different user models for different numbers of user types, K = 1, 2, 4, 8, 16, 32, 64. We then fine-tune the model with our human-rater sequential data, by updating the CLIP encoder weights and the user-choice model parameters. Our human-rater data is a rich source of both"}, {"title": "6.3. PASTA Results", "content": "Using our user model, we generate more than 30,000 simulated user-agent trajectories. We also generate reward labels for our human-rater data using the user model, and train our state-action value function in three different settings: using only human rater data, using only simulated data, and using both data sources. The T2I agents trained by these algorithms are evaluated with human raters, for which the raters judge whether the current turn's chosen image was better, worse, or equally preferred to that in the previous turn. To make PASTA more efficient for human-rater studies, we distill the T2I agent into a single, fine-tuned Gemini 1.5 Flash [49] LMM, and serve that in real-time to generate the proposed image slates directly (without explicit prompt expansions). We also conduct an experiment with simulated users, using our user model, whose results are described in Appendix F together with other experiments for both PASTA and the user model.\nAnalysis. We compare our agents (each using a different state-action value function) with an untrained multi-modal Gemini 1.5 Flash model. The results of human-rater evaluations are shown in Fig. 5. These results demonstrate that training on real"}, {"title": "6.4. Abstract Prompts with Simulated Users", "content": "To visualize the differences in user preferences and PASTA's ability to adapt to them, we use PASTA with various user types using broad, abstract prompts such as \"an image of happiness\". Figure 6 shows an example of different rollouts of three distinct user types. We observe a clear preference emerging, favoring specific styles or content. All users starts with the same"}, {"title": "7. Related Work", "content": "Our work is related to recent advances in interactive and personalized image generation. Existing methods explore iterative refinement using visual feedback [54, 56] or leverage LLMs for prompt engineering [5, 17, 29], often focusing on optimization of perceptual quality or user satisfaction in a single interaction [8, 12, 55, 58]. Our PASTA framework extends this by framing multi-turn image generation as a sequential decision-making problem, where an RL agent learns to extend and personalize a prompt through a series of expansions guided by user feedback. This enables iterative refinement towards a desired visual outcome, capturing nuances in user preferences across diverse user types.\nPASTA draws upon a rich history of preference elicitation (PE) research [3, 7, 23, 43], especially that in content-based recommender systems [4, 32, 40, 47], and addresses the challenge of optimizing PE for diverse downstream tasks. Existing research highlights the importance of diversity in recommendations [45] and explores diversity in elicitation strategies [35]. PASTA tackles this by training an agent that considers both immediate user feedback and long-term goals to generate personalized images. Our novel data collection and simulation methodology further support the training of robust user simulators, which we show to greatly improve policy learning."}, {"title": "8. Conclusion", "content": "This work introduced PASTA, a novel RL agent for personalized and sequential T2I generation. We formulated the problem of iterative prompt expansions with LMMs and T2I models as a sequential decision-making problem that drives a collaborative, multi-turn image generation with the user. Critically, we have produced a new dataset that captures sequential interactions between an agent and a user, which we release to support further investigation of new T2I techniques in the research community. We developed user utility and choice models, learned from this dataset, which we used to create a user simulator that enabled generation of additional synthetic data (which we release as well). Finally, we used our sequential data to train PASTA, and demonstrated its effectiveness through comprehensive human evaluations, showing significant improvements in user satisfaction."}, {"title": "A. Latent Markov Decision Process (LMDP)", "content": "We begin with defining Latent Markov Decision Process introduced in [16, 26].\nDefinition 1. Suppose that a set of MDPs $\\mathcal{M}$ with a joint state space $\\mathcal{S}$ and joint action space $\\mathcal{A}$ in finite horizon setting with horizon H. let K = |$\\mathcal{M}$|, S = |$\\mathcal{S}$| and A = |$\\mathcal{A}$|. Each MDP $M_k \\in \\mathcal{M}$ it a tuple ($\\mathcal{S}, \\mathcal{A}, T_k, R_k, v_k$) where $T_k : \\mathcal{S} \\times \\mathcal{A} \\rightarrow \\Delta_{\\mathcal{S}}$ is a transition probability that maps a state-action pair into a distribution over the next state, $R_k : \\mathcal{S} \\times \\mathcal{A} \\leftrightarrow \\Delta_{[0,1]}$ a probability measure for rewards that maps a state-action pair into a distribution over rewards and initial state distribution $v_k$. Let $\\eta_1, \\eta_2, ..., \\eta_K$ be the mixing weights of LMDPs such that at the start of every episode, one MDP $M_k \\in \\mathcal{M}$ is randomly chosen with probability $\\eta_k$.\nThe goal of the problem is to find a policy $\\pi$ within a policy class $\\Pi$ that maximizes the expected return:\n$\\nu_{\\mathcal{M}}^{\\pi} = \\max_{\\pi \\in \\Pi} \\sum_{k=1}^{K} \\eta_k \\mathbb{E}_{\\tau \\sim M_k,\\pi} [\\sum_{t=1}^{H} r_t],$\nwhere $\\mathbb{E}[.]$ is expectation taken over the k-th MDP with policy $\\pi$. The policy $\\pi : \\mathcal{H} \\times \\mathcal{S} \\rightarrow \\Delta_{\\mathcal{A}}$ maps the current state and history into distribution over actions. Generally, the history is all experience seen so far $\\mathcal{H} = (\\mathcal{S}, \\mathcal{A}, [0, 1])^*$. When the model parameters are known, history is a sufficient statistics and can be summarized into belief states:\n$b_1(k) = \\frac{\\eta_kv_k(s_1)}{\\sum_{k'}\\eta_{k'}v_{k'}(s_1)},$\n$b_{t+1}(k) = \\frac{b_t(k)T_k(s_{t+1} | s_t, a_t)R_k(r_t | s_t, a_t)}{\\sum_k b_t(k)T_k(s_{t+1} | s_t, a_t)R_k(r_t | s_t, a_t)}.$\nWe formulate our reinforcement learning problem as a latent MDP, where each user from a discrete set induces distinct transition and reward kernels. Specifically, in our sequential text-to-image decision problem the state space is a prompt and image slate $\\mathcal{S} = P \\times I^{M*L}$ and the action space is the prompt slate $\\mathcal{A} = P^L$. The transition kernel for the k-th user type is composed of the T2I model and user choice model, i.e. the next state is the tuple composed of slate of images generated from the chosen slate (the action) and chosen prompt:\n$s_{t+1} = (\\{{\\{I_{m,\\ell,t}\\}_{m=1}^{M}\\}_{{\\ell}=1}^{L}}, p_{c_t}),$\nwhere $I_{m,\\ell,t} \\sim G(p_{\\ell,t})$ and $c_t \\sim C(k, p_0, I_{1,1,t}, ..., I_{M,L,t})$. The reward kernel corresponds to the user's utility function:\n$r_t \\sim R(k, p_0, I_{1,c_t,t},..., I_{M,c_t,t}).$\nSimilar to standard LMDPs, the posterior update during user model training in Sec. 5.3 mirrors the belief state computation, driven by observed samples collected from diverse human raters."}, {"title": "B. Human Rater Dataset", "content": "To gather data for training and evaluating PASTA, we conducted a human rater study. We recruited paid contractors as raters, and utilized a web-based interface where raters interacted with a Gemini 1.5 Flash Model acting as the agent in our multi-turn image generation process."}, {"title": "B.1. Rater Instructions: Interaction Phase", "content": "Each rater participated in a five-turn interaction session. At the beginning of the session, raters were presented with the task's instructions. They were then asked to provide an initial text prompt, with a maximum length of 12 words, representing a specific visual concept they wanted to see realized as an image. The guidelines stressed that prompts should encapsulate meaningful intent (thinking about a specific image they wished to see) and avoid generic prompts (e.g., \"a temple\u201d). Instead, they were encouraged to use descriptive prompts within the 12-word limit (e.g., \u201cAn ancient Egyptian temple with hieroglyphs\u201d). They"}, {"title": "B.3. Limitations of Rater Study", "content": "A recognized limitation of the rater study conducted in this work is the potential bias between groups of raters. In Figure 5 we compare PASTA trained on different datasets with the pre-trained Gemini Flash model. We could not concurrently run different experiment arms because of a lack of A/B testing infrastructure which shows different agents to raters at random. Consequently, the timing of experiment arms and rater availability could have introduced bias. Furthermore, the rater pool might not be fully representative of all potential user demographics for PASTA, and the initial prompts provided to raters might not have sufficient diversity.\nHuman errors and system issues, such as connection timeouts, were observed during the study. We did not implement any formal mechanisms for raters to report errors during specific sessions although raters can somehow \u201ccorrect\u201d their mistakes during the evaluation phase. Raters could prematurely abandon sessions without recording data from problematic sessions. Raters could provide a critique of the presented images but subsequent generations might not be aligned with these critiques. Although raters were explicitly instructed not to evaluate based on critique alignment, it might still cause worse rater experience for those who provided critiques."}, {"title": "B.4. Generating Rewards for Human Rater Data with User Model", "content": "To leverage human rater data for value model training, we utilize our learned user utility function to generate rewards for each interaction step. For each trajectory, we first compute the posterior distribution over user types using the model described in Appendix D.3. We then employ the utility function of the most likely user type to generate rewards for each timestep in the trajectory."}, {"title": "C. Offline Simulated Sequential Data", "content": "This section describes the generation of an offline dataset for training PASTA. We detail the process of creating interaction trajectories with simulated users, based on our trained user model and a random prompt selection policy. We also address"}]}