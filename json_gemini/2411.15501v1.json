{"title": "Instruct or Interact? Exploring and Eliciting LLMs' Capability in Code Snippet Adaptation Through Prompt Engineering", "authors": ["Tanghaoran Zhang", "Yue Yu", "Xinjun Mao", "Shangwen Wang", "Kang Yang", "Yao Lu", "Zhang Zhang", "Yuxin Zhao"], "abstract": "Code snippet adaptation is a fundamental activity in the software development process. Unlike code generation, code snippet adaptation is not a \"free creation\", which requires developers to tailor a given code snippet in order to fit specific requirements and the code context. Recently, large language models (LLMs) have confirmed their effectiveness in the code generation task with promising results. However, their performance on code snippet adaptation, a reuse-oriented and context-dependent code change prediction task, is still unclear. To bridge this gap, we conduct an empirical study to investigate the performance and issues of LLMs on the adaptation task. We first evaluate the adaptation performances of three popular LLMs and compare them to the code generation task. Our result indicates that their adaptation ability is weaker than generation, with a nearly 15% decrease on pass@1 and more context-related errors. By manually inspecting 200 cases, we further investigate the causes of LLMs' sub-optimal performance, which can be classified into three categories, i.e., Unclear Requirement, Requirement Misalignment and Context Misapplication. Based on the above empirical research, we propose an interactive prompting approach to eliciting LLMs' ability on the adaptation task. Specifically, we enhance the prompt by enriching the context and decomposing the task, which alleviates context misapplication and improves requirement understanding. Besides, we enable LLMs' reflection by requiring them to interact with a human or a LLM counselor, compensating for unclear requirement. Our experimental result reveals that our approach greatly improve LLMs' adaptation performance. The best-performing Human-LLM interaction successfully solves 159 out of the 202 identified defects and improves the pass@1 and pass@5 by over 40% compared to the initial instruction-based prompt. Considering human efforts, we suggest multi-agent interaction as a trade-off, which can achieve comparable performance with excellent generalization ability. We deem that our approach could provide methodological assistance for autonomous code snippet reuse and adaptation with LLMs.", "sections": [{"title": "I. INTRODUCTION", "content": "With the thriving growth of the open-source community, software reuse is widely adopted to efficiently deliver high-quality software products. Besides component-based reuse, reusing online code snippets has become a common practice in modern software development [1]\u2013[6]. These public available snippets from various platforms, e.g., GitHub and Stack Overflow, are widely reviewed and proofed by a large number of open-source contributors. Compared to writing the code from scratch, leveraging the recognized knowledge to build software has lower cost and less risks [7]-[9]. However, online code snippets often fail to meet the specific needs of developers. Hence, apart from simple copy-and-paste, developers are usually required to adapt these code snippets according to their development contexts to ensure the correctness and maintainability of the code [10]\u2013[12]. Over the years, several code snippet adaptation techniques and tools have been proposed to facilitate this daily activity [13]\u2013[16], but it is still a pending issue for its automation.\nRecent advancements in artificial intelligence have been marked by the emergence of large language models (LLMs), such as ChatGPT [17]. These LLMs are distinguished by their large scale of parameters and emergent abilities [18], [19] in processing natural language, which stems from extensive training on diverse data sources. This training empowers them with abilities applicable to numerous software engineering tasks, e.g., code generation [20], [21], code summarization [22], and automated program repair [20], [23]\u2013[26]. Code snippet adaptation can also be performed with LLMs' generation ability when the task is properly described. Therefore, it is imperative to investigate the potential of LLMs to perform adaptation.\nLLMs are utilized through the \"Pre-train, Prompt, and Predict\" paradigm [27], which requires users to engage with them through a set of textual inputs, i.e., prompt. Prompts enable us to teach LLMs unseen tasks with no need for fine-tuning them or modifying their architectures, known as programming in natural language [28]. For instance, Alice could ask LLMs to debug her code by simply entering \u201cPlease help me find the bugs in the following code: <code>\" as an input prompt. Besides, the prompt selection could significantly influence the capabilities of LLMs [27]\u2013[29]. Therefore, crafting appropriate prompts is the way of eliciting the ability of LLMs to"}, {"title": "II. EMPIRICAL STUDY", "content": "To evaluate the performance and issues of LLMs in code snippet adaptation, we structure the goal of our empirical study in the following research questions.\nRQ1: How effective are LLMs on the adaptation task? Existing studies have evaluated LLMs' performance on a wide spectrum of software engineering tasks. However, their effectiveness on the code snippet adaptation task remains unexplored. To this end, we investigate the adaptation performance of three widely used LLMs' in different settings.\nRQ2: What are the current issues of LLMs' adaptations? To better utilize LLMs for code snippet adaptation, it is necessary to understand their current limitations from their adaptation results. To this end, we analyze the test results of the adapted code and their failure-inducing defects by inspecting 1,000 GPT-3.5 adapted snippets in 200 cases.\nRQ3: What are the root causes of LLMs' adaptation failures? This RQ aims to understand why LLMs fail to adapt snippets to their context. To achieve this goal, we conduct a thematic analysis to summarize the underlying reasons of LLMs' failures. The answer is of great importance to understanding their nature of performing adaptations."}, {"title": "D. RQ1: LLMs' Adaptation Performance", "content": "Table I illustrates the performance of our selected LLMs in different temperature settings and parameter sizes. For the adaptation task, GPT-3.5 performs the best, achieving 52.34 and 60.98 in pass@1 and pass@5 respectively. Llama-3-8B ranks the second with a competitive pass@5 score 58.29 when the temperature is 0.6, but its pass@1 is much lower than GPT-3.5. The performance of CodeLlama on all parameter sizes are similar and inferior to the above two LLMs. Its best pass@1 and pass@5 are 40.10 and 50.24. Specifically, CodeLlama-34B achieves the best pass@1 and CodeBLEU scores, while its 7B version behaves better when the temperature rises.\nAs for the temperature parameter, its optimal setting of GPT-3.5 is 0.8 on the adaptation task, as evidenced by a marginal improvement in pass@1 and a significant improvement in pass@5 (p < 0.001, Mann-Whitney U test [41]). For instruction-tuned LLMs, there is a similar trend. The lower temperature leads to better pass@1 and CodeBLEU scores. A slightly higher temperature, e.g., 0.6 or 0.8, improves the pass@5 score. The reason behind is that a higher temperature introduces more variability in LLMs' outputs, facilitating a broader exploration of potential adaptations. In contrast, a temperature setting of 0 stabilizes the generated code structure, resulting in higher CodeBLEU scores. In all, we adopt the optimal temperature setting, i.e., 0.8, for GPT-3.5 in the following experiments.\nTaking the best performing LLM (i.e., GPT-3.5) as an example, its performance in code snippet adaptation exhibits a decrease of 14.73, 14.63, and 8.40 in pass@1, pass@5, and CodeBLEU compared to that of code generation. As introduced in Section II-B, their prompts contain the same contextual information. The reason behind the performance gap may be that GPT-3.5 is a generative model designed for predicting the next token, making it well-suited for code generation from scratch. However, adaptation tasks require the comprehension of multiple fragments of a compound corpus and context-based modifications to existing code snippets. This poses a challenging task for current generative LLMs.\nIn addition to the model type, size and its temperature setting, which can be seen as factors from the model aspect, factors from the task aspect may also impact LLMs' adaptation performance, e.g., the complexity of the task. In our study, we use the adaptation size, a.k.a., the number of the required AST edits from retrieved snippet to the canonical solution as the measure for task complexity.\nFig. 3 illustrates the distribution of the adaptation size and its relationship with the pass@1 score of GPT-3.5. All adaptation cases are grouped into seven by the adaptation size with an interval of 10. The majority of the cases have the adaptation size below 30. There is a decreasing trend of GPT-3.5's pass@1 with the rising of adaptation size. Two obvious"}, {"title": "E. RQ2: Issues in LLMs' Adaptations", "content": "To obtain an in-depth understanding of the issues of LLMs' sub-optimal adaptation performance, we perform test analysis on the results of the most advanced model, GPT-3.5, with its best settings in RQ1. Among 200 selected cases, there are a total of 87 cases where all five adapted snippets passed all the tests. There are 35 cases where at least one adaptation passed the tests and 78 cases where all adaptations failed."}, {"title": "F. RQ3: Root Causes of LLMs' Failures", "content": "Though thematic analysis, we further summarize the root causes of our identified defects. As shown in Table III, three major categories and nine subcategories are obtained.\nUnclear Requirement refers to the adaptation failures where developers describe their requirements vaguely or ambiguously in the prompt. LLMs can hardly identify the potential issues in the retrieved snippet and perform intended adaptations without a specific and accurate clarification. Under this category, we identify two subcategories.\nAmbiguous Literal (11): Developers' requirement may include key concepts with multiple interpretations. Limited by the training data, LLMs are prone to be misled by these ambiguous expressions and generate erroneous adaptations. For instance, the requirement of the calculate_sector_area method (ClassEval_5) specifying a parameter angle as \"angle of sector\" without indicating whether it is measured in degrees or radians. Likewise, the FidelityPromo method (ClassEval_33) requires LLMs to \"calculate the discount\" without indicating whether the returned discount is a percentage, a rate or a numerical value. LLMs failed in both cases as they misinterpreted the above concepts. This subcategory indicates LLMs are sensitive to certain literals, thus ambiguous ones may prevent LLMs from correctly adapting the snippet.\nUnspecific Instruction (2): Developers may use overly concise language to describe their requirement, making it difficult for LLMs to determine the specific details of the task or the desired outcome. This issue is more serious in highly customized and complex task. For instance, the filter method (ClassEval_0) describes its requirement as \u201cfilter the incoming request based on certain rules and conditions.\" It is hard for LLMs to make adaptations without the specific constraints.\nRequirement Misalignment refers to LLMs' failures when aligning the function in the retrieved snippet to the requirement during the adaptation. Different from the Unclear Requirement category, developers explain the instructions clearly in the requirement part. This pitfall highlights LLMs' preferences to overgeneralization instead of being restricted.\nMethod Signature (40): This subcategory describes the misalignment of the signature of adapted method. Although we provide the correct and complete signature through the instructions, LLM's adapted method may mismatch the provided signature. The issues include changing a static method to a non-static one, adapting the method with a new name, wrongly adapting the parameter types or numbers, returning a wrong type of value, etc. It indicates that LLMs' inability to subject to the basic constraints for code.\nOperational Logic (76): This subcategory refers to LLMs' failures to infer and adapt the specific operations the snippet performs, including algorithms, conditionals, and how the code processes the inputs or achieves the desired outputs. Hence, the mismatch of the adapted logic and the requirement leads to failed tests. For instance, the previous_song method (ClassEval_61) requires the LLM to \u201cswitch to the previous song in the playlist\u201d and \u201creturn False if there was no previous song.\u201d The LLM retained the looping logic through the playlist, as shown in Fig. 6. Additionally, LLMs may autonomously apply string case-insensitivity (i.e., \u201clower()\") or round off numerical computations (i.e., \u201cround()\u201d) during the adaptation. Above cases indicate LLMs' reasoning relies heavily on existing knowledge and statistically frequent solutions, even when they conflict with proposed new requirements.\nError Handling and Edge Cases (28): This cause refers to LLMs' inability to handle unexpected inputs and edge cases correctly, focusing on the robustness aspect of their adapted code. LLMs may ignore handling, mishandling or overly handling the special case, which caused the adapted method cannot pass all tests. For example, in the method\""}, {"title": "Context Misapplication", "content": "Context Misapplication refers to LLMs' misuse of the context knowledge, i.e., data, code, internal representations. We further divided it into four subclasses, including field misapplication, method misapplication, environment-related misapplication and internal context misapplication.\nField Misapplication (13): This refers to LLMs' misinterpretation and misuse of fields. As shown in Fig. 7, the cart field is a list of dictionaries representing different products. The LLM misinterpreted the semantics of the field and converted the list to a set, leading to TypeError because the dict type is unhashable in Python.\nMethod Misapplication (11): Similarly, method misapplication indicates LLMs' method misuse, e.g., invoking wrong methods, invoking them with wrong parameters, misinterpreting the return value, etc. For instance, in the solve method (ClassEval_35), the LLM mistook the return value of get_possible_moves method as the next state rather than the move for next state calculation, leading to a TypeError.\nEnvironment-related Misapplication (17): This refers to LLMs' misinterpretation of the environment, i.e., the scope/boundary of the context and its available resources. For instance, LLMs may use unimported packages, non-existent methods or deprecated APIs. Without the precise perception of the environment, LLMs could also omit the package name, leading to a series of NameErrors.\nInternal Context Misapplication (4): This refers to LLMs' inability to comprehend and operate the internal context, e.g., tables in the database. As illustrated in Fig. 9, the LLM failed to identify the schema of the books table through its creation and wrongly applied an UPDATE operation to the table and caused sqlite3.OperationalError. It raises the concerns for the implicit context when using LLMs to make adaptations in certain domain specific scenarios."}, {"title": "III. OUR PROMPTING APPROACH", "content": "Our empirical results demonstrate that the adaptation performance of LLMs is sub-optimal when using current instruction-based prompt (denoted as Initial Prompt). Therefore, to address the current pitfalls of LLMs, we propose an interactive prompting approach to utilizing LLMs in code snippet adaptation. We first enhance our prompt by enriching the context and decomposing the task. The resulting prompt is denoted as Enhanced Prompt. Additionally, we integrate an interactive workflow to the prompt-based conversation through a \"flipped interaction\" process to utilize LLMs' reflection ability. It is implemented with two schemes, one through a human-LLM collaboration and the other through a multi-agent collaboration. The resulting prompts are denoted as Human-LLM Prompt and Multi-Agent Prompt. The detailed design is described in the following."}, {"title": "A. Prompt Enhancements", "content": "Enrich the Context. In real adaptation scenarios, developers may possess complete method code in their context rather than merely a class skeleton. Therefore, we first consider expanding the class context in the Initial Prompt to alleviate the Context Misapplication issue. Specifically, we provide docstrings, input-output descriptions, and complete implementations for all the methods independent from the target. This information suggests the function and correct usage of contextual elements. Furthermore, we extract and specify dependencies of the target method to alleviate the Requirement Misalignment issue. It pushes LLMs to adapt the misaligned requirements with explicit dependency constraints. We implement this by adding the following statements to the end of the instruction body: \"It should be implemented using libraries: [Packages], fields: [Fields], methods: [Methods].\" or \"It should be implemented without using any external libraries, member variables, or methods.\"\nDecompose the Task. The adaptation prompt encompasses requirements, the reused snippet, and the class context. Its overwhelming information leads to LLMs' misinterpretation of the requirements and the context. We consider decomposing the adaptation task into context understanding and adaptation prediction process to further resolve the Requirement Misalignment and Context Misapplication issues. Specifically, we re-organize our prompt design as a multi-turn conversation, as described in the Fig. 10(a). In the first turn, we only address the context understanding task by simply providing the class context. The followed instruction makes LLMs understand the context but do not generate explanations to reduce costs. In the second turn, we require LLMs to predict adaptations. Splitting the lengthy prompt to shorter ones may alleviate LLMs' burden of handling extremely long context and make them solve the task step by step. Remembering the target class context before adaptation could also refresh their pre-existing knowledge, hence addressing their overgeneralization nature."}, {"title": "B. Interactive Workflow Integration", "content": "Our empirical findings suggest it is challenging for LLMs to adapt code snippets accurately in a single iteration, particularly with Unclear Requirement. Drawing inspiration from the communication process between users and developers in software requirement elicitation, we introduce a \"flipped interaction\" process [42] into the prompting process. Its core idea is to utilize the reflection ability of LLMs. Specifically, we require LLMs to assess the sufficiency and clarity of existing information for the adaptation task. If LLMs are aware of any missed information or any confusion, they could ask questions for more information in the subsequent conversation. This process encourages LLMs to comprehend the adaptation task actively and to refine their memories with information they concern about. To answer the raised questions, we implement this interaction with the following two schemes.\nHuman-LLM Interaction. The first scheme, Human-LLM Prompt, requires developers to provide feedback for LLMs' questions, as shown in Fig. 10(b). To enable a fair comparison with automated prompting approaches, we provide the Enhanced Prompt for our participants to understand the requirement, snippet and the target context. Similar to the automated prompting process, our participants should repeatedly interact with LLMs 5 times on each adaptation task to eliminate randomness. To reduce the bias introduced by human repetition, we required participants to provide consistent answers to the same or similar questions asked by LLMs during the experiment. Considering the communication overhead, we conduct the human-LLM interaction on the 200 selected cases with the best GPT-3.5 model, which costs 96 human-hours. All participants have over five years of Python programming experience, which is qualified for the interaction.\nMulti-Agent Interaction. Although experienced human developers can pinpoint the problems for adaptation, it is time-consuming for them to employ diagnoses and interactions in practices. To alleviate the human overhead, our second scheme, Multi-Agent-Counselor (MAC) Prompt, enables the interaction based on a multi-agent framework. As illustrated in Fig. 10(c), we introduce another LLM as the counselor. In the first turn, it is also initialized with the context understanding prompt. Then the counselor LLM generates answers based on the retrieved snippets and predefined instructions. Finally, the executor LLM accepts the answers and makes adaptations. Apart from the \"executor-counselor\u201d interaction, we implement an \"executor-evaluator\" interaction workflow as a baseline, denoted as Multi-Agent Evaluator (MAE) Prompt, leveraging LLMs' ability to assess generated code [43]. Fig. 10(d) illustrates its process. An evaluator LLM, also initialized with the context understanding prompt, reviews the adapted method generated by the executor LLM. Then it generates a list of issues based on its assessment and instructs the executor to regenerated an adaptation to address the issues. All agents above are profiled using role-playing system prompts without further training or fine-tuning.\nConfigurations of the Interactive Workflow. As we require LLMs to assess the sufficiency of information in the interactive workflow of the Human-LLM and MAC Prompt, it is uncertain when LLMs will terminate the conversation. Therefore, we conducted a preliminary experiment to determine two relevant configurations, the number of iterations and the number of questions. To this end, we randomly picked 20 methods from outside of our 200 selected cases and conducted the interaction with GPT-3.5, where the question number was not restricted. In all 20 cases, GPT-3.5 generated code right after an iteration in which all the answers were provided. They could ask up to 8 questions with the average number is 3.2. Therefore, as illustrated in Fig. 10, our prompt limits LLMs to ask at most 3 questions and extracts the code block after one iteration. If no code block is provided, the interaction will still be terminated and return an empty string."}, {"title": "IV. EVALUATION", "content": "In this section, we evaluate the effectiveness of our prompt enhancements and interactive workflow on the adaptation task."}, {"title": "A. Effectiveness of the Enhanced Prompt", "content": "Table IV presents the adaptation performance of prompting GPT-3.5 with the Enhanced Prompt. It achieves a pass@1 score of 67.76% and a pass@5 score of 74.39%, which represents an improvement of 29.46% and 21.99% compared to the Initial Prompt. Furthermore, its CodeBLEU increases from 47.05 to 60.16, marking a 27.86% relative improvement. The result indicates that Enhanced Prompt substantially enhances the performance of LLMs in the adaptation task. We also conduct an ablation study to examine the effects of three prompt enhancements within the Enhanced Prompt. Among them, enlarging the code context offers the most significant assistance to GPT-3.5. It results in an 8.54% gain in pass@1 and an 8.05% and pass@5. The decomposition strategy is also effective, marking a 5.61% improvement in pass@1. The dependency information exhibits a slight improvement in LLMs' adaptation performance."}, {"title": "B. Effectiveness of the Human-LLM Prompt", "content": "The adaptation performance of the Human-LLM Prompt on our 200 selected cases is shown in Table V. It achieves the best performance among all prompt design. Guided with the human-written response, GPT-3.5 obtain 74.50 in pass@1 and 87.00 in pass@5, respectively, which outperforms the Initial Prompt by 41.4% and 42.6%. However, its CodeBLEU value is slightly lower than the Enhanced Prompt. The reason is that the executor tends to make adaptations corresponding to various questions that the counselor has asked, leading to differences in the code structure. The decrease in this heuristic metric does not harm the accuracy of adaptation results.\nWe further inspect how our identified defects are mitigated by our prompt enhancements and interactive workflow. As shown in Table VI, the Human-LLM Prompt solves 159 out of 202 defects, which accounts for 78.7%. Our approach is effective in resolving defects across most categories, particularly for Method Signature, Field Misapplication and Internal Context Misapplication. However, two defects caused by Unspecific Instruction are still unresolved even with human feedback. The reason may be that the task is highly customized and contains a wealth of contextual knowledge, which could be hard to explain in the interaction. As for the communication"}, {"title": "C. Effectiveness of the Multi-Agent Prompt", "content": "As illustrated in Table V, the MAC Prompt achieves 71.80, 82.50 and 60.45 in pass@1, pass@5 and CodeBLEU. It could further improve the pass@5 score by 4.50 compared to the Enhanced Prompt. For the MAE Prompt baseline, it is inferior to the MAC Prompt on all metrics and obtains an extremely low pass@1, even lower than the Enhanced Prompt. It indicates that the Evaluator tends to provide inaccurate feedback to the Executor, leading to extra erroneous adaptations. Note that although the Human-LLM Prompt still hits the ceiling with the best adaptation performance, its human-in-the-loop interaction may limit the practical use of this approach. The MAC Prompt could be an excellent trade-off between performance and overhead, as supported by our experimental results.\nTo further explore the application of the MAC Prompt, we evaluate its effectiveness with our instruction-tuned LLMs on the whole ClassEval dataset. We choose LLMs with the best performing settings, i.e., CodeLlama-34B-temp0.6 and Llama-"}, {"title": "Finding 4: Our proposed prompt enhancements and interactive workflow greatly improve LLMs' adaptation performance. Human-LLM interaction achieves the ideal performance and resolves most identified defects, while our MAC interaction can fully automate this process with a similar performance and no human intervention.", "content": "Finding 4: Our proposed prompt enhancements and interactive workflow greatly improve LLMs' adaptation performance. Human-LLM interaction achieves the ideal performance and resolves most identified defects, while our MAC interaction can fully automate this process with a similar performance and no human intervention."}, {"title": "V. RELATED WORK", "content": "Prior studies on adaptation mainly focus on improving the efficiency. Cottrell et al. [13] present Jigsaw to integrate snippets into developers' code, which reduces their efforts on simple adaptations such as renaming. Wightman et al. [14] develop a lightweight Eclipse plug-in, SnipMatch. It supports the search and integration of code templates in the code context. Zhang et al. [10] develop a Chrome plugin, ExampleStack, to generate templates from developers' historical modifications on code snippets. Reid et al. [15] propose NLP2TestableCode to reduce the compilation errors during adaptation and generate tests based on input/output types in the context. Terragni et al. [16] present APIZATOR to transform code snippets on Stack Overflow to well-formed methods. However, there is not a general-purpose predictive approach for automated adaptation. The emergence of LLMs provides opportunities for this task because of their strong natural language processing ability. This paper explores their adaptation capabilities to provide insights for this research field."}, {"title": "B. Prompt Engineering for Software Engineering Tasks", "content": "Modern LLMs have demonstrated their capabilities in various software tasks [44], [45], including code generation, program repair [23], [25], [46] and code maintenance [47]-[50]. Enhancing LLMs' performance in these domains often involves prompt engineering. This includes handcrafted prompt design, the Chain-of-Thought (CoT) strategy [51], and constructing effective demonstrations [20], [52], [53]. Specifically, Liu et al. [54] improve code generation prompts using the CoT strategy with multi-step optimization. Cao et al. [24] explore various prompt templates for deep learning-based program repair. Rodriguez et al. [29] boost LLMs' performance in software traceability by guiding them to perform intermediate reasoning. Gao et al. [20] focus on constructing effective demonstrations for code intelligence tasks. These strategies have shown significant impact on their respective tasks, but their applicability to other tasks, i.e., code snippet adaptation, needs further exploration.\nFurthermore, prior work has presented reusable prompting patterns for LLMs in software engineering [42], [55]. White et al. [42] introduce a structured prompt framework with general patterns, including the flipped interaction pattern, which enables LLMs to gather information actively. This pattern is further extended in our work to support requirement refinement in code snippet adaptation."}, {"title": "VI. THREATS TO VALIDITY", "content": "Internal Validity. 1) The inherent randomness in LLM outputs, particularly in GPT-3.5, can lead to various adaptations. We mitigated this by requesting results five times from the LLMs and analyzing them using pass@1, pass@5, and CodeBLEU metrics, aiming for a more reliable assessment of their performance; 2) Our study only considers using three popular LLMs due to the expense. Involving more and newer models could provide more insights and potentially improve the performance; 3) This paper focuses on utilizing LLMs for software reuse. This paradigm could be changed when generative Als are powerful enough to build any software satisfying developers' needs from scratch.\nExternal Validity. 1) In the snippet retrieval phase, we use LLM-generated code instead of real snippets from open-source communities, which may not fully represent real-world adaptation scenarios. To better understand LLMs' adaptations, future studies could build a comprehensive dataset based on developers adaptation practices, e.g., adaptations from Stack Overflow, to further validate the generalization of our findings; 2) In this paper, we construct the adaptation cases based on the handcrafted ClassEval dataset with only 410 Python methods. However, they may not fully capture the complexity and diversity of real-world adaptation scenarios. To support adaptations in a broad context, future research could integrate an effective context retrieval module to our approach in real application scenarios; 3) Our empirical study and experimental evaluation only focus on three representative LLMs. Different LLMs could have different distributions of their adaptation issues and root causes. It is also worth investigating whether our approach is effective for other series of LLMs. However, our identification of LLMs' failure reasons is actually independent of the model and our inspection on adequate samples could reveal useful findings to LLMs' adaptations.\nConstruct Validity. 1) The human-designed adaptation prompts in our study might not represent the optimal perfor-"}, {"title": "VII. CONCLUSION", "content": "In this paper, we first conduct an empirical study to explore LLMs' capabilities in code snippet adaptation. Due to their sub-optimal performance compared to code generation, we further investigate the problems and causes by analyzing the adapted snippet along with their test results. Based on our empirical findings, we propose an interactive prompting approach to mitigating current problems. It enables the reflection ability of LLMs to improve their context awareness. Our evaluation result shows the effectiveness of our approach. Human-LLM interaction could achieve the best performance but introduce human efforts. We propose a multi-agent interaction that could achieve comparable performance as a trade-off. Its generalization ability is also validated. Our research findings provide insights for LLMs' current limitations and highlight the effectiveness of interactive prompting. We believe that our approach could facilitate the practical application of LLMs in real-world snippet reuse and adaptation."}]}