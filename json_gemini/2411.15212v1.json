{"title": "Effective Analog ICs Floorplanning with Relational Graph Neural Networks and Reinforcement Learning", "authors": ["Davide Basso", "Luca Bortolussi", "Mirjana Videnovic-Misic", "Husni Habal"], "abstract": "Analog integrated circuit (IC) floorplanning is typically a manual process with the placement of components (devices and modules) planned by a layout engineer. This process is further complicated by the interdependence of floorplanning and routing steps, numerous electric and layout-dependent constraints, as well as the high level of customization expected in analog design. This paper presents a novel automatic floorplanning algorithm based on reinforcement learning. It is augmented by a relational graph convolutional neural network model for encoding circuit features and positional constraints. The combination of these two machine learning methods enables knowledge transfer across different circuit designs with distinct topologies and constraints, increasing the generalization ability of the solution. Applied to 6 industrial circuits, our approach surpassed established floorplanning techniques in terms of speed, area and half-perimeter wire length. When integrated into a procedural generator for layout completion, overall layout time was reduced by 67.3% with a 8.3% mean area reduction compared to manual layout.", "sections": [{"title": "I. INTRODUCTION", "content": "Designing the layout of analog circuits is a crucial and complex task requiring significant expertise due to their susceptibility to noise, parasitics, alongside stringent topological requirements. This often leads to multiple iterations for layout engineers to achieve an optimal result.\nThe procedure involves two distinct yet closely entangled steps to place and connect circuit devices, called floorplanning and routing respectively. Metaheuristics as simulated annealing (SA), particle swarm optimization (PSO), and genetic algorithms (GA) [1] have been employed to streamline the floorplanning step. However, a primary drawback is that they cannot utilize past or external knowledge to enhance exploration of the solution space, as they optimize each problem instance anew.\nWorks as [2] instead opted to leverage machine learning (ML) based solutions, specifically Graph Neural Networks (GNNs), to improve the generalization capabilities in generating optimal floorplans spanning across diverse circuit topologies. Reinforcement learning (RL) techniques have become effective in tackling combinatorial problems [3], such as floorplanning, by optimally navigating and focusing on the most promising solution space regions. Since floorplanning can be framed as a sequential decision-making process within a Markov Decision Process (MDP), RL techniques have led to state-of-the-art outcomes in several digital layout applications [4]\u2013[8]."}, {"title": "II. RELATED WORKS", "content": "Floorplanning automation has extensively relied on meta-heuristic techniques such as SA, GA or PSO that, combined with topological model like Sequence Pair (SP) [14] or B*-Tree [15] to describe component's relative placement, targeted to minimize an objective function through a stochastic search process. Although producing compact floorplans, these methods leave insufficient space for routing tracks, ultimately resulting in unusable layouts. Additionally, implementing geometric constraints is challenging and tends to increase the already lengthy optimization runtimes, especially with more complex circuits.\nRecently, learning-based methodologies have emerged as promising alternatives. For instance, [2] trained a GNN to predict analog ICs performance based on device placement, plugging it into an SA optimizer, but did not use it to directly produce a floorplan or consider positional constraints. Gusm\u00e3o et al. in [16] developed an unsupervised encoder-decoder model using attention mechanisms and R-GCNs to embed topological constraints (symmetry and proximity) and generate floorplans; yet, validation to remove overlaps is required and no routing related optimization metric is considered. Ahmadi et al. [17] trained an RL agent to place FinFET modules on a grid, minimizing symmetry and alignment errors, area, and wirelength, but the assumption of fixed device shapes limited the approach's flexibility. This work addresses the aforementioned limitations."}, {"title": "III. PRELIMINARIES", "content": null}, {"title": "A. Graph Neural Networks", "content": "Circuit netlists and layouts can be represented as graphs, leading EDA methods to benefit from applying GNNs to various design stages [18]. A graph is described as a tuple G = (V, E), where V is the set of nodes and E is the set of edges. The neighborhood of node v, denoted as N(v), is u (u,v) \u2208 \u0395. A graph is directed if edge direction matters; otherwise, it is undirected. A graph with multiple types of nodes or edges is defined as heterogeneous, whereas one with a single type is homogeneous. GNNs are deep learning models that natively operate on graph-structured inputs, aiming to learn continuous embedding vectors per node through a message passing process [19]. Nodes exchange vector-based messages with adjacent nodes over iterations, progressively enriching their state with neighborhood context. Graph Convolutional Neural Networks (GCNs) [20] aggregate features as follows:\n$h^{(l+1)}_u = \u03c3 ( \\sum_{v \\in N(u)} W^{(l)} h^{(l)}_v / c_u ),$\nbeing hl+1) the updated node feature vector, o a non-linear differentiable function, I the 1-th GCN layer, h) the neighboring nodes feature vector, W(l) \u2208 Rd\u00d7d a learnable weight matrix, and cu a normalization factor. R-GCN extend the aggregation process to heterogeneous graphs, accounting for different node relationships:\n$h^{(l+1)}_u = \u03c3 ( W_0^{(l)} h^{(l)}_u + \\sum_{r\u2208R} \\sum_{v\u2208N_r(u)} \\frac{W_r^{(l)}}{C_{u,r}} h^{(l)}_v ),$\nwhere Nr(u) is the set of neighbors of node u under relation reR, W(l) \u2208 Ris a learnable weight vector for a node's self-connection in each layer, and finally Cu,r is again a normalization constant that varies with the R-GCN task."}, {"title": "B. Reinforcement Learning", "content": "RL techniques train an agent to discover effective solutions through interactions with the environment, utilizing an MDP framework denoted as (S, A, P, R, \u03b3). At each time step t, the RL agent is in a state st \u2208 S and chooses an action at \u2208 A to execute. Following the transition probability P, the agent transitions to a new state St+1 and receives a reward rt \u2208 R indicating the impact of its action, discounted by y, to balance the relevance of immediate versus future rewards. The agent eventually learns an optimal policy \u03c0*(a|s) that maximizes the expected sum of discounted rewards $G_t = \\sum_{k=0}^{\\infty} \u03b3^k R_{t+k+1}$."}, {"title": "IV. METHODOLOGY", "content": null}, {"title": "A. Reinforcement Learning Formulation", "content": "Enhancing the floorplanning algorithm by Basso et al. in [13], we propose an integration of an R-GCN model for chip representation learning with an RL agent. The agent selects a shape of a circuit components and places it on a discretized grid corresponding to the layout space. The floorplanning problem is formulated as the following MDP:\nStates: The state st combines detailed information encoded by the R-GCN model, such as circuit graph g and current instance nk 32-dimensional embeddings, as well as local feature maps extracted at the pixel level by a CNN. The local feature maps consist of a 32\u00d732 grid representation fg \u2208 {0,1}32\u00d732, and two reward-related masks showing the increase in the placement empty space and wirelength proxy metric, similar to [4], respectively denoted as fds, fw \u2208 [0,1}32\u00d732. Additionally, there are three positional masks fp \u2208 {0,1}3\u00d732\u00d732, used also for action masking, delineating the admissible placement cells for the next block given the three possible shapes and adherence to non-overlapping and optionally defined spatial constraints.\nActions: The action at at time step t consists in selecting one of three possible shapes for the current block bt and determining the grid cell to place its lower left corner.\nRewards: We define a partial reward rt as the negative increase of proxy wirelength and empty space in the floorplan after at. The end of episode reward is instead"}, {"title": "B. Preliminary Structure Recognition (SR) and Functional Block Configuration", "content": "Given an input schematic, we use Infineon's GCN-based SR tool [21] to detect circuit functional blocks. Following [13], we generate different block shapes by keeping a fixed total device width, i.e. area, and tailoring internal routing and device placement based on the recognized functional structure. These configurations are then provided to the RL agent."}, {"title": "C. R-GCN Circuit Representation Learning", "content": "An optimal floorplanning algorithm should be capable to generalize its performances across various circuit configurations and constraints. R-GCNs can effectively handle graphs of varying dimensions and topology, thanks to their permutation invariance. Works as [6], [22], [23] proposed to pre-train a GNN model on the supervised task of predicting rewards of input circuit graphs. By aligning training tasks with the RL agent's goals, circuit embeddings produced for subsequent stages capture meaningful signals, enhancing RL agent's decision-making with augmented generalization capabilities. In our setting, shown in Figure 2, circuits are represented as undirected graphs where each node vi corresponds to a functional block or single device. The edges ei represent relationships between nodes (u,v), which can be connectivity (if they are connected in the netlist), horizontal or vertical alignment, or horizontal or vertical symmetry. A node feature vector xu \u2208 X includes the block area, internal parameters like transistor or resistance stripe width, terminals routing direction, pin counts, and a 28-dimensional one-hot encoding of the block's functional structure (e.g., current mirror, differential pair, cascode, etc.)."}, {"title": "D. R-GCN and RL-based Floorplanning", "content": "Given the R-GCN trained model, we remove the final FC layers and use the remaining part as encoder for the RL agent, enhancing the transfer learning capabilities of our methodology. Figure 4 provides an overview of the RL architecture. We train the RL agent using a masked version of Proximal Policy Optimization (PPO) [24], a state-of-the-art on-policy algorithm.\nDesigning Action Space and Masking: Large action spaces can hinder RL convergence towards an optimal policy. Therefore, we discretize the layout canvas into a 32\u00d732 grid fg, effectively balancing performance and accuracy by containing the action space while ensuring constraints satisfaction. The grid height H and width Ware computed as W=H=$\\sqrt{\\frac{1}{\\pi} \\sum_{i=1}^{Rmax} A_i}$ being A\u2081 the area of the ith circuit device and Rmax=11 the maximum empirically derived aspect ratio for a floorplan. Since both H and W depend on the size of each functional block, this design choice accommodates any complex circuit placement. The agent can choose from 3 candidate shapes for a circuit structure, similar to the flexibility human designers have. Combined with selecting the cell for placing the lower-left corner of a block, this results in an action space A of size 3\u00d732\u00d732=3072. To prevent further escalation of action space dimensionality, we use a heuristic inspired by [22], which arranges block placement in order of decreasing size.\nAs mentioned earlier, our floorplanning methodology can handle fundamental positional requirements such as symmetry, alignment, and guarantee the absence of device overlap. Works as [25] prove that policy gradient algorithms can take advantage of masking procedures to avoid selecting invalid actions based on state information. Therefore, at each episode step, we generate three positional masks fp, one for each possible candidate shape. These masks are obtained by combining two binary matrices: one representing partial placement and the other symmetry or alignment constraint masks. In the first one, a value of 1 indicates an available cell, while 0 signifies an already occupied one. The second one designates with 1s where the corresponding constraint would be satisfied (based on the placement of blocks belonging to a constraint group and the"}, {"title": "2) State Design", "content": "Given that the R-GCN model does not supply detailed location information for specific circuit instances during an episode, we enrich the agent's state representation. As mentioned in Section IV-A, this is achieved by combining the current node, i.e. block bt, and graph, i.e. circuit, embeddings nk and g with 6 additional grid-based masks: fg, fw, fds and fp, which reflect partial placements, wire and dead space, i.e. empty space, increases, and valid positions. This augmented state aids the RL agent in identifying the optimal placement to optimize rewards. Lai et al. [4] first proposed this approach, using fp, fg and computing fw as the increase in proxy wirelength when placing bt in a specific position. Our approach extends the positional mask fp to account for multiple device shapes and introduces the dead space mask fds, a normalized, continuous matrix indicating the increase in empty space if bt is placed in a certain location. To construct fds, we iterate over all available cells on the placement grid, compute the resulting dead space from placing bt, and subtract the previous dead space value. Already occupied locations are set to the maximum increment 1 to mask invalid positions. Figure 5 provides a visual example of fw and fds."}, {"title": "3) Policy Design", "content": "Treating grid masks as analogous to images, as suggested by [5], leverages CNNs' effectiveness in generating informative embeddings. For this reason, we concatenate the masks forming a single tensor of dimension 6\u00d732\u00d732 and feed them to a CNN. The convolution layers use a 3\u00d73 kernel size with stride of 1, padding of 1 and 16, 32, 32, 64, 64 filter channels, followed by one FC layer to produce a 512-dimensional embedding vector. Finally, after concatenating the R-GCN embedding and CNN outputs, this"}, {"title": "4) Reward Shaping", "content": "As delineated in Section IV-A, the agent's goal is to minimize two primary metrics: area occupation and half-perimeter wirelength (HPWL). HPWL, a widely used approximation for true wirelength, is computed as the half-perimeter of all nets' bounding boxes:\nnHPWL = $\\sum_{i=1}^{n} max(x_i) - min(x_i) + max(y_i) \u2013 min(y_i),$\nwhere xi, Yi are the endpoints of a net, and n is the total count of nets in the netlist. The dead space DS in a floorplan F is computed as $\\frac{1}{F_{area}} \\sum_{i=1}^{m} A_i$. To better guide the agent during an episode rollout, we provide intermediate rewards rt based on the increase in partial floorplan dead space and HPWL, computed from the currently placed instances after possibly taking action at. The intermediate reward is defined as:\n$r_t = -(A_{ds} + \u0394_{HPWL}),$"}, {"title": "5) RL Training Schedule", "content": "Our methodology is designed to develop a single RL agent capable of generating optimal floorplans for a wide array of circuit types and constraints. To achieve this, we use a hybrid curriculum learning (HCL) approach, described in [26], which incrementally presents the agent circuits of growing complexity. Starting with smaller circuits, we interleave them with random sampling of new circuit instances and constraints. This method maintains the agent's exposure to complex scenarios and prevents the loss of previously acquired knowledge, thereby enhancing transfer-ability to new, unseen instances. The circuits used to train the RL are 3 operational transconductance amplifiers (OTAs) and 2 bias ones, respectively encompassing 3,5,8,3 and 9 blocks, to ensure enough diversity in the data."}, {"title": "E. Routing and Final Layout Generation", "content": "Once a feasible floorplan is generated, we construct an OARSMT for each net to minimize wirelength and avoid obstacles. Unlike [13], which required congestion estimation to reserve space for routing channels, our method yields routing ready floorplans. The global routing tree is segmented into conduits, detailing connections and layers, guiding ANAGEN's router to finalize circuit connections. This approach enhances ANAGEN's capabilities, potentially making it competitive with current state-of-the-art techniques [27], [28], while preserving the correctness inherent in procedural generation and leveraging RL for advanced floorplan generation."}, {"title": "V. EXPERIMENTAL RESULTS", "content": "Our floorplanning pipeline is built in Python 3.9, using DGL [29] and Stable Baselines3 [30] libraries for implementing respectively the R-GCN and RL models."}, {"title": "A. RL Placement Training Setup", "content": "To foster policy robustness and reduce convergence time, we use 16 parallel environments to gather multiple experiences. Following the HCL approach from Section IV-D5, we train each circuit for 4096 episodes. During the first half of the episodes we avoid introducing new constraints or circuits. After this phase, we begin sampling new circuit instances and constraints"}, {"title": "C. Evaluation of Complete Layouts", "content": "We plug our novel floorplanning algorithm into the pipeline proposed in [13] and validate its effectiveness by comparing"}, {"title": "VI. CONCLUSIONS AND FUTURE RESEARCH", "content": "This paper proposes a combined R-GCN and RL-based methodology for analog ICs floorplanning, ensuring alignment, symmetry, and no overlap constraints compliance. Our approach can generalize and transfer knowledge across different type of circuits, including new, unseen ones. The generated floorplans not only outperform established baselines but also, when integrated into a procedural generation pipeline, yield complete layouts of comparable quality to human generated ones in significantly reduced runtime. In the future, we aim to augment the floorplan algorithm with detailed routing information to further condition device placement towards easier and more efficient routing configurations."}]}