{"title": "PhyPlan: Compositional and Adaptive Physical Task Reasoning with Physics-Informed Skill Networks for Robot Manipulators", "authors": ["Harshil Vagadia", "Mudit Chopra", "Abhinav Barnawal", "Tamajit Banerjee", "Shreshth Tuli", "Souvik Chakraborty", "Rohan Paul"], "abstract": "Given the task of positioning a ball-like object to a goal region beyond direct reach, humans can often throw, slide, or rebound objects against the wall to attain the goal. However, enabling robots to reason similarly is non-trivial. Existing methods for physical reasoning are data-hungry and struggle with complexity and uncertainty inherent in the real world. This paper presents PhyPlan, a novel physics-informed planning framework that combines physics-informed neural networks (PINNS) with modified Monte Carlo Tree Search (MCTS) to enable embodied agents to perform dynamic physical tasks. PhyPlan leverages PINNs to simulate and predict outcomes of actions in a fast and accurate manner and uses MCTS for planning. It dynamically determines whether to consult a PINN-based simulator (coarse but fast) or engage directly with the actual environment (fine but slow) to determine optimal policy. Evaluation with robots in simulated 3D environments demonstrates the ability of our approach to solve 3D-physical reasoning tasks involving the composition of dynamic skills. Quantitatively, PhyPlan excels in several aspects: (i) it achieves lower regret when learning novel tasks compared to state-of-the-art, (ii) it expedites skill learning and enhances the speed of physical reasoning, (iii) it demonstrates higher data efficiency compared to a physics un-informed approach.", "sections": [{"title": "Introduction", "content": "Consider the scenario where the robot is asked to put a ball inside an empty box. The robot must improvise and dynamically interact with the objects in the environment if the box is far away. It may need to throw the ball, slide it across, or bounce it off a wedge to make it reach the box. Reasoning about the effects of various actions is a challenging task. Although the solution may appear self-evident to humans, a robot requires several demonstrations of each alternative to attain proficiency in a given task. Learning to reason with physical skills is a hallmark of intelligence. Increasing evidence is emerging that humans possess an intuitive physics engine involved in perceptual and goal-directed reasoning (1, 2). In essence, such reasoning requires (implicit or explicit) learning of predictive models of skills and the ability to chain them to attain a stated goal.\nRecent efforts such as (1) learn to solve physical tasks by exploring interactions in a physics simulator. The reliance on a realistic physical simulator during training slows the learning process. Others, such as (3) benchmark, propose alternative model-free approaches and show their limitation in long-horizon reasoning, mainly when rewards are sparse (using a physical tool in a specific way to acquire large rewards). This motivates an investigation into the availability and use of \"physical priors\" to scale skill learning to longer horizons. The work of (4) addresses scalability by adopting symbolic abstractions over physical skills expressed as governing physics equations with free parameters learnable from data. Such symbolic abstractions can be embedded into a planning domain for PDDL-style planning to arrive at multi-step plans. However, the hand-encoding of exact physics equations leads to brittleness during deployment in case of modelling errors.\nIn response, this study introduces a physics-informed skill acquisition and planning model that efficiently extends to multi-step reasoning tasks, offering advantages in (i) data efficiency, (ii) rapid physical reasoning, and (iii) reduced training time compared to methods relying on intricate physics simulations. Simulation time exhibits exponential growth when chaining skills, particularly in continuous state spaces. To address this, we leverage a limited number of physical demonstrations within the simulation engine to acquire primitive skills such as throwing, sliding, and rebounding from observed trajectories. Subsequently, we train a PINN-based predictive model. The acquired skills are then incorporated"}, {"title": "Problem Formulation", "content": "We consider a robot manipulator operating in a tabletop environment. Assume that the robot can position its end-effector at a desired configuration, grasp an object, move it while holding it, and release the object-oriented at a target pose. The environment consists of dynamic objects (box or a ball) and static objects such as a wedge, a flat surface (such as a table), a positioning plank (for instance, a bridge), a box-like object, a pendulum-like assembly capable of swinging a plan that rotates on a hinge. The dynamic object can interact with other objects, e.g., by sliding over a surface, colliding and rebounding from a wall, or more naturally under gravity (dropping vertically or falling as a projectile), finally coming to rest on a ground surface or inside a container-like object. Formally, let $s_t \\in S$ denote the world state at time $t$, observed as an image $i_t$ from a depth camera. Let $O$ denote the set of objects in the environment, where each $o \\in O$ has a set of modes, $M(o)$ (degrees of freedom) for interacting with the environment. For example, a wedge may be po-"}, {"title": "Learning Physics-Informed Skill Models", "content": "Instead of learning a policy directly, we factor the learning problem above as one of learning a model that depicts how an object engages with the environment and then using this model to plan multi-step interactions to reach the intended goals. We postulate that a learnt physical interaction model would provide a strong prior for the planner and help them make informed decisions. We consider the problem of learning a model for physical skills such as bouncing a ball-like object off a wedge, sliding over an object, swinging a pendulum, throwing an object as a projectile and hitting an object with a pendulum. Here, we interpret a physical skill as a model that predicts the state trajectory of an object as it undergoes a dynamic interaction with another object.\nGoverning physics of such a skill can be represented as $Y_t = g(Y)$, where $Y$ represents the state variables, $Y_t$ represents the time derivative of the state variable, and $g(Y)$ represents a non-linear function of $Y$. For instance, skills like throwing can be modelled as projectile motion with a governing differential equation: $d^2y/dt^2 + g = 0$ representing motion in the vertical axis and $dx/dt-v_{xinit} = 0$ representing motion in the horizontal plane. Here, $x$ and $y$ represent instantaneous position w.r.t. inertial frame in the horizontal and vertical direction, respectively, and $v_{xinit}$ represents the initial horizontal velocity. Similarly, other skills like swinging can be modelled by pendulum dynamics and sliding as friction dynamics.\nWe introduce a neural network that predicts the object's state during dynamic interaction continuously parameterised by time. Such interactions can be simulated in a physics engine by using numerical integration schemes. However, since we aim to perform multi-step interactions, simulating outcomes during training is often intractable. Hence,"}, {"title": "Physical Task Reasoning with Learnt Skills", "content": "The skill models developed so far allow the robot to predict how the object's trajectory will evolve during a dynamic physical interaction. This can be viewed as single-step reasoning. We now consider how to compose skills to solve a task requiring multi-step reasoning. For each task, we first figure out a chain of skills as described in Figure 4, and beginning with the initial state, we use a skill model to predict the state of the environment after each skill is executed. The state after the execution of the last skill is the final state of the environment, which is then evaluated for how near the ball/box/puck could get to the goal. The PINN-based Roll-outs are semantic and approximately evaluate the model's performance based on how far the ball lands from the goal without involving the simulator.\nWe assume the chain of skills to perform the tasks, and PhyPlan finds the best action in each skill, such as the angle at which to swing the pendulum and the angle of the pendulum's plane in Launch task, the angle at which to place the wedge and the height of the ball in Bounce task, etc. We run an MCTS planning algorithm with PINN-based Rollouts (Algorithm 1) to search the continuous action space by uniformly sampling D (Discretization Factor) actions within the bounds of the space. The lines coloured blue in Algorithm 1 are the places where we differ from conventional MCTS. Also, $n_{action}$ and $V_{action}$ are the number of trials and the value of action, respectively. Algorithm 2 ex-"}, {"title": "Algorithm 1: Physics-Informed MCTS", "content": "Input: node\nOutput: Reward\n1: if node is terminal then\n2:  return rv $\\leftarrow$ PINN-Rollout(node)\n3: end if\n4: $A \\leftarrow$ actions considered in node\n5: if $A$ > 0 then\n6:  $\\alpha \\leftarrow \\underset{\\alpha \\in A}{\\operatorname{argmax}} V_a + \\alpha \\frac{\\operatorname{log} n}{n_a}$\n7:  newNode $\\leftarrow$ child of node by taking action\n8:  rv $\\leftarrow$ MCTS (newNode)\n9:  $V_{action} \\leftarrow (V_{action} \\times N_{action} + rv) / (n_{action} + 1)$\n10:  $N_{action} \\leftarrow N_{action} + 1$\n11: else\n12:  $A \\leftarrow$ sample arms at node\n13:  for $a \\in A$ do\n14:   child $\\leftarrow$ child of node by taking arm a\n15:   $V_a \\leftarrow$ PINN-Rollout (child)\n16:   $n_a \\leftarrow 1$\n17:  end for\n18:  rv $\\leftarrow$ PINN-Rollout (node)\n19: end if\n20: return rv"}, {"title": "Algorithm 2: Adaptive Physical Task Reasoning", "content": "Input: initial state, goal position, num_attempts\nOutput: Regret\n1: for attempt in num_attempts do\n2:  Gaussian_Process.fit(actions, rewards)\n3:  phy_reward $\\leftarrow$ 0\n4:  for K iterations do\n5:   reward $\\leftarrow$ MCTS(root node)\n6:   reward  action  action sequence in Monte Carlo Tree for\n7:   if reward > phy_reward then\n8:    phy_reward $\\leftarrow$ reward\n9:    best_action $\\leftarrow$ action\n10:   end if\n11:  end for\n12:  reward_sim $\\leftarrow$ execute best_action in simulation\n13:  best_reward $\\leftarrow$ max(best_reward, reward_sim)\n14:  Append best_action to actions\n15:  Append reward_sim-phy_reward to rewards\n16: end for\n17: return regret $\\leftarrow$ (opt_reward-best_reward) / opt_reward"}, {"title": "A. Experimentation Setup.", "content": "Virtual Skill Learning: We deploy Isaac Gym (5), a physics simulator and reinforcement learning framework for training robots and skill learning. The task involves learning physical skills like swinging, sliding, throwing, hitting and bouncing (Figure 3). Each task is simulated, and a Physics-Informed Skill model is trained over the data received. Consider the case of throwing a ball in the simulator, which provides velocity-displacement pair values of the projectile at different time instances as training input to the model. Similar experiments are done for each skill.\nPerception-based Skill Learning: We also developed a perception-based pipeline to infer the object state directly from RGB-D data to replicate real lab-based settings. The experimentation scene in the simulator is captured via an inbuilt depth camera, which provides time series images of the experiment. On the gathered image dataset, segmentation techniques involving a combination of unsupervised detection methods like Segment Anything (19), Grounding Dino (20) and traditional methods like edge detection are used for object detection."}, {"title": "B. Evaluation of Learnt Skills.", "content": "We evaluate the prediction accuracy of the proposed physics-informed skill network on simulation data and the physical parameter estimation in the unknown environment."}, {"title": "C. Evaluation on Physical Reasoning Tasks.", "content": "Dataset and Baselines We created a benchmark data set composed of four tasks: Launch, Slide, Bounce, and Bridge, which require the robot to use physical skills sequentially to attain the goal. For example, the Bridge task requires a dynamic object like a ball to be positioned into a goal region. The task requires chaining multiple interactions with objects. The robot can interact with the environment multiple times, resetting the dynamic object after each complete rollout. Within each trial, the robot can set the objects in the desired configuration before releasing the dynamic object. The robot only observes an image of the scene. We also implemented the following baselines for comparisons: (i) Random policy: actions sampled"}, {"title": "Task Learning Efficacy.", "content": "We built two versions of our model called PhyPlan with perception and PhyPlan without perception. The former uses perception-based PINN models and extracts the goal and ball positions from the task image. The latter uses PINN models without perception and obtains goal and ball positions directly from the simulator. Figure 7 illustrates the task learning efficiency measured as regret, defined in Section. The lower the regret, the better the performance. Both versions of PhyPlan achieve lower regret than the baselines. As the task complexity increases, the difference between PhyPlans' performance and the baselines widens. For instance, PhyPlan performs close to the baselines in the Sliding task, whereas it outperforms baselines by a high margin in the Bridge task. Further, PhyPlan without perception generally performs better than PhyPlan with perception because of the inherent noise in the Perception-based skill models and the errors in detecting the goal and ball positions. Figure 8 shows a qualitative comparison between PhyPlan and DQN-Adaptive in the Bridge task. PhyPlan demonstrates the capability of long-horizon reasoning in substantially less number of actions/trials. Further, Figure 9 compares our approach with its variant, which uses a full simulation engine during training instead of learnt skills. The results indicate the ben-"}, {"title": "Adaption of Reward Model.", "content": "We also evaluated the effectiveness of GP-UCB in adapting to the task execution environment, beginning with the pre-trained Physics-Informed Skill Models. Figure 10 illustrates the final positions where the dynamic object (the ball) lands relative to the goal after the robot performs the Bounce task. The Random model spans the whole action space, the DQN-Simple model is skewed as it does not learn online, the DQN-Adaptive model converges to the goal position in later iterations, PhyPlan without GP-UCB does not explore as it does not learn the prediction error, and PhyPlan with GP-UCB learns the prediction error to converge to the goal position quickly. Further, PhyPlan with GP-UCB appears to learn more systematically, exploring and refining within a confined action space.\nFigure 11 demonstrates a simulated Franka Emika robot learning to perform the tasks. The robot interacts with the objects using crane grasping with a task space controller for trajectory tracking. The Figure shows the robot learning (i) the ball's release height and the wedge's pose in Bounce task; (ii) the pendulum's plane and angle in the Launch and the Bridge tasks; (iii) the bridge's pose in the Bridge task for the ball or puck to fall into the goal region. The policy is learned jointly for all objects, and the actions are delegated to the robot with the desired object in its kinematic range. Once the robots position the tools, the ball or the pendulum is dropped. Further, note that the robot learns to use the bridge effectively; a physical reasoning task reported earlier (1) to be challenging to learn for model-free methods, highlighting the PhyPlan's adaptability to long-horizon tasks. The results indicate the ability to generalise to new goal locations by using objects in the environments to aid task success."}, {"title": "Conclusions", "content": "We consider the problem of training a robot to perform tasks requiring a compositional use of physical skills such as throwing, hitting, sliding, etc. Our approach leverages physics-governing equations to efficiently learn neural predictive models of physical skills from data. For goal-directed task reasoning, we embed learnt skills into an MCTS procedure that utilises the skill model to perform rapid rollouts during training (as opposed to using a full-scale physics simulation engine, which is slow in practice). Finally, we leverage the GP-UCB approach for online adaptations to compensate for modelling errors in skills by selective rollouts with real dynamics simulation. The approach is data efficient and performs better than the physics-uninformed DQN-based model. Future work will attempt to (a) learn the sequential composition of skills and plan the tasks in-situ, thus alleviating the assumption of learning all skills independently, (b) evaluate the model on a real robot, (c) investigate skill model for skills with complex or intractable physics, and (d) investigate PhyPlan's scalability to physical reasoning tasks involving large chain of physical skills."}, {"title": "A. Skill Learning.", "content": "In Table 1, we present a comprehensive overview of the Fully Connected Neural Network architecture implemented within various Physics-Informed Skill Networks. These networks integrate neural network models and physics-based governing equations specific to each skill. Additionally, Table 2 lists the inputs, outputs, and governing equations pertinent to each skill network. In this context, the subscript init signifies the initial value of a physical parameter, and $t_{query}$ denotes the time instance for which the output quantity is required. The terms $V_{hor}$ and $U_{ver}$ are used to represent planar and vertical velocities, respectively, with the latter directed towards gravity (g). Key parameters such as the angle $\\theta$ of a wedge in the bouncing skill and the masses $m_1$ and $m_2$ of colliding objects in the hitting skill are also inputs to their respective skill networks. Note that the skills like Bouncing and Hitting are exclusively trained through data-driven approaches, and hence, they do not incorporate any governing differential equations."}, {"title": "B. Algorithm.", "content": "In Section 4 of the main paper, we delineate the intricacies of the algorithm employed in our study. Broadly, the Adaptive Physical Task Reasoning serves as the encompassing algorithm that invokes the Physics-Informed MCTS to acquire a nuanced reward model and adapt to the dynamic environment. The Physics-Informed MCTS performs semantic rollouts via the PINN-Rollout algorithm to approximate the value of actions, subsequently expanding the MCTS tree as per the insights gained. The procedural details of the semantic rollouts, featuring skill chaining with pre-trained Physics-Informed Skill Networks,"}, {"title": "Algorithm 3: PINN-Rollout", "content": "Input: init_node\nOutput: Reward\n1: Skills Sequence of physical skills.\n2: for s$\\in$ Actions do\n3:  time_step $\\leftarrow$ tquery\n4:  time $\\leftarrow$ time_step\n5:  while s is not finished do\n6:   new_node $\\leftarrow$ Skill Model (init_node, time)\n7:   time $\\leftarrow$ time + time_step\n8:  end while\n9:  init_node $\\leftarrow$ new_node\n10: end for\n11: rv $\\leftarrow$ reward obtained\n12: action $\\leftarrow$ sequence of actions chosen for rollout\n13: $\\mu$, $\\sigma$ $\\leftarrow$ Gaussian_Process.predict(action)\n14: return rv + $(\\mu+\\beta^{1/2} \\times \\sigma)$"}, {"title": "C. Code and Resources.", "content": "The code implementation associated with this research are available on GitHub at the following link:\nhttps://github.com/phyplan/PhyPlan\nThe datasets and additional resources are available on OneDrive at http://bit.ly/phyplan-resources\nPlease visit the our website at https://phyplan.github.io/ for further details."}]}