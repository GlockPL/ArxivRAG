{"title": "On the Design and Analysis of LLM-Based Algorithms", "authors": ["Yanxi Chen", "Yaliang Li", "Bolin Ding", "Jingren Zhou"], "abstract": "We initiate a formal investigation into the design and analysis of LLM-based algorithms, i.e. algorithms\nthat contain one or multiple calls of large language models (LLMs) as sub-routines and critically rely\non the capabilities of LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems, have achieved re-\nmarkable empirical success, the design and optimization of them have mostly relied on heuristics and\ntrial-and-errors, which is largely due to a lack of formal and analytical study for these algorithms. To\nfill this gap, we start by identifying the computational-graph representation of LLM-based algorithms,\nthe design principle of task decomposition, and some key abstractions, which then facilitate our for-\nmal analysis for the accuracy and efficiency of LLM-based algorithms, despite the black-box nature of\nLLMs. We further consider parallel decomposition for a case study, providing extensive analytical and\nempirical study for four concrete examples of this pattern. Our proposed framework holds promise for\nadvancing LLM-based algorithms, by revealing the reasons behind curious empirical phenomena, guiding\nthe choices of hyperparameters, predicting the empirical performance of algorithms, and inspiring new\nalgorithm design. To promote further study of LLM-based algorithms, we release our source code at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.", "sections": [{"title": "1 Introduction", "content": "The rapid developments of pre-trained large language models (LLMs) in the past few years [BCE+23,\nWTB+22, SMK23] have given rise to the new paradigm of algorithmic problem solving by utilizing LLMs\nas general-purpose solvers and prompting them with task descriptions plus additional inputs that can help\nenhance their performance [WWS+22, KGR+22, ZYY24]. Meanwhile, it is also well recognized that even\nthe best LLMs today still exhibit various limitations, such as finite context window sizes and difficulty\nwith complex reasoning. Some of these limitations are fundamental [MS23, PNP24, TTC+24, HR24], and\nresolving them requires new breakthroughs. Moreover, in resource-constrained scenarios where using the\nstate-of-the-art LLMs is not feasible, one might have to resort to smaller and weaker LLMs for solving\ncomplex tasks.\nAll these have motivated the developments of what we call LLM-based algorithms, namely, algorithms\nthat contain one or multiple LLM calls as sub-routines and fundamentally rely on the capabilities of LLMs.\nIn its most basic form, an LLM-based algorithm can be a combination of one or multiple LLM calls and some\nprompt engineering [YYZ+23, BBK+23, LLLD23, SLC+23, PKH+24, CLGI24]. More advanced examples\ninclude LLM-powered agent systems [MDL+23, WBZ+23, HZC+23, GLP+24, SST+23, ZWK+24, PKB+24,\nXCG+23] and compound AI systems [ZKC+24, CDH+24] that augment LLMs with additional abilities like\ntool use and long-term memory, as well as the emerging paradigm of LLM programming [SSC+23, KTF+23,\nKSM+24, ZYX+24].\nLLM-based algorithms, in a similar spirit to neuro-symbolic programming [CEP+21, GK23] and learning-\naugmented algorithms [MV22, LM22], combine the advantages of both LLMs and traditional algorithms. An\nLLM-based algorithm, designed with human's intelligence and knowledge of algorithmic problem solving, can\nexhibit much better controllability and interpretability, stronger performance that is less reliant on delicate\nprompting or extensive trial-and-errors, and capabilities that far exceed what could possibly be achieved by\ndirectly prompting the LLM for a solution.\nThe rapid developments of LLM-based algorithms naturally raise the question: is it possible to provide\nany formal analysis or guarantee for LLM-based algorithms? This seems like a daunting task at first glance,\ndue to the black-box nature of LLMs. Indeed, in prior works of this field, LLM-based algorithms have\nbeen mostly designed in a heuristic manner, and evaluated empirically with limited insights beyond the\nmeasurements of a few error or cost metrics on certain benchmarks. In contrast, formal analysis of LLM-\nbased algorithms, if it does exist, can bring various potential benefits, including but not limited to revealing\nthe reasons behind curious empirical phenomena, instructing choices of hyperparameters, predicting the\nempirical performance of LLM-based algorithms, and even inspiring new algorithm design.\nMain contributions. The goal of this work is to initiate a formal investigation into the design and analysis\nof LLM-based algorithms. Our contributions to filling this gap, and thereby advancing the field of LLM-based\nalgorithms, are summarized as follows.\n\u2022 In Section 2, we start by formulating LLM-based algorithms as computational graphs consisting of LLM\nnodes and non-LLM nodes, and identifying the design principle of task decomposition. We further\nintroduce some key abstractions, based on which formal analysis for the accuracy and efficiency of\nLLM-based algorithms is developed.\n\u2022 As a case study, in Section 3 we provide an in-depth study of parallel decomposition, a basic pattern\nof task decomposition and a building block for more sophisticated LLM-based algorithms. After in-\ntroducing the general formulation and analysis for this pattern, we present our algorithm design and\nanalysis for four diverse tasks, which are further validated by extensive numerical experiments.\n\u2022 We derive various novel insights from our analytical and empirical study. For example, considering\nthe hyperparameter m that represents the granularity of parallel decomposition, our analysis explains\nwhy error and cost metrics of an LLM-based algorithm are monotone in m in certain cases while\nnon-monotone in others, which in turn guides the choices of m for achieving the desired accuracy or\nefficiency. Our work exemplifies how to leverage the proposed framework for systematic design and\nanalysis of practical LLM-based algorithms, which can potentially inspire future work in this area."}, {"title": "2 LLM-based algorithms", "content": "This section introduces a formal framework for the design and analysis of LLM-based algorithms. In the\nfollowing, we present an overview for the definition, design principle, and analysis of LLM-based algorithms.\nFurther details are provided afterwards.\nDefinition. Generally speaking, an LLM-based algorithm is simply an algorithm that contains one or\nmultiple LLM calls as its key components. Examples of LLM-based algorithms range from one single LLM\ncall, to LLM-powered agent systems or compound AI systems consisting of a mixture of LLM calls and\nnon-LLM programs. One way of formally representing an LLM-based algorithm is using a computational\ngraph, which will soon be elaborated in Section 2.1.\nDesign principle: task decomposition. An LLM-based algorithm might utilize multiple LLM calls and\nnon-LLM programs to handle sub-tasks derived from the original problem, whose outputs together give rise\nto the final solution of the overall algorithm. This naturally implies the principle of task decomposition.\nIndeed, one key aspect of designing an LLM-based algorithm is to figure out how to decompose the original\ntask into sub-tasks appropriately, so that each of them can be handled by one LLM call or non-LLM program\naccurately and efficiently, while the performance of the overall algorithm is also guaranteed.\nThe principle of task decomposition has been widely adopted, either explicitly or implicitly, in prior works\non LLM-based algorithms. There are various reasons why task decomposition is beneficial and oftentimes\ncrucial for solving problems with LLMs, some of which are listed below:\n\u2022 Despite the rapid developments of Transformer-based LLMs, their context windows are still finite,\nwhich limits the problem size that one LLM call can handle. For an LLM that have a very large\nor even (in theory) infinite context window, its accuracy of solving a class of problems, e.g., long-\ntext retrieval or summarization, will typically decay with the size of the problem, in which case task\ndecomposition is still favorable.\n\u2022 Other classes of problems might have limited problem sizes, but require complex reasoning for solving\nthem correctly. In this case, LLM-based algorithms can benefit substantially from sequential task\ndecomposition such as step-by-step thinking and acting [YZY+23], or parallel decomposition like gen-\nerating multiple reasoning paths and then aggregating them for a better solution [SLC+23].\n\u2022 Aside from accuracy, it is possible that better efficiency can be achieved with task decomposition,\ncompared with one single LLM call. For instance, the input problem might be decomposed into\nmultiple independent sub-tasks that can be solved in parallel, while solving the problem with one LLM\ncall via autoregressive decoding, which is inherently sequential, suffers from a larger end-to-end latency.\nDespite all these benefits, it is also possible that fine-grained task decomposition can incur higher errors or\ncosts in certain cases. Choosing the appropriate decomposition is crucial for achieving good performance of\nthe overall algorithm, be it a trade-off between accuracy and efficiency or the best of both worlds.\nRemark 1. It is worth noting that one single LLM call can also be treated from the perspective of task\ndecomposition, which is a special case of LLM-based algorithms. Thus the analysis proposed in this work\nstill holds for this case.\nAnalysis: accuracy and efficiency. Given a task and the corresponding LLM-based algorithm with spe-\ncific configurations, we aim to analyze its performance, namely how accurately and efficiently the algorithm\nsolves the task, akin to analysis for any generic algorithm. Following the principle of task decomposition,\nthis is done by analyzing for each LLM call or non-LLM program first, and then for the overall algorithm.\nGiven that LLMs are regarded as black-box general-purpose problem solvers using natural language as input\nand output, we find it useful and necessary to leverage certain abstractions, to be introduced in Section 2.2,\nin order to facilitate formal analysis that will soon be presented in Section 2.3. One practical usage of formal\nanalysis is to predict the performance of an LLM-based algorithm before actually running it, which will in\nturn help to optimize certain hyperparameters in the configurations or inspire better algorithm design."}, {"title": "2.1 Algorithms as computational graphs", "content": "We formulate an LLM-based algorithm as a computational graph. Each graph node takes some inputs from\nits predecessor nodes, executes certain operations, and returns some outputs. The nodes can be categorized\ninto two types, which we refer to as LLM nodes and non-LLM nodes, demonstrated in Figure 1.\n\u2022 Within an LLM node, the operations consist of a prompter that formats a prompt based on the inputs,\nan LLM call that processes the prompt, and a parser that extracts the targeted information from the\nLLM's response. The prompter and parser, designed for the sub-task of the current node, serve as\ntranslators between natural language and traditional data structures. Following the principle of task\ndecomposition, we assume without loss of generality that each LLM node contains one single LLM call.\n\u2022 Within a non-LLM node, the operations can be anything that does not involve LLMs. Examples\ninclude a symbolic algorithm, an API call for a search engine, or a classical machine learning model.\nGiven such nodes, the computational graph is built by connecting them with directed edges that specify\nthe data flows within the LLM-based algorithm. For example, Figure 2a shows the graph for the pattern\nof parallel decomposition, which divides the input problem into parallel sub-tasks, solves each of them with\none LLM node, and aggregates the results for the final solution; Figure 2b illustrates an algorithm for book-\nlength summarization [CLGI24, Ope24b], which divides the input text into multiple chunks and maintains a\nglobal summary via incremental updating; and Figure 2c represents the ReAct algorithm [YZY+23], which\nconsists of multiple iterations of reasoning, tool use and aggregation. The computational-graph formulation\nis expressive enough to cover these diverse LLM-based algorithms that have been proposed separately in\nprior works, and facilitates formal analysis for all of them in a unified manner. Configurations of an LLM-\nbased algorithm include how task decomposition is done (reflected by the graph topology and the sub-tasks\nof graph nodes), the methods of prompting and parsing for each LLM node, configurations of the LLM(s)\nbeing used, and so on. The LLM nodes within one graph might use the same or different backbone LLMs."}, {"title": "2.2 Key abstractions", "content": "Let us introduce a few key abstractions that facilitate our formal analysis of LLM-based algorithms, namely,\nhow accurately and efficiently an LLM-based algorithm solve a task. A summary of these abstractions and\ntheir relation can be found in Figure 3.\nError and cost metrics. The accuracy and efficiency of each graph node and the overall LLM-based\nalgorithm can be quantified by certain error metrics and cost metrics respectively. The performance of\nindividual graph nodes, together with the graph topology, implies the performance of the overall algorithm.\nFor each specific task or algorithm, one might define multiple error metrics and cost metrics, which can be\nanalyzed in a unified manner within our proposed framework.\nError metrics are task-specific in general. Moreover, the overall algorithm and the LLM nodes within\nit might have different error metrics. For instance, consider an LLM-based algorithm that answers a given\nquestion by first retrieving relevant sentences from the input text with multiple LLM nodes, and then\nreasoning over the extracted sentences with another LLM node. In this case, the overall algorithm is"}, {"title": "2.3 Formal analysis: accuracy and efficiency", "content": "Given the aforementioned formulations and abstractions, we are now ready to consider formal analysis for the\naccuracy and efficiency of LLM-based algorithms. More specifically, one can first analyze the error and cost\nmetrics for each individual node within the computational graph; these, combined with the graph topology,\nlead to results about the error and cost metrics of the overall algorithm.\nWe elaborate this approach in the following. Unless otherwise specified, our analysis is deterministic, for\na given task instance and fixed random seed(s).\nAnalysis of error metrics. The error metrics of the output of each LLM node, which are calculated with\nrespect to what the output should have been if all nodes accomplish their tasks with exact accuracy, depend\non the characteristics, i.e. capabilities and limitations, of the LLM, as well as the specific problem instance\nand random seed(s). For a certain node v, the error of its output y can be bounded by some function fu\n(which depends on the aforementioned factors if v is an LLM node) of the errors of its inputs x\u2081, x\u2082,..., x\u2096,\ni.e. the outputs of its predecessor nodes:\n$\\mathcal{E}(y) \\leq f_v (\\mathcal{E}(x_1), \\mathcal{E}(x_2),...,\\mathcal{E}(x_k)).\\qquad(1)$\nThe function fu can be general, with some examples listed in the following:\n\u2022 $\\mathcal{E}(y) \\leq \\sum_{i \\in [k]} \\mathcal{E}(x_i)$ or $\\sqrt{\\sum_{i \\in [k]} \\mathcal{E}(x_i)}$. Such linear relationship can appear in, for example, a simple\ncounting task that will be elaborated in Section 3.3."}, {"title": "2.4 Practical considerations", "content": "Let us comment on a few practical considerations about the aforementioned abstractions and analysis.\n\u2022 Regarding the capabilities and limitations of LLMs in a specific task, the black-box nature of LLMs\ncan make it challenging to analytically and accurately quantify these factors, in which case one might\nresort to measuring and profiling in practice. On the positive side, it is oftentimes easier to make\ncertain qualitative assumptions. For example, in many tasks of interest, a larger problem instance is\nharder than a smaller one, and thus incurs larger error metrics of an LLM call. For practical purposes\nlike optimizing certain hyperparameters of an LLM-based algorithm, such weak assumptions might be\nsufficient already.\n\u2022 Understanding LLM inference service, especially from a system perspective, is crucial for in-depth\nanalysis of cost metrics. LLM inference service can be diverse in practice: for example, LLMs might\nrun on CPUs in a personal laptop or on a distributed GPU cluster, inference might be compute-\nbound or memory-bound, the complexity of long-sequence processing and generation might be linear\nor quadratic, parallelism at various levels (e.g. multiple LLMs deployed on multiple machines, or batch\ninference with one LLM) might be supported, and so on. Fortunately, all these are covered by our\nproposed framework in a unified manner."}, {"title": "3 Case study: parallel decomposition", "content": "This section focuses on parallel decomposition, a basic pattern visualized in Figure 2a. An algorithm of this\npattern first divides the input problem into multiple independent sub-tasks, solves each of them with one\nLLM node, and finally aggregates the results with an LLM or non-LLM node for the final solution. The\nintermediate sub-tasks can be solved sequentially or in parallel, which has impacts on certain cost metrics\nof the overall algorithm. This basic form of task decomposition can be used as a building block for more\nsophisticated algorithms. Despite its simplicity, interesting analysis and a wide variety of concrete tasks and\nalgorithms can be derived from this pattern, which are presented in the remaining of this section."}, {"title": "3.1 Notations and analysis", "content": "Let us start by defining some formal notations that will be useful in our analysis.\n\u2022 We denote the size of the input problem instance as n. In our study of LLM-based algorithms, the\nterm \"size\" here typically means the length, e.g. the number of characters or tokens, of the text that\nrepresents the input problem instance; it can also be defined in terms of traditional data structures,\nsuch as the length of a list.\n\u2022 Let k denote the number of parallel sub-tasks after decomposition, and m\u1d62 denote the size of the i-th\nsub-task, where i \u2208 [k]. It is assumed that m\u1d62 \u2264 m for some maximum value m that can fit into\nthe context window of the LLM used in the algorithm. For most of our analysis, we will assume for\nsimplicity that m\u1d62 = m for all i \u2208 [k], and that k = O(n/m).\n\u2022 We denote p > 1 as the maximum degree of parallelism supported by LLM inference service.\n\u2022 Finally, let L\u209b\u1d67\u209b be an upper bound for the length of the system prompt of each LLM call. The\npresence of L\u209b\u1d67\u209b, which can be large in practice, is essentially due to the fact that the LLM is used as a\ngeneral-purpose sub-routine, hence specifying the concrete task for each LLM call constitutes part of\nthe complexity.\nThese notations are summarized in Table 1 for convenient reference.\nFor notational convenience, we will often write f(n) \u2264 g(n) in place of f(n) = O(g(n)), which means\nthere exists a universal constant C > 0 such that f(n) < C \u00b7 g(n) for any positive integer n. In addition,\nf(n) = g(n) means f(n) \u2264 g(n) and g(n) \u2264 f(n) both hold. To further simplify notation, we will often\nomit the big-O notation in the input variables of cost functions C\u209a\u1d63\u2091 and C\ud835\udcb9\u2091\ud835\udcb8; for example, given a prompt of\nlength L\u209b\u1d67\u209b+O(n), we will write the cost of the prefilling phase as C\u209a\u1d63\u2091(L\u209b\u1d67\u209b+n) rather than C\u209a\u1d63\u2091(L\u209b\u1d67\u209b+O(n)).\nAnalysis of cost metrics. We assume for simplicity that all parallel LLM nodes share the same LLM\nmodel and inference service, and thus the same cost functions C\u209a\u1d63\u2091 and C\ud835\udcb9\u2091\ud835\udcb8.\nIn many cases, the cost of the overall algorithm is a simple sum of the costs of all LLM calls involved.\nExamples include the financial cost when the LLM APIs are charged by tokens, and the end-to-end latency\nwhen all LLM calls are executed sequentially. In such cases, we can write the total cost C as\nC = C(sub-tasks) + C(aggregation),\nwhere C(aggregation) is the cost of the final aggregation step, and\nC(sub-tasks) = k \u00d7 C(one sub-task)\n\u2264 k \u00d7 (C\u209a\u1d63\u2091(L\u209b\u1d67\u209b + m) + C\ud835\udcb9\u2091\ud835\udcb8(L\u209b\u1d67\u209b + m, L\ud835\udcb9\u2091\ud835\udcb8))\n\u2264 $\\frac{n}{m}$\u00d7 (C\u209a\u1d63\u2091(L\u209b\u1d67\u209b + m) + C\ud835\udcb9\u2091\ud835\udcb8(L\u209b\u1d67\u209b + m, L\ud835\udcb9\u2091\ud835\udcb8)).\\qquad(4)$\nHere, the first inequality follows Eq. (2), the second follows k \u2264 n/m, and L\ud835\udcb9\u2091\ud835\udcb8 = L\ud835\udcb9\u2091\ud835\udcb8(m) is task-specific,\nwhich can be O(1) or O(m) for example."}, {"title": "3.2 Concrete examples and experiment settings", "content": "The rest of this section is dedicated to the analytical and empirical study of a few specific tasks and their\ncorresponding LLM-based algorithms that follow the pattern of parallel decomposition.\nConcrete examples. Tasks that we consider in the rest of this section include counting, sorting, retrieval,\nand retrieval-augmented generation (RAG), whose details will soon be explained in their corresponding\nsubsections. For each task, we specify the concrete LLM-based algorithm, analyze its performance in terms\nof error and cost metrics, and validate our analysis with numerical experiments. Error metrics are task-\nspecific, while cost metrics of interest are common among tasks, including the total prefilling length and\ndecoding length, the total number of LLM calls, and the end-to-end latency with sequential or parallel LLM\ncalls. These concrete examples not only confirm the practical advantages of LLM-based algorithms, but also\nverify that our analysis can help explain or predict the empirical performance of LLM-based algorithms,\nreveal the reasons behind some interesting empirical phenomena, and instruct the design of algorithms or\nchoices of hyperparameters, e.g. the sub-task size m.\nExperiment settings. We use the following LLMs in our experiments, which cover a wide range of LLM\ncharacteristics and inference service:\n\u2022 A Llama-3-8B model [Met24], supported by ollama [oll23] and running on a Macbook Pro with a M2\nPro chip and 16GB memory;\n\u2022 A Llama-3-70B model [Met24], supported by vLLM [KLZ+23] and running on a server with 4 Nvidia\nA100-80G GPUs;\n\u2022 A GPT-4-Turbo model [Ope24a], accessed via API queries.\nAll of these LLMs are chat models. Each LLM call involved in our algorithms is prompted in a chat format,\nbased on the sub-task that it is responsible for. Interested readers are referred to the source code for the\nprompts used in our experiments.\nBelow are a few more details about our experiments. (1) For ideal parallelism of LLM calls, we consider\nparallelism degree p = 4 and p = \u221e. Latencies in the presence of parallelism are simulated according to\nEq. (3). (2) In all experiments, the number of tokens for a piece of text is estimated using the same tokenizer,\nnamely the c1100k_base encoding of the tiktoken package\u00b9. This simplification has no effect on the major\nconclusions from our experiment results. (3) Our experiment results include curves of some error metric\n(in blue) or cost metric (in red) versus the problem size n or sub-task size m. For each curve, we plot the\nmean and standard deviation of measured metrics from multiple independent trials, i.e. multiple randomly\ngenerated task instances."}, {"title": "3.3 Example: counting", "content": "As a warm-up exercise, we consider a simple counting task formulated as follows: given a string of length\nn consisting of letters and digits, the task is to count the number of digits in it. This can be seen as an\nabstraction or synthetic version of more generic counting tasks in practice.\n3.3.1 Algorithm\nBelow is an LLM-based algorithm that follows the pattern of parallel decomposition:\n1. Divide the input string into k disjoint sub-strings of lengths m\u2081, m\u2082,..., m\u2096;\n2. For each i \u2208 [k], let LLM count the number of digits in the i-th sub-string, and return its answer y\u1d62;\n3. The final solution of the algorithm is $y = \\sum_{i \\in [k]} y_i$.\nFor each LLM call, we prompt the LLM to generate its answer without intermediate reasoning steps\u00b2. Thus\nit is reasonable to assume, based on the LLM's capability of instruction following, that the text generated\nby each LLM call has length L\ud835\udcb9\u2091\ud835\udcb8 = O(1) and can be parsed into a count number.\n3.3.2 Analysis\nLet us assume for notational convenience that m\u1d62 = m for all i \u2208 [k], and k = n/m is an integer.\nError metrics. Denote the ground-truth count for the complete string as y*, and the ground-truth count\nfor the i-th sub-string as y\u1d62*.\n\u2022 Let $\\mathcal{E}$ represent the absolute counting error, then\n$\\mathcal{E}(y) := |y - y^* | = |\\sum_{i \\in [k]} (y_i - y_i^*) | \\leq \\sum_{i \\in [k]} | y_i - y_i^*| = \\sum_{i \\in [k]} \\mathcal{E}(y_i).$\nIn other words, the overall error is upper bounded by the sum of errors of sub-tasks.\n\u2022 If we let $\\mathcal{E}$ represent the normalized counting error instead, then\n$\\mathcal{E}(y) := \\frac{|y - y^*|}{n} \\leq \\frac{1}{n} \\sum_{i \\in [k]} |y_i - y_i^*| = \\frac{1}{kn} \\sum_{i \\in [k]} \\mathcal{E}(y_i).$\nWith this metric, the overall error is upper bounded by the average of errors of sub-tasks.\nCost metrics. Our analysis of cost metrics follows Section 3.1. More specifically, we have L\ud835\udcb9\u2091\ud835\udcb8 = O(1) by\nassumption, and C(aggregation) = 0 since the final step of the algorithm is done by a non-LLM node.\n\u2022 Considering the total prefilling length and decoding length as cost metrics, one has\nC(prefilling) \u2264 k \u00d7 (Lsys + m) = $\\frac{n}{m}$ \u00d7 (Lsys + m)\nC(decoding) \u2264 k \u00d7 1 = k = $\\frac{n}{m}$,\nboth of which are monotonely decreasing in m.\n\u2022 More generally, the sum of costs of all LLM calls is\nC = C(sub-tasks) \u2264 k \u00d7 (Cpre (Lsys + m) + Cdec (Lsys + m, 1))\n= n \u00d7$\\frac{ Cpre(Lsys + m) + Cdec(Lsys + m, 1)}{m}$\nThe optimal choice of m under various conditions has been discussed in Section 3.1."}, {"title": "3.4 Example: sorting", "content": "For a more challenging example, let us consider the classical sorting problem: given a list of n numbers\nx\u2208 R, the task is to sort it in ascending order.\n3.4.1 Algorithm\nBelow is an LLM-based algorithm for sorting a list, which generalizes the naive approach of sorting the list\nwith one single LLM call:\n1. Divide the input list x into k disjoint sub-lists x\u2081,...,x\u2096 of lengths m\u2081,..., m\u2096;\n2. For each i \u2208 [k], use one LLM call to sort the i-th sub-list, which returns a solution y\u1d62;\n3. Merge the sub-lists y\u2081,..., y\u2096 into a single list y using a symbolic algorithm.\nWe note two details about this algorithm. (1) To ensure efficiency and stability, for each LLM call, we\nprompt the LLM to generate the sorted list directly, without intermediate reasoning steps. It has been\nverified empirically that the LLMs considered in our experiments can follow such instructions, and generate\ntext of length Ldec(m) = O(m) that can be easily parsed into a list. (2) Step 3 of the algorithm relies\non a classical symbolic algorithm for merging two sorted lists, which maintains two moving pointers, one\nfor each list, and chooses each entry of the merged list by comparing the values corresponding to the two\npointers. Merging multiple sorted lists can be done by merging one pair of lists at a time, in an incremental or\nhierarchical manner. Python code for these procedures can be found in Listing 1 at the end of this subsection.\nAlthough they are designed under the assumption that the input lists are sorted, they can also be applied to\ninput lists that are not fully sorted, which can possibly happen within the LLM-based algorithm since the\ninput lists in Step 3 are generated by LLM calls in Step 2.\n3.4.2 Analysis\nLet us assume for notational convenience that m\u1d62 = m for all i \u2208 [k], and k = n/m is an integer.\nError metrics. Compared to counting, there are more diverse phenomena in the sorting task in terms of\nerror metrics. In particular, multiple possible failure modes exist in sorting with an LLM-based algorithm:\n1. The output list might not be monotone;\n2. The length of the output list might be larger or smaller than that of the input list;\n3. The output list might contain numbers that do not match exactly those of the input list.\nBased on these failure modes, we define the following error metrics for sorting a list, where y denotes the\nsolution returned by the algorithm and y* denotes the ground-truth solution:\n\u2022 Exact-match error: $\\mathcal{E}$ = 0 if y matches y* exactly, and $\\mathcal{E}$ = 1 otherwise;\n\u2022 Non-monotonicity error: $\\mathcal{E} = \\sum_{i \\in [n-1]} max{y_i - y_{i+1},0}$, which is zero if and only if y is sorted;\n\u2022 Length-mismatch error: $\\mathcal{E} = \\frac{1}{n} |len(y) - len(y^*)| = \\frac{1}{n} |len(y) - n|$;\n\u2022 Fuzzy l\u2080 and fuzzy normalized l\u2081 errors: we first convert, via simple extending or truncating, the\noutput solution y to a version \u0177 that matches the length n of the input list, and then calculate\nthe fuzzy l\u2080 error as $\\mathcal{E} = ||\\hat{y} - y^*||_{\\infty} = \\max_{i \\in [n]} |y_i - y_i^*|$, or the fuzzy normalized l\u2081 error as\n$\\mathcal{E} = ||\\hat{y} - y^*||_1 = \\sum_{i \\in [n]} |y_i - y_i^*|$.\nNote that the same error metrics can be similarly defined for each parallel sub-task in Step 2 of the LLM-\nbased algorithm, and it is reasonable to expect that they become smaller as the sub-task size m decreases.\nOn the other hand, analyzing the error metrics of the overall algorithm after the final merging step can be\nmore complicated, and might be an interesting theoretical problem on its own. As an example, focusing on\nthe third failure mode and the l\u2080 error metric, we have the following guarantee, whose proof is deferred\nafter the experiments in this subsection."}, {"title": "3.5 Example: retrieval", "content": "We study a more realistic application of LLM-based algorithms", "The passcode to\nthe {targeted object, e.g. red door} is {6-digit passcode}\\\" is randomly inserted into a piece of long text (the\nhaystack). The algorithm is asked to answer the question \\\"What is the passcode to the targeted object}?\\\"\nTo make the problem more challenging and fun, we let the haystack consist of alike sentences of the form\n\\\"The passcode to the {colored object, e.g. red lock or green door} is {6-digit passcode}\\\", with colored objects\ndifferent from the targeted object. This allows us to investigate both sides of retrieval capabilities of LLMs\nand LLM-based algorithms: retrieving the targeted information correctly, while avoiding being confused or\nmisled by background information that might seem relevant to the question [SCM+23": ".", "decomposition": "n1. Divide the input text of length n into k overlapping chunks of lengths m\u2081", "I don't know\" if it believes that\nthere is not sufficient information in the corresponding chunk, e.g. when the chunk simply does not contain\nthe needle. Such answers will be excluded from the final step of the algorithm. (4) In the final step of\nmajority voting, it is possible that there are multiple (say h) candidate solutions with the same frequency,\nin which case we let the```json\n{\n  ": "itle", "On the Design and Analysis of LLM-Based Algorithms": "authors\": [\n    \"Yanxi Chen", "Yaliang Li": "Bolin Ding", "Jingren Zhou": "abstract\": \"We initiate a formal investigation into the design and analysis of LLM-based algorithms, i.e. algorithms\nthat contain one or multiple calls of large language models (LLMs) as sub-routines and critically rely\non the capabilities of LLMs. While LLM-based algorithms, ranging from basic LLM calls with prompt\nengineering to complicated LLM-powered agent systems and compound AI systems, have achieved re-\nmarkable empirical success, the design and optimization of them have mostly relied on heuristics and\ntrial-and-errors, which is largely due to a lack of formal and analytical study for these algorithms. To\nfill this gap, we start by identifying the computational-graph representation of LLM-based algorithms,\nthe design principle of task decomposition, and some key abstractions, which then facilitate our for-\nmal analysis for the accuracy and efficiency of LLM-based algorithms, despite the black-box nature of\nLLMs. We further consider parallel decomposition for a case study, providing extensive analytical and\nempirical study for four concrete examples of this pattern. Our proposed framework holds promise for\nadvancing LLM-based algorithms, by revealing the reasons behind curious empirical phenomena, guiding\nthe choices of hyperparameters, predicting the empirical performance of algorithms, and inspiring new\nalgorithm design. To promote further study of LLM-based algorithms, we release our source code at\nhttps://github.com/modelscope/agentscope/tree/main/examples/paper_llm_based_algorithm.", "sections": [{"title": "1 Introduction", "content": "The rapid developments of pre-trained large language models (LLMs) in the past few years [BCE+23,\nWTB+22, SMK23] have given rise to the new paradigm of algorithmic problem solving by utilizing LLMs\nas general-purpose solvers and prompting them with task descriptions plus additional inputs that can help\nenhance their performance [WWS+22, KGR+22, ZYY24]. Meanwhile, it is also well recognized that even\nthe best LLMs today still exhibit various limitations, such as finite context window sizes and difficulty\nwith complex reasoning. Some of these limitations are fundamental [MS23, PNP24, TTC+24, HR24], and\nresolving them requires new breakthroughs. Moreover, in resource-constrained scenarios where using the\nstate-of-the-art LLMs is not feasible, one might have to resort to smaller and weaker LLMs for solving\ncomplex tasks.\nAll these have motivated the developments of what we call LLM-based algorithms, namely, algorithms\nthat contain one or multiple LLM calls as sub-routines and fundamentally rely on the capabilities of LLMs.\nIn its most basic form, an LLM-based algorithm can be a combination of one or multiple LLM calls and some\nprompt engineering [YYZ+23, BBK+23, LLLD23, SLC+23, PKH+24, CLGI24]. More advanced examples\ninclude LLM-powered agent systems [MDL+23, WBZ+23, HZC+23, GLP+24, SST+23, ZWK+24, PKB+24,\nXCG+23] and compound AI systems [ZKC+24, CDH+24] that augment LLMs with additional abilities like\ntool use and long-term memory, as well as the emerging paradigm of LLM programming [SSC+23, KTF+23,\nKSM+24, ZYX+24].\nLLM-based algorithms, in a similar spirit to neuro-symbolic programming [CEP+21, GK23] and learning-\naugmented algorithms [MV22, LM22], combine the advantages of both LLMs and traditional algorithms. An\nLLM-based algorithm, designed with human's intelligence and knowledge of algorithmic problem solving, can\nexhibit much better controllability and interpretability, stronger performance that is less reliant on delicate\nprompting or extensive trial-and-errors, and capabilities that far exceed what could possibly be achieved by\ndirectly prompting the LLM for a solution.\nThe rapid developments of LLM-based algorithms naturally raise the question: is it possible to provide\nany formal analysis or guarantee for LLM-based algorithms? This seems like a daunting task at first glance,\ndue to the black-box nature of LLMs. Indeed, in prior works of this field, LLM-based algorithms have\nbeen mostly designed in a heuristic manner, and evaluated empirically with limited insights beyond the\nmeasurements of a few error or cost metrics on certain benchmarks. In contrast, formal analysis of LLM-\nbased algorithms, if it does exist, can bring various potential benefits, including but not limited to revealing\nthe reasons behind curious empirical phenomena, instructing choices of hyperparameters, predicting the\nempirical performance of LLM-based algorithms, and even inspiring new algorithm design.\nMain contributions. The goal of this work is to initiate a formal investigation into the design and analysis\nof LLM-based algorithms. Our contributions to filling this gap, and thereby advancing the field of LLM-based\nalgorithms, are summarized as follows.\n\u2022 In Section 2, we start by formulating LLM-based algorithms as computational graphs consisting of LLM\nnodes and non-LLM nodes, and identifying the design principle of task decomposition. We further\nintroduce some key abstractions, based on which formal analysis for the accuracy and efficiency of\nLLM-based algorithms is developed.\n\u2022 As a case study, in Section 3 we provide an in-depth study of parallel decomposition, a basic pattern\nof task decomposition and a building block for more sophisticated LLM-based algorithms. After in-\ntroducing the general formulation and analysis for this pattern, we present our algorithm design and\nanalysis for four diverse tasks, which are further validated by extensive numerical experiments.\n\u2022 We derive various novel insights from our analytical and empirical study. For example, considering\nthe hyperparameter m that represents the granularity of parallel decomposition, our analysis explains\nwhy error and cost metrics of an LLM-based algorithm are monotone in m in certain cases while\nnon-monotone in others, which in turn guides the choices of m for achieving the desired accuracy or\nefficiency. Our work exemplifies how to leverage the proposed framework for systematic design and\nanalysis of practical LLM-based algorithms, which can potentially inspire future work in this area."}, {"title": "2 LLM-based algorithms", "content": "This section introduces a formal framework for the design and analysis of LLM-based algorithms. In the\nfollowing, we present an overview for the definition, design principle, and analysis of LLM-based algorithms.\nFurther details are provided afterwards.\nDefinition. Generally speaking, an LLM-based algorithm is simply an algorithm that contains one or\nmultiple LLM calls as its key components. Examples of LLM-based algorithms range from one single LLM\ncall, to LLM-powered agent systems or compound AI systems consisting of a mixture of LLM calls and\nnon-LLM programs. One way of formally representing an LLM-based algorithm is using a computational\ngraph, which will soon be elaborated in Section 2.1.\nDesign principle: task decomposition. An LLM-based algorithm might utilize multiple LLM calls and\nnon-LLM programs to handle sub-tasks derived from the original problem, whose outputs together give rise\nto the final solution of the overall algorithm. This naturally implies the principle of task decomposition.\nIndeed, one key aspect of designing an LLM-based algorithm is to figure out how to decompose the original\ntask into sub-tasks appropriately, so that each of them can be handled by one LLM call or non-LLM program\naccurately and efficiently, while the performance of the overall algorithm is also guaranteed.\nThe principle of task decomposition has been widely adopted, either explicitly or implicitly, in prior works\non LLM-based algorithms. There are various reasons why task decomposition is beneficial and oftentimes\ncrucial for solving problems with LLMs, some of which are listed below:\n\u2022 Despite the rapid developments of Transformer-based LLMs, their context windows are still finite,\nwhich limits the problem size that one LLM call can handle. For an LLM that have a very large\nor even (in theory) infinite context window, its accuracy of solving a class of problems, e.g., long-\ntext retrieval or summarization, will typically decay with the size of the problem, in which case task\ndecomposition is still favorable.\n\u2022 Other classes of problems might have limited problem sizes, but require complex reasoning for solving\nthem correctly. In this case, LLM-based algorithms can benefit substantially from sequential task\ndecomposition such as step-by-step thinking and acting [YZY+23], or parallel decomposition like gen-\nerating multiple reasoning paths and then aggregating them for a better solution [SLC+23].\n\u2022 Aside from accuracy, it is possible that better efficiency can be achieved with task decomposition,\ncompared with one single LLM call. For instance, the input problem might be decomposed into\nmultiple independent sub-tasks that can be solved in parallel, while solving the problem with one LLM\ncall via autoregressive decoding, which is inherently sequential, suffers from a larger end-to-end latency.\nDespite all these benefits, it is also possible that fine-grained task decomposition can incur higher errors or\ncosts in certain cases. Choosing the appropriate decomposition is crucial for achieving good performance of\nthe overall algorithm, be it a trade-off between accuracy and efficiency or the best of both worlds.\nRemark 1. It is worth noting that one single LLM call can also be treated from the perspective of task\ndecomposition, which is a special case of LLM-based algorithms. Thus the analysis proposed in this work\nstill holds for this case.\nAnalysis: accuracy and efficiency. Given a task and the corresponding LLM-based algorithm with spe-\ncific configurations, we aim to analyze its performance, namely how accurately and efficiently the algorithm\nsolves the task, akin to analysis for any generic algorithm. Following the principle of task decomposition,\nthis is done by analyzing for each LLM call or non-LLM program first, and then for the overall algorithm.\nGiven that LLMs are regarded as black-box general-purpose problem solvers using natural language as input\nand output, we find it useful and necessary to leverage certain abstractions, to be introduced in Section 2.2,\nin order to facilitate formal analysis that will soon be presented in Section 2.3. One practical usage of formal\nanalysis is to predict the performance of an LLM-based algorithm before actually running it, which will in\nturn help to optimize certain hyperparameters in the configurations or inspire better algorithm design."}, {"title": "2.1 Algorithms as computational graphs", "content": "We formulate an LLM-based algorithm as a computational graph. Each graph node takes some inputs from\nits predecessor nodes, executes certain operations, and returns some outputs. The nodes can be categorized\ninto two types, which we refer to as LLM nodes and non-LLM nodes, demonstrated in Figure 1.\n\u2022 Within an LLM node, the operations consist of a prompter that formats a prompt based on the inputs,\nan LLM call that processes the prompt, and a parser that extracts the targeted information from the\nLLM's response. The prompter and parser, designed for the sub-task of the current node, serve as\ntranslators between natural language and traditional data structures. Following the principle of task\ndecomposition, we assume without loss of generality that each LLM node contains one single LLM call.\n\u2022 Within a non-LLM node, the operations can be anything that does not involve LLMs. Examples\ninclude a symbolic algorithm, an API call for a search engine, or a classical machine learning model.\nGiven such nodes, the computational graph is built by connecting them with directed edges that specify\nthe data flows within the LLM-based algorithm. For example, Figure 2a shows the graph for the pattern\nof parallel decomposition, which divides the input problem into parallel sub-tasks, solves each of them with\none LLM node, and aggregates the results for the final solution; Figure 2b illustrates an algorithm for book-\nlength summarization [CLGI24, Ope24b], which divides the input text into multiple chunks and maintains a\nglobal summary via incremental updating; and Figure 2c represents the ReAct algorithm [YZY+23], which\nconsists of multiple iterations of reasoning, tool use and aggregation. The computational-graph formulation\nis expressive enough to cover these diverse LLM-based algorithms that have been proposed separately in\nprior works, and facilitates formal analysis for all of them in a unified manner. Configurations of an LLM-\nbased algorithm include how task decomposition is done (reflected by the graph topology and the sub-tasks\nof graph nodes), the methods of prompting and parsing for each LLM node, configurations of the LLM(s)\nbeing used, and so on. The LLM nodes within one graph might use the same or different backbone LLMs."}, {"title": "2.2 Key abstractions", "content": "Let us introduce a few key abstractions that facilitate our formal analysis of LLM-based algorithms, namely,\nhow accurately and efficiently an LLM-based algorithm solve a task. A summary of these abstractions and\ntheir relation can be found in Figure 3.\nError and cost metrics. The accuracy and efficiency of each graph node and the overall LLM-based\nalgorithm can be quantified by certain error metrics and cost metrics respectively. The performance of\nindividual graph nodes, together with the graph topology, implies the performance of the overall algorithm.\nFor each specific task or algorithm, one might define multiple error metrics and cost metrics, which can be\nanalyzed in a unified manner within our proposed framework.\nError metrics are task-specific in general. Moreover, the overall algorithm and the LLM nodes within\nit might have different error metrics. For instance, consider an LLM-based algorithm that answers a given\nquestion by first retrieving relevant sentences from the input text with multiple LLM nodes, and then\nreasoning over the extracted sentences with another LLM node. In this case, the overall algorithm is"}, {"title": "2.3 Formal analysis: accuracy and efficiency", "content": "Given the aforementioned formulations and abstractions, we are now ready to consider formal analysis for the\naccuracy and efficiency of LLM-based algorithms. More specifically, one can first analyze the error and cost\nmetrics for each individual node within the computational graph; these, combined with the graph topology,\nlead to results about the error and cost metrics of the overall algorithm.\nWe elaborate this approach in the following. Unless otherwise specified, our analysis is deterministic, for\na given task instance and fixed random seed(s).\nAnalysis of error metrics. The error metrics of the output of each LLM node, which are calculated with\nrespect to what the output should have been if all nodes accomplish their tasks with exact accuracy, depend\non the characteristics, i.e. capabilities and limitations, of the LLM, as well as the specific problem instance\nand random seed(s). For a certain node v, the error of its output y can be bounded by some function fu\n(which depends on the aforementioned factors if v is an LLM node) of the errors of its inputs x\u2081, x\u2082,..., x\u2096,\ni.e. the outputs of its predecessor nodes:\n$\\mathcal{E}(y) \\leq f_v (\\mathcal{E}(x_1), \\mathcal{E}(x_2),...,\\mathcal{E}(x_k)).\\qquad(1)$\nThe function fu can be general, with some examples listed in the following:\n\u2022 $\\mathcal{E}(y) \\leq \\sum_{i \\in [k]} \\mathcal{E}(x_i)$ or $\\sqrt{\\sum_{i \\in [k]} \\mathcal{E}(x_i)}$. Such linear relationship can appear in, for example, a simple\ncounting task that will be elaborated in Section 3.3."}, {"title": "2.4 Practical considerations", "content": "Let us comment on a few practical considerations about the aforementioned abstractions and analysis.\n\u2022 Regarding the capabilities and limitations of LLMs in a specific task, the black-box nature of LLMs\ncan make it challenging to analytically and accurately quantify these factors, in which case one might\nresort to measuring and profiling in practice. On the positive side, it is oftentimes easier to make\ncertain qualitative assumptions. For example, in many tasks of interest, a larger problem instance is\nharder than a smaller one, and thus incurs larger error metrics of an LLM call. For practical purposes\nlike optimizing certain hyperparameters of an LLM-based algorithm, such weak assumptions might be\nsufficient already.\n\u2022 Understanding LLM inference service, especially from a system perspective, is crucial for in-depth\nanalysis of cost metrics. LLM inference service can be diverse in practice: for example, LLMs might\nrun on CPUs in a personal laptop or on a distributed GPU cluster, inference might be compute-\nbound or memory-bound, the complexity of long-sequence processing and generation might be linear\nor quadratic, parallelism at various levels (e.g. multiple LLMs deployed on multiple machines, or batch\ninference with one LLM) might be supported, and so on. Fortunately, all these are covered by our\nproposed framework in a unified manner."}, {"title": "3 Case study: parallel decomposition", "content": "This section focuses on parallel decomposition, a basic pattern visualized in Figure 2a. An algorithm of this\npattern first divides the input problem into multiple independent sub-tasks, solves each of them with one\nLLM node, and finally aggregates the results with an LLM or non-LLM node for the final solution. The\nintermediate sub-tasks can be solved sequentially or in parallel, which has impacts on certain cost metrics\nof the overall algorithm. This basic form of task decomposition can be used as a building block for more\nsophisticated algorithms. Despite its simplicity, interesting analysis and a wide variety of concrete tasks and\nalgorithms can be derived from this pattern, which are presented in the remaining of this section."}, {"title": "3.1 Notations and analysis", "content": "Let us start by defining some formal notations that will be useful in our analysis.\n\u2022 We denote the size of the input problem instance as n. In our study of LLM-based algorithms, the\nterm \"size\" here typically means the length, e.g. the number of characters or tokens, of the text that\nrepresents the input problem instance; it can also be defined in terms of traditional data structures,\nsuch as the length of a list.\n\u2022 Let k denote the number of parallel sub-tasks after decomposition, and m\u1d62 denote the size of the i-th\nsub-task, where i \u2208 [k]. It is assumed that m\u1d62 \u2264 m for some maximum value m that can fit into\nthe context window of the LLM used in the algorithm. For most of our analysis, we will assume for\nsimplicity that m\u1d62 = m for all i \u2208 [k], and that k = O(n/m).\n\u2022 We denote p > 1 as the maximum degree of parallelism supported by LLM inference service.\n\u2022 Finally, let L\u209b\u1d67\u209b be an upper bound for the length of the system prompt of each LLM call. The\npresence of L\u209b\u1d67\u209b, which can be large in practice, is essentially due to the fact that the LLM is used as a\ngeneral-purpose sub-routine, hence specifying the concrete task for each LLM call constitutes part of\nthe complexity.\nThese notations are summarized in Table 1 for convenient reference.\nFor notational convenience, we will often write f(n) \u2264 g(n) in place of f(n) = O(g(n)), which means\nthere exists a universal constant C > 0 such that f(n) < C \u00b7 g(n) for any positive integer n. In addition,\nf(n) = g(n) means f(n) \u2264 g(n) and g(n) \u2264 f(n) both hold. To further simplify notation, we will often\nomit the big-O notation in the input variables of cost functions C\u209a\u1d63\u2091 and C\ud835\udcb9\u2091\ud835\udcb8; for example, given a prompt of\nlength L\u209b\u1d67\u209b+O(n), we will write the cost of the prefilling phase as C\u209a\u1d63\u2091(L\u209b\u1d67\u209b+n) rather than C\u209a\u1d63\u2091(L\u209b\u1d67\u209b+O(n)).\nAnalysis of cost metrics. We assume for simplicity that all parallel LLM nodes share the same LLM\nmodel and inference service, and thus the same cost functions C\u209a\u1d63\u2091 and C\ud835\udcb9\u2091\ud835\udcb8.\nIn many cases, the cost of the overall algorithm is a simple sum of the costs of all LLM calls involved.\nExamples include the financial cost when the LLM APIs are charged by tokens, and the end-to-end latency\nwhen all LLM calls are executed sequentially. In such cases, we can write the total cost C as\nC = C(sub-tasks) + C(aggregation),\nwhere C(aggregation) is the cost of the final aggregation step, and\nC(sub-tasks) = k \u00d7 C(one sub-task)\n\u2264 k \u00d7 (C\u209a\u1d63\u2091(L\u209b\u1d67\u209b + m) + C\ud835\udcb9\u2091\ud835\udcb8(L\u209b\u1d67\u209b + m, L\ud835\udcb9\u2091\ud835\udcb8))\n\u2264 $\\frac{n}{m}$\u00d7 (C\u209a\u1d63\u2091(L\u209b\u1d67\u209b + m) + C\ud835\udcb9\u2091\ud835\udcb8(L\u209b\u1d67\u209b + m, L\ud835\udcb9\u2091\ud835\udcb8)).\\qquad(4)$\nHere, the first inequality follows Eq. (2), the second follows k \u2264 n/m, and L\ud835\udcb9\u2091\ud835\udcb8 = L\ud835\udcb9\u2091\ud835\udcb8(m) is task-specific,\nwhich can be O(1) or O(m) for example."}, {"title": "3.3 Example: counting", "content": "As a warm-up exercise, we consider a simple counting task formulated as follows: given a string of length\nn consisting of letters and digits, the task is to count the number of digits in it. This can be seen as an\nabstraction or synthetic version of more generic counting tasks in practice.\n3.3.1 Algorithm\nBelow is an LLM-based algorithm that follows the pattern of parallel decomposition:\n1. Divide the input string into k disjoint sub-strings of lengths m\u2081, m\u2082,..., m\u2096;\n2. For each i \u2208 [k], let LLM count the number of digits in the i-th sub-string, and return its answer y\u1d62;\n3. The final solution of the algorithm is $y = \\sum_{i \\in [k]} y_i$.\nFor each LLM call, we prompt the LLM to generate its answer without intermediate reasoning steps\u00b2. Thus\nit is reasonable to assume, based on the LLM's capability of instruction following, that the text generated\nby each LLM call has length L\ud835\udcb9\u2091\ud835\udcb8 = O(1) and can be parsed into a count number.\n3.3.2 Analysis\nLet us assume for notational convenience that m\u1d62 = m for all i \u2208 [k], and k = n/m is an integer.\nError metrics. Denote the ground-truth count for the complete string as y*, and the ground-truth count\nfor the i-th sub-string as y\u1d62*.\n\u2022 Let $\\mathcal{E}$ represent the absolute counting error, then\n$\\mathcal{E}(y) := |y - y^* | = |\\sum_{i \\in [k]} (y_i - y_i^*) | \\leq \\sum_{i \\in [k]} | y_i - y_i^*| = \\sum_{i \\in [k]} \\mathcal{E}(y_i).$\nIn other words, the overall error is upper bounded by the sum of errors of sub-tasks.\n\u2022 If we let $\\mathcal{E}$ represent the normalized counting error instead, then\n$\\mathcal{E}(y) := \\frac{|y - y^*|}{n} \\leq \\frac{1}{n} \\sum_{i \\in [k]} |y_i - y_i^*| = \\frac{1}{kn} \\sum_{i \\in [k]} \\mathcal{E}(y_i).$\nWith this metric, the overall error is upper bounded by the average of errors of sub-tasks.\nCost metrics. Our analysis of cost metrics follows Section 3.1. More specifically, we have L\ud835\udcb9\u2091\ud835\udcb8 = O(1) by\nassumption, and C(aggregation) = 0 since the final step of the algorithm is done by a non-LLM node.\n\u2022 Considering the total prefilling length and decoding length as cost metrics, one has\nC(prefilling) \u2264 k \u00d7 (Lsys + m) = $\\frac{n}{m}$ \u00d7 (Lsys + m)\nC(decoding) \u2264 k \u00d7 1 = k = $\\frac{n}{m}$,\nboth of which are monotonely decreasing in m.\n\u2022 More generally, the sum of costs of all LLM calls is\nC = C(sub-tasks) \u2264 k \u00d7 (Cpre (Lsys + m) + Cdec (Lsys + m, 1))\n= n \u00d7$\\frac{ Cpre(Lsys + m) + Cdec(Lsys + m, 1)}{m}$\nThe optimal choice of m under various conditions has been discussed in Section 3.1."}, {"title": "3.4 Example: sorting", "content": "For a more challenging example, let us consider the classical sorting problem: given a list of n numbers\nx\u2208 R, the task is to sort it in ascending order.\n3.4.1 Algorithm\nBelow is an LLM-based algorithm for sorting a list, which generalizes the naive approach of sorting the list\nwith one single LLM call:\n1. Divide the input list x into k disjoint sub-lists x\u2081,...,x\u2096 of lengths m\u2081,..., m\u2096;\n2. For each i \u2208 [k], use one LLM call to sort the i-th sub-list, which returns a solution y\u1d62;\n3. Merge the sub-lists y\u2081,..., y\u2096 into a single list y using a symbolic algorithm.\nWe note two details about this algorithm. (1) To ensure efficiency and stability, for each LLM call, we\nprompt the LLM to generate the sorted list directly, without intermediate reasoning steps. It has been\nverified empirically that the LLMs considered in our experiments can follow such instructions, and generate\ntext of length Ldec(m) = O(m) that can be easily parsed into a list. (2) Step 3 of the algorithm relies\non a classical symbolic algorithm for merging two sorted lists, which maintains two moving pointers, one\nfor each list, and chooses each entry of the merged list by comparing the values corresponding to the two\npointers. Merging multiple sorted lists can be done by merging one pair of lists at a time, in an incremental or\nhierarchical manner. Python code for these procedures can be found in Listing 1 at the end of this subsection.\nAlthough they are designed under the assumption that the input lists are sorted, they can also be applied to\ninput lists that are not fully sorted, which can possibly happen within the LLM-based algorithm since the\ninput lists in Step 3 are generated by LLM calls in Step 2.\n3.4.2 Analysis\nLet us assume for notational convenience that m\u1d62 = m for all i \u2208 [k], and k = n/m is an integer.\nError metrics. Compared to counting, there are more diverse phenomena in the sorting task in terms of\nerror metrics. In particular, multiple possible failure modes exist in sorting with an LLM-based algorithm:\n1. The output list might not be monotone;\n2. The length of the output list might be larger or smaller than that of the input list;\n3. The output list might contain numbers that do not match exactly those of the input list.\nBased on these failure modes, we define the following error metrics for sorting a list, where y denotes the\nsolution returned by the algorithm and y* denotes the ground-truth solution:\n\u2022 Exact-match error: $\\mathcal{E}$ = 0 if y matches y* exactly, and $\\mathcal{E}$ = 1 otherwise;\n\u2022 Non-monotonicity error: $\\mathcal{E} = \\sum_{i \\in [n-1]} max{y_i - y_{i+1},0}$, which is zero if and only if y is sorted;\n\u2022 Length-mismatch error: $\\mathcal{E} = \\frac{1}{n} |len(y) - len(y^*)| = \\frac{1}{n} |len(y) - n|$;\n\u2022 Fuzzy l\u2080 and fuzzy normalized l\u2081 errors: we first convert, via simple extending or truncating, the\noutput solution y to a version \u0177 that matches the length n of the input list, and then calculate\nthe fuzzy l\u2080 error as $\\mathcal{E} = ||\\hat{y} - y^*||_{\\infty} = \\max_{i \\in [n]} |y_i - y_i^*|$, or the fuzzy normalized l\u2081 error as\n$\\mathcal{E} = ||\\hat{y} - y^*||_1 = \\sum_{i \\in [n]} |y_i - y_i^*|$.\nNote that the same error metrics can be similarly defined for each parallel sub-task in Step 2 of the LLM-\nbased algorithm, and it is reasonable to expect that they become smaller as the sub-task size m decreases.\nOn the other hand, analyzing the error metrics of the overall algorithm after the final merging step can be\nmore complicated, and might be an interesting theoretical problem on its own. As an example, focusing on\nthe third failure mode and the l\u2080 error metric, we have the following guarantee, whose proof is deferred\nafter the experiments in this subsection."}, {"title": "3.5 Example: retrieval", "content": "We study a more realistic application of LLM-based algorithms, which is answering a given question that\nrequires retrieving some key information from a long piece of text. Our design of this task draws inspiration\nfrom the needle-in-a-haystack benchmark [Kam23] and other similar benchmarks that have been widely\nadopted for evaluating the long-context capability of LLMs as well as techniques of retrieval-augmented\ngeneration [WBC+15, Rou23, KBA+24, ZCH+24].\nConsider the following setting as an example. A key message (the needle) of the form \u201cThe passcode to\nthe {targeted object, e.g. red door} is {6-digit passcode}\" is randomly inserted into a piece of long text (the\nhaystack). The algorithm is asked to answer the question \"What is the passcode to the targeted object}?\"\nTo make the problem more challenging and fun, we let the haystack consist of alike sentences of the form\n\"The passcode to the {colored object, e.g. red lock or green door} is {6-digit passcode}\", with colored objects\ndifferent from the targeted object. This allows us to investigate both sides of retrieval capabilities of LLMs\nand LLM-based algorithms: retrieving the targeted information correctly, while avoiding being confused or\nmisled by background information that might seem relevant to the question [SCM+23].\nNote that while we use this concrete setting for our empirical study, the proposed algorithm and analysis\nin the following are actually applicable to generic settings of this retrieval task.\n3.5.1 Algorithm\nWe consider the following LLM-based algorithm that follows the pattern of parallel decomposition:\n1. Divide the input text of length n into k overlapping chunks of lengths m\u2081,..., m\u2096;\n2. For each chunk, use one LLM call to try to answer the question based on that chunk;\n3. Generate the final answer by majority voting.\nWe note a few details about this algorithm. (1) All lengths involved here are measured by the number of\ncharacters. (2) In the first step of chunking, we let each pair of adjacent chunks share an overlap that is\nlarger than the length of the needle, to ensure that the needle will appears as a whole in at least one chunk.\n(3) For each LLM call in the second step, the LLM is prompted to answer \"I don't know\" if it believes that\nthere is not sufficient information in the corresponding chunk, e.g. when the chunk simply does not contain\nthe needle. Such answers will be excluded from the final step of the algorithm. (4) In the final step of\nmajority voting, it is possible that there are multiple (say h) candidate solutions with the same frequency,\nin which case we let the algorithm return the list of such candidates. If this list contains the ground-truth\nsolution, we calculate the exact-match error as 1 \u2013 1/h in our experiments."}, {"title": "3.6 Example: retrieval-augmented generation", "content": "In our last example, we consider a multiple-needle generalization of the previous retrieval task, which can be\nregarded as a synthetic and simplified version of retrieval-augmented generation (RAG) [LPP+20, GXG+24].\nIn particular, we consider fine-grained sentence-level retrieval by LLMs, rather than document-level or chunk-\nlevel retrieval by certain similarity measure of dense embedding vectors; in other words, we skip the embed-\nding steps in practical RAG.\nMore concretely, we let the input text be composed of sentences of the form \"The {i}-th digit of the\npasscode to the {colored object}\" is {digit}\", where i\u2208 [6]. The algorithm is asked to answer the question\n\"What is the 6-digit passcode to the targeted object}?\" Compared with the previous single-needle retrieval\ntask, here the algorithm need to retrieve multiple needles, each for one digit of the targeted object, in order\nto answer the question correctly; moreover, the final aggregation step requires certain capability of reasoning\nor summarization over the retrieved needles.\nNote again that while we focus on this specific setting in our experiments, the algorithm and analysis in\nthe following are actually applicable to generic settings of this RAG task.\n3.6.1 Algorithm\nWe consider the following LLM-based algorithm for solving this task:\n1. Divide the input text of length n into k overlapping chunks of lengths m\u2081,..., m\u2096;\n2. For each chunk, use one LLM call to retrieve sentences that can be useful for answering the question;\n3. Put the retrieved sentences together, based on which one LLM call is invoked for answering the question.\nWe note a few details about this algorithm. (1) For each chunk in Step 2, we prompt the LLM to retrieve\nrelevant sentences, or return \"None\" if no relevant sentence is found in that chunk. Such \"None\" results will\nbe excluded from Step 3 of the algorithm. (2) Unlike previous examples, the final aggregation step of this\nalgorithm involves an LLM node, which adds to the cost metrics of the overall algorithm. (3) For simplicity,\nwe assume that the number of needles (i.e. length of the passcode) and the length of each needle are both\nO(1). For more general cases, the final aggregation step might benefit from further task decomposition.\n(4) We allow the algorithm to return a partial answer, by placing a special character in the digit(s) of the\npasscode that it is uncertain about."}, {"title": "4 Conclusion and discussion", "content": "We have introduced a formal framework for studying the"}]}]}