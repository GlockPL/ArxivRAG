{"title": "Ranking Across Different Content Types: The Robust Beauty of Multinomial Blending", "authors": ["Jan Malte Lichtenberg", "Giuseppe Di Benedetto", "Matteo Ruffini"], "abstract": "An increasing number of media streaming services have expanded their offerings to include entities of multiple content types. For instance, audio streaming services that started by offering music only, now also offer podcasts, merchandise items, and videos. Ranking items across different content types into a single slate poses a significant challenge for traditional learning-to-rank (LTR) algorithms due to differing user engagement patterns for different content types. We explore a simple method for cross-content-type ranking, called multinomial blending (MB), which can be used in conjunction with most existing LTR algorithms. We compare MB to existing baselines not only in terms of ranking quality but also from other industry-relevant perspectives such as interpretability, ease-of-use, and stability in dynamic environments with changing user behavior and ranking model retraining. Finally, we report the results of an A/B test from an Amazon Music ranking use-case.", "sections": [{"title": "INTRODUCTION", "content": "It is increasingly common for streaming media services to offer content of different types the users can engage with. For instance, news websites often show articles and videos; music streaming platforms can provide music, podcasts, videos, and merchandise. Traditionally, many of these main home pages follow a retrieve and re-ranking approach. Whereas the retrieval step can just be turned into multiple retrieval steps (e.g., one per content type), applying traditional learning-to-rank (LTR) algorithms to the multi-content-type setting comes with several challenges: 1) disjoint item feature sets across content types; 2) cold-start problem if one content type is introduced in a later stage; and 3) different engagement patterns and reward signals. For instance, users tend to listen to music tracks much more frequently and repeatedly than podcast episodes, which are typically listened to only once and less frequently due to their longer time commitment.\nThese discrepancies can lead to highly unbalanced exposure for items of specific content types. In particular, if short-term reward signals such as clicks are used to optimize the ranking policy, items from low-frequency engagement (or slow) content types (e.g., podcasts) might be disadvantaged due to their low average engagement, despite potentially being more relevant or having higher long-term value than items from high-frequency engagement (or fast) content types (e.g., music). Such reduced level of exposure can lead to even less engagement, which in turn leads the ranking algorithm to provide even less exposure to the slow items. This vicious circle maximizes short-term engagement but drowns out items from slow content types, which can ultimately lead to decreased long-term user satisfaction and negative effects on content creators from the slower content types.\nIn this work, we analyze the cross-content-type ranking problem using the example of ranking podcasts and music widgets on a single page. The diversity and fairness literature [e.g., 7, 11-13] provides various approaches that could be adapted to the cross-content-type ranking problem. However, as described below, these methods, which were often evaluated in academic settings with static data sets, are difficult to employ in dynamic, industry-scale settings. We propose a simple-yet-efficient method called multinomial blending (MB) which trades-off personalized diversification in favour of interpretability and ease-of-use in order to comply with business requirements described in Section 2. In Section 4 we describe how we use MB in practice at Amazon Music and report the results of an A/B test."}, {"title": "THE CROSS-CONTENT-TYPE (CCT) RANKING PROBLEM", "content": "Setting. We are interested in ranking a set of n items $d_j, j = 1, . . ., n$, where each item belongs to a single content type $c(d_j) \\in \\{1, ..., C\\}$, into a slate of length k. Score-based learning-to-rank (LTR) algorithms [5] learn a function $h: R^P \\rightarrow R$ that scores each item using a feature representation $x_j \\in R^P$ as input [9]. The ranking is then typically produced either by deterministically sorting the items by scores or via repeated softmax sampling [13]. We assume that the score-based LTR algorithm is frequently re-trained to tackle non-stationarity.\nRequirements. The overarching goal for the CCT ranking model is to provide a Pareto improvement in some engagement metric (for example, click-through rate). More specifically, the ranker should boost engagement with items from the slow content type while not harming overall engagement. In an industry setting, we additionally define the following product requirements for the CCT ranker:\n\u2022 controllable content-type exposure budgets through interpretable parameters;\n\u2022 compatibility with model retraining and stability in non-stationary user environments;\n\u2022 personalized ranking within content-types.\nIn our use-case, exposure budgets for different content types are product requirements that are defined at business level. Such requirements are not considered by classic LTR algorithms optimizing for a short term metric such as clicks. Therefore different approaches have been proposed with the goal of boosting exposure of specific content types.\nPolicies that optimize for diversity. To tackle the content-type diversity problem, some works have tried to optimize for diversity by changing the loss function of the LTR algorithm [7, 11-13]. Despite being formally sound, such approaches are not easy to apply in practice: (i) they lack content-type exposure guarantees; (ii) the hyper-parameters regulating the trade-off between ranking performance and diversity are difficult to interpret; and (iii) model retraining usually requires re-tuning of the diversity parameter to ensure that content-type exposure remains as desired.\nReward shaping [10]. Another option is to assign different weights to clicks from different content types. However, estimating the relative importance of reward signals across various content types is a difficult endeavour on its own, and predicting how this reward shaping will influence the actual exposure of different content types becomes even more complex, particularly in environments characterized by dynamic user behavior patterns.\nPost-processing approaches to ensuring diversity. One way of ensuring that items from each content type receive sufficient exposure is to put in place manual overrides that tie particular items from under-represented content types to certain positions. While this strategy can indeed provide exposure to slow content types, it has several down-sides: the manual overrides are slightly personalized or not personalized at all; overrides need to be curated and lifecycle-managed requiring additional labor; and they can bias propensity estimates for counterfactual off-policy estimators [2, 4, 6, 8]. More principled post-processing approaches include various re-ranking methods, which perturb the initial, personalized ranking to satisfy diversity constraints while still retaining personalization information contained in the ranking scores. For instance, the maximal marginal relevance (MMR) [3] approach to produces diverse rankings can be easily adapted to the CCT ranking problem (see Appendix). That said, the MMR approach suffers from the same limitations outlined further above for diversity-optimizing policies.\nIn the next section, we discuss a simple alternative approach that directly reasons about the exposure given to the various content types as opposed to approaches that control exposure indirectly via modifications of reward signals or loss functions."}, {"title": "MULTINOMIAL BLENDING (MB)", "content": "Multinomial blending is defined by a multinomial probability distribution M with vector of sampling probabilities $p = [P_1, P_2, ..., P_C]$, potentially defined by the business team, where C is the number of content types. Figure 1 illustrates the blending procedure. The scoring function h is used once to score all candidate items (darker color indicates higher score) and candidates are ranked by decreasing scores within each content type. To select the next action, MB first samples a content type according to $c \\sim M(p)$ and then selects the highest-scoring remaining candidate from that content type. This procedure is repeated until the slate is filled to the desired size (k = 4 in Figure 1). MB satisfies the desiderata listed above:\n\u2022 Interpretability. Each parameter $p_c$ maps to the expected content-type average exposure: on average, $p_c * 100\\%$ of the slate will be covered by items from content type c.\n\u2022 Stability. The average exposure guarantees are independent of the underlying scoring function h and therefore remain stable even after model re-training or non-stationary user behavior.\n\u2022 Within-content-type personalization. The personalized ranking of items within each content type is preserved, thereby preserving personalization quality as learned by the original scoring function.\nOne downside of MB is that content-type exposure itself is not personalized, that is, average exposure allocations of the various content types are constant across different users and prediction contexts. Developing methods that achieve full personalization while maintaining the stability and interpretability benefits of MB is an interesting direction for future work. In the Appendix, we describe a heuristic modification of MB that replaces the equality exposure guarantees by lower-bound exposure guarantees."}, {"title": "EXPERIMENTS", "content": "We A/B-tested multinomial blending on an Amazon Music CCT ranking problem where podcast and music containers were ranked into the same slate. To counter-act under-exposure as produced by the production ranker (a Deep PropDCG model [1]), the existing approach (control treatment) was to define a set of manual ranking overrides that would boost podcast exposure in a slightly personalized manner. We compared the existing approach to a) using the existing ranker with MMR diversification and b) using the existing ranker with MB diversification. For the MMR treatment, the diversity parameter was tuned offline to ensure the desired podcast exposure. During offline evaluation, operational limitations of the MMR approach became apparent: various market places required different MMR penalty parameters and optimal diversification rates would change over time. On the other hand, for the MB approach, we were able to select a single diversification rate, reducing the operational burden of having to maintain different model versions. Furthermore, we observed that restarting the model could lead to changing score distribution and thus to different diversification behavior. Table 1 shows that both treatments obtained a Pareto-improvement over the control treatment."}, {"title": "DISCUSSION AND CONCLUSION", "content": "Despite in the A/B test both MMR and MB achieved a Pareto improvement in podcast listening time and the overall engagement metric, MB was globally launched due to its operational advantages. Other than the launch metrics, MB outperformed MMR in podcast user acquisition metrics (e.g., count of users' first podcast streams), which is expected as MMR tends to surface fewer podcasts to users who are not interested in this content type, while MB ensures a certain podcast exposure at user level. As discussed above, MB is easy and intuitive to set up and update over time (e.g., if business goals change), while MMR requires counterfactual evaluation (and hence data collection) in order for its trade-off parameter to be tuned. Moreover, MB is computationally more efficient, as it does not require sequential re-computation of the scores as in MMR (see Appendix), and it allows to compute the propensity matrix in closed form, which can be used in off-policy evaluation [8] (see Appendix)."}, {"title": "A APPENDIX", "content": "A.1 MMR selection for ranking diversification.\nThe Maximal Marginal Relevance (MMR) re-ranking introduced in [3] is a widely used approach for diversification. It requires items relevance scores s(dj) = h(xj), a similarity metric $D(d_j, S_k)$ between one item dj and a list of items $S_k$, and a trade-off parameter $\\lambda \\in [0, 1]$ to balance relevance and diversity. The re-ranking occurs sequentially: the first ranked item is the one with highest relevance score, and given the first k items $S_k$ in the list, the (k + 1)-th is chosen, among the remaining items, as the arg max of the score $s'(d_j) = ds(d_j) \u2013 (1 \u2212 \\lambda)D(S_k, d_j)$. The similarity score is arbitrary, for instance the maximum of the cosine distance between the candidate item and the already ranked ones based on pre-computed embeddings. In the case presented in the paper where content-types are music and podcast, we opted for a simple similarity metric defined as the proportion of already ranked items from the candidate's content-type, namely $D(d_j, S_k) = \\frac{1}{k} \\sum_{d \\in S_k} 1[c(d) = c(d_j)]$, penalising items whose content-type has higher exposure in the partial ranking. Figure 2 provides an illustration of a CCT-ranking procedure using MMR.\nA.2 MB with \"at least p%\" exposure guarantees in the C = 2 case.\nAs described in the main text, each parameter pc of MB maps to the content-type average exposure (pc * 100% of the slate will be covered by items from content type c). Consider a situation with C = 2 content types, one slow content type cs and one fast content type cf. Assume that the slow content type would receive, say, 10% average exposure, and that this average is composed of many users that are even less exposed to items form cs (because they never engage with items from cs), whereas a few other users are exposed to cs considerably more than the average. Now assume that for business reasons, the average exposure of cs should be boosted to 20%, thus using MB and setting $p_{cs} = 0.2$. In such a situation, while the majority of users will see a boost in exposure for cs, the few users who had already seen a higher exposure of cs will see a drop in exposure to 20%. To avoid this case, one can apply the following modification to MB: for a given ranking inference, if the baseline ranker already provides at least the desired exposure to cs, the blending procedure is not triggered and the original ranking is used. This ensures a lower bound on the exposure of cs.\nA.3 MB propensity estimation\nA propensity matrix $P \\in [0, 1]^{k \\times k}$ defines the probability distribution of where in the ranking each candidate item is likely to end up. Propensity matrices are a main building block for many counterfactual LTR algorithms or off-policy evaluators [8]. Specifically, $P_{ij} = P[i, j]$ describes the probability of the i-th highest-scoring item to end up in position j of the ranking. This probability not only depends on the score of item i but also on (1) the item's content type (denoted by c(di)), (2) how many items of the same content type have already been ranked, and (3) how many are yet to be ranked.\nThe propensity matrix for MB allows the following closed form solution. If di is the k-th highest-scoring action of content type ci, then the propensity P[di, j] is the probability that k \u2013 1 actions have been ranked until position j (given by a binomial distribution) times the probability that it will be ranked in position k (given by a Bernoulli trial). Using SciPy [14] notation, this is simply given by binom(j-1, Pc\u2081).pmf(k-1) * pc\u2081, where binom(n, p).pmf(m) is the probability that there were m successes in a Binomial trial with n experiments and success probability p."}]}