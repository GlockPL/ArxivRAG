{"title": "Ranking Across Different Content Types:\nThe Robust Beauty of Multinomial Blending", "authors": ["Jan Malte Lichtenberg", "Giuseppe Di Benedetto", "Matteo Ruffini"], "abstract": "An increasing number of media streaming services have expanded\ntheir offerings to include entities of multiple content types. For\ninstance, audio streaming services that started by offering music\nonly, now also offer podcasts, merchandise items, and videos. Rank-\ning items across different content types into a single slate poses\na significant challenge for traditional learning-to-rank (LTR) al-\ngorithms due to differing user engagement patterns for different\ncontent types. We explore a simple method for cross-content-type\nranking, called multinomial blending (MB), which can be used in\nconjunction with most existing LTR algorithms. We compare MB\nto existing baselines not only in terms of ranking quality but also\nfrom other industry-relevant perspectives such as interpretability,\nease-of-use, and stability in dynamic environments with changing\nuser behavior and ranking model retraining. Finally, we report the\nresults of an A/B test from an Amazon Music ranking use-case.", "sections": [{"title": "INTRODUCTION", "content": "It is increasingly common for streaming media services to offer\ncontent of different types the users can engage with. For instance,\nnews websites often show articles and videos; music streaming\nplatforms can provide music, podcasts, videos, and merchandise.\nTraditionally, many of these main home pages follow a retrieve and\nre-ranking approach. Whereas the retrieval step can just be turned\ninto multiple retrieval steps (e.g., one per content type), applying\ntraditional learning-to-rank (LTR) algorithms to the multi-content-\ntype setting comes with several challenges: 1) disjoint item feature\nsets across content types; 2) cold-start problem if one content type\nis introduced in a later stage; and 3) different engagement patterns\nand reward signals. For instance, users tend to listen to music tracks\nmuch more frequently and repeatedly than podcast episodes, which\nare typically listened to only once and less frequently due to their\nlonger time commitment.\nThese discrepancies can lead to highly unbalanced exposure for\nitems of specific content types. In particular, if short-term reward\nsignals such as clicks are used to optimize the ranking policy, items\nfrom low-frequency engagement (or slow) content types (e.g., pod-\ncasts) might be disadvantaged due to their low average engagement,\ndespite potentially being more relevant or having higher long-term\nvalue than items from high-frequency engagement (or fast) con-\ntent types (e.g., music). Such reduced level of exposure can lead to\neven less engagement, which in turn leads the ranking algorithm\nto provide even less exposure to the slow items. This vicious circle\nmaximizes short-term engagement but drowns out items from slow\ncontent types, which can ultimately lead to decreased long-term\nuser satisfaction and negative effects on content creators from the\nslower content types.\nIn this work, we analyze the cross-content-type ranking prob-\nlem using the example of ranking podcasts and music widgets on\na single page. The diversity and fairness literature [e.g., 7, 11-13]\nprovides various approaches that could be adapted to the cross-\ncontent-type ranking problem. However, as described below, these\nmethods, which were often evaluated in academic settings with\nstatic data sets, are difficult to employ in dynamic, industry-scale\nsettings. We propose a simple-yet-efficient method called multino-\nmial blending (MB) which trades-off personalized diversification\nin favour of interpretability and ease-of-use in order to comply\nwith business requirements described in Section 2. In Section 4 we\ndescribe how we use MB in practice at Amazon Music and report\nthe results of an A/B test."}, {"title": "2 THE CROSS-CONTENT-TYPE (CCT)\nRANKING PROBLEM", "content": "Setting. We are interested in ranking a set of n items $d_j$, $j = 1, . . ., n$,\nwhere each item belongs to a single content type c($d_j$) \u2208 {1, ..., C},\ninto a slate of length k. Score-based learning-to-rank (LTR) algo-\nrithms [5] learn a function $h: \\mathbb{R}^P \\rightarrow \\mathbb{R}$ that scores each item using\na feature representation $x_j$ \u2208 $\\mathbb{R}^P$ as input [9]. The ranking is then\ntypically produced either by deterministically sorting the items\nby scores or via repeated softmax sampling [13]. We assume that\nthe score-based LTR algorithm is frequently re-trained to tackle\nnon-stationarity.\nRequirements. The overarching goal for the CCT ranking model\nis to provide a Pareto improvement in some engagement metric (for\nexample, click-through rate). More specifically, the ranker should"}, {"title": "3 MULTINOMIAL BLENDING (MB)", "content": "Multinomial blending is defined by a multinomial probability distri-\nbution M with vector of sampling probabilities $\\textbf{p} = [P_1, P_2, ..., P_C]$,\npotentially defined by the business team, where C is the number of"}, {"title": "4 EXPERIMENTS", "content": "We A/B-tested multinomial blending on an Amazon Music CCT\nranking problem where podcast and music containers were ranked\ninto the same slate. To counter-act under-exposure as produced by\nthe production ranker (a Deep PropDCG model [1]), the existing\napproach (control treatment) was to define a set of manual ranking\noverrides that would boost podcast exposure in a slightly personal-\nized manner. We compared the existing approach to a) using the\nexisting ranker with MMR diversification and b) using the exist-\ning ranker with MB diversification. For the MMR treatment, the\ndiversity parameter was tuned offline to ensure the desired podcast\nexposure. During offline evaluation, operational limitations of the\nMMR approach became apparent: various market places required\ndifferent MMR penalty parameters and optimal diversification rates\nwould change over time. On the other hand, for the MB approach,\nwe were able to select a single diversification rate, reducing the\noperational burden of having to maintain different model versions.\nFurthermore, we observed that restarting the model could lead to\nchanging score distribution and thus to different diversification\nbehavior. Table 1 shows that both treatments obtained a Pareto-\nimprovement over the control treatment."}, {"title": "5 DISCUSSION AND CONCLUSION", "content": "Despite in the A/B test both MMR and MB achieved a Pareto im-\nprovement in podcast listening time and the overall engagement\nmetric, MB was globally launched due to its operational advantages.\nOther than the launch metrics, MB outperformed MMR in podcast\nuser acquisition metrics (e.g., count of users' first podcast streams),\nwhich is expected as MMR tends to surface fewer podcasts to users\nwho are not interested in this content type, while MB ensures a\ncertain podcast exposure at user level. As discussed above, MB is\neasy and intuitive to set up and update over time (e.g., if business\ngoals change), while MMR requires counterfactual evaluation (and\nhence data collection) in order for its trade-off parameter to be\ntuned. Moreover, MB is computationally more efficient, as it does\nnot require sequential re-computation of the scores as in MMR\n(see Appendix), and it allows to compute the propensity matrix in\nclosed form, which can be used in off-policy evaluation [8] (see\nAppendix)."}, {"title": "A APPENDIX", "content": ""}, {"title": "A.1 MMR selection for ranking diversification.", "content": "The Maximal Marginal Relevance (MMR) re-ranking introduced\nin [3] is a widely used approach for diversification. It requires items\nrelevance scores s($d_j$) = h($x_j$), a similarity metric D($d_j$, $S_k$) be-\ntween one item $d_j$ and a list of items $S_k$, and a trade-off parameter\n\u03bb \u2208 [0, 1] to balance relevance and diversity. The re-ranking occurs\nsequentially: the first ranked item is the one with highest relevance\nscore, and given the first k items $S_k$ in the list, the (k + 1)-th is\nchosen, among the remaining items, as the arg max of the score\ns' ($d_j$) = $ds(d_j) \u2013 (1 \u2212 1)D(S_k, d_j)$. The similarity score is arbitrary,\nfor instance the maximum of the cosine distance between the can-\ndidate item and the already ranked ones based on pre-computed\nembeddings. In the case presented in the paper where content-types\nare music and podcast, we opted for a simple similarity metric de-\nfined as the proportion of already ranked items from the candidate's\ncontent-type, namely D($d_j$, $S_k$) = $\\frac{1}{\\vert{S_k}\\vert}$ $\\sum_{d\u2208S_k} 1[c(d) = c(d_j)]$, penal-\nising items whose content-type has higher exposure in the partial\nranking. Figure 2 provides an illustration of a CCT-ranking proce-\ndure using MMR."}, {"title": "A.2 MB with \"at least p%\" exposure guarantees\nin the C = 2 case.", "content": "As described in the main text, each parameter $p_c$ of MB maps to\nthe content-type average exposure ($p_c * 100$% of the slate will be\ncovered by items from content type c). Consider a situation with\nC = 2 content types, one slow content type $c_s$ and one fast content\ntype $c_f$. Assume that the slow content type would receive, say, 10%\naverage exposure, and that this average is composed of many users\nthat are even less exposed to items form $c_s$ (because they never\nengage with items from $c_s$), whereas a few other users are exposed\nto $c_s$ considerably more than the average. Now assume that for\nbusiness reasons, the average exposure of $c_s$ should be boosted\nto 20%, thus using MB and setting $p_{c_s}$ = 0.2. In such a situation,\nwhile the majority of users will see a boost in exposure for $c_s$, the\nfew users who had already seen a higher exposure of $c_s$ will see\na drop in exposure to 20%. To avoid this case, one can apply the\nfollowing modification to MB: for a given ranking inference, if the\nbaseline ranker already provides at least the desired exposure to $c_s$,\nthe blending procedure is not triggered and the original ranking is\nused. This ensures a lower bound on the exposure of $c_s$."}, {"title": "A.3 MB propensity estimation", "content": "A propensity matrix $P$ \u2208 [0, 1]$^{k\u00d7k}$ defines the probability distribu-\ntion of where in the ranking each candidate item is likely to end\nup. Propensity matrices are a main building block for many coun-\nterfactual LTR algorithms or off-policy evaluators [8]. Specifically,\n$P_{ij}$ = P[i, j] describes the probability of the i-th highest-scoring\nitem to end up in position j of the ranking. This probability not only\ndepends on the score of item i but also on (1) the item's content\ntype (denoted by c(di)), (2) how many items of the same content\ntype have already been ranked, and (3) how many are yet to be\nranked.\nThe propensity matrix for MB allows the following closed form\nsolution. If di is the k-th highest-scoring action of content type $c_i$,\nthen the propensity P[di, j] is the probability that k \u2013 1 actions\nhave been ranked until position j (given by a binomial distribution)\ntimes the probability that it will be ranked in position k (given by a\nBernoulli trial). Using SciPy [14] notation, this is simply given by\nbinom(j-1, $P_{c_\u2081}$).pmf(k-1) * $P_{c_\u2081}$, where binom(n, p).pmf(m)\nis the probability that there were m successes in a Binomial trial\nwith n experiments and success probability p."}]}