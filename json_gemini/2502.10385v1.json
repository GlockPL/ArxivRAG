{"title": "Simplifying DINO via Coding Rate Regularization", "authors": ["Ziyang Wu", "Jingyuan Zhang", "Druv Pai", "Xudong Wang", "Chandan Singh", "Jianwei Yang", "Jianfeng Gao", "Yi Ma"], "abstract": "DINO and DINOv2 are two model families being widely used to learn representations from unlabeled imagery data at large scales. Their learned representations often enable state-of-the-art performance for downstream tasks, such as image classification and segmentation. However, they employ many empirically motivated design choices and their training pipelines are highly complex and unstable many hyperparameters need to be carefully tuned to ensure that the representations do not collapse which poses considerable difficulty to improving them or adapting them to new domains. In this work, we posit that we can remove most such-motivated idiosyncrasies in the pre-training pipelines, and only need to add an explicit coding rate term in the loss function to avoid collapse of the representations. As a result, we obtain highly simplified variants of the DINO and DINOv2 which we call SimDINO and SimDINOv2, respectively. Remarkably, these simplified models are more robust to different design choices, such as network architecture and hyperparameters, and they learn even higher-quality representations, measured by performance on downstream tasks, offering a Pareto improvement over the corresponding DINO and DINOv2 models. This work highlights the potential of using simplifying design principles to improve the empirical practice of deep learning.", "sections": [{"title": "1. Introduction", "content": "Self-supervised learning (SSL) is the toolkit of choice to learn representations for large datasets of unlabeled images (Hadsell et al., 2006; Oord et al., 2018; Wu et al., 2018; Grill et al., 2020; He et al., 2020; Bardes et al., 2021; Chen & He, 2021; Caron et al., 2021; Zhou et al., 2021; He et al., 2022; Assran et al., 2023; Oquab et al., 2023), captioned images (Radford et al., 2021), videos (Feichtenhofer et al., 2022), and text (Radford et al., 2018; Devlin, 2018; Radford et al., 2019; Brown et al., 2020), among other modalities. In the context of image SSL, there are two main approaches: reconstructive (He et al., 2022), where the goal is to reconstruct some function of the true image data from a \"view\", i.e., corruption or augmentation, and contrastive (Hadsell et al., 2006), where the goal is, for each image, to have the features of different views of the image all be close, and features of views of different images be far.\nWithin contrastive SSL, a key challenge lies in preventing representation collapse, where models learn trivial solutions that map all inputs to the same output. One common approach to address this is through the use of negative samples, which explicitly encourages representations of different images to be dissimilar. Thus far, the success of using negative samples depends on having a large batch size (Wu et al., 2018; He et al., 2020), which poses computational challenges at scale. Methods which attempt to avoid this bottleneck by using negative samples in more implicit and indirect ways to avoid collapse (Caron et al., 2021) can cope with smaller batch sizes, but often require training pipelines with many components and hyperparameters carefully tuned to avoid collapse, making them difficult to train.\nThe state-of-the-art for image SSL is generally considered to be the DINOv2 model family (Oquab et al., 2023). It is built on the DINO model family (Caron et al., 2021). Both classes of models are trained using contrastive SSL and thus run into the representation collapse issue. While DINOv2 explicitly and directly uses negative samples to avoid collapse, it inherits much of its training pipeline from DINO, which uses negative samples more indirectly. As such, both model families' training pipelines are highly complex and unstable, requiring many tweaks and careful hyperparameter selection in order for the training to converge for a given architecture. Despite this capriciousness, the trained models' representations are highly useful for downstream tasks, and are widely used (Baharoon et al., 2023; Wei et al., 2024).\nOur contributions. In this work, we remove many tweaks and hyperparameters from the DINO and DINOv2 training pipelines, replacing them with a term in the objective which explicitly uses negative samples. We show empirically that"}, {"title": "2. Methods: Simplifying DINO and DINOv2", "content": "2.1. Recap of the Original DINO Pipeline\nThe goal of DINO is to learn an aggregate representation of the input image which contains information about large-scale semantics of the input (e.g., the locations and properties of different objects in the image). They do this via a pre-training pipeline (Caron et al., 2021) which is depicted in Figure 1(a), and we also describe it throughout this section. The main idea is to take multiple views (i.e., different crops) of the data, and ensure that the features generated by these views are consistent with each other (in a sense which will be made precise shortly) as much as possible. If the views each contain a salient part of the input such as a central object, the feature corresponding to any view would then contain information about this central object. The end goal is that the feature of any large-enough view contains information about all relevant objects in the input image, which can then be extracted for use in downstream tasks such as image classification or image segmentation.\nIn the rest of the section, we will discuss the pre-training pipeline. As is common in contrastive SSL, the DINO framework uses two networks: a so-called teacher network parameterized by $\\theta_t \\in \\Theta$, and a so-called student network parameterized by $\\theta_s \\in \\Theta$. During pre-training, the loss will encourage the student's representation to align with the teacher's representation, even as the teacher is simultaneously updated using student weights; this is self-distillation, and can be viewed as an optimization strategy or even im-"}, {"title": "2.2. From DINO to SimDINO", "content": "To go from DINO to SimDINO, we ask the question:\nCan we directly compare $z_{cls}^s$ and $z_{cls}^g$?\nIf we could do this, then we could avoid the large DINO head, the centering operation, the softmaxes, and the cross-entropy based loss. However, the mechanism in DINO for avoiding representation collapse via negative samples would therefore be removed. Thus, we have a second question:\nCan we efficiently use the negative samples'\nfeatures explicitly to enforce non-collapse?\nFor the first question, we argue that the most simple squared Euclidean distance, namely\n$d_{e^{2}}(x, y) := \\frac{1}{2}||x - y||^{2}$\nworks at least as well as the cross-entropy-based functional (5) applied to an affine transformation of the features, as in (4). For the second question, we argue that we may directly penalize the covariance of the features in order to avoid collapse, as follows. For a hyperparameter $\\varepsilon > 0$, the (total) coding rate (Ma et al., 2007; Yu et al., 2020; Li et al., 2022) of a symmetric positive semidefinite matrix $\\Gamma \\in \\mathbb{R}^{d \\times d}$ is\n$R_{\\varepsilon}(\\Gamma) := \\frac{1}{2} \\logdet(I + \\frac{\\Gamma}{\\varepsilon^{2}})$\nIn words, $R_{\\varepsilon}$ is an approximation to the rate distortion of a Gaussian random variable with covariance $\\Gamma$ (and this approximation is perfect in the limit $\\varepsilon \\rightarrow 0$). More concretely, it is a measure of size of the covariance, even if the underlying variables are non-Gaussian. Thus one way to ensure non-collapse is to add $-R_{\\varepsilon}(Cov[z_{cls}(0_s)])$ as a regularizer in the objective, leading to the loss\n$\\mathcal{L}_{SimDINO}(0_s, 0_t) := \\mathbb{E}[d_{e^{2}}(z_{cls}(0_s), z_{cls}^g(0_t))]$\n$- \\gamma R_{\\varepsilon}(Cov[z_{cls}(0_s)])$,\nwhere $\\gamma > 0$ is a hyperparameter. Note that $d_{e^{2}}(z_{cls}, z_{cls}^g) = -z_{cls}^Tz_{cls}^g$ since $z_{cls}, z_{cls}^g \\in \\mathbb{S}^{d-1}$.\nWhen training, similar to DINO, we estimate the expectation and covariance in (9) by a type of plug-in estimator. Namely, the expectation is estimated similar to DINO, just using $d_{e^{2}}$ instead of $d_{CE}$. To estimate the coding rate, we sub-sample several $z_{cls}(0_s)$ over both $\\mathcal{X}$ and $v_c$, estimate $Cov[z_{cls}(0_s)]$ on that sub-sample via plug-in, estimate $R_{\\varepsilon}$ of the population covariance by calculating it on the sample covariance, then average the estimates over all sub-samples.\nWe conjecture that the latter estimator has lower variance compared to the naive plug-in estimator for $Cov[z_{cls}(0_s)]$ as it is similar to variance-reduction methods in statistics (Kahn & Marshall, 1953), which we hypothesize might be a factor as to why SimDINO can handle a smaller batch size than other contrastive SSL methods that explicitly use negative samples but avoid collapse using higher-variance or more implicit regularizers.\nThe overall pipeline is shown in Figure 1(b). Note that it is much simpler than DINO. We provide pseudocode for the training pipeline in Algorithm 1 in Appendix D."}, {"title": "2.3. From DINOv2 to SimDINOv2", "content": "The pipeline of the DINOv2 framework (Oquab et al., 2023), as shown in Figure 1(c), is built upon the DINO pipeline, and has two main goals: first, learn an aggregate representation which contains large-scale semantics of the input (i.e., the goal of DINO); second, learn patch-based representations which have fine-grained semantic information about each patch and its local neighborhood. The main new ideas to achieve this, drawn from the iBOT pipeline (Zhou et al., 2021), are that the input to the student has some masked patches, and that the loss also computes similarity of the patch-based features. To see why this works, consider if some patches are masked, and the model is able to predict masked patches using their unmasked neighbors, then from each patch the model can extract strong information about the semantics of nearby patches, which is an idea similar in spirit to masked autoencoding (He et al., 2022). Thus, these two ideas from iBOT would furnish our model with informative patch-based representations.\nWe now discuss the DINOv2 pipeline, before discussing our modifications. Formally, starting with tokenized images $\\mathcal{X} \\in \\mathbb{R}^{D \\times N}$, we take a view $v_m$ sampled at random; the\nIn practice, we only let $v_c$ be a global view for efficiency, and offer the following heuristic justification. If the expected similarity term in (9) is large, then there is little difference between the features of local and global views. Hence $Cov[z_{cls}(0_s)] \\approx Cov[z_{cls}^g(0_s)]$, where $v_{g}$ is a randomly sampled global view."}, {"title": "3. Experimental Verification", "content": "In this section, we empirically investigate and evaluate our proposed SimDINO and SimDINOv2 models and compare them to the original DINO and DINOv2 model families. In particular, we examine their differences in training dynamics and learned representation both quantitatively and qualitatively. Overall, our experiments show that our proposed SimDINO model families can achieve better performance and learn representations of higher quality than the original DINO families while being significantly simpler and more robust to variations in hyperparameters and architecture."}, {"title": "3.1. Experimental Setup", "content": "Model architecture. Since our method is directly built upon DINO and DINOv2, we adopt settings as close as pos-"}, {"title": "4. Related Work", "content": "In this section, we identify several previous works which the SimDINO and SimDINOv2 methodologies are similar to or build on. We have already discussed similarities to DINO and DINOv2 in depth so we omit this comparison.\nSiamese contrastive SSL. Siamese contrastive learning, archetyped by SimCLR (Chen et al., 2020) and SimSiam (Chen & He, 2021) among others, uses the same network to encode different augmentations (i.e., views) of the same input, and pushes the features of these augmentations together, similar to SimDINO. SimCLR uses explicit negative samples in the loss, while SimSiam manipulates the loss gradient structure using stop-gradients to avoid collapse. Both methods' losses measure alignment or difference via the squared Euclidean distance (equivalently the dot product) of the features. In contrast, SimDINO uses two separate networks - the teacher and student that update via self-distillation. Furthermore, SimDINO uses the inner product of features in the loss, but it also uses a coding rate regularizer instead of implicitly contrasting negative samples or using the more bespoke contrastive loss in SimCLR.\nExplicit covariance regularization in SSL. There have also been works that use explicit penalization of the first- and second-order statistics of the features, such as VICReg (Bardes et al., 2021). VICReg uses completely separate networks to encode two augmentations of the same input batch, and then explicitly penalizes the alignment of those features (via Euclidean distance) as well as the features' variance and covariance within the batch, aiming to whiten the features as much as possible. In spirit, this is similar to SimDINO, which also penalizes the alignment and the features' covariance, albeit using a different covariance regularizer and not penalizing the features' variance. Also, SimDINO uses self-distillation to train the teacher network, while VICReg uses two separate networks.\nSelf-distillation in SSL. Several works such as MoCo (He et al., 2020) and BYOL (Grill et al., 2020) train two networks, a teacher and a student, via self-distillation by setting the teacher weights to be an exponential moving average of the student weights. While MoCo uses explicit negative samples from previous batches in its InfoNCE loss computed on a given batch, BYOL does not use negative"}, {"title": "5. Conclusion", "content": "In this work, we identify that the reasons for many empirically motivated design choices in the original DINO and DINOv2 are to avoid collapse of the learned representation. We show that these complicated design choices can be significantly reduced or simplified by adding a coding-rate-related regularization term. The resulting simplified models, called SimDINO and SimDINOv2, are even better in terms of performance for downstream tasks, and their pretraining pipelines are much more robust to different settings and hyperparameters, offering a Pareto improvement against the DINO and DINOv2 model families. Our work demonstrates the value of simplifying deep learning pipelines as well as making tradeoffs as explicit as possible when designing high-performance vision SSL models.\nIn light of these overarching contributions, there are several possible opportunities for future work. On the theoretical side, our simplified framework provides an entry point for studying the geometric properties of the global optima of self-supervised learning losses. Further study in Appendix F.4 shows that in the framework of the paper, it is possible to set up a self-supervised objective that does not require self-distillation to optimize, making a theoretical analysis much easier, while the resulting model is still quite powerful and practically usable. On the empirical side, one can apply the paradigm of making implicit design choices more explicitly present in the loss to more self-supervised learning frameworks, making existing pipelines more stable and the resulting models of better performance."}, {"title": "A. Formal Description of Local and Global Views", "content": "Each local view, say $v_c$ acts as follows, given an input image $\\mathcal{X}$ of shape $(C, H, W)$. First, for a hyperparameter $p_{loc} \\in [0, 1]$, it crops a rectangular component from $\\mathcal{X}$ of shape $(C, H_c, W_c)$, where $H_c$ and $W_c$ are chosen such that $H_cW_c = p_{loc}HW$, i.e., the crop is a fraction $p_{loc}$ of the whole image. Then the component is resized to shape $(C, S_{loc}, S_{loc})$, where $S_{loc}$ is a hyperparameter, and then divided into $N_{loc} := S_{loc}^2/P^2$ square patches of shape $(C, P, P)$, where the patch size $P$ is a hyperparameter. Each patch is unrolled into a vector of length $D := CP^2$, and the $N_{loc}$ unrolled vectors are placed in raster order to get the output $X_c \\in \\mathbb{R}^{N_{loc} \\times D}$. Each global view $v_g$ acts the same as a local view, except that the corresponding hyperparameters $p_{glo}, S_{glo}$ are larger than their local counterparts $p_{loc}, S_{loc}$ (hence also $N_{glo}$ vs. $N_{loc}$), while the patch size $P$ (hence dimension $D$) remains the same.\nWe use these local and global views for training. For evaluation or inference, we do a similar procedure: given $\\mathcal{X}$ of shape $(C, H, W)$, we resize $\\mathcal{X}$ proportionally so that its shorter edge is length $L_{eval}$, then take a square crop from the center of shape $(C, S_{eval}, S_{eval})$. This sequence is divided into $N_{eval} := S_{eval}^2/P^2$ square patches of length $(C, P, P)$; each patch is unrolled into a vector of length $D := CP^2$, and the $N_{eval}$ unrolled vectors are placed in raster order to get the output $X_c \\in \\mathbb{R}^{N_{eval} \\times D}$"}, {"title": "B. Complex Interactions in DINO and Their Removal", "content": "We wish to showcase a finer point about why the DINO pipeline is so unstable. Notice that\n$\\begin{aligned}\nCE(p, q) &= - \\sum_{i=1}^{m} p_i \\log q_i\\\\\n&= - \\sum_{i=1}^{m} p_i \\log (p_i/q_i) - \\sum_{i=1}^{m} p_i \\log p_i\\\\\n&= KL(p, q) + H(p)\n\\end{aligned}$\nwhere KL is the KL divergence, and H is the entropy of a probability distribution. Therefore,\n$d_{CE}(p, q) = \\frac{KL(p, q) + KL(q, p)}{2} + \\frac{1}{2}(H(p) + H(q))$.\nThe first term $d_{JS}(p, q)$ is the Jensen-Shannon divergence, which encourages $p \\approx q$. The second term encourages the entropy of p and q to be low, namely closer to one-hot vectors.\nNow consider the DINO objective:\n$\\mathcal{L}_{DINO}(\\theta_s, \\theta_t, \\eta^{DINO}_s, \\eta^{DINO}_t, \\mu) \\\\\n= \\mathbb{E}[d_{CE}(p_{cls}^{s}(0_s, \\eta^{DINO}_s), p_{cls}^{g}(0_t, \\eta^{DINO}_t, \\mu))]\\\\\n= \\mathbb{E}[d_{JS}(p_{cls}^{s}(0_s, \\eta^{DINO}_s), p_{cls}^{g}(0_t, \\eta^{DINO}_t, \\mu)) + \\frac{H(p_{cls}^{s}(0_s, \\eta^{DINO}_s) + H(p_{cls}^{g}(0_t, \\eta^{DINO}_t, \\mu))}{2}].$\nSuppose that, for example, $h_{\\eta_s^{DINO}}$ and $h_{\\eta_t^{DINO}}$ had ranges as a multiple of the all-ones vector, and $\\mu$ were a constant multiple of the ones vector. Then the first term in the loss would be minimized, but the second term would become as large as possible (since both $p_{cls}$ would be just $\\frac{1}{m}1_m$, i.e., probability vectors corresponding to the uniform distribution), so this would not be the optimal solution in general. This implies that the learned $h_{\\eta_s^{DINO}}$ and $h_{\\eta_t^{DINO}}$ in general would not both be degenerate. This enables the tradeoff between the EMA parameter $\\lambda$ and the temperature parameter $\\tau$ which enables non-collapse. If the objective just involved the JS divergence and not the entropy term, or else had $h_{\\eta_s^{DINO}}$ be degenerate (manually set and frozen, for instance), or else didn't have a carefully set tradeoff between $\\lambda$ and $\\tau$, then the model would collapse. However, SimDINO removes all of this complexity and replaces it with an explicit coding-rate-type term."}, {"title": "C. Theory for Hyperparameter Scaling", "content": "Let d, n be positive integers. Our main theorem is the following.\nTheorem C.1 (Scale of $\\nabla R_\\varepsilon$). We have\n$\\underset{\\begin{subarray}{c}\nZ \\in \\mathbb{R}^{d \\times n}\\\\\n||Z_i||_2=1\\quad \\forall i\n\\end{subarray}}{\\text{max}} ||\\nabla_Z R_\\varepsilon(\\frac{Z Z^T}{n})||_F < \\frac{\\sqrt{d \\min \\{d,n\\}/n}}{4 \\varepsilon}$ \nProof. Let $\\alpha := d/(n\\varepsilon^2)$ and let $f: \\mathbb{R}^{d \\times n} \\rightarrow \\mathbb{R}$ be defined by\n$f(Z) := \\logdet(I + \\alpha Z Z^T)$,\ni.e., $f(Z) = 2 R_\\varepsilon(\\frac{Z Z^T}{n})$. Now, let $r := \\min \\{d, n\\}$. For any matrix $M$, let $\\sigma_i(M)$ be its $i$th largest singular value, for\n$i = 1, ..., d$. First, note that since $||Z_i||_2=1$ for all $i$, it holds\n$\\sum_{i=1}^{r} \\sigma_i(Z)^2 = \\sum_{i=1}^{r} \\sigma_i(Z)^2 = \\sum_{i=1}^{d} \\sigma_i(Z Z^T) = tr(Z Z^T) = \\sum_{i=1}^{d} (Z Z^T)_{ii} = \\sum_{i=1}^{d} ||Z_i||_2^2 = d$.\nNow, we can simplify the gradient. It holds\n$\\nabla f(Z) = \\alpha (I + \\alpha Z Z^T)^{-1} Z$.\nThus, it holds that\n$||\\nabla f(Z)||_F = \\sqrt{tr([\\nabla f(Z)][\\nabla f(Z)])}$=$ \\alpha^2 \\sqrt{tr(Z^T (I + \\alpha Z Z^T)^{-2} Z)}$.\nUsing that the trace is the sum of singular values, it holds by taking the SVD of $Z$ that\n$\\sqrt{tr(Z^T (I + \\alpha Z Z^T)^{-2} Z)} = \\sum_{i=1}^{r} \\frac{\\sigma_i(Z)^2}{[1 + \\alpha \\sigma_i(Z)^2]^2}.$\nIn this case we directly optimize over the singular values, obtaining the problem\n$\\begin{aligned}\n\\underset{\\begin{subarray}{c}\nZ \\in \\mathbb{R}^{d \\times n}\\\\\n||Z_i||_2=1\\quad \\forall i\n\\end{subarray}}{\\text{max}} ||\\nabla f(Z)||_F &< \\alpha^2 \\underset{\\begin{subarray}{c}\nx_i \\in \\mathbb{R}\\\\\nx_i > 0 \\\\ \\forall i \\\\\n\\sum_{i=1}^{r} x_i = d\n\\end{subarray}}{\\text{max}} \\sum_{i=1}^{r} \\frac{x_i}{[1 + \\alpha x_i]^2} .\n\\end{aligned}$\nThe function $t \\rightarrow \\frac{t}{(1 + \\alpha t)^2}$ on $[0,\\infty)$ has a global maximum at $t = \\frac{1}{\\alpha}$, and the value is $\\frac{1}{4 \\alpha}$. Therefore it follows that\n$\\begin{aligned}\n\\underset{\\begin{subarray}{c}\nx_i \\in \\mathbb{R}\\\\\nx_i > 0 \\\\ \\forall i \\\\\n\\sum_{i=1}^{r} x_i = d\n\\end{subarray}}{\\text{max}} \\sum_{i=1}^{r} \\frac{x_i}{[1 + \\alpha x_i]^2} &< \\frac{r}{\\alpha^2} \\sum_{i=1}^{r} \\frac{x_i}{[1 + \\alpha x_i]^2}  = \\frac{r}{4 \\alpha}.\n\\end{aligned}$\nUnpacking this notation, we obtain\n$||f(Z)||_F < \\alpha^2 \\frac{r}{4 \\alpha} = \\alpha \\frac{d min \\{d,n\\}}{4 n \\varepsilon^2}$.\nTaking square roots, it holds\n$||\\nabla f(Z)||_F < \\sqrt{\\frac{d min \\{d,n\\}}{n}}$.\nTherefore,\n$\\begin{aligned}\n||\\nabla_Z R_\\varepsilon(\\frac{Z Z^T}{n})||_F &< \\frac{1}{2} ||\\nabla f(Z)||_F  \\le  \\frac{\\sqrt{d \\min \\{d,n\\}/n}}{4 \\varepsilon}\n\\end{aligned}$ as desired."}, {"title": "D. Training Pipeline Pseudocode", "content": "In this section we provide pseudocode for the training pipelines of SimDINO and SimDINOv2.\nAlgorithm 1 SimDINO training pipeline.\n#fs, ft: student and teacher networks, this time outputting ONLY the cls token feature\n# eps: coding rate regularization quantization hyperparameter\n# gamma: coding rate regularization strength hyperparameter\n# lam: teacher network EMA rate\nft.params = fs.params\nfor x in loader: # load a minibatch x of B samples\nxg, x1 = global_views(x), local_views(x) # (B, M_glo, D, N_glo), (B, M_loc, D, N_loc)\nzsg, zsl = fs(xg), fs(xl) # student output (B, M_glo, d), (B, M_loc, d)\nztg = ft (xg) # teacher output (B, M_glo, d)\nZS = cat ([zsg, zsl), dim=1) # (B, M, d) where M = M_loc + M_glo\nsq_dists = sum((zs.view(B,M,1,d) - ztg.view(B,1,M_glo,d)) ** 2, dim=3) # (B, M, M_glo)\nzsg_bdim = zsg.transpose(0, 1) # (M_glo, B, d)\nCovs = zsg_bdim.transpose(-2, -1) @ zsg_bdim / B # (M_glo, d, d)\nR_eps = batch_logdet (I_d.unsqueeze (0) + d/(eps**2) * Covs) # (M_glo)\nloss = mean (sq_dists) + gamma * mean (R_eps)\nloss.backward() # back-propagate\n# student and teacher updates\nupdate (fs) # SGD or Adam\nft.params = lam * ft.params + (1 - lam) * fs.params\nAlgorithm 2 SimDINOv2 training pipeline.\n#fs, ft: student and teacher networks, this time outputting BOTH the cls token feature\nand patch token features\n# eps: coding rate regularization quantization hyperparameter\n# gamma: coding rate regularization strength hyperparameter\n# lam: teacher network EMA rate\n# alpha: proportion of patches that get masked\nft.params = fs.params\nfor x in loader: # load a minibatch x of B samples\nm = generate_mask (x, alpha) # boolean mask (B, N)\nxg, x1 = global_views(x), local_views(x) # (B, M_glo, D, N_glo), (B, M_loc, D, N_loc)\nxmg, xml = apply_mask (xg, m), apply_mask (xl, m) # (B, M_glo, N_glo), (B, M_loc, N_loc)\nzsg, Zsg = fs(xmg) # student on masked global views (B, M_glo, d), (B, M_glo, N, d)\nzsl, Zsl = fs(xml) # student on masked local views (B, M_loc, d), (B, M_loc, N, d)\nztg, Ztg = ft (xg) # teacher output on global views (B, M_glo, d), (B, M_glo, N, d)\nZS = cat ([zsg, zsl), dim=1) # (B, M, d), M = M_loc + M_glo\nZs = cat([Zsg, Zsl], dim=1) # (B, M, N, d)\nsq_dists = sum((zs.view(B,M,1,d) - ztg.view(B,1,M_glo,d)) ** 2, dim=3) # (B, M, M_glo)\npsq_dists = mean(\nsum((Zs.view(B,M,1,N,d) - Ztg.view(B,1,M_glo,N,d)) ** 2, dim=4) * m.view(B,1,1,N),\ndim=3\n) # (B, M, M_glo)\nzsg_bdim = zsg.transpose(0, 1) # (M_glo, B, d)\nCovs = zsg_bdim.transpose(-2, -1) @ zsg_bdim / B # (M_glo, d, d)\nR_eps = batch_logdet (I_d.unsqueeze (0) + d/(eps**2) * Covs) # (M_glo)\nloss = (mean (sq_dists) + mean (psq_dists))/2 - gamma * mean (R_eps)\nloss.backward() # back-propagate\n# student and teacher updates\nupdate (fs) # SGD or Adam\nft.params = lam * ft.params + (1 - lam) * fs.params"}, {"title": "E. Implementation Details", "content": "The training codes and hyperparameters for SimDINO and SimDINOv2 are derived from the released official settings in DINO and DINOv2 separately, see Table 4 for detailed comparison. Notes that for SimDINOv2, we choose to use bfloat16 dtype in student backbone parameters and reductions for better numerical stability while other modules uses the same FSDP mixed precision settings from DINOv2."}, {"title": "F. Additional Experiments", "content": "F.1. Ablations on Stability of DINO Training\nIn Table 5, we study the optimization behavior and stability of DINO by varying hyperparameters that are specific to its pipeline. Specifically, we select teacher momentum, whether to apply normalization for the last layer, and teacher temperature. We vary each of them and study their impact on DINO training. As shown in Table 5, moderate adjustments for each component leads to divergence (during early training stages). These results suggest DINO training can be highly"}]}