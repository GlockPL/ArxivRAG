{"title": "EXPLAINABLE ARTIFICIAL INTELLIGENCE (\u03a7\u0391\u0399): FROM INHERENT EXPLAINABILITY TO LARGE LANGUAGE MODELS", "authors": ["Fuseini Mumuni", "Alhassan Mumuni"], "abstract": "Artificial Intelligence (AI) has continued to achieve tremendous success in recent times. However, the decision logic of these frameworks is often not transparent, making it difficult for stakeholders to understand, interpret or explain their behavior. This limitation hinders trust in machine learning systems and causes a general reluctance towards their adoption in practical applications, particularly in mission-critical domains like healthcare and autonomous driving. Explainable AI (XAI) techniques facilitate the explainability or interpretability of machine learning models, enabling users to discern the basis of the decision and possibly avert undesirable behavior. This comprehensive survey details the advancements of explainable AI methods, from inherently interpretable models to modern approaches for achieving interpretability of various black box models, including large language models (LLMs). Additionally, we review explainable AI techniques that leverage LLM and vision-language model (VLM) frameworks to automate or improve the explainability of other machine learning models. The use of LLM and VLM as interpretability methods particularly enables high-level, semantically meaningful explanations of model decisions and behavior. Throughout the paper, we highlight the scientific principles, strengths and weaknesses of state-of-the-art methods and outline different areas of improvement. Where appropriate, we also present qualitative and quantitative comparison results of various methods to show how they compare. Finally, we discuss the key challenges of XAI and directions for future research.", "sections": [{"title": "INTRODUCTION", "content": null}, {"title": "1.1 Machine learning and explainable AI", "content": "Artificial Intelligence (AI) has received significant attention in recent times due to its immense power and remarkable potential to solve seemingly intractable problems across various real-world domains. Machine learning enables AI systems to extract relevant relationships from data through a training process, and then use this knowledge to make accurate inference on new instances during operation. Several machine learning methods have been developed and used extensively in diverse practical applications. In some of these applications (e.g., [1, 2, 3, 4]) the accuracy of machine learning models has reached parity with or even surpassed human experts. Deep learning methods have generally attained far higher accuracies than other machine learning models. Unfortunately, this performance gain comes at a cost of excessive data demand as well as overall model complexity that obfuscates the decision logic of the machine learning system. Thus, most data-driven deep learning methods are characteristically black box in nature, where inputs are mapped directly to predictions without any accompanying information showing how the decisions are arrived at. However, for high-stake applications in domains like healthcare, finance, and robotics, it is necessary for stakeholders to understand the underlying rationale behind the decisions of artificial intelligence systems. For this reason, there has been an increasing need to use transparent or interpretable models whose predictions can be understood by developers and other relevant stakeholders such as regulators, managers, and end users. In cases where models are not already interpretable, it is often necessary to devise methods to explain their behavior or predictions. Explainable artificial intelligence (XAI) aims at achieving this goal."}, {"title": "1.2 Interpretability or explainability?", "content": "Interpretability and explainability are closely related concepts, and these terms are often used interchangeably. However, several authors (e.g., [5, 6, 7, 8, 9]) have attempted to clarify their respective meanings and subtle differences. And, although there is not yet a universal consensus on what the exact technical definitions of these terms should be, a fuzzy notion of what they mean is not a subject in dispute. Specifically, an interpretable model is considered to be one which highlights some link between input and output spaces, or one whose prediction mechanism is obvious and can be understood from its internal architecture or working principle. For example, a sparse decision tree maps its inputs to outputs in an intuitive way. In contrast, explainability is generally understood to mean the ability of a model (or its surrogate) to provide explicit descriptions of what informs the decisions by means of logical rules, question-answering, or similarly rich modes of human-understandable information. Despite differences in meaning, we use these terms loosely and interchangeably in this work, as is commonly practiced in the literature."}, {"title": "1.3 Benefits of Explainable AI", "content": "Some of the most important benefits and advantages of using XAI methods in machine learning include:\n\nTransparency [10, 11, 12]: Explainable AI enhances transparency of machine learning systems, fostering understanding by human stakeholders. Understanding the operation of artificial intelligent systems allows developers and users to maximize their strengths while minimizing potential risks.\nTrust [13], [14]: Understanding the decisions of machine learning models enhances trust in these systems and increases the willingness of decision-makers to adopt them in mission-critical applications.\nFairness [15]: Explainability methods can help to uncover bias in model predictions and, thus, ensure that the resulting decisions fair to all stakeholders irrespective of gender, race, religion or any disadvantaged group.\nSafety [16], [17]: Explainable AI can reveal behaviors of machine learning systems which do not meet safety requirements, allowing these problems to be addressed prior to, or even after deployment. This ultimately leads to improved safety. Similarly, explainability can help to ensure that machine learning models adhere to ethical standards and good practices.\nAccountability [18]:When decisions of machine learning models lead to catastrophic outcomes, model explanations can help to establish the rationales of such decisions or understand why the model failed and whether it is caused by error or negligence on the part of any stakeholder. This eventually enables culpable parties to be held accountable for any losses resulting from poor design or misuse.\n(vi) Model debugging [19], [20]: Interpretability of machine learning systems can allow undesirable behavior to be detected and corrected through subsequent refinement. Such refinements can improve prediction accuracy and prevent undesirable prediction outcomes that may arise when a model learns spurious correlations from data. This problem, known as the Clever Hans [21] is encountered frequently in machine learning."}, {"title": "1.4 Important XAI Concepts", "content": "We briefly described some of the most important concepts of explainable AI in this subsection. These concepts are used extensively in the literature and throughout this paper.\n\nScope: Explanations can have a global or local scope depending on their coverage. Global explanations provide insights on a model's behavior in general whereas local explanations focus on explaining individual predictions. Some explainability methods can explain both global behavior and local instances at the same time.\nApplicability: An explainability method is described as model-agnostic if its implementation is independent of the model it explains. On the other hand, model-specific methods use techniques whose implementations are specific to the machine learning model being explained. One advantage of model-specific explainability methods is that the approach may allow details of the machine learning model to be incorporated to obtain better and tailored explanations. However, some of these techniques may interfere with the original black-box model in ways that degrade prediction performance. In general, model- agnostic explainability methods are more versatile and can be applied to any black box model without \"opening it\".\nImplementation stage: Interpretability methods can be incorporated into machine learning systems at different stages. This leads to two broad families of approach: Post-hoc interpretability and ante-hoc interpretabilityPost-hoc interpretability methods are applied after training the model. Majority of XAI techniques fall in this group. The key idea of post-hoc explainability is to explain model behavior in an unobtrusive manner. However, the approach may compromise faithfulness as the explanation may not always follow the true rationale of predictions. Ante-hoc interpretability techniques (e.g., concept bottleneck models [22] seek to endow models with explainability at training or design time by simultaneously learning features for classification as well as features that influence predictions. The approach enhances faithfulness but some of the techniques used may impose additional constraints that ultimately harm prediction accuracy."}, {"title": "1.5 Accuracy vs interpretability", "content": "Inherently or intrinsically interpretable models are relatively simple frameworks and by virtue of this property, the basis of their predictions can be intuitively understood. Intrinsic interpretability is exhibited in varying degrees across different model families, as shown in Figure 2. The most interpretable family of models include rule sets, decision trees and linear models. Owing to their simplicity they often yield modest or relatively low accuracies, particularly on complex tasks.\nOn the other hand, larger and more complex models like deep neural networks possess the capacity to capture desirable information from high dimensional training data and thus, achieve impressive accuracies but their decisions are not readily understood by stakeholders. They are therefore referred to as opaque or black box models. As Figure 2 shows, the explainability and accuracies of machine learning models generally tend to trend in opposite directions, i.e., the higher the accuracy, the less interpretable. This accuracy-explainability tradeoff, illustrated in Figure 2, is a widely studied phenomenon [23], [24], [25]) with some efforts [26], [27]aimed at carefully balancing these competing goals."}, {"title": "1.6 Motivation and work outline", "content": "Owing to the practical importance and the ever-increasing volume of work on explainable artificial intelligence, there exist many authoritative surveys (e.g., [28], [29], [30], [31], [32]) on this subject. There are also reviews dedicated to large language models [33], [34]or specialized domains like medical or healthcare applications [35], [36], [37]. In addition, another recent study [38] reviews state-of-the-art methods that utilize prior knowledge representation in various forms (including logical rules, knowledge graphs) and LLMs to improve the explainability of deep learning models. The study [38]also discusses techniques that employ prior knowledge to improve adversarial robustness and zero-shot generalization. Although there is a proliferation of well-written reviews on the subject, most of the existing works focus on more general concepts of explainability and often present a high-level description of the relevant methods. Only a few works (e.g., [29], [32]) provide adequate depth or present the specific techniques in adequate detail (this includes explaining the working principle or clearly highlighting their strengths and weaknesses). Moreover, to the best of our knowledge, no surveys \u2014not even the most recently published studies at the time of submission of this work - cover some of the emerging but hugely important developments in the field. In particular, no works emphasize the increasing use of vision-language models and large language models in enhancing and automating the explainability of other black box models. For these reasons, we are motivated to fill the gap by comprehensively presenting, and discussing the pertinent issues on these methods. We present a comprehensive review of methods covering all categories of interpretability methods. We dedicate a whole section (section 5) of this work for the approaches that use vision language models and LLMs in their interpretability pipelines. Uniquely, we also present detailed quantitative results on some of the most popular methods.\nThis paper is structured as follows: Section 1 presents general introduction to the study. Section 2 covers intrinsically interpretable frameworks, their key principles, challenges and workarounds. Section 3 covers methods of explaining general black box models. The interpretability of large language models is presented in detail in Section 4. In Section 5 we present techniques that leverage vision-language and large language models for improving and automating explainable AI. In Section 6 we discuss important issues, current state and probable and needed developments in the future. We conclude the work in Section 7 by summarizing the important points."}, {"title": "2 INHERENT INTERPRETABILITY OF WHITE-BOX MODELS", "content": "Research in explainable artificial intelligence is gaining increasing momentum owing to the importance of this field and the opportunities it presents. However, some models are already inherently interpretable by their very design. In other words, they are self-explainable in that they allow developers or users to gauge into their inner workings or decision-making rationale without making additional modifications specifically for this purpose. These inherently interpretable or white box models are widely used to solve problems in diverse machine learning tasks. Since interpretability is intrinsic to the models, the explanations are likely to be more faithful to the decision of the models. Additionally, no extra design or training time is invested in ensuring interpretability. This category of models includes linear models, generalized additive models, decision trees, Bayesian networks, etc. We describe the most popular ones here."}, {"title": "2.1 Interpretability of linear models", "content": "Linear models are a class of machine learning frameworks that capture input features describing relevant properties and accompanying coefficients or weights that specify the relative strength of each feature. For linear models, as shown by the simplified equation [1], the contribution of each feature (x1, x2, etc.) to the final prediction y can be intuitively understood by observing the corresponding coefficients (k1, k2, etc.) that weight the features or descriptors. In the equation ao is a baseline. The magnitude and sign of the coefficient respectively describe the degree and direction (i.e., whether a higher value of the descriptor lowers or amplifies the outcome) of the influence a particular feature exerts on the model's prediction.\n$$y = a_0 + k_1x_1 + k_2x_2 + \\dots + k_nx_n$$\nThus, the interpretability of this class of models is straightforward from their intrinsic components and the understanding of the input data. However, in most practical settings, interpretability of simple linear models is complicated without additional workarounds [39], [40], [41]. We briefly discuss the common scenarios in the next paragraphs.\nBy keeping all other input variables constant, regression coefficients in linear models can be analyzed to understand the contribution of a given feature to the output. Also, zero-order correlation coefficients [42] can help to reveal the importance of a given feature to the model's prediction without considering the influence of all other remaining features. This interpretation approach assumes that the input variables are uncorrelated. However, in most practical situations, the presence of feature correlations is inevitable. Feature correlation or collinearity is a property whereby a certain input feature is a function of another. Under this condition, coefficient values no longer provide adequate information about the contribution of input features to the output. Hence, collinearity may undermine the validity of interpretability methods if this behavior is not controlled for.\nTo improve the interpretability of linear regression models under in collinearity conditions, more tailored techniques are required. For instance, Commonality analysis [43], [44], [45] allows the variance in the output to be partitioned into the components associated with each predictor exclusively, and the components of the variance that is due to the interaction (or common effects) of input features.\nCommonality analysis uses ccommonality coefficients to extract information on predictors and their interactions. Negative commonality values for a model may represent suppressor relations, where the variables act to suppress confounding variance in other predictors, and thus, increase these predictors' contributions to the model's overall variance [46].\nThe presence of covariates [47], [48] may also undermine the natural interpretability of machine learning frameworks such as linear regression models. Covariates are confounding factors or variables that often mask the true effect of input features. Covariates are especially common in medical domains where extraneous factors beyond those being investigated can significantly influence the predictions of machine learning models. For example, a machine learning model designed to predict the outcome of a new treatment may be influenced by a host of unrelated factors such as a patient's diet, stress levels and even genetics. In cases like this, the observed features alone cannot explain the behavior of the model when important covariates are not adequately accounted for. Fortunately, many effective techniques [39], [49], [50], [51] are available for addressing this challenge. The popular ones include propensity score matching and treating covariates as additional input variables.\nBesides model complexity or size, the interpretability of linear models also depends on the meaningfulness of input features. However, in some application domains, raw feature values may not readily be informative from human perspective. Moreover, in applications where inputs are high-dimensional or represent low-level features, it becomes challenging for humans to understand the importance of individual input units in the model's decision. For example, in image classification, pixels are the fundamental variables that are processed to determine the outcome. Yet, individual pixels in isolation do not provide relevant clues on the model's decision. Furthermore, approaches focusing on individual pixels for interpretability would be vulnerable to noise arising from variations in pixel values that do not affect the perceptual quality of the given image.\nData misalignment: machine learning models may use different kinds of data or data from different sources during training or at test time. In such situations, the data may not be aligned in terms of their distribution or scale. This may pose significant challenges for linear models which are required to be interpretable. Intrinsic interpretability can be hindered when some skewed inputs in a certain dataset unduly influence the prediction. Similarly, data points of larger values can dominate and obscure smaller ones. These problems in machine learning are usually addressed by data preprocessing strategies such as data cleaning, normalization and standardization. Current approaches of implementing these techniques are quite laborious and inefficient, although automated data preprocessing methods are now available than can achieve impressive results with minimal effort. These automated data pre-processing methods, discussed extensively in a recent review [52], are now beginning to have a significant impact on improving the efficiency of handling undesirable data in machine learning.\nAlthough the techniques may improve prediction accuracy and enable the machine learning model to attend to all data points in an equitable manner, they are still not guaranteed to restore interpretability. This is because the preprocessing may change the semantic meaning (e.g., dimension or unit) of some inputs."}, {"title": "2.2 Interpretability of generalized additive models", "content": "Generalized additive models (GAMs) [53], [54]are a class of machine learning frameworks where the expected value of the response or prediction is a linear combination of features which are independently modeled by smooth functions, also known as shape functions. The functions describe each predictor's influence on the output, and can be non-linear, thus, allowing more intricate behaviors to be captured. In principle, of a generalized additive model can be viewed as a multiple linear model where the weighting coefficients for features x are replaced with appropriate functions f1, f2, etc., as shown in equation (2), where F(x) is the output function an \u03b2 is a bias term.\n$$F(x) = \\beta_0 + f_1(x_1) + f_2(x_2) + \\dots+f_n(x_n)$$\nThe relevant functions are learned exclusively from training data without the overly restrictive assumption of constant weighting, as is the case for conventional linear models. This flexibility enables better approximation of intricate patterns and relations that map features in the training data to outputs, even when these relationships are non-linear. \nThe interpretability of GAMs stems from the fact that the outputs of these models are derived from a linear combination of smooth functions, each of which describing a relation that projects a single input feature to the output. By plotting each relation as a function of a specific input feature, the contribution of individual features to the prediction can be visualize separately. For instance, it can be seen that the expected value of the response variable Y changes linearly in reverse correlation with the input variable x2 when all other variables are kept unchanged.\nGAMs are highly interpretable if they are adequately simple. This is usually the case when the models are sparse with a small number of shape functions which must necessarily be smooth. Regularization has been successfully employed to control either sparsity (e.g., [55], [56]), smoothness (e.g., [57]) or both qualities ([58], [59]) simultaneously. However, enforcing sparsity and smoothness constraints may compromise accuracy. In particular, higher sparsity can reduce the model's capacity to learn complex relationships in high-dimensional data while smoothness may limit the ability to model natural discontinuities in data. Additionally, conventional GAM frameworks assume that input features do not interact. This assumption leads to simple designs that are easy to explain but the enhanced interpretability may compromise accuracy. Consequently, Lou et al. propose to include pairwise interactions of features in the GA2M [60] extension. The authors show that these two-dimensional interactions substantially improve accuracies of GAMs while still maintaining interpretability.\nAnother difficulty with the interpretability of generalized additive models concerns how to handle multiclass prediction tasks. With binary classification problems, model decisions are easy to explain by analyzing the shape functions that map input features to final outputs. However, for multiclass problems, interpretability remains a challenge even for relatively simple models. Specifically, in the multiclass case, the contribution of a particular feature to the final class prediction cannot be understood simply by observing the projection from its shape function to the given class score. In fact, a feature may increase the logit for a given class but reduce the final score for the class after normalization. This can happen if the feature increases the logits for other classes by a larger amount. Thus, multiclass prediction problems pose a unique explainability challenge.\nTo address this difficulty, Zhang et al. [61] propose a post-processing technique that enforces compliance of GAMS with two key interpretability axioms: monotonicity, which imposes monotonicity constraint on predictor functions and the average class score; and smoothness, which requires shape functions to be devoid of noise-induced discontinuities. The method transforms a less interpretable multiclass prediction model into a more interpretable one in a post-hoc manner."}, {"title": "2.3 Interpretability of decision trees", "content": "Decision trees are simple but effective machine learning models that have achieved tremendous success in a wide range of tasks, mainly within classification and regression domains. They enjoy a long-standing popularity dating back many decades [62], [63]. Besides their performance, decision trees are also known for their transparency. That is, the internal structures of these models directly provide insights on their predictions. Owing to these qualities, there is significant research interest in interpretability-related properties of decision trees such as model depth, number of nodes, and sparsity.\nDecision trees are organized in hierarchical tree structures with nodes implementing tests of logical conditions, branches denoting the results of these tests, and leaf nodes denoting final labels or predictions that satisfy the evaluated conditions leading to these nodes. A given outcome or prediction of a decision tree can be explained simply by tracing the decision path to the point where the prediction is made.\nTraditional decision tree construction methods employ splitting strategies that rely on greedy algorithms. Popular among these techniques are CART [64] and C4.5 [65], [66] frameworks which construct trees in a top-down manner do not provide effective mechanisms to deal with errors at top hierarchies when they are observed later in the construction process. Furthermore, the approaches often lead to only locally optimal trees as these methods do not usually consider future states, and hence struggle to ensure global optimality.\nOne of the most significant limitations of decision trees is their poor generalization ability. Decision trees tend to easily overfit their training data and thus experience a significant performance drop when tested on new data. A model is said to overfit if it learns to produce sufficiently accurate predictions on training data but fails to perform adequately on unseen data. For decision trees, this can happen when the tree is larger (i.e., deeper tree with more branches) and more complex, and thus has more capacity and sophistication to capture more detailed and intricate characteristics (sometimes caused by noise or outliers) specific to the training data rather than general attributes. Some complex trees are constructed as an ensemble of trees involving a large number of subtrees. Conversely, overly simple decision trees do not possess the capacity to capture sufficient number of relevant features from data, and are therefore neither able to perform well on the training set nor generalize to unseen data.\nIn practice, there are techniques available for mitigating both overfitting (e.g., [67]) and underfitting (e.g., [68]) of decision trees. However, considering all factors, larger trees generally achieve better accuracies than simpler or sparse decision trees, especially on more complex tasks. Sparsity, on the hand, favors transparency and is therefore regarded as a desirable property from explainability point of view. Consequently, research interest in sparse decision trees remains high.\nOptimal Sparse Decision Trees (OSDT) [69] and Generalized and Scalable Optimal Sparse Decision Trees (GOSDT) [70] are state-of-the-art methods of achieving sparsity without severely compromising performance. In particular, GOSDT generates sparse and globally optimal decision trees on multiple objectives. The method employs dynamic programming paradigm that enables efficient exploration of the search space of all possible subtrees. GOSDT also addresses the problem of imbalanced data. Although GOSDT is relatively fast, it still takes a significant amount of training time when dealing with high dimensional data. McTavish et al. [71] propose to reduce the runtime by using guessing technique to mimic a high-performing black-box reference model.\nAlthough a high proportion of studies on the interpretability of decision trees tend to focus on improving their white-box characteristics, some studies attempt to engineer post-hoc methods to interpret opaque trees. For instance, TreeExplainer [72] extracts local explanations from individual predictions of tree models using S HAP scores [73], and then combines multiple local explanations to obtain global interpretability. The method also detects feature interactions using the Shapley interaction index. In contrast to designing decision trees for inherent interpretability, methods based on post-hoc explainability can allow developers to fully focus on performance related metrics. This will reduce the need for increasing model sparsity, which largely harms performance. However, potential compromise in faithfulness a price to pay for adopting post-hoc explanations in favor of inherent explainability.\nPopular rule-based interpretability approaches like rule sets [74], [75] are constructed from simple if-then rules that are evaluated to arrive at a decision or prediction. In classification problems, rule-based methods can be used to learn discriminative rules that relate input features to category labels. Since symbolic rules are easy to understand, methods that utilize these constructs are inherently explainable. Their main limitation is their inability to handle more complex problems that involve high-dimensional data like images. In these domains, rule extraction methods (e.g., extraction methods (e.g., [76]) can be used to mine symbolic rules from a black box model like neural network that is pre-trained on the relevant data. However, the interpretability of extracted rules may be limited especially for large and complex networks."}, {"title": "2.4 Other inherently explainable methods", "content": null}, {"title": "2.4.1 Neural additive models", "content": "Neural Additive Models (NAMs) [77], [78] represent another class of intrinsically interpretable models capable of higher performance on complex tasks compared to other intrinsically interpretable models like linear models and GAMs. A Neural Additive Model is essentially a neural network which is composed of sub-networks each of which is responsible for handling a single input feature. The interpretability of NAMs stems from the fact that constituent sub-networks are simple and additive, i.e., the overall model is a linear combination of these components. Also, owing to the power of neural networks. NAMs have a performance advantage over other inherently interpretable methods like regression models, shallow decision trees and sparse generalized additive models."}, {"title": "2.4.2 Scorecards", "content": "Another widely employed intrinsically interpretable method utilizes scorecards [79], [80] for decision-making. Scorecards are linear prediction models that assign numeric weights or points to features to signify their relative importance to the final score. Features can be given positive or negative points to reflect the direction in which they influence the overall score. For some predictors, scorecards typically use binary features to denote the presence or absence of a given feature. Scorecards are mostly suitable for tasks where predictors have a clear and unambiguous influence on the outcome. Application domains include handling electronic health data and data for credit risk assessment."}, {"title": "3 Explainability of black box models", "content": "In this section, we detail the methods used to explain black box models. Feature attribution methods are by far the commonest group of approaches. We review various sub-categories of this family of approaches, as well as other popular interpretability methods."}, {"title": "3.1 Overview of Feature attribution methods", "content": "Feature attribution methods of interpretability cover a broad range of techniques that use various means to show the contribution of input features to the given prediction by the model. The explanations can be shown in various ways, including heatmaps, bar and scatter plots. This class of approaches include interpretability techniques that can be applied in domains that use images, text and tabular data. Some of the most popular attribution methods are gradient-based approaches and model-agnostic methods like SHAP [73] and LIME [81]."}, {"title": "3.2 Gradient-based methods", "content": "Gradient-based interpretability frameworks are a class of feature attribution methods that leverage the gradient computation in neural networks to determine feature importance. These methods attempt to explain the prediction of AI models by highlighting, in the form of a heatmap, regions in an image that are considered to have contributed to the given decision. Some methods in this category (e.g., Score-CAM [82]) do not technically use gradient information per se but are nonetheless classified among these methods owing to their strict membership of a broad family of differentiable approaches. Popular gradient-based interpretability methods are described in the next subsections."}, {"title": "3.2.1 Gradient saliency", "content": "Saliency visualization methods achieve interpretability by providing simple visualization cues that highlight relevant regions of the sample of interest. Gradient saliency, introduced by Simonyan et al. [83] for explaining image classification decisions, generates explanations by constructing visual saliency maps from computed gradients of a neural network. The method is one of the earliest examples of gradient-based frameworks that have remain relevant to date. The technique uses the partial derivative or gradient (through back-propagation) of the final prediction score with respect to the input image. These gradients are used to construct the saliency or attribution map that highlights the relative importance of each pixel in the input. The approach is relatively simple and computationally efficient.\nDespite these strengths, vanilla gradients violate an important sensitivity axiom [84] causing the saliency maps to show excessive noise. The sensitivity axiom requires that an attribution method should highlight a feature as relevant if a model produces different predictions for a given input and a baseline which differ only by this feature. This problem is partly caused by the gradient saturation problem, which originates from the backpropagation of gradients through several layers of a neural network with non-linear compression functions. Specifically, saturation can result from the computations at the activation functions like sigmoid or the negative input region of the Rectified Linear Unit (ReLU) [85]. The problem causes some of the gradients with respect to the input to diminish and subsequently results in these smaller gradients being overshadowed and obfuscated by larger ones which may not be relevant to the classification task, and thus, manifesting as noise in the saliency map."}, {"title": "3.2.2 Enhanced Gradient-based Interpretability", "content": "Gradient \u00d7 input: In order to improve the invariance of common gradient-based methods to input shifts, the gradient \u00d7 input technique [86], [87] utilizes the product of the gradient and input for relevancy attribution. This technique prevents the tendency of the computed saliency scores to be undesirably altered by inconsequential changes (such as small constant shifts) in the input which do not usually affect the prediction itself.\nSmoothGrad: In SmoothGrad [89], Smilkov et al. propose to reduce visual noise associated with gradients by first generating a new set of images through the addition of noise to the original image and then averaging the saliency maps for all images to obtain the final heatmap. The intuition behind the technique is that the computed derivative is characterized by noisy fluctuations, but averaging over slightly different maps helps to cancel out this noise. However, SmoothGrad requires a couple of passes through the DNN model, resulting in extra computational overhead.\nOther modifications of this basic approach have been proposed. For instance, NoiseGrad [90] applies noise to the network weights instead of the input space and VarGrad [91] uses the variance of the attribution output instead of the average."}, {"title": "3.2.3 Backpropagation-based methods", "content": "Backpropagation-based methods generate relevancy maps by modifying the backpropagation algorithm (specifically, at the ReLU activation function) and using this new rule to backpropagate the final output through successive layers to the input in order to obtain the importance of each input feature to the prediction.\nDeconvolutional Networks: The DeConvolutional Network or DeConvNet [92]framework, proposed by Zeiler and Fergus, selectively projects activations (i.e., all but the target activation is set to zero) backwards to reconstruct the input so as to determine which pixels contributed to the given activations. To achieve this objective, a forward pass is first required to identify so-called switches, or positions of maximum values in the max pooling regions. The switches help to place features in their correct locations in the reconstructed signals when they are passed through ReLU activation functions to obtain positive-only outputs. One fundamental difference between DeConvNets and gradient saliency [83] lies in how they handle information at ReLUs. In gradient saliency, during the backpropagation of scores, the gradient passing through a ReLU in the backward pass is masked out (i.e., set to zero) if the information flowing through the same ReLU in the forward pass is negative. However, during the backpropagation of scores through DeConvNets, the output from a ReLU is zero only when the current input is negative (i.e., regardless of the value in the forward pass). The technique is also sometimes referred to as occlusion or perturbation-based method because it allows attribution to be computed by systematically occluding (replacing appropriate patches with noise, zero or random values) different patches of the input image and evaluating the resulting reduction in prediction probability score. When attributing feature importance, the occluded patches that result in significant decrease in the class score are considered influential and are thus, shown in the visualization maps. An illustration of interpretability by the input occlusion method is depicted in Figure 9.\nGuided Backpropagation: With guided Backpropagation [93], Springenberg et al. replace max pooling layers with large-stride convolutions to achieve dimensionality reduction. This modification obviates the need for the forward pass step to estimate switch variables, leading to better reconstruction results and improved visualization quality of the final heatmap. Another fundamental difference is in the propagation rule used by these methods. Specifically, while DeConvNet uses the back-projecting RELU to set all negative entries of the top gradients to zero, Guided Backpropagation combines the information processing rules at ReLUs adopted by Gradient Saliency and DeConvNets. That is, the method masks out entries to ReLUs for which either the gradients in the backward pass or the data in the forward pass are negative. By this design, Guided Backpropagation achieves higher interpretability quality with sharper visualizations.\nThe main strengths of DeConvNet and Guided Backpropagation techniques lie in their relatively high-resolution heatmaps. However, these methods are not class discriminative. Class-discriminative behavior (illustrated in Figure 10) describes the ability of interpretability methods to correctly highlight target classes in images containing multiple classes. Moreover, owing to the masking out of negative entries, neither method is suitable for capturing features that contribute negatively to the prediction. Furthermore, like gradient saliency, these backpropagation-based methods do not satisfy the sensitivity axiom owing to the propagation of gradients through deep neural networks. This problem ultimately leads to degraded performance."}, {"title": "3.2.4 Activation propagation-based methods", "content": "This class of methods compute attributions by assuming that the activation of a given neuron in the final layer of the network represents the relevance of the neuron to the prediction. These neuron activations are then propagated backward to the input. The various frameworks (e.g.", "94": "DTD [95", "96": "differ by their relevancy propagation details. In general", "Propagation": "Layer-wise Relevance Propagation (LRP) [94"}, {"94": "also show that LRP can be approximated by Taylor decomposition.\nAlthough LRP was originally proposed to enhance the interpretability of image classification decisions, the method has since been extended to solve interpretability problems in domains that use other input data types including audio (e.g., [97"}]}