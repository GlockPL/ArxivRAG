{"title": "ChaI-TeA: A Benchmark for Evaluating Autocompletion of Interactions with LLM-based Chatbots", "authors": ["Shani Goren", "Oren Kalinsky", "Tomer Stav", "Yuri Rapoport", "Yaron Fairstein", "Ram Yazdy", "Nachshon Cohen", "Alexander Libov", "Guy Kushilevitz"], "abstract": "The rise of LLMs has deflected a growing portion of human-computer interactions towards LLM-based chatbots. The remarkable abilities of these models allow users to interact using long, diverse natural language text covering a wide range of topics and styles. Phrasing these messages is a time and effort consuming task, calling for an autocomplete solution to assist users. We present Chal-TeA: Chat Interaction Autocomplete; An autcomplete evaluation framework for LLM-based chatbot interactions. The framework includes a formal definition of the task, coupled with suitable datasets and metrics. We use the framework to evaluate 9 models on the defined auto completion task, finding that while current off-the-shelf models perform fairly, there is still much room for improvement, mainly in ranking of the generated suggestions. We provide insights for practitioners working on this task and open new research directions for researchers in the field. We release our framework, to serve as a foundation for future research.", "sections": [{"title": "1 Introduction", "content": "Large Language Models (LLMs) have revolutionized many NLP applications (Brown et al., 2020). A prominent example is automatic chatbots; what used to be confined, topic-specific applications often requiring the user to use restricted language or choose from a closed list of interaction options, have been transformed. These applications, powered by LLMs, are now one-stop-shops successfully communicating in unbounded natural language while acting as experts on a wide variety of topics (Achiam et al., 2023; Anil et al., 2023). Due to their remarkable abilities, LLM-based chatbots differ significantly from prior human-computer communication methods. Interactions with these chatbots are usually long, unique and cover a large range of topics and language styles, using unstructured natural language. Due to this nature, users invest much time and thought in communicating their needs to the chatbot, calling for solutions to reduce their effort (Lehmann and Buschek, 2021). AutoComplete (AC) methods have been shown to be effective in saving users' time and reducing their cognitive load in many different use-cases, suggesting that such a solution might be of value for the LLM-chatbot interaction use-case also. The popular query autocomplete scenario (Cai et al., 2016) focuses on search queries. Classic solutions often rely on recurrence, making them irrelevant for the long unique natural language text found in chatbot interactions (Lehmann and Buschek, 2021). Later solutions include generative models (Sordoni et al., 2015; Park and Chiba, 2017), but still focus on short semi-structured queries. Code autocomplete (Liang et al., 2024) deals with structured language, and often relies on the ability to run the code and check its output in order to evaluate solutions. Lastly, email (human-human) interactions (Chen et al., 2019), which bear a closer resemblance to human-chatbot interactions due to their natural language communication, also differ in several key aspects. These include the number of participants and their roles, the more formal writing style of emails and the nature of the topics discussed. More Generally speaking, human-human textual interactions (e.g. emails, but also texts from other kinds of messaging platforms) differ from human-chatbot interactions in the fact that human-chatbot interactions involve a human and a model-based assistant, making them more instructional and knowledge-seeking. For example, the prompts \u201cGive me the latest updates of the war in Ukraine as of the 31st of January.\" and \"Write a web scraping program in python capable of ...\" are taken from the OASST dataset used in this work to demonstrate typical examples for a human-chatbot interaction, which are highly unlikely to be found in a human-human messaging platform.\nIn this paper, we introduce the task of autocompleting user interactions with LLM-based chatbots. We present ChaI-TeA: Chat Interaction Autocomplete; A framework for evaluating autocomplete solutions for LLM-based chatbot interactions. It includes a formal definition of the task, suitable datasets tailored for auto complete, suitable metrics, and baseline results. We go on to highlight some valuable insights. First, we explore how performance can be traded off for lower latency, a key factor in autocomplete solutions. Second, we show that models can exploit distant history to suggest completions. Third, it is beneficial to enable completions of various lengths (as opposed to only single words or full turns). We highlight a key factor in improving these solutions: we find that models tend to generate completion suggestions well, but are not as good at ranking these generated suggestions. Given that users can ingest a small amount of suggestions at each turn, ranking is an important component in an offered solution. Therefore, we advocate for future research in the field to focus on this aspect."}, {"title": "2 Task Definition", "content": "The chatbot interaction completion task focuses on completing user turns in user-chatbot interactions. Similarly to (Chitnis et al., 2024), we model it as a sequential task; completions are suggested at each typing step (i.e., after a user types a character). Formally, at each step t, an autocomplete solution (denoted by AC) is given a context C containing all previous conversation turns, originating from both the user and the chatbot, and the prefix of the current user turn denoted as pt. The autocomplete solution should then return a set of k completions, Ct1,..., Ctk, possibly of varying lengths.\nEach completion step can be described as:\nAC(C, pt) = {Ct1, Ct2, ..., Ctk}\nAfter receiving the set of completions, the user can either accept a completion or continue typing. If a completion ct\u2081 is selected, the prefix is updated such that pt+1 = pt + Ct\u2081. Then, whether the user selected a completion or continued typing, a new completion step is initiated, until reaching the end of the user's turn. A single completion step is illustrated in Figure 1, and full turns completions can be found in the Appendix in Table 6."}, {"title": "3 Experimentation", "content": "3.1 Datasets\nOpen Assistant (OASST) (K\u00f6pf et al., 2024) is a human-annotated assistant conversation corpus. ShareGPT contains user-LLM-chatbots conversations collected by the ShareGPT API.\nTo curate the data for our task, we take all English conversations and for each user-turn extract all possible prefixes and pair each with the entire conversation history up to that point as its context. The suffix of the original prompt is the ground truth completion.\n3.2 Metrics\nAs solutions are allowed to propose k completions at each step, metrics evaluate the performance taking k into account, denoted as @k.\nAs we are looking to form a benchmark, we turn to metrics that can be computed offline. We remark that ideally, we would also like to measure the user's saved time or reduced cognitive load but doing so would require running some experiment or user study for each new proposed solution.\nFor simplicity, we simulate acceptances (i.e., is one of the proposed completions accepted by the user?) using exact match comparison to the ground truth user turn.\nSaved typing. Inspired by code completion metrics (Jiang et al., 2024), our goal is to save the user typing effort. Therefore, we seek a metric that quantifies the portion of the text completed by the AC solution. While simply dividing the length of the accepted text with the length of the full turn would achieve this, this metric would not consider the number of acceptances needed to generate the accepted text\u00b3. To mitigate this issue, we propose the following metric:\nsaved@k = \\frac{len(accepted\\_text) - #acceptances}{len(full\\_turn) - 1}\nwhere len(x) is the number of characters in string x. No acceptances during the user's turn lead to a score of 0% while a single acceptance completing the full turn leads to a score of 100%.\nLatency. Latency is a critical factor that cannot be overlooked when assessing AC solutions. Even if the completions are perfect, they are rendered useless if the user proceeds to type before receiving the suggestions. We report the mean and the 90th percentile (p90) of the inference time.\n3.3 Autocomplete Solutions\nAs our task resembles the language modeling task, a called-for solution is utilizing LMs. This allows us to experiment with a wide variety of models ranging in size, latency and quality. Our evaluation encompassed a diverse set of popular LMs: Mistral-7B (Jiang et al., 2023), Gemma-7B (Mesnard et al., 2024), Phi-3-mini (Abdin et al., 2024), GPT-2-XL (Radford et al., 2018), Mamba (Gu and Dao, 2023), and SmolLm. We also evaluate instruct-tuned variants of these models whenever one is available (Zephyr, Gemma). Inference was performed on a single NVIDIA A10G GPU, taking 150 hours in total.\nTo generate k completions from the LMs, we adopt the following procedure: we provide the model with the full context concatenated with the prompt prefix. We then use the model to generate nc completions sampled with temperature 1.0, stopping when reaching EOS or after nt tokens. Since completions can vary in length, each word-prefix of a completion can also be considered as a standalone completion. Hence, this process generates up to nent completion candidates. Finally, we choose the k suggestions to present to the user by ranking the completions based on their perplexity score, as computed using the LM probabilities7.\n3.4 Initiating Suggestion Generation\nSuggesting completions after each character has some downsides compared to suggesting only at an end of a word. First, as the average length of an English word is more than 4 characters, the computational cost more than quadruples. Second, it has been shown that when typing, users tend to pause much longer between words than between same-word characters (Conijn, 2020). This allows more room to suggest completions between words. Third, LLMs are known to under perform on character level tasks (Shin and Kaneko, 2024)9.\nTo compare how frequently character level suggestions are accepted compared to word level suggestions, we also tracked acceptance rate: the percentage of completion steps that ended in an acceptance.\nResults on the OpenAssistant validation set (nc = 5,nt = 20) show that mid-word suggestions degrade the acceptance rate by ~ 60% while only slightly improving saved@k by ~ 3.2%. Interestingly, Mamba, which uses a character-level tokenizer, behaves similarly to the other models. We conclude that mid-word suggestions are rarely accepted, and do not justify their drawbacks. Additional efforts are needed to make mid-word suggestions effective, which we leave for future work. For the remainder of this paper, completion suggestions are provided only at the end of a word.\nThe perplexity of a sequence W = w1, w2,..., wn:\nPPL(W) = exp{- \\frac{1}{n} \\sum_{i=1}^{n} log p(w_i|w_1,..., w_{i-1})}"}, {"title": "3.5 Benchmarking ChaI-TeA", "content": "We benchmark all models described in Section 3.3 on both curated datasets (Section 3.1). Results on OASST for varying k values are shown in Figure 2. We consider k values up to 100, which encompasses all generated completions (at most, nc \u00d7 nt), to show the potential given a perfect ranking solution. While current models are able to perform fairly on this task \u2013 saving the user the typing of up to 45% of the characters \u2013 there is still much room for improvement. There is a noticeably large performance gap between small, realistic, k values and larger values, suggesting that while in many cases models are able to generate the correct completion, their ranking of completions is far from perfect. In line with prior work (Manakul et al., 2023; Ren et al., 2023; Fadeeva et al., 2024), we conclude that perplexity is insufficient for confidence ranking. Full benchmark results on OASST and ShareGPT can be found in Appendix B.\nFinally, we observe that further improvement can be gained by fine-tuning models on the AC task. Detailed results are presented in Appendix C."}, {"title": "4 Further Analysis", "content": "Latency-Performance Trade-Off. Given the practical importance of latency in AC solutions, we explore how performance can be traded off for reduced latency. To illustrate this trade-off, we varied the previously mentioned hyperparameters nc and nt, as well as the context length given to the model. We capped the conversation history concatenated with the turn prefix at different lengths, to determine whether giving the model access to the entire conversation context is 1. helpful and 2. worthy of the extra latency costs.\nSuggestions are offered between words, meaning that once the user begins typing the next word they become irrelevant. Hence, we find it appropriate to use the mean time between typed words \u2013 718 ms, reported by (Conijn, 2020) \u2013 as a benchmark.\nResults per latency budget, presented in Table 2, show that it is preferable to generate more completions, while reducing the number of generated tokens and context length. Also, additional context is beneficial, suggesting that information useful for autocomplete can sometimes be found far before the end of the prefix.\nVarying completion lengths. A common practice for autocomplete practitioners wanting to simplify their methods is restricting completions to single words. The other end of this scale, also widely used, is allowing only full completions- completing until the end of the query/function/sentence. To this end, we compare completions of varying lengths to single word and full sentence completions to check whether allowing any-length completions improves quality. Average results across all models are presented In Table 3. saved@k metric improves for k = 100 when allowing suggestions of varying length, indicating this can improve the user's typing experience. The fact that this is not the case for the lower k values indicates, once more, that the ranking method we use (the model's perplexity) is far from ideal.\nCharacteristics of completions. We observe that different models are able to generate diverse"}, {"title": "5 Conclusions", "content": "In this work, we showcase the task of autocompleting user interactions with LLM-based chat-bots. We formally define the task and design an evaluation framework, and use it to test 11 different models. Results show that while LMs are able to perform fairly, there is room for a tailored solution to improve upon them, especially in the ranking of completion candidates. We show that models can exploit distant history, that enabling completions of different lengths is beneficial and that reducing latency for this task should be done by reducing context length and length of completions as opposed to generating less completions. We hope our framework will encourage further work in this area, which we believe holds great potential value for users across various LLM chat-bot applications."}, {"title": "Limitations", "content": "Exact Match. We use exact-match to simulate acceptances. While this is standard practice in autocomplete works, it may not fully represent real-world scenarios in which a user might accept a completion even if it's not the exact wording they were thinking of. Although some works use generation metrics like BLEU or ROUGE to simulate full sentence acceptances, these metrics fail to capture semantic similarity between partial completion suggestions and ground truths, making them a problematic solution because even a very high score may not represent an accept and vice versa. Moreover, it is a non-trivial task to infer what a user will accept after semantic partial matches since the text diverged from the ground truth. We evaluated using the Claude3-Sonnet model to determine whether a suggestion should be accepted or not and discovered this to be a very challenging task. Thus, we leave it for future work.\nDatasets. Both datasets used have one significant limitation: they where collected without the presence of an autocomplete solution. It is possible that users alter their behavior when completion suggestions are presented to them. If this is true, it will not be reflected in our framework. We note that taking this into account is far from trivial, because even if data is collected in the presence of some autocomplete solution, this data will be biased towards the specific solution used in the collection process, giving an unfair advantage when judging solutions similar to it.\nWord-level completions. Most of the results presented in this paper assume completions are only suggested at the end of words. While this is possible to achieve in a real-world scenario, it would require some component assessing whether an end of a word is reached or not. This solution will have to run online, and in short latency. Since our experiments are run offline, the full turn was available for us and we could simply check when the end of a word was reached."}]}